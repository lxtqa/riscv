From 277fdd1de7110a889f6fd4c2da2ddfdf2f28416f Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Samuel=20Gro=C3=9F?= <saelo@chromium.org>
Date: Wed, 15 Dec 2021 14:39:15 +0100
Subject: [PATCH] V8 Sandbox rebranding
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This CL renames a number of things related to the V8 sandbox.
Mainly, what used to be under V8_HEAP_SANDBOX is now under
V8_SANDBOXED_EXTERNAL_POINTERS, while the previous V8 VirtualMemoryCage
is now simply the V8 Sandbox:

V8_VIRTUAL_MEMORY_CAGE => V8_SANDBOX
V8_HEAP_SANDBOX => V8_SANDBOXED_EXTERNAL_POINTERS
V8_CAGED_POINTERS => V8_SANDBOXED_POINTERS
V8VirtualMemoryCage => Sandbox
CagedPointer => SandboxedPointer
fake cage => partially reserved sandbox
src/security => src/sandbox

This naming scheme should simplify things: the sandbox is now the large
region of virtual address space inside which V8 mainly operates and
which should be considered untrusted. Mechanisms like sandboxed pointers
are then used to attempt to prevent escapes from the sandbox (i.e.
corruption of memory outside of it). Furthermore, the new naming scheme
avoids the confusion with the various other "cages" in V8, in
particular, the VirtualMemoryCage class, by dropping that name entirely.

Future sandbox features are developed under their own V8_SANDBOX_X flag,
and will, once final, be merged into V8_SANDBOX. Current future features
are sandboxed external pointers (using the external pointer table), and
sandboxed pointers (pointers guaranteed to point into the sandbox, e.g.
because they are encoded as offsets). This CL then also introduces a new
build flag, v8_enable_sandbox_future, which enables all future features.

Bug: v8:10391
Change-Id: I5174ea8f5ab40fb96a04af10853da735ad775c96
Cq-Include-Trybots: luci.v8.try:v8_linux64_heap_sandbox_dbg_ng,v8_linux_arm64_sim_heap_sandbox_dbg_ng
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3322981
Reviewed-by: Hannes Payer <hpayer@chromium.org>
Reviewed-by: Igor Sheludko <ishell@chromium.org>
Reviewed-by: Michael Achenbach <machenbach@chromium.org>
Reviewed-by: Toon Verwaest <verwaest@chromium.org>
Commit-Queue: Samuel Gro√ü <saelo@chromium.org>
Cr-Commit-Position: refs/heads/main@{#78384}
---
 BUILD.bazel                                   |  19 +-
 BUILD.gn                                      |  87 ++++---
 bazel/defs.bzl                                |   2 +-
 include/v8-array-buffer.h                     |   4 +-
 include/v8-context.h                          |   4 +-
 include/v8-initialization.h                   |  76 +++---
 include/v8-internal.h                         | 172 +++++++------
 include/v8-object.h                           |   4 +-
 include/v8-primitive.h                        |   4 +-
 include/v8config.h                            |  14 +-
 infra/mb/mb_config.pyl                        |   8 +-
 src/api/api.cc                                |  88 +++----
 src/base/bounded-page-allocator.h             |   1 -
 src/base/platform/platform.h                  |   8 +-
 src/builtins/builtins-typed-array-gen.cc      |   4 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  26 +-
 src/codegen/arm64/macro-assembler-arm64.h     |  26 +-
 src/codegen/code-stub-assembler.cc            |  62 ++---
 src/codegen/code-stub-assembler.h             |  31 +--
 src/codegen/external-reference.cc             |  20 +-
 src/codegen/external-reference.h              |  40 ++--
 src/codegen/machine-type.cc                   |   4 +-
 src/codegen/machine-type.h                    |  14 +-
 src/codegen/tnode.h                           |   6 +-
 src/codegen/x64/macro-assembler-x64.cc        |  30 +--
 src/codegen/x64/macro-assembler-x64.h         |  22 +-
 .../lazy-compile-dispatcher.cc                |   2 +-
 src/compiler/access-builder.cc                |  18 +-
 .../backend/arm/instruction-selector-arm.cc   |   4 +-
 .../backend/arm64/code-generator-arm64.cc     |   9 +-
 .../backend/arm64/instruction-codes-arm64.h   |   4 +-
 .../arm64/instruction-scheduler-arm64.cc      |   4 +-
 .../arm64/instruction-selector-arm64.cc       |   8 +-
 .../backend/ia32/instruction-selector-ia32.cc |   4 +-
 src/compiler/backend/instruction.cc           |   4 +-
 src/compiler/backend/instruction.h            |   2 +-
 .../loong64/instruction-selector-loong64.cc   |   4 +-
 .../backend/mips/instruction-selector-mips.cc |   8 +-
 .../mips64/instruction-selector-mips64.cc     |   8 +-
 .../backend/ppc/instruction-selector-ppc.cc   |   4 +-
 src/compiler/backend/register-allocation.h    |   2 +-
 .../riscv64/instruction-selector-riscv64.cc   |   8 +-
 .../backend/s390/instruction-selector-s390.cc |   4 +-
 .../backend/x64/code-generator-x64.cc         |  24 +-
 .../backend/x64/instruction-codes-x64.h       |   4 +-
 .../backend/x64/instruction-scheduler-x64.cc  |   4 +-
 .../backend/x64/instruction-selector-x64.cc   |   8 +-
 .../js-native-context-specialization.cc       |   2 +-
 src/compiler/load-elimination.cc              |   6 +-
 src/compiler/machine-graph-verifier.cc        |   2 +-
 src/compiler/machine-operator.cc              |   4 +-
 src/compiler/memory-lowering.cc               |  20 +-
 src/compiler/memory-optimizer.cc              |   7 +-
 src/compiler/representation-change.cc         |   6 +-
 src/compiler/simplified-lowering.cc           |   4 +-
 src/compiler/simplified-operator.h            |   6 +-
 src/compiler/types.h                          |   6 +-
 src/compiler/wasm-compiler.cc                 |  10 +-
 src/d8/d8.cc                                  |   6 +-
 src/diagnostics/objects-printer.cc            |   4 +-
 src/execution/isolate-data.h                  |   8 +-
 src/execution/isolate-utils-inl.h             |   6 +-
 src/execution/isolate.cc                      |   2 +-
 src/execution/isolate.h                       |   6 +-
 src/flags/flag-definitions.h                  |  18 +-
 src/handles/global-handles.cc                 |   2 +-
 src/heap/factory.h                            |   6 +-
 src/heap/local-factory.h                      |   8 +-
 src/heap/mark-compact.cc                      |   2 +-
 src/init/bootstrapper.cc                      |   6 +-
 src/init/isolate-allocator.cc                 |  29 +--
 src/init/v8.cc                                |  24 +-
 src/init/v8.h                                 |   4 +-
 src/objects/backing-store.cc                  |  57 ++---
 src/objects/code-inl.h                        |   6 +-
 src/objects/contexts-inl.h                    |   2 +-
 src/objects/embedder-data-slot-inl.h          |  14 +-
 src/objects/embedder-data-slot.h              |  23 +-
 src/objects/foreign-inl.h                     |   4 +-
 src/objects/js-array-buffer-inl.h             |  12 +-
 src/objects/objects-inl.h                     |  25 +-
 src/objects/objects.h                         |  15 +-
 src/objects/string-inl.h                      |   6 +-
 src/objects/turbofan-types.tq                 |   2 +-
 src/{security => sandbox}/OWNERS              |   0
 .../external-pointer-inl.h                    |  22 +-
 .../external-pointer-table.cc                 |   2 +-
 .../external-pointer-table.h                  |   8 +-
 src/{security => sandbox}/external-pointer.h  |   6 +-
 .../vm-cage.cc => sandbox/sandbox.cc}         | 225 +++++++++---------
 src/sandbox/sandbox.h                         | 204 ++++++++++++++++
 src/sandbox/sandboxed-pointer-inl.h           |  48 ++++
 src/sandbox/sandboxed-pointer.h               |  23 ++
 src/security/caged-pointer-inl.h              |  53 -----
 src/security/caged-pointer.h                  |  23 --
 src/security/vm-cage.h                        | 205 ----------------
 src/snapshot/deserializer.cc                  |  12 +-
 src/snapshot/mksnapshot.cc                    |   6 +-
 src/snapshot/serializer.cc                    |   8 +-
 src/utils/allocation.cc                       |  12 +-
 src/utils/allocation.h                        |  23 +-
 src/wasm/baseline/liftoff-compiler.cc         |   2 +-
 src/wasm/c-api.cc                             |   6 +-
 src/wasm/wasm-objects-inl.h                   |   2 +-
 src/wasm/wasm-objects.tq                      |   6 +-
 test/cctest/cctest.cc                         |   4 +-
 test/cctest/test-api.cc                       |   4 +-
 test/fuzzer/fuzzer-support.cc                 |   6 +-
 test/inspector/inspector-test.cc              |   6 +-
 test/mjsunit/mjsunit.status                   |   4 +-
 test/mkgrokdump/mkgrokdump.cc                 |   6 +-
 test/unittests/BUILD.gn                       |   2 +-
 test/unittests/heap/unmapper-unittest.cc      |  25 +-
 test/unittests/run-all-unittests.cc           |   4 +-
 test/unittests/sandbox/sandbox-unittest.cc    | 155 ++++++++++++
 .../security/virtual-memory-cage-unittest.cc  | 152 ------------
 tools/debug_helper/get-object-properties.cc   |   4 +-
 tools/testrunner/base_runner.py               |   8 +-
 tools/unittests/run_tests_test.py             |   2 +-
 .../testdata/testroot1/v8_build_config.json   |   2 +-
 .../testdata/testroot2/v8_build_config.json   |   2 +-
 .../testdata/testroot3/v8_build_config.json   |   2 +-
 122 files changed, 1289 insertions(+), 1278 deletions(-)
 rename src/{security => sandbox}/OWNERS (100%)
 rename src/{security => sandbox}/external-pointer-inl.h (90%)
 rename src/{security => sandbox}/external-pointer-table.cc (94%)
 rename src/{security => sandbox}/external-pointer-table.h (90%)
 rename src/{security => sandbox}/external-pointer.h (93%)
 rename src/{security/vm-cage.cc => sandbox/sandbox.cc} (50%)
 create mode 100644 src/sandbox/sandbox.h
 create mode 100644 src/sandbox/sandboxed-pointer-inl.h
 create mode 100644 src/sandbox/sandboxed-pointer.h
 delete mode 100644 src/security/caged-pointer-inl.h
 delete mode 100644 src/security/caged-pointer.h
 delete mode 100644 src/security/vm-cage.h
 create mode 100644 test/unittests/sandbox/sandbox-unittest.cc
 delete mode 100644 test/unittests/security/virtual-memory-cage-unittest.cc

diff --git a/BUILD.bazel b/BUILD.bazel
index f05e11eeac..3d37f45ced 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -71,14 +71,13 @@ config_setting(
 # v8_verify_torque_generation_invariance
 # v8_enable_snapshot_compression
 # v8_control_flow_integrity
-# v8_enable_virtual_memory_cage
+# v8_enable_sandbox
 # cppgc_enable_caged_heap
 # cppgc_enable_check_assignments_in_prefinalizers
 # cppgc_enable_object_names
 # cppgc_enable_verify_heap
 # cppgc_enable_young_generation
 # v8_enable_zone_compression
-# v8_enable_heap_sandbox
 # v8_enable_precise_zone_stats
 # v8_enable_swiss_name_dictionary
 # v8_generate_external_defines_header
@@ -1918,14 +1917,14 @@ filegroup(
         "src/runtime/runtime-weak-refs.cc",
         "src/runtime/runtime.cc",
         "src/runtime/runtime.h",
-        "src/security/external-pointer-table.cc",
-        "src/security/vm-cage.cc",
-        "src/security/caged-pointer-inl.h",
-        "src/security/caged-pointer.h",
-        "src/security/external-pointer-inl.h",
-        "src/security/external-pointer-table.h",
-        "src/security/external-pointer.h",
-        "src/security/vm-cage.h",
+        "src/sandbox/external-pointer-inl.h",
+        "src/sandbox/external-pointer.h",
+        "src/sandbox/external-pointer-table.cc",
+        "src/sandbox/external-pointer-table.h",
+        "src/sandbox/sandbox.cc",
+        "src/sandbox/sandbox.h",
+        "src/sandbox/sandboxed-pointer-inl.h",
+        "src/sandbox/sandboxed-pointer.h",
         "src/base/sanitizer/asan.h",
         "src/base/sanitizer/lsan-page-allocator.cc",
         "src/base/sanitizer/lsan-page-allocator.h",
diff --git a/BUILD.gn b/BUILD.gn
index 5fef580593..7ef8c1f2e0 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -302,14 +302,20 @@ declare_args() {
   # Sets -DV8_COMPRESS_ZONES.
   v8_enable_zone_compression = ""
 
-  # Enable V8 heap sandbox experimental feature.
-  # Sets -DV8_HEAP_SANDBOX.
-  v8_enable_heap_sandbox = ""
+  # Enable the experimental V8 sandbox.
+  # Sets -DV8_SANDBOX.
+  v8_enable_sandbox = false
 
-  # Enable the Virtual Memory Cage, which contains the pointer compression cage
-  # as well as ArrayBuffer BackingStores and WASM memory cages.
-  # Sets -DV8_VIRTUAL_MEMORY_CAGE.
-  v8_enable_virtual_memory_cage = ""
+  # Enable external pointer sandboxing. Requires v8_enable_sandbox.
+  # Sets -DV8_SANDBOXED_EXTERNAL_POINRTERS.
+  v8_enable_sandboxed_external_pointers = false
+
+  # Enable sandboxed pointers. Requires v8_enable_sandbox.
+  # Sets -DV8_SANDBOXED_POINTERS.
+  v8_enable_sandboxed_pointers = false
+
+  # Enable all available sandbox features. Implies v8_enable_sandbox.
+  v8_enable_sandbox_future = false
 
   # Experimental feature for collecting per-class zone memory stats.
   # Requires use_rtti = true
@@ -403,12 +409,6 @@ if (v8_enable_fast_torque == "") {
 if (v8_enable_zone_compression == "") {
   v8_enable_zone_compression = false
 }
-if (v8_enable_heap_sandbox == "") {
-  v8_enable_heap_sandbox = false
-}
-if (v8_enable_virtual_memory_cage == "") {
-  v8_enable_virtual_memory_cage = v8_enable_heap_sandbox
-}
 if (v8_enable_short_builtin_calls == "") {
   v8_enable_short_builtin_calls =
       v8_current_cpu == "x64" || (!is_android && v8_current_cpu == "arm64")
@@ -473,9 +473,16 @@ if (build_with_chromium && v8_current_cpu == "arm64" &&
   v8_control_flow_integrity = true
 }
 
-# Enable the virtual memory cage on 64-bit Chromium builds.
+# Enable the v8 sandbox on 64-bit Chromium builds.
 if (build_with_chromium && v8_enable_pointer_compression_shared_cage) {
-  v8_enable_virtual_memory_cage = true
+  v8_enable_sandbox = true
+}
+
+# Enable all available sandbox features if sandbox future is enabled.
+if (v8_enable_sandbox_future) {
+  v8_enable_sandboxed_pointers = true
+  v8_enable_sandboxed_external_pointers = true
+  v8_enable_sandbox = true
 }
 
 assert(!v8_disable_write_barriers || v8_enable_single_generation,
@@ -500,18 +507,18 @@ assert(!v8_enable_map_packing || v8_current_cpu == "x64",
 assert(!v8_enable_external_code_space || v8_enable_pointer_compression,
        "External code space feature requires pointer compression")
 
-assert(!v8_enable_heap_sandbox || v8_enable_pointer_compression,
-       "V8 Heap Sandbox requires pointer compression")
+assert(!v8_enable_sandbox || v8_enable_pointer_compression_shared_cage,
+       "Sandbox requires the shared pointer compression cage")
 
-assert(!v8_enable_heap_sandbox || !v8_enable_external_code_space,
-       "V8 Heap Sandbox is not compatible with external code space YET")
+assert(!v8_enable_sandboxed_pointers || v8_enable_sandbox,
+       "Sandboxed pointers require the sandbox")
 
-assert(!v8_enable_heap_sandbox || v8_enable_virtual_memory_cage,
-       "The Heap Sandbox requires the virtual memory cage")
+assert(!v8_enable_sandboxed_external_pointers || v8_enable_sandbox,
+       "Sandboxed external pointers require the sandbox")
 
 assert(
-    !v8_enable_virtual_memory_cage || v8_enable_pointer_compression_shared_cage,
-    "V8 VirtualMemoryCage requires the shared pointer compression cage")
+    !v8_enable_sandboxed_external_pointers || !v8_enable_external_code_space,
+    "Sandboxed external pointers are not compatible with external code space YET")
 
 assert(
     !v8_enable_pointer_compression_shared_cage || v8_enable_pointer_compression,
@@ -673,8 +680,9 @@ external_v8_defines = [
   "V8_COMPRESS_POINTERS_IN_ISOLATE_CAGE",
   "V8_31BIT_SMIS_ON_64BIT_ARCH",
   "V8_COMPRESS_ZONES",
-  "V8_HEAP_SANDBOX",
-  "V8_VIRTUAL_MEMORY_CAGE",
+  "V8_SANDBOX",
+  "V8_SANDBOXED_POINTERS",
+  "V8_SANDBOXED_EXTERNAL_POINTERS",
   "V8_DEPRECATION_WARNINGS",
   "V8_IMMINENT_DEPRECATION_WARNINGS",
   "V8_NO_ARGUMENTS_ADAPTOR",
@@ -702,11 +710,14 @@ if (v8_enable_pointer_compression || v8_enable_31bit_smis_on_64bit_arch) {
 if (v8_enable_zone_compression) {
   enabled_external_v8_defines += [ "V8_COMPRESS_ZONES" ]
 }
-if (v8_enable_heap_sandbox) {
-  enabled_external_v8_defines += [ "V8_HEAP_SANDBOX" ]
+if (v8_enable_sandbox) {
+  enabled_external_v8_defines += [ "V8_SANDBOX" ]
+}
+if (v8_enable_sandboxed_pointers) {
+  enabled_external_v8_defines += [ "V8_SANDBOXED_POINTERS" ]
 }
-if (v8_enable_virtual_memory_cage) {
-  enabled_external_v8_defines += [ "V8_VIRTUAL_MEMORY_CAGE" ]
+if (v8_enable_sandboxed_external_pointers) {
+  enabled_external_v8_defines += [ "V8_SANDBOXED_EXTERNAL_POINTERS" ]
 }
 if (v8_deprecation_warnings) {
   enabled_external_v8_defines += [ "V8_DEPRECATION_WARNINGS" ]
@@ -2197,7 +2208,7 @@ action("v8_dump_build_config") {
     "v8_enable_pointer_compression=$v8_enable_pointer_compression",
     "v8_enable_pointer_compression_shared_cage=" +
         "$v8_enable_pointer_compression_shared_cage",
-    "v8_enable_virtual_memory_cage=$v8_enable_virtual_memory_cage",
+    "v8_enable_sandbox=$v8_enable_sandbox",
     "v8_enable_third_party_heap=$v8_enable_third_party_heap",
     "v8_enable_webassembly=$v8_enable_webassembly",
     "v8_dict_property_const_tracking=$v8_dict_property_const_tracking",
@@ -3315,12 +3326,12 @@ v8_header_set("v8_internal_headers") {
     "src/roots/roots.h",
     "src/runtime/runtime-utils.h",
     "src/runtime/runtime.h",
-    "src/security/caged-pointer-inl.h",
-    "src/security/caged-pointer.h",
-    "src/security/external-pointer-inl.h",
-    "src/security/external-pointer-table.h",
-    "src/security/external-pointer.h",
-    "src/security/vm-cage.h",
+    "src/sandbox/external-pointer-inl.h",
+    "src/sandbox/external-pointer-table.h",
+    "src/sandbox/external-pointer.h",
+    "src/sandbox/sandbox.h",
+    "src/sandbox/sandboxed-pointer-inl.h",
+    "src/sandbox/sandboxed-pointer.h",
     "src/snapshot/code-serializer.h",
     "src/snapshot/context-deserializer.h",
     "src/snapshot/context-serializer.h",
@@ -4340,8 +4351,8 @@ v8_source_set("v8_base_without_compiler") {
     "src/runtime/runtime-typedarray.cc",
     "src/runtime/runtime-weak-refs.cc",
     "src/runtime/runtime.cc",
-    "src/security/external-pointer-table.cc",
-    "src/security/vm-cage.cc",
+    "src/sandbox/external-pointer-table.cc",
+    "src/sandbox/sandbox.cc",
     "src/snapshot/code-serializer.cc",
     "src/snapshot/context-deserializer.cc",
     "src/snapshot/context-serializer.cc",
diff --git a/bazel/defs.bzl b/bazel/defs.bzl
index fc428ba16c..53fccf92e7 100644
--- a/bazel/defs.bzl
+++ b/bazel/defs.bzl
@@ -453,7 +453,7 @@ def build_config_content(cpu, icu):
         ("v8_enable_webassembly", "false"),
         ("v8_control_flow_integrity", "false"),
         ("v8_enable_single_generation", "false"),
-        ("v8_enable_virtual_memory_cage", "false"),
+        ("v8_enable_sandbox", "false"),
         ("v8_target_cpu", cpu),
     ])
 
diff --git a/include/v8-array-buffer.h b/include/v8-array-buffer.h
index 0ce2b65368..e9047b79ce 100644
--- a/include/v8-array-buffer.h
+++ b/include/v8-array-buffer.h
@@ -175,8 +175,8 @@ class V8_EXPORT ArrayBuffer : public Object {
     /**
      * Convenience allocator.
      *
-     * When the virtual memory cage is enabled, this allocator will allocate its
-     * backing memory inside the cage. Otherwise, it will rely on malloc/free.
+     * When the sandbox is enabled, this allocator will allocate its backing
+     * memory inside the sandbox. Otherwise, it will rely on malloc/free.
      *
      * Caller takes ownership, i.e. the returned object needs to be freed using
      * |delete allocator| once it is no longer in use.
diff --git a/include/v8-context.h b/include/v8-context.h
index d398ac4b21..5f3b17e3f7 100644
--- a/include/v8-context.h
+++ b/include/v8-context.h
@@ -387,10 +387,10 @@ void* Context::GetAlignedPointerFromEmbedderData(int index) {
       I::ReadTaggedPointerField(ctx, I::kNativeContextEmbedderDataOffset);
   int value_offset =
       I::kEmbedderDataArrayHeaderSize + (I::kEmbedderDataSlotSize * index);
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   value_offset += I::kEmbedderDataSlotRawPayloadOffset;
 #endif
-  internal::Isolate* isolate = I::GetIsolateForHeapSandbox(ctx);
+  internal::Isolate* isolate = I::GetIsolateForSandbox(ctx);
   return reinterpret_cast<void*>(
       I::ReadExternalPointerField(isolate, embedder_data, value_offset,
                                   internal::kEmbedderDataSlotPayloadTag));
diff --git a/include/v8-initialization.h b/include/v8-initialization.h
index 7a2ae9316a..4281a69cb6 100644
--- a/include/v8-initialization.h
+++ b/include/v8-initialization.h
@@ -99,8 +99,10 @@ class V8_EXPORT V8 {
     const int kBuildConfiguration =
         (internal::PointerCompressionIsEnabled() ? kPointerCompression : 0) |
         (internal::SmiValuesAre31Bits() ? k31BitSmis : 0) |
-        (internal::HeapSandboxIsEnabled() ? kHeapSandbox : 0) |
-        (internal::VirtualMemoryCageIsEnabled() ? kVirtualMemoryCage : 0);
+        (internal::SandboxedExternalPointersAreEnabled()
+             ? kSandboxedExternalPointers
+             : 0) |
+        (internal::SandboxIsEnabled() ? kSandbox : 0);
     return Initialize(kBuildConfiguration);
   }
 
@@ -184,61 +186,71 @@ class V8_EXPORT V8 {
   V8_DEPRECATE_SOON("Use DisposePlatform()")
   static void ShutdownPlatform() { DisposePlatform(); }
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
+#ifdef V8_SANDBOX
   //
-  // Virtual Memory Cage related API.
+  // Sandbox related API.
   //
   // This API is not yet stable and subject to changes in the future.
   //
 
   /**
-   * Initializes the virtual memory cage for V8.
+   * Initializes the V8 sandbox.
    *
    * This must be invoked after the platform was initialized but before V8 is
-   * initialized. The virtual memory cage is torn down during platform shutdown.
+   * initialized. The sandbox is torn down during platform shutdown.
    * Returns true on success, false otherwise.
    *
-   * TODO(saelo) Once it is no longer optional to create the virtual memory
-   * cage when compiling with V8_VIRTUAL_MEMORY_CAGE, the cage initialization
-   * will likely happen as part of V8::Initialize, at which point this function
-   * should be removed.
+   * TODO(saelo) Once it is no longer optional to initialize the sandbox when
+   * compiling with V8_SANDBOX, the sandbox initialization will likely happen
+   * as part of V8::Initialize, at which point this function should be removed.
    */
-  static bool InitializeVirtualMemoryCage();
+  static bool InitializeSandbox();
+  V8_DEPRECATE_SOON("Use InitializeSandbox()")
+  static bool InitializeVirtualMemoryCage() { return InitializeSandbox(); }
 
   /**
-   * Provides access to the virtual memory cage page allocator.
+   * Provides access to the virtual address subspace backing the sandbox.
    *
-   * This allocator allocates pages inside the virtual memory cage. It can for
-   * example be used to obtain virtual memory for ArrayBuffer backing stores,
-   * which must be located inside the cage.
+   * This can be used to allocate pages inside the sandbox, for example to
+   * obtain virtual memory for ArrayBuffer backing stores, which must be
+   * located inside the sandbox.
    *
-   * It should be assumed that an attacker can corrupt data inside the cage,
-   * and so in particular the contents of pages returned by this allocator,
-   * arbitrarily and concurrently. Due to this, it is recommended to to only
-   * place pure data buffers in pages obtained through this allocator.
+   * It should be assumed that an attacker can corrupt data inside the sandbox,
+   * and so in particular the contents of pages allocagted in this virtual
+   * address space, arbitrarily and concurrently. Due to this, it is
+   * recommended to to only place pure data buffers in them.
    *
-   * This function must only be called after initializing the virtual memory
-   * cage and V8.
+   * This function must only be called after initializing the sandbox.
    */
+  static VirtualAddressSpace* GetSandboxAddressSpace();
+  V8_DEPRECATE_SOON("Use GetSandboxAddressSpace()")
   static PageAllocator* GetVirtualMemoryCagePageAllocator();
 
   /**
-   * Returns the size of the virtual memory cage in bytes.
+   * Returns the size of the sandbox in bytes.
    *
-   * If the cage has not been initialized, or if the initialization failed,
+   * If the sandbox has not been initialized, or if the initialization failed,
    * this returns zero.
    */
-  static size_t GetVirtualMemoryCageSizeInBytes();
+  static size_t GetSandboxSizeInBytes();
+  V8_DEPRECATE_SOON("Use GetSandboxSizeInBytes()")
+  static size_t GetVirtualMemoryCageSizeInBytes() {
+    return GetSandboxSizeInBytes();
+  }
 
   /**
-   * Returns whether the virtual memory cage is configured securely.
+   * Returns whether the sandbox is configured securely.
    *
-   * If V8 cannot create a proper virtual memory cage, it will fall back to
-   * creating a cage that doesn't have the desired security properties but at
-   * least still allows V8 to function. This API can be used to determine if
-   * such an insecure cage is being used, in which case it will return false.
+   * If V8 cannot create a proper sandbox, it will fall back to creating a
+   * sandbox that doesn't have the desired security properties but at least
+   * still allows V8 to function. This API can be used to determine if such an
+   * insecure sandbox is being used, in which case it will return false.
    */
-  static bool IsUsingSecureVirtualMemoryCage();
+  static bool IsSandboxConfiguredSecurely();
+  V8_DEPRECATE_SOON("Use IsSandboxConfiguredSecurely()")
+  static bool IsUsingSecureVirtualMemoryCage() {
+    return IsSandboxConfiguredSecurely();
+  }
 #endif
 
   /**
@@ -274,8 +286,8 @@ class V8_EXPORT V8 {
   enum BuildConfigurationFeatures {
     kPointerCompression = 1 << 0,
     k31BitSmis = 1 << 1,
-    kHeapSandbox = 1 << 2,
-    kVirtualMemoryCage = 1 << 3,
+    kSandboxedExternalPointers = 1 << 2,
+    kSandbox = 1 << 3,
   };
 
   /**
diff --git a/include/v8-internal.h b/include/v8-internal.h
index f49b54557c..aaa464baba 100644
--- a/include/v8-internal.h
+++ b/include/v8-internal.h
@@ -121,8 +121,8 @@ constexpr bool PointerCompressionIsEnabled() {
   return kApiTaggedSize != kApiSystemPointerSize;
 }
 
-constexpr bool HeapSandboxIsEnabled() {
-#ifdef V8_HEAP_SANDBOX
+constexpr bool SandboxedExternalPointersAreEnabled() {
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   return true;
 #else
   return false;
@@ -131,14 +131,14 @@ constexpr bool HeapSandboxIsEnabled() {
 
 using ExternalPointer_t = Address;
 
-// If the heap sandbox is enabled, these tag values will be ORed with the
-// external pointers in the external pointer table to prevent use of pointers of
-// the wrong type. When a pointer is loaded, it is ANDed with the inverse of the
-// expected type's tag. The tags are constructed in a way that guarantees that a
-// failed type check will result in one or more of the top bits of the pointer
-// to be set, rendering the pointer inacessible. This construction allows
-// performing the type check and removing GC marking bits from the pointer at
-// the same time.
+// If sandboxed external pointers are enabled, these tag values will be ORed
+// with the external pointers in the external pointer table to prevent use of
+// pointers of the wrong type. When a pointer is loaded, it is ANDed with the
+// inverse of the expected type's tag. The tags are constructed in a way that
+// guarantees that a failed type check will result in one or more of the top
+// bits of the pointer to be set, rendering the pointer inacessible. This
+// construction allows performing the type check and removing GC marking bits
+// from the pointer at the same time.
 enum ExternalPointerTag : uint64_t {
   kExternalPointerNullTag = 0x0000000000000000,
   kExternalStringResourceTag = 0x00ff000000000000,       // 0b000000011111111
@@ -214,7 +214,7 @@ class Internals {
   static const int kFixedArrayHeaderSize = 2 * kApiTaggedSize;
   static const int kEmbedderDataArrayHeaderSize = 2 * kApiTaggedSize;
   static const int kEmbedderDataSlotSize = kApiSystemPointerSize;
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   static const int kEmbedderDataSlotRawPayloadOffset = kApiTaggedSize;
 #endif
   static const int kNativeContextEmbedderDataOffset = 6 * kApiTaggedSize;
@@ -432,9 +432,9 @@ class Internals {
 #endif
   }
 
-  V8_INLINE static internal::Isolate* GetIsolateForHeapSandbox(
+  V8_INLINE static internal::Isolate* GetIsolateForSandbox(
       internal::Address obj) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     return internal::IsolateFromNeverReadOnlySpaceObject(obj);
 #else
     // Not used in non-sandbox mode.
@@ -445,7 +445,7 @@ class Internals {
   V8_INLINE static Address DecodeExternalPointer(
       const Isolate* isolate, ExternalPointer_t encoded_pointer,
       ExternalPointerTag tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     return internal::DecodeExternalPointerImpl(isolate, encoded_pointer, tag);
 #else
     return encoded_pointer;
@@ -455,7 +455,7 @@ class Internals {
   V8_INLINE static internal::Address ReadExternalPointerField(
       internal::Isolate* isolate, internal::Address heap_object_ptr, int offset,
       ExternalPointerTag tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     internal::ExternalPointer_t encoded_value =
         ReadRawField<uint32_t>(heap_object_ptr, offset);
     // We currently have to treat zero as nullptr in embedder slots.
@@ -486,99 +486,97 @@ class Internals {
 #endif  // V8_COMPRESS_POINTERS
 };
 
-constexpr bool VirtualMemoryCageIsEnabled() {
-#ifdef V8_VIRTUAL_MEMORY_CAGE
+constexpr bool SandboxIsEnabled() {
+#ifdef V8_SANDBOX
   return true;
 #else
   return false;
 #endif
 }
 
-// CagedPointers are guaranteed to point into the virtual memory cage. This is
-// achieved for example by storing them as offset from the cage base rather
-// than as raw pointers.
-using CagedPointer_t = Address;
+// SandboxedPointers are guaranteed to point into the sandbox. This is achieved
+// for example by storing them as offset rather than as raw pointers.
+using SandboxedPointer_t = Address;
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
+#ifdef V8_SANDBOX_IS_AVAILABLE
 
 #define GB (1ULL << 30)
 #define TB (1ULL << 40)
 
-// Size of the virtual memory cage, excluding the guard regions surrounding it.
-constexpr size_t kVirtualMemoryCageSizeLog2 = 40;  // 1 TB
-constexpr size_t kVirtualMemoryCageSize = 1ULL << kVirtualMemoryCageSizeLog2;
+// Size of the sandbox, excluding the guard regions surrounding it.
+constexpr size_t kSandboxSizeLog2 = 40;  // 1 TB
+constexpr size_t kSandboxSize = 1ULL << kSandboxSizeLog2;
 
-// Required alignment of the virtual memory cage. For simplicity, we require the
+// Required alignment of the sandbox. For simplicity, we require the
 // size of the guard regions to be a multiple of this, so that this specifies
-// the alignment of the cage including and excluding surrounding guard regions.
-// The alignment requirement is due to the pointer compression cage being
-// located at the start of the virtual memory cage.
-constexpr size_t kVirtualMemoryCageAlignment =
-    Internals::kPtrComprCageBaseAlignment;
-
-// Caged pointers are stored inside the heap as offset from the cage base
-// shifted to the left. This way, it is guaranteed that the offset is smaller
-// than the cage size after shifting it to the right again. This constant
-// specifies the shift amount.
-constexpr uint64_t kCagedPointerShift = 64 - kVirtualMemoryCageSizeLog2;
-
-// Size of the guard regions surrounding the virtual memory cage. This assumes a
-// worst-case scenario of a 32-bit unsigned index being used to access an array
-// of 64-bit values.
-constexpr size_t kVirtualMemoryCageGuardRegionSize = 32ULL * GB;
-
-static_assert((kVirtualMemoryCageGuardRegionSize %
-               kVirtualMemoryCageAlignment) == 0,
-              "The size of the virtual memory cage guard region must be a "
+// the alignment of the sandbox including and excluding surrounding guard
+// regions. The alignment requirement is due to the pointer compression cage
+// being located at the start of the sandbox.
+constexpr size_t kSandboxAlignment = Internals::kPtrComprCageBaseAlignment;
+
+// Sandboxed pointers are stored inside the heap as offset from the sandbox
+// base shifted to the left. This way, it is guaranteed that the offset is
+// smaller than the sandbox size after shifting it to the right again. This
+// constant specifies the shift amount.
+constexpr uint64_t kSandboxedPointerShift = 64 - kSandboxSizeLog2;
+
+// Size of the guard regions surrounding the sandbox. This assumes a worst-case
+// scenario of a 32-bit unsigned index used to access an array of 64-bit
+// values.
+constexpr size_t kSandboxGuardRegionSize = 32ULL * GB;
+
+static_assert((kSandboxGuardRegionSize % kSandboxAlignment) == 0,
+              "The size of the guard regions around the sandbox must be a "
               "multiple of its required alignment.");
 
-// Minimum size of the virtual memory cage, excluding the guard regions
-// surrounding it. If the cage reservation fails, its size is currently halved
-// until either the reservation succeeds or the minimum size is reached. A
-// minimum of 32GB allows the 4GB pointer compression region as well as the
-// ArrayBuffer partition and two 10GB WASM memory cages to fit into the cage.
-// 32GB should also be the minimum possible size of the userspace address space
-// as there are some machine configurations with only 36 virtual address bits.
-constexpr size_t kVirtualMemoryCageMinimumSize = 32ULL * GB;
-
-static_assert(kVirtualMemoryCageMinimumSize <= kVirtualMemoryCageSize,
-              "The minimal size of the virtual memory cage must be smaller or "
-              "equal to the regular size.");
-
-// On OSes where reservation virtual memory is too expensive to create a real
-// cage, notably Windows pre 8.1, we create a fake cage that doesn't actually
-// reserve most of the memory, and so doesn't have the desired security
-// properties, but still ensures that objects that should be located inside the
-// cage are allocated within kVirtualMemoryCageSize bytes from the start of the
-// cage, and so appear to be inside the cage. The minimum size of the virtual
-// memory range that is actually reserved for a fake cage is specified by this
-// constant and should be big enough to contain the pointer compression region
-// as well as the ArrayBuffer partition.
-constexpr size_t kFakeVirtualMemoryCageMinReservationSize = 8ULL * GB;
-
-static_assert(kVirtualMemoryCageMinimumSize >
+// Minimum size of the sandbox, excluding the guard regions surrounding it. If
+// the virtual memory reservation for the sandbox fails, its size is currently
+// halved until either the reservation succeeds or the minimum size is reached.
+// A minimum of 32GB allows the 4GB pointer compression region as well as the
+// ArrayBuffer partition and two 10GB WASM memory cages to fit into the
+// sandbox. 32GB should also be the minimum possible size of the userspace
+// address space as there are some machine configurations with only 36 virtual
+// address bits.
+constexpr size_t kSandboxMinimumSize = 32ULL * GB;
+
+static_assert(kSandboxMinimumSize <= kSandboxSize,
+              "The minimal size of the sandbox must be smaller or equal to the "
+              "regular size.");
+
+// On OSes where reserving virtual memory is too expensive to reserve the
+// entire address space backing the sandbox, notably Windows pre 8.1, we create
+// a partially reserved sandbox that doesn't actually reserve most of the
+// memory, and so doesn't have the desired security properties as unrelated
+// memory allocations could end up inside of it, but which still ensures that
+// objects that should be located inside the sandbox are allocated within
+// kSandboxSize bytes from the start of the sandbox. The minimum size of the
+// region that is actually reserved for such a sandbox is specified by this
+// constant and should be big enough to contain the pointer compression cage as
+// well as the ArrayBuffer partition.
+constexpr size_t kSandboxMinimumReservationSize = 8ULL * GB;
+
+static_assert(kSandboxMinimumSize > Internals::kPtrComprCageReservationSize,
+              "The sandbox must be larger than the pointer compression cage "
+              "contained within it.");
+static_assert(kSandboxMinimumReservationSize >
                   Internals::kPtrComprCageReservationSize,
-              "The virtual memory cage must be larger than the pointer "
-              "compression cage contained within it.");
-static_assert(kFakeVirtualMemoryCageMinReservationSize >
-                  Internals::kPtrComprCageReservationSize,
-              "The reservation for a fake virtual memory cage must be larger "
-              "than the pointer compression cage contained within it.");
-
-// For now, even if the virtual memory cage is enabled, we still allow backing
-// stores to be allocated outside of it as fallback. This will simplify the
-// initial rollout. However, if the heap sandbox is also enabled, we already use
-// the "enforcing mode" of the virtual memory cage. This is useful for testing.
-#ifdef V8_HEAP_SANDBOX
-constexpr bool kAllowBackingStoresOutsideCage = false;
+              "The minimum reservation size for a sandbox must be larger than "
+              "the pointer compression cage contained within it.");
+
+// For now, even if the sandbox is enabled, we still allow backing stores to be
+// allocated outside of it as fallback. This will simplify the initial rollout.
+// However, if sandboxed pointers are also enabled, we must always place
+// backing stores inside the sandbox as they will be referenced though them.
+#ifdef V8_SANDBOXED_POINTERS
+constexpr bool kAllowBackingStoresOutsideSandbox = false;
 #else
-constexpr bool kAllowBackingStoresOutsideCage = true;
-#endif  // V8_HEAP_SANDBOX
+constexpr bool kAllowBackingStoresOutsideSandbox = true;
+#endif  // V8_SANDBOXED_POINTERS
 
 #undef GB
 #undef TB
 
-#endif  // V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
+#endif  // V8_SANDBOX_IS_AVAILABLE
 
 // Only perform cast check for types derived from v8::Data since
 // other types do not implement the Cast method.
diff --git a/include/v8-object.h b/include/v8-object.h
index e047c413ac..772ca8c767 100644
--- a/include/v8-object.h
+++ b/include/v8-object.h
@@ -744,10 +744,10 @@ void* Object::GetAlignedPointerFromInternalField(int index) {
   auto instance_type = I::GetInstanceType(obj);
   if (v8::internal::CanHaveInternalField(instance_type)) {
     int offset = I::kJSObjectHeaderSize + (I::kEmbedderDataSlotSize * index);
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     offset += I::kEmbedderDataSlotRawPayloadOffset;
 #endif
-    internal::Isolate* isolate = I::GetIsolateForHeapSandbox(obj);
+    internal::Isolate* isolate = I::GetIsolateForSandbox(obj);
     A value = I::ReadExternalPointerField(
         isolate, obj, offset, internal::kEmbedderDataSlotPayloadTag);
     return reinterpret_cast<void*>(value);
diff --git a/include/v8-primitive.h b/include/v8-primitive.h
index 11c01876c7..7071e7386d 100644
--- a/include/v8-primitive.h
+++ b/include/v8-primitive.h
@@ -787,7 +787,7 @@ String::ExternalStringResource* String::GetExternalStringResource() const {
 
   ExternalStringResource* result;
   if (I::IsExternalTwoByteString(I::GetInstanceType(obj))) {
-    internal::Isolate* isolate = I::GetIsolateForHeapSandbox(obj);
+    internal::Isolate* isolate = I::GetIsolateForSandbox(obj);
     A value =
         I::ReadExternalPointerField(isolate, obj, I::kStringResourceOffset,
                                     internal::kExternalStringResourceTag);
@@ -811,7 +811,7 @@ String::ExternalStringResourceBase* String::GetExternalStringResourceBase(
   ExternalStringResourceBase* resource;
   if (type == I::kExternalOneByteRepresentationTag ||
       type == I::kExternalTwoByteRepresentationTag) {
-    internal::Isolate* isolate = I::GetIsolateForHeapSandbox(obj);
+    internal::Isolate* isolate = I::GetIsolateForSandbox(obj);
     A value =
         I::ReadExternalPointerField(isolate, obj, I::kStringResourceOffset,
                                     internal::kExternalStringResourceTag);
diff --git a/include/v8config.h b/include/v8config.h
index 1242d4289c..c8d8d8397d 100644
--- a/include/v8config.h
+++ b/include/v8config.h
@@ -578,17 +578,11 @@ V8 shared library set USING_V8_SHARED.
 
 #endif  // V8_OS_WIN
 
-// The virtual memory cage is available (i.e. defined) when pointer compression
-// is enabled, but it is only used when V8_VIRTUAL_MEMORY_CAGE is enabled as
-// well. This allows better test coverage of the cage.
+// The sandbox is available (i.e. defined) when pointer compression
+// is enabled, but it is only used when V8_SANDBOX is enabled as
+// well. This allows better test coverage of the sandbox.
 #if defined(V8_COMPRESS_POINTERS)
-#define V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
-#endif
-
-// CagedPointers are currently only used if the heap sandbox is enabled.
-// In the future, they will be enabled when the virtual memory cage is enabled.
-#if defined(V8_HEAP_SANDBOX)
-#define V8_CAGED_POINTERS
+#define V8_SANDBOX_IS_AVAILABLE
 #endif
 
 // From C++17 onwards, static constexpr member variables are defined to be
diff --git a/infra/mb/mb_config.pyl b/infra/mb/mb_config.pyl
index 82964dd7d4..27726af5c2 100644
--- a/infra/mb/mb_config.pyl
+++ b/infra/mb/mb_config.pyl
@@ -578,9 +578,9 @@
     'debug_x64_header_includes': [
       'debug_bot', 'x64', 'v8_check_header_includes'],
     'debug_x64_heap_sandbox': [
-      'debug_bot', 'x64', 'v8_enable_heap_sandbox'],
+      'debug_bot', 'x64', 'v8_enable_sandbox_future'],
     'debug_x64_heap_sandbox_arm64_sim': [
-      'debug_bot', 'simulate_arm64', 'v8_enable_heap_sandbox'],
+      'debug_bot', 'simulate_arm64', 'v8_enable_sandbox_future'],
     'debug_x64_minimal_symbols': [
       'debug_bot', 'x64', 'minimal_symbols'],
     'debug_x64_non_default_cppgc': [
@@ -922,8 +922,8 @@
       'gn_args': 'v8_enable_runtime_call_stats=false',
     },
 
-    'v8_enable_heap_sandbox': {
-      'gn_args': 'v8_enable_heap_sandbox=true',
+    'v8_enable_sandbox_future': {
+      'gn_args': 'v8_enable_sandbox_future=true',
     },
 
     'v8_enable_lite_mode': {
diff --git a/src/api/api.cc b/src/api/api.cc
index 1c4e397620..893cc23192 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -115,8 +115,8 @@
 #include "src/profiler/tick-sample.h"
 #include "src/regexp/regexp-utils.h"
 #include "src/runtime/runtime.h"
-#include "src/security/external-pointer.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/external-pointer.h"
+#include "src/sandbox/sandbox.h"
 #include "src/snapshot/code-serializer.h"
 #include "src/snapshot/embedded/embedded-data.h"
 #include "src/snapshot/snapshot.h"
@@ -389,11 +389,11 @@ void V8::SetSnapshotDataBlob(StartupData* snapshot_blob) {
 
 namespace {
 
-#ifdef V8_HEAP_SANDBOX
-// ArrayBufferAllocator to use when the heap sandbox is enabled, in which case
-// all ArrayBuffer backing stores need to be allocated inside the virtual
-// memory cage. Note, the current implementation is extremely inefficient as it
-// uses the BoundedPageAllocator. In the future, we'll need a proper allocator
+#ifdef V8_SANDBOXED_POINTERS
+// ArrayBufferAllocator to use when sandboxed pointers are used in which case
+// all ArrayBuffer backing stores need to be allocated inside the sandbox.
+// Note, the current implementation is extremely inefficient as it uses the
+// BoundedPageAllocator. In the future, we'll need a proper allocator
 // implementation.
 class ArrayBufferAllocator : public v8::ArrayBuffer::Allocator {
  public:
@@ -461,7 +461,7 @@ class ArrayBufferAllocator : public v8::ArrayBuffer::Allocator {
     return new_data;
   }
 };
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_POINTERS
 
 struct SnapshotCreatorData {
   explicit SnapshotCreatorData(Isolate* isolate)
@@ -5850,7 +5850,7 @@ String::ExternalStringResource* String::GetExternalStringResourceSlow() const {
   }
 
   if (i::StringShape(str).IsExternalTwoByte()) {
-    internal::Isolate* isolate = I::GetIsolateForHeapSandbox(str.ptr());
+    internal::Isolate* isolate = I::GetIsolateForSandbox(str.ptr());
     internal::Address value = I::ReadExternalPointerField(
         isolate, str.ptr(), I::kStringResourceOffset,
         internal::kExternalStringResourceTag);
@@ -5894,7 +5894,7 @@ String::ExternalStringResourceBase* String::GetExternalStringResourceBaseSlow(
   *encoding_out = static_cast<Encoding>(type & I::kStringEncodingMask);
   if (i::StringShape(str).IsExternalOneByte() ||
       i::StringShape(str).IsExternalTwoByte()) {
-    internal::Isolate* isolate = I::GetIsolateForHeapSandbox(string);
+    internal::Isolate* isolate = I::GetIsolateForSandbox(string);
     internal::Address value =
         I::ReadExternalPointerField(isolate, string, I::kStringResourceOffset,
                                     internal::kExternalStringResourceTag);
@@ -6083,10 +6083,8 @@ void v8::V8::InitializePlatform(Platform* platform) {
   i::V8::InitializePlatform(platform);
 }
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-bool v8::V8::InitializeVirtualMemoryCage() {
-  return i::V8::InitializeVirtualMemoryCage();
-}
+#ifdef V8_SANDBOX
+bool v8::V8::InitializeSandbox() { return i::V8::InitializeSandbox(); }
 #endif
 
 void v8::V8::DisposePlatform() { i::V8::DisposePlatform(); }
@@ -6110,23 +6108,24 @@ bool v8::V8::Initialize(const int build_config) {
         kEmbedderSmiValueSize, internal::kSmiValueSize);
   }
 
-  const bool kEmbedderHeapSandbox = (build_config & kHeapSandbox) != 0;
-  if (kEmbedderHeapSandbox != V8_HEAP_SANDBOX_BOOL) {
+  const bool kEmbedderSandboxedExternalPointers =
+      (build_config & kSandboxedExternalPointers) != 0;
+  if (kEmbedderSandboxedExternalPointers !=
+      V8_SANDBOXED_EXTERNAL_POINTERS_BOOL) {
     FATAL(
         "Embedder-vs-V8 build configuration mismatch. On embedder side "
-        "heap sandbox is %s while on V8 side it's %s.",
-        kEmbedderHeapSandbox ? "ENABLED" : "DISABLED",
-        V8_HEAP_SANDBOX_BOOL ? "ENABLED" : "DISABLED");
+        "sandboxed external pointers is %s while on V8 side it's %s.",
+        kEmbedderSandboxedExternalPointers ? "ENABLED" : "DISABLED",
+        V8_SANDBOXED_EXTERNAL_POINTERS_BOOL ? "ENABLED" : "DISABLED");
   }
 
-  const bool kEmbedderVirtualMemoryCage =
-      (build_config & kVirtualMemoryCage) != 0;
-  if (kEmbedderVirtualMemoryCage != V8_VIRTUAL_MEMORY_CAGE_BOOL) {
+  const bool kEmbedderSandbox = (build_config & kSandbox) != 0;
+  if (kEmbedderSandbox != V8_SANDBOX_BOOL) {
     FATAL(
         "Embedder-vs-V8 build configuration mismatch. On embedder side "
-        "virtual memory cage is %s while on V8 side it's %s.",
-        kEmbedderVirtualMemoryCage ? "ENABLED" : "DISABLED",
-        V8_VIRTUAL_MEMORY_CAGE_BOOL ? "ENABLED" : "DISABLED");
+        "sandbox is %s while on V8 side it's %s.",
+        kEmbedderSandbox ? "ENABLED" : "DISABLED",
+        V8_SANDBOX_BOOL ? "ENABLED" : "DISABLED");
   }
 
   i::V8::Initialize();
@@ -6246,31 +6245,38 @@ void v8::V8::InitializeExternalStartupDataFromFile(const char* snapshot_blob) {
 
 const char* v8::V8::GetVersion() { return i::Version::GetVersion(); }
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
+#ifdef V8_SANDBOX
+VirtualAddressSpace* v8::V8::GetSandboxAddressSpace() {
+  Utils::ApiCheck(i::GetProcessWideSandbox()->is_initialized(),
+                  "v8::V8::GetSandboxAddressSpace",
+                  "The sandbox must be initialized first.");
+  return i::GetProcessWideSandbox()->address_space();
+}
+
 PageAllocator* v8::V8::GetVirtualMemoryCagePageAllocator() {
-  Utils::ApiCheck(i::GetProcessWideVirtualMemoryCage()->is_initialized(),
+  Utils::ApiCheck(i::GetProcessWideSandbox()->is_initialized(),
                   "v8::V8::GetVirtualMemoryCagePageAllocator",
-                  "The virtual memory cage must be initialized first.");
-  return i::GetProcessWideVirtualMemoryCage()->page_allocator();
+                  "The sandbox must be initialized first.");
+  return i::GetProcessWideSandbox()->page_allocator();
 }
 
-size_t v8::V8::GetVirtualMemoryCageSizeInBytes() {
-  if (!i::GetProcessWideVirtualMemoryCage()->is_initialized()) {
+size_t v8::V8::GetSandboxSizeInBytes() {
+  if (!i::GetProcessWideSandbox()->is_initialized()) {
     return 0;
   } else {
-    return i::GetProcessWideVirtualMemoryCage()->size();
+    return i::GetProcessWideSandbox()->size();
   }
 }
 
-bool v8::V8::IsUsingSecureVirtualMemoryCage() {
-  Utils::ApiCheck(i::GetProcessWideVirtualMemoryCage()->is_initialized(),
-                  "v8::V8::IsUsingSecureVirtualMemoryCage",
-                  "The virtual memory cage must be initialized first.");
-  // TODO(saelo) For now, we only treat a fake cage as insecure. Once we use
-  // caged pointers that assume that the cage has a constant size, we'll also
-  // treat cages smaller than the default size as insecure because caged
-  // pointers can then access memory outside of them.
-  return !i::GetProcessWideVirtualMemoryCage()->is_fake_cage();
+bool v8::V8::IsSandboxConfiguredSecurely() {
+  Utils::ApiCheck(i::GetProcessWideSandbox()->is_initialized(),
+                  "v8::V8::IsSandoxConfiguredSecurely",
+                  "The sandbox must be initialized first.");
+  // TODO(saelo) For now, we only treat a partially reserved sandbox as
+  // insecure. Once we use sandboxed pointers, which assume that the sandbox
+  // has a fixed size, we'll also treat sandboxes with a smaller size as
+  // insecure because these pointers can then access memory outside of them.
+  return !i::GetProcessWideSandbox()->is_partially_reserved();
 }
 #endif
 
diff --git a/src/base/bounded-page-allocator.h b/src/base/bounded-page-allocator.h
index 07c5cda307..ade9aa2d34 100644
--- a/src/base/bounded-page-allocator.h
+++ b/src/base/bounded-page-allocator.h
@@ -27,7 +27,6 @@ enum class PageInitializationMode {
 // pre-reserved region of virtual space. This class requires the virtual space
 // to be kept reserved during the lifetime of this object.
 // The main application of bounded page allocator are
-//  - the V8 virtual memory cage
 //  - V8 heap pointer compression which requires the whole V8 heap to be
 //    allocated within a contiguous range of virtual address space,
 //  - executable page allocation, which allows to use PC-relative 32-bit code
diff --git a/src/base/platform/platform.h b/src/base/platform/platform.h
index 53a7267889..fa7d1aa4e1 100644
--- a/src/base/platform/platform.h
+++ b/src/base/platform/platform.h
@@ -144,10 +144,10 @@ class V8_BASE_EXPORT OS {
   // On Windows, ensure the newer memory API is loaded if available.  This
   // includes function like VirtualAlloc2 and MapViewOfFile3.
   // TODO(chromium:1218005) this should probably happen as part of Initialize,
-  // but that is currently invoked too late, after the virtual memory cage
-  // is initialized. However, eventually the virtual memory cage initialization
-  // will happen as part of V8::Initialize, at which point this function can
-  // probably be merged into OS::Initialize.
+  // but that is currently invoked too late, after the sandbox is initialized.
+  // However, eventually the sandbox initialization will probably happen as
+  // part of V8::Initialize, at which point this function can probably be
+  // merged into OS::Initialize.
   static void EnsureWin32MemoryAPILoaded();
 #endif
 
diff --git a/src/builtins/builtins-typed-array-gen.cc b/src/builtins/builtins-typed-array-gen.cc
index 00b040f03f..6f2b4e8e55 100644
--- a/src/builtins/builtins-typed-array-gen.cc
+++ b/src/builtins/builtins-typed-array-gen.cc
@@ -65,8 +65,8 @@ TNode<JSArrayBuffer> TypedArrayBuiltinsAssembler::AllocateEmptyOnHeapBuffer(
 
   StoreObjectFieldNoWriteBarrier(buffer, JSArrayBuffer::kByteLengthOffset,
                                  UintPtrConstant(0));
-  StoreCagedPointerToObject(buffer, JSArrayBuffer::kBackingStoreOffset,
-                            EmptyBackingStoreBufferConstant());
+  StoreSandboxedPointerToObject(buffer, JSArrayBuffer::kBackingStoreOffset,
+                                EmptyBackingStoreBufferConstant());
   StoreObjectFieldNoWriteBarrier(buffer, JSArrayBuffer::kExtensionOffset,
                                  IntPtrConstant(0));
   for (int offset = JSArrayBuffer::kHeaderSize;
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index 58920c343a..2e5002df5f 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -3072,40 +3072,40 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   Bind(&done);
 }
 
-void TurboAssembler::EncodeCagedPointer(const Register& value) {
+void TurboAssembler::EncodeSandboxedPointer(const Register& value) {
   ASM_CODE_COMMENT(this);
-#ifdef V8_CAGED_POINTERS
+#ifdef V8_SANDBOXED_POINTERS
   Sub(value, value, kPtrComprCageBaseRegister);
-  Mov(value, Operand(value, LSL, kCagedPointerShift));
+  Mov(value, Operand(value, LSL, kSandboxedPointerShift));
 #else
   UNREACHABLE();
 #endif
 }
 
-void TurboAssembler::DecodeCagedPointer(const Register& value) {
+void TurboAssembler::DecodeSandboxedPointer(const Register& value) {
   ASM_CODE_COMMENT(this);
-#ifdef V8_CAGED_POINTERS
+#ifdef V8_SANDBOXED_POINTERS
   Add(value, kPtrComprCageBaseRegister,
-      Operand(value, LSR, kCagedPointerShift));
+      Operand(value, LSR, kSandboxedPointerShift));
 #else
   UNREACHABLE();
 #endif
 }
 
-void TurboAssembler::LoadCagedPointerField(const Register& destination,
-                                           const MemOperand& field_operand) {
+void TurboAssembler::LoadSandboxedPointerField(
+    const Register& destination, const MemOperand& field_operand) {
   ASM_CODE_COMMENT(this);
   Ldr(destination, field_operand);
-  DecodeCagedPointer(destination);
+  DecodeSandboxedPointer(destination);
 }
 
-void TurboAssembler::StoreCagedPointerField(
+void TurboAssembler::StoreSandboxedPointerField(
     const Register& value, const MemOperand& dst_field_operand) {
   ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.AcquireX();
   Mov(scratch, value);
-  EncodeCagedPointer(scratch);
+  EncodeSandboxedPointer(scratch);
   Str(scratch, dst_field_operand);
 }
 
@@ -3115,7 +3115,7 @@ void TurboAssembler::LoadExternalPointerField(Register destination,
                                               Register isolate_root) {
   DCHECK(!AreAliased(destination, isolate_root));
   ASM_CODE_COMMENT(this);
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   UseScratchRegisterScope temps(this);
   Register external_table = temps.AcquireX();
   if (isolate_root == no_reg) {
@@ -3134,7 +3134,7 @@ void TurboAssembler::LoadExternalPointerField(Register destination,
   }
 #else
   Ldr(destination, field_operand);
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 void TurboAssembler::MaybeSaveRegisters(RegList registers) {
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 7c972bd307..b59d3ad946 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1439,23 +1439,23 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void I64x2AllTrue(Register dst, VRegister src);
 
   // ---------------------------------------------------------------------------
-  // V8 Heap sandbox support
+  // V8 Sandbox support
 
-  // Transform a CagedPointer from/to its encoded form, which is used when the
-  // pointer is stored on the heap and ensures that the pointer will always
-  // point into the virtual memory cage.
-  void EncodeCagedPointer(const Register& value);
-  void DecodeCagedPointer(const Register& value);
+  // Transform a SandboxedPointer from/to its encoded form, which is used when
+  // the pointer is stored on the heap and ensures that the pointer will always
+  // point into the sandbox.
+  void EncodeSandboxedPointer(const Register& value);
+  void DecodeSandboxedPointer(const Register& value);
 
-  // Load and decode a CagedPointer from the heap.
-  void LoadCagedPointerField(const Register& destination,
-                             const MemOperand& field_operand);
-  // Encode and store a CagedPointer to the heap.
-  void StoreCagedPointerField(const Register& value,
-                              const MemOperand& dst_field_operand);
+  // Load and decode a SandboxedPointer from the heap.
+  void LoadSandboxedPointerField(const Register& destination,
+                                 const MemOperand& field_operand);
+  // Encode and store a SandboxedPointer to the heap.
+  void StoreSandboxedPointerField(const Register& value,
+                                  const MemOperand& dst_field_operand);
 
   // Loads a field containing off-heap pointer and does necessary decoding
-  // if V8 heap sandbox is enabled.
+  // if sandboxed external pointers are enabled.
   void LoadExternalPointerField(Register destination, MemOperand field_operand,
                                 ExternalPointerTag tag,
                                 Register isolate_root = Register::no_reg());
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 9ccac02abb..970ebff744 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -1540,48 +1540,48 @@ void CodeStubAssembler::BranchIfToBooleanIsTrue(TNode<Object> value,
   }
 }
 
-TNode<RawPtrT> CodeStubAssembler::LoadCagedPointerFromObject(
+TNode<RawPtrT> CodeStubAssembler::LoadSandboxedPointerFromObject(
     TNode<HeapObject> object, TNode<IntPtrT> field_offset) {
-#ifdef V8_CAGED_POINTERS
+#ifdef V8_SANDBOXED_POINTERS
   return ReinterpretCast<RawPtrT>(
-      LoadObjectField<CagedPtrT>(object, field_offset));
+      LoadObjectField<SandboxedPtrT>(object, field_offset));
 #else
   return LoadObjectField<RawPtrT>(object, field_offset);
-#endif  // V8_CAGED_POINTERS
+#endif  // V8_SANDBOXED_POINTERS
 }
 
-void CodeStubAssembler::StoreCagedPointerToObject(TNode<HeapObject> object,
-                                                  TNode<IntPtrT> offset,
-                                                  TNode<RawPtrT> pointer) {
-#ifdef V8_CAGED_POINTERS
-  TNode<CagedPtrT> caged_pointer = ReinterpretCast<CagedPtrT>(pointer);
+void CodeStubAssembler::StoreSandboxedPointerToObject(TNode<HeapObject> object,
+                                                      TNode<IntPtrT> offset,
+                                                      TNode<RawPtrT> pointer) {
+#ifdef V8_SANDBOXED_POINTERS
+  TNode<SandboxedPtrT> sbx_ptr = ReinterpretCast<SandboxedPtrT>(pointer);
 #ifdef DEBUG
-  // Verify pointer points into the cage.
-  TNode<ExternalReference> cage_base_address =
-      ExternalConstant(ExternalReference::virtual_memory_cage_base_address());
-  TNode<ExternalReference> cage_end_address =
-      ExternalConstant(ExternalReference::virtual_memory_cage_end_address());
-  TNode<UintPtrT> cage_base = Load<UintPtrT>(cage_base_address);
-  TNode<UintPtrT> cage_end = Load<UintPtrT>(cage_end_address);
-  CSA_DCHECK(this, UintPtrGreaterThanOrEqual(caged_pointer, cage_base));
-  CSA_DCHECK(this, UintPtrLessThan(caged_pointer, cage_end));
+  // Verify pointer points into the sandbox.
+  TNode<ExternalReference> sandbox_base_address =
+      ExternalConstant(ExternalReference::sandbox_base_address());
+  TNode<ExternalReference> sandbox_end_address =
+      ExternalConstant(ExternalReference::sandbox_end_address());
+  TNode<UintPtrT> sandbox_base = Load<UintPtrT>(sandbox_base_address);
+  TNode<UintPtrT> sandbox_end = Load<UintPtrT>(sandbox_end_address);
+  CSA_DCHECK(this, UintPtrGreaterThanOrEqual(sbx_ptr, sandbox_base));
+  CSA_DCHECK(this, UintPtrLessThan(sbx_ptr, sandbox_end));
 #endif  // DEBUG
-  StoreObjectFieldNoWriteBarrier<CagedPtrT>(object, offset, caged_pointer);
+  StoreObjectFieldNoWriteBarrier<SandboxedPtrT>(object, offset, sbx_ptr);
 #else
   StoreObjectFieldNoWriteBarrier<RawPtrT>(object, offset, pointer);
-#endif  // V8_CAGED_POINTERS
+#endif  // V8_SANDBOXED_POINTERS
 }
 
 TNode<RawPtrT> CodeStubAssembler::EmptyBackingStoreBufferConstant() {
-#ifdef V8_CAGED_POINTERS
-  // TODO(chromium:1218005) consider creating a LoadCagedPointerConstant() if
-  // more of these constants are required later on.
+#ifdef V8_SANDBOXED_POINTERS
+  // TODO(chromium:1218005) consider creating a LoadSandboxedPointerConstant()
+  // if more of these constants are required later on.
   TNode<ExternalReference> empty_backing_store_buffer =
       ExternalConstant(ExternalReference::empty_backing_store_buffer());
   return Load<RawPtrT>(empty_backing_store_buffer);
 #else
   return ReinterpretCast<RawPtrT>(IntPtrConstant(0));
-#endif  // V8_CAGED_POINTERS
+#endif  // V8_SANDBOXED_POINTERS
 }
 
 TNode<ExternalPointerT> CodeStubAssembler::ChangeUint32ToExternalPointer(
@@ -1598,7 +1598,7 @@ TNode<Uint32T> CodeStubAssembler::ChangeExternalPointerToUint32(
 
 void CodeStubAssembler::InitializeExternalPointerField(TNode<HeapObject> object,
                                                        TNode<IntPtrT> offset) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   TNode<ExternalReference> external_pointer_table_address = ExternalConstant(
       ExternalReference::external_pointer_table_address(isolate()));
   TNode<Uint32T> table_length = UncheckedCast<Uint32T>(
@@ -1639,7 +1639,7 @@ void CodeStubAssembler::InitializeExternalPointerField(TNode<HeapObject> object,
 TNode<RawPtrT> CodeStubAssembler::LoadExternalPointerFromObject(
     TNode<HeapObject> object, TNode<IntPtrT> offset,
     ExternalPointerTag external_pointer_tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   TNode<ExternalReference> external_pointer_table_address = ExternalConstant(
       ExternalReference::external_pointer_table_address(isolate()));
   TNode<RawPtrT> table = UncheckedCast<RawPtrT>(
@@ -1661,13 +1661,13 @@ TNode<RawPtrT> CodeStubAssembler::LoadExternalPointerFromObject(
   return UncheckedCast<RawPtrT>(UncheckedCast<WordT>(entry));
 #else
   return LoadObjectField<RawPtrT>(object, offset);
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 void CodeStubAssembler::StoreExternalPointerToObject(
     TNode<HeapObject> object, TNode<IntPtrT> offset, TNode<RawPtrT> pointer,
     ExternalPointerTag external_pointer_tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   TNode<ExternalReference> external_pointer_table_address = ExternalConstant(
       ExternalReference::external_pointer_table_address(isolate()));
   TNode<RawPtrT> table = UncheckedCast<RawPtrT>(
@@ -1690,7 +1690,7 @@ void CodeStubAssembler::StoreExternalPointerToObject(
                       value);
 #else
   StoreObjectFieldNoWriteBarrier<RawPtrT>(object, offset, pointer);
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 TNode<Object> CodeStubAssembler::LoadFromParentFrame(int offset) {
@@ -13893,8 +13893,8 @@ void CodeStubAssembler::ThrowIfArrayBufferViewBufferIsDetached(
 
 TNode<RawPtrT> CodeStubAssembler::LoadJSArrayBufferBackingStorePtr(
     TNode<JSArrayBuffer> array_buffer) {
-  return LoadCagedPointerFromObject(array_buffer,
-                                    JSArrayBuffer::kBackingStoreOffset);
+  return LoadSandboxedPointerFromObject(array_buffer,
+                                        JSArrayBuffer::kBackingStoreOffset);
 }
 
 TNode<JSArrayBuffer> CodeStubAssembler::LoadJSArrayBufferViewBuffer(
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index 5fa6231f0f..4d8e630e56 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -27,7 +27,7 @@
 #include "src/objects/swiss-name-dictionary.h"
 #include "src/objects/tagged-index.h"
 #include "src/roots/roots.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "torque-generated/exported-macros-assembler.h"
 
 namespace v8 {
@@ -1048,22 +1048,23 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   //
 
   // Load a caged pointer value from an object.
-  TNode<RawPtrT> LoadCagedPointerFromObject(TNode<HeapObject> object,
-                                            int offset) {
-    return LoadCagedPointerFromObject(object, IntPtrConstant(offset));
+  TNode<RawPtrT> LoadSandboxedPointerFromObject(TNode<HeapObject> object,
+                                                int offset) {
+    return LoadSandboxedPointerFromObject(object, IntPtrConstant(offset));
   }
 
-  TNode<RawPtrT> LoadCagedPointerFromObject(TNode<HeapObject> object,
-                                            TNode<IntPtrT> offset);
+  TNode<RawPtrT> LoadSandboxedPointerFromObject(TNode<HeapObject> object,
+                                                TNode<IntPtrT> offset);
 
   // Stored a caged pointer value to an object.
-  void StoreCagedPointerToObject(TNode<HeapObject> object, int offset,
-                                 TNode<RawPtrT> pointer) {
-    StoreCagedPointerToObject(object, IntPtrConstant(offset), pointer);
+  void StoreSandboxedPointerToObject(TNode<HeapObject> object, int offset,
+                                     TNode<RawPtrT> pointer) {
+    StoreSandboxedPointerToObject(object, IntPtrConstant(offset), pointer);
   }
 
-  void StoreCagedPointerToObject(TNode<HeapObject> object,
-                                 TNode<IntPtrT> offset, TNode<RawPtrT> pointer);
+  void StoreSandboxedPointerToObject(TNode<HeapObject> object,
+                                     TNode<IntPtrT> offset,
+                                     TNode<RawPtrT> pointer);
 
   TNode<RawPtrT> EmptyBackingStoreBufferConstant();
 
@@ -1145,14 +1146,14 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 
   TNode<RawPtrT> LoadJSTypedArrayExternalPointerPtr(
       TNode<JSTypedArray> holder) {
-    return LoadCagedPointerFromObject(holder,
-                                      JSTypedArray::kExternalPointerOffset);
+    return LoadSandboxedPointerFromObject(holder,
+                                          JSTypedArray::kExternalPointerOffset);
   }
 
   void StoreJSTypedArrayExternalPointerPtr(TNode<JSTypedArray> holder,
                                            TNode<RawPtrT> value) {
-    StoreCagedPointerToObject(holder, JSTypedArray::kExternalPointerOffset,
-                              value);
+    StoreSandboxedPointerToObject(holder, JSTypedArray::kExternalPointerOffset,
+                                  value);
   }
 
   // Load value from current parent frame by given offset in bytes.
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 075eaf8c09..aac1e3a2f6 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -226,28 +226,28 @@ ExternalReference ExternalReference::handle_scope_implementer_address(
   return ExternalReference(isolate->handle_scope_implementer_address());
 }
 
-#ifdef V8_CAGED_POINTERS
-ExternalReference ExternalReference::virtual_memory_cage_base_address() {
-  return ExternalReference(GetProcessWideVirtualMemoryCage()->base_address());
+#ifdef V8_SANDBOXED_POINTERS
+ExternalReference ExternalReference::sandbox_base_address() {
+  return ExternalReference(GetProcessWideSandbox()->base_address());
 }
 
-ExternalReference ExternalReference::virtual_memory_cage_end_address() {
-  return ExternalReference(GetProcessWideVirtualMemoryCage()->end_address());
+ExternalReference ExternalReference::sandbox_end_address() {
+  return ExternalReference(GetProcessWideSandbox()->end_address());
 }
 
 ExternalReference ExternalReference::empty_backing_store_buffer() {
-  return ExternalReference(GetProcessWideVirtualMemoryCage()
+  return ExternalReference(GetProcessWideSandbox()
                                ->constants()
                                .empty_backing_store_buffer_address());
 }
-#endif  // V8_CAGED_POINTERS
+#endif  // V8_SANDBOXED_POINTERS
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
 ExternalReference ExternalReference::external_pointer_table_address(
     Isolate* isolate) {
   return ExternalReference(isolate->external_pointer_table_address());
 }
-#endif
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 
 ExternalReference ExternalReference::interpreter_dispatch_table_address(
     Isolate* isolate) {
@@ -1377,7 +1377,7 @@ FUNCTION_REFERENCE(
     js_finalization_registry_remove_cell_from_unregister_token_map,
     JSFinalizationRegistry::RemoveCellFromUnregisterTokenMap)
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
 FUNCTION_REFERENCE(external_pointer_table_grow_table_function,
                    ExternalPointerTable::GrowTable)
 #endif
diff --git a/src/codegen/external-reference.h b/src/codegen/external-reference.h
index a0c27d207e..496784fbd1 100644
--- a/src/codegen/external-reference.h
+++ b/src/codegen/external-reference.h
@@ -80,7 +80,7 @@ class StatsCounter;
     "Isolate::thread_in_wasm_flag_address_address")                            \
   V(javascript_execution_assert, "javascript_execution_assert")                \
   EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_EXTERNAL_CODE_SPACE(V)                  \
-  EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_HEAP_SANDBOX(V)
+  EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_SANDBOXED_EXTERNAL_POINTERS(V)
 
 #ifdef V8_EXTERNAL_CODE_SPACE
 #define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_EXTERNAL_CODE_SPACE(V) \
@@ -89,14 +89,14 @@ class StatsCounter;
 #define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_EXTERNAL_CODE_SPACE(V)
 #endif  // V8_EXTERNAL_CODE_SPACE
 
-#ifdef V8_HEAP_SANDBOX
-#define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_HEAP_SANDBOX(V) \
-  V(external_pointer_table_address,                          \
-    "Isolate::external_pointer_table_address("               \
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
+#define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_SANDBOXED_EXTERNAL_POINTERS(V) \
+  V(external_pointer_table_address,                                         \
+    "Isolate::external_pointer_table_address("                              \
     ")")
 #else
-#define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_HEAP_SANDBOX(V)
-#endif  // V8_HEAP_SANDBOX
+#define EXTERNAL_REFERENCE_LIST_WITH_ISOLATE_SANDBOXED_EXTERNAL_POINTERS(V)
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 
 #define EXTERNAL_REFERENCE_LIST(V)                                             \
   V(abort_with_reason, "abort_with_reason")                                    \
@@ -306,8 +306,8 @@ class StatsCounter;
   V(re_experimental_match_for_call_from_js,                                    \
     "ExperimentalRegExp::MatchForCallFromJs")                                  \
   EXTERNAL_REFERENCE_LIST_INTL(V)                                              \
-  EXTERNAL_REFERENCE_LIST_VIRTUAL_MEMORY_CAGE(V)                               \
-  EXTERNAL_REFERENCE_LIST_HEAP_SANDBOX(V)
+  EXTERNAL_REFERENCE_LIST_SANDBOX(V)                                           \
+  EXTERNAL_REFERENCE_LIST_SANDBOXED_EXTERNAL_POINTERS(V)
 #ifdef V8_INTL_SUPPORT
 #define EXTERNAL_REFERENCE_LIST_INTL(V)                               \
   V(intl_convert_one_byte_to_lower, "intl_convert_one_byte_to_lower") \
@@ -318,22 +318,22 @@ class StatsCounter;
 #define EXTERNAL_REFERENCE_LIST_INTL(V)
 #endif  // V8_INTL_SUPPORT
 
-#ifdef V8_CAGED_POINTERS
-#define EXTERNAL_REFERENCE_LIST_VIRTUAL_MEMORY_CAGE(V)               \
-  V(virtual_memory_cage_base_address, "V8VirtualMemoryCage::base()") \
-  V(virtual_memory_cage_end_address, "V8VirtualMemoryCage::end()")   \
+#ifdef V8_SANDBOXED_POINTERS
+#define EXTERNAL_REFERENCE_LIST_SANDBOX(V)   \
+  V(sandbox_base_address, "Sandbox::base()") \
+  V(sandbox_end_address, "Sandbox::end()")   \
   V(empty_backing_store_buffer, "EmptyBackingStoreBuffer()")
 #else
-#define EXTERNAL_REFERENCE_LIST_VIRTUAL_MEMORY_CAGE(V)
-#endif  // V8_CAGED_POINTERS
+#define EXTERNAL_REFERENCE_LIST_SANDBOX(V)
+#endif  // V8_SANDBOXED_POINTERS
 
-#ifdef V8_HEAP_SANDBOX
-#define EXTERNAL_REFERENCE_LIST_HEAP_SANDBOX(V) \
-  V(external_pointer_table_grow_table_function, \
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
+#define EXTERNAL_REFERENCE_LIST_SANDBOXED_EXTERNAL_POINTERS(V) \
+  V(external_pointer_table_grow_table_function,                \
     "ExternalPointerTable::GrowTable")
 #else
-#define EXTERNAL_REFERENCE_LIST_HEAP_SANDBOX(V)
-#endif  // V8_HEAP_SANDBOX
+#define EXTERNAL_REFERENCE_LIST_SANDBOXED_EXTERNAL_POINTERS(V)
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 
 // An ExternalReference represents a C++ address used in the generated
 // code. All references to C++ functions and variables must be encapsulated
diff --git a/src/codegen/machine-type.cc b/src/codegen/machine-type.cc
index 5679563bd1..2059f05b87 100644
--- a/src/codegen/machine-type.cc
+++ b/src/codegen/machine-type.cc
@@ -57,8 +57,8 @@ const char* MachineReprToString(MachineRepresentation rep) {
       return "kRepCompressed";
     case MachineRepresentation::kMapWord:
       return "kRepMapWord";
-    case MachineRepresentation::kCagedPointer:
-      return "kRepCagedPointer";
+    case MachineRepresentation::kSandboxedPointer:
+      return "kRepSandboxedPointer";
   }
   UNREACHABLE();
 }
diff --git a/src/codegen/machine-type.h b/src/codegen/machine-type.h
index 981ac9783f..f77d98dfff 100644
--- a/src/codegen/machine-type.h
+++ b/src/codegen/machine-type.h
@@ -41,8 +41,8 @@ enum class MachineRepresentation : uint8_t {
   kCompressedPointer,  // (compressed) HeapObject
   kCompressed,         // (compressed) Object (Smi or HeapObject)
   // A 64-bit pointer encoded in a way (e.g. as offset) that guarantees it will
-  // point into the virtual memory cage.
-  kCagedPointer,
+  // point into the sandbox.
+  kSandboxedPointer,
   // FP and SIMD representations must be last, and in order of increasing size.
   kFloat32,
   kFloat64,
@@ -225,8 +225,8 @@ class MachineType {
     return MachineType(MachineRepresentation::kCompressed,
                        MachineSemantic::kAny);
   }
-  constexpr static MachineType CagedPointer() {
-    return MachineType(MachineRepresentation::kCagedPointer,
+  constexpr static MachineType SandboxedPointer() {
+    return MachineType(MachineRepresentation::kSandboxedPointer,
                        MachineSemantic::kNone);
   }
   constexpr static MachineType Bool() {
@@ -267,8 +267,8 @@ class MachineType {
         return MachineType::AnyCompressed();
       case MachineRepresentation::kCompressedPointer:
         return MachineType::CompressedPointer();
-      case MachineRepresentation::kCagedPointer:
-        return MachineType::CagedPointer();
+      case MachineRepresentation::kSandboxedPointer:
+        return MachineType::SandboxedPointer();
       default:
         UNREACHABLE();
     }
@@ -362,7 +362,7 @@ V8_EXPORT_PRIVATE inline constexpr int ElementSizeLog2Of(
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
       return kTaggedSizeLog2;
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       return kSystemPointerSizeLog2;
     default:
       UNREACHABLE();
diff --git a/src/codegen/tnode.h b/src/codegen/tnode.h
index 4dfb9a1741..2d9b16d4f4 100644
--- a/src/codegen/tnode.h
+++ b/src/codegen/tnode.h
@@ -35,9 +35,9 @@ struct RawPtrT : WordT {
   static constexpr MachineType kMachineType = MachineType::Pointer();
 };
 
-// A RawPtrT that is guaranteed to point into the virtual memory cage.
-struct CagedPtrT : WordT {
-  static constexpr MachineType kMachineType = MachineType::CagedPointer();
+// A RawPtrT that is guaranteed to point into the sandbox.
+struct SandboxedPtrT : WordT {
+  static constexpr MachineType kMachineType = MachineType::SandboxedPointer();
 };
 
 template <class To>
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 6ac8017ca8..04950059ce 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -27,7 +27,7 @@
 #include "src/logging/counters.h"
 #include "src/objects/objects-inl.h"
 #include "src/objects/smi.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/snapshot/snapshot.h"
 
 // Satisfy cpplint check, but don't include platform-specific header. It is
@@ -376,40 +376,40 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   }
 }
 
-void TurboAssembler::EncodeCagedPointer(Register value) {
+void TurboAssembler::EncodeSandboxedPointer(Register value) {
   ASM_CODE_COMMENT(this);
-#ifdef V8_CAGED_POINTERS
+#ifdef V8_SANDBOXED_POINTERS
   subq(value, kPtrComprCageBaseRegister);
-  shlq(value, Immediate(kCagedPointerShift));
+  shlq(value, Immediate(kSandboxedPointerShift));
 #else
   UNREACHABLE();
 #endif
 }
 
-void TurboAssembler::DecodeCagedPointer(Register value) {
+void TurboAssembler::DecodeSandboxedPointer(Register value) {
   ASM_CODE_COMMENT(this);
-#ifdef V8_CAGED_POINTERS
-  shrq(value, Immediate(kCagedPointerShift));
+#ifdef V8_SANDBOXED_POINTERS
+  shrq(value, Immediate(kSandboxedPointerShift));
   addq(value, kPtrComprCageBaseRegister);
 #else
   UNREACHABLE();
 #endif
 }
 
-void TurboAssembler::LoadCagedPointerField(Register destination,
-                                           Operand field_operand) {
+void TurboAssembler::LoadSandboxedPointerField(Register destination,
+                                               Operand field_operand) {
   ASM_CODE_COMMENT(this);
   movq(destination, field_operand);
-  DecodeCagedPointer(destination);
+  DecodeSandboxedPointer(destination);
 }
 
-void TurboAssembler::StoreCagedPointerField(Operand dst_field_operand,
-                                            Register value) {
+void TurboAssembler::StoreSandboxedPointerField(Operand dst_field_operand,
+                                                Register value) {
   ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(value, kScratchRegister));
   DCHECK(!dst_field_operand.AddressUsesRegister(kScratchRegister));
   movq(kScratchRegister, value);
-  EncodeCagedPointer(kScratchRegister);
+  EncodeSandboxedPointer(kScratchRegister);
   movq(dst_field_operand, kScratchRegister);
 }
 
@@ -417,7 +417,7 @@ void TurboAssembler::LoadExternalPointerField(
     Register destination, Operand field_operand, ExternalPointerTag tag,
     Register scratch, IsolateRootLocation isolateRootLocation) {
   DCHECK(!AreAliased(destination, scratch));
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   DCHECK(!field_operand.AddressUsesRegister(scratch));
   if (isolateRootLocation == IsolateRootLocation::kInRootRegister) {
     DCHECK(root_array_available_);
@@ -438,7 +438,7 @@ void TurboAssembler::LoadExternalPointerField(
   }
 #else
   movq(destination, field_operand);
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 void TurboAssembler::MaybeSaveRegisters(RegList registers) {
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 262162ded0..634bc72737 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -593,22 +593,22 @@ class V8_EXPORT_PRIVATE TurboAssembler
   void DecompressAnyTagged(Register destination, Operand field_operand);
 
   // ---------------------------------------------------------------------------
-  // V8 Heap sandbox support
+  // V8 Sandbox support
 
-  // Transform a CagedPointer from/to its encoded form, which is used when the
-  // pointer is stored on the heap and ensures that the pointer will always
-  // point into the virtual memory cage.
-  void EncodeCagedPointer(Register value);
-  void DecodeCagedPointer(Register value);
+  // Transform a SandboxedPointer from/to its encoded form, which is used when
+  // the pointer is stored on the heap and ensures that the pointer will always
+  // point into the sandbox.
+  void EncodeSandboxedPointer(Register value);
+  void DecodeSandboxedPointer(Register value);
 
-  // Load and decode a CagedPointer from the heap.
-  void LoadCagedPointerField(Register destination, Operand field_operand);
-  // Encode and store a CagedPointer to the heap.
-  void StoreCagedPointerField(Operand dst_field_operand, Register value);
+  // Load and decode a SandboxedPointer from the heap.
+  void LoadSandboxedPointerField(Register destination, Operand field_operand);
+  // Encode and store a SandboxedPointer to the heap.
+  void StoreSandboxedPointerField(Operand dst_field_operand, Register value);
 
   enum class IsolateRootLocation { kInScratchRegister, kInRootRegister };
   // Loads a field containing off-heap pointer and does necessary decoding
-  // if V8 heap sandbox is enabled.
+  // if sandboxed external pointers are enabled.
   void LoadExternalPointerField(Register destination, Operand field_operand,
                                 ExternalPointerTag tag, Register scratch,
                                 IsolateRootLocation isolateRootLocation =
diff --git a/src/compiler-dispatcher/lazy-compile-dispatcher.cc b/src/compiler-dispatcher/lazy-compile-dispatcher.cc
index a67af3fd61..8e611c3785 100644
--- a/src/compiler-dispatcher/lazy-compile-dispatcher.cc
+++ b/src/compiler-dispatcher/lazy-compile-dispatcher.cc
@@ -24,7 +24,7 @@
 #include "src/parsing/parse-info.h"
 #include "src/parsing/parser.h"
 #include "src/roots/roots.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/tasks/cancelable-task.h"
 #include "src/tasks/task-utils.h"
 #include "src/zone/zone-list-inl.h"  // crbug.com/v8/8816
diff --git a/src/compiler/access-builder.cc b/src/compiler/access-builder.cc
index f929b98b0c..642df6b2bb 100644
--- a/src/compiler/access-builder.cc
+++ b/src/compiler/access-builder.cc
@@ -421,9 +421,9 @@ FieldAccess AccessBuilder::ForJSTypedArrayExternalPointer() {
       JSTypedArray::kExternalPointerOffset,
       MaybeHandle<Name>(),
       MaybeHandle<Map>(),
-#ifdef V8_CAGED_POINTERS
-      Type::CagedPointer(),
-      MachineType::CagedPointer(),
+#ifdef V8_SANDBOXED_POINTERS
+      Type::SandboxedPointer(),
+      MachineType::SandboxedPointer(),
 #else
       Type::ExternalPointer(),
       MachineType::Pointer(),
@@ -442,9 +442,9 @@ FieldAccess AccessBuilder::ForJSDataViewDataPointer() {
       JSDataView::kDataPointerOffset,
       MaybeHandle<Name>(),
       MaybeHandle<Map>(),
-#ifdef V8_CAGED_POINTERS
-      Type::CagedPointer(),
-      MachineType::CagedPointer(),
+#ifdef V8_SANDBOXED_POINTERS
+      Type::SandboxedPointer(),
+      MachineType::SandboxedPointer(),
 #else
       Type::ExternalPointer(),
       MachineType::Pointer(),
@@ -753,13 +753,13 @@ FieldAccess AccessBuilder::ForExternalStringResourceData() {
       ExternalString::kResourceDataOffset,
       Handle<Name>(),
       MaybeHandle<Map>(),
-      V8_HEAP_SANDBOX_BOOL ? Type::SandboxedExternalPointer()
-                           : Type::ExternalPointer(),
+      V8_SANDBOXED_EXTERNAL_POINTERS_BOOL ? Type::SandboxedExternalPointer()
+                                          : Type::ExternalPointer(),
       MachineType::Pointer(),
       kNoWriteBarrier,
       ConstFieldInfo::None(),
       false,
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
       kExternalStringResourceDataTag,
 #endif
   };
diff --git a/src/compiler/backend/arm/instruction-selector-arm.cc b/src/compiler/backend/arm/instruction-selector-arm.cc
index 3ad4e720c4..590a06b7d6 100644
--- a/src/compiler/backend/arm/instruction-selector-arm.cc
+++ b/src/compiler/backend/arm/instruction-selector-arm.cc
@@ -626,7 +626,7 @@ void InstructionSelector::VisitLoad(Node* node) {
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
@@ -664,7 +664,7 @@ ArchOpcode GetStoreOpcode(MachineRepresentation rep) {
       return kArmVst1S128;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
diff --git a/src/compiler/backend/arm64/code-generator-arm64.cc b/src/compiler/backend/arm64/code-generator-arm64.cc
index 0cc6e5bafe..090a1e9c08 100644
--- a/src/compiler/backend/arm64/code-generator-arm64.cc
+++ b/src/compiler/backend/arm64/code-generator-arm64.cc
@@ -1894,8 +1894,8 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ AtomicDecompressAnyTagged(i.OutputRegister(), i.InputRegister(0),
                                    i.InputRegister(1), i.TempRegister(0));
       break;
-    case kArm64LdrDecodeCagedPointer:
-      __ LoadCagedPointerField(i.OutputRegister(), i.MemoryOperand());
+    case kArm64LdrDecodeSandboxedPointer:
+      __ LoadSandboxedPointerField(i.OutputRegister(), i.MemoryOperand());
       break;
     case kArm64Str:
       EmitOOLTrapIfNeeded(zone(), this, opcode, instr, __ pc_offset());
@@ -1910,8 +1910,9 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ AtomicStoreTaggedField(i.InputRegister(2), i.InputRegister(0),
                                 i.InputRegister(1), i.TempRegister(0));
       break;
-    case kArm64StrEncodeCagedPointer:
-      __ StoreCagedPointerField(i.InputOrZeroRegister64(0), i.MemoryOperand(1));
+    case kArm64StrEncodeSandboxedPointer:
+      __ StoreSandboxedPointerField(i.InputOrZeroRegister64(0),
+                                    i.MemoryOperand(1));
       break;
     case kArm64LdrS:
       EmitOOLTrapIfNeeded(zone(), this, opcode, instr, __ pc_offset());
diff --git a/src/compiler/backend/arm64/instruction-codes-arm64.h b/src/compiler/backend/arm64/instruction-codes-arm64.h
index 46d5314f4f..f52c999106 100644
--- a/src/compiler/backend/arm64/instruction-codes-arm64.h
+++ b/src/compiler/backend/arm64/instruction-codes-arm64.h
@@ -201,8 +201,8 @@ namespace compiler {
   V(Arm64LdarDecompressAnyTagged)                    \
   V(Arm64StrCompressTagged)                          \
   V(Arm64StlrCompressTagged)                         \
-  V(Arm64LdrDecodeCagedPointer)                      \
-  V(Arm64StrEncodeCagedPointer)                      \
+  V(Arm64LdrDecodeSandboxedPointer)                  \
+  V(Arm64StrEncodeSandboxedPointer)                  \
   V(Arm64DmbIsh)                                     \
   V(Arm64DsbIsb)                                     \
   V(Arm64Sxtl)                                       \
diff --git a/src/compiler/backend/arm64/instruction-scheduler-arm64.cc b/src/compiler/backend/arm64/instruction-scheduler-arm64.cc
index 3cffa51e90..a21f454c8b 100644
--- a/src/compiler/backend/arm64/instruction-scheduler-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-scheduler-arm64.cc
@@ -317,7 +317,7 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kArm64LdarDecompressTaggedSigned:
     case kArm64LdarDecompressTaggedPointer:
     case kArm64LdarDecompressAnyTagged:
-    case kArm64LdrDecodeCagedPointer:
+    case kArm64LdrDecodeSandboxedPointer:
     case kArm64Peek:
     case kArm64LoadSplat:
     case kArm64LoadLane:
@@ -341,7 +341,7 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kArm64Str:
     case kArm64StrCompressTagged:
     case kArm64StlrCompressTagged:
-    case kArm64StrEncodeCagedPointer:
+    case kArm64StrEncodeSandboxedPointer:
     case kArm64DmbIsh:
     case kArm64DsbIsb:
     case kArm64StoreLane:
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index 7f34b6594e..a63fb8d9e5 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -839,8 +839,8 @@ void InstructionSelector::VisitLoad(Node* node) {
       opcode = kArm64Ldr;
       immediate_mode = kLoadStoreImm64;
       break;
-    case MachineRepresentation::kCagedPointer:
-      opcode = kArm64LdrDecodeCagedPointer;
+    case MachineRepresentation::kSandboxedPointer:
+      opcode = kArm64LdrDecodeSandboxedPointer;
       immediate_mode = kLoadStoreImm64;
       break;
     case MachineRepresentation::kSimd128:
@@ -943,8 +943,8 @@ void InstructionSelector::VisitStore(Node* node) {
         immediate_mode =
             COMPRESS_POINTERS_BOOL ? kLoadStoreImm32 : kLoadStoreImm64;
         break;
-      case MachineRepresentation::kCagedPointer:
-        opcode = kArm64StrEncodeCagedPointer;
+      case MachineRepresentation::kSandboxedPointer:
+        opcode = kArm64StrEncodeSandboxedPointer;
         immediate_mode = kLoadStoreImm64;
         break;
       case MachineRepresentation::kWord64:
diff --git a/src/compiler/backend/ia32/instruction-selector-ia32.cc b/src/compiler/backend/ia32/instruction-selector-ia32.cc
index 6f92f491e0..3a4fb705b6 100644
--- a/src/compiler/backend/ia32/instruction-selector-ia32.cc
+++ b/src/compiler/backend/ia32/instruction-selector-ia32.cc
@@ -276,7 +276,7 @@ ArchOpcode GetLoadOpcode(LoadRepresentation load_rep) {
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
@@ -632,7 +632,7 @@ ArchOpcode GetStoreOpcode(MachineRepresentation rep) {
       return kIA32Movdqu;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
diff --git a/src/compiler/backend/instruction.cc b/src/compiler/backend/instruction.cc
index 7167ef75eb..3b8f011835 100644
--- a/src/compiler/backend/instruction.cc
+++ b/src/compiler/backend/instruction.cc
@@ -259,7 +259,7 @@ std::ostream& operator<<(std::ostream& os, const InstructionOperand& op) {
         case MachineRepresentation::kCompressed:
           os << "|c";
           break;
-        case MachineRepresentation::kCagedPointer:
+        case MachineRepresentation::kSandboxedPointer:
           os << "|cg";
           break;
         case MachineRepresentation::kMapWord:
@@ -931,7 +931,7 @@ static MachineRepresentation FilterRepresentation(MachineRepresentation rep) {
     case MachineRepresentation::kSimd128:
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       return rep;
     case MachineRepresentation::kNone:
     case MachineRepresentation::kMapWord:
diff --git a/src/compiler/backend/instruction.h b/src/compiler/backend/instruction.h
index 37a8209b6b..d979acd06f 100644
--- a/src/compiler/backend/instruction.h
+++ b/src/compiler/backend/instruction.h
@@ -553,7 +553,7 @@ class LocationOperand : public InstructionOperand {
       case MachineRepresentation::kTagged:
       case MachineRepresentation::kCompressedPointer:
       case MachineRepresentation::kCompressed:
-      case MachineRepresentation::kCagedPointer:
+      case MachineRepresentation::kSandboxedPointer:
         return true;
       case MachineRepresentation::kBit:
       case MachineRepresentation::kWord8:
diff --git a/src/compiler/backend/loong64/instruction-selector-loong64.cc b/src/compiler/backend/loong64/instruction-selector-loong64.cc
index 10d22fcaa2..4f03f99acd 100644
--- a/src/compiler/backend/loong64/instruction-selector-loong64.cc
+++ b/src/compiler/backend/loong64/instruction-selector-loong64.cc
@@ -467,7 +467,7 @@ void InstructionSelector::VisitLoad(Node* node) {
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
     case MachineRepresentation::kSimd128:
@@ -546,7 +546,7 @@ void InstructionSelector::VisitStore(Node* node) {
         break;
       case MachineRepresentation::kCompressedPointer:  // Fall through.
       case MachineRepresentation::kCompressed:         // Fall through.
-      case MachineRepresentation::kCagedPointer:       // Fall through.
+      case MachineRepresentation::kSandboxedPointer:   // Fall through.
       case MachineRepresentation::kMapWord:            // Fall through.
       case MachineRepresentation::kNone:
       case MachineRepresentation::kSimd128:
diff --git a/src/compiler/backend/mips/instruction-selector-mips.cc b/src/compiler/backend/mips/instruction-selector-mips.cc
index 1278b3e2f7..67a28630a3 100644
--- a/src/compiler/backend/mips/instruction-selector-mips.cc
+++ b/src/compiler/backend/mips/instruction-selector-mips.cc
@@ -370,7 +370,7 @@ void InstructionSelector::VisitLoad(Node* node) {
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
@@ -451,7 +451,7 @@ void InstructionSelector::VisitStore(Node* node) {
         break;
       case MachineRepresentation::kCompressedPointer:  // Fall through.
       case MachineRepresentation::kCompressed:         // Fall through.
-      case MachineRepresentation::kCagedPointer:       // Fall through.
+      case MachineRepresentation::kSandboxedPointer:   // Fall through.
       case MachineRepresentation::kWord64:             // Fall through.
       case MachineRepresentation::kMapWord:            // Fall through.
       case MachineRepresentation::kNone:
@@ -1428,7 +1428,7 @@ void InstructionSelector::VisitUnalignedLoad(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
@@ -1483,7 +1483,7 @@ void InstructionSelector::VisitUnalignedStore(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kWord64:             // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
diff --git a/src/compiler/backend/mips64/instruction-selector-mips64.cc b/src/compiler/backend/mips64/instruction-selector-mips64.cc
index 6b62a7c694..4f5738ddad 100644
--- a/src/compiler/backend/mips64/instruction-selector-mips64.cc
+++ b/src/compiler/backend/mips64/instruction-selector-mips64.cc
@@ -503,7 +503,7 @@ void InstructionSelector::VisitLoad(Node* node) {
       opcode = kMips64MsaLd;
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
@@ -577,7 +577,7 @@ void InstructionSelector::VisitStore(Node* node) {
         break;
       case MachineRepresentation::kCompressedPointer:  // Fall through.
       case MachineRepresentation::kCompressed:         // Fall through.
-      case MachineRepresentation::kCagedPointer:       // Fall through.
+      case MachineRepresentation::kSandboxedPointer:   // Fall through.
       case MachineRepresentation::kMapWord:            // Fall through.
       case MachineRepresentation::kNone:
         UNREACHABLE();
@@ -1861,7 +1861,7 @@ void InstructionSelector::VisitUnalignedLoad(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
@@ -1916,7 +1916,7 @@ void InstructionSelector::VisitUnalignedStore(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:       // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
diff --git a/src/compiler/backend/ppc/instruction-selector-ppc.cc b/src/compiler/backend/ppc/instruction-selector-ppc.cc
index e658a8f2d7..dedd268dde 100644
--- a/src/compiler/backend/ppc/instruction-selector-ppc.cc
+++ b/src/compiler/backend/ppc/instruction-selector-ppc.cc
@@ -193,7 +193,7 @@ static void VisitLoadCommon(InstructionSelector* selector, Node* node,
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:  // Fall through.
+    case MachineRepresentation::kSandboxedPointer:  // Fall through.
 #ifdef V8_COMPRESS_POINTERS
       opcode = kPPC_LoadWordS32;
       mode = kInt16Imm_4ByteAligned;
@@ -339,7 +339,7 @@ void VisitStoreCommon(InstructionSelector* selector, Node* node,
         break;
       case MachineRepresentation::kCompressedPointer:  // Fall through.
       case MachineRepresentation::kCompressed:
-      case MachineRepresentation::kCagedPointer:  // Fall through.
+      case MachineRepresentation::kSandboxedPointer:  // Fall through.
 #ifdef V8_COMPRESS_POINTERS
         opcode = kPPC_StoreCompressTagged;
         break;
diff --git a/src/compiler/backend/register-allocation.h b/src/compiler/backend/register-allocation.h
index 11a4a5b964..33a0854777 100644
--- a/src/compiler/backend/register-allocation.h
+++ b/src/compiler/backend/register-allocation.h
@@ -51,7 +51,7 @@ inline int ByteWidthForStackSlot(MachineRepresentation rep) {
     case MachineRepresentation::kWord16:
     case MachineRepresentation::kWord32:
     case MachineRepresentation::kFloat32:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       return kSystemPointerSize;
     case MachineRepresentation::kTaggedSigned:
     case MachineRepresentation::kTaggedPointer:
diff --git a/src/compiler/backend/riscv64/instruction-selector-riscv64.cc b/src/compiler/backend/riscv64/instruction-selector-riscv64.cc
index 6ec4df95c2..905250d8ef 100644
--- a/src/compiler/backend/riscv64/instruction-selector-riscv64.cc
+++ b/src/compiler/backend/riscv64/instruction-selector-riscv64.cc
@@ -542,7 +542,7 @@ void InstructionSelector::VisitLoad(Node* node) {
 #else
                                                  // Fall through.
 #endif
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
     case MachineRepresentation::kMapWord:  // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
@@ -622,7 +622,7 @@ void InstructionSelector::VisitStore(Node* node) {
 #else
         UNREACHABLE();
 #endif
-      case MachineRepresentation::kCagedPointer:
+      case MachineRepresentation::kSandboxedPointer:
       case MachineRepresentation::kMapWord:            // Fall through.
       case MachineRepresentation::kNone:
         UNREACHABLE();
@@ -1731,7 +1731,7 @@ void InstructionSelector::VisitUnalignedLoad(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
@@ -1786,7 +1786,7 @@ void InstructionSelector::VisitUnalignedStore(Node* node) {
     case MachineRepresentation::kBit:                // Fall through.
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:         // Fall through.
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
     case MachineRepresentation::kMapWord:            // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
diff --git a/src/compiler/backend/s390/instruction-selector-s390.cc b/src/compiler/backend/s390/instruction-selector-s390.cc
index 8fc0830fad..44f77e2312 100644
--- a/src/compiler/backend/s390/instruction-selector-s390.cc
+++ b/src/compiler/backend/s390/instruction-selector-s390.cc
@@ -292,7 +292,7 @@ ArchOpcode SelectLoadOpcode(LoadRepresentation load_rep) {
       break;
     case MachineRepresentation::kCompressedPointer:  // Fall through.
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:  // Fall through.
+    case MachineRepresentation::kSandboxedPointer:  // Fall through.
 #ifdef V8_COMPRESS_POINTERS
       opcode = kS390_LoadWordS32;
       break;
@@ -775,7 +775,7 @@ static void VisitGeneralStore(
         break;
       case MachineRepresentation::kCompressedPointer:  // Fall through.
       case MachineRepresentation::kCompressed:
-      case MachineRepresentation::kCagedPointer:  // Fall through.
+      case MachineRepresentation::kSandboxedPointer:  // Fall through.
 #ifdef V8_COMPRESS_POINTERS
         opcode = kS390_StoreCompressTagged;
         break;
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index ab9e1833a4..b7ab84593a 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -344,8 +344,8 @@ void EmitStore(TurboAssembler* tasm, Operand operand, Register value,
       case MachineRepresentation::kTagged:
         tasm->StoreTaggedField(operand, value);
         break;
-      case MachineRepresentation::kCagedPointer:
-        tasm->StoreCagedPointerField(operand, value);
+      case MachineRepresentation::kSandboxedPointer:
+        tasm->StoreSandboxedPointerField(operand, value);
         break;
       default:
         UNREACHABLE();
@@ -514,11 +514,11 @@ template <std::memory_order order>
 Register GetTSANValueRegister(TurboAssembler* tasm, Register value,
                               X64OperandConverter& i,
                               MachineRepresentation rep) {
-  if (rep == MachineRepresentation::kCagedPointer) {
-    // CagedPointers need to be encoded.
+  if (rep == MachineRepresentation::kSandboxedPointer) {
+    // SandboxedPointers need to be encoded.
     Register value_reg = i.TempRegister(1);
     tasm->movq(value_reg, value);
-    tasm->EncodeCagedPointer(value_reg);
+    tasm->EncodeSandboxedPointer(value_reg);
     return value_reg;
   }
   return value;
@@ -535,9 +535,9 @@ Register GetTSANValueRegister<std::memory_order_relaxed>(
     MachineRepresentation rep) {
   Register value_reg = i.TempRegister(1);
   tasm->movq(value_reg, value);
-  if (rep == MachineRepresentation::kCagedPointer) {
-    // CagedPointers need to be encoded.
-    tasm->EncodeCagedPointer(value_reg);
+  if (rep == MachineRepresentation::kSandboxedPointer) {
+    // SandboxedPointers need to be encoded.
+    tasm->EncodeSandboxedPointer(value_reg);
   }
   return value_reg;
 }
@@ -2386,18 +2386,18 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       break;
     }
-    case kX64MovqDecodeCagedPointer: {
+    case kX64MovqDecodeSandboxedPointer: {
       CHECK(instr->HasOutput());
       Operand address(i.MemoryOperand());
       Register dst = i.OutputRegister();
       __ movq(dst, address);
-      __ DecodeCagedPointer(dst);
+      __ DecodeSandboxedPointer(dst);
       EmitTSANRelaxedLoadOOLIfNeeded(zone(), this, tasm(), address, i,
                                      DetermineStubCallMode(),
                                      kSystemPointerSize);
       break;
     }
-    case kX64MovqEncodeCagedPointer: {
+    case kX64MovqEncodeSandboxedPointer: {
       CHECK(!instr->HasOutput());
       size_t index = 0;
       Operand operand = i.MemoryOperand(&index);
@@ -2405,7 +2405,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register value(i.InputRegister(index));
       EmitTSANAwareStore<std::memory_order_relaxed>(
           zone(), this, tasm(), operand, value, i, DetermineStubCallMode(),
-          MachineRepresentation::kCagedPointer);
+          MachineRepresentation::kSandboxedPointer);
       break;
     }
     case kX64Movq:
diff --git a/src/compiler/backend/x64/instruction-codes-x64.h b/src/compiler/backend/x64/instruction-codes-x64.h
index bf2c9b00c7..c2c6ca946e 100644
--- a/src/compiler/backend/x64/instruction-codes-x64.h
+++ b/src/compiler/backend/x64/instruction-codes-x64.h
@@ -163,8 +163,8 @@ namespace compiler {
   V(X64MovqDecompressTaggedPointer)                  \
   V(X64MovqDecompressAnyTagged)                      \
   V(X64MovqCompressTagged)                           \
-  V(X64MovqEncodeCagedPointer)                       \
-  V(X64MovqDecodeCagedPointer)                       \
+  V(X64MovqEncodeSandboxedPointer)                   \
+  V(X64MovqDecodeSandboxedPointer)                   \
   V(X64BitcastFI)                                    \
   V(X64BitcastDL)                                    \
   V(X64BitcastIF)                                    \
diff --git a/src/compiler/backend/x64/instruction-scheduler-x64.cc b/src/compiler/backend/x64/instruction-scheduler-x64.cc
index a0a972b4e5..4cc187ae06 100644
--- a/src/compiler/backend/x64/instruction-scheduler-x64.cc
+++ b/src/compiler/backend/x64/instruction-scheduler-x64.cc
@@ -396,8 +396,8 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kX64MovqDecompressTaggedPointer:
     case kX64MovqDecompressAnyTagged:
     case kX64MovqCompressTagged:
-    case kX64MovqDecodeCagedPointer:
-    case kX64MovqEncodeCagedPointer:
+    case kX64MovqDecodeSandboxedPointer:
+    case kX64MovqEncodeSandboxedPointer:
     case kX64Movq:
     case kX64Movsd:
     case kX64Movss:
diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index 2c6e4ad671..3aec65221f 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -297,8 +297,8 @@ ArchOpcode GetLoadOpcode(LoadRepresentation load_rep) {
     case MachineRepresentation::kWord64:
       opcode = kX64Movq;
       break;
-    case MachineRepresentation::kCagedPointer:
-      opcode = kX64MovqDecodeCagedPointer;
+    case MachineRepresentation::kSandboxedPointer:
+      opcode = kX64MovqDecodeSandboxedPointer;
       break;
     case MachineRepresentation::kSimd128:
       opcode = kX64Movdqu;
@@ -336,8 +336,8 @@ ArchOpcode GetStoreOpcode(StoreRepresentation store_rep) {
       return kX64MovqCompressTagged;
     case MachineRepresentation::kWord64:
       return kX64Movq;
-    case MachineRepresentation::kCagedPointer:
-      return kX64MovqEncodeCagedPointer;
+    case MachineRepresentation::kSandboxedPointer:
+      return kX64MovqEncodeSandboxedPointer;
     case MachineRepresentation::kSimd128:
       return kX64Movdqu;
     case MachineRepresentation::kNone:  // Fall through.
diff --git a/src/compiler/js-native-context-specialization.cc b/src/compiler/js-native-context-specialization.cc
index c260a7ff9f..7d18b4563b 100644
--- a/src/compiler/js-native-context-specialization.cc
+++ b/src/compiler/js-native-context-specialization.cc
@@ -2557,7 +2557,7 @@ JSNativeContextSpecialization::BuildPropertyStore(
       case MachineRepresentation::kBit:
       case MachineRepresentation::kCompressedPointer:
       case MachineRepresentation::kCompressed:
-      case MachineRepresentation::kCagedPointer:
+      case MachineRepresentation::kSandboxedPointer:
       case MachineRepresentation::kWord8:
       case MachineRepresentation::kWord16:
       case MachineRepresentation::kWord32:
diff --git a/src/compiler/load-elimination.cc b/src/compiler/load-elimination.cc
index 357b866ca7..7f0dee2125 100644
--- a/src/compiler/load-elimination.cc
+++ b/src/compiler/load-elimination.cc
@@ -1070,7 +1070,7 @@ Reduction LoadElimination::ReduceLoadElement(Node* node) {
     case MachineRepresentation::kFloat32:
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       // TODO(turbofan): Add support for doing the truncations.
       break;
     case MachineRepresentation::kFloat64:
@@ -1127,7 +1127,7 @@ Reduction LoadElimination::ReduceStoreElement(Node* node) {
     case MachineRepresentation::kFloat32:
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       // TODO(turbofan): Add support for doing the truncations.
       break;
     case MachineRepresentation::kFloat64:
@@ -1432,7 +1432,7 @@ LoadElimination::IndexRange LoadElimination::FieldIndexOf(
     case MachineRepresentation::kMapWord:
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
       break;
   }
   int representation_size = ElementSizeInBytes(rep);
diff --git a/src/compiler/machine-graph-verifier.cc b/src/compiler/machine-graph-verifier.cc
index 5d2ab6990c..7ff3f2da67 100644
--- a/src/compiler/machine-graph-verifier.cc
+++ b/src/compiler/machine-graph-verifier.cc
@@ -996,7 +996,7 @@ class MachineRepresentationChecker {
         // happens in dead code.
         return IsAnyTagged(actual);
       case MachineRepresentation::kCompressedPointer:
-      case MachineRepresentation::kCagedPointer:
+      case MachineRepresentation::kSandboxedPointer:
       case MachineRepresentation::kFloat32:
       case MachineRepresentation::kFloat64:
       case MachineRepresentation::kSimd128:
diff --git a/src/compiler/machine-operator.cc b/src/compiler/machine-operator.cc
index 56b298eb55..26513dd05d 100644
--- a/src/compiler/machine-operator.cc
+++ b/src/compiler/machine-operator.cc
@@ -667,7 +667,7 @@ std::ostream& operator<<(std::ostream& os, TruncateKind kind) {
   V(MapInHeader)             \
   V(AnyTagged)               \
   V(CompressedPointer)       \
-  V(CagedPointer)            \
+  V(SandboxedPointer)        \
   V(AnyCompressed)
 
 #define MACHINE_REPRESENTATION_LIST(V) \
@@ -683,7 +683,7 @@ std::ostream& operator<<(std::ostream& os, TruncateKind kind) {
   V(kTaggedPointer)                    \
   V(kTagged)                           \
   V(kCompressedPointer)                \
-  V(kCagedPointer)                     \
+  V(kSandboxedPointer)                 \
   V(kCompressed)
 
 #define LOAD_TRANSFORM_LIST(V) \
diff --git a/src/compiler/memory-lowering.cc b/src/compiler/memory-lowering.cc
index a3127e5d96..0a961b501e 100644
--- a/src/compiler/memory-lowering.cc
+++ b/src/compiler/memory-lowering.cc
@@ -13,7 +13,7 @@
 #include "src/compiler/node.h"
 #include "src/compiler/simplified-operator.h"
 #include "src/roots/roots-inl.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 
 #if V8_ENABLE_WEBASSEMBLY
 #include "src/wasm/wasm-linkage.h"
@@ -408,8 +408,8 @@ Reduction MemoryLowering::ReduceLoadElement(Node* node) {
 
 Node* MemoryLowering::DecodeExternalPointer(
     Node* node, ExternalPointerTag external_pointer_tag) {
-#ifdef V8_HEAP_SANDBOX
-  DCHECK(V8_HEAP_SANDBOX_BOOL);
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
+  DCHECK(V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);
   DCHECK(node->opcode() == IrOpcode::kLoad);
   Node* effect = NodeProperties::GetEffectInput(node);
   Node* control = NodeProperties::GetControlInput(node);
@@ -440,7 +440,7 @@ Node* MemoryLowering::DecodeExternalPointer(
   return decoded_ptr;
 #else
   return node;
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 Reduction MemoryLowering::ReduceLoadMap(Node* node) {
@@ -465,7 +465,7 @@ Reduction MemoryLowering::ReduceLoadField(Node* node) {
   Node* offset = __ IntPtrConstant(access.offset - access.tag());
   node->InsertInput(graph_zone(), 1, offset);
   MachineType type = access.machine_type;
-  if (V8_HEAP_SANDBOX_BOOL &&
+  if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL &&
       access.type.Is(Type::SandboxedExternalPointer())) {
     // External pointer table indices are 32bit numbers
     type = MachineType::Uint32();
@@ -478,9 +478,9 @@ Reduction MemoryLowering::ReduceLoadField(Node* node) {
 
   NodeProperties::ChangeOp(node, machine()->Load(type));
 
-  if (V8_HEAP_SANDBOX_BOOL &&
+  if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL &&
       access.type.Is(Type::SandboxedExternalPointer())) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     ExternalPointerTag tag = access.external_pointer_tag;
 #else
     ExternalPointerTag tag = kExternalPointerNullTag;
@@ -535,11 +535,11 @@ Reduction MemoryLowering::ReduceStoreField(Node* node,
   DCHECK_EQ(IrOpcode::kStoreField, node->opcode());
   FieldAccess const& access = FieldAccessOf(node->op());
   // External pointer must never be stored by optimized code.
-  DCHECK_IMPLIES(V8_HEAP_SANDBOX_BOOL,
+  DCHECK_IMPLIES(V8_SANDBOXED_EXTERNAL_POINTERS_BOOL,
                  !access.type.Is(Type::ExternalPointer()) &&
                      !access.type.Is(Type::SandboxedExternalPointer()));
-  // CagedPointers are not currently stored by optimized code.
-  DCHECK(!access.type.Is(Type::CagedPointer()));
+  // SandboxedPointers are not currently stored by optimized code.
+  DCHECK(!access.type.Is(Type::SandboxedPointer()));
   MachineType machine_type = access.machine_type;
   Node* object = node->InputAt(0);
   Node* value = node->InputAt(1);
diff --git a/src/compiler/memory-optimizer.cc b/src/compiler/memory-optimizer.cc
index 41e0f21310..4736987744 100644
--- a/src/compiler/memory-optimizer.cc
+++ b/src/compiler/memory-optimizer.cc
@@ -343,11 +343,12 @@ void MemoryOptimizer::VisitLoadField(Node* node, AllocationState const* state) {
   EnqueueUses(node, state);
 
   // Node can be replaced under two cases:
-  //   1. V8_HEAP_SANDBOX_BOOL is enabled and loading an external pointer value.
+  //   1. V8_SANDBOXED_EXTERNAL_POINTERS_BOOL is enabled and loading an external
+  //   pointer value.
   //   2. V8_MAP_PACKING_BOOL is enabled.
-  DCHECK_IMPLIES(!V8_HEAP_SANDBOX_BOOL && !V8_MAP_PACKING_BOOL,
+  DCHECK_IMPLIES(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL && !V8_MAP_PACKING_BOOL,
                  reduction.replacement() == node);
-  if ((V8_HEAP_SANDBOX_BOOL || V8_MAP_PACKING_BOOL) &&
+  if ((V8_SANDBOXED_EXTERNAL_POINTERS_BOOL || V8_MAP_PACKING_BOOL) &&
       reduction.replacement() != node) {
     ReplaceUsesAndKillNode(node, reduction.replacement());
   }
diff --git a/src/compiler/representation-change.cc b/src/compiler/representation-change.cc
index f5bddd4510..7bca129c29 100644
--- a/src/compiler/representation-change.cc
+++ b/src/compiler/representation-change.cc
@@ -242,7 +242,7 @@ Node* RepresentationChanger::GetRepresentationFor(
       return node;
     case MachineRepresentation::kCompressed:
     case MachineRepresentation::kCompressedPointer:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
     case MachineRepresentation::kMapWord:
       UNREACHABLE();
   }
@@ -1247,8 +1247,8 @@ Node* RepresentationChanger::GetWord64RepresentationFor(
           jsgraph()->common()->DeadValue(MachineRepresentation::kWord64),
           unreachable);
     }
-  } else if (output_rep == MachineRepresentation::kCagedPointer) {
-    if (output_type.Is(Type::CagedPointer())) {
+  } else if (output_rep == MachineRepresentation::kSandboxedPointer) {
+    if (output_type.Is(Type::SandboxedPointer())) {
       return node;
     } else {
       return TypeError(node, output_rep, output_type,
diff --git a/src/compiler/simplified-lowering.cc b/src/compiler/simplified-lowering.cc
index bbdbdfefd8..943550159c 100644
--- a/src/compiler/simplified-lowering.cc
+++ b/src/compiler/simplified-lowering.cc
@@ -160,7 +160,7 @@ UseInfo TruncatingUseInfoFromRepresentation(MachineRepresentation rep) {
       return UseInfo::Bool();
     case MachineRepresentation::kCompressedPointer:
     case MachineRepresentation::kCompressed:
-    case MachineRepresentation::kCagedPointer:
+    case MachineRepresentation::kSandboxedPointer:
     case MachineRepresentation::kSimd128:
     case MachineRepresentation::kNone:
       break;
@@ -1075,7 +1075,7 @@ class RepresentationSelector {
       return MachineRepresentation::kWord64;
     } else if (type.Is(Type::ExternalPointer()) ||
                type.Is(Type::SandboxedExternalPointer()) ||
-               type.Is(Type::CagedPointer())) {
+               type.Is(Type::SandboxedPointer())) {
       return MachineType::PointerRepresentation();
     }
     return MachineRepresentation::kTagged;
diff --git a/src/compiler/simplified-operator.h b/src/compiler/simplified-operator.h
index f575715911..ae57d9f793 100644
--- a/src/compiler/simplified-operator.h
+++ b/src/compiler/simplified-operator.h
@@ -81,7 +81,7 @@ struct FieldAccess {
   ConstFieldInfo const_field_info;      // the constness of this access, and the
                                     // field owner map, if the access is const
   bool is_store_in_literal;  // originates from a kStoreInLiteral access
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   ExternalPointerTag external_pointer_tag = kExternalPointerNullTag;
 #endif
   bool maybe_initializing_or_transitioning_store;  // store is potentially
@@ -104,7 +104,7 @@ struct FieldAccess {
               WriteBarrierKind write_barrier_kind,
               ConstFieldInfo const_field_info = ConstFieldInfo::None(),
               bool is_store_in_literal = false,
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
               ExternalPointerTag external_pointer_tag = kExternalPointerNullTag,
 #endif
               bool maybe_initializing_or_transitioning_store = false)
@@ -117,7 +117,7 @@ struct FieldAccess {
         write_barrier_kind(write_barrier_kind),
         const_field_info(const_field_info),
         is_store_in_literal(is_store_in_literal),
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
         external_pointer_tag(external_pointer_tag),
 #endif
         maybe_initializing_or_transitioning_store(
diff --git a/src/compiler/types.h b/src/compiler/types.h
index d4b129f242..b8f2d90bd1 100644
--- a/src/compiler/types.h
+++ b/src/compiler/types.h
@@ -130,7 +130,7 @@ namespace compiler {
   /* TODO(v8:10391): Remove this type once all ExternalPointer usages are */ \
   /* sandbox-ready. */                     \
   V(SandboxedExternalPointer, uint64_t{1} << 30)  \
-  V(CagedPointer,             uint64_t{1} << 31)
+  V(SandboxedPointer,         uint64_t{1} << 31)
 
 // We split the macro list into two parts because the Torque equivalent in
 // turbofan-types.tq uses two 32bit bitfield structs.
@@ -208,8 +208,8 @@ namespace compiler {
   V(Unique,                       kBoolean | kUniqueName | kNull | \
                                   kUndefined | kHole | kReceiver) \
   V(Internal,                     kHole | kExternalPointer | \
-                                  kSandboxedExternalPointer | kCagedPointer | \
-                                  kOtherInternal) \
+                                  kSandboxedExternalPointer | \
+                                  kSandboxedPointer | kOtherInternal) \
   V(NonInternal,                  kPrimitive | kReceiver) \
   V(NonBigInt,                    kNonBigIntPrimitive | kReceiver) \
   V(NonNumber,                    kBigInt | kUnique | kString | kInternal) \
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index 206b894e87..9d6f1386c3 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -694,9 +694,9 @@ Node* WasmGraphBuilder::BuildLoadIsolateRoot() {
       // that the generated code is Isolate independent.
       return LOAD_INSTANCE_FIELD(IsolateRoot, MachineType::Pointer());
     case kWasmApiFunctionRefMode:
-      // Note: Even if V8_HEAP_SANDBOX, the pointer to the isolate root is not
-      // encoded, much like the case above. TODO(manoskouk): Decode the pointer
-      // here if that changes.
+      // Note: Even if V8_SANDBOXED_EXTERNAL_POINTERS, the pointer to the
+      // isolate root is not encoded, much like the case above. TODO(manoskouk):
+      // Decode the pointer here if that changes.
       return gasm_->Load(
           MachineType::Pointer(), Param(0),
           wasm::ObjectAccess::ToTagged(WasmApiFunctionRef::kIsolateRootOffset));
@@ -3222,7 +3222,7 @@ Node* WasmGraphBuilder::BuildIndirectCall(
 }
 
 Node* WasmGraphBuilder::BuildUnsandboxExternalPointer(Node* external_pointer) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   Node* isolate_root = BuildLoadIsolateRoot();
   Node* table =
       gasm_->LoadFromObject(MachineType::Pointer(), isolate_root,
@@ -3285,7 +3285,7 @@ Node* WasmGraphBuilder::BuildCallRef(const wasm::FunctionSig* real_sig,
         wasm::ObjectAccess::ToTagged(WasmInternalFunction::kCodeOffset));
     Node* call_target;
     if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      CHECK(!V8_HEAP_SANDBOX_BOOL);  // Not supported yet.
+      CHECK(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);  // Not supported yet.
       call_target = gasm_->LoadImmutableFromObject(
           MachineType::Pointer(), wrapper_code,
           wasm::ObjectAccess::ToTagged(
diff --git a/src/d8/d8.cc b/src/d8/d8.cc
index 66de0e7a5c..050cbdc78d 100644
--- a/src/d8/d8.cc
+++ b/src/d8/d8.cc
@@ -5175,9 +5175,9 @@ int Shell::Main(int argc, char* argv[]) {
     V8::SetFlagsFromString("--redirect-code-traces-to=code.asm");
   }
   v8::V8::InitializePlatform(g_platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   v8::V8::Initialize();
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index fac406c3fb..1bfc47fd5e 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -600,7 +600,7 @@ static void JSObjectPrintBody(std::ostream& os, JSObject obj,
   }
   int embedder_fields = obj.GetEmbedderFieldCount();
   if (embedder_fields > 0) {
-    Isolate* isolate = GetIsolateForHeapSandbox(obj);
+    Isolate* isolate = GetIsolateForSandbox(obj);
     os << " - embedder fields = {";
     for (int i = 0; i < embedder_fields; i++) {
       os << "\n    ";
@@ -788,7 +788,7 @@ void ObjectBoilerplateDescription::ObjectBoilerplateDescriptionPrint(
 }
 
 void EmbedderDataArray::EmbedderDataArrayPrint(std::ostream& os) {
-  Isolate* isolate = GetIsolateForHeapSandbox(*this);
+  Isolate* isolate = GetIsolateForSandbox(*this);
   PrintHeader(os, "EmbedderDataArray");
   os << "\n - length: " << length();
   EmbedderDataSlot start(*this, 0);
diff --git a/src/execution/isolate-data.h b/src/execution/isolate-data.h
index ca514657de..e9f19727e0 100644
--- a/src/execution/isolate-data.h
+++ b/src/execution/isolate-data.h
@@ -12,7 +12,7 @@
 #include "src/execution/thread-local-top.h"
 #include "src/heap/linear-allocation-area.h"
 #include "src/roots/roots.h"
-#include "src/security/external-pointer-table.h"
+#include "src/sandbox/external-pointer-table.h"
 #include "src/utils/utils.h"
 #include "testing/gtest/include/gtest/gtest_prod.h"  // nogncheck
 
@@ -65,12 +65,12 @@ class Isolate;
 #define ISOLATE_DATA_FIELDS_EXTERNAL_CODE_SPACE(V)
 #endif  // V8_EXTERNAL_CODE_SPACE
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
 #define ISOLATE_DATA_FIELDS_HEAP_SANDBOX(V) \
   V(kExternalPointerTableOffset, kSystemPointerSize * 3, external_pointer_table)
 #else
 #define ISOLATE_DATA_FIELDS_HEAP_SANDBOX(V)
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 
 // This class contains a collection of data accessible from both C++ runtime
 // and compiled code (including builtins, interpreter bytecode handlers and
@@ -241,7 +241,7 @@ class IsolateData final {
 #endif
 
   // Table containing pointers to external objects.
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   ExternalPointerTable external_pointer_table_;
 #endif
 
diff --git a/src/execution/isolate-utils-inl.h b/src/execution/isolate-utils-inl.h
index d044f3e646..44de6b52c8 100644
--- a/src/execution/isolate-utils-inl.h
+++ b/src/execution/isolate-utils-inl.h
@@ -86,10 +86,10 @@ V8_INLINE bool GetIsolateFromHeapObject(HeapObject object, Isolate** isolate) {
 #endif  // V8_COMPRESS_POINTERS_IN_ISOLATE_CAGE, V8_ENABLE_THIRD_PARTY_HEAP
 }
 
-// Use this function instead of Internals::GetIsolateForHeapSandbox for internal
+// Use this function instead of Internals::GetIsolateForSandbox for internal
 // code, as this function is fully inlinable.
-V8_INLINE static Isolate* GetIsolateForHeapSandbox(HeapObject object) {
-#ifdef V8_HEAP_SANDBOX
+V8_INLINE static Isolate* GetIsolateForSandbox(HeapObject object) {
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   return GetIsolateFromWritableObject(object);
 #else
   // Not used in non-sandbox mode.
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 54e99ce1cd..404ef7398b 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -3127,7 +3127,7 @@ void Isolate::CheckIsolateLayout() {
   STATIC_ASSERT(Internals::kBuiltinTier0EntryTableSize ==
                 Builtins::kBuiltinTier0Count * kSystemPointerSize);
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   CHECK_EQ(static_cast<int>(OFFSET_OF(ExternalPointerTable, buffer_)),
            Internals::kExternalPointerTableBufferOffset);
   CHECK_EQ(static_cast<int>(OFFSET_OF(ExternalPointerTable, length_)),
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index 7f8aeb8eaf..7ae2e9b659 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -39,8 +39,8 @@
 #include "src/objects/debug-objects.h"
 #include "src/objects/js-objects.h"
 #include "src/runtime/runtime.h"
-#include "src/security/external-pointer-table.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/external-pointer-table.h"
+#include "src/sandbox/sandbox.h"
 #include "src/strings/unicode.h"
 #include "src/utils/allocation.h"
 
@@ -1860,7 +1860,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   LocalHeap* main_thread_local_heap();
   LocalHeap* CurrentLocalHeap();
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   ExternalPointerTable& external_pointer_table() {
     return isolate_data_.external_pointer_table_;
   }
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index bc67c5f45f..aefe40ccdc 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -169,21 +169,21 @@ struct MaybeBoolFlag {
 #define COMPRESS_POINTERS_IN_SHARED_CAGE_BOOL false
 #endif
 
-#ifdef V8_HEAP_SANDBOX
-#define V8_HEAP_SANDBOX_BOOL true
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
+#define V8_SANDBOXED_EXTERNAL_POINTERS_BOOL true
 #else
-#define V8_HEAP_SANDBOX_BOOL false
+#define V8_SANDBOXED_EXTERNAL_POINTERS_BOOL false
 #endif
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-#define V8_VIRTUAL_MEMORY_CAGE_BOOL true
+#ifdef V8_SANDBOX
+#define V8_SANDBOX_BOOL true
 #else
-#define V8_VIRTUAL_MEMORY_CAGE_BOOL false
+#define V8_SANDBOX_BOOL false
 #endif
 
-// D8's MultiMappedAllocator is only available on Linux, and only if the virtual
-// memory cage is not enabled.
-#if V8_OS_LINUX && !V8_VIRTUAL_MEMORY_CAGE_BOOL
+// D8's MultiMappedAllocator is only available on Linux, and only if the sandbox
+// is not enabled.
+#if V8_OS_LINUX && !V8_SANDBOX_BOOL
 #define MULTI_MAPPED_ALLOCATOR_AVAILABLE true
 #else
 #define MULTI_MAPPED_ALLOCATOR_AVAILABLE false
diff --git a/src/handles/global-handles.cc b/src/handles/global-handles.cc
index a5cc8672b6..6587260cfa 100644
--- a/src/handles/global-handles.cc
+++ b/src/handles/global-handles.cc
@@ -385,7 +385,7 @@ namespace {
 
 void ExtractInternalFields(JSObject jsobject, void** embedder_fields, int len) {
   int field_count = jsobject.GetEmbedderFieldCount();
-  Isolate* isolate = GetIsolateForHeapSandbox(jsobject);
+  Isolate* isolate = GetIsolateForSandbox(jsobject);
   for (int i = 0; i < len; ++i) {
     if (field_count == i) break;
     void* pointer;
diff --git a/src/heap/factory.h b/src/heap/factory.h
index 3f3cc32830..5053df160c 100644
--- a/src/heap/factory.h
+++ b/src/heap/factory.h
@@ -1007,13 +1007,13 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
   }
 
   // This is the real Isolate that will be used for allocating and accessing
-  // external pointer entries when V8_HEAP_SANDBOX is enabled.
+  // external pointer entries when V8_SANDBOXED_EXTERNAL_POINTERS is enabled.
   Isolate* isolate_for_heap_sandbox() const {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     return isolate();
 #else
     return nullptr;
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
   }
 
   bool CanAllocateInReadOnlySpace();
diff --git a/src/heap/local-factory.h b/src/heap/local-factory.h
index 8737e3bfa1..84275ef9d2 100644
--- a/src/heap/local-factory.h
+++ b/src/heap/local-factory.h
@@ -68,13 +68,13 @@ class V8_EXPORT_PRIVATE LocalFactory : public FactoryBase<LocalFactory> {
   }
 
   // This is the real Isolate that will be used for allocating and accessing
-  // external pointer entries when V8_HEAP_SANDBOX is enabled.
+  // external pointer entries when V8_SANDBOXED_EXTERNAL_POINTERS is enabled.
   Isolate* isolate_for_heap_sandbox() {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     return isolate_for_heap_sandbox_;
 #else
     return nullptr;
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
   }
 
   inline bool CanAllocateInReadOnlySpace() { return false; }
@@ -86,7 +86,7 @@ class V8_EXPORT_PRIVATE LocalFactory : public FactoryBase<LocalFactory> {
   // ------
 
   ReadOnlyRoots roots_;
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   Isolate* isolate_for_heap_sandbox_;
 #endif
 #ifdef DEBUG
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index f046c6f8b7..ff61d6f765 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -3117,7 +3117,7 @@ static inline SlotCallbackResult UpdateStrongCodeSlot(
         CodeDataContainer::cast(HeapObject::FromAddress(
             slot.address() - CodeDataContainer::kCodeOffset));
     Code code = code_data_container.code(code_cage_base);
-    Isolate* isolate_for_sandbox = GetIsolateForHeapSandbox(host);
+    Isolate* isolate_for_sandbox = GetIsolateForSandbox(host);
     code_data_container.UpdateCodeEntryPoint(isolate_for_sandbox, code);
     return result;
   }
diff --git a/src/init/bootstrapper.cc b/src/init/bootstrapper.cc
index 947d8381d8..1601543507 100644
--- a/src/init/bootstrapper.cc
+++ b/src/init/bootstrapper.cc
@@ -6162,9 +6162,9 @@ Genesis::Genesis(
   // TODO(v8:10391): The reason is that the NativeContext::microtask_queue
   // serialization is not actually supported, and therefore the field is
   // serialized as raw data instead of being serialized as ExternalReference.
-  // As a result, when V8 heap sandbox is enabled, the external pointer entry
-  // is not allocated for microtask queue field during deserialization, so we
-  // allocate it manually here.
+  // As a result, when sandboxed external pointers are enabled, the external
+  // pointer entry is not allocated for microtask queue field during
+  // deserialization, so we allocate it manually here.
   native_context()->AllocateExternalPointerEntries(isolate);
 
   native_context()->set_microtask_queue(
diff --git a/src/init/isolate-allocator.cc b/src/init/isolate-allocator.cc
index 853d7f9358..b6bc83710c 100644
--- a/src/init/isolate-allocator.cc
+++ b/src/init/isolate-allocator.cc
@@ -8,7 +8,7 @@
 #include "src/common/ptr-compr.h"
 #include "src/execution/isolate.h"
 #include "src/heap/code-range.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/sandbox.h"
 #include "src/utils/memcopy.h"
 #include "src/utils/utils.h"
 
@@ -76,32 +76,23 @@ void IsolateAllocator::InitializeOncePerProcess() {
 #ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
   PtrComprCageReservationParams params;
   base::AddressRegion existing_reservation;
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  // TODO(chromium:1218005) avoid the name collision with
-  // v8::internal::VirtualMemoryCage and ideally figure out a clear naming
-  // scheme for the different types of virtual memory cages.
-
-  // For now, we allow the virtual memory cage to be disabled even when
-  // compiling with v8_enable_virtual_memory_cage. This fallback will be
-  // disallowed in the future, at the latest once ArrayBuffers are referenced
-  // through an offset rather than a raw pointer.
-  if (GetProcessWideVirtualMemoryCage()->is_disabled()) {
-    CHECK(kAllowBackingStoresOutsideCage);
+#ifdef V8_SANDBOX
+  // For now, we allow the sandbox to be disabled even when compiling with
+  // v8_enable_sandbox. This fallback will be disallowed in the future, at the
+  // latest once sandboxed pointers are enabled.
+  if (GetProcessWideSandbox()->is_disabled()) {
+    CHECK(kAllowBackingStoresOutsideSandbox);
   } else {
-    auto cage = GetProcessWideVirtualMemoryCage();
+    auto cage = GetProcessWideSandbox();
     CHECK(cage->is_initialized());
-    // The pointer compression cage must be placed at the start of the virtual
-    // memory cage.
+    // The pointer compression cage must be placed at the start of the sandbox.
+    //
     // TODO(chromium:12180) this currently assumes that no other pages were
     // allocated through the cage's page allocator in the meantime. In the
     // future, the cage initialization will happen just before this function
     // runs, and so this will be guaranteed. Currently however, it is possible
     // that the embedder accidentally uses the cage's page allocator prior to
     // initializing V8, in which case this CHECK will likely fail.
-    // TODO(chromium:12180) here we rely on our BoundedPageAllocators to
-    // respect the hint parameter. Instead, it would probably be better to add
-    // a new API that guarantees this, either directly to the PageAllocator
-    // interface or to a derived one.
     void* hint = reinterpret_cast<void*>(cage->base());
     void* base = cage->page_allocator()->AllocatePages(
         hint, params.reservation_size, params.base_alignment,
diff --git a/src/init/v8.cc b/src/init/v8.cc
index 5172d5da9a..46e2b9335b 100644
--- a/src/init/v8.cc
+++ b/src/init/v8.cc
@@ -24,7 +24,7 @@
 #include "src/objects/elements.h"
 #include "src/objects/objects-inl.h"
 #include "src/profiler/heap-profiler.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/sandbox.h"
 #include "src/snapshot/snapshot.h"
 #include "src/tracing/tracing-category-observer.h"
 
@@ -73,12 +73,12 @@ void V8::Dispose() {
 void V8::InitializeOncePerProcess() {
   CHECK(platform_);
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!GetProcessWideVirtualMemoryCage()->is_initialized()) {
+#ifdef V8_SANDBOX
+  if (!GetProcessWideSandbox()->is_initialized()) {
     // For now, we still allow the cage to be disabled even if V8 was compiled
-    // with V8_VIRTUAL_MEMORY_CAGE. This will eventually be forbidden.
-    CHECK(kAllowBackingStoresOutsideCage);
-    GetProcessWideVirtualMemoryCage()->Disable();
+    // with V8_SANDBOX. This will eventually be forbidden.
+    CHECK(kAllowBackingStoresOutsideSandbox);
+    GetProcessWideSandbox()->Disable();
   }
 #endif
 
@@ -217,12 +217,12 @@ void V8::InitializePlatform(v8::Platform* platform) {
 #endif
 }
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-bool V8::InitializeVirtualMemoryCage() {
+#ifdef V8_SANDBOX
+bool V8::InitializeSandbox() {
   // Platform must have been initialized already.
   CHECK(platform_);
   v8::VirtualAddressSpace* vas = GetPlatformVirtualAddressSpace();
-  return GetProcessWideVirtualMemoryCage()->Initialize(vas);
+  return GetProcessWideSandbox()->Initialize(vas);
 }
 #endif
 
@@ -236,10 +236,10 @@ void V8::DisposePlatform() {
   v8::tracing::TracingCategoryObserver::TearDown();
   v8::base::SetPrintStackTrace(nullptr);
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
+#ifdef V8_SANDBOX
   // TODO(chromium:1218005) alternatively, this could move to its own
-  // public TearDownVirtualMemoryCage function.
-  GetProcessWideVirtualMemoryCage()->TearDown();
+  // public TearDownSandbox function.
+  GetProcessWideSandbox()->TearDown();
 #endif
 
   platform_ = nullptr;
diff --git a/src/init/v8.h b/src/init/v8.h
index edd5be247d..92c33fed73 100644
--- a/src/init/v8.h
+++ b/src/init/v8.h
@@ -29,8 +29,8 @@ class V8 : public AllStatic {
                                                    const char* location,
                                                    bool is_heap_oom = false);
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  static bool InitializeVirtualMemoryCage();
+#ifdef V8_SANDBOX
+  static bool InitializeSandbox();
 #endif
 
   static void InitializePlatform(v8::Platform* platform);
diff --git a/src/objects/backing-store.cc b/src/objects/backing-store.cc
index 5dca72929a..3fe1beb075 100644
--- a/src/objects/backing-store.cc
+++ b/src/objects/backing-store.cc
@@ -10,7 +10,7 @@
 #include "src/execution/isolate.h"
 #include "src/handles/global-handles.h"
 #include "src/logging/counters.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/sandbox.h"
 
 #if V8_ENABLE_WEBASSEMBLY
 #include "src/trap-handler/trap-handler.h"
@@ -55,13 +55,15 @@ enum class AllocationStatus {
   kOtherFailure  // Failed for an unknown reason
 };
 
-// Attempts to allocate memory inside the virtual memory cage currently fall
-// back to allocating memory outside of the cage if necessary. Once this
-// fallback is no longer allowed/possible, these cases will become allocation
-// failures instead. To track the frequency of such events, the outcome of
-// memory allocation attempts inside the cage is reported to UMA.
+// Attempts to allocate memory inside the sandbox currently fall back to
+// allocating memory outside of the sandbox if necessary. Once this fallback is
+// no longer allowed/possible, these cases will become allocation failures
+// instead. To track the frequency of such events, the outcome of memory
+// allocation attempts inside the sandbox is reported to UMA.
 //
 // See caged_memory_allocation_outcome in counters-definitions.h
+// This class and the entry in counters-definitions.h use the term "cage"
+// instead of "sandbox" for historical reasons.
 enum class CagedMemoryAllocationOutcome {
   kSuccess,      // Allocation succeeded inside the cage
   kOutsideCage,  // Allocation failed inside the cage but succeeded outside
@@ -107,18 +109,17 @@ void RecordStatus(Isolate* isolate, AllocationStatus status) {
       static_cast<int>(status));
 }
 
-// When the virtual memory cage is active, this function records the outcome of
-// attempts to allocate memory inside the cage which fall back to allocating
-// memory outside of the cage. Passing a value of nullptr for the result
-// indicates that the memory could not be allocated at all.
-void RecordCagedMemoryAllocationResult(Isolate* isolate, void* result) {
-  // This metric is only meaningful when the virtual memory cage is active.
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (GetProcessWideVirtualMemoryCage()->is_initialized()) {
+// When the sandbox is active, this function records the outcome of attempts to
+// allocate memory inside the sandbox which fall back to allocating memory
+// outside of the sandbox. Passing a value of nullptr for the result indicates
+// that the memory could not be allocated at all.
+void RecordSandboxMemoryAllocationResult(Isolate* isolate, void* result) {
+  // This metric is only meaningful when the sandbox is active.
+#ifdef V8_SANDBOX
+  if (GetProcessWideSandbox()->is_initialized()) {
     CagedMemoryAllocationOutcome outcome;
     if (result) {
-      bool allocation_in_cage =
-          GetProcessWideVirtualMemoryCage()->Contains(result);
+      bool allocation_in_cage = GetProcessWideSandbox()->Contains(result);
       outcome = allocation_in_cage ? CagedMemoryAllocationOutcome::kSuccess
                                    : CagedMemoryAllocationOutcome::kOutsideCage;
     } else {
@@ -210,11 +211,11 @@ BackingStore::~BackingStore() {
   // TODO(saelo) here and elsewhere in this file, replace with
   // GetArrayBufferPageAllocator once the fallback to the platform page
   // allocator is no longer allowed.
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (GetProcessWideVirtualMemoryCage()->Contains(buffer_start_)) {
-    page_allocator = GetVirtualMemoryCagePageAllocator();
+#ifdef V8_SANDBOX
+  if (GetProcessWideSandbox()->Contains(buffer_start_)) {
+    page_allocator = GetSandboxPageAllocator();
   } else {
-    DCHECK(kAllowBackingStoresOutsideCage);
+    DCHECK(kAllowBackingStoresOutsideSandbox);
   }
 #endif
 
@@ -428,18 +429,18 @@ std::unique_ptr<BackingStore> BackingStore::TryAllocateAndPartiallyCommitMemory(
   void* allocation_base = nullptr;
   PageAllocator* page_allocator = GetPlatformPageAllocator();
   auto allocate_pages = [&] {
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-    page_allocator = GetVirtualMemoryCagePageAllocator();
+#ifdef V8_SANDBOX
+    page_allocator = GetSandboxPageAllocator();
     allocation_base = AllocatePages(page_allocator, nullptr, reservation_size,
                                     page_size, PageAllocator::kNoAccess);
     if (allocation_base) return true;
     // We currently still allow falling back to the platform page allocator if
-    // the cage page allocator fails. This will eventually be removed.
+    // the sandbox page allocator fails. This will eventually be removed.
     // TODO(chromium:1218005) once we forbid the fallback, we should have a
     // single API, e.g. GetArrayBufferPageAllocator(), that returns the correct
-    // page allocator to use here depending on whether the virtual memory cage
-    // is enabled or not.
-    if (!kAllowBackingStoresOutsideCage) return false;
+    // page allocator to use here depending on whether the sandbox is enabled
+    // or not.
+    if (!kAllowBackingStoresOutsideSandbox) return false;
     page_allocator = GetPlatformPageAllocator();
 #endif
     allocation_base = AllocatePages(page_allocator, nullptr, reservation_size,
@@ -449,7 +450,7 @@ std::unique_ptr<BackingStore> BackingStore::TryAllocateAndPartiallyCommitMemory(
   if (!gc_retry(allocate_pages)) {
     // Page allocator could not reserve enough pages.
     RecordStatus(isolate, AllocationStatus::kOtherFailure);
-    RecordCagedMemoryAllocationResult(isolate, nullptr);
+    RecordSandboxMemoryAllocationResult(isolate, nullptr);
     TRACE_BS("BSw:try   failed to allocate pages\n");
     return {};
   }
@@ -486,7 +487,7 @@ std::unique_ptr<BackingStore> BackingStore::TryAllocateAndPartiallyCommitMemory(
 
   RecordStatus(isolate, did_retry ? AllocationStatus::kSuccessAfterRetry
                                   : AllocationStatus::kSuccess);
-  RecordCagedMemoryAllocationResult(isolate, allocation_base);
+  RecordSandboxMemoryAllocationResult(isolate, allocation_base);
 
   ResizableFlag resizable =
       is_wasm_memory ? ResizableFlag::kNotResizable : ResizableFlag::kResizable;
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index d800fd38a5..6c547af615 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -921,7 +921,7 @@ ACCESSORS(CodeDataContainer, next_code_link, Object, kNextCodeLinkOffset)
 
 PtrComprCageBase CodeDataContainer::code_cage_base() const {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  CHECK(!V8_HEAP_SANDBOX_BOOL);
+  CHECK(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);
   Address code_cage_base_hi =
       ReadField<Tagged_t>(kCodeCageBaseUpper32BitsOffset);
   return PtrComprCageBase(code_cage_base_hi << 32);
@@ -932,7 +932,7 @@ PtrComprCageBase CodeDataContainer::code_cage_base() const {
 
 void CodeDataContainer::set_code_cage_base(Address code_cage_base) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  CHECK(!V8_HEAP_SANDBOX_BOOL);
+  CHECK(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);
   Tagged_t code_cage_base_hi = static_cast<Tagged_t>(code_cage_base >> 32);
   WriteField<Tagged_t>(kCodeCageBaseUpper32BitsOffset, code_cage_base_hi);
 #else
@@ -967,7 +967,7 @@ Code CodeDataContainer::code(PtrComprCageBase cage_base,
 
 DEF_GETTER(CodeDataContainer, code_entry_point, Address) {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
-  Isolate* isolate = GetIsolateForHeapSandbox(*this);
+  Isolate* isolate = GetIsolateForSandbox(*this);
   return ReadExternalPointerField(kCodeEntryPointOffset, isolate,
                                   kCodeEntryPointTag);
 }
diff --git a/src/objects/contexts-inl.h b/src/objects/contexts-inl.h
index fee92ae98b..1a38480db5 100644
--- a/src/objects/contexts-inl.h
+++ b/src/objects/contexts-inl.h
@@ -271,7 +271,7 @@ Map Context::GetInitialJSArrayMap(ElementsKind kind) const {
 }
 
 DEF_GETTER(NativeContext, microtask_queue, MicrotaskQueue*) {
-  Isolate* isolate = GetIsolateForHeapSandbox(*this);
+  Isolate* isolate = GetIsolateForSandbox(*this);
   return reinterpret_cast<MicrotaskQueue*>(ReadExternalPointerField(
       kMicrotaskQueueOffset, isolate, kNativeContextMicrotaskQueueTag));
 }
diff --git a/src/objects/embedder-data-slot-inl.h b/src/objects/embedder-data-slot-inl.h
index 983a0b0ad4..00be28168a 100644
--- a/src/objects/embedder-data-slot-inl.h
+++ b/src/objects/embedder-data-slot-inl.h
@@ -28,7 +28,7 @@ EmbedderDataSlot::EmbedderDataSlot(JSObject object, int embedder_field_index)
           object, object.GetEmbedderFieldOffset(embedder_field_index))) {}
 
 void EmbedderDataSlot::AllocateExternalPointerEntry(Isolate* isolate) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   // TODO(v8:10391, saelo): Use InitExternalPointerField() once
   // ExternalPointer_t is 4-bytes.
   uint32_t index = isolate->external_pointer_table().allocate();
@@ -75,7 +75,7 @@ void EmbedderDataSlot::store_tagged(JSObject object, int embedder_field_index,
   WRITE_BARRIER(object, slot_offset + kTaggedPayloadOffset, value);
 #ifdef V8_COMPRESS_POINTERS
   // See gc_safe_store() for the reasons behind two stores and why the second is
-  // only done if !V8_HEAP_SANDBOX_BOOL
+  // only done if !V8_SANDBOXED_EXTERNAL_POINTERS_BOOL
   ObjectSlot(FIELD_ADDR(object, slot_offset + kRawPayloadOffset))
       .Relaxed_Store(Smi::zero());
 #endif
@@ -88,7 +88,7 @@ bool EmbedderDataSlot::ToAlignedPointer(Isolate* isolate,
   // phase which is propely synched with GC (concurrent marker may still look
   // at the tagged part of the embedder slot but read-only access is ok).
   Address raw_value;
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   uint32_t index = base::Memory<uint32_t>(address() + kRawPayloadOffset);
   raw_value = isolate->external_pointer_table().get(index) &
               ~kEmbedderDataSlotPayloadTag;
@@ -109,7 +109,7 @@ bool EmbedderDataSlot::ToAlignedPointer(Isolate* isolate,
 
 bool EmbedderDataSlot::ToAlignedPointerSafe(Isolate* isolate,
                                             void** out_pointer) const {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   uint32_t index = base::Memory<uint32_t>(address() + kRawPayloadOffset);
   Address raw_value;
   if (isolate->external_pointer_table().is_valid_index(index)) {
@@ -121,14 +121,14 @@ bool EmbedderDataSlot::ToAlignedPointerSafe(Isolate* isolate,
   return false;
 #else
   return ToAlignedPointer(isolate, out_pointer);
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 bool EmbedderDataSlot::store_aligned_pointer(Isolate* isolate, void* ptr) {
   Address value = reinterpret_cast<Address>(ptr);
   if (!HAS_SMI_TAG(value)) return false;
-#ifdef V8_HEAP_SANDBOX
-  if (V8_HEAP_SANDBOX_BOOL) {
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
+  if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL) {
     AllocateExternalPointerEntry(isolate);
     // Raw payload contains the table index. Object slots don't support loading
     // of raw values, so we just "reinterpret cast" Object value to index.
diff --git a/src/objects/embedder-data-slot.h b/src/objects/embedder-data-slot.h
index 6213b7b333..23d85f9cff 100644
--- a/src/objects/embedder-data-slot.h
+++ b/src/objects/embedder-data-slot.h
@@ -46,8 +46,8 @@ class EmbedderDataSlot
   // The raw payload is located in the other "tagged" part of the full pointer
   // and cotains the upper part of aligned address. The raw part is not expected
   // to look like a tagged value.
-  // When V8_HEAP_SANDBOX is defined the raw payload contains an index into the
-  // external pointer table.
+  // When V8_SANDBOXED_EXTERNAL_POINTERS is defined the raw payload contains an
+  // index into the external pointer table.
   static constexpr int kRawPayloadOffset = kTaggedSize - kTaggedPayloadOffset;
 #endif
   static constexpr int kRequiredPtrAlignment = kSmiTagSize;
@@ -72,17 +72,18 @@ class EmbedderDataSlot
   // the pointer-like value. Note, that some Smis could still look like an
   // aligned pointers.
   // Returns true on success.
-  // When V8 heap sandbox is enabled, calling this method when the raw part of
-  // the slot does not contain valid external pointer table index is undefined
-  // behaviour and most likely result in crashes.
+  // When sandboxed external pointers are enabled, calling this method when the
+  // raw part of the slot does not contain valid external pointer table index
+  // is undefined behaviour and most likely result in crashes.
   V8_INLINE bool ToAlignedPointer(Isolate* isolate, void** out_result) const;
 
-  // Same as ToAlignedPointer() but with a workaround for V8 heap sandbox.
-  // When V8 heap sandbox is enabled, this method doesn't crash when the raw
-  // part of the slot contains "undefined" instead of a correct external table
-  // entry index (see Factory::InitializeJSObjectBody() for details).
-  // Returns true when the external pointer table index was pointing to a valid
-  // entry, otherwise false.
+  // Same as ToAlignedPointer() but with a workaround for sandboxed external
+  // pointers.  When sandboxed external pointers are enabled, this method
+  // doesn't crash when the raw part of the slot contains "undefined" instead
+  // of a correct external table entry index (see
+  // Factory::InitializeJSObjectBody() for details).  Returns true when the
+  // external pointer table index was pointing to a valid entry, otherwise
+  // false.
   //
   // Call this function if you are not sure whether the slot contains valid
   // external pointer or not.
diff --git a/src/objects/foreign-inl.h b/src/objects/foreign-inl.h
index 8947653861..b655d82365 100644
--- a/src/objects/foreign-inl.h
+++ b/src/objects/foreign-inl.h
@@ -9,7 +9,7 @@
 #include "src/heap/heap-write-barrier-inl.h"
 #include "src/objects/foreign.h"
 #include "src/objects/objects-inl.h"
-#include "src/security/external-pointer-inl.h"
+#include "src/sandbox/external-pointer-inl.h"
 
 // Has to be the last include (doesn't have include guards):
 #include "src/objects/object-macros.h"
@@ -28,7 +28,7 @@ bool Foreign::IsNormalized(Object value) {
 }
 
 DEF_GETTER(Foreign, foreign_address, Address) {
-  Isolate* isolate = GetIsolateForHeapSandbox(*this);
+  Isolate* isolate = GetIsolateForSandbox(*this);
   return ReadExternalPointerField(kForeignAddressOffset, isolate,
                                   kForeignForeignAddressTag);
 }
diff --git a/src/objects/js-array-buffer-inl.h b/src/objects/js-array-buffer-inl.h
index 0fd66630ca..2ae88cf31e 100644
--- a/src/objects/js-array-buffer-inl.h
+++ b/src/objects/js-array-buffer-inl.h
@@ -36,14 +36,14 @@ void JSArrayBuffer::set_byte_length(size_t value) {
 }
 
 DEF_GETTER(JSArrayBuffer, backing_store, void*) {
-  Address value = ReadCagedPointerField(kBackingStoreOffset, cage_base);
+  Address value = ReadSandboxedPointerField(kBackingStoreOffset, cage_base);
   return reinterpret_cast<void*>(value);
 }
 
 void JSArrayBuffer::set_backing_store(Isolate* isolate, void* value) {
   DCHECK(IsValidBackingStorePointer(value));
   Address addr = reinterpret_cast<Address>(value);
-  WriteCagedPointerField(kBackingStoreOffset, isolate, addr);
+  WriteSandboxedPointerField(kBackingStoreOffset, isolate, addr);
 }
 
 std::shared_ptr<BackingStore> JSArrayBuffer::GetBackingStore() const {
@@ -251,12 +251,12 @@ void JSTypedArray::set_length(size_t value) {
 }
 
 DEF_GETTER(JSTypedArray, external_pointer, Address) {
-  return ReadCagedPointerField(kExternalPointerOffset, cage_base);
+  return ReadSandboxedPointerField(kExternalPointerOffset, cage_base);
 }
 
 void JSTypedArray::set_external_pointer(Isolate* isolate, Address value) {
   DCHECK(IsValidBackingStorePointer(reinterpret_cast<void*>(value)));
-  WriteCagedPointerField(kExternalPointerOffset, isolate, value);
+  WriteSandboxedPointerField(kExternalPointerOffset, isolate, value);
 }
 
 Address JSTypedArray::ExternalPointerCompensationForOnHeapArray(
@@ -366,14 +366,14 @@ MaybeHandle<JSTypedArray> JSTypedArray::Validate(Isolate* isolate,
 }
 
 DEF_GETTER(JSDataView, data_pointer, void*) {
-  Address value = ReadCagedPointerField(kDataPointerOffset, cage_base);
+  Address value = ReadSandboxedPointerField(kDataPointerOffset, cage_base);
   return reinterpret_cast<void*>(value);
 }
 
 void JSDataView::set_data_pointer(Isolate* isolate, void* ptr) {
   DCHECK(IsValidBackingStorePointer(ptr));
   Address value = reinterpret_cast<Address>(ptr);
-  WriteCagedPointerField(kDataPointerOffset, isolate, value);
+  WriteSandboxedPointerField(kDataPointerOffset, isolate, value);
 }
 
 }  // namespace internal
diff --git a/src/objects/objects-inl.h b/src/objects/objects-inl.h
index 56734dab1c..f888f588b4 100644
--- a/src/objects/objects-inl.h
+++ b/src/objects/objects-inl.h
@@ -42,8 +42,8 @@
 #include "src/objects/tagged-impl-inl.h"
 #include "src/objects/tagged-index.h"
 #include "src/objects/templates.h"
-#include "src/security/caged-pointer-inl.h"
-#include "src/security/external-pointer-inl.h"
+#include "src/sandbox/external-pointer-inl.h"
+#include "src/sandbox/sandboxed-pointer-inl.h"
 
 // Has to be the last include (doesn't have include guards):
 #include "src/objects/object-macros.h"
@@ -644,20 +644,21 @@ MaybeHandle<Object> Object::SetElement(Isolate* isolate, Handle<Object> object,
   return value;
 }
 
-Address Object::ReadCagedPointerField(size_t offset,
-                                      PtrComprCageBase cage_base) const {
-  return i::ReadCagedPointerField(field_address(offset), cage_base);
+Address Object::ReadSandboxedPointerField(size_t offset,
+                                          PtrComprCageBase cage_base) const {
+  return i::ReadSandboxedPointerField(field_address(offset), cage_base);
 }
 
-void Object::WriteCagedPointerField(size_t offset, PtrComprCageBase cage_base,
-                                    Address value) {
-  i::WriteCagedPointerField(field_address(offset), cage_base, value);
+void Object::WriteSandboxedPointerField(size_t offset,
+                                        PtrComprCageBase cage_base,
+                                        Address value) {
+  i::WriteSandboxedPointerField(field_address(offset), cage_base, value);
 }
 
-void Object::WriteCagedPointerField(size_t offset, Isolate* isolate,
-                                    Address value) {
-  i::WriteCagedPointerField(field_address(offset), PtrComprCageBase(isolate),
-                            value);
+void Object::WriteSandboxedPointerField(size_t offset, Isolate* isolate,
+                                        Address value) {
+  i::WriteSandboxedPointerField(field_address(offset),
+                                PtrComprCageBase(isolate), value);
 }
 
 void Object::InitExternalPointerField(size_t offset, Isolate* isolate) {
diff --git a/src/objects/objects.h b/src/objects/objects.h
index 914f62d33b..b48ea258b2 100644
--- a/src/objects/objects.h
+++ b/src/objects/objects.h
@@ -710,14 +710,15 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   }
 
   //
-  // CagedPointer_t field accessors.
+  // SandboxedPointer_t field accessors.
   //
-  inline Address ReadCagedPointerField(size_t offset,
-                                       PtrComprCageBase cage_base) const;
-  inline void WriteCagedPointerField(size_t offset, PtrComprCageBase cage_base,
-                                     Address value);
-  inline void WriteCagedPointerField(size_t offset, Isolate* isolate,
-                                     Address value);
+  inline Address ReadSandboxedPointerField(size_t offset,
+                                           PtrComprCageBase cage_base) const;
+  inline void WriteSandboxedPointerField(size_t offset,
+                                         PtrComprCageBase cage_base,
+                                         Address value);
+  inline void WriteSandboxedPointerField(size_t offset, Isolate* isolate,
+                                         Address value);
 
   //
   // ExternalPointer_t field accessors.
diff --git a/src/objects/string-inl.h b/src/objects/string-inl.h
index 9a75dd2d06..a8d81a2668 100644
--- a/src/objects/string-inl.h
+++ b/src/objects/string-inl.h
@@ -15,8 +15,8 @@
 #include "src/objects/smi-inl.h"
 #include "src/objects/string-table-inl.h"
 #include "src/objects/string.h"
-#include "src/security/external-pointer-inl.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer-inl.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/strings/string-hasher-inl.h"
 #include "src/utils/utils.h"
 
@@ -1052,7 +1052,7 @@ void ExternalString::AllocateExternalPointerEntries(Isolate* isolate) {
 }
 
 DEF_GETTER(ExternalString, resource_as_address, Address) {
-  Isolate* isolate = GetIsolateForHeapSandbox(*this);
+  Isolate* isolate = GetIsolateForSandbox(*this);
   return ReadExternalPointerField(kResourceOffset, isolate,
                                   kExternalStringResourceTag);
 }
diff --git a/src/objects/turbofan-types.tq b/src/objects/turbofan-types.tq
index 05e93918a0..26233a2bc1 100644
--- a/src/objects/turbofan-types.tq
+++ b/src/objects/turbofan-types.tq
@@ -44,7 +44,7 @@ bitfield struct TurbofanTypeLowBits extends uint32 {
   negative_big_int_63: bool: 1 bit;
   other_big_int: bool: 1 bit;
   sandboxed_external_pointer: bool: 1 bit;
-  caged_pointer: bool: 1 bit;
+  sandboxed_pointer: bool: 1 bit;
 }
 
 bitfield struct TurbofanTypeHighBits extends uint32 {
diff --git a/src/security/OWNERS b/src/sandbox/OWNERS
similarity index 100%
rename from src/security/OWNERS
rename to src/sandbox/OWNERS
diff --git a/src/security/external-pointer-inl.h b/src/sandbox/external-pointer-inl.h
similarity index 90%
rename from src/security/external-pointer-inl.h
rename to src/sandbox/external-pointer-inl.h
index c25543d757..b91969ec8e 100644
--- a/src/security/external-pointer-inl.h
+++ b/src/sandbox/external-pointer-inl.h
@@ -2,12 +2,12 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_SECURITY_EXTERNAL_POINTER_INL_H_
-#define V8_SECURITY_EXTERNAL_POINTER_INL_H_
+#ifndef V8_SANDBOX_EXTERNAL_POINTER_INL_H_
+#define V8_SANDBOX_EXTERNAL_POINTER_INL_H_
 
 #include "include/v8-internal.h"
 #include "src/execution/isolate.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 
 namespace v8 {
 namespace internal {
@@ -16,7 +16,7 @@ V8_INLINE Address DecodeExternalPointer(const Isolate* isolate,
                                         ExternalPointer_t encoded_pointer,
                                         ExternalPointerTag tag) {
   STATIC_ASSERT(kExternalPointerSize == kSystemPointerSize);
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   uint32_t index = static_cast<uint32_t>(encoded_pointer);
   return isolate->external_pointer_table().get(index) & ~tag;
 #else
@@ -26,7 +26,7 @@ V8_INLINE Address DecodeExternalPointer(const Isolate* isolate,
 
 V8_INLINE void InitExternalPointerField(Address field_address,
                                         Isolate* isolate) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   static_assert(kExternalPointerSize == kSystemPointerSize,
                 "Review the code below, once kExternalPointerSize is 4-byte "
                 "the address of the field will always be aligned");
@@ -34,12 +34,12 @@ V8_INLINE void InitExternalPointerField(Address field_address,
   base::WriteUnalignedValue<ExternalPointer_t>(field_address, index);
 #else
   // Nothing to do.
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 V8_INLINE void InitExternalPointerField(Address field_address, Isolate* isolate,
                                         Address value, ExternalPointerTag tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   DCHECK_EQ(value & kExternalPointerTagMask, 0);
   ExternalPointer_t index = isolate->external_pointer_table().allocate();
   isolate->external_pointer_table().set(static_cast<uint32_t>(index),
@@ -58,7 +58,7 @@ V8_INLINE void InitExternalPointerField(Address field_address, Isolate* isolate,
   } else {
     base::Memory<ExternalPointer_t>(field_address) = encoded_value;
   }
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 V8_INLINE Address ReadExternalPointerField(Address field_address,
@@ -79,7 +79,7 @@ V8_INLINE Address ReadExternalPointerField(Address field_address,
 V8_INLINE void WriteExternalPointerField(Address field_address,
                                          Isolate* isolate, Address value,
                                          ExternalPointerTag tag) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   static_assert(kExternalPointerSize == kSystemPointerSize,
                 "Review the code below, once kExternalPointerSize is 4-byte "
                 "the address of the field will always be aligned");
@@ -99,10 +99,10 @@ V8_INLINE void WriteExternalPointerField(Address field_address,
   } else {
     base::Memory<ExternalPointer_t>(field_address) = encoded_value;
   }
-#endif  // V8_HEAP_SANDBOX
+#endif  // V8_SANDBOXED_EXTERNAL_POINTERS
 }
 
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_SECURITY_EXTERNAL_POINTER_INL_H_
+#endif  // V8_SANDBOX_EXTERNAL_POINTER_INL_H_
diff --git a/src/security/external-pointer-table.cc b/src/sandbox/external-pointer-table.cc
similarity index 94%
rename from src/security/external-pointer-table.cc
rename to src/sandbox/external-pointer-table.cc
index 90bd49e7a0..abd28339a3 100644
--- a/src/security/external-pointer-table.cc
+++ b/src/sandbox/external-pointer-table.cc
@@ -2,7 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#include "src/security/external-pointer-table.h"
+#include "src/sandbox/external-pointer-table.h"
 
 #include "src/base/platform/wrappers.h"
 
diff --git a/src/security/external-pointer-table.h b/src/sandbox/external-pointer-table.h
similarity index 90%
rename from src/security/external-pointer-table.h
rename to src/sandbox/external-pointer-table.h
index 6a96bab13c..5b1766187f 100644
--- a/src/security/external-pointer-table.h
+++ b/src/sandbox/external-pointer-table.h
@@ -2,11 +2,11 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_SECURITY_EXTERNAL_POINTER_TABLE_H_
-#define V8_SECURITY_EXTERNAL_POINTER_TABLE_H_
+#ifndef V8_SANDBOX_EXTERNAL_POINTER_TABLE_H_
+#define V8_SANDBOX_EXTERNAL_POINTER_TABLE_H_
 
 #include "src/base/platform/wrappers.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/utils/utils.h"
 
 namespace v8 {
@@ -71,4 +71,4 @@ class V8_EXPORT_PRIVATE ExternalPointerTable {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_SECURITY_EXTERNAL_POINTER_TABLE_H_
+#endif  // V8_SANDBOX_EXTERNAL_POINTER_TABLE_H_
diff --git a/src/security/external-pointer.h b/src/sandbox/external-pointer.h
similarity index 93%
rename from src/security/external-pointer.h
rename to src/sandbox/external-pointer.h
index 1c29a46b60..e1df8ff7c1 100644
--- a/src/security/external-pointer.h
+++ b/src/sandbox/external-pointer.h
@@ -2,8 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_SECURITY_EXTERNAL_POINTER_H_
-#define V8_SECURITY_EXTERNAL_POINTER_H_
+#ifndef V8_SANDBOX_EXTERNAL_POINTER_H_
+#define V8_SANDBOX_EXTERNAL_POINTER_H_
 
 #include "src/common/globals.h"
 
@@ -45,4 +45,4 @@ V8_INLINE void WriteExternalPointerField(Address field_address,
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_SECURITY_EXTERNAL_POINTER_H_
+#endif  // V8_SANDBOX_EXTERNAL_POINTER_H_
diff --git a/src/security/vm-cage.cc b/src/sandbox/sandbox.cc
similarity index 50%
rename from src/security/vm-cage.cc
rename to src/sandbox/sandbox.cc
index acd2d7c625..09c02d11e1 100644
--- a/src/security/vm-cage.cc
+++ b/src/sandbox/sandbox.cc
@@ -1,8 +1,8 @@
-// Copyright 2012 the V8 project authors. All rights reserved.
+// Copyright 2021 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#include "src/security/vm-cage.h"
+#include "src/sandbox/sandbox.h"
 
 #include "include/v8-internal.h"
 #include "src/base/bits.h"
@@ -14,7 +14,7 @@
 #include "src/base/virtual-address-space-page-allocator.h"
 #include "src/base/virtual-address-space.h"
 #include "src/flags/flags.h"
-#include "src/security/caged-pointer.h"
+#include "src/sandbox/sandboxed-pointer.h"
 #include "src/utils/allocation.h"
 
 #if defined(V8_OS_WIN)
@@ -26,10 +26,10 @@
 namespace v8 {
 namespace internal {
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
+#ifdef V8_SANDBOX_IS_AVAILABLE
 
 // Best-effort helper function to determine the size of the userspace virtual
-// address space. Used to determine appropriate cage size and placement.
+// address space. Used to determine appropriate sandbox size and placement.
 static Address DetermineAddressSpaceLimit() {
 #ifndef V8_TARGET_ARCH_64_BIT
 #error Unsupported target architecture.
@@ -42,10 +42,10 @@ static Address DetermineAddressSpaceLimit() {
   constexpr unsigned kMaxVirtualAddressBits = 64;
 
   constexpr size_t kMinVirtualAddressSpaceSize = 1ULL << kMinVirtualAddressBits;
-  static_assert(kMinVirtualAddressSpaceSize >= kVirtualMemoryCageMinimumSize,
-                "The minimum cage size should be smaller or equal to the "
-                "smallest possible userspace address space. Otherwise, larger "
-                "parts of the cage will not be usable on those platforms.");
+  static_assert(kMinVirtualAddressSpaceSize >= kSandboxMinimumSize,
+                "The minimum sandbox size should be smaller or equal to the "
+                "smallest possible userspace address space. Otherwise, large "
+                "parts of the sandbox will not be usable on those platforms.");
 
 #ifdef V8_TARGET_ARCH_X64
   base::CPU cpu;
@@ -81,27 +81,29 @@ static Address DetermineAddressSpaceLimit() {
   return address_space_limit;
 }
 
-bool V8VirtualMemoryCage::Initialize(v8::VirtualAddressSpace* vas) {
+bool Sandbox::Initialize(v8::VirtualAddressSpace* vas) {
   // Take the number of virtual address bits into account when determining the
-  // size of the cage. For example, if there are only 39 bits available, split
-  // evenly between userspace and kernel, then userspace can only address 256GB
-  // and so we use a quarter of that, 64GB, as maximum cage size.
+  // size of the sandbox. For example, if there are only 39 bits available,
+  // split evenly between userspace and kernel, then userspace can only address
+  // 256GB and so we use a quarter of that, 64GB, as maximum size.
   Address address_space_limit = DetermineAddressSpaceLimit();
-  size_t max_cage_size = address_space_limit / 4;
-  size_t cage_size = std::min(kVirtualMemoryCageSize, max_cage_size);
-  size_t size_to_reserve = cage_size;
-
-  // If the size is less than the minimum cage size though, we fall back to
-  // creating a fake cage. This happens for CPUs with only 36 virtual address
-  // bits, in which case the cage size would end up being only 8GB.
-  bool create_fake_cage = false;
-  if (cage_size < kVirtualMemoryCageMinimumSize) {
-    static_assert((8ULL * GB) >= kFakeVirtualMemoryCageMinReservationSize,
-                  "Minimum reservation size for a fake cage must be at most "
-                  "8GB to support CPUs with only 36 virtual address bits");
-    size_to_reserve = cage_size;
-    cage_size = kVirtualMemoryCageMinimumSize;
-    create_fake_cage = true;
+  size_t max_sandbox_size = address_space_limit / 4;
+  size_t sandbox_size = std::min(kSandboxSize, max_sandbox_size);
+  size_t size_to_reserve = sandbox_size;
+
+  // If the size is less than the minimum sandbox size though, we fall back to
+  // creating a partially reserved sandbox, as that allows covering more virtual
+  // address space. This happens for CPUs with only 36 virtual address bits, in
+  // which case the sandbox size would end up being only 8GB.
+  bool partially_reserve = false;
+  if (sandbox_size < kSandboxMinimumSize) {
+    static_assert(
+        (8ULL * GB) >= kSandboxMinimumReservationSize,
+        "Minimum reservation size for a partially reserved sandbox must be at "
+        "most 8GB to support CPUs with only 36 virtual address bits");
+    size_to_reserve = sandbox_size;
+    sandbox_size = kSandboxMinimumSize;
+    partially_reserve = true;
   }
 
 #if defined(V8_OS_WIN)
@@ -110,83 +112,84 @@ bool V8VirtualMemoryCage::Initialize(v8::VirtualAddressSpace* vas) {
     // apparently because the OS already charges for the memory required for
     // all page table entries. For example, a 1TB reservation increases private
     // memory usage by 2GB. As such, it is not possible to create a proper
-    // virtual memory cage there and so a fake cage is created which doesn't
-    // reserve most of the virtual memory, and so doesn't incur the cost, but
-    // also doesn't provide the desired security benefits.
-    size_to_reserve = kFakeVirtualMemoryCageMinReservationSize;
-    create_fake_cage = true;
+    // sandbox there and so a partially reserved sandbox is created which
+    // doesn't reserve most of the virtual memory, and so doesn't incur the
+    // cost, but also doesn't provide the desired security benefits.
+    size_to_reserve = kSandboxMinimumReservationSize;
+    partially_reserve = true;
   }
 #endif  // V8_OS_WIN
 
   if (!vas->CanAllocateSubspaces()) {
     // If we cannot create virtual memory subspaces, we also need to fall back
-    // to creating a fake cage. In practice, this should only happen on Windows
-    // version before Windows 10, maybe including early Windows 10 releases,
-    // where the necessary memory management APIs, in particular, VirtualAlloc2,
-    // are not available. This check should also in practice subsume the
-    // preceeding one for Windows 8 and earlier, but we'll keep both just to be
-    // sure since there the fake cage is technically required for a different
-    // reason (large virtual memory reservations being too expensive).
-    size_to_reserve = kFakeVirtualMemoryCageMinReservationSize;
-    create_fake_cage = true;
+    // to creating a partially reserved sandbox. In practice, this should only
+    // happen on Windows version before Windows 10, maybe including early
+    // Windows 10 releases, where the necessary memory management APIs, in
+    // particular, VirtualAlloc2, are not available. This check should also in
+    // practice subsume the preceeding one for Windows 8 and earlier, but we'll
+    // keep both just to be sure since there the partially reserved sandbox is
+    // technically required for a different reason (large virtual memory
+    // reservations being too expensive).
+    size_to_reserve = kSandboxMinimumReservationSize;
+    partially_reserve = true;
   }
 
-  // In any case, the (fake) cage must be at most as large as our address space.
-  DCHECK_LE(cage_size, address_space_limit);
+  // In any case, the sandbox must be at most as large as our address space.
+  DCHECK_LE(sandbox_size, address_space_limit);
 
-  if (create_fake_cage) {
-    return InitializeAsFakeCage(vas, cage_size, size_to_reserve);
+  if (partially_reserve) {
+    return InitializeAsPartiallyReservedSandbox(vas, sandbox_size,
+                                                size_to_reserve);
   } else {
-    // TODO(saelo) if this fails, we could still fall back to creating a fake
-    // cage. Decide if that would make sense.
+    // TODO(saelo) if this fails, we could still fall back to creating a
+    // partially reserved sandbox. Decide if that would make sense.
     const bool use_guard_regions = true;
-    return Initialize(vas, cage_size, use_guard_regions);
+    return Initialize(vas, sandbox_size, use_guard_regions);
   }
 }
 
-bool V8VirtualMemoryCage::Initialize(v8::VirtualAddressSpace* vas, size_t size,
-                                     bool use_guard_regions) {
+bool Sandbox::Initialize(v8::VirtualAddressSpace* vas, size_t size,
+                         bool use_guard_regions) {
   CHECK(!initialized_);
   CHECK(!disabled_);
   CHECK(base::bits::IsPowerOfTwo(size));
-  CHECK_GE(size, kVirtualMemoryCageMinimumSize);
+  CHECK_GE(size, kSandboxMinimumSize);
   CHECK(vas->CanAllocateSubspaces());
 
-  // Currently, we allow the cage to be smaller than the requested size. This
-  // way, we can gracefully handle cage reservation failures during the initial
-  // rollout and can collect data on how often these occur. In the future, we
-  // will likely either require the cage to always have a fixed size or will
-  // design CagedPointers (pointers that are guaranteed to point into the cage,
-  // e.g. because they are stored as offsets from the cage base) in a way that
-  // doesn't reduce the cage's security properties if it has a smaller size.
-  // Which of these options is ultimately taken likey depends on how frequently
-  // cage reservation failures occur in practice.
+  // Currently, we allow the sandbox to be smaller than the requested size.
+  // This way, we can gracefully handle address space reservation failures
+  // during the initial rollout and can collect data on how often these occur.
+  // In the future, we will likely either require the sandbox to always have a
+  // fixed size or will design SandboxedPointers (pointers that are guaranteed
+  // to point into the sandbox) in a way that doesn't reduce the sandbox's
+  // security properties if it has a smaller size.  Which of these options is
+  // ultimately taken likey depends on how frequently sandbox reservation
+  // failures occur in practice.
   size_t reservation_size;
-  while (!virtual_address_space_ && size >= kVirtualMemoryCageMinimumSize) {
+  while (!address_space_ && size >= kSandboxMinimumSize) {
     reservation_size = size;
     if (use_guard_regions) {
-      reservation_size += 2 * kVirtualMemoryCageGuardRegionSize;
+      reservation_size += 2 * kSandboxGuardRegionSize;
     }
 
-    Address hint =
-        RoundDown(vas->RandomPageAddress(), kVirtualMemoryCageAlignment);
+    Address hint = RoundDown(vas->RandomPageAddress(), kSandboxAlignment);
 
-    // Currently, executable memory is still allocated inside the cage. In the
-    // future, we should drop that and use kReadWrite as max_permissions.
-    virtual_address_space_ = vas->AllocateSubspace(
-        hint, reservation_size, kVirtualMemoryCageAlignment,
-        PagePermissions::kReadWriteExecute);
-    if (!virtual_address_space_) {
+    // Currently, executable memory is still allocated inside the sandbox. In
+    // the future, we should drop that and use kReadWrite as max_permissions.
+    address_space_ =
+        vas->AllocateSubspace(hint, reservation_size, kSandboxAlignment,
+                              PagePermissions::kReadWriteExecute);
+    if (!address_space_) {
       size /= 2;
     }
   }
 
-  if (!virtual_address_space_) return false;
+  if (!address_space_) return false;
 
-  reservation_base_ = virtual_address_space_->base();
+  reservation_base_ = address_space_->base();
   base_ = reservation_base_;
   if (use_guard_regions) {
-    base_ += kVirtualMemoryCageGuardRegionSize;
+    base_ += kSandboxGuardRegionSize;
   }
 
   size_ = size;
@@ -196,41 +199,40 @@ bool V8VirtualMemoryCage::Initialize(v8::VirtualAddressSpace* vas, size_t size,
   if (use_guard_regions) {
     // These must succeed since nothing was allocated in the subspace yet.
     CHECK_EQ(reservation_base_,
-             virtual_address_space_->AllocatePages(
-                 reservation_base_, kVirtualMemoryCageGuardRegionSize,
-                 vas->allocation_granularity(), PagePermissions::kNoAccess));
-    CHECK_EQ(end_,
-             virtual_address_space_->AllocatePages(
-                 end_, kVirtualMemoryCageGuardRegionSize,
+             address_space_->AllocatePages(
+                 reservation_base_, kSandboxGuardRegionSize,
                  vas->allocation_granularity(), PagePermissions::kNoAccess));
+    CHECK_EQ(end_, address_space_->AllocatePages(end_, kSandboxGuardRegionSize,
+                                                 vas->allocation_granularity(),
+                                                 PagePermissions::kNoAccess));
   }
 
-  cage_page_allocator_ =
+  sandbox_page_allocator_ =
       std::make_unique<base::VirtualAddressSpacePageAllocator>(
-          virtual_address_space_.get());
+          address_space_.get());
 
   initialized_ = true;
-  is_fake_cage_ = false;
+  is_partially_reserved_ = false;
 
   InitializeConstants();
 
   return true;
 }
 
-bool V8VirtualMemoryCage::InitializeAsFakeCage(v8::VirtualAddressSpace* vas,
-                                               size_t size,
-                                               size_t size_to_reserve) {
+bool Sandbox::InitializeAsPartiallyReservedSandbox(v8::VirtualAddressSpace* vas,
+                                                   size_t size,
+                                                   size_t size_to_reserve) {
   CHECK(!initialized_);
   CHECK(!disabled_);
   CHECK(base::bits::IsPowerOfTwo(size));
   CHECK(base::bits::IsPowerOfTwo(size_to_reserve));
-  CHECK_GE(size, kVirtualMemoryCageMinimumSize);
+  CHECK_GE(size, kSandboxMinimumSize);
   CHECK_LT(size_to_reserve, size);
 
   // Use a custom random number generator here to ensure that we get uniformly
   // distributed random numbers. We figure out the available address space
   // ourselves, and so are potentially better positioned to determine a good
-  // base address for the cage than the embedder.
+  // base address for the sandbox than the embedder.
   base::RandomNumberGenerator rng;
   if (FLAG_random_seed != 0) {
     rng.SetSeed(FLAG_random_seed);
@@ -238,20 +240,19 @@ bool V8VirtualMemoryCage::InitializeAsFakeCage(v8::VirtualAddressSpace* vas,
 
   // We try to ensure that base + size is still (mostly) within the process'
   // address space, even though we only reserve a fraction of the memory. For
-  // that, we attempt to map the cage into the first half of the usable address
-  // space. This keeps the implementation simple and should, In any realistic
-  // scenario, leave plenty of space after the cage reservation.
+  // that, we attempt to map the sandbox into the first half of the usable
+  // address space. This keeps the implementation simple and should, In any
+  // realistic scenario, leave plenty of space after the actual reservation.
   Address address_space_end = DetermineAddressSpaceLimit();
   Address highest_allowed_address = address_space_end / 2;
   DCHECK(base::bits::IsPowerOfTwo(highest_allowed_address));
   constexpr int kMaxAttempts = 10;
   for (int i = 1; i <= kMaxAttempts; i++) {
     Address hint = rng.NextInt64() % highest_allowed_address;
-    hint = RoundDown(hint, kVirtualMemoryCageAlignment);
+    hint = RoundDown(hint, kSandboxAlignment);
 
-    reservation_base_ =
-        vas->AllocatePages(hint, size_to_reserve, kVirtualMemoryCageAlignment,
-                           PagePermissions::kNoAccess);
+    reservation_base_ = vas->AllocatePages(
+        hint, size_to_reserve, kSandboxAlignment, PagePermissions::kNoAccess);
 
     if (!reservation_base_) return false;
 
@@ -271,51 +272,49 @@ bool V8VirtualMemoryCage::InitializeAsFakeCage(v8::VirtualAddressSpace* vas,
   end_ = base_ + size_;
   reservation_size_ = size_to_reserve;
   initialized_ = true;
-  is_fake_cage_ = true;
-  virtual_address_space_ =
-      std::make_unique<base::EmulatedVirtualAddressSubspace>(
-          vas, reservation_base_, reservation_size_, size_);
-  cage_page_allocator_ =
+  is_partially_reserved_ = true;
+  address_space_ = std::make_unique<base::EmulatedVirtualAddressSubspace>(
+      vas, reservation_base_, reservation_size_, size_);
+  sandbox_page_allocator_ =
       std::make_unique<base::VirtualAddressSpacePageAllocator>(
-          virtual_address_space_.get());
+          address_space_.get());
 
   InitializeConstants();
 
   return true;
 }
 
-void V8VirtualMemoryCage::InitializeConstants() {
-#ifdef V8_CAGED_POINTERS
-  // Place the empty backing store buffer at the end of the cage, so that any
+void Sandbox::InitializeConstants() {
+#ifdef V8_SANDBOXED_POINTERS
+  // Place the empty backing store buffer at the end of the sandbox, so that any
   // accidental access to it will most likely hit a guard page.
   constants_.set_empty_backing_store_buffer(base_ + size_ - 1);
 #endif
 }
 
-void V8VirtualMemoryCage::TearDown() {
+void Sandbox::TearDown() {
   if (initialized_) {
     // This destroys the sub space and frees the underlying reservation.
-    virtual_address_space_.reset();
-    cage_page_allocator_.reset();
+    address_space_.reset();
+    sandbox_page_allocator_.reset();
     base_ = kNullAddress;
     end_ = kNullAddress;
     size_ = 0;
     reservation_base_ = kNullAddress;
     reservation_size_ = 0;
     initialized_ = false;
-    is_fake_cage_ = false;
-#ifdef V8_CAGED_POINTERS
+    is_partially_reserved_ = false;
+#ifdef V8_SANDBOXED_POINTERS
     constants_.Reset();
 #endif
   }
   disabled_ = false;
 }
 
-#endif  // V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
+#endif  // V8_SANDBOX_IS_AVAILABLE
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-DEFINE_LAZY_LEAKY_OBJECT_GETTER(V8VirtualMemoryCage,
-                                GetProcessWideVirtualMemoryCage)
+#ifdef V8_SANDBOX
+DEFINE_LAZY_LEAKY_OBJECT_GETTER(Sandbox, GetProcessWideSandbox)
 #endif
 
 }  // namespace internal
diff --git a/src/sandbox/sandbox.h b/src/sandbox/sandbox.h
new file mode 100644
index 0000000000..1f255b5e07
--- /dev/null
+++ b/src/sandbox/sandbox.h
@@ -0,0 +1,204 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_SANDBOX_SANDBOX_H_
+#define V8_SANDBOX_SANDBOX_H_
+
+#include "include/v8-internal.h"
+#include "include/v8-platform.h"
+#include "src/common/globals.h"
+#include "testing/gtest/include/gtest/gtest_prod.h"  // nogncheck
+
+namespace v8 {
+
+namespace internal {
+
+#ifdef V8_SANDBOX_IS_AVAILABLE
+
+/**
+ * The V8 Sandbox.
+ *
+ * When enabled, V8 reserves a large region of virtual address space - the
+ * sandbox - and places most of its objects inside of it. It is then assumed
+ * that an attacker can, by exploiting a vulnerability in V8, corrupt memory
+ * inside the sandbox arbitrarily and from different threads. The sandbox
+ * attempts to stop an attacker from corrupting other memory in the process.
+ *
+ * The sandbox relies on a number of different mechanisms to achieve its goal.
+ * For example, objects inside the sandbox can reference each other through
+ * offsets from the start of the sandbox ("sandboxed pointers") instead of raw
+ * pointers, and external objects can be referenced through indices into a
+ * per-Isolate table of external pointers ("sandboxed external pointers").
+ *
+ * The pointer compression region, which contains most V8 objects, and inside
+ * of which compressed (32-bit) pointers are used, is located at the start of
+ * the sandbox. The remainder of the sandbox is mostly used for memory
+ * buffers, in particular ArrayBuffer backing stores and WASM memory cages.
+ *
+ * As the embedder is responsible for providing ArrayBuffer allocators, V8
+ * exposes the virtual address space backing the sandbox to the embedder.
+ */
+class V8_EXPORT_PRIVATE Sandbox {
+ public:
+  // +-  ~~~  -+----------------------------------------  ~~~  -+-  ~~~  -+
+  // |  32 GB  |                 (Ideally) 1 TB                 |  32 GB  |
+  // |         |                                                |         |
+  // | Guard   |      4 GB      :  ArrayBuffer backing stores,  | Guard   |
+  // | Region  |    V8 Heap     :  WASM memory buffers, and     | Region  |
+  // | (front) |     Region     :  any other sandboxed objects. | (back)  |
+  // +-  ~~~  -+----------------+-----------------------  ~~~  -+-  ~~~  -+
+  //           ^                                                ^
+  //           base                                             base + size
+
+  Sandbox() = default;
+
+  Sandbox(const Sandbox&) = delete;
+  Sandbox& operator=(Sandbox&) = delete;
+
+  bool Initialize(v8::VirtualAddressSpace* vas);
+  void Disable() {
+    CHECK(!initialized_);
+    disabled_ = true;
+  }
+
+  void TearDown();
+
+  bool is_initialized() const { return initialized_; }
+  bool is_disabled() const { return disabled_; }
+  bool is_enabled() const { return !disabled_; }
+  bool is_partially_reserved() const { return is_partially_reserved_; }
+
+  Address base() const { return base_; }
+  Address end() const { return end_; }
+  size_t size() const { return size_; }
+
+  Address base_address() const { return reinterpret_cast<Address>(&base_); }
+  Address end_address() const { return reinterpret_cast<Address>(&end_); }
+  Address size_address() const { return reinterpret_cast<Address>(&size_); }
+
+  v8::PageAllocator* page_allocator() const {
+    return sandbox_page_allocator_.get();
+  }
+
+  v8::VirtualAddressSpace* address_space() const {
+    return address_space_.get();
+  }
+
+  bool Contains(Address addr) const {
+    return addr >= base_ && addr < base_ + size_;
+  }
+
+  bool Contains(void* ptr) const {
+    return Contains(reinterpret_cast<Address>(ptr));
+  }
+
+#ifdef V8_SANDBOXED_POINTERS
+  class SandboxedPointerConstants final {
+   public:
+    Address empty_backing_store_buffer() const {
+      return empty_backing_store_buffer_;
+    }
+    Address empty_backing_store_buffer_address() const {
+      return reinterpret_cast<Address>(&empty_backing_store_buffer_);
+    }
+    void set_empty_backing_store_buffer(Address value) {
+      empty_backing_store_buffer_ = value;
+    }
+
+    void Reset() { empty_backing_store_buffer_ = 0; }
+
+   private:
+    Address empty_backing_store_buffer_ = 0;
+  };
+  const SandboxedPointerConstants& constants() const { return constants_; }
+#endif
+
+ private:
+  // The SequentialUnmapperTest calls the private Initialize method to create a
+  // sandbox without guard regions, which would consume too much memory.
+  friend class SequentialUnmapperTest;
+
+  // These tests call the private Initialize methods below.
+  FRIEND_TEST(SandboxTest, InitializationWithSize);
+  FRIEND_TEST(SandboxTest, PartiallyReservedSandboxInitialization);
+  FRIEND_TEST(SandboxTest, PartiallyReservedSandboxPageAllocation);
+
+  // We allow tests to disable the guard regions around the sandbox. This is
+  // useful for example for tests like the SequentialUnmapperTest which track
+  // page allocations and so would incur a large overhead from the guard
+  // regions. The provided virtual address space must be able to allocate
+  // subspaces. The size must be a multiple of the allocation granularity of the
+  // virtual memory space.
+  bool Initialize(v8::VirtualAddressSpace* vas, size_t size,
+                  bool use_guard_regions);
+
+  // Used when reserving virtual memory is too expensive. A partially reserved
+  // sandbox does not reserve all of its virtual memory and so doesn't have the
+  // desired security properties as unrelated mappings could end up inside of
+  // it and be corrupted. The size and size_to_reserve parameters must be
+  // multiples of the allocation granularity of the virtual address space.
+  bool InitializeAsPartiallyReservedSandbox(v8::VirtualAddressSpace* vas,
+                                            size_t size,
+                                            size_t size_to_reserve);
+
+  // Initialize the constant objects for this sandbox. Called by the Initialize
+  // methods above.
+  void InitializeConstants();
+
+  Address base_ = kNullAddress;
+  Address end_ = kNullAddress;
+  size_t size_ = 0;
+
+  // Base and size of the virtual memory reservation backing this sandbox.
+  // These can be different from the sandbox base and size due to guard regions
+  // or when a fake sandbox is used.
+  Address reservation_base_ = kNullAddress;
+  size_t reservation_size_ = 0;
+
+  bool initialized_ = false;
+  bool disabled_ = false;
+  bool is_partially_reserved_ = false;
+
+  // The virtual address subspace backing the sandbox.
+  std::unique_ptr<v8::VirtualAddressSpace> address_space_;
+
+  // The page allocator instance for this sandbox.
+  std::unique_ptr<v8::PageAllocator> sandbox_page_allocator_;
+
+#ifdef V8_SANDBOXED_POINTERS
+  // Constant objects inside this sandbox.
+  SandboxedPointerConstants constants_;
+#endif
+};
+
+#endif  // V8_SANDBOX_IS_AVAILABLE
+
+#ifdef V8_SANDBOX
+// This function is only available when the sandbox is actually used.
+V8_EXPORT_PRIVATE Sandbox* GetProcessWideSandbox();
+#endif
+
+V8_INLINE bool IsValidBackingStorePointer(void* ptr) {
+#ifdef V8_SANDBOX
+  Address addr = reinterpret_cast<Address>(ptr);
+  return kAllowBackingStoresOutsideSandbox || addr == kNullAddress ||
+         GetProcessWideSandbox()->Contains(addr);
+#else
+  return true;
+#endif
+}
+
+V8_INLINE void* EmptyBackingStoreBuffer() {
+#ifdef V8_SANDBOXED_POINTERS
+  return reinterpret_cast<void*>(
+      GetProcessWideSandbox()->constants().empty_backing_store_buffer());
+#else
+  return nullptr;
+#endif
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SANDBOX_SANDBOX_H_
diff --git a/src/sandbox/sandboxed-pointer-inl.h b/src/sandbox/sandboxed-pointer-inl.h
new file mode 100644
index 0000000000..cf367ecf8c
--- /dev/null
+++ b/src/sandbox/sandboxed-pointer-inl.h
@@ -0,0 +1,48 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_SANDBOX_SANDBOXED_POINTER_INL_H_
+#define V8_SANDBOX_SANDBOXED_POINTER_INL_H_
+
+#include "include/v8-internal.h"
+#include "src/execution/isolate.h"
+#include "src/sandbox/sandboxed-pointer.h"
+
+namespace v8 {
+namespace internal {
+
+V8_INLINE Address ReadSandboxedPointerField(Address field_address,
+                                            PtrComprCageBase cage_base) {
+#ifdef V8_SANDBOXED_POINTERS
+  SandboxedPointer_t sandboxed_pointer =
+      base::ReadUnalignedValue<SandboxedPointer_t>(field_address);
+
+  Address offset = sandboxed_pointer >> kSandboxedPointerShift;
+  Address pointer = cage_base.address() + offset;
+  return pointer;
+#else
+  return base::ReadUnalignedValue<Address>(field_address);
+#endif
+}
+
+V8_INLINE void WriteSandboxedPointerField(Address field_address,
+                                          PtrComprCageBase cage_base,
+                                          Address pointer) {
+#ifdef V8_SANDBOXED_POINTERS
+  // The pointer must point into the sandbox.
+  DCHECK(GetProcessWideSandbox()->Contains(pointer));
+
+  Address offset = pointer - cage_base.address();
+  SandboxedPointer_t sandboxed_pointer = offset << kSandboxedPointerShift;
+  base::WriteUnalignedValue<SandboxedPointer_t>(field_address,
+                                                sandboxed_pointer);
+#else
+  base::WriteUnalignedValue<Address>(field_address, pointer);
+#endif
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SANDBOX_SANDBOXED_POINTER_INL_H_
diff --git a/src/sandbox/sandboxed-pointer.h b/src/sandbox/sandboxed-pointer.h
new file mode 100644
index 0000000000..8490d49815
--- /dev/null
+++ b/src/sandbox/sandboxed-pointer.h
@@ -0,0 +1,23 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_SANDBOX_SANDBOXED_POINTER_H_
+#define V8_SANDBOX_SANDBOXED_POINTER_H_
+
+#include "src/common/globals.h"
+
+namespace v8 {
+namespace internal {
+
+V8_INLINE Address ReadSandboxedPointerField(Address field_address,
+                                            PtrComprCageBase cage_base);
+
+V8_INLINE void WriteSandboxedPointerField(Address field_address,
+                                          PtrComprCageBase cage_base,
+                                          Address value);
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SANDBOX_SANDBOXED_POINTER_H_
diff --git a/src/security/caged-pointer-inl.h b/src/security/caged-pointer-inl.h
deleted file mode 100644
index 93cd95a6bf..0000000000
--- a/src/security/caged-pointer-inl.h
+++ /dev/null
@@ -1,53 +0,0 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#ifndef V8_SECURITY_CAGED_POINTER_INL_H_
-#define V8_SECURITY_CAGED_POINTER_INL_H_
-
-#include "include/v8-internal.h"
-#include "src/execution/isolate.h"
-#include "src/security/caged-pointer.h"
-
-namespace v8 {
-namespace internal {
-
-V8_INLINE Address ReadCagedPointerField(Address field_address,
-                                        PtrComprCageBase cage_base) {
-#ifdef V8_CAGED_POINTERS
-  // Caged pointers are currently only used if the sandbox is enabled.
-  DCHECK(V8_HEAP_SANDBOX_BOOL);
-
-  CagedPointer_t caged_pointer =
-      base::ReadUnalignedValue<CagedPointer_t>(field_address);
-
-  Address offset = caged_pointer >> kCagedPointerShift;
-  Address pointer = cage_base.address() + offset;
-  return pointer;
-#else
-  return base::ReadUnalignedValue<Address>(field_address);
-#endif
-}
-
-V8_INLINE void WriteCagedPointerField(Address field_address,
-                                      PtrComprCageBase cage_base,
-                                      Address pointer) {
-#ifdef V8_CAGED_POINTERS
-  // Caged pointers are currently only used if the sandbox is enabled.
-  DCHECK(V8_HEAP_SANDBOX_BOOL);
-
-  // The pointer must point into the virtual memory cage.
-  DCHECK(GetProcessWideVirtualMemoryCage()->Contains(pointer));
-
-  Address offset = pointer - cage_base.address();
-  CagedPointer_t caged_pointer = offset << kCagedPointerShift;
-  base::WriteUnalignedValue<CagedPointer_t>(field_address, caged_pointer);
-#else
-  base::WriteUnalignedValue<Address>(field_address, pointer);
-#endif
-}
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_SECURITY_CAGED_POINTER_INL_H_
diff --git a/src/security/caged-pointer.h b/src/security/caged-pointer.h
deleted file mode 100644
index 30c3b40db8..0000000000
--- a/src/security/caged-pointer.h
+++ /dev/null
@@ -1,23 +0,0 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#ifndef V8_SECURITY_CAGED_POINTER_H_
-#define V8_SECURITY_CAGED_POINTER_H_
-
-#include "src/common/globals.h"
-
-namespace v8 {
-namespace internal {
-
-V8_INLINE Address ReadCagedPointerField(Address field_address,
-                                        PtrComprCageBase cage_base);
-
-V8_INLINE void WriteCagedPointerField(Address field_address,
-                                      PtrComprCageBase cage_base,
-                                      Address value);
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_SECURITY_CAGED_POINTER_H_
diff --git a/src/security/vm-cage.h b/src/security/vm-cage.h
deleted file mode 100644
index b3f54d9bd1..0000000000
--- a/src/security/vm-cage.h
+++ /dev/null
@@ -1,205 +0,0 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#ifndef V8_SECURITY_VM_CAGE_H_
-#define V8_SECURITY_VM_CAGE_H_
-
-#include "include/v8-internal.h"
-#include "src/base/bounded-page-allocator.h"
-#include "src/common/globals.h"
-
-namespace v8 {
-
-namespace internal {
-
-#ifdef V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
-
-/**
- * V8 Virtual Memory Cage.
- *
- * When the virtual memory cage is enabled, V8 will reserve a large region of
- * virtual address space - the cage - and place most of its objects inside of
- * it. This allows these objects to reference each other through offsets rather
- * than raw pointers, which in turn makes it harder for an attacker to abuse
- * them in an exploit.
- *
- * The pointer compression region, which contains most V8 objects, and inside
- * of which compressed (32-bit) pointers are used, is located at the start of
- * the virtual memory cage. The remainder of the cage is mostly used for memory
- * buffers, in particular ArrayBuffer backing stores and WASM memory cages.
- *
- * It should be assumed that an attacker is able to corrupt data arbitrarily
- * and concurrently inside the virtual memory cage. The heap sandbox, of which
- * the virtual memory cage is one building block, attempts to then stop an
- * attacker from corrupting data outside of the cage.
- *
- * As the embedder is responsible for providing ArrayBuffer allocators, v8
- * exposes a page allocator for the virtual memory cage to the embedder.
- *
- * TODO(chromium:1218005) come up with a coherent naming scheme for this class
- * and the other "cages" in v8.
- */
-class V8_EXPORT_PRIVATE V8VirtualMemoryCage {
- public:
-  // +-  ~~~  -+----------------------------------------  ~~~  -+-  ~~~  -+
-  // |  32 GB  |                 (Ideally) 1 TB                 |  32 GB  |
-  // |         |                                                |         |
-  // | Guard   |      4 GB      :  ArrayBuffer backing stores,  | Guard   |
-  // | Region  |    V8 Heap     :  WASM memory buffers, and     | Region  |
-  // | (front) |     Region     :  any other caged objects.     | (back)  |
-  // +-  ~~~  -+----------------+-----------------------  ~~~  -+-  ~~~  -+
-  //           ^                                                ^
-  //           base                                             base + size
-
-  V8VirtualMemoryCage() = default;
-
-  V8VirtualMemoryCage(const V8VirtualMemoryCage&) = delete;
-  V8VirtualMemoryCage& operator=(V8VirtualMemoryCage&) = delete;
-
-  bool Initialize(v8::VirtualAddressSpace* vas);
-  void Disable() {
-    CHECK(!initialized_);
-    disabled_ = true;
-  }
-
-  void TearDown();
-
-  bool is_initialized() const { return initialized_; }
-  bool is_disabled() const { return disabled_; }
-  bool is_enabled() const { return !disabled_; }
-  bool is_fake_cage() const { return is_fake_cage_; }
-
-  Address base() const { return base_; }
-  Address end() const { return end_; }
-  size_t size() const { return size_; }
-
-  Address base_address() const { return reinterpret_cast<Address>(&base_); }
-  Address end_address() const { return reinterpret_cast<Address>(&end_); }
-  Address size_address() const { return reinterpret_cast<Address>(&size_); }
-
-  v8::PageAllocator* page_allocator() const {
-    return cage_page_allocator_.get();
-  }
-
-  v8::VirtualAddressSpace* virtual_address_space() const {
-    return virtual_address_space_.get();
-  }
-
-  bool Contains(Address addr) const {
-    return addr >= base_ && addr < base_ + size_;
-  }
-
-  bool Contains(void* ptr) const {
-    return Contains(reinterpret_cast<Address>(ptr));
-  }
-
-#ifdef V8_CAGED_POINTERS
-  class CagedPointerConstants final {
-   public:
-    Address empty_backing_store_buffer() const {
-      return empty_backing_store_buffer_;
-    }
-    Address empty_backing_store_buffer_address() const {
-      return reinterpret_cast<Address>(&empty_backing_store_buffer_);
-    }
-    void set_empty_backing_store_buffer(Address value) {
-      empty_backing_store_buffer_ = value;
-    }
-
-    void Reset() { empty_backing_store_buffer_ = 0; }
-
-   private:
-    Address empty_backing_store_buffer_ = 0;
-  };
-  const CagedPointerConstants& constants() const { return constants_; }
-#endif
-
- private:
-  // The SequentialUnmapperTest calls the private Initialize method to create a
-  // cage without guard regions, which would otherwise consume too much memory.
-  friend class SequentialUnmapperTest;
-
-  // These tests call the private Initialize methods below.
-  FRIEND_TEST(VirtualMemoryCageTest, InitializationWithSize);
-  FRIEND_TEST(VirtualMemoryCageTest, InitializationAsFakeCage);
-  FRIEND_TEST(VirtualMemoryCageTest, FakeCagePageAllocation);
-
-  // We allow tests to disable the guard regions around the cage. This is useful
-  // for example for tests like the SequentialUnmapperTest which track page
-  // allocations and so would incur a large overhead from the guard regions.
-  // The provided virtual address space must be able to allocate subspaces.
-  // The size must be a multiple of the allocation granularity of the virtual
-  // memory space.
-  bool Initialize(v8::VirtualAddressSpace* vas, size_t size,
-                  bool use_guard_regions);
-
-  // Used on OSes where reserving virtual memory is too expensive. A fake cage
-  // does not reserve all of the virtual memory and so doesn't have the desired
-  // security properties.
-  // The size and size_to_reserve parameters must be multiples of the
-  // allocation granularity of the virtual address space.
-  bool InitializeAsFakeCage(v8::VirtualAddressSpace* vas, size_t size,
-                            size_t size_to_reserve);
-
-  // Initialize the caged pointer constants for this cage. Called by the
-  // Initialize methods above.
-  void InitializeConstants();
-
-  Address base_ = kNullAddress;
-  Address end_ = kNullAddress;
-  size_t size_ = 0;
-
-  // Base and size of the virtual memory reservation backing this cage. These
-  // can be different from the cage base and size due to guard regions or when a
-  // fake cage is used.
-  Address reservation_base_ = kNullAddress;
-  size_t reservation_size_ = 0;
-
-  bool initialized_ = false;
-  bool disabled_ = false;
-  bool is_fake_cage_ = false;
-
-  // The virtual address subspace backing the cage.
-  std::unique_ptr<v8::VirtualAddressSpace> virtual_address_space_;
-
-  // The page allocator instance for this cage.
-  std::unique_ptr<v8::PageAllocator> cage_page_allocator_;
-
-#ifdef V8_CAGED_POINTERS
-  // CagedPointer constants inside this cage.
-  CagedPointerConstants constants_;
-#endif
-};
-
-#endif  // V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
-
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-// This function is only available when the cage is actually used.
-V8_EXPORT_PRIVATE V8VirtualMemoryCage* GetProcessWideVirtualMemoryCage();
-#endif
-
-V8_INLINE bool IsValidBackingStorePointer(void* ptr) {
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  Address addr = reinterpret_cast<Address>(ptr);
-  return kAllowBackingStoresOutsideCage || addr == kNullAddress ||
-         GetProcessWideVirtualMemoryCage()->Contains(addr);
-#else
-  return true;
-#endif
-}
-
-V8_INLINE void* EmptyBackingStoreBuffer() {
-#ifdef V8_CAGED_POINTERS
-  return reinterpret_cast<void*>(GetProcessWideVirtualMemoryCage()
-                                     ->constants()
-                                     .empty_backing_store_buffer());
-#else
-  return nullptr;
-#endif
-}
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_SECURITY_VM_CAGE_H_
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index fb3c41888e..997e953009 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -32,7 +32,7 @@
 #include "src/objects/slots.h"
 #include "src/objects/string.h"
 #include "src/roots/roots.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/snapshot/embedded/embedded-data.h"
 #include "src/snapshot/references.h"
 #include "src/snapshot/serializer-deserializer.h"
@@ -996,11 +996,12 @@ int Deserializer<IsolateT>::ReadSingleBytecodeData(byte data,
     case kSandboxedExternalReference:
     case kExternalReference: {
       Address address = ReadExternalReferenceCase();
-      if (V8_HEAP_SANDBOX_BOOL && data == kSandboxedExternalReference) {
+      if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL &&
+          data == kSandboxedExternalReference) {
         return WriteExternalPointer(slot_accessor.slot(), address,
                                     kForeignForeignAddressTag);
       } else {
-        DCHECK(!V8_HEAP_SANDBOX_BOOL);
+        DCHECK(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);
         return WriteAddress(slot_accessor.slot(), address);
       }
     }
@@ -1159,11 +1160,12 @@ int Deserializer<IsolateT>::ReadSingleBytecodeData(byte data,
       } else {
         address = reinterpret_cast<Address>(NoExternalReferencesCallback);
       }
-      if (V8_HEAP_SANDBOX_BOOL && data == kSandboxedApiReference) {
+      if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL &&
+          data == kSandboxedApiReference) {
         return WriteExternalPointer(slot_accessor.slot(), address,
                                     kForeignForeignAddressTag);
       } else {
-        DCHECK(!V8_HEAP_SANDBOX_BOOL);
+        DCHECK(!V8_SANDBOXED_EXTERNAL_POINTERS_BOOL);
         return WriteAddress(slot_accessor.slot(), address);
       }
     }
diff --git a/src/snapshot/mksnapshot.cc b/src/snapshot/mksnapshot.cc
index 5687172e60..6e2faff280 100644
--- a/src/snapshot/mksnapshot.cc
+++ b/src/snapshot/mksnapshot.cc
@@ -240,9 +240,9 @@ int main(int argc, char** argv) {
   v8::V8::InitializeICUDefaultLocation(argv[0]);
   std::unique_ptr<v8::Platform> platform = v8::platform::NewDefaultPlatform();
   v8::V8::InitializePlatform(platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   v8::V8::Initialize();
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index 2ae6fc17b1..db00728c24 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -559,13 +559,13 @@ void Serializer::ObjectSerializer::SerializeExternalString() {
   if (serializer_->external_reference_encoder_.TryEncode(resource).To(
           &reference)) {
     DCHECK(reference.is_from_api());
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     uint32_t external_pointer_entry =
         string->GetResourceRefForDeserialization();
 #endif
     string->SetResourceRefForSerialization(reference.index());
     SerializeObject();
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
     string->SetResourceRefForSerialization(external_pointer_entry);
 #else
     string->set_address_as_resource(isolate(), resource);
@@ -946,14 +946,14 @@ void Serializer::ObjectSerializer::OutputExternalReference(Address target,
     sink_->Put(FixedRawDataWithSize::Encode(size_in_tagged), "FixedRawData");
     sink_->PutRaw(reinterpret_cast<byte*>(&target), target_size, "Bytes");
   } else if (encoded_reference.is_from_api()) {
-    if (V8_HEAP_SANDBOX_BOOL && sandboxify) {
+    if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL && sandboxify) {
       sink_->Put(kSandboxedApiReference, "SandboxedApiRef");
     } else {
       sink_->Put(kApiReference, "ApiRef");
     }
     sink_->PutInt(encoded_reference.index(), "reference index");
   } else {
-    if (V8_HEAP_SANDBOX_BOOL && sandboxify) {
+    if (V8_SANDBOXED_EXTERNAL_POINTERS_BOOL && sandboxify) {
       sink_->Put(kSandboxedExternalReference, "SandboxedExternalRef");
     } else {
       sink_->Put(kExternalReference, "ExternalRef");
diff --git a/src/utils/allocation.cc b/src/utils/allocation.cc
index 569c67fd25..033cdc32f0 100644
--- a/src/utils/allocation.cc
+++ b/src/utils/allocation.cc
@@ -19,7 +19,7 @@
 #include "src/base/virtual-address-space.h"
 #include "src/flags/flags.h"
 #include "src/init/v8.h"
-#include "src/security/vm-cage.h"
+#include "src/sandbox/sandbox.h"
 #include "src/utils/memcopy.h"
 
 #if V8_LIBC_BIONIC
@@ -96,15 +96,15 @@ v8::VirtualAddressSpace* GetPlatformVirtualAddressSpace() {
   return vas.get();
 }
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-v8::PageAllocator* GetVirtualMemoryCagePageAllocator() {
+#ifdef V8_SANDBOX
+v8::PageAllocator* GetSandboxPageAllocator() {
   // TODO(chromium:1218005) remove this code once the cage is no longer
   // optional.
-  if (GetProcessWideVirtualMemoryCage()->is_disabled()) {
+  if (GetProcessWideSandbox()->is_disabled()) {
     return GetPlatformPageAllocator();
   } else {
-    CHECK(GetProcessWideVirtualMemoryCage()->is_initialized());
-    return GetProcessWideVirtualMemoryCage()->page_allocator();
+    CHECK(GetProcessWideSandbox()->is_initialized());
+    return GetProcessWideSandbox()->page_allocator();
   }
 }
 #endif
diff --git a/src/utils/allocation.h b/src/utils/allocation.h
index 623214db7b..25d391af0d 100644
--- a/src/utils/allocation.h
+++ b/src/utils/allocation.h
@@ -106,19 +106,19 @@ V8_EXPORT_PRIVATE v8::PageAllocator* GetPlatformPageAllocator();
 // pointer.
 V8_EXPORT_PRIVATE v8::VirtualAddressSpace* GetPlatformVirtualAddressSpace();
 
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-// Returns the virtual memory cage page allocator instance for allocating pages
-// inside the virtual memory cage. Guaranteed to be a valid pointer.
-V8_EXPORT_PRIVATE v8::PageAllocator* GetVirtualMemoryCagePageAllocator();
+#ifdef V8_SANDBOX
+// Returns the page allocator instance for allocating pages inside the sandbox.
+// Guaranteed to be a valid pointer.
+V8_EXPORT_PRIVATE v8::PageAllocator* GetSandboxPageAllocator();
 #endif
 
-// Returns the appropriate page allocator to use for ArrayBuffer backing stores.
-// If the virtual memory cage is enabled, these must be allocated inside the
-// cage and so this will be the CagePageAllocator. Otherwise it will be the
-// PlatformPageAllocator.
+// Returns the appropriate page allocator to use for ArrayBuffer backing
+// stores. If the sandbox is enabled, these must be allocated inside the
+// sandbox and so this will be the SandboxPageAllocator. Otherwise it will be
+// the PlatformPageAllocator.
 inline v8::PageAllocator* GetArrayBufferPageAllocator() {
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  return GetVirtualMemoryCagePageAllocator();
+#ifdef V8_SANDBOX
+  return GetSandboxPageAllocator();
 #else
   return GetPlatformPageAllocator();
 #endif
@@ -334,9 +334,6 @@ class VirtualMemory final {
 //   and the base bias size must be AllocatePageSize-aligned.
 // - The base alignment may be kAnyBaseAlignment to denote any alignment is
 //   acceptable. In this case the base bias size does not need to be aligned.
-//
-// TODO(chromium:1218005) can we either combine this class and
-// v8::VirtualMemoryCage in v8-platform.h or rename one of the two?
 class VirtualMemoryCage {
  public:
   VirtualMemoryCage();
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index 202027f45c..798cff9910 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -6086,7 +6086,7 @@ class LiftoffCompiler {
           wasm::ObjectAccess::ToTagged(WasmInternalFunction::kRefOffset),
           pinned);
 
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
       LOAD_INSTANCE_FIELD(temp.gp(), IsolateRoot, kSystemPointerSize, pinned);
       __ LoadExternalPointer(target.gp(), func_ref.gp(),
                              WasmInternalFunction::kForeignAddressOffset,
diff --git a/src/wasm/c-api.cc b/src/wasm/c-api.cc
index 6b9d7e9de6..5be2520c56 100644
--- a/src/wasm/c-api.cc
+++ b/src/wasm/c-api.cc
@@ -395,9 +395,9 @@ auto Engine::make(own<Config>&& config) -> own<Engine> {
   if (!engine) return own<Engine>();
   engine->platform = v8::platform::NewDefaultPlatform();
   v8::V8::InitializePlatform(engine->platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   v8::V8::Initialize();
diff --git a/src/wasm/wasm-objects-inl.h b/src/wasm/wasm-objects-inl.h
index a61ec2da12..b708969e42 100644
--- a/src/wasm/wasm-objects-inl.h
+++ b/src/wasm/wasm-objects-inl.h
@@ -630,7 +630,7 @@ void WasmArray::EncodeElementSizeInMap(int element_size, Map map) {
 int WasmArray::DecodeElementSizeFromMap(Map map) { return map.WasmByte1(); }
 
 void WasmTypeInfo::clear_foreign_address(Isolate* isolate) {
-#ifdef V8_HEAP_SANDBOX
+#ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   // Due to the type-specific pointer tags for external pointers, we need to
   // allocate an entry in the table here even though it will just store nullptr.
   AllocateExternalPointerEntries(isolate);
diff --git a/src/wasm/wasm-objects.tq b/src/wasm/wasm-objects.tq
index 8525d530fd..c3837b5510 100644
--- a/src/wasm/wasm-objects.tq
+++ b/src/wasm/wasm-objects.tq
@@ -14,9 +14,9 @@ extern class WasmInstanceObject extends JSObject;
 // Represents the context of a function that is defined through the JS or C
 // APIs. Corresponds to the WasmInstanceObject passed to a Wasm function
 // reference.
-// TODO(manoskouk): If V8_HEAP_SANDBOX, we cannot encode the isolate_root as a
-// sandboxed pointer, because that would require having access to the isolate
-// root in the first place.
+// TODO(manoskouk): If V8_SANDBOXED_EXTERNAL_POINTERS, we cannot encode the
+// isolate_root as a sandboxed pointer, because that would require having access
+// to the isolate root in the first place.
 extern class WasmApiFunctionRef extends HeapObject {
   isolate_root: RawPtr;
   native_context: NativeContext;
diff --git a/test/cctest/cctest.cc b/test/cctest/cctest.cc
index 04f3167b68..500434ef9f 100644
--- a/test/cctest/cctest.cc
+++ b/test/cctest/cctest.cc
@@ -335,8 +335,8 @@ int main(int argc, char* argv[]) {
   v8::V8::InitializeICUDefaultLocation(argv[0]);
   std::unique_ptr<v8::Platform> platform(v8::platform::NewDefaultPlatform());
   v8::V8::InitializePlatform(platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  CHECK(v8::V8::InitializeVirtualMemoryCage());
+#ifdef V8_SANDBOX
+  CHECK(v8::V8::InitializeSandbox());
 #endif
   cppgc::InitializeProcess(platform->GetPageAllocator());
   using HelpOptions = v8::internal::FlagList::HelpOptions;
diff --git a/test/cctest/test-api.cc b/test/cctest/test-api.cc
index 3f78c42454..8535761148 100644
--- a/test/cctest/test-api.cc
+++ b/test/cctest/test-api.cc
@@ -3065,7 +3065,7 @@ THREADED_TEST(InternalFieldsAlignedPointers) {
   CheckAlignedPointerInInternalField(obj, stack_allocated);
 
   // The aligned pointer must have the top bits be zero on 64-bit machines (at
-  // least if the heap sandbox is enabled).
+  // least if the sandboxed external pointers are enabled).
   void* huge = reinterpret_cast<void*>(0x0000fffffffffffe);
   CheckAlignedPointerInInternalField(obj, huge);
 
@@ -3143,7 +3143,7 @@ THREADED_TEST(EmbedderDataAlignedPointers) {
   CHECK_EQ(3, (*env)->GetNumberOfEmbedderDataFields());
 
   // The aligned pointer must have the top bits be zero on 64-bit machines (at
-  // least if the heap sandbox is enabled).
+  // least if the sandboxed external pointers are enabled).
   void* huge = reinterpret_cast<void*>(0x0000fffffffffffe);
   CheckAlignedPointerInEmbedderData(&env, 3, huge);
   CHECK_EQ(4, (*env)->GetNumberOfEmbedderDataFields());
diff --git a/test/fuzzer/fuzzer-support.cc b/test/fuzzer/fuzzer-support.cc
index 2eda4fafc9..51e905490d 100644
--- a/test/fuzzer/fuzzer-support.cc
+++ b/test/fuzzer/fuzzer-support.cc
@@ -23,9 +23,9 @@ FuzzerSupport::FuzzerSupport(int* argc, char*** argv) {
   v8::V8::InitializeExternalStartupData((*argv)[0]);
   platform_ = v8::platform::NewDefaultPlatform();
   v8::V8::InitializePlatform(platform_.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   v8::V8::Initialize();
diff --git a/test/inspector/inspector-test.cc b/test/inspector/inspector-test.cc
index 81395445ac..e4e54ee6f9 100644
--- a/test/inspector/inspector-test.cc
+++ b/test/inspector/inspector-test.cc
@@ -750,9 +750,9 @@ int InspectorTestMain(int argc, char* argv[]) {
   v8::V8::InitializeICUDefaultLocation(argv[0]);
   std::unique_ptr<Platform> platform(platform::NewDefaultPlatform());
   v8::V8::InitializePlatform(platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   FLAG_abort_on_contradictory_flags = true;
diff --git a/test/mjsunit/mjsunit.status b/test/mjsunit/mjsunit.status
index a4a6c679ea..0cc3a936d2 100644
--- a/test/mjsunit/mjsunit.status
+++ b/test/mjsunit/mjsunit.status
@@ -1303,9 +1303,9 @@
 }], # arch not in (x64, ia32, arm64, arm, s390x, ppc64, mipsel, mips64el, loong64)
 
 ##############################################################################
-['system != linux or virtual_memory_cage == True', {
+['system != linux or sandbox == True', {
   # Multi-mapped mock allocator is only available on Linux, and only if the
-  # virtual memory cage is not enabled.
+  # sandbox is not enabled.
   'regress/regress-crbug-1041232': [SKIP],
   'regress/regress-crbug-1104608': [SKIP],
 }],
diff --git a/test/mkgrokdump/mkgrokdump.cc b/test/mkgrokdump/mkgrokdump.cc
index f539889b9f..83c10814e7 100644
--- a/test/mkgrokdump/mkgrokdump.cc
+++ b/test/mkgrokdump/mkgrokdump.cc
@@ -115,9 +115,9 @@ static int DumpHeapConstants(FILE* out, const char* argv0) {
   // Start up V8.
   std::unique_ptr<v8::Platform> platform = v8::platform::NewDefaultPlatform();
   v8::V8::InitializePlatform(platform.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-  if (!v8::V8::InitializeVirtualMemoryCage()) {
-    FATAL("Could not initialize the virtual memory cage");
+#ifdef V8_SANDBOX
+  if (!v8::V8::InitializeSandbox()) {
+    FATAL("Could not initialize the sandbox");
   }
 #endif
   v8::V8::Initialize();
diff --git a/test/unittests/BUILD.gn b/test/unittests/BUILD.gn
index de1816be87..b40bf994d2 100644
--- a/test/unittests/BUILD.gn
+++ b/test/unittests/BUILD.gn
@@ -375,7 +375,7 @@ v8_source_set("unittests_sources") {
     "regress/regress-crbug-938251-unittest.cc",
     "run-all-unittests.cc",
     "runtime/runtime-debug-unittest.cc",
-    "security/virtual-memory-cage-unittest.cc",
+    "sandbox/sandbox-unittest.cc",
     "strings/char-predicates-unittest.cc",
     "strings/unicode-unittest.cc",
     "tasks/background-compile-task-unittest.cc",
diff --git a/test/unittests/heap/unmapper-unittest.cc b/test/unittests/heap/unmapper-unittest.cc
index b0b6371ca1..27c7e0163b 100644
--- a/test/unittests/heap/unmapper-unittest.cc
+++ b/test/unittests/heap/unmapper-unittest.cc
@@ -233,9 +233,9 @@ class TrackingPageAllocator : public ::v8::PageAllocator {
   PagePermissionsMap page_permissions_;
 };
 
-// This test is currently incompatible with the VirtualMemoryCage. Enable it
-// once the VirtualMemorySpace interface is stable.
-#if !V8_OS_FUCHSIA && !V8_VIRTUAL_MEMORY_CAGE
+// This test is currently incompatible with the sandbox. Enable it
+// once the VirtualAddressSpace interface is stable.
+#if !V8_OS_FUCHSIA && !V8_SANDBOX
 class SequentialUnmapperTest : public TestWithIsolate {
  public:
   SequentialUnmapperTest() = default;
@@ -255,15 +255,14 @@ class SequentialUnmapperTest : public TestWithIsolate {
 #ifdef V8_COMPRESS_POINTERS_IN_SHARED_CAGE
     // Reinitialize the process-wide pointer cage so it can pick up the
     // TrackingPageAllocator.
-    // The pointer cage must be destroyed before the virtual memory cage.
+    // The pointer cage must be destroyed before the sandbox.
     IsolateAllocator::FreeProcessWidePtrComprCageForTesting();
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-    // Reinitialze the virtual memory cage so it uses the TrackingPageAllocator.
-    GetProcessWideVirtualMemoryCage()->TearDown();
+#ifdef V8_SANDBOX
+    // Reinitialze the sandbox so it uses the TrackingPageAllocator.
+    GetProcessWideSandbox()->TearDown();
     constexpr bool use_guard_regions = false;
-    CHECK(GetProcessWideVirtualMemoryCage()->Initialize(
-        tracking_page_allocator_, kVirtualMemoryCageMinimumSize,
-        use_guard_regions));
+    CHECK(GetProcessWideSandbox()->Initialize(
+        tracking_page_allocator_, kSandboxMinimumSize, use_guard_regions));
 #endif
     IsolateAllocator::InitializeOncePerProcess();
 #endif
@@ -277,8 +276,8 @@ class SequentialUnmapperTest : public TestWithIsolate {
     // freed until process teardown.
     IsolateAllocator::FreeProcessWidePtrComprCageForTesting();
 #endif
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-    GetProcessWideVirtualMemoryCage()->TearDown();
+#ifdef V8_SANDBOX
+    GetProcessWideSandbox()->TearDown();
 #endif
     i::FLAG_concurrent_sweeping = old_flag_;
     CHECK(tracking_page_allocator_->IsEmpty());
@@ -364,7 +363,7 @@ TEST_F(SequentialUnmapperTest, UnmapOnTeardown) {
   tracking_page_allocator()->CheckIsFree(page->address(), page_size);
 #endif  // V8_COMPRESS_POINTERS
 }
-#endif  // !V8_OS_FUCHSIA && !V8_VIRTUAL_MEMORY_CAGE
+#endif  // !V8_OS_FUCHSIA && !V8_SANDBOX
 
 }  // namespace internal
 }  // namespace v8
diff --git a/test/unittests/run-all-unittests.cc b/test/unittests/run-all-unittests.cc
index 3cef764855..8437ac0acb 100644
--- a/test/unittests/run-all-unittests.cc
+++ b/test/unittests/run-all-unittests.cc
@@ -21,8 +21,8 @@ class DefaultPlatformEnvironment final : public ::testing::Environment {
         0, v8::platform::IdleTaskSupport::kEnabled);
     ASSERT_TRUE(platform_.get() != nullptr);
     v8::V8::InitializePlatform(platform_.get());
-#ifdef V8_VIRTUAL_MEMORY_CAGE
-    ASSERT_TRUE(v8::V8::InitializeVirtualMemoryCage());
+#ifdef V8_SANDBOX
+    ASSERT_TRUE(v8::V8::InitializeSandbox());
 #endif
     cppgc::InitializeProcess(platform_->GetPageAllocator());
     v8::V8::Initialize();
diff --git a/test/unittests/sandbox/sandbox-unittest.cc b/test/unittests/sandbox/sandbox-unittest.cc
new file mode 100644
index 0000000000..aa28b7c931
--- /dev/null
+++ b/test/unittests/sandbox/sandbox-unittest.cc
@@ -0,0 +1,155 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/sandbox/sandbox.h"
+
+#include <vector>
+
+#include "src/base/virtual-address-space.h"
+#include "test/unittests/test-utils.h"
+
+#ifdef V8_SANDBOX_IS_AVAILABLE
+
+namespace v8 {
+namespace internal {
+
+TEST(SandboxTest, Initialization) {
+  base::VirtualAddressSpace vas;
+
+  Sandbox sandbox;
+
+  EXPECT_FALSE(sandbox.is_initialized());
+  EXPECT_FALSE(sandbox.is_disabled());
+  EXPECT_FALSE(sandbox.is_partially_reserved());
+  EXPECT_EQ(sandbox.size(), 0UL);
+
+  EXPECT_TRUE(sandbox.Initialize(&vas));
+
+  EXPECT_TRUE(sandbox.is_initialized());
+  EXPECT_NE(sandbox.base(), 0UL);
+  EXPECT_GT(sandbox.size(), 0UL);
+
+  sandbox.TearDown();
+
+  EXPECT_FALSE(sandbox.is_initialized());
+}
+
+TEST(SandboxTest, InitializationWithSize) {
+  base::VirtualAddressSpace vas;
+  // This test only works if virtual memory subspaces can be allocated.
+  if (!vas.CanAllocateSubspaces()) return;
+
+  Sandbox sandbox;
+  size_t size = kSandboxMinimumSize;
+  const bool use_guard_regions = false;
+  EXPECT_TRUE(sandbox.Initialize(&vas, size, use_guard_regions));
+
+  EXPECT_TRUE(sandbox.is_initialized());
+  EXPECT_FALSE(sandbox.is_partially_reserved());
+  EXPECT_EQ(sandbox.size(), size);
+
+  sandbox.TearDown();
+}
+
+TEST(SandboxTest, PartiallyReservedSandboxInitialization) {
+  base::VirtualAddressSpace vas;
+  Sandbox sandbox;
+  // Total size of the sandbox.
+  size_t size = kSandboxSize;
+  // Size of the virtual memory that is actually reserved at the start of the
+  // sandbox.
+  size_t reserved_size = 2 * vas.allocation_granularity();
+  EXPECT_TRUE(
+      sandbox.InitializeAsPartiallyReservedSandbox(&vas, size, reserved_size));
+
+  EXPECT_TRUE(sandbox.is_initialized());
+  EXPECT_TRUE(sandbox.is_partially_reserved());
+  EXPECT_NE(sandbox.base(), 0UL);
+  EXPECT_EQ(sandbox.size(), size);
+
+  sandbox.TearDown();
+
+  EXPECT_FALSE(sandbox.is_initialized());
+}
+
+TEST(SandboxTest, Contains) {
+  base::VirtualAddressSpace vas;
+  Sandbox sandbox;
+  EXPECT_TRUE(sandbox.Initialize(&vas));
+
+  Address base = sandbox.base();
+  size_t size = sandbox.size();
+  base::RandomNumberGenerator rng(::testing::FLAGS_gtest_random_seed);
+
+  EXPECT_TRUE(sandbox.Contains(base));
+  EXPECT_TRUE(sandbox.Contains(base + size - 1));
+  for (int i = 0; i < 10; i++) {
+    size_t offset = rng.NextInt64() % size;
+    EXPECT_TRUE(sandbox.Contains(base + offset));
+  }
+
+  EXPECT_FALSE(sandbox.Contains(base - 1));
+  EXPECT_FALSE(sandbox.Contains(base + size));
+  for (int i = 0; i < 10; i++) {
+    Address addr = rng.NextInt64();
+    if (addr < base || addr >= base + size) {
+      EXPECT_FALSE(sandbox.Contains(addr));
+    }
+  }
+
+  sandbox.TearDown();
+}
+
+void TestPageAllocationInSandbox(Sandbox& sandbox) {
+  const size_t kAllocatinSizesInPages[] = {1, 1, 2, 3, 5, 8, 13, 21, 34};
+  constexpr int kNumAllocations = arraysize(kAllocatinSizesInPages);
+
+  VirtualAddressSpace* vas = sandbox.address_space();
+  size_t allocation_granularity = vas->allocation_granularity();
+  std::vector<Address> allocations;
+  for (int i = 0; i < kNumAllocations; i++) {
+    size_t length = allocation_granularity * kAllocatinSizesInPages[i];
+    size_t alignment = allocation_granularity;
+    Address ptr = vas->AllocatePages(VirtualAddressSpace::kNoHint, length,
+                                     alignment, PagePermissions::kNoAccess);
+    EXPECT_NE(ptr, kNullAddress);
+    EXPECT_TRUE(sandbox.Contains(ptr));
+    allocations.push_back(ptr);
+  }
+
+  for (int i = 0; i < kNumAllocations; i++) {
+    size_t length = allocation_granularity * kAllocatinSizesInPages[i];
+    EXPECT_TRUE(vas->FreePages(allocations[i], length));
+  }
+}
+
+TEST(SandboxTest, PageAllocation) {
+  base::VirtualAddressSpace vas;
+  Sandbox sandbox;
+  EXPECT_TRUE(sandbox.Initialize(&vas));
+
+  TestPageAllocationInSandbox(sandbox);
+
+  sandbox.TearDown();
+}
+
+TEST(SandboxTest, PartiallyReservedSandboxPageAllocation) {
+  base::VirtualAddressSpace vas;
+  Sandbox sandbox;
+  size_t size = kSandboxSize;
+  // Only reserve two pages so the test will allocate memory inside and outside
+  // of the reserved region.
+  size_t reserved_size = 2 * vas.allocation_granularity();
+  EXPECT_TRUE(
+      sandbox.InitializeAsPartiallyReservedSandbox(&vas, size, reserved_size));
+
+  TestPageAllocationInSandbox(sandbox);
+
+  sandbox.TearDown();
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_SANDBOX_IS_AVAILABLE
diff --git a/test/unittests/security/virtual-memory-cage-unittest.cc b/test/unittests/security/virtual-memory-cage-unittest.cc
deleted file mode 100644
index 8ee4381b0f..0000000000
--- a/test/unittests/security/virtual-memory-cage-unittest.cc
+++ /dev/null
@@ -1,152 +0,0 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#include <vector>
-
-#include "src/base/virtual-address-space.h"
-#include "src/security/vm-cage.h"
-#include "test/unittests/test-utils.h"
-
-#ifdef V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
-
-namespace v8 {
-namespace internal {
-
-TEST(VirtualMemoryCageTest, Initialization) {
-  base::VirtualAddressSpace vas;
-
-  V8VirtualMemoryCage cage;
-
-  EXPECT_FALSE(cage.is_initialized());
-  EXPECT_FALSE(cage.is_disabled());
-  EXPECT_FALSE(cage.is_fake_cage());
-  EXPECT_EQ(cage.size(), 0UL);
-
-  EXPECT_TRUE(cage.Initialize(&vas));
-
-  EXPECT_TRUE(cage.is_initialized());
-  EXPECT_NE(cage.base(), 0UL);
-  EXPECT_GT(cage.size(), 0UL);
-
-  cage.TearDown();
-
-  EXPECT_FALSE(cage.is_initialized());
-}
-
-TEST(VirtualMemoryCageTest, InitializationWithSize) {
-  base::VirtualAddressSpace vas;
-  // This test only works if virtual memory subspaces can be allocated.
-  if (!vas.CanAllocateSubspaces()) return;
-
-  V8VirtualMemoryCage cage;
-  size_t size = kVirtualMemoryCageMinimumSize;
-  const bool use_guard_regions = false;
-  EXPECT_TRUE(cage.Initialize(&vas, size, use_guard_regions));
-
-  EXPECT_TRUE(cage.is_initialized());
-  EXPECT_FALSE(cage.is_fake_cage());
-  EXPECT_EQ(cage.size(), size);
-
-  cage.TearDown();
-}
-
-TEST(VirtualMemoryCageTest, InitializationAsFakeCage) {
-  base::VirtualAddressSpace vas;
-  V8VirtualMemoryCage cage;
-  // Total size of the fake cage.
-  size_t size = kVirtualMemoryCageSize;
-  // Size of the virtual memory that is actually reserved at the start of the
-  // cage.
-  size_t reserved_size = 2 * vas.allocation_granularity();
-  EXPECT_TRUE(cage.InitializeAsFakeCage(&vas, size, reserved_size));
-
-  EXPECT_TRUE(cage.is_initialized());
-  EXPECT_TRUE(cage.is_fake_cage());
-  EXPECT_NE(cage.base(), 0UL);
-  EXPECT_EQ(cage.size(), size);
-
-  cage.TearDown();
-
-  EXPECT_FALSE(cage.is_initialized());
-}
-
-TEST(VirtualMemloryCageTest, Contains) {
-  base::VirtualAddressSpace vas;
-  V8VirtualMemoryCage cage;
-  EXPECT_TRUE(cage.Initialize(&vas));
-
-  Address base = cage.base();
-  size_t size = cage.size();
-  base::RandomNumberGenerator rng(::testing::FLAGS_gtest_random_seed);
-
-  EXPECT_TRUE(cage.Contains(base));
-  EXPECT_TRUE(cage.Contains(base + size - 1));
-  for (int i = 0; i < 10; i++) {
-    size_t offset = rng.NextInt64() % size;
-    EXPECT_TRUE(cage.Contains(base + offset));
-  }
-
-  EXPECT_FALSE(cage.Contains(base - 1));
-  EXPECT_FALSE(cage.Contains(base + size));
-  for (int i = 0; i < 10; i++) {
-    Address addr = rng.NextInt64();
-    if (addr < base || addr >= base + size) {
-      EXPECT_FALSE(cage.Contains(addr));
-    }
-  }
-
-  cage.TearDown();
-}
-
-void TestCagePageAllocation(V8VirtualMemoryCage& cage) {
-  const size_t kAllocatinSizesInPages[] = {1, 1, 2, 3, 5, 8, 13, 21, 34};
-  constexpr int kNumAllocations = arraysize(kAllocatinSizesInPages);
-
-  VirtualAddressSpace* vas = cage.virtual_address_space();
-  size_t allocation_granularity = vas->allocation_granularity();
-  std::vector<Address> allocations;
-  for (int i = 0; i < kNumAllocations; i++) {
-    size_t length = allocation_granularity * kAllocatinSizesInPages[i];
-    size_t alignment = allocation_granularity;
-    Address ptr = vas->AllocatePages(VirtualAddressSpace::kNoHint, length,
-                                     alignment, PagePermissions::kNoAccess);
-    EXPECT_NE(ptr, kNullAddress);
-    EXPECT_TRUE(cage.Contains(ptr));
-    allocations.push_back(ptr);
-  }
-
-  for (int i = 0; i < kNumAllocations; i++) {
-    size_t length = allocation_granularity * kAllocatinSizesInPages[i];
-    EXPECT_TRUE(vas->FreePages(allocations[i], length));
-  }
-}
-
-TEST(VirtualMemoryCageTest, PageAllocation) {
-  base::VirtualAddressSpace vas;
-  V8VirtualMemoryCage cage;
-  EXPECT_TRUE(cage.Initialize(&vas));
-
-  TestCagePageAllocation(cage);
-
-  cage.TearDown();
-}
-
-TEST(VirtualMemoryCageTest, FakeCagePageAllocation) {
-  base::VirtualAddressSpace vas;
-  V8VirtualMemoryCage cage;
-  size_t size = kVirtualMemoryCageSize;
-  // Only reserve two pages so the test will allocate memory inside and outside
-  // of the reserved region.
-  size_t reserved_size = 2 * vas.allocation_granularity();
-  EXPECT_TRUE(cage.InitializeAsFakeCage(&vas, size, reserved_size));
-
-  TestCagePageAllocation(cage);
-
-  cage.TearDown();
-}
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_VIRTUAL_MEMORY_CAGE_IS_AVAILABLE
diff --git a/tools/debug_helper/get-object-properties.cc b/tools/debug_helper/get-object-properties.cc
index 10ef48cbba..43a67941ac 100644
--- a/tools/debug_helper/get-object-properties.cc
+++ b/tools/debug_helper/get-object-properties.cc
@@ -11,7 +11,7 @@
 #include "src/execution/frames.h"
 #include "src/execution/isolate-utils.h"
 #include "src/objects/string-inl.h"
-#include "src/security/external-pointer.h"
+#include "src/sandbox/external-pointer.h"
 #include "src/strings/unicode-inl.h"
 #include "torque-generated/class-debug-readers.h"
 #include "torque-generated/debug-macros.h"
@@ -350,7 +350,7 @@ class ReadStringVisitor : public TqObjectVisitor {
       ExternalPointer_t resource_data =
           GetOrFinish(object->GetResourceDataValue(accessor_));
 #ifdef V8_COMPRESS_POINTERS
-      Isolate* isolate = GetIsolateForHeapSandbox(
+      Isolate* isolate = GetIsolateForSandbox(
           HeapObject::unchecked_cast(Object(heap_addresses_.any_heap_pointer)));
       uintptr_t data_address = static_cast<uintptr_t>(DecodeExternalPointer(
           isolate, resource_data, kExternalStringResourceDataTag));
diff --git a/tools/testrunner/base_runner.py b/tools/testrunner/base_runner.py
index 4b62fe7b9b..df0143cfeb 100644
--- a/tools/testrunner/base_runner.py
+++ b/tools/testrunner/base_runner.py
@@ -192,7 +192,7 @@ class BuildConfig(object):
     self.lite_mode = build_config['v8_enable_lite_mode']
     self.pointer_compression = build_config['v8_enable_pointer_compression']
     self.pointer_compression_shared_cage = build_config['v8_enable_pointer_compression_shared_cage']
-    self.virtual_memory_cage = build_config['v8_enable_virtual_memory_cage']
+    self.sandbox = build_config['v8_enable_sandbox']
     self.third_party_heap = build_config['v8_enable_third_party_heap']
     self.webassembly = build_config['v8_enable_webassembly']
     self.dict_property_const_tracking = build_config['v8_dict_property_const_tracking']
@@ -237,8 +237,8 @@ class BuildConfig(object):
       detected_options.append('pointer_compression')
     if self.pointer_compression_shared_cage:
       detected_options.append('pointer_compression_shared_cage')
-    if self.virtual_memory_cage:
-      detected_options.append('virtual_memory_cage')
+    if self.sandbox:
+      detected_options.append('sandbox')
     if self.third_party_heap:
       detected_options.append('third_party_heap')
     if self.webassembly:
@@ -736,7 +736,7 @@ class BaseTestRunner(object):
       "lite_mode": self.build_config.lite_mode,
       "pointer_compression": self.build_config.pointer_compression,
       "pointer_compression_shared_cage": self.build_config.pointer_compression_shared_cage,
-      "virtual_memory_cage": self.build_config.virtual_memory_cage,
+      "sandbox": self.build_config.sandbox,
       "dict_property_const_tracking": self.build_config.dict_property_const_tracking,
     }
 
diff --git a/tools/unittests/run_tests_test.py b/tools/unittests/run_tests_test.py
index 89acacaaa3..207a3485f5 100755
--- a/tools/unittests/run_tests_test.py
+++ b/tools/unittests/run_tests_test.py
@@ -351,7 +351,7 @@ class SystemTest(unittest.TestCase):
           v8_enable_verify_csa=False, v8_enable_lite_mode=False,
           v8_enable_pointer_compression=False,
           v8_enable_pointer_compression_shared_cage=False,
-          v8_enable_virtual_memory_cage=False)
+          v8_enable_sandbox=False)
       result = run_tests(
           basedir,
           '--progress=verbose',
diff --git a/tools/unittests/testdata/testroot1/v8_build_config.json b/tools/unittests/testdata/testroot1/v8_build_config.json
index d5d0f9981d..77c07ec5f1 100644
--- a/tools/unittests/testdata/testroot1/v8_build_config.json
+++ b/tools/unittests/testdata/testroot1/v8_build_config.json
@@ -22,7 +22,7 @@
   "v8_enable_lite_mode": false,
   "v8_enable_pointer_compression": true,
   "v8_enable_pointer_compression_shared_cage": true,
-  "v8_enable_virtual_memory_cage": false,
+  "v8_enable_sandbox": false,
   "v8_control_flow_integrity": false,
   "v8_enable_single_generation": false,
   "v8_enable_third_party_heap": false,
diff --git a/tools/unittests/testdata/testroot2/v8_build_config.json b/tools/unittests/testdata/testroot2/v8_build_config.json
index 590af4d59a..2de8e13d38 100644
--- a/tools/unittests/testdata/testroot2/v8_build_config.json
+++ b/tools/unittests/testdata/testroot2/v8_build_config.json
@@ -22,7 +22,7 @@
   "v8_enable_lite_mode": false,
   "v8_enable_pointer_compression": false,
   "v8_enable_pointer_compression_shared_cage": false,
-  "v8_enable_virtual_memory_cage": false,
+  "v8_enable_sandbox": false,
   "v8_control_flow_integrity": false,
   "v8_enable_single_generation": false,
   "v8_enable_third_party_heap": false,
diff --git a/tools/unittests/testdata/testroot3/v8_build_config.json b/tools/unittests/testdata/testroot3/v8_build_config.json
index d5d0f9981d..77c07ec5f1 100644
--- a/tools/unittests/testdata/testroot3/v8_build_config.json
+++ b/tools/unittests/testdata/testroot3/v8_build_config.json
@@ -22,7 +22,7 @@
   "v8_enable_lite_mode": false,
   "v8_enable_pointer_compression": true,
   "v8_enable_pointer_compression_shared_cage": true,
-  "v8_enable_virtual_memory_cage": false,
+  "v8_enable_sandbox": false,
   "v8_control_flow_integrity": false,
   "v8_enable_single_generation": false,
   "v8_enable_third_party_heap": false,
-- 
2.35.1

