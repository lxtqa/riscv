From ca5ace9b407580beaf0fabfb0bc8d2941f9d7a43 Mon Sep 17 00:00:00 2001
From: Jakob Linke <jgruber@chromium.org>
Date: Thu, 30 Mar 2023 09:00:37 +0200
Subject: [PATCH] Various Code/IStream cleanups

- Consistent lower_case names for InstructionStart/End/Size
- Rename entry/code_entry/code_entry_point to instruction_start
- Move IStream::kMaxArguments to Code
- Add IStream::body_size s.t. IStream objects know their own layout
- Add more notes why we alloc Code before IStream objects
- Remove dead code
- Remove obsolete ToCode and FromCode conversion helpers
- Tag both Code and IStream in heap-snapshot-generator

No-Try: true
Bug: v8:13789
Change-Id: Ifb7b8a10f5568c2099e3250d0fdf6867bee75804
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4381444
Commit-Queue: Jakob Linke <jgruber@chromium.org>
Reviewed-by: Toon Verwaest <verwaest@chromium.org>
Reviewed-by: Igor Sheludko <ishell@chromium.org>
Cr-Commit-Position: refs/heads/main@{#86788}
---
 src/api/api.cc                                |   4 +-
 src/builtins/arm/builtins-arm.cc              |   8 +-
 src/builtins/arm64/builtins-arm64.cc          |   8 +-
 src/builtins/builtins-regexp-gen.cc           |   2 +-
 src/builtins/builtins.cc                      |   4 +-
 src/builtins/ia32/builtins-ia32.cc            |   8 +-
 src/builtins/loong64/builtins-loong64.cc      |   6 +-
 src/builtins/mips64/builtins-mips64.cc        |   6 +-
 src/builtins/ppc/builtins-ppc.cc              |   8 +-
 src/builtins/riscv/builtins-riscv.cc          |   6 +-
 src/builtins/s390/builtins-s390.cc            |   8 +-
 src/builtins/setup-builtins-internal.cc       |   4 +-
 src/builtins/wasm.tq                          |   4 +-
 src/builtins/x64/builtins-x64.cc              |   8 +-
 src/codegen/arm/macro-assembler-arm.cc        |  11 +-
 src/codegen/arm/macro-assembler-arm.h         |   2 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |   9 +-
 src/codegen/arm64/macro-assembler-arm64.h     |   2 +-
 src/codegen/code-reference.cc                 |   6 +-
 src/codegen/code-stub-assembler.cc            |   6 +-
 src/codegen/code-stub-assembler.h             |  18 +-
 src/codegen/ia32/macro-assembler-ia32.cc      |  11 +-
 src/codegen/ia32/macro-assembler-ia32.h       |   2 +-
 .../loong64/macro-assembler-loong64.cc        |  16 +-
 src/codegen/loong64/macro-assembler-loong64.h |   3 +-
 src/codegen/mips64/macro-assembler-mips64.cc  |  18 +-
 src/codegen/mips64/macro-assembler-mips64.h   |   3 +-
 src/codegen/ppc/macro-assembler-ppc.cc        |  14 +-
 src/codegen/ppc/macro-assembler-ppc.h         |   2 +-
 src/codegen/reloc-info.cc                     |   4 +-
 src/codegen/riscv/macro-assembler-riscv.cc    |  13 +-
 src/codegen/riscv/macro-assembler-riscv.h     |   2 +-
 src/codegen/s390/macro-assembler-s390.cc      |  11 +-
 src/codegen/s390/macro-assembler-s390.h       |   2 +-
 src/codegen/x64/macro-assembler-x64.cc        |   9 +-
 src/codegen/x64/macro-assembler-x64.h         |   2 +-
 src/compiler/backend/code-generator.cc        |   6 +-
 .../backend/x64/code-generator-x64.cc         |   4 +-
 src/compiler/wasm-compiler.cc                 |   2 +-
 src/debug/debug.cc                            |   6 +-
 src/deoptimizer/deoptimizer.cc                |  22 +--
 src/diagnostics/objects-debug.cc              |  12 +-
 src/diagnostics/objects-printer.cc            |   4 +-
 src/diagnostics/perf-jit.cc                   |   8 +-
 src/execution/execution.cc                    |   6 +-
 src/execution/frames.cc                       |   2 +-
 src/execution/isolate.cc                      |  22 +--
 src/execution/simulator.h                     |   2 +-
 src/heap/code-stats.cc                        |   4 +-
 src/heap/factory-base.cc                      |   2 +-
 src/heap/factory.cc                           | 167 ++++++++--------
 src/heap/factory.h                            |   9 +-
 src/heap/mark-compact.cc                      |   8 +-
 src/heap/remembered-set.h                     |   2 +-
 src/ic/ic.cc                                  |   2 +-
 src/interpreter/interpreter.cc                |   4 +-
 src/logging/log.cc                            |   7 +-
 src/maglev/maglev-ir.h                        |  12 +-
 src/objects/code-inl.h                        | 181 +++++++----------
 src/objects/code.cc                           |  23 ++-
 src/objects/code.h                            | 183 +++++++-----------
 src/objects/js-function-inl.h                 |   4 +-
 src/objects/js-function.h                     |   2 +-
 src/objects/objects-body-descriptors-inl.h    |   4 +-
 src/objects/objects.cc                        |   2 +-
 src/parsing/parser-base.h                     |   7 +-
 src/parsing/parser.cc                         |   2 +-
 src/profiler/cpu-profiler.cc                  |   4 +-
 src/profiler/heap-snapshot-generator.cc       |  30 +--
 src/profiler/profiler-listener.cc             |   2 +-
 src/regexp/regexp-macro-assembler.cc          |   2 +-
 src/runtime/runtime-regexp.cc                 |   9 +-
 src/snapshot/deserializer.cc                  |  10 +-
 src/snapshot/embedded/embedded-data.cc        |   2 +-
 src/snapshot/read-only-deserializer.cc        |   4 +-
 src/snapshot/read-only-serializer.cc          |  21 +-
 src/snapshot/read-only-serializer.h           |   9 +-
 src/snapshot/serializer.cc                    |  16 +-
 src/snapshot/serializer.h                     |   2 +-
 src/wasm/baseline/liftoff-compiler.cc         |   2 +-
 src/wasm/wasm-code-manager.cc                 |   4 +-
 test/cctest/compiler/codegen-tester.h         |   2 +-
 test/cctest/compiler/test-code-generator.cc   |   2 +-
 test/cctest/test-api.cc                       |   9 +-
 test/cctest/test-assembler-arm64.cc           |   4 +-
 test/cctest/test-assembler-mips64.cc          |   6 +-
 test/cctest/test-disasm-regex-helper.cc       |   4 +-
 test/cctest/test-heap-profiler.cc             |   6 +-
 test/cctest/test-sync-primitives-arm64.cc     |   4 +-
 test/cctest/test-unwinder-code-pages.cc       |  16 +-
 test/cctest/wasm/test-gc.cc                   |   4 +-
 test/common/call-tester.h                     |  11 +-
 test/fuzzer/multi-return.cc                   |   4 +-
 .../assembler/disasm-ia32-unittest.cc         |   4 +-
 .../assembler/disasm-x64-unittest.cc          |   4 +-
 .../assembler/macro-assembler-x64-unittest.cc |   2 +-
 .../unittests/codegen/code-layout-unittest.cc |  13 +-
 test/unittests/codegen/code-pages-unittest.cc |   4 +-
 test/unittests/compiler/codegen-tester.h      |   2 +-
 test/unittests/logging/log-unittest.cc        |   6 +-
 test/unittests/parser/parsing-unittest.cc     |   8 +-
 101 files changed, 569 insertions(+), 634 deletions(-)

diff --git a/src/api/api.cc b/src/api/api.cc
index fd4ec6ae2a..f8f0ea4a9e 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -10097,8 +10097,8 @@ JSEntryStubs Isolate::GetJSEntryStubs() {
   for (auto& pair : stubs) {
     i::Code js_entry = i_isolate->builtins()->code(pair.first);
     pair.second->code.start =
-        reinterpret_cast<const void*>(js_entry.InstructionStart());
-    pair.second->code.length_in_bytes = js_entry.InstructionSize();
+        reinterpret_cast<const void*>(js_entry.instruction_start());
+    pair.second->code.length_in_bytes = js_entry.instruction_size();
   }
 
   return entry_stubs;
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index b7996476c7..cdfbe6d4c1 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -1516,7 +1516,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ ldr(r2,
          FieldMemOperand(r2, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(r2, r2);
+  __ LoadCodeInstructionStart(r2, r2);
   __ b(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -1779,7 +1779,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
   __ ldr(r1,
          FieldMemOperand(r0, Code::kDeoptimizationDataOrInterpreterDataOffset));
 
-  __ LoadCodeEntry(r0, r0);
+  __ LoadCodeInstructionStart(r0, r0);
 
   {
     ConstantPoolUnavailableScope constant_pool_unavailable(masm);
@@ -2014,7 +2014,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
@@ -3687,7 +3687,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index 4792cc0e17..c95d6dabf9 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -1756,7 +1756,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedField(
       x1, FieldMemOperand(x1, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(x1, x1);
+  __ LoadCodeInstructionStart(x1, x1);
   __ B(&trampoline_loaded);
 
   __ Bind(&builtin_trampoline);
@@ -2018,7 +2018,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       x1, FieldMemOperand(x1, FixedArray::OffsetOfElementAt(
                                   DeoptimizationData::kOsrPcOffsetIndex)));
 
-  __ LoadCodeEntry(x0, x0);
+  __ LoadCodeInstructionStart(x0, x0);
 
   // Compute the target address = code_entry + osr_offset
   // <entry_addr> = <code_entry> + <osr_offset>
@@ -2344,7 +2344,7 @@ void Generate_PrepareForCopyingVarargs(MacroAssembler* masm, Register argc,
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
@@ -5784,7 +5784,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ Add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister, padreg);
 
diff --git a/src/builtins/builtins-regexp-gen.cc b/src/builtins/builtins-regexp-gen.cc
index 32e9ae742c..a67ab2c589 100644
--- a/src/builtins/builtins-regexp-gen.cc
+++ b/src/builtins/builtins-regexp-gen.cc
@@ -580,7 +580,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
     MachineType arg8_type = type_tagged;
     TNode<JSRegExp> arg8 = regexp;
 
-    TNode<RawPtrT> code_entry = GetCodeEntry(code);
+    TNode<RawPtrT> code_entry = LoadCodeInstructionStart(code);
 
     // AIX uses function descriptors on CFunction calls. code_entry in this case
     // may also point to a Regex interpreter entry trampoline which does not
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index ea41f048ef..603152995f 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -272,7 +272,7 @@ void Builtins::PrintBuiltinSize() {
     const char* kind = KindNameOf(builtin);
     Code code = Builtins::code(builtin);
     PrintF(stdout, "%s Builtin, %s, %d\n", kind, builtin_name,
-           code.InstructionSize());
+           code.instruction_size());
   }
 }
 
@@ -369,7 +369,7 @@ Handle<Code> Builtins::CreateInterpreterEntryTrampolineForProfiling(
       Builtin::kInterpreterEntryTrampolineForProfiling);
 
   CodeDesc desc;
-  desc.buffer = reinterpret_cast<byte*>(code.InstructionStart());
+  desc.buffer = reinterpret_cast<byte*>(code.instruction_start());
 
   int instruction_size = code.instruction_size();
   desc.buffer_size = instruction_size;
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index 5e8ba2dee6..19bb725c7a 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -1431,7 +1431,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ mov(scratch,
          FieldOperand(scratch, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(scratch, scratch);
+  __ LoadCodeInstructionStart(scratch, scratch);
   __ jmp(&trampoline_loaded, Label::kNear);
 
   __ bind(&builtin_trampoline);
@@ -2065,7 +2065,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
@@ -2751,7 +2751,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                kHeapObjectTag));
   __ SmiUntag(ecx);
 
-  __ LoadCodeEntry(eax, eax);
+  __ LoadCodeInstructionStart(eax, eax);
 
   // Compute the target address = code_entry + osr_offset
   __ add(eax, ecx);
@@ -4311,7 +4311,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
            kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ add(code_obj, kReturnRegister0);
   __ pop(kInterpreterAccumulatorRegister);
 
diff --git a/src/builtins/loong64/builtins-loong64.cc b/src/builtins/loong64/builtins-loong64.cc
index fc89e454d8..2620c83a29 100644
--- a/src/builtins/loong64/builtins-loong64.cc
+++ b/src/builtins/loong64/builtins-loong64.cc
@@ -1512,7 +1512,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedField(
       t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(t0, t0);
+  __ LoadCodeInstructionStart(t0, t0);
   __ Branch(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -1781,7 +1781,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                       DeoptimizationData::kOsrPcOffsetIndex) -
                                       kHeapObjectTag));
 
-  __ LoadCodeEntry(maybe_target_code, maybe_target_code);
+  __ LoadCodeInstructionStart(maybe_target_code, maybe_target_code);
 
   // Compute the target address = code_entry + osr_offset
   // <entry_addr> = <code_entry> + <osr_offset>
@@ -3693,7 +3693,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0, a4);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ Add_d(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index 9c2a8a1bcb..8cb45bfd46 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -1475,7 +1475,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
             Operand(INTERPRETER_DATA_TYPE));
 
   __ Ld(t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(t0, t0);
+  __ LoadCodeInstructionStart(t0, t0);
   __ Branch(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -1738,7 +1738,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                      DeoptimizationData::kOsrPcOffsetIndex) -
                                      kHeapObjectTag));
 
-  __ LoadCodeEntry(maybe_target_code, maybe_target_code);
+  __ LoadCodeInstructionStart(maybe_target_code, maybe_target_code);
 
   // Compute the target address = code_entry + osr_offset
   // <entry_addr> = <code_entry> + <osr_offset>
@@ -3727,7 +3727,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0, a4);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ Daddu(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index ed3fcfd55d..5cfe351574 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -231,7 +231,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
   __ Pop(code_obj);
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ AddS64(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
@@ -443,7 +443,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       __ LoadConstantPoolPointerRegisterFromCodeTargetAddress(r3);
     }
 
-    __ LoadCodeEntry(r3, r3);
+    __ LoadCodeInstructionStart(r3, r3);
 
     // Load the OSR entrypoint offset from the deoptimization data.
     // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
@@ -1788,7 +1788,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   __ LoadTaggedField(
       r5, FieldMemOperand(r5, InterpreterData::kInterpreterTrampolineOffset),
       r0);
-  __ LoadCodeEntry(r5, r5);
+  __ LoadCodeInstructionStart(r5, r5);
   __ b(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -2225,7 +2225,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
diff --git a/src/builtins/riscv/builtins-riscv.cc b/src/builtins/riscv/builtins-riscv.cc
index 3404562785..a74bf19bcc 100644
--- a/src/builtins/riscv/builtins-riscv.cc
+++ b/src/builtins/riscv/builtins-riscv.cc
@@ -1532,7 +1532,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedField(
       t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(t0, t0);
+  __ LoadCodeInstructionStart(t0, t0);
   __ BranchShort(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -1796,7 +1796,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                      DeoptimizationData::kOsrPcOffsetIndex) -
                                      kHeapObjectTag));
 
-  __ LoadCodeEntry(a0, a0);
+  __ LoadCodeInstructionStart(a0, a0);
 
   // Compute the target address = code_entry + osr_offset
   // <entry_addr> = <code_entry> + <osr_offset>
@@ -3785,7 +3785,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ AddWord(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index ed4a8720c3..562f0ccb07 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -228,7 +228,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0, r1);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ AddS64(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
@@ -332,7 +332,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       r3, FieldMemOperand(r3, FixedArray::OffsetOfElementAt(
                                   DeoptimizationData::kOsrPcOffsetIndex)));
 
-  __ LoadCodeEntry(r2, r2);
+  __ LoadCodeInstructionStart(r2, r2);
 
   // Compute the target address = code_entry + osr_offset
   // <entry_addr> = <code_entry> + <osr_offset>
@@ -1811,7 +1811,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedField(
       r4, FieldMemOperand(r4, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(r4, r4);
+  __ LoadCodeInstructionStart(r4, r4);
   __ b(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
@@ -2230,7 +2230,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index ca7b4ca57c..bd09ae25c7 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -256,7 +256,7 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
             Builtins::IsIsolateIndependent(target_code.builtin_id()));
         if (!target_code.is_builtin()) continue;
         Code new_target = builtins->code(target_code.builtin_id());
-        rinfo->set_target_address(new_target.InstructionStart(),
+        rinfo->set_target_address(new_target.instruction_start(),
                                   UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       } else {
         DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
@@ -271,7 +271,7 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
       flush_icache = true;
     }
     if (flush_icache) {
-      FlushInstructionCache(code.InstructionStart(), code.instruction_size());
+      FlushInstructionCache(code.instruction_start(), code.instruction_size());
     }
   }
 }
diff --git a/src/builtins/wasm.tq b/src/builtins/wasm.tq
index 6a6096bb45..83636f99c1 100644
--- a/src/builtins/wasm.tq
+++ b/src/builtins/wasm.tq
@@ -513,7 +513,7 @@ builtin WasmI64AtomicWait(
 
 // Type feedback collection support for `call_ref`.
 
-extern macro GetCodeEntry(Code): RawPtr;
+extern macro LoadCodeInstructionStart(Code): RawPtr;
 
 struct TargetAndInstance {
   target: RawPtr;
@@ -524,7 +524,7 @@ macro GetTargetAndInstance(funcref: WasmInternalFunction): TargetAndInstance {
   const ref = funcref.ref;
   let target = funcref.call_target_ptr;
   if (Signed(target) == IntPtrConstant(0)) {
-    target = GetCodeEntry(funcref.code);
+    target = LoadCodeInstructionStart(funcref.code);
   }
   return TargetAndInstance{target: target, instance: ref};
 }
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 31d01a2099..aae2f1bb2c 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -1447,7 +1447,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedField(
       rbx, FieldOperand(rbx, InterpreterData::kInterpreterTrampolineOffset));
-  __ LoadCodeEntry(rbx, rbx);
+  __ LoadCodeInstructionStart(rbx, rbx);
   __ jmp(&trampoline_loaded, Label::kNear);
 
   __ bind(&builtin_trampoline);
@@ -2064,7 +2064,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 }  // namespace
 
 // static
-// TODO(v8:11615): Observe InstructionStream::kMaxArguments in
+// TODO(v8:11615): Observe Code::kMaxArguments in
 // CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
                                                Handle<Code> code) {
@@ -2699,7 +2699,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       FieldOperand(deopt_data, FixedArray::OffsetOfElementAt(
                                    DeoptimizationData::kOsrPcOffsetIndex)));
 
-  __ LoadCodeEntry(rax, rax);
+  __ LoadCodeInstructionStart(rax, rax);
 
   // Compute the target address = code_entry + osr_offset
   __ addq(rax, rbx);
@@ -5247,7 +5247,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ movq(arg_reg_3, kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
+  __ LoadCodeInstructionStart(code_obj, code_obj);
   __ addq(code_obj, kReturnRegister0);
   __ popq(kInterpreterAccumulatorRegister);
 
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index 9be1d37e03..9a7b512196 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -339,21 +339,22 @@ void MacroAssembler::TailCallBuiltin(Builtin builtin, Condition cond) {
   }
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
-  ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
+  ldr(destination, FieldMemOperand(code_object, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Jump(code_object);
 }
 
@@ -1921,7 +1922,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   // into the optimized functions list, then tail call the optimized code.
   __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure);
   static_assert(kJavaScriptCallCodeStartRegister == r2, "ABI mismatch");
-  __ LoadCodeEntry(r2, optimized_code_entry);
+  __ LoadCodeInstructionStart(r2, optimized_code_entry);
   __ Jump(r2);
 
   // Optimized code slot contains deoptimized code or code is cleared and
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 971b21661b..297b140375 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -335,7 +335,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void TailCallBuiltin(Builtin builtin, Condition cond = al);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index 2e54ce8bd1..125cc988da 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -2367,21 +2367,22 @@ void MacroAssembler::TailCallBuiltin(Builtin builtin, Condition cond) {
   }
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
-  Ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
+  Ldr(destination, FieldMemOperand(code_object, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   UseScratchRegisterScope temps(this);
   if (code_object != x17) {
     temps.Exclude(x17);
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 887d3b0e01..30c2a660eb 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1050,7 +1050,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void TailCallBuiltin(Builtin builtin, Condition cond = al);
 
   // Load code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/code-reference.cc b/src/codegen/code-reference.cc
index df7a7cd57f..702a85e06d 100644
--- a/src/codegen/code-reference.cc
+++ b/src/codegen/code-reference.cc
@@ -22,9 +22,9 @@ struct CodeOps {
   Handle<Code> code;
 
   Address constant_pool() const { return code->constant_pool(); }
-  Address instruction_start() const { return code->InstructionStart(); }
-  Address instruction_end() const { return code->InstructionEnd(); }
-  int instruction_size() const { return code->InstructionSize(); }
+  Address instruction_start() const { return code->instruction_start(); }
+  Address instruction_end() const { return code->instruction_end(); }
+  int instruction_size() const { return code->instruction_size(); }
   const byte* relocation_start() const { return code->relocation_start(); }
   const byte* relocation_end() const { return code->relocation_end(); }
   int relocation_size() const { return code->relocation_size(); }
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index c5fdaac240..102afc425c 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -15698,9 +15698,9 @@ TNode<Code> CodeStubAssembler::GetSharedFunctionInfoCode(
   return sfi_code.value();
 }
 
-TNode<RawPtrT> CodeStubAssembler::GetCodeEntry(TNode<Code> code) {
-  return LoadObjectField<RawPtrT>(code,
-                                  IntPtrConstant(Code::kCodeEntryPointOffset));
+TNode<RawPtrT> CodeStubAssembler::LoadCodeInstructionStart(TNode<Code> code) {
+  return LoadObjectField<RawPtrT>(
+      code, IntPtrConstant(Code::kInstructionStartOffset));
 }
 
 TNode<BoolT> CodeStubAssembler::IsMarkedForDeoptimization(TNode<Code> code) {
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index fa41984c6a..62dfa59949 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -837,23 +837,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 
   void FastCheck(TNode<BoolT> condition);
 
-  // TODO(v8:11880): remove once InstructionStream::bytecode_or_interpreter_data
-  // field is cached in or moved to Code.
-  TNode<InstructionStream> FromCodeNonBuiltin(TNode<Code> code) {
-    // Compute the InstructionStream object pointer from the code entry point.
-    TNode<RawPtrT> code_entry = Load<RawPtrT>(
-        code, IntPtrConstant(Code::kCodeEntryPointOffset - kHeapObjectTag));
-    TNode<Object> o = BitcastWordToTagged(IntPtrSub(
-        code_entry,
-        IntPtrConstant(InstructionStream::kHeaderSize - kHeapObjectTag)));
-    return CAST(o);
-  }
-
-  TNode<Code> ToCode(TNode<InstructionStream> code) {
-    return LoadObjectField<Code>(code, InstructionStream::kCodeOffset);
-  }
-
-  TNode<RawPtrT> GetCodeEntry(TNode<Code> code);
+  TNode<RawPtrT> LoadCodeInstructionStart(TNode<Code> code);
   TNode<BoolT> IsMarkedForDeoptimization(TNode<Code> code);
 
   // The following Call wrappers call an object according to the semantics that
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index 77d07785f5..00a8309062 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -750,7 +750,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
                                          ecx);
   static_assert(kJavaScriptCallCodeStartRegister == ecx, "ABI mismatch");
   __ Pop(optimized_code_entry);
-  __ LoadCodeEntry(ecx, optimized_code_entry);
+  __ LoadCodeInstructionStart(ecx, optimized_code_entry);
   __ Pop(edx);
   __ Pop(eax);
   __ jmp(ecx);
@@ -2000,18 +2000,19 @@ Operand MacroAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
   return Operand(kRootRegister, IsolateData::BuiltinEntrySlotOffset(builtin));
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
-  mov(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
+  mov(destination, FieldOperand(code_object, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   switch (jump_mode) {
     case JumpMode::kJump:
       jmp(code_object);
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 39ee0f106e..3ae8d9965f 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -159,7 +159,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void TailCallBuiltin(Builtin builtin);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/loong64/macro-assembler-loong64.cc b/src/codegen/loong64/macro-assembler-loong64.cc
index ceb2fef4d7..8b7be1b8c4 100644
--- a/src/codegen/loong64/macro-assembler-loong64.cc
+++ b/src/codegen/loong64/macro-assembler-loong64.cc
@@ -4179,16 +4179,17 @@ void MacroAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                             : Deoptimizer::kEagerDeoptExitSize);
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination,
-                                   Register code_data_container_object) {
+void MacroAssembler::LoadCodeInstructionStart(
+    Register destination, Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
   Ld_d(destination, FieldMemOperand(code_data_container_object,
-                                    Code::kCodeEntryPointOffset));
+                                    Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_data_container_object, code_data_container_object);
+  LoadCodeInstructionStart(code_data_container_object,
+                           code_data_container_object);
   Call(code_data_container_object);
 }
 
@@ -4196,7 +4197,8 @@ void MacroAssembler::JumpCodeObject(Register code_data_container_object,
                                     JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_data_container_object, code_data_container_object);
+  LoadCodeInstructionStart(code_data_container_object,
+                           code_data_container_object);
   Jump(code_data_container_object);
 }
 
@@ -4231,7 +4233,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure);
 
   static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-  __ LoadCodeEntry(a2, optimized_code_entry);
+  __ LoadCodeInstructionStart(a2, optimized_code_entry);
   __ Jump(a2);
 
   // Optimized code slot contains deoptimized code or code is cleared and
@@ -4281,7 +4283,7 @@ void MacroAssembler::GenerateTailCallToReturnedCode(
          kJavaScriptCallArgCountRegister, kJavaScriptCallTargetRegister);
 
     CallRuntime(function_id, 1);
-    LoadCodeEntry(a2, a0);
+    LoadCodeInstructionStart(a2, a0);
     // Restore target function, new target and actual argument count.
     Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
         kJavaScriptCallArgCountRegister);
diff --git a/src/codegen/loong64/macro-assembler-loong64.h b/src/codegen/loong64/macro-assembler-loong64.h
index 4930ef6c50..61d37a25ef 100644
--- a/src/codegen/loong64/macro-assembler-loong64.h
+++ b/src/codegen/loong64/macro-assembler-loong64.h
@@ -216,7 +216,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void TailCallBuiltin(Builtin builtin);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_data_container_object);
+  void LoadCodeInstructionStart(Register destination,
+                                Register code_data_container_object);
   void CallCodeObject(Register code_data_container_object);
   void JumpCodeObject(Register code_data_container_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/mips64/macro-assembler-mips64.cc b/src/codegen/mips64/macro-assembler-mips64.cc
index ecbd3fdb9b..6dfb403e34 100644
--- a/src/codegen/mips64/macro-assembler-mips64.cc
+++ b/src/codegen/mips64/macro-assembler-mips64.cc
@@ -6185,16 +6185,17 @@ void MacroAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                             : Deoptimizer::kEagerDeoptExitSize);
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination,
-                                   Register code_data_container_object) {
+void MacroAssembler::LoadCodeInstructionStart(
+    Register destination, Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
-  Ld(destination,
-     FieldMemOperand(code_data_container_object, Code::kCodeEntryPointOffset));
+  Ld(destination, FieldMemOperand(code_data_container_object,
+                                  Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_data_container_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_data_container_object, code_data_container_object);
+  LoadCodeInstructionStart(code_data_container_object,
+                           code_data_container_object);
   Call(code_data_container_object);
 }
 
@@ -6202,7 +6203,8 @@ void MacroAssembler::JumpCodeObject(Register code_data_container_object,
                                     JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_data_container_object, code_data_container_object);
+  LoadCodeInstructionStart(code_data_container_object,
+                           code_data_container_object);
   Jump(code_data_container_object);
 }
 
@@ -6240,7 +6242,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
                                          scratch1, scratch2);
 
   static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-  __ LoadCodeEntry(a2, optimized_code_entry);
+  __ LoadCodeInstructionStart(a2, optimized_code_entry);
   __ Jump(a2);
 
   // Optimized code slot contains deoptimized code or code is cleared and
@@ -6300,7 +6302,7 @@ void MacroAssembler::GenerateTailCallToReturnedCode(
   }
 
   static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-  LoadCodeEntry(a2, v0);
+  LoadCodeInstructionStart(a2, v0);
   Jump(a2);
 }
 
diff --git a/src/codegen/mips64/macro-assembler-mips64.h b/src/codegen/mips64/macro-assembler-mips64.h
index 2494edb217..9b8b03fd3c 100644
--- a/src/codegen/mips64/macro-assembler-mips64.h
+++ b/src/codegen/mips64/macro-assembler-mips64.h
@@ -264,7 +264,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   void TailCallBuiltin(Builtin builtin);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_data_container_object);
+  void LoadCodeInstructionStart(Register destination,
+                                Register code_data_container_object);
   void CallCodeObject(Register code_data_container_object);
   void JumpCodeObject(Register code_data_container_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/ppc/macro-assembler-ppc.cc b/src/codegen/ppc/macro-assembler-ppc.cc
index c60b26f307..7824f8d332 100644
--- a/src/codegen/ppc/macro-assembler-ppc.cc
+++ b/src/codegen/ppc/macro-assembler-ppc.cc
@@ -1178,7 +1178,8 @@ void MacroAssembler::LoadConstantPoolPointerRegisterFromCodeTargetAddress(
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
 
   // TODO(miladfarca): Pass in scratch registers.
-  LoadU64(ip, FieldMemOperand(code_target_address, Code::kCodeEntryPointOffset),
+  LoadU64(ip,
+          FieldMemOperand(code_target_address, Code::kInstructionStartOffset),
           r0);
   LoadU32(r0,
           FieldMemOperand(code_target_address, Code::kInstructionSizeOffset),
@@ -2025,7 +2026,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure, scratch,
                                          r8);
   static_assert(kJavaScriptCallCodeStartRegister == r5, "ABI mismatch");
-  __ LoadCodeEntry(r5, optimized_code_entry);
+  __ LoadCodeInstructionStart(r5, optimized_code_entry);
   __ Jump(r5);
 
   // Optimized code slot contains deoptimized code or code is cleared and
@@ -5085,22 +5086,23 @@ MemOperand MacroAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
                     IsolateData::BuiltinEntrySlotOffset(builtin));
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadU64(destination,
-          FieldMemOperand(code_object, Code::kCodeEntryPointOffset), r0);
+          FieldMemOperand(code_object, Code::kInstructionStartOffset), r0);
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Jump(code_object);
 }
 
diff --git a/src/codegen/ppc/macro-assembler-ppc.h b/src/codegen/ppc/macro-assembler-ppc.h
index c9e28283b2..37671bb5c0 100644
--- a/src/codegen/ppc/macro-assembler-ppc.h
+++ b/src/codegen/ppc/macro-assembler-ppc.h
@@ -745,7 +745,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   MemOperand EntryFromBuiltinAsOperand(Builtin builtin);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index b4bd0d2ecb..e2c4351bd7 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -518,8 +518,8 @@ void RelocInfo::Verify(Isolate* isolate) {
       Address target = target_internal_reference();
       Address pc = target_internal_reference_address();
       Code lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
-      CHECK_GE(target, lookup_result.InstructionStart());
-      CHECK_LT(target, lookup_result.InstructionEnd());
+      CHECK_GE(target, lookup_result.instruction_start());
+      CHECK_LT(target, lookup_result.instruction_end());
       break;
     }
     case OFF_HEAP_TARGET: {
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 3fa69f10e5..c03d8af753 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -124,7 +124,7 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure);
 
   static_assert(kJavaScriptCallCodeStartRegister == a2, "ABI mismatch");
-  __ LoadCodeEntry(a2, optimized_code_entry);
+  __ LoadCodeInstructionStart(a2, optimized_code_entry);
   __ Jump(a2);
 
   // Optimized code slot contains deoptimized code or code is cleared and
@@ -175,7 +175,7 @@ void MacroAssembler::GenerateTailCallToReturnedCode(
 
     CallRuntime(function_id, 1);
     // Use the return value before restoring a0
-    LoadCodeEntry(a2, a0);
+    LoadCodeInstructionStart(a2, a0);
     // Restore target function, new target and actual argument count.
     Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
         kJavaScriptCallArgCountRegister);
@@ -6156,21 +6156,22 @@ void MacroAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                             : Deoptimizer::kEagerDeoptExitSize);
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code) {
   ASM_CODE_COMMENT(this);
-  LoadWord(destination, FieldMemOperand(code, Code::kCodeEntryPointOffset));
+  LoadWord(destination, FieldMemOperand(code, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code, code);
+  LoadCodeInstructionStart(code, code);
   Call(code);
 }
 
 void MacroAssembler::JumpCodeObject(Register code, JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code, code);
+  LoadCodeInstructionStart(code, code);
   Jump(code);
 }
 
diff --git a/src/codegen/riscv/macro-assembler-riscv.h b/src/codegen/riscv/macro-assembler-riscv.h
index 193b10658f..bdad8c2d3c 100644
--- a/src/codegen/riscv/macro-assembler-riscv.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -285,7 +285,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
       RelocInfo::Mode rmode = RelocInfo::INTERNAL_REFERENCE_ENCODED);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/s390/macro-assembler-s390.cc b/src/codegen/s390/macro-assembler-s390.cc
index 7f88a15259..d04fb746d6 100644
--- a/src/codegen/s390/macro-assembler-s390.cc
+++ b/src/codegen/s390/macro-assembler-s390.cc
@@ -2020,7 +2020,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure, scratch,
                                          r7);
   static_assert(kJavaScriptCallCodeStartRegister == r4, "ABI mismatch");
-  __ LoadCodeEntry(r4, optimized_code_entry);
+  __ LoadCodeInstructionStart(r4, optimized_code_entry);
   __ Jump(r4);
 
   // Optimized code slot contains deoptimized code or code is cleared and
@@ -4942,22 +4942,23 @@ MemOperand MacroAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
                     IsolateData::BuiltinEntrySlotOffset(builtin));
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadU64(destination,
-          FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
+          FieldMemOperand(code_object, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
   ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   Jump(code_object);
 }
 
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index 06edec6516..747434b702 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -138,7 +138,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
   MemOperand EntryFromBuiltinAsOperand(Builtin builtin);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index f362456e7f..052fa9280f 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -2260,18 +2260,19 @@ void MacroAssembler::TailCallBuiltin(Builtin builtin, Condition cc) {
   }
 }
 
-void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
+void MacroAssembler::LoadCodeInstructionStart(Register destination,
+                                              Register code_object) {
   ASM_CODE_COMMENT(this);
-  movq(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
+  movq(destination, FieldOperand(code_object, Code::kInstructionStartOffset));
 }
 
 void MacroAssembler::CallCodeObject(Register code_object) {
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   call(code_object);
 }
 
 void MacroAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
-  LoadCodeEntry(code_object, code_object);
+  LoadCodeInstructionStart(code_object, code_object);
   switch (jump_mode) {
     case JumpMode::kJump:
       jmp(code_object);
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 9c84b4956c..b4c49deef7 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -410,7 +410,7 @@ class V8_EXPORT_PRIVATE MacroAssembler
   void TailCallBuiltin(Builtin builtin, Condition cc);
 
   // Load the code entry point from the Code object.
-  void LoadCodeEntry(Register destination, Register code_object);
+  void LoadCodeInstructionStart(Register destination, Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/compiler/backend/code-generator.cc b/src/compiler/backend/code-generator.cc
index 04826d22b1..b8704b7dd2 100644
--- a/src/compiler/backend/code-generator.cc
+++ b/src/compiler/backend/code-generator.cc
@@ -523,9 +523,9 @@ MaybeHandle<Code> CodeGenerator::FinalizeCode() {
     return {};
   }
 
-  LOG_CODE_EVENT(isolate(), CodeLinePosInfoRecordEvent(code->InstructionStart(),
-                                                       *source_positions,
-                                                       JitCodeEvent::JIT_CODE));
+  LOG_CODE_EVENT(isolate(), CodeLinePosInfoRecordEvent(
+                                code->instruction_start(), *source_positions,
+                                JitCodeEvent::JIT_CODE));
 
   return code;
 }
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index 61c9b0c42b..d5742479e7 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -1291,7 +1291,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         DCHECK_IMPLIES(
             instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),
             reg == kJavaScriptCallCodeStartRegister);
-        __ LoadCodeEntry(reg, reg);
+        __ LoadCodeInstructionStart(reg, reg);
         __ call(reg);
       }
       RecordCallPosition(instr);
@@ -1351,7 +1351,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         DCHECK_IMPLIES(
             instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),
             reg == kJavaScriptCallCodeStartRegister);
-        __ LoadCodeEntry(reg, reg);
+        __ LoadCodeInstructionStart(reg, reg);
         __ jmp(reg);
       }
       unwinding_info_writer_.MarkBlockWillExit();
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index a89c299ac6..e5d27fcd9e 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -3062,7 +3062,7 @@ Node* WasmGraphBuilder::BuildCallRef(const wasm::FunctionSig* sig,
         wasm::ObjectAccess::ToTagged(WasmInternalFunction::kCodeOffset));
     Node* call_target = gasm_->LoadFromObject(
         MachineType::Pointer(), wrapper_code,
-        wasm::ObjectAccess::ToTagged(Code::kCodeEntryPointOffset));
+        wasm::ObjectAccess::ToTagged(Code::kInstructionStartOffset));
     gasm_->Goto(&end_label, call_target);
   }
 
diff --git a/src/debug/debug.cc b/src/debug/debug.cc
index e985065c2b..fb7c3f2bea 100644
--- a/src/debug/debug.cc
+++ b/src/debug/debug.cc
@@ -1482,10 +1482,10 @@ class DiscardBaselineCodeVisitor : public ThreadVisitor {
         Address advance;
         if (bytecode_offset == kFunctionEntryBytecodeOffset) {
           advance = BUILTIN_CODE(isolate, BaselineOutOfLinePrologueDeopt)
-                        ->InstructionStart();
+                        ->instruction_start();
         } else {
           advance = BUILTIN_CODE(isolate, InterpreterEnterAtNextBytecode)
-                        ->InstructionStart();
+                        ->instruction_start();
         }
         PointerAuthentication::ReplacePC(pc_addr, advance, kSystemPointerSize);
         InterpretedFrame::cast(it.Reframe())
@@ -1506,7 +1506,7 @@ class DiscardBaselineCodeVisitor : public ThreadVisitor {
                   ? Builtin::kInterpreterEnterAtBytecode
                   : Builtin::kInterpreterEnterAtNextBytecode;
           Address advance_pc =
-              isolate->builtins()->code(advance).InstructionStart();
+              isolate->builtins()->code(advance).instruction_start();
           PointerAuthentication::ReplacePC(pc_addr, advance_pc,
                                            kSystemPointerSize);
         }
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index ee5ee898a0..d9460e655e 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -263,7 +263,7 @@ class ActivationsFinder : public ThreadVisitor {
           // Replace the current pc on the stack with the trampoline.
           // TODO(v8:10026): avoid replacing a signed pointer.
           Address* pc_addr = it.frame()->pc_address();
-          Address new_pc = code.InstructionStart() + trampoline_pc;
+          Address new_pc = code.instruction_start() + trampoline_pc;
           PointerAuthentication::ReplacePC(pc_addr, new_pc, kSystemPointerSize);
         }
       }
@@ -474,7 +474,7 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(compiled_code_.deoptimization_data());
   Address deopt_start =
-      compiled_code_.InstructionStart() + deopt_data.DeoptExitStart().value();
+      compiled_code_.instruction_start() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * kEagerDeoptExitSize;
@@ -1116,7 +1116,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
   CHECK_EQ(0u, frame_writer.top_offset());
 
   const intptr_t pc =
-      static_cast<intptr_t>(dispatch_builtin.InstructionStart());
+      static_cast<intptr_t>(dispatch_builtin.instruction_start());
   if (is_topmost) {
     // Only the pc of the topmost frame needs to be signed since it is
     // authenticated at the end of the DeoptimizationEntry builtin.
@@ -1149,7 +1149,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
     // Set the continuation for the topmost frame.
     Code continuation = builtins->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
-        static_cast<intptr_t>(continuation.InstructionStart()));
+        static_cast<intptr_t>(continuation.instruction_start()));
   }
 }
 
@@ -1342,7 +1342,7 @@ void Deoptimizer::DoComputeConstructStubFrame(TranslatedFrame* translated_frame,
 
   // Compute this frame's PC.
   DCHECK(bytecode_offset.IsValidForConstructStub());
-  Address start = construct_stub.InstructionStart();
+  Address start = construct_stub.instruction_start();
   const int pc_offset =
       bytecode_offset == BytecodeOffset::ConstructStubCreate()
           ? isolate_->heap()->construct_stub_create_deopt_pc_offset().value()
@@ -1383,7 +1383,7 @@ void Deoptimizer::DoComputeConstructStubFrame(TranslatedFrame* translated_frame,
     DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
     Code continuation = builtins->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
-        static_cast<intptr_t>(continuation.InstructionStart()));
+        static_cast<intptr_t>(continuation.instruction_start()));
   }
 }
 
@@ -1814,17 +1814,17 @@ void Deoptimizer::DoComputeBuiltinContinuation(
     // authenticated at the end of the DeoptimizationEntry builtin.
     const intptr_t top_most_pc = PointerAuthentication::SignAndCheckPC(
         isolate(),
-        static_cast<intptr_t>(continue_to_builtin.InstructionStart()),
+        static_cast<intptr_t>(continue_to_builtin.instruction_start()),
         frame_writer.frame()->GetTop());
     output_frame->SetPc(top_most_pc);
   } else {
     output_frame->SetPc(
-        static_cast<intptr_t>(continue_to_builtin.InstructionStart()));
+        static_cast<intptr_t>(continue_to_builtin.instruction_start()));
   }
 
   Code continuation = isolate()->builtins()->code(Builtin::kNotifyDeoptimized);
   output_frame->SetContinuation(
-      static_cast<intptr_t>(continuation.InstructionStart()));
+      static_cast<intptr_t>(continuation.instruction_start()));
 }
 
 void Deoptimizer::MaterializeHeapObjects() {
@@ -1897,7 +1897,7 @@ Address GetDeoptCallPCFromReturnPC(Address return_pc, Code code) {
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(code.deoptimization_data());
   Address deopt_start =
-      code.InstructionStart() + deopt_data.DeoptExitStart().value();
+      code.instruction_start() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * Deoptimizer::kEagerDeoptExitSize;
@@ -1957,7 +1957,7 @@ unsigned Deoptimizer::ComputeIncomingArgumentSize(SharedFunctionInfo shared) {
 }
 
 Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(Code code, Address pc) {
-  CHECK(code.InstructionStart() <= pc && pc <= code.InstructionEnd());
+  CHECK(code.instruction_start() <= pc && pc <= code.instruction_end());
   SourcePosition last_position = SourcePosition::Unknown();
   DeoptimizeReason last_reason = DeoptimizeReason::kUnknown;
   uint32_t last_node_id = 0;
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index fc5335431f..7c8f1dae72 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -1115,23 +1115,23 @@ void Code::CodeVerify(Isolate* isolate) {
     // Ensure the cached code entry point corresponds to the InstructionStream
     // object associated with this Code.
 #if defined(V8_COMPRESS_POINTERS) && defined(V8_SHORT_BUILTIN_CALLS)
-    if (istream.instruction_start() == code_entry_point()) {
+    if (istream.instruction_start() == instruction_start()) {
       // Most common case, all good.
     } else {
       // When shared pointer compression cage is enabled and it has the
       // embedded code blob copy then the
       // InstructionStream::instruction_start() might return the address of
       // the remapped builtin regardless of whether the builtins copy existed
-      // when the code_entry_point value was cached in the Code (see
+      // when the instruction_start value was cached in the Code (see
       // InstructionStream::OffHeapInstructionStart()).  So, do a reverse
-      // Code object lookup via code_entry_point value to ensure it
+      // Code object lookup via instruction_start value to ensure it
       // corresponds to this current Code object.
       Code lookup_result =
-          isolate->heap()->FindCodeForInnerPointer(code_entry_point());
+          isolate->heap()->FindCodeForInnerPointer(instruction_start());
       CHECK_EQ(lookup_result, *this);
     }
 #else
-    CHECK_EQ(istream.instruction_start(), code_entry_point());
+    CHECK_EQ(istream.instruction_start(), instruction_start());
 #endif  // V8_COMPRESS_POINTERS && V8_SHORT_BUILTIN_CALLS
   }
 }
@@ -1149,7 +1149,7 @@ void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
                 IsAligned(instruction_start(), kCodeAlignment));
   CHECK_EQ(*this, code(kAcquireLoad).instruction_stream());
   CHECK(V8_ENABLE_THIRD_PARTY_HEAP_BOOL ||
-        CodeSize() <= MemoryChunkLayout::MaxRegularCodeObjectSize() ||
+        Size() <= MemoryChunkLayout::MaxRegularCodeObjectSize() ||
         isolate->heap()->InSpace(*this, CODE_LO_SPACE));
   Address last_gc_pc = kNullAddress;
 
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index 30d8095f62..9aa577007c 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -1893,8 +1893,8 @@ void Code::CodePrint(std::ostream& os) {
   if (has_instruction_stream()) {
     os << "\n - instruction_stream: " << Brief(raw_instruction_stream());
   }
-  os << "\n - code_entry_point: "
-     << reinterpret_cast<void*>(code_entry_point());
+  os << "\n - instruction_start: "
+     << reinterpret_cast<void*>(instruction_start());
   os << "\n - kind_specific_flags: " << kind_specific_flags(kRelaxedLoad);
   os << "\n";
   if (has_instruction_stream()) {
diff --git a/src/diagnostics/perf-jit.cc b/src/diagnostics/perf-jit.cc
index b3758680d1..24f1ae53c0 100644
--- a/src/diagnostics/perf-jit.cc
+++ b/src/diagnostics/perf-jit.cc
@@ -249,12 +249,12 @@ void LinuxPerfJitLogger::LogRecordedBuffer(
   }
 
   const char* code_name = name;
-  uint8_t* code_pointer = reinterpret_cast<uint8_t*>(code.InstructionStart());
+  uint8_t* code_pointer = reinterpret_cast<uint8_t*>(code.instruction_start());
 
   // Unwinding info comes right after debug info.
   if (v8_flags.perf_prof_unwinding_info) LogWriteUnwindingInfo(code);
 
-  WriteJitCodeLoadEntry(code_pointer, code.InstructionSize(), code_name,
+  WriteJitCodeLoadEntry(code_pointer, code.instruction_size(), code_name,
                         length);
 }
 
@@ -379,7 +379,7 @@ void LinuxPerfJitLogger::LogWriteDebugInfo(Code code,
 
   debug_info.event_ = PerfJitCodeLoad::kDebugInfo;
   debug_info.time_stamp_ = GetTimestamp();
-  debug_info.address_ = code.InstructionStart();
+  debug_info.address_ = code.instruction_start();
   debug_info.entry_count_ = entry_count;
 
   // Add the sizes of fixed parts of entries.
@@ -389,7 +389,7 @@ void LinuxPerfJitLogger::LogWriteDebugInfo(Code code,
   debug_info.size_ = size + padding;
   LogWriteBytes(reinterpret_cast<const char*>(&debug_info), sizeof(debug_info));
 
-  Address code_start = code.InstructionStart();
+  Address code_start = code.instruction_start();
 
   last_script = Smi::zero();
   int script_names_index = 0;
diff --git a/src/execution/execution.cc b/src/execution/execution.cc
index 00da2d38a7..36f584eedd 100644
--- a/src/execution/execution.cc
+++ b/src/execution/execution.cc
@@ -417,7 +417,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> Invoke(Isolate* isolate,
           Address receiver, intptr_t argc, Address** argv)>;
       // clang-format on
       JSEntryFunction stub_entry =
-          JSEntryFunction::FromAddress(isolate, code->InstructionStart());
+          JSEntryFunction::FromAddress(isolate, code->instruction_start());
 
       Address orig_func = params.new_target->ptr();
       Address func = params.target->ptr();
@@ -437,7 +437,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> Invoke(Isolate* isolate,
           Address root_register_value, MicrotaskQueue* microtask_queue)>;
       // clang-format on
       JSEntryFunction stub_entry =
-          JSEntryFunction::FromAddress(isolate, code->InstructionStart());
+          JSEntryFunction::FromAddress(isolate, code->instruction_start());
 
       RCS_SCOPE(isolate, RuntimeCallCounterId::kJS_Execution);
       value = Object(stub_entry.Call(isolate->isolate_data()->isolate_root(),
@@ -619,7 +619,7 @@ void Execution::CallWasm(Isolate* isolate, Handle<Code> wrapper_code,
   using WasmEntryStub = GeneratedCode<Address(
       Address target, Address object_ref, Address argv, Address c_entry_fp)>;
   WasmEntryStub stub_entry =
-      WasmEntryStub::FromAddress(isolate, wrapper_code->InstructionStart());
+      WasmEntryStub::FromAddress(isolate, wrapper_code->instruction_start());
 
   // Save and restore context around invocation and block the
   // allocation of handles without explicit handle scopes.
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index 2803b901d6..a9fcaa5921 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -587,7 +587,7 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
 
   // Keep the old pc offset before visiting the code since we need it to
   // calculate the new pc after a potential InstructionStream move.
-  const uintptr_t pc_offset_from_start = old_pc - holder.InstructionStart();
+  const uintptr_t pc_offset_from_start = old_pc - holder.instruction_start();
 
   // Visit.
   GcSafeCode visited_holder = holder;
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 94803ec59c..828ef41889 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -449,9 +449,9 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     // code cage base and code entry point. Other data fields must remain the
     // same.
     static_assert(Code::kCodePointerFieldsStrongEndOffset ==
-                  Code::kCodeEntryPointOffset);
+                  Code::kInstructionStartOffset);
 
-    static_assert(Code::kCodeEntryPointOffsetEnd + 1 == Code::kFlagsOffset);
+    static_assert(Code::kInstructionStartOffsetEnd + 1 == Code::kFlagsOffset);
     static_assert(Code::kFlagsOffsetEnd + 1 == Code::kBuiltinIdOffset);
     static_assert(Code::kBuiltinIdOffsetEnd + 1 ==
                   Code::kKindSpecificFlagsOffset);
@@ -473,7 +473,7 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
                   Code::kCodeCommentsOffsetOffset);
     static_assert(Code::kCodeCommentsOffsetOffsetEnd + 1 ==
                   Code::kUnalignedSize);
-    constexpr int kStartOffset = Code::kFlagsOffset;
+    static constexpr int kStartOffset = Code::kFlagsOffset;
 
     for (int j = kStartOffset; j < Code::kUnalignedSize; j++) {
       hash = base::hash_combine(hash, size_t{code_ptr[j]});
@@ -2011,7 +2011,7 @@ Object Isolate::UnwindAndFindHandler() {
         // Jump directly to the optimized frames return, to immediately fall
         // into the deoptimizer.
         const int offset =
-            static_cast<int>(frame->pc() - code.InstructionStart());
+            static_cast<int>(frame->pc() - code.instruction_start());
 
         // Compute the stack pointer from the frame pointer. This ensures that
         // argument slots on the stack are dropped as returning would.
@@ -2019,7 +2019,7 @@ Object Isolate::UnwindAndFindHandler() {
         Address return_sp = frame->fp() +
                             StandardFrameConstants::kFixedFrameSizeAboveFp -
                             code.stack_slots() * kSystemPointerSize;
-        return FoundHandler(Context(), code.InstructionStart(), offset,
+        return FoundHandler(Context(), code.instruction_start(), offset,
                             code.constant_pool(), return_sp, frame->fp(),
                             visited_frames);
       }
@@ -2027,7 +2027,7 @@ Object Isolate::UnwindAndFindHandler() {
 
       debug()->clear_restart_frame();
       Code code = *BUILTIN_CODE(this, RestartFrameTrampoline);
-      return FoundHandler(Context(), code.InstructionStart(), 0,
+      return FoundHandler(Context(), code.instruction_start(), 0,
                           code.constant_pool(), kNullAddress, frame->fp(),
                           visited_frames);
     }
@@ -2056,7 +2056,7 @@ Object Isolate::UnwindAndFindHandler() {
         thread_local_top()->handler_ = handler->next_address();
         Code code = frame->LookupCode();
         HandlerTable table(code);
-        Address instruction_start = code.InstructionStart();
+        Address instruction_start = code.instruction_start();
         int return_offset = static_cast<int>(frame->pc() - instruction_start);
         int handler_offset = table.LookupReturn(return_offset);
         DCHECK_NE(-1, handler_offset);
@@ -2128,7 +2128,7 @@ Object Isolate::UnwindAndFindHandler() {
           // If the target code is lazy deoptimized, we jump to the original
           // return address, but we make a note that we are throwing, so
           // that the deoptimizer can do the right thing.
-          offset = static_cast<int>(frame->pc() - code.InstructionStart());
+          offset = static_cast<int>(frame->pc() - code.instruction_start());
           set_deoptimizer_lazy_throw(true);
         }
 
@@ -2203,7 +2203,7 @@ Object Isolate::UnwindAndFindHandler() {
           // Patch the context register directly on the frame, so that we don't
           // need to have a context read + write in the baseline code.
           sp_frame->PatchContext(context);
-          return FoundHandler(Context(), code.InstructionStart(), pc_offset,
+          return FoundHandler(Context(), code.instruction_start(), pc_offset,
                               code.constant_pool(), return_sp, sp_frame->fp(),
                               visited_frames);
         } else {
@@ -2219,7 +2219,7 @@ Object Isolate::UnwindAndFindHandler() {
           // because at a minimum, an exit frame into C++ has to separate
           // it and the context in which this C++ code runs.
           CHECK_GE(visited_frames, 1);
-          return FoundHandler(context, code.InstructionStart(), 0,
+          return FoundHandler(context, code.instruction_start(), 0,
                               code.constant_pool(), return_sp, frame->fp(),
                               visited_frames - 1);
         }
@@ -2243,7 +2243,7 @@ Object Isolate::UnwindAndFindHandler() {
         // Reconstruct the stack pointer from the frame pointer.
         Address return_sp = js_frame->fp() - js_frame->GetSPToFPDelta();
         Code code = js_frame->LookupCode();
-        return FoundHandler(Context(), code.InstructionStart(), 0,
+        return FoundHandler(Context(), code.instruction_start(), 0,
                             code.constant_pool(), return_sp, frame->fp(),
                             visited_frames);
       }
diff --git a/src/execution/simulator.h b/src/execution/simulator.h
index c685609e3d..84708083cd 100644
--- a/src/execution/simulator.h
+++ b/src/execution/simulator.h
@@ -111,7 +111,7 @@ class GeneratedCode {
   }
 
   static GeneratedCode FromCode(Isolate* isolate, Code code) {
-    return FromAddress(isolate, code.InstructionStart());
+    return FromAddress(isolate, code.instruction_start());
   }
 
 #ifdef USE_SIMULATOR
diff --git a/src/heap/code-stats.cc b/src/heap/code-stats.cc
index 66d1791c26..86e5e62adf 100644
--- a/src/heap/code-stats.cc
+++ b/src/heap/code-stats.cc
@@ -222,8 +222,8 @@ void CodeStatistics::CollectCodeCommentStatistics(AbstractCode obj,
     cit.Next();
   }
 
-  DCHECK(0 <= prev_pc_offset && prev_pc_offset <= code.InstructionSize());
-  delta += static_cast<int>(code.InstructionSize() - prev_pc_offset);
+  DCHECK(0 <= prev_pc_offset && prev_pc_offset <= code.instruction_size());
+  delta += static_cast<int>(code.instruction_size() - prev_pc_offset);
   EnterComment(isolate, "NoComment", delta);
 }
 #endif
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index 13b712bf94..2889ffa1bc 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -86,7 +86,7 @@ Handle<Code> FactoryBase<Impl>::NewCode(const NewCodeOptions& options) {
   code.set_kind_specific_flags(options.kind_specific_flags, kRelaxedStore);
   Isolate* isolate_for_sandbox = impl()->isolate_for_sandbox();
   code.set_raw_instruction_stream(Smi::zero(), SKIP_WRITE_BARRIER);
-  code.init_code_entry_point(isolate_for_sandbox, kNullAddress);
+  code.init_instruction_start(isolate_for_sandbox, kNullAddress);
   code.set_instruction_size(options.instruction_size);
   code.set_metadata_size(options.metadata_size);
   code.set_relocation_info(*options.reloc_info);
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 8fb5c02e54..334340c740 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -105,17 +105,31 @@ Factory::CodeBuilder::CodeBuilder(LocalIsolate* local_isolate,
       kind_(kind),
       position_table_(isolate_->factory()->empty_byte_array()) {}
 
+Handle<ByteArray> Factory::CodeBuilder::NewByteArray(
+    int length, AllocationType allocation) {
+  return V8_UNLIKELY(CompiledWithConcurrentBaseline())
+             ? local_isolate_->factory()->NewByteArray(length, allocation)
+             : isolate_->factory()->NewByteArray(length, allocation);
+}
+
+MaybeHandle<InstructionStream> Factory::CodeBuilder::NewInstructionStream(
+    bool retry_allocation_or_fail) {
+  return V8_UNLIKELY(CompiledWithConcurrentBaseline())
+             ? AllocateConcurrentSparkplugInstructionStream(
+                   retry_allocation_or_fail)
+             : AllocateInstructionStream(retry_allocation_or_fail);
+}
+
+Handle<Code> Factory::CodeBuilder::NewCode(const NewCodeOptions& options) {
+  return V8_UNLIKELY(CompiledWithConcurrentBaseline())
+             ? local_isolate_->factory()->NewCode(options)
+             : isolate_->factory()->NewCode(options);
+}
+
 MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     bool retry_allocation_or_fail) {
-  const auto factory = isolate_->factory();
-  // Allocate objects needed for code initialization.
   Handle<ByteArray> reloc_info =
-      CompiledWithConcurrentBaseline()
-          ? local_isolate_->factory()->NewByteArray(code_desc_.reloc_size,
-                                                    AllocationType::kOld)
-          : factory->NewByteArray(code_desc_.reloc_size, AllocationType::kOld);
-
-  Handle<Code> code;
+      NewByteArray(code_desc_.reloc_size, AllocationType::kOld);
 
   NewCodeOptions new_code_options = {
       /*kind=*/kind_,
@@ -137,18 +151,13 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
           ? interpreter_data_
           : deoptimization_data_,
       /*bytecode_offsets_or_source_position_table=*/position_table_};
-
-  if (CompiledWithConcurrentBaseline()) {
-    code = local_isolate_->factory()->NewCode(new_code_options);
-  } else {
-    code = factory->NewCode(new_code_options);
-  }
+  Handle<Code> code = NewCode(new_code_options);
 
   // Basic block profiling data for builtins is stored in the JS heap rather
   // than in separately-allocated C++ objects. Allocate that data now if
   // appropriate.
   Handle<OnHeapBasicBlockProfilerData> on_heap_profiler_data;
-  if (profiler_data_ && isolate_->IsGeneratingEmbeddedBuiltins()) {
+  if (V8_UNLIKELY(profiler_data_ && isolate_->IsGeneratingEmbeddedBuiltins())) {
     on_heap_profiler_data = profiler_data_->CopyToJSHeap(isolate_);
 
     // Add the on-heap data to a global list, which keeps it alive and allows
@@ -160,84 +169,78 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     isolate_->heap()->SetBasicBlockProfilingData(new_list);
   }
 
-  static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-  Heap* heap = isolate_->heap();
-  CodePageCollectionMemoryModificationScope code_allocation(heap);
+  {
+    static_assert(InstructionStream::kOnHeapBodyIsContiguous);
+    CodePageCollectionMemoryModificationScope code_allocation(isolate_->heap());
 
-  Handle<InstructionStream> instruction_stream;
-  if (CompiledWithConcurrentBaseline()) {
-    if (!AllocateConcurrentSparkplugInstructionStream(retry_allocation_or_fail)
-             .ToHandle(&instruction_stream)) {
+    Handle<InstructionStream> istream;
+    if (!NewInstructionStream(retry_allocation_or_fail).ToHandle(&istream)) {
       return {};
     }
-  } else if (!AllocateInstructionStream(retry_allocation_or_fail)
-                  .ToHandle(&instruction_stream)) {
-    return {};
-  }
 
-  {
-    InstructionStream raw_istream = *instruction_stream;
-    DisallowGarbageCollection no_gc;
+    {
+      DisallowGarbageCollection no_gc;
+      InstructionStream raw_istream = *istream;
 
-    // This might impact direct concurrent reads from TF if we are resetting
-    // this field. We currently assume it's immutable thus a relaxed read (after
-    // passing IsPendingAllocation).
-    raw_istream.set_code(*code, kReleaseStore);
-
-    // Allow self references to created code object by patching the handle to
-    // point to the newly allocated InstructionStream object.
-    Handle<Object> self_reference;
-    if (self_reference_.ToHandle(&self_reference)) {
-      DCHECK(self_reference->IsOddball());
-      DCHECK_EQ(Oddball::cast(*self_reference).kind(),
-                Oddball::kSelfReferenceMarker);
-      DCHECK_NE(kind_, CodeKind::BASELINE);
-      if (isolate_->IsGeneratingEmbeddedBuiltins()) {
-        isolate_->builtins_constants_table_builder()->PatchSelfReference(
-            self_reference, instruction_stream);
+      if (V8_EXTERNAL_CODE_SPACE_BOOL) {
+        raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
+      }
+      raw_istream.set_body_size(new_code_options.instruction_size +
+                                new_code_options.metadata_size);
+      DCHECK_EQ(raw_istream.body_size(), code->body_size());
+      raw_istream.set_code(*code, kReleaseStore);
+      raw_istream.clear_padding();
+
+      // Allow self references to created code object by patching the handle to
+      // point to the newly allocated InstructionStream object.
+      Handle<Object> self_reference;
+      if (self_reference_.ToHandle(&self_reference)) {
+        DCHECK(self_reference->IsOddball());
+        DCHECK_EQ(Oddball::cast(*self_reference).kind(),
+                  Oddball::kSelfReferenceMarker);
+        DCHECK_NE(kind_, CodeKind::BASELINE);
+        if (isolate_->IsGeneratingEmbeddedBuiltins()) {
+          isolate_->builtins_constants_table_builder()->PatchSelfReference(
+              self_reference, istream);
+        }
+        self_reference.PatchValue(raw_istream);
       }
-      self_reference.PatchValue(*instruction_stream);
-    }
-
-    // Likewise, any references to the basic block counters marker need to be
-    // updated to point to the newly-allocated counters array.
-    if (!on_heap_profiler_data.is_null()) {
-      isolate_->builtins_constants_table_builder()
-          ->PatchBasicBlockCountersReference(
-              handle(on_heap_profiler_data->counts(), isolate_));
-    }
 
-    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
-    }
-    code->SetInstructionStreamAndEntryPoint(isolate_, raw_istream);
+      // Likewise, any references to the basic block counters marker need to be
+      // updated to point to the newly-allocated counters array.
+      if (V8_UNLIKELY(!on_heap_profiler_data.is_null())) {
+        isolate_->builtins_constants_table_builder()
+            ->PatchBasicBlockCountersReference(
+                handle(on_heap_profiler_data->counts(), isolate_));
+      }
 
-    // Migrate generated code.
-    // The generated code can contain embedded objects (typically from
-    // handles) in a pointer-to-tagged-value format (i.e. with indirection
-    // like a handle) that are dereferenced during the copy to point directly
-    // to the actual heap objects. These pointers can include references to
-    // the code object itself, through the self_reference parameter.
-    code->CopyFromNoFlush(*reloc_info, heap, code_desc_);
+      code->SetInstructionStreamAndInstructionStart(isolate_, raw_istream);
 
-    code->ClearInstructionStreamPadding();
+      // Migrate generated code.
+      // The generated code can contain embedded objects (typically from
+      // handles) in a pointer-to-tagged-value format (i.e. with indirection
+      // like a handle) that are dereferenced during the copy to point directly
+      // to the actual heap objects. These pointers can include references to
+      // the code object itself, through the self_reference parameter.
+      code->CopyFromNoFlush(*reloc_info, isolate_->heap(), code_desc_);
 
 #ifdef VERIFY_HEAP
-    if (v8_flags.verify_heap) {
-      HeapObject::VerifyCodePointer(isolate_, raw_istream);
-    }
+      if (v8_flags.verify_heap) {
+        HeapObject::VerifyCodePointer(isolate_, raw_istream);
+      }
 #endif
 
-    // Flush the instruction cache before changing the permissions.
-    // Note: we do this before setting permissions to ReadExecute because on
-    // some older ARM kernels there is a bug which causes an access error on
-    // cache flush instructions to trigger access error on non-writable memory.
-    // See https://bugs.chromium.org/p/v8/issues/detail?id=8157
-    code->FlushICache();
+      // Flush the instruction cache before changing the permissions.
+      // Note: we do this before setting permissions to ReadExecute because on
+      // some older ARM kernels there is a bug which causes an access error on
+      // cache flush instructions to trigger access error on non-writable
+      // memory. See https://bugs.chromium.org/p/v8/issues/detail?id=8157
+      code->FlushICache();
+    }
   }
 
-  if (V8_UNLIKELY(profiler_data_ && v8_flags.turbo_profiling_verbose)) {
 #ifdef ENABLE_DISASSEMBLER
+  if (V8_UNLIKELY(profiler_data_ && v8_flags.turbo_profiling_verbose)) {
     std::ostringstream os;
     code->Disassemble(nullptr, os, isolate_);
     if (!on_heap_profiler_data.is_null()) {
@@ -248,8 +251,8 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     } else {
       profiler_data_->SetCode(os);
     }
-#endif  // ENABLE_DISASSEMBLER
   }
+#endif  // ENABLE_DISASSEMBLER
 
   return code;
 }
@@ -2544,8 +2547,8 @@ Handle<Code> Factory::NewOffHeapTrampolineFor(Handle<Code> code,
       Handle<ByteArray>(read_only_roots().empty_byte_array(), isolate())};
 
   Handle<Code> off_heap_trampoline = NewCode(new_code_options);
-  off_heap_trampoline->set_code_entry_point(isolate(),
-                                            code->code_entry_point());
+  off_heap_trampoline->set_instruction_start(isolate(),
+                                             code->instruction_start());
 
   DCHECK_EQ(code->inlined_bytecode_size(), 0);
   DCHECK_EQ(code->osr_offset(), BytecodeOffset::None());
@@ -3252,8 +3255,8 @@ MaybeHandle<JSBoundFunction> Factory::NewJSBoundFunction(
     Handle<JSReceiver> target_function, Handle<Object> bound_this,
     base::Vector<Handle<Object>> bound_args) {
   DCHECK(target_function->IsCallable());
-  static_assert(InstructionStream::kMaxArguments <= FixedArray::kMaxLength);
-  if (bound_args.length() >= InstructionStream::kMaxArguments) {
+  static_assert(Code::kMaxArguments <= FixedArray::kMaxLength);
+  if (bound_args.length() >= Code::kMaxArguments) {
     THROW_NEW_ERROR(isolate(),
                     NewRangeError(MessageTemplate::kTooManyArguments),
                     JSBoundFunction);
diff --git a/src/heap/factory.h b/src/heap/factory.h
index 3890807e1a..86c1a2d780 100644
--- a/src/heap/factory.h
+++ b/src/heap/factory.h
@@ -1021,14 +1021,19 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
       return *this;
     }
 
-    inline bool CompiledWithConcurrentBaseline() const;
-
    private:
     MaybeHandle<Code> BuildInternal(bool retry_allocation_or_fail);
+
+    // Dispatches to support concurrent allocation.
+    inline bool CompiledWithConcurrentBaseline() const;
+    Handle<ByteArray> NewByteArray(int length, AllocationType allocation);
+    MaybeHandle<InstructionStream> NewInstructionStream(
+        bool retry_allocation_or_fail);
     MaybeHandle<InstructionStream> AllocateInstructionStream(
         bool retry_allocation_or_fail);
     MaybeHandle<InstructionStream> AllocateConcurrentSparkplugInstructionStream(
         bool retry_allocation_or_fail);
+    Handle<Code> NewCode(const NewCodeOptions& options);
 
     Isolate* const isolate_;
     LocalIsolate* local_isolate_;
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index ca636f4c48..42e5342601 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -3142,9 +3142,9 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
     if (v8_flags.flush_baseline_code && flushing_candidate.HasBaselineCode()) {
       baseline_code =
           Code::cast(flushing_candidate.function_data(kAcquireLoad));
-      // Safe to do a relaxed load here since the Code was
-      // acquire-loaded.
-      baseline_istream = FromCode(baseline_code, isolate(), kRelaxedLoad);
+      // Safe to do a relaxed load here since the Code was acquire-loaded.
+      baseline_istream = baseline_code.instruction_stream(
+          baseline_code.code_cage_base(isolate()), kRelaxedLoad);
       baseline_bytecode_or_interpreter_data =
           baseline_code.bytecode_or_interpreter_data();
     }
@@ -3826,7 +3826,7 @@ static inline void UpdateStrongCodeSlot(HeapObject host,
     InstructionStream instruction_stream =
         code.instruction_stream(code_cage_base);
     Isolate* isolate_for_sandbox = GetIsolateForSandbox(host);
-    code.UpdateCodeEntryPoint(isolate_for_sandbox, instruction_stream);
+    code.UpdateInstructionStart(isolate_for_sandbox, instruction_stream);
   }
 }
 
diff --git a/src/heap/remembered-set.h b/src/heap/remembered-set.h
index 1a2ebb0981..c31ef1967d 100644
--- a/src/heap/remembered-set.h
+++ b/src/heap/remembered-set.h
@@ -328,7 +328,7 @@ class UpdateTypedSlotHelper {
     SlotCallbackResult result = callback(FullMaybeObjectSlot(&code));
     DCHECK(!HasWeakHeapObjectTag(code));
     if (code != old_code) {
-      base::Memory<Address>(entry_address) = code.entry();
+      base::Memory<Address>(entry_address) = code.instruction_start();
     }
     return result;
   }
diff --git a/src/ic/ic.cc b/src/ic/ic.cc
index 9b1a82eab3..a0c5672c71 100644
--- a/src/ic/ic.cc
+++ b/src/ic/ic.cc
@@ -159,7 +159,7 @@ void IC::TraceIC(const char* type, Handle<Object> name, State old_state,
     code_offset = baseline_frame->GetBytecodeOffset();
     code = AbstractCode::cast(baseline_frame->GetBytecodeArray());
   } else {
-    code_offset = static_cast<int>(frame->pc() - function.code_entry_point());
+    code_offset = static_cast<int>(frame->pc() - function.instruction_start());
   }
   JavaScriptFrame::CollectFunctionAndOffsetForICStats(function, code,
                                                       code_offset);
diff --git a/src/interpreter/interpreter.cc b/src/interpreter/interpreter.cc
index a39483f768..fe89983f54 100644
--- a/src/interpreter/interpreter.cc
+++ b/src/interpreter/interpreter.cc
@@ -122,7 +122,7 @@ void Interpreter::SetBytecodeHandler(Bytecode bytecode,
   DCHECK(!handler.has_instruction_stream());
   DCHECK(handler.kind() == CodeKind::BYTECODE_HANDLER);
   size_t index = GetDispatchTableIndex(bytecode, operand_scale);
-  dispatch_table_[index] = handler.InstructionStart();
+  dispatch_table_[index] = handler.instruction_start();
 }
 
 // static
@@ -343,7 +343,7 @@ void Interpreter::Initialize() {
   Handle<Code> code = BUILTIN_CODE(isolate_, InterpreterEntryTrampoline);
   DCHECK(builtins->is_initialized());
   DCHECK(!code->has_instruction_stream());
-  interpreter_entry_trampoline_instruction_start_ = code->InstructionStart();
+  interpreter_entry_trampoline_instruction_start_ = code->instruction_start();
 
   // Initialize the dispatch table.
   ForEachBytecode([=](Bytecode bytecode, OperandScale operand_scale) {
diff --git a/src/logging/log.cc b/src/logging/log.cc
index ca97693465..6f368436a3 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -915,7 +915,7 @@ void JitLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
   event.type = JitCodeEvent::CODE_MOVED;
   event.code_type = JitCodeEvent::JIT_CODE;
   event.code_start = reinterpret_cast<void*>(from.instruction_start());
-  event.code_len = from.unchecked_code().instruction_size();
+  event.code_len = from.unchecked_code(kAcquireLoad).instruction_size();
   event.new_code_start = reinterpret_cast<void*>(to.instruction_start());
   event.isolate = reinterpret_cast<v8::Isolate*>(isolate_);
 
@@ -1627,8 +1627,9 @@ void V8FileLogger::CodeDisableOptEvent(Handle<AbstractCode> code,
 void V8FileLogger::ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
                                      const char* kind, const char* reason) {
   MSG_BUILDER();
-  msg << Event::kCodeDeopt << kNext << Time() << kNext << code->CodeSize()
-      << kNext << reinterpret_cast<void*>(code->InstructionStart());
+  msg << Event::kCodeDeopt << kNext << Time() << kNext
+      << code->InstructionStreamObjectSize() << kNext
+      << reinterpret_cast<void*>(code->instruction_start());
 
   std::ostringstream deopt_location;
   int inlining_id = -1;
diff --git a/src/maglev/maglev-ir.h b/src/maglev/maglev-ir.h
index 2e153bb188..8abc30b9e0 100644
--- a/src/maglev/maglev-ir.h
+++ b/src/maglev/maglev-ir.h
@@ -6178,8 +6178,7 @@ class Call : public ValueNodeT<Call> {
 
   // We need enough inputs to have these fixed inputs plus the maximum arguments
   // to a function call.
-  static_assert(kMaxInputs >=
-                kFixedInputCount + InstructionStream::kMaxArguments);
+  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
@@ -6235,8 +6234,7 @@ class Construct : public ValueNodeT<Construct> {
 
   // We need enough inputs to have these fixed inputs plus the maximum arguments
   // to a function call.
-  static_assert(kMaxInputs >=
-                kFixedInputCount + InstructionStream::kMaxArguments);
+  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
@@ -6518,8 +6516,7 @@ class CallSelf : public ValueNodeT<CallSelf> {
 
   // We need enough inputs to have these fixed inputs plus the maximum arguments
   // to a function call.
-  static_assert(kMaxInputs >=
-                kFixedInputCount + InstructionStream::kMaxArguments);
+  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
@@ -6574,8 +6571,7 @@ class CallKnownJSFunction : public ValueNodeT<CallKnownJSFunction> {
 
   // We need enough inputs to have these fixed inputs plus the maximum arguments
   // to a function call.
-  static_assert(kMaxInputs >=
-                kFixedInputCount + InstructionStream::kMaxArguments);
+  static_assert(kMaxInputs >= kFixedInputCount + Code::kMaxArguments);
 
   // This ctor is used when for variable input counts.
   // Inputs must be initialized manually.
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 253cec901c..77390eb10b 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -53,8 +53,8 @@ Code GcSafeCode::UnsafeCastToCode() const {
 
 #define GCSAFE_CODE_FWD_ACCESSOR(ReturnType, Name) \
   ReturnType GcSafeCode::Name() const { return UnsafeCastToCode().Name(); }
-GCSAFE_CODE_FWD_ACCESSOR(Address, InstructionStart)
-GCSAFE_CODE_FWD_ACCESSOR(Address, InstructionEnd)
+GCSAFE_CODE_FWD_ACCESSOR(Address, instruction_start)
+GCSAFE_CODE_FWD_ACCESSOR(Address, instruction_end)
 GCSAFE_CODE_FWD_ACCESSOR(bool, is_builtin)
 GCSAFE_CODE_FWD_ACCESSOR(Builtin, builtin_id)
 GCSAFE_CODE_FWD_ACCESSOR(CodeKind, kind)
@@ -92,7 +92,7 @@ Address GcSafeCode::constant_pool(InstructionStream istream) const {
 bool GcSafeCode::CanDeoptAt(Isolate* isolate, Address pc) const {
   DeoptimizationData deopt_data = DeoptimizationData::unchecked_cast(
       UnsafeCastToCode().unchecked_deoptimization_data());
-  Address code_start_address = InstructionStart();
+  Address code_start_address = instruction_start();
   for (int i = 0; i < deopt_data.DeoptCount(); i++) {
     if (deopt_data.Pc(i).value() == -1) continue;
     Address address = code_start_address + deopt_data.Pc(i).value();
@@ -112,7 +112,7 @@ Object GcSafeCode::raw_instruction_stream(
 int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
   Map map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    return GetCode().InstructionSize();
+    return GetCode().instruction_size();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
     return GetBytecodeArray().length();
@@ -158,7 +158,7 @@ int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
 Address AbstractCode::InstructionStart(PtrComprCageBase cage_base) {
   Map map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    return GetCode().InstructionStart();
+    return GetCode().instruction_start();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
     return GetBytecodeArray().GetFirstBytecodeAddress();
@@ -168,7 +168,7 @@ Address AbstractCode::InstructionStart(PtrComprCageBase cage_base) {
 Address AbstractCode::InstructionEnd(PtrComprCageBase cage_base) {
   Map map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    return GetCode().InstructionEnd();
+    return GetCode().instruction_end();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
     BytecodeArray bytecode_array = GetBytecodeArray();
@@ -239,6 +239,12 @@ BytecodeArray AbstractCode::GetBytecodeArray() {
 
 OBJECT_CONSTRUCTORS_IMPL(InstructionStream, HeapObject)
 NEVER_READ_ONLY_SPACE_IMPL(InstructionStream)
+INT_ACCESSORS(InstructionStream, body_size, kBodySizeOffset)
+
+Address InstructionStream::body_end() const {
+  static_assert(kOnHeapBodyIsContiguous);
+  return instruction_start() + body_size();
+}
 
 INT_ACCESSORS(Code, instruction_size, kInstructionSizeOffset)
 INT_ACCESSORS(Code, metadata_size, kMetadataSizeOffset)
@@ -291,7 +297,6 @@ ACCESSORS_CHECKED2(Code, bytecode_offset_table, ByteArray, kPositionTableOffset,
       name, type, offset, !ObjectInYoungGeneration(value),               \
       !ObjectInYoungGeneration(value))
 
-// Concurrent marker needs to access kind specific flags in code.
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(code, Code, kCodeOffset)
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(raw_code, HeapObject, kCodeOffset)
 #undef RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS
@@ -325,45 +330,6 @@ void InstructionStream::set_main_cage_base(Address cage_base, RelaxedStoreTag) {
 #endif
 }
 
-Code InstructionStream::GCSafeCode(AcquireLoadTag) const {
-  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
-  HeapObject object =
-      TaggedField<HeapObject, kCodeOffset>::Acquire_Load(cage_base, *this);
-  DCHECK(!ObjectInYoungGeneration(object));
-  Code code = ForwardingAddress(Code::unchecked_cast(object));
-  return code;
-}
-
-// Helper functions for converting InstructionStream objects to
-// Code and back.
-inline Code ToCode(InstructionStream code) { return code.code(kAcquireLoad); }
-
-inline InstructionStream FromCode(Code code) {
-  DCHECK(code.has_instruction_stream());
-  // Compute the InstructionStream object pointer from the code entry point.
-  Address ptr =
-      code.code_entry_point() - InstructionStream::kHeaderSize + kHeapObjectTag;
-  return InstructionStream::cast(Object(ptr));
-}
-
-inline InstructionStream FromCode(Code code, PtrComprCageBase code_cage_base,
-                                  RelaxedLoadTag tag) {
-  DCHECK(code.has_instruction_stream());
-  // Since the code entry point field is not aligned we can't load it atomically
-  // and use for InstructionStream object pointer calculation. So, we load and
-  // decompress the code field.
-  return code.instruction_stream(code_cage_base, tag);
-}
-
-inline InstructionStream FromCode(Code code, Isolate* isolate,
-                                  RelaxedLoadTag tag) {
-#ifdef V8_EXTERNAL_CODE_SPACE
-  return FromCode(code, PtrComprCageBase{isolate->code_cage_base()}, tag);
-#else
-  return FromCode(code, GetPtrComprCageBase(code), tag);
-#endif  // V8_EXTERNAL_CODE_SPACE
-}
-
 // TODO(jgruber): Remove this method once main_cage_base is gone.
 void InstructionStream::WipeOutHeader() {
   WRITE_FIELD(*this, kCodeOffset, Smi::FromInt(0));
@@ -372,13 +338,6 @@ void InstructionStream::WipeOutHeader() {
   }
 }
 
-void Code::ClearInstructionStreamPadding() {
-  // Clear the padding after `body_end`.
-  size_t trailing_padding_size =
-      CodeSize() - InstructionStream::kHeaderSize - body_size();
-  memset(reinterpret_cast<void*>(body_end()), 0, trailing_padding_size);
-}
-
 ByteArray Code::SourcePositionTable(Isolate* isolate,
                                     SharedFunctionInfo sfi) const {
   if (!has_instruction_stream()) {
@@ -392,27 +351,24 @@ ByteArray Code::SourcePositionTable(Isolate* isolate,
   return source_position_table(isolate);
 }
 
-Address Code::body_start() const { return InstructionStart(); }
+Address Code::body_start() const { return instruction_start(); }
 
 Address Code::body_end() const { return body_start() + body_size(); }
 
 int Code::body_size() const { return instruction_size() + metadata_size(); }
 
-// TODO(jgruber): Remove instruction_size.
-int Code::InstructionSize() const { return instruction_size(); }
-
 Address InstructionStream::instruction_start() const {
   return field_address(kHeaderSize);
 }
 
-Address Code::InstructionEnd() const {
-  return InstructionStart() + instruction_size();
+Address Code::instruction_end() const {
+  return instruction_start() + instruction_size();
 }
 
 Address Code::metadata_start() const {
   if (has_instruction_stream()) {
     static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-    return InstructionStart() + instruction_size();
+    return instruction_start() + instruction_size();
   }
   // An embedded builtin. Remapping is irrelevant wrt the metadata section so
   // we can simply use the global blob.
@@ -424,7 +380,7 @@ Address Code::metadata_start() const {
 }
 
 Address Code::InstructionStart(Isolate* isolate, Address pc) const {
-  if (V8_LIKELY(has_instruction_stream())) return code_entry_point();
+  if (V8_LIKELY(has_instruction_stream())) return instruction_start();
   // Note we intentionally don't bounds-check that `pc` is within the returned
   // instruction area.
   return EmbeddedData::FromBlobForPc(isolate, pc)
@@ -432,12 +388,12 @@ Address Code::InstructionStart(Isolate* isolate, Address pc) const {
 }
 
 Address Code::InstructionEnd(Isolate* isolate, Address pc) const {
-  return InstructionStart(isolate, pc) + InstructionSize();
+  return InstructionStart(isolate, pc) + instruction_size();
 }
 
 int Code::GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const {
   const Address offset = pc - InstructionStart(isolate, pc);
-  DCHECK_LE(offset, InstructionSize());
+  DCHECK_LE(offset, instruction_size());
   return static_cast<int>(offset);
 }
 
@@ -445,15 +401,6 @@ Address Code::metadata_end() const {
   return metadata_start() + metadata_size();
 }
 
-int Code::SizeIncludingMetadata() const {
-  int size = CodeSize();
-  size += relocation_info().Size();
-  if (kind() != CodeKind::BASELINE) {
-    size += deoptimization_data().Size();
-  }
-  return size;
-}
-
 Address Code::safepoint_table_address() const {
   return metadata_start() + safepoint_table_offset();
 }
@@ -497,7 +444,7 @@ FixedArray Code::unchecked_deoptimization_data() const {
           *this));
 }
 
-Code InstructionStream::unchecked_code() const {
+Code InstructionStream::unchecked_code(AcquireLoadTag tag) const {
   PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
   return Code::unchecked_cast(
       TaggedField<HeapObject, kCodeOffset>::Acquire_Load(cage_base, *this));
@@ -519,28 +466,34 @@ int Code::relocation_size() const {
   return V8_LIKELY(has_instruction_stream()) ? relocation_info().length() : 0;
 }
 
-Address InstructionStream::entry() const { return instruction_start(); }
-
 bool Code::contains(Isolate* isolate, Address inner_pointer) const {
   const Address start = InstructionStart(isolate, inner_pointer);
   if (inner_pointer < start) return false;
-  return inner_pointer < start + InstructionSize();
+  return inner_pointer < start + instruction_size();
 }
 
-// static
-void Code::CopyRelocInfoToByteArray(ByteArray dest, const CodeDesc& desc) {
-  DCHECK_EQ(dest.length(), desc.reloc_size);
-  CopyBytes(dest.GetDataStartAddress(),
-            desc.buffer + desc.buffer_size - desc.reloc_size,
-            static_cast<size_t>(desc.reloc_size));
+int InstructionStream::Size() const { return SizeFor(body_size()); }
+int Code::InstructionStreamObjectSize() const {
+  return InstructionStream::SizeFor(body_size());
 }
 
-int InstructionStream::CodeSize() const {
-  return SizeFor(Code::unchecked_cast(raw_code(kAcquireLoad)).body_size());
+void InstructionStream::clear_padding() {
+  // Header padding.
+  memset(reinterpret_cast<void*>(address() + kUnalignedSize), 0,
+         kHeaderSize - kUnalignedSize);
+  // Trailing padding.
+  memset(reinterpret_cast<void*>(body_end()), 0,
+         TrailingPaddingSizeFor(body_size()));
 }
-int Code::CodeSize() const { return InstructionStream::SizeFor(body_size()); }
 
-DEF_GETTER(InstructionStream, Size, int) { return CodeSize(); }
+int Code::SizeIncludingMetadata() const {
+  int size = InstructionStreamObjectSize();
+  size += relocation_info().Size();
+  if (kind() != CodeKind::BASELINE) {
+    size += deoptimization_data().Size();
+  }
+  return size;
+}
 
 CodeKind Code::kind() const {
   static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
@@ -556,7 +509,7 @@ int Code::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
       ByteArray::cast(bytecode_offset_table()), bytecodes);
-  Address pc = baseline_pc - InstructionStart();
+  Address pc = baseline_pc - instruction_start();
   offset_iterator.AdvanceToPCOffset(pc);
   return offset_iterator.current_bytecode_offset();
 }
@@ -668,12 +621,6 @@ inline void Code::set_is_promise_rejection(bool value) {
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
-inline HandlerTable::CatchPrediction
-InstructionStream::GetBuiltinCatchPrediction() const {
-  if (is_promise_rejection()) return HandlerTable::PROMISE;
-  return HandlerTable::UNCAUGHT;
-}
-
 inline HandlerTable::CatchPrediction Code::GetBuiltinCatchPrediction() const {
   if (is_promise_rejection()) return HandlerTable::PROMISE;
   return HandlerTable::UNCAUGHT;
@@ -902,11 +849,18 @@ bool Code::has_instruction_stream(RelaxedLoadTag tag) const {
 }
 
 PtrComprCageBase Code::code_cage_base() const {
+#ifdef V8_EXTERNAL_CODE_SPACE
+  return code_cage_base(GetIsolateFromWritableObject(*this));
+#else   // V8_EXTERNAL_CODE_SPACE
+  return code_cage_base(nullptr /* parameter unused */);
+#endif  // V8_EXTERNAL_CODE_SPACE
+}
+
+PtrComprCageBase Code::code_cage_base(Isolate* isolate) const {
 #ifdef V8_EXTERNAL_CODE_SPACE
   // Only available if the current Code object is not in RO space (otherwise we
   // can't grab the current Isolate from it).
   DCHECK(!InReadOnlySpace());
-  Isolate* isolate = GetIsolateFromWritableObject(*this);
   return PtrComprCageBase(isolate->code_cage_base());
 #else   // V8_EXTERNAL_CODE_SPACE
   // Without external code space: `code_cage_base == main_cage_base`. We can
@@ -946,43 +900,42 @@ Object Code::raw_instruction_stream(PtrComprCageBase cage_base,
   return ExternalCodeField<Object>::Relaxed_Load(cage_base, *this);
 }
 
-DEF_GETTER(Code, code_entry_point, Address) {
-  return ReadField<Address>(kCodeEntryPointOffset);
+DEF_GETTER(Code, instruction_start, Address) {
+  return ReadField<Address>(kInstructionStartOffset);
 }
 
-void Code::init_code_entry_point(Isolate* isolate, Address value) {
-  set_code_entry_point(isolate, value);
+void Code::init_instruction_start(Isolate* isolate, Address value) {
+  set_instruction_start(isolate, value);
 }
 
-void Code::set_code_entry_point(Isolate* isolate, Address value) {
-  WriteField<Address>(kCodeEntryPointOffset, value);
+void Code::set_instruction_start(Isolate* isolate, Address value) {
+  WriteField<Address>(kInstructionStartOffset, value);
 }
 
-void Code::SetInstructionStreamAndEntryPoint(Isolate* isolate_for_sandbox,
-                                             InstructionStream code,
-                                             WriteBarrierMode mode) {
+void Code::SetInstructionStreamAndInstructionStart(Isolate* isolate_for_sandbox,
+                                                   InstructionStream code,
+                                                   WriteBarrierMode mode) {
   set_raw_instruction_stream(code, mode);
-  set_code_entry_point(isolate_for_sandbox, code.instruction_start());
+  set_instruction_start(isolate_for_sandbox, code.instruction_start());
 }
 
-void Code::SetEntryPointForOffHeapBuiltin(Isolate* isolate_for_sandbox,
-                                          Address entry) {
+void Code::SetInstructionStartForOffHeapBuiltin(Isolate* isolate_for_sandbox,
+                                                Address entry) {
   DCHECK(!has_instruction_stream());
-  set_code_entry_point(isolate_for_sandbox, entry);
+  set_instruction_start(isolate_for_sandbox, entry);
 }
 
-void Code::SetCodeEntryPointForSerialization(Isolate* isolate, Address entry) {
-  set_code_entry_point(isolate, entry);
+void Code::SetInstructionStartForSerialization(Isolate* isolate,
+                                               Address entry) {
+  set_instruction_start(isolate, entry);
 }
 
-void Code::UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
-                                InstructionStream istream) {
+void Code::UpdateInstructionStart(Isolate* isolate_for_sandbox,
+                                  InstructionStream istream) {
   DCHECK_EQ(raw_instruction_stream(), istream);
-  set_code_entry_point(isolate_for_sandbox, istream.instruction_start());
+  set_instruction_start(isolate_for_sandbox, istream.instruction_start());
 }
 
-Address Code::InstructionStart() const { return code_entry_point(); }
-
 void Code::clear_padding() {
   memset(reinterpret_cast<void*>(address() + kUnalignedSize), 0,
          kSize - kUnalignedSize);
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 908c8752b5..72fa909141 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -49,7 +49,7 @@ void Code::ClearEmbeddedObjects(Heap* heap) {
 }
 
 void InstructionStream::Relocate(intptr_t delta) {
-  Code code = unchecked_code();
+  Code code = unchecked_code(kAcquireLoad);
   // This is called during evacuation and code.instruction_stream() will point
   // to the old object. So pass *this directly to the RelocIterator.
   for (RelocIterator it(code, *this, code.unchecked_relocation_info(),
@@ -61,21 +61,24 @@ void InstructionStream::Relocate(intptr_t delta) {
 }
 
 void Code::FlushICache() const {
-  FlushInstructionCache(InstructionStart(), instruction_size());
+  FlushInstructionCache(instruction_start(), instruction_size());
 }
 
 void Code::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
                            const CodeDesc& desc) {
   // Copy code.
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-  CopyBytes(reinterpret_cast<byte*>(InstructionStart()), desc.buffer,
+  CopyBytes(reinterpret_cast<byte*>(instruction_start()), desc.buffer,
             static_cast<size_t>(desc.instr_size));
   // TODO(jgruber,v8:11036): Merge with the above.
-  CopyBytes(reinterpret_cast<byte*>(InstructionStart() + desc.instr_size),
+  CopyBytes(reinterpret_cast<byte*>(instruction_start() + desc.instr_size),
             desc.unwinding_info, static_cast<size_t>(desc.unwinding_info_size));
 
   // Copy reloc info.
-  CopyRelocInfoToByteArray(reloc_info, desc);
+  DCHECK_EQ(reloc_info.length(), desc.reloc_size);
+  CopyBytes(reloc_info.GetDataStartAddress(),
+            desc.buffer + desc.buffer_size - desc.reloc_size,
+            static_cast<size_t>(desc.reloc_size));
 
   // Unbox handles and relocate.
   RelocateFromDesc(reloc_info, heap, desc);
@@ -97,8 +100,8 @@ void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
       // code object.
       Handle<HeapObject> p = it.rinfo()->target_object_handle(origin);
       DCHECK(p->IsCode(GetPtrComprCageBaseSlow(*p)));
-      InstructionStream code = FromCode(Code::cast(*p));
-      it.rinfo()->set_target_address(code.instruction_start(),
+      InstructionStream istream = Code::cast(*p).instruction_stream();
+      it.rinfo()->set_target_address(istream.instruction_start(),
                                      UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
     } else if (RelocInfo::IsNearBuiltinEntry(mode)) {
       // Rewrite builtin IDs to PC-relative offset to the builtin entry point.
@@ -124,7 +127,7 @@ void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
 #endif
     } else {
       intptr_t delta =
-          InstructionStart() - reinterpret_cast<Address>(desc.buffer);
+          instruction_start() - reinterpret_cast<Address>(desc.buffer);
       it.rinfo()->apply(delta);
     }
   }
@@ -404,9 +407,9 @@ void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
   os << "address = " << reinterpret_cast<void*>(code.ptr()) << "\n\n";
 
   {
-    int code_size = code.InstructionSize();
+    int code_size = code.instruction_size();
     os << "Instructions (size = " << code_size << ")\n";
-    DisassembleCodeRange(isolate, os, code, code.InstructionStart(), code_size,
+    DisassembleCodeRange(isolate, os, code, code.instruction_start(), code_size,
                          current_pc);
 
     if (int pool_size = code.constant_pool_size()) {
diff --git a/src/objects/code.h b/src/objects/code.h
index 8cdb423553..8ddcc65ccd 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -68,16 +68,13 @@ class Register;
 //  |                          |  <-- MS + unwinding_info_offset()
 //  +--------------------------+  <-- MetadataEnd()
 //
-// TODO(jgruber): Code currently contains many aliases for InstructionStream
-// functions. These will eventually move to the Code object. Once done, put all
-// these declarations in a decent order and move over comments from the current
-// declarations in InstructionStream.
 class Code : public HeapObject {
  public:
   // When V8_EXTERNAL_CODE_SPACE is enabled, InstructionStream objects are
   // allocated in a separate pointer compression cage instead of the cage where
   // all the other objects are allocated.
   inline PtrComprCageBase code_cage_base() const;
+  inline PtrComprCageBase code_cage_base(Isolate* isolate) const;
 
   // Back-reference to the InstructionStream object.
   //
@@ -98,42 +95,37 @@ class Code : public HeapObject {
   // InstructionStream doesn't exist yet - in this situation,
   // has_instruction_stream is `false` but will change to `true` once
   // InstructionStream has also been initialized.
+  // Unfortunately, it's not easily possible to avoid this. The
+  // InstructionStream can't be allocated first, since relocation requires
+  // access to Code::relocation_info.
   inline bool has_instruction_stream() const;
   inline bool has_instruction_stream(RelaxedLoadTag) const;
 
-  // The cached value of instruction_stream().InstructionStart(), *or* a
-  // pointer to the off-heap entry point for embedded builtins.
-  DECL_GETTER(code_entry_point, Address)
-
-  // Aliases for code_entry_point for API compatibility with InstructionStream.
-  inline Address InstructionStart() const;
-  inline Address InstructionEnd() const;
-  inline int InstructionSize() const;
+  // The start of the associated instruction stream. Points either into an
+  // on-heap InstructionStream object, or to the beginning of an embedded
+  // builtin.
+  DECL_GETTER(instruction_start, Address)
+  DECL_PRIMITIVE_ACCESSORS(instruction_size, int)
+  inline Address instruction_end() const;
 
-  inline void SetInstructionStreamAndEntryPoint(
+  inline void SetInstructionStreamAndInstructionStart(
       Isolate* isolate_for_sandbox, InstructionStream code,
       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
-  inline void SetEntryPointForOffHeapBuiltin(Isolate* isolate_for_sandbox,
-                                             Address entry);
-  inline void SetCodeEntryPointForSerialization(Isolate* isolate,
-                                                Address entry);
-  // Updates the value of the code entry point. `istream` must be equal to
-  // the instruction_stream() value.
-  inline void UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
-                                   InstructionStream istream);
+  inline void SetInstructionStartForOffHeapBuiltin(Isolate* isolate_for_sandbox,
+                                                   Address entry);
+  inline void SetInstructionStartForSerialization(Isolate* isolate,
+                                                  Address entry);
+  inline void UpdateInstructionStart(Isolate* isolate_for_sandbox,
+                                     InstructionStream istream);
 
   DECL_RELAXED_UINT16_ACCESSORS(kind_specific_flags)
 
-  // Initializes internal flags field which stores cached values of some
-  // properties of the respective InstructionStream object.
   inline void initialize_flags(CodeKind kind, Builtin builtin_id,
                                bool is_turbofanned, int stack_slots);
 
   // Clear uninitialized padding space. This ensures that the snapshot content
   // is deterministic.
   inline void clear_padding();
-  // Clear the padding in the InstructionStream
-  inline void ClearInstructionStreamPadding();
 
   // Flushes the instruction cache for the executable instructions of this code
   // object. Make sure to call this while the code is still writable.
@@ -150,7 +142,6 @@ class Code : public HeapObject {
 
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
-  DECL_PRIMITIVE_ACCESSORS(instruction_size, int)
   DECL_PRIMITIVE_ACCESSORS(metadata_size, int)
   // [handler_table_offset]: The offset where the exception handler table
   // starts.
@@ -248,7 +239,6 @@ class Code : public HeapObject {
   inline byte* relocation_end() const;
   inline int relocation_size() const;
 
-  // [safepoint_table_offset]: The offset where the safepoint table starts.
   inline int safepoint_table_offset() const { return 0; }
 
   inline Address body_start() const;
@@ -258,7 +248,12 @@ class Code : public HeapObject {
   inline Address metadata_start() const;
   inline Address metadata_end() const;
 
-  inline int CodeSize() const;
+  // The size of the associated InstructionStream object, if it exists.
+  inline int InstructionStreamObjectSize() const;
+
+  // TODO(jgruber): This function tries to account for various parts of the
+  // object graph, but is incomplete. Take it as a lower bound for the memory
+  // associated with this Code object.
   inline int SizeIncludingMetadata() const;
 
   // The following functions include support for short builtin calls:
@@ -304,11 +299,6 @@ class Code : public HeapObject {
   void CopyFromNoFlush(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
   void RelocateFromDesc(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
 
-  // Copy the RelocInfo portion of |desc| to |dest|. The ByteArray must be
-  // exactly the same size as the RelocInfo in |desc|.
-  static inline void CopyRelocInfoToByteArray(ByteArray dest,
-                                              const CodeDesc& desc);
-
   bool IsIsolateIndependent(Isolate* isolate);
 
   inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
@@ -342,8 +332,6 @@ class Code : public HeapObject {
 #endif  // ENABLE_DISASSEMBLER
 
   DECL_CAST(Code)
-
-  // Dispatched behavior.
   DECL_PRINTER(Code)
   DECL_VERIFIER(Code)
 
@@ -360,9 +348,8 @@ class Code : public HeapObject {
   /* Raw data fields. */                                                      \
   /* Data or code not directly visited by GC directly starts here. */         \
   V(kDataStart, 0)                                                            \
-  V(kCodeEntryPointOffset, kSystemPointerSize)                                \
+  V(kInstructionStartOffset, kSystemPointerSize)                              \
   /* The serializer needs to copy bytes starting from here verbatim. */       \
-  /* Objects embedded into code is visited via reloc info. */                 \
   V(kFlagsOffset, kInt32Size)                                                 \
   V(kBuiltinIdOffset, kInt16Size)                                             \
   V(kKindSpecificFlagsOffset, kInt16Size)                                     \
@@ -370,7 +357,6 @@ class Code : public HeapObject {
   V(kMetadataSizeOffset, kIntSize)                                            \
   V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
   V(kOsrOffsetOffset, kInt32Size)                                             \
-  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
   V(kHandlerTableOffsetOffset, kIntSize)                                      \
   V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
   V(kConstantPoolOffsetOffset, V8_EMBEDDED_CONSTANT_POOL_BOOL ? kIntSize : 0) \
@@ -398,16 +384,15 @@ class Code : public HeapObject {
   V(KindField, CodeKind, 4, _)      \
   V(IsTurbofannedField, bool, 1, _) \
   V(StackSlotsField, int, 24, _)
-  /* The other 3 bits are still free. */
-  // TODO(v8:13784): merge this with KindSpecificFlags by dropping the
-  // IsPromiseRejection field or taking one bit from the StackSlots field.
-
   DEFINE_BIT_FIELDS(FLAGS_BIT_FIELDS)
 #undef FLAGS_BIT_FIELDS
-  static_assert(kCodeKindCount <= KindField::kNumValues);
+  // TODO(v8:13784): merge this with KindSpecificFlags by dropping the
+  // IsPromiseRejection field or taking one bit from the StackSlots field.
+  // The other 3 bits are still free.
   static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 29);
   static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
                 FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
+  static_assert(kCodeKindCount <= KindField::kNumValues);
 
   // KindSpecificFlags layout.
 #define KIND_SPECIFIC_FLAGS_BIT_FIELDS(V, _)  \
@@ -416,7 +401,8 @@ class Code : public HeapObject {
   V(CanHaveWeakObjectsField, bool, 1, _)      \
   V(IsPromiseRejectionField, bool, 1, _)
   DEFINE_BIT_FIELDS(KIND_SPECIFIC_FLAGS_BIT_FIELDS)
-#undef CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS
+#undef KIND_SPECIFIC_FLAGS_BIT_FIELDS
+  // The other 12 bits are still free.
   static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
   static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
                 FIELD_SIZE(Code::kKindSpecificFlagsOffset) * kBitsPerByte);
@@ -427,12 +413,14 @@ class Code : public HeapObject {
 
   class OptimizedCodeIterator;
 
+  // Reserve one argument count value as the "don't adapt arguments" sentinel.
+  static const int kArgumentsBits = 16;
+  static const int kMaxArguments = (1 << kArgumentsBits) - 2;
+
  private:
-  inline void init_code_entry_point(Isolate* isolate, Address initial_value);
-  inline void set_code_entry_point(Isolate* isolate, Address value);
+  inline void init_instruction_start(Isolate* isolate, Address initial_value);
+  inline void set_instruction_start(Isolate* isolate, Address value);
 
-  // Contains cached values of some flags of the from the respective
-  // InstructionStream object.
   DECL_RELAXED_UINT16_ACCESSORS(flags)
 
   enum BytecodeToPCPosition {
@@ -448,12 +436,10 @@ class Code : public HeapObject {
 
   template <typename IsolateT>
   friend class Deserializer;
-  friend class ReadOnlyDeserializer;  // For init_code_entry_point.
-  friend class GcSafeCode;  // For OffHeapFoo functions.
+  friend class ReadOnlyDeserializer;  // For init_instruction_start.
   friend Factory;
   friend FactoryBase<Factory>;
   friend FactoryBase<LocalFactory>;
-  friend Isolate;
 
   OBJECT_CONSTRUCTORS(Code, HeapObject);
 };
@@ -485,8 +471,8 @@ class GcSafeCode : public HeapObject {
   inline Code UnsafeCastToCode() const;
 
   // Safe accessors (these just forward to Code methods).
-  inline Address InstructionStart() const;
-  inline Address InstructionEnd() const;
+  inline Address instruction_start() const;
+  inline Address instruction_end() const;
   inline bool is_builtin() const;
   inline Builtin builtin_id() const;
   inline CodeKind kind() const;
@@ -528,7 +514,6 @@ class InstructionStream : public HeapObject {
   //
   //  +--------------------------+
   //  |          header          |
-  //  | padded to code alignment |
   //  +--------------------------+  <-- body_start()
   //  |       instructions       |   == instruction_start()
   //  |           ...            |
@@ -542,7 +527,7 @@ class InstructionStream : public HeapObject {
   //  |                          |  <-- MS + unwinding_info_offset()
   //  | padded to obj alignment  |
   //  +--------------------------+  <-- metadata_end() == body_end()
-  //  | padded to code alignment |
+  //  | padded to kCodeAlignmentMinusCodeHeader
   //  +--------------------------+
   //
   // In other words, the variable-size 'body' consists of 'instructions' and
@@ -563,10 +548,21 @@ class InstructionStream : public HeapObject {
   // [code]: The associated Code object.
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
   DECL_RELEASE_ACQUIRE_ACCESSORS(raw_code, HeapObject)
+  inline Code unchecked_code(AcquireLoadTag tag) const;
+
+  // When V8_EXTERNAL_CODE_SPACE is enabled, InstructionStream objects are
+  // allocated in a separate pointer compression cage instead of the cage where
+  // all the other objects are allocated. This field contains cage base value
+  // which is used for decompressing the references to non-InstructionStream
+  // objects (map, deoptimization_data, etc.).
+  inline PtrComprCageBase main_cage_base() const;
+  inline PtrComprCageBase main_cage_base(RelaxedLoadTag) const;
+  inline void set_main_cage_base(Address cage_base, RelaxedStoreTag);
 
-  // A convenience wrapper around raw_code that will do an unchecked cast for
-  // you.
-  inline Code unchecked_code() const;
+  // The size of the entire body section, containing instructions and inlined
+  // metadata.
+  DECL_PRIMITIVE_ACCESSORS(body_size, int)
+  inline Address body_end() const;
 
   // The entire code object including its header is copied verbatim to the
   // snapshot so that it can be written in one, fast, memcpy during
@@ -577,43 +573,28 @@ class InstructionStream : public HeapObject {
   // out the to-be-overwritten header data for reproducible snapshots.
   inline void WipeOutHeader();
 
-  // When V8_EXTERNAL_CODE_SPACE is enabled, InstructionStream objects are
-  // allocated in a separate pointer compression cage instead of the cage where
-  // all the other objects are allocated. This field contains cage base value
-  // which is used for decompressing the references to non-InstructionStream
-  // objects (map, deoptimization_data, etc.).
-  inline PtrComprCageBase main_cage_base() const;
-  inline PtrComprCageBase main_cage_base(RelaxedLoadTag) const;
-  inline void set_main_cage_base(Address cage_base, RelaxedStoreTag);
-
   static inline InstructionStream FromTargetAddress(Address address);
   static inline InstructionStream FromEntryAddress(Address location_of_address);
 
-  // InstructionStream entry point.
-  inline Address entry() const;
-
   // Relocate the code by delta bytes. Called to signal that this code
   // object has been moved by delta bytes.
   void Relocate(intptr_t delta);
 
-  // Returns the object size for a given body (used for allocation).
-  static int SizeFor(int body_size) {
-    return RoundUp(kHeaderSize + body_size, kCodeAlignment);
-  }
-
-  inline int CodeSize() const;
+  inline void clear_padding();
 
-  // Hides HeapObject::Size(...) and redirects queries to CodeSize().
-  DECL_GETTER(Size, int)
+  static constexpr int TrailingPaddingSizeFor(int body_size) {
+    return RoundUp<kCodeAlignment>(kHeaderSize + body_size) - kHeaderSize -
+           body_size;
+  }
+  static constexpr int SizeFor(int body_size) {
+    return kHeaderSize + body_size + TrailingPaddingSizeFor(body_size);
+  }
+  inline int Size() const;
 
   DECL_CAST(InstructionStream)
-
-  // Dispatched behavior.
   DECL_PRINTER(InstructionStream)
   DECL_VERIFIER(InstructionStream)
 
-  inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
-
   // Layout description.
 #define ISTREAM_FIELDS(V)                                             \
   V(kCodeOffset, kTaggedSize)                                         \
@@ -621,16 +602,18 @@ class InstructionStream : public HeapObject {
   V(kDataStart, 0)                                                    \
   V(kMainCageBaseUpper32BitsOffset,                                   \
     V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                    \
+  V(kBodySizeOffset, kIntSize)                                        \
+  V(kUnalignedSize, OBJECT_POINTER_PADDING(kUnalignedSize))           \
   V(kHeaderSize, 0)
-
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, ISTREAM_FIELDS)
 #undef ISTREAM_FIELDS
 
   static_assert(kCodeAlignment > kHeaderSize);
   // We do two things to ensure kCodeAlignment of the entry address:
-  // 1) add kCodeAlignmentMinusCodeHeader padding once in the beginning of every
-  //    MemoryChunk
-  // 2) Round up all IStream allocations to a multiple of kCodeAlignment
+  // 1) Add kCodeAlignmentMinusCodeHeader padding once in the beginning of every
+  //    MemoryChunk.
+  // 2) Round up all IStream allocations to a multiple of kCodeAlignment, see
+  //    TrailingPaddingSizeFor.
   // Together, the IStream object itself will always start at offset
   // kCodeAlignmentMinusCodeHeader, which aligns the entry to kCodeAlignment.
   static constexpr int kCodeAlignmentMinusCodeHeader =
@@ -638,18 +621,7 @@ class InstructionStream : public HeapObject {
 
   class BodyDescriptor;
 
-  static const int kArgumentsBits = 16;
-  // Reserve one argument count value as the "don't adapt arguments" sentinel.
-  static const int kMaxArguments = (1 << kArgumentsBits) - 2;
-
  private:
-  friend class RelocIterator;
-  friend class EvacuateVisitorBase;
-
-  inline Code GCSafeCode(AcquireLoadTag) const;
-
-  bool is_promise_rejection() const;
-
   OBJECT_CONSTRUCTORS(InstructionStream, HeapObject);
 };
 
@@ -669,13 +641,6 @@ class Code::OptimizedCodeIterator {
   DISALLOW_GARBAGE_COLLECTION(no_gc)
 };
 
-// Helper functions for converting InstructionStream objects to
-// Code and back.
-inline Code ToCode(InstructionStream code);
-inline InstructionStream FromCode(Code code);
-inline InstructionStream FromCode(Code code, Isolate* isolate, RelaxedLoadTag);
-inline InstructionStream FromCode(Code code, PtrComprCageBase, RelaxedLoadTag);
-
 // AbstractCode is a helper wrapper around {Code|BytecodeArray}.
 class AbstractCode : public HeapObject {
  public:
@@ -722,13 +687,13 @@ class AbstractCode : public HeapObject {
   OBJECT_CONSTRUCTORS(AbstractCode, HeapObject);
 };
 
-// Dependent code is conceptually the list of {InstructionStream,
-// DependencyGroup} tuples associated with an object, where the dependency group
-// is a reason that could lead to a deopt of the corresponding code.
+// Dependent code is conceptually the list of {Code, DependencyGroup} tuples
+// associated with an object, where the dependency group is a reason that could
+// lead to a deopt of the corresponding code.
 //
 // Implementation details: DependentCode is a weak array list containing
-// entries, where each entry consists of a (weak) InstructionStream object and
-// the DependencyGroups bitset as a Smi.
+// entries, where each entry consists of a (weak) Code object and the
+// DependencyGroups bitset as a Smi.
 //
 // Note the underlying weak array list currently never shrinks physically (the
 // contents may shrink).
diff --git a/src/objects/js-function-inl.h b/src/objects/js-function-inl.h
index 152c00d1b1..bddd51d4c9 100644
--- a/src/objects/js-function-inl.h
+++ b/src/objects/js-function-inl.h
@@ -80,8 +80,8 @@ void JSFunction::set_code(Code value, ReleaseStoreTag, WriteBarrierMode mode) {
 }
 RELEASE_ACQUIRE_ACCESSORS(JSFunction, context, Context, kContextOffset)
 
-Address JSFunction::code_entry_point() const {
-  return Code::cast(code()).code_entry_point();
+Address JSFunction::instruction_start() const {
+  return Code::cast(code()).instruction_start();
 }
 
 // TODO(ishell): Why relaxed read but release store?
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index da353d2a66..2a599df432 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -122,7 +122,7 @@ class JSFunction : public TorqueGeneratedJSFunction<
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
 
   // Returns the address of the function code's instruction start.
-  inline Address code_entry_point() const;
+  inline Address instruction_start() const;
 
   // Get the abstract code associated with the function, which will either be
   // a InstructionStream object or a BytecodeArray.
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 3f1d1b703a..347d56a9b0 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -986,7 +986,7 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
     IteratePointers(obj, kCodeOffset, kDataStart, v);
 
     InstructionStream istream = InstructionStream::cast(obj);
-    Code code = istream.unchecked_code();
+    Code code = istream.unchecked_code(kAcquireLoad);
     RelocIterator it(code, istream, code.unchecked_relocation_info(),
                      kRelocModeMask);
     v->VisitRelocInfo(&it);
@@ -999,7 +999,7 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
   }
 
   static inline int SizeOf(Map map, HeapObject object) {
-    return InstructionStream::unchecked_cast(object).CodeSize();
+    return InstructionStream::unchecked_cast(object).Size();
   }
 };
 
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index 26a157c6cf..288d842b7c 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -2279,7 +2279,7 @@ int HeapObject::SizeFromMap(Map map) const {
 #undef MAKE_TORQUE_SIZE_FOR
 
   if (instance_type == INSTRUCTION_STREAM_TYPE) {
-    return InstructionStream::unchecked_cast(*this).CodeSize();
+    return InstructionStream::unchecked_cast(*this).Size();
   }
   if (instance_type == COVERAGE_INFO_TYPE) {
     return CoverageInfo::SizeFor(
diff --git a/src/parsing/parser-base.h b/src/parsing/parser-base.h
index ff5af7dfec..2250706351 100644
--- a/src/parsing/parser-base.h
+++ b/src/parsing/parser-base.h
@@ -2864,8 +2864,7 @@ typename ParserBase<Impl>::ExpressionT ParserBase<Impl>::ParseObjectLiteral() {
   // this runtime function. Here, we make sure that the number of
   // properties is less than number of arguments allowed for a runtime
   // call.
-  if (has_rest_property &&
-      properties.length() > InstructionStream::kMaxArguments) {
+  if (has_rest_property && properties.length() > Code::kMaxArguments) {
     expression_scope()->RecordPatternError(Scanner::Location(pos, position()),
                                            MessageTemplate::kTooManyArguments);
   }
@@ -2921,7 +2920,7 @@ void ParserBase<Impl>::ParseArguments(
     if (!Check(Token::COMMA)) break;
   }
 
-  if (args->length() > InstructionStream::kMaxArguments) {
+  if (args->length() > Code::kMaxArguments) {
     ReportMessage(MessageTemplate::kTooManyArguments);
     return;
   }
@@ -3968,7 +3967,7 @@ void ParserBase<Impl>::ParseFormalParameterList(FormalParametersT* parameters) {
   if (peek() != Token::RPAREN) {
     while (true) {
       // Add one since we're going to be adding a parameter.
-      if (parameters->arity + 1 > InstructionStream::kMaxArguments) {
+      if (parameters->arity + 1 > Code::kMaxArguments) {
         ReportMessage(MessageTemplate::kTooManyParameters);
         return;
       }
diff --git a/src/parsing/parser.cc b/src/parsing/parser.cc
index eacebe6250..adfdab6c1c 100644
--- a/src/parsing/parser.cc
+++ b/src/parsing/parser.cc
@@ -2596,7 +2596,7 @@ void Parser::DeclareArrowFunctionFormalParameters(
 
   AddArrowFunctionFormalParameters(parameters, expr, params_loc.end_pos);
 
-  if (parameters->arity > InstructionStream::kMaxArguments) {
+  if (parameters->arity > Code::kMaxArguments) {
     ReportMessageAt(params_loc, MessageTemplate::kMalformedArrowFunParamList);
     return;
   }
diff --git a/src/profiler/cpu-profiler.cc b/src/profiler/cpu-profiler.cc
index 5972ef6cba..64955fda73 100644
--- a/src/profiler/cpu-profiler.cc
+++ b/src/profiler/cpu-profiler.cc
@@ -416,8 +416,8 @@ void ProfilerCodeObserver::LogBuiltins() {
     CodeEventsContainer evt_rec(CodeEventRecord::Type::kReportBuiltin);
     ReportBuiltinEventRecord* rec = &evt_rec.ReportBuiltinEventRecord_;
     Code code = builtins->code(builtin);
-    rec->instruction_start = code.InstructionStart();
-    rec->instruction_size = code.InstructionSize();
+    rec->instruction_start = code.instruction_start();
+    rec->instruction_size = code.instruction_size();
     rec->builtin = builtin;
     CodeEventHandlerInternal(evt_rec);
   }
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index 37d807ed87..dab9903a01 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -1500,16 +1500,21 @@ void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
 
 void V8HeapExplorer::ExtractSharedFunctionInfoReferences(
     HeapEntry* entry, SharedFunctionInfo shared) {
-  std::unique_ptr<char[]> name = shared.DebugNameCStr();
-  Code code = shared.GetCode(isolate());
-  HeapObject maybe_code_obj =
-      code.has_instruction_stream() ? FromCode(code) : HeapObject::cast(code);
-  if (name[0] != '\0') {
-    TagObject(maybe_code_obj,
-              names_->GetFormatted("(code for %s)", name.get()));
-  } else {
-    TagObject(maybe_code_obj,
-              names_->GetFormatted("(%s code)", CodeKindToString(code.kind())));
+  {
+    std::unique_ptr<char[]> name = shared.DebugNameCStr();
+    Code code = shared.GetCode(isolate());
+    TagObject(code, name[0] != '\0'
+                        ? names_->GetFormatted("(code for %s)", name.get())
+                        : names_->GetFormatted("(%s code)",
+                                               CodeKindToString(code.kind())));
+    if (code.has_instruction_stream()) {
+      TagObject(
+          code.instruction_stream(),
+          name[0] != '\0'
+              ? names_->GetFormatted("(instruction stream for %s)", name.get())
+              : names_->GetFormatted("(%s instruction stream)",
+                                     CodeKindToString(code.kind())));
+    }
   }
 
   Object name_or_scope_info = shared.name_or_scope_info(kAcquireLoad);
@@ -1575,9 +1580,10 @@ void V8HeapExplorer::ExtractWeakCellReferences(HeapEntry* entry,
 }
 
 void V8HeapExplorer::TagBuiltinCodeObject(Code code, const char* name) {
-  TagObject(code, names_->GetFormatted("(%s builtin handle)", name));
+  TagObject(code, names_->GetFormatted("(%s builtin code)", name));
   if (code.has_instruction_stream()) {
-    TagObject(FromCode(code), names_->GetFormatted("(%s builtin)", name));
+    TagObject(code.instruction_stream(),
+              names_->GetFormatted("(%s builtin instruction stream)", name));
   }
 }
 
diff --git a/src/profiler/profiler-listener.cc b/src/profiler/profiler-listener.cc
index 688bdc1027..e2a5e10802 100644
--- a/src/profiler/profiler-listener.cc
+++ b/src/profiler/profiler-listener.cc
@@ -336,7 +336,7 @@ void ProfilerListener::CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
   CodeEventsContainer evt_rec(CodeEventRecord::Type::kCodeDeopt);
   CodeDeoptEventRecord* rec = &evt_rec.CodeDeoptEventRecord_;
   Deoptimizer::DeoptInfo info = Deoptimizer::GetDeoptInfo(*code, pc);
-  rec->instruction_start = code->InstructionStart();
+  rec->instruction_start = code->instruction_start();
   rec->deopt_reason = DeoptimizeReasonToString(info.deopt_reason);
   rec->deopt_id = info.deopt_id;
   rec->pc = pc;
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index 26c405cd69..8a248aaed5 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -288,7 +288,7 @@ int NativeRegExpMacroAssembler::CheckStackGuardState(
   DisallowGarbageCollection no_gc;
   Address old_pc = PointerAuthentication::AuthenticatePC(return_address, 0);
   DCHECK_LE(re_code.instruction_start(), old_pc);
-  DCHECK_LE(old_pc, re_code.code(kAcquireLoad).InstructionEnd());
+  DCHECK_LE(old_pc, re_code.code(kAcquireLoad).instruction_end());
 
   StackLimitCheck check(isolate);
   bool js_has_overflowed = check.JsHasOverflowed();
diff --git a/src/runtime/runtime-regexp.cc b/src/runtime/runtime-regexp.cc
index 52cc7f653a..4399f4bb8f 100644
--- a/src/runtime/runtime-regexp.cc
+++ b/src/runtime/runtime-regexp.cc
@@ -38,14 +38,13 @@ uint32_t GetArgcForReplaceCallable(uint32_t num_captures,
                                    bool has_named_captures) {
   const uint32_t kAdditionalArgsWithoutNamedCaptures = 2;
   const uint32_t kAdditionalArgsWithNamedCaptures = 3;
-  if (num_captures > InstructionStream::kMaxArguments) return -1;
+  if (num_captures > Code::kMaxArguments) return -1;
   uint32_t argc = has_named_captures
                       ? num_captures + kAdditionalArgsWithNamedCaptures
                       : num_captures + kAdditionalArgsWithoutNamedCaptures;
-  static_assert(InstructionStream::kMaxArguments <
-                std::numeric_limits<uint32_t>::max() -
-                    kAdditionalArgsWithNamedCaptures);
-  return (argc > InstructionStream::kMaxArguments) ? -1 : argc;
+  static_assert(Code::kMaxArguments < std::numeric_limits<uint32_t>::max() -
+                                          kAdditionalArgsWithNamedCaptures);
+  return (argc > Code::kMaxArguments) ? -1 : argc;
 }
 
 // Looks up the capture of the given name. Returns the (1-based) numbered
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index ec64819c9f..682bce0492 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -494,14 +494,14 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
     }
   } else if (InstanceTypeChecker::IsCode(instance_type)) {
     Code code = Code::cast(raw_obj);
-    code.init_code_entry_point(main_thread_isolate(), kNullAddress);
+    code.init_instruction_start(main_thread_isolate(), kNullAddress);
     if (!code.has_instruction_stream()) {
-      code.SetEntryPointForOffHeapBuiltin(
+      code.SetInstructionStartForOffHeapBuiltin(
           main_thread_isolate(), EmbeddedData::FromBlob(main_thread_isolate())
                                      .InstructionStartOf(code.builtin_id()));
     } else {
-      code.UpdateCodeEntryPoint(main_thread_isolate(),
-                                code.instruction_stream());
+      code.UpdateInstructionStart(main_thread_isolate(),
+                                  code.instruction_stream());
     }
   } else if (InstanceTypeChecker::IsMap(instance_type)) {
     if (v8_flags.log_maps) {
@@ -782,7 +782,7 @@ void DeserializerRelocInfoVisitor::VisitInternalReference(RelocInfo* rinfo) {
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
   DCHECK_LT(static_cast<unsigned>(target_offset),
             static_cast<unsigned>(rinfo->code().instruction_size()));
-  Address target = rinfo->code().InstructionStart() + target_offset;
+  Address target = rinfo->code().instruction_start() + target_offset;
   Assembler::deserialization_set_target_internal_reference_at(
       rinfo->pc(), target, rinfo->rmode());
 }
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index b9b3eb1e09..6f7aaf29be 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -339,7 +339,7 @@ EmbeddedData EmbeddedData::NewFromIsolate(Isolate* isolate) {
     uint8_t* dst = raw_code_start + offset;
     DCHECK_LE(RawCodeOffset() + offset + code.instruction_size(),
               blob_code_size);
-    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.InstructionStart()),
+    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.instruction_start()),
                 code.instruction_size());
   }
 
diff --git a/src/snapshot/read-only-deserializer.cc b/src/snapshot/read-only-deserializer.cc
index 15eacf19aa..929460e402 100644
--- a/src/snapshot/read-only-deserializer.cc
+++ b/src/snapshot/read-only-deserializer.cc
@@ -144,12 +144,12 @@ void ReadOnlyDeserializer::PostProcessNewObjectsIfStaticRootsEnabled() {
 
     if (InstanceTypeChecker::IsCode(instance_type)) {
       Code code = Code::cast(object);
-      code.init_code_entry_point(main_thread_isolate(), kNullAddress);
+      code.init_instruction_start(main_thread_isolate(), kNullAddress);
       // RO space only contains builtin Code objects which don't have an
       // attached InstructionStream.
       DCHECK(code.is_builtin());
       DCHECK(!code.has_instruction_stream());
-      code.SetEntryPointForOffHeapBuiltin(
+      code.SetInstructionStartForOffHeapBuiltin(
           main_thread_isolate(), EmbeddedData::FromBlob(main_thread_isolate())
                                      .InstructionStartOf(code.builtin_id()));
     }
diff --git a/src/snapshot/read-only-serializer.cc b/src/snapshot/read-only-serializer.cc
index c9540a09ee..a57dec7cc6 100644
--- a/src/snapshot/read-only-serializer.cc
+++ b/src/snapshot/read-only-serializer.cc
@@ -166,8 +166,8 @@ void ReadOnlySerializer::SerializeReadOnlyRoots() {
   // TODO(v8:13840, olivf): Integrate this into the ReadOnlyHeapImageSerializer
   // without actually having to wipe.
   isolate()->heap()->read_only_space()->Unseal();
-  CodeEntryPointVector saved_entry_points;
-  WipeCodeEntryPointsForDeterministicSerialization(saved_entry_points);
+  InstructionStartVector saved_entry_points;
+  WipeInstructionStartsForDeterministicSerialization(saved_entry_points);
 
   // WasmNull's payload is aligned to the OS page and consists of
   // WasmNull::kPayloadSize bytes of unmapped memory. To avoid inflating the
@@ -189,7 +189,7 @@ void ReadOnlySerializer::SerializeReadOnlyRoots() {
       isolate()->read_only_heap()->read_only_space(), sink_, isolate(),
       unmapped);
 
-  RestoreCodeEntryPoints(saved_entry_points);
+  RestoreInstructionStarts(saved_entry_points);
   isolate()->heap()->read_only_space()->Seal(
       ReadOnlySpace::SealMode::kDoNotDetachFromHeap);
 
@@ -210,28 +210,29 @@ void ReadOnlySerializer::SerializeReadOnlyRoots() {
 }
 
 #ifdef V8_STATIC_ROOTS
-void ReadOnlySerializer::WipeCodeEntryPointsForDeterministicSerialization(
-    ReadOnlySerializer::CodeEntryPointVector& saved_entry_points) {
+void ReadOnlySerializer::WipeInstructionStartsForDeterministicSerialization(
+    ReadOnlySerializer::InstructionStartVector& saved_entry_points) {
   // See also ObjectSerializer::OutputRawData.
   ReadOnlyHeapObjectIterator iterator(isolate()->read_only_heap());
   for (HeapObject object = iterator.Next(); !object.is_null();
        object = iterator.Next()) {
     if (!object.IsCode()) continue;
     Code code = Code::cast(object);
-    saved_entry_points.push_back(code.code_entry_point());
-    code.SetCodeEntryPointForSerialization(isolate(), kNullAddress);
+    saved_entry_points.push_back(code.instruction_start());
+    code.SetInstructionStartForSerialization(isolate(), kNullAddress);
   }
 }
 
-void ReadOnlySerializer::RestoreCodeEntryPoints(
-    const ReadOnlySerializer::CodeEntryPointVector& saved_entry_points) {
+void ReadOnlySerializer::RestoreInstructionStarts(
+    const ReadOnlySerializer::InstructionStartVector& saved_entry_points) {
   int i = 0;
   ReadOnlyHeapObjectIterator iterator(isolate()->read_only_heap());
   for (HeapObject object = iterator.Next(); !object.is_null();
        object = iterator.Next()) {
     if (!object.IsCode()) continue;
     Code code = Code::cast(object);
-    code.SetCodeEntryPointForSerialization(isolate(), saved_entry_points[i++]);
+    code.SetInstructionStartForSerialization(isolate(),
+                                             saved_entry_points[i++]);
   }
 }
 #endif  // V8_STATIC_ROOTS
diff --git a/src/snapshot/read-only-serializer.h b/src/snapshot/read-only-serializer.h
index 4e5062c7da..72e3ef8abb 100644
--- a/src/snapshot/read-only-serializer.h
+++ b/src/snapshot/read-only-serializer.h
@@ -43,10 +43,11 @@ class V8_EXPORT_PRIVATE ReadOnlySerializer : public RootsSerializer {
                                          Handle<HeapObject> obj);
 
 #ifdef V8_STATIC_ROOTS
-  using CodeEntryPointVector = std::vector<Address>;
-  void WipeCodeEntryPointsForDeterministicSerialization(
-      CodeEntryPointVector& saved_entry_points);
-  void RestoreCodeEntryPoints(const CodeEntryPointVector& saved_entry_points);
+  using InstructionStartVector = std::vector<Address>;
+  void WipeInstructionStartsForDeterministicSerialization(
+      InstructionStartVector& saved_entry_points);
+  void RestoreInstructionStarts(
+      const InstructionStartVector& saved_entry_points);
 #endif  // V8_STATIC_ROOTS
 
 #ifdef DEBUG
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index 9f5178a455..c76b0ad15d 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -420,16 +420,16 @@ void Serializer::InitializeCodeAddressMap() {
   code_address_map_ = std::make_unique<CodeAddressMap>(isolate_);
 }
 
-InstructionStream Serializer::CopyCode(InstructionStream code) {
+InstructionStream Serializer::CopyCode(InstructionStream istream) {
   code_buffer_.clear();  // Clear buffer without deleting backing store.
   // Add InstructionStream padding which is usually added by the allocator.
   // While this doesn't guarantee the exact same alignment, it's enough to
   // fulfill the alignment requirements of writes during relocation.
   code_buffer_.resize(InstructionStream::kCodeAlignmentMinusCodeHeader);
-  int size = code.CodeSize();
+  int size = istream.Size();
   code_buffer_.insert(code_buffer_.end(),
-                      reinterpret_cast<byte*>(code.address()),
-                      reinterpret_cast<byte*>(code.address() + size));
+                      reinterpret_cast<byte*>(istream.address()),
+                      reinterpret_cast<byte*>(istream.address() + size));
   // When pointer compression is enabled the checked cast will try to
   // decompress map field of off-heap InstructionStream object.
   return InstructionStream::unchecked_cast(
@@ -1108,7 +1108,7 @@ void Serializer::ObjectSerializer::VisitExternalReference(RelocInfo* rinfo) {
 }
 
 void Serializer::ObjectSerializer::VisitInternalReference(RelocInfo* rinfo) {
-  Address entry = rinfo->code().InstructionStart();
+  Address entry = rinfo->code().instruction_start();
   DCHECK_GE(rinfo->target_internal_reference(), entry);
   uintptr_t target_offset = rinfo->target_internal_reference() - entry;
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
@@ -1250,13 +1250,13 @@ void Serializer::ObjectSerializer::OutputRawData(Address up_to) {
                                sizeof(field_value),
                                reinterpret_cast<const byte*>(&field_value));
     } else if (object_->IsCode(cage_base)) {
-      // code_entry_point field contains a raw value that will be recomputed
+      // instruction_start field contains a raw value that will be recomputed
       // after deserialization, so write zeros to keep the snapshot
       // deterministic.
       static byte field_value[kSystemPointerSize] = {0};
       OutputRawWithCustomField(sink_, object_start, base, bytes_to_output,
-                               Code::kCodeEntryPointOffset, sizeof(field_value),
-                               field_value);
+                               Code::kInstructionStartOffset,
+                               sizeof(field_value), field_value);
     } else if (object_->IsSeqString()) {
       // SeqStrings may contain padding. Serialize the padding bytes as 0s to
       // make the snapshot content deterministic.
diff --git a/src/snapshot/serializer.h b/src/snapshot/serializer.h
index 86a8cec8bd..94fa37ef28 100644
--- a/src/snapshot/serializer.h
+++ b/src/snapshot/serializer.h
@@ -279,7 +279,7 @@ class Serializer : public SerializerDeserializer {
   // of the serializer.  Initialize it on demand.
   void InitializeCodeAddressMap();
 
-  InstructionStream CopyCode(InstructionStream code);
+  InstructionStream CopyCode(InstructionStream istream);
 
   void QueueDeferredObject(HeapObject obj) {
     DCHECK_NULL(reference_map_.LookupReference(obj));
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index f0887de7d2..52c7e5dcde 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -7822,7 +7822,7 @@ class LiftoffCompiler {
       __ LoadTaggedPointer(
           target.gp(), func_ref.gp(), no_reg,
           wasm::ObjectAccess::ToTagged(WasmInternalFunction::kCodeOffset));
-      __ LoadCodeEntry(target.gp(), target.gp());
+      __ LoadCodeInstructionStart(target.gp(), target.gp());
       // Fall through to {perform_call}.
 
       __ bind(&perform_call);
diff --git a/src/wasm/wasm-code-manager.cc b/src/wasm/wasm-code-manager.cc
index 68977ba8cf..419b0af6bf 100644
--- a/src/wasm/wasm-code-manager.cc
+++ b/src/wasm/wasm-code-manager.cc
@@ -932,7 +932,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
 
   // Metadata offsets in InstructionStream objects are relative to the start of
   // the metadata section, whereas WasmCode expects offsets relative to
-  // InstructionStart.
+  // instruction_start.
   const int base_offset = code->instruction_size();
   // TODO(jgruber,v8:8758): Remove this translation. It exists only because
   // InstructionStream objects contains real offsets but WasmCode expects an
@@ -951,7 +951,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
 
   // Apply the relocation delta by iterating over the RelocInfo.
   intptr_t delta = reinterpret_cast<Address>(dst_code_bytes.begin()) -
-                   code->InstructionStart();
+                   code->instruction_start();
   int mode_mask =
       RelocInfo::kApplyMask | RelocInfo::ModeMask(RelocInfo::WASM_STUB_CALL);
   auto jump_tables_ref =
diff --git a/test/cctest/compiler/codegen-tester.h b/test/cctest/compiler/codegen-tester.h
index 219650def3..58b8e1c10d 100644
--- a/test/cctest/compiler/codegen-tester.h
+++ b/test/cctest/compiler/codegen-tester.h
@@ -87,7 +87,7 @@ class RawMachineAssemblerTester : public HandleAndZoneScope,
           &info, main_isolate(), call_descriptor(), graph(),
           AssemblerOptions::Default(main_isolate()), schedule);
     }
-    return code_.ToHandleChecked()->code_entry_point();
+    return code_.ToHandleChecked()->instruction_start();
   }
 
  private:
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index 7d86c2e359..cb195ad12e 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -1664,7 +1664,7 @@ TEST(Regress_1171759) {
           .ToHandleChecked();
 
   std::shared_ptr<wasm::NativeModule> module =
-      AllocateNativeModule(handles.main_isolate(), code->InstructionSize());
+      AllocateNativeModule(handles.main_isolate(), code->instruction_size());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
   byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
 
diff --git a/test/cctest/test-api.cc b/test/cctest/test-api.cc
index 1f0c5d849d..f973150f1f 100644
--- a/test/cctest/test-api.cc
+++ b/test/cctest/test-api.cc
@@ -26999,19 +26999,20 @@ TEST(GetJSEntryStubs) {
   v8::JSEntryStubs entry_stubs = isolate->GetJSEntryStubs();
 
   v8::JSEntryStub entry_stub = entry_stubs.js_entry_stub;
-  CHECK_EQ(i_isolate->builtins()->code(i::Builtin::kJSEntry).InstructionStart(),
-           reinterpret_cast<i::Address>(entry_stub.code.start));
+  CHECK_EQ(
+      i_isolate->builtins()->code(i::Builtin::kJSEntry).instruction_start(),
+      reinterpret_cast<i::Address>(entry_stub.code.start));
 
   v8::JSEntryStub construct_stub = entry_stubs.js_construct_entry_stub;
   CHECK_EQ(i_isolate->builtins()
                ->code(i::Builtin::kJSConstructEntry)
-               .InstructionStart(),
+               .instruction_start(),
            reinterpret_cast<i::Address>(construct_stub.code.start));
 
   v8::JSEntryStub microtask_stub = entry_stubs.js_run_microtasks_entry_stub;
   CHECK_EQ(i_isolate->builtins()
                ->code(i::Builtin::kJSRunMicrotasksEntry)
-               .InstructionStart(),
+               .instruction_start(),
            reinterpret_cast<i::Address>(microtask_stub.code.start));
 }
 
diff --git a/test/cctest/test-assembler-arm64.cc b/test/cctest/test-assembler-arm64.cc
index 829b124fd6..b22722507e 100644
--- a/test/cctest/test-assembler-arm64.cc
+++ b/test/cctest/test-assembler-arm64.cc
@@ -167,7 +167,7 @@ static void InitializeVM() {
   START_AFTER_RESET();
 
 #define RUN() \
-  simulator.RunFrom(reinterpret_cast<Instruction*>(code->code_entry_point()))
+  simulator.RunFrom(reinterpret_cast<Instruction*>(code->instruction_start()))
 
 #define END()                                                                  \
   __ Debug("End test.", __LINE__, TRACE_DISABLE | LOG_ALL);                    \
@@ -6789,7 +6789,7 @@ static void LdrLiteralRangeHelper(
   END();
 
   if (outcome == EmitExpected) {
-    Address pool_start = code->InstructionStart() + pc_offset_before_emission;
+    Address pool_start = code->instruction_start() + pc_offset_before_emission;
     Instruction* branch = reinterpret_cast<Instruction*>(pool_start);
     CHECK(branch->IsImmBranch());
     CHECK_EQ(expected_pool_size, branch->ImmPCOffset());
diff --git a/test/cctest/test-assembler-mips64.cc b/test/cctest/test-assembler-mips64.cc
index fecd5779b2..5cea9f8872 100644
--- a/test/cctest/test-assembler-mips64.cc
+++ b/test/cctest/test-assembler-mips64.cc
@@ -4731,7 +4731,7 @@ uint64_t run_aluipc(int16_t offset) {
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 
   auto f = GeneratedCode<F2>::FromCode(isolate, *code);
-  PC = (uint64_t)code->code_entry_point();  // Set the program counter.
+  PC = (uint64_t)code->instruction_start();  // Set the program counter.
 
   uint64_t res = reinterpret_cast<uint64_t>(f.Call(0, 0, 0, 0, 0));
 
@@ -4784,7 +4784,7 @@ uint64_t run_auipc(int16_t offset) {
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 
   auto f = GeneratedCode<F2>::FromCode(isolate, *code);
-  PC = (uint64_t)code->code_entry_point();  // Set the program counter.
+  PC = (uint64_t)code->instruction_start();  // Set the program counter.
 
   uint64_t res = reinterpret_cast<uint64_t>(f.Call(0, 0, 0, 0, 0));
 
@@ -5780,7 +5780,7 @@ uint64_t run_addiupc(int32_t imm19) {
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 
   auto f = GeneratedCode<F2>::FromCode(isolate, *code);
-  PC = (uint64_t)code->code_entry_point();  // Set the program counter.
+  PC = (uint64_t)code->instruction_start();  // Set the program counter.
 
   uint64_t res = reinterpret_cast<uint64_t>(f.Call(0, 0, 0, 0, 0));
 
diff --git a/test/cctest/test-disasm-regex-helper.cc b/test/cctest/test-disasm-regex-helper.cc
index 8c3b251d15..d23f7f0332 100644
--- a/test/cctest/test-disasm-regex-helper.cc
+++ b/test/cctest/test-disasm-regex-helper.cc
@@ -22,8 +22,8 @@ std::string DisassembleFunction(const char* function) {
 
   Isolate* isolate = CcTest::i_isolate();
   Handle<Code> code(f->code(), isolate);
-  Address begin = code->InstructionStart();
-  Address end = code->InstructionEnd();
+  Address begin = code->instruction_start();
+  Address end = code->instruction_end();
   std::ostringstream os;
   Disassembler::Decode(isolate, os, reinterpret_cast<byte*>(begin),
                        reinterpret_cast<byte*>(end), CodeReference(code));
diff --git a/test/cctest/test-heap-profiler.cc b/test/cctest/test-heap-profiler.cc
index d9d363eeae..c83cbb5252 100644
--- a/test/cctest/test-heap-profiler.cc
+++ b/test/cctest/test-heap-profiler.cc
@@ -2733,7 +2733,7 @@ TEST(CheckCodeNames) {
   const char* builtin_path1[] = {
       "::(GC roots)",
       "::(Builtins)",
-      "::(KeyedLoadIC_PolymorphicName builtin handle)",
+      "::(KeyedLoadIC_PolymorphicName builtin code)",
   };
   const v8::HeapGraphNode* node = GetNodeByPath(
       env->GetIsolate(), snapshot, builtin_path1, arraysize(builtin_path1));
@@ -2742,13 +2742,13 @@ TEST(CheckCodeNames) {
   const char* builtin_path2[] = {
       "::(GC roots)",
       "::(Builtins)",
-      "::(CompileLazy builtin handle)",
+      "::(CompileLazy builtin code)",
   };
   node = GetNodeByPath(env->GetIsolate(), snapshot, builtin_path2,
                        arraysize(builtin_path2));
   CHECK(node);
   v8::String::Utf8Value node_name(env->GetIsolate(), node->GetName());
-  CHECK_EQ(0, strcmp("(CompileLazy builtin handle)", *node_name));
+  CHECK_EQ(0, strcmp("(CompileLazy builtin code)", *node_name));
 }
 
 
diff --git a/test/cctest/test-sync-primitives-arm64.cc b/test/cctest/test-sync-primitives-arm64.cc
index dbba048339..e33b8bb11e 100644
--- a/test/cctest/test-sync-primitives-arm64.cc
+++ b/test/cctest/test-sync-primitives-arm64.cc
@@ -205,7 +205,7 @@ void TestInvalidateExclusiveAccess(TestData initial_data, MemoryAccess access1,
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
 
   TestData t = initial_data;
-  Simulator::current(isolate)->Call<void>(code->code_entry_point(), &t);
+  Simulator::current(isolate)->Call<void>(code->instruction_start(), &t);
   int res = Simulator::current(isolate)->wreg(0);
 
   CHECK_EQ(expected_res, res);
@@ -275,7 +275,7 @@ int ExecuteMemoryAccess(Isolate* isolate, TestData* test_data,
   masm.GetCode(isolate, &desc);
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
-  Simulator::current(isolate)->Call<void>(code->code_entry_point(), test_data);
+  Simulator::current(isolate)->Call<void>(code->instruction_start(), test_data);
   return Simulator::current(isolate)->wreg(0);
 }
 
diff --git a/test/cctest/test-unwinder-code-pages.cc b/test/cctest/test-unwinder-code-pages.cc
index 2a7533e3c4..fe295f75de 100644
--- a/test/cctest/test-unwinder-code-pages.cc
+++ b/test/cctest/test-unwinder-code-pages.cc
@@ -170,9 +170,9 @@ TEST(Unwind_BuiltinPCInMiddle_Success_CodePagesAPI) {
   // Put the current PC inside of a valid builtin.
   Code builtin = *BUILTIN_CODE(i_isolate, StringEqual);
   const uintptr_t offset = 40;
-  CHECK_LT(offset, builtin.InstructionSize());
+  CHECK_LT(offset, builtin.instruction_size());
   register_state.pc =
-      reinterpret_cast<void*>(builtin.InstructionStart() + offset);
+      reinterpret_cast<void*>(builtin.instruction_start() + offset);
 
   bool unwound = v8::Unwinder::TryUnwindV8Frames(
       entry_stubs, pages_length, code_pages, &register_state, stack_base);
@@ -226,7 +226,7 @@ TEST(Unwind_BuiltinPCAtStart_Success_CodePagesAPI) {
   // Put the current PC at the start of a valid builtin, so that we are setting
   // up the frame.
   Code builtin = *BUILTIN_CODE(i_isolate, StringEqual);
-  register_state.pc = reinterpret_cast<void*>(builtin.InstructionStart());
+  register_state.pc = reinterpret_cast<void*>(builtin.instruction_start());
 
   bool unwound = v8::Unwinder::TryUnwindV8Frames(
       entry_stubs, pages_length, code_pages, &register_state, stack_base);
@@ -306,7 +306,7 @@ TEST(Unwind_CodeObjectPCInMiddle_Success_CodePagesAPI) {
   // deopt check happens before frame setup).
   const uintptr_t offset = code.instruction_size() - 20;
   CHECK_LT(offset, code.instruction_size());
-  Address pc = code.InstructionStart() + offset;
+  Address pc = code.instruction_start() + offset;
   register_state.pc = reinterpret_cast<void*>(pc);
 
   // Get code pages from the API now that the code obejct exists and check that
@@ -456,7 +456,7 @@ TEST(Unwind_JSEntry_Fail_CodePagesAPI) {
   RegisterState register_state;
 
   Code js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
-  byte* start = reinterpret_cast<byte*>(js_entry.InstructionStart());
+  byte* start = reinterpret_cast<byte*>(js_entry.instruction_start());
   register_state.pc = start + 10;
 
   bool unwound = v8::Unwinder::TryUnwindV8Frames(
@@ -638,8 +638,8 @@ TEST(PCIsInV8_InJSEntryRange_CodePagesAPI) {
   CHECK_LE(pages_length, arraysize(code_pages));
 
   Code js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
-  byte* start = reinterpret_cast<byte*>(js_entry.InstructionStart());
-  size_t length = js_entry.InstructionSize();
+  byte* start = reinterpret_cast<byte*>(js_entry.instruction_start());
+  size_t length = js_entry.instruction_size();
 
   void* pc = start;
   CHECK(v8::Unwinder::PCIsInV8(pages_length, code_pages, pc));
@@ -677,7 +677,7 @@ TEST(PCIsInV8_LargeCodeObject_CodePagesAPI) {
 
   CHECK(i_isolate->heap()->InSpace(foo_code->instruction_stream(),
                                    CODE_LO_SPACE));
-  byte* start = reinterpret_cast<byte*>(foo_code->InstructionStart());
+  byte* start = reinterpret_cast<byte*>(foo_code->instruction_start());
 
   MemoryRange code_pages[v8::Isolate::kMinCodePagesBufferSize];
   size_t pages_length =
diff --git a/test/cctest/wasm/test-gc.cc b/test/cctest/wasm/test-gc.cc
index bddfe64e8f..40e23d0ef3 100644
--- a/test/cctest/wasm/test-gc.cc
+++ b/test/cctest/wasm/test-gc.cc
@@ -1572,8 +1572,8 @@ WASM_COMPILED_EXEC_TEST(FunctionRefs) {
       Handle<WasmInternalFunction>::cast(result_cast_reference)->external(),
       tester.isolate()));
 
-  CHECK_EQ(cast_function->code().InstructionStart(),
-           cast_function_reference->code().InstructionStart());
+  CHECK_EQ(cast_function->code().instruction_start(),
+           cast_function_reference->code().instruction_start());
 
   tester.CheckResult(test_deprecated, 1);
   tester.CheckResult(test_fail_deprecated, 0);
diff --git a/test/common/call-tester.h b/test/common/call-tester.h
index 6659dc4caf..ec05ea00d2 100644
--- a/test/common/call-tester.h
+++ b/test/common/call-tester.h
@@ -53,17 +53,18 @@ Object CallHelper<Object>::Call(Params... args) {
 template <typename T>
 class CodeRunner : public CallHelper<T> {
  public:
-  CodeRunner(Isolate* isolate, Handle<InstructionStream> code,
+  CodeRunner(Isolate* isolate, Handle<InstructionStream> istream,
              MachineSignature* csig)
-      : CallHelper<T>(isolate, csig), code_(code) {}
+      : CallHelper<T>(isolate, csig), istream_(istream) {}
   CodeRunner(Isolate* isolate, Handle<Code> code, MachineSignature* csig)
-      : CallHelper<T>(isolate, csig), code_(FromCode(*code), isolate) {}
+      : CallHelper<T>(isolate, csig),
+        istream_(code->instruction_stream(), isolate) {}
   ~CodeRunner() override = default;
 
-  Address Generate() override { return code_->entry(); }
+  Address Generate() override { return istream_->instruction_start(); }
 
  private:
-  Handle<InstructionStream> code_;
+  Handle<InstructionStream> istream_;
 };
 
 }  // namespace compiler
diff --git a/test/fuzzer/multi-return.cc b/test/fuzzer/multi-return.cc
index e54e095cb1..576b42e911 100644
--- a/test/fuzzer/multi-return.cc
+++ b/test/fuzzer/multi-return.cc
@@ -160,7 +160,7 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
   InputProvider input(data, size);
   // Create randomized descriptor.
   size_t param_count = input.NumNonZeroBytes(0, kNumTypes);
-  if (param_count > InstructionStream::kMaxArguments) return 0;
+  if (param_count > Code::kMaxArguments) return 0;
 
   size_t return_count = input.NumNonZeroBytes(param_count + 1, kNumTypes);
   if (return_count > wasm::kV8MaxWasmFunctionReturns) return 0;
@@ -246,7 +246,7 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
           .ToHandleChecked();
 
   std::shared_ptr<wasm::NativeModule> module =
-      AllocateNativeModule(i_isolate, code->InstructionSize());
+      AllocateNativeModule(i_isolate, code->instruction_size());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
   byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
   // Generate wrapper.
diff --git a/test/unittests/assembler/disasm-ia32-unittest.cc b/test/unittests/assembler/disasm-ia32-unittest.cc
index ecb4717013..17d89b1e0f 100644
--- a/test/unittests/assembler/disasm-ia32-unittest.cc
+++ b/test/unittests/assembler/disasm-ia32-unittest.cc
@@ -984,8 +984,8 @@ TEST_F(DisasmIa320Test, DisasmIa320) {
 #ifdef OBJECT_PRINT
   StdoutStream os;
   code->Print(os);
-  Address begin = code->InstructionStart();
-  Address end = code->InstructionEnd();
+  Address begin = code->instruction_start();
+  Address end = code->instruction_end();
   disasm::Disassembler::Disassemble(stdout, reinterpret_cast<byte*>(begin),
                                     reinterpret_cast<byte*>(end));
 #endif
diff --git a/test/unittests/assembler/disasm-x64-unittest.cc b/test/unittests/assembler/disasm-x64-unittest.cc
index e309fe7ebf..eeb8442b9e 100644
--- a/test/unittests/assembler/disasm-x64-unittest.cc
+++ b/test/unittests/assembler/disasm-x64-unittest.cc
@@ -305,8 +305,8 @@ TEST_F(DisasmX64Test, DisasmX64) {
 #ifdef OBJECT_PRINT
   StdoutStream os;
   code->Print(os);
-  Address begin = code->InstructionStart();
-  Address end = code->InstructionStart();
+  Address begin = code->instruction_start();
+  Address end = code->instruction_start();
   disasm::Disassembler::Disassemble(stdout, reinterpret_cast<byte*>(begin),
                                     reinterpret_cast<byte*>(end));
 #endif
diff --git a/test/unittests/assembler/macro-assembler-x64-unittest.cc b/test/unittests/assembler/macro-assembler-x64-unittest.cc
index 9924b620ee..ee70fb7b23 100644
--- a/test/unittests/assembler/macro-assembler-x64-unittest.cc
+++ b/test/unittests/assembler/macro-assembler-x64-unittest.cc
@@ -517,7 +517,7 @@ TEST_F(MacroAssemblerX64Test, EmbeddedObj) {
   code->Print(os);
 #endif
   using myF0 = Address();
-  auto f = GeneratedCode<myF0>::FromAddress(isolate, code->code_entry_point());
+  auto f = GeneratedCode<myF0>::FromAddress(isolate, code->instruction_start());
   Object result = Object(f.Call());
   CHECK_EQ(old_array->ptr(), result.ptr());
 
diff --git a/test/unittests/codegen/code-layout-unittest.cc b/test/unittests/codegen/code-layout-unittest.cc
index 40dbdea5dc..d59b0cb699 100644
--- a/test/unittests/codegen/code-layout-unittest.cc
+++ b/test/unittests/codegen/code-layout-unittest.cc
@@ -47,11 +47,12 @@ TEST_F(CodeLayoutTest, CodeLayoutWithoutUnwindingInfo) {
           .Build();
 
   CHECK(!code->has_unwinding_info());
-  CHECK_EQ(code->InstructionSize(), buffer_size);
-  CHECK_EQ(0, memcmp(reinterpret_cast<void*>(code->InstructionStart()), buffer,
+  CHECK_EQ(code->instruction_size(), buffer_size);
+  CHECK_EQ(0, memcmp(reinterpret_cast<void*>(code->instruction_start()), buffer,
                      buffer_size));
-  CHECK_EQ(static_cast<int>(code->InstructionEnd() - code->InstructionStart()),
-           buffer_size);
+  CHECK_EQ(
+      static_cast<int>(code->instruction_end() - code->instruction_start()),
+      buffer_size);
 }
 
 TEST_F(CodeLayoutTest, CodeLayoutWithUnwindingInfo) {
@@ -94,14 +95,14 @@ TEST_F(CodeLayoutTest, CodeLayoutWithUnwindingInfo) {
 
   CHECK(code->has_unwinding_info());
   CHECK_EQ(code->body_size(), buffer_size + unwinding_info_size);
-  CHECK_EQ(0, memcmp(reinterpret_cast<void*>(code->InstructionStart()), buffer,
+  CHECK_EQ(0, memcmp(reinterpret_cast<void*>(code->instruction_start()), buffer,
                      buffer_size));
   CHECK_EQ(code->unwinding_info_size(), unwinding_info_size);
   CHECK_EQ(memcmp(reinterpret_cast<void*>(code->unwinding_info_start()),
                   unwinding_info, unwinding_info_size),
            0);
   CHECK_EQ(
-      static_cast<int>(code->unwinding_info_end() - code->InstructionStart()),
+      static_cast<int>(code->unwinding_info_end() - code->instruction_start()),
       buffer_size + unwinding_info_size);
 }
 
diff --git a/test/unittests/codegen/code-pages-unittest.cc b/test/unittests/codegen/code-pages-unittest.cc
index e8a156581e..6c1c458bc3 100644
--- a/test/unittests/codegen/code-pages-unittest.cc
+++ b/test/unittests/codegen/code-pages-unittest.cc
@@ -153,7 +153,7 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodeRange) {
   // We don't produce optimized code when run with --no-turbofan and
   // --no-maglev.
   if (!code.is_optimized_code()) return;
-  InstructionStream foo_code = FromCode(code);
+  InstructionStream foo_code = code.instruction_stream();
 
   EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
@@ -203,7 +203,7 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodePages) {
       // We don't produce optimized code when run with --no-turbofan and
       // --no-maglev.
       if (!code.is_optimized_code()) return;
-      InstructionStream foo_code = FromCode(code);
+      InstructionStream foo_code = code.instruction_stream();
 
       EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
diff --git a/test/unittests/compiler/codegen-tester.h b/test/unittests/compiler/codegen-tester.h
index 146d9907f7..94caad0b8b 100644
--- a/test/unittests/compiler/codegen-tester.h
+++ b/test/unittests/compiler/codegen-tester.h
@@ -86,7 +86,7 @@ class RawMachineAssemblerTester : public CallHelper<ReturnType>,
           &info, isolate_, call_descriptor(), graph(),
           AssemblerOptions::Default(isolate_), schedule);
     }
-    return code_.ToHandleChecked()->code_entry_point();
+    return code_.ToHandleChecked()->instruction_start();
   }
 
   Zone* zone() { return zone_; }
diff --git a/test/unittests/logging/log-unittest.cc b/test/unittests/logging/log-unittest.cc
index 6e0c8480ec..3c17eebcf2 100644
--- a/test/unittests/logging/log-unittest.cc
+++ b/test/unittests/logging/log-unittest.cc
@@ -1224,12 +1224,14 @@ TEST_F(LogTest, BuiltinsNotLoggedAsLazyCompile) {
 
     // Should only be logged as "Builtin" with a name, never as "Function".
     v8::base::SNPrintF(buffer, ",0x%" V8PRIxPTR ",%d,BooleanConstructor",
-                       builtin->InstructionStart(), builtin->InstructionSize());
+                       builtin->instruction_start(),
+                       builtin->instruction_size());
     CHECK(logger.ContainsLine(
         {"code-creation,Builtin,2,", std::string(buffer.begin())}));
 
     v8::base::SNPrintF(buffer, ",0x%" V8PRIxPTR ",%d,",
-                       builtin->InstructionStart(), builtin->InstructionSize());
+                       builtin->instruction_start(),
+                       builtin->instruction_size());
     CHECK(!logger.ContainsLine(
         {"code-creation,JS,2,", std::string(buffer.begin())}));
   }
diff --git a/test/unittests/parser/parsing-unittest.cc b/test/unittests/parser/parsing-unittest.cc
index 9048d7f1fa..a7ce2f6792 100644
--- a/test/unittests/parser/parsing-unittest.cc
+++ b/test/unittests/parser/parsing-unittest.cc
@@ -2981,12 +2981,12 @@ TEST_F(ParsingTest, TooManyArguments) {
   const char* context_data[][2] = {{"foo(", "0)"}, {nullptr, nullptr}};
 
   using v8::internal::InstructionStream;
-  char statement[InstructionStream::kMaxArguments * 2 + 1];
-  for (int i = 0; i < InstructionStream::kMaxArguments; ++i) {
+  char statement[Code::kMaxArguments * 2 + 1];
+  for (int i = 0; i < Code::kMaxArguments; ++i) {
     statement[2 * i] = '0';
     statement[2 * i + 1] = ',';
   }
-  statement[InstructionStream::kMaxArguments * 2] = 0;
+  statement[Code::kMaxArguments * 2] = 0;
 
   const char* statement_data[] = {statement, nullptr};
 
@@ -9061,7 +9061,7 @@ TEST_F(ParsingTest, ObjectRestNegativeTestSlow) {
 
   using v8::internal::InstructionStream;
   std::string statement;
-  for (int i = 0; i < InstructionStream::kMaxArguments; ++i) {
+  for (int i = 0; i < Code::kMaxArguments; ++i) {
     statement += std::to_string(i) + " : " + "x, ";
   }
   statement += "...y";
-- 
2.35.1

