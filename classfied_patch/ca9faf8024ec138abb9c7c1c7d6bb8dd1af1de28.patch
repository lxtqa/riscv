From ca9faf8024ec138abb9c7c1c7d6bb8dd1af1de28 Mon Sep 17 00:00:00 2001
From: Stephen Roettger <sroettger@google.com>
Date: Thu, 16 Mar 2023 11:30:04 +0100
Subject: [PATCH] Reland "Move data fields from InstructionStream to Code"
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This is a reland of commit ff909db74eefcf68656e190bd6e3fc0edc0f7790

Fix: update the padding size for arm64 without ptr compression

"no-try: true" because of infra issues (see original description)

Original change's description:
> Move data fields from InstructionStream to Code
>
> and use the Code object directly wherever possible, effectively treating
> the Code+IStream tuple as a single object.
>
> This cl includes:
> * moving nearly all data fields from IStream to Code
> * passing around/storing the code object instead of istream in various places
> * remove unnecessary loads (e.g. in the arch specific code, we don't need to load the istream anymore)
> * add a code member to reloc_info besides istream
> * add a RelocIterator constructor that takes code + istream (during serialization / gc, the ptrs between are not always up to date)
> * remove InstructionStream from various visit* functions since it's passed in RelocInfo anyway
>
> Submitting with `No-Try: true` due to infra issues with linux-rel. (crbug.com/1423329#c12). I tested linux-rel separately in crbug.com/4338946 and it passes, so the assumption is that it will also pass in the v8 autoroll tests.
>
> No-Try: true
> Cq-Include-Trybots: luci.v8.try:v8_linux64_gc_stress_custom_snapshot_dbg,v8_linux64_gc_stress_dbg,v8_linux_gc_stress_dbg,v8_mac64_gc_stress_dbg
> Bug: v8:13784
> Change-Id: I91716f0dc0059517746e271c9fe70403c20a01eb
> Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4295670
> Reviewed-by: Jakob Linke <jgruber@chromium.org>
> Reviewed-by: Clemens Backes <clemensb@chromium.org>
> Commit-Queue: Stephen Röttger <sroettger@google.com>
> Reviewed-by: Dominik Inführ <dinfuehr@chromium.org>
> Cr-Commit-Position: refs/heads/main@{#86478}

No-Try: true
Bug: v8:13784
Change-Id: I1790f47ed7720265b7be6fd3799fa0c3cde3d27d
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4341976
Reviewed-by: Clemens Backes <clemensb@chromium.org>
Commit-Queue: Stephen Röttger <sroettger@google.com>
Reviewed-by: Michael Lippautz <mlippautz@chromium.org>
Cr-Commit-Position: refs/heads/main@{#86487}
---
 src/api/api.cc                                |   7 +-
 src/builtins/arm/builtins-arm.cc              |  20 +-
 src/builtins/arm64/builtins-arm64.cc          |  20 +-
 src/builtins/builtins.cc                      |   2 +-
 src/builtins/builtins.h                       |   2 +-
 src/builtins/ia32/builtins-ia32.cc            |  20 +-
 src/builtins/setup-builtins-internal.cc       |  16 +-
 src/builtins/x64/builtins-x64.cc              |  16 +-
 src/codegen/arm/assembler-arm-inl.h           |   8 +-
 src/codegen/arm/assembler-arm.cc              |   2 +-
 src/codegen/arm/assembler-arm.h               |   2 +-
 src/codegen/arm/macro-assembler-arm.cc        |  11 +-
 src/codegen/arm/macro-assembler-arm.h         |   6 -
 src/codegen/arm64/assembler-arm64-inl.h       |   6 +-
 src/codegen/arm64/assembler-arm64.cc          |   4 +-
 src/codegen/arm64/assembler-arm64.h           |   5 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  14 +-
 src/codegen/arm64/macro-assembler-arm64.h     |   6 -
 src/codegen/code-stub-assembler.cc            |   7 +-
 src/codegen/compiler.cc                       |  15 +-
 src/codegen/external-reference.cc             |   4 +-
 src/codegen/handler-table.cc                  |   4 -
 src/codegen/ia32/assembler-ia32-inl.h         |   6 +-
 src/codegen/ia32/assembler-ia32.cc            |   2 +-
 src/codegen/ia32/assembler-ia32.h             |   2 +-
 src/codegen/ia32/macro-assembler-ia32.cc      |  10 +-
 src/codegen/ia32/macro-assembler-ia32.h       |   6 -
 src/codegen/maglev-safepoint-table.cc         |   7 -
 src/codegen/maglev-safepoint-table.h          |   2 -
 src/codegen/reloc-info.cc                     |  85 ++-
 src/codegen/reloc-info.h                      |  47 +-
 src/codegen/safepoint-table.cc                |   5 -
 src/codegen/source-position.cc                |   2 +-
 src/codegen/source-position.h                 |   2 +-
 src/codegen/x64/assembler-x64-inl.h           |   6 +-
 src/codegen/x64/assembler-x64.cc              |   2 +-
 src/codegen/x64/assembler-x64.h               |   2 +-
 src/codegen/x64/macro-assembler-x64.cc        |  12 +-
 src/codegen/x64/macro-assembler-x64.h         |   6 -
 .../backend/arm/code-generator-arm.cc         |   2 +-
 .../backend/ia32/code-generator-ia32.cc       |   2 +-
 src/compiler/heap-refs.cc                     |  22 +-
 src/compiler/heap-refs.h                      |  10 -
 src/compiler/pipeline.cc                      |   5 +-
 src/debug/debug-evaluate.cc                   |   2 +-
 src/deoptimizer/deoptimizer.cc                |  33 +-
 src/deoptimizer/deoptimizer.h                 |   8 +-
 src/diagnostics/disassembler.cc               |  18 +-
 src/diagnostics/objects-debug.cc              |  21 +-
 src/diagnostics/objects-printer.cc            |   4 -
 src/execution/frames.cc                       |   6 +-
 src/execution/isolate.cc                      |  38 +-
 src/extensions/statistics-extension.cc        |   4 +-
 src/heap/concurrent-marking.cc                |   8 +-
 src/heap/evacuation-verifier.cc               |  15 +-
 src/heap/evacuation-verifier.h                |  10 +-
 src/heap/factory-base.cc                      |  47 +-
 src/heap/factory-base.h                       |  23 +-
 src/heap/factory.cc                           | 122 ++--
 src/heap/heap-verifier.cc                     |  37 +-
 src/heap/heap-write-barrier-inl.h             |  13 +-
 src/heap/heap-write-barrier.cc                |  17 +-
 src/heap/heap-write-barrier.h                 |   5 +-
 src/heap/heap.cc                              |  69 +-
 src/heap/heap.h                               |   8 +-
 src/heap/mark-compact-inl.h                   |   5 +-
 src/heap/mark-compact.cc                      | 135 ++--
 src/heap/mark-compact.h                       |  12 +-
 src/heap/marking-barrier.cc                   |  13 +-
 src/heap/marking-barrier.h                    |   3 +-
 src/heap/marking-visitor-inl.h                |  33 +-
 src/heap/marking-visitor.h                    |  23 +-
 src/heap/object-stats.cc                      |  20 +-
 src/heap/read-only-spaces.cc                  |   7 +-
 src/heap/remembered-set-inl.h                 |  14 +-
 src/heap/scavenger-inl.h                      |  16 +-
 src/heap/scavenger.cc                         |  16 +-
 src/heap/sweeper.cc                           |  16 +-
 src/heap/weak-object-worklists.h              |   2 +-
 src/logging/code-events.h                     |  13 +-
 src/logging/log.cc                            |  19 +-
 src/logging/log.h                             |  23 +-
 src/objects/code-inl.h                        | 602 +++++-------------
 src/objects/code.cc                           |  86 +--
 src/objects/code.h                            | 529 ++++++---------
 src/objects/js-function-inl.h                 |   5 -
 src/objects/js-function.h                     |   6 -
 src/objects/js-regexp.cc                      |   4 +-
 src/objects/js-regexp.h                       |   3 +-
 src/objects/objects-body-descriptors-inl.h    |  22 +-
 src/objects/objects.cc                        |   3 +-
 src/objects/visitors.h                        |  25 +-
 src/profiler/heap-snapshot-generator.cc       |  41 +-
 src/profiler/heap-snapshot-generator.h        |   2 +-
 src/profiler/profiler-listener.cc             |  11 +-
 src/profiler/profiler-listener.h              |   9 +-
 src/regexp/arm/regexp-macro-assembler-arm.cc  |   3 +-
 .../arm64/regexp-macro-assembler-arm64.cc     |   3 +-
 .../ia32/regexp-macro-assembler-ia32.cc       |   3 +-
 src/regexp/regexp-macro-assembler.cc          |   2 +-
 src/regexp/regexp.cc                          |   9 +-
 src/regexp/x64/regexp-macro-assembler-x64.cc  |   3 +-
 src/runtime/runtime-compiler.cc               |   6 +-
 src/runtime/runtime-test.cc                   |   6 +-
 src/snapshot/code-serializer.cc               |   2 +-
 src/snapshot/deserializer.cc                  |  58 +-
 src/snapshot/embedded/embedded-data.cc        |  24 +-
 src/snapshot/embedded/embedded-file-writer.cc |   2 +-
 src/snapshot/serializer.cc                    |  81 +--
 src/snapshot/serializer.h                     |  16 +-
 src/snapshot/startup-serializer.cc            |  14 +-
 src/wasm/module-compiler.cc                   |   7 +-
 src/wasm/wasm-code-manager.cc                 |   6 +-
 src/wasm/wasm-code-manager.h                  |   2 +-
 test/cctest/compiler/codegen-tester.h         |   4 -
 test/cctest/compiler/function-tester.cc       |  15 -
 test/cctest/compiler/function-tester.h        |   2 -
 test/cctest/compiler/test-code-generator.cc   |   4 +-
 test/cctest/compiler/test-multiple-return.cc  |  24 +-
 .../cctest/heap/test-concurrent-allocation.cc |  13 +-
 test/cctest/heap/test-heap.cc                 |  21 +-
 test/cctest/test-cpu-profiler.cc              |   2 +-
 test/cctest/test-serialize.cc                 |   4 +-
 test/cctest/test-unwinder-code-pages.cc       |  21 +-
 test/fuzzer/multi-return.cc                   |   3 +-
 .../assembler/macro-assembler-x64-unittest.cc |   3 +-
 test/unittests/compiler/codegen-tester.h      |   4 -
 test/unittests/compiler/function-tester.cc    |  15 -
 test/unittests/regexp/regexp-unittest.cc      |  29 +-
 tools/gen-postmortem-metadata.py              |   7 +-
 130 files changed, 1219 insertions(+), 1867 deletions(-)

diff --git a/src/api/api.cc b/src/api/api.cc
index 2e8aba7846..05846797e9 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -6946,10 +6946,9 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
   void VisitMapPointer(i::HeapObject host) final {
     VisitPointer(host, host.map_slot());
   }
-  void VisitCodePointer(i::HeapObject host, i::CodeObjectSlot slot) final {}
-  void VisitCodeTarget(i::InstructionStream host, i::RelocInfo* rinfo) final {}
-  void VisitEmbeddedPointer(i::InstructionStream host,
-                            i::RelocInfo* rinfo) final {}
+  void VisitCodePointer(i::Code host, i::CodeObjectSlot slot) final {}
+  void VisitCodeTarget(i::RelocInfo* rinfo) final {}
+  void VisitEmbeddedPointer(i::RelocInfo* rinfo) final {}
   void VisitCustomWeakPointers(i::HeapObject host, i::ObjectSlot start,
                                i::ObjectSlot end) final {}
 
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index aecf2e5219..6a928db24b 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -1774,20 +1774,15 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ LeaveFrame(StackFrame::STUB);
   }
 
-  __ LoadCodeInstructionStreamNonBuiltin(r0, r0);
-
   // Load deoptimization data from the code object.
   // <deopt_data> = <code>[#deoptimization_data_offset]
-  __ ldr(
-      r1,
-      FieldMemOperand(
-          r0, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
+  __ ldr(r1,
+         FieldMemOperand(r0, Code::kDeoptimizationDataOrInterpreterDataOffset));
+
+  __ LoadCodeEntry(r0, r0);
 
   {
     ConstantPoolUnavailableScope constant_pool_unavailable(masm);
-    __ add(r0, r0,
-           Operand(InstructionStream::kHeaderSize -
-                   kHeapObjectTag));  // InstructionStream start
 
     // Load the OSR entrypoint offset from the deoptimization data.
     // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
@@ -3608,7 +3603,6 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, r3);
   }
-  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = r2;
@@ -3676,17 +3670,15 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
+  __ LoadCodeEntry(code_obj, code_obj);
   __ add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
     UseScratchRegisterScope temps(masm);
     ResetBytecodeAge(masm, kInterpreterBytecodeArrayRegister, temps.Acquire());
-    Generate_OSREntry(masm, code_obj,
-                      Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
+    Generate_OSREntry(masm, code_obj);
   } else {
-    __ add(code_obj, code_obj,
-           Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
     __ Jump(code_obj);
   }
   __ Trap();  // Unreachable.
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index c16655c918..b6eaf6e330 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -2005,14 +2005,11 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ LeaveFrame(StackFrame::STUB);
   }
 
-  __ LoadCodeInstructionStreamNonBuiltin(x0, x0);
-
   // Load deoptimization data from the code object.
   // <deopt_data> = <code>[#deoptimization_data_offset]
   __ LoadTaggedField(
       x1,
-      FieldMemOperand(
-          x0, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
+      FieldMemOperand(x0, Code::kDeoptimizationDataOrInterpreterDataOffset));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
@@ -2020,10 +2017,11 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       x1, FieldMemOperand(x1, FixedArray::OffsetOfElementAt(
                                   DeoptimizationData::kOsrPcOffsetIndex)));
 
-  // Compute the target address = code_obj + header_size + osr_offset
-  // <entry_addr> = <code_obj> + #header_size + <osr_offset>
-  __ Add(x0, x0, x1);
-  Generate_OSREntry(masm, x0, InstructionStream::kHeaderSize - kHeapObjectTag);
+  __ LoadCodeEntry(x0, x0);
+
+  // Compute the target address = code_entry + osr_offset
+  // <entry_addr> = <code_entry> + <osr_offset>
+  Generate_OSREntry(masm, x0, x1);
 }
 
 }  // namespace
@@ -5736,7 +5734,6 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, x3);
   }
-  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = x2;
@@ -5803,15 +5800,14 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
+  __ LoadCodeEntry(code_obj, code_obj);
   __ Add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister, padreg);
 
   if (is_osr) {
     ResetBytecodeAge(masm, kInterpreterBytecodeArrayRegister);
-    Generate_OSREntry(masm, code_obj,
-                      InstructionStream::kHeaderSize - kHeapObjectTag);
+    Generate_OSREntry(masm, code_obj);
   } else {
-    __ Add(code_obj, code_obj, InstructionStream::kHeaderSize - kHeapObjectTag);
     __ Jump(code_obj);
   }
   __ Trap();  // Unreachable.
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index 6992ffd9f3..2c84a811bc 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -283,7 +283,7 @@ Address Builtins::CppEntryOf(Builtin builtin) {
 }
 
 // static
-bool Builtins::IsBuiltin(const InstructionStream code) {
+bool Builtins::IsBuiltin(const Code code) {
   return Builtins::IsBuiltinId(code.builtin_id());
 }
 
diff --git a/src/builtins/builtins.h b/src/builtins/builtins.h
index b58147027c..91811adb8b 100644
--- a/src/builtins/builtins.h
+++ b/src/builtins/builtins.h
@@ -174,7 +174,7 @@ class Builtins {
 
   // True, iff the given code object is a builtin. Note that this does not
   // necessarily mean that its kind is InstructionStream::BUILTIN.
-  static bool IsBuiltin(const InstructionStream code);
+  static bool IsBuiltin(const Code code);
 
   // As above, but safe to access off the main thread since the check is done
   // by handle location. Similar to Heap::IsRootHandle.
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index 0e3c4fc634..bd02ab53e6 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -2741,13 +2741,9 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ leave();
   }
 
-  __ LoadCodeInstructionStreamNonBuiltin(eax, eax);
-
   // Load deoptimization data from the code object.
-  __ mov(ecx,
-         Operand(eax,
-                 InstructionStream::kDeoptimizationDataOrInterpreterDataOffset -
-                     kHeapObjectTag));
+  __ mov(ecx, Operand(eax, Code::kDeoptimizationDataOrInterpreterDataOffset -
+                               kHeapObjectTag));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   __ mov(ecx, Operand(ecx, FixedArray::OffsetOfElementAt(
@@ -2755,9 +2751,10 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                kHeapObjectTag));
   __ SmiUntag(ecx);
 
-  // Compute the target address = code_obj + header_size + osr_offset
-  __ lea(eax, Operand(eax, ecx, times_1,
-                      InstructionStream::kHeaderSize - kHeapObjectTag));
+  __ LoadCodeEntry(eax, eax);
+
+  // Compute the target address = code_entry + osr_offset
+  __ add(eax, ecx);
 
   Generate_OSREntry(masm, eax);
 }
@@ -4250,7 +4247,6 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, ecx);
   }
-  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = ecx;
@@ -4316,8 +4312,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
            kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ lea(code_obj, FieldOperand(code_obj, kReturnRegister0, times_1,
-                                InstructionStream::kHeaderSize));
+  __ LoadCodeEntry(code_obj, code_obj);
+  __ add(code_obj, kReturnRegister0);
   __ pop(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index 11d08c8391..ca7b4ca57c 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -243,19 +243,19 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
   PtrComprCageBase cage_base(isolate);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    InstructionStream code = FromCode(builtins->code(builtin));
+    Code code = builtins->code(builtin);
     isolate->heap()->UnprotectAndRegisterMemoryChunk(
         code, UnprotectMemoryOrigin::kMainThread);
     bool flush_icache = false;
     for (RelocIterator it(code, kRelocMask); !it.done(); it.next()) {
       RelocInfo* rinfo = it.rinfo();
       if (RelocInfo::IsCodeTargetMode(rinfo->rmode())) {
-        InstructionStream target =
-            InstructionStream::FromTargetAddress(rinfo->target_address());
-        DCHECK_IMPLIES(RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
-                       Builtins::IsIsolateIndependent(target.builtin_id()));
-        if (!target.is_builtin()) continue;
-        Code new_target = builtins->code(target.builtin_id());
+        Code target_code = Code::FromTargetAddress(rinfo->target_address());
+        DCHECK_IMPLIES(
+            RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
+            Builtins::IsIsolateIndependent(target_code.builtin_id()));
+        if (!target_code.is_builtin()) continue;
+        Code new_target = builtins->code(target_code.builtin_id());
         rinfo->set_target_address(new_target.InstructionStart(),
                                   UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       } else {
@@ -271,7 +271,7 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
       flush_icache = true;
     }
     if (flush_icache) {
-      FlushInstructionCache(code.instruction_start(), code.instruction_size());
+      FlushInstructionCache(code.InstructionStart(), code.instruction_size());
     }
   }
 }
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index cc06b15938..846e3eac9b 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -2686,14 +2686,11 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ leave();
   }
 
-  __ LoadCodeInstructionStreamNonBuiltin(rax, rax);
-
   // Load deoptimization data from the code object.
   const TaggedRegister deopt_data(rbx);
   __ LoadTaggedField(
       deopt_data,
-      FieldOperand(
-          rax, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
+      FieldOperand(rax, Code::kDeoptimizationDataOrInterpreterDataOffset));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   __ SmiUntagField(
@@ -2701,8 +2698,10 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       FieldOperand(deopt_data, FixedArray::OffsetOfElementAt(
                                    DeoptimizationData::kOsrPcOffsetIndex)));
 
-  // Compute the target address = code_obj + header_size + osr_offset
-  __ leaq(rax, FieldOperand(rax, rbx, times_1, InstructionStream::kHeaderSize));
+  __ LoadCodeEntry(rax, rax);
+
+  // Compute the target address = code_entry + osr_offset
+  __ addq(rax, rbx);
 
   Generate_OSREntry(masm, rax);
 }
@@ -5181,7 +5180,6 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, r11);
   }
-  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = r11;
@@ -5248,8 +5246,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ movq(arg_reg_3, kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ leaq(code_obj, FieldOperand(code_obj, kReturnRegister0, times_1,
-                                 InstructionStream::kHeaderSize));
+  __ LoadCodeEntry(code_obj, code_obj);
+  __ addq(code_obj, kReturnRegister0);
   __ popq(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
diff --git a/src/codegen/arm/assembler-arm-inl.h b/src/codegen/arm/assembler-arm-inl.h
index 7b912318a8..0a0880db06 100644
--- a/src/codegen/arm/assembler-arm-inl.h
+++ b/src/codegen/arm/assembler-arm-inl.h
@@ -112,8 +112,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                    icache_flush_mode);
-  if (!host().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(host(), this, target, write_barrier_mode);
+  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
   }
 }
 
@@ -191,8 +191,8 @@ void Assembler::emit(Instr x) {
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address constant_pool_entry, InstructionStream code, Address target) {
-  DCHECK(!Builtins::IsIsolateIndependentBuiltin(code.code(kAcquireLoad)));
+    Address constant_pool_entry, Code code, Address target) {
+  DCHECK(!Builtins::IsIsolateIndependentBuiltin(code));
   Memory<Address>(constant_pool_entry) = target;
 }
 
diff --git a/src/codegen/arm/assembler-arm.cc b/src/codegen/arm/assembler-arm.cc
index 8e4e1171e9..179c309acb 100644
--- a/src/codegen/arm/assembler-arm.cc
+++ b/src/codegen/arm/assembler-arm.cc
@@ -5254,7 +5254,7 @@ void Assembler::dq(uint64_t value, RelocInfo::Mode rmode) {
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
   DCHECK_GE(buffer_space(), kMaxRelocSize);  // too late to grow buffer here
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/arm/assembler-arm.h b/src/codegen/arm/assembler-arm.h
index cd68628b24..9444abea18 100644
--- a/src/codegen/arm/assembler-arm.h
+++ b/src/codegen/arm/assembler-arm.h
@@ -367,7 +367,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the constant pool on ARM).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address constant_pool_entry, InstructionStream code, Address target);
+      Address constant_pool_entry, Code code, Address target);
 
   // Get the size of the special target encoded at 'location'.
   inline static int deserialization_special_target_size(Address location);
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index 55958db53b..9be1d37e03 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -344,15 +344,6 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
-void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                                         Register code_object) {
-  ASM_CODE_COMMENT(this);
-  // Compute the InstructionStream object pointer from the code entry point.
-  ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
-  sub(destination, destination,
-      Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
-}
-
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadCodeEntry(code_object, code_object);
@@ -399,7 +390,7 @@ void MacroAssembler::Drop(Register count, Condition cond) {
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code,
                                                        Register scratch) {
   ldr(scratch, FieldMemOperand(code, Code::kKindSpecificFlagsOffset));
-  tst(scratch, Operand(1 << InstructionStream::kMarkedForDeoptimizationBit));
+  tst(scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
 }
 
 Operand MacroAssembler::ClearedValue() const {
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 53b2406e3f..971b21661b 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -336,12 +336,6 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
-  // Load code entry point from the Code object and compute
-  // InstructionStream object pointer out of it. Must not be used for
-  // Codes corresponding to builtins, because their entry points
-  // values point to the embedded instruction stream in .text section.
-  void LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index d6801ac7f3..d564d06274 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -548,7 +548,7 @@ int Assembler::deserialization_special_target_size(Address location) {
 }
 
 void Assembler::deserialization_set_special_target_at(Address location,
-                                                      InstructionStream code,
+                                                      Code code,
                                                       Address target) {
   Instruction* instr = reinterpret_cast<Instruction*>(location);
   if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {
@@ -696,8 +696,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(host(), this, target, write_barrier_mode);
+  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
   }
 }
 
diff --git a/src/codegen/arm64/assembler-arm64.cc b/src/codegen/arm64/assembler-arm64.cc
index fbaa2ad36e..f753e0bcc8 100644
--- a/src/codegen/arm64/assembler-arm64.cc
+++ b/src/codegen/arm64/assembler-arm64.cc
@@ -4533,7 +4533,7 @@ void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data,
   DCHECK(constpool_.IsBlocked());
 
   // We do not try to reuse pool constants.
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
                   InstructionStream());
 
   DCHECK_GE(buffer_space(), kMaxRelocSize);  // too late to grow buffer here
@@ -4660,7 +4660,7 @@ intptr_t Assembler::MaxPCOffsetAfterVeneerPoolIfEmittedNow(size_t margin) {
 void Assembler::RecordVeneerPool(int location_offset, int size) {
   Assembler::BlockPoolsScope block_pools(this, PoolEmissionCheck::kSkip);
   RelocInfo rinfo(reinterpret_cast<Address>(buffer_start_) + location_offset,
-                  RelocInfo::VENEER_POOL, static_cast<intptr_t>(size),
+                  RelocInfo::VENEER_POOL, static_cast<intptr_t>(size), Code(),
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/arm64/assembler-arm64.h b/src/codegen/arm64/assembler-arm64.h
index 431a1abba7..5c10dd8697 100644
--- a/src/codegen/arm64/assembler-arm64.h
+++ b/src/codegen/arm64/assembler-arm64.h
@@ -277,8 +277,9 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination. 'location' here can be either the pc of
   // an immediate branch or the address of an entry in the constant pool.
   // This is for calls and branches within generated code.
-  inline static void deserialization_set_special_target_at(
-      Address location, InstructionStream code, Address target);
+  inline static void deserialization_set_special_target_at(Address location,
+                                                           Code code,
+                                                           Address target);
 
   // Get the size of the special target encoded at 'location'.
   inline static int deserialization_special_target_size(Address location);
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index b79bf1a433..ee7f5a37be 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -2372,15 +2372,6 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   Ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
-void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                                         Register code_object) {
-  ASM_CODE_COMMENT(this);
-  // Compute the InstructionStream object pointer from the code entry point.
-  Ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
-  Sub(destination, destination,
-      Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
-}
-
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadCodeEntry(code_object, code_object);
@@ -2460,8 +2451,7 @@ void MacroAssembler::BailoutIfDeoptimized() {
                   MemOperand(kJavaScriptCallCodeStartRegister, offset));
   Ldr(scratch.W(), FieldMemOperand(scratch, Code::kKindSpecificFlagsOffset));
   Label not_deoptimized;
-  Tbz(scratch.W(), InstructionStream::kMarkedForDeoptimizationBit,
-      &not_deoptimized);
+  Tbz(scratch.W(), Code::kMarkedForDeoptimizationBit, &not_deoptimized);
   Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
        RelocInfo::CODE_TARGET);
   Bind(&not_deoptimized);
@@ -2695,7 +2685,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
 void MacroAssembler::JumpIfCodeIsMarkedForDeoptimization(
     Register code, Register scratch, Label* if_marked_for_deoptimization) {
   Ldr(scratch.W(), FieldMemOperand(code, Code::kKindSpecificFlagsOffset));
-  Tbnz(scratch.W(), InstructionStream::kMarkedForDeoptimizationBit,
+  Tbnz(scratch.W(), Code::kMarkedForDeoptimizationBit,
        if_marked_for_deoptimization);
 }
 
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 7ccfa59818..9fc898ad12 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1050,12 +1050,6 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 
   // Load code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
-  // Load code entry point from the Code object and compute
-  // InstructionStream object pointer out of it. Must not be used for
-  // Codes corresponding to builtins, because their entry points
-  // values point to the embedded instruction stream in .text section.
-  void LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 11a9934ee1..75ffdee22d 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -3179,8 +3179,7 @@ TNode<BytecodeArray> CodeStubAssembler::LoadSharedFunctionInfoBytecodeArray(
                           Int32Constant(static_cast<int>(CodeKind::BASELINE))));
 #endif  // DEBUG
     TNode<HeapObject> baseline_data = LoadObjectField<HeapObject>(
-        FromCodeNonBuiltin(code),
-        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
+        code, Code::kDeoptimizationDataOrInterpreterDataOffset);
     var_result = baseline_data;
   }
   Goto(&check_for_interpreter_data);
@@ -15687,8 +15686,8 @@ TNode<RawPtrT> CodeStubAssembler::GetCodeEntry(TNode<Code> code) {
 }
 
 TNode<BoolT> CodeStubAssembler::IsMarkedForDeoptimization(TNode<Code> code) {
-  return IsSetWord32<InstructionStream::MarkedForDeoptimizationField>(
-      LoadObjectField<Int32T>(code, Code::kKindSpecificFlagsOffset));
+  return IsSetWord32<Code::MarkedForDeoptimizationField>(
+      LoadObjectField<Int16T>(code, Code::kKindSpecificFlagsOffset));
 }
 
 TNode<JSFunction> CodeStubAssembler::AllocateFunctionWithMapAndContext(
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index d33fa41890..22f743a4a4 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -1731,18 +1731,13 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   }
 
   // The object graph for a newly compiled Script shouldn't yet contain any
-  // InstructionStream. If any of these functions are called, then that would
-  // indicate that the graph was not disjoint from the rest of the heap as
-  // expected.
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
-    UNREACHABLE();
-  }
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    UNREACHABLE();
-  }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  // Code. If any of these functions are called, then that would indicate that
+  // the graph was not disjoint from the rest of the heap as expected.
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     UNREACHABLE();
   }
+  void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
  private:
   enum ObjectKind {
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 27e860dbfc..5b9981e923 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -748,7 +748,7 @@ namespace {
 static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
                                              int bytecode_offset,
                                              Address raw_bytecode_array) {
-  InstructionStream code_obj = InstructionStream::cast(Object(raw_code_obj));
+  Code code_obj = Code::cast(Object(raw_code_obj));
   BytecodeArray bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj.GetBaselineStartPCForBytecodeOffset(bytecode_offset,
@@ -758,7 +758,7 @@ static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
 static uintptr_t BaselinePCForNextExecutedBytecode(Address raw_code_obj,
                                                    int bytecode_offset,
                                                    Address raw_bytecode_array) {
-  InstructionStream code_obj = InstructionStream::cast(Object(raw_code_obj));
+  Code code_obj = Code::cast(Object(raw_code_obj));
   BytecodeArray bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj.GetBaselinePCForNextExecutedBytecode(bytecode_offset,
diff --git a/src/codegen/handler-table.cc b/src/codegen/handler-table.cc
index f2147a171b..a00f9113ff 100644
--- a/src/codegen/handler-table.cc
+++ b/src/codegen/handler-table.cc
@@ -19,10 +19,6 @@
 namespace v8 {
 namespace internal {
 
-HandlerTable::HandlerTable(InstructionStream code)
-    : HandlerTable(code.handler_table_address(), code.handler_table_size(),
-                   kReturnAddressBasedEncoding) {}
-
 HandlerTable::HandlerTable(Code code)
     : HandlerTable(code.HandlerTableAddress(), code.handler_table_size(),
                    kReturnAddressBasedEncoding) {}
diff --git a/src/codegen/ia32/assembler-ia32-inl.h b/src/codegen/ia32/assembler-ia32-inl.h
index 5ecbc5416b..d5040b8aa4 100644
--- a/src/codegen/ia32/assembler-ia32-inl.h
+++ b/src/codegen/ia32/assembler-ia32-inl.h
@@ -98,8 +98,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!host().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(host(), this, target, write_barrier_mode);
+  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
   }
 }
 
@@ -224,7 +224,7 @@ void Assembler::set_target_address_at(Address pc, Address constant_pool,
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, InstructionStream code, Address target) {
+    Address instruction_payload, Code code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code.constant_pool() : kNullAddress,
                         target);
diff --git a/src/codegen/ia32/assembler-ia32.cc b/src/codegen/ia32/assembler-ia32.cc
index a9ee1ffccf..bb76c82a6f 100644
--- a/src/codegen/ia32/assembler-ia32.cc
+++ b/src/codegen/ia32/assembler-ia32.cc
@@ -3407,7 +3407,7 @@ void Assembler::dd(Label* label) {
 
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/ia32/assembler-ia32.h b/src/codegen/ia32/assembler-ia32.h
index 918ef36e91..54cbbf6886 100644
--- a/src/codegen/ia32/assembler-ia32.h
+++ b/src/codegen/ia32/assembler-ia32.h
@@ -421,7 +421,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x86).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, InstructionStream code, Address target);
+      Address instruction_payload, Code code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index 965d68f57e..77d07785f5 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -708,7 +708,7 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
 
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code) {
   test(FieldOperand(code, Code::kKindSpecificFlagsOffset),
-       Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
+       Immediate(1 << Code::kMarkedForDeoptimizationBit));
 }
 
 Immediate MacroAssembler::ClearedValue() const {
@@ -2005,14 +2005,6 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   mov(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
-void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                                         Register code_object) {
-  ASM_CODE_COMMENT(this);
-  // Compute the InstructionStream object pointer from the code entry point.
-  mov(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
-  sub(destination, Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
-}
-
 void MacroAssembler::CallCodeObject(Register code_object) {
   LoadCodeEntry(code_object, code_object);
   call(code_object);
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 13946396f9..39ee0f106e 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -160,12 +160,6 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
-  // Load code entry point from the Code object and compute
-  // InstructionStream object pointer out of it. Must not be used for
-  // Codes corresponding to builtins, because their entry points
-  // values point to the embedded instruction stream in .text section.
-  void LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/maglev-safepoint-table.cc b/src/codegen/maglev-safepoint-table.cc
index 24374fdb5d..50adb61efd 100644
--- a/src/codegen/maglev-safepoint-table.cc
+++ b/src/codegen/maglev-safepoint-table.cc
@@ -12,13 +12,6 @@
 namespace v8 {
 namespace internal {
 
-MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
-                                           InstructionStream code)
-    : MaglevSafepointTable(code.instruction_start(),
-                           code.safepoint_table_address()) {
-  DCHECK(code.is_maglevved());
-}
-
 MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
                                            Code code)
     : MaglevSafepointTable(code.InstructionStart(isolate, pc),
diff --git a/src/codegen/maglev-safepoint-table.h b/src/codegen/maglev-safepoint-table.h
index 6d9569bbb7..492346917f 100644
--- a/src/codegen/maglev-safepoint-table.h
+++ b/src/codegen/maglev-safepoint-table.h
@@ -66,8 +66,6 @@ class MaglevSafepointTable {
  public:
   // The isolate and pc arguments are used for figuring out whether pc
   // belongs to the embedded or un-embedded code blob.
-  explicit MaglevSafepointTable(Isolate* isolate, Address pc,
-                                InstructionStream code);
   explicit MaglevSafepointTable(Isolate* isolate, Address pc, Code code);
   MaglevSafepointTable(const MaglevSafepointTable&) = delete;
   MaglevSafepointTable& operator=(const MaglevSafepointTable&) = delete;
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index 1a4b24fe14..a7c4be40c4 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -253,56 +253,64 @@ void RelocIterator::next() {
   done_ = true;
 }
 
-RelocIterator::RelocIterator(InstructionStream code, int mode_mask)
-    : RelocIterator(code, code.unchecked_relocation_info(), mode_mask) {}
-
 RelocIterator::RelocIterator(Code code, int mode_mask)
-    : RelocIterator(code.instruction_stream(),
-                    code.instruction_stream().unchecked_relocation_info(),
-                    mode_mask) {}
+    : RelocIterator(code, code.unchecked_relocation_info(), mode_mask) {}
 
-RelocIterator::RelocIterator(InstructionStream code, ByteArray relocation_info,
+RelocIterator::RelocIterator(Code code, ByteArray relocation_info,
+                             int mode_mask)
+    : RelocIterator(
+          code,
+          InstructionStream::unchecked_cast(code.raw_instruction_stream()),
+          InstructionStream::unchecked_cast(code.raw_instruction_stream())
+              .instruction_start(),
+          code.constant_pool(), relocation_info.GetDataEndAddress(),
+          relocation_info.GetDataStartAddress(), mode_mask) {}
+
+RelocIterator::RelocIterator(Code code, InstructionStream instruction_stream,
+                             ByteArray relocation_info, Address constant_pool,
                              int mode_mask)
-    : RelocIterator(code, code.instruction_start(), code.constant_pool(),
+    : RelocIterator(code, instruction_stream,
+                    instruction_stream.instruction_start(), constant_pool,
                     relocation_info.GetDataEndAddress(),
                     relocation_info.GetDataStartAddress(), mode_mask) {}
 
 RelocIterator::RelocIterator(const CodeReference code_reference, int mode_mask)
-    : RelocIterator(InstructionStream(), code_reference.instruction_start(),
-                    code_reference.constant_pool(),
-                    code_reference.relocation_end(),
-                    code_reference.relocation_start(), mode_mask) {}
-
-RelocIterator::RelocIterator(EmbeddedData* embedded_data,
-                             InstructionStream code, int mode_mask)
-    : RelocIterator(code,
+    : RelocIterator(
+          Code(), InstructionStream(), code_reference.instruction_start(),
+          code_reference.constant_pool(), code_reference.relocation_end(),
+          code_reference.relocation_start(), mode_mask) {}
+
+RelocIterator::RelocIterator(EmbeddedData* embedded_data, Code code,
+                             int mode_mask)
+    : RelocIterator(code, code.instruction_stream(),
                     embedded_data->InstructionStartOfBuiltin(code.builtin_id()),
                     code.constant_pool(),
                     code.relocation_start() + code.relocation_size(),
                     code.relocation_start(), mode_mask) {}
 
 RelocIterator::RelocIterator(const CodeDesc& desc, int mode_mask)
-    : RelocIterator(InstructionStream(), reinterpret_cast<Address>(desc.buffer),
-                    0, desc.buffer + desc.buffer_size,
-                    desc.buffer + desc.buffer_size - desc.reloc_size,
-                    mode_mask) {}
+    : RelocIterator(
+          Code(), InstructionStream(), reinterpret_cast<Address>(desc.buffer),
+          0, desc.buffer + desc.buffer_size,
+          desc.buffer + desc.buffer_size - desc.reloc_size, mode_mask) {}
 
 RelocIterator::RelocIterator(base::Vector<byte> instructions,
                              base::Vector<const byte> reloc_info,
                              Address const_pool, int mode_mask)
-    : RelocIterator(InstructionStream(),
+    : RelocIterator(Code(), InstructionStream(),
                     reinterpret_cast<Address>(instructions.begin()), const_pool,
                     reloc_info.begin() + reloc_info.size(), reloc_info.begin(),
                     mode_mask) {}
 
-RelocIterator::RelocIterator(InstructionStream host, Address pc,
-                             Address constant_pool, const byte* pos,
+RelocIterator::RelocIterator(Code code, InstructionStream instruction_stream,
+                             Address pc, Address constant_pool, const byte* pos,
                              const byte* end, int mode_mask)
     : pos_(pos), end_(end), mode_mask_(mode_mask) {
   // Relocation info is read backwards.
   DCHECK_GE(pos_, end_);
-  rinfo_.host_ = host;
+  rinfo_.code_ = code;
   rinfo_.pc_ = pc;
+  rinfo_.instruction_stream_ = instruction_stream;
   rinfo_.constant_pool_ = constant_pool;
   if (mode_mask_ == 0) pos_ = end_;
   next();
@@ -355,11 +363,12 @@ void RelocInfo::set_target_address(Address target,
          IsWasmCall(rmode_));
   Assembler::set_target_address_at(pc_, constant_pool_, target,
                                    icache_flush_mode);
-  if (!host().is_null() && IsCodeTargetMode(rmode_) &&
+  if (!instruction_stream().is_null() && IsCodeTargetMode(rmode_) &&
       !v8_flags.disable_write_barriers) {
     InstructionStream target_code =
         InstructionStream::FromTargetAddress(target);
-    WriteBarrierForCode(host(), this, target_code, write_barrier_mode);
+    WriteBarrierForCode(instruction_stream(), this, target_code,
+                        write_barrier_mode);
   }
 }
 
@@ -388,16 +397,6 @@ bool RelocInfo::HasTargetAddressAddress() const {
   return (ModeMask(rmode_) & kTargetAddressAddressModeMask) != 0;
 }
 
-bool RelocInfo::RequiresRelocationAfterCodegen(const CodeDesc& desc) {
-  RelocIterator it(desc, RelocInfo::PostCodegenRelocationMask());
-  return !it.done();
-}
-
-bool RelocInfo::RequiresRelocation(InstructionStream code) {
-  RelocIterator it(code, RelocInfo::kApplyMask);
-  return !it.done();
-}
-
 #ifdef ENABLE_DISASSEMBLER
 const char* RelocInfo::RelocModeName(RelocInfo::Mode rmode) {
   switch (rmode) {
@@ -470,11 +469,10 @@ void RelocInfo::Print(Isolate* isolate, std::ostream& os) {
        << ")";
   } else if (IsCodeTargetMode(rmode_)) {
     const Address code_target = target_address();
-    InstructionStream code = InstructionStream::FromTargetAddress(code_target);
-    DCHECK(code.IsInstructionStream());
-    os << " (" << CodeKindToString(code.kind());
-    if (Builtins::IsBuiltin(code)) {
-      os << " " << Builtins::name(code.builtin_id());
+    Code target_code = Code::FromTargetAddress(code_target);
+    os << " (" << CodeKindToString(target_code.kind());
+    if (Builtins::IsBuiltin(target_code)) {
+      os << " " << Builtins::name(target_code.builtin_id());
     }
     os << ")  (" << reinterpret_cast<const void*>(target_address()) << ")";
   } else if (IsConstPool(rmode_)) {
@@ -520,9 +518,8 @@ void RelocInfo::Verify(Isolate* isolate) {
       Address target = target_internal_reference();
       Address pc = target_internal_reference_address();
       Code lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
-      InstructionStream code = lookup_result.instruction_stream();
-      CHECK(target >= code.instruction_start());
-      CHECK(target < code.instruction_end());
+      CHECK_GE(target, lookup_result.InstructionStart());
+      CHECK_LT(target, lookup_result.InstructionEnd());
       break;
     }
     case OFF_HEAP_TARGET: {
diff --git a/src/codegen/reloc-info.h b/src/codegen/reloc-info.h
index 169cb6a650..0f5b64757c 100644
--- a/src/codegen/reloc-info.h
+++ b/src/codegen/reloc-info.h
@@ -114,12 +114,14 @@ class RelocInfo {
 
   RelocInfo() = default;
 
-  RelocInfo(Address pc, Mode rmode, intptr_t data, InstructionStream host,
+  RelocInfo(Address pc, Mode rmode, intptr_t data, Code code,
+            InstructionStream instruction_stream,
             Address constant_pool = kNullAddress)
       : pc_(pc),
         rmode_(rmode),
         data_(data),
-        host_(host),
+        code_(code),
+        instruction_stream_(instruction_stream),
         constant_pool_(constant_pool) {
     DCHECK_IMPLIES(!COMPRESS_POINTERS_BOOL,
                    rmode != COMPRESSED_EMBEDDED_OBJECT);
@@ -213,7 +215,8 @@ class RelocInfo {
   Address pc() const { return pc_; }
   Mode rmode() const { return rmode_; }
   intptr_t data() const { return data_; }
-  InstructionStream host() const { return host_; }
+  Code code() const { return code_; }
+  InstructionStream instruction_stream() const { return instruction_stream_; }
   Address constant_pool() const { return constant_pool_; }
 
   // Apply a relocation by delta bytes. When the code object is moved, PC
@@ -317,23 +320,18 @@ class RelocInfo {
   void Visit(ObjectVisitor* visitor) {
     Mode mode = rmode();
     if (IsEmbeddedObjectMode(mode)) {
-      visitor->VisitEmbeddedPointer(host(), this);
+      visitor->VisitEmbeddedPointer(this);
     } else if (IsCodeTargetMode(mode)) {
-      visitor->VisitCodeTarget(host(), this);
+      visitor->VisitCodeTarget(this);
     } else if (IsExternalReference(mode)) {
-      visitor->VisitExternalReference(host(), this);
+      visitor->VisitExternalReference(this);
     } else if (IsInternalReference(mode) || IsInternalReferenceEncoded(mode)) {
-      visitor->VisitInternalReference(host(), this);
+      visitor->VisitInternalReference(this);
     } else if (IsBuiltinEntryMode(mode)) {
-      visitor->VisitOffHeapTarget(host(), this);
+      visitor->VisitOffHeapTarget(this);
     }
   }
 
-  // Check whether the given code contains relocation information that
-  // either is position-relative or movable by the garbage collector.
-  static bool RequiresRelocationAfterCodegen(const CodeDesc& desc);
-  static bool RequiresRelocation(InstructionStream code);
-
 #ifdef ENABLE_DISASSEMBLER
   // Printing
   static const char* RelocModeName(Mode rmode);
@@ -375,7 +373,8 @@ class RelocInfo {
   Address pc_;
   Mode rmode_;
   intptr_t data_ = 0;
-  InstructionStream host_;
+  Code code_;
+  InstructionStream instruction_stream_;
   Address constant_pool_ = kNullAddress;
   friend class RelocIterator;
 };
@@ -429,16 +428,15 @@ class RelocInfoWriter {
 // A mask can be specified to skip unwanted modes.
 class V8_EXPORT_PRIVATE RelocIterator : public Malloced {
  public:
-  // Create a new iterator positioned at
-  // the beginning of the reloc info.
-  // Relocation information with mode k is included in the
-  // iteration iff bit k of mode_mask is set.
-  explicit RelocIterator(InstructionStream code, int mode_mask = -1);
+  // Create a new iterator positioned at the beginning of the reloc info.
+  // Relocation information with mode k is included in the iteration iff bit k
+  // of mode_mask is set.
   explicit RelocIterator(Code code, int mode_mask = -1);
-  explicit RelocIterator(InstructionStream code, ByteArray relocation_info,
-                         int mode_mask);
-  explicit RelocIterator(EmbeddedData* embedded_data, InstructionStream code,
+  explicit RelocIterator(Code code, ByteArray relocation_info, int mode_mask);
+  explicit RelocIterator(Code code, InstructionStream instruction_stream,
+                         ByteArray relocation_info, Address constant_pool,
                          int mode_mask);
+  explicit RelocIterator(EmbeddedData* embedded_data, Code code, int mode_mask);
   explicit RelocIterator(const CodeDesc& desc, int mode_mask = -1);
   explicit RelocIterator(const CodeReference code_reference,
                          int mode_mask = -1);
@@ -461,8 +459,9 @@ class V8_EXPORT_PRIVATE RelocIterator : public Malloced {
   }
 
  private:
-  RelocIterator(InstructionStream host, Address pc, Address constant_pool,
-                const byte* pos, const byte* end, int mode_mask);
+  RelocIterator(Code code, InstructionStream instruction_stream, Address pc,
+                Address constant_pool, const byte* pos, const byte* end,
+                int mode_mask);
 
   // Advance* moves the position before/after reading.
   // *Read* reads from current byte(s) into rinfo_.
diff --git a/src/codegen/safepoint-table.cc b/src/codegen/safepoint-table.cc
index ef3793a064..b1ae0d8a7e 100644
--- a/src/codegen/safepoint-table.cc
+++ b/src/codegen/safepoint-table.cc
@@ -20,11 +20,6 @@
 namespace v8 {
 namespace internal {
 
-SafepointTable::SafepointTable(Isolate* isolate, Address pc,
-                               InstructionStream code)
-    : SafepointTable(code.instruction_start(), code.safepoint_table_address()) {
-}
-
 SafepointTable::SafepointTable(Isolate* isolate, Address pc, Code code)
     : SafepointTable(code.InstructionStart(isolate, pc),
                      code.SafepointTableAddress()) {}
diff --git a/src/codegen/source-position.cc b/src/codegen/source-position.cc
index d705ebd2f6..60ff9f8f98 100644
--- a/src/codegen/source-position.cc
+++ b/src/codegen/source-position.cc
@@ -126,7 +126,7 @@ void SourcePosition::PrintJson(std::ostream& out) const {
   }
 }
 
-void SourcePosition::Print(std::ostream& out, InstructionStream code) const {
+void SourcePosition::Print(std::ostream& out, Code code) const {
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(code.deoptimization_data());
   if (!isInlined()) {
diff --git a/src/codegen/source-position.h b/src/codegen/source-position.h
index 0f9922b6a3..5d6b1a93f7 100644
--- a/src/codegen/source-position.h
+++ b/src/codegen/source-position.h
@@ -85,7 +85,7 @@ class SourcePosition final {
       Isolate* isolate, OptimizedCompilationInfo* cinfo) const;
   SourcePositionInfo FirstInfo(Isolate* isolate, Code code) const;
 
-  void Print(std::ostream& out, InstructionStream code) const;
+  void Print(std::ostream& out, Code code) const;
   void PrintJson(std::ostream& out) const;
 
   int ScriptOffset() const {
diff --git a/src/codegen/x64/assembler-x64-inl.h b/src/codegen/x64/assembler-x64-inl.h
index 1aabc0c7b3..dc51e84ff4 100644
--- a/src/codegen/x64/assembler-x64-inl.h
+++ b/src/codegen/x64/assembler-x64-inl.h
@@ -215,7 +215,7 @@ void Assembler::deserialization_set_target_internal_reference_at(
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, InstructionStream code, Address target) {
+    Address instruction_payload, Code code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code.constant_pool() : kNullAddress,
                         target);
@@ -349,8 +349,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!host().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(host(), this, target, write_barrier_mode);
+  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
   }
 }
 
diff --git a/src/codegen/x64/assembler-x64.cc b/src/codegen/x64/assembler-x64.cc
index ff650d91e4..b271271de5 100644
--- a/src/codegen/x64/assembler-x64.cc
+++ b/src/codegen/x64/assembler-x64.cc
@@ -4492,7 +4492,7 @@ void Assembler::dq(Label* label) {
 
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index 6ede04b50a..a8b7d77536 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -530,7 +530,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x64).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, InstructionStream code, Address target);
+      Address instruction_payload, Code code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 68434a1992..6c1c25db82 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -2264,14 +2264,6 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   movq(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
-void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                                         Register code_object) {
-  ASM_CODE_COMMENT(this);
-  // Compute the InstructionStream object pointer from the code entry point.
-  movq(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
-  subq(destination, Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
-}
-
 void MacroAssembler::CallCodeObject(Register code_object) {
   LoadCodeEntry(code_object, code_object);
   call(code_object);
@@ -2572,7 +2564,7 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
 
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code) {
   testl(FieldOperand(code, Code::kKindSpecificFlagsOffset),
-        Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
+        Immediate(1 << Code::kMarkedForDeoptimizationBit));
 }
 
 Immediate MacroAssembler::ClearedValue() const {
@@ -3330,7 +3322,7 @@ void MacroAssembler::BailoutIfDeoptimized(Register scratch) {
   int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;
   LoadTaggedField(scratch, Operand(kJavaScriptCallCodeStartRegister, offset));
   testl(FieldOperand(scratch, Code::kKindSpecificFlagsOffset),
-        Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
+        Immediate(1 << Code::kMarkedForDeoptimizationBit));
   Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
        RelocInfo::CODE_TARGET, not_zero);
 }
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 674cd8e331..24cf7fcfab 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -411,12 +411,6 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
-  // Load code entry point from the Code object and compute
-  // InstructionStream object pointer out of it. Must not be used for
-  // Codes corresponding to builtins, because their entry points
-  // values point to the embedded instruction stream in .text section.
-  void LoadCodeInstructionStreamNonBuiltin(Register destination,
-                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/compiler/backend/arm/code-generator-arm.cc b/src/compiler/backend/arm/code-generator-arm.cc
index 1fa882fc4e..43c94b3e9d 100644
--- a/src/compiler/backend/arm/code-generator-arm.cc
+++ b/src/compiler/backend/arm/code-generator-arm.cc
@@ -649,7 +649,7 @@ void CodeGenerator::BailoutIfDeoptimized() {
   int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;
   __ ldr(scratch, MemOperand(kJavaScriptCallCodeStartRegister, offset));
   __ ldr(scratch, FieldMemOperand(scratch, Code::kKindSpecificFlagsOffset));
-  __ tst(scratch, Operand(1 << InstructionStream::kMarkedForDeoptimizationBit));
+  __ tst(scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
   __ Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
           RelocInfo::CODE_TARGET, ne);
 }
diff --git a/src/compiler/backend/ia32/code-generator-ia32.cc b/src/compiler/backend/ia32/code-generator-ia32.cc
index 3f97b94d9c..fe1df09b98 100644
--- a/src/compiler/backend/ia32/code-generator-ia32.cc
+++ b/src/compiler/backend/ia32/code-generator-ia32.cc
@@ -664,7 +664,7 @@ void CodeGenerator::BailoutIfDeoptimized() {
   __ push(eax);  // Push eax so we can use it as a scratch register.
   __ mov(eax, Operand(kJavaScriptCallCodeStartRegister, offset));
   __ test(FieldOperand(eax, Code::kKindSpecificFlagsOffset),
-          Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
+          Immediate(1 << Code::kMarkedForDeoptimizationBit));
   __ pop(eax);  // Restore eax.
 
   Label skip;
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index c82abacf7a..dbde14e3fc 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -2270,9 +2270,10 @@ std::ostream& operator<<(std::ostream& os, const ObjectRef& ref) {
   }
 }
 
-namespace {
+unsigned CodeRef::GetInlinedBytecodeSize() const {
+  Code code = *object();
+  if (!code.has_instruction_stream()) return 0;
 
-unsigned GetInlinedBytecodeSizeImpl(InstructionStream code) {
   unsigned value = code.inlined_bytecode_size();
   if (value > 0) {
     // Don't report inlined bytecode size if the code object was already
@@ -2282,23 +2283,6 @@ unsigned GetInlinedBytecodeSizeImpl(InstructionStream code) {
   return value;
 }
 
-}  // namespace
-
-unsigned InstructionStreamRef::GetInlinedBytecodeSize() const {
-  return GetInlinedBytecodeSizeImpl(*object());
-}
-
-unsigned CodeRef::GetInlinedBytecodeSize() const {
-  Code code = *object();
-  Object maybe_istream = code.raw_instruction_stream(kRelaxedLoad);
-  if (maybe_istream == Smi::zero()) return 0;
-
-  // Safe to do a relaxed conversion to InstructionStream here since
-  // Code::instruction_stream field is modified only by GC and the Code was
-  // acquire-loaded.
-  return GetInlinedBytecodeSizeImpl(InstructionStream::cast(maybe_istream));
-}
-
 #undef BIMODAL_ACCESSOR
 #undef BIMODAL_ACCESSOR_B
 #undef BIMODAL_ACCESSOR_C
diff --git a/src/compiler/heap-refs.h b/src/compiler/heap-refs.h
index 231494ad62..6695e684b7 100644
--- a/src/compiler/heap-refs.h
+++ b/src/compiler/heap-refs.h
@@ -113,7 +113,6 @@ enum class RefSerializationKind {
   BACKGROUND_SERIALIZED(BigInt)                                               \
   NEVER_SERIALIZED(CallHandlerInfo)                                           \
   NEVER_SERIALIZED(Cell)                                                      \
-  NEVER_SERIALIZED(InstructionStream)                                         \
   NEVER_SERIALIZED(Code)                                                      \
   NEVER_SERIALIZED(Context)                                                   \
   NEVER_SERIALIZED(DescriptorArray)                                           \
@@ -1105,15 +1104,6 @@ class JSGlobalProxyRef : public JSObjectRef {
   Handle<JSGlobalProxy> object() const;
 };
 
-class InstructionStreamRef : public HeapObjectRef {
- public:
-  DEFINE_REF_CONSTRUCTOR(InstructionStream, HeapObjectRef)
-
-  Handle<InstructionStream> object() const;
-
-  unsigned GetInlinedBytecodeSize() const;
-};
-
 class CodeRef : public HeapObjectRef {
  public:
   DEFINE_REF_CONSTRUCTOR(Code, HeapObjectRef)
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index c169835e67..58d4b9ae95 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -1332,17 +1332,16 @@ PipelineCompilationJob::Status PipelineCompilationJob::FinalizeJobImpl(
 
 void PipelineCompilationJob::RegisterWeakObjectsInOptimizedCode(
     Isolate* isolate, Handle<NativeContext> context, Handle<Code> code) {
-  Handle<InstructionStream> istream(code->instruction_stream(), isolate);
   std::vector<Handle<Map>> maps;
   DCHECK(code->is_optimized_code());
   {
     DisallowGarbageCollection no_gc;
     PtrComprCageBase cage_base(isolate);
     int const mode_mask = RelocInfo::EmbeddedObjectModeMask();
-    for (RelocIterator it(*istream, mode_mask); !it.done(); it.next()) {
+    for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
       DCHECK(RelocInfo::IsEmbeddedObjectMode(it.rinfo()->rmode()));
       HeapObject target_object = it.rinfo()->target_object(cage_base);
-      if (istream->IsWeakObjectInOptimizedCode(target_object)) {
+      if (code->IsWeakObjectInOptimizedCode(target_object)) {
         if (target_object.IsMap(cage_base)) {
           maps.push_back(handle(Map::cast(target_object), isolate));
         }
diff --git a/src/debug/debug-evaluate.cc b/src/debug/debug-evaluate.cc
index 686d43d81f..48349f193d 100644
--- a/src/debug/debug-evaluate.cc
+++ b/src/debug/debug-evaluate.cc
@@ -1234,7 +1234,7 @@ void DebugEvaluate::VerifyTransitiveBuiltins(Isolate* isolate) {
   for (Builtin caller = Builtins::kFirst; caller <= Builtins::kLast; ++caller) {
     DebugInfo::SideEffectState state = BuiltinGetSideEffectState(caller);
     if (state != DebugInfo::kHasNoSideEffect) continue;
-    InstructionStream code = FromCode(isolate->builtins()->code(caller));
+    Code code = isolate->builtins()->code(caller);
     int mode = RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
                RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
 
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index a6ab2452cc..ee5ee898a0 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -338,9 +338,8 @@ void Deoptimizer::DeoptimizeAll(Isolate* isolate) {
 
   // Mark all code, then deoptimize.
   {
-    InstructionStream::OptimizedCodeIterator it(isolate);
-    for (InstructionStream code = it.Next(); !code.is_null();
-         code = it.Next()) {
+    Code::OptimizedCodeIterator it(isolate);
+    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
       code.set_marked_for_deoptimization(true);
     }
   }
@@ -384,9 +383,8 @@ void Deoptimizer::DeoptimizeAllOptimizedCodeWithFunction(
   // Mark all code that inlines this function, then deoptimize.
   bool any_marked = false;
   {
-    InstructionStream::OptimizedCodeIterator it(isolate);
-    for (InstructionStream code = it.Next(); !code.is_null();
-         code = it.Next()) {
+    Code::OptimizedCodeIterator it(isolate);
+    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
       if (code.Inlines(*function)) {
         code.set_marked_for_deoptimization(true);
         any_marked = true;
@@ -449,10 +447,9 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   }
 
   DCHECK_NE(from, kNullAddress);
-  compiled_code_ =
-      isolate_->heap()->FindCodeForInnerPointer(from).instruction_stream();
+  compiled_code_ = isolate_->heap()->FindCodeForInnerPointer(from);
   DCHECK(!compiled_code_.is_null());
-  DCHECK(compiled_code_.IsInstructionStream());
+  DCHECK(compiled_code_.IsCode());
 
   DCHECK(function.IsJSFunction());
 #ifdef DEBUG
@@ -477,7 +474,7 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(compiled_code_.deoptimization_data());
   Address deopt_start =
-      compiled_code_.instruction_start() + deopt_data.DeoptExitStart().value();
+      compiled_code_.InstructionStart() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * kEagerDeoptExitSize;
@@ -506,8 +503,8 @@ Handle<JSFunction> Deoptimizer::function() const {
   return Handle<JSFunction>(function_, isolate());
 }
 
-Handle<InstructionStream> Deoptimizer::compiled_code() const {
-  return Handle<InstructionStream>(compiled_code_, isolate());
+Handle<Code> Deoptimizer::compiled_code() const {
+  return Handle<Code>(compiled_code_, isolate());
 }
 
 Deoptimizer::~Deoptimizer() {
@@ -604,12 +601,11 @@ void Deoptimizer::TraceDeoptEnd(double deopt_duration) {
 }
 
 // static
-void Deoptimizer::TraceMarkForDeoptimization(InstructionStream code,
+void Deoptimizer::TraceMarkForDeoptimization(Isolate* isolate, Code code,
                                              const char* reason) {
   if (!v8_flags.trace_deopt && !v8_flags.log_deopt) return;
 
   DisallowGarbageCollection no_gc;
-  Isolate* isolate = code.GetIsolate();
   Object maybe_data = code.deoptimization_data();
   if (maybe_data == ReadOnlyRoots(isolate).empty_fixed_array()) return;
 
@@ -1895,13 +1891,13 @@ namespace {
 // points to immediately after the deopt call).
 //
 // See also the Deoptimizer constructor.
-Address GetDeoptCallPCFromReturnPC(Address return_pc, InstructionStream code) {
+Address GetDeoptCallPCFromReturnPC(Address return_pc, Code code) {
   DCHECK_GT(Deoptimizer::kEagerDeoptExitSize, 0);
   DCHECK_GT(Deoptimizer::kLazyDeoptExitSize, 0);
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(code.deoptimization_data());
   Address deopt_start =
-      code.instruction_start() + deopt_data.DeoptExitStart().value();
+      code.InstructionStart() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * Deoptimizer::kEagerDeoptExitSize;
@@ -1960,9 +1956,8 @@ unsigned Deoptimizer::ComputeIncomingArgumentSize(SharedFunctionInfo shared) {
   return parameter_slots * kSystemPointerSize;
 }
 
-Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(InstructionStream code,
-                                                 Address pc) {
-  CHECK(code.instruction_start() <= pc && pc <= code.instruction_end());
+Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(Code code, Address pc) {
+  CHECK(code.InstructionStart() <= pc && pc <= code.InstructionEnd());
   SourcePosition last_position = SourcePosition::Unknown();
   DeoptimizeReason last_reason = DeoptimizeReason::kUnknown;
   uint32_t last_node_id = 0;
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index 0cdf66460e..02eec6d87b 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -43,7 +43,7 @@ class Deoptimizer : public Malloced {
     const int deopt_id;
   };
 
-  static DeoptInfo GetDeoptInfo(InstructionStream code, Address from);
+  static DeoptInfo GetDeoptInfo(Code code, Address from);
   DeoptInfo GetDeoptInfo() const {
     return Deoptimizer::GetDeoptInfo(compiled_code_, from_);
   }
@@ -51,7 +51,7 @@ class Deoptimizer : public Malloced {
   static const char* MessageFor(DeoptimizeKind kind);
 
   Handle<JSFunction> function() const;
-  Handle<InstructionStream> compiled_code() const;
+  Handle<Code> compiled_code() const;
   DeoptimizeKind deopt_kind() const { return deopt_kind_; }
 
   // Where the deopt exit occurred *in the outermost frame*, i.e in the
@@ -130,7 +130,7 @@ class Deoptimizer : public Malloced {
   V8_EXPORT_PRIVATE static const int kLazyDeoptExitSize;
 
   // Tracing.
-  static void TraceMarkForDeoptimization(InstructionStream code,
+  static void TraceMarkForDeoptimization(Isolate* isolate, Code code,
                                          const char* reason);
   static void TraceEvictFromOptimizedCodeCache(Isolate* isolate,
                                                SharedFunctionInfo sfi,
@@ -189,7 +189,7 @@ class Deoptimizer : public Malloced {
 
   Isolate* isolate_;
   JSFunction function_;
-  InstructionStream compiled_code_;
+  Code compiled_code_;
   unsigned deopt_exit_index_;
   BytecodeOffset bytecode_offset_in_outermost_frame_ = BytecodeOffset::None();
   DeoptimizeKind deopt_kind_;
diff --git a/src/diagnostics/disassembler.cc b/src/diagnostics/disassembler.cc
index 241251b156..6f0b60f41a 100644
--- a/src/diagnostics/disassembler.cc
+++ b/src/diagnostics/disassembler.cc
@@ -380,17 +380,17 @@ static int DecodeIt(Isolate* isolate, ExternalReferenceEncoder* ref_encoder,
       const CodeReference& host = code;
       Address constant_pool =
           host.is_null() ? kNullAddress : host.constant_pool();
-      InstructionStream instruction_stream;
+      Handle<Code> code_handle;
       if (host.is_code()) {
-        instruction_stream = host.as_code()->instruction_stream();
-      }
+        code_handle = host.as_code();
 
-      RelocInfo relocinfo(pcs[i], rmodes[i], datas[i], instruction_stream,
-                          constant_pool);
+        RelocInfo relocinfo(pcs[i], rmodes[i], datas[i], *code_handle,
+                            code_handle->instruction_stream(), constant_pool);
 
-      bool first_reloc_info = (i == 0);
-      PrintRelocInfo(out, isolate, ref_encoder, os, code, &relocinfo,
-                     first_reloc_info);
+        bool first_reloc_info = (i == 0);
+        PrintRelocInfo(out, isolate, ref_encoder, os, code, &relocinfo,
+                       first_reloc_info);
+      }
     }
 
     // If this is a constant pool load and we haven't found any RelocInfo
@@ -402,7 +402,7 @@ static int DecodeIt(Isolate* isolate, ExternalReferenceEncoder* ref_encoder,
     // by IsInConstantPool() below.
     if (pcs.empty() && !code.is_null() && !decoding_constant_pool) {
       RelocInfo dummy_rinfo(reinterpret_cast<Address>(prev_pc),
-                            RelocInfo::NO_INFO, 0, InstructionStream());
+                            RelocInfo::NO_INFO, 0, Code(), InstructionStream());
       if (dummy_rinfo.IsInConstantPool()) {
         Address constant_pool_entry_address =
             dummy_rinfo.constant_pool_entry_address();
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index ce9a3fb117..cdce7778c4 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -1102,9 +1102,15 @@ void Code::CodeVerify(Isolate* isolate) {
   CHECK(IsCode());
   if (raw_instruction_stream() != Smi::zero()) {
     InstructionStream istream = instruction_stream();
-    CHECK_EQ(istream.kind(), kind());
-    CHECK_EQ(istream.builtin_id(), builtin_id());
     CHECK_EQ(istream.code(kAcquireLoad), *this);
+    CHECK_EQ(safepoint_table_offset(), 0);
+    CHECK_LE(safepoint_table_offset(), handler_table_offset());
+    CHECK_LE(handler_table_offset(), constant_pool_offset());
+    CHECK_LE(constant_pool_offset(), code_comments_offset());
+    CHECK_LE(code_comments_offset(), unwinding_info_offset());
+    CHECK_LE(unwinding_info_offset(), metadata_size());
+
+    relocation_info().ObjectVerify(isolate);
 
     // Ensure the cached code entry point corresponds to the InstructionStream
     // object associated with this Code.
@@ -1132,14 +1138,8 @@ void Code::CodeVerify(Isolate* isolate) {
 
 void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
   CHECK(
-      IsAligned(instruction_size(),
+      IsAligned(code(kAcquireLoad).instruction_size(),
                 static_cast<unsigned>(InstructionStream::kMetadataAlignment)));
-  CHECK_EQ(safepoint_table_offset(), 0);
-  CHECK_LE(safepoint_table_offset(), handler_table_offset());
-  CHECK_LE(handler_table_offset(), constant_pool_offset());
-  CHECK_LE(constant_pool_offset(), code_comments_offset());
-  CHECK_LE(code_comments_offset(), unwinding_info_offset());
-  CHECK_LE(unwinding_info_offset(), metadata_size());
 #if !defined(_MSC_VER) || defined(__clang__)
   // See also: PlatformEmbeddedFileWriterWin::AlignToCodeAlignment.
   CHECK_IMPLIES(!ReadOnlyHeap::Contains(*this),
@@ -1148,13 +1148,12 @@ void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
   CHECK_IMPLIES(!ReadOnlyHeap::Contains(*this),
                 IsAligned(instruction_start(), kCodeAlignment));
   CHECK_EQ(*this, code(kAcquireLoad).instruction_stream());
-  relocation_info().ObjectVerify(isolate);
   CHECK(V8_ENABLE_THIRD_PARTY_HEAP_BOOL ||
         CodeSize() <= MemoryChunkLayout::MaxRegularCodeObjectSize() ||
         isolate->heap()->InSpace(*this, CODE_LO_SPACE));
   Address last_gc_pc = kNullAddress;
 
-  for (RelocIterator it(*this); !it.done(); it.next()) {
+  for (RelocIterator it(code(kAcquireLoad)); !it.done(); it.next()) {
     it.rinfo()->Verify(isolate);
     // Ensure that GC will not iterate twice over the same pointer.
     if (RelocInfo::IsGCRelocMode(it.rinfo()->rmode())) {
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index d4b3d731df..d652609a7c 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -1879,10 +1879,6 @@ void InstructionStream::InstructionStreamPrint(std::ostream& os) {
   PrintHeader(os, "InstructionStream");
   Code the_code = code(kAcquireLoad);
   os << "\n - code: " << Brief(the_code);
-  if (is_builtin()) {
-    os << "\n - builtin_id: " << Builtins::name(builtin_id());
-  }
-  os << "\n";
 #ifdef ENABLE_DISASSEMBLER
   the_code.Disassemble(nullptr, os, GetIsolate());
 #endif
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index b54322f8a4..e78b7d3954 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -610,7 +610,7 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
   // TODO(v8:10026): avoid replacing a signed pointer.
   PointerAuthentication::ReplacePC(pc_address, new_pc, kSystemPointerSize);
   if (V8_EMBEDDED_CONSTANT_POOL_BOOL && constant_pool_address != nullptr) {
-    *constant_pool_address = istream.constant_pool();
+    *constant_pool_address = visited_holder.constant_pool();
   }
 }
 
@@ -2448,12 +2448,12 @@ void InterpretedFrame::PatchBytecodeArray(BytecodeArray bytecode_array) {
 }
 
 int BaselineFrame::GetBytecodeOffset() const {
-  InstructionStream code = LookupCode().instruction_stream();
+  Code code = LookupCode();
   return code.GetBytecodeOffsetForBaselinePC(this->pc(), GetBytecodeArray());
 }
 
 intptr_t BaselineFrame::GetPCForBytecodeOffset(int bytecode_offset) const {
-  InstructionStream code = LookupCode().instruction_stream();
+  Code code = LookupCode();
   return code.GetBaselineStartPCForBytecodeOffset(bytecode_offset,
                                                   GetBytecodeArray());
 }
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index d7d67c3a8b..caada1b50d 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -445,9 +445,9 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     DCHECK(Internals::HasHeapObjectTag(code.ptr()));
     uint8_t* const code_ptr = reinterpret_cast<uint8_t*>(code.address());
 
-    // These static asserts ensure we don't miss relevant fields. We don't
-    // hash code cage base and code entry point. Other data fields must
-    // remain the same.
+    // These static asserts ensure we don't miss relevant fields. We don't hash
+    // code cage base and code entry point. Other data fields must remain the
+    // same.
     static_assert(Code::kCodePointerFieldsStrongEndOffset ==
                   Code::kCodeEntryPointOffset);
 
@@ -456,6 +456,22 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     static_assert(Code::kBuiltinIdOffsetEnd + 1 ==
                   Code::kKindSpecificFlagsOffset);
     static_assert(Code::kKindSpecificFlagsOffsetEnd + 1 ==
+                  Code::kInstructionSizeOffset);
+    static_assert(Code::kInstructionSizeOffsetEnd + 1 ==
+                  Code::kMetadataSizeOffset);
+    static_assert(Code::kMetadataSizeOffsetEnd + 1 ==
+                  Code::kInlinedBytecodeSizeOffset);
+    static_assert(Code::kInlinedBytecodeSizeOffsetEnd + 1 ==
+                  Code::kOsrOffsetOffset);
+    static_assert(Code::kOsrOffsetOffsetEnd + 1 ==
+                  Code::kHandlerTableOffsetOffset);
+    static_assert(Code::kHandlerTableOffsetOffsetEnd + 1 ==
+                  Code::kUnwindingInfoOffsetOffset);
+    static_assert(Code::kUnwindingInfoOffsetOffsetEnd + 1 ==
+                  Code::kConstantPoolOffsetOffset);
+    static_assert(Code::kConstantPoolOffsetOffsetEnd + 1 ==
+                  Code::kCodeCommentsOffsetOffset);
+    static_assert(Code::kCodeCommentsOffsetOffsetEnd + 1 ==
                   Code::kUnalignedSize);
     constexpr int kStartOffset = Code::kFlagsOffset;
 
@@ -1986,7 +2002,7 @@ Object Isolate::UnwindAndFindHandler() {
       CHECK(frame->is_java_script());
 
       if (frame->is_turbofan()) {
-        InstructionStream code = frame->LookupCode().instruction_stream();
+        Code code = frame->LookupCode();
         // The debugger triggers lazy deopt for the "to-be-restarted" frame
         // immediately when the CDP event arrives while paused.
         CHECK(code.marked_for_deoptimization());
@@ -1995,7 +2011,7 @@ Object Isolate::UnwindAndFindHandler() {
         // Jump directly to the optimized frames return, to immediately fall
         // into the deoptimizer.
         const int offset =
-            static_cast<int>(frame->pc() - code.instruction_start());
+            static_cast<int>(frame->pc() - code.InstructionStart());
 
         // Compute the stack pointer from the frame pointer. This ensures that
         // argument slots on the stack are dropped as returning would.
@@ -2003,7 +2019,7 @@ Object Isolate::UnwindAndFindHandler() {
         Address return_sp = frame->fp() +
                             StandardFrameConstants::kFixedFrameSizeAboveFp -
                             code.stack_slots() * kSystemPointerSize;
-        return FoundHandler(Context(), code.instruction_start(), offset,
+        return FoundHandler(Context(), code.InstructionStart(), offset,
                             code.constant_pool(), return_sp, frame->fp(),
                             visited_frames);
       }
@@ -2038,9 +2054,9 @@ Object Isolate::UnwindAndFindHandler() {
       case StackFrame::C_WASM_ENTRY: {
         StackHandler* handler = frame->top_handler();
         thread_local_top()->handler_ = handler->next_address();
-        InstructionStream code = frame->LookupCode().instruction_stream();
+        Code code = frame->LookupCode();
         HandlerTable table(code);
-        Address instruction_start = code.instruction_start();
+        Address instruction_start = code.InstructionStart();
         int return_offset = static_cast<int>(frame->pc() - instruction_start);
         int handler_offset = table.LookupReturn(return_offset);
         DCHECK_NE(-1, handler_offset);
@@ -2182,12 +2198,12 @@ Object Isolate::UnwindAndFindHandler() {
 
         if (frame->is_baseline()) {
           BaselineFrame* sp_frame = BaselineFrame::cast(js_frame);
-          InstructionStream code = sp_frame->LookupCode().instruction_stream();
+          Code code = sp_frame->LookupCode();
           intptr_t pc_offset = sp_frame->GetPCForBytecodeOffset(offset);
           // Patch the context register directly on the frame, so that we don't
           // need to have a context read + write in the baseline code.
           sp_frame->PatchContext(context);
-          return FoundHandler(Context(), code.instruction_start(), pc_offset,
+          return FoundHandler(Context(), code.InstructionStart(), pc_offset,
                               code.constant_pool(), return_sp, sp_frame->fp(),
                               visited_frames);
         } else {
@@ -4734,7 +4750,7 @@ bool Isolate::use_optimizer() {
 
 void Isolate::IncreaseTotalRegexpCodeGenerated(Handle<HeapObject> code) {
   PtrComprCageBase cage_base(this);
-  DCHECK(code->IsInstructionStream(cage_base) || code->IsByteArray(cage_base));
+  DCHECK(code->IsCode(cage_base) || code->IsByteArray(cage_base));
   total_regexp_code_generated_ += code->Size(cage_base);
 }
 
diff --git a/src/extensions/statistics-extension.cc b/src/extensions/statistics-extension.cc
index b1299da7da..ea333762c9 100644
--- a/src/extensions/statistics-extension.cc
+++ b/src/extensions/statistics-extension.cc
@@ -144,8 +144,8 @@ void StatisticsExtension::GetCounters(
     for (HeapObject obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       Object maybe_source_positions;
-      if (obj.IsInstructionStream()) {
-        InstructionStream code = InstructionStream::cast(obj);
+      if (obj.IsCode()) {
+        Code code = Code::cast(obj);
         reloc_info_total += code.relocation_info().Size();
         // Baseline code doesn't have source positions since it uses
         // interpreter code positions.
diff --git a/src/heap/concurrent-marking.cc b/src/heap/concurrent-marking.cc
index 54a971a70f..73a43c249c 100644
--- a/src/heap/concurrent-marking.cc
+++ b/src/heap/concurrent-marking.cc
@@ -235,13 +235,11 @@ class ConcurrentMarkingVisitor final
   ConcurrentMarkingState* marking_state() { return &marking_state_; }
 
  private:
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target) {
-    if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target))
-      return;
+  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target) {
+    if (!MarkCompactCollector::ShouldRecordRelocSlot(rinfo, target)) return;
 
     MarkCompactCollector::RecordRelocSlotInfo info =
-        MarkCompactCollector::ProcessRelocInfo(host, rinfo, target);
+        MarkCompactCollector::ProcessRelocInfo(rinfo, target);
 
     MemoryChunkData& data = (*memory_chunk_data_)[info.memory_chunk];
     if (!data.typed_slots) {
diff --git a/src/heap/evacuation-verifier.cc b/src/heap/evacuation-verifier.cc
index 973eabc6d0..5134b7f02f 100644
--- a/src/heap/evacuation-verifier.cc
+++ b/src/heap/evacuation-verifier.cc
@@ -25,8 +25,7 @@ void EvacuationVerifier::VisitPointers(HeapObject host, MaybeObjectSlot start,
   VerifyPointers(start, end);
 }
 
-void EvacuationVerifier::VisitCodePointer(HeapObject host,
-                                          CodeObjectSlot slot) {
+void EvacuationVerifier::VisitCodePointer(Code host, CodeObjectSlot slot) {
   VerifyCodePointer(slot);
 }
 
@@ -112,14 +111,12 @@ void FullEvacuationVerifier::VerifyCodePointer(CodeObjectSlot slot) {
     VerifyHeapObjectImpl(code);
   }
 }
-void FullEvacuationVerifier::VisitCodeTarget(InstructionStream host,
-                                             RelocInfo* rinfo) {
+void FullEvacuationVerifier::VisitCodeTarget(RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
-void FullEvacuationVerifier::VisitEmbeddedPointer(InstructionStream host,
-                                                  RelocInfo* rinfo) {
+void FullEvacuationVerifier::VisitEmbeddedPointer(RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 void FullEvacuationVerifier::VerifyRootPointers(FullObjectSlot start,
@@ -158,14 +155,12 @@ void YoungGenerationEvacuationVerifier::VerifyCodePointer(CodeObjectSlot slot) {
     VerifyHeapObjectImpl(code);
   }
 }
-void YoungGenerationEvacuationVerifier::VisitCodeTarget(InstructionStream host,
-                                                        RelocInfo* rinfo) {
+void YoungGenerationEvacuationVerifier::VisitCodeTarget(RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
-void YoungGenerationEvacuationVerifier::VisitEmbeddedPointer(
-    InstructionStream host, RelocInfo* rinfo) {
+void YoungGenerationEvacuationVerifier::VisitEmbeddedPointer(RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 void YoungGenerationEvacuationVerifier::VerifyRootPointers(FullObjectSlot start,
diff --git a/src/heap/evacuation-verifier.h b/src/heap/evacuation-verifier.h
index 764b939766..039d793f33 100644
--- a/src/heap/evacuation-verifier.h
+++ b/src/heap/evacuation-verifier.h
@@ -26,7 +26,7 @@ class EvacuationVerifier : public ObjectVisitorWithCageBases,
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
@@ -70,8 +70,8 @@ class FullEvacuationVerifier : public EvacuationVerifier {
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override;
   void VerifyPointers(MaybeObjectSlot start, MaybeObjectSlot end) override;
   void VerifyCodePointer(CodeObjectSlot slot) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitCodeTarget(RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override;
 };
 
@@ -91,8 +91,8 @@ class YoungGenerationEvacuationVerifier : public EvacuationVerifier {
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override;
   void VerifyPointers(MaybeObjectSlot start, MaybeObjectSlot end) override;
   void VerifyCodePointer(CodeObjectSlot slot) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitCodeTarget(RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override;
 };
 
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index c903c328b2..3242976926 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -74,19 +74,43 @@ Handle<AccessorPair> FactoryBase<Impl>::NewAccessorPair() {
 }
 
 template <typename Impl>
-Handle<Code> FactoryBase<Impl>::NewCode(int flags, AllocationType allocation) {
+Handle<Code> FactoryBase<Impl>::NewCode(const NewCodeOptions& options) {
   Map map = read_only_roots().code_map();
   int size = map.instance_size();
-  DCHECK_NE(allocation, AllocationType::kYoung);
-  Code data_container =
-      Code::cast(AllocateRawWithImmortalMap(size, allocation, map));
+  DCHECK_NE(options.allocation, AllocationType::kYoung);
+  Code code =
+      Code::cast(AllocateRawWithImmortalMap(size, options.allocation, map));
   DisallowGarbageCollection no_gc;
-  data_container.set_kind_specific_flags(flags, kRelaxedStore);
+  code.initialize_flags(options.kind, options.builtin, options.is_turbofanned,
+                        options.stack_slots);
+  code.set_kind_specific_flags(options.kind_specific_flags, kRelaxedStore);
   Isolate* isolate_for_sandbox = impl()->isolate_for_sandbox();
-  data_container.set_raw_instruction_stream(Smi::zero(), SKIP_WRITE_BARRIER);
-  data_container.init_code_entry_point(isolate_for_sandbox, kNullAddress);
-  data_container.clear_padding();
-  return handle(data_container, isolate());
+  code.set_raw_instruction_stream(Smi::zero(), SKIP_WRITE_BARRIER);
+  code.init_code_entry_point(isolate_for_sandbox, kNullAddress);
+  code.set_instruction_size(options.instruction_size);
+  code.set_metadata_size(options.metadata_size);
+  code.set_relocation_info(*options.reloc_info);
+  code.set_inlined_bytecode_size(options.inlined_bytecode_size);
+  code.set_osr_offset(options.osr_offset);
+  code.set_handler_table_offset(options.handler_table_offset);
+  code.set_constant_pool_offset(options.constant_pool_offset);
+  code.set_code_comments_offset(options.code_comments_offset);
+  code.set_unwinding_info_offset(options.unwinding_info_offset);
+
+  if (options.kind == CodeKind::BASELINE) {
+    code.set_bytecode_or_interpreter_data(
+        *options.bytecode_or_deoptimization_data);
+    code.set_bytecode_offset_table(
+        *options.bytecode_offsets_or_source_position_table);
+  } else {
+    code.set_deoptimization_data(
+        FixedArray::cast(*options.bytecode_or_deoptimization_data));
+    code.set_source_position_table(
+        *options.bytecode_offsets_or_source_position_table);
+  }
+
+  code.clear_padding();
+  return handle(code, isolate());
 }
 
 template <typename Impl>
@@ -325,7 +349,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfoForLiteral(
     FunctionLiteral* literal, Handle<Script> script, bool is_toplevel) {
   FunctionKind kind = literal->kind();
   Handle<SharedFunctionInfo> shared = NewSharedFunctionInfo(
-      literal->GetName(isolate()), MaybeHandle<InstructionStream>(),
+      literal->GetName(isolate()), MaybeHandle<HeapObject>(),
       Builtin::kCompileLazy, kind);
   SharedFunctionInfo::InitFromFunctionLiteral(isolate(), shared, literal,
                                               is_toplevel);
@@ -429,8 +453,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfo(
     // If we pass function_data then we shouldn't pass a builtin index, and
     // the function_data should not be code with a builtin.
     DCHECK(!Builtins::IsBuiltinId(builtin));
-    DCHECK_IMPLIES(function_data->IsInstructionStream(),
-                   !InstructionStream::cast(*function_data).is_builtin());
+    DCHECK(!function_data->IsInstructionStream());
     raw.set_function_data(*function_data, kReleaseStore);
   } else if (Builtins::IsBuiltinId(builtin)) {
     raw.set_builtin_id(builtin);
diff --git a/src/heap/factory-base.h b/src/heap/factory-base.h
index d4b66b07b9..9739d52e40 100644
--- a/src/heap/factory-base.h
+++ b/src/heap/factory-base.h
@@ -8,6 +8,7 @@
 #include "src/base/export-template.h"
 #include "src/base/strings.h"
 #include "src/common/globals.h"
+#include "src/objects/code-kind.h"
 #include "src/objects/fixed-array.h"
 #include "src/objects/function-kind.h"
 #include "src/objects/instance-type.h"
@@ -60,6 +61,26 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) TorqueGeneratedFactory {
 #include "torque-generated/factory.inc"
 };
 
+struct NewCodeOptions {
+  CodeKind kind;
+  Builtin builtin;
+  bool is_turbofanned;
+  int stack_slots;
+  int kind_specific_flags;
+  AllocationType allocation;
+  int instruction_size;
+  int metadata_size;
+  unsigned int inlined_bytecode_size;
+  BytecodeOffset osr_offset;
+  int handler_table_offset;
+  int constant_pool_offset;
+  int code_comments_offset;
+  int32_t unwinding_info_offset;
+  Handle<ByteArray> reloc_info;
+  Handle<HeapObject> bytecode_or_deoptimization_data;
+  Handle<ByteArray> bytecode_offsets_or_source_position_table;
+};
+
 template <typename Impl>
 class FactoryBase : public TorqueGeneratedFactory<Impl> {
  public:
@@ -99,7 +120,7 @@ class FactoryBase : public TorqueGeneratedFactory<Impl> {
   Handle<AccessorPair> NewAccessorPair();
 
   // Creates a new Code for a InstructionStream object.
-  Handle<Code> NewCode(int flags, AllocationType allocation);
+  Handle<Code> NewCode(const NewCodeOptions& options);
 
   // Allocates a fixed array initialized with undefined values.
   Handle<FixedArray> NewFixedArray(
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 1f88a8e065..4c17596e7d 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -115,15 +115,34 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
           : factory->NewByteArray(code_desc_.reloc_size, AllocationType::kOld);
 
   Handle<Code> code;
+
+  NewCodeOptions new_code_options = {
+      /*kind=*/kind_,
+      /*builtin=*/builtin_,
+      /*is_turbofanned=*/is_turbofanned_,
+      /*stack_slots=*/stack_slots_,
+      /*kind_specific_flags=*/kind_specific_flags_,
+      /*allocation=*/AllocationType::kOld,
+      /*instruction_size=*/code_desc_.instruction_size(),
+      /*metadata_size=*/code_desc_.metadata_size(),
+      /*inlined_bytecode_size=*/inlined_bytecode_size_,
+      /*osr_offset=*/osr_offset_,
+      /*handler_table_offset=*/code_desc_.handler_table_offset_relative(),
+      /*constant_pool_offset=*/code_desc_.constant_pool_offset_relative(),
+      /*code_comments_offset=*/code_desc_.code_comments_offset_relative(),
+      /*unwinding_info_offset=*/code_desc_.unwinding_info_offset_relative(),
+      /*reloc_info=*/reloc_info,
+      /*bytecode_or_deoptimization_data=*/kind_ == CodeKind::BASELINE
+          ? interpreter_data_
+          : deoptimization_data_,
+      /*bytecode_offsets_or_source_position_table=*/position_table_};
+
   if (CompiledWithConcurrentBaseline()) {
-    code = local_isolate_->factory()->NewCode(0, AllocationType::kOld);
+    code = local_isolate_->factory()->NewCode(new_code_options);
   } else {
-    code = factory->NewCode(0, AllocationType::kOld);
+    code = factory->NewCode(new_code_options);
   }
 
-  code->initialize_flags(kind_, builtin_, is_turbofanned_);
-  code->set_kind_specific_flags(kind_specific_flags_, kRelaxedStore);
-
   // Basic block profiling data for builtins is stored in the JS heap rather
   // than in separately-allocated C++ objects. Allocate that data now if
   // appropriate.
@@ -159,32 +178,10 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     InstructionStream raw_istream = *instruction_stream;
     DisallowGarbageCollection no_gc;
 
-    raw_istream.set_instruction_size(code_desc_.instruction_size());
-    raw_istream.set_metadata_size(code_desc_.metadata_size());
-    raw_istream.set_relocation_info(*reloc_info);
-    raw_istream.initialize_flags(kind_, is_turbofanned_, stack_slots_);
-    raw_istream.set_builtin_id(builtin_);
     // This might impact direct concurrent reads from TF if we are resetting
     // this field. We currently assume it's immutable thus a relaxed read (after
     // passing IsPendingAllocation).
-    raw_istream.set_inlined_bytecode_size(inlined_bytecode_size_);
-    raw_istream.set_osr_offset(osr_offset_);
     raw_istream.set_code(*code, kReleaseStore);
-    if (kind_ == CodeKind::BASELINE) {
-      raw_istream.set_bytecode_or_interpreter_data(*interpreter_data_);
-      raw_istream.set_bytecode_offset_table(*position_table_);
-    } else {
-      raw_istream.set_deoptimization_data(*deoptimization_data_);
-      raw_istream.set_source_position_table(*position_table_);
-    }
-    raw_istream.set_handler_table_offset(
-        code_desc_.handler_table_offset_relative());
-    raw_istream.set_constant_pool_offset(
-        code_desc_.constant_pool_offset_relative());
-    raw_istream.set_code_comments_offset(
-        code_desc_.code_comments_offset_relative());
-    raw_istream.set_unwinding_info_offset(
-        code_desc_.unwinding_info_offset_relative());
 
     // Allow self references to created code object by patching the handle to
     // point to the newly allocated InstructionStream object.
@@ -209,20 +206,21 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
               handle(on_heap_profiler_data->counts(), isolate_));
     }
 
+    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
+      raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
+    }
+    code->SetInstructionStreamAndEntryPoint(isolate_, raw_istream);
+
     // Migrate generated code.
     // The generated code can contain embedded objects (typically from
     // handles) in a pointer-to-tagged-value format (i.e. with indirection
     // like a handle) that are dereferenced during the copy to point directly
     // to the actual heap objects. These pointers can include references to
     // the code object itself, through the self_reference parameter.
-    raw_istream.CopyFromNoFlush(*reloc_info, heap, code_desc_);
+    code->CopyFromNoFlush(*reloc_info, heap, code_desc_);
 
-    raw_istream.clear_padding();
+    code->ClearInstructionStreamPadding();
 
-    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
-    }
-    code->SetInstructionStreamAndEntryPoint(isolate_, raw_istream);
 #ifdef VERIFY_HEAP
     if (v8_flags.verify_heap) {
       HeapObject::VerifyCodePointer(isolate_, raw_istream);
@@ -234,7 +232,7 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     // some older ARM kernels there is a bug which causes an access error on
     // cache flush instructions to trigger access error on non-writable memory.
     // See https://bugs.chromium.org/p/v8/issues/detail?id=8157
-    raw_istream.FlushICache();
+    code->FlushICache();
   }
 
   if (V8_UNLIKELY(profiler_data_ && v8_flags.turbo_profiling_verbose)) {
@@ -278,13 +276,13 @@ MaybeHandle<InstructionStream> Factory::CodeBuilder::AllocateInstructionStream(
   DisallowGarbageCollection no_gc;
   result.set_map_after_allocation(
       *isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
-  Handle<InstructionStream> code =
+  Handle<InstructionStream> istream =
       handle(InstructionStream::cast(result), isolate_);
-  DCHECK(IsAligned(code->address(), kCodeAlignment));
+  DCHECK(IsAligned(istream->address(), kCodeAlignment));
   DCHECK_IMPLIES(
       !V8_ENABLE_THIRD_PARTY_HEAP_BOOL && !heap->code_region().is_empty(),
-      heap->code_region().contains(code->address()));
-  return code;
+      heap->code_region().contains(istream->address()));
+  return istream;
 }
 
 MaybeHandle<InstructionStream>
@@ -303,10 +301,10 @@ Factory::CodeBuilder::AllocateConcurrentSparkplugInstructionStream(
   DisallowGarbageCollection no_gc;
   result.set_map_after_allocation(
       *local_isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
-  Handle<InstructionStream> code =
+  Handle<InstructionStream> istream =
       handle(InstructionStream::cast(result), local_isolate_);
-  DCHECK(IsAligned(code->address(), kCodeAlignment));
-  return code;
+  DCHECK(IsAligned(istream->address(), kCodeAlignment));
+  return istream;
 }
 
 MaybeHandle<Code> Factory::CodeBuilder::TryBuild() {
@@ -2502,16 +2500,38 @@ Handle<Code> Factory::NewOffHeapTrampolineFor(Handle<Code> code,
   CHECK_NE(0, isolate()->embedded_blob_code_size());
   CHECK(Builtins::IsIsolateIndependentBuiltin(*code));
 
-  const int no_flags = 0;
-  Handle<Code> off_heap_trampoline = NewCode(no_flags, AllocationType::kOld);
-
-  off_heap_trampoline->initialize_flags(code->kind(), code->builtin_id(),
-                                        code->is_turbofanned());
-  off_heap_trampoline->set_kind_specific_flags(
-      code->kind_specific_flags(kRelaxedLoad), kRelaxedStore);
+  NewCodeOptions new_code_options = {
+      /*kind=*/code->kind(),
+      /*builtin=*/code->builtin_id(),
+      /*is_turbofanned=*/code->is_turbofanned(),
+      /*stack_slots=*/code->stack_slots(),
+      /*kind_specific_flags=*/code->kind_specific_flags(kRelaxedLoad),
+      /*allocation=*/AllocationType::kOld,
+      /*instruction_size=*/code->instruction_size(),
+      /*metadata_size=*/code->metadata_size(),
+      /*inlined_bytecode_size=*/code->inlined_bytecode_size(),
+      /*osr_offset=*/code->osr_offset(),
+      /*handler_table_offset=*/code->handler_table_offset(),
+      /*constant_pool_offset=*/code->constant_pool_offset(),
+      /*code_comments_offset=*/code->code_comments_offset(),
+      /*unwinding_info_offset=*/code->unwinding_info_offset(),
+      /*reloc_info=*/
+      Handle<ByteArray>(read_only_roots().empty_byte_array(), isolate()),
+      /*bytecode_or_deoptimization_data=*/
+      Handle<FixedArray>(read_only_roots().empty_fixed_array(), isolate()),
+      /*bytecode_offsets_or_source_position_table=*/
+      Handle<ByteArray>(read_only_roots().empty_byte_array(), isolate())};
+
+  Handle<Code> off_heap_trampoline = NewCode(new_code_options);
   off_heap_trampoline->set_code_entry_point(isolate(),
                                             code->code_entry_point());
-  return Handle<Code>::cast(off_heap_trampoline);
+
+  DCHECK_EQ(code->instruction_size(), code->OffHeapInstructionSize());
+  DCHECK_EQ(code->metadata_size(), code->OffHeapMetadataSize());
+  DCHECK_EQ(code->inlined_bytecode_size(), 0);
+  DCHECK_EQ(code->osr_offset(), BytecodeOffset::None());
+
+  return off_heap_trampoline;
 }
 
 Handle<BytecodeArray> Factory::CopyBytecodeArray(Handle<BytecodeArray> source) {
@@ -3380,8 +3400,8 @@ Handle<SharedFunctionInfo> Factory::NewSharedFunctionInfoForApiFunction(
 
 Handle<SharedFunctionInfo> Factory::NewSharedFunctionInfoForBuiltin(
     MaybeHandle<String> maybe_name, Builtin builtin, FunctionKind kind) {
-  return NewSharedFunctionInfo(maybe_name, MaybeHandle<InstructionStream>(),
-                               builtin, kind);
+  return NewSharedFunctionInfo(maybe_name, MaybeHandle<HeapObject>(), builtin,
+                               kind);
 }
 
 int Factory::NumberToStringCacheHash(Smi number) {
diff --git a/src/heap/heap-verifier.cc b/src/heap/heap-verifier.cc
index f52f6203f0..b550d621e7 100644
--- a/src/heap/heap-verifier.cc
+++ b/src/heap/heap-verifier.cc
@@ -62,9 +62,9 @@ class VerifyPointersVisitor : public ObjectVisitorWithCageBases,
                      ObjectSlot end) override;
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
+  void VisitCodeTarget(RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
@@ -97,8 +97,7 @@ void VerifyPointersVisitor::VisitPointers(HeapObject host,
   VerifyPointers(host, start, end);
 }
 
-void VerifyPointersVisitor::VisitCodePointer(HeapObject host,
-                                             CodeObjectSlot slot) {
+void VerifyPointersVisitor::VisitCodePointer(Code host, CodeObjectSlot slot) {
   Object maybe_code = slot.load(code_cage_base());
   HeapObject code;
   // The slot might contain smi during Code creation.
@@ -164,15 +163,13 @@ void VerifyPointersVisitor::VerifyPointers(HeapObject host,
   VerifyPointersImpl(start, end);
 }
 
-void VerifyPointersVisitor::VisitCodeTarget(InstructionStream host,
-                                            RelocInfo* rinfo) {
+void VerifyPointersVisitor::VisitCodeTarget(RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
 
-void VerifyPointersVisitor::VisitEmbeddedPointer(InstructionStream host,
-                                                 RelocInfo* rinfo) {
+void VerifyPointersVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 
@@ -477,17 +474,18 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     if (ShouldHaveBeenRecorded(
             host, MaybeObject::FromObject(slot.load(code_cage_base())))) {
       CHECK_GT(untyped_->count(slot.address()), 0);
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     Object target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
+    if (ShouldHaveBeenRecorded(rinfo->instruction_stream(),
+                               MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kCodeEntry, rinfo->pc()) ||
             (rinfo->IsInConstantPool() &&
              InTypedSet(SlotType::kConstPoolCodeEntry,
@@ -495,9 +493,10 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     Object target = rinfo->target_object(cage_base());
-    if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
+    if (ShouldHaveBeenRecorded(rinfo->instruction_stream(),
+                               MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kEmbeddedObjectFull, rinfo->pc()) ||
             InTypedSet(SlotType::kEmbeddedObjectCompressed, rinfo->pc()) ||
             (rinfo->IsInConstantPool() &&
@@ -607,20 +606,16 @@ class SlotCollectingVisitor final : public ObjectVisitor {
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
 #ifdef V8_EXTERNAL_CODE_SPACE
     code_slots_.push_back(slot);
 #endif
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
-    UNREACHABLE();
-  }
+  void VisitCodeTarget(RelocInfo* rinfo) final { UNREACHABLE(); }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
-    UNREACHABLE();
-  }
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
   void VisitMapPointer(HeapObject object) override {}  // do nothing by default
 
diff --git a/src/heap/heap-write-barrier-inl.h b/src/heap/heap-write-barrier-inl.h
index 6e60ad03ed..dd850bbf01 100644
--- a/src/heap/heap-write-barrier-inl.h
+++ b/src/heap/heap-write-barrier-inl.h
@@ -30,8 +30,8 @@ V8_EXPORT_PRIVATE void Heap_CombinedGenerationalAndSharedBarrierSlow(
 V8_EXPORT_PRIVATE void Heap_CombinedGenerationalAndSharedEphemeronBarrierSlow(
     EphemeronHashTable table, Address slot, HeapObject value);
 
-V8_EXPORT_PRIVATE void Heap_GenerationalBarrierForCodeSlow(
-    InstructionStream host, RelocInfo* rinfo, HeapObject object);
+V8_EXPORT_PRIVATE void Heap_GenerationalBarrierForCodeSlow(RelocInfo* rinfo,
+                                                           HeapObject object);
 
 V8_EXPORT_PRIVATE void Heap_GenerationalEphemeronKeyBarrierSlow(
     Heap* heap, HeapObject table, Address slot);
@@ -146,7 +146,7 @@ inline void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
   }
 
   DCHECK_EQ(mode, UPDATE_WRITE_BARRIER);
-  GenerationalBarrierForCode(host, rinfo, value);
+  GenerationalBarrierForCode(rinfo, value);
   WriteBarrier::Shared(host, rinfo, value);
   WriteBarrier::Marking(host, rinfo, value);
 }
@@ -214,13 +214,12 @@ inline void CombinedEphemeronWriteBarrier(EphemeronHashTable host,
   }
 }
 
-inline void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                       HeapObject object) {
+inline void GenerationalBarrierForCode(RelocInfo* rinfo, HeapObject object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
   heap_internals::MemoryChunk* object_chunk =
       heap_internals::MemoryChunk::FromHeapObject(object);
   if (!object_chunk->InYoungGeneration()) return;
-  Heap_GenerationalBarrierForCodeSlow(host, rinfo, object);
+  Heap_GenerationalBarrierForCodeSlow(rinfo, object);
 }
 
 inline WriteBarrierMode GetWriteBarrierModeForObject(
@@ -306,7 +305,7 @@ void WriteBarrier::Shared(InstructionStream host, RelocInfo* reloc_info,
       heap_internals::MemoryChunk::FromHeapObject(value);
   if (!value_chunk->InWritableSharedSpace()) return;
 
-  SharedSlow(host, reloc_info, value);
+  SharedSlow(reloc_info, value);
 }
 
 void WriteBarrier::Marking(JSArrayBuffer host,
diff --git a/src/heap/heap-write-barrier.cc b/src/heap/heap-write-barrier.cc
index 25e533d969..b730ce261e 100644
--- a/src/heap/heap-write-barrier.cc
+++ b/src/heap/heap-write-barrier.cc
@@ -68,10 +68,9 @@ void WriteBarrier::MarkingSlow(InstructionStream host, RelocInfo* reloc_info,
   marking_barrier->Write(host, reloc_info, value);
 }
 
-void WriteBarrier::SharedSlow(InstructionStream host, RelocInfo* reloc_info,
-                              HeapObject value) {
+void WriteBarrier::SharedSlow(RelocInfo* reloc_info, HeapObject value) {
   MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(host, reloc_info, value);
+      MarkCompactCollector::ProcessRelocInfo(reloc_info, value);
 
   base::MutexGuard write_scope(info.memory_chunk->mutex());
   RememberedSet<OLD_TO_SHARED>::InsertTyped(info.memory_chunk, info.slot_type,
@@ -164,17 +163,7 @@ int WriteBarrier::SharedFromCode(Address raw_host, Address raw_slot) {
 bool WriteBarrier::IsImmortalImmovableHeapObject(HeapObject object) {
   BasicMemoryChunk* basic_chunk = BasicMemoryChunk::FromHeapObject(object);
   // All objects in readonly space are immortal and immovable.
-  if (basic_chunk->InReadOnlySpace()) return true;
-  MemoryChunk* chunk = MemoryChunk::FromHeapObject(object);
-  // There are also objects in "regular" spaces which are immortal and
-  // immovable. Objects on a page that can get compacted are movable and can be
-  // filtered out.
-  if (!chunk->IsFlagSet(MemoryChunk::NEVER_EVACUATE)) return false;
-  // Builtins don't have InstructionStream objects (instead, they point
-  // directly into off-heap code streams).
-  DCHECK_IMPLIES(object.IsInstructionStream(),
-                 !InstructionStream::cast(object).is_builtin());
-  return false;
+  return basic_chunk->InReadOnlySpace();
 }
 #endif
 
diff --git a/src/heap/heap-write-barrier.h b/src/heap/heap-write-barrier.h
index 0ea460ac2d..f44af9a9f1 100644
--- a/src/heap/heap-write-barrier.h
+++ b/src/heap/heap-write-barrier.h
@@ -44,8 +44,7 @@ void CombinedEphemeronWriteBarrier(EphemeronHashTable object, ObjectSlot slot,
                                    Object value, WriteBarrierMode mode);
 
 // Generational write barrier.
-void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
-                                HeapObject object);
+void GenerationalBarrierForCode(RelocInfo* rinfo, HeapObject object);
 
 inline bool IsReadOnlyHeapObject(HeapObject object);
 
@@ -105,7 +104,7 @@ class V8_EXPORT_PRIVATE WriteBarrier {
                                                            size_t argc,
                                                            void** values);
 
-  static void SharedSlow(InstructionStream host, RelocInfo*, HeapObject value);
+  static void SharedSlow(RelocInfo*, HeapObject value);
 
   friend class Heap;
 };
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index 61194c6648..9acdabf3e0 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -158,9 +158,8 @@ void Heap_CombinedGenerationalAndSharedEphemeronBarrierSlow(
   Heap::CombinedGenerationalAndSharedEphemeronBarrierSlow(table, slot, value);
 }
 
-void Heap_GenerationalBarrierForCodeSlow(InstructionStream host,
-                                         RelocInfo* rinfo, HeapObject object) {
-  Heap::GenerationalBarrierForCodeSlow(host, rinfo, object);
+void Heap_GenerationalBarrierForCodeSlow(RelocInfo* rinfo, HeapObject object) {
+  Heap::GenerationalBarrierForCodeSlow(rinfo, object);
 }
 
 void Heap::SetConstructStubCreateDeoptPCOffset(int pc_offset) {
@@ -1261,19 +1260,14 @@ void Heap::PublishPendingAllocations() {
   code_lo_space_->ResetPendingObject();
 }
 
-void Heap::InvalidateCodeDeoptimizationData(InstructionStream code) {
-  CodePageMemoryModificationScope modification_scope(code);
-  code.set_deoptimization_data(ReadOnlyRoots(this).empty_fixed_array());
-}
-
 void Heap::DeoptMarkedAllocationSites() {
   // TODO(hpayer): If iterating over the allocation sites list becomes a
   // performance issue, use a cache data structure in heap instead.
 
-  ForeachAllocationSite(allocation_sites_list(), [](AllocationSite site) {
+  ForeachAllocationSite(allocation_sites_list(), [this](AllocationSite site) {
     if (site.deopt_dependent_code()) {
       DependentCode::MarkCodeForDeoptimization(
-          site, DependentCode::kAllocationSiteTenuringChangedGroup);
+          isolate_, site, DependentCode::kAllocationSiteTenuringChangedGroup);
       site.set_deopt_dependent_code(false);
     }
   });
@@ -6289,7 +6283,7 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
       MarkPointers(start, end);
     }
 
-    void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+    void VisitCodePointer(Code host, CodeObjectSlot slot) override {
       Object maybe_code = slot.load(code_cage_base());
       HeapObject heap_object;
       if (maybe_code.GetHeapObject(&heap_object)) {
@@ -6297,12 +6291,12 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
       }
     }
 
-    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
+    void VisitCodeTarget(RelocInfo* rinfo) final {
       InstructionStream target =
           InstructionStream::FromTargetAddress(rinfo->target_address());
       MarkHeapObject(target);
     }
-    void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final {
+    void VisitEmbeddedPointer(RelocInfo* rinfo) final {
       MarkHeapObject(rinfo->target_object(cage_base()));
     }
 
@@ -6823,43 +6817,35 @@ GcSafeCode Heap::GcSafeGetCodeFromInstructionStream(
   return GcSafeCode::unchecked_cast(istream.raw_code(kAcquireLoad));
 }
 
-bool Heap::GcSafeInstructionStreamContains(InstructionStream instruction_stream,
+bool Heap::GcSafeInstructionStreamContains(InstructionStream istream,
                                            Address addr) {
-  Map map = GcSafeMapOfHeapObject(instruction_stream);
+  Map map = GcSafeMapOfHeapObject(istream);
   DCHECK_EQ(map, ReadOnlyRoots(this).instruction_stream_map());
 
   Builtin builtin_lookup_result =
       OffHeapInstructionStream::TryLookupCode(isolate(), addr);
   if (Builtins::IsBuiltinId(builtin_lookup_result)) {
     // Builtins don't have InstructionStream objects.
-    DCHECK(!Builtins::IsBuiltinId(instruction_stream.builtin_id()));
+    DCHECK(!Builtins::IsBuiltinId(istream.code(kAcquireLoad).builtin_id()));
     return false;
   }
 
-  Address start = instruction_stream.address();
-  Address end = start + instruction_stream.SizeFromMap(map);
+  Address start = istream.address();
+  Address end = start + istream.SizeFromMap(map);
   return start <= addr && addr < end;
 }
 
-base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
-    Address inner_pointer) {
-  Builtin maybe_builtin =
-      OffHeapInstructionStream::TryLookupCode(isolate(), inner_pointer);
-  if (Builtins::IsBuiltinId(maybe_builtin)) {
-    return GcSafeCode::cast(isolate()->builtins()->code(maybe_builtin));
-  }
-
+base::Optional<InstructionStream>
+Heap::GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     Address start = tp_heap_->GetObjectFromInnerPointer(inner_pointer);
-    return GcSafeGetCodeFromInstructionStream(HeapObject::FromAddress(start),
-                                              inner_pointer);
+    return InstructionStream::unchecked_cast(HeapObject::FromAddress(start));
   }
 
   // Check if the inner pointer points into a large object chunk.
   LargePage* large_page = code_lo_space()->FindPage(inner_pointer);
   if (large_page != nullptr) {
-    return GcSafeGetCodeFromInstructionStream(large_page->GetObject(),
-                                              inner_pointer);
+    return InstructionStream::unchecked_cast(large_page->GetObject());
   }
 
   if (V8_LIKELY(code_space()->Contains(inner_pointer))) {
@@ -6870,13 +6856,27 @@ base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
     Address start =
         page->GetCodeObjectRegistry()->GetCodeObjectStartFromInnerAddress(
             inner_pointer);
-    return GcSafeGetCodeFromInstructionStream(HeapObject::FromAddress(start),
-                                              inner_pointer);
+    return InstructionStream::unchecked_cast(HeapObject::FromAddress(start));
   }
 
   return {};
 }
 
+base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
+    Address inner_pointer) {
+  Builtin maybe_builtin =
+      OffHeapInstructionStream::TryLookupCode(isolate(), inner_pointer);
+  if (Builtins::IsBuiltinId(maybe_builtin)) {
+    return GcSafeCode::cast(isolate()->builtins()->code(maybe_builtin));
+  }
+
+  base::Optional<InstructionStream> maybe_istream =
+      GcSafeTryFindInstructionStreamForInnerPointer(inner_pointer);
+  if (!maybe_istream) return {};
+
+  return GcSafeGetCodeFromInstructionStream(*maybe_istream, inner_pointer);
+}
+
 Code Heap::FindCodeForInnerPointer(Address inner_pointer) {
   return GcSafeFindCodeForInnerPointer(inner_pointer).UnsafeCastToCode();
 }
@@ -7079,11 +7079,10 @@ void Heap::WriteBarrierForRange(HeapObject object, TSlot start_slot,
   }
 }
 
-void Heap::GenerationalBarrierForCodeSlow(InstructionStream host,
-                                          RelocInfo* rinfo, HeapObject object) {
+void Heap::GenerationalBarrierForCodeSlow(RelocInfo* rinfo, HeapObject object) {
   DCHECK(InYoungGeneration(object));
   const MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(host, rinfo, object);
+      MarkCompactCollector::ProcessRelocInfo(rinfo, object);
 
   RememberedSet<OLD_TO_NEW>::InsertTyped(info.memory_chunk, info.slot_type,
                                          info.offset);
diff --git a/src/heap/heap.h b/src/heap/heap.h
index d2511d7ed8..3a5cc2612e 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -485,7 +485,7 @@ class Heap {
   V8_EXPORT_PRIVATE static void EphemeronKeyWriteBarrierFromCode(
       Address raw_object, Address address, Isolate* isolate);
   V8_EXPORT_PRIVATE static void GenerationalBarrierForCodeSlow(
-      InstructionStream host, RelocInfo* rinfo, HeapObject value);
+      RelocInfo* rinfo, HeapObject value);
   V8_EXPORT_PRIVATE static bool PageFlagsAreConsistent(HeapObject object);
 
   // Notifies the heap that is ok to start marking or other activities that
@@ -1162,10 +1162,6 @@ class Heap {
   void SetConstructStubInvokeDeoptPCOffset(int pc_offset);
   void SetInterpreterEntryReturnPCOffset(int pc_offset);
 
-  // Invalidates references in the given {code} object that are referenced
-  // transitively from the deoptimization data. Mutates write-protected code.
-  void InvalidateCodeDeoptimizationData(InstructionStream code);
-
   void DeoptMarkedAllocationSites();
 
   // ===========================================================================
@@ -1565,6 +1561,8 @@ class Heap {
   GcSafeCode GcSafeFindCodeForInnerPointer(Address inner_pointer);
   base::Optional<GcSafeCode> GcSafeTryFindCodeForInnerPointer(
       Address inner_pointer);
+  base::Optional<InstructionStream>
+  GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer);
   // Only intended for use from the `jco` gdb macro.
   base::Optional<Code> TryFindCodeForInnerPointerForPrinting(
       Address inner_pointer);
diff --git a/src/heap/mark-compact-inl.h b/src/heap/mark-compact-inl.h
index 29aa49a397..1556fc0d64 100644
--- a/src/heap/mark-compact-inl.h
+++ b/src/heap/mark-compact-inl.h
@@ -100,10 +100,9 @@ void MainMarkingVisitor<MarkingState>::RecordSlot(HeapObject object, TSlot slot,
 }
 
 template <typename MarkingState>
-void MainMarkingVisitor<MarkingState>::RecordRelocSlot(InstructionStream host,
-                                                       RelocInfo* rinfo,
+void MainMarkingVisitor<MarkingState>::RecordRelocSlot(RelocInfo* rinfo,
                                                        HeapObject target) {
-  MarkCompactCollector::RecordRelocSlot(host, rinfo, target);
+  MarkCompactCollector::RecordRelocSlot(rinfo, target);
 }
 
 template <LiveObjectIterationMode mode>
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index cb2091c7ad..a28b338f54 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -131,7 +131,7 @@ class MarkingVerifier : public ObjectVisitorWithCageBases, public RootVisitor {
     VerifyPointers(start, end);
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     VerifyCodePointer(slot);
   }
 
@@ -273,16 +273,16 @@ class FullMarkingVerifier : public MarkingVerifier {
     VerifyPointersImpl(start, end);
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
     HeapObject target_object = rinfo->target_object(cage_base());
-    if (!host.IsWeakObject(target_object)) {
+    if (!rinfo->code().IsWeakObject(target_object)) {
       VerifyHeapObjectImpl(target_object);
     }
   }
@@ -1046,14 +1046,14 @@ class MarkCompactCollector::RootMarkingVisitor final : public RootVisitor {
     Object istream_or_smi_zero = *istream_or_smi_zero_slot;
     DCHECK(istream_or_smi_zero == Smi::zero() ||
            istream_or_smi_zero.IsInstructionStream());
-    DCHECK_EQ(Code::cast(*code_slot).raw_instruction_stream(),
-              istream_or_smi_zero);
+    Code code = Code::cast(*code_slot);
+    DCHECK_EQ(code.raw_instruction_stream(), istream_or_smi_zero);
+
+    // We must not remove deoptimization literals which may be needed in
+    // order to successfully deoptimize.
+    code.IterateDeoptimizationLiterals(this);
 
     if (istream_or_smi_zero != Smi::zero()) {
-      InstructionStream istream = InstructionStream::cast(istream_or_smi_zero);
-      // We must not remove deoptimization literals which may be needed in
-      // order to successfully deoptimize.
-      istream.IterateDeoptimizationLiterals(this);
       VisitRootPointer(Root::kStackRoots, nullptr, istream_or_smi_zero_slot);
     }
 
@@ -1108,7 +1108,7 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     MarkObject(host, slot.load(code_cage_base()));
   }
 
@@ -1118,14 +1118,14 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    MarkObject(host, target);
+    MarkObject(rinfo->instruction_stream(), target);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
-    MarkObject(host, rinfo->target_object(cage_base()));
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+    MarkObject(rinfo->instruction_stream(), rinfo->target_object(cage_base()));
   }
 
  private:
@@ -1163,7 +1163,7 @@ class MarkCompactCollector::ClientCustomRootBodyMarkingVisitor final
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     MarkObject(host, slot.load(code_cage_base()));
   }
 
@@ -1173,14 +1173,14 @@ class MarkCompactCollector::ClientCustomRootBodyMarkingVisitor final
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    MarkObject(host, target);
+    MarkObject(rinfo->instruction_stream(), target);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
-    MarkObject(host, rinfo->target_object(cage_base()));
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+    MarkObject(rinfo->instruction_stream(), rinfo->target_object(cage_base()));
   }
 
  private:
@@ -1225,7 +1225,7 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     UNREACHABLE();
   }
 
@@ -1238,13 +1238,9 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-    UNREACHABLE();
-  }
+  void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
-    UNREACHABLE();
-  }
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
  private:
   V8_INLINE void CheckForSharedObject(HeapObject host, ObjectSlot slot,
@@ -1386,16 +1382,11 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
                        MaybeObjectSlot end) override {
       UNREACHABLE();
     }
-    void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
-      UNREACHABLE();
-    }
-    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
-      UNREACHABLE();
-    }
-    void VisitEmbeddedPointer(InstructionStream host,
-                              RelocInfo* rinfo) override {
+    void VisitCodePointer(Code host, CodeObjectSlot slot) override {
       UNREACHABLE();
     }
+    void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
+    void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
    private:
     ExternalPointerTable* table_;
@@ -1481,7 +1472,7 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
+  inline void VisitCodePointer(Code host, CodeObjectSlot slot) final {
     // This code is similar to the implementation of VisitPointer() modulo
     // new kind of slot.
     DCHECK(!HasWeakHeapObjectTag(slot.load(code_cage_base())));
@@ -1514,9 +1505,7 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitCodeTarget(InstructionStream host,
-                              RelocInfo* rinfo) override {
-    DCHECK_EQ(host, rinfo->host());
+  inline void VisitCodeTarget(RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
@@ -1524,24 +1513,20 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     // the old-to-new remembered set.
     DCHECK(!Heap::InYoungGeneration(target));
     DCHECK(!target.InWritableSharedSpace());
-    heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, target);
+    heap_->mark_compact_collector()->RecordRelocSlot(rinfo, target);
   }
 
-  inline void VisitEmbeddedPointer(InstructionStream host,
-                                   RelocInfo* rinfo) override {
-    DCHECK_EQ(host, rinfo->host());
+  inline void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
     HeapObject object = rinfo->target_object(cage_base());
-    GenerationalBarrierForCode(host, rinfo, object);
-    WriteBarrier::Shared(host, rinfo, object);
-    heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, object);
+    GenerationalBarrierForCode(rinfo, object);
+    WriteBarrier::Shared(rinfo->instruction_stream(), rinfo, object);
+    heap_->mark_compact_collector()->RecordRelocSlot(rinfo, object);
   }
 
   // Entries that are skipped for recording.
-  inline void VisitExternalReference(InstructionStream host,
-                                     RelocInfo* rinfo) final {}
-  inline void VisitInternalReference(InstructionStream host,
-                                     RelocInfo* rinfo) final {}
+  inline void VisitExternalReference(RelocInfo* rinfo) final {}
+  inline void VisitInternalReference(RelocInfo* rinfo) final {}
   inline void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                    ExternalPointerTag tag) final {}
 
@@ -1690,8 +1675,8 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
     } else if (dest == CODE_SPACE) {
       DCHECK_CODEOBJECT_SIZE(size, base->heap_->code_space());
       base->heap_->CopyBlock(dst_addr, src_addr, size);
-      InstructionStream code = InstructionStream::cast(dst);
-      code.Relocate(dst_addr - src_addr);
+      InstructionStream istream = InstructionStream::cast(dst);
+      istream.Relocate(dst_addr - src_addr);
       if (mode != MigrationMode::kFast)
         base->ExecuteMigrationObservers(dest, src, dst, size);
       // In case the object's map gets relocated during GC we load the old map
@@ -2473,10 +2458,9 @@ void MarkCompactCollector::ProcessTopOptimizedFrame(ObjectVisitor* visitor,
     if (it.frame()->is_optimized()) {
       GcSafeCode lookup_result = it.frame()->GcSafeLookupCode();
       if (!lookup_result.has_instruction_stream()) return;
-      InstructionStream istream = InstructionStream::unchecked_cast(
-          lookup_result.raw_instruction_stream());
-      DCHECK_NE(istream, Smi::zero());
-      if (!istream.CanDeoptAt(isolate, it.frame()->pc())) {
+      if (!lookup_result.CanDeoptAt(isolate, it.frame()->pc())) {
+        InstructionStream istream = InstructionStream::unchecked_cast(
+            lookup_result.raw_instruction_stream());
         PtrComprCageBase cage_base(isolate);
         InstructionStream::BodyDescriptor::IterateBody(istream.map(cage_base),
                                                        istream, visitor);
@@ -3032,15 +3016,15 @@ void MarkCompactCollector::ClearNonLiveReferences() {
 }
 
 void MarkCompactCollector::MarkDependentCodeForDeoptimization() {
-  std::pair<HeapObject, InstructionStream> weak_object_in_code;
+  std::pair<HeapObject, Code> weak_object_in_code;
   while (local_weak_objects()->weak_objects_in_code_local.Pop(
       &weak_object_in_code)) {
     HeapObject object = weak_object_in_code.first;
-    InstructionStream code = weak_object_in_code.second;
+    Code code = weak_object_in_code.second;
     if (!non_atomic_marking_state()->IsBlackOrGrey(object) &&
         !code.embedded_objects_cleared()) {
       if (!code.marked_for_deoptimization()) {
-        code.SetMarkedForDeoptimization("weak objects");
+        code.SetMarkedForDeoptimization(isolate(), "weak objects");
         have_code_to_deoptimize_ = true;
       }
       code.ClearEmbeddedObjects(heap_);
@@ -3165,7 +3149,7 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
       // acquire-loaded.
       baseline_istream = FromCode(baseline_code, isolate(), kRelaxedLoad);
       baseline_bytecode_or_interpreter_data =
-          baseline_istream.bytecode_or_interpreter_data(isolate());
+          baseline_code.bytecode_or_interpreter_data();
     }
     // During flushing a BytecodeArray is transformed into an UncompiledData in
     // place. Seeing an UncompiledData here implies that another
@@ -3611,10 +3595,10 @@ bool MarkCompactCollector::IsOnEvacuationCandidate(MaybeObject obj) {
 }
 
 // static
-bool MarkCompactCollector::ShouldRecordRelocSlot(InstructionStream host,
-                                                 RelocInfo* rinfo,
+bool MarkCompactCollector::ShouldRecordRelocSlot(RelocInfo* rinfo,
                                                  HeapObject target) {
-  MemoryChunk* source_chunk = MemoryChunk::FromHeapObject(host);
+  MemoryChunk* source_chunk =
+      MemoryChunk::FromHeapObject(rinfo->instruction_stream());
   BasicMemoryChunk* target_chunk = BasicMemoryChunk::FromHeapObject(target);
   return target_chunk->IsEvacuationCandidate() &&
          !source_chunk->ShouldSkipEvacuationSlotRecording();
@@ -3622,10 +3606,7 @@ bool MarkCompactCollector::ShouldRecordRelocSlot(InstructionStream host,
 
 // static
 MarkCompactCollector::RecordRelocSlotInfo
-MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
-                                       HeapObject target) {
-  DCHECK_EQ(host, rinfo->host());
-
+MarkCompactCollector::ProcessRelocInfo(RelocInfo* rinfo, HeapObject target) {
   RecordRelocSlotInfo result;
   const RelocInfo::Mode rmode = rinfo->rmode();
   Address addr;
@@ -3655,7 +3636,8 @@ MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
     }
   }
 
-  MemoryChunk* const source_chunk = MemoryChunk::FromHeapObject(host);
+  MemoryChunk* const source_chunk =
+      MemoryChunk::FromHeapObject(rinfo->instruction_stream());
   const uintptr_t offset = addr - source_chunk->address();
   DCHECK_LT(offset, static_cast<uintptr_t>(TypedSlotSet::kMaxOffset));
   result.memory_chunk = source_chunk;
@@ -3666,11 +3648,10 @@ MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
 }
 
 // static
-void MarkCompactCollector::RecordRelocSlot(InstructionStream host,
-                                           RelocInfo* rinfo,
+void MarkCompactCollector::RecordRelocSlot(RelocInfo* rinfo,
                                            HeapObject target) {
-  if (!ShouldRecordRelocSlot(host, rinfo, target)) return;
-  RecordRelocSlotInfo info = ProcessRelocInfo(host, rinfo, target);
+  if (!ShouldRecordRelocSlot(rinfo, target)) return;
+  RecordRelocSlotInfo info = ProcessRelocInfo(rinfo, target);
 
   // Access to TypeSlots need to be protected, since LocalHeaps might
   // publish code in the background thread.
@@ -3886,7 +3867,7 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(host, cage_base(),
                                                  code_cage_base(), slot);
   }
@@ -3912,12 +3893,12 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
     }
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
@@ -5497,12 +5478,12 @@ class YoungGenerationMarkingVerifier : public MarkingVerifier {
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
   }
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override {
diff --git a/src/heap/mark-compact.h b/src/heap/mark-compact.h
index 14a75424f8..77fc1e1886 100644
--- a/src/heap/mark-compact.h
+++ b/src/heap/mark-compact.h
@@ -213,8 +213,7 @@ class MainMarkingVisitor final
   template <typename TSlot>
   void RecordSlot(HeapObject object, TSlot slot, HeapObject target);
 
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target);
+  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
 
   MarkingState* marking_state() { return marking_state_; }
 
@@ -382,14 +381,11 @@ class MarkCompactCollector final : public CollectorBase {
 
   static V8_EXPORT_PRIVATE bool IsMapOrForwarded(Map map);
 
-  static bool ShouldRecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                                    HeapObject target);
-  static RecordRelocSlotInfo ProcessRelocInfo(InstructionStream host,
-                                              RelocInfo* rinfo,
+  static bool ShouldRecordRelocSlot(RelocInfo* rinfo, HeapObject target);
+  static RecordRelocSlotInfo ProcessRelocInfo(RelocInfo* rinfo,
                                               HeapObject target);
 
-  static void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                              HeapObject target);
+  static void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
   V8_INLINE static void RecordSlot(HeapObject object, ObjectSlot slot,
                                    HeapObject target);
   V8_INLINE static void RecordSlot(HeapObject object, HeapObjectSlot slot,
diff --git a/src/heap/marking-barrier.cc b/src/heap/marking-barrier.cc
index 741acd0848..8b746d2751 100644
--- a/src/heap/marking-barrier.cc
+++ b/src/heap/marking-barrier.cc
@@ -77,9 +77,9 @@ void MarkingBarrier::Write(InstructionStream host, RelocInfo* reloc_info,
     if (is_main_thread_barrier_) {
       // An optimization to avoid allocating additional typed slots for the
       // main thread.
-      major_collector_->RecordRelocSlot(host, reloc_info, value);
+      major_collector_->RecordRelocSlot(reloc_info, value);
     } else {
-      RecordRelocSlot(host, reloc_info, value);
+      RecordRelocSlot(reloc_info, value);
     }
   }
 }
@@ -146,13 +146,12 @@ void MarkingBarrier::Write(DescriptorArray descriptor_array,
   }
 }
 
-void MarkingBarrier::RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                                     HeapObject target) {
-  DCHECK(IsCurrentMarkingBarrier(host));
-  if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target)) return;
+void MarkingBarrier::RecordRelocSlot(RelocInfo* rinfo, HeapObject target) {
+  DCHECK(IsCurrentMarkingBarrier(rinfo->instruction_stream()));
+  if (!MarkCompactCollector::ShouldRecordRelocSlot(rinfo, target)) return;
 
   MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(host, rinfo, target);
+      MarkCompactCollector::ProcessRelocInfo(rinfo, target);
 
   auto& typed_slots = typed_slots_map_[info.memory_chunk];
   if (!typed_slots) {
diff --git a/src/heap/marking-barrier.h b/src/heap/marking-barrier.h
index 35d59d3bcb..382267edf6 100644
--- a/src/heap/marking-barrier.h
+++ b/src/heap/marking-barrier.h
@@ -66,8 +66,7 @@ class MarkingBarrier {
 
   inline bool WhiteToGreyAndPush(HeapObject value);
 
-  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
-                       HeapObject target);
+  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
 
   bool IsCurrentMarkingBarrier(HeapObject verification_candidate);
 
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index 4241fabdf6..03038f6c14 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -103,7 +103,7 @@ MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitPointersImpl(
 template <typename ConcreteVisitor, typename MarkingState>
 V8_INLINE void
 MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodePointerImpl(
-    HeapObject host, CodeObjectSlot slot) {
+    Code host, CodeObjectSlot slot) {
   Object object =
       slot.Relaxed_Load(ObjectVisitorWithCageBases::code_cage_base());
   HeapObject heap_object;
@@ -117,34 +117,35 @@ MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodePointerImpl(
 
 template <typename ConcreteVisitor, typename MarkingState>
 void MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitEmbeddedPointer(
-    InstructionStream host, RelocInfo* rinfo) {
+    RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
   HeapObject object =
       rinfo->target_object(ObjectVisitorWithCageBases::cage_base());
   if (!ShouldMarkObject(object)) return;
 
   if (!concrete_visitor()->marking_state()->IsBlackOrGrey(object)) {
-    if (host.IsWeakObject(object)) {
+    if (rinfo->code().IsWeakObject(object)) {
       local_weak_objects_->weak_objects_in_code_local.Push(
-          std::make_pair(object, host));
-      AddWeakReferenceForReferenceSummarizer(host, object);
+          std::make_pair(object, rinfo->code()));
+      AddWeakReferenceForReferenceSummarizer(rinfo->instruction_stream(),
+                                             object);
     } else {
-      MarkObject(host, object);
+      MarkObject(rinfo->instruction_stream(), object);
     }
   }
-  concrete_visitor()->RecordRelocSlot(host, rinfo, object);
+  concrete_visitor()->RecordRelocSlot(rinfo, object);
 }
 
 template <typename ConcreteVisitor, typename MarkingState>
 void MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodeTarget(
-    InstructionStream host, RelocInfo* rinfo) {
+    RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
 
   if (!ShouldMarkObject(target)) return;
-  MarkObject(host, target);
-  concrete_visitor()->RecordRelocSlot(host, rinfo, target);
+  MarkObject(rinfo->instruction_stream(), target);
+  concrete_visitor()->RecordRelocSlot(rinfo, target);
 }
 
 template <typename ConcreteVisitor, typename MarkingState>
@@ -216,16 +217,10 @@ int MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitSharedFunctionInfo(
     // then we have to visit the bytecode but not the baseline code.
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
     Code baseline_code = Code::cast(shared_info.function_data(kAcquireLoad));
-    // Safe to do a relaxed load here since the Code was
-    // acquire-loaded.
-    InstructionStream baseline_istream =
-        FromCode(baseline_code, ObjectVisitorWithCageBases::code_cage_base(),
-                 kRelaxedLoad);
     // Visit the bytecode hanging off baseline code.
-    VisitPointer(
-        baseline_istream,
-        baseline_istream.RawField(
-            InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
+    VisitPointer(baseline_code,
+                 baseline_code.RawField(
+                     Code::kDeoptimizationDataOrInterpreterDataOffset));
     local_weak_objects_->code_flushing_candidates_local.Push(shared_info);
   } else {
     // In other cases, record as a flushing candidate since we have old
diff --git a/src/heap/marking-visitor.h b/src/heap/marking-visitor.h
index 876e30048e..f2c069e5a6 100644
--- a/src/heap/marking-visitor.h
+++ b/src/heap/marking-visitor.h
@@ -98,13 +98,11 @@ class MarkingVisitorBase : public HeapVisitor<int, ConcreteVisitor> {
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
+  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final {
     VisitCodePointerImpl(host, slot);
   }
-  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
-                                      RelocInfo* rinfo) final;
-  V8_INLINE void VisitCodeTarget(InstructionStream host,
-                                 RelocInfo* rinfo) final;
+  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final;
+  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final;
   void VisitCustomWeakPointers(HeapObject host, ObjectSlot start,
                                ObjectSlot end) final {
     // Weak list pointers should be ignored during marking. The lists are
@@ -149,7 +147,7 @@ class MarkingVisitorBase : public HeapVisitor<int, ConcreteVisitor> {
 
   // Similar to VisitPointersImpl() but using code cage base for loading from
   // the slot.
-  V8_INLINE void VisitCodePointerImpl(HeapObject host, CodeObjectSlot slot);
+  V8_INLINE void VisitCodePointerImpl(Code host, CodeObjectSlot slot);
 
   V8_INLINE void VisitDescriptorsForMap(Map map);
 
@@ -216,8 +214,7 @@ class YoungGenerationMarkingVisitorBase
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE void VisitCodePointer(HeapObject host,
-                                  CodeObjectSlot slot) override {
+  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // InstructionStream slots never appear in new space because
     // Code objects, the only object that can contain code pointers, are
@@ -233,15 +230,13 @@ class YoungGenerationMarkingVisitorBase
     VisitPointerImpl(host, slot);
   }
 
-  V8_INLINE void VisitCodeTarget(InstructionStream host,
-                                 RelocInfo* rinfo) final {
-    // InstructionStream objects are not expected in new space.
+  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final {
+    // Code objects are not expected in new space.
     UNREACHABLE();
   }
 
-  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
-                                      RelocInfo* rinfo) final {
-    // InstructionStream objects are not expected in new space.
+  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final {
+    // Code objects are not expected in new space.
     UNREACHABLE();
   }
 
diff --git a/src/heap/object-stats.cc b/src/heap/object-stats.cc
index 1531f39734..c533834ee1 100644
--- a/src/heap/object-stats.cc
+++ b/src/heap/object-stats.cc
@@ -97,17 +97,16 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
     *tagged_fields_count_ += (end - start);
   }
 
-  V8_INLINE void VisitCodePointer(HeapObject host,
-                                  CodeObjectSlot slot) override {
+  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     *tagged_fields_count_ += 1;
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     // InstructionStream target is most likely encoded as a relative 32-bit
     // offset and not as a full tagged value, so there's nothing to count.
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     *tagged_fields_count_ += 1;
   }
 
@@ -1037,19 +1036,20 @@ ObjectStats::VirtualInstanceType CodeKindToVirtualInstanceType(CodeKind kind) {
 }  // namespace
 
 void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
-    InstructionStream code) {
-  RecordSimpleVirtualObjectStats(HeapObject(), code,
+    InstructionStream istream) {
+  Code code = istream.code(kAcquireLoad);
+  RecordSimpleVirtualObjectStats(HeapObject(), istream,
                                  CodeKindToVirtualInstanceType(code.kind()));
-  RecordSimpleVirtualObjectStats(code, code.relocation_info(),
+  RecordSimpleVirtualObjectStats(istream, code.relocation_info(),
                                  ObjectStats::RELOC_INFO_TYPE);
   if (CodeKindIsOptimizedJSFunction(code.kind())) {
     Object source_position_table = code.source_position_table();
     if (source_position_table.IsHeapObject()) {
-      RecordSimpleVirtualObjectStats(code,
+      RecordSimpleVirtualObjectStats(istream,
                                      HeapObject::cast(source_position_table),
                                      ObjectStats::SOURCE_POSITION_TABLE_TYPE);
     }
-    RecordSimpleVirtualObjectStats(code, code.deoptimization_data(),
+    RecordSimpleVirtualObjectStats(istream, code.deoptimization_data(),
                                    ObjectStats::DEOPTIMIZATION_DATA_TYPE);
     DeoptimizationData input_data =
         DeoptimizationData::cast(code.deoptimization_data());
@@ -1065,7 +1065,7 @@ void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
     Object target = it.rinfo()->target_object(cage_base());
     if (target.IsFixedArrayExact(cage_base())) {
       RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
-          code, HeapObject::cast(target), ObjectStats::EMBEDDED_OBJECT_TYPE);
+          istream, HeapObject::cast(target), ObjectStats::EMBEDDED_OBJECT_TYPE);
     }
   }
 }
diff --git a/src/heap/read-only-spaces.cc b/src/heap/read-only-spaces.cc
index 7701664983..869668245a 100644
--- a/src/heap/read-only-spaces.cc
+++ b/src/heap/read-only-spaces.cc
@@ -442,12 +442,7 @@ class ReadOnlySpaceObjectIterator : public ObjectIterator {
       cur_addr_ += ALIGN_TO_ALLOCATION_ALIGNMENT(obj_size);
       DCHECK_LE(cur_addr_, cur_end_);
       if (!obj.IsFreeSpaceOrFiller()) {
-        if (obj.IsInstructionStream()) {
-          DCHECK(InstructionStream::cast(obj).is_builtin());
-          DCHECK_CODEOBJECT_SIZE(obj_size, space_);
-        } else {
-          DCHECK_OBJECT_SIZE(obj_size);
-        }
+        DCHECK_OBJECT_SIZE(obj_size);
         return obj;
       }
     }
diff --git a/src/heap/remembered-set-inl.h b/src/heap/remembered-set-inl.h
index 2de82cc6af..d30f887122 100644
--- a/src/heap/remembered-set-inl.h
+++ b/src/heap/remembered-set-inl.h
@@ -20,19 +20,20 @@ SlotCallbackResult UpdateTypedSlotHelper::UpdateTypedSlot(Heap* heap,
                                                           Callback callback) {
   switch (slot_type) {
     case SlotType::kCodeEntry: {
-      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, InstructionStream());
+      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, Code(),
+                      InstructionStream());
       return UpdateCodeTarget(&rinfo, callback);
     }
     case SlotType::kConstPoolCodeEntry: {
       return UpdateCodeEntry(addr, callback);
     }
     case SlotType::kEmbeddedObjectCompressed: {
-      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0,
+      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0, Code(),
                       InstructionStream());
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
     case SlotType::kEmbeddedObjectFull: {
-      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0,
+      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0, Code(),
                       InstructionStream());
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
@@ -63,19 +64,20 @@ HeapObject UpdateTypedSlotHelper::GetTargetObject(Heap* heap,
                                                   Address addr) {
   switch (slot_type) {
     case SlotType::kCodeEntry: {
-      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, InstructionStream());
+      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, Code(),
+                      InstructionStream());
       return InstructionStream::FromTargetAddress(rinfo.target_address());
     }
     case SlotType::kConstPoolCodeEntry: {
       return InstructionStream::FromEntryAddress(addr);
     }
     case SlotType::kEmbeddedObjectCompressed: {
-      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0,
+      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0, Code(),
                       InstructionStream());
       return rinfo.target_object(heap->isolate());
     }
     case SlotType::kEmbeddedObjectFull: {
-      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0,
+      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0, Code(),
                       InstructionStream());
       return rinfo.target_object(heap->isolate());
     }
diff --git a/src/heap/scavenger-inl.h b/src/heap/scavenger-inl.h
index ea827b163f..dacbda3c30 100644
--- a/src/heap/scavenger-inl.h
+++ b/src/heap/scavenger-inl.h
@@ -478,12 +478,10 @@ class ScavengeVisitor final : public NewSpaceVisitor<ScavengeVisitor> {
 
   V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final;
-  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final;
+  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final;
 
-  V8_INLINE void VisitCodeTarget(InstructionStream host,
-                                 RelocInfo* rinfo) final;
-  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
-                                      RelocInfo* rinfo) final;
+  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final;
+  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final;
   V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
   V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
   V8_INLINE int VisitJSApiObject(Map map, JSObject object);
@@ -508,7 +506,7 @@ void ScavengeVisitor::VisitPointers(HeapObject host, MaybeObjectSlot start,
   return VisitPointersImpl(host, start, end);
 }
 
-void ScavengeVisitor::VisitCodePointer(HeapObject host, CodeObjectSlot slot) {
+void ScavengeVisitor::VisitCodePointer(Code host, CodeObjectSlot slot) {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // InstructionStream slots never appear in new space because
   // Code objects, the only object that can contain code pointers, are
@@ -516,8 +514,7 @@ void ScavengeVisitor::VisitCodePointer(HeapObject host, CodeObjectSlot slot) {
   UNREACHABLE();
 }
 
-void ScavengeVisitor::VisitCodeTarget(InstructionStream host,
-                                      RelocInfo* rinfo) {
+void ScavengeVisitor::VisitCodeTarget(RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
 #ifdef DEBUG
@@ -530,8 +527,7 @@ void ScavengeVisitor::VisitCodeTarget(InstructionStream host,
   DCHECK_EQ(old_target, target);
 }
 
-void ScavengeVisitor::VisitEmbeddedPointer(InstructionStream host,
-                                           RelocInfo* rinfo) {
+void ScavengeVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
   HeapObject heap_object = rinfo->target_object(cage_base());
 #ifdef DEBUG
   HeapObject old_heap_object = heap_object;
diff --git a/src/heap/scavenger.cc b/src/heap/scavenger.cc
index 5111ee6641..36cf21adb9 100644
--- a/src/heap/scavenger.cc
+++ b/src/heap/scavenger.cc
@@ -61,7 +61,7 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
+  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // InstructionStream slots never appear in new space because
     // Code objects, the only object that can contain code pointers, are
@@ -69,17 +69,17 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
     UNREACHABLE();
   }
 
-  V8_INLINE void VisitCodeTarget(InstructionStream host,
-                                 RelocInfo* rinfo) final {
+  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    HandleSlot(host, FullHeapObjectSlot(&target), target);
+    HandleSlot(rinfo->instruction_stream(), FullHeapObjectSlot(&target),
+               target);
   }
-  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
-                                      RelocInfo* rinfo) final {
-    PtrComprCageBase cage_base = host.main_cage_base();
+  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final {
+    PtrComprCageBase cage_base = GetPtrComprCageBase(rinfo->code());
     HeapObject heap_object = rinfo->target_object(cage_base);
-    HandleSlot(host, FullHeapObjectSlot(&heap_object), heap_object);
+    HandleSlot(rinfo->instruction_stream(), FullHeapObjectSlot(&heap_object),
+               heap_object);
   }
 
   inline void VisitEphemeron(HeapObject obj, int entry, ObjectSlot key,
diff --git a/src/heap/sweeper.cc b/src/heap/sweeper.cc
index c260c8f47e..6d57875231 100644
--- a/src/heap/sweeper.cc
+++ b/src/heap/sweeper.cc
@@ -817,7 +817,7 @@ class PromotedPageRecordMigratedSlotVisitor
     }
   }
 
-  inline void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
+  inline void VisitCodePointer(Code host, CodeObjectSlot slot) final {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // This code is similar to the implementation of VisitPointer() modulo
     // new kind of slot.
@@ -835,18 +835,12 @@ class PromotedPageRecordMigratedSlotVisitor
     VisitPointer(host, key);
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
-    UNREACHABLE();
-  }
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final {
-    UNREACHABLE();
-  }
+  void VisitCodeTarget(RelocInfo* rinfo) final { UNREACHABLE(); }
+  void VisitEmbeddedPointer(RelocInfo* rinfo) final { UNREACHABLE(); }
 
   // Entries that are skipped for recording.
-  inline void VisitExternalReference(InstructionStream host,
-                                     RelocInfo* rinfo) final {}
-  inline void VisitInternalReference(InstructionStream host,
-                                     RelocInfo* rinfo) final {}
+  inline void VisitExternalReference(RelocInfo* rinfo) final {}
+  inline void VisitInternalReference(RelocInfo* rinfo) final {}
   inline void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                    ExternalPointerTag tag) final {}
 
diff --git a/src/heap/weak-object-worklists.h b/src/heap/weak-object-worklists.h
index 68749b9bc4..a921a41507 100644
--- a/src/heap/weak-object-worklists.h
+++ b/src/heap/weak-object-worklists.h
@@ -19,7 +19,7 @@ struct Ephemeron {
 };
 
 using HeapObjectAndSlot = std::pair<HeapObject, HeapObjectSlot>;
-using HeapObjectAndCode = std::pair<HeapObject, InstructionStream>;
+using HeapObjectAndCode = std::pair<HeapObject, Code>;
 class EphemeronHashTable;
 class JSFunction;
 class SharedFunctionInfo;
diff --git a/src/logging/code-events.h b/src/logging/code-events.h
index 562eaff77a..9faee22ab6 100644
--- a/src/logging/code-events.h
+++ b/src/logging/code-events.h
@@ -95,12 +95,11 @@ class LogEventListener {
   virtual void CodeMovingGCEvent() = 0;
   virtual void CodeDisableOptEvent(Handle<AbstractCode> code,
                                    Handle<SharedFunctionInfo> shared) = 0;
-  virtual void CodeDeoptEvent(Handle<InstructionStream> code,
-                              DeoptimizeKind kind, Address pc,
-                              int fp_to_sp_delta) = 0;
+  virtual void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
+                              Address pc, int fp_to_sp_delta) = 0;
   // These events can happen when 1. an assumption made by optimized code fails
   // or 2. a weakly embedded object dies.
-  virtual void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  virtual void CodeDependencyChangeEvent(Handle<Code> code,
                                          Handle<SharedFunctionInfo> shared,
                                          const char* reason) = 0;
   // Called during GC shortly after any weak references to code objects are
@@ -243,14 +242,14 @@ class Logger {
       listener->CodeDisableOptEvent(code, shared);
     }
   }
-  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                      Address pc, int fp_to_sp_delta) {
+  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                      int fp_to_sp_delta) {
     base::MutexGuard guard(&mutex_);
     for (auto listener : listeners_) {
       listener->CodeDeoptEvent(code, kind, pc, fp_to_sp_delta);
     }
   }
-  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) {
     base::MutexGuard guard(&mutex_);
diff --git a/src/logging/log.cc b/src/logging/log.cc
index 3b4caa6fa0..0d1aa645e8 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -625,7 +625,8 @@ void ExternalLogEventListener::CodeMoveEvent(InstructionStream from,
                                              InstructionStream to) {
   CodeEvent code_event;
   InitializeCodeEvent(isolate_, &code_event, from.instruction_start(),
-                      to.instruction_start(), to.instruction_size());
+                      to.instruction_start(),
+                      to.code(kAcquireLoad).instruction_size());
   code_event_handler_->Handle(reinterpret_cast<v8::CodeEvent*>(&code_event));
 }
 
@@ -914,7 +915,7 @@ void JitLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
   event.type = JitCodeEvent::CODE_MOVED;
   event.code_type = JitCodeEvent::JIT_CODE;
   event.code_start = reinterpret_cast<void*>(from.instruction_start());
-  event.code_len = from.instruction_size();
+  event.code_len = from.unchecked_code().instruction_size();
   event.new_code_start = reinterpret_cast<void*>(to.instruction_start());
   event.isolate = reinterpret_cast<v8::Isolate*>(isolate_);
 
@@ -1623,12 +1624,11 @@ void V8FileLogger::CodeDisableOptEvent(Handle<AbstractCode> code,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::ProcessDeoptEvent(Handle<InstructionStream> code,
-                                     SourcePosition position, const char* kind,
-                                     const char* reason) {
+void V8FileLogger::ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
+                                     const char* kind, const char* reason) {
   MSG_BUILDER();
   msg << Event::kCodeDeopt << kNext << Time() << kNext << code->CodeSize()
-      << kNext << reinterpret_cast<void*>(code->instruction_start());
+      << kNext << reinterpret_cast<void*>(code->InstructionStart());
 
   std::ostringstream deopt_location;
   int inlining_id = -1;
@@ -1646,16 +1646,15 @@ void V8FileLogger::ProcessDeoptEvent(Handle<InstructionStream> code,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::CodeDeoptEvent(Handle<InstructionStream> code,
-                                  DeoptimizeKind kind, Address pc,
-                                  int fp_to_sp_delta) {
+void V8FileLogger::CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
+                                  Address pc, int fp_to_sp_delta) {
   if (!is_logging() || !v8_flags.log_deopt) return;
   Deoptimizer::DeoptInfo info = Deoptimizer::GetDeoptInfo(*code, pc);
   ProcessDeoptEvent(code, info.position, Deoptimizer::MessageFor(kind),
                     DeoptimizeReasonToString(info.deopt_reason));
 }
 
-void V8FileLogger::CodeDependencyChangeEvent(Handle<InstructionStream> code,
+void V8FileLogger::CodeDependencyChangeEvent(Handle<Code> code,
                                              Handle<SharedFunctionInfo> sfi,
                                              const char* reason) {
   if (!is_logging() || !v8_flags.log_deopt) return;
diff --git a/src/logging/log.h b/src/logging/log.h
index d411da512b..c0720c9f27 100644
--- a/src/logging/log.h
+++ b/src/logging/log.h
@@ -197,17 +197,16 @@ class V8FileLogger : public LogEventListener {
   void CodeMovingGCEvent() override;
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override;
-  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                      Address pc, int fp_to_sp_delta) override;
-  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                      int fp_to_sp_delta) override;
+  void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override;
   void FeedbackVectorEvent(FeedbackVector vector, AbstractCode code);
   void WeakCodeClearEvent() override {}
 
-  void ProcessDeoptEvent(Handle<InstructionStream> code,
-                         SourcePosition position, const char* kind,
-                         const char* reason);
+  void ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
+                         const char* kind, const char* reason);
 
   // Emits a code line info record event.
   void CodeLinePosInfoRecordEvent(Address code_start,
@@ -433,9 +432,9 @@ class V8_EXPORT_PRIVATE CodeEventLogger : public LogEventListener {
   void SharedFunctionInfoMoveEvent(Address from, Address to) override {}
   void NativeContextMoveEvent(Address from, Address to) override {}
   void CodeMovingGCEvent() override {}
-  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                      Address pc, int fp_to_sp_delta) override {}
-  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                      int fp_to_sp_delta) override {}
+  void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override {}
@@ -505,9 +504,9 @@ class ExternalLogEventListener : public LogEventListener {
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
   void CodeMovingGCEvent() override {}
-  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                      Address pc, int fp_to_sp_delta) override {}
-  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                      int fp_to_sp_delta) override {}
+  void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override {}
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 2bcf2d1ad6..2878dd6559 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -69,6 +69,9 @@ GCSAFE_CODE_FWD_ACCESSOR(bool, is_turbofanned)
 GCSAFE_CODE_FWD_ACCESSOR(bool, has_tagged_outgoing_params)
 GCSAFE_CODE_FWD_ACCESSOR(bool, marked_for_deoptimization)
 GCSAFE_CODE_FWD_ACCESSOR(Object, raw_instruction_stream)
+GCSAFE_CODE_FWD_ACCESSOR(int, stack_slots)
+GCSAFE_CODE_FWD_ACCESSOR(Address, constant_pool)
+GCSAFE_CODE_FWD_ACCESSOR(Address, SafepointTableAddress)
 #undef GCSAFE_CODE_FWD_ACCESSOR
 
 int GcSafeCode::GetOffsetFromInstructionStart(Isolate* isolate,
@@ -81,10 +84,22 @@ Address GcSafeCode::InstructionStart(Isolate* isolate, Address pc) const {
 }
 
 Address GcSafeCode::InstructionEnd(Isolate* isolate, Address pc) const {
-  return V8_LIKELY(has_instruction_stream())
-             ? InstructionStream::unchecked_cast(raw_instruction_stream())
-                   .instruction_end()
-             : UnsafeCastToCode().OffHeapInstructionEnd(isolate, pc);
+  return UnsafeCastToCode().InstructionEnd(isolate, pc);
+}
+
+bool GcSafeCode::CanDeoptAt(Isolate* isolate, Address pc) const {
+  DeoptimizationData deopt_data = DeoptimizationData::unchecked_cast(
+      UnsafeCastToCode().unchecked_deoptimization_data());
+  Address code_start_address = InstructionStart();
+  for (int i = 0; i < deopt_data.DeoptCount(); i++) {
+    if (deopt_data.Pc(i).value() == -1) continue;
+    Address address = code_start_address + deopt_data.Pc(i).value();
+    if (address == pc &&
+        deopt_data.GetBytecodeOffset(i) != BytecodeOffset::None()) {
+      return true;
+    }
+  }
+  return false;
 }
 
 int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
@@ -126,10 +141,7 @@ ByteArray AbstractCode::SourcePositionTable(Isolate* isolate,
 int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
   Map map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    Code code = GetCode();
-    return code.has_instruction_stream()
-               ? FromCode(code).SizeIncludingMetadata(cage_base)
-               : 0;
+    return GetCode().SizeIncludingMetadata();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
     return GetBytecodeArray().SizeIncludingMetadata();
@@ -221,33 +233,30 @@ BytecodeArray AbstractCode::GetBytecodeArray() {
 OBJECT_CONSTRUCTORS_IMPL(InstructionStream, HeapObject)
 NEVER_READ_ONLY_SPACE_IMPL(InstructionStream)
 
-INT_ACCESSORS(InstructionStream, instruction_size, kInstructionSizeOffset)
-INT_ACCESSORS(InstructionStream, metadata_size, kMetadataSizeOffset)
-INT_ACCESSORS(InstructionStream, handler_table_offset,
-              kHandlerTableOffsetOffset)
-INT_ACCESSORS(InstructionStream, code_comments_offset,
-              kCodeCommentsOffsetOffset)
-INT32_ACCESSORS(InstructionStream, unwinding_info_offset,
-                kUnwindingInfoOffsetOffset)
-
-// Same as ACCESSORS_CHECKED2 macro but with InstructionStream as a host and
-// using main_cage_base() for computing the base.
-#define INSTRUCTION_STREAM_ACCESSORS_CHECKED2(name, type, offset,           \
-                                              get_condition, set_condition) \
-  type InstructionStream::name() const {                                    \
-    PtrComprCageBase cage_base = main_cage_base();                          \
-    return InstructionStream::name(cage_base);                              \
-  }                                                                         \
-  type InstructionStream::name(PtrComprCageBase cage_base) const {          \
-    type value = TaggedField<type, offset>::load(cage_base, *this);         \
-    DCHECK(get_condition);                                                  \
-    return value;                                                           \
-  }                                                                         \
-  void InstructionStream::set_##name(type value, WriteBarrierMode mode) {   \
-    DCHECK(set_condition);                                                  \
-    TaggedField<type, offset>::store(*this, value);                         \
-    CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);                  \
-  }
+INT_ACCESSORS(Code, instruction_size, kInstructionSizeOffset)
+INT_ACCESSORS(Code, metadata_size, kMetadataSizeOffset)
+INT_ACCESSORS(Code, handler_table_offset, kHandlerTableOffsetOffset)
+INT_ACCESSORS(Code, code_comments_offset, kCodeCommentsOffsetOffset)
+INT32_ACCESSORS(Code, unwinding_info_offset, kUnwindingInfoOffsetOffset)
+ACCESSORS(Code, relocation_info, ByteArray, kRelocationInfoOffset)
+ACCESSORS_CHECKED2(Code, deoptimization_data, FixedArray,
+                   kDeoptimizationDataOrInterpreterDataOffset,
+                   kind() != CodeKind::BASELINE,
+                   kind() != CodeKind::BASELINE &&
+                       !ObjectInYoungGeneration(value))
+ACCESSORS_CHECKED2(Code, bytecode_or_interpreter_data, HeapObject,
+                   kDeoptimizationDataOrInterpreterDataOffset,
+                   kind() == CodeKind::BASELINE,
+                   kind() == CodeKind::BASELINE &&
+                       !ObjectInYoungGeneration(value))
+ACCESSORS_CHECKED2(Code, source_position_table, ByteArray, kPositionTableOffset,
+                   kind() != CodeKind::BASELINE,
+                   kind() != CodeKind::BASELINE &&
+                       !ObjectInYoungGeneration(value))
+ACCESSORS_CHECKED2(Code, bytecode_offset_table, ByteArray, kPositionTableOffset,
+                   kind() == CodeKind::BASELINE,
+                   kind() == CodeKind::BASELINE &&
+                       !ObjectInYoungGeneration(value))
 
 // Same as RELEASE_ACQUIRE_ACCESSORS_CHECKED2 macro but with InstructionStream
 // as a host and using main_cage_base(kRelaxedLoad) for computing the base.
@@ -270,41 +279,14 @@ INT32_ACCESSORS(InstructionStream, unwinding_info_offset,
     CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);                  \
   }
 
-#define INSTRUCTION_STREAM_ACCESSORS(name, type, offset) \
-  INSTRUCTION_STREAM_ACCESSORS_CHECKED2(name, type, offset, true, true)
-
 #define RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(name, type, offset) \
   RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS_CHECKED2(                 \
       name, type, offset, !ObjectInYoungGeneration(value),               \
       !ObjectInYoungGeneration(value))
 
-INSTRUCTION_STREAM_ACCESSORS(relocation_info, ByteArray, kRelocationInfoOffset)
-
-INSTRUCTION_STREAM_ACCESSORS_CHECKED2(
-    deoptimization_data, FixedArray, kDeoptimizationDataOrInterpreterDataOffset,
-    kind() != CodeKind::BASELINE,
-    kind() != CodeKind::BASELINE && !ObjectInYoungGeneration(value))
-INSTRUCTION_STREAM_ACCESSORS_CHECKED2(
-    bytecode_or_interpreter_data, HeapObject,
-    kDeoptimizationDataOrInterpreterDataOffset, kind() == CodeKind::BASELINE,
-    kind() == CodeKind::BASELINE && !ObjectInYoungGeneration(value))
-
-INSTRUCTION_STREAM_ACCESSORS_CHECKED2(source_position_table, ByteArray,
-                                      kPositionTableOffset,
-                                      kind() != CodeKind::BASELINE,
-                                      kind() != CodeKind::BASELINE &&
-                                          !ObjectInYoungGeneration(value))
-INSTRUCTION_STREAM_ACCESSORS_CHECKED2(bytecode_offset_table, ByteArray,
-                                      kPositionTableOffset,
-                                      kind() == CodeKind::BASELINE,
-                                      kind() == CodeKind::BASELINE &&
-                                          !ObjectInYoungGeneration(value))
-
 // Concurrent marker needs to access kind specific flags in code.
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(code, Code, kCodeOffset)
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(raw_code, HeapObject, kCodeOffset)
-#undef INSTRUCTION_STREAM_ACCESSORS
-#undef INSTRUCTION_STREAM_ACCESSORS_CHECKED2
 #undef RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS
 #undef RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS_CHECKED2
 
@@ -349,17 +331,6 @@ Code InstructionStream::GCSafeCode(AcquireLoadTag) const {
 // Code and back.
 inline Code ToCode(InstructionStream code) { return code.code(kAcquireLoad); }
 
-inline Handle<Code> ToCode(Handle<InstructionStream> code, Isolate* isolate) {
-  return handle(ToCode(*code), isolate);
-}
-
-inline MaybeHandle<Code> ToCode(MaybeHandle<InstructionStream> maybe_code,
-                                Isolate* isolate) {
-  Handle<InstructionStream> code;
-  if (maybe_code.ToHandle(&code)) return ToCode(code, isolate);
-  return {};
-}
-
 inline InstructionStream FromCode(Code code) {
   DCHECK(code.has_instruction_stream());
   // Compute the InstructionStream object pointer from the code entry point.
@@ -386,22 +357,20 @@ inline InstructionStream FromCode(Code code, Isolate* isolate,
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
+// TODO(jgruber): Remove this method once main_cage_base is gone.
 void InstructionStream::WipeOutHeader() {
-  WRITE_FIELD(*this, kRelocationInfoOffset, Smi::FromInt(0));
-  WRITE_FIELD(*this, kDeoptimizationDataOrInterpreterDataOffset,
-              Smi::FromInt(0));
-  WRITE_FIELD(*this, kPositionTableOffset, Smi::FromInt(0));
   WRITE_FIELD(*this, kCodeOffset, Smi::FromInt(0));
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     set_main_cage_base(kNullAddress, kRelaxedStore);
   }
 }
 
-void InstructionStream::clear_padding() {
+void Code::ClearInstructionStreamPadding() {
   // Clear the padding between the header and `body_start`.
-  if (FIELD_SIZE(kOptionalPaddingOffset) != 0) {
-    memset(reinterpret_cast<void*>(address() + kOptionalPaddingOffset), 0,
-           FIELD_SIZE(kOptionalPaddingOffset));
+  if (FIELD_SIZE(InstructionStream::kOptionalPaddingOffset) != 0) {
+    memset(reinterpret_cast<void*>(instruction_stream().address() +
+                                   InstructionStream::kOptionalPaddingOffset),
+           0, FIELD_SIZE(InstructionStream::kOptionalPaddingOffset));
   }
 
   // Clear the padding after `body_end`.
@@ -415,11 +384,7 @@ ByteArray Code::SourcePositionTable(Isolate* isolate,
   if (!has_instruction_stream()) {
     return GetReadOnlyRoots().empty_byte_array();
   }
-  return instruction_stream().SourcePositionTable(isolate, sfi);
-}
 
-ByteArray InstructionStream::SourcePositionTable(Isolate* isolate,
-                                                 SharedFunctionInfo sfi) const {
   DisallowGarbageCollection no_gc;
   if (kind() == CodeKind::BASELINE) {
     return sfi.GetBytecodeArray(isolate).SourcePositionTable(isolate);
@@ -427,38 +392,25 @@ ByteArray InstructionStream::SourcePositionTable(Isolate* isolate,
   return source_position_table(isolate);
 }
 
-Address InstructionStream::body_start() const { return instruction_start(); }
+Address Code::body_start() const { return InstructionStart(); }
 
-Address InstructionStream::body_end() const {
-  return body_start() + body_size();
-}
+Address Code::body_end() const { return body_start() + body_size(); }
 
-int InstructionStream::body_size() const {
-  return instruction_size() + metadata_size();
-}
+int Code::body_size() const { return instruction_size() + metadata_size(); }
 
-int Code::InstructionSize() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().instruction_size()
-             : OffHeapInstructionSize();
-}
+// TODO(jgruber): Remove instruction_size.
+int Code::InstructionSize() const { return instruction_size(); }
 
 Address InstructionStream::instruction_start() const {
   return field_address(kHeaderSize);
 }
 
-Address InstructionStream::instruction_end() const {
-  return instruction_start() + instruction_size();
-}
-
 Address Code::InstructionEnd() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().instruction_end()
-             : OffHeapInstructionEnd();
+  return InstructionStart() + instruction_size();
 }
 
-Address InstructionStream::metadata_start() const {
-  return instruction_start() + instruction_size();
+Address Code::metadata_start() const {
+  return InstructionStart() + instruction_size();
 }
 
 Address Code::InstructionStart(Isolate* isolate, Address pc) const {
@@ -469,7 +421,7 @@ Address Code::InstructionStart(Isolate* isolate, Address pc) const {
 
 Address Code::InstructionEnd(Isolate* isolate, Address pc) const {
   return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().instruction_end()
+             ? InstructionEnd()
              : OffHeapInstructionEnd(isolate, pc);
 }
 
@@ -480,83 +432,52 @@ int Code::GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const {
   return static_cast<int>(offset);
 }
 
-Address InstructionStream::metadata_end() const {
+Address Code::metadata_end() const {
   return metadata_start() + metadata_size();
 }
 
-DEF_GETTER(InstructionStream, SizeIncludingMetadata, int) {
+int Code::SizeIncludingMetadata() const {
   int size = CodeSize();
-  size += relocation_info(cage_base).Size();
+  size += relocation_info().Size();
   if (kind() != CodeKind::BASELINE) {
-    size += deoptimization_data(cage_base).Size();
+    size += deoptimization_data().Size();
   }
   return size;
 }
 
-Address InstructionStream::safepoint_table_address() const {
+Address Code::safepoint_table_address() const {
   return metadata_start() + safepoint_table_offset();
 }
 
-int InstructionStream::safepoint_table_size() const {
-  DCHECK_GE(handler_table_offset() - safepoint_table_offset(), 0);
-  return handler_table_offset() - safepoint_table_offset();
-}
-
-bool InstructionStream::has_safepoint_table() const {
-  return safepoint_table_size() > 0;
-}
-
 Address Code::SafepointTableAddress() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().safepoint_table_address()
-             : OffHeapSafepointTableAddress();
-}
-
-Address GcSafeCode::SafepointTableAddress() const {
-  Code unsafe_this = UnsafeCastToCode();
-  return V8_LIKELY(has_instruction_stream())
-             ? InstructionStream::unchecked_cast(
-                   unsafe_this.raw_instruction_stream(kRelaxedLoad))
-                   .safepoint_table_address()
-             : unsafe_this.OffHeapSafepointTableAddress();
+  return V8_LIKELY(has_instruction_stream()) ? safepoint_table_address()
+                                             : OffHeapSafepointTableAddress();
 }
 
 int Code::safepoint_table_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().safepoint_table_size()
-             : OffHeapSafepointTableSize();
+  return handler_table_offset() - safepoint_table_offset();
 }
 
 bool Code::has_safepoint_table() const { return safepoint_table_size() > 0; }
 
-Address InstructionStream::handler_table_address() const {
+Address Code::handler_table_address() const {
   return metadata_start() + handler_table_offset();
 }
 
-int InstructionStream::handler_table_size() const {
-  DCHECK_GE(constant_pool_offset() - handler_table_offset(), 0);
-  return constant_pool_offset() - handler_table_offset();
-}
-
-bool InstructionStream::has_handler_table() const {
-  return handler_table_size() > 0;
-}
-
 Address Code::HandlerTableAddress() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().handler_table_address()
-             : OffHeapHandlerTableAddress();
+  return V8_LIKELY(has_instruction_stream()) ? handler_table_address()
+                                             : OffHeapHandlerTableAddress();
 }
 
 int Code::handler_table_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().handler_table_size()
-             : OffHeapHandlerTableSize();
+  return constant_pool_offset() - handler_table_offset();
 }
 
 bool Code::has_handler_table() const { return handler_table_size() > 0; }
 
-int InstructionStream::constant_pool_size() const {
+int Code::constant_pool_size() const {
+  if V8_UNLIKELY (!has_instruction_stream()) return OffHeapConstantPoolSize();
+
   const int size = code_comments_offset() - constant_pool_offset();
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     DCHECK_EQ(size, 0);
@@ -566,52 +487,39 @@ int InstructionStream::constant_pool_size() const {
   return size;
 }
 
-bool InstructionStream::has_constant_pool() const {
-  return constant_pool_size() > 0;
-}
-
-int Code::constant_pool_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().constant_pool_size()
-             : OffHeapConstantPoolSize();
-}
-
 bool Code::has_constant_pool() const { return constant_pool_size() > 0; }
 
-ByteArray InstructionStream::unchecked_relocation_info() const {
-  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
+ByteArray Code::unchecked_relocation_info() const {
   return ByteArray::unchecked_cast(
-      TaggedField<HeapObject, kRelocationInfoOffset>::load(cage_base, *this));
-}
-
-byte* InstructionStream::relocation_start() const {
-  return unchecked_relocation_info().GetDataStartAddress();
+      TaggedField<HeapObject, kRelocationInfoOffset>::load(*this));
 }
 
-byte* InstructionStream::relocation_end() const {
-  return unchecked_relocation_info().GetDataEndAddress();
+FixedArray Code::unchecked_deoptimization_data() const {
+  return FixedArray::unchecked_cast(
+      TaggedField<HeapObject, kDeoptimizationDataOrInterpreterDataOffset>::load(
+          *this));
 }
 
-int InstructionStream::relocation_size() const {
-  return unchecked_relocation_info().length();
+Code InstructionStream::unchecked_code() const {
+  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
+  return Code::unchecked_cast(
+      TaggedField<HeapObject, kCodeOffset>::Acquire_Load(cage_base, *this));
 }
 
 byte* Code::relocation_start() const {
   return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().relocation_start()
+             ? relocation_info().GetDataStartAddress()
              : nullptr;
 }
 
 byte* Code::relocation_end() const {
   return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().relocation_end()
+             ? relocation_info().GetDataEndAddress()
              : nullptr;
 }
 
 int Code::relocation_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().relocation_size()
-             : 0;
+  return V8_LIKELY(has_instruction_stream()) ? relocation_info().length() : 0;
 }
 
 Address InstructionStream::entry() const { return instruction_start(); }
@@ -628,40 +536,42 @@ bool Code::contains(Isolate* isolate, Address inner_pointer) {
 }
 
 // static
-void InstructionStream::CopyRelocInfoToByteArray(ByteArray dest,
-                                                 const CodeDesc& desc) {
+void Code::CopyRelocInfoToByteArray(ByteArray dest, const CodeDesc& desc) {
   DCHECK_EQ(dest.length(), desc.reloc_size);
   CopyBytes(dest.GetDataStartAddress(),
             desc.buffer + desc.buffer_size - desc.reloc_size,
             static_cast<size_t>(desc.reloc_size));
 }
 
-int InstructionStream::CodeSize() const { return SizeFor(body_size()); }
+int InstructionStream::CodeSize() const {
+  return SizeFor(Code::unchecked_cast(raw_code(kAcquireLoad)).body_size());
+}
+int Code::CodeSize() const { return InstructionStream::SizeFor(body_size()); }
 
 DEF_GETTER(InstructionStream, Size, int) { return CodeSize(); }
 
-CodeKind InstructionStream::kind() const {
+CodeKind Code::kind() const {
   static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return KindField::decode(flags);
 }
 
-int InstructionStream::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                                      BytecodeArray bytecodes) {
+int Code::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
+                                         BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK(!is_baseline_trampoline_builtin());
   if (is_baseline_leave_frame_builtin()) return kFunctionExitBytecodeOffset;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
       ByteArray::cast(bytecode_offset_table()), bytecodes);
-  Address pc = baseline_pc - instruction_start();
+  Address pc = baseline_pc - InstructionStart();
   offset_iterator.AdvanceToPCOffset(pc);
   return offset_iterator.current_bytecode_offset();
 }
 
-uintptr_t InstructionStream::GetBaselinePCForBytecodeOffset(
-    int bytecode_offset, BytecodeToPCPosition position,
-    BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselinePCForBytecodeOffset(int bytecode_offset,
+                                               BytecodeToPCPosition position,
+                                               BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -677,20 +587,20 @@ uintptr_t InstructionStream::GetBaselinePCForBytecodeOffset(
   return pc;
 }
 
-uintptr_t InstructionStream::GetBaselineStartPCForBytecodeOffset(
-    int bytecode_offset, BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
+                                                    BytecodeArray bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtStartOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t InstructionStream::GetBaselineEndPCForBytecodeOffset(
-    int bytecode_offset, BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
+                                                  BytecodeArray bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtEndOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t InstructionStream::GetBaselinePCForNextExecutedBytecode(
-    int bytecode_offset, BytecodeArray bytecodes) {
+uintptr_t Code::GetBaselinePCForNextExecutedBytecode(int bytecode_offset,
+                                                     BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -711,31 +621,6 @@ uintptr_t InstructionStream::GetBaselinePCForNextExecutedBytecode(
   }
 }
 
-void InstructionStream::initialize_flags(CodeKind kind, bool is_turbofanned,
-                                         int stack_slots) {
-  CHECK(0 <= stack_slots && stack_slots < StackSlotsField::kMax);
-  DCHECK(!CodeKindIsInterpretedJSFunction(kind));
-  uint32_t flags = KindField::encode(kind) |
-                   IsTurbofannedField::encode(is_turbofanned) |
-                   StackSlotsField::encode(stack_slots);
-  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
-  RELAXED_WRITE_UINT32_FIELD(*this, kFlagsOffset, flags);
-  DCHECK_IMPLIES(stack_slots != 0, uses_safepoint_table());
-  DCHECK_IMPLIES(!uses_safepoint_table(), stack_slots == 0);
-}
-
-inline bool InstructionStream::is_interpreter_trampoline_builtin() const {
-  return IsInterpreterTrampolineBuiltin(builtin_id());
-}
-
-inline bool InstructionStream::is_baseline_trampoline_builtin() const {
-  return IsBaselineTrampolineBuiltin(builtin_id());
-}
-
-inline bool InstructionStream::is_baseline_leave_frame_builtin() const {
-  return builtin_id() == Builtin::kBaselineLeaveFrame;
-}
-
 inline bool Code::checks_tiering_state() const {
   bool checks_state = (builtin_id() == Builtin::kCompileLazy ||
                        builtin_id() == Builtin::kInterpreterEntryTrampoline ||
@@ -749,15 +634,6 @@ inline constexpr bool CodeKindHasTaggedOutgoingParams(CodeKind kind) {
          kind != CodeKind::C_WASM_ENTRY && kind != CodeKind::WASM_FUNCTION;
 }
 
-inline bool InstructionStream::has_tagged_outgoing_params() const {
-#if V8_ENABLE_WEBASSEMBLY
-  return CodeKindHasTaggedOutgoingParams(kind()) &&
-         builtin_id() != Builtin::kWasmCompileLazy;
-#else
-  return CodeKindHasTaggedOutgoingParams(kind());
-#endif
-}
-
 inline bool Code::has_tagged_outgoing_params() const {
 #if V8_ENABLE_WEBASSEMBLY
   return CodeKindHasTaggedOutgoingParams(kind()) &&
@@ -767,67 +643,39 @@ inline bool Code::has_tagged_outgoing_params() const {
 #endif
 }
 
-inline bool InstructionStream::is_turbofanned() const {
+inline bool Code::is_turbofanned() const {
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return IsTurbofannedField::decode(flags);
 }
 
-inline bool Code::is_turbofanned() const {
-  return IsTurbofannedField::decode(flags(kRelaxedLoad));
-}
-
-bool InstructionStream::is_maglevved() const {
-  return kind() == CodeKind::MAGLEV;
-}
-
 inline bool Code::is_maglevved() const { return kind() == CodeKind::MAGLEV; }
 
 inline bool Code::can_have_weak_objects() const {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int32_t flags = kind_specific_flags(kRelaxedLoad);
-  return InstructionStream::CanHaveWeakObjectsField::decode(flags);
+  int16_t flags = kind_specific_flags(kRelaxedLoad);
+  return CanHaveWeakObjectsField::decode(flags);
 }
 
 inline void Code::set_can_have_weak_objects(bool value) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int32_t previous = kind_specific_flags(kRelaxedLoad);
-  int32_t updated =
-      InstructionStream::CanHaveWeakObjectsField::update(previous, value);
+  int16_t previous = kind_specific_flags(kRelaxedLoad);
+  int16_t updated = CanHaveWeakObjectsField::update(previous, value);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
-inline bool InstructionStream::can_have_weak_objects() const {
-  DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  Code container = code(kAcquireLoad);
-  return container.can_have_weak_objects();
-}
-
-inline void InstructionStream::set_can_have_weak_objects(bool value) {
-  DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  Code container = code(kAcquireLoad);
-  container.set_can_have_weak_objects(value);
-}
-
 inline bool Code::is_promise_rejection() const {
   DCHECK_EQ(kind(), CodeKind::BUILTIN);
-  int32_t flags = kind_specific_flags(kRelaxedLoad);
-  return InstructionStream::IsPromiseRejectionField::decode(flags);
+  int16_t flags = kind_specific_flags(kRelaxedLoad);
+  return IsPromiseRejectionField::decode(flags);
 }
 
 inline void Code::set_is_promise_rejection(bool value) {
   DCHECK_EQ(kind(), CodeKind::BUILTIN);
-  int32_t previous = kind_specific_flags(kRelaxedLoad);
-  int32_t updated =
-      InstructionStream::IsPromiseRejectionField::update(previous, value);
+  int16_t previous = kind_specific_flags(kRelaxedLoad);
+  int16_t updated = IsPromiseRejectionField::update(previous, value);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
-inline bool InstructionStream::is_promise_rejection() const {
-  DCHECK_EQ(kind(), CodeKind::BUILTIN);
-  Code container = code(kAcquireLoad);
-  return container.is_promise_rejection();
-}
-
 inline HandlerTable::CatchPrediction
 InstructionStream::GetBuiltinCatchPrediction() const {
   if (is_promise_rejection()) return HandlerTable::PROMISE;
@@ -839,122 +687,67 @@ inline HandlerTable::CatchPrediction Code::GetBuiltinCatchPrediction() const {
   return HandlerTable::UNCAUGHT;
 }
 
-Builtin InstructionStream::builtin_id() const {
-  int index = RELAXED_READ_INT_FIELD(*this, kBuiltinIndexOffset);
-  DCHECK(index == static_cast<int>(Builtin::kNoBuiltinId) ||
-         Builtins::IsBuiltinId(index));
-  return static_cast<Builtin>(index);
-}
-
-void InstructionStream::set_builtin_id(Builtin builtin) {
-  DCHECK(builtin == Builtin::kNoBuiltinId || Builtins::IsBuiltinId(builtin));
-  RELAXED_WRITE_INT_FIELD(*this, kBuiltinIndexOffset,
-                          static_cast<int>(builtin));
-}
-
-bool InstructionStream::is_builtin() const {
-  return builtin_id() != Builtin::kNoBuiltinId;
-}
-
-unsigned InstructionStream::inlined_bytecode_size() const {
+unsigned Code::inlined_bytecode_size() const {
   unsigned size = RELAXED_READ_UINT_FIELD(*this, kInlinedBytecodeSizeOffset);
   DCHECK(CodeKindIsOptimizedJSFunction(kind()) || size == 0);
   return size;
 }
 
-void InstructionStream::set_inlined_bytecode_size(unsigned size) {
+void Code::set_inlined_bytecode_size(unsigned size) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()) || size == 0);
   RELAXED_WRITE_UINT_FIELD(*this, kInlinedBytecodeSizeOffset, size);
 }
 
-BytecodeOffset InstructionStream::osr_offset() const {
+BytecodeOffset Code::osr_offset() const {
   return BytecodeOffset(RELAXED_READ_INT32_FIELD(*this, kOsrOffsetOffset));
 }
 
-void InstructionStream::set_osr_offset(BytecodeOffset offset) {
+void Code::set_osr_offset(BytecodeOffset offset) {
   RELAXED_WRITE_INT32_FIELD(*this, kOsrOffsetOffset, offset.ToInt());
 }
 
-bool InstructionStream::uses_safepoint_table() const {
-  return is_turbofanned() || is_maglevved() || is_wasm_code();
-}
-
 bool Code::uses_safepoint_table() const {
   return is_turbofanned() || is_maglevved() || is_wasm_code();
 }
 
-int InstructionStream::stack_slots() const {
+int Code::stack_slots() const {
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   const int slots = StackSlotsField::decode(flags);
   DCHECK_IMPLIES(!uses_safepoint_table(), slots == 0);
   return slots;
 }
 
-int Code::stack_slots() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().stack_slots()
-             : OffHeapStackSlots();
-}
-
-int GcSafeCode::stack_slots() const {
-  Code unsafe_this = UnsafeCastToCode();
-  return V8_LIKELY(has_instruction_stream())
-             ? InstructionStream::unchecked_cast(
-                   unsafe_this.raw_instruction_stream(kRelaxedLoad))
-                   .stack_slots()
-             : unsafe_this.OffHeapStackSlots();
-}
-
 bool Code::marked_for_deoptimization() const {
   DCHECK(CodeKindCanDeoptimize(kind()));
-  int32_t flags = kind_specific_flags(kRelaxedLoad);
-  return InstructionStream::MarkedForDeoptimizationField::decode(flags);
-}
-
-bool InstructionStream::marked_for_deoptimization() const {
-  DCHECK(CodeKindCanDeoptimize(kind()));
-  return code(kAcquireLoad).marked_for_deoptimization();
+  int16_t flags = kind_specific_flags(kRelaxedLoad);
+  return MarkedForDeoptimizationField::decode(flags);
 }
 
 void Code::set_marked_for_deoptimization(bool flag) {
   DCHECK(CodeKindCanDeoptimize(kind()));
   DCHECK_IMPLIES(flag, AllowDeoptimization::IsAllowed(GetIsolate()));
-  int32_t previous = kind_specific_flags(kRelaxedLoad);
-  int32_t updated =
-      InstructionStream::MarkedForDeoptimizationField::update(previous, flag);
+  int16_t previous = kind_specific_flags(kRelaxedLoad);
+  int16_t updated = MarkedForDeoptimizationField::update(previous, flag);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
-void InstructionStream::set_marked_for_deoptimization(bool flag) {
-  code(kAcquireLoad).set_marked_for_deoptimization(flag);
-}
-
-bool InstructionStream::embedded_objects_cleared() const {
+bool Code::embedded_objects_cleared() const {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int32_t flags = code(kAcquireLoad).kind_specific_flags(kRelaxedLoad);
-  return EmbeddedObjectsClearedField::decode(flags);
+  int16_t flags = kind_specific_flags(kRelaxedLoad);
+  return Code::EmbeddedObjectsClearedField::decode(flags);
 }
 
-void InstructionStream::set_embedded_objects_cleared(bool flag) {
+void Code::set_embedded_objects_cleared(bool flag) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
   DCHECK_IMPLIES(flag, marked_for_deoptimization());
-  Code container = code(kAcquireLoad);
-  int32_t previous = container.kind_specific_flags(kRelaxedLoad);
-  int32_t updated = EmbeddedObjectsClearedField::update(previous, flag);
-  container.set_kind_specific_flags(updated, kRelaxedStore);
-}
-
-bool InstructionStream::is_optimized_code() const {
-  return CodeKindIsOptimizedJSFunction(kind());
-}
-
-bool InstructionStream::is_wasm_code() const {
-  return kind() == CodeKind::WASM_FUNCTION;
+  int16_t previous = kind_specific_flags(kRelaxedLoad);
+  int16_t updated = Code::EmbeddedObjectsClearedField::update(previous, flag);
+  set_kind_specific_flags(updated, kRelaxedStore);
 }
 
 bool Code::is_wasm_code() const { return kind() == CodeKind::WASM_FUNCTION; }
 
-int InstructionStream::constant_pool_offset() const {
+int Code::constant_pool_offset() const {
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     // Redirection needed since the field doesn't exist in this case.
     return code_comments_offset();
@@ -962,7 +755,7 @@ int InstructionStream::constant_pool_offset() const {
   return ReadField<int>(kConstantPoolOffsetOffset);
 }
 
-void InstructionStream::set_constant_pool_offset(int value) {
+void Code::set_constant_pool_offset(int value) {
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     // Redirection needed since the field doesn't exist in this case.
     return;
@@ -971,78 +764,31 @@ void InstructionStream::set_constant_pool_offset(int value) {
   WriteField<int>(kConstantPoolOffsetOffset, value);
 }
 
-Address InstructionStream::constant_pool() const {
-  if (!has_constant_pool()) return kNullAddress;
-  return metadata_start() + constant_pool_offset();
-}
-
 Address Code::constant_pool() const {
   if (!has_constant_pool()) return kNullAddress;
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().constant_pool()
-             : OffHeapConstantPoolAddress();
-}
-
-Address InstructionStream::code_comments() const {
-  return metadata_start() + code_comments_offset();
-}
-
-int InstructionStream::code_comments_size() const {
-  DCHECK_GE(unwinding_info_offset() - code_comments_offset(), 0);
-  return unwinding_info_offset() - code_comments_offset();
-}
-
-bool InstructionStream::has_code_comments() const {
-  return code_comments_size() > 0;
+  return V8_LIKELY(has_instruction_stream()) ? constant_pool()
+                                             : OffHeapConstantPoolAddress();
 }
 
 Address Code::code_comments() const {
   return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().code_comments()
+             ? metadata_start() + code_comments_offset()
              : OffHeapCodeCommentsAddress();
 }
 
 int Code::code_comments_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().code_comments_size()
-             : OffHeapCodeCommentsSize();
+  return unwinding_info_offset() - code_comments_offset();
 }
 
 bool Code::has_code_comments() const { return code_comments_size() > 0; }
 
-Address InstructionStream::unwinding_info_start() const {
+Address Code::unwinding_info_start() const {
   return metadata_start() + unwinding_info_offset();
 }
 
-Address InstructionStream::unwinding_info_end() const { return metadata_end(); }
-
-int InstructionStream::unwinding_info_size() const {
-  DCHECK_GE(unwinding_info_end(), unwinding_info_start());
-  return static_cast<int>(unwinding_info_end() - unwinding_info_start());
-}
-
-bool InstructionStream::has_unwinding_info() const {
-  return unwinding_info_size() > 0;
-}
-
-Address Code::unwinding_info_start() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().unwinding_info_start()
-             : OffHeapUnwindingInfoAddress();
-}
-
-Address Code::unwinding_info_end() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().metadata_end()
-             : OffHeapMetadataEnd();
-}
+Address Code::unwinding_info_end() const { return metadata_end(); }
 
 int Code::unwinding_info_size() const {
-  return V8_LIKELY(has_instruction_stream())
-             ? instruction_stream().unwinding_info_size()
-             : OffHeapUnwindingInfoSize();
-
-  DCHECK_GE(unwinding_info_end(), unwinding_info_start());
   return static_cast<int>(unwinding_info_end() - unwinding_info_start());
 }
 
@@ -1066,6 +812,11 @@ InstructionStream InstructionStream::FromTargetAddress(Address address) {
   return InstructionStream::unchecked_cast(code);
 }
 
+// static
+Code Code::FromTargetAddress(Address address) {
+  return InstructionStream::FromTargetAddress(address).code(kAcquireLoad);
+}
+
 // static
 InstructionStream InstructionStream::FromEntryAddress(
     Address location_of_address) {
@@ -1077,15 +828,15 @@ InstructionStream InstructionStream::FromEntryAddress(
   return InstructionStream::unchecked_cast(code);
 }
 
-bool InstructionStream::CanContainWeakObjects() {
+bool Code::CanContainWeakObjects() {
   return is_optimized_code() && can_have_weak_objects();
 }
 
-bool InstructionStream::IsWeakObject(HeapObject object) {
+bool Code::IsWeakObject(HeapObject object) {
   return (CanContainWeakObjects() && IsWeakObjectInOptimizedCode(object));
 }
 
-bool InstructionStream::IsWeakObjectInOptimizedCode(HeapObject object) {
+bool Code::IsWeakObjectInOptimizedCode(HeapObject object) {
   Map map_object = object.map(kAcquireLoad);
   if (InstanceTypeChecker::IsMap(map_object)) {
     return Map::cast(object).CanTransition();
@@ -1095,18 +846,16 @@ bool InstructionStream::IsWeakObjectInOptimizedCode(HeapObject object) {
          InstanceTypeChecker::IsContext(map_object);
 }
 
-bool InstructionStream::IsWeakObjectInDeoptimizationLiteralArray(
-    Object object) {
+bool Code::IsWeakObjectInDeoptimizationLiteralArray(Object object) {
   // Maps must be strong because they can be used as part of the description for
   // how to materialize an object upon deoptimization, in which case it is
   // possible to reach the code that requires the Map without anything else
   // holding a strong pointer to that Map.
   return object.IsHeapObject() && !object.IsMap() &&
-         InstructionStream::IsWeakObjectInOptimizedCode(
-             HeapObject::cast(object));
+         Code::IsWeakObjectInOptimizedCode(HeapObject::cast(object));
 }
 
-void InstructionStream::IterateDeoptimizationLiterals(RootVisitor* v) {
+void Code::IterateDeoptimizationLiterals(RootVisitor* v) {
   if (kind() == CodeKind::BASELINE) return;
 
   auto deopt_data = DeoptimizationData::cast(deoptimization_data());
@@ -1126,8 +875,8 @@ void InstructionStream::IterateDeoptimizationLiterals(RootVisitor* v) {
 
 // This field has to have relaxed atomic accessors because it is accessed in the
 // concurrent marker.
-static_assert(FIELD_SIZE(Code::kKindSpecificFlagsOffset) == kInt32Size);
-RELAXED_INT32_ACCESSORS(Code, kind_specific_flags, kKindSpecificFlagsOffset)
+static_assert(FIELD_SIZE(Code::kKindSpecificFlagsOffset) == kInt16Size);
+RELAXED_UINT16_ACCESSORS(Code, kind_specific_flags, kKindSpecificFlagsOffset)
 
 Object Code::raw_instruction_stream() const {
   PtrComprCageBase cage_base = code_cage_base();
@@ -1223,8 +972,6 @@ void Code::UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
 
 Address Code::InstructionStart() const { return code_entry_point(); }
 
-Address Code::body_size() const { return instruction_stream().body_size(); }
-
 void Code::clear_padding() {
   memset(reinterpret_cast<void*>(address() + kUnalignedSize), 0,
          kSize - kUnalignedSize);
@@ -1239,16 +986,21 @@ static_assert(static_cast<int>(Builtin::kNoBuiltinId) == -1);
 static_assert(Builtins::kBuiltinCount < std::numeric_limits<int16_t>::max());
 
 void Code::initialize_flags(CodeKind kind, Builtin builtin_id,
-                            bool is_turbofanned) {
-  uint16_t value =
-      KindField::encode(kind) | IsTurbofannedField::encode(is_turbofanned);
+                            bool is_turbofanned, int stack_slots) {
+  CHECK(0 <= stack_slots && stack_slots < StackSlotsField::kMax);
+  DCHECK(!CodeKindIsInterpretedJSFunction(kind));
+  uint32_t value = KindField::encode(kind) |
+                   IsTurbofannedField::encode(is_turbofanned) |
+                   StackSlotsField::encode(stack_slots);
+  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
+  RELAXED_WRITE_UINT32_FIELD(*this, kFlagsOffset, value);
   set_flags(value, kRelaxedStore);
+  DCHECK_IMPLIES(stack_slots != 0, uses_safepoint_table());
+  DCHECK_IMPLIES(!uses_safepoint_table(), stack_slots == 0);
 
   WriteField<int16_t>(kBuiltinIdOffset, static_cast<int16_t>(builtin_id));
 }
 
-CodeKind Code::kind() const { return KindField::decode(flags(kRelaxedLoad)); }
-
 Builtin Code::builtin_id() const {
   // Rely on sign-extension when converting int16_t to int to preserve
   // kNoBuiltinId value.
@@ -1276,32 +1028,6 @@ inline bool Code::is_baseline_leave_frame_builtin() const {
   return builtin_id() == Builtin::kBaselineLeaveFrame;
 }
 
-//
-// A collection of getters and predicates that forward queries to associated
-// InstructionStream object.
-//
-
-#define DEF_PRIMITIVE_FORWARDING_CODE_GETTER(name, type) \
-  type Code::name() const { return FromCode(*this).name(); }
-
-#define DEF_FORWARDING_CODE_GETTER(name, type,                      \
-                                   result_if_no_instruction_stream) \
-  DEF_GETTER(Code, name, type) {                                    \
-    if (!has_instruction_stream()) {                                \
-      return GetReadOnlyRoots().result_if_no_instruction_stream();  \
-    }                                                               \
-    return FromCode(*this).name(cage_base);                         \
-  }
-
-DEF_FORWARDING_CODE_GETTER(deoptimization_data, FixedArray, empty_fixed_array)
-DEF_FORWARDING_CODE_GETTER(bytecode_or_interpreter_data, HeapObject,
-                           empty_fixed_array)
-DEF_FORWARDING_CODE_GETTER(source_position_table, ByteArray, empty_byte_array)
-DEF_FORWARDING_CODE_GETTER(bytecode_offset_table, ByteArray, empty_byte_array)
-
-#undef DEF_PRIMITIVE_FORWARDING_CODE_GETTER
-#undef DEF_FORWARDING_CODE_GETTER
-
 byte BytecodeArray::get(int index) const {
   DCHECK(index >= 0 && index < this->length());
   return ReadField<byte>(kHeaderSize + index * kCharSize);
@@ -1514,7 +1240,7 @@ inline Object DeoptimizationLiteralArray::get(PtrComprCageBase cage_base,
 
 inline void DeoptimizationLiteralArray::set(int index, Object value) {
   MaybeObject maybe = MaybeObject::FromObject(value);
-  if (InstructionStream::IsWeakObjectInDeoptimizationLiteralArray(value)) {
+  if (Code::IsWeakObjectInDeoptimizationLiteralArray(value)) {
     maybe = MaybeObject::MakeWeak(maybe);
   }
   Set(index, maybe);
@@ -1531,11 +1257,11 @@ void DependentCode::DeoptimizeDependencyGroups(Isolate* isolate, ObjectT object,
 
 // static
 template <typename ObjectT>
-bool DependentCode::MarkCodeForDeoptimization(ObjectT object,
+bool DependentCode::MarkCodeForDeoptimization(Isolate* isolate, ObjectT object,
                                               DependencyGroups groups) {
   // Shared objects are designed to never invalidate code.
   DCHECK(!object.InSharedHeap());
-  return object.dependent_code().MarkCodeForDeoptimization(groups);
+  return object.dependent_code().MarkCodeForDeoptimization(isolate, groups);
 }
 
 }  // namespace internal
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 8bc05ceeba..1ca28beb30 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -172,7 +172,7 @@ int Code::OffHeapStackSlots() const {
   return d.StackSlotsOf(builtin);
 }
 
-void InstructionStream::ClearEmbeddedObjects(Heap* heap) {
+void Code::ClearEmbeddedObjects(Heap* heap) {
   HeapObject undefined = ReadOnlyRoots(heap).undefined_value();
   int mode_mask = RelocInfo::EmbeddedObjectModeMask();
   for (RelocIterator it(*this, mode_mask); !it.done(); it.next()) {
@@ -183,24 +183,30 @@ void InstructionStream::ClearEmbeddedObjects(Heap* heap) {
 }
 
 void InstructionStream::Relocate(intptr_t delta) {
-  for (RelocIterator it(*this, RelocInfo::kApplyMask); !it.done(); it.next()) {
+  Code code = unchecked_code();
+  // This is called during evacuation and code.instruction_stream() will point
+  // to the old object. So pass *this directly to the RelocIterator and use a
+  // dummy Code() since it's not needed.
+  for (RelocIterator it(Code(), *this, code.unchecked_relocation_info(),
+                        code.constant_pool(), RelocInfo::kApplyMask);
+       !it.done(); it.next()) {
     it.rinfo()->apply(delta);
   }
-  FlushICache();
+  FlushInstructionCache(instruction_start(), code.instruction_size());
 }
 
-void InstructionStream::FlushICache() const {
-  FlushInstructionCache(instruction_start(), instruction_size());
+void Code::FlushICache() const {
+  FlushInstructionCache(InstructionStart(), instruction_size());
 }
 
-void InstructionStream::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
-                                        const CodeDesc& desc) {
+void Code::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
+                           const CodeDesc& desc) {
   // Copy code.
-  static_assert(kOnHeapBodyIsContiguous);
-  CopyBytes(reinterpret_cast<byte*>(instruction_start()), desc.buffer,
+  static_assert(InstructionStream::kOnHeapBodyIsContiguous);
+  CopyBytes(reinterpret_cast<byte*>(InstructionStart()), desc.buffer,
             static_cast<size_t>(desc.instr_size));
   // TODO(jgruber,v8:11036): Merge with the above.
-  CopyBytes(reinterpret_cast<byte*>(instruction_start() + desc.instr_size),
+  CopyBytes(reinterpret_cast<byte*>(InstructionStart() + desc.instr_size),
             desc.unwinding_info, static_cast<size_t>(desc.unwinding_info_size));
 
   // Copy reloc info.
@@ -210,8 +216,8 @@ void InstructionStream::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
   RelocateFromDesc(reloc_info, heap, desc);
 }
 
-void InstructionStream::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
-                                         const CodeDesc& desc) {
+void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
+                            const CodeDesc& desc) {
   // Unbox handles and relocate.
   Assembler* origin = desc.origin;
   const int mode_mask = RelocInfo::PostCodegenRelocationMask();
@@ -253,7 +259,7 @@ void InstructionStream::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
 #endif
     } else {
       intptr_t delta =
-          instruction_start() - reinterpret_cast<Address>(desc.buffer);
+          InstructionStart() - reinterpret_cast<Address>(desc.buffer);
       it.rinfo()->apply(delta);
     }
   }
@@ -330,22 +336,7 @@ int AbstractCode::SourceStatementPosition(PtrComprCageBase cage_base,
   return statement_position;
 }
 
-bool InstructionStream::CanDeoptAt(Isolate* isolate, Address pc) {
-  DeoptimizationData deopt_data =
-      DeoptimizationData::cast(deoptimization_data());
-  Address code_start_address = instruction_start();
-  for (int i = 0; i < deopt_data.DeoptCount(); i++) {
-    if (deopt_data.Pc(i).value() == -1) continue;
-    Address address = code_start_address + deopt_data.Pc(i).value();
-    if (address == pc &&
-        deopt_data.GetBytecodeOffset(i) != BytecodeOffset::None()) {
-      return true;
-    }
-  }
-  return false;
-}
-
-bool InstructionStream::IsIsolateIndependent(Isolate* isolate) {
+bool Code::IsIsolateIndependent(Isolate* isolate) {
   static constexpr int kModeMask =
       RelocInfo::AllRealModesMask() &
       ~RelocInfo::ModeMask(RelocInfo::CONST_POOL) &
@@ -380,10 +371,8 @@ bool InstructionStream::IsIsolateIndependent(Isolate* isolate) {
       if (OffHeapInstructionStream::PcIsOffHeap(isolate, target_address))
         continue;
 
-      InstructionStream target =
-          InstructionStream::FromTargetAddress(target_address);
-      CHECK(target.IsInstructionStream());
-      if (Builtins::IsIsolateIndependentBuiltin(target.code(kAcquireLoad))) {
+      Code target = Code::FromTargetAddress(target_address);
+      if (Builtins::IsIsolateIndependentBuiltin(target)) {
         continue;
       }
     }
@@ -395,7 +384,7 @@ bool InstructionStream::IsIsolateIndependent(Isolate* isolate) {
 #endif
 }
 
-bool InstructionStream::Inlines(SharedFunctionInfo sfi) {
+bool Code::Inlines(SharedFunctionInfo sfi) {
   // We can only check for inlining for optimized code.
   DCHECK(is_optimized_code());
   DisallowGarbageCollection no_gc;
@@ -411,8 +400,7 @@ bool InstructionStream::Inlines(SharedFunctionInfo sfi) {
   return false;
 }
 
-InstructionStream::OptimizedCodeIterator::OptimizedCodeIterator(
-    Isolate* isolate)
+Code::OptimizedCodeIterator::OptimizedCodeIterator(Isolate* isolate)
     : isolate_(isolate),
       safepoint_scope_(std::make_unique<SafepointScope>(
           isolate, isolate->is_shared_space_isolate()
@@ -422,7 +410,7 @@ InstructionStream::OptimizedCodeIterator::OptimizedCodeIterator(
           isolate->heap()->code_space()->GetObjectIterator(isolate->heap())),
       state_(kIteratingCodeSpace) {}
 
-InstructionStream InstructionStream::OptimizedCodeIterator::Next() {
+Code Code::OptimizedCodeIterator::Next() {
   while (true) {
     HeapObject object = object_iterator_->Next();
     if (object.is_null()) {
@@ -446,10 +434,11 @@ InstructionStream InstructionStream::OptimizedCodeIterator::Next() {
           state_ = kDone;
           V8_FALLTHROUGH;
         case kDone:
-          return InstructionStream();
+          return Code();
       }
     }
-    InstructionStream code = InstructionStream::cast(object);
+    InstructionStream istream = InstructionStream::cast(object);
+    Code code = istream.code(kAcquireLoad);
     if (!CodeKindCanDeoptimize(code.kind())) continue;
     return code;
   }
@@ -648,10 +637,8 @@ void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
   }
 
   os << "RelocInfo (size = " << code.relocation_size() << ")\n";
-  // TODO(jgruber): Update this once relocations are based on Code, not
-  // InstructionStream objects.
   if (code.has_instruction_stream()) {
-    for (RelocIterator it(code.instruction_stream()); !it.done(); it.next()) {
+    for (RelocIterator it(code); !it.done(); it.next()) {
       it.rinfo()->Print(isolate, os);
     }
   }
@@ -951,7 +938,7 @@ void DependentCode::IterateAndCompact(const IterateAndCompactFn& fn) {
 }
 
 bool DependentCode::MarkCodeForDeoptimization(
-    DependentCode::DependencyGroups deopt_groups) {
+    Isolate* isolate, DependentCode::DependencyGroups deopt_groups) {
   DisallowGarbageCollection no_gc;
 
   bool marked_something = false;
@@ -959,7 +946,7 @@ bool DependentCode::MarkCodeForDeoptimization(
     if ((groups & deopt_groups) == 0) return false;
 
     if (!code.marked_for_deoptimization()) {
-      code.SetMarkedForDeoptimization("code dependencies");
+      code.SetMarkedForDeoptimization(isolate, "code dependencies");
       marked_something = true;
     }
 
@@ -987,7 +974,7 @@ int DependentCode::FillEntryFromBack(int index, int length) {
 void DependentCode::DeoptimizeDependencyGroups(
     Isolate* isolate, DependentCode::DependencyGroups groups) {
   DisallowGarbageCollection no_gc_scope;
-  bool marked_something = MarkCodeForDeoptimization(groups);
+  bool marked_something = MarkCodeForDeoptimization(isolate, groups);
   if (marked_something) {
     DCHECK(AllowCodeDependencyChange::IsAllowed());
     Deoptimizer::DeoptimizeMarkedCode(isolate);
@@ -999,14 +986,9 @@ DependentCode DependentCode::empty_dependent_code(const ReadOnlyRoots& roots) {
   return DependentCode::cast(roots.empty_weak_array_list());
 }
 
-void InstructionStream::SetMarkedForDeoptimization(const char* reason) {
-  set_marked_for_deoptimization(true);
-  Deoptimizer::TraceMarkForDeoptimization(*this, reason);
-}
-
-void Code::SetMarkedForDeoptimization(const char* reason) {
+void Code::SetMarkedForDeoptimization(Isolate* isolate, const char* reason) {
   set_marked_for_deoptimization(true);
-  Deoptimizer::TraceMarkForDeoptimization(FromCode(*this), reason);
+  Deoptimizer::TraceMarkForDeoptimization(isolate, *this, reason);
 }
 
 const char* DependentCode::DependencyGroupName(DependencyGroup group) {
diff --git a/src/objects/code.h b/src/objects/code.h
index b344429446..d1e656e6cc 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -118,18 +118,22 @@ class Code : public HeapObject {
   inline void UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
                                    InstructionStream code);
 
-  DECL_RELAXED_INT32_ACCESSORS(kind_specific_flags)
+  DECL_RELAXED_UINT16_ACCESSORS(kind_specific_flags)
 
   // Initializes internal flags field which stores cached values of some
   // properties of the respective InstructionStream object.
   inline void initialize_flags(CodeKind kind, Builtin builtin_id,
-                               bool is_turbofanned);
-
-  inline Address body_size() const;
+                               bool is_turbofanned, int stack_slots);
 
   // Clear uninitialized padding space. This ensures that the snapshot content
   // is deterministic.
   inline void clear_padding();
+  // Clear the padding in the InstructionStream
+  inline void ClearInstructionStreamPadding();
+
+  // Flushes the instruction cache for the executable instructions of this code
+  // object. Make sure to call this while the code is still writable.
+  void FlushICache() const;
 
   DECL_PRIMITIVE_ACCESSORS(can_have_weak_objects, bool)
   DECL_PRIMITIVE_ACCESSORS(marked_for_deoptimization, bool)
@@ -142,6 +146,38 @@ class Code : public HeapObject {
 
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
+  DECL_PRIMITIVE_ACCESSORS(instruction_size, int)
+  DECL_PRIMITIVE_ACCESSORS(metadata_size, int)
+  // [handler_table_offset]: The offset where the exception handler table
+  // starts.
+  DECL_PRIMITIVE_ACCESSORS(handler_table_offset, int)
+  // [unwinding_info_offset]: Offset of the unwinding info section.
+  DECL_PRIMITIVE_ACCESSORS(unwinding_info_offset, int32_t)
+  // [deoptimization_data]: Array containing data for deopt for non-baseline
+  // code.
+  DECL_ACCESSORS(deoptimization_data, FixedArray)
+  // [bytecode_or_interpreter_data]: BytecodeArray or InterpreterData for
+  // baseline code.
+  DECL_ACCESSORS(bytecode_or_interpreter_data, HeapObject)
+  // [source_position_table]: ByteArray for the source positions table for
+  // non-baseline code.
+  DECL_ACCESSORS(source_position_table, ByteArray)
+  // [bytecode_offset_table]: ByteArray for the bytecode offset for baseline
+  // code.
+  DECL_ACCESSORS(bytecode_offset_table, ByteArray)
+  // [relocation_info]: InstructionStream relocation information
+  DECL_ACCESSORS(relocation_info, ByteArray)
+  DECL_PRIMITIVE_ACCESSORS(inlined_bytecode_size, unsigned)
+  DECL_PRIMITIVE_ACCESSORS(osr_offset, BytecodeOffset)
+  // [code_comments_offset]: Offset of the code comment section.
+  DECL_PRIMITIVE_ACCESSORS(code_comments_offset, int)
+  // [constant_pool offset]: Offset of the constant pool.
+  DECL_PRIMITIVE_ACCESSORS(constant_pool_offset, int)
+
+  // Unchecked accessors to be used during GC.
+  inline ByteArray unchecked_relocation_info() const;
+  inline FixedArray unchecked_deoptimization_data() const;
+
   inline CodeKind kind() const;
   inline Builtin builtin_id() const;
   inline bool is_builtin() const;
@@ -177,11 +213,6 @@ class Code : public HeapObject {
   // reserved in the code prologue; otherwise 0.
   inline int stack_slots() const;
 
-  DECL_GETTER(deoptimization_data, FixedArray)
-  DECL_GETTER(bytecode_or_interpreter_data, HeapObject)
-  DECL_GETTER(source_position_table, ByteArray)
-  DECL_GETTER(bytecode_offset_table, ByteArray)
-
   inline ByteArray SourcePositionTable(Isolate* isolate,
                                        SharedFunctionInfo sfi) const;
 
@@ -213,6 +244,23 @@ class Code : public HeapObject {
   inline byte* relocation_end() const;
   inline int relocation_size() const;
 
+  // [safepoint_table_offset]: The offset where the safepoint table starts.
+  inline int safepoint_table_offset() const { return 0; }
+
+  inline Address body_start() const;
+  inline Address body_end() const;
+  inline int body_size() const;
+
+  inline Address metadata_start() const;
+  inline Address metadata_end() const;
+
+  inline Address handler_table_address() const;
+
+  inline Address safepoint_table_address() const;
+
+  inline int CodeSize() const;
+  inline int SizeIncludingMetadata() const;
+
   // When builtins un-embedding is enabled for the Isolate
   // (see Isolate::is_short_builtin_calls_enabled()) then both embedded and
   // un-embedded builtins might be exeuted and thus two kinds of |pc|s might
@@ -243,7 +291,60 @@ class Code : public HeapObject {
 
   inline int GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const;
 
-  void SetMarkedForDeoptimization(const char* reason);
+  void SetMarkedForDeoptimization(Isolate* isolate, const char* reason);
+
+  inline bool CanContainWeakObjects();
+
+  inline bool IsWeakObject(HeapObject object);
+
+  static inline bool IsWeakObjectInOptimizedCode(HeapObject object);
+
+  static inline bool IsWeakObjectInDeoptimizationLiteralArray(Object object);
+
+  // This function should be called only from GC.
+  void ClearEmbeddedObjects(Heap* heap);
+
+  // [embedded_objects_cleared]: If CodeKindIsOptimizedJSFunction(kind), tells
+  // whether the embedded objects in the code marked for deoptimization were
+  // cleared. Note that embedded_objects_cleared() implies
+  // marked_for_deoptimization().
+  inline bool embedded_objects_cleared() const;
+  inline void set_embedded_objects_cleared(bool flag);
+
+  // Migrate code from desc without flushing the instruction cache.
+  void CopyFromNoFlush(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
+  void RelocateFromDesc(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
+
+  // Copy the RelocInfo portion of |desc| to |dest|. The ByteArray must be
+  // exactly the same size as the RelocInfo in |desc|.
+  static inline void CopyRelocInfoToByteArray(ByteArray dest,
+                                              const CodeDesc& desc);
+
+  bool IsIsolateIndependent(Isolate* isolate);
+
+  inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
+                                                       BytecodeArray bytecodes);
+
+  inline uintptr_t GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
+                                                     BytecodeArray bytecodes);
+
+  // Returns true if the function is inlined in the code.
+  bool Inlines(SharedFunctionInfo sfi);
+
+  // Returns the PC of the next bytecode in execution order.
+  // If the bytecode at the given offset is JumpLoop, the PC of the jump target
+  // is returned. Other jumps are not allowed.
+  // For other bytecodes this is equivalent to
+  // GetBaselineEndPCForBytecodeOffset.
+  inline uintptr_t GetBaselinePCForNextExecutedBytecode(
+      int bytecode_offset, BytecodeArray bytecodes);
+
+  inline int GetBytecodeOffsetForBaselinePC(Address baseline_pc,
+                                            BytecodeArray bytecodes);
+
+  inline void IterateDeoptimizationLiterals(RootVisitor* v);
+
+  static inline Code FromTargetAddress(Address address);
 
 #ifdef ENABLE_DISASSEMBLER
   V8_EXPORT_PRIVATE void Disassemble(const char* name, std::ostream& os,
@@ -258,19 +359,35 @@ class Code : public HeapObject {
   DECL_VERIFIER(Code)
 
 // Layout description.
-#define CODE_DATA_FIELDS(V)                                 \
-  /* Strong pointer fields. */                              \
-  V(kPointerFieldsStrongEndOffset, 0)                       \
-  /* Strong InstructionStream pointer fields. */            \
-  V(kInstructionStreamOffset, kTaggedSize)                  \
-  V(kCodePointerFieldsStrongEndOffset, 0)                   \
-  /* Raw data fields. */                                    \
-  V(kCodeEntryPointOffset, kSystemPointerSize)              \
-  V(kFlagsOffset, kUInt16Size)                              \
-  V(kBuiltinIdOffset, kInt16Size)                           \
-  V(kKindSpecificFlagsOffset, kInt32Size)                   \
-  V(kUnalignedSize, OBJECT_POINTER_PADDING(kUnalignedSize)) \
-  /* Total size. */                                         \
+#define CODE_DATA_FIELDS(V)                                                   \
+  /* Strong pointer fields. */                                                \
+  V(kRelocationInfoOffset, kTaggedSize)                                       \
+  V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
+  V(kPositionTableOffset, kTaggedSize)                                        \
+  V(kPointerFieldsStrongEndOffset, 0)                                         \
+  /* Strong InstructionStream pointer fields. */                              \
+  V(kInstructionStreamOffset, kTaggedSize)                                    \
+  V(kCodePointerFieldsStrongEndOffset, 0)                                     \
+  /* Raw data fields. */                                                      \
+  /* Data or code not directly visited by GC directly starts here. */         \
+  V(kDataStart, 0)                                                            \
+  V(kCodeEntryPointOffset, kSystemPointerSize)                                \
+  /* The serializer needs to copy bytes starting from here verbatim. */       \
+  /* Objects embedded into code is visited via reloc info. */                 \
+  V(kFlagsOffset, kInt32Size)                                                 \
+  V(kBuiltinIdOffset, kInt16Size)                                             \
+  V(kKindSpecificFlagsOffset, kInt16Size)                                     \
+  V(kInstructionSizeOffset, kIntSize)                                         \
+  V(kMetadataSizeOffset, kIntSize)                                            \
+  V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
+  V(kOsrOffsetOffset, kInt32Size)                                             \
+  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
+  V(kHandlerTableOffsetOffset, kIntSize)                                      \
+  V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
+  V(kConstantPoolOffsetOffset, V8_EMBEDDED_CONSTANT_POOL_BOOL ? kIntSize : 0) \
+  V(kCodeCommentsOffsetOffset, kIntSize)                                      \
+  V(kUnalignedSize, OBJECT_POINTER_PADDING(kUnalignedSize))                   \
+  /* Total size. */                                                           \
   V(kSize, 0)
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, CODE_DATA_FIELDS)
@@ -288,16 +405,38 @@ class Code : public HeapObject {
   class BodyDescriptor;
 
   // Flags layout.
-#define FLAGS_BIT_FIELDS(V, _) \
-  V(KindField, CodeKind, 4, _) \
-  V(IsTurbofannedField, bool, 1, _)
-  /* The other 10 bits are still free. */
+#define FLAGS_BIT_FIELDS(V, _)      \
+  V(KindField, CodeKind, 4, _)      \
+  V(IsTurbofannedField, bool, 1, _) \
+  V(StackSlotsField, int, 24, _)
+  /* The other 3 bits are still free. */
+  // TODO(v8:13784): merge this with KindSpecificFlags by dropping the
+  // IsPromiseRejection field or taking one bit from the StackSlots field.
 
   DEFINE_BIT_FIELDS(FLAGS_BIT_FIELDS)
 #undef FLAGS_BIT_FIELDS
-  static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 5);
+  static_assert(kCodeKindCount <= KindField::kNumValues);
+  static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 29);
   static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
-                FIELD_SIZE(Code::kFlagsOffset) * kBitsPerByte);
+                FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
+
+  // KindSpecificFlags layout.
+#define KIND_SPECIFIC_FLAGS_BIT_FIELDS(V, _)  \
+  V(MarkedForDeoptimizationField, bool, 1, _) \
+  V(EmbeddedObjectsClearedField, bool, 1, _)  \
+  V(CanHaveWeakObjectsField, bool, 1, _)      \
+  V(IsPromiseRejectionField, bool, 1, _)
+  DEFINE_BIT_FIELDS(KIND_SPECIFIC_FLAGS_BIT_FIELDS)
+#undef CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS
+  static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
+  static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
+                FIELD_SIZE(Code::kKindSpecificFlagsOffset) * kBitsPerByte);
+
+  // The {marked_for_deoptimization} field is accessed from generated code.
+  static const int kMarkedForDeoptimizationBit =
+      MarkedForDeoptimizationField::kShift;
+
+  class OptimizedCodeIterator;
 
  private:
   inline void init_code_entry_point(Isolate* isolate, Address initial_value);
@@ -324,6 +463,17 @@ class Code : public HeapObject {
   V8_EXPORT_PRIVATE int OffHeapUnwindingInfoSize() const;
   V8_EXPORT_PRIVATE int OffHeapStackSlots() const;
 
+  enum BytecodeToPCPosition {
+    kPcAtStartOfBytecode,
+    // End of bytecode equals the start of the next bytecode.
+    // We need it when we deoptimize to the next bytecode (lazy deopt or deopt
+    // of non-topmost frame).
+    kPcAtEndOfBytecode
+  };
+  inline uintptr_t GetBaselinePCForBytecodeOffset(int bytecode_offset,
+                                                  BytecodeToPCPosition position,
+                                                  BytecodeArray bytecodes);
+
   template <typename IsolateT>
   friend class Deserializer;
   friend class GcSafeCode;  // For OffHeapFoo functions.
@@ -376,14 +526,16 @@ class GcSafeCode : public HeapObject {
   inline bool has_tagged_outgoing_params() const;
   inline bool marked_for_deoptimization() const;
   inline Object raw_instruction_stream() const;
+  inline Address constant_pool() const;
+  inline int stack_slots() const;
 
   inline int GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const;
   inline Address InstructionStart(Isolate* isolate, Address pc) const;
   inline Address InstructionEnd(Isolate* isolate, Address pc) const;
+  inline bool CanDeoptAt(Isolate* isolate, Address pc) const;
 
   // Accessors that had to be modified to be used in GC settings.
   inline Address SafepointTableAddress() const;
-  inline int stack_slots() const;
 
  private:
   OBJECT_CONSTRUCTORS(GcSafeCode, HeapObject);
@@ -395,10 +547,6 @@ class InstructionStream : public HeapObject {
  public:
   NEVER_READ_ONLY_SPACE
 
-  // Opaque data type for encapsulating code flags like kind, inline cache
-  // state, and arguments count.
-  using Flags = uint32_t;
-
   // All InstructionStream objects have the following layout:
   //
   //  +--------------------------+
@@ -430,153 +578,18 @@ class InstructionStream : public HeapObject {
   static constexpr bool kBodyIsContiguous =
       kOnHeapBodyIsContiguous && kOffHeapBodyIsContiguous;
 
-  inline Address body_start() const;
-  inline Address body_end() const;
-  inline int body_size() const;
-
   inline Address instruction_start() const;
-  inline Address instruction_end() const;
-
-  inline int instruction_size() const;
-  inline void set_instruction_size(int value);
-
-  inline Address metadata_start() const;
-  inline Address metadata_end() const;
-  inline int metadata_size() const;
-  inline void set_metadata_size(int value);
 
   // The metadata section is aligned to this value.
   static constexpr int kMetadataAlignment = kIntSize;
 
-  // [safepoint_table_offset]: The offset where the safepoint table starts.
-  inline int safepoint_table_offset() const { return 0; }
-  inline Address safepoint_table_address() const;
-  inline int safepoint_table_size() const;
-  inline bool has_safepoint_table() const;
-
-  // [handler_table_offset]: The offset where the exception handler table
-  // starts.
-  inline int handler_table_offset() const;
-  inline void set_handler_table_offset(int offset);
-  inline Address handler_table_address() const;
-  inline int handler_table_size() const;
-  inline bool has_handler_table() const;
-
-  // [constant_pool offset]: Offset of the constant pool.
-  inline int constant_pool_offset() const;
-  inline void set_constant_pool_offset(int offset);
-  inline Address constant_pool() const;
-  inline int constant_pool_size() const;
-  inline bool has_constant_pool() const;
-
-  // [code_comments_offset]: Offset of the code comment section.
-  inline int code_comments_offset() const;
-  inline void set_code_comments_offset(int offset);
-  inline Address code_comments() const;
-  inline int code_comments_size() const;
-  inline bool has_code_comments() const;
-
-  // [unwinding_info_offset]: Offset of the unwinding info section.
-  inline int32_t unwinding_info_offset() const;
-  inline void set_unwinding_info_offset(int32_t offset);
-  inline Address unwinding_info_start() const;
-  inline Address unwinding_info_end() const;
-  inline int unwinding_info_size() const;
-  inline bool has_unwinding_info() const;
-
-  // [relocation_info]: InstructionStream relocation information
-  DECL_ACCESSORS(relocation_info, ByteArray)
-
-  // This function should be called only from GC.
-  void ClearEmbeddedObjects(Heap* heap);
-
-  // [deoptimization_data]: Array containing data for deopt for non-baseline
-  // code.
-  DECL_ACCESSORS(deoptimization_data, FixedArray)
-  // [bytecode_or_interpreter_data]: BytecodeArray or InterpreterData for
-  // baseline code.
-  DECL_ACCESSORS(bytecode_or_interpreter_data, HeapObject)
-
-  // [source_position_table]: ByteArray for the source positions table for
-  // non-baseline code.
-  DECL_ACCESSORS(source_position_table, ByteArray)
-  // [bytecode_offset_table]: ByteArray for the bytecode offset for baseline
-  // code.
-  DECL_ACCESSORS(bytecode_offset_table, ByteArray)
-
-  // If source positions have not been collected or an exception has been thrown
-  // this will return empty_byte_array.
-  inline ByteArray SourcePositionTable(Isolate* isolate,
-                                       SharedFunctionInfo sfi) const;
-
   // [code]: The associated Code object.
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
   DECL_RELEASE_ACQUIRE_ACCESSORS(raw_code, HeapObject)
 
-  // Unchecked accessors to be used during GC.
-  inline ByteArray unchecked_relocation_info() const;
-
-  inline int relocation_size() const;
-
-  // [kind]: Access to specific code kind.
-  inline CodeKind kind() const;
-
-  inline bool is_optimized_code() const;
-  inline bool is_wasm_code() const;
-
-  inline bool is_interpreter_trampoline_builtin() const;
-  inline bool is_baseline_trampoline_builtin() const;
-  inline bool is_baseline_leave_frame_builtin() const;
-
-  // Tells whether the outgoing parameters of this code are tagged pointers.
-  inline bool has_tagged_outgoing_params() const;
-
-  // [is_turbofanned]: Tells whether the code object was generated by the
-  // TurboFan optimizing compiler.
-  inline bool is_turbofanned() const;
-
-  // TODO(jgruber): Reconsider these predicates; we should probably merge them
-  // and rename to something appropriate.
-  inline bool is_maglevved() const;
-
-  // [can_have_weak_objects]: If CodeKindIsOptimizedJSFunction(kind), tells
-  // whether the embedded objects in code should be treated weakly.
-  inline bool can_have_weak_objects() const;
-  inline void set_can_have_weak_objects(bool value);
-
-  // [builtin]: For builtins, tells which builtin index the code object
-  // has. The builtin index is a non-negative integer for builtins, and
-  // Builtin::kNoBuiltinId (-1) otherwise.
-  inline Builtin builtin_id() const;
-  inline void set_builtin_id(Builtin builtin);
-  inline bool is_builtin() const;
-
-  inline unsigned inlined_bytecode_size() const;
-  inline void set_inlined_bytecode_size(unsigned size);
-
-  inline BytecodeOffset osr_offset() const;
-  inline void set_osr_offset(BytecodeOffset offset);
-
-  // [uses_safepoint_table]: Whether this InstructionStream object uses
-  // safepoint tables (note the table may still be empty, see
-  // has_safepoint_table).
-  inline bool uses_safepoint_table() const;
-
-  // [stack_slots]: If {uses_safepoint_table()}, the number of stack slots
-  // reserved in the code prologue; otherwise 0.
-  inline int stack_slots() const;
-
-  // [marked_for_deoptimization]: If CodeKindCanDeoptimize(kind), tells whether
-  // the code is going to be deoptimized.
-  inline bool marked_for_deoptimization() const;
-  inline void set_marked_for_deoptimization(bool flag);
-
-  // [embedded_objects_cleared]: If CodeKindIsOptimizedJSFunction(kind), tells
-  // whether the embedded objects in the code marked for deoptimization were
-  // cleared. Note that embedded_objects_cleared() implies
-  // marked_for_deoptimization().
-  inline bool embedded_objects_cleared() const;
-  inline void set_embedded_objects_cleared(bool flag);
+  // A convenience wrapper around raw_code that will do an unchecked cast for
+  // you.
+  inline Code unchecked_code() const;
 
   // The entire code object including its header is copied verbatim to the
   // snapshot so that it can be written in one, fast, memcpy during
@@ -596,27 +609,9 @@ class InstructionStream : public HeapObject {
   inline PtrComprCageBase main_cage_base(RelaxedLoadTag) const;
   inline void set_main_cage_base(Address cage_base, RelaxedStoreTag);
 
-  // Clear uninitialized padding space. This ensures that the snapshot content
-  // is deterministic. Depending on the V8 build mode there could be no padding.
-  inline void clear_padding();
-  // Initialize the flags field. Similar to clear_padding above this ensure that
-  // the snapshot content is deterministic.
-  inline void initialize_flags(CodeKind kind, bool is_turbofanned,
-                               int stack_slots);
-
   static inline InstructionStream FromTargetAddress(Address address);
   static inline InstructionStream FromEntryAddress(Address location_of_address);
 
-  // Returns the size of code and its metadata. This includes the size of code
-  // relocation information, deoptimization data.
-  DECL_GETTER(SizeIncludingMetadata, int)
-
-  // Returns the address of the first relocation info (read backwards!).
-  inline byte* relocation_start() const;
-
-  // Returns the address right after the relocation info (read backwards!).
-  inline byte* relocation_end() const;
-
   // InstructionStream entry point.
   inline Address entry() const;
 
@@ -627,36 +622,6 @@ class InstructionStream : public HeapObject {
   // object has been moved by delta bytes.
   void Relocate(intptr_t delta);
 
-  // Migrate code from desc without flushing the instruction cache.
-  void CopyFromNoFlush(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
-  void RelocateFromDesc(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
-
-  // Copy the RelocInfo portion of |desc| to |dest|. The ByteArray must be
-  // exactly the same size as the RelocInfo in |desc|.
-  static inline void CopyRelocInfoToByteArray(ByteArray dest,
-                                              const CodeDesc& desc);
-
-  inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
-                                                       BytecodeArray bytecodes);
-
-  inline uintptr_t GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
-                                                     BytecodeArray bytecodes);
-
-  // Returns the PC of the next bytecode in execution order.
-  // If the bytecode at the given offset is JumpLoop, the PC of the jump target
-  // is returned. Other jumps are not allowed.
-  // For other bytecodes this is equivalent to
-  // GetBaselineEndPCForBytecodeOffset.
-  inline uintptr_t GetBaselinePCForNextExecutedBytecode(
-      int bytecode_offset, BytecodeArray bytecodes);
-
-  inline int GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                            BytecodeArray bytecodes);
-
-  // Flushes the instruction cache for the executable instructions of this code
-  // object. Make sure to call this while the code is still writable.
-  void FlushICache() const;
-
   // Returns the object size for a given body (used for allocation).
   static int SizeFor(int body_size) {
     return RoundUp(kHeaderSize + body_size, kCodeAlignment);
@@ -673,56 +638,19 @@ class InstructionStream : public HeapObject {
   DECL_PRINTER(InstructionStream)
   DECL_VERIFIER(InstructionStream)
 
-  bool CanDeoptAt(Isolate* isolate, Address pc);
-
-  void SetMarkedForDeoptimization(const char* reason);
-
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
-  bool IsIsolateIndependent(Isolate* isolate);
-
-  inline bool CanContainWeakObjects();
-
-  inline bool IsWeakObject(HeapObject object);
-
-  static inline bool IsWeakObjectInOptimizedCode(HeapObject object);
-
-  static inline bool IsWeakObjectInDeoptimizationLiteralArray(Object object);
-
-  inline void IterateDeoptimizationLiterals(RootVisitor* v);
-
-  // Returns true if the function is inlined in the code.
-  bool Inlines(SharedFunctionInfo sfi);
-
-  class OptimizedCodeIterator;
-
   // Layout description.
-#define ISTREAM_FIELDS(V)                                                     \
-  V(kRelocationInfoOffset, kTaggedSize)                                       \
-  V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
-  V(kPositionTableOffset, kTaggedSize)                                        \
-  V(kCodeOffset, kTaggedSize)                                                 \
-  /* Data or code not directly visited by GC directly starts here. */         \
-  /* The serializer needs to copy bytes starting from here verbatim. */       \
-  /* Objects embedded into code is visited via reloc info. */                 \
-  V(kDataStart, 0)                                                            \
-  V(kMainCageBaseUpper32BitsOffset,                                           \
-    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                            \
-  V(kInstructionSizeOffset, kIntSize)                                         \
-  V(kMetadataSizeOffset, kIntSize)                                            \
-  V(kFlagsOffset, kInt32Size)                                                 \
-  V(kBuiltinIndexOffset, kIntSize)                                            \
-  V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
-  V(kOsrOffsetOffset, kInt32Size)                                             \
-  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
-  V(kHandlerTableOffsetOffset, kIntSize)                                      \
-  V(kConstantPoolOffsetOffset, V8_EMBEDDED_CONSTANT_POOL_BOOL ? kIntSize : 0) \
-  V(kCodeCommentsOffsetOffset, kIntSize)                                      \
-  V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
-  V(kUnalignedHeaderSize, 0)                                                  \
-  /* Add padding to align the instruction start following right after */      \
-  /* the InstructionStream object header. */                                  \
-  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset))     \
+#define ISTREAM_FIELDS(V)                                                 \
+  V(kCodeOffset, kTaggedSize)                                             \
+  /* Data or code not directly visited by GC directly starts here. */     \
+  V(kDataStart, 0)                                                        \
+  V(kMainCageBaseUpper32BitsOffset,                                       \
+    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                        \
+  V(kUnalignedHeaderSize, 0)                                              \
+  /* Add padding to align the instruction start following right after */  \
+  /* the InstructionStream object header. */                              \
+  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset)) \
   V(kHeaderSize, 0)
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, ISTREAM_FIELDS)
@@ -732,30 +660,30 @@ class InstructionStream : public HeapObject {
   // object header due to padding for code alignment.
 #if V8_TARGET_ARCH_ARM64
   static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 4 : (COMPRESS_POINTERS_BOOL ? 8 : 20);
+      V8_EXTERNAL_CODE_SPACE_BOOL ? 20 : (COMPRESS_POINTERS_BOOL ? 24 : 16);
 #elif V8_TARGET_ARCH_MIPS64
   static constexpr int kHeaderPaddingSize = 20;
 #elif V8_TARGET_ARCH_LOONG64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 8 : 20);
+  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 20);
 #elif V8_TARGET_ARCH_X64
   static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 4 : (COMPRESS_POINTERS_BOOL ? 8 : 52);
+      V8_EXTERNAL_CODE_SPACE_BOOL ? 52 : (COMPRESS_POINTERS_BOOL ? 56 : 48);
 #elif V8_TARGET_ARCH_ARM
-  static constexpr int kHeaderPaddingSize = 8;
+  static constexpr int kHeaderPaddingSize = 24;
 #elif V8_TARGET_ARCH_IA32
-  static constexpr int kHeaderPaddingSize = 8;
+  static constexpr int kHeaderPaddingSize = 24;
 #elif V8_TARGET_ARCH_MIPS
-  static constexpr int kHeaderPaddingSize = 8;
+  static constexpr int kHeaderPaddingSize = 24;
 #elif V8_TARGET_ARCH_PPC64
   static constexpr int kHeaderPaddingSize =
       V8_EMBEDDED_CONSTANT_POOL_BOOL ? (COMPRESS_POINTERS_BOOL ? 4 : 48)
                                      : (COMPRESS_POINTERS_BOOL ? 8 : 52);
 #elif V8_TARGET_ARCH_S390X
-  static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 8 : 20;
+  static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 24 : 20;
 #elif V8_TARGET_ARCH_RISCV64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 8 : 20);
+  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 20);
 #elif V8_TARGET_ARCH_RISCV32
-  static constexpr int kHeaderPaddingSize = 8;
+  static constexpr int kHeaderPaddingSize = 24;
 #else
 #error Unknown architecture.
 #endif
@@ -763,34 +691,6 @@ class InstructionStream : public HeapObject {
 
   class BodyDescriptor;
 
-  // Flags layout.  base::BitField<type, shift, size>.
-#define ISTREAM_FLAGS_BIT_FIELDS(V, _) \
-  V(KindField, CodeKind, 4, _)         \
-  V(IsTurbofannedField, bool, 1, _)    \
-  V(StackSlotsField, int, 24, _)
-  DEFINE_BIT_FIELDS(ISTREAM_FLAGS_BIT_FIELDS)
-#undef ISTREAM_FLAGS_BIT_FIELDS
-  static_assert(kCodeKindCount <= KindField::kNumValues);
-  static_assert(ISTREAM_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 29);
-  static_assert(ISTREAM_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
-                FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
-
-  // KindSpecificFlags layout.
-#define ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS(V, _) \
-  V(MarkedForDeoptimizationField, bool, 1, _)        \
-  V(EmbeddedObjectsClearedField, bool, 1, _)         \
-  V(CanHaveWeakObjectsField, bool, 1, _)             \
-  V(IsPromiseRejectionField, bool, 1, _)
-  DEFINE_BIT_FIELDS(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS)
-#undef ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS
-  static_assert(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
-  static_assert(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
-                FIELD_SIZE(Code::kKindSpecificFlagsOffset) * kBitsPerByte);
-
-  // The {marked_for_deoptimization} field is accessed from generated code.
-  static const int kMarkedForDeoptimizationBit =
-      MarkedForDeoptimizationField::kShift;
-
   static const int kArgumentsBits = 16;
   // Reserve one argument count value as the "don't adapt arguments" sentinel.
   static const int kMaxArguments = (1 << kArgumentsBits) - 2;
@@ -803,26 +703,15 @@ class InstructionStream : public HeapObject {
 
   bool is_promise_rejection() const;
 
-  enum BytecodeToPCPosition {
-    kPcAtStartOfBytecode,
-    // End of bytecode equals the start of the next bytecode.
-    // We need it when we deoptimize to the next bytecode (lazy deopt or deopt
-    // of non-topmost frame).
-    kPcAtEndOfBytecode
-  };
-  inline uintptr_t GetBaselinePCForBytecodeOffset(int bytecode_offset,
-                                                  BytecodeToPCPosition position,
-                                                  BytecodeArray bytecodes);
-
   OBJECT_CONSTRUCTORS(InstructionStream, HeapObject);
 };
 
-class InstructionStream::OptimizedCodeIterator {
+class Code::OptimizedCodeIterator {
  public:
   explicit OptimizedCodeIterator(Isolate* isolate);
   OptimizedCodeIterator(const OptimizedCodeIterator&) = delete;
   OptimizedCodeIterator& operator=(const OptimizedCodeIterator&) = delete;
-  InstructionStream Next();
+  Code Next();
 
  private:
   Isolate* isolate_;
@@ -836,7 +725,6 @@ class InstructionStream::OptimizedCodeIterator {
 // Helper functions for converting InstructionStream objects to
 // Code and back.
 inline Code ToCode(InstructionStream code);
-inline Handle<Code> ToCode(Handle<InstructionStream> code, Isolate* isolate);
 inline InstructionStream FromCode(Code code);
 inline InstructionStream FromCode(Code code, Isolate* isolate, RelaxedLoadTag);
 inline InstructionStream FromCode(Code code, PtrComprCageBase, RelaxedLoadTag);
@@ -953,7 +841,7 @@ class DependentCode : public WeakArrayList {
                                          DependencyGroups groups);
 
   template <typename ObjectT>
-  static bool MarkCodeForDeoptimization(ObjectT object,
+  static bool MarkCodeForDeoptimization(Isolate* isolate, ObjectT object,
                                         DependencyGroups groups);
 
   V8_EXPORT_PRIVATE static DependentCode empty_dependent_code(
@@ -978,7 +866,8 @@ class DependentCode : public WeakArrayList {
                                               DependencyGroups groups,
                                               Handle<Code> code);
 
-  bool MarkCodeForDeoptimization(DependencyGroups deopt_groups);
+  bool MarkCodeForDeoptimization(Isolate* isolate,
+                                 DependencyGroups deopt_groups);
 
   void DeoptimizeDependencyGroups(Isolate* isolate, DependencyGroups groups);
 
diff --git a/src/objects/js-function-inl.h b/src/objects/js-function-inl.h
index 08c6797453..152c00d1b1 100644
--- a/src/objects/js-function-inl.h
+++ b/src/objects/js-function-inl.h
@@ -80,11 +80,6 @@ void JSFunction::set_code(Code value, ReleaseStoreTag, WriteBarrierMode mode) {
 }
 RELEASE_ACQUIRE_ACCESSORS(JSFunction, context, Context, kContextOffset)
 
-void JSFunction::set_code(InstructionStream code, ReleaseStoreTag,
-                          WriteBarrierMode mode) {
-  set_code(ToCode(code), kReleaseStore, mode);
-}
-
 Address JSFunction::code_entry_point() const {
   return Code::cast(code()).code_entry_point();
 }
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index e24882d1a8..da353d2a66 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -120,12 +120,6 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // are fully initialized.
   DECL_ACCESSORS(code, Code)
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
-  // Convenient overloads to avoid unnecessary InstructionStream <->
-  // Code conversions.
-  // TODO(v8:11880): remove once |code| accessors are migrated to
-  // Code.
-  inline void set_code(InstructionStream code, ReleaseStoreTag,
-                       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Returns the address of the function code's instruction start.
   inline Address code_entry_point() const;
diff --git a/src/objects/js-regexp.cc b/src/objects/js-regexp.cc
index 715ed0db04..2fc3c9149d 100644
--- a/src/objects/js-regexp.cc
+++ b/src/objects/js-regexp.cc
@@ -157,8 +157,8 @@ Object JSRegExp::code(bool is_latin1) const {
   return value;
 }
 
-void JSRegExp::set_code(bool is_latin1, Handle<InstructionStream> code) {
-  SetDataAt(code_index(is_latin1), ToCode(*code));
+void JSRegExp::set_code(bool is_latin1, Handle<Code> code) {
+  SetDataAt(code_index(is_latin1), *code);
 }
 
 Object JSRegExp::bytecode(bool is_latin1) const {
diff --git a/src/objects/js-regexp.h b/src/objects/js-regexp.h
index 1a2eab8bf5..b1a8c1336a 100644
--- a/src/objects/js-regexp.h
+++ b/src/objects/js-regexp.h
@@ -68,8 +68,7 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   inline String atom_pattern() const;
   // This could be a Smi kUninitializedValue or InstructionStream.
   V8_EXPORT_PRIVATE Object code(bool is_latin1) const;
-  V8_EXPORT_PRIVATE void set_code(bool is_unicode,
-                                  Handle<InstructionStream> code);
+  V8_EXPORT_PRIVATE void set_code(bool is_unicode, Handle<Code> code);
   // This could be a Smi kUninitializedValue or ByteArray.
   V8_EXPORT_PRIVATE Object bytecode(bool is_latin1) const;
   // Sets the bytecode as well as initializing trampoline slots to the
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 886f62513e..769fe4272b 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -940,11 +940,8 @@ class CoverageInfo::BodyDescriptor final : public BodyDescriptorBase {
 
 class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
  public:
-  static_assert(kRelocationInfoOffset + kTaggedSize ==
-                kDeoptimizationDataOrInterpreterDataOffset);
-  static_assert(kDeoptimizationDataOrInterpreterDataOffset + kTaggedSize ==
-                kPositionTableOffset);
-  static_assert(kPositionTableOffset + kTaggedSize == kCodeOffset);
+  static_assert(static_cast<int>(HeapObject::kHeaderSize) ==
+                static_cast<int>(kCodeOffset));
   static_assert(kCodeOffset + kTaggedSize == kDataStart);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
@@ -966,9 +963,12 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
   template <typename ObjectVisitor>
   static inline void IterateBody(Map map, HeapObject obj, ObjectVisitor* v) {
     // GC does not visit data/code in the header and in the body directly.
-    IteratePointers(obj, kRelocationInfoOffset, kDataStart, v);
+    IteratePointers(obj, kCodeOffset, kDataStart, v);
 
-    RelocIterator it(InstructionStream::cast(obj), kRelocModeMask);
+    InstructionStream istream = InstructionStream::cast(obj);
+    Code code = istream.unchecked_code();
+    RelocIterator it(code, istream, code.unchecked_relocation_info(),
+                     code.constant_pool(), kRelocModeMask);
     v->VisitRelocInfo(&it);
   }
 
@@ -1060,11 +1060,11 @@ class Code::BodyDescriptor final : public BodyDescriptorBase {
   template <typename ObjectVisitor>
   static inline void IterateBody(Map map, HeapObject obj, int object_size,
                                  ObjectVisitor* v) {
-    // No strong pointers to iterate.
-    static_assert(static_cast<int>(HeapObject::kHeaderSize) ==
-                  static_cast<int>(Code::kPointerFieldsStrongEndOffset));
+    IteratePointers(obj, HeapObject::kHeaderSize,
+                    Code::kPointerFieldsStrongEndOffset, v);
 
-    v->VisitCodePointer(obj, obj.RawCodeField(kInstructionStreamOffset));
+    v->VisitCodePointer(Code::cast(obj),
+                        obj.RawCodeField(kInstructionStreamOffset));
   }
 
   static inline int SizeOf(Map map, HeapObject object) { return Code::kSize; }
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index 77279dc4b1..43ea4a7350 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -2036,7 +2036,8 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case INSTRUCTION_STREAM_TYPE: {
-      InstructionStream code = InstructionStream::cast(*this);
+      InstructionStream istream = InstructionStream::cast(*this);
+      Code code = istream.code(kAcquireLoad);
       os << "<InstructionStream " << CodeKindToString(code.kind());
       if (code.is_builtin()) {
         os << " " << Builtins::name(code.builtin_id());
diff --git a/src/objects/visitors.h b/src/objects/visitors.h
index 651ee080d1..ab16adf31d 100644
--- a/src/objects/visitors.h
+++ b/src/objects/visitors.h
@@ -136,7 +136,7 @@ class ObjectVisitor {
   // slot. The values may be modified on return. Not used when
   // V8_EXTERNAL_CODE_SPACE is not enabled (the InstructionStream pointer slots
   // are visited as a part of on-heap slot visitation - via VisitPointers()).
-  virtual void VisitCodePointer(HeapObject host, CodeObjectSlot slot) = 0;
+  virtual void VisitCodePointer(Code host, CodeObjectSlot slot) = 0;
 
   // Custom weak pointers must be ignored by the GC but not other
   // visitors. They're used for e.g., lists that are recreated after GC. The
@@ -164,36 +164,23 @@ class ObjectVisitor {
     VisitPointer(host, value);
   }
 
-  // To allow lazy clearing of inline caches the visitor has
-  // a rich interface for iterating over InstructionStream objects ...
+  virtual void VisitCodeTarget(RelocInfo* rinfo) = 0;
 
-  // Visits a code target in the instruction stream.
-  virtual void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) = 0;
+  virtual void VisitEmbeddedPointer(RelocInfo* rinfo) = 0;
 
-  // Visit pointer embedded into a code object.
-  virtual void VisitEmbeddedPointer(InstructionStream host,
-                                    RelocInfo* rinfo) = 0;
+  virtual void VisitExternalReference(RelocInfo* rinfo) {}
 
-  // Visits an external reference embedded into a code object.
-  virtual void VisitExternalReference(InstructionStream host,
-                                      RelocInfo* rinfo) {}
-
-  // Visits an external pointer.
   virtual void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                     ExternalPointerTag tag) {}
 
-  // Visits an (encoded) internal reference.
-  virtual void VisitInternalReference(InstructionStream host,
-                                      RelocInfo* rinfo) {}
+  virtual void VisitInternalReference(RelocInfo* rinfo) {}
 
-  // Visits an off-heap target or near builtin entry in the instruction stream.
   // TODO(ishell): rename to VisitBuiltinEntry.
-  virtual void VisitOffHeapTarget(InstructionStream host, RelocInfo* rinfo) {}
+  virtual void VisitOffHeapTarget(RelocInfo* rinfo) {}
 
   // Visits the relocation info using the given iterator.
   void VisitRelocInfo(RelocIterator* it);
 
-  // Visits the object's map pointer, decoding as necessary
   virtual void VisitMapPointer(HeapObject host) { UNREACHABLE(); }
 };
 
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index 82c35c5815..efc9bfdcab 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -188,7 +188,8 @@ class HeapEntryVerifier {
   // Objects that have been checked via a call to CheckStrongReference or
   // CheckWeakReference, or deliberately skipped via a call to
   // MarkReferenceCheckedWithoutChecking.
-  std::unordered_set<HeapObject, Object::Hasher> checked_objects_;
+  std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>
+      checked_objects_;
 
   // Objects transitively retained by the primary object. The objects in the set
   // at index i are retained by the primary object via a chain of i+1
@@ -1075,19 +1076,19 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
     VisitSlotImpl(code_cage_base(), slot);
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitCodeTarget(RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VisitHeapObjectImpl(target, -1);
   }
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
     HeapObject object = rinfo->target_object(cage_base());
-    if (host.IsWeakObject(object)) {
+    if (rinfo->code().IsWeakObject(object)) {
       generator_->SetWeakReference(parent_, next_index_++, object, {});
     } else {
       VisitHeapObjectImpl(object, -1);
@@ -1169,8 +1170,8 @@ void V8HeapExplorer::ExtractReferences(HeapEntry* entry, HeapObject obj) {
     ExtractAccessorInfoReferences(entry, AccessorInfo::cast(obj));
   } else if (obj.IsAccessorPair()) {
     ExtractAccessorPairReferences(entry, AccessorPair::cast(obj));
-  } else if (obj.IsInstructionStream()) {
-    ExtractCodeReferences(entry, InstructionStream::cast(obj));
+  } else if (obj.IsCode()) {
+    ExtractCodeReferences(entry, Code::cast(obj));
   } else if (obj.IsCell()) {
     ExtractCellReferences(entry, Cell::cast(obj));
   } else if (obj.IsFeedbackCell()) {
@@ -1580,29 +1581,29 @@ void V8HeapExplorer::TagBuiltinCodeObject(Code code, const char* name) {
   }
 }
 
-void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry,
-                                           InstructionStream code) {
+void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
+  if (!code.has_instruction_stream()) return;
+
   TagObject(code.relocation_info(), "(code relocation info)", HeapEntry::kCode);
   SetInternalReference(entry, "relocation_info", code.relocation_info(),
-                       InstructionStream::kRelocationInfoOffset);
+                       Code::kRelocationInfoOffset);
 
   if (code.kind() == CodeKind::BASELINE) {
     TagObject(code.bytecode_or_interpreter_data(), "(interpreter data)");
-    SetInternalReference(
-        entry, "interpreter_data", code.bytecode_or_interpreter_data(),
-        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
+    SetInternalReference(entry, "interpreter_data",
+                         code.bytecode_or_interpreter_data(),
+                         Code::kDeoptimizationDataOrInterpreterDataOffset);
     TagObject(code.bytecode_offset_table(), "(bytecode offset table)",
               HeapEntry::kCode);
     SetInternalReference(entry, "bytecode_offset_table",
                          code.bytecode_offset_table(),
-                         InstructionStream::kPositionTableOffset);
+                         Code::kPositionTableOffset);
   } else {
     DeoptimizationData deoptimization_data =
         DeoptimizationData::cast(code.deoptimization_data());
     TagObject(deoptimization_data, "(code deopt data)", HeapEntry::kCode);
-    SetInternalReference(
-        entry, "deoptimization_data", deoptimization_data,
-        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
+    SetInternalReference(entry, "deoptimization_data", deoptimization_data,
+                         Code::kDeoptimizationDataOrInterpreterDataOffset);
     if (deoptimization_data.length() > 0) {
       TagObject(deoptimization_data.TranslationByteArray(), "(code deopt data)",
                 HeapEntry::kCode);
@@ -1615,7 +1616,7 @@ void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry,
               HeapEntry::kCode);
     SetInternalReference(entry, "source_position_table",
                          code.source_position_table(),
-                         InstructionStream::kPositionTableOffset);
+                         Code::kPositionTableOffset);
   }
 }
 
@@ -2070,8 +2071,8 @@ class RootsReferencesExtractor : public RootVisitor {
                         FullObjectSlot istream_or_smi_zero_slot) final {
     Object istream_or_smi_zero = *istream_or_smi_zero_slot;
     if (istream_or_smi_zero != Smi::zero()) {
-      InstructionStream istream = InstructionStream::cast(istream_or_smi_zero);
-      istream.IterateDeoptimizationLiterals(this);
+      Code code = Code::cast(*code_slot);
+      code.IterateDeoptimizationLiterals(this);
       VisitRootPointer(Root::kStackRoots, nullptr, istream_or_smi_zero_slot);
     }
     VisitRootPointer(Root::kStackRoots, nullptr, code_slot);
diff --git a/src/profiler/heap-snapshot-generator.h b/src/profiler/heap-snapshot-generator.h
index d8f6628f1d..216833e2fd 100644
--- a/src/profiler/heap-snapshot-generator.h
+++ b/src/profiler/heap-snapshot-generator.h
@@ -433,7 +433,7 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   void ExtractAccessorInfoReferences(HeapEntry* entry,
                                      AccessorInfo accessor_info);
   void ExtractAccessorPairReferences(HeapEntry* entry, AccessorPair accessors);
-  void ExtractCodeReferences(HeapEntry* entry, InstructionStream code);
+  void ExtractCodeReferences(HeapEntry* entry, Code code);
   void ExtractCellReferences(HeapEntry* entry, Cell cell);
   void ExtractJSWeakRefReferences(HeapEntry* entry, JSWeakRef js_weak_ref);
   void ExtractWeakCellReferences(HeapEntry* entry, WeakCell weak_cell);
diff --git a/src/profiler/profiler-listener.cc b/src/profiler/profiler-listener.cc
index dd0631664a..688bdc1027 100644
--- a/src/profiler/profiler-listener.cc
+++ b/src/profiler/profiler-listener.cc
@@ -331,13 +331,12 @@ void ProfilerListener::CodeDisableOptEvent(Handle<AbstractCode> code,
   DispatchCodeEvent(evt_rec);
 }
 
-void ProfilerListener::CodeDeoptEvent(Handle<InstructionStream> code,
-                                      DeoptimizeKind kind, Address pc,
-                                      int fp_to_sp_delta) {
+void ProfilerListener::CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
+                                      Address pc, int fp_to_sp_delta) {
   CodeEventsContainer evt_rec(CodeEventRecord::Type::kCodeDeopt);
   CodeDeoptEventRecord* rec = &evt_rec.CodeDeoptEventRecord_;
   Deoptimizer::DeoptInfo info = Deoptimizer::GetDeoptInfo(*code, pc);
-  rec->instruction_start = code->instruction_start();
+  rec->instruction_start = code->InstructionStart();
   rec->deopt_reason = DeoptimizeReasonToString(info.deopt_reason);
   rec->deopt_id = info.deopt_id;
   rec->pc = pc;
@@ -386,7 +385,7 @@ const char* ProfilerListener::GetFunctionName(SharedFunctionInfo shared) {
   }
 }
 
-void ProfilerListener::AttachDeoptInlinedFrames(Handle<InstructionStream> code,
+void ProfilerListener::AttachDeoptInlinedFrames(Handle<Code> code,
                                                 CodeDeoptEventRecord* rec) {
   int deopt_id = rec->deopt_id;
   SourcePosition last_position = SourcePosition::Unknown();
@@ -416,7 +415,7 @@ void ProfilerListener::AttachDeoptInlinedFrames(Handle<InstructionStream> code,
       // scope limits their lifetime.
       HandleScope scope(isolate_);
       std::vector<SourcePositionInfo> stack =
-          last_position.InliningStack(isolate_, code->code(kAcquireLoad));
+          last_position.InliningStack(isolate_, *code);
       CpuProfileDeoptFrame* deopt_frames =
           new CpuProfileDeoptFrame[stack.size()];
 
diff --git a/src/profiler/profiler-listener.h b/src/profiler/profiler-listener.h
index 3517ab56b5..eec7b08bfa 100644
--- a/src/profiler/profiler-listener.h
+++ b/src/profiler/profiler-listener.h
@@ -63,9 +63,9 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
   void CodeMovingGCEvent() override {}
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override;
-  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                      Address pc, int fp_to_sp_delta) override;
-  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                      int fp_to_sp_delta) override;
+  void CodeDependencyChangeEvent(Handle<Code> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override;
@@ -94,8 +94,7 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
  private:
   const char* GetFunctionName(SharedFunctionInfo);
 
-  void AttachDeoptInlinedFrames(Handle<InstructionStream> code,
-                                CodeDeoptEventRecord* rec);
+  void AttachDeoptInlinedFrames(Handle<Code> code, CodeDeoptEventRecord* rec);
   Name InferScriptName(Name name, SharedFunctionInfo info);
   V8_INLINE void DispatchCodeEvent(const CodeEventsContainer& evt_rec) {
     observer_->CodeEventHandler(evt_rec);
diff --git a/src/regexp/arm/regexp-macro-assembler-arm.cc b/src/regexp/arm/regexp-macro-assembler-arm.cc
index 46ae0aaa7f..89d3ea6252 100644
--- a/src/regexp/arm/regexp-macro-assembler-arm.cc
+++ b/src/regexp/arm/regexp-macro-assembler-arm.cc
@@ -1010,10 +1010,9 @@ Handle<HeapObject> RegExpMacroAssemblerARM::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
-  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(istream);
+  return Handle<HeapObject>::cast(code);
 }
 
 
diff --git a/src/regexp/arm64/regexp-macro-assembler-arm64.cc b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
index d453922f6b..fe1b0f6e04 100644
--- a/src/regexp/arm64/regexp-macro-assembler-arm64.cc
+++ b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
@@ -1198,10 +1198,9 @@ Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
-  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(istream);
+  return Handle<HeapObject>::cast(code);
 }
 
 
diff --git a/src/regexp/ia32/regexp-macro-assembler-ia32.cc b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
index 0d448569fa..33f79d3050 100644
--- a/src/regexp/ia32/regexp-macro-assembler-ia32.cc
+++ b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
@@ -1075,10 +1075,9 @@ Handle<HeapObject> RegExpMacroAssemblerIA32::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
-  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(istream);
+  return Handle<HeapObject>::cast(code);
 }
 
 void RegExpMacroAssemblerIA32::GoTo(Label* to) { BranchOrBacktrack(to); }
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index 2fcb0a425e..912175258a 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -288,7 +288,7 @@ int NativeRegExpMacroAssembler::CheckStackGuardState(
   DisallowGarbageCollection no_gc;
   Address old_pc = PointerAuthentication::AuthenticatePC(return_address, 0);
   DCHECK_LE(re_code.instruction_start(), old_pc);
-  DCHECK_LE(old_pc, re_code.instruction_end());
+  DCHECK_LE(old_pc, re_code.code(kAcquireLoad).InstructionEnd());
 
   StackLimitCheck check(isolate);
   bool js_has_overflowed = check.JsHasOverflowed();
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index 1b7b4b23f4..e6e7d140a7 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -579,8 +579,8 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
   Handle<FixedArray> data =
       Handle<FixedArray>(FixedArray::cast(re->data()), isolate);
   if (compile_data.compilation_target == RegExpCompilationTarget::kNative) {
-    InstructionStream code = InstructionStream::cast(*compile_data.code);
-    data->set(JSRegExp::code_index(is_one_byte), ToCode(code));
+    Code code = Code::cast(*compile_data.code);
+    data->set(JSRegExp::code_index(is_one_byte), code);
 
     // Reset bytecode to uninitialized. In case we use tier-up we know that
     // tier-up has happened this way.
@@ -1023,10 +1023,9 @@ bool RegExpImpl::Compile(Isolate* isolate, Zone* zone, RegExpCompileData* data,
         data->compilation_target == RegExpCompilationTarget::kNative) {
       CodeTracer::Scope trace_scope(isolate->GetCodeTracer());
       OFStream os(trace_scope.file());
-      Code code =
-          Handle<InstructionStream>::cast(result.code)->code(kAcquireLoad);
+      Handle<Code> code = Handle<Code>::cast(result.code);
       std::unique_ptr<char[]> pattern_cstring = pattern->ToCString();
-      code.Disassemble(pattern_cstring.get(), os, isolate);
+      code->Disassemble(pattern_cstring.get(), os, isolate);
     }
 #endif
     if (v8_flags.print_regexp_bytecode &&
diff --git a/src/regexp/x64/regexp-macro-assembler-x64.cc b/src/regexp/x64/regexp-macro-assembler-x64.cc
index 3377858ee2..90ff2696a5 100644
--- a/src/regexp/x64/regexp-macro-assembler-x64.cc
+++ b/src/regexp/x64/regexp-macro-assembler-x64.cc
@@ -1123,10 +1123,9 @@ Handle<HeapObject> RegExpMacroAssemblerX64::GetCode(Handle<String> source) {
   Handle<Code> code = Factory::CodeBuilder(isolate, code_desc, CodeKind::REGEXP)
                           .set_self_reference(masm_.CodeObject())
                           .Build();
-  Handle<InstructionStream> istream(code->instruction_stream(), isolate);
   PROFILE(isolate,
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(istream);
+  return Handle<HeapObject>::cast(code);
 }
 
 void RegExpMacroAssemblerX64::GoTo(Label* to) { BranchOrBacktrack(to); }
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 1503f11005..591f5c8d9d 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -369,7 +369,7 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   Handle<JSFunction> function = deoptimizer->function();
   // For OSR the optimized code isn't installed on the function, so get the
   // code object from deoptimizer.
-  Handle<InstructionStream> optimized_code = deoptimizer->compiled_code();
+  Handle<Code> optimized_code = deoptimizer->compiled_code();
   const DeoptimizeKind deopt_kind = deoptimizer->deopt_kind();
   const DeoptimizeReason deopt_reason =
       deoptimizer->GetDeoptInfo().deopt_reason;
@@ -415,11 +415,11 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   // the loop should pay for the deoptimization costs.
   const BytecodeOffset osr_offset = optimized_code->osr_offset();
   if (osr_offset.IsNone()) {
-    Deoptimizer::DeoptimizeFunction(*function, ToCode(*optimized_code));
+    Deoptimizer::DeoptimizeFunction(*function, *optimized_code);
     DeoptAllOsrLoopsContainingDeoptExit(isolate, *function, deopt_exit_offset);
   } else if (DeoptExitIsInsideOsrLoop(isolate, *function, deopt_exit_offset,
                                       osr_offset)) {
-    Deoptimizer::DeoptimizeFunction(*function, ToCode(*optimized_code));
+    Deoptimizer::DeoptimizeFunction(*function, *optimized_code);
   }
 
   return ReadOnlyRoots(isolate).undefined_value();
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index ce84ce10c7..819e22ef7d 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -1667,9 +1667,9 @@ RUNTIME_FUNCTION(Runtime_EnableCodeLoggingForTesting) {
     void CodeMovingGCEvent() final {}
     void CodeDisableOptEvent(Handle<AbstractCode> code,
                              Handle<SharedFunctionInfo> shared) final {}
-    void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
-                        Address pc, int fp_to_sp_delta) final {}
-    void CodeDependencyChangeEvent(Handle<InstructionStream> code,
+    void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
+                        int fp_to_sp_delta) final {}
+    void CodeDependencyChangeEvent(Handle<Code> code,
                                    Handle<SharedFunctionInfo> shared,
                                    const char* reason) final {}
     void WeakCodeClearEvent() final {}
diff --git a/src/snapshot/code-serializer.cc b/src/snapshot/code-serializer.cc
index 9765bc8026..c83e8e4581 100644
--- a/src/snapshot/code-serializer.cc
+++ b/src/snapshot/code-serializer.cc
@@ -276,7 +276,7 @@ void CreateInterpreterDataForDeserializedCode(Isolate* isolate,
     interpreter_data->set_bytecode_array(info->GetBytecodeArray(isolate));
     interpreter_data->set_interpreter_trampoline(*code);
     if (info->HasBaselineCode()) {
-      FromCode(info->baseline_code(kAcquireLoad))
+      info->baseline_code(kAcquireLoad)
           .set_bytecode_or_interpreter_data(*interpreter_data);
     } else {
       info->set_interpreter_data(*interpreter_data);
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index a3f0f86f5d..91037ae6e8 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -730,11 +730,11 @@ class DeserializerRelocInfoVisitor {
     DCHECK_EQ(current_object_, objects_->size());
   }
 
-  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo);
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo);
-  void VisitExternalReference(InstructionStream host, RelocInfo* rinfo);
-  void VisitInternalReference(InstructionStream host, RelocInfo* rinfo);
-  void VisitOffHeapTarget(InstructionStream host, RelocInfo* rinfo);
+  void VisitCodeTarget(RelocInfo* rinfo);
+  void VisitEmbeddedPointer(RelocInfo* rinfo);
+  void VisitExternalReference(RelocInfo* rinfo);
+  void VisitInternalReference(RelocInfo* rinfo);
+  void VisitOffHeapTarget(RelocInfo* rinfo);
 
  private:
   Isolate* isolate() { return deserializer_->isolate(); }
@@ -745,22 +745,19 @@ class DeserializerRelocInfoVisitor {
   int current_object_;
 };
 
-void DeserializerRelocInfoVisitor::VisitCodeTarget(InstructionStream host,
-                                                   RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitCodeTarget(RelocInfo* rinfo) {
   HeapObject object = *objects_->at(current_object_++);
   rinfo->set_target_address(
       InstructionStream::cast(object).instruction_start());
 }
 
-void DeserializerRelocInfoVisitor::VisitEmbeddedPointer(InstructionStream host,
-                                                        RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
   HeapObject object = *objects_->at(current_object_++);
   // Embedded object reference must be a strong one.
   rinfo->set_target_object(isolate()->heap(), object);
 }
 
-void DeserializerRelocInfoVisitor::VisitExternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitExternalReference(RelocInfo* rinfo) {
   byte data = source().Get();
   CHECK_EQ(data, Deserializer<Isolate>::kExternalReference);
 
@@ -769,14 +766,13 @@ void DeserializerRelocInfoVisitor::VisitExternalReference(
   if (rinfo->IsCodedSpecially()) {
     Address location_of_branch_data = rinfo->pc();
     Assembler::deserialization_set_special_target_at(location_of_branch_data,
-                                                     host, address);
+                                                     rinfo->code(), address);
   } else {
     WriteUnalignedValue(rinfo->target_address_address(), address);
   }
 }
 
-void DeserializerRelocInfoVisitor::VisitInternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitInternalReference(RelocInfo* rinfo) {
   byte data = source().Get();
   CHECK_EQ(data, Deserializer<Isolate>::kInternalReference);
 
@@ -784,14 +780,13 @@ void DeserializerRelocInfoVisitor::VisitInternalReference(
   int target_offset = source().GetInt();
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
   DCHECK_LT(static_cast<unsigned>(target_offset),
-            static_cast<unsigned>(host.instruction_size()));
-  Address target = host.entry() + target_offset;
+            static_cast<unsigned>(rinfo->code().instruction_size()));
+  Address target = rinfo->code().InstructionStart() + target_offset;
   Assembler::deserialization_set_target_internal_reference_at(
       rinfo->pc(), target, rinfo->rmode());
 }
 
-void DeserializerRelocInfoVisitor::VisitOffHeapTarget(InstructionStream host,
-                                                      RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitOffHeapTarget(RelocInfo* rinfo) {
   // Currently we don't serialize code that contains near builtin entries.
   DCHECK_NE(rinfo->rmode(), RelocInfo::NEAR_BUILTIN_ENTRY);
 
@@ -809,7 +804,7 @@ void DeserializerRelocInfoVisitor::VisitOffHeapTarget(InstructionStream host,
   if (RelocInfo::OffHeapTargetIsCodedSpecially()) {
     Address location_of_branch_data = rinfo->pc();
     Assembler::deserialization_set_special_target_at(location_of_branch_data,
-                                                     host, address);
+                                                     rinfo->code(), address);
   } else {
     WriteUnalignedValue(rinfo->target_address_address(), address);
   }
@@ -1175,7 +1170,8 @@ int Deserializer<IsolateT>::ReadVariableRawData(byte data,
   return size_in_tagged;
 }
 
-// Deserialize raw code directly into the body of the code object.
+// Custom deserialization for a Code object and its associated InstructionStream
+// object.
 template <typename IsolateT>
 template <typename SlotAccessor>
 int Deserializer<IsolateT>::ReadCodeBody(byte data,
@@ -1191,15 +1187,16 @@ int Deserializer<IsolateT>::ReadCodeBody(byte data,
 
   {
     DisallowGarbageCollection no_gc;
-    InstructionStream code = InstructionStream::cast(*slot_accessor.object());
+    InstructionStream istream =
+        InstructionStream::cast(*slot_accessor.object());
 
-    // First deserialize the code itself.
-    source_.CopyRaw(
-        reinterpret_cast<void*>(code.address() + InstructionStream::kDataStart),
-        size_in_bytes);
+    // First deserialize the untagged region of the InstructionStream object.
+    source_.CopyRaw(reinterpret_cast<void*>(istream.address() +
+                                            InstructionStream::kDataStart),
+                    size_in_bytes);
   }
 
-  // Then deserialize the code header
+  // Then deserialize the InstructionStream header
   ReadData(slot_accessor.object(), HeapObject::kHeaderSize / kTaggedSize,
            InstructionStream::kDataStart / kTaggedSize);
 
@@ -1220,12 +1217,15 @@ int Deserializer<IsolateT>::ReadCodeBody(byte data,
   {
     DisallowGarbageCollection no_gc;
 
-    InstructionStream code = InstructionStream::cast(*slot_accessor.object());
+    InstructionStream istream =
+        InstructionStream::cast(*slot_accessor.object());
     if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      code.set_main_cage_base(isolate()->cage_base(), kRelaxedStore);
+      istream.set_main_cage_base(isolate()->cage_base(), kRelaxedStore);
     }
+    Code code = istream.code(kAcquireLoad);
     DeserializerRelocInfoVisitor visitor(this, &preserialized_objects);
-    for (RelocIterator it(code,
+    for (RelocIterator it(code, istream, code.relocation_info(),
+                          code.constant_pool(),
                           InstructionStream::BodyDescriptor::kRelocModeMask);
          !it.done(); it.next()) {
       it.rinfo()->Visit(&visitor);
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index 201c30fa77..d7193d02d2 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -187,7 +187,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    InstructionStream code = FromCode(isolate->builtins()->code(builtin));
+    Code code = isolate->builtins()->code(builtin);
     RelocIterator on_heap_it(code, kRelocMask);
     RelocIterator off_heap_it(blob, code, kRelocMask);
 
@@ -204,13 +204,12 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
 
       RelocInfo* rinfo = on_heap_it.rinfo();
       DCHECK_EQ(rinfo->rmode(), off_heap_it.rinfo()->rmode());
-      InstructionStream target =
-          InstructionStream::FromTargetAddress(rinfo->target_address());
-      CHECK(Builtins::IsIsolateIndependentBuiltin(target.code(kAcquireLoad)));
+      Code target_code = Code::FromTargetAddress(rinfo->target_address());
+      CHECK(Builtins::IsIsolateIndependentBuiltin(target_code));
 
       // Do not emit write-barrier for off-heap writes.
       off_heap_it.rinfo()->set_off_heap_target_address(
-          blob->InstructionStartOfBuiltin(target.builtin_id()));
+          blob->InstructionStartOfBuiltin(target_code.builtin_id()));
 
       on_heap_it.next();
       off_heap_it.next();
@@ -227,15 +226,14 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
 }
 
 void EnsureRelocatable(Code code) {
-  InstructionStream instruction_stream = FromCode(code);
-  if (instruction_stream.relocation_size() == 0) return;
+  if (code.relocation_size() == 0) return;
 
   // On some architectures (arm) the builtin might have a non-empty reloc
   // info containing a CONST_POOL entry. These entries don't have to be
   // updated when InstructionStream object is relocated, so it's safe to drop
   // the reloc info alltogether. If it wasn't the case then we'd have to store
   // it in the metadata.
-  for (RelocIterator it(instruction_stream); !it.done(); it.next()) {
+  for (RelocIterator it(code); !it.done(); it.next()) {
     CHECK_EQ(it.rinfo()->rmode(), RelocInfo::CONST_POOL);
   }
 }
@@ -255,7 +253,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    InstructionStream code = FromCode(builtins->code(builtin));
+    Code code = builtins->code(builtin);
 
     // Sanity-check that the given builtin is isolate-independent.
     if (!code.IsIsolateIndependent(isolate)) {
@@ -334,7 +332,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    InstructionStream code = FromCode(builtins->code(builtin));
+    Code code = builtins->code(builtin);
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].metadata_offset;
     uint8_t* dst = raw_metadata_start + offset;
@@ -352,13 +350,13 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    InstructionStream code = FromCode(builtins->code(builtin));
+    Code code = builtins->code(builtin);
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].instruction_offset;
     uint8_t* dst = raw_code_start + offset;
     DCHECK_LE(RawCodeOffset() + offset + code.instruction_size(),
               blob_code_size);
-    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.instruction_start()),
+    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.InstructionStart()),
                 code.instruction_size());
   }
 
@@ -388,7 +386,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   if (DEBUG_BOOL) {
     for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
          ++builtin) {
-      InstructionStream code = FromCode(builtins->code(builtin));
+      Code code = builtins->code(builtin);
 
       CHECK_EQ(d.InstructionSizeOfBuiltin(builtin), code.instruction_size());
       CHECK_EQ(d.MetadataSizeOfBuiltin(builtin), code.metadata_size());
diff --git a/src/snapshot/embedded/embedded-file-writer.cc b/src/snapshot/embedded/embedded-file-writer.cc
index ba982259b6..9ce32f64e8 100644
--- a/src/snapshot/embedded/embedded-file-writer.cc
+++ b/src/snapshot/embedded/embedded-file-writer.cc
@@ -269,7 +269,7 @@ void EmbeddedFileWriter::PrepareBuiltinSourcePositionMap(Builtins* builtins) {
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     // Retrieve the SourcePositionTable and copy it.
-    InstructionStream code = FromCode(builtins->code(builtin));
+    Code code = builtins->code(builtin);
     ByteArray source_position_table = code.source_position_table();
     std::vector<unsigned char> data(source_position_table.GetDataStartAddress(),
                                     source_position_table.GetDataEndAddress());
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index a2f3547147..bf4b850330 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -260,8 +260,8 @@ bool Serializer::SerializePendingObject(HeapObject obj) {
 }
 
 bool Serializer::ObjectIsBytecodeHandler(HeapObject obj) const {
-  if (!obj.IsInstructionStream()) return false;
-  return (InstructionStream::cast(obj).kind() == CodeKind::BYTECODE_HANDLER);
+  if (!obj.IsCode()) return false;
+  return (Code::cast(obj).kind() == CodeKind::BYTECODE_HANDLER);
 }
 
 void Serializer::PutRoot(RootIndex root) {
@@ -870,8 +870,8 @@ void Serializer::ObjectSerializer::SerializeContent(Map map, int size) {
   HeapObject raw = *object_;
   UnlinkWeakNextScope unlink_weak_next(isolate()->heap(), raw);
   if (raw.IsInstructionStream()) {
-    // For code objects, perform a custom serialization.
-    SerializeCode(map, size);
+    // For InstructionStream objects, perform a custom serialization.
+    SerializeInstructionStream(map, size);
   } else {
     // For other objects, iterate references first.
     raw.IterateBody(map, size, this);
@@ -954,7 +954,7 @@ void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
   }
 }
 
-void Serializer::ObjectSerializer::VisitCodePointer(HeapObject host,
+void Serializer::ObjectSerializer::VisitCodePointer(Code host,
                                                     CodeObjectSlot slot) {
   // A version of VisitPointers() customized for CodeObjectSlot.
   HandleScope scope(isolate());
@@ -1046,12 +1046,12 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
   explicit RelocInfoObjectPreSerializer(Serializer* serializer)
       : serializer_(serializer) {}
 
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* target) {
+  void VisitEmbeddedPointer(RelocInfo* target) {
     HeapObject object = target->target_object(isolate());
     serializer_->SerializeObject(handle(object, isolate()));
     num_serialized_objects_++;
   }
-  void VisitCodeTarget(InstructionStream host, RelocInfo* target) {
+  void VisitCodeTarget(RelocInfo* target) {
 #ifdef V8_TARGET_ARCH_ARM
     DCHECK(!RelocInfo::IsRelativeCodeTarget(target->rmode()));
 #endif
@@ -1061,9 +1061,9 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
     num_serialized_objects_++;
   }
 
-  void VisitExternalReference(InstructionStream host, RelocInfo* rinfo) {}
-  void VisitInternalReference(InstructionStream host, RelocInfo* rinfo) {}
-  void VisitOffHeapTarget(InstructionStream host, RelocInfo* target) {}
+  void VisitExternalReference(RelocInfo* rinfo) {}
+  void VisitInternalReference(RelocInfo* rinfo) {}
+  void VisitOffHeapTarget(RelocInfo* target) {}
 
   int num_serialized_objects() const { return num_serialized_objects_; }
 
@@ -1074,8 +1074,7 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
   int num_serialized_objects_ = 0;
 };
 
-void Serializer::ObjectSerializer::VisitEmbeddedPointer(InstructionStream host,
-                                                        RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitEmbeddedPointer(RelocInfo* rinfo) {
   // Target object should be pre-serialized by RelocInfoObjectPreSerializer, so
   // just track the pointer's existence as kTaggedSize in
   // bytes_processed_so_far_.
@@ -1084,8 +1083,7 @@ void Serializer::ObjectSerializer::VisitEmbeddedPointer(InstructionStream host,
   bytes_processed_so_far_ += kTaggedSize;
 }
 
-void Serializer::ObjectSerializer::VisitExternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitExternalReference(RelocInfo* rinfo) {
   Address target = rinfo->target_external_reference();
   DCHECK_NE(target,
             kNullAddress);  // InstructionStream does not reference null.
@@ -1096,14 +1094,12 @@ void Serializer::ObjectSerializer::VisitExternalReference(
                           kExternalPointerNullTag);
 }
 
-void Serializer::ObjectSerializer::VisitInternalReference(
-    InstructionStream host, RelocInfo* rinfo) {
-  Handle<InstructionStream> istream = Handle<InstructionStream>::cast(object_);
-  Address entry = istream->entry();
+void Serializer::ObjectSerializer::VisitInternalReference(RelocInfo* rinfo) {
+  Address entry = rinfo->code().InstructionStart();
   DCHECK_GE(rinfo->target_internal_reference(), entry);
   uintptr_t target_offset = rinfo->target_internal_reference() - entry;
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-  DCHECK_LT(target_offset, istream->instruction_size());
+  DCHECK_LT(target_offset, rinfo->code().instruction_size());
   sink_->Put(kInternalReference, "InternalRef");
   sink_->PutInt(target_offset, "internal ref value");
 }
@@ -1147,8 +1143,7 @@ void Serializer::ObjectSerializer::VisitExternalPointer(
   }
 }
 
-void Serializer::ObjectSerializer::VisitOffHeapTarget(InstructionStream host,
-                                                      RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitOffHeapTarget(RelocInfo* rinfo) {
   static_assert(EmbeddedData::kTableSize == Builtins::kBuiltinCount);
 
   // Currently we don't serialize code that contains near builtin entries.
@@ -1165,8 +1160,7 @@ void Serializer::ObjectSerializer::VisitOffHeapTarget(InstructionStream host,
   sink_->PutInt(static_cast<int>(builtin), "builtin index");
 }
 
-void Serializer::ObjectSerializer::VisitCodeTarget(InstructionStream host,
-                                                   RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitCodeTarget(RelocInfo* rinfo) {
   // Target object should be pre-serialized by RelocInfoObjectPreSerializer, so
   // just track the pointer's existence as kTaggedSize in
   // bytes_processed_so_far_.
@@ -1267,7 +1261,8 @@ void Serializer::ObjectSerializer::OutputRawData(Address up_to) {
   }
 }
 
-void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
+void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
+                                                              int size) {
   static const int kWipeOutModeMask =
       RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
       RelocInfo::ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT) |
@@ -1278,33 +1273,38 @@ void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
       RelocInfo::ModeMask(RelocInfo::OFF_HEAP_TARGET);
 
   DCHECK_EQ(HeapObject::kHeaderSize, bytes_processed_so_far_);
-  Handle<InstructionStream> on_heap_code =
+  Handle<InstructionStream> on_heap_istream =
       Handle<InstructionStream>::cast(object_);
+  Handle<Code> code = handle(on_heap_istream->code(kAcquireLoad), isolate_);
 
   // With enabled pointer compression normal accessors no longer work for
   // off-heap objects, so we have to get the relocation info data via the
-  // on-heap code object.
-  ByteArray relocation_info = on_heap_code->unchecked_relocation_info();
-
-  // To make snapshots reproducible, we make a copy of the code object
-  // and wipe all pointers in the copy, which we then serialize.
-  InstructionStream off_heap_code = serializer_->CopyCode(*on_heap_code);
-  for (RelocIterator it(off_heap_code, relocation_info, kWipeOutModeMask);
+  // on-heap InstructionStream object.
+  // TODO(v8:13784): we can clean this up since we moved all data fields from
+  // InstructionStream to Code
+  ByteArray relocation_info = code->unchecked_relocation_info();
+
+  // To make snapshots reproducible, we make a copy of the InstructionStream
+  // object and wipe all pointers in the copy, which we then serialize.
+  InstructionStream off_heap_istream = serializer_->CopyCode(*on_heap_istream);
+  for (RelocIterator it(*code, off_heap_istream, relocation_info,
+                        code->constant_pool(), kWipeOutModeMask);
        !it.done(); it.next()) {
     RelocInfo* rinfo = it.rinfo();
     rinfo->WipeOut();
   }
   // We need to wipe out the header fields *after* wiping out the
   // relocations, because some of these fields are needed for the latter.
-  off_heap_code.WipeOutHeader();
+  off_heap_istream.WipeOutHeader();
 
   // Initially skip serializing the code header. We'll serialize it after the
   // InstructionStream body, so that the various fields the InstructionStream
   // needs for iteration are already valid.
+  // TODO(v8:13784): rename to kInstructionStreamBody
   sink_->Put(kCodeBody, "kCodeBody");
 
   // Now serialize the wiped off-heap InstructionStream, as length + data.
-  Address start = off_heap_code.address() + InstructionStream::kDataStart;
+  Address start = off_heap_istream.address() + InstructionStream::kDataStart;
   int bytes_to_output = size - InstructionStream::kDataStart;
   DCHECK(IsAligned(bytes_to_output, kTaggedSize));
   int tagged_to_output = bytes_to_output / kTaggedSize;
@@ -1323,12 +1323,13 @@ void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
   // InstructionStream::BodyDescriptor here as we don't yet want to walk the
   // RelocInfos.
   DCHECK_EQ(HeapObject::kHeaderSize, bytes_processed_so_far_);
-  VisitPointers(*on_heap_code, on_heap_code->RawField(HeapObject::kHeaderSize),
-                on_heap_code->RawField(InstructionStream::kDataStart));
+  VisitPointers(*on_heap_istream,
+                on_heap_istream->RawField(HeapObject::kHeaderSize),
+                on_heap_istream->RawField(InstructionStream::kDataStart));
   DCHECK_EQ(bytes_processed_so_far_, InstructionStream::kDataStart);
 
   // Now serialize RelocInfos. We can't allocate during a RelocInfo walk during
-  // deserualization, so we have two passes for RelocInfo serialization:
+  // deserialization, so we have two passes for RelocInfo serialization:
   //   1. A pre-serializer which serializes all allocatable objects in the
   //      RelocInfo, followed by a kSynchronize bytecode, and
   //   2. A walk the RelocInfo with this serializer, serializing any objects
@@ -1339,7 +1340,8 @@ void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
   // TODO(leszeks): We only really need to pre-serialize objects which need
   // serialization, i.e. no backrefs or roots.
   RelocInfoObjectPreSerializer pre_serializer(serializer_);
-  for (RelocIterator it(*on_heap_code, relocation_info,
+  for (RelocIterator it(*code, *on_heap_istream, relocation_info,
+                        code->constant_pool(),
                         InstructionStream::BodyDescriptor::kRelocModeMask);
        !it.done(); it.next()) {
     it.rinfo()->Visit(&pre_serializer);
@@ -1350,7 +1352,8 @@ void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
   // Finally serialize all RelocInfo objects in the on-heap InstructionStream,
   // knowing that we will not do a recursive serialization.
   // TODO(leszeks): Add a scope that DCHECKs this.
-  for (RelocIterator it(*on_heap_code, relocation_info,
+  for (RelocIterator it(*code, *on_heap_istream, relocation_info,
+                        code->constant_pool(),
                         InstructionStream::BodyDescriptor::kRelocModeMask);
        !it.done(); it.next()) {
     it.rinfo()->Visit(this);
diff --git a/src/snapshot/serializer.h b/src/snapshot/serializer.h
index 733567ccad..86a8cec8bd 100644
--- a/src/snapshot/serializer.h
+++ b/src/snapshot/serializer.h
@@ -462,14 +462,12 @@ class Serializer::ObjectSerializer : public ObjectVisitor {
                      ObjectSlot end) override;
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
-  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* target) override;
-  void VisitExternalReference(InstructionStream host,
-                              RelocInfo* rinfo) override;
-  void VisitInternalReference(InstructionStream host,
-                              RelocInfo* rinfo) override;
-  void VisitCodeTarget(InstructionStream host, RelocInfo* target) override;
-  void VisitOffHeapTarget(InstructionStream host, RelocInfo* target) override;
+  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
+  void VisitEmbeddedPointer(RelocInfo* target) override;
+  void VisitExternalReference(RelocInfo* rinfo) override;
+  void VisitInternalReference(RelocInfo* rinfo) override;
+  void VisitCodeTarget(RelocInfo* target) override;
+  void VisitOffHeapTarget(RelocInfo* target) override;
 
   void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                             ExternalPointerTag tag) override;
@@ -487,7 +485,7 @@ class Serializer::ObjectSerializer : public ObjectVisitor {
   void OutputExternalReference(Address target, int target_size, bool sandboxify,
                                ExternalPointerTag tag);
   void OutputRawData(Address up_to);
-  void SerializeCode(Map map, int size);
+  void SerializeInstructionStream(Map map, int size);
   uint32_t SerializeBackingStore(void* backing_store, int32_t byte_length,
                                  Maybe<int32_t> max_byte_length);
   void SerializeJSTypedArray();
diff --git a/src/snapshot/startup-serializer.cc b/src/snapshot/startup-serializer.cc
index 981c910ea4..f7af6a4ee6 100644
--- a/src/snapshot/startup-serializer.cc
+++ b/src/snapshot/startup-serializer.cc
@@ -85,15 +85,11 @@ StartupSerializer::~StartupSerializer() {
 #ifdef DEBUG
 namespace {
 
-bool IsUnexpectedCodeObject(Isolate* isolate, HeapObject obj) {
+bool IsUnexpectedInstructionStreamObject(Isolate* isolate, HeapObject obj) {
   if (!obj.IsInstructionStream()) return false;
-
-  InstructionStream code = InstructionStream::cast(obj);
-  if (code.kind() == CodeKind::REGEXP) return false;
-  if (!code.is_builtin()) return true;
-
-  // An on-heap builtin.
-  return true;
+  // TODO(jgruber): Is REGEXP code still fully supported?
+  return InstructionStream::cast(obj).code(kAcquireLoad).kind() !=
+         CodeKind::REGEXP;
 }
 
 }  // namespace
@@ -113,7 +109,7 @@ void StartupSerializer::SerializeObjectImpl(Handle<HeapObject> obj) {
   {
     DisallowGarbageCollection no_gc;
     HeapObject raw = *obj;
-    DCHECK(!IsUnexpectedCodeObject(isolate(), raw));
+    DCHECK(!IsUnexpectedInstructionStreamObject(isolate(), raw));
     if (SerializeHotObject(raw)) return;
     if (IsRootAndHasBeenSerialized(raw) && SerializeRoot(raw)) return;
   }
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index 8737c1c68e..3c4820ba58 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1455,11 +1455,8 @@ namespace {
 
 void RecordStats(Code code, Counters* counters) {
   if (!code.has_instruction_stream()) return;
-  InstructionStream instruction_stream = FromCode(code);
-  counters->wasm_generated_code_size()->Increment(
-      instruction_stream.body_size());
-  counters->wasm_reloc_size()->Increment(
-      instruction_stream.relocation_info().length());
+  counters->wasm_generated_code_size()->Increment(code.body_size());
+  counters->wasm_reloc_size()->Increment(code.relocation_info().length());
 }
 
 enum CompilationExecutionResult : int8_t { kNoMoreUnits, kYield };
diff --git a/src/wasm/wasm-code-manager.cc b/src/wasm/wasm-code-manager.cc
index bdb55457c0..69847eb05f 100644
--- a/src/wasm/wasm-code-manager.cc
+++ b/src/wasm/wasm-code-manager.cc
@@ -907,7 +907,7 @@ CompilationEnv NativeModule::CreateCompilationEnv() const {
           compilation_state()->dynamic_tiering()};
 }
 
-WasmCode* NativeModule::AddCodeForTesting(Handle<InstructionStream> code) {
+WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
   CodeSpaceWriteScope code_space_write_scope(this);
   const size_t relocation_size = code->relocation_size();
   base::OwnedVector<byte> reloc_info;
@@ -916,7 +916,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<InstructionStream> code) {
         base::Vector<byte>{code->relocation_start(), relocation_size});
   }
   Handle<ByteArray> source_pos_table(code->source_position_table(),
-                                     code->GetIsolate());
+                                     code->instruction_stream().GetIsolate());
   base::OwnedVector<byte> source_pos =
       base::OwnedVector<byte>::NewForOverwrite(source_pos_table->length());
   if (source_pos_table->length() > 0) {
@@ -950,7 +950,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<InstructionStream> code) {
 
   // Apply the relocation delta by iterating over the RelocInfo.
   intptr_t delta = reinterpret_cast<Address>(dst_code_bytes.begin()) -
-                   code->instruction_start();
+                   code->InstructionStart();
   int mode_mask =
       RelocInfo::kApplyMask | RelocInfo::ModeMask(RelocInfo::WASM_STUB_CALL);
   auto jump_tables_ref =
diff --git a/src/wasm/wasm-code-manager.h b/src/wasm/wasm-code-manager.h
index 2b0d32af8b..9760c22dea 100644
--- a/src/wasm/wasm-code-manager.h
+++ b/src/wasm/wasm-code-manager.h
@@ -656,7 +656,7 @@ class V8_EXPORT_PRIVATE NativeModule final {
       ExecutionTier tier);
 
   // Adds anonymous code for testing purposes.
-  WasmCode* AddCodeForTesting(Handle<InstructionStream> code);
+  WasmCode* AddCodeForTesting(Handle<Code> code);
 
   // Allocates and initializes the {lazy_compile_table_} and initializes the
   // first jump table with jumps to the {lazy_compile_table_}.
diff --git a/test/cctest/compiler/codegen-tester.h b/test/cctest/compiler/codegen-tester.h
index 44eabde611..219650def3 100644
--- a/test/cctest/compiler/codegen-tester.h
+++ b/test/cctest/compiler/codegen-tester.h
@@ -72,10 +72,6 @@ class RawMachineAssemblerTester : public HandleAndZoneScope,
 
   void GenerateCode() { Generate(); }
 
-  Handle<InstructionStream> GetInstructionStream() {
-    return handle(GetCode()->instruction_stream(), main_isolate());
-  }
-
   Handle<Code> GetCode() {
     Generate();
     return code_.ToHandleChecked();
diff --git a/test/cctest/compiler/function-tester.cc b/test/cctest/compiler/function-tester.cc
index 614c799556..f3bfd5bb11 100644
--- a/test/cctest/compiler/function-tester.cc
+++ b/test/cctest/compiler/function-tester.cc
@@ -38,18 +38,6 @@ FunctionTester::FunctionTester(Graph* graph, int param_count)
   CompileGraph(graph);
 }
 
-FunctionTester::FunctionTester(Handle<InstructionStream> code, int param_count)
-    : isolate(main_isolate()),
-      canonical(isolate),
-      function((v8_flags.allow_natives_syntax = true,
-                NewFunction(BuildFunction(param_count).c_str()))),
-      flags_(0) {
-  CHECK(!code.is_null());
-  CHECK(code->IsInstructionStream());
-  Compile(function);
-  function->set_code(ToCode(*code), kReleaseStore);
-}
-
 FunctionTester::FunctionTester(Handle<Code> code, int param_count)
     : isolate(main_isolate()),
       canonical(isolate),
@@ -62,9 +50,6 @@ FunctionTester::FunctionTester(Handle<Code> code, int param_count)
   function->set_code(*code, kReleaseStore);
 }
 
-FunctionTester::FunctionTester(Handle<InstructionStream> code)
-    : FunctionTester(code, 0) {}
-
 FunctionTester::FunctionTester(Handle<Code> code) : FunctionTester(code, 0) {}
 
 void FunctionTester::CheckThrows(Handle<Object> a) {
diff --git a/test/cctest/compiler/function-tester.h b/test/cctest/compiler/function-tester.h
index 6f58997982..09249f40b0 100644
--- a/test/cctest/compiler/function-tester.h
+++ b/test/cctest/compiler/function-tester.h
@@ -25,11 +25,9 @@ class FunctionTester : public InitializedHandleScope {
 
   FunctionTester(Graph* graph, int param_count);
 
-  FunctionTester(Handle<InstructionStream> code, int param_count);
   FunctionTester(Handle<Code> code, int param_count);
 
   // Assumes VoidDescriptor call interface.
-  explicit FunctionTester(Handle<InstructionStream> code);
   explicit FunctionTester(Handle<Code> code);
 
   Isolate* isolate;
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index 7242134dc1..7d86c2e359 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -1666,9 +1666,7 @@ TEST(Regress_1171759) {
   std::shared_ptr<wasm::NativeModule> module =
       AllocateNativeModule(handles.main_isolate(), code->InstructionSize());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
-  Handle<InstructionStream> istream(code->instruction_stream(),
-                                    handles.main_isolate());
-  byte* code_start = module->AddCodeForTesting(istream)->instructions().begin();
+  byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
 
   // Generate a minimal calling function, to push stack arguments.
   RawMachineAssemblerTester<int32_t> mt;
diff --git a/test/cctest/compiler/test-multiple-return.cc b/test/cctest/compiler/test-multiple-return.cc
index afc36d47c8..9c65ee1582 100644
--- a/test/cctest/compiler/test-multiple-return.cc
+++ b/test/cctest/compiler/test-multiple-return.cc
@@ -174,8 +174,6 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
         code->Disassemble("multi_value", os, handles.main_isolate());
       }
 #endif
-      Handle<InstructionStream> istream(code->instruction_stream(),
-                                        handles.main_isolate());
 
       const int a = 47, b = 12;
       int expect = 0;
@@ -187,10 +185,10 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
       }
 
       std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
-          handles.main_isolate(), istream->instruction_size());
+          handles.main_isolate(), code->instruction_size());
       wasm::WasmCodeRefScope wasm_code_ref_scope;
       byte* code_start =
-          module->AddCodeForTesting(istream)->instructions().begin();
+          module->AddCodeForTesting(code)->instructions().begin();
 
       RawMachineAssemblerTester<int32_t> mt(CodeKind::JS_TO_WASM_FUNCTION);
       const int input_count = 2 + param_count;
@@ -282,14 +280,11 @@ void ReturnLastValue(MachineType type) {
                             AssemblerOptions::Default(handles.main_isolate()),
                             m.ExportForTest())
                             .ToHandleChecked();
-    Handle<InstructionStream> istream(code->instruction_stream(),
-                                      handles.main_isolate());
 
-    std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
-        handles.main_isolate(), istream->instruction_size());
+    std::shared_ptr<wasm::NativeModule> module =
+        AllocateNativeModule(handles.main_isolate(), code->instruction_size());
     wasm::WasmCodeRefScope wasm_code_ref_scope;
-    byte* code_start =
-        module->AddCodeForTesting(istream)->instructions().begin();
+    byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
 
     // Generate caller.
     int expect = return_count - 1;
@@ -348,14 +343,11 @@ void ReturnSumOfReturns(MachineType type) {
                             AssemblerOptions::Default(handles.main_isolate()),
                             m.ExportForTest())
                             .ToHandleChecked();
-    Handle<InstructionStream> istream(code->instruction_stream(),
-                                      handles.main_isolate());
 
-    std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
-        handles.main_isolate(), istream->instruction_size());
+    std::shared_ptr<wasm::NativeModule> module =
+        AllocateNativeModule(handles.main_isolate(), code->instruction_size());
     wasm::WasmCodeRefScope wasm_code_ref_scope;
-    byte* code_start =
-        module->AddCodeForTesting(istream)->instructions().begin();
+    byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
 
     // Generate caller.
     RawMachineAssemblerTester<int32_t> mt;
diff --git a/test/cctest/heap/test-concurrent-allocation.cc b/test/cctest/heap/test-concurrent-allocation.cc
index fdef32ed33..b2ea8e238d 100644
--- a/test/cctest/heap/test-concurrent-allocation.cc
+++ b/test/cctest/heap/test-concurrent-allocation.cc
@@ -470,7 +470,7 @@ UNINITIALIZED_TEST(ConcurrentWriteBarrier) {
 
 class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
  public:
-  explicit ConcurrentRecordRelocSlotThread(Heap* heap, InstructionStream code,
+  explicit ConcurrentRecordRelocSlotThread(Heap* heap, Code code,
                                            HeapObject value)
       : v8::base::Thread(base::Thread::Options("ThreadWithLocalHeap")),
         heap_(heap),
@@ -491,7 +491,7 @@ class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
   }
 
   Heap* heap_;
-  InstructionStream code_;
+  Code code_;
   HeapObject value_;
 };
 
@@ -510,7 +510,7 @@ UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
   Isolate* i_isolate = reinterpret_cast<Isolate*>(isolate);
   Heap* heap = i_isolate->heap();
   {
-    InstructionStream code;
+    Code code;
     HeapObject value;
     CodePageCollectionMemoryModificationScopeForTesting code_scope(heap);
     {
@@ -529,11 +529,8 @@ UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
 #endif
       CodeDesc desc;
       masm.GetCode(i_isolate, &desc);
-      Handle<InstructionStream> code_handle(
-          Factory::CodeBuilder(i_isolate, desc, CodeKind::FOR_TESTING)
-              .Build()
-              ->instruction_stream(),
-          i_isolate);
+      Handle<Code> code_handle =
+          Factory::CodeBuilder(i_isolate, desc, CodeKind::FOR_TESTING).Build();
       heap::AbandonCurrentlyFreeMemory(heap->old_space());
       Handle<HeapNumber> value_handle(
           i_isolate->factory()->NewHeapNumber<AllocationType::kOld>(1.1));
diff --git a/test/cctest/heap/test-heap.cc b/test/cctest/heap/test-heap.cc
index 4ff118ead2..1866552074 100644
--- a/test/cctest/heap/test-heap.cc
+++ b/test/cctest/heap/test-heap.cc
@@ -185,9 +185,8 @@ static void CheckNumber(Isolate* isolate, double value, const char* string) {
   CHECK(String::cast(*print_string).IsOneByteEqualTo(base::CStrVector(string)));
 }
 
-void CheckEmbeddedObjectsAreEqual(Isolate* isolate,
-                                  Handle<InstructionStream> lhs,
-                                  Handle<InstructionStream> rhs) {
+void CheckEmbeddedObjectsAreEqual(Isolate* isolate, Handle<Code> lhs,
+                                  Handle<Code> rhs) {
   int mode_mask = RelocInfo::ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT);
   PtrComprCageBase cage_base(isolate);
   RelocIterator lhs_it(*lhs, mode_mask);
@@ -4338,7 +4337,7 @@ TEST(CellsInOptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<InstructionStream> code;
+  Handle<Code> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4362,7 +4361,7 @@ TEST(CellsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCode(bar->code()), isolate);
+    code = handle(bar->code(), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4387,7 +4386,7 @@ TEST(ObjectsInOptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<InstructionStream> code;
+  Handle<Code> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4409,7 +4408,7 @@ TEST(ObjectsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCode(bar->code()), isolate);
+    code = handle(bar->code(), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4434,7 +4433,7 @@ TEST(NewSpaceObjectsInOptimizedCode) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(isolate);
-  Handle<InstructionStream> code;
+  Handle<Code> code;
   {
     LocalContext context;
     HandleScope scope(isolate);
@@ -4475,7 +4474,7 @@ TEST(NewSpaceObjectsInOptimizedCode) {
     HeapVerifier::VerifyHeap(CcTest::heap());
 #endif
     CHECK(!bar->code().marked_for_deoptimization());
-    code = handle(FromCode(bar->code()), isolate);
+    code = handle(bar->code(), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4499,7 +4498,7 @@ TEST(ObjectsInEagerlyDeoptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<InstructionStream> code;
+  Handle<Code> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4522,7 +4521,7 @@ TEST(ObjectsInEagerlyDeoptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCode(bar->code()), isolate);
+    code = handle(bar->code(), isolate);
     code = scope.CloseAndEscape(code);
   }
 
diff --git a/test/cctest/test-cpu-profiler.cc b/test/cctest/test-cpu-profiler.cc
index dc44d60ead..09b0d0541f 100644
--- a/test/cctest/test-cpu-profiler.cc
+++ b/test/cctest/test-cpu-profiler.cc
@@ -4286,7 +4286,7 @@ int GetSourcePositionEntryCount(i::Isolate* isolate, const char* source,
   i::Handle<i::JSFunction> function = i::Handle<i::JSFunction>::cast(
       v8::Utils::OpenHandle(*CompileRun(source)));
   if (function->ActiveTierIsIgnition()) return -1;
-  i::Handle<i::InstructionStream> code(i::FromCode(function->code()), isolate);
+  i::Handle<i::Code> code(function->code(), isolate);
   i::SourcePositionTableIterator iterator(
       ByteArray::cast(code->source_position_table()));
 
diff --git a/test/cctest/test-serialize.cc b/test/cctest/test-serialize.cc
index 49e09e60c2..eb4a9db631 100644
--- a/test/cctest/test-serialize.cc
+++ b/test/cctest/test-serialize.cc
@@ -1597,9 +1597,7 @@ int CountBuiltins() {
   int counter = 0;
   for (HeapObject obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
-    if (obj.IsInstructionStream() &&
-        InstructionStream::cast(obj).kind() == CodeKind::BUILTIN)
-      counter++;
+    if (obj.IsCode() && Code::cast(obj).kind() == CodeKind::BUILTIN) counter++;
   }
   return counter;
 }
diff --git a/test/cctest/test-unwinder-code-pages.cc b/test/cctest/test-unwinder-code-pages.cc
index 795126b8e4..fe72a45d89 100644
--- a/test/cctest/test-unwinder-code-pages.cc
+++ b/test/cctest/test-unwinder-code-pages.cc
@@ -301,13 +301,12 @@ TEST(Unwind_CodeObjectPCInMiddle_Success_CodePagesAPI) {
   // --no-maglev.
   if (!code.is_optimized_code()) return;
 
-  InstructionStream instruction_stream = FromCode(code);
   // We don't want the offset too early or it could be the `push rbp`
   // instruction (which is not at the start of generated code, because the lazy
   // deopt check happens before frame setup).
-  const uintptr_t offset = instruction_stream.instruction_size() - 20;
-  CHECK_LT(offset, instruction_stream.instruction_size());
-  Address pc = instruction_stream.instruction_start() + offset;
+  const uintptr_t offset = code.instruction_size() - 20;
+  CHECK_LT(offset, code.instruction_size());
+  Address pc = code.InstructionStart() + offset;
   register_state.pc = reinterpret_cast<void*>(pc);
 
   // Get code pages from the API now that the code obejct exists and check that
@@ -673,14 +672,12 @@ TEST(PCIsInV8_LargeCodeObject_CodePagesAPI) {
   desc.unwinding_info = nullptr;
   desc.unwinding_info_size = 0;
   desc.origin = nullptr;
-  Handle<InstructionStream> foo_code(
-      Factory::CodeBuilder(i_isolate, desc, CodeKind::WASM_FUNCTION)
-          .Build()
-          ->instruction_stream(),
-      i_isolate);
-
-  CHECK(i_isolate->heap()->InSpace(*foo_code, CODE_LO_SPACE));
-  byte* start = reinterpret_cast<byte*>(foo_code->instruction_start());
+  Handle<Code> foo_code =
+      Factory::CodeBuilder(i_isolate, desc, CodeKind::WASM_FUNCTION).Build();
+
+  CHECK(i_isolate->heap()->InSpace(foo_code->instruction_stream(),
+                                   CODE_LO_SPACE));
+  byte* start = reinterpret_cast<byte*>(foo_code->InstructionStart());
 
   MemoryRange code_pages[v8::Isolate::kMinCodePagesBufferSize];
   size_t pages_length =
diff --git a/test/fuzzer/multi-return.cc b/test/fuzzer/multi-return.cc
index 11746b4a1b..09c49ec9f8 100644
--- a/test/fuzzer/multi-return.cc
+++ b/test/fuzzer/multi-return.cc
@@ -244,12 +244,11 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
                                        AssemblerOptions::Default(i_isolate),
                                        callee.ExportForTest())
           .ToHandleChecked();
-  Handle<InstructionStream> istream(code->instruction_stream(), i_isolate);
 
   std::shared_ptr<wasm::NativeModule> module =
       AllocateNativeModule(i_isolate, code->InstructionSize());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
-  byte* code_start = module->AddCodeForTesting(istream)->instructions().begin();
+  byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
   // Generate wrapper.
   int expect = 0;
 
diff --git a/test/unittests/assembler/macro-assembler-x64-unittest.cc b/test/unittests/assembler/macro-assembler-x64-unittest.cc
index 8ea56c12c4..9924b620ee 100644
--- a/test/unittests/assembler/macro-assembler-x64-unittest.cc
+++ b/test/unittests/assembler/macro-assembler-x64-unittest.cc
@@ -530,8 +530,7 @@ TEST_F(MacroAssemblerX64Test, EmbeddedObj) {
 
   // Test the user-facing reloc interface.
   const int mode_mask = RelocInfo::EmbeddedObjectModeMask();
-  for (RelocIterator it(code->instruction_stream(), mode_mask); !it.done();
-       it.next()) {
+  for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
     RelocInfo::Mode mode = it.rinfo()->rmode();
     if (RelocInfo::IsCompressedEmbeddedObject(mode)) {
       CHECK_EQ(*my_array, it.rinfo()->target_object(cage_base));
diff --git a/test/unittests/compiler/codegen-tester.h b/test/unittests/compiler/codegen-tester.h
index 68248381b4..146d9907f7 100644
--- a/test/unittests/compiler/codegen-tester.h
+++ b/test/unittests/compiler/codegen-tester.h
@@ -72,10 +72,6 @@ class RawMachineAssemblerTester : public CallHelper<ReturnType>,
 
   void GenerateCode() { Generate(); }
 
-  Handle<InstructionStream> GetInstructionStream() {
-    return handle(GetCode()->instruction_stream(), isolate_);
-  }
-
   Handle<Code> GetCode() {
     Generate();
     return code_.ToHandleChecked();
diff --git a/test/unittests/compiler/function-tester.cc b/test/unittests/compiler/function-tester.cc
index 3f9b582df9..d7e7356c76 100644
--- a/test/unittests/compiler/function-tester.cc
+++ b/test/unittests/compiler/function-tester.cc
@@ -52,18 +52,6 @@ FunctionTester::FunctionTester(Isolate* isolate, Graph* graph, int param_count)
   CompileGraph(graph);
 }
 
-FunctionTester::FunctionTester(Isolate* isolate, Handle<InstructionStream> code,
-                               int param_count)
-    : isolate(isolate),
-      canonical(isolate),
-      function((v8_flags.allow_natives_syntax = true,
-                NewFunction(BuildFunction(param_count).c_str()))),
-      flags_(0) {
-  CHECK(!code.is_null());
-  Compile(function);
-  function->set_code(ToCode(*code), kReleaseStore);
-}
-
 FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
                                int param_count)
     : isolate(isolate),
@@ -76,9 +64,6 @@ FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
   function->set_code(*code, kReleaseStore);
 }
 
-FunctionTester::FunctionTester(Isolate* isolate, Handle<InstructionStream> code)
-    : FunctionTester(isolate, code, 0) {}
-
 void FunctionTester::CheckThrows(Handle<Object> a) {
   TryCatch try_catch(reinterpret_cast<v8::Isolate*>(isolate));
   MaybeHandle<Object> no_result = Call(a);
diff --git a/test/unittests/regexp/regexp-unittest.cc b/test/unittests/regexp/regexp-unittest.cc
index f42b455b39..28799cdceb 100644
--- a/test/unittests/regexp/regexp-unittest.cc
+++ b/test/unittests/regexp/regexp-unittest.cc
@@ -641,8 +641,7 @@ class ContextInitializer {
 
 // Create new JSRegExp object with only necessary fields (for this tests)
 // initialized.
-static Handle<JSRegExp> CreateJSRegExp(Handle<String> source,
-                                       Handle<InstructionStream> code,
+static Handle<JSRegExp> CreateJSRegExp(Handle<String> source, Handle<Code> code,
                                        bool is_unicode = false) {
   Isolate* isolate = reinterpret_cast<i::Isolate*>(v8::Isolate::GetCurrent());
   Factory* factory = isolate->factory();
@@ -682,7 +681,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSuccess) {
 
   Handle<String> source = factory->NewStringFromStaticChars("");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   int captures[4] = {42, 37, 87, 117};
@@ -729,7 +728,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSimple) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^foo");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   int captures[4] = {42, 37, 87, 117};
@@ -785,7 +784,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSimpleUC16) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^foo");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code, true);
 
   int captures[4] = {42, 37, 87, 117};
@@ -843,7 +842,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBacktrack) {
 
   Handle<String> source = factory->NewStringFromStaticChars("..........");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("foofoo");
@@ -881,7 +880,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackReferenceLATIN1) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^(..)..\1");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("fooofo");
@@ -924,7 +923,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackReferenceUC16) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^(..)..\1");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code, true);
 
   const base::uc16 input_data[6] = {'f', 0x2028, 'o', 'o', 'f', 0x2028};
@@ -977,7 +976,7 @@ TEST_F(RegExpTest, MacroAssemblernativeAtStart) {
 
   Handle<String> source = factory->NewStringFromStaticChars("(^f|ob)");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("foobar");
@@ -1028,7 +1027,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackRefNoCase) {
   Handle<String> source =
       factory->NewStringFromStaticChars("^(abc)\1\1(?!\1)...(?!\1)");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("aBcAbCABCxYzab");
@@ -1120,7 +1119,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeRegisters) {
 
   Handle<String> source = factory->NewStringFromStaticChars("<loop test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -1157,7 +1156,7 @@ TEST_F(RegExpTest, MacroAssemblerStackOverflow) {
   Handle<String> source =
       factory->NewStringFromStaticChars("<stack overflow test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -1197,7 +1196,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeLotsOfRegisters) {
   Handle<String> source =
       factory->NewStringFromStaticChars("<huge register space test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
+  Handle<Code> code = Handle<Code>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -2361,8 +2360,8 @@ TEST_F(RegExpTestWithContext, UnicodePropertyEscapeCodeSize) {
     CHECK_LT(ByteArray::cast(maybe_bytecode).Size(), kMaxSize);
   } else if (maybe_code.IsCode()) {
     // On x64, excessive inlining produced >360KB.
-    CHECK_LT(FromCode(Code::cast(maybe_code)).Size(), kMaxSize);
-    CHECK_EQ(FromCode(Code::cast(maybe_code)).kind(), CodeKind::REGEXP);
+    CHECK_LT(Code::cast(maybe_code).Size(), kMaxSize);
+    CHECK_EQ(Code::cast(maybe_code).kind(), CodeKind::REGEXP);
   } else {
     UNREACHABLE();
   }
diff --git a/tools/gen-postmortem-metadata.py b/tools/gen-postmortem-metadata.py
index 69b0f1ff1b..4d171267a9 100644
--- a/tools/gen-postmortem-metadata.py
+++ b/tools/gen-postmortem-metadata.py
@@ -164,11 +164,11 @@ consts_misc = [
     },
     {
         'name': 'CodeKindFieldMask',
-        'value': 'InstructionStream::KindField::kMask'
+        'value': 'Code::KindField::kMask'
     },
     {
         'name': 'CodeKindFieldShift',
-        'value': 'InstructionStream::KindField::kShift'
+        'value': 'Code::KindField::kShift'
     },
     {
         'name': 'DeoptimizationDataInlinedFunctionCountIndex',
@@ -549,10 +549,7 @@ extras_accessors = [
     'SharedFunctionInfo, flags, int, kFlagsOffset',
     'SharedFunctionInfo, length, uint16_t, kLengthOffset',
     'SlicedString, parent, String, kParentOffset',
-    'InstructionStream, flags, uint32_t, kFlagsOffset',
     'InstructionStream, instruction_start, uintptr_t, kHeaderSize',
-    'InstructionStream, instruction_size, int, kInstructionSizeOffset',
-    'InstructionStream, deoptimization_data, FixedArray, kDeoptimizationDataOrInterpreterDataOffset',
     'String, length, int32_t, kLengthOffset',
     'DescriptorArray, header_size, uintptr_t, kHeaderSize',
     'ConsString, first, String, kFirstOffset',
-- 
2.35.1

