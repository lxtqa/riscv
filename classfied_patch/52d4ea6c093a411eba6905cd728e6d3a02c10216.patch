From 52d4ea6c093a411eba6905cd728e6d3a02c10216 Mon Sep 17 00:00:00 2001
From: Jakob Gruber <jgruber@chromium.org>
Date: Thu, 17 Mar 2022 09:59:47 +0100
Subject: [PATCH] [osr] Minor refactors in OSR-related code

- Restructure the runtime function implementation.
- Rename osr_loop_nesting_level to osr_urgency and add helpers.

The motivation for the latter: I've always struggled with the
`osr_loop_nesting_level` term; it neither matches terminology of
what it's compared against (= the loop depth), nor implies what it's
used for (= osr is triggered when `loop depth < osr nesting level`).

In this CL it's renamed to `osr_urgency` to reflect that as urgency
rises, we consider more and more loops as OSR candidates.

Bug: v8:12161
Change-Id: I194ec5a3f1f02526641af1c7796ee0956b6fd3a1
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3528735
Reviewed-by: Patrick Thier <pthier@chromium.org>
Reviewed-by: Shu-yu Guo <syg@chromium.org>
Reviewed-by: Tobias Tebbi <tebbi@chromium.org>
Commit-Queue: Jakob Gruber <jgruber@chromium.org>
Cr-Commit-Position: refs/heads/main@{#79509}
---
 src/baseline/baseline-batch-compiler.cc  |   3 +-
 src/baseline/baseline-compiler.cc        |  10 +-
 src/builtins/arm/builtins-arm.cc         |  14 +--
 src/builtins/arm64/builtins-arm64.cc     |  12 +-
 src/builtins/ia32/builtins-ia32.cc       |  13 +--
 src/builtins/loong64/builtins-loong64.cc |  14 +--
 src/builtins/mips/builtins-mips.cc       |  12 +-
 src/builtins/mips64/builtins-mips64.cc   |  12 +-
 src/builtins/ppc/builtins-ppc.cc         |   4 +-
 src/builtins/riscv64/builtins-riscv64.cc |  12 +-
 src/builtins/s390/builtins-s390.cc       |  16 ++-
 src/builtins/x64/builtins-x64.cc         |  11 +-
 src/codegen/compiler.cc                  |   4 +-
 src/execution/tiering-manager.cc         |  11 +-
 src/heap/factory-base.cc                 |   2 +-
 src/heap/factory.cc                      |   2 +-
 src/interpreter/control-flow-builders.cc |  15 ++-
 src/interpreter/interpreter-assembler.cc |   4 +-
 src/interpreter/interpreter-assembler.h  |   4 +-
 src/interpreter/interpreter-generator.cc |  10 +-
 src/objects/code-inl.h                   |  18 ++-
 src/objects/code.cc                      |   4 +-
 src/objects/code.h                       |  28 ++---
 src/objects/code.tq                      |   4 +-
 src/runtime/runtime-compiler.cc          | 138 ++++++++++++-----------
 src/runtime/runtime-test.cc              |   3 +-
 src/snapshot/deserializer.cc             |   2 +-
 test/cctest/test-serialize.cc            |   2 +-
 28 files changed, 190 insertions(+), 194 deletions(-)

diff --git a/src/baseline/baseline-batch-compiler.cc b/src/baseline/baseline-batch-compiler.cc
index e0f528bcbe..c11b78e74e 100644
--- a/src/baseline/baseline-batch-compiler.cc
+++ b/src/baseline/baseline-batch-compiler.cc
@@ -71,9 +71,8 @@ class BaselineCompilerTask {
     }
     shared_function_info_->set_baseline_code(ToCodeT(*code), kReleaseStore);
     if (V8_LIKELY(FLAG_use_osr)) {
-      // Arm back edges for OSR
       shared_function_info_->GetBytecodeArray(isolate)
-          .set_osr_loop_nesting_level(AbstractCode::kMaxLoopNestingMarker);
+          .RequestOsrAtNextOpportunity();
     }
     if (FLAG_trace_baseline_concurrent_compilation) {
       CodeTracer::Scope scope(isolate->GetCodeTracer());
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index e057535020..d2acaee1ce 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -1930,12 +1930,12 @@ void BaselineCompiler::VisitJumpLoop() {
   Label osr_not_armed;
   {
     ASM_CODE_COMMENT_STRING(&masm_, "OSR Check Armed");
-    Register osr_level = scratch;
-    __ LoadRegister(osr_level, interpreter::Register::bytecode_array());
-    __ LoadByteField(osr_level, osr_level,
-                     BytecodeArray::kOsrLoopNestingLevelOffset);
+    Register osr_urgency = scratch;
+    __ LoadRegister(osr_urgency, interpreter::Register::bytecode_array());
+    __ LoadByteField(osr_urgency, osr_urgency,
+                     BytecodeArray::kOsrUrgencyOffset);
     int loop_depth = iterator().GetImmediateOperand(1);
-    __ JumpIfByte(Condition::kUnsignedLessThanEqual, osr_level, loop_depth,
+    __ JumpIfByte(Condition::kUnsignedLessThanEqual, osr_urgency, loop_depth,
                   &osr_not_armed);
     CallBuiltin<Builtin::kBaselineOnStackReplacement>();
   }
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index fe2536fa0a..df6a961880 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -1145,15 +1145,14 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
     {
       UseScratchRegisterScope temps(masm);
       Register scratch = temps.Acquire();
       __ mov(scratch, Operand(0));
       __ strh(scratch,
-              FieldMemOperand(bytecodeArray,
-                              BytecodeArray::kOsrLoopNestingLevelOffset));
+              FieldMemOperand(bytecodeArray, BytecodeArray::kOsrUrgencyOffset));
     }
 
     __ Push(argc, bytecodeArray);
@@ -1299,11 +1298,11 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ mov(r9, Operand(0));
   __ strh(r9, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                              BytecodeArray::kOsrLoopNestingLevelOffset));
+                              BytecodeArray::kOsrUrgencyOffset));
 
   // Load the initial bytecode offset.
   __ mov(kInterpreterBytecodeOffsetRegister,
@@ -3681,9 +3680,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
     __ mov(scratch, Operand(0));
-    __ strh(scratch,
-            FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                            BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ strh(scratch, FieldMemOperand(kInterpreterBytecodeArrayRegister,
+                                     BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index e6321c614c..bcb4809380 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -1320,10 +1320,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ Strh(wzr, FieldMemOperand(bytecode_array,
-                                 BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ Strh(wzr,
+            FieldMemOperand(bytecode_array, BytecodeArray::kOsrUrgencyOffset));
 
     __ Push(argc, bytecode_array);
 
@@ -1479,10 +1479,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ Strh(wzr, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                               BytecodeArray::kOsrLoopNestingLevelOffset));
+                               BytecodeArray::kOsrUrgencyOffset));
 
   // Load the initial bytecode offset.
   __ Mov(kInterpreterBytecodeOffsetRegister,
@@ -4202,7 +4202,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // TODO(pthier): Separate baseline Sparkplug from TF arming and don't disarm
     // Sparkplug here.
     __ Strh(wzr, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                 BytecodeArray::kOsrLoopNestingLevelOffset));
+                                 BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj, Code::kHeaderSize - kHeapObjectTag);
   } else {
     __ Add(code_obj, code_obj, Code::kHeaderSize - kHeapObjectTag);
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index c217c6c7c3..7867e47496 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -1134,10 +1134,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ mov_w(FieldOperand(kInterpreterBytecodeArrayRegister,
-                        BytecodeArray::kOsrLoopNestingLevelOffset),
+                        BytecodeArray::kOsrUrgencyOffset),
            Immediate(0));
 
   // Push bytecode array.
@@ -1754,11 +1754,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ mov_w(
-        FieldOperand(bytecode_array, BytecodeArray::kOsrLoopNestingLevelOffset),
-        Immediate(0));
+    __ mov_w(FieldOperand(bytecode_array, BytecodeArray::kOsrUrgencyOffset),
+             Immediate(0));
     __ Push(bytecode_array);
 
     // Baseline code frames store the feedback vector where interpreter would
@@ -4296,7 +4295,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // TODO(pthier): Separate baseline Sparkplug from TF arming and don't disarm
     // Sparkplug here.
     __ mov_w(FieldOperand(kInterpreterBytecodeArrayRegister,
-                          BytecodeArray::kOsrLoopNestingLevelOffset),
+                          BytecodeArray::kOsrUrgencyOffset),
              Immediate(0));
     Generate_OSREntry(masm, code_obj);
   } else {
diff --git a/src/builtins/loong64/builtins-loong64.cc b/src/builtins/loong64/builtins-loong64.cc
index 8033944139..7a412ad793 100644
--- a/src/builtins/loong64/builtins-loong64.cc
+++ b/src/builtins/loong64/builtins-loong64.cc
@@ -1122,11 +1122,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
     __ St_h(zero_reg,
-            FieldMemOperand(bytecodeArray,
-                            BytecodeArray::kOsrLoopNestingLevelOffset));
+            FieldMemOperand(bytecodeArray, BytecodeArray::kOsrUrgencyOffset));
 
     __ Push(argc, bytecodeArray);
 
@@ -1283,10 +1282,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ St_h(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+                                    BytecodeArray::kOsrUrgencyOffset));
 
   // Load initial bytecode offset.
   __ li(kInterpreterBytecodeOffsetRegister,
@@ -3698,9 +3697,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // TODO(liuyu): Remove Ld as arm64 after register reallocation.
     __ Ld_d(kInterpreterBytecodeArrayRegister,
             MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
-    __ St_h(zero_reg,
-            FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                            BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ St_h(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
+                                      BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
diff --git a/src/builtins/mips/builtins-mips.cc b/src/builtins/mips/builtins-mips.cc
index 64ecb55f23..4109048b56 100644
--- a/src/builtins/mips/builtins-mips.cc
+++ b/src/builtins/mips/builtins-mips.cc
@@ -1119,10 +1119,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ sh(zero_reg, FieldMemOperand(bytecodeArray,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ sh(zero_reg,
+          FieldMemOperand(bytecodeArray, BytecodeArray::kOsrUrgencyOffset));
 
     __ Push(argc, bytecodeArray);
 
@@ -1277,10 +1277,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                  BytecodeArray::kOsrLoopNestingLevelOffset));
+                                  BytecodeArray::kOsrUrgencyOffset));
 
   // Load initial bytecode offset.
   __ li(kInterpreterBytecodeOffsetRegister,
@@ -4147,7 +4147,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ Lw(kInterpreterBytecodeArrayRegister,
           MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
     __ sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+                                    BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index 85872b3d5c..38657cad4d 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -1126,10 +1126,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ Sh(zero_reg, FieldMemOperand(bytecodeArray,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ Sh(zero_reg,
+          FieldMemOperand(bytecodeArray, BytecodeArray::kOsrUrgencyOffset));
 
     __ Push(argc, bytecodeArray);
 
@@ -1284,10 +1284,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                  BytecodeArray::kOsrLoopNestingLevelOffset));
+                                  BytecodeArray::kOsrUrgencyOffset));
 
   // Load initial bytecode offset.
   __ li(kInterpreterBytecodeOffsetRegister,
@@ -3724,7 +3724,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ Ld(kInterpreterBytecodeArrayRegister,
           MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
     __ Sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+                                    BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index 96322fcc4b..06222f7778 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -1138,12 +1138,12 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ li(r8, Operand(0));
   __ StoreU16(r8,
               FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                              BytecodeArray::kOsrLoopNestingLevelOffset),
+                              BytecodeArray::kOsrUrgencyOffset),
               r0);
 
   // Load initial bytecode offset.
diff --git a/src/builtins/riscv64/builtins-riscv64.cc b/src/builtins/riscv64/builtins-riscv64.cc
index 11a8f5156c..11a01162d2 100644
--- a/src/builtins/riscv64/builtins-riscv64.cc
+++ b/src/builtins/riscv64/builtins-riscv64.cc
@@ -1184,10 +1184,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ Sh(zero_reg, FieldMemOperand(bytecodeArray,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ Sh(zero_reg,
+          FieldMemOperand(bytecodeArray, BytecodeArray::kOsrUrgencyOffset));
 
     __ Push(argc, bytecodeArray);
 
@@ -1348,10 +1348,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ Sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                  BytecodeArray::kOsrLoopNestingLevelOffset));
+                                  BytecodeArray::kOsrUrgencyOffset));
 
   // Load initial bytecode offset.
   __ li(kInterpreterBytecodeOffsetRegister,
@@ -3817,7 +3817,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ Ld(kInterpreterBytecodeArrayRegister,
           MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
     __ Sh(zero_reg, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                    BytecodeArray::kOsrLoopNestingLevelOffset));
+                                    BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index 9b328cf3fc..5bc69a7c02 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -206,9 +206,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (is_osr) {
     Register scratch = r1;
     __ mov(scratch, Operand(0));
-    __ StoreU16(scratch,
-                FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                BytecodeArray::kOsrLoopNestingLevelOffset));
+    __ StoreU16(scratch, FieldMemOperand(kInterpreterBytecodeArrayRegister,
+                                         BytecodeArray::kOsrUrgencyOffset));
     Generate_OSREntry(masm, code_obj, Code::kHeaderSize - kHeapObjectTag);
   } else {
     __ AddS64(code_obj, code_obj, Operand(Code::kHeaderSize - kHeapObjectTag));
@@ -1420,14 +1419,13 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // are 8-bit fields next to each other, so we could just optimize by writing
     // a 16-bit. These static asserts guard our assumption is valid.
     STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                  BytecodeArray::kOsrUrgencyOffset + kCharSize);
     STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
     {
       Register scratch = r0;
       __ mov(scratch, Operand(0));
-      __ StoreU16(scratch,
-                  FieldMemOperand(bytecodeArray,
-                                  BytecodeArray::kOsrLoopNestingLevelOffset));
+      __ StoreU16(scratch, FieldMemOperand(bytecodeArray,
+                                           BytecodeArray::kOsrUrgencyOffset));
     }
 
     __ Push(argc, bytecodeArray);
@@ -1584,12 +1582,12 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ mov(r1, Operand(0));
   __ StoreU16(r1,
               FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                              BytecodeArray::kOsrLoopNestingLevelOffset),
+                              BytecodeArray::kOsrUrgencyOffset),
               r0);
 
   // Load the initial bytecode offset.
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 9ffd1ea2be..1197bc900b 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -1228,10 +1228,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // 8-bit fields next to each other, so we could just optimize by writing a
   // 16-bit. These static asserts guard our assumption is valid.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                BytecodeArray::kOsrUrgencyOffset + kCharSize);
   STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ movw(FieldOperand(kInterpreterBytecodeArrayRegister,
-                       BytecodeArray::kOsrLoopNestingLevelOffset),
+                       BytecodeArray::kOsrUrgencyOffset),
           Immediate(0));
 
   // Load initial bytecode offset.
@@ -1743,10 +1743,9 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
       // are 8-bit fields next to each other, so we could just optimize by
       // writing a 16-bit. These static asserts guard our assumption is valid.
       STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                    BytecodeArray::kOsrLoopNestingLevelOffset + kCharSize);
+                    BytecodeArray::kOsrUrgencyOffset + kCharSize);
       STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-      __ movw(FieldOperand(bytecode_array,
-                           BytecodeArray::kOsrLoopNestingLevelOffset),
+      __ movw(FieldOperand(bytecode_array, BytecodeArray::kOsrUrgencyOffset),
               Immediate(0));
       __ Push(bytecode_array);
 
@@ -5121,7 +5120,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // TODO(pthier): Separate baseline Sparkplug from TF arming and don't disarm
     // Sparkplug here.
     __ movw(FieldOperand(kInterpreterBytecodeArrayRegister,
-                         BytecodeArray::kOsrLoopNestingLevelOffset),
+                         BytecodeArray::kOsrUrgencyOffset),
             Immediate(0));
     Generate_OSREntry(masm, code_obj);
   } else {
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index df237d44bd..808f6c023e 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -2092,9 +2092,7 @@ bool Compiler::CompileSharedWithBaseline(Isolate* isolate,
     shared->set_baseline_code(ToCodeT(*code), kReleaseStore);
 
     if (V8_LIKELY(FLAG_use_osr)) {
-      // Arm back edges for OSR
-      shared->GetBytecodeArray(isolate).set_osr_loop_nesting_level(
-          AbstractCode::kMaxLoopNestingMarker);
+      shared->GetBytecodeArray(isolate).RequestOsrAtNextOpportunity();
     }
   }
   double time_taken_ms = time_taken.InMillisecondsF();
diff --git a/src/execution/tiering-manager.cc b/src/execution/tiering-manager.cc
index e87b170a60..be8cf1a55e 100644
--- a/src/execution/tiering-manager.cc
+++ b/src/execution/tiering-manager.cc
@@ -170,9 +170,9 @@ void TieringManager::AttemptOnStackReplacement(UnoptimizedFrame* frame,
   }
 
   DCHECK(frame->is_unoptimized());
-  int level = frame->GetBytecodeArray().osr_loop_nesting_level();
-  frame->GetBytecodeArray().set_osr_loop_nesting_level(std::min(
-      {level + loop_nesting_levels, AbstractCode::kMaxLoopNestingMarker}));
+  const int urgency = frame->GetBytecodeArray().osr_urgency();
+  frame->GetBytecodeArray().set_osr_urgency(
+      std::min({urgency + loop_nesting_levels, BytecodeArray::kMaxOsrUrgency}));
 }
 
 namespace {
@@ -230,7 +230,7 @@ void TieringManager::MaybeOptimizeFrame(JSFunction function,
   if (frame->is_unoptimized()) {
     if (V8_UNLIKELY(FLAG_always_osr)) {
       AttemptOnStackReplacement(UnoptimizedFrame::cast(frame),
-                                AbstractCode::kMaxLoopNestingMarker);
+                                BytecodeArray::kMaxOsrUrgency);
       // Fall through and do a normal optimized compile as well.
     } else if (MaybeOSR(function, UnoptimizedFrame::cast(frame))) {
       return;
@@ -297,8 +297,7 @@ OptimizationDecision TieringManager::ShouldOptimize(JSFunction function,
       int jump_target_offset = iterator.GetJumpTargetOffset();
       if (jump_offset >= current_offset &&
           current_offset >= jump_target_offset) {
-        bytecode.set_osr_loop_nesting_level(iterator.GetImmediateOperand(1) +
-                                            1);
+        bytecode.set_osr_urgency(iterator.GetImmediateOperand(1) + 1);
         return OptimizationDecision::TurbofanHotAndStable();
       }
     }
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index 5c31a72186..7d01a8b318 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -229,7 +229,7 @@ Handle<BytecodeArray> FactoryBase<Impl>::NewBytecodeArray(
   instance.set_parameter_count(parameter_count);
   instance.set_incoming_new_target_or_generator_register(
       interpreter::Register::invalid_value());
-  instance.set_osr_loop_nesting_level(0);
+  instance.reset_osr_urgency();
   instance.set_bytecode_age(BytecodeArray::kNoAgeBytecodeAge);
   instance.set_constant_pool(*constant_pool);
   instance.set_handler_table(read_only_roots().empty_byte_array(),
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index d41521cdba..20d02c6732 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -2416,7 +2416,7 @@ Handle<BytecodeArray> Factory::CopyBytecodeArray(Handle<BytecodeArray> source) {
   copy.set_handler_table(raw_source.handler_table());
   copy.set_source_position_table(raw_source.source_position_table(kAcquireLoad),
                                  kReleaseStore);
-  copy.set_osr_loop_nesting_level(raw_source.osr_loop_nesting_level());
+  copy.set_osr_urgency(raw_source.osr_urgency());
   copy.set_bytecode_age(raw_source.bytecode_age());
   raw_source.CopyBytecodesTo(copy);
   return handle(copy, isolate());
diff --git a/src/interpreter/control-flow-builders.cc b/src/interpreter/control-flow-builders.cc
index 11640bcf3c..9d9465a676 100644
--- a/src/interpreter/control-flow-builders.cc
+++ b/src/interpreter/control-flow-builders.cc
@@ -77,12 +77,15 @@ void LoopBuilder::JumpToHeader(int loop_depth, LoopBuilder* const parent_loop) {
     // they are a nested inner loop too, a Jump to its parent's JumpToHeader.
     parent_loop->JumpToLoopEnd();
   } else {
-    // Pass the proper loop nesting level to the backwards branch, to trigger
-    // on-stack replacement when armed for the given loop nesting depth.
-    int level = std::min(loop_depth, AbstractCode::kMaxLoopNestingMarker - 1);
-    // Loop must have closed form, i.e. all loop elements are within the loop,
-    // the loop header precedes the body and next elements in the loop.
-    builder()->JumpLoop(&loop_header_, level, source_position_);
+    // Pass the proper loop depth to the backwards branch for triggering OSR.
+    // For purposes of OSR, the loop depth is capped at `kMaxOsrUrgency - 1`.
+    // Once that urgency is reached, all loops become OSR candidates.
+    //
+    // The loop must have closed form, i.e. all loop elements are within the
+    // loop, the loop header precedes the body and next elements in the loop.
+    builder()->JumpLoop(&loop_header_,
+                        std::min(loop_depth, BytecodeArray::kMaxOsrUrgency - 1),
+                        source_position_);
   }
 }
 
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index e06053b628..05c4c41c46 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -1314,9 +1314,9 @@ void InterpreterAssembler::UpdateInterruptBudgetOnReturn() {
   UpdateInterruptBudget(profiling_weight, true);
 }
 
-TNode<Int8T> InterpreterAssembler::LoadOsrNestingLevel() {
+TNode<Int8T> InterpreterAssembler::LoadOsrUrgency() {
   return LoadObjectField<Int8T>(BytecodeArrayTaggedPointer(),
-                                BytecodeArray::kOsrLoopNestingLevelOffset);
+                                BytecodeArray::kOsrUrgencyOffset);
 }
 
 void InterpreterAssembler::Abort(AbortReason abort_reason) {
diff --git a/src/interpreter/interpreter-assembler.h b/src/interpreter/interpreter-assembler.h
index 9855dedda3..9936d04d01 100644
--- a/src/interpreter/interpreter-assembler.h
+++ b/src/interpreter/interpreter-assembler.h
@@ -234,8 +234,8 @@ class V8_EXPORT_PRIVATE InterpreterAssembler : public CodeStubAssembler {
   // Updates the profiler interrupt budget for a return.
   void UpdateInterruptBudgetOnReturn();
 
-  // Returns the OSR nesting level from the bytecode header.
-  TNode<Int8T> LoadOsrNestingLevel();
+  // Returns the OSR urgency from the bytecode header.
+  TNode<Int8T> LoadOsrUrgency();
 
   // Dispatch to the bytecode.
   void Dispatch();
diff --git a/src/interpreter/interpreter-generator.cc b/src/interpreter/interpreter-generator.cc
index 47d0060700..2de47febe6 100644
--- a/src/interpreter/interpreter-generator.cc
+++ b/src/interpreter/interpreter-generator.cc
@@ -2166,18 +2166,18 @@ IGNITION_HANDLER(JumpIfJSReceiverConstant, InterpreterAssembler) {
 // JumpLoop <imm> <loop_depth>
 //
 // Jump by the number of bytes represented by the immediate operand |imm|. Also
-// performs a loop nesting check, a stack check, and potentially triggers OSR in
-// case the current OSR level matches (or exceeds) the specified |loop_depth|.
+// performs a loop nesting check, a stack check, and potentially triggers OSR
+// in case `loop_depth < osr_urgency`.
 IGNITION_HANDLER(JumpLoop, InterpreterAssembler) {
   TNode<IntPtrT> relative_jump = Signed(BytecodeOperandUImmWord(0));
   TNode<Int32T> loop_depth = BytecodeOperandImm(1);
-  TNode<Int8T> osr_level = LoadOsrNestingLevel();
+  TNode<Int8T> osr_urgency = LoadOsrUrgency();
   TNode<Context> context = GetContext();
 
   // Check if OSR points at the given {loop_depth} are armed by comparing it to
-  // the current {osr_level} loaded from the header of the BytecodeArray.
+  // the current {osr_urgency} loaded from the header of the BytecodeArray.
   Label ok(this), osr_armed(this, Label::kDeferred);
-  TNode<BoolT> condition = Int32GreaterThanOrEqual(loop_depth, osr_level);
+  TNode<BoolT> condition = Int32GreaterThanOrEqual(loop_depth, osr_urgency);
   Branch(condition, &ok, &osr_armed);
 
   BIND(&ok);
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 36f04a424e..d690e6dcde 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -1156,14 +1156,14 @@ void BytecodeArray::set_incoming_new_target_or_generator_register(
   }
 }
 
-int BytecodeArray::osr_loop_nesting_level() const {
-  return ACQUIRE_READ_INT8_FIELD(*this, kOsrLoopNestingLevelOffset);
+int BytecodeArray::osr_urgency() const {
+  return ACQUIRE_READ_INT8_FIELD(*this, kOsrUrgencyOffset);
 }
 
-void BytecodeArray::set_osr_loop_nesting_level(int depth) {
-  DCHECK(0 <= depth && depth <= AbstractCode::kMaxLoopNestingMarker);
-  STATIC_ASSERT(AbstractCode::kMaxLoopNestingMarker < kMaxInt8);
-  RELEASE_WRITE_INT8_FIELD(*this, kOsrLoopNestingLevelOffset, depth);
+void BytecodeArray::set_osr_urgency(int urgency) {
+  DCHECK(0 <= urgency && urgency <= BytecodeArray::kMaxOsrUrgency);
+  STATIC_ASSERT(BytecodeArray::kMaxOsrUrgency < kMaxInt8);
+  RELEASE_WRITE_INT8_FIELD(*this, kOsrUrgencyOffset, urgency);
 }
 
 BytecodeArray::Age BytecodeArray::bytecode_age() const {
@@ -1171,6 +1171,12 @@ BytecodeArray::Age BytecodeArray::bytecode_age() const {
   return static_cast<Age>(RELAXED_READ_INT8_FIELD(*this, kBytecodeAgeOffset));
 }
 
+void BytecodeArray::reset_osr_urgency() { set_osr_urgency(0); }
+
+void BytecodeArray::RequestOsrAtNextOpportunity() {
+  set_osr_urgency(kMaxOsrUrgency);
+}
+
 void BytecodeArray::set_bytecode_age(BytecodeArray::Age age) {
   DCHECK_GE(age, kFirstBytecodeAge);
   DCHECK_LE(age, kLastBytecodeAge);
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 96bda038db..515703c812 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -623,8 +623,8 @@ void BytecodeArray::Disassemble(std::ostream& os) {
   os << "Parameter count " << parameter_count() << "\n";
   os << "Register count " << register_count() << "\n";
   os << "Frame size " << frame_size() << "\n";
-  os << "OSR nesting level: " << osr_loop_nesting_level() << "\n";
-  os << "Bytecode Age: " << bytecode_age() << "\n";
+  os << "OSR urgency: " << osr_urgency() << "\n";
+  os << "Bytecode age: " << bytecode_age() << "\n";
 
   Address base_address = GetFirstBytecodeAddress();
   SourcePositionTableIterator source_positions(SourcePositionTable());
diff --git a/src/objects/code.h b/src/objects/code.h
index 2ae72478e1..b519d588dc 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -817,10 +817,6 @@ class AbstractCode : public HeapObject {
   inline Code GetCode();
   inline BytecodeArray GetBytecodeArray();
 
-  // Max loop nesting marker used to postpose OSR. We don't take loop
-  // nesting that is deeper than 5 levels into account.
-  static const int kMaxLoopNestingMarker = 6;
-
   OBJECT_CONSTRUCTORS(AbstractCode, HeapObject);
 
  private:
@@ -953,36 +949,35 @@ class BytecodeArray
     return OBJECT_POINTER_ALIGN(kHeaderSize + length);
   }
 
-  // Setter and getter
   inline byte get(int index) const;
   inline void set(int index, byte value);
 
-  // Returns data start address.
   inline Address GetFirstBytecodeAddress();
 
-  // Accessors for frame size.
   inline int32_t frame_size() const;
   inline void set_frame_size(int32_t frame_size);
 
-  // Accessor for register count (derived from frame_size).
+  // Note: The register count is derived from frame_size.
   inline int register_count() const;
 
-  // Accessors for parameter count (including implicit 'this' receiver).
+  // Note: the parameter count includes the implicit 'this' receiver.
   inline int32_t parameter_count() const;
   inline void set_parameter_count(int32_t number_of_parameters);
 
-  // Register used to pass the incoming new.target or generator object from the
-  // fucntion call.
   inline interpreter::Register incoming_new_target_or_generator_register()
       const;
   inline void set_incoming_new_target_or_generator_register(
       interpreter::Register incoming_new_target_or_generator_register);
 
-  // Accessors for OSR loop nesting level.
-  inline int osr_loop_nesting_level() const;
-  inline void set_osr_loop_nesting_level(int depth);
+  // The [osr_urgency] controls when OSR is attempted, and is incremented as
+  // the function becomes hotter. When the current loop depth is less than the
+  // osr_urgency, JumpLoop calls into runtime to attempt OSR optimization.
+  static constexpr int kMaxOsrUrgency = 6;
+  inline int osr_urgency() const;
+  inline void set_osr_urgency(int urgency);
+  inline void reset_osr_urgency();
+  inline void RequestOsrAtNextOpportunity();
 
-  // Accessors for bytecode's code age.
   inline Age bytecode_age() const;
   inline void set_bytecode_age(Age age);
 
@@ -999,7 +994,6 @@ class BytecodeArray
   // as it would if no attempt was ever made to collect source positions.
   inline void SetSourcePositionsFailedToCollect();
 
-  // Dispatched behavior.
   inline int BytecodeArraySize();
 
   inline int raw_instruction_size();
@@ -1026,7 +1020,7 @@ class BytecodeArray
   // InterpreterEntryTrampoline expects these fields to be next to each other
   // and writes a 16-bit value to reset them.
   STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                kOsrLoopNestingLevelOffset + kCharSize);
+                kOsrUrgencyOffset + kCharSize);
 
   // Maximal memory consumption for a single BytecodeArray.
   static const int kMaxSize = 512 * MB;
diff --git a/src/objects/code.tq b/src/objects/code.tq
index c51b187107..aa7bcf4798 100644
--- a/src/objects/code.tq
+++ b/src/objects/code.tq
@@ -22,7 +22,9 @@ extern class BytecodeArray extends FixedArrayBase {
   frame_size: int32;
   parameter_size: int32;
   incoming_new_target_or_generator_register: int32;
-  osr_loop_nesting_level: int8;
+  // TODO(jgruber): We only use 3 bits for the urgency; consider folding
+  // into other fields.
+  osr_urgency: int8;
   bytecode_age: int8;
 }
 
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 9c7686a14a..e57ecdaeeb 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -228,18 +228,23 @@ RUNTIME_FUNCTION(Runtime_VerifyType) {
   return *obj;
 }
 
-static bool IsSuitableForOnStackReplacement(Isolate* isolate,
-                                            Handle<JSFunction> function) {
+namespace {
+
+bool IsSuitableForOnStackReplacement(Isolate* isolate,
+                                     Handle<JSFunction> function) {
   // Don't OSR during serialization.
   if (isolate->serializer_enabled()) return false;
+
   // Keep track of whether we've succeeded in optimizing.
   if (function->shared().optimization_disabled()) return false;
+
   // TODO(chromium:1031479): Currently, OSR triggering mechanism is tied to the
   // bytecode array. So, it might be possible to mark closure in one native
   // context and optimize a closure from a different native context. So check if
   // there is a feedback vector before OSRing. We don't expect this to happen
   // often.
   if (!function->has_feedback_vector()) return false;
+
   // If we are trying to do OSR when there are already optimized
   // activations of the function, it means (a) the function is directly or
   // indirectly recursive and (b) an optimized invocation has been
@@ -253,8 +258,6 @@ static bool IsSuitableForOnStackReplacement(Isolate* isolate,
   return true;
 }
 
-namespace {
-
 BytecodeOffset DetermineEntryAndDisarmOSRForUnoptimized(
     JavaScriptFrame* js_frame) {
   UnoptimizedFrame* frame = reinterpret_cast<UnoptimizedFrame*>(js_frame);
@@ -273,8 +276,8 @@ BytecodeOffset DetermineEntryAndDisarmOSRForUnoptimized(
   DCHECK(frame->is_unoptimized());
   DCHECK(frame->function().shared().HasBytecodeArray());
 
-  // Reset the OSR loop nesting depth to disarm back edges.
-  bytecode->set_osr_loop_nesting_level(0);
+  // Disarm all back edges.
+  bytecode->reset_osr_urgency();
 
   // Return a BytecodeOffset representing the bytecode offset of the back
   // branch.
@@ -286,11 +289,9 @@ BytecodeOffset DetermineEntryAndDisarmOSRForUnoptimized(
 RUNTIME_FUNCTION(Runtime_CompileForOnStackReplacement) {
   HandleScope handle_scope(isolate);
   DCHECK_EQ(0, args.length());
+  DCHECK(FLAG_use_osr);
 
-  // Only reachable when OST is enabled.
-  CHECK(FLAG_use_osr);
-
-  // Determine frame triggering OSR request.
+  // Determine the frame that triggered the OSR request.
   JavaScriptFrameIterator it(isolate);
   JavaScriptFrame* frame = it.frame();
   DCHECK(frame->is_unoptimized());
@@ -313,72 +314,75 @@ RUNTIME_FUNCTION(Runtime_CompileForOnStackReplacement) {
         Compiler::GetOptimizedCodeForOSR(isolate, function, osr_offset, frame);
   }
 
-  // Check whether we ended up with usable optimized code.
   Handle<CodeT> result;
-  if (maybe_result.ToHandle(&result) &&
-      CodeKindIsOptimizedJSFunction(result->kind())) {
-    DeoptimizationData data =
-        DeoptimizationData::cast(result->deoptimization_data());
-
-    if (data.OsrPcOffset().value() >= 0) {
-      DCHECK(BytecodeOffset(data.OsrBytecodeOffset().value()) == osr_offset);
-      if (FLAG_trace_osr) {
-        CodeTracer::Scope scope(isolate->GetCodeTracer());
-        PrintF(scope.file(),
-               "[OSR - Entry at OSR bytecode offset %d, offset %d in optimized "
-               "code]\n",
-               osr_offset.ToInt(), data.OsrPcOffset().value());
-      }
-
-      DCHECK(result->is_turbofanned());
-      if (function->feedback_vector().invocation_count() <= 1 &&
-          function->HasOptimizationMarker()) {
-        // With lazy feedback allocation we may not have feedback for the
-        // initial part of the function that was executed before we allocated a
-        // feedback vector. Reset any optimization markers for such functions.
-        //
-        // TODO(mythria): Instead of resetting the optimization marker here we
-        // should only mark a function for optimization if it has sufficient
-        // feedback. We cannot do this currently since we OSR only after we mark
-        // a function for optimization. We should instead change it to be based
-        // based on number of ticks.
-        DCHECK(!function->IsInOptimizationQueue());
-        function->ClearOptimizationMarker();
-      }
-      // TODO(mythria): Once we have OSR code cache we may not need to mark
-      // the function for non-concurrent compilation. We could arm the loops
-      // early so the second execution uses the already compiled OSR code and
-      // the optimization occurs concurrently off main thread.
-      if (!function->HasAvailableOptimizedCode() &&
-          function->feedback_vector().invocation_count() > 1) {
-        // If we're not already optimized, set to optimize non-concurrently on
-        // the next call, otherwise we'd run unoptimized once more and
-        // potentially compile for OSR again.
-        if (FLAG_trace_osr) {
-          CodeTracer::Scope scope(isolate->GetCodeTracer());
-          PrintF(scope.file(), "[OSR - Re-marking ");
-          function->PrintName(scope.file());
-          PrintF(scope.file(), " for non-concurrent optimization]\n");
-        }
-        function->SetOptimizationMarker(
-            OptimizationMarker::kCompileTurbofan_NotConcurrent);
-      }
-      return *result;
+  if (!maybe_result.ToHandle(&result)) {
+    // No OSR'd code available.
+    if (FLAG_trace_osr) {
+      CodeTracer::Scope scope(isolate->GetCodeTracer());
+      PrintF(scope.file(), "[OSR - Failed: ");
+      function->PrintName(scope.file());
+      PrintF(scope.file(), " at OSR bytecode offset %d]\n", osr_offset.ToInt());
     }
+
+    if (!function->HasAttachedOptimizedCode()) {
+      function->set_code(function->shared().GetCode(), kReleaseStore);
+    }
+
+    return {};
   }
 
-  // Failed.
+  DCHECK(!result.is_null());
+  DCHECK(result->is_turbofanned());
+  DCHECK(CodeKindIsOptimizedJSFunction(result->kind()));
+
+  DeoptimizationData data =
+      DeoptimizationData::cast(result->deoptimization_data());
+  DCHECK_EQ(BytecodeOffset(data.OsrBytecodeOffset().value()), osr_offset);
+  DCHECK_GE(data.OsrPcOffset().value(), 0);
+
   if (FLAG_trace_osr) {
     CodeTracer::Scope scope(isolate->GetCodeTracer());
-    PrintF(scope.file(), "[OSR - Failed: ");
-    function->PrintName(scope.file());
-    PrintF(scope.file(), " at OSR bytecode offset %d]\n", osr_offset.ToInt());
+    PrintF(scope.file(),
+           "[OSR - Entry at OSR bytecode offset %d, offset %d in optimized "
+           "code]\n",
+           osr_offset.ToInt(), data.OsrPcOffset().value());
+  }
+
+  if (function->feedback_vector().invocation_count() <= 1 &&
+      function->HasOptimizationMarker()) {
+    // With lazy feedback allocation we may not have feedback for the
+    // initial part of the function that was executed before we allocated a
+    // feedback vector. Reset any optimization markers for such functions.
+    //
+    // TODO(mythria): Instead of resetting the optimization marker here we
+    // should only mark a function for optimization if it has sufficient
+    // feedback. We cannot do this currently since we OSR only after we mark
+    // a function for optimization. We should instead change it to be based
+    // based on number of ticks.
+    DCHECK(!function->IsInOptimizationQueue());
+    function->ClearOptimizationMarker();
   }
 
-  if (!function->HasAttachedOptimizedCode()) {
-    function->set_code(function->shared().GetCode(), kReleaseStore);
+  // TODO(mythria): Once we have OSR code cache we may not need to mark
+  // the function for non-concurrent compilation. We could arm the loops
+  // early so the second execution uses the already compiled OSR code and
+  // the optimization occurs concurrently off main thread.
+  if (!function->HasAvailableOptimizedCode() &&
+      function->feedback_vector().invocation_count() > 1) {
+    // If we're not already optimized, set to optimize non-concurrently on the
+    // next call, otherwise we'd run unoptimized once more and potentially
+    // compile for OSR again.
+    if (FLAG_trace_osr) {
+      CodeTracer::Scope scope(isolate->GetCodeTracer());
+      PrintF(scope.file(), "[OSR - Re-marking ");
+      function->PrintName(scope.file());
+      PrintF(scope.file(), " for non-concurrent optimization]\n");
+    }
+    function->SetOptimizationMarker(
+        OptimizationMarker::kCompileTurbofan_NotConcurrent);
   }
-  return Object();
+
+  return *result;
 }
 
 static Object CompileGlobalEval(Isolate* isolate,
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index a351e85e93..77ae98f332 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -584,8 +584,7 @@ RUNTIME_FUNCTION(Runtime_OptimizeOsr) {
   // Make the profiler arm all back edges in unoptimized code.
   if (it.frame()->is_unoptimized()) {
     isolate->tiering_manager()->AttemptOnStackReplacement(
-        UnoptimizedFrame::cast(it.frame()),
-        AbstractCode::kMaxLoopNestingMarker);
+        UnoptimizedFrame::cast(it.frame()), BytecodeArray::kMaxOsrUrgency);
   }
 
   return ReadOnlyRoots(isolate).undefined_value();
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index 40f1cbdefc..e4cd79c04a 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -560,7 +560,7 @@ void Deserializer<IsolateT>::PostProcessNewObject(Handle<Map> map,
   } else if (InstanceTypeChecker::IsBytecodeArray(instance_type)) {
     // TODO(mythria): Remove these once we store the default values for these
     // fields in the serializer.
-    BytecodeArray::cast(raw_obj).set_osr_loop_nesting_level(0);
+    BytecodeArray::cast(raw_obj).reset_osr_urgency();
   } else if (InstanceTypeChecker::IsDescriptorArray(instance_type)) {
     DCHECK(InstanceTypeChecker::IsStrongDescriptorArray(instance_type));
     Handle<DescriptorArray> descriptors = Handle<DescriptorArray>::cast(obj);
diff --git a/test/cctest/test-serialize.cc b/test/cctest/test-serialize.cc
index 9c19b399df..8c4d6b4722 100644
--- a/test/cctest/test-serialize.cc
+++ b/test/cctest/test-serialize.cc
@@ -2670,7 +2670,7 @@ TEST(CodeSerializerAfterExecute) {
     Handle<SharedFunctionInfo> sfi = v8::Utils::OpenHandle(*script);
     CHECK(sfi->HasBytecodeArray());
     BytecodeArray bytecode = sfi->GetBytecodeArray(i_isolate2);
-    CHECK_EQ(bytecode.osr_loop_nesting_level(), 0);
+    CHECK_EQ(bytecode.osr_urgency(), 0);
 
     {
       DisallowCompilation no_compile_expected(i_isolate2);
-- 
2.35.1

