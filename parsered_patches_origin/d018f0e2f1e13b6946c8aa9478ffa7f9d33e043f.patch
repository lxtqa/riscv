From d018f0e2f1e13b6946c8aa9478ffa7f9d33e043f Mon Sep 17 00:00:00 2001
From: jingpeiyang <jingpeiyang@eswincomputing.com>
Date: Tue, 23 May 2023 19:22:27 +0800
Subject: [PATCH] [riscv32]Implement simd for liftoff and turbofan

Bug: v8:13986

Signed-off-by: jingpeiyang <jingpeiyang@eswincomputing.com>
Change-Id: I231f66d5838a102f91f1f2bdb2fd55dcea66c8f6
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4323697
Reviewed-by: Yahan Lu <yahan@iscas.ac.cn>
Reviewed-by: Michael Achenbach <machenbach@chromium.org>
Reviewed-by: Jakob Linke <jgruber@chromium.org>
Reviewed-by: Thibaud Michaud <thibaudm@chromium.org>
Reviewed-by: Jakob Kummerow <jkummerow@chromium.org>
Reviewed-by: Clemens Backes <clemensb@chromium.org>
Commit-Queue: Yahan Lu <yahan@iscas.ac.cn>
Cr-Commit-Position: refs/heads/main@{#87978}
---
 BUILD.gn                                      |   6 +
 src/codegen/register-configuration.cc         |   5 +-
 src/codegen/riscv/macro-assembler-riscv.cc    |  36 +-
 src/codegen/riscv/macro-assembler-riscv.h     |   2 +-
 src/compiler/backend/code-generator.cc        |   2 +-
 src/compiler/backend/instruction-selector.cc  |   5 +-
 .../backend/riscv/code-generator-riscv.cc     | 174 ++++++-
 .../backend/riscv/instruction-codes-riscv.h   |   2 +
 .../riscv/instruction-scheduler-riscv.cc      |   2 +
 .../riscv/instruction-selector-riscv32.cc     |  17 +
 src/execution/riscv/simulator-riscv.cc        |  23 +-
 src/execution/riscv/simulator-riscv.h         |   6 +-
 .../baseline/riscv/liftoff-assembler-riscv.h  | 109 ++--
 .../riscv/liftoff-assembler-riscv32.h         | 323 ++++++------
 .../riscv/liftoff-assembler-riscv64.h         |  66 +--
 src/wasm/wasm-linkage.h                       |   2 +-
 test/cctest/test-assembler-riscv32.cc         | 483 +++++++++++++-----
 test/wasm-spec-tests/wasm-spec-tests.status   |   3 -
 tools/testrunner/base_runner.py               |   3 +-
 19 files changed, 818 insertions(+), 451 deletions(-)

diff --git a/BUILD.gn b/BUILD.gn
index 8159be8281..7fdb525f82 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -1299,6 +1299,12 @@ config("toolchain") {
     defines += [ "V8_TARGET_ARCH_RISCV32" ]
     defines += [ "__riscv_xlen=32" ]
     defines += [ "CAN_USE_FPU_INSTRUCTIONS" ]
+
+    # TODO(riscv32): Add condition riscv_use_rvv and riscv_rvv_vlen here after
+    # 4538202 merge.
+    if (target_is_simulator) {
+      defines += [ "CAN_USE_RVV_INSTRUCTIONS" ]
+    }
   }
 
   if (v8_current_cpu == "x86") {
diff --git a/src/codegen/register-configuration.cc b/src/codegen/register-configuration.cc
index 8b7b9f0010..67561fb13c 100644
--- a/src/codegen/register-configuration.cc
+++ b/src/codegen/register-configuration.cc
@@ -65,11 +65,12 @@ static_assert(RegisterConfiguration::kMaxFPRegisters >=
 
 static int get_num_simd128_registers() {
   return
-#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
       Simd128Register::kNumRegisters;
 #else
       0;
-#endif  // V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#endif  // V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 ||
+        // V8_TARGET_ARCH_PPC64
 }
 
 static int get_num_simd256_registers() { return 0; }
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 4578eb4468..9b19c9f513 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -3106,8 +3106,8 @@ void MacroAssembler::RoundHelper(VRegister dst, VRegister src, Register scratch,
     } else {
 #ifdef V8_TARGET_ARCH_RISCV64
       fmv_d_x(kScratchDoubleReg, zero_reg);
-#else
-      UNIMPLEMENTED();
+#elif V8_TARGET_ARCH_RISCV32
+      fcvt_d_w(kScratchDoubleReg, zero_reg);
 #endif
     }
     vfadd_vf(dst, src, kScratchDoubleReg, MaskType::Mask);
@@ -5127,6 +5127,7 @@ void MacroAssembler::WasmRvvGtU(VRegister dst, VRegister lhs, VRegister rhs,
   vmerge_vx(dst, kScratchReg, dst);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void MacroAssembler::WasmRvvS128const(VRegister dst, const uint8_t imms[16]) {
   uint64_t vals[2];
   memcpy(vals, imms, sizeof(vals));
@@ -5137,12 +5138,28 @@ void MacroAssembler::WasmRvvS128const(VRegister dst, const uint8_t imms[16]) {
   li(kScratchReg, vals[0]);
   vmv_sx(dst, kScratchReg);
 }
+#elif V8_TARGET_ARCH_RISCV32
+void MacroAssembler::WasmRvvS128const(VRegister dst, const uint8_t imms[16]) {
+  uint32_t vals[4];
+  memcpy(vals, imms, sizeof(vals));
+  VU.set(kScratchReg, VSew::E32, Vlmul::m1);
+  li(kScratchReg, vals[3]);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, vals[2]);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, vals[1]);
+  vmv_vx(dst, kScratchReg);
+  li(kScratchReg, vals[0]);
+  vmv_sx(dst, kScratchReg);
+  vslideup_vi(dst, kSimd128ScratchReg, 2);
+}
+#endif
 
 void MacroAssembler::LoadLane(int ts, VRegister dst, uint8_t laneidx,
                               MemOperand src) {
   if (ts == 8) {
     Lbu(kScratchReg2, src);
-    VU.set(kScratchReg, E64, m1);
+    VU.set(kScratchReg, E32, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
     VU.set(kScratchReg, E8, m1);
@@ -5160,11 +5177,19 @@ void MacroAssembler::LoadLane(int ts, VRegister dst, uint8_t laneidx,
     vmv_sx(v0, kScratchReg);
     vmerge_vx(dst, kScratchReg2, dst);
   } else if (ts == 64) {
+#if V8_TARGET_ARCH_RISCV64
     LoadWord(kScratchReg2, src);
     VU.set(kScratchReg, E64, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
     vmerge_vx(dst, kScratchReg2, dst);
+#elif V8_TARGET_ARCH_RISCV32
+    LoadDouble(kScratchDoubleReg, src);
+    VU.set(kScratchReg, E64, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vfmerge_vf(dst, kScratchDoubleReg, dst);
+#endif
   } else {
     UNREACHABLE();
   }
@@ -5191,8 +5216,13 @@ void MacroAssembler::StoreLane(int sz, VRegister src, uint8_t laneidx,
     DCHECK_EQ(sz, 64);
     VU.set(kScratchReg, E64, m1);
     vslidedown_vi(kSimd128ScratchReg, src, laneidx);
+#if V8_TARGET_ARCH_RISCV64
     vmv_xs(kScratchReg, kSimd128ScratchReg);
     StoreWord(kScratchReg, dst);
+#elif V8_TARGET_ARCH_RISCV32
+    vfmv_fs(kScratchDoubleReg, kSimd128ScratchReg);
+    StoreDouble(kScratchDoubleReg, dst);
+#endif
   }
 }
 // -----------------------------------------------------------------------------
diff --git a/src/codegen/riscv/macro-assembler-riscv.h b/src/codegen/riscv/macro-assembler-riscv.h
index 2c23eacf61..139d67d69e 100644
--- a/src/codegen/riscv/macro-assembler-riscv.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -1173,7 +1173,6 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 
   void WasmRvvEq(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,
                  Vlmul lmul);
-
   void WasmRvvNe(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,
                  Vlmul lmul);
   void WasmRvvGeS(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,
@@ -1184,6 +1183,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
                   Vlmul lmul);
   void WasmRvvGtU(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,
                   Vlmul lmul);
+
   void WasmRvvS128const(VRegister dst, const uint8_t imms[16]);
 
   void LoadLane(int sz, VRegister dst, uint8_t laneidx, MemOperand src);
diff --git a/src/compiler/backend/code-generator.cc b/src/compiler/backend/code-generator.cc
index 63d912abd2..ef033ec414 100644
--- a/src/compiler/backend/code-generator.cc
+++ b/src/compiler/backend/code-generator.cc
@@ -313,7 +313,7 @@ void CodeGenerator::AssembleCode() {
         masm()->InitializeRootRegister();
       }
     }
-#ifdef V8_TARGET_ARCH_RISCV64
+#ifdef CAN_USE_RVV_INSTRUCTIONS
     // RVV uses VectorUnit to emit vset{i}vl{i}, reducing the static and dynamic
     // overhead of the vset{i}vl{i} instruction. However there are some jumps
     // back between blocks. the Rvv instruction may get an incorrect vtype. so
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index 319f9f8be3..c5d33b81f1 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -3076,7 +3076,7 @@ void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
         // !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_S390 &&
         // !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_LOONG64
 
-#if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM
+#if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_RISCV32
 // This is only needed on 32-bit to split the 64-bit value into two operands.
 void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
   UNIMPLEMENTED();
@@ -3084,7 +3084,8 @@ void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
 void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
   UNIMPLEMENTED();
 }
-#endif  // !V8_TARGET_ARCH_IA32
+#endif  // !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM &&
+        // !V8_TARGET_ARCH_RISCV32
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
 #if !V8_TARGET_ARCH_ARM64
diff --git a/src/compiler/backend/riscv/code-generator-riscv.cc b/src/compiler/backend/riscv/code-generator-riscv.cc
index bdd3c08ae1..958dd70b90 100644
--- a/src/compiler/backend/riscv/code-generator-riscv.cc
+++ b/src/compiler/backend/riscv/code-generator-riscv.cc
@@ -543,11 +543,6 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
     __ VU.set(kScratchReg, E32, m1);                      \
     __ OP(i.OutputSimd128Register(), i.InputRegister(0)); \
     break;                                                \
-  }                                                       \
-  case kRiscvI64x2##instr: {                              \
-    __ VU.set(kScratchReg, E64, m1);                      \
-    __ OP(i.OutputSimd128Register(), i.InputRegister(0)); \
-    break;                                                \
   }
 
 #define ASSEMBLE_RVV_UNOP_INTEGER_VV(instr, OP)                  \
@@ -2260,8 +2255,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvS128Load64Zero: {
       Simd128Register dst = i.OutputSimd128Register();
       __ VU.set(kScratchReg, E64, m1);
+#if V8_TARGET_ARCH_RISCV64
       __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_sx(dst, kScratchReg);
+#elif V8_TARGET_ARCH_RISCV32
+      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());
+      __ vfmv_sf(dst, kScratchDoubleReg);
+#endif
       break;
     }
     case kRiscvS128LoadLane: {
@@ -2280,16 +2280,26 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kRiscvS128Load64ExtendS: {
       __ VU.set(kScratchReg, E64, m1);
+#if V8_TARGET_ARCH_RISCV64
       __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_vx(kSimd128ScratchReg, kScratchReg);
+#elif V8_TARGET_ARCH_RISCV32
+      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());
+      __ vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
+#endif
       __ VU.set(kScratchReg, i.InputInt8(2), m1);
       __ vsext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);
       break;
     }
     case kRiscvS128Load64ExtendU: {
       __ VU.set(kScratchReg, E64, m1);
+#if V8_TARGET_ARCH_RISCV64
       __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_vx(kSimd128ScratchReg, kScratchReg);
+#elif V8_TARGET_ARCH_RISCV32
+      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());
+      __ vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
+#endif
       __ VU.set(kScratchReg, i.InputInt8(2), m1);
       __ vzext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);
       break;
@@ -2299,20 +2309,28 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       switch (i.InputInt8(2)) {
         case E8:
           __ Lb(kScratchReg, i.MemoryOperand());
+          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
           break;
         case E16:
           __ Lh(kScratchReg, i.MemoryOperand());
+          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
           break;
         case E32:
           __ Lw(kScratchReg, i.MemoryOperand());
+          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
           break;
         case E64:
+#if V8_TARGET_ARCH_RISCV64
           __ LoadWord(kScratchReg, i.MemoryOperand());
+          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
+#elif V8_TARGET_ARCH_RISCV32
+          __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());
+          __ vfmv_vf(i.OutputSimd128Register(), kScratchDoubleReg);
+#endif
           break;
         default:
           UNREACHABLE();
       }
-      __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
       break;
     }
     case kRiscvS128AllOnes: {
@@ -2389,11 +2407,23 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       if (!(instr->InputAt(1)->IsImmediate())) {
         index = i.InputSimd128Register(1);
       } else {
+#if V8_TARGET_ARCH_RISCV64
         __ VU.set(kScratchReg, E64, m1);
         __ li(kScratchReg, i.InputInt64(1));
         __ vmv_vi(kSimd128ScratchReg3, -1);
         __ vmv_sx(kSimd128ScratchReg3, kScratchReg);
         index = kSimd128ScratchReg3;
+#elif V8_TARGET_ARCH_RISCV32
+        int64_t intput_int64 = i.InputInt64(1);
+        int32_t input_int32[2];
+        memcpy(input_int32, &intput_int64, sizeof(intput_int64));
+        __ VU.set(kScratchReg, E32, m1);
+        __ li(kScratchReg, input_int32[1]);
+        __ vmv_vx(kSimd128ScratchReg3, kScratchReg);
+        __ li(kScratchReg, input_int32[0]);
+        __ vmv_sx(kSimd128ScratchReg3, kScratchReg);
+        index = kSimd128ScratchReg3;
+#endif
       }
       __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));
       if (i.OutputSimd128Register() == i.InputSimd128Register(0)) {
@@ -2515,8 +2545,8 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),
                        i.InputInt8(1));
       __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);
-      __ slli(i.OutputRegister(), i.OutputRegister(), 64 - 8);
-      __ srli(i.OutputRegister(), i.OutputRegister(), 64 - 8);
+      __ slli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 8);
+      __ srli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 8);
       break;
     }
     case kRiscvI8x16ExtractLaneS: {
@@ -2531,8 +2561,8 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),
                        i.InputInt8(1));
       __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);
-      __ slli(i.OutputRegister(), i.OutputRegister(), 64 - 16);
-      __ srli(i.OutputRegister(), i.OutputRegister(), 64 - 16);
+      __ slli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 16);
+      __ srli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 16);
       break;
     }
     case kRiscvI16x8ExtractLaneS: {
@@ -2719,11 +2749,25 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                  MaskType::Mask);
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvI64x2ExtractLane: {
       __ WasmRvvExtractLane(i.OutputRegister(), i.InputSimd128Register(0),
                             i.InputInt8(1), E64, m1);
       break;
     }
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvI64x2ExtractLane: {
+      uint8_t imm_lane_idx = i.InputInt8(1);
+      __ VU.set(kScratchReg, E32, m1);
+      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),
+                       (imm_lane_idx << 0x1) + 1);
+      __ vmv_xs(i.OutputRegister(1), kSimd128ScratchReg);
+      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),
+                       (imm_lane_idx << 0x1));
+      __ vmv_xs(i.OutputRegister(0), kSimd128ScratchReg);
+      break;
+    }
+#endif
     case kRiscvI8x16Eq: {
       __ WasmRvvEq(i.OutputSimd128Register(), i.InputSimd128Register(0),
                    i.InputSimd128Register(1), E8, m1);
@@ -2907,6 +2951,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vmerge_vx(dst, i.InputRegister(2), src);
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvI64x2ReplaceLane: {
       Simd128Register src = i.InputSimd128Register(0);
       Simd128Register dst = i.OutputSimd128Register();
@@ -2916,6 +2961,23 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vmerge_vx(dst, i.InputRegister(2), src);
       break;
     }
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvI64x2ReplaceLaneI32Pair: {
+      Simd128Register src = i.InputSimd128Register(0);
+      Simd128Register dst = i.OutputSimd128Register();
+      Register int64_low = i.InputRegister(2);
+      Register int64_high = i.InputRegister(3);
+      __ VU.set(kScratchReg, E32, m1);
+      __ vmv_vx(kSimd128ScratchReg, int64_high);
+      __ vmv_sx(kSimd128ScratchReg, int64_low);
+      __ VU.set(kScratchReg, E64, m1);
+      __ li(kScratchReg, 0x1 << i.InputInt8(1));
+      __ vmv_sx(v0, kScratchReg);
+      __ vfmv_fs(kScratchDoubleReg, kSimd128ScratchReg);
+      __ vfmerge_vf(dst, kScratchDoubleReg, src);
+      break;
+    }
+#endif
     case kRiscvI32x4ReplaceLane: {
       Simd128Register src = i.InputSimd128Register(0);
       Simd128Register dst = i.OutputSimd128Register();
@@ -2984,15 +3046,14 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvI64x2AllTrue: {
       __ VU.set(kScratchReg, E64, m1);
       Register dst = i.OutputRegister();
-      Label all_true;
-      __ li(kScratchReg, -1);
-      __ vmv_sx(kSimd128ScratchReg, kScratchReg);
+      Label notalltrue;
+      __ vmv_vi(kSimd128ScratchReg, -1);
       __ vredminu_vs(kSimd128ScratchReg, i.InputSimd128Register(0),
                      kSimd128ScratchReg);
       __ vmv_xs(dst, kSimd128ScratchReg);
-      __ beqz(dst, &all_true);
+      __ beqz(dst, &notalltrue);
       __ li(dst, 1);
-      __ bind(&all_true);
+      __ bind(&notalltrue);
       break;
     }
     case kRiscvI32x4AllTrue: {
@@ -3012,29 +3073,27 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvI16x8AllTrue: {
       __ VU.set(kScratchReg, E16, m1);
       Register dst = i.OutputRegister();
-      Label all_true;
-      __ li(kScratchReg, -1);
-      __ vmv_sx(kSimd128ScratchReg, kScratchReg);
+      Label notalltrue;
+      __ vmv_vi(kSimd128ScratchReg, -1);
       __ vredminu_vs(kSimd128ScratchReg, i.InputSimd128Register(0),
                      kSimd128ScratchReg);
       __ vmv_xs(dst, kSimd128ScratchReg);
-      __ beqz(dst, &all_true);
+      __ beqz(dst, &notalltrue);
       __ li(dst, 1);
-      __ bind(&all_true);
+      __ bind(&notalltrue);
       break;
     }
     case kRiscvI8x16AllTrue: {
       __ VU.set(kScratchReg, E8, m1);
       Register dst = i.OutputRegister();
-      Label all_true;
-      __ li(kScratchReg, -1);
-      __ vmv_sx(kSimd128ScratchReg, kScratchReg);
+      Label notalltrue;
+      __ vmv_vi(kSimd128ScratchReg, -1);
       __ vredminu_vs(kSimd128ScratchReg, i.InputSimd128Register(0),
                      kSimd128ScratchReg);
       __ vmv_xs(dst, kSimd128ScratchReg);
-      __ beqz(dst, &all_true);
+      __ beqz(dst, &notalltrue);
       __ li(dst, 1);
-      __ bind(&all_true);
+      __ bind(&notalltrue);
       break;
     }
     case kRiscvI8x16Shuffle: {
@@ -3042,6 +3101,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                 src0 = i.InputSimd128Register(0),
                 src1 = i.InputSimd128Register(1);
 
+#if V8_TARGET_ARCH_RISCV64
       int64_t imm1 = make_uint64(i.InputInt32(3), i.InputInt32(2));
       int64_t imm2 = make_uint64(i.InputInt32(5), i.InputInt32(4));
       __ VU.set(kScratchReg, VSew::E64, Vlmul::m1);
@@ -3050,6 +3110,18 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
       __ li(kScratchReg, imm1);
       __ vmv_sx(kSimd128ScratchReg, kScratchReg);
+#elif V8_TARGET_ARCH_RISCV32
+      __ VU.set(kScratchReg, VSew::E32, Vlmul::m1);
+      __ li(kScratchReg, i.InputInt32(5));
+      __ vmv_vx(kSimd128ScratchReg2, kScratchReg);
+      __ li(kScratchReg, i.InputInt32(4));
+      __ vmv_sx(kSimd128ScratchReg2, kScratchReg);
+      __ li(kScratchReg, i.InputInt32(3));
+      __ vmv_vx(kSimd128ScratchReg, kScratchReg);
+      __ li(kScratchReg, i.InputInt32(2));
+      __ vmv_sx(kSimd128ScratchReg, kScratchReg);
+      __ vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 2);
+#endif
 
       __ VU.set(kScratchReg, E8, m1);
       if (dst == src0) {
@@ -3215,6 +3287,39 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);
       break;
     }
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvF64x2Min: {
+      __ VU.set(kScratchReg, E64, m1);
+      const int32_t kNaN = 0x7ff80000L, kNaNShift = 32;
+      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));
+      __ vmfeq_vv(kSimd128ScratchReg, i.InputSimd128Register(1),
+                  i.InputSimd128Register(1));
+      __ vand_vv(v0, v0, kSimd128ScratchReg);
+      __ li(kScratchReg, kNaN);
+      __ li(kScratchReg2, kNaNShift);
+      __ vmv_vx(kSimd128ScratchReg, kScratchReg);
+      __ vsll_vx(kSimd128ScratchReg, kSimd128ScratchReg, kScratchReg2);
+      __ vfmin_vv(kSimd128ScratchReg, i.InputSimd128Register(1),
+                  i.InputSimd128Register(0), Mask);
+      __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);
+      break;
+    }
+    case kRiscvF64x2Max: {
+      __ VU.set(kScratchReg, E64, m1);
+      const int32_t kNaN = 0x7ff80000L, kNaNShift = 32;
+      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));
+      __ vmfeq_vv(kSimd128ScratchReg, i.InputSimd128Register(1),
+                  i.InputSimd128Register(1));
+      __ vand_vv(v0, v0, kSimd128ScratchReg);
+      __ li(kScratchReg, kNaN);
+      __ li(kScratchReg2, kNaNShift);
+      __ vmv_vx(kSimd128ScratchReg, kScratchReg);
+      __ vsll_vx(kSimd128ScratchReg, kSimd128ScratchReg, kScratchReg2);
+      __ vfmax_vv(kSimd128ScratchReg, i.InputSimd128Register(1),
+                  i.InputSimd128Register(0), Mask);
+      __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);
+      break;
+    }
 #endif
     case kRiscvF64x2Div: {
       __ VU.set(kScratchReg, E64, m1);
@@ -3327,8 +3432,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kRiscvF32x4Splat: {
       (__ VU).set(kScratchReg, E32, m1);
-      __ fmv_x_w(kScratchReg, i.InputSingleRegister(0));
-      __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
+      __ vfmv_vf(i.OutputSimd128Register(), i.InputDoubleRegister(0));
       break;
     }
     case kRiscvF32x4Add: {
@@ -3634,6 +3738,22 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       ASSEMBLE_RVV_UNOP_INTEGER_VR(Splat, vmv_vx)
       ASSEMBLE_RVV_BINOP_INTEGER(Add, vadd_vv)
       ASSEMBLE_RVV_BINOP_INTEGER(Sub, vsub_vv)
+#if V8_TARGET_ARCH_RISCV64
+    case kRiscvI64x2Splat: {
+      __ VU.set(kScratchReg, E64, m1);
+      __ vmv_vx(i.OutputSimd128Register(), i.InputRegister(0));
+      break;
+    }
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvI64x2SplatI32Pair: {
+      __ VU.set(kScratchReg, E32, m1);
+      __ vmv_vi(v0, 0b0101);
+      __ vmv_vx(kSimd128ScratchReg, i.InputRegister(1));
+      __ vmerge_vx(i.OutputSimd128Register(), i.InputRegister(0),
+                   kSimd128ScratchReg);
+      break;
+    }
+#endif
     case kRiscvVwadd: {
       __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));
       __ vwadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),
diff --git a/src/compiler/backend/riscv/instruction-codes-riscv.h b/src/compiler/backend/riscv/instruction-codes-riscv.h
index 9e27bbb829..56441bb776 100644
--- a/src/compiler/backend/riscv/instruction-codes-riscv.h
+++ b/src/compiler/backend/riscv/instruction-codes-riscv.h
@@ -262,8 +262,10 @@ namespace compiler {
   V(RiscvF64x2Trunc)                      \
   V(RiscvF64x2NearestInt)                 \
   V(RiscvI64x2Splat)                      \
+  V(RiscvI64x2SplatI32Pair)               \
   V(RiscvI64x2ExtractLane)                \
   V(RiscvI64x2ReplaceLane)                \
+  V(RiscvI64x2ReplaceLaneI32Pair)         \
   V(RiscvI64x2Add)                        \
   V(RiscvI64x2Sub)                        \
   V(RiscvI64x2Mul)                        \
diff --git a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
index 4d0ec6a1dc..0c630c2843 100644
--- a/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
+++ b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
@@ -120,8 +120,10 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvF64x2Trunc:
     case kRiscvF64x2NearestInt:
     case kRiscvI64x2Splat:
+    case kRiscvI64x2SplatI32Pair:
     case kRiscvI64x2ExtractLane:
     case kRiscvI64x2ReplaceLane:
+    case kRiscvI64x2ReplaceLaneI32Pair:
     case kRiscvI64x2Add:
     case kRiscvI64x2Sub:
     case kRiscvI64x2Mul:
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv32.cc b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
index 68ff98b0d4..78ed60c534 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv32.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
@@ -1138,6 +1138,23 @@ void InstructionSelector::VisitInt32PairMul(Node* node) {
   VisitInt32PairBinop<4>(this, kRiscvMulPair, kRiscvMul32, node);
 }
 
+void InstructionSelector::VisitI64x2SplatI32Pair(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand low = g.UseRegister(node->InputAt(0));
+  InstructionOperand high = g.UseRegister(node->InputAt(1));
+  Emit(kRiscvI64x2SplatI32Pair, g.DefineAsRegister(node), low, high);
+}
+
+void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand operand = g.UseRegister(node->InputAt(0));
+  InstructionOperand lane = g.UseImmediate(OpParameter<int32_t>(node->op()));
+  InstructionOperand low = g.UseRegister(node->InputAt(1));
+  InstructionOperand high = g.UseRegister(node->InputAt(2));
+  Emit(kRiscvI64x2ReplaceLaneI32Pair, g.DefineSameAsFirst(node), operand, lane,
+       low, high);
+}
+
 // Shared routine for multiple shift operations.
 static void VisitWord32PairShift(InstructionSelector* selector,
                                  InstructionCode opcode, Node* node) {
diff --git a/src/execution/riscv/simulator-riscv.cc b/src/execution/riscv/simulator-riscv.cc
index d670a05f56..b2a284eb74 100644
--- a/src/execution/riscv/simulator-riscv.cc
+++ b/src/execution/riscv/simulator-riscv.cc
@@ -1291,9 +1291,7 @@ struct type_sew_t<128> {
   set_rvv_vstart(0);                                                           \
   if (v8_flags.trace_sim) {                                                    \
     __int128_t value = Vregister_[rvv_vd_reg()];                               \
-    SNPrintF(trace_buf_,                                                       \
-             "%016" REGIx_FORMAT "%016" REGIx_FORMAT                           \
-             " <-- 0x%016" REGIx_FORMAT,                                       \
+    SNPrintF(trace_buf_, "%016" PRIx64 "%016" PRIx64 " <-- 0x%016" PRIx64,     \
              *(reinterpret_cast<int64_t*>(&value) + 1),                        \
              *reinterpret_cast<int64_t*>(&value),                              \
              (uint64_t)(get_register(rs1_reg())));                             \
@@ -1317,9 +1315,7 @@ struct type_sew_t<128> {
   set_rvv_vstart(0);                                                           \
   if (v8_flags.trace_sim) {                                                    \
     __int128_t value = Vregister_[rvv_vd_reg()];                               \
-    SNPrintF(trace_buf_,                                                       \
-             "%016" REGIx_FORMAT "%016" REGIx_FORMAT                           \
-             " --> 0x%016" REGIx_FORMAT,                                       \
+    SNPrintF(trace_buf_, "%016" PRIx64 "%016" PRIx64 " --> 0x%016" PRIx64,     \
              *(reinterpret_cast<int64_t*>(&value) + 1),                        \
              *reinterpret_cast<int64_t*>(&value),                              \
              (uint64_t)(get_register(rs1_reg())));                             \
@@ -1392,14 +1388,14 @@ static inline uint8_t get_round(int vxrm, uint64_t v, uint8_t shift) {
 
 template <typename Src, typename Dst>
 inline Dst signed_saturation(Src v, uint n) {
-  Dst smax = (Dst)(INTPTR_MAX >> (64 - n));
-  Dst smin = (Dst)(INTPTR_MIN >> (64 - n));
+  Dst smax = (Dst)(INTPTR_MAX >> (sizeof(intptr_t) * 8 - n));
+  Dst smin = (Dst)(INTPTR_MIN >> (sizeof(intptr_t) * 8 - n));
   return (v > smax) ? smax : ((v < smin) ? smin : (Dst)v);
 }
 
 template <typename Src, typename Dst>
 inline Dst unsigned_saturation(Src v, uint n) {
-  Dst umax = (Dst)(UINTPTR_MAX >> (64 - n));
+  Dst umax = (Dst)(UINTPTR_MAX >> (sizeof(uintptr_t) * 8 - n));
   return (v > umax) ? umax : ((v < 0) ? 0 : (Dst)v);
 }
 
@@ -1853,7 +1849,7 @@ void RiscvDebugger::Debug() {
 #ifdef CAN_USE_RVV_INSTRUCTIONS
             } else if (vregnum != kInvalidVRegister) {
               __int128_t v = GetVRegisterValue(vregnum);
-              PrintF("\t%s:0x%016" REGIx_FORMAT "%016" REGIx_FORMAT "\n",
+              PrintF("\t%s:0x%016" PRIx64 "%016" PRIx64 "\n",
                      VRegisters::Name(vregnum), (uint64_t)(v >> 64),
                      (uint64_t)v);
 #endif
@@ -6328,6 +6324,7 @@ void Simulator::DecodeRvvMVV() {
     }
     case RO_V_VWXUNARY0: {
       if (rvv_vs1_reg() == 0) {
+        // vmv.x.s
         switch (rvv_vsew()) {
           case E8:
             set_rd(Rvvelt<type_sew_t<8>::type>(rvv_vs2_reg(), 0));
@@ -6347,7 +6344,8 @@ void Simulator::DecodeRvvMVV() {
         set_rvv_vstart(0);
         rvv_trace_vd();
       } else if (rvv_vs1_reg() == 0b10000) {
-        uint64_t cnt = 0;
+        // vpopc
+        reg_t cnt = 0;
         RVV_VI_GENERAL_LOOP_BASE
         RVV_VI_LOOP_MASK_SKIP()
         const uint8_t idx = i / 64;
@@ -6358,7 +6356,8 @@ void Simulator::DecodeRvvMVV() {
         set_register(rd_reg(), cnt);
         rvv_trace_vd();
       } else if (rvv_vs1_reg() == 0b10001) {
-        int64_t index = -1;
+        // vfirst
+        sreg_t index = -1;
         RVV_VI_GENERAL_LOOP_BASE
         RVV_VI_LOOP_MASK_SKIP()
         const uint8_t idx = i / 64;
diff --git a/src/execution/riscv/simulator-riscv.h b/src/execution/riscv/simulator-riscv.h
index c744bf9921..9f32b2e6f7 100644
--- a/src/execution/riscv/simulator-riscv.h
+++ b/src/execution/riscv/simulator-riscv.h
@@ -70,6 +70,8 @@ T Nabs(T a) {
 }
 
 #if defined(USE_SIMULATOR)
+typedef signed __int128_t __attribute__((__mode__(__TI__)));
+typedef unsigned __uint128_t __attribute__((__mode__(__TI__)));
 // Running with a simulator.
 
 #include "src/base/hashmap.h"
@@ -842,8 +844,8 @@ class Simulator : public SimulatorBase {
         if (trace_buf_[i] == '\0') break;
       }
       SNPrintF(trace_buf_.SubVector(i, trace_buf_.length()),
-               "  sew:%s lmul:%s vstart:%lu vl:%lu", rvv_sew_s(), rvv_lmul_s(),
-               rvv_vstart(), rvv_vl());
+               "  sew:%s lmul:%s vstart:%" PRId64 "vl:%" PRId64, rvv_sew_s(),
+               rvv_lmul_s(), rvv_vstart(), rvv_vl());
     }
   }
 
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
index e7f15bfe48..41fe865dca 100644
--- a/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
@@ -367,6 +367,35 @@ void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,
   bind(&done);
 }
 
+void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
+                                          LiftoffRegister lhs,
+                                          LiftoffRegister rhs,
+                                          const uint8_t shuffle[16],
+                                          bool is_swizzle) {
+  VRegister dst_v = dst.fp().toV();
+  VRegister lhs_v = lhs.fp().toV();
+  VRegister rhs_v = rhs.fp().toV();
+
+  WasmRvvS128const(kSimd128ScratchReg2, shuffle);
+
+  VU.set(kScratchReg, E8, m1);
+  VRegister temp =
+      GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
+  if (dst_v == lhs_v) {
+    vmv_vv(temp, lhs_v);
+    lhs_v = temp;
+  } else if (dst_v == rhs_v) {
+    vmv_vv(temp, rhs_v);
+    rhs_v = temp;
+  }
+  vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg2);
+  vadd_vi(kSimd128ScratchReg2, kSimd128ScratchReg2,
+          -16);  // The indices in range [16, 31] select the i - 16-th element
+                 // of rhs
+  vrgather_vv(kSimd128ScratchReg, rhs_v, kSimd128ScratchReg2);
+  vor_vv(dst_v, dst_v, kSimd128ScratchReg);
+}
+
 void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,
                                           LiftoffRegister lhs,
                                           LiftoffRegister rhs) {
@@ -410,12 +439,6 @@ void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,
   vmv_vx(dst.fp().toV(), src.gp());
 }
 
-void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,
-                                        LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  vmv_vx(dst.fp().toV(), src.gp());
-}
-
 void LiftoffAssembler::emit_i64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs) {
   WasmRvvEq(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E64, m1);
@@ -439,8 +462,13 @@ void LiftoffAssembler::emit_i64x2_ge_s(LiftoffRegister dst, LiftoffRegister lhs,
 void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,
                                         LiftoffRegister src) {
   VU.set(kScratchReg, E32, m1);
-  fmv_x_w(kScratchReg, src.fp());
-  vmv_vx(dst.fp().toV(), kScratchReg);
+  vfmv_vf(dst.fp().toV(), src.fp());
+}
+
+void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  vfmv_vf(dst.fp().toV(), src.fp());
 }
 
 void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,
@@ -1107,14 +1135,13 @@ void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,
 void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,
                                           LiftoffRegister src) {
   VU.set(kScratchReg, E16, m1);
-  Label alltrue;
-  li(kScratchReg, -1);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  Label notalltrue;
+  vmv_vi(kSimd128ScratchReg, -1);
   vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
-  beqz(dst.gp(), &alltrue);
+  beqz(dst.gp(), &notalltrue);
   li(dst.gp(), 1);
-  bind(&alltrue);
+  bind(&notalltrue);
 }
 
 void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,
@@ -1252,14 +1279,13 @@ void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,
 void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,
                                           LiftoffRegister src) {
   VU.set(kScratchReg, E32, m1);
-  Label alltrue;
-  li(kScratchReg, -1);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  Label notalltrue;
+  vmv_vi(kSimd128ScratchReg, -1);
   vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
-  beqz(dst.gp(), &alltrue);
+  beqz(dst.gp(), &notalltrue);
   li(dst.gp(), 1);
-  bind(&alltrue);
+  bind(&notalltrue);
 }
 
 void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,
@@ -1459,14 +1485,13 @@ void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,
 void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,
                                           LiftoffRegister src) {
   VU.set(kScratchReg, E64, m1);
-  Label alltrue;
-  li(kScratchReg, -1);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  Label notalltrue;
+  vmv_vi(kSimd128ScratchReg, -1);
   vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
-  beqz(dst.gp(), &alltrue);
+  beqz(dst.gp(), &notalltrue);
   li(dst.gp(), 1);
-  bind(&alltrue);
+  bind(&notalltrue);
 }
 
 void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,
@@ -1971,40 +1996,40 @@ void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,
   vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);
 }
 
-void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,
+void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,
                                                  LiftoffRegister lhs,
                                                  uint8_t imm_lane_idx) {
   VU.set(kScratchReg, E8, m1);
   vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
+  slli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 8);
+  srli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 8);
 }
 
-void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,
+void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,
                                                  LiftoffRegister lhs,
                                                  uint8_t imm_lane_idx) {
   VU.set(kScratchReg, E8, m1);
   vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
-  slli(dst.gp(), dst.gp(), 64 - 8);
-  srli(dst.gp(), dst.gp(), 64 - 8);
 }
 
-void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,
+void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,
                                                  LiftoffRegister lhs,
                                                  uint8_t imm_lane_idx) {
   VU.set(kScratchReg, E16, m1);
   vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
+  slli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 16);
+  srli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 16);
 }
 
-void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,
+void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,
                                                  LiftoffRegister lhs,
                                                  uint8_t imm_lane_idx) {
   VU.set(kScratchReg, E16, m1);
   vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
   vmv_xs(dst.gp(), kSimd128ScratchReg);
-  slli(dst.gp(), dst.gp(), 64 - 16);
-  srli(dst.gp(), dst.gp(), 64 - 16);
 }
 
 void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,
@@ -2015,14 +2040,6 @@ void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,
   vmv_xs(dst.gp(), kSimd128ScratchReg);
 }
 
-void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,
-                                               LiftoffRegister lhs,
-                                               uint8_t imm_lane_idx) {
-  VU.set(kScratchReg, E64, m1);
-  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
-  vmv_xs(dst.gp(), kSimd128ScratchReg);
-}
-
 void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,
                                                LiftoffRegister lhs,
                                                uint8_t imm_lane_idx) {
@@ -2070,25 +2087,25 @@ void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,
   vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());
 }
 
-void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,
+void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,
                                                LiftoffRegister src1,
                                                LiftoffRegister src2,
                                                uint8_t imm_lane_idx) {
-  VU.set(kScratchReg, E64, m1);
+  VU.set(kScratchReg, E32, m1);
   li(kScratchReg, 0x1 << imm_lane_idx);
   vmv_sx(v0, kScratchReg);
-  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());
+  fmv_x_w(kScratchReg, src2.fp());
+  vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
 }
 
-void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,
+void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
                                                LiftoffRegister src1,
                                                LiftoffRegister src2,
                                                uint8_t imm_lane_idx) {
-  VU.set(kScratchReg, E32, m1);
+  VU.set(kScratchReg, E64, m1);
   li(kScratchReg, 0x1 << imm_lane_idx);
   vmv_sx(v0, kScratchReg);
-  fmv_x_w(kScratchReg, src2.fp());
-  vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
+  vfmerge_vf(dst.fp().toV(), src2.fp(), src1.fp().toV());
 }
 
 void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
index 00fd61cc22..4a76868e3a 100644
--- a/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
@@ -59,26 +59,30 @@ inline MemOperand GetHalfStackSlot(int offset, RegPairHalf half) {
 
 inline MemOperand GetMemOp(LiftoffAssembler* assm, Register addr,
                            Register offset, uintptr_t offset_imm,
-                           Register scratch, unsigned shift_amount = 0) {
-  DCHECK_NE(scratch, kScratchReg2);
+                           unsigned shift_amount = 0) {
   DCHECK_NE(addr, kScratchReg2);
   DCHECK_NE(offset, kScratchReg2);
-  if (offset != no_reg) {
+  if (is_uint31(offset_imm)) {
+    int32_t offset_imm32 = static_cast<int32_t>(offset_imm);
+    if (offset == no_reg) return MemOperand(addr, offset_imm32);
     if (shift_amount != 0) {
-      assm->CalcScaledAddress(scratch, addr, offset, shift_amount);
+      assm->CalcScaledAddress(kScratchReg2, addr, offset, shift_amount);
     } else {
-      assm->AddWord(scratch, offset, addr);
+      assm->AddWord(kScratchReg2, offset, addr);
     }
-    addr = scratch;
+    return MemOperand(kScratchReg2, offset_imm32);
   }
-  if (is_int31(offset_imm)) {
-    int32_t offset_imm32 = static_cast<int32_t>(offset_imm);
-    return MemOperand(addr, offset_imm32);
-  } else {
-    assm->li(kScratchReg2, Operand(offset_imm));
-    assm->AddWord(kScratchReg2, addr, kScratchReg2);
-    return MemOperand(kScratchReg2, 0);
+  // Offset immediate does not fit in 31 bits.
+  assm->li(kScratchReg2, offset_imm);
+  assm->AddWord(kScratchReg2, kScratchReg2, addr);
+  if (offset != no_reg) {
+    if (shift_amount != 0) {
+      assm->CalcScaledAddress(kScratchReg2, kScratchReg2, offset, shift_amount);
+    } else {
+      assm->AddWord(kScratchReg2, kScratchReg2, offset);
+    }
   }
+  return MemOperand(kScratchReg2, 0);
 }
 
 inline void Load(LiftoffAssembler* assm, LiftoffRegister dst, Register base,
@@ -221,8 +225,7 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
                                           LiftoffRegList pinned,
                                           SkipWriteBarrier skip_write_barrier) {
   Register scratch = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
-  MemOperand dst_op =
-      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, scratch);
+  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
   StoreTaggedField(src.gp(), dst_op);
 
   if (skip_write_barrier || v8_flags.disable_write_barriers) return;
@@ -245,8 +248,8 @@ void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             bool /* is_load_mem */, bool /* i64_offset */,
                             bool needs_shift) {
   unsigned shift_amount = needs_shift ? type.size_log_2() : 0;
-  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm,
-                                        kScratchReg, shift_amount);
+  MemOperand src_op =
+      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, shift_amount);
 
   if (protected_load_pc) *protected_load_pc = pc_offset();
   switch (type.value()) {
@@ -292,7 +295,7 @@ void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
     case LoadType::kI64Load: {
       Lw(dst.low_gp(), src_op);
       src_op = liftoff::GetMemOp(this, src_addr, offset_reg,
-                                 offset_imm + kSystemPointerSize, kScratchReg);
+                                 offset_imm + kSystemPointerSize);
       Lw(dst.high_gp(), src_op);
     } break;
     case LoadType::kF32Load:
@@ -327,10 +330,7 @@ void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
                              StoreType type, LiftoffRegList pinned,
                              uint32_t* protected_store_pc, bool is_store_mem,
                              bool i64_offset) {
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.Acquire();
-  MemOperand dst_op =
-      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, scratch);
+  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
 
 #if defined(V8_TARGET_BIG_ENDIAN)
   if (is_store_mem) {
@@ -369,7 +369,7 @@ void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
     case StoreType::kI64Store: {
       MacroAssembler::Sw(src.low_gp(), dst_op);
       dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg,
-                                 offset_imm + kSystemPointerSize, scratch);
+                                 offset_imm + kSystemPointerSize);
       MacroAssembler::Sw(src.high_gp(), dst_op);
       break;
     }
@@ -953,7 +953,8 @@ void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
   if (kind != kS128) {
     MacroAssembler::Move(dst, src);
   } else {
-    MacroAssembler::vmv_vv(dst.toV(), dst.toV());
+    VU.set(kScratchReg, E8, m1);
+    MacroAssembler::vmv_vv(dst.toV(), src.toV());
   }
 }
 
@@ -1250,6 +1251,8 @@ void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,
                           kScratchReg, kScratchReg2);
 }
 
+// Implemented by the host function in external-reference.h(Call to host
+// function wasm::xxx).
 bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
                                      LiftoffRegister rhs,
                                      Label* trap_div_by_zero,
@@ -1366,7 +1369,7 @@ void LiftoffAssembler::emit_i64_shli(LiftoffRegister dst, LiftoffRegister src,
   DCHECK_NE(dst.high_gp(), kScratchReg);
 
   MacroAssembler::ShlPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
-                          amount, kScratchReg, kScratchReg2);
+                          amount & 63, kScratchReg, kScratchReg2);
 }
 
 void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,
@@ -1389,7 +1392,7 @@ void LiftoffAssembler::emit_i64_sari(LiftoffRegister dst, LiftoffRegister src,
   DCHECK_NE(dst.high_gp(), kScratchReg);
 
   MacroAssembler::SarPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
-                          amount, kScratchReg, kScratchReg2);
+                          amount & 63, kScratchReg, kScratchReg2);
 }
 
 void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,
@@ -1412,7 +1415,7 @@ void LiftoffAssembler::emit_i64_shri(LiftoffRegister dst, LiftoffRegister src,
   DCHECK_NE(dst.high_gp(), kScratchReg);
 
   MacroAssembler::ShrPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
-                          amount, kScratchReg, kScratchReg2);
+                          amount & 63, kScratchReg, kScratchReg2);
 }
 
 #define FP_UNOP_RETURN_FALSE(name)                                             \
@@ -1553,6 +1556,16 @@ bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
   }
 }
 
+void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  VU.set(kScratchReg, E32, m1);
+  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), (imm_lane_idx << 0x1) + 1);
+  vmv_xs(dst.high_gp(), kSimd128ScratchReg);
+  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx << 0x1);
+  vmv_xs(dst.low_gp(), kSimd128ScratchReg);
+}
+
 void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
   slli(dst, src, 32 - 8);
   srai(dst, dst, 32 - 8);
@@ -1711,41 +1724,42 @@ void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
                                      uint32_t* protected_load_pc) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
-  MemOperand src_op =
-      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, scratch);
+  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   VRegister dst_v = dst.fp().toV();
   *protected_load_pc = pc_offset();
 
   MachineType memtype = type.mem_type();
   if (transform == LoadTransformationKind::kExtend) {
-    Lw(scratch, src_op);
+    // TODO(RISCV): need to confirm the performance impact of using floating
+    // point registers.
+    LoadDouble(kScratchDoubleReg, src_op);
     if (memtype == MachineType::Int8()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       VU.set(kScratchReg, E16, m1);
       vsext_vf2(dst_v, kSimd128ScratchReg);
     } else if (memtype == MachineType::Uint8()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       VU.set(kScratchReg, E16, m1);
       vzext_vf2(dst_v, kSimd128ScratchReg);
     } else if (memtype == MachineType::Int16()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       VU.set(kScratchReg, E32, m1);
       vsext_vf2(dst_v, kSimd128ScratchReg);
     } else if (memtype == MachineType::Uint16()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       VU.set(kScratchReg, E32, m1);
       vzext_vf2(dst_v, kSimd128ScratchReg);
     } else if (memtype == MachineType::Int32()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       vsext_vf2(dst_v, kSimd128ScratchReg);
     } else if (memtype == MachineType::Uint32()) {
       VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
+      vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);
       vzext_vf2(dst_v, kSimd128ScratchReg);
     }
   } else if (transform == LoadTransformationKind::kZeroExtend) {
@@ -1755,11 +1769,10 @@ void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
       Lw(scratch, src_op);
       vmv_sx(dst_v, scratch);
     } else {
-      // TODO(RISCV): need review
       DCHECK_EQ(MachineType::Int64(), memtype);
       VU.set(kScratchReg, E64, m1);
-      Lw(scratch, src_op);
-      vmv_sx(dst_v, scratch);
+      LoadDouble(kScratchDoubleReg, src_op);
+      vfmv_sf(dst_v, kScratchDoubleReg);
     }
   } else {
     DCHECK_EQ(LoadTransformationKind::kSplat, transform);
@@ -1776,10 +1789,9 @@ void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
       Lw(scratch, src_op);
       vmv_vx(dst_v, scratch);
     } else if (memtype == MachineType::Int64()) {
-      // TODO(RISCV): need review
       VU.set(kScratchReg, E64, m1);
-      Lw(scratch, src_op);
-      vmv_vx(dst_v, scratch);
+      LoadDouble(kScratchDoubleReg, src_op);
+      vfmv_vf(dst_v, kScratchDoubleReg);
     }
   }
 }
@@ -1791,13 +1803,12 @@ void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,
                                 bool /* i64_offfset */) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
-  MemOperand src_op =
-      liftoff::GetMemOp(this, addr, offset_reg, offset_imm, scratch);
+  MemOperand src_op = liftoff::GetMemOp(this, addr, offset_reg, offset_imm);
   MachineType mem_type = type.mem_type();
   *protected_load_pc = pc_offset();
   if (mem_type == MachineType::Int8()) {
     Lbu(scratch, src_op);
-    VU.set(kScratchReg, E64, m1);
+    VU.set(kScratchReg, E32, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
     VU.set(kScratchReg, E8, m1);
@@ -1815,11 +1826,11 @@ void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,
     vmv_sx(v0, kScratchReg);
     vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
   } else if (mem_type == MachineType::Int64()) {
-    Lw(scratch, src_op);
-    VU.set(kScratchReg, E32, m1);
+    LoadDouble(kScratchDoubleReg, src_op);
+    VU.set(kScratchReg, E64, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
-    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+    vfmerge_vf(dst.fp().toV(), kScratchDoubleReg, dst.fp().toV());
   } else {
     UNREACHABLE();
   }
@@ -1830,9 +1841,7 @@ void LiftoffAssembler::StoreLane(Register dst, Register offset,
                                  StoreType type, uint8_t lane,
                                  uint32_t* protected_store_pc,
                                  bool /* i64_offfset */) {
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.Acquire();
-  MemOperand dst_op = liftoff::GetMemOp(this, dst, offset, offset_imm, scratch);
+  MemOperand dst_op = liftoff::GetMemOp(this, dst, offset, offset_imm);
   if (protected_store_pc) *protected_store_pc = pc_offset();
   MachineRepresentation rep = type.mem_rep();
   if (rep == MachineRepresentation::kWord8) {
@@ -1854,155 +1863,141 @@ void LiftoffAssembler::StoreLane(Register dst, Register offset,
     DCHECK_EQ(MachineRepresentation::kWord64, rep);
     VU.set(kScratchReg, E64, m1);
     vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
-    vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sw(kScratchReg, dst_op);
+    vfmv_fs(kScratchDoubleReg, kSimd128ScratchReg);
+    StoreDouble(kScratchDoubleReg, dst_op);
   }
 }
 
-void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
-                                          LiftoffRegister lhs,
-                                          LiftoffRegister rhs,
-                                          const uint8_t shuffle[16],
-                                          bool is_swizzle) {
-  // VRegister dst_v = dst.fp().toV();
-  // VRegister lhs_v = lhs.fp().toV();
-  // VRegister rhs_v = rhs.fp().toV();
-
-  // uint64_t imm1 = *(reinterpret_cast<const uint64_t*>(shuffle));
-  // uint64_t imm2 = *((reinterpret_cast<const uint64_t*>(shuffle)) + 1);
-  // VU.set(kScratchReg, VSew::E64, Vlmul::m1);
-  // li(kScratchReg, imm2);
-  // vmv_sx(kSimd128ScratchReg2, kScratchReg);
-  // vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
-  // li(kScratchReg, imm1);
-  // vmv_sx(kSimd128ScratchReg, kScratchReg);
-
-  // VU.set(kScratchReg, E8, m1);
-  // VRegister temp =
-  //     GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
-  // if (dst_v == lhs_v) {
-  //   vmv_vv(temp, lhs_v);
-  //   lhs_v = temp;
-  // } else if (dst_v == rhs_v) {
-  //   vmv_vv(temp, rhs_v);
-  //   rhs_v = temp;
-  // }
-  // vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg);
-  // vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg,
-  //         -16);  // The indices in range [16, 31] select the i - 16-th
-  //         element
-  //                // of rhs
-  // vrgather_vv(kSimd128ScratchReg2, rhs_v, kSimd128ScratchReg);
-  // vor_vv(dst_v, dst_v, kSimd128ScratchReg2);
-  bailout(kSimd, "emit_i8x16_shuffle");
-}
-
-void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
+void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,
                                         LiftoffRegister src) {
-  // VU.set(kScratchReg, E64, m1);
-  // fmv_x_d(kScratchReg, src.fp());
-  // vmv_vx(dst.fp().toV(), kScratchReg);
-  bailout(kSimd, "emit_f64x2_splat");
+  VU.set(kScratchReg, E32, m1);
+  vmv_vi(v0, 0b0101);
+  vmv_vx(kSimd128ScratchReg, src.high_gp());
+  vmerge_vx(dst.fp().toV(), src.low_gp(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  VU.set(kScratchReg, E32, m1);
+  vmv_vx(kSimd128ScratchReg, src2.high_gp());
+  vmv_sx(kSimd128ScratchReg, src2.low_gp());
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x1 << imm_lane_idx);
+  vmv_sx(v0, kScratchReg);
+  vfmv_fs(kScratchDoubleReg, kSimd128ScratchReg);
+  vfmerge_vf(dst.fp().toV(), kScratchDoubleReg, src1.fp().toV());
 }
 
 void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
                                       LiftoffRegister rhs) {
-  // VU.set(kScratchReg, E64, m1);
-  // const int64_t kNaN = 0x7ff8000000000000L;
-  // vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
-  // vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
-  // vand_vv(v0, v0, kSimd128ScratchReg);
-  // li(kScratchReg, kNaN);
-  // vmv_vx(kSimd128ScratchReg, kScratchReg);
-  // vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
-  // vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
-  bailout(kSimd, "emit_f64x2_min");
+  VU.set(kScratchReg, E64, m1);
+  const int32_t kNaN = 0x7ff80000L, kNaNShift = 32;
+  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  vand_vv(v0, v0, kSimd128ScratchReg);
+  li(kScratchReg, kNaN);
+  li(kScratchReg2, kNaNShift);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  vsll_vx(kSimd128ScratchReg, kSimd128ScratchReg, kScratchReg2);
+  vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
 }
 
 void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,
                                       LiftoffRegister rhs) {
-  // VU.set(kScratchReg, E64, m1);
-  // const int64_t kNaN = 0x7ff8000000000000L;
-  // vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
-  // vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
-  // vand_vv(v0, v0, kSimd128ScratchReg);
-  // li(kScratchReg, kNaN);
-  // vmv_vx(kSimd128ScratchReg, kScratchReg);
-  // vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
-  // vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
-  bailout(kSimd, "emit_f64x2_max");
+  VU.set(kScratchReg, E64, m1);
+  const int32_t kNaN = 0x7ff80000L, kNaNShift = 32;
+  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  vand_vv(v0, v0, kSimd128ScratchReg);
+  li(kScratchReg, kNaN);
+  li(kScratchReg2, kNaNShift);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  vsll_vx(kSimd128ScratchReg, kSimd128ScratchReg, kScratchReg2);
+  vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
 }
 
 void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,
                                                           LiftoffRegister src) {
-  // VU.set(kScratchReg, E64, m1);
+  VU.set(kScratchReg, E32, m1);
   // li(kScratchReg, 0x0006000400020000);
-  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x00060004);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x00020000);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
   // li(kScratchReg, 0x0007000500030001);
-  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  // VU.set(kScratchReg, E16, m1);
-  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  // VU.set(kScratchReg, E16, mf2);
-  // vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-  bailout(kSimd, "emit_i32x4_extadd_pairwise_i16x8_s");
+  li(kScratchReg, 0x00070005);
+  vmv_vx(kSimd128ScratchReg3, kScratchReg);
+  li(kScratchReg, 0x00030001);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E16, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E16, mf2);
+  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
 }
 
 void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,
                                                           LiftoffRegister src) {
-  // VU.set(kScratchReg, E64, m1);
+  VU.set(kScratchReg, E32, m1);
   // li(kScratchReg, 0x0006000400020000);
-  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x00060004);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x00020000);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
   // li(kScratchReg, 0x0007000500030001);
-  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  // VU.set(kScratchReg, E16, m1);
-  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  // VU.set(kScratchReg, E16, mf2);
-  // vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-  bailout(kSimd, "emit_i32x4_extadd_pairwise_i16x8_u");
+  li(kScratchReg, 0x00070005);
+  vmv_vx(kSimd128ScratchReg3, kScratchReg);
+  li(kScratchReg, 0x00030001);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E16, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E16, mf2);
+  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
 }
 
 void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,
                                                           LiftoffRegister src) {
-  // VU.set(kScratchReg, E64, m1);
+  VU.set(kScratchReg, E32, m1);
   // li(kScratchReg, 0x0E0C0A0806040200);
-  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0E0C0A08);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x06040200);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
   // li(kScratchReg, 0x0F0D0B0907050301);
-  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  // VU.set(kScratchReg, E8, m1);
-  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  // VU.set(kScratchReg, E8, mf2);
-  // vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-  bailout(kSimd, "emit_i16x8_extadd_pairwise_i8x16_s");
+  li(kScratchReg, 0x0F0D0B09);
+  vmv_vx(kSimd128ScratchReg3, kScratchReg);
+  li(kScratchReg, 0x07050301);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E8, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E8, mf2);
+  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
 }
 
 void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,
                                                           LiftoffRegister src) {
-  // VU.set(kScratchReg, E64, m1);
+  VU.set(kScratchReg, E32, m1);
   // li(kScratchReg, 0x0E0C0A0806040200);
-  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0E0C0A08);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x06040200);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
   // li(kScratchReg, 0x0F0D0B0907050301);
-  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  // VU.set(kScratchReg, E8, m1);
-  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  // VU.set(kScratchReg, E8, mf2);
-  // vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-  bailout(kSimd, "emit_i16x8_extadd_pairwise_i8x16_u");
-}
-
-void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
-                                               LiftoffRegister src1,
-                                               LiftoffRegister src2,
-                                               uint8_t imm_lane_idx) {
-  // VU.set(kScratchReg, E64, m1);
-  // li(kScratchReg, 0x1 << imm_lane_idx);
-  // vmv_sx(v0, kScratchReg);
-  // fmv_x_d(kScratchReg, src2.fp());
-  // vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
-  bailout(kSimd, "emit_f64x2_replace_lane");
+  li(kScratchReg, 0x0F0D0B09);
+  vmv_vx(kSimd128ScratchReg3, kScratchReg);
+  li(kScratchReg, 0x07050301);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E8, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E8, mf2);
+  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
 }
 
 void LiftoffAssembler::CallC(const ValueKindSig* sig,
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
index e2ba92741f..f36591cdf7 100644
--- a/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
@@ -1250,6 +1250,14 @@ bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
   }
 }
 
+void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,
+                                               LiftoffRegister lhs,
+                                               uint8_t imm_lane_idx) {
+  VU.set(kScratchReg, E64, m1);
+  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);
+  vmv_xs(dst.gp(), kSimd128ScratchReg);
+}
+
 void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
   slliw(dst, src, 32 - 8);
   sraiw(dst, dst, 32 - 8);
@@ -1500,46 +1508,20 @@ void LiftoffAssembler::StoreLane(Register dst, Register offset,
   }
 }
 
-void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
-                                          LiftoffRegister lhs,
-                                          LiftoffRegister rhs,
-                                          const uint8_t shuffle[16],
-                                          bool is_swizzle) {
-  VRegister dst_v = dst.fp().toV();
-  VRegister lhs_v = lhs.fp().toV();
-  VRegister rhs_v = rhs.fp().toV();
-
-  uint64_t imm1 = *(reinterpret_cast<const uint64_t*>(shuffle));
-  uint64_t imm2 = *((reinterpret_cast<const uint64_t*>(shuffle)) + 1);
-  VU.set(kScratchReg, VSew::E64, Vlmul::m1);
-  li(kScratchReg, imm2);
-  vmv_sx(kSimd128ScratchReg2, kScratchReg);
-  vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
-  li(kScratchReg, imm1);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-
-  VU.set(kScratchReg, E8, m1);
-  VRegister temp =
-      GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
-  if (dst_v == lhs_v) {
-    vmv_vv(temp, lhs_v);
-    lhs_v = temp;
-  } else if (dst_v == rhs_v) {
-    vmv_vv(temp, rhs_v);
-    rhs_v = temp;
-  }
-  vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg);
-  vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg,
-          -16);  // The indices in range [16, 31] select the i - 16-th element
-                 // of rhs
-  vrgather_vv(kSimd128ScratchReg2, rhs_v, kSimd128ScratchReg);
-  vor_vv(dst_v, dst_v, kSimd128ScratchReg2);
+void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  vmv_vx(dst.fp().toV(), src.gp());
 }
 
-void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
-                                        LiftoffRegister src) {
+void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
   VU.set(kScratchReg, E64, m1);
-  vfmv_vf(dst.fp().toV(), src.fp());
+  li(kScratchReg, 0x1 << imm_lane_idx);
+  vmv_sx(v0, kScratchReg);
+  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());
 }
 
 void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
@@ -1632,16 +1614,6 @@ void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,
   vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
 }
 
-void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
-                                               LiftoffRegister src1,
-                                               LiftoffRegister src2,
-                                               uint8_t imm_lane_idx) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x1 << imm_lane_idx);
-  vmv_sx(v0, kScratchReg);
-  vfmerge_vf(dst.fp().toV(), src2.fp(), src1.fp().toV());
-}
-
 void LiftoffAssembler::CallC(const ValueKindSig* sig,
                              const LiftoffRegister* args,
                              const LiftoffRegister* rets,
diff --git a/src/wasm/wasm-linkage.h b/src/wasm/wasm-linkage.h
index b8fbc3f055..85e09256db 100644
--- a/src/wasm/wasm-linkage.h
+++ b/src/wasm/wasm-linkage.h
@@ -111,7 +111,7 @@ constexpr DoubleRegister kFpReturnRegisters[] = {d0, d2};
 // == riscv64 =================================================================
 // ===========================================================================
 // Note that kGpParamRegisters and kFpParamRegisters are used in
-// Builtins::Generate_WasmCompileLazy (builtins-riscv64.cc)
+// Builtins::Generate_WasmCompileLazy (builtins-riscv.cc)
 constexpr Register kGpParamRegisters[] = {a0, a2, a3, a4, a5, a6, a7};
 constexpr Register kGpReturnRegisters[] = {a0, a1};
 constexpr DoubleRegister kFpParamRegisters[] = {fa0, fa1, fa2, fa3,
diff --git a/test/cctest/test-assembler-riscv32.cc b/test/cctest/test-assembler-riscv32.cc
index 8f0e6a9c76..01e03fd7fc 100644
--- a/test/cctest/test-assembler-riscv32.cc
+++ b/test/cctest/test-assembler-riscv32.cc
@@ -492,9 +492,8 @@ TEST(RISCV_UTEST_fmv_d_double) {
            base::bit_cast<int64_t>(dst));
 }
 
-// Test fmv_d
-// double not a canonical NaN
-TEST(RISCV_UTEST_fmv_d_double_NAN_BOX) {
+// Test signaling NaN in FMV.D
+TEST(RISCV_UTEST_fmv_d_double_signaling_NaN) {
   CcTest::InitializeVM();
 
   int64_t src = base::bit_cast<int64_t>(0x7ff4000000000000);
@@ -1073,7 +1072,7 @@ TEST(NAN_BOX) {
     CHECK_EQ((uint32_t)base::bit_cast<uint32_t>(1234.56f), res);
   }
 
-  // Test NaN boxing in FMV.S
+  // Test signaling NaN in FMV.S
   {
     auto fn = [](MacroAssembler& assm) {
       __ fmv_w_x(fa0, a0);
@@ -1771,7 +1770,7 @@ TEST(li_estimate) {
       __ vl(v2, a0, 0, SEW);                                                 \
       __ vs(v2, a1, 0, SEW);                                                 \
     };                                                                       \
-    GenAndRunTest<int32_t, int64_t>((int64_t)src, (int64_t)dst, fn);         \
+    GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, fn);         \
     CHECK(!memcmp(src, dst, sizeof(src)));                                   \
   }
 
@@ -1793,11 +1792,141 @@ TEST(RVV_VFMV) {
       __ vfmv_vf(v2, fa1);
       __ vs(v2, a1, 0, VSew::E32);
     };
-    GenAndRunTest<int32_t, int64_t>((int64_t)&src, (int64_t)dst, fn);
+    GenAndRunTest<int32_t, int32_t>((int32_t)&src, (int32_t)dst, fn);
     CHECK(!memcmp(ref, dst, sizeof(ref)));
   }
 }
 
+TEST(RVV_VFMV_signaling_NaN) {
+  if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;
+  CcTest::InitializeVM();
+
+  {
+    constexpr uint32_t n = 2;
+    int64_t rs1_fval = 0x7FF4000000000000;
+    int64_t dst[n] = {0};
+    auto fn = [](MacroAssembler& assm) {
+      __ VU.set(t0, VSew::E64, Vlmul::m1);
+      __ fld(ft0, a0, 0);
+      __ vfmv_vf(v1, ft0);
+      __ vs(v1, a1, 0, VSew::E64);
+    };
+    GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)dst, fn);
+    for (uint32_t i = 0; i < n; i++) {
+      CHECK_EQ(rs1_fval, dst[i]);
+    }
+  }
+
+  {
+    constexpr uint32_t n = 4;
+    int32_t rs1_fval = 0x7F400000;
+    int32_t dst[n] = {0};
+    auto fn = [](MacroAssembler& assm) {
+      __ VU.set(t0, VSew::E32, Vlmul::m1);
+      __ flw(ft0, a0, 0);
+      __ vfmv_vf(v1, ft0);
+      __ vs(v1, a1, 0, VSew::E32);
+    };
+    GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)dst, fn);
+    for (uint32_t i = 0; i < n; i++) {
+      CHECK_EQ(rs1_fval, dst[i]);
+    }
+  }
+}
+
+TEST(RVV_VFNEG_signaling_NaN) {
+  if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;
+  CcTest::InitializeVM();
+
+  {
+    constexpr uint32_t n = 2;
+    int64_t rs1_fval = 0x7FF4000000000000;
+    int64_t expected_fval = 0xFFF4000000000000;
+    int64_t dst[n] = {0};
+    auto fn = [](MacroAssembler& assm) {
+      __ VU.set(t0, VSew::E64, Vlmul::m1);
+      __ fld(ft0, a0, 0);
+      __ vfmv_vf(v1, ft0);
+      __ vfneg_vv(v2, v1);
+      __ vs(v2, a1, 0, VSew::E64);
+    };
+    GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)dst, fn);
+    for (uint32_t i = 0; i < n; i++) {
+      CHECK_EQ(expected_fval, dst[i]);
+    }
+  }
+
+  {
+    constexpr uint32_t n = 4;
+    int32_t rs1_fval = 0x7F400000;
+    int32_t expected_fval = 0xFF400000;
+    int32_t dst[n] = {0};
+    auto fn = [](MacroAssembler& assm) {
+      __ VU.set(t0, VSew::E32, Vlmul::m1);
+      __ flw(ft0, a0, 0);
+      __ vfmv_vf(v1, ft0);
+      __ vfneg_vv(v2, v1);
+      __ vs(v2, a1, 0, VSew::E32);
+    };
+    GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)dst, fn);
+    for (uint32_t i = 0; i < n; i++) {
+      CHECK_EQ(expected_fval, dst[i]);
+    }
+  }
+}
+
+// Tests for Floating-Point scalar move instructions between vector and scalar f
+// register
+#define UTEST_RVV_VF_MV_FORM_WITH_RES(instr_name, reg1, reg2, width, type)   \
+  TEST(RISCV_UTEST_##instr_name##_##width) {                                 \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    CcTest::InitializeVM();                                                  \
+    constexpr uint32_t n = kRvvVLEN / width;                                 \
+    for (type fval : compiler::ValueHelper::GetVector<type>()) {             \
+      int##width##_t rs1_fval = base::bit_cast<int##width##_t>(fval);        \
+      int##width##_t res[n] = {0};                                           \
+      for (uint32_t i = 0; i < n; i++) res[i] = (rs1_fval + i + 1);          \
+      auto fn = [](MacroAssembler& assm) {                                   \
+        __ VU.set(t0, VSew::E##width, Vlmul::m1);                            \
+        width == 32 ? __ flw(ft0, a0, 0) : __ fld(ft0, a0, 0);               \
+        __ vl(v1, a1, 0, VSew::E##width);                                    \
+        __ instr_name(reg1, reg2);                                           \
+        __ fsd(ft0, a0, 0);                                                  \
+        __ vs(v1, a1, 0, VSew::E##width);                                    \
+      };                                                                     \
+      GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)res, fn); \
+      for (uint32_t i = 0; i < n; i++) {                                     \
+        CHECK_EQ(i == 0 ? rs1_fval : res[i], res[i]);                        \
+      }                                                                      \
+    }                                                                        \
+  }                                                                          \
+  TEST(RISCV_UTEST_##instr_name##_##width##_##sNaN) {                        \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    CcTest::InitializeVM();                                                  \
+    constexpr uint32_t n = kRvvVLEN / width;                                 \
+    int##width##_t rs1_fval = width == 32 ? 0x7F400000 : 0x7FF4000000000000; \
+    int##width##_t res[n] = {0};                                             \
+    for (uint32_t i = 0; i < n; i++) res[i] = (rs1_fval + i + 1);            \
+    auto fn = [](MacroAssembler& assm) {                                     \
+      __ VU.set(t0, VSew::E##width, Vlmul::m1);                              \
+      width == 32 ? __ flw(ft0, a0, 0) : __ fld(ft0, a0, 0);                 \
+      __ vl(v1, a1, 0, VSew::E##width);                                      \
+      __ instr_name(reg1, reg2);                                             \
+      __ fsd(ft0, a0, 0);                                                    \
+      __ vs(v1, a1, 0, VSew::E##width);                                      \
+    };                                                                       \
+    GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval, (int32_t)res, fn);   \
+    for (uint32_t i = 0; i < n; i++) {                                       \
+      CHECK_EQ(i == 0 ? rs1_fval : res[i], res[i]);                          \
+    }                                                                        \
+  }
+
+UTEST_RVV_VF_MV_FORM_WITH_RES(vfmv_fs, ft0, v1, 32, float)
+UTEST_RVV_VF_MV_FORM_WITH_RES(vfmv_fs, ft0, v1, 64, double)
+UTEST_RVV_VF_MV_FORM_WITH_RES(vfmv_sf, v1, ft0, 32, float)
+UTEST_RVV_VF_MV_FORM_WITH_RES(vfmv_sf, v1, ft0, 64, double)
+#undef UTEST_RVV_VF_MV_FORM_WITH_RES
+
 inline int32_t ToImm5(int32_t v) {
   int32_t smax = (int32_t)(INT64_MAX >> (64 - 5));
   int32_t smin = (int32_t)(INT64_MIN >> (64 - 5));
@@ -1975,7 +2104,7 @@ UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 32, ARRAY_INT32, std::min<uint32_t>)
       __ vfmv_vf(v1, fa1);                                                 \
       __ instr_name(v0, v0, v1);                                           \
       __ vfmv_fs(fa0, v0);                                                 \
-      __ li(a3, Operand(int64_t(result)));                                 \
+      __ li(a3, Operand(int32_t(result)));                                 \
       __ vs(v0, a3, 0, E32);                                               \
     };                                                                     \
     for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {     \
@@ -1994,16 +2123,19 @@ UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 32, ARRAY_INT32, std::min<uint32_t>)
     double result[2] = {0.0};                                              \
     auto fn = [&result](MacroAssembler& assm) {                            \
       __ VU.set(t0, VSew::E64, Vlmul::m1);                                 \
+      __ fld(fa0, a0, 0);                                                  \
+      __ fld(fa1, a1, 0);                                                  \
       __ vfmv_vf(v0, fa0);                                                 \
       __ vfmv_vf(v1, fa1);                                                 \
       __ instr_name(v0, v0, v1);                                           \
       __ vfmv_fs(fa0, v0);                                                 \
-      __ li(a3, Operand(int64_t(result)));                                 \
+      __ li(a3, Operand(int32_t(result)));                                 \
       __ vs(v0, a3, 0, E64);                                               \
     };                                                                     \
     for (double rs1_fval : compiler::ValueHelper::GetVector<double>()) {   \
       for (double rs2_fval : compiler::ValueHelper::GetVector<double>()) { \
-        GenAndRunTest<double, double>(rs1_fval, rs2_fval, fn);             \
+        GenAndRunTest<int32_t, int32_t>((int32_t)&rs1_fval,                \
+                                        (int32_t)&rs2_fval, fn);           \
         for (int i = 0; i < 2; i++) {                                      \
           CHECK_DOUBLE_EQ(UseCanonicalNan<double>(expect_res), result[i]); \
           result[i] = 0.0;                                                 \
@@ -2035,11 +2167,14 @@ UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 32, ARRAY_INT32, std::min<uint32_t>)
 #define UTEST_RVV_VF_VV_FORM_WITH_OP(instr_name, tested_op) \
   UTEST_RVV_VF_VV_FORM_WITH_RES(instr_name, ((rs1_fval)tested_op(rs2_fval)))
 
-#define UTEST_RVV_VF_VF_FORM_WITH_OP(instr_name, tested_op) \
-  UTEST_RVV_VF_VF_FORM_WITH_RES(instr_name, ((rs1_fval)tested_op(rs2_fval)))
+#define UTEST_RVV_VF_VF_FORM_WITH_OP(instr_name, array, tested_op) \
+  UTEST_RVV_VF_VF_FORM_WITH_RES(instr_name, array,                 \
+                                ((rs1_fval)tested_op(rs2_fval)))
+
+#define ARRAY_FLOAT compiler::ValueHelper::GetVector<float>()
 
 UTEST_RVV_VF_VV_FORM_WITH_OP(vfadd_vv, +)
-// UTEST_RVV_VF_VF_FORM_WITH_OP(vfadd_vf, ARRAY_FLOAT, +)
+UTEST_RVV_VF_VF_FORM_WITH_OP(vfadd_vf, ARRAY_FLOAT, +)
 UTEST_RVV_VF_VV_FORM_WITH_OP(vfsub_vv, -)
 // UTEST_RVV_VF_VF_FORM_WITH_OP(vfsub_vf, ARRAY_FLOAT, -)
 UTEST_RVV_VF_VV_FORM_WITH_OP(vfmul_vv, *)
@@ -2072,12 +2207,12 @@ UTEST_RVV_VF_VV_FORM_WITH_OP(vfdiv_vv, /)
       }                                                                        \
       __ vfmv_vf(v4, fa1);                                                     \
       __ instr_name(v0, v2, v4);                                               \
-      __ li(t1, Operand(int64_t(result)));                                     \
+      __ li(t1, Operand(int32_t(result)));                                     \
       __ vs(v0, t1, 0, VSew::E64);                                             \
     };                                                                         \
     for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {         \
       for (float rs2_fval : compiler::ValueHelper::GetVector<float>()) {       \
-        GenAndRunTest<double, float>(rs1_fval, rs2_fval, fn);                  \
+        GenAndRunTest<double*, float>(rs1_fval, rs2_fval, fn);                 \
         for (size_t i = 0; i < n; i++) {                                       \
           CHECK_DOUBLE_EQ(                                                     \
               check_fn(rs1_fval, rs2_fval)                                     \
@@ -2112,12 +2247,12 @@ UTEST_RVV_VF_VV_FORM_WITH_OP(vfdiv_vv, /)
         __ vfmv_vf(v2, fa0);                                                   \
       }                                                                        \
       __ instr_name(v0, v2, fa1);                                              \
-      __ li(t1, Operand(int64_t(result)));                                     \
+      __ li(t1, Operand(int32_t(result)));                                     \
       __ vs(v0, t1, 0, VSew::E64);                                             \
     };                                                                         \
     for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {         \
       for (float rs2_fval : compiler::ValueHelper::GetVector<float>()) {       \
-        GenAndRunTest<double, float>(rs1_fval, rs2_fval, fn);                  \
+        GenAndRunTest<double*, float>(rs1_fval, rs2_fval, fn);                 \
         for (size_t i = 0; i < n; i++) {                                       \
           CHECK_DOUBLE_EQ(                                                     \
               check_fn(rs1_fval, rs2_fval)                                     \
@@ -2174,83 +2309,98 @@ UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwmul_vf, *, false, is_invalid_fmul)
 
 // Tests for vector widening floating-point fused multiply-add Instructions
 // between vectors
-#define UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(instr_name, array, expect_res)     \
-  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                             \
-    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                        \
-    CcTest::InitializeVM();                                                   \
-    auto fn = [](MacroAssembler& assm) {                                      \
-      __ VU.set(t0, VSew::E32, Vlmul::m1);                                    \
-      __ vfmv_vf(v0, fa0);                                                    \
-      __ vfmv_vf(v2, fa1);                                                    \
-      __ vfmv_vf(v4, fa2);                                                    \
-      __ instr_name(v0, v2, v4);                                              \
-      __ VU.set(t0, VSew::E64, Vlmul::m1);                                    \
-      __ vfmv_fs(fa0, v0);                                                    \
-    };                                                                        \
-    for (float rs1_fval : array) {                                            \
-      for (float rs2_fval : array) {                                          \
-        for (float rs3_fval : array) {                                        \
-          double rs1_dval = base::bit_cast<double>(                           \
-              (uint64_t)base::bit_cast<uint32_t>(rs1_fval) << 32 |            \
-              base::bit_cast<uint32_t>(rs1_fval));                            \
-          double rs2_dval = static_cast<double>(rs2_fval);                    \
-          double rs3_dval = static_cast<double>(rs3_fval);                    \
-          double res =                                                        \
-              GenAndRunTest<double, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
-          CHECK_DOUBLE_EQ((expect_res), res);                                 \
-        }                                                                     \
-      }                                                                       \
-    }                                                                         \
+#define UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(instr_name, float_array,   \
+                                           double_array, expect_res)  \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                     \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                \
+    CcTest::InitializeVM();                                           \
+    double addend_arr[2] = {0};                                       \
+    float right_mul_arr[4] = {0};                                     \
+    float left_mul_arr[4] = {0};                                      \
+    auto fn = [](MacroAssembler& assm) {                              \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                            \
+      __ vl(v0, a0, 0, VSew::E32);                                    \
+      __ vl(v2, a1, 0, VSew::E32);                                    \
+      __ vl(v4, a2, 0, VSew::E32);                                    \
+      __ instr_name(v0, v2, v4);                                      \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                            \
+      __ vs(v0, a0, 0, VSew::E64);                                    \
+    };                                                                \
+    for (double rs1_dval : double_array) {                            \
+      for (float rs2_fval : float_array) {                            \
+        for (float rs3_fval : float_array) {                          \
+          for (double& src : addend_arr) src = rs1_dval;              \
+          for (float& src : right_mul_arr) src = rs2_fval;            \
+          for (float& src : left_mul_arr) src = rs3_fval;             \
+          double rs2_dval = static_cast<double>(rs2_fval);            \
+          double rs3_dval = static_cast<double>(rs3_fval);            \
+          GenAndRunTest<int32_t, int32_t>((int32_t)addend_arr,        \
+                                          (int32_t)right_mul_arr,     \
+                                          (int32_t)left_mul_arr, fn); \
+          for (uint32_t i = 0; i < 2; i++) {                          \
+            CHECK_DOUBLE_EQ((expect_res), addend_arr[i]);             \
+          }                                                           \
+        }                                                             \
+      }                                                               \
+    }                                                                 \
   }
 
 // Tests for vector single-width floating-point fused multiply-add Instructions
 // between vectors and scalar
-#define UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(instr_name, array, expect_res)     \
-  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                             \
-    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                        \
-    CcTest::InitializeVM();                                                   \
-    auto fn = [](MacroAssembler& assm) {                                      \
-      __ VU.set(t0, VSew::E32, Vlmul::m1);                                    \
-      __ vfmv_vf(v0, fa0);                                                    \
-      __ vfmv_vf(v2, fa2);                                                    \
-      __ instr_name(v0, fa1, v2);                                             \
-      __ VU.set(t0, VSew::E64, Vlmul::m1);                                    \
-      __ vfmv_fs(fa0, v0);                                                    \
-    };                                                                        \
-    for (float rs1_fval : array) {                                            \
-      for (float rs2_fval : array) {                                          \
-        for (float rs3_fval : array) {                                        \
-          double rs1_dval = base::bit_cast<double>(                           \
-              (uint64_t)base::bit_cast<uint32_t>(rs1_fval) << 32 |            \
-              base::bit_cast<uint32_t>(rs1_fval));                            \
-          double rs2_dval = static_cast<double>(rs2_fval);                    \
-          double rs3_dval = static_cast<double>(rs3_fval);                    \
-          double res =                                                        \
-              GenAndRunTest<double, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
-          CHECK_DOUBLE_EQ((expect_res), res);                                 \
-        }                                                                     \
-      }                                                                       \
-    }                                                                         \
+#define UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(instr_name, float_array,    \
+                                           double_array, expect_res)   \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                      \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                 \
+    CcTest::InitializeVM();                                            \
+    double addend_arr[2] = {0};                                        \
+    float right_mul_arr[4] = {0};                                      \
+    auto fn = [](MacroAssembler& assm) {                               \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                             \
+      __ vl(v0, a0, 0, VSew::E32);                                     \
+      __ flw(fa1, a1, 0);                                              \
+      __ vl(v2, a2, 0, VSew::E32);                                     \
+      __ instr_name(v0, fa1, v2);                                      \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                             \
+      __ vs(v0, a0, 0, VSew::E64);                                     \
+    };                                                                 \
+    for (double rs1_dval : double_array) {                             \
+      for (float rs2_fval : float_array) {                             \
+        for (float rs3_fval : float_array) {                           \
+          for (double& src : addend_arr) src = rs1_dval;               \
+          for (float& src : right_mul_arr) src = rs3_fval;             \
+          double rs2_dval = static_cast<double>(rs2_fval);             \
+          double rs3_dval = static_cast<double>(rs3_fval);             \
+          GenAndRunTest<int32_t, int32_t>((int32_t)addend_arr,         \
+                                          (int32_t)&rs2_fval,          \
+                                          (int32_t)right_mul_arr, fn); \
+          for (uint32_t i = 0; i < 2; i++) {                           \
+            CHECK_DOUBLE_EQ((expect_res), addend_arr[i]);              \
+          }                                                            \
+        }                                                              \
+      }                                                                \
+    }                                                                  \
   }
 
 #define ARRAY_FLOAT compiler::ValueHelper::GetVector<float>()
-UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmacc_vv, ARRAY_FLOAT,
+#define ARRAY_DOUBLE compiler::ValueHelper::GetVector<double>()
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmacc_vv, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, rs3_dval, rs1_dval))
-UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmacc_vf, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmacc_vf, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, rs3_dval, rs1_dval))
-UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmacc_vv, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmacc_vv, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, -rs3_dval, -rs1_dval))
-UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmacc_vf, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmacc_vf, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, -rs3_dval, -rs1_dval))
-UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmsac_vv, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmsac_vv, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, rs3_dval, -rs1_dval))
-UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmsac_vf, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmsac_vf, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, rs3_dval, -rs1_dval))
-UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmsac_vv, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmsac_vv, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, -rs3_dval, rs1_dval))
-UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmsac_vf, ARRAY_FLOAT,
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmsac_vf, ARRAY_FLOAT, ARRAY_DOUBLE,
                                    std::fma(rs2_dval, -rs3_dval, rs1_dval))
 
+#undef ARRAY_DOUBLE
 #undef ARRAY_FLOAT
 #undef UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES
 #undef UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES
@@ -2348,13 +2498,15 @@ UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmsac_vf, ARRAY_FLOAT,
   TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                      \
     if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                 \
     CcTest::InitializeVM();                                            \
-    auto fn = [](MacroAssembler& assm) {                               \
+    double result = 0;                                                 \
+    auto fn = [&result](MacroAssembler& assm) {                        \
       __ VU.set(t0, VSew::E32, Vlmul::m1);                             \
       __ vfmv_vf(v2, fa0);                                             \
       __ vfmv_vf(v4, fa0);                                             \
       __ instr_name(v0, v2, v4);                                       \
       __ VU.set(t0, VSew::E64, Vlmul::m1);                             \
-      __ vfmv_fs(fa0, v0);                                             \
+      __ li(a0, Operand(int32_t(&result)));                            \
+      __ vs(v0, a0, 0, VSew::E64);                                     \
     };                                                                 \
     for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) { \
       std::vector<double> temp_arr(kRvvVLEN / 32,                      \
@@ -2369,8 +2521,8 @@ UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmsac_vf, ARRAY_FLOAT,
           break;                                                       \
         }                                                              \
       }                                                                \
-      double res = GenAndRunTest<double, float>(rs1_fval, fn);         \
-      CHECK_DOUBLE_EQ(UseCanonicalNan<double>(expect_res), res);       \
+      GenAndRunTest<double*, float>(rs1_fval, fn);                     \
+      CHECK_DOUBLE_EQ(UseCanonicalNan<double>(expect_res), result);    \
     }                                                                  \
   }
 
@@ -2436,7 +2588,7 @@ static inline uint8_t get_round(int vxrm, uint64_t v, uint8_t shift) {
           ref = base::saturated_cast<sign##int16_t>(                         \
               (static_cast<sign##int32_t>(x) >> shift) +                     \
               get_round(vxrm, x, shift));                                    \
-        GenAndRunTest<int32_t, int64_t>((int64_t)t.src, (int64_t)t.dst, fn); \
+        GenAndRunTest<int32_t, int32_t>((int32_t)t.src, (int32_t)t.dst, fn); \
         CHECK(!memcmp(t.dst, t.ref, sizeof(t.ref)));                         \
       }                                                                      \
     }                                                                        \
@@ -2465,7 +2617,7 @@ UTEST_RVV_VNCLIP_E32M2_E16M1(vnclip_vi, )
         __ instr_name(v2, v1);                                              \
         __ vs(v2, a1, 0, VSew::E##width);                                   \
       };                                                                    \
-      GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn);      \
+      GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, fn);      \
       for (uint32_t i = 0; i < n; i++) {                                    \
         CHECK_EQ(expect_res, dst[i]);                                       \
       }                                                                     \
@@ -2502,40 +2654,54 @@ UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf2, int16_t, 16, 8, ARRAY(int8_t),
 
 #undef UTEST_RVV_VI_VIE_FORM_WITH_RES
 
+static constexpr double double_sNaN[] = {
+    std::numeric_limits<double>::signaling_NaN(),
+    -std::numeric_limits<double>::signaling_NaN()};
+static constexpr float float_sNaN[] = {
+    std::numeric_limits<float>::signaling_NaN(),
+    -std::numeric_limits<float>::signaling_NaN()};
 // Tests for vector Floating-Point merge instruction
-#define UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(type, int_type, width,     \
-                                              expect_res)                \
-  TEST(RISCV_UTEST_vfmerge_vf_##type) {                                  \
-    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                   \
-    constexpr uint32_t n = kRvvVLEN / width;                             \
-    CcTest::InitializeVM();                                              \
-    for (type fval : compiler::ValueHelper::GetVector<type>()) {         \
-      int_type rs1_fval = base::bit_cast<int_type>(fval);                \
-      for (uint32_t mask = 0; mask < (1 << n); mask++) {                 \
-        int_type src[n] = {0};                                           \
-        int_type dst[n] = {0};                                           \
-        dst[0] = rs1_fval;                                               \
-        for (uint32_t i = 0; i < n; i++) src[i] = i;                     \
-        auto fn = [mask](MacroAssembler& assm) {                         \
-          __ VU.set(t0, VSew::E##width, Vlmul::m1);                      \
-          __ vl(v1, a0, 0, VSew::E##width);                              \
-          __ vl(v24, a1, 0, VSew::E##width);                             \
-          __ vmv_vi(v0, mask);                                           \
-          __ vfmv_fs(ft0, v24);                                          \
-          __ vfmerge_vf(v2, ft0, v1);                                    \
-          __ vs(v2, a1, 0, VSew::E##width);                              \
-        };                                                               \
-        GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn); \
-        for (uint32_t i = 0; i < n; i++) {                               \
-          CHECK_EQ(expect_res, dst[i]);                                  \
-        }                                                                \
-      }                                                                  \
-    }                                                                    \
-  }
-
-UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(double, int64_t, 64,
+#define UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(number, type, int_type, width, \
+                                              array, expect_res)             \
+  TEST(RISCV_UTEST_vfmerge_vf_##type##_##number) {                           \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    constexpr uint32_t n = kRvvVLEN / width;                                 \
+    CcTest::InitializeVM();                                                  \
+    for (type fval : array) {                                                \
+      int_type rs1_fval = base::bit_cast<int_type>(fval);                    \
+      for (uint32_t mask = 0; mask < (1 << n); mask++) {                     \
+        int_type src[n] = {0};                                               \
+        int_type dst[n] = {0};                                               \
+        dst[0] = rs1_fval;                                                   \
+        for (uint32_t i = 0; i < n; i++) src[i] = i;                         \
+        auto fn = [mask](MacroAssembler& assm) {                             \
+          __ VU.set(t0, VSew::E##width, Vlmul::m1);                          \
+          __ vl(v1, a0, 0, VSew::E##width);                                  \
+          __ vl(v24, a1, 0, VSew::E##width);                                 \
+          __ vmv_vi(v0, mask);                                               \
+          __ vfmv_fs(ft0, v24);                                              \
+          __ vfmerge_vf(v2, ft0, v1);                                        \
+          __ vs(v2, a1, 0, VSew::E##width);                                  \
+        };                                                                   \
+        GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, fn);     \
+        for (uint32_t i = 0; i < n; i++) {                                   \
+          CHECK_EQ(expect_res, dst[i]);                                      \
+        }                                                                    \
+      }                                                                      \
+    }                                                                        \
+  }
+
+UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(
+    1, double, int64_t, 64, compiler::ValueHelper::GetVector<double>(),
+    ((mask >> i) & 0x1) ? rs1_fval : src[i])
+UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(2, float, int32_t, 32,
+                                      compiler::ValueHelper::GetVector<float>(),
+                                      ((mask >> i) & 0x1) ? rs1_fval : src[i])
+UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(3, double, int64_t, 64,
+                                      base::ArrayVector(double_sNaN),
                                       ((mask >> i) & 0x1) ? rs1_fval : src[i])
-UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(float, int32_t, 32,
+UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(4, float, int32_t, 32,
+                                      base::ArrayVector(float_sNaN),
                                       ((mask >> i) & 0x1) ? rs1_fval : src[i])
 #undef UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES
 
@@ -2557,7 +2723,7 @@ UTEST_RVV_VF_VFMERGE_VF_FORM_WITH_RES(float, int32_t, 32,
           __ instr_name(v2, v1, offset);                                     \
           __ vs(v2, a1, 0, VSew::E##width);                                  \
         };                                                                   \
-        GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn);     \
+        GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, fn);     \
         for (uint32_t i = 0; i < n; i++) {                                   \
           CHECK_EQ(expect_res, dst[i]);                                      \
         }                                                                    \
@@ -2620,7 +2786,7 @@ UTEST_RVV_VP_VSLIDE_VI_FORM_WITH_RES(vslideup_vi, uint8_t, 8, ARRAY(uint8_t),
           __ vs(v2, a1, 0, VSew::E##width);                                  \
         };                                                                   \
         type rs2_val = (type)offset;                                         \
-        GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, rs2_val, \
+        GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, rs2_val, \
                                         fn);                                 \
         for (uint32_t i = 0; i < n; i++) {                                   \
           CHECK_EQ(expect_res, dst[i]);                                      \
@@ -2690,7 +2856,7 @@ UTEST_RVV_VP_VSLIDE_VX_FORM_WITH_RES(vslideup_vx, uint8_t, 8, ARRAY(uint8_t),
         __ vs(v2, a1, 0, VSew::E##width);                                     \
       };                                                                      \
       type rs2_val = x + x;                                                   \
-      GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, rs2_val,    \
+      GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, rs2_val,    \
                                       fn);                                    \
       for (uint32_t i = 0; i < n; i++) {                                      \
         CHECK_EQ(expect_res, dst[i]);                                         \
@@ -2699,9 +2865,9 @@ UTEST_RVV_VP_VSLIDE_VX_FORM_WITH_RES(vslideup_vx, uint8_t, 8, ARRAY(uint8_t),
   }
 
 // Test for vslide1down_vx
-UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1down_vx, int64_t, 64,
-                                      ARRAY(int64_t),
-                                      (i + 1) < n ? src[i + 1] : rs2_val)
+// UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1down_vx, int64_t, 64,
+//                                       ARRAY(int64_t),
+//                                       (i + 1) < n ? src[i + 1] : rs2_val)
 UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1down_vx, int32_t, 32,
                                       ARRAY(int32_t),
                                       (i + 1) < n ? src[i + 1] : rs2_val)
@@ -2722,8 +2888,9 @@ UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1down_vx, uint8_t, 8,
                                       (i + 1) < n ? src[i + 1] : rs2_val)
 
 // Test for vslide1up_vx
-UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, int64_t, 64, ARRAY(int64_t),
-                                      (int64_t)i < 1 ? rs2_val : src[i - 1])
+// UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, int64_t, 64,
+// ARRAY(int64_t),
+//                                       (int64_t)i < 1 ? rs2_val : src[i - 1])
 UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, int32_t, 32, ARRAY(int32_t),
                                       (int32_t)i < 1 ? rs2_val : src[i - 1])
 UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, int16_t, 16, ARRAY(int16_t),
@@ -2743,7 +2910,7 @@ UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, uint8_t, 8, ARRAY(uint8_t),
 
 #define UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(instr_name, type, width, fval, \
                                               array, expect_res)             \
-  TEST(RISCV_UTEST_##instr_name##_##width) {                                 \
+  TEST(RISCV_UTEST_##instr_name##_##width##_##fval) {                        \
     if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
     constexpr uint32_t n = kRvvVLEN / width;                                 \
     CcTest::InitializeVM();                                                  \
@@ -2759,7 +2926,7 @@ UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, uint8_t, 8, ARRAY(uint8_t),
         __ instr_name(v2, v1, fa0);                                          \
         __ vs(v2, a1, 0, VSew::E##width);                                    \
       };                                                                     \
-      GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn);       \
+      GenAndRunTest<int32_t, int32_t>((int32_t)src, (int32_t)dst, fn);       \
       for (uint32_t i = 0; i < n; i++) {                                     \
         CHECK_EQ(expect_res, dst[i]);                                        \
       }                                                                      \
@@ -2767,17 +2934,31 @@ UTEST_RVV_VP_VSLIDE1_VX_FORM_WITH_RES(vslide1up_vx, uint8_t, 8, ARRAY(uint8_t),
   }
 
 // Test for vfslide1down_vf
-UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int64_t, 64, 1234.56,
-                                      ARRAY(int64_t),
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int64_t, 64,
+                                      0x40934A3D70A3D70A, ARRAY(int64_t),
+                                      (i + 1) < n ? src[i + 1] : src[0])
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int32_t, 32, 0x449A51EC,
+                                      ARRAY(int32_t),
+                                      (i + 1) < n ? src[i + 1] : src[0])
+// Test for vfslide1down_vf_signaling_NaN
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int64_t, 64,
+                                      0x7FF4000000000000, ARRAY(int64_t),
                                       (i + 1) < n ? src[i + 1] : src[0])
-UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int32_t, 32, 1234.56f,
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1down_vf, int32_t, 32, 0x7F400000,
                                       ARRAY(int32_t),
                                       (i + 1) < n ? src[i + 1] : src[0])
 // Test for vfslide1up_vf
-UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int64_t, 64, 1234.56,
-                                      ARRAY(int64_t),
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int64_t, 64,
+                                      0x40934A3D70A3D70A, ARRAY(int64_t),
+                                      (int64_t)i < 1 ? src[0] : src[i - 1])
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int32_t, 32, 0x449A51EC,
+                                      ARRAY(int32_t),
+                                      (int32_t)i < 1 ? src[0] : src[i - 1])
+// Test for vfslide1up_vf_signaling_NaN
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int64_t, 64,
+                                      0x7FF4000000000000, ARRAY(int64_t),
                                       (int64_t)i < 1 ? src[0] : src[i - 1])
-UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int32_t, 32, 1234.56f,
+UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int32_t, 32, 0x7F400000,
                                       ARRAY(int32_t),
                                       (int32_t)i < 1 ? src[0] : src[i - 1])
 #undef UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES
@@ -2786,10 +2967,10 @@ UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int32_t, 32, 1234.56f,
 #define UTEST_VFIRST_M_WITH_WIDTH(width)                            \
   TEST(RISCV_UTEST_vfirst_m_##width) {                              \
     if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;              \
-    constexpr uint32_t vlen = 128;                                  \
-    constexpr uint32_t n = vlen / width;                            \
+    constexpr int32_t vlen = 128;                                   \
+    constexpr int32_t n = vlen / width;                             \
     CcTest::InitializeVM();                                         \
-    for (uint32_t i = 0; i <= n; i++) {                             \
+    for (int32_t i = 0; i <= n; i++) {                              \
       uint64_t src[2] = {0};                                        \
       src[0] = 1 << i;                                              \
       auto fn = [](MacroAssembler& assm) {                          \
@@ -2797,8 +2978,8 @@ UTEST_RVV_VP_VSLIDE1_VF_FORM_WITH_RES(vfslide1up_vf, int32_t, 32, 1234.56f,
         __ vl(v2, a0, 0, VSew::E##width);                           \
         __ vfirst_m(a0, v2);                                        \
       };                                                            \
-      auto res = GenAndRunTest<int64_t, int64_t>((int64_t)src, fn); \
-      CHECK_EQ(i < n ? i : (int64_t)-1, res);                       \
+      auto res = GenAndRunTest<int32_t, int32_t>((int32_t)src, fn); \
+      CHECK_EQ(i < n ? i : (int32_t)-1, res);                       \
     }                                                               \
   }
 
@@ -2823,8 +3004,8 @@ UTEST_VFIRST_M_WITH_WIDTH(8)
         __ vl(v2, a0, 0, VSew::E##width);                             \
         __ vcpop_m(a0, v2);                                           \
       };                                                              \
-      auto res = GenAndRunTest<int64_t, int64_t>((int64_t)src, fn);   \
-      CHECK_EQ(std::__popcount(src[0]), res);                         \
+      auto res = GenAndRunTest<int32_t, int32_t>((int32_t)src, fn);   \
+      CHECK_EQ(__builtin_popcountl(src[0]), res);                     \
     }                                                                 \
   }
 
@@ -2833,6 +3014,32 @@ UTEST_VCPOP_M_WITH_WIDTH(32)
 UTEST_VCPOP_M_WITH_WIDTH(16)
 UTEST_VCPOP_M_WITH_WIDTH(8)
 
+TEST(RISCV_UTEST_WasmRvvS128const) {
+  if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;
+  CcTest::InitializeVM();
+  for (uint64_t x : compiler::ValueHelper::GetVector<int64_t>()) {
+    for (uint64_t y : compiler::ValueHelper::GetVector<int64_t>()) {
+      uint64_t src[2] = {x, y};
+      uint8_t vals[16];
+      volatile uint32_t result[kRvvVLEN / 32] = {0};
+      memcpy(vals, src, sizeof(vals));
+      auto fn = [vals, &result](MacroAssembler& assm) {
+        __ Push(kScratchReg);
+        __ WasmRvvS128const(v10, vals);
+        __ li(t1, Operand(int32_t(result)));
+        __ VU.set(t0, VSew::E32, Vlmul::m1);
+        __ vs(v10, t1, 0, VSew::E32);
+        __ Pop(kScratchReg);
+      };
+      GenAndRunTest(fn);
+      volatile uint64_t* result_addr =
+          reinterpret_cast<volatile uint64_t*>(&result[0]);
+      CHECK_EQ((uint64_t)*result_addr, x);
+      CHECK_EQ((uint64_t) * (result_addr + 1), y);
+    }
+  }
+}
+
 #undef UTEST_VCPOP_M_WITH_WIDTH
 #endif  // CAN_USE_RVV_INSTRUCTIONS
 #undef __
diff --git a/test/wasm-spec-tests/wasm-spec-tests.status b/test/wasm-spec-tests/wasm-spec-tests.status
index fb3184645b..7e858a5a45 100644
--- a/test/wasm-spec-tests/wasm-spec-tests.status
+++ b/test/wasm-spec-tests/wasm-spec-tests.status
@@ -157,9 +157,6 @@
    'skip-stack-guard-page': '--sim-stack-size=8192',
    'proposals/tail-call/skip-stack-guard-page': '--sim-stack-size=8192',
 
-   # SIMD is not fully implemented yet.
-   'simd*': [SKIP],
-
    'func': ['variant == stress', SKIP],
 }],  # 'arch == riscv32'
 
diff --git a/tools/testrunner/base_runner.py b/tools/testrunner/base_runner.py
index b604816b6e..57a98952dc 100644
--- a/tools/testrunner/base_runner.py
+++ b/tools/testrunner/base_runner.py
@@ -562,8 +562,7 @@ class BaseTestRunner(object):
         not self.build_config.simd_mips):
       return True
 
-    if (self.build_config.arch == 'loong64' or
-        self.build_config.arch == 'riscv32'):
+    if self.build_config.arch == 'loong64':
       return True
 
     # S390 hosts without VEF1 do not support Simd.
-- 
2.35.1

