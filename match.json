[[["./v8/src/regexp/riscv/regexp-macro-assembler-riscv.h", "./v8/src/regexp/arm64/regexp-macro-assembler-arm64.h"], 1.0, 1.0, [[{"name": "RegExpMacroAssemblerARM64", "content": "class V8_EXPORT_PRIVATE RegExpMacroAssemblerARM64\n    : public NativeRegExpMacroAssembler {\n public:\n  RegExpMacroAssemblerARM64(Isolate* isolate, Zone* zone, Mode mode,\n                            int registers_to_save);\n  ~RegExpMacroAssemblerARM64() override;\n  void AbortedCodeGeneration() override;\n  int stack_limit_slack() override;\n  void AdvanceCurrentPosition(int by) override;\n  void AdvanceRegister(int reg, int by) override;\n  void Backtrack() override;\n  void Bind(Label* label) override;\n  void CheckAtStart(int cp_offset, Label* on_at_start) override;\n  void CheckCharacter(unsigned c, Label* on_equal) override;\n  void CheckCharacterAfterAnd(unsigned c, unsigned mask,\n                              Label* on_equal) override;\n  void CheckCharacterGT(base::uc16 limit, Label* on_greater) override;\n  void CheckCharacterLT(base::uc16 limit, Label* on_less) override;\n  void CheckCharacters(base::Vector<const base::uc16> str, int cp_offset,\n                       Label* on_failure, bool check_end_of_string);\n  // A \"greedy loop\" is a loop that is both greedy and with a simple\n  // body. It has a particularly simple implementation.\n  void CheckGreedyLoop(Label* on_tos_equals_current_position) override;\n  void CheckNotAtStart(int cp_offset, Label* on_not_at_start) override;\n  void CheckNotBackReference(int start_reg, bool read_backward,\n                             Label* on_no_match) override;\n  void CheckNotBackReferenceIgnoreCase(int start_reg, bool read_backward,\n                                       bool unicode,\n                                       Label* on_no_match) override;\n  void CheckNotCharacter(unsigned c, Label* on_not_equal) override;\n  void CheckNotCharacterAfterAnd(unsigned c, unsigned mask,\n                                 Label* on_not_equal) override;\n  void CheckNotCharacterAfterMinusAnd(base::uc16 c, base::uc16 minus,\n                                      base::uc16 mask,\n                                      Label* on_not_equal) override;\n  void CheckCharacterInRange(base::uc16 from, base::uc16 to,\n                             Label* on_in_range) override;\n  void CheckCharacterNotInRange(base::uc16 from, base::uc16 to,\n                                Label* on_not_in_range) override;\n  bool CheckCharacterInRangeArray(const ZoneList<CharacterRange>* ranges,\n                                  Label* on_in_range) override;\n  bool CheckCharacterNotInRangeArray(const ZoneList<CharacterRange>* ranges,\n                                     Label* on_not_in_range) override;\n  void CheckBitInTable(Handle<ByteArray> table, Label* on_bit_set) override;\n\n  // Checks whether the given offset from the current position is before\n  // the end of the string.\n  void CheckPosition(int cp_offset, Label* on_outside_input) override;\n  bool CheckSpecialClassRanges(StandardCharacterSet type,\n                               Label* on_no_match) override;\n  void BindJumpTarget(Label* label = nullptr) override;\n  void Fail() override;\n  Handle<HeapObject> GetCode(Handle<String> source) override;\n  void GoTo(Label* label) override;\n  void IfRegisterGE(int reg, int comparand, Label* if_ge) override;\n  void IfRegisterLT(int reg, int comparand, Label* if_lt) override;\n  void IfRegisterEqPos(int reg, Label* if_eq) override;\n  IrregexpImplementation Implementation() override;\n  void LoadCurrentCharacterUnchecked(int cp_offset,\n                                     int character_count) override;\n  void PopCurrentPosition() override;\n  void PopRegister(int register_index) override;\n  void PushBacktrack(Label* label) override;\n  void PushCurrentPosition() override;\n  void PushRegister(int register_index,\n                    StackCheckFlag check_stack_limit) override;\n  void ReadCurrentPositionFromRegister(int reg) override;\n  void ReadStackPointerFromRegister(int reg) override;\n  void SetCurrentPositionFromEnd(int by) override;\n  void SetRegister(int register_index, int to) override;\n  bool Succeed() override;\n  void WriteCurrentPositionToRegister(int reg, int cp_offset) override;\n  void ClearRegisters(int reg_from, int reg_to) override;\n  void WriteStackPointerToRegister(int reg) override;\n\n  // Called from RegExp if the stack-guard is triggered.\n  // If the code object is relocated, the return address is fixed before\n  // returning.\n  // {raw_code} is an Address because this is called via ExternalReference.\n  static int CheckStackGuardState(Address* return_address, Address raw_code,\n                                  Address re_frame, int start_offset,\n                                  const uint8_t** input_start,\n                                  const uint8_t** input_end,\n                                  uintptr_t extra_space);\n\n private:\n  static constexpr int kFramePointerOffset = 0;\n\n  // Above the frame pointer - Stored registers and stack passed parameters.\n  static constexpr int kReturnAddressOffset =\n      kFramePointerOffset + kSystemPointerSize;\n  // Callee-saved registers (x19-x28).\n  static constexpr int kNumCalleeSavedRegisters = 10;\n  static constexpr int kCalleeSavedRegistersOffset =\n      kReturnAddressOffset + kSystemPointerSize;\n\n  // Below the frame pointer - the stack frame type marker and locals.\n  static constexpr int kFrameTypeOffset =\n      kFramePointerOffset - kSystemPointerSize;\n  static_assert(kFrameTypeOffset ==\n                CommonFrameConstants::kContextOrFrameTypeOffset);\n  static constexpr int kPaddingAfterFrameType = kSystemPointerSize;\n  // Register parameters stored by setup code.\n  static constexpr int kIsolateOffset =\n      kFrameTypeOffset - kPaddingAfterFrameType - kSystemPointerSize;\n  static constexpr int kDirectCallOffset = kIsolateOffset - kSystemPointerSize;\n  // For the case of global regular expression, we have room to store at least\n  // one set of capture results.  For the case of non-global regexp, we ignore\n  // this value.\n  static constexpr int kNumOutputRegistersOffset =\n      kDirectCallOffset - kSystemPointerSize;\n  static constexpr int kInputStringOffset =\n      kNumOutputRegistersOffset - kSystemPointerSize;\n  // When adding local variables remember to push space for them in\n  // the frame in GetCode.\n  static constexpr int kSuccessfulCapturesOffset =\n      kInputStringOffset - kSystemPointerSize;\n  static constexpr int kBacktrackCountOffset =\n      kSuccessfulCapturesOffset - kSystemPointerSize;\n  // Stores the initial value of the regexp stack pointer in a\n  // position-independent representation (in case the regexp stack grows and\n  // thus moves).\n  static constexpr int kRegExpStackBasePointerOffset =\n      kBacktrackCountOffset - kSystemPointerSize;\n  // A padding slot to preserve alignment.\n  static constexpr int kStackLocalPadding =\n      kRegExpStackBasePointerOffset - kSystemPointerSize;\n  static constexpr int kNumberOfStackLocals = 4;\n\n  // First position register address on the stack. Following positions are\n  // below it. A position is a 32 bit value.\n  static constexpr int kFirstRegisterOnStackOffset =\n      kStackLocalPadding - kWRegSize;\n  // A capture is a 64 bit value holding two position.\n  static constexpr int kFirstCaptureOnStackOffset =\n      kStackLocalPadding - kXRegSize;\n\n  static constexpr int kInitialBufferSize = 1024;\n\n  // Registers x0 to x7 are used to store the first captures, they need to be\n  // retained over calls to C++ code.\n  void PushCachedRegisters();\n  void PopCachedRegisters();\n\n  // When initializing registers to a non-position value we can unroll\n  // the loop. Set the limit of registers to unroll.\n  static constexpr int kNumRegistersToUnroll = 16;\n\n  // We are using x0 to x7 as a register cache. Each hardware register must\n  // contain one capture, that is two 32 bit registers. We can cache at most\n  // 16 registers.\n  static constexpr int kNumCachedRegisters = 16;\n\n  void CallCFunctionFromIrregexpCode(ExternalReference function,\n                                     int num_arguments);\n\n  // Check whether preemption has been requested.\n  void CheckPreemption();\n\n  // Check whether we are exceeding the stack limit on the backtrack stack.\n  void CheckStackLimit();\n\n  void CallCheckStackGuardState(Register scratch,\n                                Operand extra_space = Operand(0));\n  void CallIsCharacterInRangeArray(const ZoneList<CharacterRange>* ranges);\n\n  // Location of a 32 bit position register.\n  MemOperand register_location(int register_index);\n\n  // Location of a 64 bit capture, combining two position registers.\n  MemOperand capture_location(int register_index, Register scratch);\n\n  // Register holding the current input position as negative offset from\n  // the end of the string.\n  static constexpr Register current_input_offset() { return w21; }\n\n  // The register containing the current character after LoadCurrentCharacter.\n  static constexpr Register current_character() { return w22; }\n\n  // Register holding address of the end of the input string.\n  static constexpr Register input_end() { return x25; }\n\n  // Register holding address of the start of the input string.\n  static constexpr Register input_start() { return x26; }\n\n  // Register holding the offset from the start of the string where we should\n  // start matching.\n  static constexpr Register start_offset() { return w27; }\n\n  // Pointer to the output array's first element.\n  static constexpr Register output_array() { return x28; }\n\n  // Register holding the frame address. Local variables, parameters and\n  // regexp registers are addressed relative to this.\n  static constexpr Register frame_pointer() { return fp; }\n\n  // The register containing the backtrack stack top. Provides a meaningful\n  // name to the register.\n  static constexpr Register backtrack_stackpointer() { return x23; }\n\n  // Register holding pointer to the current code object.\n  static constexpr Register code_pointer() { return x20; }\n\n  // Register holding the value used for clearing capture registers.\n  static constexpr Register string_start_minus_one() { return w24; }\n  // The top 32 bit of this register is used to store this value\n  // twice. This is used for clearing more than one register at a time.\n  static constexpr Register twice_non_position_value() { return x24; }\n\n  // Byte size of chars in the string to match (decided by the Mode argument)\n  int char_size() const { return static_cast<int>(mode_); }\n\n  // Equivalent to a conditional branch to the label, unless the label\n  // is nullptr, in which case it is a conditional Backtrack.\n  void BranchOrBacktrack(Condition condition, Label* to);\n\n  // Compares reg against immmediate before calling BranchOrBacktrack.\n  // It makes use of the Cbz and Cbnz instructions.\n  void CompareAndBranchOrBacktrack(Register reg,\n                                   int immediate,\n                                   Condition condition,\n                                   Label* to);\n\n  inline void CallIf(Label* to, Condition condition);\n\n  // Save and restore the link register on the stack in a way that\n  // is GC-safe.\n  inline void SaveLinkRegister();\n  inline void RestoreLinkRegister();\n\n  // Pushes the value of a register on the backtrack stack. Decrements the\n  // stack pointer by a word size and stores the register's value there.\n  inline void Push(Register source);\n\n  // Pops a value from the backtrack stack. Reads the word at the stack pointer\n  // and increments it by a word size.\n  inline void Pop(Register target);\n\n  // This state indicates where the register actually is.\n  enum RegisterState {\n    STACKED,     // Resides in memory.\n    CACHED_LSW,  // Least Significant Word of a 64 bit hardware register.\n    CACHED_MSW   // Most Significant Word of a 64 bit hardware register.\n  };\n\n  RegisterState GetRegisterState(int register_index) {\n    DCHECK_LE(0, register_index);\n    if (register_index >= kNumCachedRegisters) {\n      return STACKED;\n    } else {\n      if ((register_index % 2) == 0) {\n        return CACHED_LSW;\n      } else {\n        return CACHED_MSW;\n      }\n    }\n  }\n\n  // Store helper that takes the state of the register into account.\n  inline void StoreRegister(int register_index, Register source);\n\n  // Returns a hardware W register that holds the value of the capture\n  // register.\n  //\n  // This function will try to use an existing cache register (w0-w7) for the\n  // result. Otherwise, it will load the value into maybe_result.\n  //\n  // If the returned register is anything other than maybe_result, calling code\n  // must not write to it.\n  inline Register GetRegister(int register_index, Register maybe_result);\n\n  // Returns the harware register (x0-x7) holding the value of the capture\n  // register.\n  // This assumes that the state of the register is not STACKED.\n  inline Register GetCachedRegister(int register_index);\n\n  void LoadRegExpStackPointerFromMemory(Register dst);\n  void StoreRegExpStackPointerToMemory(Register src, Register scratch);\n  void PushRegExpBasePointer(Register stack_pointer, Register scratch);\n  void PopRegExpBasePointer(Register stack_pointer_out, Register scratch);\n\n  Isolate* isolate() const { return masm_->isolate(); }\n\n  const std::unique_ptr<MacroAssembler> masm_;\n  const NoRootArrayScope no_root_array_scope_;\n\n  // Which mode to generate code for (LATIN1 or UC16).\n  const Mode mode_;\n\n  // One greater than maximal register index actually used.\n  int num_registers_;\n\n  // Number of registers to output at the end (the saved registers\n  // are always 0..num_saved_registers_-1)\n  const int num_saved_registers_;\n\n  // Labels used internally.\n  Label entry_label_;\n  Label start_label_;\n  Label success_label_;\n  Label backtrack_label_;\n  Label exit_label_;\n  Label check_preempt_label_;\n  Label stack_overflow_label_;\n  Label fallback_label_;\n}", "name_and_para": "class V8_EXPORT_PRIVATE RegExpMacroAssemblerARM64\n    : public NativeRegExpMacroAssembler "}, {"name": "RegExpMacroAssemblerRISCV", "content": "class V8_EXPORT_PRIVATE RegExpMacroAssemblerRISCV\n    : public NativeRegExpMacroAssembler {\n public:\n  RegExpMacroAssemblerRISCV(Isolate* isolate, Zone* zone, Mode mode,\n                            int registers_to_save);\n  ~RegExpMacroAssemblerRISCV() override;\n  int stack_limit_slack() override;\n  void AdvanceCurrentPosition(int by) override;\n  void AdvanceRegister(int reg, int by) override;\n  void Backtrack() override;\n  void Bind(Label* label) override;\n  void CheckAtStart(int cp_offset, Label* on_at_start) override;\n  void CheckCharacter(uint32_t c, Label* on_equal) override;\n  void CheckCharacterAfterAnd(uint32_t c, uint32_t mask,\n                              Label* on_equal) override;\n  void CheckCharacterGT(base::uc16 limit, Label* on_greater) override;\n  void CheckCharacterLT(base::uc16 limit, Label* on_less) override;\n  // A \"greedy loop\" is a loop that is both greedy and with a simple\n  // body. It has a particularly simple implementation.\n  void CheckGreedyLoop(Label* on_tos_equals_current_position) override;\n  void CheckNotAtStart(int cp_offset, Label* on_not_at_start) override;\n  void CheckNotBackReference(int start_reg, bool read_backward,\n                             Label* on_no_match) override;\n  void CheckNotBackReferenceIgnoreCase(int start_reg, bool read_backward,\n                                       bool unicode,\n                                       Label* on_no_match) override;\n  void CheckNotCharacter(uint32_t c, Label* on_not_equal) override;\n  void CheckNotCharacterAfterAnd(uint32_t c, uint32_t mask,\n                                 Label* on_not_equal) override;\n  void CheckNotCharacterAfterMinusAnd(base::uc16 c, base::uc16 minus,\n                                      base::uc16 mask,\n                                      Label* on_not_equal) override;\n  void CheckCharacterInRange(base::uc16 from, base::uc16 to,\n                             Label* on_in_range) override;\n  void CheckCharacterNotInRange(base::uc16 from, base::uc16 to,\n                                Label* on_not_in_range) override;\n  bool CheckCharacterInRangeArray(const ZoneList<CharacterRange>* ranges,\n                                  Label* on_in_range) override;\n  bool CheckCharacterNotInRangeArray(const ZoneList<CharacterRange>* ranges,\n                                     Label* on_not_in_range) override;\n  void CheckBitInTable(Handle<ByteArray> table, Label* on_bit_set) override;\n\n  // Checks whether the given offset from the current position is before\n  // the end of the string.\n  void CheckPosition(int cp_offset, Label* on_outside_input) override;\n  bool CheckSpecialClassRanges(StandardCharacterSet type,\n                               Label* on_no_match) override;\n  void Fail() override;\n  Handle<HeapObject> GetCode(Handle<String> source) override;\n  void GoTo(Label* label) override;\n  void IfRegisterGE(int reg, int comparand, Label* if_ge) override;\n  void IfRegisterLT(int reg, int comparand, Label* if_lt) override;\n  void IfRegisterEqPos(int reg, Label* if_eq) override;\n  IrregexpImplementation Implementation() override;\n  void LoadCurrentCharacterUnchecked(int cp_offset,\n                                     int character_count) override;\n  void PopCurrentPosition() override;\n  void PopRegister(int register_index) override;\n  void PushBacktrack(Label* label) override;\n  void PushCurrentPosition() override;\n  void PushRegister(int register_index,\n                    StackCheckFlag check_stack_limit) override;\n  void ReadCurrentPositionFromRegister(int reg) override;\n  void ReadStackPointerFromRegister(int reg) override;\n  void SetCurrentPositionFromEnd(int by) override;\n  void SetRegister(int register_index, int to) override;\n  bool Succeed() override;\n  void WriteCurrentPositionToRegister(int reg, int cp_offset) override;\n  void ClearRegisters(int reg_from, int reg_to) override;\n  void WriteStackPointerToRegister(int reg) override;\n#ifdef RISCV_HAS_NO_UNALIGNED\n  bool CanReadUnaligned() const override;\n#endif\n  // Called from RegExp if the stack-guard is triggered.\n  // If the code object is relocated, the return address is fixed before\n  // returning.\n  // {raw_code} is an Address because this is called via ExternalReference.\n  static int64_t CheckStackGuardState(Address* return_address, Address raw_code,\n                                      Address re_frame, uintptr_t extra_space);\n\n  void print_regexp_frame_constants();\n\n private:\n  // Offsets from frame_pointer() of function parameters and stored registers.\n  static constexpr int kFramePointerOffset = 0;\n\n  // Above the frame pointer - Stored registers and stack passed parameters.\n  // Registers s1 to s8, fp, and ra.\n  static constexpr int kStoredRegistersOffset = kFramePointerOffset;\n  // Return address (stored from link register, read into pc on return).\n\n  // This 9 is 8 s-regs (s1..s11) plus fp.\n  static constexpr int kNumCalleeRegsToRetain = 12;\n  static constexpr int kReturnAddressOffset =\n      kStoredRegistersOffset + kNumCalleeRegsToRetain * kSystemPointerSize;\n\n  // Stack frame header.\n  static constexpr int kStackFrameHeaderOffset = kReturnAddressOffset;\n  // Below the frame pointer - the stack frame type marker and locals.\n  static constexpr int kFrameTypeOffset =\n      kFramePointerOffset - kSystemPointerSize;\n  static_assert(kFrameTypeOffset ==\n                (V8_EMBEDDED_CONSTANT_POOL_BOOL\n                     ? kSystemPointerSize +\n                           CommonFrameConstants::kContextOrFrameTypeOffset\n                     : CommonFrameConstants::kContextOrFrameTypeOffset));\n  // Register parameters stored by setup code.\n  static constexpr int kIsolateOffset = kFrameTypeOffset - kSystemPointerSize;\n  static constexpr int kDirectCallOffset = kIsolateOffset - kSystemPointerSize;\n  static constexpr int kNumOutputRegistersOffset =\n      kDirectCallOffset - kSystemPointerSize;\n  static constexpr int kRegisterOutputOffset =\n      kNumOutputRegistersOffset - kSystemPointerSize;\n  static constexpr int kInputEndOffset =\n      kRegisterOutputOffset - kSystemPointerSize;\n  static constexpr int kInputStartOffset = kInputEndOffset - kSystemPointerSize;\n  static constexpr int kStartIndexOffset =\n      kInputStartOffset - kSystemPointerSize;\n  static constexpr int kInputStringOffset =\n      kStartIndexOffset - kSystemPointerSize;\n  // When adding local variables remember to push space for them in\n  // the frame in GetCode.\n  static constexpr int kSuccessfulCapturesOffset =\n      kInputStringOffset - kSystemPointerSize;\n  static constexpr int kStringStartMinusOneOffset =\n      kSuccessfulCapturesOffset - kSystemPointerSize;\n  static constexpr int kBacktrackCountOffset =\n      kStringStartMinusOneOffset - kSystemPointerSize;\n  // Stores the initial value of the regexp stack pointer in a\n  // position-independent representation (in case the regexp stack grows and\n  // thus moves).\n  static constexpr int kRegExpStackBasePointerOffset =\n      kBacktrackCountOffset - kSystemPointerSize;\n  static constexpr int kNumberOfStackLocals = 4;\n  // First register address. Following registers are below it on the stack.\n  static constexpr int kRegisterZeroOffset =\n      kRegExpStackBasePointerOffset - kSystemPointerSize;\n\n  // Initial size of code buffer.\n  static constexpr int kInitialBufferSize = 1024;\n\n  void CallCFunctionFromIrregexpCode(ExternalReference function,\n                                     int num_arguments);\n  void PushCallerSavedRegisters();\n  void PopCallerSavedRegisters();\n\n  // Check whether preemption has been requested.\n  void CheckPreemption();\n\n  // Check whether we are exceeding the stack limit on the backtrack stack.\n  void CheckStackLimit();\n\n  void CallCheckStackGuardState(Register scratch,\n                                Operand extra_space_for_variables = Operand(0));\n  void CallIsCharacterInRangeArray(const ZoneList<CharacterRange>* ranges);\n\n  // The ebp-relative location of a regexp register.\n  MemOperand register_location(int register_index);\n\n  // Register holding the current input position as negative offset from\n  // the end of the string.\n  static constexpr Register current_input_offset() { return s2; }\n\n  // The register containing the current character after LoadCurrentCharacter.\n  static constexpr Register current_character() { return s5; }\n\n  // Register holding address of the end of the input string.\n  static constexpr Register end_of_input_address() { return s6; }\n\n  // Register holding the frame address. Local variables, parameters and\n  // regexp registers are addressed relative to this.\n  static constexpr Register frame_pointer() { return fp; }\n\n  // The register containing the backtrack stack top. Provides a meaningful\n  // name to the register.\n  // s7 should not be used here because baseline sparkplug uses s7 as context\n  // register.\n  static constexpr Register backtrack_stackpointer() { return s8; }\n\n  // Register holding pointer to the current code object.\n  static constexpr Register code_pointer() { return s1; }\n\n  // Byte size of chars in the string to match (decided by the Mode argument).\n  inline int char_size() const { return static_cast<int>(mode_); }\n\n  // Equivalent to a conditional branch to the label, unless the label\n  // is nullptr, in which case it is a conditional Backtrack.\n  void BranchOrBacktrack(Label* to, Condition condition, Register rs,\n                         const Operand& rt);\n\n  // Call and return internally in the generated code in a way that\n  // is GC-safe (i.e., doesn't leave absolute code addresses on the stack)\n  inline void SafeCall(Label* to, Condition cond, Register rs,\n                       const Operand& rt);\n  inline void SafeReturn();\n  inline void SafeCallTarget(Label* name);\n\n  // Pushes the value of a register on the backtrack stack. Decrements the\n  // stack pointer by a word size and stores the register's value there.\n  inline void Push(Register source);\n\n  // Pops a value from the backtrack stack. Reads the word at the stack pointer\n  // and increments it by a word size.\n  inline void Pop(Register target);\n\n  void LoadRegExpStackPointerFromMemory(Register dst);\n  void StoreRegExpStackPointerToMemory(Register src, Register scratch);\n  void PushRegExpBasePointer(Register stack_pointer, Register scratch);\n  void PopRegExpBasePointer(Register stack_pointer_out, Register scratch);\n\n  Isolate* isolate() const { return masm_->isolate(); }\n\n  const std::unique_ptr<MacroAssembler> masm_;\n  const NoRootArrayScope no_root_array_scope_;\n\n  // Which mode to generate code for (Latin1 or UC16).\n  const Mode mode_;\n\n  // One greater than maximal register index actually used.\n  int num_registers_;\n\n  // Number of registers to output at the end (the saved registers\n  // are always 0..num_saved_registers_-1).\n  const int num_saved_registers_;\n\n  // Labels used internally.\n  Label entry_label_;\n  Label start_label_;\n  Label success_label_;\n  Label backtrack_label_;\n  Label exit_label_;\n  Label check_preempt_label_;\n  Label stack_overflow_label_;\n  Label internal_failure_label_;\n  Label fallback_label_;\n}", "name_and_para": "class V8_EXPORT_PRIVATE RegExpMacroAssemblerRISCV\n    : public NativeRegExpMacroAssembler "}]]], [["./v8/src/regexp/riscv/regexp-macro-assembler-riscv.cc", "./v8/src/regexp/arm64/regexp-macro-assembler-arm64.cc"], 0.8289473684210527, 0.9402985074626866, [[{"name": "RegExpMacroAssemblerARM64::LoadCurrentCharacterUnchecked", "content": "void RegExpMacroAssemblerARM64::LoadCurrentCharacterUnchecked(int cp_offset,\n                                                              int characters) {\n  Register offset = current_input_offset();\n\n  // The ldr, str, ldrh, strh instructions can do unaligned accesses, if the CPU\n  // and the operating system running on the target allow it.\n  // If unaligned load/stores are not supported then this function must only\n  // be used to load a single character at a time.\n\n  // ARMv8 supports unaligned accesses but V8 or the kernel can decide to\n  // disable it.\n  // TODO(pielan): See whether or not we should disable unaligned accesses.\n  if (!CanReadUnaligned()) {\n    DCHECK_EQ(1, characters);\n  }\n\n  if (cp_offset != 0) {\n    if (v8_flags.debug_code) {\n      __ Mov(x10, cp_offset * char_size());\n      __ Add(x10, x10, Operand(current_input_offset(), SXTW));\n      __ Cmp(x10, Operand(w10, SXTW));\n      // The offset needs to fit in a W register.\n      __ Check(eq, AbortReason::kOffsetOutOfRange);\n    } else {\n      __ Add(w10, current_input_offset(), cp_offset * char_size());\n    }\n    offset = w10;\n  }\n\n  if (mode_ == LATIN1) {\n    if (characters == 4) {\n      __ Ldr(current_character(), MemOperand(input_end(), offset, SXTW));\n    } else if (characters == 2) {\n      __ Ldrh(current_character(), MemOperand(input_end(), offset, SXTW));\n    } else {\n      DCHECK_EQ(1, characters);\n      __ Ldrb(current_character(), MemOperand(input_end(), offset, SXTW));\n    }\n  } else {\n    DCHECK(mode_ == UC16);\n    if (characters == 2) {\n      __ Ldr(current_character(), MemOperand(input_end(), offset, SXTW));\n    } else {\n      DCHECK_EQ(1, characters);\n      __ Ldrh(current_character(), MemOperand(input_end(), offset, SXTW));\n    }\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::LoadCurrentCharacterUnchecked(int cp_offset,\n                                                              int characters) "}, {"name": "RegExpMacroAssemblerRISCV::LoadCurrentCharacterUnchecked", "content": "void RegExpMacroAssemblerRISCV::LoadCurrentCharacterUnchecked(int cp_offset,\n                                                              int characters) {\n  Register offset = current_input_offset();\n  if (cp_offset != 0) {\n    // kScratchReg2 is not being used to store the capture start index at this\n    // point.\n    __ AddWord(kScratchReg2, current_input_offset(),\n               Operand(cp_offset * char_size()));\n    offset = kScratchReg2;\n  }\n  // If unaligned load/stores are not supported then this function must only\n  // be used to load a single character at a time.\n  if (!CanReadUnaligned()) {\n    DCHECK_EQ(1, characters);\n  }\n\n  if (mode_ == LATIN1) {\n    if (characters == 4) {\n      __ AddWord(kScratchReg, end_of_input_address(), offset);\n      __ Load32U(current_character(), MemOperand(kScratchReg));\n    } else if (characters == 2) {\n      __ AddWord(kScratchReg, end_of_input_address(), offset);\n      __ Lhu(current_character(), MemOperand(kScratchReg));\n    } else {\n      DCHECK_EQ(1, characters);\n      __ AddWord(kScratchReg, end_of_input_address(), offset);\n      __ Lbu(current_character(), MemOperand(kScratchReg));\n    }\n  } else {\n    DCHECK_EQ(UC16, mode_);\n    if (characters == 2) {\n      __ AddWord(kScratchReg, end_of_input_address(), offset);\n      __ Load32U(current_character(), MemOperand(kScratchReg));\n    } else {\n      DCHECK_EQ(1, characters);\n      __ AddWord(kScratchReg, end_of_input_address(), offset);\n      __ Lhu(current_character(), MemOperand(kScratchReg));\n    }\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::LoadCurrentCharacterUnchecked(int cp_offset,\n                                                              int characters) "}], [{"name": "RegExpMacroAssemblerARM64::register_location", "content": "MemOperand RegExpMacroAssemblerARM64::register_location(int register_index) {\n  DCHECK(register_index < (1<<30));\n  DCHECK_LE(kNumCachedRegisters, register_index);\n  if (num_registers_ <= register_index) {\n    num_registers_ = register_index + 1;\n  }\n  register_index -= kNumCachedRegisters;\n  int offset = kFirstRegisterOnStackOffset - register_index * kWRegSize;\n  return MemOperand(frame_pointer(), offset);\n}", "name_and_para": "MemOperand RegExpMacroAssemblerARM64::register_location(int register_index) "}, {"name": "RegExpMacroAssemblerRISCV::register_location", "content": "MemOperand RegExpMacroAssemblerRISCV::register_location(int register_index) {\n  DCHECK(register_index < (1 << 30));\n  if (num_registers_ <= register_index) {\n    num_registers_ = register_index + 1;\n  }\n  return MemOperand(frame_pointer(),\n                    kRegisterZeroOffset - register_index * kSystemPointerSize);\n}", "name_and_para": "MemOperand RegExpMacroAssemblerRISCV::register_location(int register_index) "}], [{"name": "RegExpMacroAssemblerARM64::Pop", "content": "void RegExpMacroAssemblerARM64::Pop(Register target) {\n  DCHECK(target.Is32Bits());\n  DCHECK_NE(target, backtrack_stackpointer());\n  __ Ldr(target,\n         MemOperand(backtrack_stackpointer(), kWRegSize, PostIndex));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::Pop(Register target) "}, {"name": "RegExpMacroAssemblerRISCV::Pop", "content": "void RegExpMacroAssemblerRISCV::Pop(Register target) {\n  DCHECK(target != backtrack_stackpointer());\n  __ Lw(target, MemOperand(backtrack_stackpointer()));\n  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(), kIntSize);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::Pop(Register target) "}], [{"name": "RegExpMacroAssemblerARM64::Push", "content": "void RegExpMacroAssemblerARM64::Push(Register source) {\n  DCHECK(source.Is32Bits());\n  DCHECK_NE(source, backtrack_stackpointer());\n  __ Str(source,\n         MemOperand(backtrack_stackpointer(),\n                    -static_cast<int>(kWRegSize),\n                    PreIndex));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::Push(Register source) "}, {"name": "RegExpMacroAssemblerRISCV::Push", "content": "void RegExpMacroAssemblerRISCV::Push(Register source) {\n  DCHECK(source != backtrack_stackpointer());\n  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(),\n             Operand(-kIntSize));\n  __ Sw(source, MemOperand(backtrack_stackpointer()));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::Push(Register source) "}], [{"name": "RegExpMacroAssemblerARM64::CheckStackLimit", "content": "void RegExpMacroAssemblerARM64::CheckStackLimit() {\n  ExternalReference stack_limit =\n      ExternalReference::address_of_regexp_stack_limit_address(isolate());\n  __ Mov(x10, stack_limit);\n  __ Ldr(x10, MemOperand(x10));\n  __ Cmp(backtrack_stackpointer(), x10);\n  CallIf(&stack_overflow_label_, ls);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckStackLimit() "}, {"name": "RegExpMacroAssemblerRISCV::CheckStackLimit", "content": "void RegExpMacroAssemblerRISCV::CheckStackLimit() {\n  ExternalReference stack_limit =\n      ExternalReference::address_of_regexp_stack_limit_address(\n          masm_->isolate());\n\n  __ li(a0, Operand(stack_limit));\n  __ LoadWord(a0, MemOperand(a0));\n  SafeCall(&stack_overflow_label_, Uless_equal, backtrack_stackpointer(),\n           Operand(a0));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckStackLimit() "}], [{"name": "RegExpMacroAssemblerARM64::CheckPreemption", "content": "void RegExpMacroAssemblerARM64::CheckPreemption() {\n  // Check for preemption.\n  ExternalReference stack_limit =\n      ExternalReference::address_of_jslimit(isolate());\n  __ Mov(x10, stack_limit);\n  __ Ldr(x10, MemOperand(x10));\n  __ Cmp(sp, x10);\n  CallIf(&check_preempt_label_, ls);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckPreemption() "}, {"name": "RegExpMacroAssemblerRISCV::CheckPreemption", "content": "void RegExpMacroAssemblerRISCV::CheckPreemption() {\n  // Check for preemption.\n  ExternalReference stack_limit =\n      ExternalReference::address_of_jslimit(masm_->isolate());\n  __ li(a0, Operand(stack_limit));\n  __ LoadWord(a0, MemOperand(a0));\n  SafeCall(&check_preempt_label_, Uless_equal, sp, Operand(a0));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckPreemption() "}], [{"name": "RegExpMacroAssemblerARM64::CallCFunctionFromIrregexpCode", "content": "void RegExpMacroAssemblerARM64::CallCFunctionFromIrregexpCode(\n    ExternalReference function, int num_arguments) {\n  // Irregexp code must not set fast_c_call_caller_fp and fast_c_call_caller_pc\n  // since\n  //\n  // 1. it may itself have been called using CallCFunction and nested calls are\n  //    unsupported, and\n  // 2. it may itself have been called directly from C where the frame pointer\n  //    might not be set (-fomit-frame-pointer), and thus frame iteration would\n  //    fail.\n  //\n  // See also: crbug.com/v8/12670#c17.\n  __ CallCFunction(function, num_arguments, SetIsolateDataSlots::kNo);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CallCFunctionFromIrregexpCode(\n    ExternalReference function, int num_arguments) "}, {"name": "RegExpMacroAssemblerRISCV::CallCFunctionFromIrregexpCode", "content": "void RegExpMacroAssemblerRISCV::CallCFunctionFromIrregexpCode(\n    ExternalReference function, int num_arguments) {\n  // Irregexp code must not set fast_c_call_caller_fp and fast_c_call_caller_pc\n  // since\n  //\n  // 1. it may itself have been called using CallCFunction and nested calls are\n  //    unsupported, and\n  // 2. it may itself have been called directly from C where the frame pointer\n  //    might not be set (-fomit-frame-pointer), and thus frame iteration would\n  //    fail.\n  //\n  // See also: crbug.com/v8/12670#c17.\n  __ CallCFunction(function, num_arguments, SetIsolateDataSlots::kNo);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CallCFunctionFromIrregexpCode(\n    ExternalReference function, int num_arguments) "}], [{"name": "RegExpMacroAssemblerARM64::BranchOrBacktrack", "content": "void RegExpMacroAssemblerARM64::BranchOrBacktrack(Condition condition,\n                                                  Label* to) {\n  if (condition == al) {  // Unconditional.\n    if (to == nullptr) {\n      Backtrack();\n      return;\n    }\n    __ B(to);\n    return;\n  }\n  if (to == nullptr) {\n    to = &backtrack_label_;\n  }\n  __ B(condition, to);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::BranchOrBacktrack(Condition condition,\n                                                  Label* to) "}, {"name": "RegExpMacroAssemblerRISCV::BranchOrBacktrack", "content": "void RegExpMacroAssemblerRISCV::BranchOrBacktrack(Label* to,\n                                                  Condition condition,\n                                                  Register rs,\n                                                  const Operand& rt) {\n  if (condition == al) {  // Unconditional.\n    if (to == nullptr) {\n      Backtrack();\n      return;\n    }\n    __ jmp(to);\n    return;\n  }\n  if (to == nullptr) {\n    __ Branch(&backtrack_label_, condition, rs, rt);\n    return;\n  }\n  __ Branch(to, condition, rs, rt);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::BranchOrBacktrack(Label* to,\n                                                  Condition condition,\n                                                  Register rs,\n                                                  const Operand& rt) "}], [{"name": "RegExpMacroAssemblerARM64::CallCheckStackGuardState", "content": "void RegExpMacroAssemblerARM64::CallCheckStackGuardState(Register scratch,\n                                                         Operand extra_space) {\n  DCHECK(!isolate()->IsGeneratingEmbeddedBuiltins());\n  DCHECK(!masm_->options().isolate_independent_code);\n\n  // Allocate space on the stack to store the return address. The\n  // CheckStackGuardState C++ function will override it if the code\n  // moved. Allocate extra space for 2 arguments passed by pointers.\n  // AAPCS64 requires the stack to be 16 byte aligned.\n  int alignment = masm_->ActivationFrameAlignment();\n  DCHECK_EQ(alignment % 16, 0);\n  int align_mask = (alignment / kXRegSize) - 1;\n  int xreg_to_claim = (3 + align_mask) & ~align_mask;\n\n  __ Claim(xreg_to_claim);\n\n  __ Mov(x6, extra_space);\n  // CheckStackGuardState needs the end and start addresses of the input string.\n  __ Poke(input_end(), 2 * kSystemPointerSize);\n  __ Add(x5, sp, 2 * kSystemPointerSize);\n  __ Poke(input_start(), kSystemPointerSize);\n  __ Add(x4, sp, kSystemPointerSize);\n\n  __ Mov(w3, start_offset());\n  // RegExp code frame pointer.\n  __ Mov(x2, frame_pointer());\n  // InstructionStream of self.\n  __ Mov(x1, Operand(masm_->CodeObject()));\n\n  // We need to pass a pointer to the return address as first argument.\n  // DirectCEntry will place the return address on the stack before calling so\n  // the stack pointer will point to it.\n  __ Mov(x0, sp);\n\n  DCHECK_EQ(scratch, x10);\n  ExternalReference check_stack_guard_state =\n      ExternalReference::re_check_stack_guard_state();\n  __ Mov(scratch, check_stack_guard_state);\n\n  __ CallBuiltin(Builtin::kDirectCEntry);\n\n  // The input string may have been moved in memory, we need to reload it.\n  __ Peek(input_start(), kSystemPointerSize);\n  __ Peek(input_end(), 2 * kSystemPointerSize);\n\n  __ Drop(xreg_to_claim);\n\n  // Reload the InstructionStream pointer.\n  __ Mov(code_pointer(), Operand(masm_->CodeObject()));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CallCheckStackGuardState(Register scratch,\n                                                         Operand extra_space) "}, {"name": "RegExpMacroAssemblerRISCV::CallCheckStackGuardState", "content": "void RegExpMacroAssemblerRISCV::CallCheckStackGuardState(Register scratch,\n                                                         Operand extra_space) {\n  DCHECK(!isolate()->IsGeneratingEmbeddedBuiltins());\n  DCHECK(!masm_->options().isolate_independent_code);\n\n  int stack_alignment = base::OS::ActivationFrameAlignment();\n\n  // Align the stack pointer and save the original sp value on the stack.\n  __ mv(scratch, sp);\n  __ SubWord(sp, sp, Operand(kSystemPointerSize));\n  DCHECK(base::bits::IsPowerOfTwo(stack_alignment));\n  __ And(sp, sp, Operand(-stack_alignment));\n  __ StoreWord(scratch, MemOperand(sp));\n\n  __ li(a3, extra_space);\n  __ mv(a2, frame_pointer());\n  // InstructionStream of self.\n  __ li(a1, Operand(masm_->CodeObject()), CONSTANT_SIZE);\n\n  // We need to make room for the return address on the stack.\n  DCHECK(IsAligned(stack_alignment, kSystemPointerSize));\n  __ SubWord(sp, sp, Operand(stack_alignment));\n\n  // The stack pointer now points to cell where the return address will be\n  // written. Arguments are in registers, meaning we treat the return address as\n  // argument 5. Since DirectCEntry will handle allocating space for the C\n  // argument slots, we don't need to care about that here. This is how the\n  // stack will look (sp meaning the value of sp at this moment):\n  // [sp + 3] - empty slot if needed for alignment.\n  // [sp + 2] - saved sp.\n  // [sp + 1] - second word reserved for return value.\n  // [sp + 0] - first word reserved for return value.\n\n  // a0 will point to the return address, placed by DirectCEntry.\n  __ mv(a0, sp);\n\n  ExternalReference stack_guard_check =\n      ExternalReference::re_check_stack_guard_state();\n  __ li(t6, Operand(stack_guard_check));\n\n  EmbeddedData d = EmbeddedData::FromBlob();\n  CHECK(Builtins::IsIsolateIndependent(Builtin::kDirectCEntry));\n  Address entry = d.InstructionStartOf(Builtin::kDirectCEntry);\n  __ li(kScratchReg, Operand(entry, RelocInfo::OFF_HEAP_TARGET));\n  __ Call(kScratchReg);\n\n  // DirectCEntry allocated space for the C argument slots so we have to\n  // drop them with the return address from the stack with loading saved sp.\n  // At this point stack must look:\n  // [sp + 7] - empty slot if needed for alignment.\n  // [sp + 6] - saved sp.\n  // [sp + 5] - second word reserved for return value.\n  // [sp + 4] - first word reserved for return value.\n  // [sp + 3] - C argument slot.\n  // [sp + 2] - C argument slot.\n  // [sp + 1] - C argument slot.\n  // [sp + 0] - C argument slot.\n  __ LoadWord(sp, MemOperand(sp, stack_alignment + kCArgsSlotsSize));\n\n  __ li(code_pointer(), Operand(masm_->CodeObject()));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CallCheckStackGuardState(Register scratch,\n                                                         Operand extra_space) "}], [{"name": "RegExpMacroAssemblerARM64::CheckPosition", "content": "void RegExpMacroAssemblerARM64::CheckPosition(int cp_offset,\n                                              Label* on_outside_input) {\n  if (cp_offset >= 0) {\n    CompareAndBranchOrBacktrack(current_input_offset(),\n                                -cp_offset * char_size(), ge, on_outside_input);\n  } else {\n    __ Add(w12, current_input_offset(), Operand(cp_offset * char_size()));\n    __ Cmp(w12, string_start_minus_one());\n    BranchOrBacktrack(le, on_outside_input);\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckPosition(int cp_offset,\n                                              Label* on_outside_input) "}, {"name": "RegExpMacroAssemblerRISCV::CheckPosition", "content": "void RegExpMacroAssemblerRISCV::CheckPosition(int cp_offset,\n                                              Label* on_outside_input) {\n  if (cp_offset >= 0) {\n    BranchOrBacktrack(on_outside_input, ge, current_input_offset(),\n                      Operand(-cp_offset * char_size()));\n  } else {\n    __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n    __ AddWord(a0, current_input_offset(), Operand(cp_offset * char_size()));\n    BranchOrBacktrack(on_outside_input, le, a0, Operand(a1));\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckPosition(int cp_offset,\n                                              Label* on_outside_input) "}], [{"name": "RegExpMacroAssemblerARM64::CheckStackGuardState", "content": "int RegExpMacroAssemblerARM64::CheckStackGuardState(\n    Address* return_address, Address raw_code, Address re_frame,\n    int start_index, const uint8_t** input_start, const uint8_t** input_end,\n    uintptr_t extra_space) {\n  Tagged<InstructionStream> re_code =\n      InstructionStream::cast(Tagged<Object>(raw_code));\n  return NativeRegExpMacroAssembler::CheckStackGuardState(\n      frame_entry<Isolate*>(re_frame, kIsolateOffset), start_index,\n      static_cast<RegExp::CallOrigin>(\n          frame_entry<int>(re_frame, kDirectCallOffset)),\n      return_address, re_code,\n      frame_entry_address<Address>(re_frame, kInputStringOffset), input_start,\n      input_end, extra_space);\n}", "name_and_para": "int RegExpMacroAssemblerARM64::CheckStackGuardState(\n    Address* return_address, Address raw_code, Address re_frame,\n    int start_index, const uint8_t** input_start, const uint8_t** input_end,\n    uintptr_t extra_space) "}, {"name": "RegExpMacroAssemblerRISCV::CheckStackGuardState", "content": "int64_t RegExpMacroAssemblerRISCV::CheckStackGuardState(Address* return_address,\n                                                        Address raw_code,\n                                                        Address re_frame,\n                                                        uintptr_t extra_space) {\n  Tagged<InstructionStream> re_code =\n      InstructionStream::cast(Tagged<Object>(raw_code));\n  return NativeRegExpMacroAssembler::CheckStackGuardState(\n      frame_entry<Isolate*>(re_frame, kIsolateOffset),\n      static_cast<int>(frame_entry<int64_t>(re_frame, kStartIndexOffset)),\n      static_cast<RegExp::CallOrigin>(\n          frame_entry<int64_t>(re_frame, kDirectCallOffset)),\n      return_address, re_code,\n      frame_entry_address<Address>(re_frame, kInputStringOffset),\n      frame_entry_address<const uint8_t*>(re_frame, kInputStartOffset),\n      frame_entry_address<const uint8_t*>(re_frame, kInputEndOffset),\n      extra_space);\n}", "name_and_para": "int64_t RegExpMacroAssemblerRISCV::CheckStackGuardState(Address* return_address,\n                                                        Address raw_code,\n                                                        Address re_frame,\n                                                        uintptr_t extra_space) "}], [{"name": "frame_entry_address", "content": "static T* frame_entry_address(Address re_frame, int frame_offset) {\n  return reinterpret_cast<T*>(re_frame + frame_offset);\n}", "name_and_para": "static T* frame_entry_address(Address re_frame, int frame_offset) "}, {"name": "frame_entry_address", "content": "static T* frame_entry_address(Address re_frame, int frame_offset) {\n  return reinterpret_cast<T*>(re_frame + frame_offset);\n}", "name_and_para": "static T* frame_entry_address(Address re_frame, int frame_offset) "}], [{"name": "frame_entry", "content": "static T& frame_entry(Address re_frame, int frame_offset) {\n  return *reinterpret_cast<T*>(re_frame + frame_offset);\n}", "name_and_para": "static T& frame_entry(Address re_frame, int frame_offset) "}, {"name": "frame_entry", "content": "static T& frame_entry(Address re_frame, int frame_offset) {\n  return reinterpret_cast<T&>(Memory<int32_t>(re_frame + frame_offset));\n}", "name_and_para": "static T& frame_entry(Address re_frame, int frame_offset) "}], [{"name": "RegExpMacroAssemblerARM64::ClearRegisters", "content": "void RegExpMacroAssemblerARM64::ClearRegisters(int reg_from, int reg_to) {\n  DCHECK(reg_from <= reg_to);\n  int num_registers = reg_to - reg_from + 1;\n\n  // If the first capture register is cached in a hardware register but not\n  // aligned on a 64-bit one, we need to clear the first one specifically.\n  if ((reg_from < kNumCachedRegisters) && ((reg_from % 2) != 0)) {\n    StoreRegister(reg_from, string_start_minus_one());\n    num_registers--;\n    reg_from++;\n  }\n\n  // Clear cached registers in pairs as far as possible.\n  while ((num_registers >= 2) && (reg_from < kNumCachedRegisters)) {\n    DCHECK(GetRegisterState(reg_from) == CACHED_LSW);\n    __ Mov(GetCachedRegister(reg_from), twice_non_position_value());\n    reg_from += 2;\n    num_registers -= 2;\n  }\n\n  if ((num_registers % 2) == 1) {\n    StoreRegister(reg_from, string_start_minus_one());\n    num_registers--;\n    reg_from++;\n  }\n\n  if (num_registers > 0) {\n    // If there are some remaining registers, they are stored on the stack.\n    DCHECK_LE(kNumCachedRegisters, reg_from);\n\n    // Move down the indexes of the registers on stack to get the correct offset\n    // in memory.\n    reg_from -= kNumCachedRegisters;\n    reg_to -= kNumCachedRegisters;\n    // We should not unroll the loop for less than 2 registers.\n    static_assert(kNumRegistersToUnroll > 2);\n    // We position the base pointer to (reg_from + 1).\n    int base_offset =\n        kFirstRegisterOnStackOffset - kWRegSize - (kWRegSize * reg_from);\n    if (num_registers > kNumRegistersToUnroll) {\n      Register base = x10;\n      __ Add(base, frame_pointer(), base_offset);\n\n      Label loop;\n      __ Mov(x11, num_registers);\n      __ Bind(&loop);\n      __ Str(twice_non_position_value(),\n             MemOperand(base, -kSystemPointerSize, PostIndex));\n      __ Sub(x11, x11, 2);\n      __ Cbnz(x11, &loop);\n    } else {\n      for (int i = reg_from; i <= reg_to; i += 2) {\n        __ Str(twice_non_position_value(),\n               MemOperand(frame_pointer(), base_offset));\n        base_offset -= kWRegSize * 2;\n      }\n    }\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::ClearRegisters(int reg_from, int reg_to) "}, {"name": "RegExpMacroAssemblerRISCV::ClearRegisters", "content": "void RegExpMacroAssemblerRISCV::ClearRegisters(int reg_from, int reg_to) {\n  DCHECK(reg_from <= reg_to);\n  __ LoadWord(a0, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n  for (int reg = reg_from; reg <= reg_to; reg++) {\n    __ StoreWord(a0, register_location(reg));\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::ClearRegisters(int reg_from, int reg_to) "}], [{"name": "RegExpMacroAssemblerARM64::WriteCurrentPositionToRegister", "content": "void RegExpMacroAssemblerARM64::WriteCurrentPositionToRegister(int reg,\n                                                               int cp_offset) {\n  Register position = current_input_offset();\n  if (cp_offset != 0) {\n    position = w10;\n    __ Add(position, current_input_offset(), cp_offset * char_size());\n  }\n  StoreRegister(reg, position);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::WriteCurrentPositionToRegister(int reg,\n                                                               int cp_offset) "}, {"name": "RegExpMacroAssemblerRISCV::WriteCurrentPositionToRegister", "content": "void RegExpMacroAssemblerRISCV::WriteCurrentPositionToRegister(int reg,\n                                                               int cp_offset) {\n  if (cp_offset == 0) {\n    __ StoreWord(current_input_offset(), register_location(reg));\n  } else {\n    __ AddWord(a0, current_input_offset(), Operand(cp_offset * char_size()));\n    __ StoreWord(a0, register_location(reg));\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::WriteCurrentPositionToRegister(int reg,\n                                                               int cp_offset) "}], [{"name": "RegExpMacroAssemblerARM64::Succeed", "content": "bool RegExpMacroAssemblerARM64::Succeed() {\n  __ B(&success_label_);\n  return global();\n}", "name_and_para": "bool RegExpMacroAssemblerARM64::Succeed() "}, {"name": "RegExpMacroAssemblerRISCV::Succeed", "content": "bool RegExpMacroAssemblerRISCV::Succeed() {\n  __ jmp(&success_label_);\n  return global();\n}", "name_and_para": "bool RegExpMacroAssemblerRISCV::Succeed() "}], [{"name": "RegExpMacroAssemblerARM64::SetRegister", "content": "void RegExpMacroAssemblerARM64::SetRegister(int register_index, int to) {\n  DCHECK(register_index >= num_saved_registers_);  // Reserved for positions!\n  Register set_to = wzr;\n  if (to != 0) {\n    set_to = w10;\n    __ Mov(set_to, to);\n  }\n  StoreRegister(register_index, set_to);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::SetRegister(int register_index, int to) "}, {"name": "RegExpMacroAssemblerRISCV::SetRegister", "content": "void RegExpMacroAssemblerRISCV::SetRegister(int register_index, int to) {\n  DCHECK(register_index >= num_saved_registers_);  // Reserved for positions!\n  __ li(a0, Operand(to));\n  __ StoreWord(a0, register_location(register_index));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::SetRegister(int register_index, int to) "}], [{"name": "RegExpMacroAssemblerARM64::SetCurrentPositionFromEnd", "content": "void RegExpMacroAssemblerARM64::SetCurrentPositionFromEnd(int by) {\n  Label after_position;\n  __ Cmp(current_input_offset(), -by * char_size());\n  __ B(ge, &after_position);\n  __ Mov(current_input_offset(), -by * char_size());\n  // On RegExp code entry (where this operation is used), the character before\n  // the current position is expected to be already loaded.\n  // We have advanced the position, so it's safe to read backwards.\n  LoadCurrentCharacterUnchecked(-1, 1);\n  __ Bind(&after_position);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::SetCurrentPositionFromEnd(int by) "}, {"name": "RegExpMacroAssemblerRISCV::SetCurrentPositionFromEnd", "content": "void RegExpMacroAssemblerRISCV::SetCurrentPositionFromEnd(int by) {\n  Label after_position;\n  __ BranchShort(&after_position, ge, current_input_offset(),\n                 Operand(-by * char_size()));\n  __ li(current_input_offset(), -by * char_size());\n  // On RegExp code entry (where this operation is used), the character before\n  // the current position is expected to be already loaded.\n  // We have advanced the position, so it's safe to read backwards.\n  LoadCurrentCharacterUnchecked(-1, 1);\n  __ bind(&after_position);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::SetCurrentPositionFromEnd(int by) "}], [{"name": "RegExpMacroAssemblerARM64::ReadStackPointerFromRegister", "content": "void RegExpMacroAssemblerARM64::ReadStackPointerFromRegister(int reg) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  Register read_from = GetRegister(reg, w10);\n  __ Mov(x11, ref);\n  __ Ldr(x11, MemOperand(x11));\n  __ Add(backtrack_stackpointer(), x11, Operand(read_from, SXTW));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::ReadStackPointerFromRegister(int reg) "}, {"name": "RegExpMacroAssemblerRISCV::ReadStackPointerFromRegister", "content": "void RegExpMacroAssemblerRISCV::ReadStackPointerFromRegister(int reg) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ li(a1, ref);\n  __ LoadWord(a1, MemOperand(a1));\n  __ Lw(backtrack_stackpointer(), register_location(reg));\n  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(), a1);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::ReadStackPointerFromRegister(int reg) "}], [{"name": "RegExpMacroAssemblerARM64::WriteStackPointerToRegister", "content": "void RegExpMacroAssemblerARM64::WriteStackPointerToRegister(int reg) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ Mov(x10, ref);\n  __ Ldr(x10, MemOperand(x10));\n  __ Sub(x10, backtrack_stackpointer(), x10);\n  if (v8_flags.debug_code) {\n    __ Cmp(x10, Operand(w10, SXTW));\n    // The stack offset needs to fit in a W register.\n    __ Check(eq, AbortReason::kOffsetOutOfRange);\n  }\n  StoreRegister(reg, w10);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::WriteStackPointerToRegister(int reg) "}, {"name": "RegExpMacroAssemblerRISCV::WriteStackPointerToRegister", "content": "void RegExpMacroAssemblerRISCV::WriteStackPointerToRegister(int reg) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ li(a0, ref);\n  __ LoadWord(a0, MemOperand(a0));\n  __ SubWord(a0, backtrack_stackpointer(), a0);\n  __ Sw(a0, register_location(reg));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::WriteStackPointerToRegister(int reg) "}], [{"name": "RegExpMacroAssemblerARM64::ReadCurrentPositionFromRegister", "content": "void RegExpMacroAssemblerARM64::ReadCurrentPositionFromRegister(int reg) {\n  RegisterState register_state = GetRegisterState(reg);\n  switch (register_state) {\n    case STACKED:\n      __ Ldr(current_input_offset(), register_location(reg));\n      break;\n    case CACHED_LSW:\n      __ Mov(current_input_offset(), GetCachedRegister(reg).W());\n      break;\n    case CACHED_MSW:\n      __ Lsr(current_input_offset().X(), GetCachedRegister(reg),\n             kWRegSizeInBits);\n      break;\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::ReadCurrentPositionFromRegister(int reg) "}, {"name": "RegExpMacroAssemblerRISCV::ReadCurrentPositionFromRegister", "content": "void RegExpMacroAssemblerRISCV::ReadCurrentPositionFromRegister(int reg) {\n  __ LoadWord(current_input_offset(), register_location(reg));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::ReadCurrentPositionFromRegister(int reg) "}], [{"name": "RegExpMacroAssemblerARM64::PushRegister", "content": "void RegExpMacroAssemblerARM64::PushRegister(int register_index,\n                                             StackCheckFlag check_stack_limit) {\n  Register to_push = GetRegister(register_index, w10);\n  Push(to_push);\n  if (check_stack_limit) CheckStackLimit();\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PushRegister(int register_index,\n                                             StackCheckFlag check_stack_limit) "}, {"name": "RegExpMacroAssemblerRISCV::PushRegister", "content": "void RegExpMacroAssemblerRISCV::PushRegister(int register_index,\n                                             StackCheckFlag check_stack_limit) {\n  __ LoadWord(a0, register_location(register_index));\n  Push(a0);\n  if (check_stack_limit) CheckStackLimit();\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PushRegister(int register_index,\n                                             StackCheckFlag check_stack_limit) "}], [{"name": "RegExpMacroAssemblerARM64::PushCurrentPosition", "content": "void RegExpMacroAssemblerARM64::PushCurrentPosition() {\n  Push(current_input_offset());\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PushCurrentPosition() "}, {"name": "RegExpMacroAssemblerRISCV::PushCurrentPosition", "content": "void RegExpMacroAssemblerRISCV::PushCurrentPosition() {\n  Push(current_input_offset());\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PushCurrentPosition() "}], [{"name": "RegExpMacroAssemblerARM64::PushBacktrack", "content": "void RegExpMacroAssemblerARM64::PushBacktrack(Label* label) {\n  if (label->is_bound()) {\n    int target = label->pos();\n    __ Mov(w10, target + InstructionStream::kHeaderSize - kHeapObjectTag);\n  } else {\n    __ Adr(x10, label, MacroAssembler::kAdrFar);\n    __ Sub(x10, x10, code_pointer());\n    if (v8_flags.debug_code) {\n      __ Cmp(x10, kWRegMask);\n      // The code offset has to fit in a W register.\n      __ Check(ls, AbortReason::kOffsetOutOfRange);\n    }\n  }\n  Push(w10);\n  CheckStackLimit();\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PushBacktrack(Label* label) "}, {"name": "RegExpMacroAssemblerRISCV::PushBacktrack", "content": "void RegExpMacroAssemblerRISCV::PushBacktrack(Label* label) {\n  if (label->is_bound()) {\n    int target = label->pos();\n    __ li(a0,\n          Operand(target + InstructionStream::kHeaderSize - kHeapObjectTag));\n  } else {\n    Assembler::BlockTrampolinePoolScope block_trampoline_pool(masm_.get());\n    Label after_constant;\n    __ BranchShort(&after_constant);\n    int offset = masm_->pc_offset();\n    int cp_offset = offset + InstructionStream::kHeaderSize - kHeapObjectTag;\n    __ emit(0);\n    masm_->label_at_put(label, offset);\n    __ bind(&after_constant);\n    if (is_int16(cp_offset)) {\n      __ Load32U(a0, MemOperand(code_pointer(), cp_offset));\n    } else {\n      __ AddWord(a0, code_pointer(), cp_offset);\n      __ Load32U(a0, MemOperand(a0, 0));\n    }\n  }\n  Push(a0);\n  CheckStackLimit();\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PushBacktrack(Label* label) "}], [{"name": "RegExpMacroAssemblerARM64::PopRegister", "content": "void RegExpMacroAssemblerARM64::PopRegister(int register_index) {\n  Pop(w10);\n  StoreRegister(register_index, w10);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PopRegister(int register_index) "}, {"name": "RegExpMacroAssemblerRISCV::PopRegister", "content": "void RegExpMacroAssemblerRISCV::PopRegister(int register_index) {\n  Pop(a0);\n  __ StoreWord(a0, register_location(register_index));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PopRegister(int register_index) "}], [{"name": "RegExpMacroAssemblerARM64::PopCurrentPosition", "content": "void RegExpMacroAssemblerARM64::PopCurrentPosition() {\n  Pop(current_input_offset());\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PopCurrentPosition() "}, {"name": "RegExpMacroAssemblerRISCV::PopCurrentPosition", "content": "void RegExpMacroAssemblerRISCV::PopCurrentPosition() {\n  Pop(current_input_offset());\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PopCurrentPosition() "}], [{"name": "RegExpMacroAssembler::IrregexpImplementation", "content": "RegExpMacroAssembler::IrregexpImplementation\n    RegExpMacroAssemblerARM64::Implementation() {\n  return kARM64Implementation;\n}", "name_and_para": "RegExpMacroAssembler::IrregexpImplementation\n    RegExpMacroAssemblerARM64::Implementation() "}, {"name": "RegExpMacroAssembler::IrregexpImplementation", "content": "RegExpMacroAssembler::IrregexpImplementation\nRegExpMacroAssemblerRISCV::Implementation() {\n  return kRISCVImplementation;\n}", "name_and_para": "RegExpMacroAssembler::IrregexpImplementation\nRegExpMacroAssemblerRISCV::Implementation() "}], [{"name": "RegExpMacroAssemblerARM64::IfRegisterEqPos", "content": "void RegExpMacroAssemblerARM64::IfRegisterEqPos(int reg, Label* if_eq) {\n  Register to_compare = GetRegister(reg, w10);\n  __ Cmp(to_compare, current_input_offset());\n  BranchOrBacktrack(eq, if_eq);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::IfRegisterEqPos(int reg, Label* if_eq) "}, {"name": "RegExpMacroAssemblerRISCV::IfRegisterEqPos", "content": "void RegExpMacroAssemblerRISCV::IfRegisterEqPos(int reg, Label* if_eq) {\n  __ LoadWord(a0, register_location(reg));\n  BranchOrBacktrack(if_eq, eq, a0, Operand(current_input_offset()));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::IfRegisterEqPos(int reg, Label* if_eq) "}], [{"name": "RegExpMacroAssemblerARM64::IfRegisterLT", "content": "void RegExpMacroAssemblerARM64::IfRegisterLT(int reg, int comparand,\n                                             Label* if_lt) {\n  Register to_compare = GetRegister(reg, w10);\n  CompareAndBranchOrBacktrack(to_compare, comparand, lt, if_lt);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::IfRegisterLT(int reg, int comparand,\n                                             Label* if_lt) "}, {"name": "RegExpMacroAssemblerRISCV::IfRegisterLT", "content": "void RegExpMacroAssemblerRISCV::IfRegisterLT(int reg, int comparand,\n                                             Label* if_lt) {\n  __ LoadWord(a0, register_location(reg));\n  BranchOrBacktrack(if_lt, lt, a0, Operand(comparand));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::IfRegisterLT(int reg, int comparand,\n                                             Label* if_lt) "}], [{"name": "RegExpMacroAssemblerARM64::IfRegisterGE", "content": "void RegExpMacroAssemblerARM64::IfRegisterGE(int reg, int comparand,\n                                             Label* if_ge) {\n  Register to_compare = GetRegister(reg, w10);\n  CompareAndBranchOrBacktrack(to_compare, comparand, ge, if_ge);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::IfRegisterGE(int reg, int comparand,\n                                             Label* if_ge) "}, {"name": "RegExpMacroAssemblerRISCV::IfRegisterGE", "content": "void RegExpMacroAssemblerRISCV::IfRegisterGE(int reg, int comparand,\n                                             Label* if_ge) {\n  __ LoadWord(a0, register_location(reg));\n  BranchOrBacktrack(if_ge, ge, a0, Operand(comparand));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::IfRegisterGE(int reg, int comparand,\n                                             Label* if_ge) "}], [{"name": "RegExpMacroAssemblerARM64::GoTo", "content": "void RegExpMacroAssemblerARM64::GoTo(Label* to) {\n  BranchOrBacktrack(al, to);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::GoTo(Label* to) "}, {"name": "RegExpMacroAssemblerRISCV::GoTo", "content": "void RegExpMacroAssemblerRISCV::GoTo(Label* to) {\n  if (to == nullptr) {\n    Backtrack();\n    return;\n  }\n  __ jmp(to);\n  return;\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::GoTo(Label* to) "}], [{"name": "RegExpMacroAssemblerARM64::GetCode", "content": "Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) {\n  Label return_w0;\n  // Finalize code - write the entry point code now we know how many\n  // registers we need.\n\n  // Entry code:\n  __ Bind(&entry_label_);\n\n  // Arguments on entry:\n  // x0:  String   input\n  // x1:  int      start_offset\n  // x2:  uint8_t*    input_start\n  // x3:  uint8_t*    input_end\n  // x4:  int*     output array\n  // x5:  int      output array size\n  // x6:  int      direct_call\n  // x7:  Isolate* isolate\n  //\n  // sp[0]:  secondary link/return address used by native call\n\n  // Tell the system that we have a stack frame.  Because the type is MANUAL, no\n  // code is generated.\n  FrameScope scope(masm_.get(), StackFrame::MANUAL);\n\n  // Stack frame setup.\n  // Push callee-saved registers.\n  const CPURegList registers_to_retain = kCalleeSaved;\n  DCHECK_EQ(registers_to_retain.Count(), kNumCalleeSavedRegisters);\n  __ PushCPURegList(registers_to_retain);\n  static_assert(kFrameTypeOffset == kFramePointerOffset - kSystemPointerSize);\n  __ EnterFrame(StackFrame::IRREGEXP);\n  // Only push the argument registers that we need.\n  static_assert(kIsolateOffset ==\n                kFrameTypeOffset - kPaddingAfterFrameType - kSystemPointerSize);\n  static_assert(kDirectCallOffset == kIsolateOffset - kSystemPointerSize);\n  static_assert(kNumOutputRegistersOffset ==\n                kDirectCallOffset - kSystemPointerSize);\n  static_assert(kInputStringOffset ==\n                kNumOutputRegistersOffset - kSystemPointerSize);\n  __ PushCPURegList(CPURegList{x0, x5, x6, x7});\n\n  // Initialize callee-saved registers.\n  __ Mov(start_offset(), w1);\n  __ Mov(input_start(), x2);\n  __ Mov(input_end(), x3);\n  __ Mov(output_array(), x4);\n\n  // Make sure the stack alignment will be respected.\n  const int alignment = masm_->ActivationFrameAlignment();\n  DCHECK_EQ(alignment % 16, 0);\n  const int align_mask = (alignment / kWRegSize) - 1;\n\n  // Make room for stack locals.\n  static constexpr int kWRegPerXReg = kXRegSize / kWRegSize;\n  DCHECK_EQ(kNumberOfStackLocals * kWRegPerXReg,\n            ((kNumberOfStackLocals * kWRegPerXReg) + align_mask) & ~align_mask);\n  __ Claim(kNumberOfStackLocals * kWRegPerXReg);\n\n  // Initialize backtrack stack pointer. It must not be clobbered from here on.\n  // Note the backtrack_stackpointer is callee-saved.\n  static_assert(backtrack_stackpointer() == x23);\n  LoadRegExpStackPointerFromMemory(backtrack_stackpointer());\n\n  // Store the regexp base pointer - we'll later restore it / write it to\n  // memory when returning from this irregexp code object.\n  PushRegExpBasePointer(backtrack_stackpointer(), x11);\n\n  // Set the number of registers we will need to allocate, that is:\n  //   - (num_registers_ - kNumCachedRegisters) (W registers)\n  const int num_stack_registers =\n      std::max(0, num_registers_ - kNumCachedRegisters);\n  const int num_wreg_to_allocate =\n      (num_stack_registers + align_mask) & ~align_mask;\n\n  {\n    // Check if we have space on the stack.\n    Label stack_limit_hit, stack_ok;\n\n    ExternalReference stack_limit =\n        ExternalReference::address_of_jslimit(isolate());\n    __ Mov(x10, stack_limit);\n    __ Ldr(x10, MemOperand(x10));\n    __ Subs(x10, sp, x10);\n    Operand extra_space_for_variables(num_wreg_to_allocate * kWRegSize);\n\n    // Handle it if the stack pointer is already below the stack limit.\n    __ B(ls, &stack_limit_hit);\n\n    // Check if there is room for the variable number of registers above\n    // the stack limit.\n    __ Cmp(x10, extra_space_for_variables);\n    __ B(hs, &stack_ok);\n\n    // Exit with OutOfMemory exception. There is not enough space on the stack\n    // for our working registers.\n    __ Mov(w0, EXCEPTION);\n    __ B(&return_w0);\n\n    __ Bind(&stack_limit_hit);\n    CallCheckStackGuardState(x10, extra_space_for_variables);\n    // If returned value is non-zero, we exit with the returned value as result.\n    __ Cbnz(w0, &return_w0);\n\n    __ Bind(&stack_ok);\n  }\n\n  // Allocate space on stack.\n  __ Claim(num_wreg_to_allocate, kWRegSize);\n\n  // Initialize success_counter and kBacktrackCountOffset with 0.\n  __ Str(wzr, MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n  __ Str(wzr, MemOperand(frame_pointer(), kBacktrackCountOffset));\n\n  // Find negative length (offset of start relative to end).\n  __ Sub(x10, input_start(), input_end());\n  if (v8_flags.debug_code) {\n    // Check that the size of the input string chars is in range.\n    __ Neg(x11, x10);\n    __ Cmp(x11, SeqTwoByteString::kMaxCharsSize);\n    __ Check(ls, AbortReason::kInputStringTooLong);\n  }\n  __ Mov(current_input_offset(), w10);\n\n  // The non-position value is used as a clearing value for the\n  // capture registers, it corresponds to the position of the first character\n  // minus one.\n  __ Sub(string_start_minus_one(), current_input_offset(), char_size());\n  __ Sub(string_start_minus_one(), string_start_minus_one(),\n         Operand(start_offset(), LSL, (mode_ == UC16) ? 1 : 0));\n  // We can store this value twice in an X register for initializing\n  // on-stack registers later.\n  __ Orr(twice_non_position_value(), string_start_minus_one().X(),\n         Operand(string_start_minus_one().X(), LSL, kWRegSizeInBits));\n\n  // Initialize code pointer register.\n  __ Mov(code_pointer(), Operand(masm_->CodeObject()));\n\n  Label load_char_start_regexp;\n  {\n    Label start_regexp;\n    // Load newline if index is at start, previous character otherwise.\n    __ Cbnz(start_offset(), &load_char_start_regexp);\n    __ Mov(current_character(), '\\n');\n    __ B(&start_regexp);\n\n    // Global regexp restarts matching here.\n    __ Bind(&load_char_start_regexp);\n    // Load previous char as initial value of current character register.\n    LoadCurrentCharacterUnchecked(-1, 1);\n    __ Bind(&start_regexp);\n  }\n\n  // Initialize on-stack registers.\n  if (num_saved_registers_ > 0) {\n    ClearRegisters(0, num_saved_registers_ - 1);\n  }\n\n  // Execute.\n  __ B(&start_label_);\n\n  if (backtrack_label_.is_linked()) {\n    __ Bind(&backtrack_label_);\n    Backtrack();\n  }\n\n  if (success_label_.is_linked()) {\n    Register first_capture_start = w15;\n\n    // Save captures when successful.\n    __ Bind(&success_label_);\n\n    if (num_saved_registers_ > 0) {\n      // V8 expects the output to be an int32_t array.\n      Register capture_start = w12;\n      Register capture_end = w13;\n      Register input_length = w14;\n\n      // Copy captures to output.\n\n      // Get string length.\n      __ Sub(x10, input_end(), input_start());\n      if (v8_flags.debug_code) {\n        // Check that the size of the input string chars is in range.\n        __ Cmp(x10, SeqTwoByteString::kMaxCharsSize);\n        __ Check(ls, AbortReason::kInputStringTooLong);\n      }\n      // input_start has a start_offset offset on entry. We need to include\n      // it when computing the length of the whole string.\n      if (mode_ == UC16) {\n        __ Add(input_length, start_offset(), Operand(w10, LSR, 1));\n      } else {\n        __ Add(input_length, start_offset(), w10);\n      }\n\n      // Copy the results to the output array from the cached registers first.\n      for (int i = 0;\n           (i < num_saved_registers_) && (i < kNumCachedRegisters);\n           i += 2) {\n        __ Mov(capture_start.X(), GetCachedRegister(i));\n        __ Lsr(capture_end.X(), capture_start.X(), kWRegSizeInBits);\n        if ((i == 0) && global_with_zero_length_check()) {\n          // Keep capture start for the zero-length check later.\n          // Note this only works when we have at least one cached register\n          // pair (otherwise we'd never reach this branch).\n          static_assert(kNumCachedRegisters > 0);\n          __ Mov(first_capture_start, capture_start);\n        }\n        // Offsets need to be relative to the start of the string.\n        if (mode_ == UC16) {\n          __ Add(capture_start, input_length, Operand(capture_start, ASR, 1));\n          __ Add(capture_end, input_length, Operand(capture_end, ASR, 1));\n        } else {\n          __ Add(capture_start, input_length, capture_start);\n          __ Add(capture_end, input_length, capture_end);\n        }\n        // The output pointer advances for a possible global match.\n        __ Stp(capture_start, capture_end,\n               MemOperand(output_array(), kSystemPointerSize, PostIndex));\n      }\n\n      // Only carry on if there are more than kNumCachedRegisters capture\n      // registers.\n      int num_registers_left_on_stack =\n          num_saved_registers_ - kNumCachedRegisters;\n      if (num_registers_left_on_stack > 0) {\n        Register base = x10;\n        // There are always an even number of capture registers. A couple of\n        // registers determine one match with two offsets.\n        DCHECK_EQ(0, num_registers_left_on_stack % 2);\n        __ Add(base, frame_pointer(), kFirstCaptureOnStackOffset);\n\n        // We can unroll the loop here, we should not unroll for less than 2\n        // registers.\n        static_assert(kNumRegistersToUnroll > 2);\n        if (num_registers_left_on_stack <= kNumRegistersToUnroll) {\n          for (int i = 0; i < num_registers_left_on_stack / 2; i++) {\n            __ Ldp(capture_end, capture_start,\n                   MemOperand(base, -kSystemPointerSize, PostIndex));\n            // Offsets need to be relative to the start of the string.\n            if (mode_ == UC16) {\n              __ Add(capture_start,\n                     input_length,\n                     Operand(capture_start, ASR, 1));\n              __ Add(capture_end, input_length, Operand(capture_end, ASR, 1));\n            } else {\n              __ Add(capture_start, input_length, capture_start);\n              __ Add(capture_end, input_length, capture_end);\n            }\n            // The output pointer advances for a possible global match.\n            __ Stp(capture_start, capture_end,\n                   MemOperand(output_array(), kSystemPointerSize, PostIndex));\n          }\n        } else {\n          Label loop;\n          __ Mov(x11, num_registers_left_on_stack);\n\n          __ Bind(&loop);\n          __ Ldp(capture_end, capture_start,\n                 MemOperand(base, -kSystemPointerSize, PostIndex));\n          if (mode_ == UC16) {\n            __ Add(capture_start, input_length, Operand(capture_start, ASR, 1));\n            __ Add(capture_end, input_length, Operand(capture_end, ASR, 1));\n          } else {\n            __ Add(capture_start, input_length, capture_start);\n            __ Add(capture_end, input_length, capture_end);\n          }\n          // The output pointer advances for a possible global match.\n          __ Stp(capture_start, capture_end,\n                 MemOperand(output_array(), kSystemPointerSize, PostIndex));\n          __ Sub(x11, x11, 2);\n          __ Cbnz(x11, &loop);\n        }\n      }\n    }\n\n    if (global()) {\n      Register success_counter = w0;\n      Register output_size = x10;\n      // Restart matching if the regular expression is flagged as global.\n\n      // Increment success counter.\n      __ Ldr(success_counter,\n             MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n      __ Add(success_counter, success_counter, 1);\n      __ Str(success_counter,\n             MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n\n      // Capture results have been stored, so the number of remaining global\n      // output registers is reduced by the number of stored captures.\n      __ Ldr(output_size,\n             MemOperand(frame_pointer(), kNumOutputRegistersOffset));\n      __ Sub(output_size, output_size, num_saved_registers_);\n      // Check whether we have enough room for another set of capture results.\n      __ Cmp(output_size, num_saved_registers_);\n      __ B(lt, &return_w0);\n\n      // The output pointer is already set to the next field in the output\n      // array.\n      // Update output size on the frame before we restart matching.\n      __ Str(output_size,\n             MemOperand(frame_pointer(), kNumOutputRegistersOffset));\n\n      // Restore the original regexp stack pointer value (effectively, pop the\n      // stored base pointer).\n      PopRegExpBasePointer(backtrack_stackpointer(), x11);\n\n      if (global_with_zero_length_check()) {\n        // Special case for zero-length matches.\n        __ Cmp(current_input_offset(), first_capture_start);\n        // Not a zero-length match, restart.\n        __ B(ne, &load_char_start_regexp);\n        // Offset from the end is zero if we already reached the end.\n        __ Cbz(current_input_offset(), &return_w0);\n        // Advance current position after a zero-length match.\n        Label advance;\n        __ bind(&advance);\n        __ Add(current_input_offset(), current_input_offset(),\n               Operand((mode_ == UC16) ? 2 : 1));\n        if (global_unicode()) CheckNotInSurrogatePair(0, &advance);\n      }\n\n      __ B(&load_char_start_regexp);\n    } else {\n      __ Mov(w0, SUCCESS);\n    }\n  }\n\n  if (exit_label_.is_linked()) {\n    // Exit and return w0.\n    __ Bind(&exit_label_);\n    if (global()) {\n      __ Ldr(w0, MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n    }\n  }\n\n  __ Bind(&return_w0);\n  // Restore the original regexp stack pointer value (effectively, pop the\n  // stored base pointer).\n  PopRegExpBasePointer(backtrack_stackpointer(), x11);\n\n  __ LeaveFrame(StackFrame::IRREGEXP);\n  __ PopCPURegList(registers_to_retain);\n  __ Ret();\n\n  Label exit_with_exception;\n  if (check_preempt_label_.is_linked()) {\n    __ Bind(&check_preempt_label_);\n\n    StoreRegExpStackPointerToMemory(backtrack_stackpointer(), x10);\n\n    SaveLinkRegister();\n    PushCachedRegisters();\n    CallCheckStackGuardState(x10);\n    // Returning from the regexp code restores the stack (sp <- fp)\n    // so we don't need to drop the link register from it before exiting.\n    __ Cbnz(w0, &return_w0);\n    // Reset the cached registers.\n    PopCachedRegisters();\n\n    LoadRegExpStackPointerFromMemory(backtrack_stackpointer());\n\n    RestoreLinkRegister();\n    __ Ret();\n  }\n\n  if (stack_overflow_label_.is_linked()) {\n    __ Bind(&stack_overflow_label_);\n\n    StoreRegExpStackPointerToMemory(backtrack_stackpointer(), x10);\n\n    SaveLinkRegister();\n    PushCachedRegisters();\n    // Call GrowStack(isolate).\n    static constexpr int kNumArguments = 1;\n    __ Mov(x0, ExternalReference::isolate_address(isolate()));\n    CallCFunctionFromIrregexpCode(ExternalReference::re_grow_stack(),\n                                  kNumArguments);\n    // If return nullptr, we have failed to grow the stack, and must exit with\n    // a stack-overflow exception.  Returning from the regexp code restores the\n    // stack (sp <- fp) so we don't need to drop the link register from it\n    // before exiting.\n    __ Cbz(w0, &exit_with_exception);\n    // Otherwise use return value as new stack pointer.\n    __ Mov(backtrack_stackpointer(), x0);\n    PopCachedRegisters();\n    RestoreLinkRegister();\n    __ Ret();\n  }\n\n  if (exit_with_exception.is_linked()) {\n    __ Bind(&exit_with_exception);\n    __ Mov(w0, EXCEPTION);\n    __ B(&return_w0);\n  }\n\n  if (fallback_label_.is_linked()) {\n    __ Bind(&fallback_label_);\n    __ Mov(w0, FALLBACK_TO_EXPERIMENTAL);\n    __ B(&return_w0);\n  }\n\n  CodeDesc code_desc;\n  masm_->GetCode(isolate(), &code_desc);\n  Handle<Code> code =\n      Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)\n          .set_self_reference(masm_->CodeObject())\n          .set_empty_source_position_table()\n          .Build();\n  PROFILE(masm_->isolate(),\n          RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));\n  return Handle<HeapObject>::cast(code);\n}", "name_and_para": "Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) "}, {"name": "RegExpMacroAssemblerRISCV::GetCode", "content": "Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {\n  Label return_a0;\n  if (masm_->has_exception()) {\n    // If the code gets corrupted due to long regular expressions and lack of\n    // space on trampolines, an internal exception flag is set. If this case\n    // is detected, we will jump into exit sequence right away.\n    __ bind_to(&entry_label_, internal_failure_label_.pos());\n  } else {\n    // Finalize code - write the entry point code now we know how many\n    // registers we need.\n\n    // Entry code:\n    __ bind(&entry_label_);\n\n    // Tell the system that we have a stack frame.  Because the type is MANUAL,\n    // no is generated.\n    FrameScope scope(masm_.get(), StackFrame::MANUAL);\n\n    // Actually emit code to start a new stack frame.\n    // Push arguments\n    // Save callee-save registers.\n    // Start new stack frame.\n    // Store link register in existing stack-cell.\n    // Order here should correspond to order of offset constants in header file.\n    // TODO(plind): we save fp..s11, but ONLY use s3 here - use the regs\n    // or dont save.\n    RegList registers_to_retain = {fp, s1, s2, s3, s4,  s5,\n                                   s6, s7, s8, s9, s10, s11};\n    DCHECK(registers_to_retain.Count() == kNumCalleeRegsToRetain);\n\n    // The remaining arguments are passed in registers, e.g.by calling the code\n    // entry as cast to a function with the signature:\n    //\n    // *int(*match)(String input_string,      // a0\n    //             int start_offset,          // a1\n    //             uint8_t* input_start,      // a2\n    //             uint8_t* input_end,        // a3\n    //             int* output,               // a4\n    //             int output_size,           // a5\n    //             int call_origin,           // a6\n    //             Isolate* isolate,          // a7\n    //             Address regexp);           // on the stack\n    RegList argument_registers = {a0, a1, a2, a3, a4, a5, a6, a7};\n\n    // According to MultiPush implementation, registers will be pushed in the\n    // order of ra, fp, then s8, ..., s1, and finally a7,...a0\n    __ MultiPush(RegList{ra} | registers_to_retain);\n\n    // Set frame pointer in space for it if this is not a direct call\n    // from generated code.\n    __ AddWord(frame_pointer(), sp, Operand(0));\n    static_assert(kFrameTypeOffset == -kSystemPointerSize);\n    __ li(kScratchReg, Operand(StackFrame::TypeToMarker(StackFrame::IRREGEXP)));\n    __ push(kScratchReg);\n    __ MultiPush(argument_registers);\n    static_assert(kSuccessfulCapturesOffset ==\n                  kInputStringOffset - kSystemPointerSize);\n    __ mv(a0, zero_reg);\n    __ push(a0);  // Make room for success counter and initialize it to 0.\n    static_assert(kStringStartMinusOneOffset ==\n                  kSuccessfulCapturesOffset - kSystemPointerSize);\n    __ push(a0);  // Make room for \"string start - 1\" constant.\n    static_assert(kBacktrackCountOffset ==\n                  kStringStartMinusOneOffset - kSystemPointerSize);\n    __ push(a0);  // The backtrack counter\n    static_assert(kRegExpStackBasePointerOffset ==\n                  kBacktrackCountOffset - kSystemPointerSize);\n    __ push(a0);  // The regexp stack base ptr.\n\n    // Initialize backtrack stack pointer. It must not be clobbered from here\n    // on. Note the backtrack_stackpointer is callee-saved.\n    static_assert(backtrack_stackpointer() == s8);\n    LoadRegExpStackPointerFromMemory(backtrack_stackpointer());\n    // Store the regexp base pointer - we'll later restore it / write it to\n    // memory when returning from this irregexp code object.\n    PushRegExpBasePointer(backtrack_stackpointer(), a1);\n\n    {\n      // Check if we have space on the stack for registers.\n      Label stack_limit_hit, stack_ok;\n\n      ExternalReference stack_limit =\n          ExternalReference::address_of_jslimit(masm_->isolate());\n      __ li(a0, Operand(stack_limit));\n      __ LoadWord(a0, MemOperand(a0));\n      __ SubWord(a0, sp, a0);\n      Operand extra_space_for_variables(num_registers_ * kSystemPointerSize);\n      // Handle it if the stack pointer is already below the stack limit.\n      __ Branch(&stack_limit_hit, le, a0, Operand(zero_reg));\n      // Check if there is room for the variable number of registers above\n      // the stack limit.\n      __ Branch(&stack_ok, uge, a0, extra_space_for_variables);\n      // Exit with OutOfMemory exception. There is not enough space on the stack\n      // for our working registers.\n      __ li(a0, Operand(EXCEPTION));\n      __ jmp(&return_a0);\n\n      __ bind(&stack_limit_hit);\n      CallCheckStackGuardState(a0, extra_space_for_variables);\n      // If returned value is non-zero, we exit with the returned value as\n      // result.\n      __ Branch(&return_a0, ne, a0, Operand(zero_reg));\n\n      __ bind(&stack_ok);\n    }\n    // Allocate space on stack for registers.\n    __ SubWord(sp, sp, Operand(num_registers_ * kSystemPointerSize));\n    // Load string end.\n    __ LoadWord(end_of_input_address(),\n                MemOperand(frame_pointer(), kInputEndOffset));\n    // Load input start.\n    __ LoadWord(a0, MemOperand(frame_pointer(), kInputStartOffset));\n    // Find negative length (offset of start relative to end).\n    __ SubWord(current_input_offset(), a0, end_of_input_address());\n    // Set a0 to address of char before start of the input string\n    // (effectively string position -1).\n    __ LoadWord(a1, MemOperand(frame_pointer(), kStartIndexOffset));\n    __ SubWord(a0, current_input_offset(), Operand(char_size()));\n    __ slli(t1, a1, (mode_ == UC16) ? 1 : 0);\n    __ SubWord(a0, a0, t1);\n    // Store this value in a local variable, for use when clearing\n    // position registers.\n    __ StoreWord(a0, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n\n    // Initialize code pointer register\n    __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);\n\n    Label load_char_start_regexp;\n    {\n      Label start_regexp;\n      // Load newline if index is at start, previous character otherwise.\n      __ Branch(&load_char_start_regexp, ne, a1, Operand(zero_reg));\n      __ li(current_character(), Operand('\\n'));\n      __ jmp(&start_regexp);\n\n      // Global regexp restarts matching here.\n      __ bind(&load_char_start_regexp);\n      // Load previous char as initial value of current character register.\n      LoadCurrentCharacterUnchecked(-1, 1);\n      __ bind(&start_regexp);\n    }\n\n    // Initialize on-stack registers.\n    if (num_saved_registers_ > 0) {  // Always is, if generated from a regexp.\n      // Fill saved registers with initial value = start offset - 1.\n      if (num_saved_registers_ > 8) {\n        // Address of register 0.\n        __ AddWord(a1, frame_pointer(), Operand(kRegisterZeroOffset));\n        __ li(a2, Operand(num_saved_registers_));\n        Label init_loop;\n        __ bind(&init_loop);\n        __ StoreWord(a0, MemOperand(a1));\n        __ AddWord(a1, a1, Operand(-kSystemPointerSize));\n        __ SubWord(a2, a2, Operand(1));\n        __ Branch(&init_loop, ne, a2, Operand(zero_reg));\n      } else {\n        for (int i = 0; i < num_saved_registers_; i++) {\n          __ StoreWord(a0, register_location(i));\n        }\n      }\n    }\n\n    __ jmp(&start_label_);\n\n    // Exit code:\n    if (success_label_.is_linked()) {\n      // Save captures when successful.\n      __ bind(&success_label_);\n      if (num_saved_registers_ > 0) {\n        // Copy captures to output.\n        __ LoadWord(a1, MemOperand(frame_pointer(), kInputStartOffset));\n        __ LoadWord(a0, MemOperand(frame_pointer(), kRegisterOutputOffset));\n        __ LoadWord(a2, MemOperand(frame_pointer(), kStartIndexOffset));\n        __ SubWord(a1, end_of_input_address(), a1);\n        // a1 is length of input in bytes.\n        if (mode_ == UC16) {\n          __ srli(a1, a1, 1);\n        }\n        // a1 is length of input in characters.\n        __ AddWord(a1, a1, Operand(a2));\n        // a1 is length of string in characters.\n\n        DCHECK_EQ(0, num_saved_registers_ % 2);\n        // Always an even number of capture registers. This allows us to\n        // unroll the loop once to add an operation between a load of a\n        // register and the following use of that register.\n        for (int i = 0; i < num_saved_registers_; i += 2) {\n          __ LoadWord(a2, register_location(i));\n          __ LoadWord(a3, register_location(i + 1));\n          if (i == 0 && global_with_zero_length_check()) {\n            // Keep capture start in a4 for the zero-length check later.\n            __ mv(s3, a2);\n          }\n          if (mode_ == UC16) {\n            __ srai(a2, a2, 1);\n            __ AddWord(a2, a2, a1);\n            __ srai(a3, a3, 1);\n            __ AddWord(a3, a3, a1);\n          } else {\n            __ AddWord(a2, a1, Operand(a2));\n            __ AddWord(a3, a1, Operand(a3));\n          }\n          // V8 expects the output to be an int32_t array.\n          __ Sw(a2, MemOperand(a0));\n          __ AddWord(a0, a0, kIntSize);\n          __ Sw(a3, MemOperand(a0));\n          __ AddWord(a0, a0, kIntSize);\n        }\n      }\n\n      if (global()) {\n        // Restart matching if the regular expression is flagged as global.\n        __ LoadWord(a0, MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n        __ LoadWord(a1, MemOperand(frame_pointer(), kNumOutputRegistersOffset));\n        __ LoadWord(a2, MemOperand(frame_pointer(), kRegisterOutputOffset));\n        // Increment success counter.\n        __ AddWord(a0, a0, 1);\n        __ StoreWord(a0,\n                     MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n        // Capture results have been stored, so the number of remaining global\n        // output registers is reduced by the number of stored captures.\n        __ SubWord(a1, a1, num_saved_registers_);\n        // Check whether we have enough room for another set of capture results.\n        __ Branch(&return_a0, lt, a1, Operand(num_saved_registers_));\n\n        __ StoreWord(a1,\n                     MemOperand(frame_pointer(), kNumOutputRegistersOffset));\n        // Advance the location for output.\n        __ AddWord(a2, a2, num_saved_registers_ * kIntSize);\n        __ StoreWord(a2, MemOperand(frame_pointer(), kRegisterOutputOffset));\n\n        // Restore the original regexp stack pointer value (effectively, pop the\n        // stored base pointer).\n        PopRegExpBasePointer(backtrack_stackpointer(), a2);\n\n        Label reload_string_start_minus_one;\n\n        if (global_with_zero_length_check()) {\n          // Special case for zero-length matches.\n          // s3: capture start index\n          // Not a zero-length match, restart.\n          __ Branch(&reload_string_start_minus_one, ne, current_input_offset(),\n                    Operand(s3));\n          // Offset from the end is zero if we already reached the end.\n          __ Branch(&exit_label_, eq, current_input_offset(),\n                    Operand(zero_reg));\n          // Advance current position after a zero-length match.\n          Label advance;\n          __ bind(&advance);\n          __ AddWord(current_input_offset(), current_input_offset(),\n                     Operand((mode_ == UC16) ? 2 : 1));\n          if (global_unicode()) CheckNotInSurrogatePair(0, &advance);\n        }\n\n        __ bind(&reload_string_start_minus_one);\n        // Prepare a0 to initialize registers with its value in the next run.\n        // Must be immediately before the jump to avoid clobbering.\n        __ LoadWord(a0,\n                    MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n\n        __ Branch(&load_char_start_regexp);\n      } else {\n        __ li(a0, Operand(SUCCESS));\n      }\n    }\n    // Exit and return a0.\n    __ bind(&exit_label_);\n    if (global()) {\n      __ LoadWord(a0, MemOperand(frame_pointer(), kSuccessfulCapturesOffset));\n    }\n\n    __ bind(&return_a0);\n    // Restore the original regexp stack pointer value (effectively, pop the\n    // stored base pointer).\n    PopRegExpBasePointer(backtrack_stackpointer(), a1);\n    // Skip sp past regexp registers and local variables..\n    __ mv(sp, frame_pointer());\n\n    // Restore registers fp..s11 and return (restoring ra to pc).\n    __ MultiPop(registers_to_retain | ra);\n\n    __ Ret();\n\n    // Backtrack code (branch target for conditional backtracks).\n    if (backtrack_label_.is_linked()) {\n      __ bind(&backtrack_label_);\n      Backtrack();\n    }\n\n    Label exit_with_exception;\n\n    // Preempt-code.\n    if (check_preempt_label_.is_linked()) {\n      SafeCallTarget(&check_preempt_label_);\n      StoreRegExpStackPointerToMemory(backtrack_stackpointer(), a1);\n      // Put regexp engine registers on stack.\n      CallCheckStackGuardState(a0);\n      // If returning non-zero, we should end execution with the given\n      // result as return value.\n      __ Branch(&return_a0, ne, a0, Operand(zero_reg));\n      LoadRegExpStackPointerFromMemory(backtrack_stackpointer());\n      // String might have moved: Reload end of string from frame.\n      __ LoadWord(end_of_input_address(),\n                  MemOperand(frame_pointer(), kInputEndOffset));\n      SafeReturn();\n    }\n\n    // Backtrack stack overflow code.\n    if (stack_overflow_label_.is_linked()) {\n      SafeCallTarget(&stack_overflow_label_);\n      // Call GrowStack(isolate).\n      StoreRegExpStackPointerToMemory(backtrack_stackpointer(), a1);\n\n      static constexpr int kNumArguments = 1;\n      __ PrepareCallCFunction(kNumArguments, 0, a0);\n      __ li(a0, ExternalReference::isolate_address(isolate()));\n      ExternalReference grow_stack = ExternalReference::re_grow_stack();\n      CallCFunctionFromIrregexpCode(grow_stack, kNumArguments);\n      // If nullptr is returned, we have failed to grow the stack, and must exit\n      // with a stack-overflow exception.\n      __ BranchShort(&exit_with_exception, eq, a0, Operand(zero_reg));\n      // Otherwise use return value as new stack pointer.\n      __ mv(backtrack_stackpointer(), a0);\n      // Restore saved registers and continue.\n      SafeReturn();\n    }\n\n    if (exit_with_exception.is_linked()) {\n      // If any of the code above needed to exit with an exception.\n      __ bind(&exit_with_exception);\n      // Exit with Result EXCEPTION(-1) to signal thrown exception.\n      __ li(a0, Operand(EXCEPTION));\n      __ jmp(&return_a0);\n    }\n\n    if (fallback_label_.is_linked()) {\n      __ bind(&fallback_label_);\n      __ li(a0, Operand(FALLBACK_TO_EXPERIMENTAL));\n      __ jmp(&return_a0);\n    }\n  }\n\n  CodeDesc code_desc;\n  masm_->GetCode(isolate(), &code_desc);\n  Handle<Code> code =\n      Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)\n          .set_self_reference(masm_->CodeObject())\n          .set_empty_source_position_table()\n          .Build();\n  LOG(masm_->isolate(),\n      RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));\n  return Handle<HeapObject>::cast(code);\n}", "name_and_para": "Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) "}], [{"name": "RegExpMacroAssemblerARM64::PopRegExpBasePointer", "content": "void RegExpMacroAssemblerARM64::PopRegExpBasePointer(Register stack_pointer_out,\n                                                     Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ Ldr(stack_pointer_out,\n         MemOperand(frame_pointer(), kRegExpStackBasePointerOffset));\n  __ Mov(scratch, ref);\n  __ Ldr(scratch, MemOperand(scratch));\n  __ Add(stack_pointer_out, stack_pointer_out, scratch);\n  StoreRegExpStackPointerToMemory(stack_pointer_out, scratch);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PopRegExpBasePointer(Register stack_pointer_out,\n                                                     Register scratch) "}, {"name": "RegExpMacroAssemblerRISCV::PopRegExpBasePointer", "content": "void RegExpMacroAssemblerRISCV::PopRegExpBasePointer(Register stack_pointer_out,\n                                                     Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ LoadWord(stack_pointer_out,\n              MemOperand(frame_pointer(), kRegExpStackBasePointerOffset));\n  __ li(scratch, Operand(ref));\n  __ LoadWord(scratch, MemOperand(scratch));\n  __ AddWord(stack_pointer_out, stack_pointer_out, scratch);\n  StoreRegExpStackPointerToMemory(stack_pointer_out, scratch);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PopRegExpBasePointer(Register stack_pointer_out,\n                                                     Register scratch) "}], [{"name": "RegExpMacroAssemblerARM64::PushRegExpBasePointer", "content": "void RegExpMacroAssemblerARM64::PushRegExpBasePointer(Register stack_pointer,\n                                                      Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ Mov(scratch, ref);\n  __ Ldr(scratch, MemOperand(scratch));\n  __ Sub(scratch, stack_pointer, scratch);\n  __ Str(scratch, MemOperand(frame_pointer(), kRegExpStackBasePointerOffset));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::PushRegExpBasePointer(Register stack_pointer,\n                                                      Register scratch) "}, {"name": "RegExpMacroAssemblerRISCV::PushRegExpBasePointer", "content": "void RegExpMacroAssemblerRISCV::PushRegExpBasePointer(Register stack_pointer,\n                                                      Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_memory_top_address(isolate());\n  __ li(scratch, Operand(ref));\n  __ LoadWord(scratch, MemOperand(scratch));\n  __ SubWord(scratch, stack_pointer, scratch);\n  __ StoreWord(scratch,\n               MemOperand(frame_pointer(), kRegExpStackBasePointerOffset));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::PushRegExpBasePointer(Register stack_pointer,\n                                                      Register scratch) "}], [{"name": "RegExpMacroAssemblerARM64::StoreRegExpStackPointerToMemory", "content": "void RegExpMacroAssemblerARM64::StoreRegExpStackPointerToMemory(\n    Register src, Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_stack_pointer(isolate());\n  __ Mov(scratch, ref);\n  __ Str(src, MemOperand(scratch));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::StoreRegExpStackPointerToMemory(\n    Register src, Register scratch) "}, {"name": "RegExpMacroAssemblerRISCV::StoreRegExpStackPointerToMemory", "content": "void RegExpMacroAssemblerRISCV::StoreRegExpStackPointerToMemory(\n    Register src, Register scratch) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_stack_pointer(isolate());\n  __ li(scratch, Operand(ref));\n  __ StoreWord(src, MemOperand(scratch));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::StoreRegExpStackPointerToMemory(\n    Register src, Register scratch) "}], [{"name": "RegExpMacroAssemblerARM64::LoadRegExpStackPointerFromMemory", "content": "void RegExpMacroAssemblerARM64::LoadRegExpStackPointerFromMemory(Register dst) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_stack_pointer(isolate());\n  __ Mov(dst, ref);\n  __ Ldr(dst, MemOperand(dst));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::LoadRegExpStackPointerFromMemory(Register dst) "}, {"name": "RegExpMacroAssemblerRISCV::LoadRegExpStackPointerFromMemory", "content": "void RegExpMacroAssemblerRISCV::LoadRegExpStackPointerFromMemory(Register dst) {\n  ExternalReference ref =\n      ExternalReference::address_of_regexp_stack_stack_pointer(isolate());\n  __ li(dst, Operand(ref));\n  __ LoadWord(dst, MemOperand(dst));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::LoadRegExpStackPointerFromMemory(Register dst) "}], [{"name": "RegExpMacroAssemblerARM64::Fail", "content": "void RegExpMacroAssemblerARM64::Fail() {\n  __ Mov(w0, FAILURE);\n  __ B(&exit_label_);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::Fail() "}, {"name": "RegExpMacroAssemblerRISCV::Fail", "content": "void RegExpMacroAssemblerRISCV::Fail() {\n  __ li(a0, Operand(FAILURE));\n  __ jmp(&exit_label_);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::Fail() "}], [{"name": "RegExpMacroAssemblerARM64::CheckSpecialClassRanges", "content": "bool RegExpMacroAssemblerARM64::CheckSpecialClassRanges(\n    StandardCharacterSet type, Label* on_no_match) {\n  // Range checks (c in min..max) are generally implemented by an unsigned\n  // (c - min) <= (max - min) check\n  // TODO(jgruber): No custom implementation (yet): s(UC16), S(UC16).\n  switch (type) {\n    case StandardCharacterSet::kWhitespace:\n      // Match space-characters.\n      if (mode_ == LATIN1) {\n        // One byte space characters are '\\t'..'\\r', ' ' and \\u00a0.\n        Label success;\n        // Check for ' ' or 0x00A0.\n        __ Cmp(current_character(), ' ');\n        __ Ccmp(current_character(), 0x00A0, ZFlag, ne);\n        __ B(eq, &success);\n        // Check range 0x09..0x0D.\n        __ Sub(w10, current_character(), '\\t');\n        CompareAndBranchOrBacktrack(w10, '\\r' - '\\t', hi, on_no_match);\n        __ Bind(&success);\n        return true;\n      }\n      return false;\n    case StandardCharacterSet::kNotWhitespace:\n      // The emitted code for generic character classes is good enough.\n      return false;\n    case StandardCharacterSet::kDigit:\n      // Match ASCII digits ('0'..'9').\n      __ Sub(w10, current_character(), '0');\n      CompareAndBranchOrBacktrack(w10, '9' - '0', hi, on_no_match);\n      return true;\n    case StandardCharacterSet::kNotDigit:\n      // Match ASCII non-digits.\n      __ Sub(w10, current_character(), '0');\n      CompareAndBranchOrBacktrack(w10, '9' - '0', ls, on_no_match);\n      return true;\n    case StandardCharacterSet::kNotLineTerminator: {\n      // Match non-newlines (not 0x0A('\\n'), 0x0D('\\r'), 0x2028 and 0x2029)\n      // Here we emit the conditional branch only once at the end to make branch\n      // prediction more efficient, even though we could branch out of here\n      // as soon as a character matches.\n      __ Cmp(current_character(), 0x0A);\n      __ Ccmp(current_character(), 0x0D, ZFlag, ne);\n      if (mode_ == UC16) {\n        __ Sub(w10, current_character(), 0x2028);\n        // If the Z flag was set we clear the flags to force a branch.\n        __ Ccmp(w10, 0x2029 - 0x2028, NoFlag, ne);\n        // ls -> !((C==1) && (Z==0))\n        BranchOrBacktrack(ls, on_no_match);\n      } else {\n        BranchOrBacktrack(eq, on_no_match);\n      }\n      return true;\n    }\n    case StandardCharacterSet::kLineTerminator: {\n      // Match newlines (0x0A('\\n'), 0x0D('\\r'), 0x2028 and 0x2029)\n      // We have to check all 4 newline characters before emitting\n      // the conditional branch.\n      __ Cmp(current_character(), 0x0A);\n      __ Ccmp(current_character(), 0x0D, ZFlag, ne);\n      if (mode_ == UC16) {\n        __ Sub(w10, current_character(), 0x2028);\n        // If the Z flag was set we clear the flags to force a fall-through.\n        __ Ccmp(w10, 0x2029 - 0x2028, NoFlag, ne);\n        // hi -> (C==1) && (Z==0)\n        BranchOrBacktrack(hi, on_no_match);\n      } else {\n        BranchOrBacktrack(ne, on_no_match);\n      }\n      return true;\n    }\n    case StandardCharacterSet::kWord: {\n      if (mode_ != LATIN1) {\n        // Table is 256 entries, so all Latin1 characters can be tested.\n        CompareAndBranchOrBacktrack(current_character(), 'z', hi, on_no_match);\n      }\n      ExternalReference map = ExternalReference::re_word_character_map();\n      __ Mov(x10, map);\n      __ Ldrb(w10, MemOperand(x10, current_character(), UXTW));\n      CompareAndBranchOrBacktrack(w10, 0, eq, on_no_match);\n      return true;\n    }\n    case StandardCharacterSet::kNotWord: {\n      Label done;\n      if (mode_ != LATIN1) {\n        // Table is 256 entries, so all Latin1 characters can be tested.\n        __ Cmp(current_character(), 'z');\n        __ B(hi, &done);\n      }\n      ExternalReference map = ExternalReference::re_word_character_map();\n      __ Mov(x10, map);\n      __ Ldrb(w10, MemOperand(x10, current_character(), UXTW));\n      CompareAndBranchOrBacktrack(w10, 0, ne, on_no_match);\n      __ Bind(&done);\n      return true;\n    }\n    case StandardCharacterSet::kEverything:\n      // Match any character.\n      return true;\n  }\n}", "name_and_para": "bool RegExpMacroAssemblerARM64::CheckSpecialClassRanges(\n    StandardCharacterSet type, Label* on_no_match) "}, {"name": "RegExpMacroAssemblerRISCV::CheckSpecialClassRanges", "content": "bool RegExpMacroAssemblerRISCV::CheckSpecialClassRanges(\n    StandardCharacterSet type, Label* on_no_match) {\n  // Range checks (c in min..max) are generally implemented by an unsigned\n  // (c - min) <= (max - min) check.\n  switch (type) {\n    case StandardCharacterSet::kWhitespace:\n      // Match space-characters.\n      if (mode_ == LATIN1) {\n        // One byte space characters are '\\t'..'\\r', ' ' and \\u00a0.\n        Label success;\n        __ BranchShort(&success, eq, current_character(), Operand(' '));\n        // Check range 0x09..0x0D.\n        __ SubWord(a0, current_character(), Operand('\\t'));\n        __ BranchShort(&success, Uless_equal, a0, Operand('\\r' - '\\t'));\n        // \\u00a0 (NBSP).\n        BranchOrBacktrack(on_no_match, ne, a0, Operand(0x00A0 - '\\t'));\n        __ bind(&success);\n        return true;\n      }\n      return false;\n    case StandardCharacterSet::kNotWhitespace:\n      // The emitted code for generic character classes is good enough.\n      return false;\n    case StandardCharacterSet::kDigit:\n      // Match Latin1 digits ('0'..'9').\n      __ SubWord(a0, current_character(), Operand('0'));\n      BranchOrBacktrack(on_no_match, Ugreater, a0, Operand('9' - '0'));\n      return true;\n    case StandardCharacterSet::kNotDigit:\n      // Match non Latin1-digits.\n      __ SubWord(a0, current_character(), Operand('0'));\n      BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand('9' - '0'));\n      return true;\n    case StandardCharacterSet::kNotLineTerminator: {\n      // Match non-newlines (not 0x0A('\\n'), 0x0D('\\r'), 0x2028 and 0x2029).\n      __ Xor(a0, current_character(), Operand(0x01));\n      // See if current character is '\\n'^1 or '\\r'^1, i.e., 0x0B or 0x0C.\n      __ SubWord(a0, a0, Operand(0x0B));\n      BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand(0x0C - 0x0B));\n      if (mode_ == UC16) {\n        // Compare original value to 0x2028 and 0x2029, using the already\n        // computed (current_char ^ 0x01 - 0x0B). I.e., check for\n        // 0x201D (0x2028 - 0x0B) or 0x201E.\n        __ SubWord(a0, a0, Operand(0x2028 - 0x0B));\n        BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand(1));\n      }\n      return true;\n    }\n    case StandardCharacterSet::kLineTerminator: {\n      // Match newlines (0x0A('\\n'), 0x0D('\\r'), 0x2028 and 0x2029).\n      __ Xor(a0, current_character(), Operand(0x01));\n      // See if current character is '\\n'^1 or '\\r'^1, i.e., 0x0B or 0x0C.\n      __ SubWord(a0, a0, Operand(0x0B));\n      if (mode_ == LATIN1) {\n        BranchOrBacktrack(on_no_match, Ugreater, a0, Operand(0x0C - 0x0B));\n      } else {\n        Label done;\n        BranchOrBacktrack(&done, Uless_equal, a0, Operand(0x0C - 0x0B));\n        // Compare original value to 0x2028 and 0x2029, using the already\n        // computed (current_char ^ 0x01 - 0x0B). I.e., check for\n        // 0x201D (0x2028 - 0x0B) or 0x201E.\n        __ SubWord(a0, a0, Operand(0x2028 - 0x0B));\n        BranchOrBacktrack(on_no_match, Ugreater, a0, Operand(1));\n        __ bind(&done);\n      }\n      return true;\n    }\n    case StandardCharacterSet::kWord: {\n      if (mode_ != LATIN1) {\n        // Table is 256 entries, so all Latin1 characters can be tested.\n        BranchOrBacktrack(on_no_match, Ugreater, current_character(),\n                          Operand('z'));\n      }\n      ExternalReference map = ExternalReference::re_word_character_map();\n      __ li(a0, Operand(map));\n      __ AddWord(a0, a0, current_character());\n      __ Lbu(a0, MemOperand(a0, 0));\n      BranchOrBacktrack(on_no_match, eq, a0, Operand(zero_reg));\n      return true;\n    }\n    case StandardCharacterSet::kNotWord: {\n      Label done;\n      if (mode_ != LATIN1) {\n        // Table is 256 entries, so all Latin1 characters can be tested.\n        __ BranchShort(&done, Ugreater, current_character(), Operand('z'));\n      }\n      ExternalReference map = ExternalReference::re_word_character_map();\n      __ li(a0, Operand(map));\n      __ AddWord(a0, a0, current_character());\n      __ Lbu(a0, MemOperand(a0, 0));\n      BranchOrBacktrack(on_no_match, ne, a0, Operand(zero_reg));\n      if (mode_ != LATIN1) {\n        __ bind(&done);\n      }\n      return true;\n    }\n    case StandardCharacterSet::kEverything:\n      // Match any character.\n      return true;\n    // No custom implementation (yet): s(UC16), S(UC16).\n    default:\n      return false;\n  }\n}", "name_and_para": "bool RegExpMacroAssemblerRISCV::CheckSpecialClassRanges(\n    StandardCharacterSet type, Label* on_no_match) "}], [{"name": "RegExpMacroAssemblerARM64::CheckBitInTable", "content": "void RegExpMacroAssemblerARM64::CheckBitInTable(\n    Handle<ByteArray> table,\n    Label* on_bit_set) {\n  __ Mov(x11, Operand(table));\n  if ((mode_ != LATIN1) || (kTableMask != String::kMaxOneByteCharCode)) {\n    __ And(w10, current_character(), kTableMask);\n    __ Add(w10, w10, ByteArray::kHeaderSize - kHeapObjectTag);\n  } else {\n    __ Add(w10, current_character(), ByteArray::kHeaderSize - kHeapObjectTag);\n  }\n  __ Ldrb(w11, MemOperand(x11, w10, UXTW));\n  CompareAndBranchOrBacktrack(w11, 0, ne, on_bit_set);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckBitInTable(\n    Handle<ByteArray> table,\n    Label* on_bit_set) "}, {"name": "RegExpMacroAssemblerRISCV::CheckBitInTable", "content": "void RegExpMacroAssemblerRISCV::CheckBitInTable(Handle<ByteArray> table,\n                                                Label* on_bit_set) {\n  __ li(a0, Operand(table));\n  if (mode_ != LATIN1 || kTableMask != String::kMaxOneByteCharCode) {\n    __ And(a1, current_character(), Operand(kTableSize - 1));\n    __ AddWord(a0, a0, a1);\n  } else {\n    __ AddWord(a0, a0, current_character());\n  }\n\n  __ Lbu(a0, FieldMemOperand(a0, ByteArray::kHeaderSize));\n  BranchOrBacktrack(on_bit_set, ne, a0, Operand(zero_reg));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckBitInTable(Handle<ByteArray> table,\n                                                Label* on_bit_set) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterNotInRangeArray", "content": "bool RegExpMacroAssemblerARM64::CheckCharacterNotInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_not_in_range) {\n  // Note: due to the arm64 oddity of x0 being a 'cached register',\n  // pushing/popping registers must happen outside of CallIsCharacterInRange\n  // s.t. we can compare the return value to 0 before popping x0.\n  PushCachedRegisters();\n  CallIsCharacterInRangeArray(ranges);\n  __ Cmp(x0, 0);\n  PopCachedRegisters();\n  BranchOrBacktrack(eq, on_not_in_range);\n  return true;\n}", "name_and_para": "bool RegExpMacroAssemblerARM64::CheckCharacterNotInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_not_in_range) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterNotInRangeArray", "content": "bool RegExpMacroAssemblerRISCV::CheckCharacterNotInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_not_in_range) {\n  CallIsCharacterInRangeArray(ranges);\n  BranchOrBacktrack(on_not_in_range, eq, a0, Operand(zero_reg));\n  return true;\n}", "name_and_para": "bool RegExpMacroAssemblerRISCV::CheckCharacterNotInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_not_in_range) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterInRangeArray", "content": "bool RegExpMacroAssemblerARM64::CheckCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_in_range) {\n  // Note: due to the arm64 oddity of x0 being a 'cached register',\n  // pushing/popping registers must happen outside of CallIsCharacterInRange\n  // s.t. we can compare the return value to 0 before popping x0.\n  PushCachedRegisters();\n  CallIsCharacterInRangeArray(ranges);\n  __ Cmp(x0, 0);\n  PopCachedRegisters();\n  BranchOrBacktrack(ne, on_in_range);\n  return true;\n}", "name_and_para": "bool RegExpMacroAssemblerARM64::CheckCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_in_range) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterInRangeArray", "content": "bool RegExpMacroAssemblerRISCV::CheckCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_in_range) {\n  CallIsCharacterInRangeArray(ranges);\n  BranchOrBacktrack(on_in_range, ne, a0, Operand(zero_reg));\n  return true;\n}", "name_and_para": "bool RegExpMacroAssemblerRISCV::CheckCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges, Label* on_in_range) "}], [{"name": "RegExpMacroAssemblerARM64::CallIsCharacterInRangeArray", "content": "void RegExpMacroAssemblerARM64::CallIsCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges) {\n  static const int kNumArguments = 3;\n  __ Mov(w0, current_character());\n  __ Mov(x1, GetOrAddRangeArray(ranges));\n  __ Mov(x2, ExternalReference::isolate_address(isolate()));\n\n  {\n    // We have a frame (set up in GetCode), but the assembler doesn't know.\n    FrameScope scope(masm_.get(), StackFrame::MANUAL);\n    CallCFunctionFromIrregexpCode(\n        ExternalReference::re_is_character_in_range_array(), kNumArguments);\n  }\n\n  __ Mov(code_pointer(), Operand(masm_->CodeObject()));\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CallIsCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges) "}, {"name": "RegExpMacroAssemblerRISCV::CallIsCharacterInRangeArray", "content": "void RegExpMacroAssemblerRISCV::CallIsCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges) {\n  static const int kNumArguments = 3;\n  __ PrepareCallCFunction(kNumArguments, a0);\n\n  __ mv(a0, current_character());\n  __ li(a1, Operand(GetOrAddRangeArray(ranges)));\n  __ li(a2, Operand(ExternalReference::isolate_address(isolate())));\n\n  {\n    // We have a frame (set up in GetCode), but the assembler doesn't know.\n    FrameScope scope(masm_.get(), StackFrame::MANUAL);\n    CallCFunctionFromIrregexpCode(\n        ExternalReference::re_is_character_in_range_array(), kNumArguments);\n  }\n  __ li(code_pointer(), Operand(masm_->CodeObject()));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CallIsCharacterInRangeArray(\n    const ZoneList<CharacterRange>* ranges) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterNotInRange", "content": "void RegExpMacroAssemblerARM64::CheckCharacterNotInRange(\n    base::uc16 from, base::uc16 to, Label* on_not_in_range) {\n  __ Sub(w10, current_character(), from);\n  // Unsigned higher condition.\n  CompareAndBranchOrBacktrack(w10, to - from, hi, on_not_in_range);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacterNotInRange(\n    base::uc16 from, base::uc16 to, Label* on_not_in_range) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterNotInRange", "content": "void RegExpMacroAssemblerRISCV::CheckCharacterNotInRange(\n    base::uc16 from, base::uc16 to, Label* on_not_in_range) {\n  __ SubWord(a0, current_character(), Operand(from));\n  // Unsigned higher condition.\n  BranchOrBacktrack(on_not_in_range, Ugreater, a0, Operand(to - from));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacterNotInRange(\n    base::uc16 from, base::uc16 to, Label* on_not_in_range) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterInRange", "content": "void RegExpMacroAssemblerARM64::CheckCharacterInRange(base::uc16 from,\n                                                      base::uc16 to,\n                                                      Label* on_in_range) {\n  __ Sub(w10, current_character(), from);\n  // Unsigned lower-or-same condition.\n  CompareAndBranchOrBacktrack(w10, to - from, ls, on_in_range);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacterInRange(base::uc16 from,\n                                                      base::uc16 to,\n                                                      Label* on_in_range) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterInRange", "content": "void RegExpMacroAssemblerRISCV::CheckCharacterInRange(base::uc16 from,\n                                                      base::uc16 to,\n                                                      Label* on_in_range) {\n  __ SubWord(a0, current_character(), Operand(from));\n  // Unsigned lower-or-same condition.\n  BranchOrBacktrack(on_in_range, Uless_equal, a0, Operand(to - from));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacterInRange(base::uc16 from,\n                                                      base::uc16 to,\n                                                      Label* on_in_range) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotCharacterAfterMinusAnd", "content": "void RegExpMacroAssemblerARM64::CheckNotCharacterAfterMinusAnd(\n    base::uc16 c, base::uc16 minus, base::uc16 mask, Label* on_not_equal) {\n  DCHECK_GT(String::kMaxUtf16CodeUnit, minus);\n  __ Sub(w10, current_character(), minus);\n  __ And(w10, w10, mask);\n  CompareAndBranchOrBacktrack(w10, c, ne, on_not_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotCharacterAfterMinusAnd(\n    base::uc16 c, base::uc16 minus, base::uc16 mask, Label* on_not_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotCharacterAfterMinusAnd", "content": "void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterMinusAnd(\n    base::uc16 c, base::uc16 minus, base::uc16 mask, Label* on_not_equal) {\n  DCHECK_GT(String::kMaxUtf16CodeUnit, minus);\n  __ SubWord(a0, current_character(), Operand(minus));\n  __ And(a0, a0, Operand(mask));\n  BranchOrBacktrack(on_not_equal, ne, a0, Operand(c));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterMinusAnd(\n    base::uc16 c, base::uc16 minus, base::uc16 mask, Label* on_not_equal) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotCharacterAfterAnd", "content": "void RegExpMacroAssemblerARM64::CheckNotCharacterAfterAnd(unsigned c,\n                                                          unsigned mask,\n                                                          Label* on_not_equal) {\n  __ And(w10, current_character(), mask);\n  CompareAndBranchOrBacktrack(w10, c, ne, on_not_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotCharacterAfterAnd(unsigned c,\n                                                          unsigned mask,\n                                                          Label* on_not_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotCharacterAfterAnd", "content": "void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterAnd(uint32_t c,\n                                                          uint32_t mask,\n                                                          Label* on_not_equal) {\n  __ And(a0, current_character(), Operand(mask));\n  Operand rhs = (c == 0) ? Operand(zero_reg) : Operand(c);\n  BranchOrBacktrack(on_not_equal, ne, a0, rhs);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterAnd(uint32_t c,\n                                                          uint32_t mask,\n                                                          Label* on_not_equal) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterAfterAnd", "content": "void RegExpMacroAssemblerARM64::CheckCharacterAfterAnd(uint32_t c,\n                                                       uint32_t mask,\n                                                       Label* on_equal) {\n  __ And(w10, current_character(), mask);\n  CompareAndBranchOrBacktrack(w10, c, eq, on_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacterAfterAnd(uint32_t c,\n                                                       uint32_t mask,\n                                                       Label* on_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterAfterAnd", "content": "void RegExpMacroAssemblerRISCV::CheckCharacterAfterAnd(uint32_t c,\n                                                       uint32_t mask,\n                                                       Label* on_equal) {\n  __ And(a0, current_character(), Operand(mask));\n  Operand rhs = (c == 0) ? Operand(zero_reg) : Operand(c);\n  BranchOrBacktrack(on_equal, eq, a0, rhs);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacterAfterAnd(uint32_t c,\n                                                       uint32_t mask,\n                                                       Label* on_equal) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotCharacter", "content": "void RegExpMacroAssemblerARM64::CheckNotCharacter(unsigned c,\n                                                  Label* on_not_equal) {\n  CompareAndBranchOrBacktrack(current_character(), c, ne, on_not_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotCharacter(unsigned c,\n                                                  Label* on_not_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotCharacter", "content": "void RegExpMacroAssemblerRISCV::CheckNotCharacter(uint32_t c,\n                                                  Label* on_not_equal) {\n  BranchOrBacktrack(on_not_equal, ne, current_character(), Operand(c));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotCharacter(uint32_t c,\n                                                  Label* on_not_equal) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotBackReference", "content": "void RegExpMacroAssemblerARM64::CheckNotBackReference(int start_reg,\n                                                      bool read_backward,\n                                                      Label* on_no_match) {\n  Label fallthrough;\n\n  Register capture_start_address = x12;\n  Register capture_end_address = x13;\n  Register current_position_address = x14;\n  Register capture_length = w15;\n\n  // Find length of back-referenced capture.\n  DCHECK_EQ(0, start_reg % 2);\n  if (start_reg < kNumCachedRegisters) {\n    __ Mov(x10, GetCachedRegister(start_reg));\n    __ Lsr(x11, GetCachedRegister(start_reg), kWRegSizeInBits);\n  } else {\n    __ Ldp(w11, w10, capture_location(start_reg, x10));\n  }\n  __ Sub(capture_length, w11, w10);  // Length to check.\n\n  // At this point, the capture registers are either both set or both cleared.\n  // If the capture length is zero, then the capture is either empty or cleared.\n  // Fall through in both cases.\n  __ CompareAndBranch(capture_length, Operand(0), eq, &fallthrough);\n\n  // Check that there are enough characters left in the input.\n  if (read_backward) {\n    __ Add(w12, string_start_minus_one(), capture_length);\n    __ Cmp(current_input_offset(), w12);\n    BranchOrBacktrack(le, on_no_match);\n  } else {\n    __ Cmn(capture_length, current_input_offset());\n    BranchOrBacktrack(gt, on_no_match);\n  }\n\n  // Compute pointers to match string and capture string\n  __ Add(capture_start_address, input_end(), Operand(w10, SXTW));\n  __ Add(capture_end_address,\n         capture_start_address,\n         Operand(capture_length, SXTW));\n  __ Add(current_position_address,\n         input_end(),\n         Operand(current_input_offset(), SXTW));\n  if (read_backward) {\n    // Offset by length when matching backwards.\n    __ Sub(current_position_address, current_position_address,\n           Operand(capture_length, SXTW));\n  }\n\n  Label loop;\n  __ Bind(&loop);\n  if (mode_ == LATIN1) {\n    __ Ldrb(w10, MemOperand(capture_start_address, 1, PostIndex));\n    __ Ldrb(w11, MemOperand(current_position_address, 1, PostIndex));\n  } else {\n    DCHECK(mode_ == UC16);\n    __ Ldrh(w10, MemOperand(capture_start_address, 2, PostIndex));\n    __ Ldrh(w11, MemOperand(current_position_address, 2, PostIndex));\n  }\n  __ Cmp(w10, w11);\n  BranchOrBacktrack(ne, on_no_match);\n  __ Cmp(capture_start_address, capture_end_address);\n  __ B(lt, &loop);\n\n  // Move current character position to position after match.\n  __ Sub(current_input_offset().X(), current_position_address, input_end());\n  if (read_backward) {\n    __ Sub(current_input_offset().X(), current_input_offset().X(),\n           Operand(capture_length, SXTW));\n  }\n\n  if (v8_flags.debug_code) {\n    __ Cmp(current_input_offset().X(), Operand(current_input_offset(), SXTW));\n    __ Ccmp(current_input_offset(), 0, NoFlag, eq);\n    // The current input offset should be <= 0, and fit in a W register.\n    __ Check(le, AbortReason::kOffsetOutOfRange);\n  }\n  __ Bind(&fallthrough);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotBackReference(int start_reg,\n                                                      bool read_backward,\n                                                      Label* on_no_match) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotBackReference", "content": "void RegExpMacroAssemblerRISCV::CheckNotBackReference(int start_reg,\n                                                      bool read_backward,\n                                                      Label* on_no_match) {\n  Label fallthrough;\n\n  // Find length of back-referenced capture.\n  __ LoadWord(a0, register_location(start_reg));\n  __ LoadWord(a1, register_location(start_reg + 1));\n  __ SubWord(a1, a1, a0);  // Length to check.\n\n  // At this point, the capture registers are either both set or both cleared.\n  // If the capture length is zero, then the capture is either empty or cleared.\n  // Fall through in both cases.\n  __ BranchShort(&fallthrough, eq, a1, Operand(zero_reg));\n\n  if (read_backward) {\n    __ LoadWord(t1, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n    __ AddWord(t1, t1, a1);\n    BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));\n  } else {\n    __ AddWord(t1, a1, current_input_offset());\n    // Check that there are enough characters left in the input.\n    BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));\n  }\n\n  // Compute pointers to match string and capture string.\n  __ AddWord(a0, a0, Operand(end_of_input_address()));\n  __ AddWord(a2, end_of_input_address(), Operand(current_input_offset()));\n  if (read_backward) {\n    __ SubWord(a2, a2, Operand(a1));\n  }\n  __ AddWord(a1, a1, Operand(a0));\n\n  Label loop;\n  __ bind(&loop);\n  if (mode_ == LATIN1) {\n    __ Lbu(a3, MemOperand(a0, 0));\n    __ addi(a0, a0, char_size());\n    __ Lbu(a4, MemOperand(a2, 0));\n    __ addi(a2, a2, char_size());\n  } else {\n    DCHECK(mode_ == UC16);\n    __ Lhu(a3, MemOperand(a0, 0));\n    __ addi(a0, a0, char_size());\n    __ Lhu(a4, MemOperand(a2, 0));\n    __ addi(a2, a2, char_size());\n  }\n  BranchOrBacktrack(on_no_match, ne, a3, Operand(a4));\n  __ Branch(&loop, lt, a0, Operand(a1));\n\n  // Move current character position to position after match.\n  __ SubWord(current_input_offset(), a2, end_of_input_address());\n  if (read_backward) {\n    __ LoadWord(t1,\n                register_location(start_reg));  // Index of start of capture.\n    __ LoadWord(a2,\n                register_location(start_reg + 1));  // Index of end of capture.\n    __ AddWord(current_input_offset(), current_input_offset(), Operand(t1));\n    __ SubWord(current_input_offset(), current_input_offset(), Operand(a2));\n  }\n  __ bind(&fallthrough);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotBackReference(int start_reg,\n                                                      bool read_backward,\n                                                      Label* on_no_match) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotBackReferenceIgnoreCase", "content": "void RegExpMacroAssemblerARM64::CheckNotBackReferenceIgnoreCase(\n    int start_reg, bool read_backward, bool unicode, Label* on_no_match) {\n  Label fallthrough;\n\n  Register capture_start_offset = w10;\n  // Save the capture length in a callee-saved register so it will\n  // be preserved if we call a C helper.\n  Register capture_length = w19;\n  DCHECK(kCalleeSaved.IncludesAliasOf(capture_length));\n\n  // Find length of back-referenced capture.\n  DCHECK_EQ(0, start_reg % 2);\n  if (start_reg < kNumCachedRegisters) {\n    __ Mov(capture_start_offset.X(), GetCachedRegister(start_reg));\n    __ Lsr(x11, GetCachedRegister(start_reg), kWRegSizeInBits);\n  } else {\n    __ Ldp(w11, capture_start_offset, capture_location(start_reg, x10));\n  }\n  __ Sub(capture_length, w11, capture_start_offset);  // Length to check.\n\n  // At this point, the capture registers are either both set or both cleared.\n  // If the capture length is zero, then the capture is either empty or cleared.\n  // Fall through in both cases.\n  __ CompareAndBranch(capture_length, Operand(0), eq, &fallthrough);\n\n  // Check that there are enough characters left in the input.\n  if (read_backward) {\n    __ Add(w12, string_start_minus_one(), capture_length);\n    __ Cmp(current_input_offset(), w12);\n    BranchOrBacktrack(le, on_no_match);\n  } else {\n    __ Cmn(capture_length, current_input_offset());\n    BranchOrBacktrack(gt, on_no_match);\n  }\n\n  if (mode_ == LATIN1) {\n    Label success;\n    Label fail;\n    Label loop_check;\n\n    Register capture_start_address = x12;\n    Register capture_end_addresss = x13;\n    Register current_position_address = x14;\n\n    __ Add(capture_start_address,\n           input_end(),\n           Operand(capture_start_offset, SXTW));\n    __ Add(capture_end_addresss,\n           capture_start_address,\n           Operand(capture_length, SXTW));\n    __ Add(current_position_address,\n           input_end(),\n           Operand(current_input_offset(), SXTW));\n    if (read_backward) {\n      // Offset by length when matching backwards.\n      __ Sub(current_position_address, current_position_address,\n             Operand(capture_length, SXTW));\n    }\n\n    Label loop;\n    __ Bind(&loop);\n    __ Ldrb(w10, MemOperand(capture_start_address, 1, PostIndex));\n    __ Ldrb(w11, MemOperand(current_position_address, 1, PostIndex));\n    __ Cmp(w10, w11);\n    __ B(eq, &loop_check);\n\n    // Mismatch, try case-insensitive match (converting letters to lower-case).\n    __ Orr(w10, w10, 0x20);  // Convert capture character to lower-case.\n    __ Orr(w11, w11, 0x20);  // Also convert input character.\n    __ Cmp(w11, w10);\n    __ B(ne, &fail);\n    __ Sub(w10, w10, 'a');\n    __ Cmp(w10, 'z' - 'a');  // Is w10 a lowercase letter?\n    __ B(ls, &loop_check);  // In range 'a'-'z'.\n    // Latin-1: Check for values in range [224,254] but not 247.\n    __ Sub(w10, w10, 224 - 'a');\n    __ Cmp(w10, 254 - 224);\n    __ Ccmp(w10, 247 - 224, ZFlag, ls);  // Check for 247.\n    __ B(eq, &fail);  // Weren't Latin-1 letters.\n\n    __ Bind(&loop_check);\n    __ Cmp(capture_start_address, capture_end_addresss);\n    __ B(lt, &loop);\n    __ B(&success);\n\n    __ Bind(&fail);\n    BranchOrBacktrack(al, on_no_match);\n\n    __ Bind(&success);\n    // Compute new value of character position after the matched part.\n    __ Sub(current_input_offset().X(), current_position_address, input_end());\n    if (read_backward) {\n      __ Sub(current_input_offset().X(), current_input_offset().X(),\n             Operand(capture_length, SXTW));\n    }\n    if (v8_flags.debug_code) {\n      __ Cmp(current_input_offset().X(), Operand(current_input_offset(), SXTW));\n      __ Ccmp(current_input_offset(), 0, NoFlag, eq);\n      // The current input offset should be <= 0, and fit in a W register.\n      __ Check(le, AbortReason::kOffsetOutOfRange);\n    }\n  } else {\n    DCHECK(mode_ == UC16);\n    int argument_count = 4;\n\n    PushCachedRegisters();\n\n    // Put arguments into arguments registers.\n    // Parameters are\n    //   x0: Address byte_offset1 - Address captured substring's start.\n    //   x1: Address byte_offset2 - Address of current character position.\n    //   w2: size_t byte_length - length of capture in bytes(!)\n    //   x3: Isolate* isolate.\n\n    // Address of start of capture.\n    __ Add(x0, input_end(), Operand(capture_start_offset, SXTW));\n    // Length of capture.\n    __ Mov(w2, capture_length);\n    // Address of current input position.\n    __ Add(x1, input_end(), Operand(current_input_offset(), SXTW));\n    if (read_backward) {\n      __ Sub(x1, x1, Operand(capture_length, SXTW));\n    }\n    // Isolate.\n    __ Mov(x3, ExternalReference::isolate_address(isolate()));\n\n    {\n      AllowExternalCallThatCantCauseGC scope(masm_.get());\n      ExternalReference function =\n          unicode\n              ? ExternalReference::re_case_insensitive_compare_unicode()\n              : ExternalReference::re_case_insensitive_compare_non_unicode();\n      CallCFunctionFromIrregexpCode(function, argument_count);\n    }\n\n    // Check if function returned non-zero for success or zero for failure.\n    // x0 is one of the registers used as a cache so it must be tested before\n    // the cache is restored.\n    __ Cmp(x0, 0);\n    PopCachedRegisters();\n    BranchOrBacktrack(eq, on_no_match);\n\n    // On success, advance position by length of capture.\n    if (read_backward) {\n      __ Sub(current_input_offset(), current_input_offset(), capture_length);\n    } else {\n      __ Add(current_input_offset(), current_input_offset(), capture_length);\n    }\n  }\n\n  __ Bind(&fallthrough);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotBackReferenceIgnoreCase(\n    int start_reg, bool read_backward, bool unicode, Label* on_no_match) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase", "content": "void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(\n    int start_reg, bool read_backward, bool unicode, Label* on_no_match) {\n  Label fallthrough;\n  __ LoadWord(a0, register_location(start_reg));  // Index of start of capture.\n  __ LoadWord(a1,\n              register_location(start_reg + 1));  // Index of end of capture.\n  __ SubWord(a1, a1, a0);                         // Length of capture.\n\n  // At this point, the capture registers are either both set or both cleared.\n  // If the capture length is zero, then the capture is either empty or cleared.\n  // Fall through in both cases.\n  __ BranchShort(&fallthrough, eq, a1, Operand(zero_reg));\n\n  if (read_backward) {\n    __ LoadWord(t1, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n    __ AddWord(t1, t1, a1);\n    BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));\n  } else {\n    __ AddWord(t1, a1, current_input_offset());\n    // Check that there are enough characters left in the input.\n    BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));\n  }\n\n  if (mode_ == LATIN1) {\n    Label success;\n    Label fail;\n    Label loop_check;\n\n    // a0 - offset of start of capture.\n    // a1 - length of capture.\n    __ AddWord(a0, a0, Operand(end_of_input_address()));\n    __ AddWord(a2, end_of_input_address(), Operand(current_input_offset()));\n    if (read_backward) {\n      __ SubWord(a2, a2, Operand(a1));\n    }\n    __ AddWord(a1, a0, Operand(a1));\n\n    // a0 - Address of start of capture.\n    // a1 - Address of end of capture.\n    // a2 - Address of current input position.\n\n    Label loop;\n    __ bind(&loop);\n    __ Lbu(a3, MemOperand(a0, 0));\n    __ addi(a0, a0, char_size());\n    __ Lbu(a4, MemOperand(a2, 0));\n    __ addi(a2, a2, char_size());\n\n    __ BranchShort(&loop_check, eq, a4, Operand(a3));\n\n    // Mismatch, try case-insensitive match (converting letters to lower-case).\n    __ Or(a3, a3, Operand(0x20));  // Convert capture character to lower-case.\n    __ Or(a4, a4, Operand(0x20));  // Also convert input character.\n    __ BranchShort(&fail, ne, a4, Operand(a3));\n    __ SubWord(a3, a3, Operand('a'));\n    __ BranchShort(&loop_check, Uless_equal, a3, Operand('z' - 'a'));\n    // Latin-1: Check for values in range [224,254] but not 247.\n    __ SubWord(a3, a3, Operand(224 - 'a'));\n    // Weren't Latin-1 letters.\n    __ BranchShort(&fail, Ugreater, a3, Operand(254 - 224));\n    // Check for 247.\n    __ BranchShort(&fail, eq, a3, Operand(247 - 224));\n\n    __ bind(&loop_check);\n    __ Branch(&loop, lt, a0, Operand(a1));\n    __ jmp(&success);\n\n    __ bind(&fail);\n    GoTo(on_no_match);\n\n    __ bind(&success);\n    // Compute new value of character position after the matched part.\n    __ SubWord(current_input_offset(), a2, end_of_input_address());\n    if (read_backward) {\n      __ LoadWord(t1,\n                  register_location(start_reg));  // Index of start of capture.\n      __ LoadWord(\n          a2, register_location(start_reg + 1));  // Index of end of capture.\n      __ AddWord(current_input_offset(), current_input_offset(), Operand(t1));\n      __ SubWord(current_input_offset(), current_input_offset(), Operand(a2));\n    }\n  } else {\n    DCHECK(mode_ == UC16);\n\n    int argument_count = 4;\n    __ PrepareCallCFunction(argument_count, a2);\n\n    // a0 - offset of start of capture.\n    // a1 - length of capture.\n\n    // Put arguments into arguments registers.\n    // Parameters are\n    //   a0: Address byte_offset1 - Address captured substring's start.\n    //   a1: Address byte_offset2 - Address of current character position.\n    //   a2: size_t byte_length - length of capture in bytes(!).\n    //   a3: Isolate* isolate.\n\n    // Address of start of capture.\n    __ AddWord(a0, a0, Operand(end_of_input_address()));\n    // Length of capture.\n    __ mv(a2, a1);\n    // Save length in callee-save register for use on return.\n    __ mv(s3, a1);\n    // Address of current input position.\n    __ AddWord(a1, current_input_offset(), Operand(end_of_input_address()));\n    if (read_backward) {\n      __ SubWord(a1, a1, Operand(s3));\n    }\n    // Isolate.\n    __ li(a3, Operand(ExternalReference::isolate_address(masm_->isolate())));\n\n    {\n      AllowExternalCallThatCantCauseGC scope(masm_.get());\n      ExternalReference function =\n          unicode\n              ? ExternalReference::re_case_insensitive_compare_unicode()\n              : ExternalReference::re_case_insensitive_compare_non_unicode();\n      CallCFunctionFromIrregexpCode(function, argument_count);\n    }\n\n    // Check if function returned non-zero for success or zero for failure.\n    BranchOrBacktrack(on_no_match, eq, a0, Operand(zero_reg));\n    // On success, increment position by length of capture.\n    if (read_backward) {\n      __ SubWord(current_input_offset(), current_input_offset(), Operand(s3));\n    } else {\n      __ AddWord(current_input_offset(), current_input_offset(), Operand(s3));\n    }\n  }\n\n  __ bind(&fallthrough);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(\n    int start_reg, bool read_backward, bool unicode, Label* on_no_match) "}], [{"name": "RegExpMacroAssemblerARM64::CheckGreedyLoop", "content": "void RegExpMacroAssemblerARM64::CheckGreedyLoop(Label* on_equal) {\n  __ Ldr(w10, MemOperand(backtrack_stackpointer()));\n  __ Cmp(current_input_offset(), w10);\n  __ Cset(x11, eq);\n  __ Add(backtrack_stackpointer(),\n         backtrack_stackpointer(), Operand(x11, LSL, kWRegSizeLog2));\n  BranchOrBacktrack(eq, on_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckGreedyLoop(Label* on_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckGreedyLoop", "content": "void RegExpMacroAssemblerRISCV::CheckGreedyLoop(Label* on_equal) {\n  Label backtrack_non_equal;\n  __ Lw(a0, MemOperand(backtrack_stackpointer(), 0));\n  __ BranchShort(&backtrack_non_equal, ne, current_input_offset(), Operand(a0));\n  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(),\n             Operand(kIntSize));\n  __ bind(&backtrack_non_equal);\n  BranchOrBacktrack(on_equal, eq, current_input_offset(), Operand(a0));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckGreedyLoop(Label* on_equal) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterLT", "content": "void RegExpMacroAssemblerARM64::CheckCharacterLT(base::uc16 limit,\n                                                 Label* on_less) {\n  CompareAndBranchOrBacktrack(current_character(), limit, lo, on_less);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacterLT(base::uc16 limit,\n                                                 Label* on_less) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterLT", "content": "void RegExpMacroAssemblerRISCV::CheckCharacterLT(base::uc16 limit,\n                                                 Label* on_less) {\n  BranchOrBacktrack(on_less, lt, current_character(), Operand(limit));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacterLT(base::uc16 limit,\n                                                 Label* on_less) "}], [{"name": "RegExpMacroAssemblerARM64::CheckNotAtStart", "content": "void RegExpMacroAssemblerARM64::CheckNotAtStart(int cp_offset,\n                                                Label* on_not_at_start) {\n  __ Add(w10, current_input_offset(),\n         Operand(-char_size() + cp_offset * char_size()));\n  __ Cmp(w10, string_start_minus_one());\n  BranchOrBacktrack(ne, on_not_at_start);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckNotAtStart(int cp_offset,\n                                                Label* on_not_at_start) "}, {"name": "RegExpMacroAssemblerRISCV::CheckNotAtStart", "content": "void RegExpMacroAssemblerRISCV::CheckNotAtStart(int cp_offset,\n                                                Label* on_not_at_start) {\n  __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n  __ AddWord(a0, current_input_offset(),\n             Operand(-char_size() + cp_offset * char_size()));\n  BranchOrBacktrack(on_not_at_start, ne, a0, Operand(a1));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckNotAtStart(int cp_offset,\n                                                Label* on_not_at_start) "}], [{"name": "RegExpMacroAssemblerARM64::CheckAtStart", "content": "void RegExpMacroAssemblerARM64::CheckAtStart(int cp_offset,\n                                             Label* on_at_start) {\n  __ Add(w10, current_input_offset(),\n         Operand(-char_size() + cp_offset * char_size()));\n  __ Cmp(w10, string_start_minus_one());\n  BranchOrBacktrack(eq, on_at_start);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckAtStart(int cp_offset,\n                                             Label* on_at_start) "}, {"name": "RegExpMacroAssemblerRISCV::CheckAtStart", "content": "void RegExpMacroAssemblerRISCV::CheckAtStart(int cp_offset,\n                                             Label* on_at_start) {\n  __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOneOffset));\n  __ AddWord(a0, current_input_offset(),\n             Operand(-char_size() + cp_offset * char_size()));\n  BranchOrBacktrack(on_at_start, eq, a0, Operand(a1));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckAtStart(int cp_offset,\n                                             Label* on_at_start) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacterGT", "content": "void RegExpMacroAssemblerARM64::CheckCharacterGT(base::uc16 limit,\n                                                 Label* on_greater) {\n  CompareAndBranchOrBacktrack(current_character(), limit, hi, on_greater);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacterGT(base::uc16 limit,\n                                                 Label* on_greater) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacterGT", "content": "void RegExpMacroAssemblerRISCV::CheckCharacterGT(base::uc16 limit,\n                                                 Label* on_greater) {\n  BranchOrBacktrack(on_greater, gt, current_character(), Operand(limit));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacterGT(base::uc16 limit,\n                                                 Label* on_greater) "}], [{"name": "RegExpMacroAssemblerARM64::CheckCharacter", "content": "void RegExpMacroAssemblerARM64::CheckCharacter(uint32_t c, Label* on_equal) {\n  CompareAndBranchOrBacktrack(current_character(), c, eq, on_equal);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::CheckCharacter(uint32_t c, Label* on_equal) "}, {"name": "RegExpMacroAssemblerRISCV::CheckCharacter", "content": "void RegExpMacroAssemblerRISCV::CheckCharacter(uint32_t c, Label* on_equal) {\n  BranchOrBacktrack(on_equal, eq, current_character(), Operand(c));\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::CheckCharacter(uint32_t c, Label* on_equal) "}], [{"name": "RegExpMacroAssemblerARM64::Bind", "content": "void RegExpMacroAssemblerARM64::Bind(Label* label) {\n  __ Bind(label);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::Bind(Label* label) "}, {"name": "RegExpMacroAssemblerRISCV::Bind", "content": "void RegExpMacroAssemblerRISCV::Bind(Label* label) { __ bind(label); }", "name_and_para": "void RegExpMacroAssemblerRISCV::Bind(Label* label) "}], [{"name": "RegExpMacroAssemblerARM64::Backtrack", "content": "void RegExpMacroAssemblerARM64::Backtrack() {\n  CheckPreemption();\n  if (has_backtrack_limit()) {\n    Label next;\n    UseScratchRegisterScope temps(masm_.get());\n    Register scratch = temps.AcquireW();\n    __ Ldr(scratch, MemOperand(frame_pointer(), kBacktrackCountOffset));\n    __ Add(scratch, scratch, 1);\n    __ Str(scratch, MemOperand(frame_pointer(), kBacktrackCountOffset));\n    __ Cmp(scratch, Operand(backtrack_limit()));\n    __ B(ne, &next);\n\n    // Backtrack limit exceeded.\n    if (can_fallback()) {\n      __ B(&fallback_label_);\n    } else {\n      // Can't fallback, so we treat it as a failed match.\n      Fail();\n    }\n\n    __ bind(&next);\n  }\n  Pop(w10);\n  __ Add(x10, code_pointer(), Operand(w10, UXTW));\n  __ Br(x10);\n}", "name_and_para": "void RegExpMacroAssemblerARM64::Backtrack() "}, {"name": "RegExpMacroAssemblerRISCV::Backtrack", "content": "void RegExpMacroAssemblerRISCV::Backtrack() {\n  CheckPreemption();\n  if (has_backtrack_limit()) {\n    Label next;\n    __ LoadWord(a0, MemOperand(frame_pointer(), kBacktrackCountOffset));\n    __ AddWord(a0, a0, Operand(1));\n    __ StoreWord(a0, MemOperand(frame_pointer(), kBacktrackCountOffset));\n    __ BranchShort(&next, ne, a0, Operand(backtrack_limit()));\n\n    // Backtrack limit exceeded.\n    if (can_fallback()) {\n      __ jmp(&fallback_label_);\n    } else {\n      // Can't fallback, so we treat it as a failed match.\n      Fail();\n    }\n\n    __ bind(&next);\n  }\n  // Pop Code offset from backtrack stack, add Code and jump to location.\n  Pop(a0);\n  __ AddWord(a0, a0, code_pointer());\n  __ Jump(a0);\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::Backtrack() "}], [{"name": "RegExpMacroAssemblerARM64::AdvanceRegister", "content": "void RegExpMacroAssemblerARM64::AdvanceRegister(int reg, int by) {\n  DCHECK((reg >= 0) && (reg < num_registers_));\n  if (by != 0) {\n    RegisterState register_state = GetRegisterState(reg);\n    switch (register_state) {\n      case STACKED:\n        __ Ldr(w10, register_location(reg));\n        __ Add(w10, w10, by);\n        __ Str(w10, register_location(reg));\n        break;\n      case CACHED_LSW: {\n        Register to_advance = GetCachedRegister(reg);\n        __ Add(to_advance, to_advance, by);\n        break;\n      }\n      case CACHED_MSW: {\n        Register to_advance = GetCachedRegister(reg);\n        // Sign-extend to int64, shift as uint64, cast back to int64.\n        __ Add(\n            to_advance, to_advance,\n            static_cast<int64_t>(static_cast<uint64_t>(static_cast<int64_t>(by))\n                                 << kWRegSizeInBits));\n        break;\n      }\n      default:\n        UNREACHABLE();\n    }\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::AdvanceRegister(int reg, int by) "}, {"name": "RegExpMacroAssemblerRISCV::AdvanceRegister", "content": "void RegExpMacroAssemblerRISCV::AdvanceRegister(int reg, int by) {\n  DCHECK_LE(0, reg);\n  DCHECK_GT(num_registers_, reg);\n  if (by != 0) {\n    __ LoadWord(a0, register_location(reg));\n    __ AddWord(a0, a0, Operand(by));\n    __ StoreWord(a0, register_location(reg));\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::AdvanceRegister(int reg, int by) "}], [{"name": "RegExpMacroAssemblerARM64::AdvanceCurrentPosition", "content": "void RegExpMacroAssemblerARM64::AdvanceCurrentPosition(int by) {\n  if (by != 0) {\n    __ Add(current_input_offset(),\n           current_input_offset(), by * char_size());\n  }\n}", "name_and_para": "void RegExpMacroAssemblerARM64::AdvanceCurrentPosition(int by) "}, {"name": "RegExpMacroAssemblerRISCV::AdvanceCurrentPosition", "content": "void RegExpMacroAssemblerRISCV::AdvanceCurrentPosition(int by) {\n  if (by != 0) {\n    __ AddWord(current_input_offset(), current_input_offset(),\n               Operand(by * char_size()));\n  }\n}", "name_and_para": "void RegExpMacroAssemblerRISCV::AdvanceCurrentPosition(int by) "}], [{"name": "RegExpMacroAssemblerARM64::stack_limit_slack", "content": "int RegExpMacroAssemblerARM64::stack_limit_slack()  {\n  return RegExpStack::kStackLimitSlack;\n}", "name_and_para": "int RegExpMacroAssemblerARM64::stack_limit_slack()  "}, {"name": "RegExpMacroAssemblerRISCV::stack_limit_slack", "content": "int RegExpMacroAssemblerRISCV::stack_limit_slack() {\n  return RegExpStack::kStackLimitSlack;\n}", "name_and_para": "int RegExpMacroAssemblerRISCV::stack_limit_slack() "}], [{"name": "RegExpMacroAssemblerARM64::~RegExpMacroAssemblerARM64", "content": "RegExpMacroAssemblerARM64::~RegExpMacroAssemblerARM64() = default;", "name_and_para": "RegExpMacroAssemblerARM64::~RegExpMacroAssemblerARM64() "}, {"name": "RegExpMacroAssemblerRISCV::~RegExpMacroAssemblerRISCV", "content": "RegExpMacroAssemblerRISCV::~RegExpMacroAssemblerRISCV() {\n  // Unuse labels in case we throw away the assembler without calling GetCode.\n  entry_label_.Unuse();\n  start_label_.Unuse();\n  success_label_.Unuse();\n  backtrack_label_.Unuse();\n  exit_label_.Unuse();\n  check_preempt_label_.Unuse();\n  stack_overflow_label_.Unuse();\n  internal_failure_label_.Unuse();\n  fallback_label_.Unuse();\n}", "name_and_para": "RegExpMacroAssemblerRISCV::~RegExpMacroAssemblerRISCV() "}], [{"name": "RegExpMacroAssemblerARM64::RegExpMacroAssemblerARM64", "content": "RegExpMacroAssemblerARM64::RegExpMacroAssemblerARM64(Isolate* isolate,\n                                                     Zone* zone, Mode mode,\n                                                     int registers_to_save)\n    : NativeRegExpMacroAssembler(isolate, zone),\n      masm_(std::make_unique<MacroAssembler>(\n          isolate, CodeObjectRequired::kYes,\n          NewAssemblerBuffer(kInitialBufferSize))),\n      no_root_array_scope_(masm_.get()),\n      mode_(mode),\n      num_registers_(registers_to_save),\n      num_saved_registers_(registers_to_save),\n      entry_label_(),\n      start_label_(),\n      success_label_(),\n      backtrack_label_(),\n      exit_label_() {\n  DCHECK_EQ(0, registers_to_save % 2);\n  // We can cache at most 16 W registers in x0-x7.\n  static_assert(kNumCachedRegisters <= 16);\n  static_assert((kNumCachedRegisters % 2) == 0);\n  __ CallTarget();\n\n  __ B(&entry_label_);   // We'll write the entry code later.\n  __ Bind(&start_label_);  // And then continue from here.\n}", "name_and_para": "RegExpMacroAssemblerARM64::RegExpMacroAssemblerARM64(Isolate* isolate,\n                                                     Zone* zone, Mode mode,\n                                                     int registers_to_save)\n    : NativeRegExpMacroAssembler(isolate, zone),\n      masm_(std::make_unique<MacroAssembler>(\n          isolate, CodeObjectRequired::kYes,\n          NewAssemblerBuffer(kInitialBufferSize))),\n      no_root_array_scope_(masm_.get()),\n      mode_(mode),\n      num_registers_(registers_to_save),\n      num_saved_registers_(registers_to_save),\n      entry_label_(),\n      start_label_(),\n      success_label_(),\n      backtrack_label_(),\n      exit_label_() "}, {"name": "RegExpMacroAssemblerRISCV::RegExpMacroAssemblerRISCV", "content": "RegExpMacroAssemblerRISCV::RegExpMacroAssemblerRISCV(Isolate* isolate,\n                                                     Zone* zone, Mode mode,\n                                                     int registers_to_save)\n    : NativeRegExpMacroAssembler(isolate, zone),\n      masm_(std::make_unique<MacroAssembler>(\n          isolate, CodeObjectRequired::kYes,\n          NewAssemblerBuffer(kInitialBufferSize))),\n      no_root_array_scope_(masm_.get()),\n      mode_(mode),\n      num_registers_(registers_to_save),\n      num_saved_registers_(registers_to_save),\n      entry_label_(),\n      start_label_(),\n      success_label_(),\n      backtrack_label_(),\n      exit_label_(),\n      internal_failure_label_() {\n  DCHECK_EQ(0, registers_to_save % 2);\n  __ jmp(&entry_label_);  // We'll write the entry code later.\n  // If the code gets too big or corrupted, an internal exception will be\n  // raised, and we will exit right away.\n  __ bind(&internal_failure_label_);\n  __ li(a0, Operand(FAILURE));\n  __ Ret();\n  __ bind(&start_label_);  // And then continue from here.\n}", "name_and_para": "RegExpMacroAssemblerRISCV::RegExpMacroAssemblerRISCV(Isolate* isolate,\n                                                     Zone* zone, Mode mode,\n                                                     int registers_to_save)\n    : NativeRegExpMacroAssembler(isolate, zone),\n      masm_(std::make_unique<MacroAssembler>(\n          isolate, CodeObjectRequired::kYes,\n          NewAssemblerBuffer(kInitialBufferSize))),\n      no_root_array_scope_(masm_.get()),\n      mode_(mode),\n      num_registers_(registers_to_save),\n      num_saved_registers_(registers_to_save),\n      entry_label_(),\n      start_label_(),\n      success_label_(),\n      backtrack_label_(),\n      exit_label_(),\n      internal_failure_label_() "}]]], [["./v8/src/wasm/baseline/riscv/liftoff-assembler-riscv64-inl.h", "./v8/src/wasm/baseline/riscv/liftoff-assembler-riscv-inl.h", "./v8/src/wasm/baseline/riscv/liftoff-assembler-riscv32-inl.h", "./v8/src/wasm/baseline/arm64/liftoff-assembler-arm64-inl.h"], 0.8256658595641646, 0.7527593818984547, [[{"name": "LiftoffStackSlots::Construct", "content": "void LiftoffStackSlots::Construct(int param_slots) {\n  DCHECK_LT(0, slots_.size());\n  // The stack pointer is required to be quadword aligned.\n  asm_->Claim(RoundUp(param_slots, 2));\n  for (auto& slot : slots_) {\n    int poke_offset = slot.dst_slot_ * kSystemPointerSize;\n    switch (slot.src_.loc()) {\n      case LiftoffAssembler::VarState::kStack: {\n        UseScratchRegisterScope temps(asm_);\n        CPURegister scratch = liftoff::AcquireByType(&temps, slot.src_.kind());\n        asm_->Ldr(scratch, liftoff::GetStackSlot(slot.src_offset_));\n        asm_->Poke(scratch, poke_offset);\n        break;\n      }\n      case LiftoffAssembler::VarState::kRegister:\n        asm_->Poke(liftoff::GetRegFromType(slot.src_.reg(), slot.src_.kind()),\n                   poke_offset);\n        break;\n      case LiftoffAssembler::VarState::kIntConst:\n        DCHECK(slot.src_.kind() == kI32 || slot.src_.kind() == kI64);\n        if (slot.src_.i32_const() == 0) {\n          Register zero_reg = slot.src_.kind() == kI32 ? wzr : xzr;\n          asm_->Poke(zero_reg, poke_offset);\n        } else {\n          UseScratchRegisterScope temps(asm_);\n          Register scratch =\n              slot.src_.kind() == kI32 ? temps.AcquireW() : temps.AcquireX();\n          asm_->Mov(scratch, int64_t{slot.src_.i32_const()});\n          asm_->Poke(scratch, poke_offset);\n        }\n        break;\n    }\n  }\n}", "name_and_para": "void LiftoffStackSlots::Construct(int param_slots) "}, {"name": "LiftoffStackSlots::Construct", "content": "void LiftoffStackSlots::Construct(int param_slots) {\n  DCHECK_LT(0, slots_.size());\n  SortInPushOrder();\n  int last_stack_slot = param_slots;\n  for (auto& slot : slots_) {\n    const int stack_slot = slot.dst_slot_;\n    int stack_decrement = (last_stack_slot - stack_slot) * kSystemPointerSize;\n    DCHECK_LT(0, stack_decrement);\n    last_stack_slot = stack_slot;\n    const LiftoffAssembler::VarState& src = slot.src_;\n    switch (src.loc()) {\n      case LiftoffAssembler::VarState::kStack:\n        if (src.kind() != kS128) {\n          asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);\n          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));\n          asm_->push(kScratchReg);\n        } else {\n          asm_->AllocateStackSpace(stack_decrement - kSimd128Size);\n          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_ - 8));\n          asm_->push(kScratchReg);\n          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));\n          asm_->push(kScratchReg);\n        }\n        break;\n      case LiftoffAssembler::VarState::kRegister: {\n        int pushed_bytes = SlotSizeInBytes(slot);\n        asm_->AllocateStackSpace(stack_decrement - pushed_bytes);\n        liftoff::push(asm_, src.reg(), src.kind());\n        break;\n      }\n      case LiftoffAssembler::VarState::kIntConst: {\n        asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);\n        asm_->li(kScratchReg, Operand(src.i32_const()));\n        asm_->push(kScratchReg);\n        break;\n      }\n    }\n  }\n}", "name_and_para": "void LiftoffStackSlots::Construct(int param_slots) "}], [{"name": "LiftoffAssembler::emit_s128_set_if_nan", "content": "void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,\n                                            Register tmp_gp,\n                                            LiftoffRegister tmp_s128,\n                                            ValueKind lane_kind) {\n  DoubleRegister tmp_fp = tmp_s128.fp();\n  if (lane_kind == kF32) {\n    Fmaxv(tmp_fp.S(), src.fp().V4S());\n  } else {\n    DCHECK_EQ(lane_kind, kF64);\n    Fmaxp(tmp_fp.D(), src.fp().V2D());\n  }\n  emit_set_if_nan(dst, tmp_fp, lane_kind);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,\n                                            Register tmp_gp,\n                                            LiftoffRegister tmp_s128,\n                                            ValueKind lane_kind) "}, {"name": "LiftoffAssembler::emit_s128_set_if_nan", "content": "void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,\n                                            Register tmp_gp,\n                                            LiftoffRegister tmp_s128,\n                                            ValueKind lane_kind) {\n  ASM_CODE_COMMENT(this);\n  if (lane_kind == kF32) {\n    VU.set(kScratchReg, E32, m1);\n    vmfeq_vv(kSimd128ScratchReg, src.fp().toV(),\n             src.fp().toV());  // scratch <- !IsNan(tmp_fp)\n  } else {\n    VU.set(kScratchReg, E64, m1);\n    DCHECK_EQ(lane_kind, kF64);\n    vmfeq_vv(kSimd128ScratchReg, src.fp().toV(),\n             src.fp().toV());  // scratch <- !IsNan(tmp_fp)\n  }\n  vmv_xs(kScratchReg, kSimd128ScratchReg);\n  not_(kScratchReg, kScratchReg);\n  andi(kScratchReg, kScratchReg, int32_t(lane_kind == kF32 ? 0xF : 0x3));\n  Sw(kScratchReg, MemOperand(dst));\n}", "name_and_para": "void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,\n                                            Register tmp_gp,\n                                            LiftoffRegister tmp_s128,\n                                            ValueKind lane_kind) "}], [{"name": "LiftoffAssembler::emit_set_if_nan", "content": "void LiftoffAssembler::emit_set_if_nan(Register dst, DoubleRegister src,\n                                       ValueKind kind) {\n  Label not_nan;\n  if (kind == kF32) {\n    Fcmp(src.S(), src.S());\n    B(eq, &not_nan);  // x != x iff isnan(x)\n    // If it's a NaN, it must be non-zero, so store that as the set value.\n    Str(src.S(), MemOperand(dst));\n  } else {\n    DCHECK_EQ(kind, kF64);\n    Fcmp(src.D(), src.D());\n    B(eq, &not_nan);  // x != x iff isnan(x)\n    // Double-precision NaNs must be non-zero in the most-significant 32\n    // bits, so store that.\n    St1(src.V4S(), 1, MemOperand(dst));\n  }\n  Bind(&not_nan);\n}", "name_and_para": "void LiftoffAssembler::emit_set_if_nan(Register dst, DoubleRegister src,\n                                       ValueKind kind) "}, {"name": "LiftoffAssembler::emit_set_if_nan", "content": "void LiftoffAssembler::emit_set_if_nan(Register dst, FPURegister src,\n                                       ValueKind kind) {\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  li(scratch, 1);\n  if (kind == kF32) {\n    feq_s(scratch, src, src);  // rd <- !isNan(src)\n  } else {\n    DCHECK_EQ(kind, kF64);\n    feq_d(scratch, src, src);  // rd <- !isNan(src)\n  }\n  seqz(scratch, scratch);\n  Sw(scratch, MemOperand(dst));\n}", "name_and_para": "void LiftoffAssembler::emit_set_if_nan(Register dst, FPURegister src,\n                                       ValueKind kind) "}], [{"name": "LiftoffAssembler::MaybeOSR", "content": "void LiftoffAssembler::MaybeOSR() {}", "name_and_para": "void LiftoffAssembler::MaybeOSR() "}, {"name": "LiftoffAssembler::MaybeOSR", "content": "void LiftoffAssembler::MaybeOSR() {}", "name_and_para": "void LiftoffAssembler::MaybeOSR() "}], [{"name": "LiftoffAssembler::DeallocateStackSlot", "content": "void LiftoffAssembler::DeallocateStackSlot(uint32_t size) {\n  // The stack pointer is required to be quadword aligned.\n  size = RoundUp(size, kQuadWordSizeInBytes);\n  Drop(size, 1);\n}", "name_and_para": "void LiftoffAssembler::DeallocateStackSlot(uint32_t size) "}, {"name": "LiftoffAssembler::DeallocateStackSlot", "content": "void LiftoffAssembler::DeallocateStackSlot(uint32_t size) {\n  AddWord(sp, sp, Operand(size));\n}", "name_and_para": "void LiftoffAssembler::DeallocateStackSlot(uint32_t size) "}], [{"name": "LiftoffAssembler::AllocateStackSlot", "content": "void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) {\n  // The stack pointer is required to be quadword aligned.\n  size = RoundUp(size, kQuadWordSizeInBytes);\n  Claim(size, 1);\n  Mov(addr, sp);\n}", "name_and_para": "void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) "}, {"name": "LiftoffAssembler::AllocateStackSlot", "content": "void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) {\n  AddWord(sp, sp, Operand(-size));\n  MacroAssembler::Move(addr, sp);\n}", "name_and_para": "void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) "}], [{"name": "LiftoffAssembler::CallBuiltin", "content": "void LiftoffAssembler::CallBuiltin(Builtin builtin) {\n  // A direct call to a builtin. Just encode the builtin index. This will be\n  // patched at relocation.\n  Call(static_cast<Address>(builtin), RelocInfo::WASM_STUB_CALL);\n}", "name_and_para": "void LiftoffAssembler::CallBuiltin(Builtin builtin) "}, {"name": "LiftoffAssembler::CallBuiltin", "content": "void LiftoffAssembler::CallBuiltin(Builtin builtin) {\n  // A direct call to a builtin. Just encode the builtin index. This will be\n  // patched at relocation.\n  Call(static_cast<Address>(builtin), RelocInfo::WASM_STUB_CALL);\n}", "name_and_para": "void LiftoffAssembler::CallBuiltin(Builtin builtin) "}], [{"name": "LiftoffAssembler::TailCallIndirect", "content": "void LiftoffAssembler::TailCallIndirect(Register target) {\n  DCHECK(target.is_valid());\n  // When control flow integrity is enabled, the target is a \"bti c\"\n  // instruction, which enforces that the jump instruction is either a \"blr\", or\n  // a \"br\" with x16 or x17 as its destination.\n  UseScratchRegisterScope temps(this);\n  temps.Exclude(x17);\n  Mov(x17, target);\n  Jump(x17);\n}", "name_and_para": "void LiftoffAssembler::TailCallIndirect(Register target) "}, {"name": "LiftoffAssembler::TailCallIndirect", "content": "void LiftoffAssembler::TailCallIndirect(Register target) {\n  if (target == no_reg) {\n    Pop(t6);\n    Jump(t6);\n  } else {\n    Jump(target);\n  }\n}", "name_and_para": "void LiftoffAssembler::TailCallIndirect(Register target) "}], [{"name": "LiftoffAssembler::CallIndirect", "content": "void LiftoffAssembler::CallIndirect(const ValueKindSig* sig,\n                                    compiler::CallDescriptor* call_descriptor,\n                                    Register target) {\n  // For Arm64, we have more cache registers than wasm parameters. That means\n  // that target will always be in a register.\n  DCHECK(target.is_valid());\n  Call(target);\n}", "name_and_para": "void LiftoffAssembler::CallIndirect(const ValueKindSig* sig,\n                                    compiler::CallDescriptor* call_descriptor,\n                                    Register target) "}, {"name": "LiftoffAssembler::CallIndirect", "content": "void LiftoffAssembler::CallIndirect(const ValueKindSig* sig,\n                                    compiler::CallDescriptor* call_descriptor,\n                                    Register target) {\n  if (target == no_reg) {\n    pop(t6);\n    Call(t6);\n  } else {\n    Call(target);\n  }\n}", "name_and_para": "void LiftoffAssembler::CallIndirect(const ValueKindSig* sig,\n                                    compiler::CallDescriptor* call_descriptor,\n                                    Register target) "}], [{"name": "LiftoffAssembler::TailCallNativeWasmCode", "content": "void LiftoffAssembler::TailCallNativeWasmCode(Address addr) {\n  Jump(addr, RelocInfo::WASM_CALL);\n}", "name_and_para": "void LiftoffAssembler::TailCallNativeWasmCode(Address addr) "}, {"name": "LiftoffAssembler::TailCallNativeWasmCode", "content": "void LiftoffAssembler::TailCallNativeWasmCode(Address addr) {\n  Jump(addr, RelocInfo::WASM_CALL);\n}", "name_and_para": "void LiftoffAssembler::TailCallNativeWasmCode(Address addr) "}], [{"name": "LiftoffAssembler::CallNativeWasmCode", "content": "void LiftoffAssembler::CallNativeWasmCode(Address addr) {\n  Call(addr, RelocInfo::WASM_CALL);\n}", "name_and_para": "void LiftoffAssembler::CallNativeWasmCode(Address addr) "}, {"name": "LiftoffAssembler::CallNativeWasmCode", "content": "void LiftoffAssembler::CallNativeWasmCode(Address addr) {\n  Call(addr, RelocInfo::WASM_CALL);\n}", "name_and_para": "void LiftoffAssembler::CallNativeWasmCode(Address addr) "}], [{"name": "LiftoffAssembler::CallC", "content": "void LiftoffAssembler::CallC(const std::initializer_list<VarState> args_list,\n                             ExternalReference ext_ref) {\n  const int num_args = static_cast<int>(args_list.size());\n  const VarState* const args = args_list.begin();\n\n  // Note: If we ever need more than eight arguments we would need to load the\n  // stack arguments to registers (via LoadToRegister) in pairs of two, then use\n  // Stp with MemOperand{sp, -2 * kSystemPointerSize, PreIndex} to push them to\n  // the stack.\n\n  // Execute the parallel register move for register parameters.\n  DCHECK_GE(arraysize(kCArgRegs), num_args);\n  ParallelMove parallel_move{this};\n  for (int reg_arg = 0; reg_arg < num_args; ++reg_arg) {\n    parallel_move.LoadIntoRegister(LiftoffRegister{kCArgRegs[reg_arg]},\n                                   args[reg_arg]);\n  }\n  parallel_move.Execute();\n\n  // Now call the C function.\n  CallCFunction(ext_ref, num_args);\n}", "name_and_para": "void LiftoffAssembler::CallC(const std::initializer_list<VarState> args_list,\n                             ExternalReference ext_ref) "}, {"name": "LiftoffAssembler::CallC", "content": "void LiftoffAssembler::CallC(const std::initializer_list<VarState> args_list,\n                             ExternalReference ext_ref) {\n  const int num_args = static_cast<int>(args_list.size());\n  const VarState* const args = args_list.begin();\n  // Note: If we ever need more than eight arguments we would need to load the\n  // stack arguments to registers (via LoadToRegister) in pairs of two, then use\n  // Stp with MemOperand{sp, -2 * kSystemPointerSize, PreIndex} to push them to\n  // the stack.\n  // Execute the parallel register move for register parameters.\n  DCHECK_GE(arraysize(kCArgRegs), num_args);\n  ParallelMove parallel_move{this};\n  for (int reg_arg = 0; reg_arg < num_args; ++reg_arg) {\n    parallel_move.LoadIntoRegister(LiftoffRegister{kCArgRegs[reg_arg]},\n                                   args[reg_arg]);\n  }\n  parallel_move.Execute();\n  // Now call the C function.\n  PrepareCallCFunction(num_args, kScratchReg);\n  CallCFunction(ext_ref, num_args);\n}", "name_and_para": "void LiftoffAssembler::CallC(const std::initializer_list<VarState> args_list,\n                             ExternalReference ext_ref) "}], [{"name": "LiftoffAssembler::CallCWithStackBuffer", "content": "void LiftoffAssembler::CallCWithStackBuffer(\n    const std::initializer_list<VarState> args, const LiftoffRegister* rets,\n    ValueKind return_kind, ValueKind out_argument_kind, int stack_bytes,\n    ExternalReference ext_ref) {\n  // The stack pointer is required to be quadword aligned.\n  int total_size = RoundUp(stack_bytes, kQuadWordSizeInBytes);\n  // Reserve space in the stack.\n  Claim(total_size, 1);\n\n  int arg_offset = 0;\n  for (const VarState& arg : args) {\n    liftoff::StoreToMemory(this, MemOperand{sp, arg_offset}, arg);\n    arg_offset += value_kind_size(arg.kind());\n  }\n  DCHECK_LE(arg_offset, stack_bytes);\n\n  // Pass a pointer to the buffer with the arguments to the C function.\n  Mov(x0, sp);\n\n  // Now call the C function.\n  constexpr int kNumCCallArgs = 1;\n  CallCFunction(ext_ref, kNumCCallArgs);\n\n  // Move return value to the right register.\n  const LiftoffRegister* next_result_reg = rets;\n  if (return_kind != kVoid) {\n    constexpr Register kReturnReg = x0;\n    if (kReturnReg != next_result_reg->gp()) {\n      Move(*next_result_reg, LiftoffRegister(kReturnReg), return_kind);\n    }\n    ++next_result_reg;\n  }\n\n  // Load potential output value from the buffer on the stack.\n  if (out_argument_kind != kVoid) {\n    Peek(liftoff::GetRegFromType(*next_result_reg, out_argument_kind), 0);\n  }\n\n  Drop(total_size, 1);\n}", "name_and_para": "void LiftoffAssembler::CallCWithStackBuffer(\n    const std::initializer_list<VarState> args, const LiftoffRegister* rets,\n    ValueKind return_kind, ValueKind out_argument_kind, int stack_bytes,\n    ExternalReference ext_ref) "}, {"name": "LiftoffAssembler::CallCWithStackBuffer", "content": "void LiftoffAssembler::CallCWithStackBuffer(\n    const std::initializer_list<VarState> args, const LiftoffRegister* rets,\n    ValueKind return_kind, ValueKind out_argument_kind, int stack_bytes,\n    ExternalReference ext_ref) {\n  AddWord(sp, sp, Operand(-stack_bytes));\n\n  int arg_offset = 0;\n  for (const VarState& arg : args) {\n    liftoff::StoreToMemory(this, MemOperand{sp, arg_offset}, arg);\n    arg_offset += value_kind_size(arg.kind());\n  }\n  DCHECK_LE(arg_offset, stack_bytes);\n\n  // Pass a pointer to the buffer with the arguments to the C function.\n  // On RISC-V, the first argument is passed in {a0}.\n  constexpr Register kFirstArgReg = a0;\n  mv(kFirstArgReg, sp);\n\n  // Now call the C function.\n  constexpr int kNumCCallArgs = 1;\n  PrepareCallCFunction(kNumCCallArgs, kScratchReg);\n  CallCFunction(ext_ref, kNumCCallArgs);\n\n  // Move return value to the right register.\n  const LiftoffRegister* next_result_reg = rets;\n  if (return_kind != kVoid) {\n    constexpr Register kReturnReg = a0;\n    if (kReturnReg != next_result_reg->gp()) {\n      Move(*next_result_reg, LiftoffRegister(kReturnReg), return_kind);\n    }\n    ++next_result_reg;\n  }\n\n  // Load potential output value from the buffer on the stack.\n  if (out_argument_kind != kVoid) {\n    liftoff::Load(this, *next_result_reg, MemOperand(sp, 0), out_argument_kind);\n  }\n\n  AddWord(sp, sp, Operand(stack_bytes));\n}", "name_and_para": "void LiftoffAssembler::CallCWithStackBuffer(\n    const std::initializer_list<VarState> args, const LiftoffRegister* rets,\n    ValueKind return_kind, ValueKind out_argument_kind, int stack_bytes,\n    ExternalReference ext_ref) "}], [{"name": "LiftoffAssembler::DropStackSlotsAndRet", "content": "void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) {\n  DropSlots(num_stack_slots);\n  Ret();\n}", "name_and_para": "void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) "}, {"name": "LiftoffAssembler::DropStackSlotsAndRet", "content": "void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) {\n  MacroAssembler::DropAndRet(static_cast<int>(num_stack_slots));\n}", "name_and_para": "void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) "}], [{"name": "LiftoffAssembler::RecordSpillsInSafepoint", "content": "void LiftoffAssembler::RecordSpillsInSafepoint(\n    SafepointTableBuilder::Safepoint& safepoint, LiftoffRegList all_spills,\n    LiftoffRegList ref_spills, int spill_offset) {\n  LiftoffRegList fp_spills = all_spills & kFpCacheRegList;\n  int spill_space_size = fp_spills.GetNumRegsSet() * kSimd128Size;\n  LiftoffRegList gp_spills = all_spills & kGpCacheRegList;\n  bool needs_padding = (gp_spills.GetNumRegsSet() & 1) != 0;\n  if (needs_padding) {\n    spill_space_size += kSystemPointerSize;\n    ++spill_offset;\n  }\n  while (!gp_spills.is_empty()) {\n    LiftoffRegister reg = gp_spills.GetLastRegSet();\n    if (ref_spills.has(reg)) {\n      safepoint.DefineTaggedStackSlot(spill_offset);\n    }\n    gp_spills.clear(reg);\n    ++spill_offset;\n    spill_space_size += kSystemPointerSize;\n  }\n  // Record the number of additional spill slots.\n  RecordOolSpillSpaceSize(spill_space_size);\n}", "name_and_para": "void LiftoffAssembler::RecordSpillsInSafepoint(\n    SafepointTableBuilder::Safepoint& safepoint, LiftoffRegList all_spills,\n    LiftoffRegList ref_spills, int spill_offset) "}, {"name": "LiftoffAssembler::RecordSpillsInSafepoint", "content": "void LiftoffAssembler::RecordSpillsInSafepoint(\n    SafepointTableBuilder::Safepoint& safepoint, LiftoffRegList all_spills,\n    LiftoffRegList ref_spills, int spill_offset) {\n  LiftoffRegList fp_spills = all_spills & kFpCacheRegList;\n  int spill_space_size = fp_spills.GetNumRegsSet() * kSimd128Size;\n  LiftoffRegList gp_spills = all_spills & kGpCacheRegList;\n  while (!gp_spills.is_empty()) {\n    LiftoffRegister reg = gp_spills.GetFirstRegSet();\n    if (ref_spills.has(reg)) {\n      safepoint.DefineTaggedStackSlot(spill_offset);\n    }\n    gp_spills.clear(reg);\n    ++spill_offset;\n    spill_space_size += kSystemPointerSize;\n  }\n  // Record the number of additional spill slots.\n  RecordOolSpillSpaceSize(spill_space_size);\n}", "name_and_para": "void LiftoffAssembler::RecordSpillsInSafepoint(\n    SafepointTableBuilder::Safepoint& safepoint, LiftoffRegList all_spills,\n    LiftoffRegList ref_spills, int spill_offset) "}], [{"name": "LiftoffAssembler::PopRegisters", "content": "void LiftoffAssembler::PopRegisters(LiftoffRegList regs) {\n  PopCPURegList(liftoff::PadVRegList(regs.GetFpList()));\n  PopCPURegList(liftoff::PadRegList(regs.GetGpList()));\n}", "name_and_para": "void LiftoffAssembler::PopRegisters(LiftoffRegList regs) "}, {"name": "LiftoffAssembler::PopRegisters", "content": "void LiftoffAssembler::PopRegisters(LiftoffRegList regs) {\n  LiftoffRegList fp_regs = regs & kFpCacheRegList;\n  int32_t fp_offset = 0;\n  while (!fp_regs.is_empty()) {\n    LiftoffRegister reg = fp_regs.GetFirstRegSet();\n    MacroAssembler::LoadDouble(reg.fp(), MemOperand(sp, fp_offset));\n    fp_regs.clear(reg);\n    fp_offset += sizeof(double);\n  }\n  if (fp_offset) AddWord(sp, sp, Operand(fp_offset));\n  LiftoffRegList gp_regs = regs & kGpCacheRegList;\n  int32_t gp_offset = 0;\n  while (!gp_regs.is_empty()) {\n    LiftoffRegister reg = gp_regs.GetLastRegSet();\n    LoadWord(reg.gp(), MemOperand(sp, gp_offset));\n    gp_regs.clear(reg);\n    gp_offset += kSystemPointerSize;\n  }\n  AddWord(sp, sp, Operand(gp_offset));\n}", "name_and_para": "void LiftoffAssembler::PopRegisters(LiftoffRegList regs) "}], [{"name": "LiftoffAssembler::PushRegisters", "content": "void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {\n  PushCPURegList(liftoff::PadRegList(regs.GetGpList()));\n  PushCPURegList(liftoff::PadVRegList(regs.GetFpList()));\n}", "name_and_para": "void LiftoffAssembler::PushRegisters(LiftoffRegList regs) "}, {"name": "LiftoffAssembler::PushRegisters", "content": "void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {\n  LiftoffRegList gp_regs = regs & kGpCacheRegList;\n  int32_t num_gp_regs = gp_regs.GetNumRegsSet();\n  if (num_gp_regs) {\n    int32_t offset = num_gp_regs * kSystemPointerSize;\n    AddWord(sp, sp, Operand(-offset));\n    while (!gp_regs.is_empty()) {\n      LiftoffRegister reg = gp_regs.GetFirstRegSet();\n      offset -= kSystemPointerSize;\n      StoreWord(reg.gp(), MemOperand(sp, offset));\n      gp_regs.clear(reg);\n    }\n    DCHECK_EQ(offset, 0);\n  }\n  LiftoffRegList fp_regs = regs & kFpCacheRegList;\n  int32_t num_fp_regs = fp_regs.GetNumRegsSet();\n  if (num_fp_regs) {\n    AddWord(sp, sp, Operand(-(num_fp_regs * kStackSlotSize)));\n    int32_t offset = 0;\n    while (!fp_regs.is_empty()) {\n      LiftoffRegister reg = fp_regs.GetFirstRegSet();\n      MacroAssembler::StoreDouble(reg.fp(), MemOperand(sp, offset));\n      fp_regs.clear(reg);\n      offset += sizeof(double);\n    }\n    DCHECK_EQ(offset, num_fp_regs * sizeof(double));\n  }\n}", "name_and_para": "void LiftoffAssembler::PushRegisters(LiftoffRegList regs) "}], [{"name": "LiftoffAssembler::AssertUnreachable", "content": "void LiftoffAssembler::AssertUnreachable(AbortReason reason) {\n  MacroAssembler::AssertUnreachable(reason);\n}", "name_and_para": "void LiftoffAssembler::AssertUnreachable(AbortReason reason) "}, {"name": "LiftoffAssembler::AssertUnreachable", "content": "void LiftoffAssembler::AssertUnreachable(AbortReason reason) {\n  if (v8_flags.debug_code) Abort(reason);\n}", "name_and_para": "void LiftoffAssembler::AssertUnreachable(AbortReason reason) "}], [{"name": "LiftoffAssembler::StackCheck", "content": "void LiftoffAssembler::StackCheck(Label* ool_code) {\n  UseScratchRegisterScope temps(this);\n  Register limit_address = temps.AcquireX();\n  LoadStackLimit(limit_address, StackLimitKind::kInterruptStackLimit);\n  Cmp(sp, limit_address);\n  B(ool_code, ls);\n}", "name_and_para": "void LiftoffAssembler::StackCheck(Label* ool_code) "}, {"name": "LiftoffAssembler::StackCheck", "content": "void LiftoffAssembler::StackCheck(Label* ool_code) {\n  UseScratchRegisterScope temps(this);\n  Register limit_address = temps.Acquire();\n  LoadStackLimit(limit_address, StackLimitKind::kInterruptStackLimit);\n  MacroAssembler::Branch(ool_code, ule, sp, Operand(limit_address));\n}", "name_and_para": "void LiftoffAssembler::StackCheck(Label* ool_code) "}], [{"name": "LiftoffAssembler::set_trap_on_oob_mem64", "content": "void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,\n                                             uint64_t oob_index) {\n  Label done;\n  Cmp(index, oob_size);\n  B(&done, kUnsignedLessThan);\n  Mov(index, oob_index);\n  bind(&done);\n}", "name_and_para": "void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,\n                                             uint64_t oob_index) "}, {"name": "LiftoffAssembler::set_trap_on_oob_mem64", "content": "void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,\n                                             uint64_t oob_index) {\n  UNREACHABLE();\n}", "name_and_para": "void LiftoffAssembler::set_trap_on_oob_mem64(Register index, uint64_t oob_size,\n                                             uint64_t oob_index) "}], [{"name": "LiftoffAssembler::emit_f64x2_qfms", "content": "void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  EMIT_QFMOP(Fmls, 2D);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}, {"name": "LiftoffAssembler::emit_f64x2_qfms", "content": "void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  VU.set(kScratchReg, E64, m1);\n  vfnmsub_vv(src1.fp().toV(), src2.fp().toV(), src3.fp().toV());\n  vmv_vv(dst.fp().toV(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}], [{"name": "LiftoffAssembler::emit_f64x2_qfma", "content": "void LiftoffAssembler::emit_f64x2_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  EMIT_QFMOP(Fmla, 2D);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}, {"name": "LiftoffAssembler::emit_f64x2_qfma", "content": "void LiftoffAssembler::emit_f64x2_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  VU.set(kScratchReg, E64, m1);\n  vfmadd_vv(src1.fp().toV(), src2.fp().toV(), src3.fp().toV());\n  vmv_vv(dst.fp().toV(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}], [{"name": "LiftoffAssembler::emit_f32x4_qfms", "content": "void LiftoffAssembler::emit_f32x4_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  EMIT_QFMOP(Fmls, 4S);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}, {"name": "LiftoffAssembler::emit_f32x4_qfms", "content": "void LiftoffAssembler::emit_f32x4_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  VU.set(kScratchReg, E32, m1);\n  vfnmsub_vv(src1.fp().toV(), src2.fp().toV(), src3.fp().toV());\n  vmv_vv(dst.fp().toV(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_qfms(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}], [{"name": "LiftoffAssembler::emit_f32x4_qfma", "content": "void LiftoffAssembler::emit_f32x4_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  EMIT_QFMOP(Fmla, 4S);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}, {"name": "LiftoffAssembler::emit_f32x4_qfma", "content": "void LiftoffAssembler::emit_f32x4_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) {\n  VU.set(kScratchReg, E32, m1);\n  vfmadd_vv(src1.fp().toV(), src2.fp().toV(), src3.fp().toV());\n  vmv_vv(dst.fp().toV(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_qfma(LiftoffRegister dst,\n                                       LiftoffRegister src1,\n                                       LiftoffRegister src2,\n                                       LiftoffRegister src3) "}], [{"name": "LiftoffAssembler::emit_i64x2_abs", "content": "void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Abs(dst.fp().V2D(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_abs", "content": "void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vv(dst.fp().toV(), src.fp().toV());\n  vmv_vv(v0, kSimd128RegZero);\n  vmslt_vv(v0, src.fp().toV(), kSimd128RegZero);\n  vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_abs", "content": "void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Abs(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_abs", "content": "void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vv(dst.fp().toV(), src.fp().toV());\n  vmv_vv(v0, kSimd128RegZero);\n  vmslt_vv(v0, src.fp().toV(), kSimd128RegZero);\n  vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s", "content": "void LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s(LiftoffRegister dst,\n                                                        LiftoffRegister lhs,\n                                                        LiftoffRegister rhs,\n                                                        LiftoffRegister acc) {\n  UseScratchRegisterScope scope(this);\n  VRegister tmp1 = scope.AcquireV(kFormat8H);\n  VRegister tmp2 = scope.AcquireV(kFormat8H);\n  Smull(tmp1, lhs.fp().V8B(), rhs.fp().V8B());\n  Smull2(tmp2, lhs.fp().V16B(), rhs.fp().V16B());\n  Addp(tmp1, tmp1, tmp2);\n  Saddlp(tmp1.V4S(), tmp1);\n  Add(dst.fp().V4S(), tmp1.V4S(), acc.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s(LiftoffRegister dst,\n                                                        LiftoffRegister lhs,\n                                                        LiftoffRegister rhs,\n                                                        LiftoffRegister acc) "}, {"name": "LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s", "content": "void LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s(LiftoffRegister dst,\n                                                        LiftoffRegister lhs,\n                                                        LiftoffRegister rhs,\n                                                        LiftoffRegister acc) {\n  DCHECK_NE(dst, acc);\n  VU.set(kScratchReg, E8, m1);\n  VRegister intermediate = kSimd128ScratchReg3;\n  VRegister kSimd128ScratchReg4 =\n      GetUnusedRegister(LiftoffRegList{LiftoffRegister(ft10)}).fp().toV();\n  vwmul_vv(intermediate, lhs.fp().toV(), rhs.fp().toV());  // i16*16 v8 v9\n\n  constexpr int32_t FIRST_INDEX = 0b0001000100010001;\n  constexpr int32_t SECOND_INDEX = 0b0010001000100010;\n  constexpr int32_t THIRD_INDEX = 0b0100010001000100;\n  constexpr int32_t FOURTH_INDEX = 0b1000100010001000;\n\n  VU.set(kScratchReg, E16, m2);\n  li(kScratchReg, FIRST_INDEX);\n  vmv_sx(v0, kScratchReg);\n  vcompress_vv(kSimd128ScratchReg, intermediate, v0);  // i16*4 a\n  li(kScratchReg, SECOND_INDEX);\n  vmv_sx(kSimd128ScratchReg2, kScratchReg);\n  vcompress_vv(v0, intermediate, kSimd128ScratchReg2);  // i16*4 b\n\n  VU.set(kScratchReg, E16, m1);\n  vwadd_vv(kSimd128ScratchReg4, kSimd128ScratchReg, v0);  // i32*4 c\n\n  VU.set(kScratchReg, E16, m2);\n  li(kScratchReg, THIRD_INDEX);\n  vmv_sx(v0, kScratchReg);\n  vcompress_vv(kSimd128ScratchReg, intermediate, v0);  // i16*4 a\n\n  li(kScratchReg, FOURTH_INDEX);\n  vmv_sx(kSimd128ScratchReg2, kScratchReg);\n  vcompress_vv(v0, intermediate, kSimd128ScratchReg2);  // i16*4 b\n\n  VU.set(kScratchReg, E16, m1);\n  vwadd_vv(kSimd128ScratchReg3, kSimd128ScratchReg, v0);  // i32*4 c\n\n  VU.set(kScratchReg, E32, m1);\n  vadd_vv(dst.fp().toV(), kSimd128ScratchReg4, kSimd128ScratchReg3);\n  vadd_vv(dst.fp().toV(), dst.fp().toV(), acc.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_dot_i8x16_i7x16_add_s(LiftoffRegister dst,\n                                                        LiftoffRegister lhs,\n                                                        LiftoffRegister rhs,\n                                                        LiftoffRegister acc) "}], [{"name": "LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s", "content": "void LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s(LiftoffRegister dst,\n                                                    LiftoffRegister lhs,\n                                                    LiftoffRegister rhs) {\n  UseScratchRegisterScope scope(this);\n  VRegister tmp1 = scope.AcquireV(kFormat8H);\n  VRegister tmp2 = scope.AcquireV(kFormat8H);\n  Smull(tmp1, lhs.fp().V8B(), rhs.fp().V8B());\n  Smull2(tmp2, lhs.fp().V16B(), rhs.fp().V16B());\n  Addp(dst.fp().V8H(), tmp1, tmp2);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s(LiftoffRegister dst,\n                                                    LiftoffRegister lhs,\n                                                    LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s", "content": "void LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s(LiftoffRegister dst,\n                                                    LiftoffRegister lhs,\n                                                    LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vwmul_vv(kSimd128ScratchReg3, lhs.fp().toV(), rhs.fp().toV());\n  VU.set(kScratchReg, E16, m2);\n\n  constexpr int32_t FIRST_INDEX = 0b0101010101010101;\n  constexpr int32_t SECOND_INDEX = 0b1010101010101010;\n  li(kScratchReg, FIRST_INDEX);\n  vmv_sx(v0, kScratchReg);\n  vcompress_vv(kSimd128ScratchReg, kSimd128ScratchReg3, v0);\n\n  li(kScratchReg, SECOND_INDEX);\n  vmv_sx(kSimd128ScratchReg2, kScratchReg);\n  vcompress_vv(v0, kSimd128ScratchReg3, kSimd128ScratchReg2);\n  VU.set(kScratchReg, E16, m1);\n  vadd_vv(dst.fp().toV(), kSimd128ScratchReg, v0);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_dot_i8x16_i7x16_s(LiftoffRegister dst,\n                                                    LiftoffRegister lhs,\n                                                    LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s", "content": "void LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2) {\n  Sqrdmulh(dst.fp().V8H(), src1.fp().V8H(), src2.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s", "content": "void LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, m1);\n  vsmul_vv(dst.fp().toV(), src1.fp().toV(), src2.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_relaxed_q15mulr_s(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_q15mulr_sat_s", "content": "void LiftoffAssembler::emit_i16x8_q15mulr_sat_s(LiftoffRegister dst,\n                                                LiftoffRegister src1,\n                                                LiftoffRegister src2) {\n  Sqrdmulh(dst.fp().V8H(), src1.fp().V8H(), src2.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_q15mulr_sat_s(LiftoffRegister dst,\n                                                LiftoffRegister src1,\n                                                LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_q15mulr_sat_s", "content": "void LiftoffAssembler::emit_i16x8_q15mulr_sat_s(LiftoffRegister dst,\n                                                LiftoffRegister src1,\n                                                LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, m1);\n  vsmul_vv(dst.fp().toV(), src1.fp().toV(), src2.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_q15mulr_sat_s(LiftoffRegister dst,\n                                                LiftoffRegister src1,\n                                                LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Umull2(dst.fp().V8H(), src1.fp().V16B(), src2.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 8);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 8);\n  VU.set(kScratchReg, E8, mf2);\n  vwmulu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Smull2(dst.fp().V8H(), src1.fp().V16B(), src2.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 8);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 8);\n  VU.set(kScratchReg, E8, mf2);\n  vwmul_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_high_i8x16_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Umull(dst.fp().V8H(), src1.fp().V8B(), src2.fp().V8B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E8, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmulu_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E8, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Smull(dst.fp().V8H(), src1.fp().V8B(), src2.fp().V8B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E8, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmul_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E8, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extmul_low_i8x16_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  Uaddlp(dst.fp().V8H(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u", "content": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vi(kSimd128ScratchReg, -1);\n  vmv_vi(kSimd128ScratchReg3, -1);\n  li(kScratchReg, 0x0E0C0A0806040200);\n  vmv_sx(kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 0x0F0D0B0907050301);\n  vmv_sx(kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg, E8, m1);\n  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);\n  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);\n  VU.set(kScratchReg, E8, mf2);\n  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  Saddlp(dst.fp().V8H(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s", "content": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vi(kSimd128ScratchReg, -1);\n  vmv_vi(kSimd128ScratchReg3, -1);\n  li(kScratchReg, 0x0E0C0A0806040200);\n  vmv_sx(kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 0x0F0D0B0907050301);\n  vmv_sx(kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg, E8, m1);\n  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);\n  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);\n  VU.set(kScratchReg, E8, mf2);\n  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_abs", "content": "void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Abs(dst.fp().V8H(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_abs", "content": "void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vv(dst.fp().toV(), src.fp().toV());\n  vmv_vv(v0, kSimd128RegZero);\n  vmslt_vv(v0, src.fp().toV(), kSimd128RegZero);\n  vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_abs", "content": "void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Abs(dst.fp().V16B(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_abs", "content": "void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vv(dst.fp().toV(), src.fp().toV());\n  vmv_vv(v0, kSimd128RegZero);\n  vmslt_vv(v0, src.fp().toV(), kSimd128RegZero);\n  vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_rounding_average_u", "content": "void LiftoffAssembler::emit_i16x8_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) {\n  Urhadd(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_rounding_average_u", "content": "void LiftoffAssembler::emit_i16x8_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg2, E16, m1);\n  vwaddu_vv(kSimd128ScratchReg, lhs.fp().toV(), rhs.fp().toV());\n  li(kScratchReg, 1);\n  vwaddu_wx(kSimd128ScratchReg3, kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 2);\n  VU.set(kScratchReg2, E32, m2);\n  vdivu_vx(kSimd128ScratchReg3, kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg2, E16, m1);\n  vnclipu_vi(dst.fp().toV(), kSimd128ScratchReg3, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_rounding_average_u", "content": "void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) {\n  Urhadd(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_rounding_average_u", "content": "void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vwaddu_vv(kSimd128ScratchReg, lhs.fp().toV(), rhs.fp().toV());\n  li(kScratchReg, 1);\n  vwaddu_wx(kSimd128ScratchReg3, kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 2);\n  VU.set(kScratchReg2, E16, m2);\n  vdivu_vx(kSimd128ScratchReg3, kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg2, E8, m1);\n  vnclipu_vi(dst.fp().toV(), kSimd128ScratchReg3, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_rounding_average_u(LiftoffRegister dst,\n                                                     LiftoffRegister lhs,\n                                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_s128_and_not", "content": "void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) {\n  Bic(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_s128_and_not", "content": "void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vnot_vv(kSimd128ScratchReg, rhs.fp().toV());\n  vand_vv(dst.fp().toV(), lhs.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_and_not(LiftoffRegister dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero", "content": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) {\n  Fcvtzu(dst.fp().V2D(), src.fp().V2D());\n  Uqxtn(dst.fp().V2S(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero", "content": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmfeq_vv(v0, src.fp().toV(), src.fp().toV());\n  vmv_vv(kSimd128ScratchReg3, src.fp().toV());\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vfncvt_xu_f_w(kSimd128ScratchReg, kSimd128ScratchReg3, MaskType::Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_u_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero", "content": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) {\n  Fcvtzs(dst.fp().V2D(), src.fp().V2D());\n  Sqxtn(dst.fp().V2S(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero", "content": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmfeq_vv(v0, src.fp().toV(), src.fp().toV());\n  vmv_vv(kSimd128ScratchReg3, src.fp().toV());\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vfncvt_x_f_w(kSimd128ScratchReg, kSimd128ScratchReg3, MaskType::Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_trunc_sat_f64x2_s_zero(LiftoffRegister dst,\n                                                         LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_uconvert_i16x8_high", "content": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Uxtl2(dst.fp().V4S(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_uconvert_i16x8_high", "content": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 4);\n  VU.set(kScratchReg, E32, m1);\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_uconvert_i16x8_low", "content": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Uxtl(dst.fp().V4S(), src.fp().V4H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_uconvert_i16x8_low", "content": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_sconvert_i16x8_high", "content": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Sxtl2(dst.fp().V4S(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_sconvert_i16x8_high", "content": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 4);\n  VU.set(kScratchReg, E32, m1);\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_sconvert_i16x8_low", "content": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Sxtl(dst.fp().V4S(), src.fp().V4H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_sconvert_i16x8_low", "content": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_i16x8_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_uconvert_i8x16_high", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Uxtl2(dst.fp().V8H(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_uconvert_i8x16_high", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 8);\n  VU.set(kScratchReg, E16, m1);\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_uconvert_i8x16_low", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Uxtl(dst.fp().V8H(), src.fp().V8B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_uconvert_i8x16_low", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_sconvert_i8x16_high", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Sxtl2(dst.fp().V8H(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_sconvert_i8x16_high", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 8);\n  VU.set(kScratchReg, E16, m1);\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_sconvert_i8x16_low", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Sxtl(dst.fp().V8H(), src.fp().V8B());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_sconvert_i8x16_low", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i8x16_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_uconvert_i32x4", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  VRegister tmp = temps.AcquireV(kFormat4S);\n  VRegister right = rhs.fp().V4S();\n  if (dst == rhs) {\n    Mov(tmp, right);\n    right = tmp;\n  }\n  Sqxtun(dst.fp().V4H(), lhs.fp().V4S());\n  Sqxtun2(dst.fp().V8H(), right);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_uconvert_i32x4", "content": "void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vv(kSimd128ScratchReg, lhs.fp().toV());  // kSimd128ScratchReg v24\n  vmv_vv(v25, rhs.fp().toV());\n  VU.set(kScratchReg, E32, m2);\n  vmax_vx(kSimd128ScratchReg, kSimd128ScratchReg, zero_reg);\n  VU.set(kScratchReg, E16, m1);\n  VU.set(FPURoundingMode::RNE);\n  vnclipu_vi(dst.fp().toV(), kSimd128ScratchReg, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_sconvert_i32x4", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  VRegister tmp = temps.AcquireV(kFormat4S);\n  VRegister right = rhs.fp().V4S();\n  if (dst == rhs) {\n    Mov(tmp, right);\n    right = tmp;\n  }\n  Sqxtn(dst.fp().V4H(), lhs.fp().V4S());\n  Sqxtn2(dst.fp().V8H(), right);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_sconvert_i32x4", "content": "void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vv(kSimd128ScratchReg, lhs.fp().toV());  // kSimd128ScratchReg v24\n  vmv_vv(v25, rhs.fp().toV());\n  VU.set(kScratchReg, E16, m1);\n  VU.set(FPURoundingMode::RNE);\n  vnclip_vi(dst.fp().toV(), kSimd128ScratchReg, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_uconvert_i16x8", "content": "void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  VRegister tmp = temps.AcquireV(kFormat8H);\n  VRegister right = rhs.fp().V8H();\n  if (dst == rhs) {\n    Mov(tmp, right);\n    right = tmp;\n  }\n  Sqxtun(dst.fp().V8B(), lhs.fp().V8H());\n  Sqxtun2(dst.fp().V16B(), right);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_uconvert_i16x8", "content": "void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vv(kSimd128ScratchReg, lhs.fp().toV());  // kSimd128ScratchReg v24\n  vmv_vv(v25, rhs.fp().toV());\n  VU.set(kScratchReg, E16, m2);\n  vmax_vx(kSimd128ScratchReg, kSimd128ScratchReg, zero_reg);\n  VU.set(kScratchReg, E8, m1);\n  VU.set(FPURoundingMode::RNE);\n  vnclipu_vi(dst.fp().toV(), kSimd128ScratchReg, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_uconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_sconvert_i16x8", "content": "void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  VRegister tmp = temps.AcquireV(kFormat8H);\n  VRegister right = rhs.fp().V8H();\n  if (dst == rhs) {\n    Mov(tmp, right);\n    right = tmp;\n  }\n  Sqxtn(dst.fp().V8B(), lhs.fp().V8H());\n  Sqxtn2(dst.fp().V16B(), right);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_sconvert_i16x8", "content": "void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vv(kSimd128ScratchReg, lhs.fp().toV());  // kSimd128ScratchReg v24\n  vmv_vv(v25, rhs.fp().toV());\n  VU.set(kScratchReg, E8, m1);\n  VU.set(FPURoundingMode::RNE);\n  vnclip_vi(dst.fp().toV(), kSimd128ScratchReg, 0);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sconvert_i16x8(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_demote_f64x2_zero", "content": "void LiftoffAssembler::emit_f32x4_demote_f64x2_zero(LiftoffRegister dst,\n                                                    LiftoffRegister src) {\n  Fcvtn(dst.fp().V2S(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_demote_f64x2_zero(LiftoffRegister dst,\n                                                    LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_demote_f64x2_zero", "content": "void LiftoffAssembler::emit_f32x4_demote_f64x2_zero(LiftoffRegister dst,\n                                                    LiftoffRegister src) {\n  VU.set(kScratchReg, E32, mf2);\n  vfncvt_f_f_w(dst.fp().toV(), src.fp().toV());\n  VU.set(kScratchReg, E32, m1);\n  vmv_vi(v0, 12);\n  vmerge_vx(dst.fp().toV(), zero_reg, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_demote_f64x2_zero(LiftoffRegister dst,\n                                                    LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_uconvert_i32x4", "content": "void LiftoffAssembler::emit_f32x4_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  Ucvtf(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_uconvert_i32x4", "content": "void LiftoffAssembler::emit_f32x4_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vfcvt_f_xu_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_uconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_sconvert_i32x4", "content": "void LiftoffAssembler::emit_f32x4_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  Scvtf(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_sconvert_i32x4", "content": "void LiftoffAssembler::emit_f32x4_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vfcvt_f_x_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sconvert_i32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_uconvert_f32x4", "content": "void LiftoffAssembler::emit_i32x4_uconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  Fcvtzu(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_uconvert_f32x4", "content": "void LiftoffAssembler::emit_i32x4_uconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vmfeq_vv(v0, src.fp().toV(), src.fp().toV());\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vfcvt_xu_f_v(dst.fp().toV(), kSimd128ScratchReg, Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_uconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_sconvert_f32x4", "content": "void LiftoffAssembler::emit_i32x4_sconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  Fcvtzs(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_sconvert_f32x4", "content": "void LiftoffAssembler::emit_i32x4_sconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vmfeq_vv(v0, src.fp().toV(), src.fp().toV());\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vfcvt_x_f_v(dst.fp().toV(), kSimd128ScratchReg, Mask);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sconvert_f32x4(LiftoffRegister dst,\n                                                 LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_s128_select", "content": "void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,\n                                        LiftoffRegister src1,\n                                        LiftoffRegister src2,\n                                        LiftoffRegister mask) {\n  if (dst != mask) {\n    Mov(dst.fp().V16B(), mask.fp().V16B());\n  }\n  Bsl(dst.fp().V16B(), src1.fp().V16B(), src2.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,\n                                        LiftoffRegister src1,\n                                        LiftoffRegister src2,\n                                        LiftoffRegister mask) "}, {"name": "LiftoffAssembler::emit_s128_select", "content": "void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,\n                                        LiftoffRegister src1,\n                                        LiftoffRegister src2,\n                                        LiftoffRegister mask) {\n  VU.set(kScratchReg, E8, m1);\n  vand_vv(kSimd128ScratchReg, src1.fp().toV(), mask.fp().toV());\n  vnot_vv(kSimd128ScratchReg2, mask.fp().toV());\n  vand_vv(kSimd128ScratchReg2, src2.fp().toV(), kSimd128ScratchReg2);\n  vor_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_select(LiftoffRegister dst,\n                                        LiftoffRegister src1,\n                                        LiftoffRegister src2,\n                                        LiftoffRegister mask) "}], [{"name": "LiftoffAssembler::emit_s128_xor", "content": "void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Eor(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_s128_xor", "content": "void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vxor_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_xor(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_s128_or", "content": "void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  Orr(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_s128_or", "content": "void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vor_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_or(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_s128_and", "content": "void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  And(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_s128_and", "content": "void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vand_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_and(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_s128_not", "content": "void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) {\n  Mvn(dst.fp().V16B(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_s128_not", "content": "void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vnot_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_s128_not(LiftoffRegister dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_s128_const", "content": "void LiftoffAssembler::emit_s128_const(LiftoffRegister dst,\n                                       const uint8_t imms[16]) {\n  uint64_t vals[2];\n  memcpy(vals, imms, sizeof(vals));\n  Movi(dst.fp().V16B(), vals[1], vals[0]);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_const(LiftoffRegister dst,\n                                       const uint8_t imms[16]) "}, {"name": "LiftoffAssembler::emit_s128_const", "content": "void LiftoffAssembler::emit_s128_const(LiftoffRegister dst,\n                                       const uint8_t imms[16]) {\n  WasmRvvS128const(dst.fp().toV(), imms);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_const(LiftoffRegister dst,\n                                       const uint8_t imms[16]) "}], [{"name": "LiftoffAssembler::emit_f64x2_le", "content": "void LiftoffAssembler::emit_f64x2_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmge(dst.fp().V2D(), rhs.fp().V2D(), lhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_le", "content": "void LiftoffAssembler::emit_f64x2_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vmfle_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_lt", "content": "void LiftoffAssembler::emit_f64x2_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmgt(dst.fp().V2D(), rhs.fp().V2D(), lhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_lt", "content": "void LiftoffAssembler::emit_f64x2_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vmflt_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_ne", "content": "void LiftoffAssembler::emit_f64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmeq(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n  Mvn(dst.fp().V2D(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_ne", "content": "void LiftoffAssembler::emit_f64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vmfne_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_eq", "content": "void LiftoffAssembler::emit_f64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmeq(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_eq", "content": "void LiftoffAssembler::emit_f64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vmfeq_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_le", "content": "void LiftoffAssembler::emit_f32x4_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmge(dst.fp().V4S(), rhs.fp().V4S(), lhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_le", "content": "void LiftoffAssembler::emit_f32x4_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmfle_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_le(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_lt", "content": "void LiftoffAssembler::emit_f32x4_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmgt(dst.fp().V4S(), rhs.fp().V4S(), lhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_lt", "content": "void LiftoffAssembler::emit_f32x4_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmflt_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_lt(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_ne", "content": "void LiftoffAssembler::emit_f32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmeq(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n  Mvn(dst.fp().V4S(), dst.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_ne", "content": "void LiftoffAssembler::emit_f32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmfne_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_eq", "content": "void LiftoffAssembler::emit_f32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Fcmeq(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_eq", "content": "void LiftoffAssembler::emit_f32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmfeq_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmv_vx(dst.fp().toV(), zero_reg);\n  vmerge_vi(dst.fp().toV(), -1, dst.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_ge_s", "content": "void LiftoffAssembler::emit_i64x2_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmge(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_ge_s", "content": "void LiftoffAssembler::emit_i64x2_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E64, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_gt_s", "content": "void LiftoffAssembler::emit_i64x2_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmgt(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_gt_s", "content": "void LiftoffAssembler::emit_i64x2_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E64, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_ne", "content": "void LiftoffAssembler::emit_i64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n  Mvn(dst.fp().V2D(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_ne", "content": "void LiftoffAssembler::emit_i64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvNe(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E64, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_eq", "content": "void LiftoffAssembler::emit_i64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_eq", "content": "void LiftoffAssembler::emit_i64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvEq(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E64, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_ge_u", "content": "void LiftoffAssembler::emit_i32x4_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhs(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_ge_u", "content": "void LiftoffAssembler::emit_i32x4_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_ge_s", "content": "void LiftoffAssembler::emit_i32x4_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmge(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_ge_s", "content": "void LiftoffAssembler::emit_i32x4_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_gt_u", "content": "void LiftoffAssembler::emit_i32x4_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhi(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_gt_u", "content": "void LiftoffAssembler::emit_i32x4_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_gt_s", "content": "void LiftoffAssembler::emit_i32x4_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmgt(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_gt_s", "content": "void LiftoffAssembler::emit_i32x4_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_ne", "content": "void LiftoffAssembler::emit_i32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n  Mvn(dst.fp().V4S(), dst.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_ne", "content": "void LiftoffAssembler::emit_i32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvNe(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_eq", "content": "void LiftoffAssembler::emit_i32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_eq", "content": "void LiftoffAssembler::emit_i32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvEq(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E32, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_ge_u", "content": "void LiftoffAssembler::emit_i16x8_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhs(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_ge_u", "content": "void LiftoffAssembler::emit_i16x8_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_ge_s", "content": "void LiftoffAssembler::emit_i16x8_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmge(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_ge_s", "content": "void LiftoffAssembler::emit_i16x8_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_gt_u", "content": "void LiftoffAssembler::emit_i16x8_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhi(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_gt_u", "content": "void LiftoffAssembler::emit_i16x8_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_gt_s", "content": "void LiftoffAssembler::emit_i16x8_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmgt(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_gt_s", "content": "void LiftoffAssembler::emit_i16x8_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_ne", "content": "void LiftoffAssembler::emit_i16x8_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n  Mvn(dst.fp().V8H(), dst.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_ne", "content": "void LiftoffAssembler::emit_i16x8_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvNe(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_eq", "content": "void LiftoffAssembler::emit_i16x8_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_eq", "content": "void LiftoffAssembler::emit_i16x8_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvEq(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E16, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_ge_u", "content": "void LiftoffAssembler::emit_i8x16_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhs(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_ge_u", "content": "void LiftoffAssembler::emit_i8x16_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ge_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_ge_s", "content": "void LiftoffAssembler::emit_i8x16_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmge(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_ge_s", "content": "void LiftoffAssembler::emit_i8x16_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGeS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ge_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_gt_u", "content": "void LiftoffAssembler::emit_i8x16_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmhi(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_gt_u", "content": "void LiftoffAssembler::emit_i8x16_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtU(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_gt_u(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_gt_s", "content": "void LiftoffAssembler::emit_i8x16_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  Cmgt(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_gt_s", "content": "void LiftoffAssembler::emit_i8x16_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  WasmRvvGtS(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_gt_s(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_ne", "content": "void LiftoffAssembler::emit_i8x16_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n  Mvn(dst.fp().V16B(), dst.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_ne", "content": "void LiftoffAssembler::emit_i8x16_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvNe(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_ne(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_eq", "content": "void LiftoffAssembler::emit_i8x16_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  Cmeq(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_eq", "content": "void LiftoffAssembler::emit_i8x16_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) {\n  WasmRvvEq(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV(), E8, m1);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_eq(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_max_u", "content": "void LiftoffAssembler::emit_i8x16_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umax(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_max_u", "content": "void LiftoffAssembler::emit_i8x16_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vmaxu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_max_s", "content": "void LiftoffAssembler::emit_i8x16_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smax(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_max_s", "content": "void LiftoffAssembler::emit_i8x16_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vmax_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_min_u", "content": "void LiftoffAssembler::emit_i8x16_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umin(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_min_u", "content": "void LiftoffAssembler::emit_i8x16_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vminu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_min_s", "content": "void LiftoffAssembler::emit_i8x16_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smin(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_min_s", "content": "void LiftoffAssembler::emit_i8x16_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vmin_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_add_sat_u", "content": "void LiftoffAssembler::emit_i8x16_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Uqadd(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_add_sat_u", "content": "void LiftoffAssembler::emit_i8x16_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vsaddu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_sub_sat_u", "content": "void LiftoffAssembler::emit_i8x16_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Uqsub(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_sub_sat_u", "content": "void LiftoffAssembler::emit_i8x16_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vssubu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_sub_sat_s", "content": "void LiftoffAssembler::emit_i8x16_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Sqsub(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_sub_sat_s", "content": "void LiftoffAssembler::emit_i8x16_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vssub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_sub", "content": "void LiftoffAssembler::emit_i8x16_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Sub(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_sub", "content": "void LiftoffAssembler::emit_i8x16_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_add_sat_s", "content": "void LiftoffAssembler::emit_i8x16_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Sqadd(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_add_sat_s", "content": "void LiftoffAssembler::emit_i8x16_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vsadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_add", "content": "void LiftoffAssembler::emit_i8x16_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Add(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_add", "content": "void LiftoffAssembler::emit_i8x16_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shri_u", "content": "void LiftoffAssembler::emit_i8x16_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat16B,\n                                       liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V16B(), lhs.fp().V16B(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shri_u", "content": "void LiftoffAssembler::emit_i8x16_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vsrl_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shr_u", "content": "void LiftoffAssembler::emit_i8x16_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V16B(), lhs.fp().V16B(), rhs.gp(), kFormat16B);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shr_u", "content": "void LiftoffAssembler::emit_i8x16_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  andi(rhs.gp(), rhs.gp(), 8 - 1);\n  vsrl_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shri_s", "content": "void LiftoffAssembler::emit_i8x16_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat16B, liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V16B(), lhs.fp().V16B(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shri_s", "content": "void LiftoffAssembler::emit_i8x16_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E8, m1);\n  vsra_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shr_s", "content": "void LiftoffAssembler::emit_i8x16_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V16B(), lhs.fp().V16B(), rhs.gp(), kFormat16B);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shr_s", "content": "void LiftoffAssembler::emit_i8x16_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  andi(rhs.gp(), rhs.gp(), 8 - 1);\n  vsra_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shli", "content": "void LiftoffAssembler::emit_i8x16_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  Shl(dst.fp().V16B(), lhs.fp().V16B(), rhs & 7);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shli", "content": "void LiftoffAssembler::emit_i8x16_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  DCHECK(is_uint5(rhs));\n  VU.set(kScratchReg, E8, m1);\n  vsll_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_shl", "content": "void LiftoffAssembler::emit_i8x16_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kLeft>(\n      this, dst.fp().V16B(), lhs.fp().V16B(), rhs.gp(), kFormat16B);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_shl", "content": "void LiftoffAssembler::emit_i8x16_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  andi(rhs.gp(), rhs.gp(), 8 - 1);\n  vsll_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_bitmask", "content": "void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VRegister temp = NoVReg;\n\n  if (CpuFeatures::IsSupported(PMULL1Q)) {\n    temp = GetUnusedRegister(kFpReg, LiftoffRegList{src}).fp();\n  }\n\n  I8x16BitMask(dst.gp(), src.fp(), temp);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_bitmask", "content": "void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmslt_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128RegZero);\n  VU.set(kScratchReg, E32, m1);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_alltrue", "content": "void LiftoffAssembler::emit_i8x16_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  liftoff::EmitAllTrue(this, dst, src, kFormat16B);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_alltrue", "content": "void LiftoffAssembler::emit_i8x16_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  Label notalltrue;\n  vmv_vi(kSimd128ScratchReg, -1);\n  vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  beqz(dst.gp(), &notalltrue);\n  li(dst.gp(), 1);\n  bind(&notalltrue);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_v128_anytrue", "content": "void LiftoffAssembler::emit_v128_anytrue(LiftoffRegister dst,\n                                         LiftoffRegister src) {\n  liftoff::EmitAnyTrue(this, dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_v128_anytrue(LiftoffRegister dst,\n                                         LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_v128_anytrue", "content": "void LiftoffAssembler::emit_v128_anytrue(LiftoffRegister dst,\n                                         LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  Label t;\n  vmv_sx(kSimd128ScratchReg, zero_reg);\n  vredmaxu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  beq(dst.gp(), zero_reg, &t);\n  li(dst.gp(), 1);\n  bind(&t);\n}", "name_and_para": "void LiftoffAssembler::emit_v128_anytrue(LiftoffRegister dst,\n                                         LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_neg", "content": "void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Neg(dst.fp().V16B(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_neg", "content": "void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_replace_lane", "content": "void LiftoffAssembler::emit_i8x16_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V16B(), src1.fp().V16B());\n  }\n  Mov(dst.fp().V16B(), imm_lane_idx, src2.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i8x16_replace_lane", "content": "void LiftoffAssembler::emit_i8x16_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E64, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  VU.set(kScratchReg, E8, m1);\n  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i8x16_extract_lane_s", "content": "void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  Smov(dst.gp().W(), lhs.fp().V16B(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i8x16_extract_lane_s", "content": "void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i8x16_extract_lane_u", "content": "void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  Umov(dst.gp().W(), lhs.fp().V16B(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i8x16_extract_lane_u", "content": "void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E8, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  slli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 8);\n  srli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i8x16_splat", "content": "void LiftoffAssembler::emit_i8x16_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V16B(), src.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_splat", "content": "void LiftoffAssembler::emit_i8x16_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E8, m1);\n  vmv_vx(dst.fp().toV(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_popcnt", "content": "void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,\n                                         LiftoffRegister src) {\n  Cnt(dst.fp().V16B(), src.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,\n                                         LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i8x16_popcnt", "content": "void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,\n                                         LiftoffRegister src) {\n  VRegister src_v = src.fp().toV();\n  VRegister dst_v = dst.fp().toV();\n  Label t, done;\n  VU.set(kScratchReg, E8, m1);\n  vmv_vv(kSimd128ScratchReg, src_v);\n  li(kScratchReg, 0xFF);\n  vxor_vx(kSimd128ScratchReg, kSimd128ScratchReg, kScratchReg);\n  vmv_vi(dst_v, 8);\n  vmv_vi(kSimd128RegZero, 0);\n  bind(&t);\n  vmsne_vi(v0, kSimd128ScratchReg, 0);\n  VU.set(kScratchReg, E16, m1);\n  vmv_xs(kScratchReg, v0);\n  beqz(kScratchReg, &done);\n  VU.set(kScratchReg, E8, m1);\n  vadd_vi(dst_v, dst_v, -1, MaskType::Mask);\n  vadd_vi(kSimd128ScratchReg2, kSimd128ScratchReg, -1, MaskType::Mask);\n  vand_vv(kSimd128ScratchReg, kSimd128ScratchReg2, kSimd128ScratchReg,\n          MaskType::Mask);\n  Branch(&t);\n  bind(&done);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,\n                                         LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_shuffle", "content": "void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs,\n                                          const uint8_t shuffle[16],\n                                          bool is_swizzle) {\n  VRegister src1 = lhs.fp();\n  VRegister src2 = rhs.fp();\n  VRegister temp = dst.fp();\n  if (dst == lhs || dst == rhs) {\n    // dst overlaps with lhs or rhs, so we need a temporary.\n    temp = GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp();\n  }\n\n  UseScratchRegisterScope scope(this);\n\n  if (src1 != src2 && !AreConsecutive(src1, src2)) {\n    // Tbl needs consecutive registers, which our scratch registers are.\n    src1 = scope.AcquireV(kFormat16B);\n    src2 = scope.AcquireV(kFormat16B);\n    DCHECK(AreConsecutive(src1, src2));\n    Mov(src1.Q(), lhs.fp().Q());\n    Mov(src2.Q(), rhs.fp().Q());\n  }\n\n  int64_t imms[2] = {0, 0};\n  for (int i = 7; i >= 0; i--) {\n    imms[0] = (imms[0] << 8) | (shuffle[i]);\n    imms[1] = (imms[1] << 8) | (shuffle[i + 8]);\n  }\n  DCHECK_EQ(0, (imms[0] | imms[1]) &\n                   (lhs == rhs ? 0xF0F0F0F0F0F0F0F0 : 0xE0E0E0E0E0E0E0E0));\n\n  Movi(temp.V16B(), imms[1], imms[0]);\n\n  if (src1 == src2) {\n    Tbl(dst.fp().V16B(), src1.V16B(), temp.V16B());\n  } else {\n    Tbl(dst.fp().V16B(), src1.V16B(), src2.V16B(), temp.V16B());\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs,\n                                          const uint8_t shuffle[16],\n                                          bool is_swizzle) "}, {"name": "LiftoffAssembler::emit_i8x16_shuffle", "content": "void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs,\n                                          const uint8_t shuffle[16],\n                                          bool is_swizzle) {\n  VRegister dst_v = dst.fp().toV();\n  VRegister lhs_v = lhs.fp().toV();\n  VRegister rhs_v = rhs.fp().toV();\n\n  WasmRvvS128const(kSimd128ScratchReg2, shuffle);\n\n  VU.set(kScratchReg, E8, m1);\n  VRegister temp =\n      GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();\n  if (dst_v == lhs_v) {\n    vmv_vv(temp, lhs_v);\n    lhs_v = temp;\n  } else if (dst_v == rhs_v) {\n    vmv_vv(temp, rhs_v);\n    rhs_v = temp;\n  }\n  vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg2);\n  vadd_vi(kSimd128ScratchReg2, kSimd128ScratchReg2,\n          -16);  // The indices in range [16, 31] select the i - 16-th element\n                 // of rhs\n  vrgather_vv(kSimd128ScratchReg, rhs_v, kSimd128ScratchReg2);\n  vor_vv(dst_v, dst_v, kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs,\n                                          const uint8_t shuffle[16],\n                                          bool is_swizzle) "}], [{"name": "LiftoffAssembler::emit_i16x8_max_u", "content": "void LiftoffAssembler::emit_i16x8_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umax(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_max_u", "content": "void LiftoffAssembler::emit_i16x8_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmaxu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_max_s", "content": "void LiftoffAssembler::emit_i16x8_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smax(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_max_s", "content": "void LiftoffAssembler::emit_i16x8_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmax_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_min_u", "content": "void LiftoffAssembler::emit_i16x8_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umin(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_min_u", "content": "void LiftoffAssembler::emit_i16x8_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vminu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_min_s", "content": "void LiftoffAssembler::emit_i16x8_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smin(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_min_s", "content": "void LiftoffAssembler::emit_i16x8_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmin_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_add_sat_u", "content": "void LiftoffAssembler::emit_i16x8_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Uqadd(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_add_sat_u", "content": "void LiftoffAssembler::emit_i16x8_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vsaddu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_mul", "content": "void LiftoffAssembler::emit_i16x8_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Mul(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_mul", "content": "void LiftoffAssembler::emit_i16x8_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vmul_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_sub_sat_u", "content": "void LiftoffAssembler::emit_i16x8_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Uqsub(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_sub_sat_u", "content": "void LiftoffAssembler::emit_i16x8_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vssubu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub_sat_u(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_sub_sat_s", "content": "void LiftoffAssembler::emit_i16x8_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Sqsub(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_sub_sat_s", "content": "void LiftoffAssembler::emit_i16x8_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vssub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_sub", "content": "void LiftoffAssembler::emit_i16x8_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Sub(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_sub", "content": "void LiftoffAssembler::emit_i16x8_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_add_sat_s", "content": "void LiftoffAssembler::emit_i16x8_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  Sqadd(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_add_sat_s", "content": "void LiftoffAssembler::emit_i16x8_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vsadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add_sat_s(LiftoffRegister dst,\n                                            LiftoffRegister lhs,\n                                            LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_add", "content": "void LiftoffAssembler::emit_i16x8_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Add(dst.fp().V8H(), lhs.fp().V8H(), rhs.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_add", "content": "void LiftoffAssembler::emit_i16x8_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shri_u", "content": "void LiftoffAssembler::emit_i16x8_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat8H,\n                                       liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V8H(), lhs.fp().V8H(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shri_u", "content": "void LiftoffAssembler::emit_i16x8_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  DCHECK(is_uint5(rhs));\n  VU.set(kScratchReg, E16, m1);\n  vsrl_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shr_u", "content": "void LiftoffAssembler::emit_i16x8_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V8H(), lhs.fp().V8H(), rhs.gp(), kFormat8H);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shr_u", "content": "void LiftoffAssembler::emit_i16x8_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  andi(rhs.gp(), rhs.gp(), 16 - 1);\n  vsrl_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shri_s", "content": "void LiftoffAssembler::emit_i16x8_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat8H, liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V8H(), lhs.fp().V8H(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shri_s", "content": "void LiftoffAssembler::emit_i16x8_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vsra_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shr_s", "content": "void LiftoffAssembler::emit_i16x8_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V8H(), lhs.fp().V8H(), rhs.gp(), kFormat8H);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shr_s", "content": "void LiftoffAssembler::emit_i16x8_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  andi(rhs.gp(), rhs.gp(), 16 - 1);\n  vsra_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shli", "content": "void LiftoffAssembler::emit_i16x8_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  Shl(dst.fp().V8H(), lhs.fp().V8H(), rhs & 15);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shli", "content": "void LiftoffAssembler::emit_i16x8_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vsll_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_shl", "content": "void LiftoffAssembler::emit_i16x8_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kLeft>(\n      this, dst.fp().V8H(), lhs.fp().V8H(), rhs.gp(), kFormat8H);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i16x8_shl", "content": "void LiftoffAssembler::emit_i16x8_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  andi(rhs.gp(), rhs.gp(), 16 - 1);\n  vsll_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i16x8_bitmask", "content": "void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  I16x8BitMask(dst.gp(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_bitmask", "content": "void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmslt_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128RegZero);\n  VU.set(kScratchReg, E32, m1);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_alltrue", "content": "void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  liftoff::EmitAllTrue(this, dst, src, kFormat8H);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_alltrue", "content": "void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  Label notalltrue;\n  vmv_vi(kSimd128ScratchReg, -1);\n  vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  beqz(dst.gp(), &notalltrue);\n  li(dst.gp(), 1);\n  bind(&notalltrue);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_neg", "content": "void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Neg(dst.fp().V8H(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_neg", "content": "void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i16x8_replace_lane", "content": "void LiftoffAssembler::emit_i16x8_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V8H(), src1.fp().V8H());\n  }\n  Mov(dst.fp().V8H(), imm_lane_idx, src2.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i16x8_replace_lane", "content": "void LiftoffAssembler::emit_i16x8_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E16, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i16x8_extract_lane_s", "content": "void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  Smov(dst.gp().W(), lhs.fp().V8H(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i16x8_extract_lane_s", "content": "void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extract_lane_s(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i16x8_extract_lane_u", "content": "void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  Umov(dst.gp().W(), lhs.fp().V8H(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i16x8_extract_lane_u", "content": "void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  slli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 16);\n  srli(dst.gp(), dst.gp(), sizeof(void*) * 8 - 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_extract_lane_u(LiftoffRegister dst,\n                                                 LiftoffRegister lhs,\n                                                 uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i16x8_splat", "content": "void LiftoffAssembler::emit_i16x8_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V8H(), src.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i16x8_splat", "content": "void LiftoffAssembler::emit_i16x8_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E16, m1);\n  vmv_vx(dst.fp().toV(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i16x8_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Umull2(dst.fp().V4S(), src1.fp().V8H(), src2.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 4);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 4);\n  VU.set(kScratchReg, E16, mf2);\n  vwmulu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Smull2(dst.fp().V4S(), src1.fp().V8H(), src2.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 4);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 4);\n  VU.set(kScratchReg, E16, mf2);\n  vwmul_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_high_i16x8_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Umull(dst.fp().V4S(), src1.fp().V4H(), src2.fp().V4H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmulu_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E16, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Smull(dst.fp().V4S(), src1.fp().V4H(), src2.fp().V4H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E16, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmul_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E16, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extmul_low_i16x8_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  Uaddlp(dst.fp().V4S(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u", "content": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vi(kSimd128ScratchReg, -1);\n  vmv_vi(kSimd128ScratchReg3, -1);\n  li(kScratchReg, 0x0006000400020000);\n  vmv_sx(kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 0x0007000500030001);\n  vmv_sx(kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg, E16, m1);\n  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);\n  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);\n  VU.set(kScratchReg, E16, mf2);\n  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  Saddlp(dst.fp().V4S(), src.fp().V8H());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vi(kSimd128ScratchReg, -1);\n  vmv_vi(kSimd128ScratchReg3, -1);\n  li(kScratchReg, 0x0006000400020000);\n  vmv_sx(kSimd128ScratchReg, kScratchReg);\n  li(kScratchReg, 0x0007000500030001);\n  vmv_sx(kSimd128ScratchReg3, kScratchReg);\n  VU.set(kScratchReg, E16, m1);\n  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);\n  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);\n  VU.set(kScratchReg, E16, mf2);\n  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,\n                                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_dot_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_dot_i16x8_s(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  UseScratchRegisterScope scope(this);\n  VRegister tmp1 = scope.AcquireV(kFormat4S);\n  VRegister tmp2 = scope.AcquireV(kFormat4S);\n  Smull(tmp1, lhs.fp().V4H(), rhs.fp().V4H());\n  Smull2(tmp2, lhs.fp().V8H(), rhs.fp().V8H());\n  Addp(dst.fp().V4S(), tmp1, tmp2);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_dot_i16x8_s(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_dot_i16x8_s", "content": "void LiftoffAssembler::emit_i32x4_dot_i16x8_s(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  VU.set(kScratchReg, E16, m1);\n  vwmul_vv(kSimd128ScratchReg3, lhs.fp().toV(), rhs.fp().toV());\n  VU.set(kScratchReg, E32, m2);\n  li(kScratchReg, 0b01010101);\n  vmv_sx(v0, kScratchReg);\n  vcompress_vv(kSimd128ScratchReg, kSimd128ScratchReg3, v0);\n\n  li(kScratchReg, 0b10101010);\n  vmv_sx(kSimd128ScratchReg2, kScratchReg);\n  vcompress_vv(v0, kSimd128ScratchReg3, kSimd128ScratchReg2);\n  VU.set(kScratchReg, E32, m1);\n  vadd_vv(dst.fp().toV(), kSimd128ScratchReg, v0);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_dot_i16x8_s(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_max_u", "content": "void LiftoffAssembler::emit_i32x4_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umax(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_max_u", "content": "void LiftoffAssembler::emit_i32x4_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmaxu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_max_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_max_s", "content": "void LiftoffAssembler::emit_i32x4_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smax(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_max_s", "content": "void LiftoffAssembler::emit_i32x4_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmax_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_max_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_min_u", "content": "void LiftoffAssembler::emit_i32x4_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Umin(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_min_u", "content": "void LiftoffAssembler::emit_i32x4_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vminu_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_min_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_min_s", "content": "void LiftoffAssembler::emit_i32x4_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  Smin(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_min_s", "content": "void LiftoffAssembler::emit_i32x4_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmin_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_min_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_mul", "content": "void LiftoffAssembler::emit_i32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Mul(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_mul", "content": "void LiftoffAssembler::emit_i32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vmul_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_sub", "content": "void LiftoffAssembler::emit_i32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Sub(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_sub", "content": "void LiftoffAssembler::emit_i32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_add", "content": "void LiftoffAssembler::emit_i32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Add(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_add", "content": "void LiftoffAssembler::emit_i32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shri_u", "content": "void LiftoffAssembler::emit_i32x4_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat4S,\n                                       liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V4S(), lhs.fp().V4S(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shri_u", "content": "void LiftoffAssembler::emit_i32x4_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E32, m1);\n  if (is_uint5(rhs % 32)) {\n    vsrl_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 32);\n  } else {\n    li(kScratchReg, rhs % 32);\n    vsrl_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shr_u", "content": "void LiftoffAssembler::emit_i32x4_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V4S(), lhs.fp().V4S(), rhs.gp(), kFormat4S);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shr_u", "content": "void LiftoffAssembler::emit_i32x4_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  andi(rhs.gp(), rhs.gp(), 32 - 1);\n  vsrl_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shri_s", "content": "void LiftoffAssembler::emit_i32x4_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat4S, liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V4S(), lhs.fp().V4S(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shri_s", "content": "void LiftoffAssembler::emit_i32x4_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E32, m1);\n  if (is_uint5(rhs % 32)) {\n    vsra_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 32);\n  } else {\n    li(kScratchReg, rhs % 32);\n    vsra_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shr_s", "content": "void LiftoffAssembler::emit_i32x4_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V4S(), lhs.fp().V4S(), rhs.gp(), kFormat4S);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shr_s", "content": "void LiftoffAssembler::emit_i32x4_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  andi(rhs.gp(), rhs.gp(), 32 - 1);\n  vsra_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shli", "content": "void LiftoffAssembler::emit_i32x4_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  Shl(dst.fp().V4S(), lhs.fp().V4S(), rhs & 31);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shli", "content": "void LiftoffAssembler::emit_i32x4_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  if (is_uint5(rhs % 32)) {\n    vsll_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 32);\n  } else {\n    li(kScratchReg, rhs % 32);\n    vsll_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_shl", "content": "void LiftoffAssembler::emit_i32x4_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kLeft>(\n      this, dst.fp().V4S(), lhs.fp().V4S(), rhs.gp(), kFormat4S);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i32x4_shl", "content": "void LiftoffAssembler::emit_i32x4_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  andi(rhs.gp(), rhs.gp(), 32 - 1);\n  vsll_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32x4_bitmask", "content": "void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  I32x4BitMask(dst.gp(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_bitmask", "content": "void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmslt_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128RegZero);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_alltrue", "content": "void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  liftoff::EmitAllTrue(this, dst, src, kFormat4S);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_alltrue", "content": "void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  Label notalltrue;\n  vmv_vi(kSimd128ScratchReg, -1);\n  vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  beqz(dst.gp(), &notalltrue);\n  li(dst.gp(), 1);\n  bind(&notalltrue);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_neg", "content": "void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Neg(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_neg", "content": "void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_replace_lane", "content": "void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V4S(), src1.fp().V4S());\n  }\n  Mov(dst.fp().V4S(), imm_lane_idx, src2.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i32x4_replace_lane", "content": "void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E32, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i32x4_extract_lane", "content": "void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  Mov(dst.gp().W(), lhs.fp().V4S(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i32x4_extract_lane", "content": "void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i32x4_splat", "content": "void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V4S(), src.gp().W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_splat", "content": "void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vmv_vx(dst.fp().toV(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_uconvert_i32x4_high", "content": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Uxtl2(dst.fp().V2D(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_uconvert_i32x4_high", "content": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 2);\n  VU.set(kScratchReg, E64, m1);\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_uconvert_i32x4_low", "content": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Uxtl(dst.fp().V2D(), src.fp().V2S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_uconvert_i32x4_low", "content": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vzext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_uconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_sconvert_i32x4_high", "content": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Sxtl2(dst.fp().V2D(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_sconvert_i32x4_high", "content": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), 2);\n  VU.set(kScratchReg, E64, m1);\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_high(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_sconvert_i32x4_low", "content": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  Sxtl(dst.fp().V2D(), src.fp().V2S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_sconvert_i32x4_low", "content": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vsext_vf2(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sconvert_i32x4_low(LiftoffRegister dst,\n                                                     LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_bitmask", "content": "void LiftoffAssembler::emit_i64x2_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  I64x2BitMask(dst.gp(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_bitmask", "content": "void LiftoffAssembler::emit_i64x2_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vx(kSimd128RegZero, zero_reg);\n  vmv_vx(kSimd128ScratchReg, zero_reg);\n  vmslt_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128RegZero);\n  VU.set(kScratchReg, E32, m1);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_bitmask(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u", "content": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Umull2(dst.fp().V2D(), src1.fp().V4S(), src2.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u", "content": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 2);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 2);\n  VU.set(kScratchReg, E32, mf2);\n  vwmulu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s", "content": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  Smull2(dst.fp().V2D(), src1.fp().V4S(), src2.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s", "content": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, src1.fp().toV(), 2);\n  vslidedown_vi(kSimd128ScratchReg2, src2.fp().toV(), 2);\n  VU.set(kScratchReg, E32, mf2);\n  vwmul_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_high_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src1,\n                                                      LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u", "content": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Umull(dst.fp().V2D(), src1.fp().V2S(), src2.fp().V2S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u", "content": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E32, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmulu_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E64, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_u(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s", "content": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  Smull(dst.fp().V2D(), src1.fp().V2S(), src2.fp().V2S());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}, {"name": "LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s", "content": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) {\n  VU.set(kScratchReg, E32, mf2);\n  VRegister dst_v = dst.fp().toV();\n  if (dst == src1 || dst == src2) {\n    dst_v = kSimd128ScratchReg3;\n  }\n  vwmul_vv(dst_v, src2.fp().toV(), src1.fp().toV());\n  if (dst == src1 || dst == src2) {\n    VU.set(kScratchReg, E64, m1);\n    vmv_vv(dst.fp().toV(), dst_v);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,\n                                                     LiftoffRegister src1,\n                                                     LiftoffRegister src2) "}], [{"name": "LiftoffAssembler::emit_i64x2_mul", "content": "void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  VRegister tmp1 = temps.AcquireV(kFormat2D);\n  VRegister tmp2 = temps.AcquireV(kFormat2D);\n\n  // Algorithm copied from code-generator-arm64.cc with minor modifications:\n  // - 2 (max number of scratch registers in Liftoff) temporaries instead of 3\n  // - 1 more Umull instruction to calculate | cg | ae |,\n  // - so, we can no longer use Umlal in the last step, and use Add instead.\n  // Refer to comments there for details.\n  Xtn(tmp1.V2S(), lhs.fp().V2D());\n  Xtn(tmp2.V2S(), rhs.fp().V2D());\n  Umull(tmp1.V2D(), tmp1.V2S(), tmp2.V2S());\n  Rev64(tmp2.V4S(), rhs.fp().V4S());\n  Mul(tmp2.V4S(), tmp2.V4S(), lhs.fp().V4S());\n  Addp(tmp2.V4S(), tmp2.V4S(), tmp2.V4S());\n  Shll(dst.fp().V2D(), tmp2.V2S(), 32);\n  Add(dst.fp().V2D(), dst.fp().V2D(), tmp1.V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_mul", "content": "void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vmul_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_sub", "content": "void LiftoffAssembler::emit_i64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Sub(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_sub", "content": "void LiftoffAssembler::emit_i64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_add", "content": "void LiftoffAssembler::emit_i64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Add(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_add", "content": "void LiftoffAssembler::emit_i64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shri_u", "content": "void LiftoffAssembler::emit_i64x2_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat2D,\n                                       liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V2D(), lhs.fp().V2D(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shri_u", "content": "void LiftoffAssembler::emit_i64x2_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E64, m1);\n  if (is_uint5(rhs % 64)) {\n    vsrl_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 64);\n  } else {\n    li(kScratchReg, rhs % 64);\n    vsrl_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shri_u(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shr_u", "content": "void LiftoffAssembler::emit_i64x2_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kUnsigned>(\n      this, dst.fp().V2D(), lhs.fp().V2D(), rhs.gp(), kFormat2D);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shr_u", "content": "void LiftoffAssembler::emit_i64x2_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  andi(rhs.gp(), rhs.gp(), 64 - 1);\n  vsrl_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shr_u(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shri_s", "content": "void LiftoffAssembler::emit_i64x2_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  liftoff::EmitSimdShiftRightImmediate<kFormat2D, liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V2D(), lhs.fp().V2D(), rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shri_s", "content": "void LiftoffAssembler::emit_i64x2_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) {\n  VU.set(kScratchReg, E64, m1);\n  if (is_uint5(rhs % 64)) {\n    vsra_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 64);\n  } else {\n    li(kScratchReg, rhs % 64);\n    vsra_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shri_s(LiftoffRegister dst,\n                                         LiftoffRegister lhs, int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shr_s", "content": "void LiftoffAssembler::emit_i64x2_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kRight,\n                         liftoff::ShiftSign::kSigned>(\n      this, dst.fp().V2D(), lhs.fp().V2D(), rhs.gp(), kFormat2D);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shr_s", "content": "void LiftoffAssembler::emit_i64x2_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  andi(rhs.gp(), rhs.gp(), 64 - 1);\n  vsra_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shr_s(LiftoffRegister dst,\n                                        LiftoffRegister lhs,\n                                        LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shli", "content": "void LiftoffAssembler::emit_i64x2_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  Shl(dst.fp().V2D(), lhs.fp().V2D(), rhs & 63);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shli", "content": "void LiftoffAssembler::emit_i64x2_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) {\n  VU.set(kScratchReg, E64, m1);\n  if (is_uint5(rhs % 64)) {\n    vsll_vi(dst.fp().toV(), lhs.fp().toV(), rhs % 64);\n  } else {\n    li(kScratchReg, rhs % 64);\n    vsll_vx(dst.fp().toV(), lhs.fp().toV(), kScratchReg);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shli(LiftoffRegister dst, LiftoffRegister lhs,\n                                       int32_t rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_shl", "content": "void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  liftoff::EmitSimdShift<liftoff::ShiftDirection::kLeft>(\n      this, dst.fp().V2D(), lhs.fp().V2D(), rhs.gp(), kFormat2D);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64x2_shl", "content": "void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  andi(rhs.gp(), rhs.gp(), 64 - 1);\n  vsll_vx(dst.fp().toV(), lhs.fp().toV(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_shl(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64x2_alltrue", "content": "void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  I64x2AllTrue(dst.gp(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_alltrue", "content": "void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  Label notalltrue;\n  vmv_vi(kSimd128ScratchReg, -1);\n  vredminu_vs(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n  beqz(dst.gp(), &notalltrue);\n  li(dst.gp(), 1);\n  bind(&notalltrue);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_alltrue(LiftoffRegister dst,\n                                          LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_neg", "content": "void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Neg(dst.fp().V2D(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_neg", "content": "void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64x2_replace_lane", "content": "void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V2D(), src1.fp().V2D());\n  }\n  Mov(dst.fp().V2D(), imm_lane_idx, src2.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i64x2_replace_lane", "content": "void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E64, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  vmerge_vx(dst.fp().toV(), src2.gp(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i64x2_extract_lane", "content": "void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  Mov(dst.gp().X(), lhs.fp().V2D(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_i64x2_extract_lane", "content": "void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E64, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vmv_xs(dst.gp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_i64x2_splat", "content": "void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V2D(), src.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64x2_splat", "content": "void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vmv_vx(dst.fp().toV(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_pmax", "content": "void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n\n  VRegister tmp = dst.fp();\n  if (dst == lhs || dst == rhs) {\n    tmp = temps.AcquireV(kFormat4S);\n  }\n\n  Fcmgt(tmp.V4S(), rhs.fp().V4S(), lhs.fp().V4S());\n  Bsl(tmp.V16B(), rhs.fp().V16B(), lhs.fp().V16B());\n\n  if (dst == lhs || dst == rhs) {\n    Mov(dst.fp().V4S(), tmp);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_pmax", "content": "void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  // a < b ? b : a\n  vmflt_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmerge_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_pmin", "content": "void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n\n  VRegister tmp = dst.fp();\n  if (dst == lhs || dst == rhs) {\n    tmp = temps.AcquireV(kFormat4S);\n  }\n\n  Fcmgt(tmp.V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n  Bsl(tmp.V16B(), rhs.fp().V16B(), lhs.fp().V16B());\n\n  if (dst == lhs || dst == rhs) {\n    Mov(dst.fp().V4S(), tmp);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_pmin", "content": "void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  // b < a ? b : a\n  vmflt_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmerge_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_relaxed_max", "content": "void LiftoffAssembler::emit_f32x4_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  Fmax(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_relaxed_max", "content": "void LiftoffAssembler::emit_f32x4_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vfmax_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_relaxed_min", "content": "void LiftoffAssembler::emit_f32x4_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  Fmin(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_relaxed_min", "content": "void LiftoffAssembler::emit_f32x4_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vfmin_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_max", "content": "void LiftoffAssembler::emit_f32x4_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmax(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_max", "content": "void LiftoffAssembler::emit_f32x4_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  const int32_t kNaN = 0x7FC00000;\n  VU.set(kScratchReg, E32, m1);\n  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());\n  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());\n  vand_vv(v0, v0, kSimd128ScratchReg);\n  li(kScratchReg, kNaN);\n  vmv_vx(kSimd128ScratchReg, kScratchReg);\n  vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_min", "content": "void LiftoffAssembler::emit_f32x4_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmin(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_min", "content": "void LiftoffAssembler::emit_f32x4_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  const int32_t kNaN = 0x7FC00000;\n  VU.set(kScratchReg, E32, m1);\n  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());\n  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());\n  vand_vv(v0, v0, kSimd128ScratchReg);\n  li(kScratchReg, kNaN);\n  vmv_vx(kSimd128ScratchReg, kScratchReg);\n  vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_div", "content": "void LiftoffAssembler::emit_f32x4_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fdiv(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_div", "content": "void LiftoffAssembler::emit_f32x4_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vfdiv_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_mul", "content": "void LiftoffAssembler::emit_f32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmul(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_mul", "content": "void LiftoffAssembler::emit_f32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vfmul_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_sub", "content": "void LiftoffAssembler::emit_f32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fsub(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_sub", "content": "void LiftoffAssembler::emit_f32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vfsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_add", "content": "void LiftoffAssembler::emit_f32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fadd(dst.fp().V4S(), lhs.fp().V4S(), rhs.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32x4_add", "content": "void LiftoffAssembler::emit_f32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E32, m1);\n  vfadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32x4_nearest_int", "content": "bool LiftoffAssembler::emit_f32x4_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  Frintn(dst.fp().V4S(), src.fp().V4S());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_nearest_int", "content": "bool LiftoffAssembler::emit_f32x4_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  Round_f(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_trunc", "content": "bool LiftoffAssembler::emit_f32x4_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Frintz(dst.fp().V4S(), src.fp().V4S());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_trunc", "content": "bool LiftoffAssembler::emit_f32x4_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Trunc_f(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_floor", "content": "bool LiftoffAssembler::emit_f32x4_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Frintm(dst.fp().V4S(), src.fp().V4S());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_floor", "content": "bool LiftoffAssembler::emit_f32x4_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Floor_f(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_ceil", "content": "bool LiftoffAssembler::emit_f32x4_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Frintp(dst.fp().V4S(), src.fp().V4S());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_ceil", "content": "bool LiftoffAssembler::emit_f32x4_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Ceil_f(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f32x4_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_sqrt", "content": "void LiftoffAssembler::emit_f32x4_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Fsqrt(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_sqrt", "content": "void LiftoffAssembler::emit_f32x4_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vfsqrt_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_neg", "content": "void LiftoffAssembler::emit_f32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Fneg(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_neg", "content": "void LiftoffAssembler::emit_f32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vfneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_abs", "content": "void LiftoffAssembler::emit_f32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Fabs(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_abs", "content": "void LiftoffAssembler::emit_f32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vfabs_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f32x4_replace_lane", "content": "void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V4S(), src1.fp().V4S());\n  }\n  Mov(dst.fp().V4S(), imm_lane_idx, src2.fp().V4S(), 0);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_f32x4_replace_lane", "content": "void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E32, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  fmv_x_w(kScratchReg, src2.fp());\n  vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_f32x4_extract_lane", "content": "void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  Mov(dst.fp().S(), lhs.fp().V4S(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_f32x4_extract_lane", "content": "void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E32, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vfmv_fs(dst.fp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_f32x4_splat", "content": "void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V4S(), src.fp().S(), 0);\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f32x4_splat", "content": "void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  vfmv_vf(dst.fp().toV(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_promote_low_f32x4", "content": "void LiftoffAssembler::emit_f64x2_promote_low_f32x4(LiftoffRegister dst,\n                                                    LiftoffRegister src) {\n  Fcvtl(dst.fp().V2D(), src.fp().V2S());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_promote_low_f32x4(LiftoffRegister dst,\n                                                    LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_promote_low_f32x4", "content": "void LiftoffAssembler::emit_f64x2_promote_low_f32x4(LiftoffRegister dst,\n                                                    LiftoffRegister src) {\n  VU.set(kScratchReg, E32, mf2);\n  if (dst.fp().toV() != src.fp().toV()) {\n    vfwcvt_f_f_v(dst.fp().toV(), src.fp().toV());\n  } else {\n    vfwcvt_f_f_v(kSimd128ScratchReg3, src.fp().toV());\n    VU.set(kScratchReg, E64, m1);\n    vmv_vv(dst.fp().toV(), kSimd128ScratchReg3);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_promote_low_f32x4(LiftoffRegister dst,\n                                                    LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_convert_low_i32x4_u", "content": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Uxtl(dst.fp().V2D(), src.fp().V2S());\n  Ucvtf(dst.fp().V2D(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_convert_low_i32x4_u", "content": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, mf2);\n  if (dst.fp().toV() != src.fp().toV()) {\n    vfwcvt_f_xu_v(dst.fp().toV(), src.fp().toV());\n  } else {\n    vfwcvt_f_xu_v(kSimd128ScratchReg3, src.fp().toV());\n    VU.set(kScratchReg, E64, m1);\n    vmv_vv(dst.fp().toV(), kSimd128ScratchReg3);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_u(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_convert_low_i32x4_s", "content": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  Sxtl(dst.fp().V2D(), src.fp().V2S());\n  Scvtf(dst.fp().V2D(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_convert_low_i32x4_s", "content": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E32, mf2);\n  if (dst.fp().toV() != src.fp().toV()) {\n    vfwcvt_f_x_v(dst.fp().toV(), src.fp().toV());\n  } else {\n    vfwcvt_f_x_v(kSimd128ScratchReg3, src.fp().toV());\n    VU.set(kScratchReg, E64, m1);\n    vmv_vv(dst.fp().toV(), kSimd128ScratchReg3);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_convert_low_i32x4_s(LiftoffRegister dst,\n                                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_relaxed_max", "content": "void LiftoffAssembler::emit_f64x2_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  Fmax(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_relaxed_max", "content": "void LiftoffAssembler::emit_f64x2_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfmax_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_relaxed_max(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_relaxed_min", "content": "void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  Fmin(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_relaxed_min", "content": "void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfmin_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,\n                                              LiftoffRegister lhs,\n                                              LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_pmax", "content": "void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n\n  VRegister tmp = dst.fp();\n  if (dst == lhs || dst == rhs) {\n    tmp = temps.AcquireV(kFormat2D);\n  }\n\n  Fcmgt(tmp.V2D(), rhs.fp().V2D(), lhs.fp().V2D());\n  Bsl(tmp.V16B(), rhs.fp().V16B(), lhs.fp().V16B());\n\n  if (dst == lhs || dst == rhs) {\n    Mov(dst.fp().V2D(), tmp);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_pmax", "content": "void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  // a < b ? b : a\n  vmflt_vv(v0, lhs.fp().toV(), rhs.fp().toV());\n  vmerge_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_pmax(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_pmin", "content": "void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  UseScratchRegisterScope temps(this);\n\n  VRegister tmp = dst.fp();\n  if (dst == lhs || dst == rhs) {\n    tmp = temps.AcquireV(kFormat2D);\n  }\n\n  Fcmgt(tmp.V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n  Bsl(tmp.V16B(), rhs.fp().V16B(), lhs.fp().V16B());\n\n  if (dst == lhs || dst == rhs) {\n    Mov(dst.fp().V2D(), tmp);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_pmin", "content": "void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  // b < a ? b : a\n  vmflt_vv(v0, rhs.fp().toV(), lhs.fp().toV());\n  vmerge_vv(dst.fp().toV(), rhs.fp().toV(), lhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_pmin(LiftoffRegister dst, LiftoffRegister lhs,\n                                       LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_max", "content": "void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmax(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_max", "content": "void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  const int64_t kNaN = 0x7ff8000000000000L;\n  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());\n  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());\n  vand_vv(v0, v0, kSimd128ScratchReg);\n  li(kScratchReg, kNaN);\n  vmv_vx(kSimd128ScratchReg, kScratchReg);\n  vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_min", "content": "void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmin(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_min", "content": "void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  const int64_t kNaN = 0x7ff8000000000000L;\n  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());\n  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());\n  vand_vv(v0, v0, kSimd128ScratchReg);\n  li(kScratchReg, kNaN);\n  vmv_vx(kSimd128ScratchReg, kScratchReg);\n  vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_div", "content": "void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fdiv(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_div", "content": "void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfdiv_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_mul", "content": "void LiftoffAssembler::emit_f64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fmul(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_mul", "content": "void LiftoffAssembler::emit_f64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfmul_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_sub", "content": "void LiftoffAssembler::emit_f64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fsub(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_sub", "content": "void LiftoffAssembler::emit_f64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfsub_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_add", "content": "void LiftoffAssembler::emit_f64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  Fadd(dst.fp().V2D(), lhs.fp().V2D(), rhs.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64x2_add", "content": "void LiftoffAssembler::emit_f64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) {\n  VU.set(kScratchReg, E64, m1);\n  vfadd_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                      LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64x2_nearest_int", "content": "bool LiftoffAssembler::emit_f64x2_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  Frintn(dst.fp().V2D(), src.fp().V2D());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_nearest_int", "content": "bool LiftoffAssembler::emit_f64x2_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  Round_d(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_nearest_int(LiftoffRegister dst,\n                                              LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_trunc", "content": "bool LiftoffAssembler::emit_f64x2_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Frintz(dst.fp().V2D(), src.fp().V2D());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_trunc", "content": "bool LiftoffAssembler::emit_f64x2_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Trunc_d(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_trunc(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_floor", "content": "bool LiftoffAssembler::emit_f64x2_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Frintm(dst.fp().V2D(), src.fp().V2D());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_floor", "content": "bool LiftoffAssembler::emit_f64x2_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Floor_d(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_floor(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_ceil", "content": "bool LiftoffAssembler::emit_f64x2_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Frintp(dst.fp().V2D(), src.fp().V2D());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_ceil", "content": "bool LiftoffAssembler::emit_f64x2_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Ceil_d(dst.fp().toV(), src.fp().toV(), kScratchReg, kSimd128ScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_f64x2_ceil(LiftoffRegister dst,\n                                       LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_sqrt", "content": "void LiftoffAssembler::emit_f64x2_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  Fsqrt(dst.fp().V2D(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_sqrt", "content": "void LiftoffAssembler::emit_f64x2_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vfsqrt_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_sqrt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_neg", "content": "void LiftoffAssembler::emit_f64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Fneg(dst.fp().V2D(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_neg", "content": "void LiftoffAssembler::emit_f64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vfneg_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_neg(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_abs", "content": "void LiftoffAssembler::emit_f64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  Fabs(dst.fp().V2D(), src.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_abs", "content": "void LiftoffAssembler::emit_f64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vfabs_vv(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_abs(LiftoffRegister dst,\n                                      LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_f64x2_replace_lane", "content": "void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  if (dst != src1) {\n    Mov(dst.fp().V2D(), src1.fp().V2D());\n  }\n  Mov(dst.fp().V2D(), imm_lane_idx, src2.fp().V2D(), 0);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_f64x2_replace_lane", "content": "void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E64, m1);\n  li(kScratchReg, 0x1 << imm_lane_idx);\n  vmv_sx(v0, kScratchReg);\n  vfmerge_vf(dst.fp().toV(), src2.fp(), src1.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,\n                                               LiftoffRegister src1,\n                                               LiftoffRegister src2,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_f64x2_extract_lane", "content": "void LiftoffAssembler::emit_f64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  Mov(dst.fp().D(), lhs.fp().V2D(), imm_lane_idx);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}, {"name": "LiftoffAssembler::emit_f64x2_extract_lane", "content": "void LiftoffAssembler::emit_f64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) {\n  VU.set(kScratchReg, E64, m1);\n  vslidedown_vi(kSimd128ScratchReg, lhs.fp().toV(), imm_lane_idx);\n  vfmv_fs(dst.fp(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_extract_lane(LiftoffRegister dst,\n                                               LiftoffRegister lhs,\n                                               uint8_t imm_lane_idx) "}], [{"name": "LiftoffAssembler::emit_f64x2_splat", "content": "void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  Dup(dst.fp().V2D(), src.fp().D(), 0);\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_f64x2_splat", "content": "void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) {\n  VU.set(kScratchReg, E64, m1);\n  vfmv_vf(dst.fp().toV(), src.fp());\n}", "name_and_para": "void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,\n                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_s128_relaxed_laneselect", "content": "void LiftoffAssembler::emit_s128_relaxed_laneselect(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2,\n                                                    LiftoffRegister mask,\n                                                    int lane_width) {\n  // ARM64 uses bytewise selection for all lane widths.\n  emit_s128_select(dst, src1, src2, mask);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_relaxed_laneselect(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2,\n                                                    LiftoffRegister mask,\n                                                    int lane_width) "}, {"name": "LiftoffAssembler::emit_s128_relaxed_laneselect", "content": "void LiftoffAssembler::emit_s128_relaxed_laneselect(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2,\n                                                    LiftoffRegister mask,\n                                                    int lane_width) {\n  // RISC-V uses bytewise selection for all lane widths.\n  emit_s128_select(dst, src1, src2, mask);\n}", "name_and_para": "void LiftoffAssembler::emit_s128_relaxed_laneselect(LiftoffRegister dst,\n                                                    LiftoffRegister src1,\n                                                    LiftoffRegister src2,\n                                                    LiftoffRegister mask,\n                                                    int lane_width) "}], [{"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero(\n    LiftoffRegister dst, LiftoffRegister src) {\n  Fcvtzu(dst.fp().V2D(), src.fp().V2D());\n  Uqxtn(dst.fp().V2S(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero(\n    LiftoffRegister dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero(\n    LiftoffRegister dst, LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vfncvt_xu_f_w(kSimd128ScratchReg, kSimd128ScratchReg);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_u_zero(\n    LiftoffRegister dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero(\n    LiftoffRegister dst, LiftoffRegister src) {\n  Fcvtzs(dst.fp().V2D(), src.fp().V2D());\n  Sqxtn(dst.fp().V2S(), dst.fp().V2D());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero(\n    LiftoffRegister dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero(\n    LiftoffRegister dst, LiftoffRegister src) {\n  VU.set(kScratchReg, E32, m1);\n  VU.set(FPURoundingMode::RTZ);\n  vmv_vv(kSimd128ScratchReg, src.fp().toV());\n  vfncvt_x_f_w(kSimd128ScratchReg, kSimd128ScratchReg);\n  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f64x2_s_zero(\n    LiftoffRegister dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u(LiftoffRegister dst,\n                                                        LiftoffRegister src) {\n  Fcvtzu(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u(LiftoffRegister dst,\n                                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u(LiftoffRegister dst,\n                                                        LiftoffRegister src) {\n  VU.set(FPURoundingMode::RTZ);\n  VU.set(kScratchReg, E32, m1);\n  vfcvt_xu_f_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_u(LiftoffRegister dst,\n                                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s(LiftoffRegister dst,\n                                                        LiftoffRegister src) {\n  Fcvtzs(dst.fp().V4S(), src.fp().V4S());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s(LiftoffRegister dst,\n                                                        LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s", "content": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s(LiftoffRegister dst,\n                                                        LiftoffRegister src) {\n  VU.set(FPURoundingMode::RTZ);\n  VU.set(kScratchReg, E32, m1);\n  vfcvt_x_f_v(dst.fp().toV(), src.fp().toV());\n}", "name_and_para": "void LiftoffAssembler::emit_i32x4_relaxed_trunc_f32x4_s(LiftoffRegister dst,\n                                                        LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i8x16_relaxed_swizzle", "content": "void LiftoffAssembler::emit_i8x16_relaxed_swizzle(LiftoffRegister dst,\n                                                  LiftoffRegister lhs,\n                                                  LiftoffRegister rhs) {\n  Tbl(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_relaxed_swizzle(LiftoffRegister dst,\n                                                  LiftoffRegister lhs,\n                                                  LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_relaxed_swizzle", "content": "void LiftoffAssembler::emit_i8x16_relaxed_swizzle(LiftoffRegister dst,\n                                                  LiftoffRegister lhs,\n                                                  LiftoffRegister rhs) {\n  emit_i8x16_swizzle(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_relaxed_swizzle(LiftoffRegister dst,\n                                                  LiftoffRegister lhs,\n                                                  LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i8x16_swizzle", "content": "void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs) {\n  Tbl(dst.fp().V16B(), lhs.fp().V16B(), rhs.fp().V16B());\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i8x16_swizzle", "content": "void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs) {\n  VU.set(kScratchReg, E8, m1);\n  if (dst == lhs) {\n    vrgather_vv(kSimd128ScratchReg, lhs.fp().toV(), rhs.fp().toV());\n    vmv_vv(dst.fp().toV(), kSimd128ScratchReg);\n  } else {\n    vrgather_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i8x16_swizzle(LiftoffRegister dst,\n                                          LiftoffRegister lhs,\n                                          LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::StoreLane", "content": "void LiftoffAssembler::StoreLane(Register dst, Register offset,\n                                 uintptr_t offset_imm, LiftoffRegister src,\n                                 StoreType type, uint8_t lane,\n                                 uint32_t* protected_store_pc,\n                                 bool i64_offset) {\n  UseScratchRegisterScope temps(this);\n  MemOperand dst_op{liftoff::GetEffectiveAddress(this, &temps, dst, offset,\n                                                 offset_imm, i64_offset)};\n  if (protected_store_pc) *protected_store_pc = pc_offset();\n\n  MachineRepresentation rep = type.mem_rep();\n  if (rep == MachineRepresentation::kWord8) {\n    st1(src.fp().B(), lane, dst_op);\n  } else if (rep == MachineRepresentation::kWord16) {\n    st1(src.fp().H(), lane, dst_op);\n  } else if (rep == MachineRepresentation::kWord32) {\n    st1(src.fp().S(), lane, dst_op);\n  } else {\n    DCHECK_EQ(MachineRepresentation::kWord64, rep);\n    st1(src.fp().D(), lane, dst_op);\n  }\n}", "name_and_para": "void LiftoffAssembler::StoreLane(Register dst, Register offset,\n                                 uintptr_t offset_imm, LiftoffRegister src,\n                                 StoreType type, uint8_t lane,\n                                 uint32_t* protected_store_pc,\n                                 bool i64_offset) "}, {"name": "LiftoffAssembler::StoreLane", "content": "void LiftoffAssembler::StoreLane(Register dst, Register offset,\n                                 uintptr_t offset_imm, LiftoffRegister src,\n                                 StoreType type, uint8_t lane,\n                                 uint32_t* protected_store_pc,\n                                 bool i64_offset) {\n  MemOperand dst_op =\n      liftoff::GetMemOp(this, dst, offset, offset_imm, i64_offset);\n  MachineRepresentation rep = type.mem_rep();\n  if (rep == MachineRepresentation::kWord8) {\n    VU.set(kScratchReg, E8, m1);\n    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);\n    vmv_xs(kScratchReg, kSimd128ScratchReg);\n    if (protected_store_pc) *protected_store_pc = pc_offset();\n    Sb(kScratchReg, dst_op);\n  } else if (rep == MachineRepresentation::kWord16) {\n    VU.set(kScratchReg, E16, m1);\n    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);\n    vmv_xs(kScratchReg, kSimd128ScratchReg);\n    if (protected_store_pc) *protected_store_pc = pc_offset();\n    Sh(kScratchReg, dst_op);\n  } else if (rep == MachineRepresentation::kWord32) {\n    VU.set(kScratchReg, E32, m1);\n    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);\n    vmv_xs(kScratchReg, kSimd128ScratchReg);\n    if (protected_store_pc) *protected_store_pc = pc_offset();\n    Sw(kScratchReg, dst_op);\n  } else {\n    DCHECK_EQ(MachineRepresentation::kWord64, rep);\n    VU.set(kScratchReg, E64, m1);\n    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);\n    vmv_xs(kScratchReg, kSimd128ScratchReg);\n    if (protected_store_pc) *protected_store_pc = pc_offset();\n    Sd(kScratchReg, dst_op);\n  }\n}", "name_and_para": "void LiftoffAssembler::StoreLane(Register dst, Register offset,\n                                 uintptr_t offset_imm, LiftoffRegister src,\n                                 StoreType type, uint8_t lane,\n                                 uint32_t* protected_store_pc,\n                                 bool i64_offset) "}], [{"name": "LiftoffAssembler::LoadLane", "content": "void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,\n                                Register addr, Register offset_reg,\n                                uintptr_t offset_imm, LoadType type,\n                                uint8_t laneidx, uint32_t* protected_load_pc,\n                                bool i64_offset) {\n  UseScratchRegisterScope temps(this);\n  MemOperand src_op{liftoff::GetEffectiveAddress(this, &temps, addr, offset_reg,\n                                                 offset_imm, i64_offset)};\n\n  MachineType mem_type = type.mem_type();\n  if (dst != src) {\n    Mov(dst.fp().Q(), src.fp().Q());\n  }\n\n  *protected_load_pc = pc_offset();\n  if (mem_type == MachineType::Int8()) {\n    ld1(dst.fp().B(), laneidx, src_op);\n  } else if (mem_type == MachineType::Int16()) {\n    ld1(dst.fp().H(), laneidx, src_op);\n  } else if (mem_type == MachineType::Int32()) {\n    ld1(dst.fp().S(), laneidx, src_op);\n  } else if (mem_type == MachineType::Int64()) {\n    ld1(dst.fp().D(), laneidx, src_op);\n  } else {\n    UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,\n                                Register addr, Register offset_reg,\n                                uintptr_t offset_imm, LoadType type,\n                                uint8_t laneidx, uint32_t* protected_load_pc,\n                                bool i64_offset) "}, {"name": "LiftoffAssembler::LoadLane", "content": "void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,\n                                Register addr, Register offset_reg,\n                                uintptr_t offset_imm, LoadType type,\n                                uint8_t laneidx, uint32_t* protected_load_pc,\n                                bool i64_offset) {\n  MemOperand src_op =\n      liftoff::GetMemOp(this, addr, offset_reg, offset_imm, i64_offset);\n  MachineType mem_type = type.mem_type();\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  if (mem_type == MachineType::Int8()) {\n    Lbu(scratch, src_op);\n    *protected_load_pc = pc_offset() - kInstrSize;\n    VU.set(kScratchReg, E64, m1);\n    li(kScratchReg, 0x1 << laneidx);\n    vmv_sx(v0, kScratchReg);\n    VU.set(kScratchReg, E8, m1);\n    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());\n  } else if (mem_type == MachineType::Int16()) {\n    Lhu(scratch, src_op);\n    *protected_load_pc = pc_offset() - kInstrSize;\n    VU.set(kScratchReg, E16, m1);\n    li(kScratchReg, 0x1 << laneidx);\n    vmv_sx(v0, kScratchReg);\n    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());\n  } else if (mem_type == MachineType::Int32()) {\n    Lwu(scratch, src_op);\n    *protected_load_pc = pc_offset() - kInstrSize;\n    VU.set(kScratchReg, E32, m1);\n    li(kScratchReg, 0x1 << laneidx);\n    vmv_sx(v0, kScratchReg);\n    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());\n  } else if (mem_type == MachineType::Int64()) {\n    Ld(scratch, src_op);\n    *protected_load_pc = pc_offset() - kInstrSize;\n    VU.set(kScratchReg, E64, m1);\n    li(kScratchReg, 0x1 << laneidx);\n    vmv_sx(v0, kScratchReg);\n    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());\n  } else {\n    UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,\n                                Register addr, Register offset_reg,\n                                uintptr_t offset_imm, LoadType type,\n                                uint8_t laneidx, uint32_t* protected_load_pc,\n                                bool i64_offset) "}], [{"name": "LiftoffAssembler::LoadTransform", "content": "void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,\n                                     Register offset_reg, uintptr_t offset_imm,\n                                     LoadType type,\n                                     LoadTransformationKind transform,\n                                     uint32_t* protected_load_pc) {\n  UseScratchRegisterScope temps(this);\n  MemOperand src_op =\n      transform == LoadTransformationKind::kSplat\n          ? MemOperand{liftoff::GetEffectiveAddress(this, &temps, src_addr,\n                                                    offset_reg, offset_imm)}\n          : liftoff::GetMemOp(this, &temps, src_addr, offset_reg, offset_imm);\n  *protected_load_pc = pc_offset();\n  MachineType memtype = type.mem_type();\n\n  if (transform == LoadTransformationKind::kExtend) {\n    if (memtype == MachineType::Int8()) {\n      Ldr(dst.fp().D(), src_op);\n      Sxtl(dst.fp().V8H(), dst.fp().V8B());\n    } else if (memtype == MachineType::Uint8()) {\n      Ldr(dst.fp().D(), src_op);\n      Uxtl(dst.fp().V8H(), dst.fp().V8B());\n    } else if (memtype == MachineType::Int16()) {\n      Ldr(dst.fp().D(), src_op);\n      Sxtl(dst.fp().V4S(), dst.fp().V4H());\n    } else if (memtype == MachineType::Uint16()) {\n      Ldr(dst.fp().D(), src_op);\n      Uxtl(dst.fp().V4S(), dst.fp().V4H());\n    } else if (memtype == MachineType::Int32()) {\n      Ldr(dst.fp().D(), src_op);\n      Sxtl(dst.fp().V2D(), dst.fp().V2S());\n    } else if (memtype == MachineType::Uint32()) {\n      Ldr(dst.fp().D(), src_op);\n      Uxtl(dst.fp().V2D(), dst.fp().V2S());\n    }\n  } else if (transform == LoadTransformationKind::kZeroExtend) {\n    if (memtype == MachineType::Int32()) {\n      Ldr(dst.fp().S(), src_op);\n    } else {\n      DCHECK_EQ(MachineType::Int64(), memtype);\n      Ldr(dst.fp().D(), src_op);\n    }\n  } else {\n    DCHECK_EQ(LoadTransformationKind::kSplat, transform);\n    if (memtype == MachineType::Int8()) {\n      ld1r(dst.fp().V16B(), src_op);\n    } else if (memtype == MachineType::Int16()) {\n      ld1r(dst.fp().V8H(), src_op);\n    } else if (memtype == MachineType::Int32()) {\n      ld1r(dst.fp().V4S(), src_op);\n    } else if (memtype == MachineType::Int64()) {\n      ld1r(dst.fp().V2D(), src_op);\n    }\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,\n                                     Register offset_reg, uintptr_t offset_imm,\n                                     LoadType type,\n                                     LoadTransformationKind transform,\n                                     uint32_t* protected_load_pc) "}, {"name": "LiftoffAssembler::LoadTransform", "content": "void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,\n                                     Register offset_reg, uintptr_t offset_imm,\n                                     LoadType type,\n                                     LoadTransformationKind transform,\n                                     uint32_t* protected_load_pc) {\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);\n  VRegister dst_v = dst.fp().toV();\n\n  MachineType memtype = type.mem_type();\n  if (transform == LoadTransformationKind::kExtend) {\n    Ld(scratch, src_op);\n    *protected_load_pc = pc_offset() - kInstrSize;\n    if (memtype == MachineType::Int8()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      VU.set(kScratchReg, E16, m1);\n      vsext_vf2(dst_v, kSimd128ScratchReg);\n    } else if (memtype == MachineType::Uint8()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      VU.set(kScratchReg, E16, m1);\n      vzext_vf2(dst_v, kSimd128ScratchReg);\n    } else if (memtype == MachineType::Int16()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      VU.set(kScratchReg, E32, m1);\n      vsext_vf2(dst_v, kSimd128ScratchReg);\n    } else if (memtype == MachineType::Uint16()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      VU.set(kScratchReg, E32, m1);\n      vzext_vf2(dst_v, kSimd128ScratchReg);\n    } else if (memtype == MachineType::Int32()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      vsext_vf2(dst_v, kSimd128ScratchReg);\n    } else if (memtype == MachineType::Uint32()) {\n      VU.set(kScratchReg, E64, m1);\n      vmv_vx(kSimd128ScratchReg, scratch);\n      vzext_vf2(dst_v, kSimd128ScratchReg);\n    }\n  } else if (transform == LoadTransformationKind::kZeroExtend) {\n    vxor_vv(dst_v, dst_v, dst_v);\n    if (memtype == MachineType::Int32()) {\n      VU.set(kScratchReg, E32, m1);\n      Lwu(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_sx(dst_v, scratch);\n    } else {\n      DCHECK_EQ(MachineType::Int64(), memtype);\n      VU.set(kScratchReg, E64, m1);\n      Ld(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_sx(dst_v, scratch);\n    }\n  } else {\n    DCHECK_EQ(LoadTransformationKind::kSplat, transform);\n    if (memtype == MachineType::Int8()) {\n      VU.set(kScratchReg, E8, m1);\n      Lb(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_vx(dst_v, scratch);\n    } else if (memtype == MachineType::Int16()) {\n      VU.set(kScratchReg, E16, m1);\n      Lh(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_vx(dst_v, scratch);\n    } else if (memtype == MachineType::Int32()) {\n      VU.set(kScratchReg, E32, m1);\n      Lw(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_vx(dst_v, scratch);\n    } else if (memtype == MachineType::Int64()) {\n      VU.set(kScratchReg, E64, m1);\n      Ld(scratch, src_op);\n      *protected_load_pc = pc_offset() - kInstrSize;\n      vmv_vx(dst_v, scratch);\n    }\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,\n                                     Register offset_reg, uintptr_t offset_imm,\n                                     LoadType type,\n                                     LoadTransformationKind transform,\n                                     uint32_t* protected_load_pc) "}], [{"name": "LiftoffAssembler::emit_smi_check", "content": "void LiftoffAssembler::emit_smi_check(Register obj, Label* target,\n                                      SmiCheckMode mode,\n                                      const FreezeCacheState& frozen) {\n  Label* smi_label = mode == kJumpOnSmi ? target : nullptr;\n  Label* not_smi_label = mode == kJumpOnNotSmi ? target : nullptr;\n  JumpIfSmi(obj, smi_label, not_smi_label);\n}", "name_and_para": "void LiftoffAssembler::emit_smi_check(Register obj, Label* target,\n                                      SmiCheckMode mode,\n                                      const FreezeCacheState& frozen) "}, {"name": "LiftoffAssembler::emit_smi_check", "content": "void LiftoffAssembler::emit_smi_check(Register obj, Label* target,\n                                      SmiCheckMode mode,\n                                      const FreezeCacheState& frozen) {\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  And(scratch, obj, Operand(kSmiTagMask));\n  Condition condition = mode == kJumpOnSmi ? eq : ne;\n  Branch(target, condition, scratch, Operand(zero_reg));\n}", "name_and_para": "void LiftoffAssembler::emit_smi_check(Register obj, Label* target,\n                                      SmiCheckMode mode,\n                                      const FreezeCacheState& frozen) "}], [{"name": "LiftoffAssembler::emit_select", "content": "bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,\n                                   LiftoffRegister true_value,\n                                   LiftoffRegister false_value,\n                                   ValueKind kind) {\n  if (kind != kI32 && kind != kI64 && kind != kF32 && kind != kF64)\n    return false;\n\n  Cmp(condition.W(), wzr);\n  switch (kind) {\n    default:\n      UNREACHABLE();\n    case kI32:\n      Csel(dst.gp().W(), true_value.gp().W(), false_value.gp().W(), ne);\n      break;\n    case kI64:\n      Csel(dst.gp().X(), true_value.gp().X(), false_value.gp().X(), ne);\n      break;\n    case kF32:\n      Fcsel(dst.fp().S(), true_value.fp().S(), false_value.fp().S(), ne);\n      break;\n    case kF64:\n      Fcsel(dst.fp().D(), true_value.fp().D(), false_value.fp().D(), ne);\n      break;\n  }\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,\n                                   LiftoffRegister true_value,\n                                   LiftoffRegister false_value,\n                                   ValueKind kind) "}, {"name": "LiftoffAssembler::emit_select", "content": "bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,\n                                   LiftoffRegister true_value,\n                                   LiftoffRegister false_value,\n                                   ValueKind kind) {\n  return false;\n}", "name_and_para": "bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,\n                                   LiftoffRegister true_value,\n                                   LiftoffRegister false_value,\n                                   ValueKind kind) "}], [{"name": "LiftoffAssembler::emit_f64_set_cond", "content": "void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  Fcmp(lhs.D(), rhs.D());\n  Cset(dst.W(), cond);\n  if (cond != ne) {\n    // If V flag set, at least one of the arguments was a Nan -> false.\n    Csel(dst.W(), wzr, dst.W(), vs);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64_set_cond", "content": "void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  FPUCondition fcond = ConditionToConditionCmpFPU(cond);\n  MacroAssembler::CompareF64(dst, fcond, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32_set_cond", "content": "void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  Fcmp(lhs.S(), rhs.S());\n  Cset(dst.W(), cond);\n  if (cond != ne) {\n    // If V flag set, at least one of the arguments was a Nan -> false.\n    Csel(dst.W(), wzr, dst.W(), vs);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32_set_cond", "content": "void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  FPUCondition fcond = ConditionToConditionCmpFPU(cond);\n  MacroAssembler::CompareF32(dst, fcond, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_set_cond(Condition cond, Register dst,\n                                         DoubleRegister lhs,\n                                         DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64_set_cond", "content": "void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) {\n  Cmp(lhs.gp().X(), rhs.gp().X());\n  Cset(dst.W(), cond);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64_set_cond", "content": "void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) {\n  MacroAssembler::CompareI(dst, lhs.gp(), Operand(rhs.gp()), cond);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_set_cond(Condition cond, Register dst,\n                                         LiftoffRegister lhs,\n                                         LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64_eqz", "content": "void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {\n  Cmp(src.gp().X(), xzr);\n  Cset(dst.W(), eq);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_eqz", "content": "void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {\n  MacroAssembler::Sltu(dst, src.gp(), 1);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32_set_cond", "content": "void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,\n                                         Register lhs, Register rhs) {\n  Cmp(lhs.W(), rhs.W());\n  Cset(dst.W(), cond);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,\n                                         Register lhs, Register rhs) "}, {"name": "LiftoffAssembler::emit_i32_set_cond", "content": "void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,\n                                         Register lhs, Register rhs) {\n  UseScratchRegisterScope temps(this);\n  Register scratch0 = temps.Acquire();\n  Register scratch1 = kScratchReg;\n  MacroAssembler::slliw(scratch0, lhs, 0);\n  MacroAssembler::slliw(scratch1, rhs, 0);\n  MacroAssembler::CompareI(dst, scratch0, Operand(scratch1), cond);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_set_cond(Condition cond, Register dst,\n                                         Register lhs, Register rhs) "}], [{"name": "LiftoffAssembler::emit_i32_eqz", "content": "void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {\n  Cmp(src.W(), wzr);\n  Cset(dst.W(), eq);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_eqz", "content": "void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {\n  MacroAssembler::slliw(dst, src, 0);\n  MacroAssembler::Sltu(dst, src, 1);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i32_cond_jumpi", "content": "void LiftoffAssembler::emit_i32_cond_jumpi(Condition cond, Label* label,\n                                           Register lhs, int32_t imm,\n                                           const FreezeCacheState& frozen) {\n  Cmp(lhs.W(), Operand(imm));\n  B(label, cond);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_cond_jumpi(Condition cond, Label* label,\n                                           Register lhs, int32_t imm,\n                                           const FreezeCacheState& frozen) "}, {"name": "LiftoffAssembler::emit_i32_cond_jumpi", "content": "void LiftoffAssembler::emit_i32_cond_jumpi(Condition cond, Label* label,\n                                           Register lhs, int32_t imm,\n                                           const FreezeCacheState& frozen) {\n  MacroAssembler::CompareTaggedAndBranch(label, cond, lhs, Operand(imm));\n}", "name_and_para": "void LiftoffAssembler::emit_i32_cond_jumpi(Condition cond, Label* label,\n                                           Register lhs, int32_t imm,\n                                           const FreezeCacheState& frozen) "}], [{"name": "LiftoffAssembler::emit_cond_jump", "content": "void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,\n                                      ValueKind kind, Register lhs,\n                                      Register rhs,\n                                      const FreezeCacheState& frozen) {\n  switch (kind) {\n    case kI32:\n      if (rhs.is_valid()) {\n        Cmp(lhs.W(), rhs.W());\n      } else {\n        Cmp(lhs.W(), wzr);\n      }\n      break;\n    case kRef:\n    case kRefNull:\n    case kRtt:\n      DCHECK(rhs.is_valid());\n      DCHECK(cond == kEqual || cond == kNotEqual);\n#if defined(V8_COMPRESS_POINTERS)\n      Cmp(lhs.W(), rhs.W());\n#else\n      Cmp(lhs.X(), rhs.X());\n#endif\n      break;\n    case kI64:\n      if (rhs.is_valid()) {\n        Cmp(lhs.X(), rhs.X());\n      } else {\n        Cmp(lhs.X(), xzr);\n      }\n      break;\n    default:\n      UNREACHABLE();\n  }\n  B(label, cond);\n}", "name_and_para": "void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,\n                                      ValueKind kind, Register lhs,\n                                      Register rhs,\n                                      const FreezeCacheState& frozen) "}, {"name": "LiftoffAssembler::emit_cond_jump", "content": "void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,\n                                      ValueKind kind, Register lhs,\n                                      Register rhs,\n                                      const FreezeCacheState& frozen) {\n  if (rhs == no_reg) {\n    if (kind == kI32) {\n      UseScratchRegisterScope temps(this);\n      Register scratch0 = temps.Acquire();\n      slliw(scratch0, lhs, 0);\n      MacroAssembler::Branch(label, cond, scratch0, Operand(zero_reg));\n    } else {\n      DCHECK(kind == kI64);\n      MacroAssembler::Branch(label, cond, lhs, Operand(zero_reg));\n    }\n  } else {\n    if (kind == kI64) {\n      MacroAssembler::Branch(label, cond, lhs, Operand(rhs));\n    } else {\n      DCHECK((kind == kI32) || (kind == kRtt) || (kind == kRef) ||\n             (kind == kRefNull));\n      MacroAssembler::CompareTaggedAndBranch(label, cond, lhs, Operand(rhs));\n    }\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_cond_jump(Condition cond, Label* label,\n                                      ValueKind kind, Register lhs,\n                                      Register rhs,\n                                      const FreezeCacheState& frozen) "}], [{"name": "LiftoffAssembler::emit_jump", "content": "void LiftoffAssembler::emit_jump(Register target) { Br(target); }", "name_and_para": "void LiftoffAssembler::emit_jump(Register target) "}, {"name": "LiftoffAssembler::emit_jump", "content": "void LiftoffAssembler::emit_jump(Register target) {\n  MacroAssembler::Jump(target);\n}", "name_and_para": "void LiftoffAssembler::emit_jump(Register target) "}], [{"name": "LiftoffAssembler::emit_jump", "content": "void LiftoffAssembler::emit_jump(Label* label) { B(label); }", "name_and_para": "void LiftoffAssembler::emit_jump(Label* label) "}, {"name": "LiftoffAssembler::emit_jump", "content": "void LiftoffAssembler::emit_jump(Register target) {\n  MacroAssembler::Jump(target);\n}", "name_and_para": "void LiftoffAssembler::emit_jump(Register target) "}], [{"name": "LiftoffAssembler::emit_i64_signextend_i32", "content": "void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,\n                                               LiftoffRegister src) {\n  sxtw(dst.gp(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,\n                                               LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_signextend_i32", "content": "void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,\n                                               LiftoffRegister src) {\n  slli(dst.gp(), src.gp(), 64 - 32);\n  srai(dst.gp(), dst.gp(), 64 - 32);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,\n                                               LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64_signextend_i16", "content": "void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,\n                                               LiftoffRegister src) {\n  sxth(dst.gp(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,\n                                               LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_signextend_i16", "content": "void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,\n                                               LiftoffRegister src) {\n  slli(dst.gp(), src.gp(), 64 - 16);\n  srai(dst.gp(), dst.gp(), 64 - 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,\n                                               LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64_signextend_i8", "content": "void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  sxtb(dst.gp(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,\n                                              LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_signextend_i8", "content": "void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,\n                                              LiftoffRegister src) {\n  slli(dst.gp(), src.gp(), 64 - 8);\n  srai(dst.gp(), dst.gp(), 64 - 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,\n                                              LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32_signextend_i16", "content": "void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {\n  sxth(dst.W(), src.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_signextend_i16", "content": "void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {\n  slliw(dst, src, 32 - 16);\n  sraiw(dst, dst, 32 - 16);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i32_signextend_i8", "content": "void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {\n  sxtb(dst.W(), src.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_signextend_i8", "content": "void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {\n  slliw(dst, src, 32 - 8);\n  sraiw(dst, dst, 32 - 8);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_type_conversion", "content": "bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,\n                                            LiftoffRegister dst,\n                                            LiftoffRegister src, Label* trap) {\n  switch (opcode) {\n    case kExprI32ConvertI64:\n      Mov(dst.gp().W(), src.gp().W());\n      return true;\n    case kExprI32SConvertF32:\n      Fcvtzs(dst.gp().W(), src.fp().S());  // f32 -> i32 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().S(), static_cast<float>(INT32_MIN));\n      // Check overflow.\n      Ccmp(dst.gp().W(), -1, VFlag, ge);\n      B(trap, vs);\n      return true;\n    case kExprI32UConvertF32:\n      Fcvtzu(dst.gp().W(), src.fp().S());  // f32 -> i32 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().S(), -1.0);\n      // Check overflow.\n      Ccmp(dst.gp().W(), -1, ZFlag, gt);\n      B(trap, eq);\n      return true;\n    case kExprI32SConvertF64: {\n      // INT32_MIN and INT32_MAX are valid results, we cannot test the result\n      // to detect the overflows. We could have done two immediate floating\n      // point comparisons but it would have generated two conditional branches.\n      UseScratchRegisterScope temps(this);\n      VRegister fp_ref = temps.AcquireD();\n      VRegister fp_cmp = temps.AcquireD();\n      Fcvtzs(dst.gp().W(), src.fp().D());  // f64 -> i32 round to zero.\n      Frintz(fp_ref, src.fp().D());        // f64 -> f64 round to zero.\n      Scvtf(fp_cmp, dst.gp().W());         // i32 -> f64.\n      // If comparison fails, we have an overflow or a NaN.\n      Fcmp(fp_cmp, fp_ref);\n      B(trap, ne);\n      return true;\n    }\n    case kExprI32UConvertF64: {\n      // INT32_MAX is a valid result, we cannot test the result to detect the\n      // overflows. We could have done two immediate floating point comparisons\n      // but it would have generated two conditional branches.\n      UseScratchRegisterScope temps(this);\n      VRegister fp_ref = temps.AcquireD();\n      VRegister fp_cmp = temps.AcquireD();\n      Fcvtzu(dst.gp().W(), src.fp().D());  // f64 -> i32 round to zero.\n      Frintz(fp_ref, src.fp().D());        // f64 -> f64 round to zero.\n      Ucvtf(fp_cmp, dst.gp().W());         // i32 -> f64.\n      // If comparison fails, we have an overflow or a NaN.\n      Fcmp(fp_cmp, fp_ref);\n      B(trap, ne);\n      return true;\n    }\n    case kExprI32SConvertSatF32:\n      Fcvtzs(dst.gp().W(), src.fp().S());\n      return true;\n    case kExprI32UConvertSatF32:\n      Fcvtzu(dst.gp().W(), src.fp().S());\n      return true;\n    case kExprI32SConvertSatF64:\n      Fcvtzs(dst.gp().W(), src.fp().D());\n      return true;\n    case kExprI32UConvertSatF64:\n      Fcvtzu(dst.gp().W(), src.fp().D());\n      return true;\n    case kExprI64SConvertSatF32:\n      Fcvtzs(dst.gp().X(), src.fp().S());\n      return true;\n    case kExprI64UConvertSatF32:\n      Fcvtzu(dst.gp().X(), src.fp().S());\n      return true;\n    case kExprI64SConvertSatF64:\n      Fcvtzs(dst.gp().X(), src.fp().D());\n      return true;\n    case kExprI64UConvertSatF64:\n      Fcvtzu(dst.gp().X(), src.fp().D());\n      return true;\n    case kExprI32ReinterpretF32:\n      Fmov(dst.gp().W(), src.fp().S());\n      return true;\n    case kExprI64SConvertI32:\n      Sxtw(dst.gp().X(), src.gp().W());\n      return true;\n    case kExprI64SConvertF32:\n      Fcvtzs(dst.gp().X(), src.fp().S());  // f32 -> i64 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().S(), static_cast<float>(INT64_MIN));\n      // Check overflow.\n      Ccmp(dst.gp().X(), -1, VFlag, ge);\n      B(trap, vs);\n      return true;\n    case kExprI64UConvertF32:\n      Fcvtzu(dst.gp().X(), src.fp().S());  // f32 -> i64 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().S(), -1.0);\n      // Check overflow.\n      Ccmp(dst.gp().X(), -1, ZFlag, gt);\n      B(trap, eq);\n      return true;\n    case kExprI64SConvertF64:\n      Fcvtzs(dst.gp().X(), src.fp().D());  // f64 -> i64 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().D(), static_cast<float>(INT64_MIN));\n      // Check overflow.\n      Ccmp(dst.gp().X(), -1, VFlag, ge);\n      B(trap, vs);\n      return true;\n    case kExprI64UConvertF64:\n      Fcvtzu(dst.gp().X(), src.fp().D());  // f64 -> i64 round to zero.\n      // Check underflow and NaN.\n      Fcmp(src.fp().D(), -1.0);\n      // Check overflow.\n      Ccmp(dst.gp().X(), -1, ZFlag, gt);\n      B(trap, eq);\n      return true;\n    case kExprI64UConvertI32:\n      Mov(dst.gp().W(), src.gp().W());\n      return true;\n    case kExprI64ReinterpretF64:\n      Fmov(dst.gp().X(), src.fp().D());\n      return true;\n    case kExprF32SConvertI32:\n      Scvtf(dst.fp().S(), src.gp().W());\n      return true;\n    case kExprF32UConvertI32:\n      Ucvtf(dst.fp().S(), src.gp().W());\n      return true;\n    case kExprF32SConvertI64:\n      Scvtf(dst.fp().S(), src.gp().X());\n      return true;\n    case kExprF32UConvertI64:\n      Ucvtf(dst.fp().S(), src.gp().X());\n      return true;\n    case kExprF32ConvertF64:\n      Fcvt(dst.fp().S(), src.fp().D());\n      return true;\n    case kExprF32ReinterpretI32:\n      Fmov(dst.fp().S(), src.gp().W());\n      return true;\n    case kExprF64SConvertI32:\n      Scvtf(dst.fp().D(), src.gp().W());\n      return true;\n    case kExprF64UConvertI32:\n      Ucvtf(dst.fp().D(), src.gp().W());\n      return true;\n    case kExprF64SConvertI64:\n      Scvtf(dst.fp().D(), src.gp().X());\n      return true;\n    case kExprF64UConvertI64:\n      Ucvtf(dst.fp().D(), src.gp().X());\n      return true;\n    case kExprF64ConvertF32:\n      Fcvt(dst.fp().D(), src.fp().S());\n      return true;\n    case kExprF64ReinterpretI64:\n      Fmov(dst.fp().D(), src.gp().X());\n      return true;\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,\n                                            LiftoffRegister dst,\n                                            LiftoffRegister src, Label* trap) "}, {"name": "LiftoffAssembler::emit_type_conversion", "content": "bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,\n                                            LiftoffRegister dst,\n                                            LiftoffRegister src, Label* trap) {\n  switch (opcode) {\n    case kExprI32ConvertI64:\n      // According to WebAssembly spec, if I64 value does not fit the range of\n      // I32, the value is undefined. Therefore, We use sign extension to\n      // implement I64 to I32 truncation\n      MacroAssembler::SignExtendWord(dst.gp(), src.gp());\n      return true;\n    case kExprI32SConvertF32:\n    case kExprI32UConvertF32:\n    case kExprI32SConvertF64:\n    case kExprI32UConvertF64:\n    case kExprI64SConvertF32:\n    case kExprI64UConvertF32:\n    case kExprI64SConvertF64:\n    case kExprI64UConvertF64:\n    case kExprF32ConvertF64: {\n      // real conversion, if src is out-of-bound of target integer types,\n      // kScratchReg is set to 0\n      switch (opcode) {\n        case kExprI32SConvertF32:\n          Trunc_w_s(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI32UConvertF32:\n          Trunc_uw_s(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI32SConvertF64:\n          Trunc_w_d(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI32UConvertF64:\n          Trunc_uw_d(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI64SConvertF32:\n          Trunc_l_s(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI64UConvertF32:\n          Trunc_ul_s(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI64SConvertF64:\n          Trunc_l_d(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprI64UConvertF64:\n          Trunc_ul_d(dst.gp(), src.fp(), kScratchReg);\n          break;\n        case kExprF32ConvertF64:\n          fcvt_s_d(dst.fp(), src.fp());\n          break;\n        default:\n          UNREACHABLE();\n      }\n\n      // Checking if trap.\n      if (trap != nullptr) {\n        MacroAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));\n      }\n\n      return true;\n    }\n    case kExprI32ReinterpretF32:\n      MacroAssembler::ExtractLowWordFromF64(dst.gp(), src.fp());\n      return true;\n    case kExprI64SConvertI32:\n      MacroAssembler::SignExtendWord(dst.gp(), src.gp());\n      return true;\n    case kExprI64UConvertI32:\n      MacroAssembler::ZeroExtendWord(dst.gp(), src.gp());\n      return true;\n    case kExprI64ReinterpretF64:\n      fmv_x_d(dst.gp(), src.fp());\n      return true;\n    case kExprF32SConvertI32: {\n      MacroAssembler::Cvt_s_w(dst.fp(), src.gp());\n      return true;\n    }\n    case kExprF32UConvertI32:\n      MacroAssembler::Cvt_s_uw(dst.fp(), src.gp());\n      return true;\n    case kExprF32ReinterpretI32:\n      fmv_w_x(dst.fp(), src.gp());\n      return true;\n    case kExprF64SConvertI32: {\n      MacroAssembler::Cvt_d_w(dst.fp(), src.gp());\n      return true;\n    }\n    case kExprF64UConvertI32:\n      MacroAssembler::Cvt_d_uw(dst.fp(), src.gp());\n      return true;\n    case kExprF64ConvertF32:\n      fcvt_d_s(dst.fp(), src.fp());\n      return true;\n    case kExprF64ReinterpretI64:\n      fmv_d_x(dst.fp(), src.gp());\n      return true;\n    case kExprI32SConvertSatF32: {\n      fcvt_w_s(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_s(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI32UConvertSatF32: {\n      fcvt_wu_s(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_s(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI32SConvertSatF64: {\n      fcvt_w_d(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_d(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI32UConvertSatF64: {\n      fcvt_wu_d(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_d(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI64SConvertSatF32: {\n      fcvt_l_s(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_s(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI64UConvertSatF32: {\n      fcvt_lu_s(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_s(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI64SConvertSatF64: {\n      fcvt_l_d(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_d(dst.gp(), src.fp());\n      return true;\n    }\n    case kExprI64UConvertSatF64: {\n      fcvt_lu_d(dst.gp(), src.fp(), RTZ);\n      Clear_if_nan_d(dst.gp(), src.fp());\n      return true;\n    }\n    default:\n      return false;\n  }\n}", "name_and_para": "bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,\n                                            LiftoffRegister dst,\n                                            LiftoffRegister src, Label* trap) "}], [{"name": "LiftoffAssembler::emit_f64_copysign", "content": "void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  DoubleRegister scratch = temps.AcquireD();\n  Ushr(scratch.V1D(), rhs.V1D(), 63);\n  if (dst != lhs) {\n    Fmov(dst.D(), lhs.D());\n  }\n  Sli(dst.V1D(), scratch.V1D(), 63);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64_copysign", "content": "void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  fsgnj_d(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32_copysign", "content": "void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  UseScratchRegisterScope temps(this);\n  DoubleRegister scratch = temps.AcquireD();\n  Ushr(scratch.V2S(), rhs.V2S(), 31);\n  if (dst != lhs) {\n    Fmov(dst.S(), lhs.S());\n  }\n  Sli(dst.V2S(), scratch.V2S(), 31);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32_copysign", "content": "void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) {\n  fsgnj_s(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_copysign(DoubleRegister dst, DoubleRegister lhs,\n                                         DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::clear_i32_upper_half", "content": "void LiftoffAssembler::clear_i32_upper_half(Register dst) { Uxtw(dst, dst); }", "name_and_para": "void LiftoffAssembler::clear_i32_upper_half(Register dst) "}, {"name": "LiftoffAssembler::clear_i32_upper_half", "content": "void LiftoffAssembler::clear_i32_upper_half(Register dst) {\n  // Don't need to clear the upper halves of i32 values for sandbox on riscv64,\n  // because we'll explicitly zero-extend their lower halves before using them\n  // for memory accesses anyway.\n}", "name_and_para": "void LiftoffAssembler::clear_i32_upper_half(Register dst) "}], [{"name": "LiftoffAssembler::emit_u32_to_uintptr", "content": "void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) {\n  Uxtw(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_u32_to_uintptr", "content": "void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) {\n  ZeroExtendWord(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i64_remu", "content": "bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  Register dst_x = dst.gp().X();\n  Register lhs_x = lhs.gp().X();\n  Register rhs_x = rhs.gp().X();\n  // Do early div.\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Udiv(scratch, lhs_x, rhs_x);\n  // Check for division by zero.\n  Cbz(rhs_x, trap_div_by_zero);\n  // Compute remainder.\n  Msub(dst_x, scratch, rhs_x, lhs_x);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i64_remu", "content": "bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));\n  MacroAssembler::Modu64(dst.gp(), lhs.gp(), rhs.gp());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i64_rems", "content": "bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  Register dst_x = dst.gp().X();\n  Register lhs_x = lhs.gp().X();\n  Register rhs_x = rhs.gp().X();\n  // Do early div.\n  // No need to check kMinInt / -1 because the result is kMinInt and then\n  // kMinInt * -1 -> kMinInt. In this case, the Msub result is therefore 0.\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Sdiv(scratch, lhs_x, rhs_x);\n  // Check for division by zero.\n  Cbz(rhs_x, trap_div_by_zero);\n  // Compute remainder.\n  Msub(dst_x, scratch, rhs_x, lhs_x);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i64_rems", "content": "bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));\n  MacroAssembler::Mod64(dst.gp(), lhs.gp(), rhs.gp());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i64_divu", "content": "bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  // Check for division by zero.\n  Cbz(rhs.gp().X(), trap_div_by_zero);\n  // Do div.\n  Udiv(dst.gp().X(), lhs.gp().X(), rhs.gp().X());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i64_divu", "content": "bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));\n  MacroAssembler::Divu64(dst.gp(), lhs.gp(), rhs.gp());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i64_divs", "content": "bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) {\n  Register dst_x = dst.gp().X();\n  Register lhs_x = lhs.gp().X();\n  Register rhs_x = rhs.gp().X();\n  bool can_use_dst = !dst_x.Aliases(lhs_x) && !dst_x.Aliases(rhs_x);\n  if (can_use_dst) {\n    // Do div early.\n    Sdiv(dst_x, lhs_x, rhs_x);\n  }\n  // Check for division by zero.\n  Cbz(rhs_x, trap_div_by_zero);\n  // Check for kMinInt / -1. This is unrepresentable.\n  Cmp(rhs_x, -1);\n  Ccmp(lhs_x, 1, NoFlag, eq);\n  B(trap_div_unrepresentable, vs);\n  if (!can_use_dst) {\n    // Do div.\n    Sdiv(dst_x, lhs_x, rhs_x);\n  }\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) "}, {"name": "LiftoffAssembler::emit_i64_divs", "content": "bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));\n\n  // Check if lhs == MinInt64 and rhs == -1, since this case is unrepresentable.\n  MacroAssembler::CompareI(kScratchReg, lhs.gp(),\n                           Operand(std::numeric_limits<int64_t>::min()), ne);\n  MacroAssembler::CompareI(kScratchReg2, rhs.gp(), Operand(-1), ne);\n  add(kScratchReg, kScratchReg, kScratchReg2);\n  MacroAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,\n                         Operand(zero_reg));\n\n  MacroAssembler::Div64(dst.gp(), lhs.gp(), rhs.gp());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,\n                                     LiftoffRegister rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) "}], [{"name": "LiftoffAssembler::emit_i32_remu", "content": "void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  Register dst_w = dst.W();\n  Register lhs_w = lhs.W();\n  Register rhs_w = rhs.W();\n  // Do early div.\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireW();\n  Udiv(scratch, lhs_w, rhs_w);\n  // Check for division by zero.\n  Cbz(rhs_w, trap_div_by_zero);\n  // Compute remainder.\n  Msub(dst_w, scratch, rhs_w, lhs_w);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i32_remu", "content": "void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));\n  MacroAssembler::Modu32(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i32_rems", "content": "void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  Register dst_w = dst.W();\n  Register lhs_w = lhs.W();\n  Register rhs_w = rhs.W();\n  // Do early div.\n  // No need to check kMinInt / -1 because the result is kMinInt and then\n  // kMinInt * -1 -> kMinInt. In this case, the Msub result is therefore 0.\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireW();\n  Sdiv(scratch, lhs_w, rhs_w);\n  // Check for division by zero.\n  Cbz(rhs_w, trap_div_by_zero);\n  // Compute remainder.\n  Msub(dst_w, scratch, rhs_w, lhs_w);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i32_rems", "content": "void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));\n  MacroAssembler::Mod32(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i32_divu", "content": "void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  // Check for division by zero.\n  Cbz(rhs.W(), trap_div_by_zero);\n  // Do div.\n  Udiv(dst.W(), lhs.W(), rhs.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}, {"name": "LiftoffAssembler::emit_i32_divu", "content": "void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));\n  MacroAssembler::Divu32(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero) "}], [{"name": "LiftoffAssembler::emit_i32_divs", "content": "void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) {\n  Register dst_w = dst.W();\n  Register lhs_w = lhs.W();\n  Register rhs_w = rhs.W();\n  bool can_use_dst = !dst_w.Aliases(lhs_w) && !dst_w.Aliases(rhs_w);\n  if (can_use_dst) {\n    // Do div early.\n    Sdiv(dst_w, lhs_w, rhs_w);\n  }\n  // Check for division by zero.\n  Cbz(rhs_w, trap_div_by_zero);\n  // Check for kMinInt / -1. This is unrepresentable.\n  Cmp(rhs_w, -1);\n  Ccmp(lhs_w, 1, NoFlag, eq);\n  B(trap_div_unrepresentable, vs);\n  if (!can_use_dst) {\n    // Do div.\n    Sdiv(dst_w, lhs_w, rhs_w);\n  }\n}", "name_and_para": "void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) "}, {"name": "LiftoffAssembler::emit_i32_divs", "content": "void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) {\n  MacroAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));\n\n  // Check if lhs == kMinInt and rhs == -1, since this case is unrepresentable.\n  MacroAssembler::CompareI(kScratchReg, lhs, Operand(kMinInt), ne);\n  MacroAssembler::CompareI(kScratchReg2, rhs, Operand(-1), ne);\n  add(kScratchReg, kScratchReg, kScratchReg2);\n  MacroAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,\n                         Operand(zero_reg));\n\n  MacroAssembler::Div32(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,\n                                     Label* trap_div_by_zero,\n                                     Label* trap_div_unrepresentable) "}], [{"name": "LiftoffAssembler::IncrementSmi", "content": "void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) {\n  UseScratchRegisterScope temps(this);\n  if (COMPRESS_POINTERS_BOOL) {\n    DCHECK(SmiValuesAre31Bits());\n    Register scratch = temps.AcquireW();\n    Ldr(scratch, MemOperand(dst.gp(), offset));\n    Add(scratch, scratch, Operand(Smi::FromInt(1)));\n    Str(scratch, MemOperand(dst.gp(), offset));\n  } else {\n    Register scratch = temps.AcquireX();\n    SmiUntag(scratch, MemOperand(dst.gp(), offset));\n    Add(scratch, scratch, Operand(1));\n    SmiTag(scratch);\n    Str(scratch, MemOperand(dst.gp(), offset));\n  }\n}", "name_and_para": "void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) "}, {"name": "LiftoffAssembler::IncrementSmi", "content": "void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) {\n  UseScratchRegisterScope temps(this);\n  if (COMPRESS_POINTERS_BOOL) {\n    DCHECK(SmiValuesAre31Bits());\n    Register scratch = temps.Acquire();\n    Lw(scratch, MemOperand(dst.gp(), offset));\n    Add32(scratch, scratch, Operand(Smi::FromInt(1)));\n    Sw(scratch, MemOperand(dst.gp(), offset));\n  } else {\n    Register scratch = temps.Acquire();\n    SmiUntag(scratch, MemOperand(dst.gp(), offset));\n    Add64(scratch, scratch, Operand(1));\n    SmiTag(scratch);\n    Sd(scratch, MemOperand(dst.gp(), offset));\n  }\n}", "name_and_para": "void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) "}], [{"name": "LiftoffAssembler::emit_i64_popcnt", "content": "bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  PopcntHelper(dst.gp().X(), src.gp().X());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_popcnt", "content": "bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,\n                                       LiftoffRegister src) {\n  MacroAssembler::Popcnt64(dst.gp(), src.gp(), kScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,\n                                       LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64_ctz", "content": "void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {\n  Rbit(dst.gp().X(), src.gp().X());\n  Clz(dst.gp().X(), dst.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_ctz", "content": "void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {\n  MacroAssembler::Ctz64(dst.gp(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i64_clz", "content": "void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {\n  Clz(dst.gp().X(), src.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) "}, {"name": "LiftoffAssembler::emit_i64_clz", "content": "void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {\n  MacroAssembler::Clz64(dst.gp(), src.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) "}], [{"name": "LiftoffAssembler::emit_i32_popcnt", "content": "bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {\n  PopcntHelper(dst.W(), src.W());\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_popcnt", "content": "bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {\n  MacroAssembler::Popcnt32(dst, src, kScratchReg);\n  return true;\n}", "name_and_para": "bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i32_ctz", "content": "void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {\n  Rbit(dst.W(), src.W());\n  Clz(dst.W(), dst.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_ctz", "content": "void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {\n  MacroAssembler::Ctz32(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i32_clz", "content": "void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {\n  Clz(dst.W(), src.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_clz(Register dst, Register src) "}, {"name": "LiftoffAssembler::emit_i32_clz", "content": "void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {\n  MacroAssembler::Clz32(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_clz(Register dst, Register src) "}], [{"name": "LiftoffAssembler::emit_i64_addi", "content": "void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int64_t imm) {\n  Add(dst.gp().X(), lhs.gp().X(), imm);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int64_t imm) "}, {"name": "LiftoffAssembler::emit_i64_addi", "content": "void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int64_t imm) {\n  MacroAssembler::Add64(dst.gp(), lhs.gp(), Operand(imm));\n}", "name_and_para": "void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int64_t imm) "}], [{"name": "LiftoffAssembler::emit_f64_neg", "content": "void LiftoffAssembler::emit_f64_neg(DoubleRegister dst, DoubleRegister src) {\n  Fneg(dst.D(), src.D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64_neg(DoubleRegister dst, DoubleRegister src) "}, {"name": "LiftoffAssembler::emit_f64_neg", "content": "void LiftoffAssembler::emit_f64_neg(DoubleRegister dst, DoubleRegister src) {\n  MacroAssembler::Neg_d(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_neg(DoubleRegister dst, DoubleRegister src) "}], [{"name": "LiftoffAssembler::emit_f64_max", "content": "void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  Fmax(dst.D(), lhs.D(), rhs.D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64_max", "content": "void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  MacroAssembler::Float64Max(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f64_min", "content": "void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  Fmin(dst.D(), lhs.D(), rhs.D());\n}", "name_and_para": "void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f64_min", "content": "void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  MacroAssembler::Float64Min(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f64_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32_neg", "content": "void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) {\n  Fneg(dst.S(), src.S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) "}, {"name": "LiftoffAssembler::emit_f32_neg", "content": "void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) {\n  MacroAssembler::Neg_s(dst, src);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) "}], [{"name": "LiftoffAssembler::emit_f32_max", "content": "void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  Fmax(dst.S(), lhs.S(), rhs.S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32_max", "content": "void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  MacroAssembler::Float32Max(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_max(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_f32_min", "content": "void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  Fmin(dst.S(), lhs.S(), rhs.S());\n}", "name_and_para": "void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}, {"name": "LiftoffAssembler::emit_f32_min", "content": "void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) {\n  MacroAssembler::Float32Min(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_f32_min(DoubleRegister dst, DoubleRegister lhs,\n                                    DoubleRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64_shr", "content": "void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  Lsr(dst.gp().X(), src.gp().X(), amount.X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}, {"name": "LiftoffAssembler::emit_i64_shr", "content": "void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  liftoff::Emit64BitShiftOperation(this, dst, src, amount,\n                                   &MacroAssembler::ShrPair);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}], [{"name": "LiftoffAssembler::emit_i64_sar", "content": "void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  Asr(dst.gp().X(), src.gp().X(), amount.X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}, {"name": "LiftoffAssembler::emit_i64_sar", "content": "void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  liftoff::Emit64BitShiftOperation(this, dst, src, amount,\n                                   &MacroAssembler::SarPair);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}], [{"name": "LiftoffAssembler::emit_i64_shl", "content": "void LiftoffAssembler::emit_i64_shl(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  Lsl(dst.gp().X(), src.gp().X(), amount.X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_shl(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}, {"name": "LiftoffAssembler::emit_i64_shl", "content": "void LiftoffAssembler::emit_i64_shl(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) {\n  ASM_CODE_COMMENT(this);\n  liftoff::Emit64BitShiftOperation(this, dst, src, amount,\n                                   &MacroAssembler::ShlPair);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_shl(LiftoffRegister dst, LiftoffRegister src,\n                                    Register amount) "}], [{"name": "LiftoffAssembler::emit_i64_muli", "content": "void LiftoffAssembler::emit_i64_muli(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int32_t imm) {\n  if (base::bits::IsPowerOfTwo(imm)) {\n    emit_i64_shli(dst, lhs, base::bits::WhichPowerOfTwo(imm));\n    return;\n  }\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Mov(scratch, imm);\n  Mul(dst.gp().X(), lhs.gp().X(), scratch);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_muli(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int32_t imm) "}, {"name": "LiftoffAssembler::emit_i64_muli", "content": "void LiftoffAssembler::emit_i64_muli(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int32_t imm) {\n  if (base::bits::IsPowerOfTwo(imm)) {\n    emit_i64_shli(dst, lhs, base::bits::WhichPowerOfTwo(imm));\n    return;\n  }\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  li(scratch, imm);\n  Mul64(dst.gp(), lhs.gp(), scratch);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_muli(LiftoffRegister dst, LiftoffRegister lhs,\n                                     int32_t imm) "}], [{"name": "LiftoffAssembler::emit_i64_mul", "content": "void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  Mul(dst.gp().X(), lhs.gp().X(), rhs.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64_mul", "content": "void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  MacroAssembler::Mul64(dst.gp(), lhs.gp(), rhs.gp());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64_sub", "content": "void LiftoffAssembler::emit_i64_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  Sub(dst.gp().X(), lhs.gp().X(), rhs.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64_sub", "content": "void LiftoffAssembler::emit_i64_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  MacroAssembler::SubPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),\n                          lhs.high_gp(), rhs.low_gp(), rhs.high_gp(),\n                          kScratchReg, kScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_sub(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i64_add", "content": "void LiftoffAssembler::emit_i64_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  Add(dst.gp().X(), lhs.gp().X(), rhs.gp().X());\n}", "name_and_para": "void LiftoffAssembler::emit_i64_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}, {"name": "LiftoffAssembler::emit_i64_add", "content": "void LiftoffAssembler::emit_i64_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) {\n  MacroAssembler::AddPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),\n                          lhs.high_gp(), rhs.low_gp(), rhs.high_gp(),\n                          kScratchReg, kScratchReg2);\n}", "name_and_para": "void LiftoffAssembler::emit_i64_add(LiftoffRegister dst, LiftoffRegister lhs,\n                                    LiftoffRegister rhs) "}], [{"name": "LiftoffAssembler::emit_i32_mul", "content": "void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {\n  Mul(dst.W(), lhs.W(), rhs.W());\n}", "name_and_para": "void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) "}, {"name": "LiftoffAssembler::emit_i32_mul", "content": "void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {\n  MacroAssembler::Mul32(dst, lhs, rhs);\n}", "name_and_para": "void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) "}], [{"name": "LiftoffAssembler::LoadSpillAddress", "content": "void LiftoffAssembler::LoadSpillAddress(Register dst, int offset,\n                                        ValueKind /* kind */) {\n  Sub(dst, fp, offset);\n}", "name_and_para": "void LiftoffAssembler::LoadSpillAddress(Register dst, int offset,\n                                        ValueKind /* kind */) "}, {"name": "LiftoffAssembler::LoadSpillAddress", "content": "void LiftoffAssembler::LoadSpillAddress(Register dst, int offset,\n                                        ValueKind /* kind */) {\n  SubWord(dst, fp, offset);\n}", "name_and_para": "void LiftoffAssembler::LoadSpillAddress(Register dst, int offset,\n                                        ValueKind /* kind */) "}], [{"name": "LiftoffAssembler::FillStackSlotsWithZero", "content": "void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {\n  // Zero 'size' bytes *below* start, byte at offset 'start' is untouched.\n  DCHECK_LE(0, start);\n  DCHECK_LT(0, size);\n  DCHECK_EQ(0, size % 4);\n  RecordUsedSpillOffset(start + size);\n\n  int max_stp_offset = -start - size;\n  // We check IsImmLSUnscaled(-start-12) because str only allows for unscaled\n  // 9-bit immediate offset [-256,256]. If start is large enough, which can\n  // happen when a function has many params (>=32 i64), str cannot be encoded\n  // properly. We can use Str, which will generate more instructions, so\n  // fallback to the general case below.\n  if (size <= 12 * kStackSlotSize &&\n      IsImmLSPair(max_stp_offset, kXRegSizeLog2) &&\n      IsImmLSUnscaled(-start - 12)) {\n    // Special straight-line code for up to 12 slots. Generates one\n    // instruction per two slots (<= 7 instructions total).\n    static_assert(kStackSlotSize == kSystemPointerSize);\n    uint32_t remainder = size;\n    for (; remainder >= 2 * kStackSlotSize; remainder -= 2 * kStackSlotSize) {\n      stp(xzr, xzr, liftoff::GetStackSlot(start + remainder));\n    }\n\n    DCHECK_GE(12, remainder);\n    switch (remainder) {\n      case 12:\n        str(xzr, liftoff::GetStackSlot(start + remainder));\n        str(wzr, liftoff::GetStackSlot(start + remainder - 8));\n        break;\n      case 8:\n        str(xzr, liftoff::GetStackSlot(start + remainder));\n        break;\n      case 4:\n        str(wzr, liftoff::GetStackSlot(start + remainder));\n        break;\n      case 0:\n        break;\n      default:\n        UNREACHABLE();\n    }\n  } else {\n    // General case for bigger counts (5-8 instructions).\n    UseScratchRegisterScope temps(this);\n    Register address_reg = temps.AcquireX();\n    // This {Sub} might use another temp register if the offset is too large.\n    Sub(address_reg, fp, start + size);\n    Register count_reg = temps.AcquireX();\n    Mov(count_reg, size / 4);\n\n    Label loop;\n    bind(&loop);\n    sub(count_reg, count_reg, 1);\n    str(wzr, MemOperand(address_reg, kSystemPointerSize / 2, PostIndex));\n    cbnz(count_reg, &loop);\n  }\n}", "name_and_para": "void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) "}, {"name": "LiftoffAssembler::FillStackSlotsWithZero", "content": "void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {\n  DCHECK_LT(0, size);\n  RecordUsedSpillOffset(start + size);\n\n  if (size <= 12 * kStackSlotSize) {\n    // Special straight-line code for up to 12 slots. Generates one\n    // instruction per slot (<= 12 instructions total).\n    uint32_t remainder = size;\n    for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {\n      Sd(zero_reg, liftoff::GetStackSlot(start + remainder));\n    }\n    DCHECK(remainder == 4 || remainder == 0);\n    if (remainder) {\n      Sw(zero_reg, liftoff::GetStackSlot(start + remainder));\n    }\n  } else {\n    // General case for bigger counts (12 instructions).\n    // Use a0 for start address (inclusive), a1 for end address (exclusive).\n    Push(a1, a0);\n    Add64(a0, fp, Operand(-start - size));\n    Add64(a1, fp, Operand(-start));\n\n    Label loop;\n    bind(&loop);\n    Sd(zero_reg, MemOperand(a0));\n    addi(a0, a0, kSystemPointerSize);\n    BranchShort(&loop, ne, a0, Operand(a1));\n\n    Pop(a1, a0);\n  }\n}", "name_and_para": "void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) "}], [{"name": "LiftoffAssembler::FillI64Half", "content": "void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {\n  UNREACHABLE();\n}", "name_and_para": "void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) "}, {"name": "LiftoffAssembler::FillI64Half", "content": "void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {\n  UNREACHABLE();\n}", "name_and_para": "void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) "}], [{"name": "LiftoffAssembler::Fill", "content": "void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) {\n  MemOperand src = liftoff::GetStackSlot(offset);\n  Ldr(liftoff::GetRegFromType(reg, kind), src);\n}", "name_and_para": "void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) "}, {"name": "LiftoffAssembler::Fill", "content": "void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) {\n  MemOperand src = liftoff::GetStackSlot(offset);\n  switch (kind) {\n    case kI32:\n      Lw(reg.gp(), src);\n      break;\n    case kI64:\n    case kRef:\n    case kRefNull:\n      Ld(reg.gp(), src);\n      break;\n    case kF32:\n      LoadFloat(reg.fp(), src);\n      break;\n    case kF64:\n      MacroAssembler::LoadDouble(reg.fp(), src);\n      break;\n    case kS128: {\n      VU.set(kScratchReg, E8, m1);\n      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;\n      if (src.offset() != 0) {\n        MacroAssembler::Add64(src_reg, src.rm(), src.offset());\n      }\n      vl(reg.fp().toV(), src_reg, 0, E8);\n      break;\n    }\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) "}], [{"name": "LiftoffAssembler::Spill", "content": "void LiftoffAssembler::Spill(int offset, WasmValue value) {\n  RecordUsedSpillOffset(offset);\n  MemOperand dst = liftoff::GetStackSlot(offset);\n  UseScratchRegisterScope temps(this);\n  CPURegister src = CPURegister::no_reg();\n  switch (value.type().kind()) {\n    case kI32:\n      if (value.to_i32() == 0) {\n        src = wzr;\n      } else {\n        src = temps.AcquireW();\n        Mov(src.W(), value.to_i32());\n      }\n      break;\n    case kI64:\n      if (value.to_i64() == 0) {\n        src = xzr;\n      } else {\n        src = temps.AcquireX();\n        Mov(src.X(), value.to_i64());\n      }\n      break;\n    default:\n      // We do not track f32 and f64 constants, hence they are unreachable.\n      UNREACHABLE();\n  }\n  Str(src, dst);\n}", "name_and_para": "void LiftoffAssembler::Spill(int offset, WasmValue value) "}, {"name": "LiftoffAssembler::Spill", "content": "void LiftoffAssembler::Spill(int offset, WasmValue value) {\n  RecordUsedSpillOffset(offset);\n  MemOperand dst = liftoff::GetStackSlot(offset);\n  switch (value.type().kind()) {\n    case kI32: {\n      UseScratchRegisterScope temps(this);\n      Register tmp = temps.Acquire();\n      MacroAssembler::li(tmp, Operand(value.to_i32()));\n      Sw(tmp, dst);\n      break;\n    }\n    case kI64:\n    case kRef:\n    case kRefNull: {\n      UseScratchRegisterScope temps(this);\n      Register tmp = temps.Acquire();\n      MacroAssembler::li(tmp, value.to_i64());\n      Sd(tmp, dst);\n      break;\n    }\n    default:\n      // kWasmF32 and kWasmF64 are unreachable, since those\n      // constants are not tracked.\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::Spill(int offset, WasmValue value) "}], [{"name": "LiftoffAssembler::Spill", "content": "void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueKind kind) {\n  RecordUsedSpillOffset(offset);\n  MemOperand dst = liftoff::GetStackSlot(offset);\n  Str(liftoff::GetRegFromType(reg, kind), dst);\n}", "name_and_para": "void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueKind kind) "}, {"name": "LiftoffAssembler::Spill", "content": "void LiftoffAssembler::Spill(int offset, WasmValue value) {\n  RecordUsedSpillOffset(offset);\n  MemOperand dst = liftoff::GetStackSlot(offset);\n  switch (value.type().kind()) {\n    case kI32: {\n      UseScratchRegisterScope temps(this);\n      Register tmp = temps.Acquire();\n      MacroAssembler::li(tmp, Operand(value.to_i32()));\n      Sw(tmp, dst);\n      break;\n    }\n    case kI64:\n    case kRef:\n    case kRefNull: {\n      UseScratchRegisterScope temps(this);\n      Register tmp = temps.Acquire();\n      MacroAssembler::li(tmp, value.to_i64());\n      Sd(tmp, dst);\n      break;\n    }\n    default:\n      // kWasmF32 and kWasmF64 are unreachable, since those\n      // constants are not tracked.\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::Spill(int offset, WasmValue value) "}], [{"name": "LiftoffAssembler::Move", "content": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) {\n  if (kind == kF32) {\n    Fmov(dst.S(), src.S());\n  } else if (kind == kF64) {\n    Fmov(dst.D(), src.D());\n  } else {\n    DCHECK_EQ(kS128, kind);\n    Mov(dst.Q(), src.Q());\n  }\n}", "name_and_para": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) "}, {"name": "LiftoffAssembler::Move", "content": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) {\n  DCHECK_NE(dst, src);\n  if (kind != kS128) {\n    MacroAssembler::Move(dst, src);\n  } else {\n    VU.set(kScratchReg, E8, m1);\n    MacroAssembler::vmv_vv(dst.toV(), src.toV());\n  }\n}", "name_and_para": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) "}], [{"name": "LiftoffAssembler::Move", "content": "void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) {\n  if (kind == kI32) {\n    Mov(dst.W(), src.W());\n  } else {\n    DCHECK(kI64 == kind || is_reference(kind));\n    Mov(dst.X(), src.X());\n  }\n}", "name_and_para": "void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) "}, {"name": "LiftoffAssembler::Move", "content": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) {\n  DCHECK_NE(dst, src);\n  if (kind != kS128) {\n    MacroAssembler::Move(dst, src);\n  } else {\n    VU.set(kScratchReg, E8, m1);\n    MacroAssembler::vmv_vv(dst.toV(), src.toV());\n  }\n}", "name_and_para": "void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,\n                            ValueKind kind) "}], [{"name": "LiftoffAssembler::MoveStackValue", "content": "void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,\n                                      ValueKind kind) {\n  UseScratchRegisterScope temps(this);\n  CPURegister scratch = liftoff::AcquireByType(&temps, kind);\n  Ldr(scratch, liftoff::GetStackSlot(src_offset));\n  Str(scratch, liftoff::GetStackSlot(dst_offset));\n}", "name_and_para": "void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,\n                                      ValueKind kind) "}, {"name": "LiftoffAssembler::MoveStackValue", "content": "void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,\n                                      ValueKind kind) {\n  DCHECK_NE(dst_offset, src_offset);\n\n  MemOperand src = liftoff::GetStackSlot(src_offset);\n  MemOperand dst = liftoff::GetStackSlot(dst_offset);\n  switch (kind) {\n    case kI32:\n      Lw(kScratchReg, src);\n      Sw(kScratchReg, dst);\n      break;\n    case kI64:\n    case kRef:\n    case kRefNull:\n    case kRtt:\n      Ld(kScratchReg, src);\n      Sd(kScratchReg, dst);\n      break;\n    case kF32:\n      LoadFloat(kScratchDoubleReg, src);\n      StoreFloat(kScratchDoubleReg, dst);\n      break;\n    case kF64:\n      MacroAssembler::LoadDouble(kScratchDoubleReg, src);\n      MacroAssembler::StoreDouble(kScratchDoubleReg, dst);\n      break;\n    case kS128: {\n      VU.set(kScratchReg, E8, m1);\n      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;\n      if (src.offset() != 0) {\n        MacroAssembler::Add64(src_reg, src.rm(), src.offset());\n      }\n      vl(kSimd128ScratchReg, src_reg, 0, E8);\n      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;\n      if (dst.offset() != 0) {\n        Add64(kScratchReg, dst.rm(), dst.offset());\n      }\n      vs(kSimd128ScratchReg, dst_reg, 0, VSew::E8);\n      break;\n    }\n    case kVoid:\n    case kI8:\n    case kI16:\n    case kBottom:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,\n                                      ValueKind kind) "}], [{"name": "LiftoffAssembler::LoadReturnStackSlot", "content": "void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,\n                                           ValueKind kind) {\n  Ldr(liftoff::GetRegFromType(dst, kind), MemOperand(sp, offset));\n}", "name_and_para": "void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,\n                                           ValueKind kind) "}, {"name": "LiftoffAssembler::LoadReturnStackSlot", "content": "void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,\n                                           ValueKind kind) {\n  liftoff::Load(this, dst, MemOperand(sp, offset), kind);\n}", "name_and_para": "void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,\n                                           ValueKind kind) "}], [{"name": "LiftoffAssembler::StoreCallerFrameSlot", "content": "void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,\n                                            uint32_t caller_slot_idx,\n                                            ValueKind kind) {\n  int32_t offset = (caller_slot_idx + 1) * LiftoffAssembler::kStackSlotSize;\n  Str(liftoff::GetRegFromType(src, kind), MemOperand(fp, offset));\n}", "name_and_para": "void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,\n                                            uint32_t caller_slot_idx,\n                                            ValueKind kind) "}, {"name": "LiftoffAssembler::StoreCallerFrameSlot", "content": "void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,\n                                            uint32_t caller_slot_idx,\n                                            ValueKind kind) {\n  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);\n  liftoff::Store(this, fp, offset, src, kind);\n}", "name_and_para": "void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,\n                                            uint32_t caller_slot_idx,\n                                            ValueKind kind) "}], [{"name": "LiftoffAssembler::LoadCallerFrameSlot", "content": "void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,\n                                           uint32_t caller_slot_idx,\n                                           ValueKind kind) {\n  int32_t offset = (caller_slot_idx + 1) * LiftoffAssembler::kStackSlotSize;\n  Ldr(liftoff::GetRegFromType(dst, kind), MemOperand(fp, offset));\n}", "name_and_para": "void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,\n                                           uint32_t caller_slot_idx,\n                                           ValueKind kind) "}, {"name": "LiftoffAssembler::LoadCallerFrameSlot", "content": "void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,\n                                           uint32_t caller_slot_idx,\n                                           ValueKind kind) {\n  MemOperand src(fp, kSystemPointerSize * (caller_slot_idx + 1));\n  liftoff::Load(this, dst, src, kind);\n}", "name_and_para": "void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,\n                                           uint32_t caller_slot_idx,\n                                           ValueKind kind) "}], [{"name": "LiftoffAssembler::AtomicFence", "content": "void LiftoffAssembler::AtomicFence() { Dmb(InnerShareable, BarrierAll); }", "name_and_para": "void LiftoffAssembler::AtomicFence() "}, {"name": "LiftoffAssembler::AtomicFence", "content": "void LiftoffAssembler::AtomicFence() { sync(); }", "name_and_para": "void LiftoffAssembler::AtomicFence() "}], [{"name": "AtomicBinop", "content": "inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,\n                        Register offset_reg, uintptr_t offset_imm,\n                        LiftoffRegister value, LiftoffRegister result,\n                        StoreType type, Binop op) {\n  LiftoffRegList pinned{dst_addr, value, result};\n  if (offset_reg != no_reg) pinned.set(offset_reg);\n  Register store_result = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();\n\n  // {LiftoffCompiler::AtomicBinop} ensures that {result} is unique.\n  DCHECK(result.gp() != value.gp() && result.gp() != dst_addr &&\n         result.gp() != offset_reg);\n\n  UseScratchRegisterScope temps(lasm);\n  Register actual_addr = liftoff::CalculateActualAddress(\n      lasm, temps, dst_addr, offset_reg, offset_imm);\n\n  if (CpuFeatures::IsSupported(LSE)) {\n    CpuFeatureScope scope(lasm, LSE);\n    switch (op) {\n      case Binop::kAnd:\n        switch (type.value()) {\n          case StoreType::kI64Store8:\n          case StoreType::kI32Store8: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ mvn(temp, value.gp().W());\n            __ ldclralb(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store16:\n          case StoreType::kI32Store16: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ mvn(temp, value.gp().W());\n            __ ldclralh(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store32:\n          case StoreType::kI32Store: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ mvn(temp, value.gp().W());\n            __ ldclral(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireX();\n            __ mvn(temp, value.gp());\n            __ ldclral(temp, result.gp(), MemOperand(actual_addr));\n            break;\n          }\n          default:\n            UNREACHABLE();\n        }\n        break;\n      case Binop::kSub:\n        switch (type.value()) {\n          case StoreType::kI64Store8:\n          case StoreType::kI32Store8: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ neg(temp, value.gp().W());\n            __ ldaddalb(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store16:\n          case StoreType::kI32Store16: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ neg(temp, value.gp().W());\n            __ ldaddalh(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store32:\n          case StoreType::kI32Store: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireW();\n            __ neg(temp, value.gp().W());\n            __ ldaddal(temp, result.gp().W(), MemOperand(actual_addr));\n            break;\n          }\n          case StoreType::kI64Store: {\n            UseScratchRegisterScope temps(lasm);\n            Register temp = temps.AcquireX();\n            __ neg(temp, value.gp());\n            __ ldaddal(temp, result.gp(), MemOperand(actual_addr));\n            break;\n          }\n          default:\n            UNREACHABLE();\n        }\n        break;\n#define ATOMIC_BINOP_CASE(op, instr)                                           \\\n  case Binop::op:                                                              \\\n    switch (type.value()) {                                                    \\\n      case StoreType::kI64Store8:                                              \\\n      case StoreType::kI32Store8:                                              \\\n        __ instr##b(value.gp().W(), result.gp().W(), MemOperand(actual_addr)); \\\n        break;                                                                 \\\n      case StoreType::kI64Store16:                                             \\\n      case StoreType::kI32Store16:                                             \\\n        __ instr##h(value.gp().W(), result.gp().W(), MemOperand(actual_addr)); \\\n        break;                                                                 \\\n      case StoreType::kI64Store32:                                             \\\n      case StoreType::kI32Store:                                               \\\n        __ instr(value.gp().W(), result.gp().W(), MemOperand(actual_addr));    \\\n        break;                                                                 \\\n      case StoreType::kI64Store:                                               \\\n        __ instr(value.gp(), result.gp(), MemOperand(actual_addr));            \\\n        break;                                                                 \\\n      default:                                                                 \\\n        UNREACHABLE();                                                         \\\n    }                                                                          \\\n    break;\n        ATOMIC_BINOP_CASE(kAdd, ldaddal)\n        ATOMIC_BINOP_CASE(kOr, ldsetal)\n        ATOMIC_BINOP_CASE(kXor, ldeoral)\n        ATOMIC_BINOP_CASE(kExchange, swpal)\n#undef ATOMIC_BINOP_CASE\n    }\n  } else {\n    // Allocate an additional {temp} register to hold the result that should be\n    // stored to memory. Note that {temp} and {store_result} are not allowed to\n    // be the same register.\n    Register temp = temps.AcquireX();\n\n    Label retry;\n    __ Bind(&retry);\n    switch (type.value()) {\n      case StoreType::kI64Store8:\n      case StoreType::kI32Store8:\n        __ ldaxrb(result.gp().W(), actual_addr);\n        break;\n      case StoreType::kI64Store16:\n      case StoreType::kI32Store16:\n        __ ldaxrh(result.gp().W(), actual_addr);\n        break;\n      case StoreType::kI64Store32:\n      case StoreType::kI32Store:\n        __ ldaxr(result.gp().W(), actual_addr);\n        break;\n      case StoreType::kI64Store:\n        __ ldaxr(result.gp().X(), actual_addr);\n        break;\n      default:\n        UNREACHABLE();\n    }\n\n    switch (op) {\n      case Binop::kAdd:\n        __ add(temp, result.gp(), value.gp());\n        break;\n      case Binop::kSub:\n        __ sub(temp, result.gp(), value.gp());\n        break;\n      case Binop::kAnd:\n        __ and_(temp, result.gp(), value.gp());\n        break;\n      case Binop::kOr:\n        __ orr(temp, result.gp(), value.gp());\n        break;\n      case Binop::kXor:\n        __ eor(temp, result.gp(), value.gp());\n        break;\n      case Binop::kExchange:\n        __ mov(temp, value.gp());\n        break;\n    }\n\n    switch (type.value()) {\n      case StoreType::kI64Store8:\n      case StoreType::kI32Store8:\n        __ stlxrb(store_result.W(), temp.W(), actual_addr);\n        break;\n      case StoreType::kI64Store16:\n      case StoreType::kI32Store16:\n        __ stlxrh(store_result.W(), temp.W(), actual_addr);\n        break;\n      case StoreType::kI64Store32:\n      case StoreType::kI32Store:\n        __ stlxr(store_result.W(), temp.W(), actual_addr);\n        break;\n      case StoreType::kI64Store:\n        __ stlxr(store_result.W(), temp.X(), actual_addr);\n        break;\n      default:\n        UNREACHABLE();\n    }\n\n    __ Cbnz(store_result.W(), &retry);\n  }\n}", "name_and_para": "inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,\n                        Register offset_reg, uintptr_t offset_imm,\n                        LiftoffRegister value, LiftoffRegister result,\n                        StoreType type, Binop op) "}, {"name": "AtomicBinop", "content": "inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,\n                        Register offset_reg, uintptr_t offset_imm,\n                        LiftoffRegister value, LiftoffRegister result,\n                        StoreType type, Binop op) {\n  LiftoffRegList pinned{dst_addr, value, result};\n  if (offset_reg != no_reg) pinned.set(offset_reg);\n  Register store_result = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();\n\n  // Make sure that {result} is unique.\n  Register result_reg = result.gp();\n  if (result_reg == value.gp() || result_reg == dst_addr ||\n      result_reg == offset_reg) {\n    result_reg = __ GetUnusedRegister(kGpReg, pinned).gp();\n  }\n\n  UseScratchRegisterScope temps(lasm);\n  Register actual_addr = liftoff::CalculateActualAddress(\n      lasm, temps, dst_addr, offset_reg, offset_imm);\n\n  // Allocate an additional {temp} register to hold the result that should be\n  // stored to memory. Note that {temp} and {store_result} are not allowed to be\n  // the same register.\n  Register temp = temps.Acquire();\n\n  Label retry;\n  __ bind(&retry);\n  switch (type.value()) {\n    case StoreType::kI64Store8:\n    case StoreType::kI32Store8:\n      __ lbu(result_reg, actual_addr, 0);\n      __ sync();\n      break;\n    case StoreType::kI64Store16:\n    case StoreType::kI32Store16:\n      __ lhu(result_reg, actual_addr, 0);\n      __ sync();\n      break;\n    case StoreType::kI64Store32:\n      __ lr_w(true, false, result_reg, actual_addr);\n      __ ZeroExtendWord(result_reg, result_reg);\n      break;\n    case StoreType::kI32Store:\n      __ lr_w(true, false, result_reg, actual_addr);\n      break;\n    case StoreType::kI64Store:\n      __ lr_d(true, false, result_reg, actual_addr);\n      break;\n    default:\n      UNREACHABLE();\n  }\n\n  switch (op) {\n    case Binop::kAdd:\n      __ add(temp, result_reg, value.gp());\n      break;\n    case Binop::kSub:\n      __ sub(temp, result_reg, value.gp());\n      break;\n    case Binop::kAnd:\n      __ and_(temp, result_reg, value.gp());\n      break;\n    case Binop::kOr:\n      __ or_(temp, result_reg, value.gp());\n      break;\n    case Binop::kXor:\n      __ xor_(temp, result_reg, value.gp());\n      break;\n    case Binop::kExchange:\n      __ mv(temp, value.gp());\n      break;\n  }\n  switch (type.value()) {\n    case StoreType::kI64Store8:\n    case StoreType::kI32Store8:\n      __ sync();\n      __ sb(temp, actual_addr, 0);\n      __ sync();\n      __ mv(store_result, zero_reg);\n      break;\n    case StoreType::kI64Store16:\n    case StoreType::kI32Store16:\n      __ sync();\n      __ sh(temp, actual_addr, 0);\n      __ sync();\n      __ mv(store_result, zero_reg);\n      break;\n    case StoreType::kI64Store32:\n    case StoreType::kI32Store:\n      __ sc_w(false, true, store_result, actual_addr, temp);\n      break;\n    case StoreType::kI64Store:\n      __ sc_d(false, true, store_result, actual_addr, temp);\n      break;\n    default:\n      UNREACHABLE();\n  }\n\n  __ bnez(store_result, &retry);\n  if (result_reg != result.gp()) {\n    __ mv(result.gp(), result_reg);\n  }\n}", "name_and_para": "inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,\n                        Register offset_reg, uintptr_t offset_imm,\n                        LiftoffRegister value, LiftoffRegister result,\n                        StoreType type, Binop op) "}], [{"name": "Binop", "content": "enum class Binop { kAdd, kSub, kAnd, kOr, kXor, kExchange }", "name_and_para": ""}, {"name": "Binop", "content": "enum class Binop { kAdd, kSub, kAnd, kOr, kXor, kExchange }", "name_and_para": ""}], [{"name": "CalculateActualAddress", "content": "inline Register CalculateActualAddress(LiftoffAssembler* lasm,\n                                       UseScratchRegisterScope& temps,\n                                       Register addr_reg, Register offset_reg,\n                                       uintptr_t offset_imm) {\n  DCHECK_NE(addr_reg, no_reg);\n  if (offset_reg == no_reg && offset_imm == 0) return addr_reg;\n  Register result = temps.AcquireX();\n  if (offset_reg == no_reg) {\n    __ Add(result, addr_reg, Operand(offset_imm));\n  } else {\n    __ Add(result, addr_reg, Operand(offset_reg));\n    if (offset_imm != 0) __ Add(result, result, Operand(offset_imm));\n  }\n  return result;\n}", "name_and_para": "inline Register CalculateActualAddress(LiftoffAssembler* lasm,\n                                       UseScratchRegisterScope& temps,\n                                       Register addr_reg, Register offset_reg,\n                                       uintptr_t offset_imm) "}, {"name": "CalculateActualAddress", "content": "inline Register CalculateActualAddress(LiftoffAssembler* lasm,\n                                       UseScratchRegisterScope& temps,\n                                       Register addr_reg, Register offset_reg,\n                                       uintptr_t offset_imm) {\n  DCHECK_NE(addr_reg, no_reg);\n  if (offset_reg == no_reg && offset_imm == 0) return addr_reg;\n  Register result = temps.Acquire();\n  if (offset_reg == no_reg) {\n    __ AddWord(result, addr_reg, Operand(offset_imm));\n  } else {\n    __ AddWord(result, addr_reg, Operand(offset_reg));\n    if (offset_imm != 0) __ AddWord(result, result, Operand(offset_imm));\n  }\n  return result;\n}", "name_and_para": "inline Register CalculateActualAddress(LiftoffAssembler* lasm,\n                                       UseScratchRegisterScope& temps,\n                                       Register addr_reg, Register offset_reg,\n                                       uintptr_t offset_imm) "}], [{"name": "LiftoffAssembler::LoadCodeEntrypointViaCodePointer", "content": "void LiftoffAssembler::LoadCodeEntrypointViaCodePointer(Register dst,\n                                                        Register src_addr,\n                                                        int32_t offset_imm) {\n  UseScratchRegisterScope temps(this);\n  MemOperand src_op =\n      liftoff::GetMemOp(this, &temps, src_addr, no_reg, offset_imm);\n  MacroAssembler::LoadCodeEntrypointViaCodePointer(dst, src_op,\n                                                   kWasmEntrypointTag);\n}", "name_and_para": "void LiftoffAssembler::LoadCodeEntrypointViaCodePointer(Register dst,\n                                                        Register src_addr,\n                                                        int32_t offset_imm) "}, {"name": "LiftoffAssembler::LoadCodeEntrypointViaCodePointer", "content": "void LiftoffAssembler::LoadCodeEntrypointViaCodePointer(Register dst,\n                                                        Register src_addr,\n                                                        int32_t offset_imm) {\n  MemOperand src_op = liftoff::GetMemOp(this, src_addr, no_reg, offset_imm);\n  MacroAssembler::LoadCodeEntrypointViaCodePointer(dst, src_op,\n                                                   kWasmEntrypointTag);\n}", "name_and_para": "void LiftoffAssembler::LoadCodeEntrypointViaCodePointer(Register dst,\n                                                        Register src_addr,\n                                                        int32_t offset_imm) "}], [{"name": "LiftoffAssembler::LoadFullPointer", "content": "void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,\n                                       int32_t offset_imm) {\n  UseScratchRegisterScope temps(this);\n  MemOperand src_op =\n      liftoff::GetMemOp(this, &temps, src_addr, no_reg, offset_imm);\n  Ldr(dst.X(), src_op);\n}", "name_and_para": "void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,\n                                       int32_t offset_imm) "}, {"name": "LiftoffAssembler::LoadFullPointer", "content": "void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,\n                                       int32_t offset_imm) {\n  MemOperand src_op = liftoff::GetMemOp(this, src_addr, no_reg, offset_imm);\n  LoadWord(dst, src_op);\n}", "name_and_para": "void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,\n                                       int32_t offset_imm) "}], [{"name": "LiftoffAssembler::LoadProtectedPointer", "content": "void LiftoffAssembler::LoadProtectedPointer(Register dst, Register src_addr,\n                                            int32_t offset_imm) {\n  LoadProtectedPointerField(dst, MemOperand{src_addr, offset_imm});\n}", "name_and_para": "void LiftoffAssembler::LoadProtectedPointer(Register dst, Register src_addr,\n                                            int32_t offset_imm) "}, {"name": "LiftoffAssembler::LoadProtectedPointer", "content": "void LiftoffAssembler::LoadProtectedPointer(Register dst, Register src_addr,\n                                            int32_t offset_imm) {\n  LoadProtectedPointerField(dst, MemOperand{src_addr, offset_imm});\n}", "name_and_para": "void LiftoffAssembler::LoadProtectedPointer(Register dst, Register src_addr,\n                                            int32_t offset_imm) "}], [{"name": "LiftoffAssembler::LoadTaggedPointer", "content": "void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,\n                                         Register offset_reg,\n                                         int32_t offset_imm,\n                                         uint32_t* protected_load_pc,\n                                         bool needs_shift) {\n  UseScratchRegisterScope temps(this);\n  unsigned shift_amount = !needs_shift ? 0 : COMPRESS_POINTERS_BOOL ? 2 : 3;\n  MemOperand src_op = liftoff::GetMemOp(this, &temps, src_addr, offset_reg,\n                                        offset_imm, false, shift_amount);\n  DCHECK(!src_op.IsPostIndex());  // See MacroAssembler::LoadStoreMacroComplex.\n  constexpr uint8_t kDecompressionInstruction = COMPRESS_POINTERS_BOOL ? 1 : 0;\n  GetProtectedInstruction<LoadOrStore::kLoad, kDecompressionInstruction>\n      collect_protected_load(this, protected_load_pc);\n  LoadTaggedField(dst, src_op);\n}", "name_and_para": "void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,\n                                         Register offset_reg,\n                                         int32_t offset_imm,\n                                         uint32_t* protected_load_pc,\n                                         bool needs_shift) "}, {"name": "LiftoffAssembler::LoadTaggedPointer", "content": "void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,\n                                         Register offset_reg,\n                                         int32_t offset_imm,\n                                         uint32_t* protected_load_pc,\n                                         bool needs_shift) {\n  unsigned shift_amount = !needs_shift ? 0 : COMPRESS_POINTERS_BOOL ? 2 : 3;\n  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm,\n                                        false, shift_amount);\n  Assembler::BlockPoolsScope blocked_pools_scope_(this, 4 * kInstrSize);\n  LoadTaggedField(dst, src_op);\n\n  // Since LoadTaggedField might start with an instruction loading an immediate\n  // argument to a register, we have to compute the {protected_load_pc} after\n  // calling it.\n  // In case of compressed pointers, there is an additional instruction\n  // (pointer decompression) after the load.\n  uint8_t protected_instruction_offset_bias =\n      COMPRESS_POINTERS_BOOL ? 2 * kInstrSize : kInstrSize;\n  if (protected_load_pc) {\n    *protected_load_pc = pc_offset() - protected_instruction_offset_bias;\n    DCHECK(InstructionAt(*protected_load_pc)->IsLoad());\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,\n                                         Register offset_reg,\n                                         int32_t offset_imm,\n                                         uint32_t* protected_load_pc,\n                                         bool needs_shift) "}], [{"name": "LiftoffAssembler::ResetOSRTarget", "content": "void LiftoffAssembler::ResetOSRTarget() {}", "name_and_para": "void LiftoffAssembler::ResetOSRTarget() "}, {"name": "LiftoffAssembler::ResetOSRTarget", "content": "void LiftoffAssembler::ResetOSRTarget() {}", "name_and_para": "void LiftoffAssembler::ResetOSRTarget() "}], [{"name": "LiftoffAssembler::SpillInstanceData", "content": "void LiftoffAssembler::SpillInstanceData(Register instance) {\n  Str(instance, liftoff::GetInstanceDataOperand());\n}", "name_and_para": "void LiftoffAssembler::SpillInstanceData(Register instance) "}, {"name": "LiftoffAssembler::SpillInstanceData", "content": "void LiftoffAssembler::SpillInstanceData(Register instance) {\n  StoreWord(instance, liftoff::GetInstanceDataOperand());\n}", "name_and_para": "void LiftoffAssembler::SpillInstanceData(Register instance) "}], [{"name": "LiftoffAssembler::LoadTaggedPointerFromInstance", "content": "void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,\n                                                     Register instance,\n                                                     int offset) {\n  DCHECK_LE(0, offset);\n  LoadTaggedField(dst, MemOperand{instance, offset});\n}", "name_and_para": "void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,\n                                                     Register instance,\n                                                     int offset) "}, {"name": "LiftoffAssembler::LoadTaggedPointerFromInstance", "content": "void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,\n                                                     Register instance,\n                                                     int offset) {\n  DCHECK_LE(0, offset);\n  LoadTaggedField(dst, MemOperand{instance, offset});\n}", "name_and_para": "void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,\n                                                     Register instance,\n                                                     int offset) "}], [{"name": "LiftoffAssembler::LoadFromInstance", "content": "void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,\n                                        int offset, int size) {\n  DCHECK_LE(0, offset);\n  MemOperand src{instance, offset};\n  switch (size) {\n    case 1:\n      Ldrb(dst.W(), src);\n      break;\n    case 4:\n      Ldr(dst.W(), src);\n      break;\n    case 8:\n      Ldr(dst, src);\n      break;\n    default:\n      UNIMPLEMENTED();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,\n                                        int offset, int size) "}, {"name": "LiftoffAssembler::LoadFromInstance", "content": "void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,\n                                        int offset, int size) {\n  DCHECK_LE(0, offset);\n  MemOperand src{instance, offset};\n  switch (size) {\n    case 1:\n      Lb(dst, MemOperand(src));\n      break;\n    case 4:\n      Lw(dst, MemOperand(src));\n      break;\n    case 8:\n      LoadWord(dst, MemOperand(src));\n      break;\n    default:\n      UNIMPLEMENTED();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,\n                                        int offset, int size) "}], [{"name": "LiftoffAssembler::LoadTrustedPointer", "content": "void LiftoffAssembler::LoadTrustedPointer(Register dst, Register src_addr,\n                                          int offset, IndirectPointerTag tag) {\n  MemOperand src{src_addr, offset};\n  LoadTrustedPointerField(dst, src, tag);\n}", "name_and_para": "void LiftoffAssembler::LoadTrustedPointer(Register dst, Register src_addr,\n                                          int offset, IndirectPointerTag tag) "}, {"name": "LiftoffAssembler::LoadTrustedPointer", "content": "void LiftoffAssembler::LoadTrustedPointer(Register dst, Register src_addr,\n                                          int offset, IndirectPointerTag tag) {\n  MemOperand src{src_addr, offset};\n  LoadTrustedPointerField(dst, src, tag);\n}", "name_and_para": "void LiftoffAssembler::LoadTrustedPointer(Register dst, Register src_addr,\n                                          int offset, IndirectPointerTag tag) "}], [{"name": "LiftoffAssembler::LoadInstanceDataFromFrame", "content": "void LiftoffAssembler::LoadInstanceDataFromFrame(Register dst) {\n  Ldr(dst, liftoff::GetInstanceDataOperand());\n}", "name_and_para": "void LiftoffAssembler::LoadInstanceDataFromFrame(Register dst) "}, {"name": "LiftoffAssembler::LoadInstanceDataFromFrame", "content": "void LiftoffAssembler::LoadInstanceDataFromFrame(Register dst) {\n  LoadWord(dst, liftoff::GetInstanceDataOperand());\n}", "name_and_para": "void LiftoffAssembler::LoadInstanceDataFromFrame(Register dst) "}], [{"name": "LiftoffAssembler::LoadConstant", "content": "void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value) {\n  switch (value.type().kind()) {\n    case kI32:\n      Mov(reg.gp().W(), value.to_i32());\n      break;\n    case kI64:\n      Mov(reg.gp().X(), value.to_i64());\n      break;\n    case kF32:\n      Fmov(reg.fp().S(), value.to_f32());\n      break;\n    case kF64:\n      Fmov(reg.fp().D(), value.to_f64());\n      break;\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value) "}, {"name": "LiftoffAssembler::LoadConstant", "content": "void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value) {\n  switch (value.type().kind()) {\n    case kI32:\n      MacroAssembler::li(reg.gp(), Operand(value.to_i32()));\n      break;\n    case kI64:\n      MacroAssembler::li(reg.gp(), Operand(value.to_i64()));\n      break;\n    case kF32:\n      MacroAssembler::LoadFPRImmediate(reg.fp(),\n                                       value.to_f32_boxed().get_bits());\n      break;\n    case kF64:\n      MacroAssembler::LoadFPRImmediate(reg.fp(),\n                                       value.to_f64_boxed().get_bits());\n      break;\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value) "}], [{"name": "LiftoffAssembler::CheckTierUp", "content": "void LiftoffAssembler::CheckTierUp(int declared_func_index, int budget_used,\n                                   Label* ool_label,\n                                   const FreezeCacheState& frozen) {\n  UseScratchRegisterScope temps{this};\n  Register budget_array = temps.AcquireX();\n\n  Register instance_data = cache_state_.cached_instance_data;\n  if (instance_data == no_reg) {\n    instance_data = budget_array;  // Reuse the temp register.\n    LoadInstanceDataFromFrame(instance_data);\n  }\n\n  constexpr int kArrayOffset = wasm::ObjectAccess::ToTagged(\n      WasmTrustedInstanceData::kTieringBudgetArrayOffset);\n  ldr(budget_array, MemOperand{instance_data, kArrayOffset});\n\n  int budget_arr_offset = kInt32Size * declared_func_index;\n  // If the offset cannot be used in the operand directly, add it once to the\n  // budget_array to avoid doing that two times below.\n  if (!IsImmLSScaled(budget_arr_offset, 2 /* log2(sizeof(i32)) */) &&\n      !IsImmLSUnscaled(budget_arr_offset)) {\n    Add(budget_array, budget_array, budget_arr_offset);\n    budget_arr_offset = 0;\n  }\n\n  Register budget = temps.AcquireW();\n  MemOperand budget_addr{budget_array, budget_arr_offset};\n  ldr(budget, budget_addr);\n  // Make sure that the {budget_used} can be used as an immediate for SUB.\n  if (budget_used > 0xFFF000) {\n    budget_used = 0xFFF000;  // 16'773'120\n  } else if (budget_used > 0xFFF) {\n    budget_used &= 0xFFF000;\n  }\n  DCHECK(IsImmAddSub(budget_used));\n  AddSub(budget, budget, Operand{budget_used}, SetFlags, SUB);\n  str(budget, budget_addr);\n  B(ool_label, mi);\n}", "name_and_para": "void LiftoffAssembler::CheckTierUp(int declared_func_index, int budget_used,\n                                   Label* ool_label,\n                                   const FreezeCacheState& frozen) "}, {"name": "LiftoffAssembler::CheckTierUp", "content": "void LiftoffAssembler::CheckTierUp(int declared_func_index, int budget_used,\n                                   Label* ool_label,\n                                   const FreezeCacheState& frozen) {\n  UseScratchRegisterScope temps(this);\n  Register budget_array = temps.Acquire();\n  Register instance_data = cache_state_.cached_instance_data;\n  if (instance_data == no_reg) {\n    instance_data = budget_array;  // Reuse the scratch register.\n    LoadInstanceDataFromFrame(instance_data);\n  }\n\n  constexpr int kArrayOffset = wasm::ObjectAccess::ToTagged(\n      WasmTrustedInstanceData::kTieringBudgetArrayOffset);\n  LoadWord(budget_array, MemOperand(instance_data, kArrayOffset));\n\n  int budget_arr_offset = kInt32Size * declared_func_index;\n  // Pick a random register from kLiftoffAssemblerGpCacheRegs.\n  // TODO(miladfarca): Use ScratchRegisterScope when available.\n  Register budget = kScratchReg;\n  MemOperand budget_addr(budget_array, budget_arr_offset);\n  Lw(budget, budget_addr);\n  Sub32(budget, budget, Operand{budget_used});\n  Sw(budget, budget_addr);\n  Branch(ool_label, lt, budget, Operand{0});\n}", "name_and_para": "void LiftoffAssembler::CheckTierUp(int declared_func_index, int budget_used,\n                                   Label* ool_label,\n                                   const FreezeCacheState& frozen) "}], [{"name": "LiftoffAssembler::NeedsAlignment", "content": "bool LiftoffAssembler::NeedsAlignment(ValueKind kind) {\n  return kind == kS128 || is_reference(kind);\n}", "name_and_para": "bool LiftoffAssembler::NeedsAlignment(ValueKind kind) "}, {"name": "LiftoffAssembler::NeedsAlignment", "content": "bool LiftoffAssembler::NeedsAlignment(ValueKind kind) {\n  switch (kind) {\n    case kS128:\n      return true;\n    default:\n      // No alignment because all other types are kStackSlotSize.\n      return false;\n  }\n}", "name_and_para": "bool LiftoffAssembler::NeedsAlignment(ValueKind kind) "}], [{"name": "LiftoffAssembler::SlotSizeForType", "content": "int LiftoffAssembler::SlotSizeForType(ValueKind kind) {\n  // TODO(zhin): Unaligned access typically take additional cycles, we should do\n  // some performance testing to see how big an effect it will take.\n  switch (kind) {\n    case kS128:\n      return value_kind_size(kind);\n    default:\n      return kStackSlotSize;\n  }\n}", "name_and_para": "int LiftoffAssembler::SlotSizeForType(ValueKind kind) "}, {"name": "LiftoffAssembler::SlotSizeForType", "content": "int LiftoffAssembler::SlotSizeForType(ValueKind kind) {\n  switch (kind) {\n    case kS128:\n      return value_kind_size(kind);\n    default:\n      return kStackSlotSize;\n  }\n}", "name_and_para": "int LiftoffAssembler::SlotSizeForType(ValueKind kind) "}], [{"name": "LiftoffAssembler::StaticStackFrameSize", "content": "constexpr int LiftoffAssembler::StaticStackFrameSize() {\n  return WasmLiftoffFrameConstants::kFeedbackVectorOffset;\n}", "name_and_para": "constexpr int LiftoffAssembler::StaticStackFrameSize() "}, {"name": "LiftoffAssembler::StaticStackFrameSize", "content": "constexpr int LiftoffAssembler::StaticStackFrameSize() {\n  return WasmLiftoffFrameConstants::kFeedbackVectorOffset;\n}", "name_and_para": "constexpr int LiftoffAssembler::StaticStackFrameSize() "}], [{"name": "LiftoffAssembler::AbortCompilation", "content": "void LiftoffAssembler::AbortCompilation() { AbortedCodeGeneration(); }", "name_and_para": "void LiftoffAssembler::AbortCompilation() "}, {"name": "LiftoffAssembler::AbortCompilation", "content": "void LiftoffAssembler::AbortCompilation() { AbortedCodeGeneration(); }", "name_and_para": "void LiftoffAssembler::AbortCompilation() "}], [{"name": "LiftoffAssembler::FinishCode", "content": "void LiftoffAssembler::FinishCode() { ForceConstantPoolEmissionWithoutJump(); }", "name_and_para": "void LiftoffAssembler::FinishCode() "}, {"name": "LiftoffAssembler::FinishCode", "content": "void LiftoffAssembler::FinishCode() { ForceConstantPoolEmissionWithoutJump(); }", "name_and_para": "void LiftoffAssembler::FinishCode() "}], [{"name": "LiftoffAssembler::PatchPrepareStackFrame", "content": "void LiftoffAssembler::PatchPrepareStackFrame(\n    int offset, SafepointTableBuilder* safepoint_table_builder,\n    bool feedback_vector_slot) {\n  // The frame_size includes the frame marker and the instance slot. Both are\n  // pushed as part of frame construction, so we don't need to allocate memory\n  // for them anymore.\n  int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;\n  // The frame setup builtin also pushes the feedback vector, and an unused\n  // slot for alignment.\n  if (feedback_vector_slot) {\n    frame_size = std::max(frame_size - 2 * kSystemPointerSize, 0);\n  }\n\n  // The stack pointer is required to be quadword aligned.\n  // Misalignment will cause a stack alignment fault.\n  DCHECK_EQ(frame_size, RoundUp(frame_size, kQuadWordSizeInBytes));\n\n  PatchingAssembler patching_assembler(AssemblerOptions{},\n                                       buffer_start_ + offset, 1);\n\n  if (V8_LIKELY(frame_size < 4 * KB)) {\n    // This is the standard case for small frames: just subtract from SP and be\n    // done with it.\n    DCHECK(IsImmAddSub(frame_size));\n    patching_assembler.PatchSubSp(frame_size);\n    return;\n  }\n\n  // The frame size is bigger than 4KB, so we might overflow the available stack\n  // space if we first allocate the frame and then do the stack check (we will\n  // need some remaining stack space for throwing the exception). That's why we\n  // check the available stack space before we allocate the frame. To do this we\n  // replace the {__ sub(sp, sp, framesize)} with a jump to OOL code that does\n  // this \"extended stack check\".\n  //\n  // The OOL code can simply be generated here with the normal assembler,\n  // because all other code generation, including OOL code, has already finished\n  // when {PatchPrepareStackFrame} is called. The function prologue then jumps\n  // to the current {pc_offset()} to execute the OOL code for allocating the\n  // large frame.\n\n  // Emit the unconditional branch in the function prologue (from {offset} to\n  // {pc_offset()}).\n  patching_assembler.b((pc_offset() - offset) >> kInstrSizeLog2);\n\n  // If the frame is bigger than the stack, we throw the stack overflow\n  // exception unconditionally. Thereby we can avoid the integer overflow\n  // check in the condition code.\n  RecordComment(\"OOL: stack check for large frame\");\n  Label continuation;\n  if (frame_size < v8_flags.stack_size * 1024) {\n    UseScratchRegisterScope temps(this);\n    Register stack_limit = temps.AcquireX();\n    LoadStackLimit(stack_limit, StackLimitKind::kRealStackLimit);\n    Add(stack_limit, stack_limit, Operand(frame_size));\n    Cmp(sp, stack_limit);\n    B(hs /* higher or same */, &continuation);\n  }\n\n  Call(static_cast<Address>(Builtin::kWasmStackOverflow),\n       RelocInfo::WASM_STUB_CALL);\n  // The call will not return; just define an empty safepoint.\n  safepoint_table_builder->DefineSafepoint(this);\n  if (v8_flags.debug_code) Brk(0);\n\n  bind(&continuation);\n\n  // Now allocate the stack space. Note that this might do more than just\n  // decrementing the SP; consult {MacroAssembler::Claim}.\n  Claim(frame_size, 1);\n\n  // Jump back to the start of the function, from {pc_offset()} to\n  // right after the reserved space for the {__ sub(sp, sp, framesize)} (which\n  // is a branch now).\n  int func_start_offset = offset + kInstrSize;\n  b((func_start_offset - pc_offset()) >> kInstrSizeLog2);\n}", "name_and_para": "void LiftoffAssembler::PatchPrepareStackFrame(\n    int offset, SafepointTableBuilder* safepoint_table_builder,\n    bool feedback_vector_slot) "}, {"name": "LiftoffAssembler::PatchPrepareStackFrame", "content": "void LiftoffAssembler::PatchPrepareStackFrame(\n    int offset, SafepointTableBuilder* safepoint_table_builder,\n    bool feedback_vector_slot) {\n  // The frame_size includes the frame marker and the instance slot. Both are\n  // pushed as part of frame construction, so we don't need to allocate memory\n  // for them anymore.\n  int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;\n  // The frame setup builtin also pushes the feedback vector.\n  if (feedback_vector_slot) {\n    frame_size -= kSystemPointerSize;\n  }\n  // We can't run out of space, just pass anything big enough to not cause the\n  // assembler to try to grow the buffer.\n  constexpr int kAvailableSpace = 256;\n  MacroAssembler patching_assembler(\n      nullptr, AssemblerOptions{}, CodeObjectRequired::kNo,\n      ExternalAssemblerBuffer(buffer_start_ + offset, kAvailableSpace));\n\n  if (V8_LIKELY(frame_size < 4 * KB)) {\n    // This is the standard case for small frames: just subtract from SP and be\n    // done with it.\n    patching_assembler.AddWord(sp, sp, Operand(-frame_size));\n    return;\n  }\n\n  // The frame size is bigger than 4KB, so we might overflow the available stack\n  // space if we first allocate the frame and then do the stack check (we will\n  // need some remaining stack space for throwing the exception). That's why we\n  // check the available stack space before we allocate the frame. To do this we\n  // replace the {__ AddWord(sp, sp, -frame_size)} with a jump to OOL code that\n  // does this \"extended stack check\".\n  //\n  // The OOL code can simply be generated here with the normal assembler,\n  // because all other code generation, including OOL code, has already finished\n  // when {PatchPrepareStackFrame} is called. The function prologue then jumps\n  // to the current {pc_offset()} to execute the OOL code for allocating the\n  // large frame.\n  // Emit the unconditional branch in the function prologue (from {offset} to\n  // {pc_offset()}).\n\n  int imm32 = pc_offset() - offset;\n  patching_assembler.GenPCRelativeJump(kScratchReg, imm32);\n\n  // If the frame is bigger than the stack, we throw the stack overflow\n  // exception unconditionally. Thereby we can avoid the integer overflow\n  // check in the condition code.\n  RecordComment(\"OOL: stack check for large frame\");\n  Label continuation;\n  if (frame_size < v8_flags.stack_size * 1024) {\n    Register stack_limit = kScratchReg;\n    LoadStackLimit(stack_limit, StackLimitKind::kRealStackLimit);\n    AddWord(stack_limit, stack_limit, Operand(frame_size));\n    Branch(&continuation, uge, sp, Operand(stack_limit));\n  }\n\n  Call(static_cast<Address>(Builtin::kWasmStackOverflow),\n       RelocInfo::WASM_STUB_CALL);\n  // The call will not return; just define an empty safepoint.\n  safepoint_table_builder->DefineSafepoint(this);\n  if (v8_flags.debug_code) stop();\n\n  bind(&continuation);\n\n  // Now allocate the stack space. Note that this might do more than just\n  // decrementing the SP;\n  AddWord(sp, sp, Operand(-frame_size));\n\n  // Jump back to the start of the function, from {pc_offset()} to\n  // right after the reserved space for the {__ AddWord(sp, sp, -framesize)}\n  // (which is a Branch now).\n  int func_start_offset = offset + 2 * kInstrSize;\n  imm32 = func_start_offset - pc_offset();\n  GenPCRelativeJump(kScratchReg, imm32);\n}", "name_and_para": "void LiftoffAssembler::PatchPrepareStackFrame(\n    int offset, SafepointTableBuilder* safepoint_table_builder,\n    bool feedback_vector_slot) "}], [{"name": "LiftoffAssembler::AlignFrameSize", "content": "void LiftoffAssembler::AlignFrameSize() {\n  // The frame_size includes the frame marker. The frame marker has already been\n  // pushed on the stack though, so we don't need to allocate memory for it\n  // anymore.\n  int frame_size = GetTotalFrameSize() - 2 * kSystemPointerSize;\n\n  static_assert(kStackSlotSize == kXRegSize,\n                \"kStackSlotSize must equal kXRegSize\");\n\n  // The stack pointer is required to be quadword aligned.\n  // Misalignment will cause a stack alignment fault.\n  int misalignment = frame_size % kQuadWordSizeInBytes;\n  if (misalignment) {\n    int padding = kQuadWordSizeInBytes - misalignment;\n    frame_size += padding;\n    max_used_spill_offset_ += padding;\n  }\n}", "name_and_para": "void LiftoffAssembler::AlignFrameSize() "}, {"name": "LiftoffAssembler::AlignFrameSize", "content": "void LiftoffAssembler::AlignFrameSize() {}", "name_and_para": "void LiftoffAssembler::AlignFrameSize() "}], [{"name": "LiftoffAssembler::PrepareTailCall", "content": "void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,\n                                       int stack_param_delta) {\n  UseScratchRegisterScope temps(this);\n  temps.Exclude(x16, x17);\n\n  // This is the previous stack pointer value (before we push the lr and the\n  // fp). We need to keep it to autenticate the lr and adjust the new stack\n  // pointer afterwards.\n  Add(x16, fp, 16);\n\n  // Load the fp and lr of the old frame, they will be pushed in the new frame\n  // during the actual call.\n#ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY\n  Ldp(fp, x17, MemOperand(fp));\n  Autib1716();\n  Mov(lr, x17);\n#else\n  Ldp(fp, lr, MemOperand(fp));\n#endif\n\n  temps.Include(x17);\n\n  Register scratch = temps.AcquireX();\n\n  // Shift the whole frame upwards, except for fp and lr.\n  int slot_count = num_callee_stack_params;\n  for (int i = slot_count - 1; i >= 0; --i) {\n    ldr(scratch, MemOperand(sp, i * 8));\n    str(scratch, MemOperand(x16, (i - stack_param_delta) * 8));\n  }\n\n  // Set the new stack pointer.\n  Sub(sp, x16, stack_param_delta * 8);\n}", "name_and_para": "void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,\n                                       int stack_param_delta) "}, {"name": "LiftoffAssembler::PrepareTailCall", "content": "void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,\n                                       int stack_param_delta) {\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n\n  // Push the return address and frame pointer to complete the stack frame.\n  LoadWord(scratch, MemOperand(fp, kSystemPointerSize));\n  Push(scratch);\n  LoadWord(scratch, MemOperand(fp, 0));\n  Push(scratch);\n\n  // Shift the whole frame upwards.\n  int slot_count = num_callee_stack_params + 2;\n  for (int i = slot_count - 1; i >= 0; --i) {\n    LoadWord(scratch, MemOperand(sp, i * kSystemPointerSize));\n    StoreWord(scratch,\n              MemOperand(fp, (i - stack_param_delta) * kSystemPointerSize));\n  }\n\n  // Set the new stack and frame pointer.\n  AddWord(sp, fp, -stack_param_delta * kSystemPointerSize);\n  Pop(ra, fp);\n}", "name_and_para": "void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,\n                                       int stack_param_delta) "}], [{"name": "LiftoffAssembler::CallFrameSetupStub", "content": "void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {\n  // TODO(jkummerow): Enable this check when we have C++20.\n  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),\n  //                         std::end(wasm::kGpParamRegisters),\n  //                         kLiftoffFrameSetupFunctionReg) ==\n  //                         std::end(wasm::kGpParamRegisters));\n\n  // On ARM64, we must push at least {lr} before calling the stub, otherwise\n  // it would get clobbered with no possibility to recover it. So just set\n  // up the frame here.\n  EnterFrame(StackFrame::WASM);\n  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),\n               WasmValue(declared_function_index));\n  CallBuiltin(Builtin::kWasmLiftoffFrameSetup);\n}", "name_and_para": "void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) "}, {"name": "LiftoffAssembler::CallFrameSetupStub", "content": "void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) {\n  // TODO(jkummerow): Enable this check when we have C++20.\n  // static_assert(std::find(std::begin(wasm::kGpParamRegisters),\n  //                         std::end(wasm::kGpParamRegisters),\n  //                         kLiftoffFrameSetupFunctionReg) ==\n  //                         std::end(wasm::kGpParamRegisters));\n\n  // On MIPS64, we must push at least {ra} before calling the stub, otherwise\n  // it would get clobbered with no possibility to recover it. So just set\n  // up the frame here.\n  EnterFrame(StackFrame::WASM);\n  LoadConstant(LiftoffRegister(kLiftoffFrameSetupFunctionReg),\n               WasmValue(declared_function_index));\n  CallBuiltin(Builtin::kWasmLiftoffFrameSetup);\n}", "name_and_para": "void LiftoffAssembler::CallFrameSetupStub(int declared_function_index) "}], [{"name": "LiftoffAssembler::PrepareStackFrame", "content": "int LiftoffAssembler::PrepareStackFrame() {\n  int offset = pc_offset();\n  InstructionAccurateScope scope(this, 1);\n  // Next we reserve the memory for the whole stack frame. We do not know yet\n  // how big the stack frame will be so we just emit a placeholder instruction.\n  // PatchPrepareStackFrame will patch this in order to increase the stack\n  // appropriately.\n  sub(sp, sp, 0);\n  return offset;\n}", "name_and_para": "int LiftoffAssembler::PrepareStackFrame() "}, {"name": "LiftoffAssembler::PrepareStackFrame", "content": "int LiftoffAssembler::PrepareStackFrame() {\n  int offset = pc_offset();\n  // When the frame size is bigger than 4KB, we need two instructions for\n  // stack checking, so we reserve space for this case.\n  addi(sp, sp, 0);\n  nop();\n  nop();\n  return offset;\n}", "name_and_para": "int LiftoffAssembler::PrepareStackFrame() "}], [{"name": "StoreToMemory", "content": "inline void StoreToMemory(LiftoffAssembler* assm, MemOperand dst,\n                          const LiftoffAssembler::VarState& src) {\n  UseScratchRegisterScope temps{assm};\n  CPURegister src_reg = LoadToRegister(assm, &temps, src);\n  assm->Str(src_reg, dst);\n}", "name_and_para": "inline void StoreToMemory(LiftoffAssembler* assm, MemOperand dst,\n                          const LiftoffAssembler::VarState& src) "}, {"name": "StoreToMemory", "content": "inline void StoreToMemory(LiftoffAssembler* assm, MemOperand dst,\n                          const LiftoffAssembler::VarState& src) {\n  UseScratchRegisterScope temps(assm);\n  if (src.is_const()) {\n    Register src_reg = no_reg;\n    if (src.i32_const() == 0) {\n      src_reg = zero_reg;\n    } else {\n      src_reg = temps.Acquire();\n      assm->li(src_reg, src.i32_const());\n    }\n    assm->StoreWord(src_reg, dst);\n  } else if (src.is_reg()) {\n    switch (src.kind()) {\n      case kI32:\n        return assm->Sw(src.reg().gp(), dst);\n      case kI64:\n      case kRef:\n      case kRefNull:\n      case kRtt:\n        return assm->Sd(src.reg().gp(), dst);\n      case kF32:\n        return assm->StoreFloat(src.reg().fp(), dst);\n      case kF64:\n        return assm->StoreDouble(src.reg().fp(), dst);\n      case kS128: {\n        assm->VU.set(kScratchReg, E8, m1);\n        Register dst_reg = temps.Acquire();\n        assm->Add64(dst_reg, dst.rm(), dst.offset());\n        assm->vs(src.reg().fp().toV(), dst_reg, 0, VSew::E8);\n        return;\n      }\n      default:\n        UNREACHABLE();\n    }\n  } else {\n    DCHECK(src.is_stack());\n    Register temp = temps.Acquire();\n    switch (src.kind()) {\n      case kI32:\n        assm->Lw(temp, GetStackSlot(src.offset()));\n        assm->Sw(temp, dst);\n        return;\n      case kI64:\n      case kRef:\n      case kRefNull:\n        assm->Ld(temp, GetStackSlot(src.offset()));\n        assm->Sd(temp, dst);\n        return;\n      case kF32:\n        assm->LoadFloat(kScratchDoubleReg, GetStackSlot(src.offset()));\n        assm->StoreFloat(kScratchDoubleReg, dst);\n        return;\n      case kF64:\n        assm->LoadDouble(kScratchDoubleReg, GetStackSlot(src.offset()));\n        assm->StoreDouble(kScratchDoubleReg, dst);\n        return;\n      case kS128: {\n        assm->VU.set(kScratchReg, E8, m1);\n        Register src_reg = temp;\n        assm->Add64(src_reg, sp, src.offset());\n        assm->vl(kScratchDoubleReg.toV(), src_reg, 0, VSew::E8);\n        Register dst_reg = temp;\n        assm->Add64(dst_reg, dst.rm(), dst.offset());\n        assm->vs(kScratchDoubleReg.toV(), dst_reg, 0, VSew::E8);\n        return;\n      }\n      default:\n        UNREACHABLE();\n    }\n  }\n}", "name_and_para": "inline void StoreToMemory(LiftoffAssembler* assm, MemOperand dst,\n                          const LiftoffAssembler::VarState& src) "}], [{"name": "GetInstanceDataOperand", "content": "inline MemOperand GetInstanceDataOperand() {\n  return GetStackSlot(WasmLiftoffFrameConstants::kInstanceDataOffset);\n}", "name_and_para": "inline MemOperand GetInstanceDataOperand() "}, {"name": "GetInstanceDataOperand", "content": "inline MemOperand GetInstanceDataOperand() {\n  return GetStackSlot(WasmLiftoffFrameConstants::kInstanceDataOffset);\n}", "name_and_para": "inline MemOperand GetInstanceDataOperand() "}], [{"name": "GetStackSlot", "content": "inline MemOperand GetStackSlot(int offset) { return MemOperand(fp, -offset); }", "name_and_para": "inline MemOperand GetStackSlot(int offset) "}, {"name": "GetStackSlot", "content": "inline MemOperand GetStackSlot(int offset) { return MemOperand(fp, -offset); }", "name_and_para": "inline MemOperand GetStackSlot(int offset) "}]]], [["./v8/src/baseline/riscv/baseline-assembler-riscv-inl.h", "./v8/src/baseline/arm64/baseline-assembler-arm64-inl.h"], 0.8266666666666667, 0.9117647058823529, [[{"name": "EnsureAccumulatorPreservedScope::AssertEqualToAccumulator", "content": "inline void EnsureAccumulatorPreservedScope::AssertEqualToAccumulator(\n    Register reg) {\n  assembler_->masm()->CmpTagged(reg, kInterpreterAccumulatorRegister);\n  assembler_->masm()->Assert(eq, AbortReason::kAccumulatorClobbered);\n}", "name_and_para": "inline void EnsureAccumulatorPreservedScope::AssertEqualToAccumulator(\n    Register reg) "}, {"name": "EnsureAccumulatorPreservedScope::AssertEqualToAccumulator", "content": "inline void EnsureAccumulatorPreservedScope::AssertEqualToAccumulator(\n    Register reg) {\n  assembler_->masm()->Assert(eq, AbortReason::kAccumulatorClobbered, reg,\n                             Operand(kInterpreterAccumulatorRegister));\n}", "name_and_para": "inline void EnsureAccumulatorPreservedScope::AssertEqualToAccumulator(\n    Register reg) "}], [{"name": "BaselineAssembler::EmitReturn", "content": "void BaselineAssembler::EmitReturn(MacroAssembler* masm) {\n  ASM_CODE_COMMENT(masm);\n  BaselineAssembler basm(masm);\n\n  Register weight = BaselineLeaveFrameDescriptor::WeightRegister();\n  Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();\n\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Update Interrupt Budget\");\n\n    Label skip_interrupt_label;\n    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);\n    __ masm()->SmiTag(params_size);\n    __ masm()->Push(params_size, kInterpreterAccumulatorRegister);\n\n    __ LoadContext(kContextRegister);\n    __ LoadFunction(kJSFunctionRegister);\n    __ masm()->PushArgument(kJSFunctionRegister);\n    __ CallRuntime(Runtime::kBytecodeBudgetInterrupt_Sparkplug, 1);\n\n    __ masm()->Pop(kInterpreterAccumulatorRegister, params_size);\n    __ masm()->SmiUntag(params_size);\n\n  __ Bind(&skip_interrupt_label);\n  }\n\n  BaselineAssembler::ScratchRegisterScope temps(&basm);\n  Register actual_params_size = temps.AcquireScratch();\n  // Compute the size of the actual parameters + receiver (in bytes).\n  __ Move(actual_params_size,\n          MemOperand(fp, StandardFrameConstants::kArgCOffset));\n\n  // If actual is bigger than formal, then we should use it to free up the stack\n  // arguments.\n  __ masm()->Cmp(params_size, actual_params_size);\n  __ masm()->Csel(params_size, actual_params_size, params_size, kLessThan);\n\n  // Leave the frame (also dropping the register file).\n  __ masm()->LeaveFrame(StackFrame::BASELINE);\n\n  // Drop receiver + arguments.\n  __ masm()->DropArguments(params_size, MacroAssembler::kCountIncludesReceiver);\n  __ masm()->Ret();\n}", "name_and_para": "void BaselineAssembler::EmitReturn(MacroAssembler* masm) "}, {"name": "BaselineAssembler::EmitReturn", "content": "void BaselineAssembler::EmitReturn(MacroAssembler* masm) {\n  ASM_CODE_COMMENT(masm);\n  BaselineAssembler basm(masm);\n\n  Register weight = BaselineLeaveFrameDescriptor::WeightRegister();\n  Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();\n\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Update Interrupt Budget\");\n\n    Label skip_interrupt_label;\n    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);\n    __ masm()->SmiTag(params_size);\n    __ masm()->Push(params_size, kInterpreterAccumulatorRegister);\n\n    __ LoadContext(kContextRegister);\n    __ LoadFunction(kJSFunctionRegister);\n    __ masm()->Push(kJSFunctionRegister);\n    __ CallRuntime(Runtime::kBytecodeBudgetInterrupt_Sparkplug, 1);\n\n    __ masm()->Pop(params_size, kInterpreterAccumulatorRegister);\n    __ masm()->SmiUntag(params_size);\n\n    __ Bind(&skip_interrupt_label);\n  }\n\n  BaselineAssembler::ScratchRegisterScope temps(&basm);\n  Register actual_params_size = temps.AcquireScratch();\n  // Compute the size of the actual parameters + receiver (in bytes).\n  __ Move(actual_params_size,\n          MemOperand(fp, StandardFrameConstants::kArgCOffset));\n\n  // If actual is bigger than formal, then we should use it to free up the stack\n  // arguments.\n  Label corrected_args_count;\n  __ masm()->Branch(&corrected_args_count, ge, params_size,\n                    Operand(actual_params_size), Label::Distance::kNear);\n  __ masm()->Move(params_size, actual_params_size);\n  __ Bind(&corrected_args_count);\n\n  // Leave the frame (also dropping the register file).\n  __ masm()->LeaveFrame(StackFrame::BASELINE);\n\n  // Drop receiver + arguments.\n  __ masm()->DropArguments(params_size, MacroAssembler::kCountIsInteger,\n                           MacroAssembler::kCountIncludesReceiver);\n  __ masm()->Ret();\n}", "name_and_para": "void BaselineAssembler::EmitReturn(MacroAssembler* masm) "}], [{"name": "BaselineAssembler::Switch", "content": "void BaselineAssembler::Switch(Register reg, int case_value_base,\n                               Label** labels, int num_labels) {\n  ASM_CODE_COMMENT(masm_);\n  Label fallthrough;\n  if (case_value_base != 0) {\n    __ Sub(reg, reg, Immediate(case_value_base));\n  }\n\n  // Mostly copied from code-generator-arm64.cc\n  ScratchRegisterScope scope(this);\n  Register temp = scope.AcquireScratch();\n  Label table;\n  JumpIf(kUnsignedGreaterThanEqual, reg, num_labels, &fallthrough);\n  __ Adr(temp, &table);\n  int entry_size_log2 = 2;\n#ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY\n  ++entry_size_log2;  // Account for BTI.\n  constexpr int instructions_per_jump_target = 1;\n#else\n  constexpr int instructions_per_jump_target = 0;\n#endif\n  constexpr int instructions_per_label = 1 + instructions_per_jump_target;\n  __ Add(temp, temp, Operand(reg, UXTW, entry_size_log2));\n  __ Br(temp);\n  {\n    const int instruction_count =\n        num_labels * instructions_per_label + instructions_per_jump_target;\n    MacroAssembler::BlockPoolsScope block_pools(masm_,\n                                                instruction_count * kInstrSize);\n    __ Bind(&table);\n    for (int i = 0; i < num_labels; ++i) {\n      __ JumpTarget();\n      __ B(labels[i]);\n    }\n    __ JumpTarget();\n    __ Bind(&fallthrough);\n  }\n}", "name_and_para": "void BaselineAssembler::Switch(Register reg, int case_value_base,\n                               Label** labels, int num_labels) "}, {"name": "BaselineAssembler::Switch", "content": "void BaselineAssembler::Switch(Register reg, int case_value_base,\n                               Label** labels, int num_labels) {\n  ASM_CODE_COMMENT(masm_);\n  Label fallthrough;\n  if (case_value_base != 0) {\n    __ SubWord(reg, reg, Operand(case_value_base));\n  }\n\n  // Mostly copied from code-generator-riscv64.cc\n  ScratchRegisterScope scope(this);\n  Label table;\n  __ Branch(&fallthrough, kUnsignedGreaterThanEqual, reg, Operand(num_labels));\n  int64_t imm64;\n  imm64 = __ branch_long_offset(&table);\n  CHECK(is_int32(imm64 + 0x800));\n  int32_t Hi20 = (((int32_t)imm64 + 0x800) >> 12);\n  int32_t Lo12 = (int32_t)imm64 << 20 >> 20;\n  __ BlockTrampolinePoolFor(2);\n  __ auipc(t6, Hi20);     // Read PC + Hi20 into t6\n  __ addi(t6, t6, Lo12);  // jump PC + Hi20 + Lo12\n\n  int entry_size_log2 = 3;\n  __ BlockTrampolinePoolFor(num_labels * 2 + 5);\n  __ CalcScaledAddress(t6, t6, reg, entry_size_log2);\n  __ Jump(t6);\n  {\n    __ bind(&table);\n    for (int i = 0; i < num_labels; ++i) {\n      __ BranchLong(labels[i]);\n    }\n    DCHECK_EQ(num_labels * 2, __ InstructionsGeneratedSince(&table));\n  }\n  __ bind(&fallthrough);\n}", "name_and_para": "void BaselineAssembler::Switch(Register reg, int case_value_base,\n                               Label** labels, int num_labels) "}], [{"name": "BaselineAssembler::Word32And", "content": "void BaselineAssembler::Word32And(Register output, Register lhs, int rhs) {\n  __ And(output, lhs, Immediate(rhs));\n}", "name_and_para": "void BaselineAssembler::Word32And(Register output, Register lhs, int rhs) "}, {"name": "BaselineAssembler::Word32And", "content": "void BaselineAssembler::Word32And(Register output, Register lhs, int rhs) {\n  __ And(output, lhs, Operand(rhs));\n}", "name_and_para": "void BaselineAssembler::Word32And(Register output, Register lhs, int rhs) "}], [{"name": "BaselineAssembler::IncrementSmi", "content": "void BaselineAssembler::IncrementSmi(MemOperand lhs) {\n  BaselineAssembler::ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  if (SmiValuesAre31Bits()) {\n    tmp = tmp.W();\n  }\n  __ Ldr(tmp, lhs);\n  __ Add(tmp, tmp, Operand(Smi::FromInt(1)));\n  __ Str(tmp, lhs);\n}", "name_and_para": "void BaselineAssembler::IncrementSmi(MemOperand lhs) "}, {"name": "BaselineAssembler::IncrementSmi", "content": "void BaselineAssembler::IncrementSmi(MemOperand lhs) {\n  BaselineAssembler::ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  ASM_CODE_COMMENT(masm_);\n  if (SmiValuesAre31Bits()) {\n    __ Lw(tmp, lhs);\n    __ Add32(tmp, tmp, Operand(Smi::FromInt(1)));\n    __ Sw(tmp, lhs);\n  } else {\n    __ LoadWord(tmp, lhs);\n    __ AddWord(tmp, tmp, Operand(Smi::FromInt(1)));\n    __ StoreWord(tmp, lhs);\n  }\n}", "name_and_para": "void BaselineAssembler::IncrementSmi(MemOperand lhs) "}], [{"name": "BaselineAssembler::StaModuleVariable", "content": "void BaselineAssembler::StaModuleVariable(Register context, Register value,\n                                          int cell_index, uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(context, context, Context::kExtensionOffset);\n  LoadTaggedField(context, context, SourceTextModule::kRegularExportsOffset);\n\n  // The actual array index is (cell_index - 1).\n  cell_index -= 1;\n  LoadFixedArrayElement(context, context, cell_index);\n  StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);\n}", "name_and_para": "void BaselineAssembler::StaModuleVariable(Register context, Register value,\n                                          int cell_index, uint32_t depth) "}, {"name": "BaselineAssembler::StaModuleVariable", "content": "void BaselineAssembler::StaModuleVariable(Register context, Register value,\n                                          int cell_index, uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(context, context, Context::kExtensionOffset);\n  LoadTaggedField(context, context, SourceTextModule::kRegularExportsOffset);\n\n  // The actual array index is (cell_index - 1).\n  cell_index -= 1;\n  LoadFixedArrayElement(context, context, cell_index);\n  StoreTaggedFieldWithWriteBarrier(context, Cell::kValueOffset, value);\n}", "name_and_para": "void BaselineAssembler::StaModuleVariable(Register context, Register value,\n                                          int cell_index, uint32_t depth) "}], [{"name": "BaselineAssembler::LdaModuleVariable", "content": "void BaselineAssembler::LdaModuleVariable(Register context, int cell_index,\n                                          uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(context, context, Context::kExtensionOffset);\n  if (cell_index > 0) {\n    LoadTaggedField(context, context, SourceTextModule::kRegularExportsOffset);\n    // The actual array index is (cell_index - 1).\n    cell_index -= 1;\n  } else {\n    LoadTaggedField(context, context, SourceTextModule::kRegularImportsOffset);\n    // The actual array index is (-cell_index - 1).\n    cell_index = -cell_index - 1;\n  }\n  LoadFixedArrayElement(context, context, cell_index);\n  LoadTaggedField(kInterpreterAccumulatorRegister, context, Cell::kValueOffset);\n}", "name_and_para": "void BaselineAssembler::LdaModuleVariable(Register context, int cell_index,\n                                          uint32_t depth) "}, {"name": "BaselineAssembler::LdaModuleVariable", "content": "void BaselineAssembler::LdaModuleVariable(Register context, int cell_index,\n                                          uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(context, context, Context::kExtensionOffset);\n  if (cell_index > 0) {\n    LoadTaggedField(context, context, SourceTextModule::kRegularExportsOffset);\n    // The actual array index is (cell_index - 1).\n    cell_index -= 1;\n  } else {\n    LoadTaggedField(context, context, SourceTextModule::kRegularImportsOffset);\n    // The actual array index is (-cell_index - 1).\n    cell_index = -cell_index - 1;\n  }\n  LoadFixedArrayElement(context, context, cell_index);\n  LoadTaggedField(kInterpreterAccumulatorRegister, context, Cell::kValueOffset);\n}", "name_and_para": "void BaselineAssembler::LdaModuleVariable(Register context, int cell_index,\n                                          uint32_t depth) "}], [{"name": "BaselineAssembler::StaContextSlot", "content": "void BaselineAssembler::StaContextSlot(Register context, Register value,\n                                       uint32_t index, uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  StoreTaggedFieldWithWriteBarrier(context, Context::OffsetOfElementAt(index),\n                                   value);\n}", "name_and_para": "void BaselineAssembler::StaContextSlot(Register context, Register value,\n                                       uint32_t index, uint32_t depth) "}, {"name": "BaselineAssembler::StaContextSlot", "content": "void BaselineAssembler::StaContextSlot(Register context, Register value,\n                                       uint32_t index, uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  StoreTaggedFieldWithWriteBarrier(context, Context::OffsetOfElementAt(index),\n                                   value);\n}", "name_and_para": "void BaselineAssembler::StaContextSlot(Register context, Register value,\n                                       uint32_t index, uint32_t depth) "}], [{"name": "BaselineAssembler::LdaContextSlot", "content": "void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,\n                                       uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(kInterpreterAccumulatorRegister, context,\n                  Context::OffsetOfElementAt(index));\n}", "name_and_para": "void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,\n                                       uint32_t depth) "}, {"name": "BaselineAssembler::LdaContextSlot", "content": "void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,\n                                       uint32_t depth) {\n  for (; depth > 0; --depth) {\n    LoadTaggedField(context, context, Context::kPreviousOffset);\n  }\n  LoadTaggedField(kInterpreterAccumulatorRegister, context,\n                  Context::OffsetOfElementAt(index));\n}", "name_and_para": "void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,\n                                       uint32_t depth) "}], [{"name": "BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded", "content": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope scratch_scope(this);\n  Register feedback_cell = scratch_scope.AcquireScratch();\n  LoadFeedbackCell(feedback_cell);\n\n  Register interrupt_budget = scratch_scope.AcquireScratch().W();\n  __ Ldr(interrupt_budget,\n         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  // Remember to set flags as part of the add!\n  __ Adds(interrupt_budget, interrupt_budget, weight.W());\n  __ Str(interrupt_budget,\n         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  if (skip_interrupt_label) __ B(ge, skip_interrupt_label);\n}", "name_and_para": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) "}, {"name": "BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded", "content": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope scratch_scope(this);\n  Register feedback_cell = scratch_scope.AcquireScratch();\n  LoadFeedbackCell(feedback_cell);\n\n  Register interrupt_budget = scratch_scope.AcquireScratch();\n  __ Lw(interrupt_budget,\n        FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  // Remember to set flags as part of the add!\n  __ Add32(interrupt_budget, interrupt_budget, weight);\n  __ Sw(interrupt_budget,\n        FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  if (skip_interrupt_label) {\n    __ Branch(skip_interrupt_label, ge, interrupt_budget, Operand(zero_reg));\n  }\n}", "name_and_para": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) "}], [{"name": "BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded", "content": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    int32_t weight, Label* skip_interrupt_label) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope scratch_scope(this);\n  Register feedback_cell = scratch_scope.AcquireScratch();\n  LoadFeedbackCell(feedback_cell);\n\n  Register interrupt_budget = scratch_scope.AcquireScratch().W();\n  __ Ldr(interrupt_budget,\n         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  // Remember to set flags as part of the add!\n  __ Adds(interrupt_budget, interrupt_budget, weight);\n  __ Str(interrupt_budget,\n         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  if (skip_interrupt_label) {\n    // Use compare flags set by Adds\n    DCHECK_LT(weight, 0);\n    __ B(ge, skip_interrupt_label);\n  }\n}", "name_and_para": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    int32_t weight, Label* skip_interrupt_label) "}, {"name": "BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded", "content": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope scratch_scope(this);\n  Register feedback_cell = scratch_scope.AcquireScratch();\n  LoadFeedbackCell(feedback_cell);\n\n  Register interrupt_budget = scratch_scope.AcquireScratch();\n  __ Lw(interrupt_budget,\n        FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  // Remember to set flags as part of the add!\n  __ Add32(interrupt_budget, interrupt_budget, weight);\n  __ Sw(interrupt_budget,\n        FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));\n  if (skip_interrupt_label) {\n    __ Branch(skip_interrupt_label, ge, interrupt_budget, Operand(zero_reg));\n  }\n}", "name_and_para": "void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(\n    Register weight, Label* skip_interrupt_label) "}], [{"name": "BaselineAssembler::TryLoadOptimizedOsrCode", "content": "void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,\n                                                Register feedback_vector,\n                                                FeedbackSlot slot,\n                                                Label* on_result,\n                                                Label::Distance) {\n  __ TryLoadOptimizedOsrCode(scratch_and_result, CodeKind::MAGLEV,\n                             feedback_vector, slot, on_result,\n                             Label::Distance::kFar);\n}", "name_and_para": "void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,\n                                                Register feedback_vector,\n                                                FeedbackSlot slot,\n                                                Label* on_result,\n                                                Label::Distance) "}, {"name": "BaselineAssembler::TryLoadOptimizedOsrCode", "content": "void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,\n                                                Register feedback_vector,\n                                                FeedbackSlot slot,\n                                                Label* on_result,\n                                                Label::Distance distance) {\n  Label fallthrough, clear_slot;\n  LoadTaggedField(scratch_and_result, feedback_vector,\n                  FeedbackVector::OffsetOfElementAt(slot.ToInt()));\n  __ LoadWeakValue(scratch_and_result, scratch_and_result, &fallthrough);\n\n  // Is it marked_for_deoptimization? If yes, clear the slot.\n  {\n    ScratchRegisterScope temps(this);\n    // The entry references a CodeWrapper object. Unwrap it now.\n    __ LoadCodePointerField(\n        scratch_and_result,\n        FieldMemOperand(scratch_and_result, CodeWrapper::kCodeOffset));\n\n    __ JumpIfCodeIsMarkedForDeoptimization(scratch_and_result,\n                                           temps.AcquireScratch(), &clear_slot);\n    Jump(on_result, distance);\n  }\n\n  __ bind(&clear_slot);\n  __ li(scratch_and_result, __ ClearedValue());\n  StoreTaggedFieldNoWriteBarrier(\n      feedback_vector, FeedbackVector::OffsetOfElementAt(slot.ToInt()),\n      scratch_and_result);\n\n  __ bind(&fallthrough);\n  Move(scratch_and_result, 0);\n}", "name_and_para": "void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,\n                                                Register feedback_vector,\n                                                FeedbackSlot slot,\n                                                Label* on_result,\n                                                Label::Distance distance) "}], [{"name": "BaselineAssembler::StoreTaggedFieldNoWriteBarrier", "content": "void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,\n                                                       int offset,\n                                                       Register value) {\n  __ StoreTaggedField(value, FieldMemOperand(target, offset));\n}", "name_and_para": "void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,\n                                                       int offset,\n                                                       Register value) "}, {"name": "BaselineAssembler::StoreTaggedFieldNoWriteBarrier", "content": "void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,\n                                                       int offset,\n                                                       Register value) {\n  __ StoreTaggedField(value, FieldMemOperand(target, offset));\n}", "name_and_para": "void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,\n                                                       int offset,\n                                                       Register value) "}], [{"name": "BaselineAssembler::StoreTaggedFieldWithWriteBarrier", "content": "void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,\n                                                         int offset,\n                                                         Register value) {\n  ASM_CODE_COMMENT(masm_);\n  __ StoreTaggedField(value, FieldMemOperand(target, offset));\n  __ RecordWriteField(target, offset, value, kLRHasNotBeenSaved,\n                      SaveFPRegsMode::kIgnore);\n}", "name_and_para": "void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,\n                                                         int offset,\n                                                         Register value) "}, {"name": "BaselineAssembler::StoreTaggedFieldWithWriteBarrier", "content": "void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,\n                                                         int offset,\n                                                         Register value) {\n  ASM_CODE_COMMENT(masm_);\n  __ StoreTaggedField(value, FieldMemOperand(target, offset));\n  __ RecordWriteField(target, offset, value, kRAHasNotBeenSaved,\n                      SaveFPRegsMode::kIgnore);\n}", "name_and_para": "void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,\n                                                         int offset,\n                                                         Register value) "}], [{"name": "BaselineAssembler::StoreTaggedSignedField", "content": "void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,\n                                               Tagged<Smi> value) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  __ Mov(tmp, Operand(value));\n  __ StoreTaggedField(tmp, FieldMemOperand(target, offset));\n}", "name_and_para": "void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,\n                                               Tagged<Smi> value) "}, {"name": "BaselineAssembler::StoreTaggedSignedField", "content": "void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,\n                                               Tagged<Smi> value) {\n  ASM_CODE_COMMENT(masm_);\n  ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  __ li(tmp, Operand(value));\n  __ StoreTaggedField(tmp, FieldMemOperand(target, offset));\n}", "name_and_para": "void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,\n                                               Tagged<Smi> value) "}], [{"name": "BaselineAssembler::LoadWord8Field", "content": "void BaselineAssembler::LoadWord8Field(Register output, Register source,\n                                       int offset) {\n  __ Ldrb(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadWord8Field(Register output, Register source,\n                                       int offset) "}, {"name": "BaselineAssembler::LoadWord8Field", "content": "void BaselineAssembler::LoadWord8Field(Register output, Register source,\n                                       int offset) {\n  __ Lb(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadWord8Field(Register output, Register source,\n                                       int offset) "}], [{"name": "BaselineAssembler::LoadWord16FieldZeroExtend", "content": "void BaselineAssembler::LoadWord16FieldZeroExtend(Register output,\n                                                  Register source, int offset) {\n  __ Ldrh(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadWord16FieldZeroExtend(Register output,\n                                                  Register source, int offset) "}, {"name": "BaselineAssembler::LoadWord16FieldZeroExtend", "content": "void BaselineAssembler::LoadWord16FieldZeroExtend(Register output,\n                                                  Register source, int offset) {\n  __ Lhu(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadWord16FieldZeroExtend(Register output,\n                                                  Register source, int offset) "}], [{"name": "BaselineAssembler::LoadTaggedSignedFieldAndUntag", "content": "void BaselineAssembler::LoadTaggedSignedFieldAndUntag(Register output,\n                                                      Register source,\n                                                      int offset) {\n  LoadTaggedSignedField(output, source, offset);\n  SmiUntag(output);\n}", "name_and_para": "void BaselineAssembler::LoadTaggedSignedFieldAndUntag(Register output,\n                                                      Register source,\n                                                      int offset) "}, {"name": "BaselineAssembler::LoadTaggedSignedFieldAndUntag", "content": "void BaselineAssembler::LoadTaggedSignedFieldAndUntag(Register output,\n                                                      Register source,\n                                                      int offset) {\n  LoadTaggedSignedField(output, source, offset);\n  SmiUntag(output);\n}", "name_and_para": "void BaselineAssembler::LoadTaggedSignedFieldAndUntag(Register output,\n                                                      Register source,\n                                                      int offset) "}], [{"name": "BaselineAssembler::LoadTaggedSignedField", "content": "void BaselineAssembler::LoadTaggedSignedField(Register output, Register source,\n                                              int offset) {\n  __ LoadTaggedSignedField(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadTaggedSignedField(Register output, Register source,\n                                              int offset) "}, {"name": "BaselineAssembler::LoadTaggedSignedField", "content": "void BaselineAssembler::LoadTaggedSignedField(Register output, Register source,\n                                              int offset) {\n  __ LoadTaggedSignedField(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadTaggedSignedField(Register output, Register source,\n                                              int offset) "}], [{"name": "BaselineAssembler::LoadTaggedField", "content": "void BaselineAssembler::LoadTaggedField(Register output, Register source,\n                                        int offset) {\n  __ LoadTaggedField(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadTaggedField(Register output, Register source,\n                                        int offset) "}, {"name": "BaselineAssembler::LoadTaggedField", "content": "void BaselineAssembler::LoadTaggedField(Register output, Register source,\n                                        int offset) {\n  __ LoadTaggedField(output, FieldMemOperand(source, offset));\n}", "name_and_para": "void BaselineAssembler::LoadTaggedField(Register output, Register source,\n                                        int offset) "}], [{"name": "BaselineAssembler::Pop", "content": "void BaselineAssembler::Pop(T... registers) {\n  detail::PopAllHelper<T...>::Pop(this, registers...);\n}", "name_and_para": "void BaselineAssembler::Pop(T... registers) "}, {"name": "BaselineAssembler::Pop", "content": "void BaselineAssembler::Pop(T... registers) {\n  detail::PopAllHelper<T...>::Pop(this, registers...);\n}", "name_and_para": "void BaselineAssembler::Pop(T... registers) "}], [{"name": "BaselineAssembler::PushReverse", "content": "void BaselineAssembler::PushReverse(T... vals) {\n  detail::PushAllReverse(this, vals...);\n}", "name_and_para": "void BaselineAssembler::PushReverse(T... vals) "}, {"name": "BaselineAssembler::PushReverse", "content": "void BaselineAssembler::PushReverse(T... vals) {\n  detail::PushAllHelper<T...>::PushReverse(this, vals...);\n}", "name_and_para": "void BaselineAssembler::PushReverse(T... vals) "}], [{"name": "BaselineAssembler::Push", "content": "int BaselineAssembler::Push(T... vals) {\n  // We have to count the pushes first, to decide whether to add padding before\n  // the first push.\n  int push_count = detail::CountPushHelper<T...>::Count(vals...);\n  if (push_count % 2 == 0) {\n    detail::PushAll(this, vals...);\n  } else {\n    detail::PushAll(this, padreg, vals...);\n  }\n  return push_count;\n}", "name_and_para": "int BaselineAssembler::Push(T... vals) "}, {"name": "BaselineAssembler::Push", "content": "int BaselineAssembler::Push(T... vals) {\n  return detail::PushAllHelper<T...>::Push(this, vals...);\n}", "name_and_para": "int BaselineAssembler::Push(T... vals) "}], [{"name": "PopAllHelper<Register>", "content": "struct PopAllHelper<Register> {\n  static void Pop(BaselineAssembler* basm, Register reg) {\n    basm->masm()->Pop(reg, padreg);\n  }\n}", "name_and_para": ""}, {"name": "PopAllHelper<Register>", "content": "struct PopAllHelper<Register> {\n  static void Pop(BaselineAssembler* basm, Register reg) {\n    basm->masm()->Pop(reg);\n  }\n}", "name_and_para": ""}], [{"name": "PopAllHelper<>", "content": "struct PopAllHelper<> {\n  static void Pop(BaselineAssembler* basm) {}\n}", "name_and_para": ""}, {"name": "PopAllHelper<>", "content": "struct PopAllHelper<> {\n  static void Pop(BaselineAssembler* basm) {}\n}", "name_and_para": ""}], [{"name": "PopAllHelper", "content": "struct PopAllHelper", "name_and_para": ""}, {"name": "PopAllHelper", "content": "struct PopAllHelper", "name_and_para": ""}], [{"name": "PushAllHelper<interpreter::RegisterList>", "content": "struct PushAllHelper<interpreter::RegisterList> {\n  static void Push(BaselineAssembler* basm, interpreter::RegisterList list) {\n    DCHECK_EQ(list.register_count() % 2, 0);\n    for (int reg_index = 0; reg_index < list.register_count(); reg_index += 2) {\n      PushAll(basm, list[reg_index], list[reg_index + 1]);\n    }\n  }\n  static void PushReverse(BaselineAssembler* basm,\n                          interpreter::RegisterList list) {\n    int reg_index = list.register_count() - 1;\n    if (reg_index % 2 == 0) {\n      // Push the padding register to round up the amount of values pushed.\n      PushAllReverse(basm, list[reg_index], padreg);\n      reg_index--;\n    }\n    for (; reg_index >= 1; reg_index -= 2) {\n      PushAllReverse(basm, list[reg_index - 1], list[reg_index]);\n    }\n  }\n}", "name_and_para": ""}, {"name": "PushAllHelper<interpreter::RegisterList>", "content": "struct PushAllHelper<interpreter::RegisterList> {\n  static int Push(BaselineAssembler* basm, interpreter::RegisterList list) {\n    for (int reg_index = 0; reg_index < list.register_count(); ++reg_index) {\n      PushAllHelper<interpreter::Register>::Push(basm, list[reg_index]);\n    }\n    return list.register_count();\n  }\n  static int PushReverse(BaselineAssembler* basm,\n                         interpreter::RegisterList list) {\n    for (int reg_index = list.register_count() - 1; reg_index >= 0;\n         --reg_index) {\n      PushAllHelper<interpreter::Register>::Push(basm, list[reg_index]);\n    }\n    return list.register_count();\n  }\n}", "name_and_para": ""}], [{"name": "PushAllHelper<Arg>", "content": "struct PushAllHelper<Arg> {\n  static void Push(BaselineAssembler* basm, Arg) { FATAL(\"Unaligned push\"); }\n  static void PushReverse(BaselineAssembler* basm, Arg arg) {\n    // Push the padding register to round up the amount of values pushed.\n    return PushAllReverse(basm, arg, padreg);\n  }\n}", "name_and_para": ""}, {"name": "PushAllHelper<Arg>", "content": "struct PushAllHelper<Arg> {\n  static int Push(BaselineAssembler* basm, Arg arg) {\n    BaselineAssembler::ScratchRegisterScope scope(basm);\n    basm->masm()->Push(ToRegister(basm, &scope, arg));\n    return 1;\n  }\n  static int PushReverse(BaselineAssembler* basm, Arg arg) {\n    return Push(basm, arg);\n  }\n}", "name_and_para": ""}], [{"name": "PushAllHelper<>", "content": "struct PushAllHelper<> {\n  static void Push(BaselineAssembler* basm) {}\n  static void PushReverse(BaselineAssembler* basm) {}\n}", "name_and_para": ""}, {"name": "PushAllHelper<>", "content": "struct PushAllHelper<> {\n  static int Push(BaselineAssembler* basm) { return 0; }\n  static int PushReverse(BaselineAssembler* basm) { return 0; }\n}", "name_and_para": ""}], [{"name": "PushAllHelper", "content": "struct PushAllHelper", "name_and_para": ""}, {"name": "PushAllHelper", "content": "struct PushAllHelper", "name_and_para": ""}], [{"name": "ToRegister", "content": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) {\n  return reg;\n}", "name_and_para": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) "}, {"name": "ToRegister", "content": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) {\n  return reg;\n}", "name_and_para": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) "}], [{"name": "ToRegister", "content": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Arg arg) {\n  Register reg = scope->AcquireScratch();\n  basm->Move(reg, arg);\n  return reg;\n}", "name_and_para": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Arg arg) "}, {"name": "ToRegister", "content": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) {\n  return reg;\n}", "name_and_para": "inline Register ToRegister(BaselineAssembler* basm,\n                           BaselineAssembler::ScratchRegisterScope* scope,\n                           Register reg) "}], [{"name": "BaselineAssembler::MoveSmi", "content": "void BaselineAssembler::MoveSmi(Register output, Register source) {\n  __ Mov(output, source);\n}", "name_and_para": "void BaselineAssembler::MoveSmi(Register output, Register source) "}, {"name": "BaselineAssembler::MoveSmi", "content": "void BaselineAssembler::MoveSmi(Register output, Register source) {\n  __ Move(output, source);\n}", "name_and_para": "void BaselineAssembler::MoveSmi(Register output, Register source) "}], [{"name": "BaselineAssembler::MoveMaybeSmi", "content": "void BaselineAssembler::MoveMaybeSmi(Register output, Register source) {\n  __ Mov(output, source);\n}", "name_and_para": "void BaselineAssembler::MoveMaybeSmi(Register output, Register source) "}, {"name": "BaselineAssembler::MoveMaybeSmi", "content": "void BaselineAssembler::MoveMaybeSmi(Register output, Register source) {\n  __ Move(output, source);\n}", "name_and_para": "void BaselineAssembler::MoveMaybeSmi(Register output, Register source) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ Mov(output, Immediate(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, Handle<HeapObject> value) {\n  __ Mov(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, Handle<HeapObject> value) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, ExternalReference reference) {\n  __ Mov(output, Operand(reference));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, ExternalReference reference) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(MemOperand output, Register source) {\n  __ Str(source, output);\n}", "name_and_para": "void BaselineAssembler::Move(MemOperand output, Register source) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) {\n  __ Mov(output, Immediate(value.ptr()));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, Tagged<TaggedIndex> value) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(interpreter::Register output, Register source) {\n  Move(RegisterFrameOperand(output), source);\n}", "name_and_para": "void BaselineAssembler::Move(interpreter::Register output, Register source) "}, {"name": "BaselineAssembler::Move", "content": "void BaselineAssembler::Move(Register output, int32_t value) {\n  __ li(output, Operand(value));\n}", "name_and_para": "void BaselineAssembler::Move(Register output, int32_t value) "}], [{"name": "BaselineAssembler::JumpIfByte", "content": "void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,\n                                   Label* target, Label::Distance) {\n  JumpIf(cc, value, Immediate(byte), target);\n}", "name_and_para": "void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,\n                                   Label* target, Label::Distance) "}, {"name": "BaselineAssembler::JumpIfByte", "content": "void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,\n                                   Label* target, Label::Distance distance) {\n  __ Branch(target, cc, value, Operand(byte), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,\n                                   Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfSmi", "content": "void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,\n                                  Label* target, Label::Distance distance) {\n  __ AssertSmi(value);\n  __ CompareTaggedAndBranch(value, smi, cc, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Tagged<Smi> smi,\n                                  Label* target, Label::Distance distance) "}, {"name": "BaselineAssembler::JumpIfSmi", "content": "void BaselineAssembler::JumpIfSmi(Condition cc, Register lhs, Register rhs,\n                                  Label* target, Label::Distance distance) {\n  // todo: compress pointer\n  __ AssertSmi(lhs);\n  __ AssertSmi(rhs);\n  __ CompareTaggedAndBranch(target, cc, lhs, Operand(rhs), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfSmi(Condition cc, Register lhs, Register rhs,\n                                  Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfPointer", "content": "void BaselineAssembler::JumpIfPointer(Condition cc, Register value,\n                                      MemOperand operand, Label* target,\n                                      Label::Distance) {\n  ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  __ Ldr(tmp, operand);\n  JumpIf(cc, value, tmp, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfPointer(Condition cc, Register value,\n                                      MemOperand operand, Label* target,\n                                      Label::Distance) "}, {"name": "BaselineAssembler::JumpIfPointer", "content": "void BaselineAssembler::JumpIfPointer(Condition cc, Register value,\n                                      MemOperand operand, Label* target,\n                                      Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register temp = temps.AcquireScratch();\n  __ LoadWord(temp, operand);\n  __ Branch(target, cc, value, Operand(temp), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfPointer(Condition cc, Register value,\n                                      MemOperand operand, Label* target,\n                                      Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfInstanceType", "content": "void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,\n                                           InstanceType instance_type,\n                                           Label* target, Label::Distance) {\n  ScratchRegisterScope temps(this);\n  Register type = temps.AcquireScratch();\n  if (v8_flags.debug_code) {\n    __ AssertNotSmi(map);\n    __ CompareObjectType(map, type, type, MAP_TYPE);\n    __ Assert(eq, AbortReason::kUnexpectedValue);\n  }\n  __ Ldrh(type, FieldMemOperand(map, Map::kInstanceTypeOffset));\n  JumpIf(cc, type, instance_type, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,\n                                           InstanceType instance_type,\n                                           Label* target, Label::Distance) "}, {"name": "BaselineAssembler::JumpIfInstanceType", "content": "void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,\n                                           InstanceType instance_type,\n                                           Label* target,\n                                           Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register type = temps.AcquireScratch();\n  if (v8_flags.debug_code) {\n    __ AssertNotSmi(map);\n    __ GetObjectType(map, type, type);\n    __ Assert(eq, AbortReason::kUnexpectedValue, type, Operand(MAP_TYPE));\n  }\n  __ LoadWord(type, FieldMemOperand(map, Map::kInstanceTypeOffset));\n  __ Branch(target, cc, type, Operand(instance_type), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,\n                                           InstanceType instance_type,\n                                           Label* target,\n                                           Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfObjectType", "content": "void BaselineAssembler::JumpIfObjectType(Condition cc, Register object,\n                                         InstanceType instance_type,\n                                         Register map, Label* target,\n                                         Label::Distance) {\n  ScratchRegisterScope temps(this);\n  Register type = temps.AcquireScratch();\n  __ LoadMap(map, object);\n  __ Ldrh(type, FieldMemOperand(map, Map::kInstanceTypeOffset));\n  JumpIf(cc, type, instance_type, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfObjectType(Condition cc, Register object,\n                                         InstanceType instance_type,\n                                         Register map, Label* target,\n                                         Label::Distance) "}, {"name": "BaselineAssembler::JumpIfObjectType", "content": "void BaselineAssembler::JumpIfObjectType(Condition cc, Register object,\n                                         InstanceType instance_type,\n                                         Register map, Label* target,\n                                         Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register type = temps.AcquireScratch();\n  __ GetObjectType(object, map, type);\n  __ Branch(target, cc, type, Operand(instance_type), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfObjectType(Condition cc, Register object,\n                                         InstanceType instance_type,\n                                         Register map, Label* target,\n                                         Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfObjectTypeFast", "content": "void BaselineAssembler::JumpIfObjectTypeFast(Condition cc, Register object,\n                                             InstanceType instance_type,\n                                             Label* target,\n                                             Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireScratch();\n  if (cc == eq || cc == ne) {\n    __ IsObjectType(object, scratch, scratch, instance_type);\n    __ B(cc, target);\n    return;\n  }\n  JumpIfObjectType(cc, object, instance_type, scratch, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfObjectTypeFast(Condition cc, Register object,\n                                             InstanceType instance_type,\n                                             Label* target,\n                                             Label::Distance distance) "}, {"name": "BaselineAssembler::JumpIfObjectTypeFast", "content": "void BaselineAssembler::JumpIfObjectTypeFast(Condition cc, Register object,\n                                             InstanceType instance_type,\n                                             Label* target,\n                                             Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireScratch();\n  if (cc == eq || cc == ne) {\n    __ JumpIfObjectType(target, cc, object, instance_type, scratch);\n    return;\n  }\n  JumpIfObjectType(cc, object, instance_type, scratch, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfObjectTypeFast(Condition cc, Register object,\n                                             InstanceType instance_type,\n                                             Label* target,\n                                             Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfJSAnyIsPrimitive", "content": "void BaselineAssembler::JumpIfJSAnyIsPrimitive(Register heap_object,\n                                               Label* target,\n                                               Label::Distance distance) {\n  __ AssertNotSmi(heap_object);\n  ScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireScratch();\n  __ JumpIfJSAnyIsPrimitive(heap_object, scratch, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfJSAnyIsPrimitive(Register heap_object,\n                                               Label* target,\n                                               Label::Distance distance) "}, {"name": "BaselineAssembler::JumpIfJSAnyIsPrimitive", "content": "void BaselineAssembler::JumpIfJSAnyIsPrimitive(Register heap_object,\n                                               Label* target,\n                                               Label::Distance distance) {\n  __ AssertNotSmi(heap_object);\n  ScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireScratch();\n  __ JumpIfJSAnyIsPrimitive(heap_object, scratch, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfJSAnyIsPrimitive(Register heap_object,\n                                               Label* target,\n                                               Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIf", "content": "void BaselineAssembler::JumpIf(Condition cc, Register lhs, const Operand& rhs,\n                               Label* target, Label::Distance) {\n  __ CompareAndBranch(lhs, rhs, cc, target);\n}", "name_and_para": "void BaselineAssembler::JumpIf(Condition cc, Register lhs, const Operand& rhs,\n                               Label* target, Label::Distance) "}, {"name": "BaselineAssembler::JumpIf", "content": "void BaselineAssembler::JumpIf(Condition cc, Register lhs, const Operand& rhs,\n                               Label* target, Label::Distance distance) {\n  __ Branch(target, cc, lhs, Operand(rhs), distance);\n}", "name_and_para": "void BaselineAssembler::JumpIf(Condition cc, Register lhs, const Operand& rhs,\n                               Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::TestAndBranch", "content": "void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,\n                                      Label* target, Label::Distance) {\n  if (cc == kZero) {\n    __ TestAndBranchIfAllClear(value, mask, target);\n  } else if (cc == kNotZero) {\n    __ TestAndBranchIfAnySet(value, mask, target);\n  } else {\n    __ Tst(value, Immediate(mask));\n    __ B(cc, target);\n  }\n}", "name_and_para": "void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,\n                                      Label* target, Label::Distance) "}, {"name": "BaselineAssembler::TestAndBranch", "content": "void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,\n                                      Label* target, Label::Distance distance) {\n  ScratchRegisterScope temps(this);\n  Register tmp = temps.AcquireScratch();\n  __ And(tmp, value, Operand(mask));\n  __ Branch(target, cc, tmp, Operand(zero_reg), distance);\n}", "name_and_para": "void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,\n                                      Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfImmediate", "content": "void BaselineAssembler::JumpIfImmediate(Condition cc, Register left, int right,\n                                        Label* target,\n                                        Label::Distance distance) {\n  JumpIf(cc, left, Immediate(right), target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfImmediate(Condition cc, Register left, int right,\n                                        Label* target,\n                                        Label::Distance distance) "}, {"name": "BaselineAssembler::JumpIfImmediate", "content": "void BaselineAssembler::JumpIfImmediate(Condition cc, Register left, int right,\n                                        Label* target,\n                                        Label::Distance distance) {\n  JumpIf(cc, left, Operand(right), target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfImmediate(Condition cc, Register left, int right,\n                                        Label* target,\n                                        Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfNotSmi", "content": "void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,\n                                     Label::Distance) {\n  __ JumpIfNotSmi(value, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,\n                                     Label::Distance) "}, {"name": "BaselineAssembler::JumpIfNotSmi", "content": "void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,\n                                     Label::Distance distance) {\n  __ JumpIfNotSmi(value, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,\n                                     Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfNotRoot", "content": "void BaselineAssembler::JumpIfNotRoot(Register value, RootIndex index,\n                                      Label* target, Label::Distance) {\n  __ JumpIfNotRoot(value, index, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfNotRoot(Register value, RootIndex index,\n                                      Label* target, Label::Distance) "}, {"name": "BaselineAssembler::JumpIfNotRoot", "content": "void BaselineAssembler::JumpIfNotRoot(Register value, RootIndex index,\n                                      Label* target, Label::Distance distance) {\n  __ JumpIfNotRoot(value, index, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfNotRoot(Register value, RootIndex index,\n                                      Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpIfRoot", "content": "void BaselineAssembler::JumpIfRoot(Register value, RootIndex index,\n                                   Label* target, Label::Distance) {\n  __ JumpIfRoot(value, index, target);\n}", "name_and_para": "void BaselineAssembler::JumpIfRoot(Register value, RootIndex index,\n                                   Label* target, Label::Distance) "}, {"name": "BaselineAssembler::JumpIfRoot", "content": "void BaselineAssembler::JumpIfRoot(Register value, RootIndex index,\n                                   Label* target, Label::Distance distance) {\n  __ JumpIfRoot(value, index, target, distance);\n}", "name_and_para": "void BaselineAssembler::JumpIfRoot(Register value, RootIndex index,\n                                   Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::Jump", "content": "void BaselineAssembler::Jump(Label* target, Label::Distance distance) {\n  __ B(target);\n}", "name_and_para": "void BaselineAssembler::Jump(Label* target, Label::Distance distance) "}, {"name": "BaselineAssembler::Jump", "content": "void BaselineAssembler::Jump(Label* target, Label::Distance distance) {\n  __ jmp(target, distance);\n}", "name_and_para": "void BaselineAssembler::Jump(Label* target, Label::Distance distance) "}], [{"name": "BaselineAssembler::JumpTarget", "content": "void BaselineAssembler::JumpTarget() { __ JumpTarget(); }", "name_and_para": "void BaselineAssembler::JumpTarget() "}, {"name": "BaselineAssembler::JumpTarget", "content": "void BaselineAssembler::JumpTarget() {\n  // Nop\n}", "name_and_para": "void BaselineAssembler::JumpTarget() "}], [{"name": "BaselineAssembler::Bind", "content": "void BaselineAssembler::Bind(Label* label) { __ Bind(label); }", "name_and_para": "void BaselineAssembler::Bind(Label* label) "}, {"name": "BaselineAssembler::Bind", "content": "void BaselineAssembler::Bind(Label* label) { __ bind(label); }", "name_and_para": "void BaselineAssembler::Bind(Label* label) "}], [{"name": "BaselineAssembler::FeedbackCellOperand", "content": "MemOperand BaselineAssembler::FeedbackCellOperand() {\n  return MemOperand(fp, BaselineFrameConstants::kFeedbackCellFromFp);\n}", "name_and_para": "MemOperand BaselineAssembler::FeedbackCellOperand() "}, {"name": "BaselineAssembler::FeedbackCellOperand", "content": "MemOperand BaselineAssembler::FeedbackCellOperand() {\n  return MemOperand(fp, BaselineFrameConstants::kFeedbackCellFromFp);\n}", "name_and_para": "MemOperand BaselineAssembler::FeedbackCellOperand() "}], [{"name": "BaselineAssembler::FeedbackVectorOperand", "content": "MemOperand BaselineAssembler::FeedbackVectorOperand() {\n  return MemOperand(fp, BaselineFrameConstants::kFeedbackVectorFromFp);\n}", "name_and_para": "MemOperand BaselineAssembler::FeedbackVectorOperand() "}, {"name": "BaselineAssembler::FeedbackVectorOperand", "content": "MemOperand BaselineAssembler::FeedbackVectorOperand() {\n  return MemOperand(fp, BaselineFrameConstants::kFeedbackVectorFromFp);\n}", "name_and_para": "MemOperand BaselineAssembler::FeedbackVectorOperand() "}], [{"name": "BaselineAssembler::RegisterFrameAddress", "content": "void BaselineAssembler::RegisterFrameAddress(\n    interpreter::Register interpreter_register, Register rscratch) {\n  return __ Add(rscratch, fp,\n                interpreter_register.ToOperand() * kSystemPointerSize);\n}", "name_and_para": "void BaselineAssembler::RegisterFrameAddress(\n    interpreter::Register interpreter_register, Register rscratch) "}, {"name": "BaselineAssembler::RegisterFrameAddress", "content": "void BaselineAssembler::RegisterFrameAddress(\n    interpreter::Register interpreter_register, Register rscratch) {\n  return __ AddWord(rscratch, fp,\n                    interpreter_register.ToOperand() * kSystemPointerSize);\n}", "name_and_para": "void BaselineAssembler::RegisterFrameAddress(\n    interpreter::Register interpreter_register, Register rscratch) "}], [{"name": "BaselineAssembler::RegisterFrameOperand", "content": "MemOperand BaselineAssembler::RegisterFrameOperand(\n    interpreter::Register interpreter_register) {\n  return MemOperand(fp, interpreter_register.ToOperand() * kSystemPointerSize);\n}", "name_and_para": "MemOperand BaselineAssembler::RegisterFrameOperand(\n    interpreter::Register interpreter_register) "}, {"name": "BaselineAssembler::RegisterFrameOperand", "content": "MemOperand BaselineAssembler::RegisterFrameOperand(\n    interpreter::Register interpreter_register) {\n  return MemOperand(fp, interpreter_register.ToOperand() * kSystemPointerSize);\n}", "name_and_para": "MemOperand BaselineAssembler::RegisterFrameOperand(\n    interpreter::Register interpreter_register) "}], [{"name": "Clobbers", "content": "inline bool Clobbers(Register target, MemOperand op) {\n  return op.base() == target || op.regoffset() == target;\n}", "name_and_para": "inline bool Clobbers(Register target, MemOperand op) "}, {"name": "Clobbers", "content": "inline bool Clobbers(Register target, MemOperand op) {\n  return op.is_reg() && op.rm() == target;\n}", "name_and_para": "inline bool Clobbers(Register target, MemOperand op) "}], [{"name": "BaselineAssembler::ScratchRegisterScope", "content": "class BaselineAssembler::ScratchRegisterScope {\n public:\n  explicit ScratchRegisterScope(BaselineAssembler* assembler)\n      : assembler_(assembler),\n        prev_scope_(assembler->scratch_register_scope_),\n        wrapped_scope_(assembler->masm()) {\n    if (!assembler_->scratch_register_scope_) {\n      // If we haven't opened a scratch scope yet, for the first one add a\n      // couple of extra registers.\n      wrapped_scope_.Include(x14, x15);\n      wrapped_scope_.Include(x19);\n    }\n    assembler_->scratch_register_scope_ = this;\n  }\n  ~ScratchRegisterScope() { assembler_->scratch_register_scope_ = prev_scope_; }\n\n  Register AcquireScratch() { return wrapped_scope_.AcquireX(); }\n\n private:\n  BaselineAssembler* assembler_;\n  ScratchRegisterScope* prev_scope_;\n  UseScratchRegisterScope wrapped_scope_;\n}", "name_and_para": ""}, {"name": "BaselineAssembler::ScratchRegisterScope", "content": "class BaselineAssembler::ScratchRegisterScope {\n public:\n  explicit ScratchRegisterScope(BaselineAssembler* assembler)\n      : assembler_(assembler),\n        prev_scope_(assembler->scratch_register_scope_),\n        wrapped_scope_(assembler->masm()) {\n    if (!assembler_->scratch_register_scope_) {\n      // If we haven't opened a scratch scope yet, for the first one add a\n      // couple of extra registers.\n      wrapped_scope_.Include(kScratchReg, kScratchReg2);\n    }\n    assembler_->scratch_register_scope_ = this;\n  }\n  ~ScratchRegisterScope() { assembler_->scratch_register_scope_ = prev_scope_; }\n\n  Register AcquireScratch() { return wrapped_scope_.Acquire(); }\n\n private:\n  BaselineAssembler* assembler_;\n  ScratchRegisterScope* prev_scope_;\n  UseScratchRegisterScope wrapped_scope_;\n}", "name_and_para": ""}]]], [["./v8/src/baseline/riscv/baseline-compiler-riscv-inl.h", "./v8/src/baseline/arm64/baseline-compiler-arm64-inl.h"], 1.0, 1.0, [[{"name": "BaselineCompiler::VerifyFrameSize", "content": "void BaselineCompiler::VerifyFrameSize() {\n  ASM_CODE_COMMENT(&masm_);\n  __ masm()->Add(x15, sp,\n                 RoundUp(InterpreterFrameConstants::kFixedFrameSizeFromFp +\n                             bytecode_->frame_size(),\n                         2 * kSystemPointerSize));\n  __ masm()->Cmp(x15, fp);\n  __ masm()->Assert(eq, AbortReason::kUnexpectedStackPointer);\n}", "name_and_para": "void BaselineCompiler::VerifyFrameSize() "}, {"name": "BaselineCompiler::VerifyFrameSize", "content": "void BaselineCompiler::VerifyFrameSize() {\n  ASM_CODE_COMMENT(&masm_);\n  __ masm()->AddWord(t0, sp,\n                     Operand(InterpreterFrameConstants::kFixedFrameSizeFromFp +\n                             bytecode_->frame_size()));\n  __ masm()->Assert(eq, AbortReason::kUnexpectedStackPointer, t0, Operand(fp));\n}", "name_and_para": "void BaselineCompiler::VerifyFrameSize() "}], [{"name": "BaselineCompiler::PrologueFillFrame", "content": "void BaselineCompiler::PrologueFillFrame() {\n  ASM_CODE_COMMENT(&masm_);\n  // Inlined register frame fill\n  interpreter::Register new_target_or_generator_register =\n      bytecode_->incoming_new_target_or_generator_register();\n  if (v8_flags.debug_code) {\n    __ masm()->CompareRoot(kInterpreterAccumulatorRegister,\n                           RootIndex::kUndefinedValue);\n    __ masm()->Assert(eq, AbortReason::kUnexpectedValue);\n  }\n  int register_count = bytecode_->register_count();\n  // Magic value\n  const int kLoopUnrollSize = 8;\n  const int new_target_index = new_target_or_generator_register.index();\n  const bool has_new_target = new_target_index != kMaxInt;\n  if (has_new_target) {\n      DCHECK_LE(new_target_index, register_count);\n      int before_new_target_count = 0;\n      for (; before_new_target_count + 2 <= new_target_index;\n           before_new_target_count += 2) {\n        __ masm()->Push(kInterpreterAccumulatorRegister,\n                        kInterpreterAccumulatorRegister);\n      }\n      if (before_new_target_count == new_target_index) {\n        __ masm()->Push(kJavaScriptCallNewTargetRegister,\n                        kInterpreterAccumulatorRegister);\n      } else {\n        DCHECK_EQ(before_new_target_count + 1, new_target_index);\n        __ masm()->Push(kInterpreterAccumulatorRegister,\n                        kJavaScriptCallNewTargetRegister);\n      }\n      // We pushed before_new_target_count registers, plus the two registers\n      // that included new_target.\n      register_count -= (before_new_target_count + 2);\n  }\n  if (register_count < 2 * kLoopUnrollSize) {\n    // If the frame is small enough, just unroll the frame fill completely.\n    for (int i = 0; i < register_count; i += 2) {\n      __ masm()->Push(kInterpreterAccumulatorRegister,\n                      kInterpreterAccumulatorRegister);\n    }\n  } else {\n    BaselineAssembler::ScratchRegisterScope temps(&basm_);\n    Register scratch = temps.AcquireScratch();\n\n    // Extract the first few registers to round to the unroll size.\n    int first_registers = register_count % kLoopUnrollSize;\n    for (int i = 0; i < first_registers; i += 2) {\n      __ masm()->Push(kInterpreterAccumulatorRegister,\n                      kInterpreterAccumulatorRegister);\n    }\n    __ Move(scratch, register_count / kLoopUnrollSize);\n    // We enter the loop unconditionally, so make sure we need to loop at least\n    // once.\n    DCHECK_GT(register_count / kLoopUnrollSize, 0);\n    Label loop;\n    __ Bind(&loop);\n    for (int i = 0; i < kLoopUnrollSize; i += 2) {\n      __ masm()->Push(kInterpreterAccumulatorRegister,\n                      kInterpreterAccumulatorRegister);\n    }\n    __ masm()->Subs(scratch, scratch, 1);\n    __ masm()->B(gt, &loop);\n  }\n}", "name_and_para": "void BaselineCompiler::PrologueFillFrame() "}, {"name": "BaselineCompiler::PrologueFillFrame", "content": "void BaselineCompiler::PrologueFillFrame() {\n  ASM_CODE_COMMENT(&masm_);\n  // Inlined register frame fill\n  interpreter::Register new_target_or_generator_register =\n      bytecode_->incoming_new_target_or_generator_register();\n  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n  int register_count = bytecode_->register_count();\n  // Magic value\n  const int kLoopUnrollSize = 8;\n  const int new_target_index = new_target_or_generator_register.index();\n  const bool has_new_target = new_target_index != kMaxInt;\n  if (has_new_target) {\n    DCHECK_LE(new_target_index, register_count);\n    __ masm()->AddWord(sp, sp,\n                       Operand(-(kSystemPointerSize * new_target_index)));\n    for (int i = 0; i < new_target_index; i++) {\n      __ masm()->StoreWord(kInterpreterAccumulatorRegister,\n                           MemOperand(sp, i * kSystemPointerSize));\n    }\n    // Push new_target_or_generator.\n    __ Push(kJavaScriptCallNewTargetRegister);\n    register_count -= new_target_index + 1;\n  }\n  if (register_count < 2 * kLoopUnrollSize) {\n    // If the frame is small enough, just unroll the frame fill completely.\n    __ masm()->AddWord(sp, sp, Operand(-(kSystemPointerSize * register_count)));\n    for (int i = 0; i < register_count; ++i) {\n      __ masm()->StoreWord(kInterpreterAccumulatorRegister,\n                           MemOperand(sp, i * kSystemPointerSize));\n    }\n  } else {\n    __ masm()->AddWord(sp, sp, Operand(-(kSystemPointerSize * register_count)));\n    for (int i = 0; i < register_count; ++i) {\n      __ masm()->StoreWord(kInterpreterAccumulatorRegister,\n                           MemOperand(sp, i * kSystemPointerSize));\n    }\n  }\n}", "name_and_para": "void BaselineCompiler::PrologueFillFrame() "}], [{"name": "BaselineCompiler::Prologue", "content": "void BaselineCompiler::Prologue() {\n  ASM_CODE_COMMENT(&masm_);\n  // Enter the frame here, since CallBuiltin will override lr.\n  __ masm()->EnterFrame(StackFrame::BASELINE);\n  DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);\n  int max_frame_size =\n      bytecode_->frame_size() + max_call_args_ * kSystemPointerSize;\n  CallBuiltin<Builtin::kBaselineOutOfLinePrologue>(\n      kContextRegister, kJSFunctionRegister, kJavaScriptCallArgCountRegister,\n      max_frame_size, kJavaScriptCallNewTargetRegister, bytecode_);\n\n  __ masm()->AssertSpAligned();\n  PrologueFillFrame();\n  __ masm()->AssertSpAligned();\n}", "name_and_para": "void BaselineCompiler::Prologue() "}, {"name": "BaselineCompiler::Prologue", "content": "void BaselineCompiler::Prologue() {\n  ASM_CODE_COMMENT(&masm_);\n  // Enter the frame here, since CallBuiltin will override lr.\n  __ masm()->EnterFrame(StackFrame::BASELINE);\n  DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);\n  int max_frame_size =\n      bytecode_->frame_size() + max_call_args_ * kSystemPointerSize;\n  CallBuiltin<Builtin::kBaselineOutOfLinePrologue>(\n      kContextRegister, kJSFunctionRegister, kJavaScriptCallArgCountRegister,\n      max_frame_size, kJavaScriptCallNewTargetRegister, bytecode_);\n  PrologueFillFrame();\n}", "name_and_para": "void BaselineCompiler::Prologue() "}]]], [["./v8/src/deoptimizer/riscv/deoptimizer-riscv.cc", "./v8/src/deoptimizer/arm64/deoptimizer-arm64.cc"], 1.0, 1.0, [[{"name": "FrameDescription::SetPc", "content": "void FrameDescription::SetPc(intptr_t pc) {\n  // TODO(v8:10026): We need to sign pointers to the embedded blob, which are\n  // stored in the isolate and code range objects.\n  if (ENABLE_CONTROL_FLOW_INTEGRITY_BOOL) {\n    CHECK(Deoptimizer::IsValidReturnAddress(PointerAuthentication::StripPAC(pc),\n                                            isolate_));\n  }\n  pc_ = pc;\n}", "name_and_para": "void FrameDescription::SetPc(intptr_t pc) "}, {"name": "FrameDescription::SetPc", "content": "void FrameDescription::SetPc(intptr_t pc) { pc_ = pc; }", "name_and_para": "void FrameDescription::SetPc(intptr_t pc) "}], [{"name": "FrameDescription::SetCallerConstantPool", "content": "void FrameDescription::SetCallerConstantPool(unsigned offset, intptr_t value) {\n  // No embedded constant pool support.\n  UNREACHABLE();\n}", "name_and_para": "void FrameDescription::SetCallerConstantPool(unsigned offset, intptr_t value) "}, {"name": "FrameDescription::SetCallerConstantPool", "content": "void FrameDescription::SetCallerConstantPool(unsigned offset, intptr_t value) {\n  // No embedded constant pool support.\n  UNREACHABLE();\n}", "name_and_para": "void FrameDescription::SetCallerConstantPool(unsigned offset, intptr_t value) "}], [{"name": "FrameDescription::SetCallerFp", "content": "void FrameDescription::SetCallerFp(unsigned offset, intptr_t value) {\n  SetFrameSlot(offset, value);\n}", "name_and_para": "void FrameDescription::SetCallerFp(unsigned offset, intptr_t value) "}, {"name": "FrameDescription::SetCallerFp", "content": "void FrameDescription::SetCallerFp(unsigned offset, intptr_t value) {\n  SetFrameSlot(offset, value);\n}", "name_and_para": "void FrameDescription::SetCallerFp(unsigned offset, intptr_t value) "}], [{"name": "FrameDescription::SetCallerPc", "content": "void FrameDescription::SetCallerPc(unsigned offset, intptr_t value) {\n  Address new_context =\n      static_cast<Address>(GetTop()) + offset + kPCOnStackSize;\n  value = PointerAuthentication::SignAndCheckPC(isolate_, value, new_context);\n  SetFrameSlot(offset, value);\n}", "name_and_para": "void FrameDescription::SetCallerPc(unsigned offset, intptr_t value) "}, {"name": "FrameDescription::SetCallerPc", "content": "void FrameDescription::SetCallerPc(unsigned offset, intptr_t value) {\n  SetFrameSlot(offset, value);\n}", "name_and_para": "void FrameDescription::SetCallerPc(unsigned offset, intptr_t value) "}], [{"name": "RegisterValues::GetDoubleRegister", "content": "Float64 RegisterValues::GetDoubleRegister(unsigned n) const {\n  V8_ASSUME(n < arraysize(simd128_registers_));\n  return base::ReadUnalignedValue<Float64>(\n      reinterpret_cast<Address>(simd128_registers_ + n));\n}", "name_and_para": "Float64 RegisterValues::GetDoubleRegister(unsigned n) const "}, {"name": "RegisterValues::GetDoubleRegister", "content": "Float64 RegisterValues::GetDoubleRegister(unsigned n) const {\n  return Float64::FromBits(\n      static_cast<uint64_t>(double_registers_[n].get_bits()));\n}", "name_and_para": "Float64 RegisterValues::GetDoubleRegister(unsigned n) const "}], [{"name": "RegisterValues::GetFloatRegister", "content": "Float32 RegisterValues::GetFloatRegister(unsigned n) const {\n  V8_ASSUME(n < arraysize(simd128_registers_));\n  return base::ReadUnalignedValue<Float32>(\n      reinterpret_cast<Address>(simd128_registers_ + n));\n}", "name_and_para": "Float32 RegisterValues::GetFloatRegister(unsigned n) const "}, {"name": "RegisterValues::GetFloatRegister", "content": "Float32 RegisterValues::GetFloatRegister(unsigned n) const {\n  return Float32::FromBits(\n      static_cast<uint32_t>(double_registers_[n].get_bits()));\n}", "name_and_para": "Float32 RegisterValues::GetFloatRegister(unsigned n) const "}]]], [["./v8/src/builtins/riscv/builtins-riscv.cc", "./v8/src/builtins/arm64/builtins-arm64.cc"], 0.7545454545454545, 0.9540229885057471, [[{"name": "Builtins::Generate_RestartFrameTrampoline", "content": "void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) {\n  // Frame is being dropped:\n  // - Look up current function on the frame.\n  // - Leave the frame.\n  // - Restart the frame by calling the function.\n\n  __ Ldr(x1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n  __ ldr(x0, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n\n  __ LeaveFrame(StackFrame::INTERPRETED);\n\n  // The arguments are already in the stack (including any necessary padding),\n  // we should not try to massage the arguments again.\n  __ Mov(x2, kDontAdaptArgumentsSentinel);\n  __ InvokeFunction(x1, x2, x0, InvokeType::kJump);\n}", "name_and_para": "void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_RestartFrameTrampoline", "content": "void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) {\n  // Frame is being dropped:\n  // - Look up current function on the frame.\n  // - Leave the frame.\n  // - Restart the frame by calling the function.\n\n  __ LoadWord(a1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n  __ LoadWord(a0, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n\n  // Pop return address and frame.\n  __ LeaveFrame(StackFrame::INTERPRETED);\n\n  __ li(a2, Operand(kDontAdaptArgumentsSentinel));\n\n  __ InvokeFunction(a1, a2, a0, InvokeType::kJump);\n}", "name_and_para": "void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_InterpreterOnStackReplacement_ToBaseline", "content": "void Builtins::Generate_InterpreterOnStackReplacement_ToBaseline(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, false, true);\n}", "name_and_para": "void Builtins::Generate_InterpreterOnStackReplacement_ToBaseline(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_InterpreterOnStackReplacement_ToBaseline", "content": "void Builtins::Generate_InterpreterOnStackReplacement_ToBaseline(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, false, true);\n}", "name_and_para": "void Builtins::Generate_InterpreterOnStackReplacement_ToBaseline(\n    MacroAssembler* masm) "}], [{"name": "Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode", "content": "void Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, true);\n}", "name_and_para": "void Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode", "content": "void Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, true);\n}", "name_and_para": "void Builtins::Generate_BaselineOrInterpreterEnterAtNextBytecode(\n    MacroAssembler* masm) "}], [{"name": "Builtins::Generate_BaselineOrInterpreterEnterAtBytecode", "content": "void Builtins::Generate_BaselineOrInterpreterEnterAtBytecode(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, false);\n}", "name_and_para": "void Builtins::Generate_BaselineOrInterpreterEnterAtBytecode(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_BaselineOrInterpreterEnterAtBytecode", "content": "void Builtins::Generate_BaselineOrInterpreterEnterAtBytecode(\n    MacroAssembler* masm) {\n  Generate_BaselineOrInterpreterEntry(masm, false);\n}", "name_and_para": "void Builtins::Generate_BaselineOrInterpreterEnterAtBytecode(\n    MacroAssembler* masm) "}], [{"name": "Generate_BaselineOrInterpreterEntry", "content": "void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,\n                                         bool next_bytecode,\n                                         bool is_osr = false) {\n  Label start;\n  __ bind(&start);\n\n  // Get function from the frame.\n  Register closure = x1;\n  __ Ldr(closure, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n\n  // Get the InstructionStream object from the shared function info.\n  Register code_obj = x22;\n  __ LoadTaggedField(\n      code_obj,\n      FieldMemOperand(closure, JSFunction::kSharedFunctionInfoOffset));\n\n  if (is_osr) {\n    ResetSharedFunctionInfoAge(masm, code_obj);\n  }\n\n  GetSharedFunctionInfoData(masm, code_obj, code_obj, x3);\n\n  // Check if we have baseline code. For OSR entry it is safe to assume we\n  // always have baseline code.\n  if (!is_osr) {\n    Label start_with_baseline;\n    __ IsObjectType(code_obj, x3, x3, CODE_TYPE);\n    __ B(eq, &start_with_baseline);\n\n    // Start with bytecode as there is no baseline code.\n    Builtin builtin = next_bytecode ? Builtin::kInterpreterEnterAtNextBytecode\n                                    : Builtin::kInterpreterEnterAtBytecode;\n    __ TailCallBuiltin(builtin);\n\n    // Start with baseline code.\n    __ bind(&start_with_baseline);\n  } else if (v8_flags.debug_code) {\n    __ IsObjectType(code_obj, x3, x3, CODE_TYPE);\n    __ Assert(eq, AbortReason::kExpectedBaselineData);\n  }\n\n  if (v8_flags.debug_code) {\n    AssertCodeIsBaseline(masm, code_obj, x3);\n  }\n\n  // Load the feedback cell and vector.\n  Register feedback_cell = x2;\n  Register feedback_vector = x15;\n  __ LoadTaggedField(feedback_cell,\n                     FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n  __ LoadTaggedField(\n      feedback_vector,\n      FieldMemOperand(feedback_cell, FeedbackCell::kValueOffset));\n\n  Label install_baseline_code;\n  // Check if feedback vector is valid. If not, call prepare for baseline to\n  // allocate it.\n  __ IsObjectType(feedback_vector, x3, x3, FEEDBACK_VECTOR_TYPE);\n  __ B(ne, &install_baseline_code);\n\n  // Save BytecodeOffset from the stack frame.\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  // Replace bytecode offset with feedback cell.\n  static_assert(InterpreterFrameConstants::kBytecodeOffsetFromFp ==\n                BaselineFrameConstants::kFeedbackCellFromFp);\n  __ Str(feedback_cell,\n         MemOperand(fp, BaselineFrameConstants::kFeedbackCellFromFp));\n  feedback_cell = no_reg;\n  // Update feedback vector cache.\n  static_assert(InterpreterFrameConstants::kFeedbackVectorFromFp ==\n                BaselineFrameConstants::kFeedbackVectorFromFp);\n  __ Str(feedback_vector,\n         MemOperand(fp, InterpreterFrameConstants::kFeedbackVectorFromFp));\n  feedback_vector = no_reg;\n\n  // Compute baseline pc for bytecode offset.\n  ExternalReference get_baseline_pc_extref;\n  if (next_bytecode || is_osr) {\n    get_baseline_pc_extref =\n        ExternalReference::baseline_pc_for_next_executed_bytecode();\n  } else {\n    get_baseline_pc_extref =\n        ExternalReference::baseline_pc_for_bytecode_offset();\n  }\n  Register get_baseline_pc = x3;\n  __ Mov(get_baseline_pc, get_baseline_pc_extref);\n\n  // If the code deoptimizes during the implicit function entry stack interrupt\n  // check, it will have a bailout ID of kFunctionEntryBytecodeOffset, which is\n  // not a valid bytecode offset.\n  // TODO(pthier): Investigate if it is feasible to handle this special case\n  // in TurboFan instead of here.\n  Label valid_bytecode_offset, function_entry_bytecode;\n  if (!is_osr) {\n    __ cmp(kInterpreterBytecodeOffsetRegister,\n           Operand(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                   kFunctionEntryBytecodeOffset));\n    __ B(eq, &function_entry_bytecode);\n  }\n\n  __ Sub(kInterpreterBytecodeOffsetRegister, kInterpreterBytecodeOffsetRegister,\n         (BytecodeArray::kHeaderSize - kHeapObjectTag));\n\n  __ bind(&valid_bytecode_offset);\n  // Get bytecode array from the stack frame.\n  __ ldr(kInterpreterBytecodeArrayRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  // Save the accumulator register, since it's clobbered by the below call.\n  __ Push(padreg, kInterpreterAccumulatorRegister);\n  {\n    __ Mov(kCArgRegs[0], code_obj);\n    __ Mov(kCArgRegs[1], kInterpreterBytecodeOffsetRegister);\n    __ Mov(kCArgRegs[2], kInterpreterBytecodeArrayRegister);\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallCFunction(get_baseline_pc, 3, 0);\n  }\n  __ LoadCodeInstructionStart(code_obj, code_obj, kJSEntrypointTag);\n  __ Add(code_obj, code_obj, kReturnRegister0);\n  __ Pop(kInterpreterAccumulatorRegister, padreg);\n\n  if (is_osr) {\n    Generate_OSREntry(masm, code_obj);\n  } else {\n    __ Jump(code_obj);\n  }\n  __ Trap();  // Unreachable.\n\n  if (!is_osr) {\n    __ bind(&function_entry_bytecode);\n    // If the bytecode offset is kFunctionEntryOffset, get the start address of\n    // the first bytecode.\n    __ Mov(kInterpreterBytecodeOffsetRegister, Operand(0));\n    if (next_bytecode) {\n      __ Mov(get_baseline_pc,\n             ExternalReference::baseline_pc_for_bytecode_offset());\n    }\n    __ B(&valid_bytecode_offset);\n  }\n\n  __ bind(&install_baseline_code);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(padreg, kInterpreterAccumulatorRegister);\n    __ PushArgument(closure);\n    __ CallRuntime(Runtime::kInstallBaselineCode, 1);\n    __ Pop(kInterpreterAccumulatorRegister, padreg);\n  }\n  // Retry from the start after installing baseline code.\n  __ B(&start);\n}", "name_and_para": "void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,\n                                         bool next_bytecode,\n                                         bool is_osr = false) "}, {"name": "Generate_BaselineOrInterpreterEntry", "content": "void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,\n                                         bool next_bytecode,\n                                         bool is_osr = false) {\n  Label start;\n  __ bind(&start);\n\n  // Get function from the frame.\n  Register closure = a1;\n  __ LoadWord(closure, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n\n  // Get the InstructionStream object from the shared function info.\n  Register code_obj = s1;\n  __ LoadTaggedField(\n      code_obj,\n      FieldMemOperand(closure, JSFunction::kSharedFunctionInfoOffset));\n\n  if (is_osr) {\n    ResetSharedFunctionInfoAge(masm, code_obj);\n  }\n\n  GetSharedFunctionInfoData(masm, code_obj, code_obj, t2);\n\n  // Check if we have baseline code. For OSR entry it is safe to assume we\n  // always have baseline code.\n  if (!is_osr) {\n    Label start_with_baseline;\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ GetObjectType(code_obj, scratch, scratch);\n    __ Branch(&start_with_baseline, eq, scratch, Operand(CODE_TYPE));\n\n    // Start with bytecode as there is no baseline code.\n    Builtin builtin = next_bytecode ? Builtin::kInterpreterEnterAtNextBytecode\n                                    : Builtin::kInterpreterEnterAtBytecode;\n    __ TailCallBuiltin(builtin);\n\n    // Start with baseline code.\n    __ bind(&start_with_baseline);\n  } else if (v8_flags.debug_code) {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ GetObjectType(code_obj, scratch, scratch);\n    __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,\n              Operand(CODE_TYPE));\n  }\n  if (v8_flags.debug_code) {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    AssertCodeIsBaseline(masm, code_obj, scratch);\n  }\n\n  // Load the feedback cell and vector.\n  Register feedback_cell = a2;\n  Register feedback_vector = t4;\n  __ LoadTaggedField(feedback_cell,\n                     FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n  __ LoadTaggedField(\n      feedback_vector,\n      FieldMemOperand(feedback_cell, FeedbackCell::kValueOffset));\n  Label install_baseline_code;\n  // Check if feedback vector is valid. If not, call prepare for baseline to\n  // allocate it.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register type = temps.Acquire();\n    __ GetObjectType(feedback_vector, type, type);\n    __ Branch(&install_baseline_code, ne, type, Operand(FEEDBACK_VECTOR_TYPE));\n  }\n  // Save BytecodeOffset from the stack frame.\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  // Replace bytecode offset with feedback cell.\n  static_assert(InterpreterFrameConstants::kBytecodeOffsetFromFp ==\n                BaselineFrameConstants::kFeedbackCellFromFp);\n  __ StoreWord(feedback_cell,\n               MemOperand(fp, BaselineFrameConstants::kFeedbackCellFromFp));\n  feedback_cell = no_reg;\n  // Update feedback vector cache.\n  static_assert(InterpreterFrameConstants::kFeedbackVectorFromFp ==\n                BaselineFrameConstants::kFeedbackVectorFromFp);\n  __ StoreWord(\n      feedback_vector,\n      MemOperand(fp, InterpreterFrameConstants::kFeedbackVectorFromFp));\n  feedback_vector = no_reg;\n\n  // Compute baseline pc for bytecode offset.\n  ExternalReference get_baseline_pc_extref;\n  if (next_bytecode || is_osr) {\n    get_baseline_pc_extref =\n        ExternalReference::baseline_pc_for_next_executed_bytecode();\n  } else {\n    get_baseline_pc_extref =\n        ExternalReference::baseline_pc_for_bytecode_offset();\n  }\n\n  Register get_baseline_pc = a3;\n  __ li(get_baseline_pc, get_baseline_pc_extref);\n\n  // If the code deoptimizes during the implicit function entry stack interrupt\n  // check, it will have a bailout ID of kFunctionEntryBytecodeOffset, which is\n  // not a valid bytecode offset.\n  // TODO(pthier): Investigate if it is feasible to handle this special case\n  // in TurboFan instead of here.\n  Label valid_bytecode_offset, function_entry_bytecode;\n  if (!is_osr) {\n    __ Branch(&function_entry_bytecode, eq, kInterpreterBytecodeOffsetRegister,\n              Operand(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                      kFunctionEntryBytecodeOffset));\n  }\n\n  __ SubWord(kInterpreterBytecodeOffsetRegister,\n             kInterpreterBytecodeOffsetRegister,\n             (BytecodeArray::kHeaderSize - kHeapObjectTag));\n\n  __ bind(&valid_bytecode_offset);\n  // Get bytecode array from the stack frame.\n  __ LoadWord(kInterpreterBytecodeArrayRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ Push(kInterpreterAccumulatorRegister);\n  {\n    __ Move(kCArgRegs[0], code_obj);\n    __ Move(kCArgRegs[1], kInterpreterBytecodeOffsetRegister);\n    __ Move(kCArgRegs[2], kInterpreterBytecodeArrayRegister);\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ PrepareCallCFunction(3, 0, a4);\n    __ CallCFunction(get_baseline_pc, 3, 0);\n  }\n  __ LoadCodeInstructionStart(code_obj, code_obj, kJSEntrypointTag);\n  __ AddWord(code_obj, code_obj, kReturnRegister0);\n  __ Pop(kInterpreterAccumulatorRegister);\n\n  if (is_osr) {\n    // Reset the OSR loop nesting depth to disarm back edges.\n    // TODO(pthier): Separate baseline Sparkplug from TF arming and don't disarm\n    // Sparkplug here.\n    __ LoadWord(\n        kInterpreterBytecodeArrayRegister,\n        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n    Generate_OSREntry(masm, code_obj);\n  } else {\n    __ Jump(code_obj);\n  }\n  __ Trap();  // Unreachable.\n\n  if (!is_osr) {\n    __ bind(&function_entry_bytecode);\n    // If the bytecode offset is kFunctionEntryOffset, get the start address of\n    // the first bytecode.\n    __ li(kInterpreterBytecodeOffsetRegister, Operand(0));\n    if (next_bytecode) {\n      __ li(get_baseline_pc,\n            ExternalReference::baseline_pc_for_bytecode_offset());\n    }\n    __ Branch(&valid_bytecode_offset);\n  }\n\n  __ bind(&install_baseline_code);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(kInterpreterAccumulatorRegister);\n    __ Push(closure);\n    __ CallRuntime(Runtime::kInstallBaselineCode, 1);\n    __ Pop(kInterpreterAccumulatorRegister);\n  }\n  // Retry from the start after installing baseline code.\n  __ Branch(&start);\n}", "name_and_para": "void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,\n                                         bool next_bytecode,\n                                         bool is_osr = false) "}], [{"name": "Builtins::Generate_DeoptimizationEntry_Lazy", "content": "void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {\n  Generate_DeoptimizationEntry(masm, DeoptimizeKind::kLazy);\n}", "name_and_para": "void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) "}, {"name": "Builtins::Generate_DeoptimizationEntry_Lazy", "content": "void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {\n  Generate_DeoptimizationEntry(masm, DeoptimizeKind::kLazy);\n}", "name_and_para": "void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_DeoptimizationEntry_Eager", "content": "void Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {\n  Generate_DeoptimizationEntry(masm, DeoptimizeKind::kEager);\n}", "name_and_para": "void Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) "}, {"name": "Builtins::Generate_DeoptimizationEntry_Eager", "content": "void Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) {\n  Generate_DeoptimizationEntry(masm, DeoptimizeKind::kEager);\n}", "name_and_para": "void Builtins::Generate_DeoptimizationEntry_Eager(MacroAssembler* masm) "}], [{"name": "Generate_DeoptimizationEntry", "content": "void Generate_DeoptimizationEntry(MacroAssembler* masm,\n                                  DeoptimizeKind deopt_kind) {\n  Isolate* isolate = masm->isolate();\n\n  // TODO(all): This code needs to be revisited. We probably only need to save\n  // caller-saved registers here. Callee-saved registers can be stored directly\n  // in the input frame.\n\n  // Save all allocatable simd128 / double registers.\n  CPURegList saved_simd128_registers(\n      kQRegSizeInBits,\n      DoubleRegList::FromBits(\n          RegisterConfiguration::Default()->allocatable_simd128_codes_mask()));\n  DCHECK_EQ(saved_simd128_registers.Count() % 2, 0);\n  __ PushCPURegList(saved_simd128_registers);\n\n  // We save all the registers except sp, lr, platform register (x18) and the\n  // masm scratches.\n  CPURegList saved_registers(CPURegister::kRegister, kXRegSizeInBits, 0, 28);\n  saved_registers.Remove(ip0);\n  saved_registers.Remove(ip1);\n  saved_registers.Remove(x18);\n  saved_registers.Combine(fp);\n  saved_registers.Align();\n  DCHECK_EQ(saved_registers.Count() % 2, 0);\n  __ PushCPURegList(saved_registers);\n\n  __ Mov(x3, Operand(ExternalReference::Create(\n                 IsolateAddressId::kCEntryFPAddress, isolate)));\n  __ Str(fp, MemOperand(x3));\n\n  const int kSavedRegistersAreaSize =\n      (saved_registers.Count() * kXRegSize) +\n      (saved_simd128_registers.Count() * kQRegSize);\n\n  // Floating point registers are saved on the stack above core registers.\n  const int kSimd128RegistersOffset = saved_registers.Count() * kXRegSize;\n\n  Register code_object = x2;\n  Register fp_to_sp = x3;\n  // Get the address of the location in the code object. This is the return\n  // address for lazy deoptimization.\n  __ Mov(code_object, lr);\n  // Compute the fp-to-sp delta.\n  __ Add(fp_to_sp, sp, kSavedRegistersAreaSize);\n  __ Sub(fp_to_sp, fp, fp_to_sp);\n\n  // Allocate a new deoptimizer object.\n  __ Ldr(x1, MemOperand(fp, CommonFrameConstants::kContextOrFrameTypeOffset));\n\n  // Ensure we can safely load from below fp.\n  DCHECK_GT(kSavedRegistersAreaSize, -StandardFrameConstants::kFunctionOffset);\n  __ Ldr(x0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n\n  // If x1 is a smi, zero x0.\n  __ Tst(x1, kSmiTagMask);\n  __ CzeroX(x0, eq);\n\n  __ Mov(x1, static_cast<int>(deopt_kind));\n  // Following arguments are already loaded:\n  //  - x2: code object address\n  //  - x3: fp-to-sp delta\n  __ Mov(x4, ExternalReference::isolate_address(isolate));\n\n  {\n    // Call Deoptimizer::New().\n    AllowExternalCallThatCantCauseGC scope(masm);\n    __ CallCFunction(ExternalReference::new_deoptimizer_function(), 5);\n  }\n\n  // Preserve \"deoptimizer\" object in register x0.\n  Register deoptimizer = x0;\n\n  // Get the input frame descriptor pointer.\n  __ Ldr(x1, MemOperand(deoptimizer, Deoptimizer::input_offset()));\n\n  // Copy core registers into the input frame.\n  CopyRegListToFrame(masm, x1, FrameDescription::registers_offset(),\n                     saved_registers, x2, x3);\n\n  // Copy simd128 / double registers to the input frame.\n  CopyRegListToFrame(masm, x1, FrameDescription::simd128_registers_offset(),\n                     saved_simd128_registers, q2, q3, kSimd128RegistersOffset);\n\n  // Mark the stack as not iterable for the CPU profiler which won't be able to\n  // walk the stack without the return address.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register is_iterable = temps.AcquireX();\n    __ Mov(is_iterable, ExternalReference::stack_is_iterable_address(isolate));\n    __ strb(xzr, MemOperand(is_iterable));\n  }\n\n  // Remove the saved registers from the stack.\n  DCHECK_EQ(kSavedRegistersAreaSize % kXRegSize, 0);\n  __ Drop(kSavedRegistersAreaSize / kXRegSize);\n\n  // Compute a pointer to the unwinding limit in register x2; that is\n  // the first stack slot not part of the input frame.\n  Register unwind_limit = x2;\n  __ Ldr(unwind_limit, MemOperand(x1, FrameDescription::frame_size_offset()));\n\n  // Unwind the stack down to - but not including - the unwinding\n  // limit and copy the contents of the activation frame to the input\n  // frame description.\n  __ Add(x3, x1, FrameDescription::frame_content_offset());\n  __ SlotAddress(x1, 0);\n  __ Lsr(unwind_limit, unwind_limit, kSystemPointerSizeLog2);\n  __ Mov(x5, unwind_limit);\n  __ CopyDoubleWords(x3, x1, x5);\n  // Since {unwind_limit} is the frame size up to the parameter count, we might\n  // end up with a unaligned stack pointer. This is later recovered when\n  // setting the stack pointer to {caller_frame_top_offset}.\n  __ Bic(unwind_limit, unwind_limit, 1);\n  __ Drop(unwind_limit);\n\n  // Compute the output frame in the deoptimizer.\n  __ Push(padreg, x0);  // Preserve deoptimizer object across call.\n  {\n    // Call Deoptimizer::ComputeOutputFrames().\n    AllowExternalCallThatCantCauseGC scope(masm);\n    __ CallCFunction(ExternalReference::compute_output_frames_function(), 1);\n  }\n  __ Pop(x4, padreg);  // Restore deoptimizer object (class Deoptimizer).\n\n  {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Ldr(scratch, MemOperand(x4, Deoptimizer::caller_frame_top_offset()));\n    __ Mov(sp, scratch);\n  }\n\n  // Replace the current (input) frame with the output frames.\n  Label outer_push_loop, outer_loop_header;\n  __ Ldrsw(x1, MemOperand(x4, Deoptimizer::output_count_offset()));\n  __ Ldr(x0, MemOperand(x4, Deoptimizer::output_offset()));\n  __ Add(x1, x0, Operand(x1, LSL, kSystemPointerSizeLog2));\n  __ B(&outer_loop_header);\n\n  __ Bind(&outer_push_loop);\n  Register current_frame = x2;\n  Register frame_size = x3;\n  __ Ldr(current_frame, MemOperand(x0, kSystemPointerSize, PostIndex));\n  __ Ldr(x3, MemOperand(current_frame, FrameDescription::frame_size_offset()));\n  __ Lsr(frame_size, x3, kSystemPointerSizeLog2);\n  __ Claim(frame_size, kXRegSize, /*assume_sp_aligned=*/false);\n\n  __ Add(x7, current_frame, FrameDescription::frame_content_offset());\n  __ SlotAddress(x6, 0);\n  __ CopyDoubleWords(x6, x7, frame_size);\n\n  __ Bind(&outer_loop_header);\n  __ Cmp(x0, x1);\n  __ B(lt, &outer_push_loop);\n\n  __ Ldr(x1, MemOperand(x4, Deoptimizer::input_offset()));\n  RestoreRegList(masm, saved_simd128_registers, x1,\n                 FrameDescription::simd128_registers_offset());\n\n  {\n    UseScratchRegisterScope temps(masm);\n    Register is_iterable = temps.AcquireX();\n    Register one = x4;\n    __ Mov(is_iterable, ExternalReference::stack_is_iterable_address(isolate));\n    __ Mov(one, Operand(1));\n    __ strb(one, MemOperand(is_iterable));\n  }\n\n  // TODO(all): ARM copies a lot (if not all) of the last output frame onto the\n  // stack, then pops it all into registers. Here, we try to load it directly\n  // into the relevant registers. Is this correct? If so, we should improve the\n  // ARM code.\n\n  // Restore registers from the last output frame.\n  // Note that lr is not in the list of saved_registers and will be restored\n  // later. We can use it to hold the address of last output frame while\n  // reloading the other registers.\n  DCHECK(!saved_registers.IncludesAliasOf(lr));\n  Register last_output_frame = lr;\n  __ Mov(last_output_frame, current_frame);\n\n  RestoreRegList(masm, saved_registers, last_output_frame,\n                 FrameDescription::registers_offset());\n\n  UseScratchRegisterScope temps(masm);\n  temps.Exclude(x17);\n  Register continuation = x17;\n  __ Ldr(continuation, MemOperand(last_output_frame,\n                                  FrameDescription::continuation_offset()));\n  __ Ldr(lr, MemOperand(last_output_frame, FrameDescription::pc_offset()));\n#ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY\n  __ Autibsp();\n#endif\n  __ Br(continuation);\n}", "name_and_para": "void Generate_DeoptimizationEntry(MacroAssembler* masm,\n                                  DeoptimizeKind deopt_kind) "}, {"name": "Generate_DeoptimizationEntry", "content": "void Generate_DeoptimizationEntry(MacroAssembler* masm,\n                                  DeoptimizeKind deopt_kind) {\n  Isolate* isolate = masm->isolate();\n\n  // Unlike on ARM we don't save all the registers, just the useful ones.\n  // For the rest, there are gaps on the stack, so the offsets remain the same.\n  const int kNumberOfRegisters = Register::kNumRegisters;\n\n  RegList restored_regs = kJSCallerSaved | kCalleeSaved;\n  RegList saved_regs = restored_regs | sp | ra;\n\n  const int kDoubleRegsSize = kDoubleSize * DoubleRegister::kNumRegisters;\n\n  // Save all double FPU registers before messing with them.\n  __ SubWord(sp, sp, Operand(kDoubleRegsSize));\n  const RegisterConfiguration* config = RegisterConfiguration::Default();\n  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {\n    int code = config->GetAllocatableDoubleCode(i);\n    const DoubleRegister fpu_reg = DoubleRegister::from_code(code);\n    int offset = code * kDoubleSize;\n    __ StoreDouble(fpu_reg, MemOperand(sp, offset));\n  }\n\n  // Push saved_regs (needed to populate FrameDescription::registers_).\n  // Leave gaps for other registers.\n  __ SubWord(sp, sp, kNumberOfRegisters * kSystemPointerSize);\n  for (int16_t i = kNumberOfRegisters - 1; i >= 0; i--) {\n    if ((saved_regs.bits() & (1 << i)) != 0) {\n      __ StoreWord(ToRegister(i), MemOperand(sp, kSystemPointerSize * i));\n    }\n  }\n\n  __ li(a2,\n        ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate));\n  __ StoreWord(fp, MemOperand(a2));\n\n  const int kSavedRegistersAreaSize =\n      (kNumberOfRegisters * kSystemPointerSize) + kDoubleRegsSize;\n\n  // Get the address of the location in the code object (a2) (return\n  // address for lazy deoptimization) and compute the fp-to-sp delta in\n  // register a4.\n  __ Move(a2, ra);\n  __ AddWord(a3, sp, Operand(kSavedRegistersAreaSize));\n\n  __ SubWord(a3, fp, a3);\n\n  // Allocate a new deoptimizer object.\n  __ PrepareCallCFunction(5, a4);\n  // Pass five arguments, according to n64 ABI.\n  __ Move(a0, zero_reg);\n  Label context_check;\n  __ LoadWord(a1,\n              MemOperand(fp, CommonFrameConstants::kContextOrFrameTypeOffset));\n  __ JumpIfSmi(a1, &context_check);\n  __ LoadWord(a0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n  __ bind(&context_check);\n  __ li(a1, Operand(static_cast<int64_t>(deopt_kind)));\n  // a2: code object address\n  // a3: fp-to-sp delta\n  __ li(a4, ExternalReference::isolate_address(isolate));\n\n  // Call Deoptimizer::New().\n  {\n    AllowExternalCallThatCantCauseGC scope(masm);\n    __ CallCFunction(ExternalReference::new_deoptimizer_function(), 5);\n  }\n\n  // Preserve \"deoptimizer\" object in register a0 and get the input\n  // frame descriptor pointer to a1 (deoptimizer->input_);\n  __ LoadWord(a1, MemOperand(a0, Deoptimizer::input_offset()));\n\n  // Copy core registers into FrameDescription::registers_[kNumRegisters].\n  DCHECK_EQ(Register::kNumRegisters, kNumberOfRegisters);\n  for (int i = 0; i < kNumberOfRegisters; i++) {\n    int offset =\n        (i * kSystemPointerSize) + FrameDescription::registers_offset();\n    if ((saved_regs.bits() & (1 << i)) != 0) {\n      __ LoadWord(a2, MemOperand(sp, i * kSystemPointerSize));\n      __ StoreWord(a2, MemOperand(a1, offset));\n    } else if (v8_flags.debug_code) {\n      __ li(a2, kDebugZapValue);\n      __ StoreWord(a2, MemOperand(a1, offset));\n    }\n  }\n\n  int double_regs_offset = FrameDescription::double_registers_offset();\n  // int simd128_regs_offset = FrameDescription::simd128_registers_offset();\n  //  Copy FPU registers to\n  //  double_registers_[DoubleRegister::kNumAllocatableRegisters]\n  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {\n    int code = config->GetAllocatableDoubleCode(i);\n    int dst_offset = code * kDoubleSize + double_regs_offset;\n    int src_offset =\n        code * kDoubleSize + kNumberOfRegisters * kSystemPointerSize;\n    __ LoadDouble(ft0, MemOperand(sp, src_offset));\n    __ StoreDouble(ft0, MemOperand(a1, dst_offset));\n  }\n  // TODO(riscv): Add Simd128 copy\n\n  // Remove the saved registers from the stack.\n  __ AddWord(sp, sp, Operand(kSavedRegistersAreaSize));\n\n  // Compute a pointer to the unwinding limit in register a2; that is\n  // the first stack slot not part of the input frame.\n  __ LoadWord(a2, MemOperand(a1, FrameDescription::frame_size_offset()));\n  __ AddWord(a2, a2, sp);\n\n  // Unwind the stack down to - but not including - the unwinding\n  // limit and copy the contents of the activation frame to the input\n  // frame description.\n  __ AddWord(a3, a1, Operand(FrameDescription::frame_content_offset()));\n  Label pop_loop;\n  Label pop_loop_header;\n  __ BranchShort(&pop_loop_header);\n  __ bind(&pop_loop);\n  __ pop(a4);\n  __ StoreWord(a4, MemOperand(a3, 0));\n  __ AddWord(a3, a3, kSystemPointerSize);\n  __ bind(&pop_loop_header);\n  __ Branch(&pop_loop, ne, a2, Operand(sp), Label::Distance::kNear);\n  // Compute the output frame in the deoptimizer.\n  __ push(a0);  // Preserve deoptimizer object across call.\n  // a0: deoptimizer object; a1: scratch.\n  __ PrepareCallCFunction(1, a1);\n  // Call Deoptimizer::ComputeOutputFrames().\n  {\n    AllowExternalCallThatCantCauseGC scope(masm);\n    __ CallCFunction(ExternalReference::compute_output_frames_function(), 1);\n  }\n  __ pop(a0);  // Restore deoptimizer object (class Deoptimizer).\n\n  __ LoadWord(sp, MemOperand(a0, Deoptimizer::caller_frame_top_offset()));\n\n  // Replace the current (input) frame with the output frames.\n  Label outer_push_loop, inner_push_loop, outer_loop_header, inner_loop_header;\n  // Outer loop state: a4 = current \"FrameDescription** output_\",\n  // a1 = one past the last FrameDescription**.\n  __ Lw(a1, MemOperand(a0, Deoptimizer::output_count_offset()));\n  __ LoadWord(a4,\n              MemOperand(a0, Deoptimizer::output_offset()));  // a4 is output_.\n  __ CalcScaledAddress(a1, a4, a1, kSystemPointerSizeLog2);\n  __ BranchShort(&outer_loop_header);\n  __ bind(&outer_push_loop);\n  // Inner loop state: a2 = current FrameDescription*, a3 = loop index.\n  __ LoadWord(a2, MemOperand(a4, 0));  // output_[ix]\n  __ LoadWord(a3, MemOperand(a2, FrameDescription::frame_size_offset()));\n  __ BranchShort(&inner_loop_header);\n  __ bind(&inner_push_loop);\n  __ SubWord(a3, a3, Operand(kSystemPointerSize));\n  __ AddWord(a6, a2, Operand(a3));\n  __ LoadWord(a7, MemOperand(a6, FrameDescription::frame_content_offset()));\n  __ push(a7);\n  __ bind(&inner_loop_header);\n  __ Branch(&inner_push_loop, ne, a3, Operand(zero_reg));\n\n  __ AddWord(a4, a4, Operand(kSystemPointerSize));\n  __ bind(&outer_loop_header);\n  __ Branch(&outer_push_loop, lt, a4, Operand(a1));\n\n  __ LoadWord(a1, MemOperand(a0, Deoptimizer::input_offset()));\n  for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {\n    int code = config->GetAllocatableDoubleCode(i);\n    const DoubleRegister fpu_reg = DoubleRegister::from_code(code);\n    int src_offset = code * kDoubleSize + double_regs_offset;\n    __ LoadDouble(fpu_reg, MemOperand(a1, src_offset));\n  }\n\n  // Push pc and continuation from the last output frame.\n  __ LoadWord(a6, MemOperand(a2, FrameDescription::pc_offset()));\n  __ push(a6);\n  __ LoadWord(a6, MemOperand(a2, FrameDescription::continuation_offset()));\n  __ push(a6);\n\n  // Technically restoring 't3' should work unless zero_reg is also restored\n  // but it's safer to check for this.\n  DCHECK(!(restored_regs.has(t3)));\n  // Restore the registers from the last output frame.\n  __ Move(t3, a2);\n  for (int i = kNumberOfRegisters - 1; i >= 0; i--) {\n    int offset =\n        (i * kSystemPointerSize) + FrameDescription::registers_offset();\n    if ((restored_regs.bits() & (1 << i)) != 0) {\n      __ LoadWord(ToRegister(i), MemOperand(t3, offset));\n    }\n  }\n\n  __ pop(t6);  // Get continuation, leave pc on stack.\n  __ pop(ra);\n  __ Jump(t6);\n  __ stop();\n}", "name_and_para": "void Generate_DeoptimizationEntry(MacroAssembler* masm,\n                                  DeoptimizeKind deopt_kind) "}], [{"name": "Builtins::Generate_DirectCEntry", "content": "void Builtins::Generate_DirectCEntry(MacroAssembler* masm) {\n  // The sole purpose of DirectCEntry is for movable callers (e.g. any general\n  // purpose InstructionStream object) to be able to call into C functions that\n  // may trigger GC and thus move the caller.\n  //\n  // DirectCEntry places the return address on the stack (updated by the GC),\n  // making the call GC safe. The irregexp backend relies on this.\n\n  __ Poke<MacroAssembler::kSignLR>(lr, 0);  // Store the return address.\n  __ Blr(x10);                              // Call the C++ function.\n  __ Peek<MacroAssembler::kAuthLR>(lr, 0);  // Return to calling code.\n  __ AssertFPCRState();\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_DirectCEntry(MacroAssembler* masm) "}, {"name": "Builtins::Generate_DirectCEntry", "content": "void Builtins::Generate_DirectCEntry(MacroAssembler* masm) {\n  // The sole purpose of DirectCEntry is for movable callers (e.g. any general\n  // purpose InstructionStream object) to be able to call into C functions that\n  // may trigger GC and thus move the caller.\n  //\n  // DirectCEntry places the return address on the stack (updated by the GC),\n  // making the call GC safe. The irregexp backend relies on this.\n\n  // Make place for arguments to fit C calling convention. Callers use\n  // EnterExitFrame/LeaveExitFrame so they handle stack restoring and we don't\n  // have to do that here. Any caller must drop kCArgsSlotsSize stack space\n  // after the call.\n  __ AddWord(sp, sp, -kCArgsSlotsSize);\n\n  __ StoreWord(ra,\n               MemOperand(sp, kCArgsSlotsSize));  // Store the return address.\n  __ Call(t6);                                    // Call the C++ function.\n  __ LoadWord(t6, MemOperand(sp, kCArgsSlotsSize));  // Return to calling code.\n\n  if (v8_flags.debug_code && v8_flags.enable_slow_asserts) {\n    // In case of an error the return address may point to a memory area\n    // filled with kZapValue by the GC. Dereference the address and check for\n    // this.\n    __ Uld(a4, MemOperand(t6));\n    __ Assert(ne, AbortReason::kReceivedInvalidReturnAddress, a4,\n              Operand(kZapValue));\n  }\n\n  __ Jump(t6);\n}", "name_and_para": "void Builtins::Generate_DirectCEntry(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_CallApiGetter", "content": "void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- cp                  : context\n  //  -- x1                  : receiver\n  //  -- x3                  : accessor info\n  //  -- x0                  : holder\n  // -----------------------------------\n\n  // Build v8::PropertyCallbackInfo::args_ array on the stack and push property\n  // name below the exit frame to make GC aware of them.\n  using PCI = PropertyCallbackInfo<v8::Value>;\n  using PCA = PropertyCallbackArguments;\n  static_assert(PCA::kShouldThrowOnErrorIndex == 0);\n  static_assert(PCA::kHolderIndex == 1);\n  static_assert(PCA::kIsolateIndex == 2);\n  static_assert(PCA::kUnusedIndex == 3);\n  static_assert(PCA::kReturnValueIndex == 4);\n  static_assert(PCA::kDataIndex == 5);\n  static_assert(PCA::kThisIndex == 6);\n  static_assert(PCA::kArgsLength == 7);\n\n  // Set up PropertyCallbackInfo's args_ on the stack as follows:\n  // Target state:\n  //   sp[0 * kSystemPointerSize]: name\n  //   sp[1 * kSystemPointerSize]: kShouldThrowOnErrorIndex   <= PCI:args_\n  //   sp[2 * kSystemPointerSize]: kHolderIndex\n  //   sp[3 * kSystemPointerSize]: kIsolateIndex\n  //   sp[4 * kSystemPointerSize]: kUnusedIndex\n  //   sp[5 * kSystemPointerSize]: kReturnValueIndex\n  //   sp[6 * kSystemPointerSize]: kDataIndex\n  //   sp[7 * kSystemPointerSize]: kThisIndex / receiver\n\n  Register name_arg = kCArgRegs[0];\n  Register property_callback_info_arg = kCArgRegs[1];\n\n  Register api_function_address = x2;\n  Register receiver = ApiGetterDescriptor::ReceiverRegister();\n  Register holder = ApiGetterDescriptor::HolderRegister();\n  Register callback = ApiGetterDescriptor::CallbackRegister();\n  Register data = x4;\n  Register undef = x5;\n  Register isolate_address = x6;\n  Register name = x7;\n  DCHECK(!AreAliased(receiver, holder, callback, data, undef, isolate_address,\n                     name));\n\n  __ LoadTaggedField(data,\n                     FieldMemOperand(callback, AccessorInfo::kDataOffset));\n  __ LoadRoot(undef, RootIndex::kUndefinedValue);\n  __ Mov(isolate_address, ExternalReference::isolate_address(masm->isolate()));\n  __ LoadTaggedField(name,\n                     FieldMemOperand(callback, AccessorInfo::kNameOffset));\n\n  // - PropertyCallbackArguments:\n  //     receiver, data, return value, isolate, holder, should_throw_on_error\n  // - These are followed by the property name, which is also pushed below the\n  //   exit frame to make the GC aware of it.\n  // - Padding\n  Register should_throw_on_error = xzr;\n  Register padding = xzr;\n  __ Push(receiver, data, undef, padding, isolate_address, holder,\n          should_throw_on_error, name);\n\n  // v8::PropertyCallbackInfo::args_ array and name handle.\n  static constexpr int kPaddingOnStackSlots = 0;\n  static constexpr int kNameOnStackSlots = 1;\n  static constexpr int kNameStackIndex = kPaddingOnStackSlots;\n  static constexpr int kPCAStackIndex =\n      kNameOnStackSlots + kPaddingOnStackSlots;\n  static constexpr int kStackUnwindSpace = PCA::kArgsLength + kPCAStackIndex;\n  static_assert(kStackUnwindSpace % 2 == 0,\n                \"slots must be a multiple of 2 for stack pointer alignment\");\n\n  __ RecordComment(\n      \"Load address of v8::PropertyAccessorInfo::args_ array and name handle.\");\n#ifdef V8_ENABLE_DIRECT_LOCAL\n  // name_arg = Local<Name>(name), name value was pushed to GC-ed stack space.\n  __ mov(name_arg, name);\n  USE(kNameStackIndex);\n#else\n  // name_arg = Local<Name>(&name), name value was pushed to GC-ed stack space.\n  __ Add(name_arg, sp, Operand(kNameStackIndex * kSystemPointerSize));\n#endif\n  // property_callback_info_arg = v8::PCI::args_ (= &ShouldThrow)\n  __ Add(property_callback_info_arg, sp,\n         Operand(kPCAStackIndex * kSystemPointerSize));\n\n  const int kApiStackSpace = 1;\n  static_assert(kApiStackSpace * kSystemPointerSize == sizeof(PCI));\n\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  __ EnterExitFrame(x10, kApiStackSpace, StackFrame::EXIT);\n\n  __ RecordComment(\"Create v8::PropertyCallbackInfo object on the stack.\");\n  // Initialize v8::PropertyCallbackInfo::args_ field.\n  __ Poke(property_callback_info_arg, 1 * kSystemPointerSize);\n  // property_callback_info_arg = v8::PropertyCallbackInfo&\n  __ SlotAddress(property_callback_info_arg, 1);\n\n  __ LoadExternalPointerField(\n      api_function_address,\n      FieldMemOperand(callback, AccessorInfo::kMaybeRedirectedGetterOffset),\n      kAccessorInfoGetterTag);\n\n  DCHECK(\n      !AreAliased(api_function_address, property_callback_info_arg, name_arg));\n\n  ExternalReference thunk_ref =\n      ExternalReference::invoke_accessor_getter_callback();\n  // Pass AccessorInfo to thunk wrapper in case profiler or side-effect\n  // checking is enabled.\n  Register thunk_arg = callback;\n\n  MemOperand return_value_operand =\n      ExitFrameCallerStackSlotOperand(kPCAStackIndex + PCA::kReturnValueIndex);\n  MemOperand* const kUseStackSpaceConstant = nullptr;\n\n  const bool with_profiling = true;\n  CallApiFunctionAndReturn(masm, with_profiling, api_function_address,\n                           thunk_ref, thunk_arg, kStackUnwindSpace,\n                           kUseStackSpaceConstant, return_value_operand);\n}", "name_and_para": "void Builtins::Generate_CallApiGetter(MacroAssembler* masm) "}, {"name": "Builtins::Generate_CallApiGetter", "content": "void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- cp                  : context\n  //  -- a1                  : receiver\n  //  -- a3                  : accessor info\n  //  -- a0                  : holder\n  // -----------------------------------\n\n  // Build v8::PropertyCallbackInfo::args_ array on the stack and push property\n  // name below the exit frame to make GC aware of them.\n  using PCI = PropertyCallbackInfo<v8::Value>;\n  using PCA = PropertyCallbackArguments;\n  static_assert(PCA::kShouldThrowOnErrorIndex == 0);\n  static_assert(PCA::kHolderIndex == 1);\n  static_assert(PCA::kIsolateIndex == 2);\n  static_assert(PCA::kUnusedIndex == 3);\n  static_assert(PCA::kReturnValueIndex == 4);\n  static_assert(PCA::kDataIndex == 5);\n  static_assert(PCA::kThisIndex == 6);\n  static_assert(PCA::kArgsLength == 7);\n\n  // Set up PropertyCallbackInfo's args_ on the stack as follows:\n  // Target state:\n  //   sp[0 * kSystemPointerSize]: name\n  //   sp[1 * kSystemPointerSize]: kShouldThrowOnErrorIndex   <= PCI:args_\n  //   sp[2 * kSystemPointerSize]: kHolderIndex\n  //   sp[3 * kSystemPointerSize]: kIsolateIndex\n  //   sp[4 * kSystemPointerSize]: kUnusedIndex\n  //   sp[5 * kSystemPointerSize]: kReturnValueIndex\n  //   sp[6 * kSystemPointerSize]: kDataIndex\n  //   sp[7 * kSystemPointerSize]: kThisIndex / receiver\n\n  Register name_arg = kCArgRegs[0];\n  Register property_callback_info_arg = kCArgRegs[1];\n\n  Register api_function_address = kCArgRegs[2];\n\n  Register receiver = ApiGetterDescriptor::ReceiverRegister();\n  Register holder = ApiGetterDescriptor::HolderRegister();\n  Register callback = ApiGetterDescriptor::CallbackRegister();\n  Register scratch = a4;\n  DCHECK(!AreAliased(receiver, holder, callback, scratch));\n\n  // Here and below +1 is for name() pushed after the args_ array.\n  using PCA = PropertyCallbackArguments;\n  __ SubWord(sp, sp, (PCA::kArgsLength + 1) * kSystemPointerSize);\n  __ StoreWord(receiver,\n               MemOperand(sp, (PCA::kThisIndex + 1) * kSystemPointerSize));\n  __ LoadTaggedField(scratch,\n                     FieldMemOperand(callback, AccessorInfo::kDataOffset));\n  __ StoreWord(scratch,\n               MemOperand(sp, (PCA::kDataIndex + 1) * kSystemPointerSize));\n  __ LoadRoot(scratch, RootIndex::kUndefinedValue);\n  __ StoreWord(scratch, MemOperand(sp, (PCA::kReturnValueIndex + 1) *\n                                           kSystemPointerSize));\n  __ StoreWord(scratch,\n               MemOperand(sp, (PCA::kUnusedIndex + 1) * kSystemPointerSize));\n  __ li(scratch, ExternalReference::isolate_address(masm->isolate()));\n  __ StoreWord(scratch,\n               MemOperand(sp, (PCA::kIsolateIndex + 1) * kSystemPointerSize));\n  __ StoreWord(holder,\n               MemOperand(sp, (PCA::kHolderIndex + 1) * kSystemPointerSize));\n  // should_throw_on_error -> false\n  DCHECK_EQ(0, Smi::zero().ptr());\n  __ StoreWord(zero_reg, MemOperand(sp, (PCA::kShouldThrowOnErrorIndex + 1) *\n                                            kSystemPointerSize));\n  __ LoadTaggedField(scratch,\n                     FieldMemOperand(callback, AccessorInfo::kNameOffset));\n  __ StoreWord(scratch, MemOperand(sp, 0 * kSystemPointerSize));\n\n  // v8::PropertyCallbackInfo::args_ array and name handle.\n  static constexpr int kPaddingOnStackSlots = 0;\n  static constexpr int kNameOnStackSlots = 1;\n  // static constexpr int kNameStackIndex = kPaddingOnStackSlots;\n  static constexpr int kPCAStackIndex =\n      kNameOnStackSlots + kPaddingOnStackSlots;\n  static constexpr int kStackUnwindSpace = PCA::kArgsLength + kPCAStackIndex;\n  static_assert(kStackUnwindSpace % 2 == 0,\n                \"slots must be a multiple of 2 for stack pointer alignment\");\n\n  __ RecordComment(\n      \"Load address of v8::PropertyAccessorInfo::args_ array and name handle.\");\n#ifdef V8_ENABLE_DIRECT_LOCAL\n  // name_arg = Local<Name>(name), name value was pushed to GC-ed stack space.\n  __ Move(name_arg, scratch);\n#else\n  // name_arg = Local<Name>(&name), name value was pushed to GC-ed stack space.\n  __ Move(name_arg, sp);\n#endif\n  // property_callback_info_arg = v8::PCI::args_ (= &ShouldThrow)\n  __ AddWord(property_callback_info_arg, name_arg,\n             Operand(1 * kSystemPointerSize));\n\n  const int kApiStackSpace = 1;\n  static_assert(kApiStackSpace * kSystemPointerSize == sizeof(PCI));\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  __ EnterExitFrame(kApiStackSpace);\n\n  __ RecordComment(\"Create v8::PropertyCallbackInfo object on the stack.\");\n  // Initialize v8::PropertyCallbackInfo::args_ field.\n  __ StoreWord(property_callback_info_arg,\n               MemOperand(sp, 1 * kSystemPointerSize));\n  // property_callback_info_arg = v8::PropertyCallbackInfo&\n  __ AddWord(property_callback_info_arg, sp, Operand(1 * kSystemPointerSize));\n\n  __ RecordComment(\"Load api_function_address\");\n  __ LoadExternalPointerField(\n      api_function_address,\n      FieldMemOperand(callback, AccessorInfo::kMaybeRedirectedGetterOffset),\n      kAccessorInfoGetterTag);\n\n  DCHECK(\n      !AreAliased(api_function_address, property_callback_info_arg, name_arg));\n\n  ExternalReference thunk_ref =\n      ExternalReference::invoke_accessor_getter_callback();\n  // Pass AccessorInfo to thunk wrapper in case profiler or side-effect\n  // checking is enabled.\n  Register thunk_arg = callback;\n\n  MemOperand return_value_operand =\n      ExitFrameCallerStackSlotOperand(kPCAStackIndex + PCA::kReturnValueIndex);\n  MemOperand* const kUseStackSpaceConstant = nullptr;\n\n  const bool with_profiling = true;\n  CallApiFunctionAndReturn(masm, with_profiling, api_function_address,\n                           thunk_ref, thunk_arg, kStackUnwindSpace,\n                           kUseStackSpaceConstant, return_value_operand);\n}", "name_and_para": "void Builtins::Generate_CallApiGetter(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_CallApiCallbackImpl", "content": "void Builtins::Generate_CallApiCallbackImpl(MacroAssembler* masm,\n                                            CallApiCallbackMode mode) {\n  // ----------- S t a t e -------------\n  // CallApiCallbackMode::kGeneric mode:\n  //  -- x2                  : arguments count (not including the receiver)\n  //  -- x3                  : call handler info\n  //  -- x0                  : holder\n  // CallApiCallbackMode::kOptimizedNoProfiling/kOptimized modes:\n  //  -- x1                  : api function address\n  //  -- x2                  : arguments count (not including the receiver)\n  //  -- x3                  : call data\n  //  -- x0                  : holder\n  // Both modes:\n  //  -- cp                  : context\n  //  -- sp[0]               : receiver\n  //  -- sp[8]               : first argument\n  //  -- ...\n  //  -- sp[(argc) * 8]      : last argument\n  // -----------------------------------\n\n  Register function_callback_info_arg = kCArgRegs[0];\n\n  Register api_function_address = no_reg;\n  Register argc = no_reg;\n  Register call_data = no_reg;\n  Register callback = no_reg;\n  Register holder = no_reg;\n  Register topmost_script_having_context = no_reg;\n  Register scratch = x4;\n  Register scratch2 = x5;\n\n  switch (mode) {\n    case CallApiCallbackMode::kGeneric:\n      argc = CallApiCallbackGenericDescriptor::ActualArgumentsCountRegister();\n      topmost_script_having_context = CallApiCallbackGenericDescriptor::\n          TopmostScriptHavingContextRegister();\n      callback =\n          CallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister();\n      holder = CallApiCallbackGenericDescriptor::HolderRegister();\n      break;\n\n    case CallApiCallbackMode::kOptimizedNoProfiling:\n    case CallApiCallbackMode::kOptimized:\n      // Caller context is always equal to current context because we don't\n      // inline Api calls cross-context.\n      topmost_script_having_context = kContextRegister;\n      api_function_address =\n          CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister();\n      argc = CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister();\n      call_data = CallApiCallbackOptimizedDescriptor::CallDataRegister();\n      holder = CallApiCallbackOptimizedDescriptor::HolderRegister();\n      break;\n  }\n  DCHECK(!AreAliased(api_function_address, topmost_script_having_context, argc,\n                     holder, call_data, callback, scratch, scratch2));\n\n  using FCA = FunctionCallbackArguments;\n  using ER = ExternalReference;\n\n  static_assert(FCA::kArgsLength == 6);\n  static_assert(FCA::kNewTargetIndex == 5);\n  static_assert(FCA::kDataIndex == 4);\n  static_assert(FCA::kReturnValueIndex == 3);\n  static_assert(FCA::kUnusedIndex == 2);\n  static_assert(FCA::kIsolateIndex == 1);\n  static_assert(FCA::kHolderIndex == 0);\n\n  // Set up FunctionCallbackInfo's implicit_args on the stack as follows:\n  // Target state:\n  //   sp[0 * kSystemPointerSize]: kHolder   <= FCA::implicit_args_\n  //   sp[1 * kSystemPointerSize]: kIsolate\n  //   sp[2 * kSystemPointerSize]: undefined (padding, unused)\n  //   sp[3 * kSystemPointerSize]: undefined (kReturnValue)\n  //   sp[4 * kSystemPointerSize]: kData\n  //   sp[5 * kSystemPointerSize]: undefined (kNewTarget)\n  // Existing state:\n  //   sp[6 * kSystemPointerSize]:            <= FCA:::values_\n\n  __ StoreRootRelative(IsolateData::topmost_script_having_context_offset(),\n                       topmost_script_having_context);\n\n  if (mode == CallApiCallbackMode::kGeneric) {\n    api_function_address = ReassignRegister(topmost_script_having_context);\n  }\n\n  // Reserve space on the stack.\n  static constexpr int kStackSize = FCA::kArgsLength;\n  static_assert(kStackSize % 2 == 0);\n  __ Claim(kStackSize, kSystemPointerSize);\n\n  // kHolder\n  __ Str(holder, MemOperand(sp, FCA::kHolderIndex * kSystemPointerSize));\n\n  // kIsolate.\n  __ Mov(scratch, ER::isolate_address(masm->isolate()));\n  __ Str(scratch, MemOperand(sp, FCA::kIsolateIndex * kSystemPointerSize));\n\n  // kPadding\n  __ Str(xzr, MemOperand(sp, FCA::kUnusedIndex * kSystemPointerSize));\n\n  // kReturnValue\n  __ LoadRoot(scratch, RootIndex::kUndefinedValue);\n  __ Str(scratch, MemOperand(sp, FCA::kReturnValueIndex * kSystemPointerSize));\n\n  // kData.\n  switch (mode) {\n    case CallApiCallbackMode::kGeneric:\n      __ LoadTaggedField(\n          scratch2,\n          FieldMemOperand(callback, FunctionTemplateInfo::kCallbackDataOffset));\n      __ Str(scratch2, MemOperand(sp, FCA::kDataIndex * kSystemPointerSize));\n      break;\n\n    case CallApiCallbackMode::kOptimizedNoProfiling:\n    case CallApiCallbackMode::kOptimized:\n      __ Str(call_data, MemOperand(sp, FCA::kDataIndex * kSystemPointerSize));\n      break;\n  }\n\n  // kNewTarget.\n  __ Str(scratch, MemOperand(sp, FCA::kNewTargetIndex * kSystemPointerSize));\n\n  // Keep a pointer to kHolder (= implicit_args) in a {holder} register.\n  // We use it below to set up the FunctionCallbackInfo object.\n  __ Mov(holder, sp);\n\n  // Allocate the v8::Arguments structure in the arguments' space, since it's\n  // not controlled by GC.\n  static constexpr int kSlotsToDropOnStackSize = 1 * kSystemPointerSize;\n  static constexpr int kApiStackSpace =\n      (FCA::kSize + kSlotsToDropOnStackSize) / kSystemPointerSize;\n  static_assert(kApiStackSpace == 4);\n  static_assert(FCA::kImplicitArgsOffset == 0);\n  static_assert(FCA::kValuesOffset == 1 * kSystemPointerSize);\n  static_assert(FCA::kLengthOffset == 2 * kSystemPointerSize);\n  const int exit_frame_params_count =\n      mode == CallApiCallbackMode::kGeneric\n          ? ApiCallbackExitFrameConstants::kAdditionalParametersCount\n          : 0;\n\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  if (mode == CallApiCallbackMode::kGeneric) {\n    ASM_CODE_COMMENT_STRING(masm, \"Push API_CALLBACK_EXIT frame arguments\");\n    __ Claim(exit_frame_params_count, kSystemPointerSize);\n\n    // Context parameter and padding.\n    static_assert(ApiCallbackExitFrameConstants::kOptionalPaddingSize ==\n                  kSystemPointerSize);\n    static_assert(ApiCallbackExitFrameConstants::kOptionalPaddingOffset ==\n                  5 * kSystemPointerSize);\n    static_assert(ApiCallbackExitFrameConstants::kContextOffset ==\n                  4 * kSystemPointerSize);\n    __ Stp(kContextRegister, xzr, MemOperand(sp, 2 * kSystemPointerSize));\n\n    // Argc parameter as a Smi.\n    static_assert(ApiCallbackExitFrameConstants::kArgcOffset ==\n                  3 * kSystemPointerSize);\n    __ SmiTag(scratch, argc);\n    __ Str(scratch, MemOperand(sp, 1 * kSystemPointerSize));\n\n    // Target parameter.\n    static_assert(ApiCallbackExitFrameConstants::kTargetOffset ==\n                  2 * kSystemPointerSize);\n    __ Str(callback, MemOperand(sp, 0 * kSystemPointerSize));\n\n    __ LoadExternalPointerField(\n        api_function_address,\n        FieldMemOperand(callback,\n                        FunctionTemplateInfo::kMaybeRedirectedCallbackOffset),\n        kFunctionTemplateInfoCallbackTag);\n\n    __ EnterExitFrame(scratch, kApiStackSpace, StackFrame::API_CALLBACK_EXIT);\n  } else {\n    __ EnterExitFrame(scratch, kApiStackSpace, StackFrame::EXIT);\n  }\n\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Initialize FunctionCallbackInfo\");\n    // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).\n    // Arguments are after the return address(pushed by EnterExitFrame()).\n    __ Str(holder, ExitFrameStackSlotOperand(FCA::kImplicitArgsOffset));\n\n    // FunctionCallbackInfo::values_ (points at the first varargs argument\n    // passed on the stack).\n    __ Add(holder, holder,\n           Operand(FCA::kArgsLengthWithReceiver * kSystemPointerSize));\n    __ Str(holder, ExitFrameStackSlotOperand(FCA::kValuesOffset));\n\n    // FunctionCallbackInfo::length_.\n    __ Str(argc, ExitFrameStackSlotOperand(FCA::kLengthOffset));\n  }\n\n  // We also store the number of slots to drop from the stack after returning\n  // from the API function here.\n  // Note: Unlike on other architectures, this stores the number of slots to\n  // drop, not the number of bytes. arm64 must always drop a slot count that is\n  // a multiple of two, and related helper functions (DropArguments) expect a\n  // register containing the slot count.\n  MemOperand stack_space_operand =\n      ExitFrameStackSlotOperand(FCA::kLengthOffset + kSlotsToDropOnStackSize);\n  __ Add(scratch, argc,\n         Operand(FCA::kArgsLengthWithReceiver + exit_frame_params_count));\n  __ Str(scratch, stack_space_operand);\n\n  __ RecordComment(\"v8::FunctionCallback's argument.\");\n  // function_callback_info_arg = v8::FunctionCallbackInfo&\n  __ add(function_callback_info_arg, sp, Operand(1 * kSystemPointerSize));\n\n  DCHECK(!AreAliased(api_function_address, function_callback_info_arg));\n\n  ExternalReference thunk_ref = ER::invoke_function_callback(mode);\n  // Pass api function address to thunk wrapper in case profiler or side-effect\n  // checking is enabled.\n  Register thunk_arg = api_function_address;\n\n  // The current frame needs to be aligned.\n  DCHECK_EQ(FCA::kArgsLength % 2, 0);\n\n  MemOperand return_value_operand = ExitFrameCallerStackSlotOperand(\n      FCA::kReturnValueIndex + exit_frame_params_count);\n  static constexpr int kUseStackSpaceOperand = 0;\n\n  const bool with_profiling =\n      mode != CallApiCallbackMode::kOptimizedNoProfiling;\n  CallApiFunctionAndReturn(masm, with_profiling, api_function_address,\n                           thunk_ref, thunk_arg, kUseStackSpaceOperand,\n                           &stack_space_operand, return_value_operand);\n}", "name_and_para": "void Builtins::Generate_CallApiCallbackImpl(MacroAssembler* masm,\n                                            CallApiCallbackMode mode) "}, {"name": "Builtins::Generate_CallApiCallbackImpl", "content": "void Builtins::Generate_CallApiCallbackImpl(MacroAssembler* masm,\n                                            CallApiCallbackMode mode) {\n  // ----------- S t a t e -------------\n  // CallApiCallbackMode::kGeneric mode:\n  //  -- a2                  : arguments count (not including the receiver)\n  //  -- a3                  : call handler info\n  //  -- a0                  : holder\n  // CallApiCallbackMode::kOptimizedNoProfiling/kOptimized modes:\n  //  -- a1                  : api function address\n  //  -- a2                  : arguments count\n  //  -- a3                  : call data\n  //  -- a0                  : holder\n  // Both modes:\n  //  -- cp                  : context\n  //  -- sp[0]               : receiver\n  //  -- sp[8]               : first argument\n  //  -- ...\n  //  -- sp[(argc) * 8]      : last argument\n  // -----------------------------------\n  Register function_callback_info_arg = kCArgRegs[0];\n\n  Register api_function_address = no_reg;\n  Register argc = no_reg;\n  Register call_data = no_reg;\n  Register callback = no_reg;\n  Register holder = no_reg;\n  Register topmost_script_having_context = no_reg;\n  Register scratch = t0;\n  Register scratch2 = t1;\n\n  switch (mode) {\n    case CallApiCallbackMode::kGeneric:\n      topmost_script_having_context = CallApiCallbackGenericDescriptor::\n          TopmostScriptHavingContextRegister();\n      argc = CallApiCallbackGenericDescriptor::ActualArgumentsCountRegister();\n      callback =\n          CallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister();\n      holder = CallApiCallbackGenericDescriptor::HolderRegister();\n      break;\n\n    case CallApiCallbackMode::kOptimizedNoProfiling:\n    case CallApiCallbackMode::kOptimized:\n      // Caller context is always equal to current context because we don't\n      // inline Api calls cross-context.\n      topmost_script_having_context = kContextRegister;\n      api_function_address =\n          CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister();\n      argc = CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister();\n      call_data = CallApiCallbackOptimizedDescriptor::CallDataRegister();\n      holder = CallApiCallbackOptimizedDescriptor::HolderRegister();\n      break;\n  }\n  DCHECK(!AreAliased(api_function_address, topmost_script_having_context, argc,\n                     holder, call_data, callback, scratch, scratch2));\n\n  using FCA = FunctionCallbackArguments;\n  using ER = ExternalReference;\n\n  static_assert(FCA::kArgsLength == 6);\n  static_assert(FCA::kNewTargetIndex == 5);\n  static_assert(FCA::kDataIndex == 4);\n  static_assert(FCA::kReturnValueIndex == 3);\n  static_assert(FCA::kUnusedIndex == 2);\n  static_assert(FCA::kIsolateIndex == 1);\n  static_assert(FCA::kHolderIndex == 0);\n\n  // Set up FunctionCallbackInfo's implicit_args on the stack as follows:\n  // Target state:\n  //   sp[0 * kSystemPointerSize]: kHolder   <= FCA::implicit_args_\n  //   sp[1 * kSystemPointerSize]: kIsolate\n  //   sp[2 * kSystemPointerSize]: undefined (padding, unused)\n  //   sp[3 * kSystemPointerSize]: undefined (kReturnValue)\n  //   sp[4 * kSystemPointerSize]: kData\n  //   sp[5 * kSystemPointerSize]: undefined (kNewTarget)\n  // Existing state:\n  //   sp[6 * kSystemPointerSize]:            <= FCA:::values_\n\n  __ StoreRootRelative(IsolateData::topmost_script_having_context_offset(),\n                       topmost_script_having_context);\n  if (mode == CallApiCallbackMode::kGeneric) {\n    api_function_address = ReassignRegister(topmost_script_having_context);\n  }\n  // Reserve space on the stack.\n  static constexpr int kStackSize = FCA::kArgsLength;\n  static_assert(kStackSize % 2 == 0);\n  __ SubWord(sp, sp, Operand(kStackSize * kSystemPointerSize));\n\n  // kHolder.\n  __ StoreWord(holder, MemOperand(sp, FCA::kHolderIndex * kSystemPointerSize));\n\n  // kIsolate.\n  __ li(scratch, ER::isolate_address(masm->isolate()));\n  __ StoreWord(scratch,\n               MemOperand(sp, FCA::kIsolateIndex * kSystemPointerSize));\n\n  // kPadding\n  __ StoreWord(zero_reg,\n               MemOperand(sp, FCA::kUnusedIndex * kSystemPointerSize));\n\n  // kReturnValue\n  __ LoadRoot(scratch, RootIndex::kUndefinedValue);\n  __ StoreWord(scratch,\n               MemOperand(sp, FCA::kReturnValueIndex * kSystemPointerSize));\n\n  // kData.\n  switch (mode) {\n    case CallApiCallbackMode::kGeneric:\n      __ LoadTaggedField(\n          scratch2,\n          FieldMemOperand(callback, FunctionTemplateInfo::kCallbackDataOffset));\n      __ StoreWord(scratch2,\n                   MemOperand(sp, FCA::kDataIndex * kSystemPointerSize));\n      break;\n\n    case CallApiCallbackMode::kOptimizedNoProfiling:\n    case CallApiCallbackMode::kOptimized:\n      __ StoreWord(call_data,\n                   MemOperand(sp, FCA::kDataIndex * kSystemPointerSize));\n      break;\n  }\n\n  // kNewTarget.\n  __ StoreWord(scratch,\n               MemOperand(sp, FCA::kNewTargetIndex * kSystemPointerSize));\n\n  // Keep a pointer to kHolder (= implicit_args) in a {holder} register.\n  // We use it below to set up the FunctionCallbackInfo object.\n  __ Move(holder, sp);\n\n  // Allocate the v8::Arguments structure in the arguments' space since\n  // it's not controlled by GC.\n  static constexpr int kSlotsToDropOnStackSize = 1 * kSystemPointerSize;\n  static constexpr int kApiStackSpace =\n      (FCA::kSize + kSlotsToDropOnStackSize) / kSystemPointerSize;\n  static_assert(kApiStackSpace == 4);\n  static_assert(FCA::kImplicitArgsOffset == 0);\n  static_assert(FCA::kValuesOffset == 1 * kSystemPointerSize);\n  static_assert(FCA::kLengthOffset == 2 * kSystemPointerSize);\n  const int exit_frame_params_count =\n      mode == CallApiCallbackMode::kGeneric\n          ? ApiCallbackExitFrameConstants::kAdditionalParametersCount\n          : 0;\n\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  if (mode == CallApiCallbackMode::kGeneric) {\n    ASM_CODE_COMMENT_STRING(masm, \"Push API_CALLBACK_EXIT frame arguments\");\n    __ AllocateStackSpace(exit_frame_params_count * kSystemPointerSize);\n\n    // No padding is required.\n    static_assert(ApiCallbackExitFrameConstants::kOptionalPaddingSize == 0);\n\n    // Context parameter.\n    static_assert(ApiCallbackExitFrameConstants::kContextOffset ==\n                  4 * kSystemPointerSize);\n    __ StoreWord(kContextRegister, MemOperand(sp, 2 * kSystemPointerSize));\n\n    // Argc parameter as a Smi.\n    static_assert(ApiCallbackExitFrameConstants::kArgcOffset ==\n                  3 * kSystemPointerSize);\n    __ SmiTag(scratch, argc);\n    __ StoreWord(scratch, MemOperand(sp, 1 * kSystemPointerSize));\n\n    // Target parameter.\n    static_assert(ApiCallbackExitFrameConstants::kTargetOffset ==\n                  2 * kSystemPointerSize);\n    __ StoreWord(callback, MemOperand(sp, 0 * kSystemPointerSize));\n\n    __ LoadExternalPointerField(\n        api_function_address,\n        FieldMemOperand(callback,\n                        FunctionTemplateInfo::kMaybeRedirectedCallbackOffset),\n        kFunctionTemplateInfoCallbackTag);\n\n    __ EnterExitFrame(kApiStackSpace, StackFrame::API_CALLBACK_EXIT);\n  } else {\n    __ EnterExitFrame(kApiStackSpace, StackFrame::EXIT);\n  }\n\n  // EnterExitFrame may align the sp.\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Initialize FunctionCallbackInfo\");\n    // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).\n    // Arguments are after the return address (pushed by EnterExitFrame()).\n    __ StoreWord(holder, ExitFrameStackSlotOperand(FCA::kImplicitArgsOffset));\n\n    // FunctionCallbackInfo::values_ (points at the first varargs argument\n    // passed on the stack).\n    __ AddWord(holder, holder,\n               Operand(FCA::kArgsLengthWithReceiver * kSystemPointerSize));\n    __ StoreWord(holder, ExitFrameStackSlotOperand(FCA::kValuesOffset));\n\n    // FunctionCallbackInfo::length_.\n    // Stored as int field, 32-bit integers within struct on stack always left\n    // justified by n64 ABI.\n    __ Sw(argc, ExitFrameStackSlotOperand(FCA::kLengthOffset));\n  }\n  // We also store the number of bytes to drop from the stack after returning\n  // from the API function here.\n  // Note: Unlike on other architectures, this stores the number of slots to\n  // drop, not the number of bytes.\n  MemOperand stack_space_operand =\n      ExitFrameStackSlotOperand(FCA::kLengthOffset + kSlotsToDropOnStackSize);\n  __ AddWord(scratch, argc,\n             Operand(FCA::kArgsLengthWithReceiver + exit_frame_params_count));\n  __ StoreWord(scratch, stack_space_operand);\n\n  __ RecordComment(\"v8::FunctionCallback's argument.\");\n  // function_callback_info_arg = v8::FunctionCallbackInfo&\n  __ AddWord(function_callback_info_arg, sp, Operand(1 * kSystemPointerSize));\n\n  DCHECK(!AreAliased(api_function_address, function_callback_info_arg));\n\n  ExternalReference thunk_ref = ER::invoke_function_callback(mode);\n\n  MemOperand return_value_operand = ExitFrameCallerStackSlotOperand(\n      FCA::kReturnValueIndex + exit_frame_params_count);\n\n  // Pass api function address to thunk wrapper in case profiler or side-effect\n  // checking is enabled.\n  Register thunk_arg = api_function_address;\n\n  static constexpr int kUseStackSpaceOperand = 0;\n\n  const bool with_profiling =\n      mode != CallApiCallbackMode::kOptimizedNoProfiling;\n  CallApiFunctionAndReturn(masm, with_profiling, api_function_address,\n                           thunk_ref, thunk_arg, kUseStackSpaceOperand,\n                           &stack_space_operand, return_value_operand);\n}", "name_and_para": "void Builtins::Generate_CallApiCallbackImpl(MacroAssembler* masm,\n                                            CallApiCallbackMode mode) "}], [{"name": "Builtins::Generate_DoubleToI", "content": "void Builtins::Generate_DoubleToI(MacroAssembler* masm) {\n  Label done;\n  Register result = x7;\n\n  DCHECK(result.Is64Bits());\n\n  HardAbortScope hard_abort(masm);  // Avoid calls to Abort.\n  UseScratchRegisterScope temps(masm);\n  Register scratch1 = temps.AcquireX();\n  Register scratch2 = temps.AcquireX();\n  DoubleRegister double_scratch = temps.AcquireD();\n\n  // Account for saved regs.\n  const int kArgumentOffset = 2 * kSystemPointerSize;\n\n  __ Push(result, scratch1);  // scratch1 is also pushed to preserve alignment.\n  __ Peek(double_scratch, kArgumentOffset);\n\n  // Try to convert with a FPU convert instruction.  This handles all\n  // non-saturating cases.\n  __ TryConvertDoubleToInt64(result, double_scratch, &done);\n  __ Fmov(result, double_scratch);\n\n  // If we reach here we need to manually convert the input to an int32.\n\n  // Extract the exponent.\n  Register exponent = scratch1;\n  __ Ubfx(exponent, result, HeapNumber::kMantissaBits,\n          HeapNumber::kExponentBits);\n\n  // It the exponent is >= 84 (kMantissaBits + 32), the result is always 0 since\n  // the mantissa gets shifted completely out of the int32_t result.\n  __ Cmp(exponent, HeapNumber::kExponentBias + HeapNumber::kMantissaBits + 32);\n  __ CzeroX(result, ge);\n  __ B(ge, &done);\n\n  // The Fcvtzs sequence handles all cases except where the conversion causes\n  // signed overflow in the int64_t target. Since we've already handled\n  // exponents >= 84, we can guarantee that 63 <= exponent < 84.\n\n  if (v8_flags.debug_code) {\n    __ Cmp(exponent, HeapNumber::kExponentBias + 63);\n    // Exponents less than this should have been handled by the Fcvt case.\n    __ Check(ge, AbortReason::kUnexpectedValue);\n  }\n\n  // Isolate the mantissa bits, and set the implicit '1'.\n  Register mantissa = scratch2;\n  __ Ubfx(mantissa, result, 0, HeapNumber::kMantissaBits);\n  __ Orr(mantissa, mantissa, 1ULL << HeapNumber::kMantissaBits);\n\n  // Negate the mantissa if necessary.\n  __ Tst(result, kXSignMask);\n  __ Cneg(mantissa, mantissa, ne);\n\n  // Shift the mantissa bits in the correct place. We know that we have to shift\n  // it left here, because exponent >= 63 >= kMantissaBits.\n  __ Sub(exponent, exponent,\n         HeapNumber::kExponentBias + HeapNumber::kMantissaBits);\n  __ Lsl(result, mantissa, exponent);\n\n  __ Bind(&done);\n  __ Poke(result, kArgumentOffset);\n  __ Pop(scratch1, result);\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_DoubleToI(MacroAssembler* masm) "}, {"name": "Builtins::Generate_DoubleToI", "content": "void Builtins::Generate_DoubleToI(MacroAssembler* masm) {\n  Label done;\n  Register result_reg = t0;\n\n  Register scratch = GetRegisterThatIsNotOneOf(result_reg);\n  Register scratch2 = GetRegisterThatIsNotOneOf(result_reg, scratch);\n  Register scratch3 = GetRegisterThatIsNotOneOf(result_reg, scratch, scratch2);\n  DoubleRegister double_scratch = kScratchDoubleReg;\n\n  // Account for saved regs.\n  const int kArgumentOffset = 4 * kSystemPointerSize;\n\n  __ Push(result_reg);\n  __ Push(scratch, scratch2, scratch3);\n\n  // Load double input.\n  __ LoadDouble(double_scratch, MemOperand(sp, kArgumentOffset));\n\n  // Try a conversion to a signed integer, if exception occurs, scratch is\n  // set to 0\n  __ Trunc_w_d(scratch3, double_scratch, scratch);\n\n  // If we had no exceptions then set result_reg and we are done.\n  Label error;\n  __ Branch(&error, eq, scratch, Operand(zero_reg), Label::Distance::kNear);\n  __ Move(result_reg, scratch3);\n  __ Branch(&done);\n  __ bind(&error);\n\n  // Load the double value and perform a manual truncation.\n  Register input_high = scratch2;\n  Register input_low = scratch3;\n\n  __ Lw(input_low, MemOperand(sp, kArgumentOffset + Register::kMantissaOffset));\n  __ Lw(input_high,\n        MemOperand(sp, kArgumentOffset + Register::kExponentOffset));\n\n  Label normal_exponent;\n  // Extract the biased exponent in result.\n  __ ExtractBits(result_reg, input_high, HeapNumber::kExponentShift,\n                 HeapNumber::kExponentBits);\n\n  // Check for Infinity and NaNs, which should return 0.\n  __ Sub32(scratch, result_reg, HeapNumber::kExponentMask);\n  __ LoadZeroIfConditionZero(\n      result_reg,\n      scratch);  // result_reg = scratch == 0 ? 0 : result_reg\n  __ Branch(&done, eq, scratch, Operand(zero_reg));\n\n  // Express exponent as delta to (number of mantissa bits + 31).\n  __ Sub32(result_reg, result_reg,\n           Operand(HeapNumber::kExponentBias + HeapNumber::kMantissaBits + 31));\n\n  // If the delta is strictly positive, all bits would be shifted away,\n  // which means that we can return 0.\n  __ Branch(&normal_exponent, le, result_reg, Operand(zero_reg),\n            Label::Distance::kNear);\n  __ Move(result_reg, zero_reg);\n  __ Branch(&done);\n\n  __ bind(&normal_exponent);\n  const int kShiftBase = HeapNumber::kNonMantissaBitsInTopWord - 1;\n  // Calculate shift.\n  __ Add32(scratch, result_reg,\n           Operand(kShiftBase + HeapNumber::kMantissaBits));\n\n  // Save the sign.\n  Register sign = result_reg;\n  result_reg = no_reg;\n  __ And(sign, input_high, Operand(HeapNumber::kSignMask));\n\n  // We must specially handle shifts greater than 31.\n  Label high_shift_needed, high_shift_done;\n  __ Branch(&high_shift_needed, lt, scratch, Operand(32),\n            Label::Distance::kNear);\n  __ Move(input_high, zero_reg);\n  __ BranchShort(&high_shift_done);\n  __ bind(&high_shift_needed);\n\n  // Set the implicit 1 before the mantissa part in input_high.\n  __ Or(input_high, input_high,\n        Operand(1 << HeapNumber::kMantissaBitsInTopWord));\n  // Shift the mantissa bits to the correct position.\n  // We don't need to clear non-mantissa bits as they will be shifted away.\n  // If they weren't, it would mean that the answer is in the 32bit range.\n  __ Sll32(input_high, input_high, scratch);\n\n  __ bind(&high_shift_done);\n\n  // Replace the shifted bits with bits from the lower mantissa word.\n  Label pos_shift, shift_done, sign_negative;\n  __ li(kScratchReg, 32);\n  __ Sub32(scratch, kScratchReg, scratch);\n  __ Branch(&pos_shift, ge, scratch, Operand(zero_reg), Label::Distance::kNear);\n\n  // Negate scratch.\n  __ Sub32(scratch, zero_reg, scratch);\n  __ Sll32(input_low, input_low, scratch);\n  __ BranchShort(&shift_done);\n\n  __ bind(&pos_shift);\n  __ Srl32(input_low, input_low, scratch);\n\n  __ bind(&shift_done);\n  __ Or(input_high, input_high, Operand(input_low));\n  // Restore sign if necessary.\n  __ Move(scratch, sign);\n  result_reg = sign;\n  sign = no_reg;\n  __ Sub32(result_reg, zero_reg, input_high);\n  __ Branch(&sign_negative, ne, scratch, Operand(zero_reg),\n            Label::Distance::kNear);\n  __ Move(result_reg, input_high);\n  __ bind(&sign_negative);\n\n  __ bind(&done);\n\n  __ StoreWord(result_reg, MemOperand(sp, kArgumentOffset));\n  __ Pop(scratch, scratch2, scratch3);\n  __ Pop(result_reg);\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_DoubleToI(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_CEntry", "content": "void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,\n                               ArgvMode argv_mode, bool builtin_exit_frame,\n                               bool switch_to_central_stack) {\n  ASM_LOCATION(\"CEntry::Generate entry\");\n\n  using ER = ExternalReference;\n\n  // Register parameters:\n  //    x0: argc (including receiver, untagged)\n  //    x1: target\n  // If argv_mode == ArgvMode::kRegister:\n  //    x11: argv (pointer to first argument)\n  //\n  // The stack on entry holds the arguments and the receiver, with the receiver\n  // at the highest address:\n  //\n  //    sp[argc-1]: receiver\n  //    sp[argc-2]: arg[argc-2]\n  //    ...           ...\n  //    sp[1]:      arg[1]\n  //    sp[0]:      arg[0]\n  //\n  // The arguments are in reverse order, so that arg[argc-2] is actually the\n  // first argument to the target function and arg[0] is the last.\n  static constexpr Register argc_input = x0;\n  static constexpr Register target_input = x1;\n  // Initialized below if ArgvMode::kStack.\n  static constexpr Register argv_input = x11;\n\n  if (argv_mode == ArgvMode::kStack) {\n    // Derive argv from the stack pointer so that it points to the first\n    // argument.\n    __ SlotAddress(argv_input, argc_input);\n    __ Sub(argv_input, argv_input, kReceiverOnStackSize);\n  }\n\n  // If ArgvMode::kStack, argc is reused below and must be retained across the\n  // call in a callee-saved register.\n  static constexpr Register argc = x22;\n\n  // Enter the exit frame.\n  const int kNoExtraSpace = 0;\n  FrameScope scope(masm, StackFrame::MANUAL);\n  __ EnterExitFrame(\n      x10, kNoExtraSpace,\n      builtin_exit_frame ? StackFrame::BUILTIN_EXIT : StackFrame::EXIT);\n\n  if (argv_mode == ArgvMode::kStack) {\n    __ Mov(argc, argc_input);\n  }\n\n#if V8_ENABLE_WEBASSEMBLY\n  if (switch_to_central_stack) {\n    SwitchToTheCentralStackIfNeeded(masm, argc_input, target_input, argv_input);\n  }\n#endif  // V8_ENABLE_WEBASSEMBLY\n\n  // x21 : argv\n  // x22 : argc\n  // x23 : call target\n  //\n  // The stack (on entry) holds the arguments and the receiver, with the\n  // receiver at the highest address:\n  //\n  //         argv[8]:     receiver\n  // argv -> argv[0]:     arg[argc-2]\n  //         ...          ...\n  //         argv[...]:   arg[1]\n  //         argv[...]:   arg[0]\n  //\n  // Immediately below (after) this is the exit frame, as constructed by\n  // EnterExitFrame:\n  //         fp[8]:    CallerPC (lr)\n  //   fp -> fp[0]:    CallerFP (old fp)\n  //         fp[-8]:   Space reserved for SPOffset.\n  //         fp[-16]:  CodeObject()\n  //         sp[...]:  Saved doubles, if saved_doubles is true.\n  //         sp[16]:   Alignment padding, if necessary.\n  //         sp[8]:   Preserved x22 (used for argc).\n  //   sp -> sp[0]:    Space reserved for the return address.\n\n  // TODO(jgruber): Swap these registers in the calling convention instead.\n  static_assert(target_input == x1);\n  static_assert(argv_input == x11);\n  __ Swap(target_input, argv_input);\n  static constexpr Register target = x11;\n  static constexpr Register argv = x1;\n  static_assert(!AreAliased(argc_input, argc, target, argv));\n\n  // Prepare AAPCS64 arguments to pass to the builtin.\n  static_assert(argc_input == x0);  // Already in the right spot.\n  static_assert(argv == x1);        // Already in the right spot.\n  __ Mov(x2, ER::isolate_address(masm->isolate()));\n\n  __ StoreReturnAddressAndCall(target);\n\n  // Result returned in x0 or x1:x0 - do not destroy these registers!\n\n  //  x0    result0      The return code from the call.\n  //  x1    result1      For calls which return ObjectPair.\n  //  x22   argc         .. only if ArgvMode::kStack.\n  const Register& result = x0;\n\n#if V8_ENABLE_WEBASSEMBLY\n  if (switch_to_central_stack) {\n    SwitchFromTheCentralStackIfNeeded(masm);\n  }\n#endif  // V8_ENABLE_WEBASSEMBLY\n\n  // Check result for exception sentinel.\n  Label exception_returned;\n  // The returned value may be a trusted object, living outside of the main\n  // pointer compression cage, so we need to use full pointer comparison here.\n  __ CompareRoot(result, RootIndex::kException, ComparisonMode::kFullPointer);\n  __ B(eq, &exception_returned);\n\n  // The call succeeded, so unwind the stack and return.\n  if (argv_mode == ArgvMode::kStack) {\n    __ Mov(x11, argc);  // x11 used as scratch, just til DropArguments below.\n    __ LeaveExitFrame(x10, x9);\n    __ DropArguments(x11);\n  } else {\n    __ LeaveExitFrame(x10, x9);\n  }\n\n  __ AssertFPCRState();\n  __ Ret();\n\n  // Handling of exception.\n  __ Bind(&exception_returned);\n\n  // Ask the runtime for help to determine the handler. This will set x0 to\n  // contain the current exception, don't clobber it.\n  {\n    FrameScope scope(masm, StackFrame::MANUAL);\n    __ Mov(x0, 0);  // argc.\n    __ Mov(x1, 0);  // argv.\n    __ Mov(x2, ER::isolate_address(masm->isolate()));\n    __ CallCFunction(ER::Create(Runtime::kUnwindAndFindExceptionHandler), 3,\n                     SetIsolateDataSlots::kNo);\n  }\n\n  // Retrieve the handler context, SP and FP.\n  __ Mov(cp, ER::Create(IsolateAddressId::kPendingHandlerContextAddress,\n                        masm->isolate()));\n  __ Ldr(cp, MemOperand(cp));\n  {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Mov(scratch, ER::Create(IsolateAddressId::kPendingHandlerSPAddress,\n                               masm->isolate()));\n    __ Ldr(scratch, MemOperand(scratch));\n    __ Mov(sp, scratch);\n  }\n  __ Mov(fp, ER::Create(IsolateAddressId::kPendingHandlerFPAddress,\n                        masm->isolate()));\n  __ Ldr(fp, MemOperand(fp));\n\n  // If the handler is a JS frame, restore the context to the frame. Note that\n  // the context will be set to (cp == 0) for non-JS frames.\n  Label not_js_frame;\n  __ Cbz(cp, &not_js_frame);\n  __ Str(cp, MemOperand(fp, StandardFrameConstants::kContextOffset));\n  __ Bind(&not_js_frame);\n\n  {\n    // Clear c_entry_fp, like we do in `LeaveExitFrame`.\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Mov(scratch,\n           ER::Create(IsolateAddressId::kCEntryFPAddress, masm->isolate()));\n    __ Str(xzr, MemOperand(scratch));\n  }\n\n  // Compute the handler entry address and jump to it. We use x17 here for the\n  // jump target, as this jump can occasionally end up at the start of\n  // InterpreterEnterAtBytecode, which when CFI is enabled starts with\n  // a \"BTI c\".\n  UseScratchRegisterScope temps(masm);\n  temps.Exclude(x17);\n  __ Mov(x17, ER::Create(IsolateAddressId::kPendingHandlerEntrypointAddress,\n                         masm->isolate()));\n  __ Ldr(x17, MemOperand(x17));\n  __ Br(x17);\n}", "name_and_para": "void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,\n                               ArgvMode argv_mode, bool builtin_exit_frame,\n                               bool switch_to_central_stack) "}, {"name": "Builtins::Generate_CEntry", "content": "void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,\n                               ArgvMode argv_mode, bool builtin_exit_frame,\n                               bool switch_to_central_stack) {\n  // Called from JavaScript; parameters are on stack as if calling JS function\n  // a0: number of arguments including receiver\n  // a1: pointer to builtin function\n  // fp: frame pointer    (restored after C call)\n  // sp: stack pointer    (restored as callee's sp after C call)\n  // cp: current context  (C callee-saved)\n  //\n  // If argv_mode == ArgvMode::kRegister:\n  // a2: pointer to the first argument\n\n  if (argv_mode == ArgvMode::kRegister) {\n    // Move argv into the correct register.\n    __ Move(s1, a2);\n  } else {\n    // Compute the argv pointer in a callee-saved register.\n    __ CalcScaledAddress(s1, sp, a0, kSystemPointerSizeLog2);\n    __ SubWord(s1, s1, kSystemPointerSize);\n  }\n\n  // Enter the exit frame that transitions from JavaScript to C++.\n  FrameScope scope(masm, StackFrame::MANUAL);\n  __ EnterExitFrame(\n      0, builtin_exit_frame ? StackFrame::BUILTIN_EXIT : StackFrame::EXIT);\n\n  // s3: number of arguments  including receiver (C callee-saved)\n  // s1: pointer to first argument (C callee-saved)\n  // s2: pointer to builtin function (C callee-saved)\n\n  // Prepare arguments for C routine.\n  // a0 = argc\n  __ Move(s3, a0);\n  __ Move(s2, a1);\n\n  // We are calling compiled C/C++ code. a0 and a1 hold our two arguments. We\n  // also need to reserve the 4 argument slots on the stack.\n\n  __ AssertStackIsAligned();\n\n  // a0 = argc, a1 = argv, a2 = isolate\n  __ li(a2, ExternalReference::isolate_address(masm->isolate()));\n  __ Move(a1, s1);\n\n  __ StoreReturnAddressAndCall(s2);\n\n  // Result returned in a0 or a1:a0 - do not destroy these registers!\n\n  // Check result for exception sentinel.\n  Label exception_returned;\n  // The returned value may be a trusted object, living outside of the main\n  // pointer compression cage, so we need to use full pointer comparison here.\n  __ CompareRootAndBranch(a0, RootIndex::kException, eq, &exception_returned,\n                          ComparisonMode::kFullPointer);\n\n  // Check that there is no exception, otherwise we\n  // should have returned the exception sentinel.\n  if (v8_flags.debug_code) {\n    Label okay;\n    ExternalReference exception_address = ExternalReference::Create(\n        IsolateAddressId::kExceptionAddress, masm->isolate());\n    __ li(a2, exception_address);\n#ifndef V8_ENABLE_SANDBOX\n    __ LoadWord(a2, MemOperand(a2));\n#else\n    __ Lwu(a2, MemOperand(a2));\n#endif\n    // Cannot use check here as it attempts to generate call into runtime.\n    __ Branch(&okay, eq, a2, RootIndex::kTheHoleValue);\n    __ stop();\n    __ bind(&okay);\n  }\n\n  // Exit C frame and return.\n  // a0:a1: result\n  // sp: stack pointer\n  // fp: frame pointer\n  Register argc = argv_mode == ArgvMode::kRegister\n                      // We don't want to pop arguments so set argc to no_reg.\n                      ? no_reg\n                      // s3: still holds argc (callee-saved).\n                      : s3;\n  __ LeaveExitFrame(argc, EMIT_RETURN);\n\n  // Handling of exception.\n  __ bind(&exception_returned);\n\n  ExternalReference pending_handler_context_address = ExternalReference::Create(\n      IsolateAddressId::kPendingHandlerContextAddress, masm->isolate());\n  ExternalReference pending_handler_entrypoint_address =\n      ExternalReference::Create(\n          IsolateAddressId::kPendingHandlerEntrypointAddress, masm->isolate());\n  ExternalReference pending_handler_fp_address = ExternalReference::Create(\n      IsolateAddressId::kPendingHandlerFPAddress, masm->isolate());\n  ExternalReference pending_handler_sp_address = ExternalReference::Create(\n      IsolateAddressId::kPendingHandlerSPAddress, masm->isolate());\n\n  // Ask the runtime for help to determine the handler. This will set a0 to\n  // contain the current exception, don't clobber it.\n  ExternalReference find_handler =\n      ExternalReference::Create(Runtime::kUnwindAndFindExceptionHandler);\n  {\n    FrameScope scope(masm, StackFrame::MANUAL);\n    __ PrepareCallCFunction(3, 0, a0);\n    __ Move(a0, zero_reg);\n    __ Move(a1, zero_reg);\n    __ li(a2, ExternalReference::isolate_address(masm->isolate()));\n    __ CallCFunction(find_handler, 3, SetIsolateDataSlots::kNo);\n  }\n\n  // Retrieve the handler context, SP and FP.\n  __ li(cp, pending_handler_context_address);\n  __ LoadWord(cp, MemOperand(cp));\n  __ li(sp, pending_handler_sp_address);\n  __ LoadWord(sp, MemOperand(sp));\n  __ li(fp, pending_handler_fp_address);\n  __ LoadWord(fp, MemOperand(fp));\n\n  // If the handler is a JS frame, restore the context to the frame. Note that\n  // the context will be set to (cp == 0) for non-JS frames.\n  Label zero;\n  __ Branch(&zero, eq, cp, Operand(zero_reg), Label::Distance::kNear);\n  __ StoreWord(cp, MemOperand(fp, StandardFrameConstants::kContextOffset));\n  __ bind(&zero);\n\n  // Clear c_entry_fp, like we do in `LeaveExitFrame`.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ li(scratch, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,\n                                             masm->isolate()));\n    __ StoreWord(zero_reg, MemOperand(scratch, 0));\n  }\n\n  // Compute the handler entry address and jump to it.\n  UseScratchRegisterScope temp(masm);\n  Register scratch = temp.Acquire();\n  __ li(scratch, pending_handler_entrypoint_address);\n  __ LoadWord(scratch, MemOperand(scratch));\n  __ Jump(scratch);\n}", "name_and_para": "void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,\n                               ArgvMode argv_mode, bool builtin_exit_frame,\n                               bool switch_to_central_stack) "}], [{"name": "Builtins::Generate_WasmToOnHeapWasmToJsTrampoline", "content": "void Builtins::Generate_WasmToOnHeapWasmToJsTrampoline(MacroAssembler* masm) {\n  // Load the code pointer from the WasmApiFunctionRef and tail-call there.\n  Register api_function_ref = wasm::kGpParamRegisters[0];\n  // Use x17 which is not in kGpParamRegisters and allows to jump to a \"bti c\"\n  // marker.\n  Register call_target = x17;\n  UseScratchRegisterScope temps{masm};\n  temps.Exclude(call_target);\n#ifdef V8_ENABLE_SANDBOX\n  __ LoadCodeEntrypointViaCodePointer(\n      call_target,\n      FieldMemOperand(api_function_ref, WasmApiFunctionRef::kCodeOffset),\n      kWasmEntrypointTag);\n#else\n  Register code = call_target;\n  __ Ldr(code,\n         FieldMemOperand(api_function_ref, WasmApiFunctionRef::kCodeOffset));\n  __ Ldr(call_target, FieldMemOperand(code, Code::kInstructionStartOffset));\n#endif\n  __ Jump(call_target);\n}", "name_and_para": "void Builtins::Generate_WasmToOnHeapWasmToJsTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmToOnHeapWasmToJsTrampoline", "content": "void Builtins::Generate_WasmToOnHeapWasmToJsTrampoline(MacroAssembler* masm) {\n  // Load the code pointer from the WasmApiFunctionRef and tail-call there.\n  Register api_function_ref = wasm::kGpParamRegisters[0];\n  // Use t6 which is not in kGpParamRegisters.\n  Register call_target = t6;\n  UseScratchRegisterScope temps{masm};\n  temps.Exclude(t6);\n#ifdef V8_ENABLE_SANDBOX\n  __ LoadCodeEntrypointViaCodePointer(\n      call_target,\n      FieldMemOperand(api_function_ref, WasmApiFunctionRef::kCodeOffset),\n      kWasmEntrypointTag);\n#else\n  Register code = call_target;\n  __ LoadTaggedField(\n      code, FieldMemOperand(api_function_ref, WasmApiFunctionRef::kCodeOffset));\n  __ LoadWord(call_target,\n              FieldMemOperand(code, Code::kInstructionStartOffset));\n#endif\n  __ Jump(call_target);\n}", "name_and_para": "void Builtins::Generate_WasmToOnHeapWasmToJsTrampoline(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmReturnPromiseOnSuspendAsm", "content": "void Builtins::Generate_WasmReturnPromiseOnSuspendAsm(MacroAssembler* masm) {\n  JSToWasmWrapperHelper(masm, true);\n}", "name_and_para": "void Builtins::Generate_WasmReturnPromiseOnSuspendAsm(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmReturnPromiseOnSuspendAsm", "content": "void Builtins::Generate_WasmReturnPromiseOnSuspendAsm(MacroAssembler* masm) {\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmReturnPromiseOnSuspendAsm(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSToWasmWrapperAsm", "content": "void Builtins::Generate_JSToWasmWrapperAsm(MacroAssembler* masm) {\n  JSToWasmWrapperHelper(masm, false);\n}", "name_and_para": "void Builtins::Generate_JSToWasmWrapperAsm(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSToWasmWrapperAsm", "content": "void Builtins::Generate_JSToWasmWrapperAsm(MacroAssembler* masm) { __ Trap(); }", "name_and_para": "void Builtins::Generate_JSToWasmWrapperAsm(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmOnStackReplace", "content": "void Builtins::Generate_WasmOnStackReplace(MacroAssembler* masm) {\n  // Only needed on x64.\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmOnStackReplace(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmOnStackReplace", "content": "void Builtins::Generate_WasmOnStackReplace(MacroAssembler* masm) {\n  // Only needed on x64.\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmOnStackReplace(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmReject", "content": "void Builtins::Generate_WasmReject(MacroAssembler* masm) {\n  Generate_WasmResumeHelper(masm, wasm::OnResume::kThrow);\n}", "name_and_para": "void Builtins::Generate_WasmReject(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmReject", "content": "void Builtins::Generate_WasmReject(MacroAssembler* masm) {\n  // TODO(v8:12191): Implement for this platform.\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmReject(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmResume", "content": "void Builtins::Generate_WasmResume(MacroAssembler* masm) {\n  Generate_WasmResumeHelper(masm, wasm::OnResume::kContinue);\n}", "name_and_para": "void Builtins::Generate_WasmResume(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmResume", "content": "void Builtins::Generate_WasmResume(MacroAssembler* masm) {\n  // TODO(v8:12191): Implement for this platform.\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmResume(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmSuspend", "content": "void Builtins::Generate_WasmSuspend(MacroAssembler* masm) {\n  auto regs = RegisterAllocator::WithAllocatableGeneralRegisters();\n  // Set up the stackframe.\n  __ EnterFrame(StackFrame::STACK_SWITCH);\n\n  DEFINE_PINNED(suspender, x0);\n  DEFINE_PINNED(context, kContextRegister);\n\n  __ Sub(sp, sp,\n         Immediate(StackSwitchFrameConstants::kNumSpillSlots *\n                   kSystemPointerSize));\n  // Set a sentinel value for the spill slots visited by the GC.\n  ResetStackSwitchFrameStackSlots(masm);\n\n  // -------------------------------------------\n  // Save current state in active jump buffer.\n  // -------------------------------------------\n  Label resume;\n  DEFINE_REG(continuation);\n  __ LoadRoot(continuation, RootIndex::kActiveContinuation);\n  DEFINE_REG(jmpbuf);\n  DEFINE_REG(scratch);\n  __ LoadExternalPointerField(\n      jmpbuf,\n      FieldMemOperand(continuation, WasmContinuationObject::kJmpbufOffset),\n      kWasmContinuationJmpbufTag);\n  FillJumpBuffer(masm, jmpbuf, &resume, scratch);\n  SwitchStackState(masm, jmpbuf, scratch, wasm::JumpBuffer::Active,\n                   wasm::JumpBuffer::Inactive);\n  __ Move(scratch, Smi::FromInt(WasmSuspenderObject::kSuspended));\n  __ StoreTaggedField(\n      scratch,\n      FieldMemOperand(suspender, WasmSuspenderObject::kStateOffset));\n  regs.ResetExcept(suspender, continuation);\n\n  DEFINE_REG(suspender_continuation);\n  __ LoadTaggedField(\n      suspender_continuation,\n      FieldMemOperand(suspender, WasmSuspenderObject::kContinuationOffset));\n  if (v8_flags.debug_code) {\n    // -------------------------------------------\n    // Check that the suspender's continuation is the active continuation.\n    // -------------------------------------------\n    // TODO(thibaudm): Once we add core stack-switching instructions, this\n    // check will not hold anymore: it's possible that the active continuation\n    // changed (due to an internal switch), so we have to update the suspender.\n    __ cmp(suspender_continuation, continuation);\n    Label ok;\n    __ B(&ok, eq);\n    __ Trap();\n    __ bind(&ok);\n  }\n  FREE_REG(continuation);\n  // -------------------------------------------\n  // Update roots.\n  // -------------------------------------------\n  DEFINE_REG(caller);\n  __ LoadTaggedField(caller,\n                     FieldMemOperand(suspender_continuation,\n                                     WasmContinuationObject::kParentOffset));\n  int32_t active_continuation_offset =\n      MacroAssembler::RootRegisterOffsetForRootIndex(\n          RootIndex::kActiveContinuation);\n  __ Str(caller, MemOperand(kRootRegister, active_continuation_offset));\n  DEFINE_REG(parent);\n  __ LoadTaggedField(\n      parent, FieldMemOperand(suspender, WasmSuspenderObject::kParentOffset));\n  int32_t active_suspender_offset =\n      MacroAssembler::RootRegisterOffsetForRootIndex(\n          RootIndex::kActiveSuspender);\n  __ Str(parent, MemOperand(kRootRegister, active_suspender_offset));\n  regs.ResetExcept(suspender, caller);\n\n  // -------------------------------------------\n  // Load jump buffer.\n  // -------------------------------------------\n  SyncStackLimit(masm, caller, suspender);\n  ASSIGN_REG(jmpbuf);\n  __ LoadExternalPointerField(\n      jmpbuf, FieldMemOperand(caller, WasmContinuationObject::kJmpbufOffset),\n      kWasmContinuationJmpbufTag);\n  __ LoadTaggedField(\n      kReturnRegister0,\n      FieldMemOperand(suspender, WasmSuspenderObject::kPromiseOffset));\n  MemOperand GCScanSlotPlace =\n      MemOperand(fp, StackSwitchFrameConstants::kGCScanSlotCountOffset);\n  __ Str(xzr, GCScanSlotPlace);\n  ASSIGN_REG(scratch)\n  LoadJumpBuffer(masm, jmpbuf, true, scratch);\n  __ Trap();\n  __ Bind(&resume, BranchTargetIdentifier::kBtiJump);\n  __ LeaveFrame(StackFrame::STACK_SWITCH);\n  __ Ret(lr);\n}", "name_and_para": "void Builtins::Generate_WasmSuspend(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmSuspend", "content": "void Builtins::Generate_WasmSuspend(MacroAssembler* masm) {\n  // TODO(v8:12191): Implement for this platform.\n  __ Trap();\n}", "name_and_para": "void Builtins::Generate_WasmSuspend(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmTrapHandlerLandingPad", "content": "void Builtins::Generate_WasmTrapHandlerLandingPad(MacroAssembler* masm) {\n  __ Add(lr, kWasmTrapHandlerFaultAddressRegister,\n         WasmFrameConstants::kProtectedInstructionReturnAddressOffset);\n  __ TailCallBuiltin(Builtin::kWasmTrapHandlerThrowTrap);\n}", "name_and_para": "void Builtins::Generate_WasmTrapHandlerLandingPad(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmTrapHandlerLandingPad", "content": "void Builtins::Generate_WasmTrapHandlerLandingPad(MacroAssembler* masm) {\n  // This builtin gets called from the WebAssembly trap handler when an\n  // out-of-bounds memory access happened or when a null reference gets\n  // dereferenced. This builtin then fakes a call from the instruction that\n  // triggered the signal to the runtime. This is done by setting a return\n  // address and then jumping to a builtin which will call further to the\n  // runtime.\n  // As the return address we use the fault address + 1. Using the fault address\n  // itself would cause problems with safepoints and source positions.\n  //\n  // The problem with safepoints is that a safepoint has to be registered at the\n  // return address, and that at most one safepoint should be registered at a\n  // location. However, there could already be a safepoint registered at the\n  // fault address if the fault address is the return address of a call.\n  //\n  // The problem with source positions is that the stack trace code looks for\n  // the source position of a call before the return address. The source\n  // position of the faulty memory access, however, is recorded at the fault\n  // address. Therefore the stack trace code would not find the source position\n  // if we used the fault address as the return address.\n  __ AddWord(ra, kWasmTrapHandlerFaultAddressRegister, 1);\n  __ TailCallBuiltin(Builtin::kWasmTrapHandlerThrowTrap);\n}", "name_and_para": "void Builtins::Generate_WasmTrapHandlerLandingPad(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmToJsWrapperAsm", "content": "void Builtins::Generate_WasmToJsWrapperAsm(MacroAssembler* masm) {\n  // Push registers in reverse order so that they are on the stack like\n  // in an array, with the first item being at the lowest address.\n  __ Push(wasm::kFpParamRegisters[7], wasm::kFpParamRegisters[6],\n          wasm::kFpParamRegisters[5], wasm::kFpParamRegisters[4]);\n  __ Push(wasm::kFpParamRegisters[3], wasm::kFpParamRegisters[2],\n          wasm::kFpParamRegisters[1], wasm::kFpParamRegisters[0]);\n\n  __ Push(wasm::kGpParamRegisters[6], wasm::kGpParamRegisters[5],\n          wasm::kGpParamRegisters[4], wasm::kGpParamRegisters[3]);\n  __ Push(wasm::kGpParamRegisters[2], wasm::kGpParamRegisters[1]);\n  // Push four more slots that will be used as fixed spill slots in the torque\n  // wrapper. Two slots for stack-switching (central stack pointer and secondary\n  // stack limit), one for the signature, and one for stack alignment.\n  __ Push(xzr, xzr, xzr, xzr);\n  __ TailCallBuiltin(Builtin::kWasmToJsWrapperCSA);\n}", "name_and_para": "void Builtins::Generate_WasmToJsWrapperAsm(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmToJsWrapperAsm", "content": "void Builtins::Generate_WasmToJsWrapperAsm(MacroAssembler* masm) {\n  int required_stack_space = arraysize(wasm::kFpParamRegisters) * kDoubleSize;\n  __ SubWord(sp, sp, Operand(required_stack_space));\n  for (int i = 0; i < static_cast<int>(arraysize(wasm::kFpParamRegisters));\n       ++i) {\n    __ StoreDouble(wasm::kFpParamRegisters[i], MemOperand(sp, i * kDoubleSize));\n  }\n\n  constexpr int num_gp = arraysize(wasm::kGpParamRegisters) - 1;\n  required_stack_space = num_gp * kSystemPointerSize;\n  __ SubWord(sp, sp, Operand(required_stack_space));\n  for (int i = 1; i < static_cast<int>(arraysize(wasm::kGpParamRegisters));\n       ++i) {\n    __ StoreWord(wasm::kGpParamRegisters[i],\n                 MemOperand(sp, (i - 1) * kSystemPointerSize));\n  }\n  // Decrement the stack to allocate a stack slot. The signature gets written\n  // into the slot in Torque.\n  __ Push(zero_reg, zero_reg, zero_reg);\n  __ TailCallBuiltin(Builtin::kWasmToJsWrapperCSA);\n}", "name_and_para": "void Builtins::Generate_WasmToJsWrapperAsm(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmDebugBreak", "content": "void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) {\n  HardAbortScope hard_abort(masm);  // Avoid calls to Abort.\n  {\n    FrameScope scope(masm, StackFrame::WASM_DEBUG_BREAK);\n\n    // Save all parameter registers. They might hold live values, we restore\n    // them after the runtime call.\n    __ PushXRegList(WasmDebugBreakFrameConstants::kPushedGpRegs);\n    __ PushQRegList(WasmDebugBreakFrameConstants::kPushedFpRegs);\n\n    // Initialize the JavaScript context with 0. CEntry will use it to\n    // set the current context on the isolate.\n    __ Move(cp, Smi::zero());\n    __ CallRuntime(Runtime::kWasmDebugBreak, 0);\n\n    // Restore registers.\n    __ PopQRegList(WasmDebugBreakFrameConstants::kPushedFpRegs);\n    __ PopXRegList(WasmDebugBreakFrameConstants::kPushedGpRegs);\n  }\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmDebugBreak", "content": "void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) {\n  HardAbortScope hard_abort(masm);  // Avoid calls to Abort.\n  {\n    FrameScope scope(masm, StackFrame::WASM_DEBUG_BREAK);\n\n    // Save all parameter registers. They might hold live values, we restore\n    // them after the runtime call.\n    __ MultiPush(WasmDebugBreakFrameConstants::kPushedGpRegs);\n    __ MultiPushFPU(WasmDebugBreakFrameConstants::kPushedFpRegs);\n\n    // Initialize the JavaScript context with 0. CEntry will use it to\n    // set the current context on the isolate.\n    __ Move(cp, Smi::zero());\n    __ CallRuntime(Runtime::kWasmDebugBreak, 0);\n\n    // Restore registers.\n    __ MultiPopFPU(WasmDebugBreakFrameConstants::kPushedFpRegs);\n    __ MultiPop(WasmDebugBreakFrameConstants::kPushedGpRegs);\n  }\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmCompileLazy", "content": "void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {\n  // The function index was put in w8 by the jump table trampoline.\n  // Sign extend and convert to Smi for the runtime call.\n  __ sxtw(kWasmCompileLazyFuncIndexRegister,\n          kWasmCompileLazyFuncIndexRegister.W());\n  __ SmiTag(kWasmCompileLazyFuncIndexRegister);\n\n  UseScratchRegisterScope temps(masm);\n  temps.Exclude(x17);\n  {\n    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    // Manually save the instance (which kSavedGpRegs skips because its\n    // other use puts it into the fixed frame anyway). The stack slot is valid\n    // because the {FrameScope} (via {EnterFrame}) always reserves it (for stack\n    // alignment reasons). The instance is needed because once this builtin is\n    // done, we'll call a regular Wasm function.\n    __ Str(kWasmInstanceRegister,\n           MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));\n\n    // Save registers that we need to keep alive across the runtime call.\n    __ PushXRegList(kSavedGpRegs);\n    __ PushQRegList(kSavedFpRegs);\n\n    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);\n    // Initialize the JavaScript context with 0. CEntry will use it to\n    // set the current context on the isolate.\n    __ Mov(cp, Smi::zero());\n    __ CallRuntime(Runtime::kWasmCompileLazy, 2);\n\n    // Untag the returned Smi into into x17 (ip1), for later use.\n    static_assert(!kSavedGpRegs.has(x17));\n    __ SmiUntag(x17, kReturnRegister0);\n\n    // Restore registers.\n    __ PopQRegList(kSavedFpRegs);\n    __ PopXRegList(kSavedGpRegs);\n    // Restore the instance from the frame.\n    __ Ldr(kWasmInstanceRegister,\n           MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));\n  }\n\n  // The runtime function returned the jump table slot offset as a Smi (now in\n  // x17). Use that to compute the jump target. Use x17 (ip1) for the branch\n  // target, to be compliant with CFI.\n  constexpr Register temp = x8;\n  static_assert(!kSavedGpRegs.has(temp));\n  __ ldr(temp, FieldMemOperand(kWasmInstanceRegister,\n                               WasmTrustedInstanceData::kJumpTableStartOffset));\n  __ add(x17, temp, Operand(x17));\n  // Finally, jump to the jump table slot for the function.\n  __ Jump(x17);\n}", "name_and_para": "void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmCompileLazy", "content": "void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {\n  // The function index was put in t0 by the jump table trampoline.\n  // Convert to Smi for the runtime call\n  __ SmiTag(kWasmCompileLazyFuncIndexRegister);\n\n  {\n    HardAbortScope hard_abort(masm);  // Avoid calls to Abort.\n    FrameScope scope(masm, StackFrame::INTERNAL);\n\n    // Save registers that we need to keep alive across the runtime call.\n    __ Push(kWasmInstanceRegister);\n    __ MultiPush(kSavedGpRegs);\n    __ MultiPushFPU(kSavedFpRegs);\n\n    __ Push(kWasmInstanceRegister, kWasmCompileLazyFuncIndexRegister);\n    // Initialize the JavaScript context with 0. CEntry will use it to\n    // set the current context on the isolate.\n    __ Move(kContextRegister, Smi::zero());\n    __ CallRuntime(Runtime::kWasmCompileLazy, 2);\n\n    __ SmiUntag(s1, a0);  // move return value to s1 since a0 will be restored\n                          // to the value before the call\n    CHECK(!kSavedGpRegs.has(s1));\n\n    // Restore registers.\n    __ MultiPopFPU(kSavedFpRegs);\n    __ MultiPop(kSavedGpRegs);\n    __ Pop(kWasmInstanceRegister);\n  }\n\n  // The runtime function returned the jump table slot offset as a Smi (now in\n  // x17). Use that to compute the jump target.\n  __ LoadWord(kScratchReg,\n              FieldMemOperand(kWasmInstanceRegister,\n                              WasmTrustedInstanceData::kJumpTableStartOffset));\n  __ AddWord(s1, s1, Operand(kScratchReg));\n  // Finally, jump to the entrypoint.\n  __ Jump(s1);\n}", "name_and_para": "void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_WasmLiftoffFrameSetup", "content": "void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {\n  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;\n  Register vector = x9;\n  Register scratch = x10;\n  Label allocate_vector, done;\n\n  __ LoadTaggedField(\n      vector, FieldMemOperand(kWasmInstanceRegister,\n                              WasmTrustedInstanceData::kFeedbackVectorsOffset));\n  __ Add(vector, vector, Operand(func_index, LSL, kTaggedSizeLog2));\n  __ LoadTaggedField(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));\n  __ JumpIfSmi(vector, &allocate_vector);\n  __ bind(&done);\n  __ Push(vector, xzr);\n  __ Ret();\n\n  __ bind(&allocate_vector);\n  // Feedback vector doesn't exist yet. Call the runtime to allocate it.\n  // We temporarily change the frame type for this, because we need special\n  // handling by the stack walker in case of GC.\n  __ Mov(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));\n  __ Str(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));\n  // Save registers.\n  __ PushXRegList(kSavedGpRegs);\n  __ PushQRegList(kSavedFpRegs);\n  __ Push<MacroAssembler::kSignLR>(lr, xzr);  // xzr is for alignment.\n\n  // Arguments to the runtime function: instance, func_index, and an\n  // additional stack slot for the NativeModule. The first pushed register\n  // is for alignment. {x0} and {x1} are picked arbitrarily.\n  __ SmiTag(func_index);\n  __ Push(x0, kWasmInstanceRegister, func_index, x1);\n  __ Mov(cp, Smi::zero());\n  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);\n  __ Mov(vector, kReturnRegister0);\n\n  // Restore registers and frame type.\n  __ Pop<MacroAssembler::kAuthLR>(xzr, lr);\n  __ PopQRegList(kSavedFpRegs);\n  __ PopXRegList(kSavedGpRegs);\n  // Restore the instance from the frame.\n  __ Ldr(kWasmInstanceRegister,\n         MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));\n  __ Mov(scratch, StackFrame::TypeToMarker(StackFrame::WASM));\n  __ Str(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));\n  __ B(&done);\n}", "name_and_para": "void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) "}, {"name": "Builtins::Generate_WasmLiftoffFrameSetup", "content": "void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) {\n  Register func_index = wasm::kLiftoffFrameSetupFunctionReg;\n  Register vector = t1;\n  Register scratch = t2;\n  Label allocate_vector, done;\n\n  __ LoadTaggedField(\n      vector, FieldMemOperand(kWasmInstanceRegister,\n                              WasmTrustedInstanceData::kFeedbackVectorsOffset));\n  __ CalcScaledAddress(vector, vector, func_index, kTaggedSizeLog2);\n  __ LoadTaggedField(vector, FieldMemOperand(vector, FixedArray::kHeaderSize));\n  __ JumpIfSmi(vector, &allocate_vector);\n  __ bind(&done);\n  __ Push(vector);\n  __ Ret();\n\n  __ bind(&allocate_vector);\n  // Feedback vector doesn't exist yet. Call the runtime to allocate it.\n  // We temporarily change the frame type for this, because we need special\n  // handling by the stack walker in case of GC.\n  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM_LIFTOFF_SETUP));\n  __ StoreWord(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));\n\n  // Save registers.\n  __ MultiPush(kSavedGpRegs);\n  __ MultiPushFPU(kSavedFpRegs);\n  __ Push(ra);\n\n  // Arguments to the runtime function: instance, func_index, and an\n  // additional stack slot for the NativeModule.\n  __ SmiTag(func_index);\n  __ Push(kWasmInstanceRegister, func_index, zero_reg);\n  __ Move(cp, Smi::zero());\n  __ CallRuntime(Runtime::kWasmAllocateFeedbackVector, 3);\n  __ mv(vector, kReturnRegister0);\n\n  // Restore registers and frame type.\n  __ Pop(ra);\n  __ MultiPopFPU(kSavedFpRegs);\n  __ MultiPop(kSavedGpRegs);\n  __ LoadWord(kWasmInstanceRegister,\n              MemOperand(fp, WasmFrameConstants::kWasmInstanceOffset));\n  __ li(scratch, StackFrame::TypeToMarker(StackFrame::WASM));\n  __ StoreWord(scratch, MemOperand(fp, TypedFrameConstants::kFrameTypeOffset));\n  __ Branch(&done);\n}", "name_and_para": "void Builtins::Generate_WasmLiftoffFrameSetup(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_Construct", "content": "void Builtins::Generate_Construct(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the constructor to call (can be any Object)\n  //  -- x3 : the new target (either the same as the constructor or\n  //          the JSFunction on which new was invoked initially)\n  // -----------------------------------\n  Register target = x1;\n  Register map = x4;\n  Register instance_type = x5;\n  DCHECK(!AreAliased(x0, target, map, instance_type));\n\n  // Check if target is a Smi.\n  Label non_constructor, non_proxy;\n  __ JumpIfSmi(target, &non_constructor);\n\n  // Check if target has a [[Construct]] internal method.\n  __ LoadTaggedField(map, FieldMemOperand(target, HeapObject::kMapOffset));\n  {\n    Register flags = x2;\n    DCHECK(!AreAliased(x0, target, map, instance_type, flags));\n    __ Ldrb(flags, FieldMemOperand(map, Map::kBitFieldOffset));\n    __ TestAndBranchIfAllClear(flags, Map::Bits1::IsConstructorBit::kMask,\n                               &non_constructor);\n  }\n\n  // Dispatch based on instance type.\n  __ CompareInstanceTypeRange(map, instance_type, FIRST_JS_FUNCTION_TYPE,\n                              LAST_JS_FUNCTION_TYPE);\n  __ TailCallBuiltin(Builtin::kConstructFunction, ls);\n\n  // Only dispatch to bound functions after checking whether they are\n  // constructors.\n  __ Cmp(instance_type, JS_BOUND_FUNCTION_TYPE);\n  __ TailCallBuiltin(Builtin::kConstructBoundFunction, eq);\n\n  // Only dispatch to proxies after checking whether they are constructors.\n  __ Cmp(instance_type, JS_PROXY_TYPE);\n  __ B(ne, &non_proxy);\n  __ TailCallBuiltin(Builtin::kConstructProxy);\n\n  // Called Construct on an exotic Object with a [[Construct]] internal method.\n  __ bind(&non_proxy);\n  {\n    // Overwrite the original receiver with the (original) target.\n    __ Poke(target, __ ReceiverOperand());\n\n    // Let the \"call_as_constructor_delegate\" take care of the rest.\n    __ LoadNativeContextSlot(target,\n                             Context::CALL_AS_CONSTRUCTOR_DELEGATE_INDEX);\n    __ TailCallBuiltin(Builtins::CallFunction());\n  }\n\n  // Called Construct on an Object that doesn't have a [[Construct]] internal\n  // method.\n  __ bind(&non_constructor);\n  __ TailCallBuiltin(Builtin::kConstructedNonConstructable);\n}", "name_and_para": "void Builtins::Generate_Construct(MacroAssembler* masm) "}, {"name": "Builtins::Generate_Construct", "content": "void Builtins::Generate_Construct(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the constructor to call (can be any Object)\n  //  -- a3 : the new target (either the same as the constructor or\n  //          the JSFunction on which new was invoked initially)\n  // -----------------------------------\n\n  Register target = a1;\n  Register map = t1;\n  Register instance_type = t2;\n  Register scratch = t6;\n  DCHECK(!AreAliased(a0, target, map, instance_type, scratch));\n\n  // Check if target is a Smi.\n  Label non_constructor, non_proxy;\n  __ JumpIfSmi(target, &non_constructor);\n\n  // Check if target has a [[Construct]] internal method.\n  __ LoadTaggedField(map, FieldMemOperand(target, HeapObject::kMapOffset));\n  {\n    Register flags = t3;\n    __ Lbu(flags, FieldMemOperand(map, Map::kBitFieldOffset));\n    __ And(flags, flags, Operand(Map::Bits1::IsConstructorBit::kMask));\n    __ Branch(&non_constructor, eq, flags, Operand(zero_reg));\n  }\n\n  // Dispatch based on instance type.\n  __ GetInstanceTypeRange(map, instance_type, FIRST_JS_FUNCTION_TYPE, scratch);\n  __ TailCallBuiltin(Builtin::kConstructFunction, Uless_equal, scratch,\n                     Operand(LAST_JS_FUNCTION_TYPE - FIRST_JS_FUNCTION_TYPE));\n\n  // Only dispatch to bound functions after checking whether they are\n  // constructors.\n  __ TailCallBuiltin(Builtin::kConstructBoundFunction, eq, instance_type,\n                     Operand(JS_BOUND_FUNCTION_TYPE));\n\n  // Only dispatch to proxies after checking whether they are constructors.\n  __ Branch(&non_proxy, ne, instance_type, Operand(JS_PROXY_TYPE));\n  __ TailCallBuiltin(Builtin::kConstructProxy);\n\n  // Called Construct on an exotic Object with a [[Construct]] internal method.\n  __ bind(&non_proxy);\n  {\n    // Overwrite the original receiver with the (original) target.\n    __ StoreReceiver(target);\n    // Let the \"call_as_constructor_delegate\" take care of the rest.\n    __ LoadNativeContextSlot(target,\n                             Context::CALL_AS_CONSTRUCTOR_DELEGATE_INDEX);\n    __ TailCallBuiltin(Builtins::CallFunction());\n  }\n\n  // Called Construct on an Object that doesn't have a [[Construct]] internal\n  // method.\n  __ bind(&non_constructor);\n  __ TailCallBuiltin(Builtin::kConstructedNonConstructable);\n}", "name_and_para": "void Builtins::Generate_Construct(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ConstructBoundFunction", "content": "void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the function to call (checked to be a JSBoundFunction)\n  //  -- x3 : the new target (checked to be a constructor)\n  // -----------------------------------\n  __ AssertConstructor(x1);\n  __ AssertBoundFunction(x1);\n\n  // Push the [[BoundArguments]] onto the stack.\n  Generate_PushBoundArguments(masm);\n\n  // Patch new.target to [[BoundTargetFunction]] if new.target equals target.\n  {\n    Label done;\n    __ CmpTagged(x1, x3);\n    __ B(ne, &done);\n    __ LoadTaggedField(\n        x3, FieldMemOperand(x1, JSBoundFunction::kBoundTargetFunctionOffset));\n    __ Bind(&done);\n  }\n\n  // Construct the [[BoundTargetFunction]] via the Construct builtin.\n  __ LoadTaggedField(\n      x1, FieldMemOperand(x1, JSBoundFunction::kBoundTargetFunctionOffset));\n  __ TailCallBuiltin(Builtin::kConstruct);\n}", "name_and_para": "void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ConstructBoundFunction", "content": "void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the function to call (checked to be a JSBoundFunction)\n  //  -- a3 : the new target (checked to be a constructor)\n  // -----------------------------------\n  __ AssertBoundFunction(a1);\n\n  // Push the [[BoundArguments]] onto the stack.\n  Generate_PushBoundArguments(masm);\n\n  // Patch new.target to [[BoundTargetFunction]] if new.target equals target.\n  Label skip;\n  __ CompareTaggedAndBranch(&skip, ne, a1, Operand(a3));\n  __ LoadTaggedField(\n      a3, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));\n  __ bind(&skip);\n\n  // Construct the [[BoundTargetFunction]] via the Construct builtin.\n  __ LoadTaggedField(\n      a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));\n  __ TailCallBuiltin(Builtin::kConstruct);\n}", "name_and_para": "void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ConstructFunction", "content": "void Builtins::Generate_ConstructFunction(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the constructor to call (checked to be a JSFunction)\n  //  -- x3 : the new target (checked to be a constructor)\n  // -----------------------------------\n  __ AssertConstructor(x1);\n  __ AssertFunction(x1);\n\n  // Calling convention for function specific ConstructStubs require\n  // x2 to contain either an AllocationSite or undefined.\n  __ LoadRoot(x2, RootIndex::kUndefinedValue);\n\n  Label call_generic_stub;\n\n  // Jump to JSBuiltinsConstructStub or JSConstructStubGeneric.\n  __ LoadTaggedField(\n      x4, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n  __ Ldr(w4, FieldMemOperand(x4, SharedFunctionInfo::kFlagsOffset));\n  __ TestAndBranchIfAllClear(\n      w4, SharedFunctionInfo::ConstructAsBuiltinBit::kMask, &call_generic_stub);\n\n  __ TailCallBuiltin(Builtin::kJSBuiltinsConstructStub);\n\n  __ bind(&call_generic_stub);\n  __ TailCallBuiltin(Builtin::kJSConstructStubGeneric);\n}", "name_and_para": "void Builtins::Generate_ConstructFunction(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ConstructFunction", "content": "void Builtins::Generate_ConstructFunction(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the constructor to call (checked to be a JSFunction)\n  //  -- a3 : the new target (checked to be a constructor)\n  // -----------------------------------\n  __ AssertConstructor(a1);\n  __ AssertFunction(a1);\n\n  // Calling convention for function specific ConstructStubs require\n  // a2 to contain either an AllocationSite or undefined.\n  __ LoadRoot(a2, RootIndex::kUndefinedValue);\n\n  Label call_generic_stub;\n\n  // Jump to JSBuiltinsConstructStub or JSConstructStubGeneric.\n  __ LoadTaggedField(\n      a4, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));\n  __ Load32U(a4, FieldMemOperand(a4, SharedFunctionInfo::kFlagsOffset));\n  __ And(a4, a4, Operand(SharedFunctionInfo::ConstructAsBuiltinBit::kMask));\n  __ Branch(&call_generic_stub, eq, a4, Operand(zero_reg),\n            Label::Distance::kNear);\n\n  __ TailCallBuiltin(Builtin::kJSBuiltinsConstructStub);\n\n  __ bind(&call_generic_stub);\n  __ TailCallBuiltin(Builtin::kJSConstructStubGeneric);\n}", "name_and_para": "void Builtins::Generate_ConstructFunction(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_Call", "content": "void Builtins::Generate_Call(MacroAssembler* masm, ConvertReceiverMode mode) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the target to call (can be any Object).\n  // -----------------------------------\n  Register target = x1;\n  Register map = x4;\n  Register instance_type = x5;\n  DCHECK(!AreAliased(x0, target, map, instance_type));\n\n  Label non_callable, class_constructor;\n  __ JumpIfSmi(target, &non_callable);\n  __ LoadMap(map, target);\n  __ CompareInstanceTypeRange(map, instance_type,\n                              FIRST_CALLABLE_JS_FUNCTION_TYPE,\n                              LAST_CALLABLE_JS_FUNCTION_TYPE);\n  __ TailCallBuiltin(Builtins::CallFunction(mode), ls);\n  __ Cmp(instance_type, JS_BOUND_FUNCTION_TYPE);\n  __ TailCallBuiltin(Builtin::kCallBoundFunction, eq);\n\n  // Check if target has a [[Call]] internal method.\n  {\n    Register flags = x4;\n    __ Ldrb(flags, FieldMemOperand(map, Map::kBitFieldOffset));\n    map = no_reg;\n    __ TestAndBranchIfAllClear(flags, Map::Bits1::IsCallableBit::kMask,\n                               &non_callable);\n  }\n\n  // Check if target is a proxy and call CallProxy external builtin\n  __ Cmp(instance_type, JS_PROXY_TYPE);\n  __ TailCallBuiltin(Builtin::kCallProxy, eq);\n\n  // Check if target is a wrapped function and call CallWrappedFunction external\n  // builtin\n  __ Cmp(instance_type, JS_WRAPPED_FUNCTION_TYPE);\n  __ TailCallBuiltin(Builtin::kCallWrappedFunction, eq);\n\n  // ES6 section 9.2.1 [[Call]] ( thisArgument, argumentsList)\n  // Check that the function is not a \"classConstructor\".\n  __ Cmp(instance_type, JS_CLASS_CONSTRUCTOR_TYPE);\n  __ B(eq, &class_constructor);\n\n  // 2. Call to something else, which might have a [[Call]] internal method (if\n  // not we raise an exception).\n  // Overwrite the original receiver with the (original) target.\n  __ Poke(target, __ ReceiverOperand());\n\n  // Let the \"call_as_function_delegate\" take care of the rest.\n  __ LoadNativeContextSlot(target, Context::CALL_AS_FUNCTION_DELEGATE_INDEX);\n  __ TailCallBuiltin(\n      Builtins::CallFunction(ConvertReceiverMode::kNotNullOrUndefined));\n\n  // 3. Call to something that is not callable.\n  __ bind(&non_callable);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ PushArgument(target);\n    __ CallRuntime(Runtime::kThrowCalledNonCallable);\n    __ Unreachable();\n  }\n\n  // 4. The function is a \"classConstructor\", need to raise an exception.\n  __ bind(&class_constructor);\n  {\n    FrameScope frame(masm, StackFrame::INTERNAL);\n    __ PushArgument(target);\n    __ CallRuntime(Runtime::kThrowConstructorNonCallableError);\n    __ Unreachable();\n  }\n}", "name_and_para": "void Builtins::Generate_Call(MacroAssembler* masm, ConvertReceiverMode mode) "}, {"name": "Builtins::Generate_Call", "content": "void Builtins::Generate_Call(MacroAssembler* masm, ConvertReceiverMode mode) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the target to call (can be any Object).\n  // -----------------------------------\n\n  Register target = a1;\n  Register map = t1;\n  Register instance_type = t2;\n  Register scratch = t6;\n  DCHECK(!AreAliased(a0, target, map, instance_type, scratch));\n\n  Label non_callable, class_constructor;\n  __ JumpIfSmi(target, &non_callable);\n  __ LoadMap(map, target);\n  __ GetInstanceTypeRange(map, instance_type, FIRST_CALLABLE_JS_FUNCTION_TYPE,\n                          scratch);\n  __ TailCallBuiltin(Builtins::CallFunction(mode), ule, scratch,\n                     Operand(LAST_CALLABLE_JS_FUNCTION_TYPE -\n                             FIRST_CALLABLE_JS_FUNCTION_TYPE));\n  __ TailCallBuiltin(Builtin::kCallBoundFunction, eq, instance_type,\n                     Operand(JS_BOUND_FUNCTION_TYPE));\n\n  // Check if target has a [[Call]] internal method.\n  {\n    Register flags = t1;\n    __ Lbu(flags, FieldMemOperand(map, Map::kBitFieldOffset));\n    map = no_reg;\n    __ And(flags, flags, Operand(Map::Bits1::IsCallableBit::kMask));\n    __ Branch(&non_callable, eq, flags, Operand(zero_reg));\n  }\n\n  __ TailCallBuiltin(Builtin::kCallProxy, eq, instance_type,\n                     Operand(JS_PROXY_TYPE));\n\n  // Check if target is a wrapped function and call CallWrappedFunction external\n  // builtin\n  __ TailCallBuiltin(Builtin::kCallWrappedFunction, eq, instance_type,\n                     Operand(JS_WRAPPED_FUNCTION_TYPE));\n\n  // ES6 section 9.2.1 [[Call]] ( thisArgument, argumentsList)\n  // Check that the function is not a \"classConstructor\".\n  __ Branch(&class_constructor, eq, instance_type,\n            Operand(JS_CLASS_CONSTRUCTOR_TYPE));\n\n  // 2. Call to something else, which might have a [[Call]] internal method (if\n  // not we raise an exception).\n  // Overwrite the original receiver with the (original) target.\n  __ StoreReceiver(target);\n  // Let the \"call_as_function_delegate\" take care of the rest.\n  __ LoadNativeContextSlot(target, Context::CALL_AS_FUNCTION_DELEGATE_INDEX);\n  __ TailCallBuiltin(\n      Builtins::CallFunction(ConvertReceiverMode::kNotNullOrUndefined));\n\n  // 3. Call to something that is not callable.\n  __ bind(&non_callable);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(target);\n    __ CallRuntime(Runtime::kThrowCalledNonCallable);\n  }\n\n  // 4. The function is a \"classConstructor\", need to raise an exception.\n  __ bind(&class_constructor);\n  {\n    FrameScope frame(masm, StackFrame::INTERNAL);\n    __ Push(target);\n    __ CallRuntime(Runtime::kThrowConstructorNonCallableError);\n  }\n}", "name_and_para": "void Builtins::Generate_Call(MacroAssembler* masm, ConvertReceiverMode mode) "}], [{"name": "Builtins::Generate_CallBoundFunctionImpl", "content": "void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the function to call (checked to be a JSBoundFunction)\n  // -----------------------------------\n  __ AssertBoundFunction(x1);\n\n  // Patch the receiver to [[BoundThis]].\n  __ LoadTaggedField(x10,\n                     FieldMemOperand(x1, JSBoundFunction::kBoundThisOffset));\n  __ Poke(x10, __ ReceiverOperand());\n\n  // Push the [[BoundArguments]] onto the stack.\n  Generate_PushBoundArguments(masm);\n\n  // Call the [[BoundTargetFunction]] via the Call builtin.\n  __ LoadTaggedField(\n      x1, FieldMemOperand(x1, JSBoundFunction::kBoundTargetFunctionOffset));\n  __ TailCallBuiltin(Builtins::Call());\n}", "name_and_para": "void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) "}, {"name": "Builtins::Generate_CallBoundFunctionImpl", "content": "void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the function to call (checked to be a JSBoundFunction)\n  // -----------------------------------\n  __ AssertBoundFunction(a1);\n\n  // Patch the receiver to [[BoundThis]].\n  {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ LoadTaggedField(scratch,\n                       FieldMemOperand(a1, JSBoundFunction::kBoundThisOffset));\n    __ StoreReceiver(scratch);\n  }\n\n  // Push the [[BoundArguments]] onto the stack.\n  Generate_PushBoundArguments(masm);\n\n  // Call the [[BoundTargetFunction]] via the Call builtin.\n  __ LoadTaggedField(\n      a1, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));\n  __ TailCallBuiltin(Builtins::Call());\n}", "name_and_para": "void Builtins::Generate_CallBoundFunctionImpl(MacroAssembler* masm) "}], [{"name": "Generate_PushBoundArguments", "content": "void Generate_PushBoundArguments(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : target (checked to be a JSBoundFunction)\n  //  -- x3 : new.target (only in case of [[Construct]])\n  // -----------------------------------\n\n  Register bound_argc = x4;\n  Register bound_argv = x2;\n\n  // Load [[BoundArguments]] into x2 and length of that into x4.\n  Label no_bound_arguments;\n  __ LoadTaggedField(\n      bound_argv, FieldMemOperand(x1, JSBoundFunction::kBoundArgumentsOffset));\n  __ SmiUntagField(bound_argc,\n                   FieldMemOperand(bound_argv, FixedArray::kLengthOffset));\n  __ Cbz(bound_argc, &no_bound_arguments);\n  {\n    // ----------- S t a t e -------------\n    //  -- x0 : the number of arguments\n    //  -- x1 : target (checked to be a JSBoundFunction)\n    //  -- x2 : the [[BoundArguments]] (implemented as FixedArray)\n    //  -- x3 : new.target (only in case of [[Construct]])\n    //  -- x4 : the number of [[BoundArguments]]\n    // -----------------------------------\n\n    Register argc = x0;\n\n    // Check for stack overflow.\n    {\n      // Check the stack for overflow. We are not trying to catch interruptions\n      // (i.e. debug break and preemption) here, so check the \"real stack\n      // limit\".\n      Label done;\n      __ LoadStackLimit(x10, StackLimitKind::kRealStackLimit);\n      // Make x10 the space we have left. The stack might already be overflowed\n      // here which will cause x10 to become negative.\n      __ Sub(x10, sp, x10);\n      // Check if the arguments will overflow the stack.\n      __ Cmp(x10, Operand(bound_argc, LSL, kSystemPointerSizeLog2));\n      __ B(gt, &done);\n      __ TailCallRuntime(Runtime::kThrowStackOverflow);\n      __ Bind(&done);\n    }\n\n    Label copy_bound_args;\n    Register total_argc = x15;\n    Register slots_to_claim = x12;\n    Register scratch = x10;\n    Register receiver = x14;\n\n    __ Sub(argc, argc, kJSArgcReceiverSlots);\n    __ Add(total_argc, argc, bound_argc);\n    __ Peek(receiver, 0);\n\n    // Round up slots_to_claim to an even number if it is odd.\n    __ Add(slots_to_claim, bound_argc, 1);\n    __ Bic(slots_to_claim, slots_to_claim, 1);\n    __ Claim(slots_to_claim, kSystemPointerSize);\n\n    __ Tbz(bound_argc, 0, &copy_bound_args);\n    {\n      Label argc_even;\n      __ Tbz(argc, 0, &argc_even);\n      // Arguments count is odd (with the receiver it's even), so there's no\n      // alignment padding above the arguments and we have to \"add\" it. We\n      // claimed bound_argc + 1, since it is odd and it was rounded up. +1 here\n      // is for stack alignment padding.\n      // 1. Shift args one slot down.\n      {\n        Register copy_from = x11;\n        Register copy_to = x12;\n        __ SlotAddress(copy_to, slots_to_claim);\n        __ Add(copy_from, copy_to, kSystemPointerSize);\n        __ CopyDoubleWords(copy_to, copy_from, argc);\n      }\n      // 2. Write a padding in the last slot.\n      __ Add(scratch, total_argc, 1);\n      __ Str(padreg, MemOperand(sp, scratch, LSL, kSystemPointerSizeLog2));\n      __ B(&copy_bound_args);\n\n      __ Bind(&argc_even);\n      // Arguments count is even (with the receiver it's odd), so there's an\n      // alignment padding above the arguments and we can reuse it. We need to\n      // claim bound_argc - 1, but we claimed bound_argc + 1, since it is odd\n      // and it was rounded up.\n      // 1. Drop 2.\n      __ Drop(2);\n      // 2. Shift args one slot up.\n      {\n        Register copy_from = x11;\n        Register copy_to = x12;\n        __ SlotAddress(copy_to, total_argc);\n        __ Sub(copy_from, copy_to, kSystemPointerSize);\n        __ CopyDoubleWords(copy_to, copy_from, argc,\n                           MacroAssembler::kSrcLessThanDst);\n      }\n    }\n\n    // If bound_argc is even, there is no alignment massage to do, and we have\n    // already claimed the correct number of slots (bound_argc).\n    __ Bind(&copy_bound_args);\n\n    // Copy the receiver back.\n    __ Poke(receiver, 0);\n    // Copy [[BoundArguments]] to the stack (below the receiver).\n    {\n      Label loop;\n      Register counter = bound_argc;\n      Register copy_to = x12;\n      __ Add(bound_argv, bound_argv, FixedArray::kHeaderSize - kHeapObjectTag);\n      __ SlotAddress(copy_to, 1);\n      __ Bind(&loop);\n      __ Sub(counter, counter, 1);\n      __ LoadTaggedField(scratch,\n                         MemOperand(bound_argv, kTaggedSize, PostIndex));\n      __ Str(scratch, MemOperand(copy_to, kSystemPointerSize, PostIndex));\n      __ Cbnz(counter, &loop);\n    }\n    // Update argc.\n    __ Add(argc, total_argc, kJSArgcReceiverSlots);\n  }\n  __ Bind(&no_bound_arguments);\n}", "name_and_para": "void Generate_PushBoundArguments(MacroAssembler* masm) "}, {"name": "Generate_PushBoundArguments", "content": "void Generate_PushBoundArguments(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : target (checked to be a JSBoundFunction)\n  //  -- a3 : new.target (only in case of [[Construct]])\n  // -----------------------------------\n  UseScratchRegisterScope temps(masm);\n  temps.Include(t0, t1);\n  Register bound_argc = a4;\n  Register bound_argv = a2;\n  // Load [[BoundArguments]] into a2 and length of that into a4.\n  Label no_bound_arguments;\n  __ LoadTaggedField(\n      bound_argv, FieldMemOperand(a1, JSBoundFunction::kBoundArgumentsOffset));\n  __ SmiUntagField(bound_argc,\n                   FieldMemOperand(bound_argv, FixedArray::kLengthOffset));\n  __ Branch(&no_bound_arguments, eq, bound_argc, Operand(zero_reg));\n  {\n    // ----------- S t a t e -------------\n    //  -- a0 : the number of arguments\n    //  -- a1 : target (checked to be a JSBoundFunction)\n    //  -- a2 : the [[BoundArguments]] (implemented as FixedArray)\n    //  -- a3 : new.target (only in case of [[Construct]])\n    //  -- a4: the number of [[BoundArguments]]\n    // -----------------------------------\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    Label done;\n    // Reserve stack space for the [[BoundArguments]].\n    {\n      // Check the stack for overflow. We are not trying to catch interruptions\n      // (i.e. debug break and preemption) here, so check the \"real stack\n      // limit\".\n      __ StackOverflowCheck(a4, temps.Acquire(), temps.Acquire(), nullptr,\n                            &done);\n      {\n        FrameScope scope(masm, StackFrame::MANUAL);\n        __ EnterFrame(StackFrame::INTERNAL);\n        __ CallRuntime(Runtime::kThrowStackOverflow);\n      }\n      __ bind(&done);\n    }\n\n    // Pop receiver.\n    __ Pop(scratch);\n\n    // Push [[BoundArguments]].\n    {\n      Label loop, done_loop;\n      __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));\n      __ AddWord(a0, a0, Operand(a4));\n      __ AddWord(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));\n      __ bind(&loop);\n      __ SubWord(a4, a4, Operand(1));\n      __ Branch(&done_loop, lt, a4, Operand(zero_reg), Label::Distance::kNear);\n      __ CalcScaledAddress(a5, a2, a4, kTaggedSizeLog2);\n      __ LoadTaggedField(kScratchReg, MemOperand(a5));\n      __ Push(kScratchReg);\n      __ Branch(&loop);\n      __ bind(&done_loop);\n    }\n\n    // Push receiver.\n    __ Push(scratch);\n  }\n  __ bind(&no_bound_arguments);\n}", "name_and_para": "void Generate_PushBoundArguments(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_CallFunction", "content": "void Builtins::Generate_CallFunction(MacroAssembler* masm,\n                                     ConvertReceiverMode mode) {\n  ASM_LOCATION(\"Builtins::Generate_CallFunction\");\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the function to call (checked to be a JSFunction)\n  // -----------------------------------\n  __ AssertCallableFunction(x1);\n\n  __ LoadTaggedField(\n      x2, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n\n  // Enter the context of the function; ToObject has to run in the function\n  // context, and we also need to take the global proxy from the function\n  // context in case of conversion.\n  __ LoadTaggedField(cp, FieldMemOperand(x1, JSFunction::kContextOffset));\n  // We need to convert the receiver for non-native sloppy mode functions.\n  Label done_convert;\n  __ Ldr(w3, FieldMemOperand(x2, SharedFunctionInfo::kFlagsOffset));\n  __ TestAndBranchIfAnySet(w3,\n                           SharedFunctionInfo::IsNativeBit::kMask |\n                               SharedFunctionInfo::IsStrictBit::kMask,\n                           &done_convert);\n  {\n    // ----------- S t a t e -------------\n    //  -- x0 : the number of arguments\n    //  -- x1 : the function to call (checked to be a JSFunction)\n    //  -- x2 : the shared function info.\n    //  -- cp : the function context.\n    // -----------------------------------\n\n    if (mode == ConvertReceiverMode::kNullOrUndefined) {\n      // Patch receiver to global proxy.\n      __ LoadGlobalProxy(x3);\n    } else {\n      Label convert_to_object, convert_receiver;\n      __ Peek(x3, __ ReceiverOperand());\n      __ JumpIfSmi(x3, &convert_to_object);\n      __ JumpIfJSAnyIsNotPrimitive(x3, x4, &done_convert);\n      if (mode != ConvertReceiverMode::kNotNullOrUndefined) {\n        Label convert_global_proxy;\n        __ JumpIfRoot(x3, RootIndex::kUndefinedValue, &convert_global_proxy);\n        __ JumpIfNotRoot(x3, RootIndex::kNullValue, &convert_to_object);\n        __ Bind(&convert_global_proxy);\n        {\n          // Patch receiver to global proxy.\n          __ LoadGlobalProxy(x3);\n        }\n        __ B(&convert_receiver);\n      }\n      __ Bind(&convert_to_object);\n      {\n        // Convert receiver using ToObject.\n        // TODO(bmeurer): Inline the allocation here to avoid building the frame\n        // in the fast case? (fall back to AllocateInNewSpace?)\n        FrameScope scope(masm, StackFrame::INTERNAL);\n        __ SmiTag(x0);\n        __ Push(padreg, x0, x1, cp);\n        __ Mov(x0, x3);\n        __ CallBuiltin(Builtin::kToObject);\n        __ Mov(x3, x0);\n        __ Pop(cp, x1, x0, padreg);\n        __ SmiUntag(x0);\n      }\n      __ LoadTaggedField(\n          x2, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n      __ Bind(&convert_receiver);\n    }\n    __ Poke(x3, __ ReceiverOperand());\n  }\n  __ Bind(&done_convert);\n\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x1 : the function to call (checked to be a JSFunction)\n  //  -- x2 : the shared function info.\n  //  -- cp : the function context.\n  // -----------------------------------\n\n  __ Ldrh(x2,\n          FieldMemOperand(x2, SharedFunctionInfo::kFormalParameterCountOffset));\n  __ InvokeFunctionCode(x1, no_reg, x2, x0, InvokeType::kJump);\n}", "name_and_para": "void Builtins::Generate_CallFunction(MacroAssembler* masm,\n                                     ConvertReceiverMode mode) "}, {"name": "Builtins::Generate_CallFunction", "content": "void Builtins::Generate_CallFunction(MacroAssembler* masm,\n                                     ConvertReceiverMode mode) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the function to call (checked to be a JSFunction)\n  // -----------------------------------\n  __ AssertFunction(a1);\n\n  __ LoadTaggedField(\n      a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));\n\n  // Enter the context of the function; ToObject has to run in the function\n  // context, and we also need to take the global proxy from the function\n  // context in case of conversion.\n  __ LoadTaggedField(cp, FieldMemOperand(a1, JSFunction::kContextOffset));\n  // We need to convert the receiver for non-native sloppy mode functions.\n  Label done_convert;\n  __ Load32U(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));\n  __ And(kScratchReg, a3,\n         Operand(SharedFunctionInfo::IsNativeBit::kMask |\n                 SharedFunctionInfo::IsStrictBit::kMask));\n  __ Branch(&done_convert, ne, kScratchReg, Operand(zero_reg));\n  {\n    // ----------- S t a t e -------------\n    //  -- a0 : the number of arguments\n    //  -- a1 : the function to call (checked to be a JSFunction)\n    //  -- a2 : the shared function info.\n    //  -- cp : the function context.\n    // -----------------------------------\n\n    if (mode == ConvertReceiverMode::kNullOrUndefined) {\n      // Patch receiver to global proxy.\n      __ LoadGlobalProxy(a3);\n    } else {\n      Label convert_to_object, convert_receiver;\n      __ LoadReceiver(a3);\n      __ JumpIfSmi(a3, &convert_to_object);\n      __ JumpIfJSAnyIsNotPrimitive(a3, a4, &done_convert);\n      if (mode != ConvertReceiverMode::kNotNullOrUndefined) {\n        Label convert_global_proxy;\n        __ JumpIfRoot(a3, RootIndex::kUndefinedValue, &convert_global_proxy);\n        __ JumpIfNotRoot(a3, RootIndex::kNullValue, &convert_to_object);\n        __ bind(&convert_global_proxy);\n        {\n          // Patch receiver to global proxy.\n          __ LoadGlobalProxy(a3);\n        }\n        __ Branch(&convert_receiver);\n      }\n      __ bind(&convert_to_object);\n      {\n        // Convert receiver using ToObject.\n        // TODO(bmeurer): Inline the allocation here to avoid building the frame\n        // in the fast case? (fall back to AllocateInNewSpace?)\n        FrameScope scope(masm, StackFrame::INTERNAL);\n        __ SmiTag(a0);\n        __ Push(a0, a1);\n        __ Move(a0, a3);\n        __ Push(cp);\n        __ CallBuiltin(Builtin::kToObject);\n        __ Pop(cp);\n        __ Move(a3, a0);\n        __ Pop(a0, a1);\n        __ SmiUntag(a0);\n      }\n      __ LoadTaggedField(\n          a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));\n      __ bind(&convert_receiver);\n    }\n    __ StoreReceiver(a3);\n  }\n  __ bind(&done_convert);\n\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a1 : the function to call (checked to be a JSFunction)\n  //  -- a2 : the shared function info.\n  //  -- cp : the function context.\n  // -----------------------------------\n\n  __ Lhu(a2,\n         FieldMemOperand(a2, SharedFunctionInfo::kFormalParameterCountOffset));\n  __ InvokeFunctionCode(a1, no_reg, a2, a0, InvokeType::kJump);\n}", "name_and_para": "void Builtins::Generate_CallFunction(MacroAssembler* masm,\n                                     ConvertReceiverMode mode) "}], [{"name": "Builtins::Generate_CallOrConstructForwardVarargs", "content": "void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,\n                                                      CallOrConstructMode mode,\n                                                      Builtin target_builtin) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x3 : the new.target (for [[Construct]] calls)\n  //  -- x1 : the target to call (can be any Object)\n  //  -- x2 : start index (to support rest parameters)\n  // -----------------------------------\n\n  Register argc = x0;\n  Register start_index = x2;\n\n  // Check if new.target has a [[Construct]] internal method.\n  if (mode == CallOrConstructMode::kConstruct) {\n    Label new_target_constructor, new_target_not_constructor;\n    __ JumpIfSmi(x3, &new_target_not_constructor);\n    __ LoadTaggedField(x5, FieldMemOperand(x3, HeapObject::kMapOffset));\n    __ Ldrb(x5, FieldMemOperand(x5, Map::kBitFieldOffset));\n    __ TestAndBranchIfAnySet(x5, Map::Bits1::IsConstructorBit::kMask,\n                             &new_target_constructor);\n    __ Bind(&new_target_not_constructor);\n    {\n      FrameScope scope(masm, StackFrame::MANUAL);\n      __ EnterFrame(StackFrame::INTERNAL);\n      __ PushArgument(x3);\n      __ CallRuntime(Runtime::kThrowNotConstructor);\n      __ Unreachable();\n    }\n    __ Bind(&new_target_constructor);\n  }\n\n  Register len = x6;\n  Label stack_done, stack_overflow;\n  __ Ldr(len, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n  __ Subs(len, len, kJSArgcReceiverSlots);\n  __ Subs(len, len, start_index);\n  __ B(le, &stack_done);\n  // Check for stack overflow.\n  __ StackOverflowCheck(len, &stack_overflow);\n\n  Generate_PrepareForCopyingVarargs(masm, argc, len);\n\n  // Push varargs.\n  {\n    Register args_fp = x5;\n    Register dst = x13;\n    // Point to the fist argument to copy from (skipping receiver).\n    __ Add(args_fp, fp,\n           CommonFrameConstants::kFixedFrameSizeAboveFp + kSystemPointerSize);\n    __ lsl(start_index, start_index, kSystemPointerSizeLog2);\n    __ Add(args_fp, args_fp, start_index);\n    // Point to the position to copy to.\n    __ SlotAddress(dst, argc);\n    // Update total number of arguments.\n    __ Add(argc, argc, len);\n    __ CopyDoubleWords(dst, args_fp, len);\n  }\n\n  __ Bind(&stack_done);\n  // Tail-call to the actual Call or Construct builtin.\n  __ TailCallBuiltin(target_builtin);\n\n  __ Bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n}", "name_and_para": "void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,\n                                                      CallOrConstructMode mode,\n                                                      Builtin target_builtin) "}, {"name": "Builtins::Generate_CallOrConstructForwardVarargs", "content": "void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,\n                                                      CallOrConstructMode mode,\n                                                      Builtin target_builtin) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a3 : the new.target (for [[Construct]] calls)\n  //  -- a1 : the target to call (can be any Object)\n  //  -- a2 : start index (to support rest parameters)\n  // -----------------------------------\n  UseScratchRegisterScope temps(masm);\n  temps.Include(t0, t1);\n  temps.Include(t2);\n  // Check if new.target has a [[Construct]] internal method.\n  if (mode == CallOrConstructMode::kConstruct) {\n    Label new_target_constructor, new_target_not_constructor;\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ JumpIfSmi(a3, &new_target_not_constructor);\n    __ LoadTaggedField(scratch, FieldMemOperand(a3, HeapObject::kMapOffset));\n    __ Lbu(scratch, FieldMemOperand(scratch, Map::kBitFieldOffset));\n    __ And(scratch, scratch, Operand(Map::Bits1::IsConstructorBit::kMask));\n    __ Branch(&new_target_constructor, ne, scratch, Operand(zero_reg),\n              Label::Distance::kNear);\n    __ bind(&new_target_not_constructor);\n    {\n      FrameScope scope(masm, StackFrame::MANUAL);\n      __ EnterFrame(StackFrame::INTERNAL);\n      __ Push(a3);\n      __ CallRuntime(Runtime::kThrowNotConstructor);\n    }\n    __ bind(&new_target_constructor);\n  }\n\n  __ Move(a6, fp);\n  __ LoadWord(a7, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n\n  Label stack_done, stack_overflow;\n  __ SubWord(a7, a7, Operand(kJSArgcReceiverSlots));\n  __ SubWord(a7, a7, a2);\n  __ Branch(&stack_done, le, a7, Operand(zero_reg));\n  {\n    // Check for stack overflow.\n    __ StackOverflowCheck(a7, a4, a5, &stack_overflow);\n\n    // Forward the arguments from the caller frame.\n\n    // Point to the first argument to copy (skipping the receiver).\n    __ AddWord(a6, a6,\n               Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +\n                       kSystemPointerSize));\n    __ CalcScaledAddress(a6, a6, a2, kSystemPointerSizeLog2);\n\n    // Move the arguments already in the stack,\n    // including the receiver and the return address.\n    // a7: Number of arguments to make room for.\n    // a0: Number of arguments already on the stack.\n    // a2: Points to first free slot on the stack after arguments were shifted.\n    Generate_AllocateSpaceAndShiftExistingArguments(masm, a7, a0, a2);\n\n    // Copy arguments from the caller frame.\n    // TODO(victorgomes): Consider using forward order as potentially more cache\n    // friendly.\n    {\n      Label loop;\n      __ bind(&loop);\n      {\n        UseScratchRegisterScope temps(masm);\n        Register scratch = temps.Acquire(), addr = temps.Acquire();\n        __ Sub32(a7, a7, Operand(1));\n        __ CalcScaledAddress(addr, a6, a7, kSystemPointerSizeLog2);\n        __ LoadWord(scratch, MemOperand(addr));\n        __ CalcScaledAddress(addr, a2, a7, kSystemPointerSizeLog2);\n        __ StoreWord(scratch, MemOperand(addr));\n        __ Branch(&loop, ne, a7, Operand(zero_reg));\n      }\n    }\n  }\n  __ bind(&stack_done);\n  // Tail-call to the actual Call or Construct builtin.\n  __ TailCallBuiltin(target_builtin);\n\n  __ bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n}", "name_and_para": "void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,\n                                                      CallOrConstructMode mode,\n                                                      Builtin target_builtin) "}], [{"name": "Builtins::Generate_CallOrConstructVarargs", "content": "void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,\n                                               Builtin target_builtin) {\n  // ----------- S t a t e -------------\n  //  -- x1 : target\n  //  -- x0 : number of parameters on the stack\n  //  -- x2 : arguments list (a FixedArray)\n  //  -- x4 : len (number of elements to push from args)\n  //  -- x3 : new.target (for [[Construct]])\n  // -----------------------------------\n  if (v8_flags.debug_code) {\n    // Allow x2 to be a FixedArray, or a FixedDoubleArray if x4 == 0.\n    Label ok, fail;\n    __ AssertNotSmi(x2, AbortReason::kOperandIsNotAFixedArray);\n    __ LoadTaggedField(x10, FieldMemOperand(x2, HeapObject::kMapOffset));\n    __ Ldrh(x13, FieldMemOperand(x10, Map::kInstanceTypeOffset));\n    __ Cmp(x13, FIXED_ARRAY_TYPE);\n    __ B(eq, &ok);\n    __ Cmp(x13, FIXED_DOUBLE_ARRAY_TYPE);\n    __ B(ne, &fail);\n    __ Cmp(x4, 0);\n    __ B(eq, &ok);\n    // Fall through.\n    __ bind(&fail);\n    __ Abort(AbortReason::kOperandIsNotAFixedArray);\n\n    __ bind(&ok);\n  }\n\n  Register arguments_list = x2;\n  Register argc = x0;\n  Register len = x4;\n\n  Label stack_overflow;\n  __ StackOverflowCheck(len, &stack_overflow);\n\n  // Skip argument setup if we don't need to push any varargs.\n  Label done;\n  __ Cbz(len, &done);\n\n  Generate_PrepareForCopyingVarargs(masm, argc, len);\n\n  // Push varargs.\n  {\n    Label loop;\n    Register src = x10;\n    Register undefined_value = x12;\n    Register scratch = x13;\n    __ Add(src, arguments_list, FixedArray::kHeaderSize - kHeapObjectTag);\n#if !V8_STATIC_ROOTS_BOOL\n    // We do not use the CompareRoot macro without static roots as it would do a\n    // LoadRoot behind the scenes and we want to avoid that in a loop.\n    Register the_hole_value = x11;\n    __ LoadTaggedRoot(the_hole_value, RootIndex::kTheHoleValue);\n#endif  // !V8_STATIC_ROOTS_BOOL\n    __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n    // TODO(all): Consider using Ldp and Stp.\n    Register dst = x16;\n    __ SlotAddress(dst, argc);\n    __ Add(argc, argc, len);  // Update new argc.\n    __ Bind(&loop);\n    __ Sub(len, len, 1);\n    __ LoadTaggedField(scratch, MemOperand(src, kTaggedSize, PostIndex));\n#if V8_STATIC_ROOTS_BOOL\n    __ CompareRoot(scratch, RootIndex::kTheHoleValue);\n#else\n    __ CmpTagged(scratch, the_hole_value);\n#endif\n    __ Csel(scratch, scratch, undefined_value, ne);\n    __ Str(scratch, MemOperand(dst, kSystemPointerSize, PostIndex));\n    __ Cbnz(len, &loop);\n  }\n  __ Bind(&done);\n  // Tail-call to the actual Call or Construct builtin.\n  __ TailCallBuiltin(target_builtin);\n\n  __ bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n}", "name_and_para": "void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,\n                                               Builtin target_builtin) "}, {"name": "Builtins::Generate_CallOrConstructVarargs", "content": "void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,\n                                               Builtin target_builtin) {\n  UseScratchRegisterScope temps(masm);\n  temps.Include(t1, t0);\n  // ----------- S t a t e -------------\n  //  -- a1 : target\n  //  -- a0 : number of parameters on the stack\n  //  -- a2 : arguments list (a FixedArray)\n  //  -- a4 : len (number of elements to push from args)\n  //  -- a3 : new.target (for [[Construct]])\n  // -----------------------------------\n  if (v8_flags.debug_code) {\n    // Allow a2 to be a FixedArray, or a FixedDoubleArray if a4 == 0.\n    Label ok, fail;\n    __ AssertNotSmi(a2);\n    __ GetObjectType(a2, kScratchReg, kScratchReg);\n    __ Branch(&ok, eq, kScratchReg, Operand(FIXED_ARRAY_TYPE),\n              Label::Distance::kNear);\n    __ Branch(&fail, ne, kScratchReg, Operand(FIXED_DOUBLE_ARRAY_TYPE),\n              Label::Distance::kNear);\n    __ Branch(&ok, eq, a4, Operand(zero_reg), Label::Distance::kNear);\n    // Fall through.\n    __ bind(&fail);\n    __ Abort(AbortReason::kOperandIsNotAFixedArray);\n\n    __ bind(&ok);\n  }\n\n  Register args = a2;\n  Register len = a4;\n\n  // Check for stack overflow.\n  Label stack_overflow;\n  __ StackOverflowCheck(len, kScratchReg, a5, &stack_overflow);\n\n  // Move the arguments already in the stack,\n  // including the receiver and the return address.\n  // a4: Number of arguments to make room for.\n  // a0: Number of arguments already on the stack.\n  // a7: Points to first free slot on the stack after arguments were shifted.\n  Generate_AllocateSpaceAndShiftExistingArguments(masm, a4, a0, a7);\n\n  // Push arguments onto the stack (thisArgument is already on the stack).\n  {\n    Label done, push, loop;\n    Register src = a6;\n    Register scratch = len;\n    UseScratchRegisterScope temps(masm);\n    Register hole_value = temps.Acquire();\n    __ AddWord(src, args, FixedArray::kHeaderSize - kHeapObjectTag);\n    __ Branch(&done, eq, len, Operand(zero_reg), Label::Distance::kNear);\n    __ SllWord(scratch, len, kTaggedSizeLog2);\n    __ SubWord(scratch, sp, Operand(scratch));\n#if !V8_STATIC_ROOTS_BOOL\n    // We do not use the Branch(reg, RootIndex) macro without static roots,\n    // as it would do a LoadRoot behind the scenes and we want to avoid that\n    // in a loop.\n    __ LoadTaggedRoot(hole_value, RootIndex::kTheHoleValue);\n#endif  // !V8_STATIC_ROOTS_BOOL\n    __ bind(&loop);\n    __ LoadTaggedField(a5, MemOperand(src));\n    __ AddWord(src, src, kTaggedSize);\n    __ CompareTaggedAndBranch(&push, ne, a5, Operand(hole_value));\n    __ LoadRoot(a5, RootIndex::kUndefinedValue);\n    __ bind(&push);\n    __ StoreWord(a5, MemOperand(a7, 0));\n    __ AddWord(a7, a7, Operand(kSystemPointerSize));\n    __ AddWord(scratch, scratch, Operand(kTaggedSize));\n    __ Branch(&loop, ne, scratch, Operand(sp));\n    __ bind(&done);\n  }\n\n  // Tail-call to the actual Call or Construct builtin.\n  __ TailCallBuiltin(target_builtin);\n\n  __ bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n}", "name_and_para": "void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,\n                                               Builtin target_builtin) "}], [{"name": "Builtins::Generate_ReflectConstruct", "content": "void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0       : argc\n  //  -- sp[0]   : receiver\n  //  -- sp[8]   : target\n  //  -- sp[16]  : argumentsList\n  //  -- sp[24]  : new.target (optional)\n  // -----------------------------------\n\n  ASM_LOCATION(\"Builtins::Generate_ReflectConstruct\");\n\n  Register argc = x0;\n  Register arguments_list = x2;\n  Register target = x1;\n  Register new_target = x3;\n  Register undefined_value = x4;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n\n  // 1. Load target into x1 (if present), argumentsList into x2 (if present),\n  // new.target into x3 (if present, otherwise use target), remove all\n  // arguments from the stack (including the receiver), and push thisArgument\n  // (if present) instead.\n  {\n    Label done;\n    __ Mov(target, undefined_value);\n    __ Mov(arguments_list, undefined_value);\n    __ Mov(new_target, undefined_value);\n    __ Cmp(argc, Immediate(JSParameterCount(1)));\n    __ B(lt, &done);\n    __ Peek(target, kSystemPointerSize);\n    __ B(eq, &done);\n    __ Peek(arguments_list, 2 * kSystemPointerSize);\n    __ Mov(new_target, target);  // new.target defaults to target\n    __ Cmp(argc, Immediate(JSParameterCount(3)));\n    __ B(lt, &done);\n    __ Peek(new_target, 3 * kSystemPointerSize);\n    __ bind(&done);\n  }\n\n  __ DropArguments(argc, MacroAssembler::kCountIncludesReceiver);\n\n  // Push receiver (undefined).\n  __ PushArgument(undefined_value);\n\n  // ----------- S t a t e -------------\n  //  -- x2      : argumentsList\n  //  -- x1      : target\n  //  -- x3      : new.target\n  //  -- sp[0]   : receiver (undefined)\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for constructor target here,\n  // since that's the first thing the Construct/ConstructWithArrayLike\n  // builtins will do.\n\n  // 3. We don't need to check explicitly for constructor new.target here,\n  // since that's the second thing the Construct/ConstructWithArrayLike\n  // builtins will do.\n\n  // 4. Construct the target with the given new.target and argumentsList.\n  __ TailCallBuiltin(Builtin::kConstructWithArrayLike);\n}", "name_and_para": "void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ReflectConstruct", "content": "void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0     : argc\n  //  -- sp[0]   : receiver\n  //  -- sp[8]   : target\n  //  -- sp[16]  : argumentsList\n  //  -- sp[24]  : new.target (optional)\n  // -----------------------------------\n  Register argc = a0;\n  Register arguments_list = a2;\n  Register target = a1;\n  Register new_target = a3;\n  Register undefined_value = a4;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n\n  // 1. Load target into a1 (if present), argumentsList into a2 (if present),\n  // new.target into a3 (if present, otherwise use target), remove all\n  // arguments from the stack (including the receiver), and push thisArgument\n  // (if present) instead.\n  {\n    // Claim (3 - argc) dummy arguments form the stack, to put the stack in a\n    // consistent state for a simple pop operation.\n    __ LoadWord(target, MemOperand(sp, kSystemPointerSize));\n    __ LoadWord(arguments_list, MemOperand(sp, 2 * kSystemPointerSize));\n    __ LoadWord(new_target, MemOperand(sp, 3 * kSystemPointerSize));\n\n    Label done0, done1, done2;\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ SubWord(scratch, argc, Operand(JSParameterCount(0)));\n    __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);\n    __ Move(arguments_list, undefined_value);  // if argc == 0\n    __ Move(new_target, undefined_value);      // if argc == 0\n    __ Move(target, undefined_value);          // if argc == 0\n    __ bind(&done0);\n\n    __ Branch(&done1, ne, scratch, Operand(1), Label::Distance::kNear);\n    __ Move(arguments_list, undefined_value);  // if argc == 1\n    __ Move(new_target, target);               // if argc == 1\n    __ bind(&done1);\n\n    __ Branch(&done2, ne, scratch, Operand(2), Label::Distance::kNear);\n    __ Move(new_target, target);  // if argc == 2\n    __ bind(&done2);\n\n    __ DropArgumentsAndPushNewReceiver(argc, undefined_value,\n                                       MacroAssembler::kCountIsInteger,\n                                       MacroAssembler::kCountIncludesReceiver);\n  }\n\n  // ----------- S t a t e -------------\n  //  -- a2    : argumentsList\n  //  -- a1    : target\n  //  -- a3    : new.target\n  //  -- sp[0] : receiver (undefined)\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for constructor target here,\n  // since that's the first thing the Construct/ConstructWithArrayLike\n  // builtins will do.\n\n  // 3. We don't need to check explicitly for constructor new.target here,\n  // since that's the second thing the Construct/ConstructWithArrayLike\n  // builtins will do.\n\n  // 4. Construct the target with the given new.target and argumentsList.\n  __ TailCallBuiltin(Builtin::kConstructWithArrayLike);\n}", "name_and_para": "void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ReflectApply", "content": "void Builtins::Generate_ReflectApply(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0     : argc\n  //  -- sp[0]  : receiver\n  //  -- sp[8]  : target         (if argc >= 1)\n  //  -- sp[16] : thisArgument   (if argc >= 2)\n  //  -- sp[24] : argumentsList  (if argc == 3)\n  // -----------------------------------\n\n  ASM_LOCATION(\"Builtins::Generate_ReflectApply\");\n\n  Register argc = x0;\n  Register arguments_list = x2;\n  Register target = x1;\n  Register this_argument = x4;\n  Register undefined_value = x3;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n\n  // 1. Load target into x1 (if present), argumentsList into x2 (if present),\n  // remove all arguments from the stack (including the receiver), and push\n  // thisArgument (if present) instead.\n  {\n    Label done;\n    __ Mov(target, undefined_value);\n    __ Mov(this_argument, undefined_value);\n    __ Mov(arguments_list, undefined_value);\n    __ Cmp(argc, Immediate(JSParameterCount(1)));\n    __ B(lt, &done);\n    __ Peek(target, kSystemPointerSize);\n    __ B(eq, &done);\n    __ Peek(this_argument, 2 * kSystemPointerSize);\n    __ Cmp(argc, Immediate(JSParameterCount(3)));\n    __ B(lt, &done);\n    __ Peek(arguments_list, 3 * kSystemPointerSize);\n    __ bind(&done);\n  }\n  __ DropArguments(argc, MacroAssembler::kCountIncludesReceiver);\n  __ PushArgument(this_argument);\n\n  // ----------- S t a t e -------------\n  //  -- x2      : argumentsList\n  //  -- x1      : target\n  //  -- sp[0]   : thisArgument\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for callable target here,\n  // since that's the first thing the Call/CallWithArrayLike builtins\n  // will do.\n\n  // 3. Apply the target to the given argumentsList.\n  __ TailCallBuiltin(Builtin::kCallWithArrayLike);\n}", "name_and_para": "void Builtins::Generate_ReflectApply(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ReflectApply", "content": "void Builtins::Generate_ReflectApply(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0     : argc\n  //  -- sp[0]  : receiver\n  //  -- sp[8]  : target         (if argc >= 1)\n  //  -- sp[16] : thisArgument   (if argc >= 2)\n  //  -- sp[24] : argumentsList  (if argc == 3)\n  // -----------------------------------\n\n  Register argc = a0;\n  Register arguments_list = a2;\n  Register target = a1;\n  Register this_argument = a5;\n  Register undefined_value = a3;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n\n  // 1. Load target into a1 (if present), argumentsList into a2 (if present),\n  // remove all arguments from the stack (including the receiver), and push\n  // thisArgument (if present) instead.\n  {\n    // Claim (3 - argc) dummy arguments form the stack, to put the stack in a\n    // consistent state for a simple pop operation.\n\n    __ LoadWord(target, MemOperand(sp, kSystemPointerSize));\n    __ LoadWord(this_argument, MemOperand(sp, 2 * kSystemPointerSize));\n    __ LoadWord(arguments_list, MemOperand(sp, 3 * kSystemPointerSize));\n\n    Label done0, done1, done2;\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ SubWord(scratch, argc, Operand(JSParameterCount(0)));\n    __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);\n    __ Move(arguments_list, undefined_value);  // if argc == 0\n    __ Move(this_argument, undefined_value);   // if argc == 0\n    __ Move(target, undefined_value);          // if argc == 0\n    __ bind(&done0);                           // argc != 0\n\n    __ Branch(&done1, ne, scratch, Operand(1), Label::Distance::kNear);\n    __ Move(arguments_list, undefined_value);  // if argc == 1\n    __ Move(this_argument, undefined_value);   // if argc == 1\n    __ bind(&done1);                           // argc > 1\n\n    __ Branch(&done2, ne, scratch, Operand(2), Label::Distance::kNear);\n    __ Move(arguments_list, undefined_value);  // if argc == 2\n    __ bind(&done2);                           // argc > 2\n\n    __ DropArgumentsAndPushNewReceiver(argc, this_argument,\n                                       MacroAssembler::kCountIsInteger,\n                                       MacroAssembler::kCountIncludesReceiver);\n  }\n\n  // ----------- S t a t e -------------\n  //  -- a2    : argumentsList\n  //  -- a1    : target\n  //  -- a3    : undefined root value\n  //  -- sp[0] : thisArgument\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for callable target here,\n  // since that's the first thing the Call/CallWithArrayLike builtins\n  // will do.\n\n  // 3. Apply the target to the given argumentsList.\n  __ TailCallBuiltin(Builtin::kCallWithArrayLike);\n}", "name_and_para": "void Builtins::Generate_ReflectApply(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_FunctionPrototypeCall", "content": "void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {\n  Register argc = x0;\n  Register function = x1;\n\n  ASM_LOCATION(\"Builtins::Generate_FunctionPrototypeCall\");\n\n  // 1. Get the callable to call (passed as receiver) from the stack.\n  __ Peek(function, __ ReceiverOperand());\n\n  // 2. Handle case with no arguments.\n  {\n    Label non_zero;\n    Register scratch = x10;\n    __ Cmp(argc, JSParameterCount(0));\n    __ B(gt, &non_zero);\n    __ LoadRoot(scratch, RootIndex::kUndefinedValue);\n    // Overwrite receiver with undefined, which will be the new receiver.\n    // We do not need to overwrite the padding slot above it with anything.\n    __ Poke(scratch, 0);\n    // Call function. The argument count is already zero.\n    __ TailCallBuiltin(Builtins::Call());\n    __ Bind(&non_zero);\n  }\n\n  Label arguments_ready;\n  // 3. Shift arguments. It depends if the arguments is even or odd.\n  // That is if padding exists or not.\n  {\n    Label even;\n    Register copy_from = x10;\n    Register copy_to = x11;\n    Register count = x12;\n    UseScratchRegisterScope temps(masm);\n    Register argc_without_receiver = temps.AcquireX();\n    __ Sub(argc_without_receiver, argc, kJSArgcReceiverSlots);\n\n    // CopyDoubleWords changes the count argument.\n    __ Mov(count, argc_without_receiver);\n    __ Tbz(argc_without_receiver, 0, &even);\n\n    // Shift arguments one slot down on the stack (overwriting the original\n    // receiver).\n    __ SlotAddress(copy_from, 1);\n    __ Sub(copy_to, copy_from, kSystemPointerSize);\n    __ CopyDoubleWords(copy_to, copy_from, count);\n    // Overwrite the duplicated remaining last argument.\n    __ Poke(padreg, Operand(argc_without_receiver, LSL, kXRegSizeLog2));\n    __ B(&arguments_ready);\n\n    // Copy arguments one slot higher in memory, overwriting the original\n    // receiver and padding.\n    __ Bind(&even);\n    __ SlotAddress(copy_from, count);\n    __ Add(copy_to, copy_from, kSystemPointerSize);\n    __ CopyDoubleWords(copy_to, copy_from, count,\n                       MacroAssembler::kSrcLessThanDst);\n    __ Drop(2);\n  }\n\n  // 5. Adjust argument count to make the original first argument the new\n  //    receiver and call the callable.\n  __ Bind(&arguments_ready);\n  __ Sub(argc, argc, 1);\n  __ TailCallBuiltin(Builtins::Call());\n}", "name_and_para": "void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) "}, {"name": "Builtins::Generate_FunctionPrototypeCall", "content": "void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {\n  // 1. Get the callable to call (passed as receiver) from the stack.\n  { __ Pop(a1); }\n\n  // 2. Make sure we have at least one argument.\n  // a0: actual number of arguments\n  {\n    Label done;\n    __ Branch(&done, ne, a0, Operand(JSParameterCount(0)),\n              Label::Distance::kNear);\n    __ PushRoot(RootIndex::kUndefinedValue);\n    __ AddWord(a0, a0, Operand(1));\n    __ bind(&done);\n  }\n\n  // 3. Adjust the actual number of arguments.\n  __ AddWord(a0, a0, -1);\n\n  // 4. Call the callable.\n  __ TailCallBuiltin(Builtins::Call());\n}", "name_and_para": "void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_FunctionPrototypeApply", "content": "void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0       : argc\n  //  -- sp[0]    : receiver\n  //  -- sp[8]    : thisArg  (if argc >= 1)\n  //  -- sp[16]   : argArray (if argc == 2)\n  // -----------------------------------\n\n  ASM_LOCATION(\"Builtins::Generate_FunctionPrototypeApply\");\n\n  Register argc = x0;\n  Register receiver = x1;\n  Register arg_array = x2;\n  Register this_arg = x3;\n  Register undefined_value = x4;\n  Register null_value = x5;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n  __ LoadRoot(null_value, RootIndex::kNullValue);\n\n  // 1. Load receiver into x1, argArray into x2 (if present), remove all\n  // arguments from the stack (including the receiver), and push thisArg (if\n  // present) instead.\n  {\n    Label done;\n    __ Mov(this_arg, undefined_value);\n    __ Mov(arg_array, undefined_value);\n    __ Peek(receiver, 0);\n    __ Cmp(argc, Immediate(JSParameterCount(1)));\n    __ B(lt, &done);\n    __ Peek(this_arg, kSystemPointerSize);\n    __ B(eq, &done);\n    __ Peek(arg_array, 2 * kSystemPointerSize);\n    __ bind(&done);\n  }\n  __ DropArguments(argc, MacroAssembler::kCountIncludesReceiver);\n  __ PushArgument(this_arg);\n\n  // ----------- S t a t e -------------\n  //  -- x2      : argArray\n  //  -- x1      : receiver\n  //  -- sp[0]   : thisArg\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for callable receiver here,\n  // since that's the first thing the Call/CallWithArrayLike builtins\n  // will do.\n\n  // 3. Tail call with no arguments if argArray is null or undefined.\n  Label no_arguments;\n  __ CmpTagged(arg_array, null_value);\n  __ CcmpTagged(arg_array, undefined_value, ZFlag, ne);\n  __ B(eq, &no_arguments);\n\n  // 4a. Apply the receiver to the given argArray.\n  __ TailCallBuiltin(Builtin::kCallWithArrayLike);\n\n  // 4b. The argArray is either null or undefined, so we tail call without any\n  // arguments to the receiver.\n  __ Bind(&no_arguments);\n  {\n    __ Mov(x0, JSParameterCount(0));\n    DCHECK_EQ(receiver, x1);\n    __ TailCallBuiltin(Builtins::Call());\n  }\n}", "name_and_para": "void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) "}, {"name": "Builtins::Generate_FunctionPrototypeApply", "content": "void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0    : argc\n  //  -- sp[0] : receiver\n  //  -- sp[4] : thisArg\n  //  -- sp[8] : argArray\n  // -----------------------------------\n\n  Register argc = a0;\n  Register arg_array = a2;\n  Register receiver = a1;\n  Register this_arg = a5;\n  Register undefined_value = a3;\n  Register scratch = a4;\n\n  __ LoadRoot(undefined_value, RootIndex::kUndefinedValue);\n\n  // 1. Load receiver into a1, argArray into a2 (if present), remove all\n  // arguments from the stack (including the receiver), and push thisArg (if\n  // present) instead.\n  {\n    // Claim (2 - argc) dummy arguments form the stack, to put the stack in a\n    // consistent state for a simple pop operation.\n\n    __ LoadWord(this_arg, MemOperand(sp, kSystemPointerSize));\n    __ LoadWord(arg_array, MemOperand(sp, 2 * kSystemPointerSize));\n\n    Label done0, done1;\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.Acquire();\n    __ SubWord(scratch, argc, JSParameterCount(0));\n    __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);\n    __ Move(arg_array, undefined_value);  // if argc == 0\n    __ Move(this_arg, undefined_value);   // if argc == 0\n    __ bind(&done0);                      // else (i.e., argc > 0)\n\n    __ Branch(&done1, ne, scratch, Operand(1), Label::Distance::kNear);\n    __ Move(arg_array, undefined_value);  // if argc == 1\n    __ bind(&done1);                      // else (i.e., argc > 1)\n\n    __ LoadWord(receiver, MemOperand(sp));\n    __ DropArgumentsAndPushNewReceiver(argc, this_arg,\n                                       MacroAssembler::kCountIsInteger,\n                                       MacroAssembler::kCountIncludesReceiver);\n  }\n\n  // ----------- S t a t e -------------\n  //  -- a2    : argArray\n  //  -- a1    : receiver\n  //  -- a3    : undefined root value\n  //  -- sp[0] : thisArg\n  // -----------------------------------\n\n  // 2. We don't need to check explicitly for callable receiver here,\n  // since that's the first thing the Call/CallWithArrayLike builtins\n  // will do.\n\n  // 3. Tail call with no arguments if argArray is null or undefined.\n  Label no_arguments;\n  __ LoadRoot(scratch, RootIndex::kNullValue);\n  __ CompareTaggedAndBranch(&no_arguments, eq, arg_array, Operand(scratch));\n  __ CompareTaggedAndBranch(&no_arguments, eq, arg_array,\n                            Operand(undefined_value));\n\n  // 4a. Apply the receiver to the given argArray.\n  __ TailCallBuiltin(Builtin::kCallWithArrayLike);\n\n  // 4b. The argArray is either null or undefined, so we tail call without any\n  // arguments to the receiver.\n  __ bind(&no_arguments);\n  {\n    __ li(a0, JSParameterCount(0));\n    DCHECK(receiver == a1);\n    __ TailCallBuiltin(Builtins::Call());\n  }\n}", "name_and_para": "void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_BaselineOnStackReplacement", "content": "void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {\n  using D = OnStackReplacementDescriptor;\n  static_assert(D::kParameterCount == 1);\n\n  __ ldr(kContextRegister,\n         MemOperand(fp, BaselineFrameConstants::kContextOffset));\n  OnStackReplacement(masm, OsrSourceTier::kBaseline,\n                     D::MaybeTargetCodeRegister());\n}", "name_and_para": "void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) "}, {"name": "Builtins::Generate_BaselineOnStackReplacement", "content": "void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {\n  using D = OnStackReplacementDescriptor;\n  static_assert(D::kParameterCount == 1);\n\n  __ LoadWord(kContextRegister,\n              MemOperand(fp, BaselineFrameConstants::kContextOffset));\n  OnStackReplacement(masm, OsrSourceTier::kBaseline,\n                     D::MaybeTargetCodeRegister());\n}", "name_and_para": "void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_InterpreterOnStackReplacement", "content": "void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {\n  using D = OnStackReplacementDescriptor;\n  static_assert(D::kParameterCount == 1);\n  OnStackReplacement(masm, OsrSourceTier::kInterpreter,\n                     D::MaybeTargetCodeRegister());\n}", "name_and_para": "void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) "}, {"name": "Builtins::Generate_InterpreterOnStackReplacement", "content": "void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {\n  using D = OnStackReplacementDescriptor;\n  static_assert(D::kParameterCount == 1);\n  OnStackReplacement(masm, OsrSourceTier::kInterpreter,\n                     D::MaybeTargetCodeRegister());\n}", "name_and_para": "void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) "}], [{"name": "OnStackReplacement", "content": "void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,\n                        Register maybe_target_code) {\n  Label jump_to_optimized_code;\n  {\n    // If maybe_target_code is not null, no need to call into runtime. A\n    // precondition here is: if maybe_target_code is a InstructionStream object,\n    // it must NOT be marked_for_deoptimization (callers must ensure this).\n    __ CompareTaggedAndBranch(x0, Smi::zero(), ne, &jump_to_optimized_code);\n  }\n\n  ASM_CODE_COMMENT(masm);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kCompileOptimizedOSR);\n  }\n\n  // If the code object is null, just return to the caller.\n  __ CompareTaggedAndBranch(x0, Smi::zero(), ne, &jump_to_optimized_code);\n  __ Ret();\n\n  __ Bind(&jump_to_optimized_code);\n  DCHECK_EQ(maybe_target_code, x0);  // Already in the right spot.\n\n  // OSR entry tracing.\n  {\n    Label next;\n    __ Mov(x1, ExternalReference::address_of_log_or_trace_osr());\n    __ Ldrsb(x1, MemOperand(x1));\n    __ Tst(x1, 0xFF);  // Mask to the LSB.\n    __ B(eq, &next);\n\n    {\n      FrameScope scope(masm, StackFrame::INTERNAL);\n      __ Push(x0, padreg);  // Preserve the code object.\n      __ CallRuntime(Runtime::kLogOrTraceOptimizedOSREntry, 0);\n      __ Pop(padreg, x0);\n    }\n\n    __ Bind(&next);\n  }\n\n  if (source == OsrSourceTier::kInterpreter) {\n    // Drop the handler frame that is be sitting on top of the actual\n    // JavaScript frame. This is the case then OSR is triggered from bytecode.\n    __ LeaveFrame(StackFrame::STUB);\n  }\n\n  // Load deoptimization data from the code object.\n  // <deopt_data> = <code>[#deoptimization_data_offset]\n  __ LoadProtectedPointerField(\n      x1,\n      FieldMemOperand(x0, Code::kDeoptimizationDataOrInterpreterDataOffset));\n\n  // Load the OSR entrypoint offset from the deoptimization data.\n  // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]\n  __ SmiUntagField(\n      x1, FieldMemOperand(x1, TrustedFixedArray::OffsetOfElementAt(\n                                  DeoptimizationData::kOsrPcOffsetIndex)));\n\n  __ LoadCodeInstructionStart(x0, x0, kJSEntrypointTag);\n\n  // Compute the target address = code_entry + osr_offset\n  // <entry_addr> = <code_entry> + <osr_offset>\n  Generate_OSREntry(masm, x0, x1);\n}", "name_and_para": "void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,\n                        Register maybe_target_code) "}, {"name": "OnStackReplacement", "content": "void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,\n                        Register maybe_target_code) {\n  Label jump_to_optimized_code;\n  {\n    // If maybe_target_code is not null, no need to call into runtime. A\n    // precondition here is: if maybe_target_code is a InstructionStream object,\n    // it must NOT be marked_for_deoptimization (callers must ensure this).\n    __ CompareTaggedAndBranch(&jump_to_optimized_code, ne, maybe_target_code,\n                              Operand(Smi::zero()));\n  }\n  ASM_CODE_COMMENT(masm);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kCompileOptimizedOSR);\n  }\n\n  // If the code object is null, just return to the caller.\n  // If the code object is null, just return to the caller.\n  __ CompareTaggedAndBranch(&jump_to_optimized_code, ne, maybe_target_code,\n                            Operand(Smi::zero()));\n  __ Ret();\n  DCHECK_EQ(maybe_target_code, a0);  // Already in the right spot.\n\n  __ bind(&jump_to_optimized_code);\n\n  // OSR entry tracing.\n  {\n    Label next;\n    __ li(a1, ExternalReference::address_of_log_or_trace_osr());\n    __ Lbu(a1, MemOperand(a1));\n    __ Branch(&next, eq, a1, Operand(zero_reg));\n\n    {\n      FrameScope scope(masm, StackFrame::INTERNAL);\n      __ Push(a0);  // Preserve the code object.\n      __ CallRuntime(Runtime::kLogOrTraceOptimizedOSREntry, 0);\n      __ Pop(a0);\n    }\n\n    __ bind(&next);\n  }\n\n  if (source == OsrSourceTier::kInterpreter) {\n    // Drop the handler frame that is be sitting on top of the actual\n    // JavaScript frame. This is the case then OSR is triggered from bytecode.\n    __ LeaveFrame(StackFrame::STUB);\n  }\n\n  // Load deoptimization data from the code object.\n  // <deopt_data> = <code>[#deoptimization_data_offset]\n  __ LoadProtectedPointerField(\n      a1, FieldMemOperand(maybe_target_code,\n                          Code::kDeoptimizationDataOrInterpreterDataOffset));\n\n  // Load the OSR entrypoint offset from the deoptimization data.\n  // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]\n  __ SmiUntagField(\n      a1, FieldMemOperand(a1, TrustedFixedArray::OffsetOfElementAt(\n                                  DeoptimizationData::kOsrPcOffsetIndex)));\n\n  __ LoadCodeInstructionStart(a0, a0, kJSEntrypointTag);\n\n  // Compute the target address = code_entry + osr_offset\n  // <entry_addr> = <code_entry> + <osr_offset>\n  Generate_OSREntry(masm, a0, Operand(a1));\n}", "name_and_para": "void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,\n                        Register maybe_target_code) "}], [{"name": "OsrSourceTier", "content": "enum class OsrSourceTier {\n  kInterpreter,\n  kBaseline,\n}", "name_and_para": ""}, {"name": "OsrSourceTier", "content": "enum class OsrSourceTier {\n  kInterpreter,\n  kBaseline,\n}", "name_and_para": ""}], [{"name": "Generate_OSREntry", "content": "void Generate_OSREntry(MacroAssembler* masm, Register entry_address,\n                       Operand offset = Operand(0)) {\n  // Pop the return address to this function's caller from the return stack\n  // buffer, since we'll never return to it.\n  Label jump;\n  __ Adr(lr, &jump);\n  __ Ret();\n\n  __ Bind(&jump);\n\n  UseScratchRegisterScope temps(masm);\n  temps.Exclude(x17);\n  if (offset.IsZero()) {\n    __ Mov(x17, entry_address);\n  } else {\n    __ Add(x17, entry_address, offset);\n  }\n  __ Br(x17);\n}", "name_and_para": "void Generate_OSREntry(MacroAssembler* masm, Register entry_address,\n                       Operand offset = Operand(0)) "}, {"name": "Generate_OSREntry", "content": "void Generate_OSREntry(MacroAssembler* masm, Register entry_address,\n                       Operand offset = Operand(0)) {\n  __ AddWord(ra, entry_address, offset);\n  // And \"return\" to the OSR entry point of the function.\n  __ Ret();\n}", "name_and_para": "void Generate_OSREntry(MacroAssembler* masm, Register entry_address,\n                       Operand offset = Operand(0)) "}], [{"name": "Builtins::Generate_NotifyDeoptimized", "content": "void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kNotifyDeoptimized);\n  }\n\n  // Pop TOS register and padding.\n  DCHECK_EQ(kInterpreterAccumulatorRegister.code(), x0.code());\n  __ Pop(x0, padreg);\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) "}, {"name": "Builtins::Generate_NotifyDeoptimized", "content": "void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kNotifyDeoptimized);\n  }\n\n  DCHECK_EQ(kInterpreterAccumulatorRegister.code(), a0.code());\n  __ LoadWord(a0, MemOperand(sp, 0 * kSystemPointerSize));\n  __ AddWord(sp, sp, Operand(1 * kSystemPointerSize));  // Remove state.\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ContinueToJavaScriptBuiltinWithResult", "content": "void Builtins::Generate_ContinueToJavaScriptBuiltinWithResult(\n    MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, true, true);\n}", "name_and_para": "void Builtins::Generate_ContinueToJavaScriptBuiltinWithResult(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_ContinueToJavaScriptBuiltinWithResult", "content": "void Builtins::Generate_ContinueToJavaScriptBuiltinWithResult(\n    MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, true, true);\n}", "name_and_para": "void Builtins::Generate_ContinueToJavaScriptBuiltinWithResult(\n    MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ContinueToJavaScriptBuiltin", "content": "void Builtins::Generate_ContinueToJavaScriptBuiltin(MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, true, false);\n}", "name_and_para": "void Builtins::Generate_ContinueToJavaScriptBuiltin(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ContinueToJavaScriptBuiltin", "content": "void Builtins::Generate_ContinueToJavaScriptBuiltin(MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, true, false);\n}", "name_and_para": "void Builtins::Generate_ContinueToJavaScriptBuiltin(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ContinueToCodeStubBuiltinWithResult", "content": "void Builtins::Generate_ContinueToCodeStubBuiltinWithResult(\n    MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, false, true);\n}", "name_and_para": "void Builtins::Generate_ContinueToCodeStubBuiltinWithResult(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_ContinueToCodeStubBuiltinWithResult", "content": "void Builtins::Generate_ContinueToCodeStubBuiltinWithResult(\n    MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, false, true);\n}", "name_and_para": "void Builtins::Generate_ContinueToCodeStubBuiltinWithResult(\n    MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ContinueToCodeStubBuiltin", "content": "void Builtins::Generate_ContinueToCodeStubBuiltin(MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, false, false);\n}", "name_and_para": "void Builtins::Generate_ContinueToCodeStubBuiltin(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ContinueToCodeStubBuiltin", "content": "void Builtins::Generate_ContinueToCodeStubBuiltin(MacroAssembler* masm) {\n  Generate_ContinueToBuiltinHelper(masm, false, false);\n}", "name_and_para": "void Builtins::Generate_ContinueToCodeStubBuiltin(MacroAssembler* masm) "}], [{"name": "Generate_ContinueToBuiltinHelper", "content": "void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,\n                                      bool java_script_builtin,\n                                      bool with_result) {\n  const RegisterConfiguration* config(RegisterConfiguration::Default());\n  int allocatable_register_count = config->num_allocatable_general_registers();\n  int frame_size = BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp +\n                   (allocatable_register_count +\n                    BuiltinContinuationFrameConstants::PaddingSlotCount(\n                        allocatable_register_count)) *\n                       kSystemPointerSize;\n\n  UseScratchRegisterScope temps(masm);\n  Register scratch = temps.AcquireX();  // Temp register is not allocatable.\n\n  // Set up frame pointer.\n  __ Add(fp, sp, frame_size);\n\n  if (with_result) {\n    if (java_script_builtin) {\n      __ mov(scratch, x0);\n    } else {\n      // Overwrite the hole inserted by the deoptimizer with the return value\n      // from the LAZY deopt point.\n      __ Str(x0, MemOperand(\n                     fp, BuiltinContinuationFrameConstants::kCallerSPOffset));\n    }\n  }\n\n  // Restore registers in pairs.\n  int offset = -BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp -\n               allocatable_register_count * kSystemPointerSize;\n  for (int i = allocatable_register_count - 1; i > 0; i -= 2) {\n    int code1 = config->GetAllocatableGeneralCode(i);\n    int code2 = config->GetAllocatableGeneralCode(i - 1);\n    Register reg1 = Register::from_code(code1);\n    Register reg2 = Register::from_code(code2);\n    __ Ldp(reg1, reg2, MemOperand(fp, offset));\n    offset += 2 * kSystemPointerSize;\n  }\n\n  // Restore first register separately, if number of registers is odd.\n  if (allocatable_register_count % 2 != 0) {\n    int code = config->GetAllocatableGeneralCode(0);\n    __ Ldr(Register::from_code(code), MemOperand(fp, offset));\n  }\n\n  if (java_script_builtin) __ SmiUntag(kJavaScriptCallArgCountRegister);\n\n  if (java_script_builtin && with_result) {\n    // Overwrite the hole inserted by the deoptimizer with the return value from\n    // the LAZY deopt point. x0 contains the arguments count, the return value\n    // from LAZY is always the last argument.\n    constexpr int return_offset =\n        BuiltinContinuationFrameConstants::kCallerSPOffset /\n            kSystemPointerSize -\n        kJSArgcReceiverSlots;\n    __ add(x0, x0, return_offset);\n    __ Str(scratch, MemOperand(fp, x0, LSL, kSystemPointerSizeLog2));\n    // Recover argument count.\n    __ sub(x0, x0, return_offset);\n  }\n\n  // Load builtin index (stored as a Smi) and use it to get the builtin start\n  // address from the builtins table.\n  Register builtin = scratch;\n  __ Ldr(\n      builtin,\n      MemOperand(fp, BuiltinContinuationFrameConstants::kBuiltinIndexOffset));\n\n  // Restore fp, lr.\n  __ Mov(sp, fp);\n  __ Pop<MacroAssembler::kAuthLR>(fp, lr);\n\n  __ LoadEntryFromBuiltinIndex(builtin, builtin);\n  __ Jump(builtin);\n}", "name_and_para": "void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,\n                                      bool java_script_builtin,\n                                      bool with_result) "}, {"name": "Generate_ContinueToBuiltinHelper", "content": "void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,\n                                      bool java_script_builtin,\n                                      bool with_result) {\n  const RegisterConfiguration* config(RegisterConfiguration::Default());\n  int allocatable_register_count = config->num_allocatable_general_registers();\n  UseScratchRegisterScope temp(masm);\n  Register scratch = temp.Acquire();\n  if (with_result) {\n    if (java_script_builtin) {\n      __ Move(scratch, a0);\n    } else {\n      // Overwrite the hole inserted by the deoptimizer with the return value\n      // from the LAZY deopt point.\n      __ StoreWord(\n          a0, MemOperand(\n                  sp, config->num_allocatable_general_registers() *\n                              kSystemPointerSize +\n                          BuiltinContinuationFrameConstants::kFixedFrameSize));\n    }\n  }\n  for (int i = allocatable_register_count - 1; i >= 0; --i) {\n    int code = config->GetAllocatableGeneralCode(i);\n    __ Pop(Register::from_code(code));\n    if (java_script_builtin && code == kJavaScriptCallArgCountRegister.code()) {\n      __ SmiUntag(Register::from_code(code));\n    }\n  }\n\n  if (with_result && java_script_builtin) {\n    // Overwrite the hole inserted by the deoptimizer with the return value from\n    // the LAZY deopt point. t0 contains the arguments count, the return value\n    // from LAZY is always the last argument.\n    constexpr int return_value_offset =\n        BuiltinContinuationFrameConstants::kFixedSlotCount -\n        kJSArgcReceiverSlots;\n    __ AddWord(a0, a0, Operand(return_value_offset));\n    __ CalcScaledAddress(t0, sp, a0, kSystemPointerSizeLog2);\n    __ StoreWord(scratch, MemOperand(t0));\n    // Recover arguments count.\n    __ SubWord(a0, a0, Operand(return_value_offset));\n  }\n\n  __ LoadWord(\n      fp,\n      MemOperand(sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));\n  // Load builtin index (stored as a Smi) and use it to get the builtin start\n  // address from the builtins table.\n  __ Pop(t6);\n  __ AddWord(sp, sp,\n             Operand(BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));\n  __ Pop(ra);\n  __ LoadEntryFromBuiltinIndex(t6, t6);\n  __ Jump(t6);\n}", "name_and_para": "void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,\n                                      bool java_script_builtin,\n                                      bool with_result) "}], [{"name": "Builtins::Generate_InterpreterEnterAtBytecode", "content": "void Builtins::Generate_InterpreterEnterAtBytecode(MacroAssembler* masm) {\n  Generate_InterpreterEnterBytecode(masm);\n}", "name_and_para": "void Builtins::Generate_InterpreterEnterAtBytecode(MacroAssembler* masm) "}, {"name": "Builtins::Generate_InterpreterEnterAtBytecode", "content": "void Builtins::Generate_InterpreterEnterAtBytecode(MacroAssembler* masm) {\n  Generate_InterpreterEnterBytecode(masm);\n}", "name_and_para": "void Builtins::Generate_InterpreterEnterAtBytecode(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_InterpreterEnterAtNextBytecode", "content": "void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) {\n  // Get bytecode array and bytecode offset from the stack frame.\n  __ ldr(kInterpreterBytecodeArrayRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  Label enter_bytecode, function_entry_bytecode;\n  __ cmp(kInterpreterBytecodeOffsetRegister,\n         Operand(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                 kFunctionEntryBytecodeOffset));\n  __ B(eq, &function_entry_bytecode);\n\n  // Load the current bytecode.\n  __ Ldrb(x1, MemOperand(kInterpreterBytecodeArrayRegister,\n                         kInterpreterBytecodeOffsetRegister));\n\n  // Advance to the next bytecode.\n  Label if_return;\n  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,\n                                kInterpreterBytecodeOffsetRegister, x1, x2, x3,\n                                &if_return);\n\n  __ bind(&enter_bytecode);\n  // Convert new bytecode offset to a Smi and save in the stackframe.\n  __ SmiTag(x2, kInterpreterBytecodeOffsetRegister);\n  __ Str(x2, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  Generate_InterpreterEnterBytecode(masm);\n\n  __ bind(&function_entry_bytecode);\n  // If the code deoptimizes during the implicit function entry stack interrupt\n  // check, it will have a bailout ID of kFunctionEntryBytecodeOffset, which is\n  // not a valid bytecode offset. Detect this case and advance to the first\n  // actual bytecode.\n  __ Mov(kInterpreterBytecodeOffsetRegister,\n         Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n  __ B(&enter_bytecode);\n\n  // We should never take the if_return path.\n  __ bind(&if_return);\n  __ Abort(AbortReason::kInvalidBytecodeAdvance);\n}", "name_and_para": "void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) "}, {"name": "Builtins::Generate_InterpreterEnterAtNextBytecode", "content": "void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) {\n  // Advance the current bytecode offset stored within the given interpreter\n  // stack frame. This simulates what all bytecode handlers do upon completion\n  // of the underlying operation.\n  __ LoadWord(kInterpreterBytecodeArrayRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ LoadWord(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister);\n\n  Label enter_bytecode, function_entry_bytecode;\n  __ Branch(&function_entry_bytecode, eq, kInterpreterBytecodeOffsetRegister,\n            Operand(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                    kFunctionEntryBytecodeOffset));\n\n  // Load the current bytecode.\n  __ AddWord(a1, kInterpreterBytecodeArrayRegister,\n             kInterpreterBytecodeOffsetRegister);\n  __ Lbu(a1, MemOperand(a1));\n\n  // Advance to the next bytecode.\n  Label if_return;\n  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,\n                                kInterpreterBytecodeOffsetRegister, a1, a2, a3,\n                                a4, &if_return);\n\n  __ bind(&enter_bytecode);\n  // Convert new bytecode offset to a Smi and save in the stackframe.\n  __ SmiTag(a2, kInterpreterBytecodeOffsetRegister);\n  __ StoreWord(\n      a2, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  Generate_InterpreterEnterBytecode(masm);\n\n  __ bind(&function_entry_bytecode);\n  // If the code deoptimizes during the implicit function entry stack interrupt\n  // check, it will have a bailout ID of kFunctionEntryBytecodeOffset, which is\n  // not a valid bytecode offset. Detect this case and advance to the first\n  // actual bytecode.\n  __ li(kInterpreterBytecodeOffsetRegister,\n        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n  __ Branch(&enter_bytecode);\n\n  // We should never take the if_return path.\n  __ bind(&if_return);\n  __ Abort(AbortReason::kInvalidBytecodeAdvance);\n}", "name_and_para": "void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) "}], [{"name": "Generate_InterpreterEnterBytecode", "content": "static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {\n  // Initialize the dispatch table register.\n  __ Mov(\n      kInterpreterDispatchTableRegister,\n      ExternalReference::interpreter_dispatch_table_address(masm->isolate()));\n\n  // Get the bytecode array pointer from the frame.\n  __ Ldr(kInterpreterBytecodeArrayRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n\n  if (v8_flags.debug_code) {\n    // Check function data field is actually a BytecodeArray object.\n    __ AssertNotSmi(\n        kInterpreterBytecodeArrayRegister,\n        AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry);\n    __ IsObjectType(kInterpreterBytecodeArrayRegister, x1, x1,\n                    BYTECODE_ARRAY_TYPE);\n    __ Assert(\n        eq, AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry);\n  }\n\n  // Get the target bytecode offset from the frame.\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  if (v8_flags.debug_code) {\n    Label okay;\n    __ cmp(kInterpreterBytecodeOffsetRegister,\n           Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n    __ B(ge, &okay);\n    __ Unreachable();\n    __ bind(&okay);\n  }\n\n  // Dispatch to the target bytecode.\n  __ Ldrb(x23, MemOperand(kInterpreterBytecodeArrayRegister,\n                          kInterpreterBytecodeOffsetRegister));\n  __ Mov(x1, Operand(x23, LSL, kSystemPointerSizeLog2));\n  __ Ldr(kJavaScriptCallCodeStartRegister,\n         MemOperand(kInterpreterDispatchTableRegister, x1));\n\n  {\n    UseScratchRegisterScope temps(masm);\n    temps.Exclude(x17);\n    __ Mov(x17, kJavaScriptCallCodeStartRegister);\n    __ Call(x17);\n  }\n\n  // We return here after having executed the function in the interpreter.\n  // Now jump to the correct point in the interpreter entry trampoline.\n  Label builtin_trampoline, trampoline_loaded;\n  Tagged<Smi> interpreter_entry_return_pc_offset(\n      masm->isolate()->heap()->interpreter_entry_return_pc_offset());\n  DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());\n\n  // If the SFI function_data is an InterpreterData, the function will have a\n  // custom copy of the interpreter entry trampoline for profiling. If so,\n  // get the custom trampoline, otherwise grab the entry address of the global\n  // trampoline.\n  __ Ldr(x1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n  __ LoadTaggedField(\n      x1, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n  GetSharedFunctionInfoData(masm, x1, x1, x2);\n  __ IsObjectType(x1, kInterpreterDispatchTableRegister,\n                  kInterpreterDispatchTableRegister, INTERPRETER_DATA_TYPE);\n  __ B(ne, &builtin_trampoline);\n\n  __ LoadProtectedPointerField(\n      x1, FieldMemOperand(x1, InterpreterData::kInterpreterTrampolineOffset));\n  __ LoadCodeInstructionStart(x1, x1, kJSEntrypointTag);\n  __ B(&trampoline_loaded);\n\n  __ Bind(&builtin_trampoline);\n  __ Mov(x1, ExternalReference::\n                 address_of_interpreter_entry_trampoline_instruction_start(\n                     masm->isolate()));\n  __ Ldr(x1, MemOperand(x1));\n\n  __ Bind(&trampoline_loaded);\n\n  {\n    UseScratchRegisterScope temps(masm);\n    temps.Exclude(x17);\n    __ Add(x17, x1, Operand(interpreter_entry_return_pc_offset.value()));\n    __ Br(x17);\n  }\n}", "name_and_para": "static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) "}, {"name": "Generate_InterpreterEnterBytecode", "content": "static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {\n  // Set the return address to the correct point in the interpreter entry\n  // trampoline.\n  Label builtin_trampoline, trampoline_loaded;\n  Tagged<Smi> interpreter_entry_return_pc_offset(\n      masm->isolate()->heap()->interpreter_entry_return_pc_offset());\n  DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());\n\n  // If the SFI function_data is an InterpreterData, the function will have a\n  // custom copy of the interpreter entry trampoline for profiling. If so,\n  // get the custom trampoline, otherwise grab the entry address of the global\n  // trampoline.\n  __ LoadWord(t0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));\n  __ LoadTaggedField(\n      t0, FieldMemOperand(t0, JSFunction::kSharedFunctionInfoOffset));\n  GetSharedFunctionInfoData(masm, t0, t0, t1);\n  __ GetObjectType(t0, kInterpreterDispatchTableRegister,\n                   kInterpreterDispatchTableRegister);\n  __ Branch(&builtin_trampoline, ne, kInterpreterDispatchTableRegister,\n            Operand(INTERPRETER_DATA_TYPE), Label::Distance::kNear);\n\n  __ LoadProtectedPointerField(\n      t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));\n  __ LoadCodeInstructionStart(t0, t0, kJSEntrypointTag);\n  __ BranchShort(&trampoline_loaded);\n\n  __ bind(&builtin_trampoline);\n  __ li(t0, ExternalReference::\n                address_of_interpreter_entry_trampoline_instruction_start(\n                    masm->isolate()));\n  __ LoadWord(t0, MemOperand(t0));\n\n  __ bind(&trampoline_loaded);\n  __ AddWord(ra, t0, Operand(interpreter_entry_return_pc_offset.value()));\n\n  // Initialize the dispatch table register.\n  __ li(kInterpreterDispatchTableRegister,\n        ExternalReference::interpreter_dispatch_table_address(masm->isolate()));\n\n  // Get the bytecode array pointer from the frame.\n  __ LoadWord(kInterpreterBytecodeArrayRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n\n  if (v8_flags.debug_code) {\n    // Check function data field is actually a BytecodeArray object.\n    __ SmiTst(kInterpreterBytecodeArrayRegister, kScratchReg);\n    __ Assert(ne,\n              AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry,\n              kScratchReg, Operand(zero_reg));\n    __ GetObjectType(kInterpreterBytecodeArrayRegister, a1, a1);\n    __ Assert(eq,\n              AbortReason::kFunctionDataShouldBeBytecodeArrayOnInterpreterEntry,\n              a1, Operand(BYTECODE_ARRAY_TYPE));\n  }\n\n  // Get the target bytecode offset from the frame.\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  if (v8_flags.debug_code) {\n    Label okay;\n    __ Branch(&okay, ge, kInterpreterBytecodeOffsetRegister,\n              Operand(BytecodeArray::kHeaderSize - kHeapObjectTag),\n              Label::Distance::kNear);\n    // Unreachable code.\n    __ break_(0xCC);\n    __ bind(&okay);\n  }\n\n  // Dispatch to the target bytecode.\n  __ AddWord(a1, kInterpreterBytecodeArrayRegister,\n             kInterpreterBytecodeOffsetRegister);\n  __ Lbu(a7, MemOperand(a1));\n  __ CalcScaledAddress(a1, kInterpreterDispatchTableRegister, a7,\n                       kSystemPointerSizeLog2);\n  __ LoadWord(kJavaScriptCallCodeStartRegister, MemOperand(a1));\n  __ Jump(kJavaScriptCallCodeStartRegister);\n}", "name_and_para": "static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_InterpreterPushArgsThenFastConstructFunction", "content": "void Builtins::Generate_InterpreterPushArgsThenFastConstructFunction(\n    MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  // -- x0 : argument count\n  // -- x1 : constructor to call (checked to be a JSFunction)\n  // -- x3 : new target\n  // -- x4 : address of the first argument\n  // -- cp : context pointer\n  // -----------------------------------\n  __ AssertFunction(x1);\n\n  // Check if target has a [[Construct]] internal method.\n  Label non_constructor;\n  __ LoadMap(x2, x1);\n  __ Ldrb(x2, FieldMemOperand(x2, Map::kBitFieldOffset));\n  __ TestAndBranchIfAllClear(x2, Map::Bits1::IsConstructorBit::kMask,\n                             &non_constructor);\n\n  // Add a stack check before pushing arguments.\n  Label stack_overflow;\n  __ StackOverflowCheck(x0, &stack_overflow);\n\n  // Enter a construct frame.\n  FrameScope scope(masm, StackFrame::MANUAL);\n  __ EnterFrame(StackFrame::FAST_CONSTRUCT);\n\n  if (v8_flags.debug_code) {\n    // Check that FrameScope pushed the context on to the stack already.\n    __ Peek(x2, 0);\n    __ Cmp(x2, cp);\n    __ Check(eq, AbortReason::kUnexpectedValue);\n  }\n\n  // Implicit receiver stored in the construct frame.\n  __ LoadRoot(x2, RootIndex::kTheHoleValue);\n  __ Push(x2, padreg);\n\n  // Push arguments + implicit receiver.\n  GenerateInterpreterPushArgs(masm, x0, x4, Register::no_reg(),\n                              ConvertReceiverMode::kNullOrUndefined,\n                              InterpreterPushArgsMode::kOther, true);\n  __ Poke(x2, 0 * kSystemPointerSize);\n\n  // Check if it is a builtin call.\n  Label builtin_call;\n  __ LoadTaggedField(\n      x2, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n  __ Ldr(w2, FieldMemOperand(x2, SharedFunctionInfo::kFlagsOffset));\n  __ TestAndBranchIfAnySet(w2, SharedFunctionInfo::ConstructAsBuiltinBit::kMask,\n                           &builtin_call);\n\n  // Check if we need to create an implicit receiver.\n  Label not_create_implicit_receiver;\n  __ DecodeField<SharedFunctionInfo::FunctionKindBits>(w2);\n  __ JumpIfIsInRange(\n      w2, static_cast<uint32_t>(FunctionKind::kDefaultDerivedConstructor),\n      static_cast<uint32_t>(FunctionKind::kDerivedConstructor),\n      &not_create_implicit_receiver);\n  NewImplicitReceiver(masm);\n  __ bind(&not_create_implicit_receiver);\n\n  // Call the function.\n  __ InvokeFunctionWithNewTarget(x1, x3, x0, InvokeType::kCall);\n\n  // ----------- S t a t e -------------\n  //  -- x0     constructor result\n  //\n  //  Stack:\n  //  -- Implicit Receiver\n  //  -- Context\n  //  -- FastConstructMarker\n  //  -- FramePointer\n  // -----------------------------------\n\n  // Store offset of return address for deoptimizer.\n  masm->isolate()->heap()->SetConstructStubInvokeDeoptPCOffset(\n      masm->pc_offset());\n\n  // If the result is an object (in the ECMA sense), we should get rid\n  // of the receiver and use the result; see ECMA-262 section 13.2.2-7\n  // on page 74.\n  Label use_receiver, do_throw, leave_and_return, check_receiver;\n\n  // If the result is undefined, we jump out to using the implicit receiver.\n  __ CompareRoot(x0, RootIndex::kUndefinedValue);\n  __ B(ne, &check_receiver);\n\n  // Throw away the result of the constructor invocation and use the\n  // on-stack receiver as the result.\n  __ Bind(&use_receiver);\n  __ Ldr(x0,\n         MemOperand(fp, FastConstructFrameConstants::kImplicitReceiverOffset));\n  __ CompareRoot(x0, RootIndex::kTheHoleValue);\n  __ B(eq, &do_throw);\n\n  __ Bind(&leave_and_return);\n  // Leave construct frame.\n  __ LeaveFrame(StackFrame::FAST_CONSTRUCT);\n  __ Ret();\n\n  // Otherwise we do a smi check and fall through to check if the return value\n  // is a valid receiver.\n  __ bind(&check_receiver);\n\n  // If the result is a smi, it is *not* an object in the ECMA sense.\n  __ JumpIfSmi(x0, &use_receiver);\n\n  // Check if the type of the result is not an object in the ECMA sense.\n  __ JumpIfJSAnyIsNotPrimitive(x0, x4, &leave_and_return);\n  __ B(&use_receiver);\n\n  __ bind(&builtin_call);\n  // TODO(victorgomes): Check the possibility to turn this into a tailcall.\n  __ InvokeFunctionWithNewTarget(x1, x3, x0, InvokeType::kCall);\n  __ LeaveFrame(StackFrame::FAST_CONSTRUCT);\n  __ Ret();\n\n  __ Bind(&do_throw);\n  // Restore the context from the frame.\n  __ Ldr(cp, MemOperand(fp, FastConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);\n  __ Unreachable();\n\n  __ Bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n  __ Unreachable();\n\n  // Called Construct on an Object that doesn't have a [[Construct]] internal\n  // method.\n  __ bind(&non_constructor);\n  __ TailCallBuiltin(Builtin::kConstructedNonConstructable);\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenFastConstructFunction(\n    MacroAssembler* masm) "}, {"name": "Builtins::Generate_InterpreterPushArgsThenFastConstructFunction", "content": "void Builtins::Generate_InterpreterPushArgsThenFastConstructFunction(\n    MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  // -- a0 : argument count\n  // -- a1 : constructor to call (checked to be a JSFunction)\n  // -- a3 : new target\n  // -- a4 : address of the first argument\n  // -- cp : context pointer\n  // -----------------------------------\n  __ AssertFunction(a1);\n\n  // Check if target has a [[Construct]] internal method.\n  Label non_constructor;\n  __ LoadMap(a2, a1);\n  __ Lbu(a2, FieldMemOperand(a2, Map::kBitFieldOffset));\n  __ And(a2, a2, Operand(Map::Bits1::IsConstructorBit::kMask));\n  __ Branch(&non_constructor, eq, a2, Operand(zero_reg));\n\n  // Add a stack check before pushing arguments.\n  Label stack_overflow;\n  __ StackOverflowCheck(a0, a2, a5, &stack_overflow);\n\n  // Enter a construct frame.\n  FrameScope scope(masm, StackFrame::MANUAL);\n  __ EnterFrame(StackFrame::FAST_CONSTRUCT);\n\n  // Implicit receiver stored in the construct frame.\n  __ LoadRoot(a2, RootIndex::kTheHoleValue);\n  __ Push(cp, a2);\n\n  // Push arguments + implicit receiver.\n  Register argc_without_receiver = a7;\n  __ SubWord(argc_without_receiver, a0, Operand(kJSArgcReceiverSlots));\n  GenerateInterpreterPushArgs(masm, argc_without_receiver, a4, a5);\n  __ Push(a2);\n\n  // Check if it is a builtin call.\n  Label builtin_call;\n  __ LoadTaggedField(\n      a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));\n  __ Load32U(a2, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));\n  __ And(a5, a2, Operand(SharedFunctionInfo::ConstructAsBuiltinBit::kMask));\n  __ Branch(&builtin_call, ne, a5, Operand(zero_reg));\n\n  // Check if we need to create an implicit receiver.\n  Label not_create_implicit_receiver;\n  __ DecodeField<SharedFunctionInfo::FunctionKindBits>(a2);\n  __ JumpIfIsInRange(\n      a2, static_cast<uint32_t>(FunctionKind::kDefaultDerivedConstructor),\n      static_cast<uint32_t>(FunctionKind::kDerivedConstructor),\n      &not_create_implicit_receiver);\n  NewImplicitReceiver(masm);\n  __ bind(&not_create_implicit_receiver);\n\n  // Call the function.\n  __ InvokeFunctionWithNewTarget(a1, a3, a0, InvokeType::kCall);\n\n  // ----------- S t a t e -------------\n  //  -- x0     constructor result\n  //\n  //  Stack:\n  //  -- Implicit Receiver\n  //  -- Context\n  //  -- FastConstructMarker\n  //  -- FramePointer\n  // -----------------------------------\n\n  // Store offset of return address for deoptimizer.\n  masm->isolate()->heap()->SetConstructStubInvokeDeoptPCOffset(\n      masm->pc_offset());\n\n  // If the result is an object (in the ECMA sense), we should get rid\n  // of the receiver and use the result; see ECMA-262 section 13.2.2-7\n  // on page 74.\n  Label use_receiver, do_throw, leave_and_return, check_receiver;\n\n  // If the result is undefined, we jump out to using the implicit receiver.\n  __ JumpIfNotRoot(a0, RootIndex::kUndefinedValue, &check_receiver);\n  // Throw away the result of the constructor invocation and use the\n  // on-stack receiver as the result.\n  __ bind(&use_receiver);\n  __ LoadWord(\n      a0, MemOperand(fp, FastConstructFrameConstants::kImplicitReceiverOffset));\n  __ JumpIfRoot(a0, RootIndex::kTheHoleValue, &do_throw);\n\n  __ bind(&leave_and_return);\n  // Leave construct frame.\n  __ LeaveFrame(StackFrame::FAST_CONSTRUCT);\n  __ Ret();\n\n  // Otherwise we do a smi check and fall through to check if the return value\n  // is a valid receiver.\n  __ bind(&check_receiver);\n\n  // If the result is a smi, it is *not* an object in the ECMA sense.\n  __ JumpIfSmi(a0, &use_receiver);\n\n  // Check if the type of the result is not an object in the ECMA sense.\n  __ JumpIfJSAnyIsNotPrimitive(a0, a4, &leave_and_return);\n  __ Branch(&use_receiver);\n\n  __ bind(&builtin_call);\n  // TODO(victorgomes): Check the possibility to turn this into a tailcall.\n  __ InvokeFunctionWithNewTarget(a1, a3, a0, InvokeType::kCall);\n  __ LeaveFrame(StackFrame::FAST_CONSTRUCT);\n  __ Ret();\n\n  __ bind(&do_throw);\n  // Restore the context from the frame.\n  __ LoadWord(cp, MemOperand(fp, FastConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);\n  // Unreachable code.\n  __ Trap();\n\n  __ bind(&stack_overflow);\n  __ TailCallRuntime(Runtime::kThrowStackOverflow);\n  // Unreachable code.\n  __ Trap();\n\n  // Called Construct on an Object that doesn't have a [[Construct]] internal\n  // method.\n  __ bind(&non_constructor);\n  __ TailCallBuiltin(Builtin::kConstructedNonConstructable);\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenFastConstructFunction(\n    MacroAssembler* masm) "}], [{"name": "NewImplicitReceiver", "content": "void NewImplicitReceiver(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  // -- x0 : the number of arguments\n  // -- x1 : constructor to call (checked to be a JSFunction)\n  // -- x3 : new target\n  //\n  //  Stack:\n  //  -- Implicit Receiver\n  //  -- [arguments without receiver]\n  //  -- Implicit Receiver\n  //  -- Context\n  //  -- FastConstructMarker\n  //  -- FramePointer\n  // -----------------------------------\n  Register implicit_receiver = x4;\n\n  // Save live registers.\n  __ SmiTag(x0);\n  __ Push(x0, x1, x3, padreg);\n  __ CallBuiltin(Builtin::kFastNewObject);\n  // Save result.\n  __ Mov(implicit_receiver, x0);\n  // Restore live registers.\n  __ Pop(padreg, x3, x1, x0);\n  __ SmiUntag(x0);\n\n  // Patch implicit receiver (in arguments)\n  __ Poke(implicit_receiver, 0 * kSystemPointerSize);\n  // Patch second implicit (in construct frame)\n  __ Str(implicit_receiver,\n         MemOperand(fp, FastConstructFrameConstants::kImplicitReceiverOffset));\n\n  // Restore context.\n  __ Ldr(cp, MemOperand(fp, FastConstructFrameConstants::kContextOffset));\n}", "name_and_para": "void NewImplicitReceiver(MacroAssembler* masm) "}, {"name": "NewImplicitReceiver", "content": "void NewImplicitReceiver(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  // -- a0 : the number of arguments\n  // -- a1 : constructor to call (checked to be a JSFunction)\n  // -- a3 : new target\n  //\n  //  Stack:\n  //  -- Implicit Receiver\n  //  -- [arguments without receiver]\n  //  -- Implicit Receiver\n  //  -- Context\n  //  -- FastConstructMarker\n  //  -- FramePointer\n  // -----------------------------------\n  Register implicit_receiver = a4;\n\n  // Save live registers.\n  __ SmiTag(a0);\n  __ Push(a0, a1, a3);\n  __ CallBuiltin(Builtin::kFastNewObject);\n  // Save result.\n  __ mv(implicit_receiver, a0);\n  __ Pop(a0, a1, a3);\n  __ SmiUntag(a0);\n\n  // Patch implicit receiver (in arguments)\n  __ StoreReceiver(implicit_receiver);\n  // Patch second implicit (in construct frame)\n  __ StoreWord(\n      implicit_receiver,\n      MemOperand(fp, FastConstructFrameConstants::kImplicitReceiverOffset));\n\n  // Restore context.\n  __ LoadWord(cp, MemOperand(fp, FastConstructFrameConstants::kContextOffset));\n}", "name_and_para": "void NewImplicitReceiver(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_ConstructForwardAllArgsImpl", "content": "void Builtins::Generate_ConstructForwardAllArgsImpl(\n    MacroAssembler* masm, ForwardWhichFrame which_frame) {\n  // ----------- S t a t e -------------\n  // -- x3 : new target\n  // -- x1 : constructor to call\n  // -----------------------------------\n  Label stack_overflow;\n\n  // Load the frame pointer into x4.\n  switch (which_frame) {\n    case ForwardWhichFrame::kCurrentFrame:\n      __ Move(x4, fp);\n      break;\n    case ForwardWhichFrame::kParentFrame:\n      __ Ldr(x4, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));\n      break;\n  }\n\n  // Load the argument count into x0.\n  __ Ldr(x0, MemOperand(x4, StandardFrameConstants::kArgCOffset));\n\n  // Point x4 to the base of the argument list to forward, excluding the\n  // receiver.\n  __ Add(x4, x4,\n         Operand((StandardFrameConstants::kFixedSlotCountAboveFp + 1) *\n                 kSystemPointerSize));\n\n  Register stack_addr = x11;\n  Register slots_to_claim = x12;\n  Register argc_without_receiver = x13;\n\n  // Round up to even number of slots.\n  __ Add(slots_to_claim, x0, 1);\n  __ Bic(slots_to_claim, slots_to_claim, 1);\n\n  __ StackOverflowCheck(slots_to_claim, &stack_overflow);\n\n  // Adjust the stack pointer.\n  __ Claim(slots_to_claim);\n  {\n    // Store padding, which may be overwritten.\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Sub(scratch, slots_to_claim, 1);\n    __ Poke(padreg, Operand(scratch, LSL, kSystemPointerSizeLog2));\n  }\n\n  // Copy the arguments.\n  __ Sub(argc_without_receiver, x0, kJSArgcReceiverSlots);\n  __ SlotAddress(stack_addr, 1);\n  __ CopyDoubleWords(stack_addr, x4, argc_without_receiver);\n\n  // Push a slot for the receiver to be constructed.\n  __ Mov(x14, Operand(0));\n  __ Poke(x14, 0);\n\n  // Call the constructor with x0, x1, and x3 unmodified.\n  __ TailCallBuiltin(Builtin::kConstruct);\n\n  __ Bind(&stack_overflow);\n  {\n    __ TailCallRuntime(Runtime::kThrowStackOverflow);\n    __ Unreachable();\n  }\n}", "name_and_para": "void Builtins::Generate_ConstructForwardAllArgsImpl(\n    MacroAssembler* masm, ForwardWhichFrame which_frame) "}, {"name": "Builtins::Generate_ConstructForwardAllArgsImpl", "content": "void Builtins::Generate_ConstructForwardAllArgsImpl(\n    MacroAssembler* masm, ForwardWhichFrame which_frame) {\n  // ----------- S t a t e -------------\n  // -- a3 : new target\n  // -- a1 : constructor to call\n  // -----------------------------------\n  Label stack_overflow;\n\n  // Load the frame pointer into a4.\n  switch (which_frame) {\n    case ForwardWhichFrame::kCurrentFrame:\n      __ Move(a4, fp);\n      break;\n    case ForwardWhichFrame::kParentFrame:\n      __ LoadWord(a4, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));\n      break;\n  }\n\n  // Load the argument count into a0.\n  __ LoadWord(a0, MemOperand(a4, StandardFrameConstants::kArgCOffset));\n  __ StackOverflowCheck(a0, a5, t0, &stack_overflow);\n\n  // Point a4 to the base of the argument list to forward, excluding the\n  // receiver.\n  __ AddWord(a4, a4,\n             Operand((StandardFrameConstants::kFixedSlotCountAboveFp + 1) *\n                     kSystemPointerSize));\n\n  // Copy arguments on the stack. a5 is a scratch register.\n  Register argc_without_receiver = a6;\n  __ SubWord(argc_without_receiver, a0, Operand(kJSArgcReceiverSlots));\n  __ PushArray(a4, argc_without_receiver);\n\n  // Push a slot for the receiver.\n  __ push(zero_reg);\n\n  // Call the constructor with a0, a1, and a3 unmodified.\n  __ TailCallBuiltin(Builtin::kConstruct);\n\n  __ bind(&stack_overflow);\n  {\n    __ TailCallRuntime(Runtime::kThrowStackOverflow);\n    // Unreachable code.\n    __ break_(0xCC);\n  }\n}", "name_and_para": "void Builtins::Generate_ConstructForwardAllArgsImpl(\n    MacroAssembler* masm, ForwardWhichFrame which_frame) "}], [{"name": "Builtins::Generate_InterpreterPushArgsThenConstructImpl", "content": "void Builtins::Generate_InterpreterPushArgsThenConstructImpl(\n    MacroAssembler* masm, InterpreterPushArgsMode mode) {\n  // ----------- S t a t e -------------\n  // -- x0 : argument count\n  // -- x3 : new target\n  // -- x1 : constructor to call\n  // -- x2 : allocation site feedback if available, undefined otherwise\n  // -- x4 : address of the first argument\n  // -----------------------------------\n  __ AssertUndefinedOrAllocationSite(x2);\n\n  // Push the arguments. num_args may be updated according to mode.\n  // spread_arg_out will be updated to contain the last spread argument, when\n  // mode == InterpreterPushArgsMode::kWithFinalSpread.\n  Register num_args = x0;\n  Register first_arg_index = x4;\n  Register spread_arg_out =\n      (mode == InterpreterPushArgsMode::kWithFinalSpread) ? x2 : no_reg;\n  GenerateInterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,\n                              ConvertReceiverMode::kNullOrUndefined, mode);\n\n  if (mode == InterpreterPushArgsMode::kArrayFunction) {\n    __ AssertFunction(x1);\n\n    // Tail call to the array construct stub (still in the caller\n    // context at this point).\n    __ TailCallBuiltin(Builtin::kArrayConstructorImpl);\n  } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // Call the constructor with x0, x1, and x3 unmodified.\n    __ TailCallBuiltin(Builtin::kConstructWithSpread);\n  } else {\n    DCHECK_EQ(InterpreterPushArgsMode::kOther, mode);\n    // Call the constructor with x0, x1, and x3 unmodified.\n    __ TailCallBuiltin(Builtin::kConstruct);\n  }\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenConstructImpl(\n    MacroAssembler* masm, InterpreterPushArgsMode mode) "}, {"name": "Builtins::Generate_InterpreterPushArgsThenConstructImpl", "content": "void Builtins::Generate_InterpreterPushArgsThenConstructImpl(\n    MacroAssembler* masm, InterpreterPushArgsMode mode) {\n  // ----------- S t a t e -------------\n  // -- a0 : argument count\n  // -- a3 : new target\n  // -- a1 : constructor to call\n  // -- a2 : allocation site feedback if available, undefined otherwise.\n  // -- a4 : address of the first argument\n  // -----------------------------------\n  Label stack_overflow;\n  __ StackOverflowCheck(a0, a5, t0, &stack_overflow);\n\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // The spread argument should not be pushed.\n    __ SubWord(a0, a0, Operand(1));\n  }\n  Register argc_without_receiver = a6;\n  __ SubWord(argc_without_receiver, a0, Operand(kJSArgcReceiverSlots));\n  // Push the arguments, This function modifies a4 and a5.\n  GenerateInterpreterPushArgs(masm, argc_without_receiver, a4, a5);\n\n  // Push a slot for the receiver.\n  __ push(zero_reg);\n\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // Pass the spread in the register a2.\n    // a4 already points to the penultimate argument, the spread\n    // lies in the next interpreter register.\n    __ LoadWord(a2, MemOperand(a4, -kSystemPointerSize));\n  } else {\n    __ AssertUndefinedOrAllocationSite(a2, t0);\n  }\n\n  if (mode == InterpreterPushArgsMode::kArrayFunction) {\n    __ AssertFunction(a1);\n\n    // Tail call to the function-specific construct stub (still in the caller\n    // context at this point).\n    __ TailCallBuiltin(Builtin::kArrayConstructorImpl);\n  } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // Call the constructor with a0, a1, and a3 unmodified.\n    __ TailCallBuiltin(Builtin::kConstructWithSpread);\n  } else {\n    DCHECK_EQ(InterpreterPushArgsMode::kOther, mode);\n    // Call the constructor with a0, a1, and a3 unmodified.\n    __ TailCallBuiltin(Builtin::kConstruct);\n  }\n\n  __ bind(&stack_overflow);\n  {\n    __ TailCallRuntime(Runtime::kThrowStackOverflow);\n    // Unreachable code.\n    __ break_(0xCC);\n  }\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenConstructImpl(\n    MacroAssembler* masm, InterpreterPushArgsMode mode) "}], [{"name": "Builtins::Generate_InterpreterPushArgsThenCallImpl", "content": "void Builtins::Generate_InterpreterPushArgsThenCallImpl(\n    MacroAssembler* masm, ConvertReceiverMode receiver_mode,\n    InterpreterPushArgsMode mode) {\n  DCHECK(mode != InterpreterPushArgsMode::kArrayFunction);\n  // ----------- S t a t e -------------\n  //  -- x0 : the number of arguments\n  //  -- x2 : the address of the first argument to be pushed. Subsequent\n  //          arguments should be consecutive above this, in the same order as\n  //          they are to be pushed onto the stack.\n  //  -- x1 : the target to call (can be any Object).\n  // -----------------------------------\n\n  // Push the arguments. num_args may be updated according to mode.\n  // spread_arg_out will be updated to contain the last spread argument, when\n  // mode == InterpreterPushArgsMode::kWithFinalSpread.\n  Register num_args = x0;\n  Register first_arg_index = x2;\n  Register spread_arg_out =\n      (mode == InterpreterPushArgsMode::kWithFinalSpread) ? x2 : no_reg;\n  GenerateInterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,\n                              receiver_mode, mode);\n\n  // Call the target.\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    __ TailCallBuiltin(Builtin::kCallWithSpread);\n  } else {\n    __ TailCallBuiltin(Builtins::Call(receiver_mode));\n  }\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenCallImpl(\n    MacroAssembler* masm, ConvertReceiverMode receiver_mode,\n    InterpreterPushArgsMode mode) "}, {"name": "Builtins::Generate_InterpreterPushArgsThenCallImpl", "content": "void Builtins::Generate_InterpreterPushArgsThenCallImpl(\n    MacroAssembler* masm, ConvertReceiverMode receiver_mode,\n    InterpreterPushArgsMode mode) {\n  DCHECK(mode != InterpreterPushArgsMode::kArrayFunction);\n  // ----------- S t a t e -------------\n  //  -- a0 : the number of arguments\n  //  -- a2 : the address of the first argument to be pushed. Subsequent\n  //          arguments should be consecutive above this, in the same order as\n  //          they are to be pushed onto the stack.\n  //  -- a1 : the target to call (can be any Object).\n  // -----------------------------------\n  Label stack_overflow;\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // The spread argument should not be pushed.\n    __ SubWord(a0, a0, Operand(1));\n  }\n\n  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {\n    __ SubWord(a3, a0, Operand(kJSArgcReceiverSlots));\n  } else {\n    __ Move(a3, a0);\n  }\n  __ StackOverflowCheck(a3, a4, t0, &stack_overflow);\n\n  // This function modifies a2 and a4.\n  GenerateInterpreterPushArgs(masm, a3, a2, a4);\n  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {\n    __ PushRoot(RootIndex::kUndefinedValue);\n  }\n\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // Pass the spread in the register a2.\n    // a2 already points to the penultime argument, the spread\n    // is below that.\n    __ LoadWord(a2, MemOperand(a2, -kSystemPointerSize));\n  }\n\n  // Call the target.\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    __ TailCallBuiltin(Builtin::kCallWithSpread);\n  } else {\n    __ TailCallBuiltin(Builtins::Call(receiver_mode));\n  }\n\n  __ bind(&stack_overflow);\n  {\n    __ TailCallRuntime(Runtime::kThrowStackOverflow);\n    // Unreachable code.\n    __ break_(0xCC);\n  }\n}", "name_and_para": "void Builtins::Generate_InterpreterPushArgsThenCallImpl(\n    MacroAssembler* masm, ConvertReceiverMode receiver_mode,\n    InterpreterPushArgsMode mode) "}], [{"name": "GenerateInterpreterPushArgs", "content": "static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,\n                                        Register first_arg_index,\n                                        Register spread_arg_out,\n                                        ConvertReceiverMode receiver_mode,\n                                        InterpreterPushArgsMode mode,\n                                        bool stack_check_already_done = false) {\n  ASM_CODE_COMMENT(masm);\n  Register last_arg_addr = x10;\n  Register stack_addr = x11;\n  Register slots_to_claim = x12;\n  Register slots_to_copy = x13;\n\n  DCHECK(!AreAliased(num_args, first_arg_index, last_arg_addr, stack_addr,\n                     slots_to_claim, slots_to_copy));\n  // spread_arg_out may alias with the first_arg_index input.\n  DCHECK(!AreAliased(spread_arg_out, last_arg_addr, stack_addr, slots_to_claim,\n                     slots_to_copy));\n\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    // Exclude final spread from slots to claim and the number of arguments.\n    __ Sub(num_args, num_args, 1);\n  }\n\n  // Round up to an even number of slots.\n  __ Add(slots_to_claim, num_args, 1);\n  __ Bic(slots_to_claim, slots_to_claim, 1);\n\n  if (!stack_check_already_done) {\n    // Add a stack check before pushing arguments.\n    Label stack_overflow, done;\n    __ StackOverflowCheck(slots_to_claim, &stack_overflow);\n    __ B(&done);\n    __ Bind(&stack_overflow);\n    __ TailCallRuntime(Runtime::kThrowStackOverflow);\n    __ Unreachable();\n    __ Bind(&done);\n  }\n\n  __ Claim(slots_to_claim);\n  {\n    // Store padding, which may be overwritten.\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Sub(scratch, slots_to_claim, 1);\n    __ Poke(padreg, Operand(scratch, LSL, kSystemPointerSizeLog2));\n  }\n\n  const bool skip_receiver =\n      receiver_mode == ConvertReceiverMode::kNullOrUndefined;\n  if (skip_receiver) {\n    __ Sub(slots_to_copy, num_args, kJSArgcReceiverSlots);\n  } else {\n    __ Mov(slots_to_copy, num_args);\n  }\n  __ SlotAddress(stack_addr, skip_receiver ? 1 : 0);\n\n  __ Sub(last_arg_addr, first_arg_index,\n         Operand(slots_to_copy, LSL, kSystemPointerSizeLog2));\n  __ Add(last_arg_addr, last_arg_addr, kSystemPointerSize);\n\n  // Load the final spread argument into spread_arg_out, if necessary.\n  if (mode == InterpreterPushArgsMode::kWithFinalSpread) {\n    __ Ldr(spread_arg_out, MemOperand(last_arg_addr, -kSystemPointerSize));\n  }\n\n  __ CopyDoubleWords(stack_addr, last_arg_addr, slots_to_copy,\n                     MacroAssembler::kDstLessThanSrcAndReverse);\n\n  if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {\n    // Store \"undefined\" as the receiver arg if we need to.\n    Register receiver = x14;\n    __ LoadRoot(receiver, RootIndex::kUndefinedValue);\n    __ Poke(receiver, 0);\n  }\n}", "name_and_para": "static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,\n                                        Register first_arg_index,\n                                        Register spread_arg_out,\n                                        ConvertReceiverMode receiver_mode,\n                                        InterpreterPushArgsMode mode,\n                                        bool stack_check_already_done = false) "}, {"name": "GenerateInterpreterPushArgs", "content": "static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,\n                                        Register start_address,\n                                        Register scratch) {\n  ASM_CODE_COMMENT(masm);\n  // Find the address of the last argument.\n  __ SubWord(scratch, num_args, Operand(1));\n  __ SllWord(scratch, scratch, kSystemPointerSizeLog2);\n  __ SubWord(start_address, start_address, scratch);\n\n  // Push the arguments.\n  __ PushArray(start_address, num_args,\n               MacroAssembler::PushArrayOrder::kReverse);\n}", "name_and_para": "static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,\n                                        Register start_address,\n                                        Register scratch) "}], [{"name": "Builtins::Generate_InterpreterEntryTrampoline", "content": "void Builtins::Generate_InterpreterEntryTrampoline(\n    MacroAssembler* masm, InterpreterEntryTrampolineMode mode) {\n  Register closure = x1;\n\n  // Get the bytecode array from the function object and load it into\n  // kInterpreterBytecodeArrayRegister.\n  Register sfi = x4;\n  __ LoadTaggedField(\n      sfi, FieldMemOperand(closure, JSFunction::kSharedFunctionInfoOffset));\n  ResetSharedFunctionInfoAge(masm, sfi);\n\n  // The bytecode array could have been flushed from the shared function info,\n  // if so, call into CompileLazy.\n  Label is_baseline, compile_lazy;\n  GetSharedFunctionInfoBytecodeOrBaseline(masm, sfi,\n                                          kInterpreterBytecodeArrayRegister,\n                                          x11, &is_baseline, &compile_lazy);\n\n  Label push_stack_frame;\n  Register feedback_vector = x2;\n  __ LoadFeedbackVector(feedback_vector, closure, x7, &push_stack_frame);\n\n#ifndef V8_JITLESS\n  // If feedback vector is valid, check for optimized code and update invocation\n  // count.\n  Label flags_need_processing;\n  Register flags = w7;\n  __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      flags, feedback_vector, CodeKind::INTERPRETED_FUNCTION,\n      &flags_need_processing);\n\n  ResetFeedbackVectorOsrUrgency(masm, feedback_vector, w7);\n\n  // Increment invocation count for the function.\n  __ Ldr(w10, FieldMemOperand(feedback_vector,\n                              FeedbackVector::kInvocationCountOffset));\n  __ Add(w10, w10, Operand(1));\n  __ Str(w10, FieldMemOperand(feedback_vector,\n                              FeedbackVector::kInvocationCountOffset));\n\n  // Open a frame scope to indicate that there is a frame on the stack.  The\n  // MANUAL indicates that the scope shouldn't actually generate code to set up\n  // the frame (that is done below).\n#else\n  // Note: By omitting the above code in jitless mode we also disable:\n  // - kFlagsLogNextExecution: only used for logging/profiling; and\n  // - kInvocationCountOffset: only used for tiering heuristics and code\n  //   coverage.\n#endif  // !V8_JITLESS\n\n  __ Bind(&push_stack_frame);\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  __ Push<MacroAssembler::kSignLR>(lr, fp);\n  __ mov(fp, sp);\n  __ Push(cp, closure);\n\n  // Load the initial bytecode offset.\n  __ Mov(kInterpreterBytecodeOffsetRegister,\n         Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n\n  // Push actual argument count, bytecode array, Smi tagged bytecode array\n  // offset and the feedback vector.\n  __ SmiTag(x6, kInterpreterBytecodeOffsetRegister);\n  __ Push(kJavaScriptCallArgCountRegister, kInterpreterBytecodeArrayRegister);\n  __ Push(x6, feedback_vector);\n\n  // Allocate the local and temporary register file on the stack.\n  Label stack_overflow;\n  {\n    // Load frame size from the BytecodeArray object.\n    __ Ldr(w11, FieldMemOperand(kInterpreterBytecodeArrayRegister,\n                                BytecodeArray::kFrameSizeOffset));\n\n    // Do a stack check to ensure we don't go over the limit.\n    __ Sub(x10, sp, Operand(x11));\n    {\n      UseScratchRegisterScope temps(masm);\n      Register scratch = temps.AcquireX();\n      __ LoadStackLimit(scratch, StackLimitKind::kRealStackLimit);\n      __ Cmp(x10, scratch);\n    }\n    __ B(lo, &stack_overflow);\n\n    // If ok, push undefined as the initial value for all register file entries.\n    // Note: there should always be at least one stack slot for the return\n    // register in the register file.\n    Label loop_header;\n    __ Lsr(x11, x11, kSystemPointerSizeLog2);\n    // Round up the number of registers to a multiple of 2, to align the stack\n    // to 16 bytes.\n    __ Add(x11, x11, 1);\n    __ Bic(x11, x11, 1);\n    __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n    __ PushMultipleTimes(kInterpreterAccumulatorRegister, x11);\n    __ Bind(&loop_header);\n  }\n\n  // If the bytecode array has a valid incoming new target or generator object\n  // register, initialize it with incoming value which was passed in x3.\n  Label no_incoming_new_target_or_generator_register;\n  __ Ldrsw(x10,\n           FieldMemOperand(\n               kInterpreterBytecodeArrayRegister,\n               BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));\n  __ Cbz(x10, &no_incoming_new_target_or_generator_register);\n  __ Str(x3, MemOperand(fp, x10, LSL, kSystemPointerSizeLog2));\n  __ Bind(&no_incoming_new_target_or_generator_register);\n\n  // Perform interrupt stack check.\n  // TODO(solanes): Merge with the real stack limit check above.\n  Label stack_check_interrupt, after_stack_check_interrupt;\n  __ LoadStackLimit(x10, StackLimitKind::kInterruptStackLimit);\n  __ Cmp(sp, x10);\n  __ B(lo, &stack_check_interrupt);\n  __ Bind(&after_stack_check_interrupt);\n\n  // The accumulator is already loaded with undefined.\n\n  // Load the dispatch table into a register and dispatch to the bytecode\n  // handler at the current bytecode offset.\n  Label do_dispatch;\n  __ bind(&do_dispatch);\n  __ Mov(\n      kInterpreterDispatchTableRegister,\n      ExternalReference::interpreter_dispatch_table_address(masm->isolate()));\n  __ Ldrb(x23, MemOperand(kInterpreterBytecodeArrayRegister,\n                          kInterpreterBytecodeOffsetRegister));\n  __ Mov(x1, Operand(x23, LSL, kSystemPointerSizeLog2));\n  __ Ldr(kJavaScriptCallCodeStartRegister,\n         MemOperand(kInterpreterDispatchTableRegister, x1));\n  __ Call(kJavaScriptCallCodeStartRegister);\n\n  __ RecordComment(\"--- InterpreterEntryReturnPC point ---\");\n  if (mode == InterpreterEntryTrampolineMode::kDefault) {\n    masm->isolate()->heap()->SetInterpreterEntryReturnPCOffset(\n        masm->pc_offset());\n  } else {\n    DCHECK_EQ(mode, InterpreterEntryTrampolineMode::kForProfiling);\n    // Both versions must be the same up to this point otherwise the builtins\n    // will not be interchangable.\n    CHECK_EQ(\n        masm->isolate()->heap()->interpreter_entry_return_pc_offset().value(),\n        masm->pc_offset());\n  }\n\n  // Any returns to the entry trampoline are either due to the return bytecode\n  // or the interpreter tail calling a builtin and then a dispatch.\n\n  __ JumpTarget();\n\n  // Get bytecode array and bytecode offset from the stack frame.\n  __ Ldr(kInterpreterBytecodeArrayRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  // Either return, or advance to the next bytecode and dispatch.\n  Label do_return;\n  __ Ldrb(x1, MemOperand(kInterpreterBytecodeArrayRegister,\n                         kInterpreterBytecodeOffsetRegister));\n  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,\n                                kInterpreterBytecodeOffsetRegister, x1, x2, x3,\n                                &do_return);\n  __ B(&do_dispatch);\n\n  __ bind(&do_return);\n  // The return value is in x0.\n  LeaveInterpreterFrame(masm, x2, x4);\n  __ Ret();\n\n  __ bind(&stack_check_interrupt);\n  // Modify the bytecode offset in the stack to be kFunctionEntryBytecodeOffset\n  // for the call to the StackGuard.\n  __ Mov(kInterpreterBytecodeOffsetRegister,\n         Operand(Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                              kFunctionEntryBytecodeOffset)));\n  __ Str(kInterpreterBytecodeOffsetRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  __ CallRuntime(Runtime::kStackGuard);\n\n  // After the call, restore the bytecode array, bytecode offset and accumulator\n  // registers again. Also, restore the bytecode offset in the stack to its\n  // previous value.\n  __ Ldr(kInterpreterBytecodeArrayRegister,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ Mov(kInterpreterBytecodeOffsetRegister,\n         Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n\n  __ SmiTag(x10, kInterpreterBytecodeOffsetRegister);\n  __ Str(x10, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  __ jmp(&after_stack_check_interrupt);\n\n#ifndef V8_JITLESS\n  __ bind(&flags_need_processing);\n  __ OptimizeCodeOrTailCallOptimizedCodeSlot(flags, feedback_vector);\n\n  __ bind(&is_baseline);\n  {\n    // Load the feedback vector from the closure.\n    __ LoadTaggedField(\n        feedback_vector,\n        FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n    __ LoadTaggedField(\n        feedback_vector,\n        FieldMemOperand(feedback_vector, FeedbackCell::kValueOffset));\n\n    Label install_baseline_code;\n    // Check if feedback vector is valid. If not, call prepare for baseline to\n    // allocate it.\n    __ LoadTaggedField(\n        x7, FieldMemOperand(feedback_vector, HeapObject::kMapOffset));\n    __ Ldrh(x7, FieldMemOperand(x7, Map::kInstanceTypeOffset));\n    __ Cmp(x7, FEEDBACK_VECTOR_TYPE);\n    __ B(ne, &install_baseline_code);\n\n    // Check the tiering state.\n    __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n        flags, feedback_vector, CodeKind::BASELINE, &flags_need_processing);\n\n    // Load the baseline code into the closure.\n    __ Move(x2, kInterpreterBytecodeArrayRegister);\n    static_assert(kJavaScriptCallCodeStartRegister == x2, \"ABI mismatch\");\n    __ ReplaceClosureCodeWithOptimizedCode(x2, closure);\n    __ JumpCodeObject(x2, kJSEntrypointTag);\n\n    __ bind(&install_baseline_code);\n    __ GenerateTailCallToReturnedCode(Runtime::kInstallBaselineCode);\n  }\n#endif  // !V8_JITLESS\n\n  __ bind(&compile_lazy);\n  __ GenerateTailCallToReturnedCode(Runtime::kCompileLazy);\n  __ Unreachable();  // Should not return.\n\n  __ bind(&stack_overflow);\n  __ CallRuntime(Runtime::kThrowStackOverflow);\n  __ Unreachable();  // Should not return.\n}", "name_and_para": "void Builtins::Generate_InterpreterEntryTrampoline(\n    MacroAssembler* masm, InterpreterEntryTrampolineMode mode) "}, {"name": "Builtins::Generate_InterpreterEntryTrampoline", "content": "void Builtins::Generate_InterpreterEntryTrampoline(\n    MacroAssembler* masm, InterpreterEntryTrampolineMode mode) {\n  Register closure = a1;\n  // Get the bytecode array from the function object and load it into\n  // kInterpreterBytecodeArrayRegister.\n  Register sfi = a4;\n  __ LoadTaggedField(\n      sfi, FieldMemOperand(closure, JSFunction::kSharedFunctionInfoOffset));\n  ResetSharedFunctionInfoAge(masm, sfi);\n\n  Label is_baseline;\n  GetSharedFunctionInfoBytecodeOrBaseline(\n      masm, sfi, kInterpreterBytecodeArrayRegister, kScratchReg, &is_baseline);\n\n  // The bytecode array could have been flushed from the shared function info,\n  // if so, call into CompileLazy.\n  Label compile_lazy;\n  __ GetObjectType(kInterpreterBytecodeArrayRegister, kScratchReg, kScratchReg);\n  __ Branch(&compile_lazy, ne, kScratchReg, Operand(BYTECODE_ARRAY_TYPE));\n\n  Label push_stack_frame;\n  Register feedback_vector = a2;\n  __ LoadFeedbackVector(feedback_vector, closure, a4, &push_stack_frame);\n\n#ifndef V8_JITLESS\n  // If feedback vector is valid, check for optimized code and update invocation\n  // count.\n\n  // Check the tiering state.\n  Label flags_need_processing;\n  Register flags = a4;\n  __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      flags, feedback_vector, CodeKind::INTERPRETED_FUNCTION,\n      &flags_need_processing);\n  ResetFeedbackVectorOsrUrgency(masm, feedback_vector, a4);\n\n  // Increment invocation count for the function.\n  __ Lw(a4, FieldMemOperand(feedback_vector,\n                            FeedbackVector::kInvocationCountOffset));\n  __ Add32(a4, a4, Operand(1));\n  __ Sw(a4, FieldMemOperand(feedback_vector,\n                            FeedbackVector::kInvocationCountOffset));\n\n  // Open a frame scope to indicate that there is a frame on the stack.  The\n  // MANUAL indicates that the scope shouldn't actually generate code to set up\n  // the frame (that is done below).\n#else\n  // Note: By omitting the above code in jitless mode we also disable:\n  // - kFlagsLogNextExecution: only used for logging/profiling; and\n  // - kInvocationCountOffset: only used for tiering heuristics and code\n  //   coverage.\n#endif  // !V8_JITLESS\n\n  __ bind(&push_stack_frame);\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  __ PushStandardFrame(closure);\n\n  // Load initial bytecode offset.\n  __ li(kInterpreterBytecodeOffsetRegister,\n        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n\n  // Push bytecode array, Smi tagged bytecode array offset, and the feedback\n  // vector.\n  __ SmiTag(a4, kInterpreterBytecodeOffsetRegister);\n  __ Push(kInterpreterBytecodeArrayRegister, a4, feedback_vector);\n\n  // Allocate the local and temporary register file on the stack.\n  Label stack_overflow;\n  {\n    // Load frame size (word) from the BytecodeArray object.\n    __ Lw(a4, FieldMemOperand(kInterpreterBytecodeArrayRegister,\n                              BytecodeArray::kFrameSizeOffset));\n\n    // Do a stack check to ensure we don't go over the limit.\n    __ SubWord(a5, sp, Operand(a4));\n    __ LoadStackLimit(a2, MacroAssembler::StackLimitKind::kRealStackLimit);\n    __ Branch(&stack_overflow, Uless, a5, Operand(a2));\n\n    // If ok, push undefined as the initial value for all register file entries.\n    Label loop_header;\n    Label loop_check;\n    __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n    __ BranchShort(&loop_check);\n    __ bind(&loop_header);\n    // TODO(rmcilroy): Consider doing more than one push per loop iteration.\n    __ push(kInterpreterAccumulatorRegister);\n    // Continue loop if not done.\n    __ bind(&loop_check);\n    __ SubWord(a4, a4, Operand(kSystemPointerSize));\n    __ Branch(&loop_header, ge, a4, Operand(zero_reg));\n  }\n\n  // If the bytecode array has a valid incoming new target or generator object\n  // register, initialize it with incoming value which was passed in a3.\n  Label no_incoming_new_target_or_generator_register;\n  __ Lw(a5, FieldMemOperand(\n                kInterpreterBytecodeArrayRegister,\n                BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));\n  __ Branch(&no_incoming_new_target_or_generator_register, eq, a5,\n            Operand(zero_reg), Label::Distance::kNear);\n  __ CalcScaledAddress(a5, fp, a5, kSystemPointerSizeLog2);\n  __ StoreWord(a3, MemOperand(a5));\n  __ bind(&no_incoming_new_target_or_generator_register);\n\n  // Perform interrupt stack check.\n  // TODO(solanes): Merge with the real stack limit check above.\n  Label stack_check_interrupt, after_stack_check_interrupt;\n  __ LoadStackLimit(a5, MacroAssembler::StackLimitKind::kInterruptStackLimit);\n  __ Branch(&stack_check_interrupt, Uless, sp, Operand(a5),\n            Label::Distance::kNear);\n  __ bind(&after_stack_check_interrupt);\n\n  // Load the dispatch table into a register and dispatch to the bytecode\n  // handler at the current bytecode offset.\n  Label do_dispatch;\n  __ bind(&do_dispatch);\n  __ li(kInterpreterDispatchTableRegister,\n        ExternalReference::interpreter_dispatch_table_address(masm->isolate()));\n  __ AddWord(a1, kInterpreterBytecodeArrayRegister,\n             kInterpreterBytecodeOffsetRegister);\n  __ Lbu(a7, MemOperand(a1));\n  __ CalcScaledAddress(kScratchReg, kInterpreterDispatchTableRegister, a7,\n                       kSystemPointerSizeLog2);\n  __ LoadWord(kJavaScriptCallCodeStartRegister, MemOperand(kScratchReg));\n  __ Call(kJavaScriptCallCodeStartRegister);\n\n  __ RecordComment(\"--- InterpreterEntryReturnPC point ---\");\n  if (mode == InterpreterEntryTrampolineMode::kDefault) {\n    masm->isolate()->heap()->SetInterpreterEntryReturnPCOffset(\n        masm->pc_offset());\n  } else {\n    DCHECK_EQ(mode, InterpreterEntryTrampolineMode::kForProfiling);\n    // Both versions must be the same up to this point otherwise the builtins\n    // will not be interchangable.\n    CHECK_EQ(\n        masm->isolate()->heap()->interpreter_entry_return_pc_offset().value(),\n        masm->pc_offset());\n  }\n\n  // Any returns to the entry trampoline are either due to the return bytecode\n  // or the interpreter tail calling a builtin and then a dispatch.\n\n  // Get bytecode array and bytecode offset from the stack frame.\n  __ LoadWord(kInterpreterBytecodeArrayRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ LoadWord(kInterpreterBytecodeOffsetRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  __ SmiUntag(kInterpreterBytecodeOffsetRegister);\n\n  // Either return, or advance to the next bytecode and dispatch.\n  Label do_return;\n  __ AddWord(a1, kInterpreterBytecodeArrayRegister,\n             kInterpreterBytecodeOffsetRegister);\n  __ Lbu(a1, MemOperand(a1));\n  AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,\n                                kInterpreterBytecodeOffsetRegister, a1, a2, a3,\n                                a4, &do_return);\n  __ Branch(&do_dispatch);\n\n  __ bind(&do_return);\n  // The return value is in a0.\n  LeaveInterpreterFrame(masm, t0, t1);\n  __ Jump(ra);\n\n  __ bind(&stack_check_interrupt);\n  // Modify the bytecode offset in the stack to be kFunctionEntryBytecodeOffset\n  // for the call to the StackGuard.\n  __ li(kInterpreterBytecodeOffsetRegister,\n        Operand(Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +\n                             kFunctionEntryBytecodeOffset)));\n  __ StoreWord(\n      kInterpreterBytecodeOffsetRegister,\n      MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n  __ CallRuntime(Runtime::kStackGuard);\n\n  // After the call, restore the bytecode array, bytecode offset and accumulator\n  // registers again. Also, restore the bytecode offset in the stack to its\n  // previous value.\n  __ LoadWord(kInterpreterBytecodeArrayRegister,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ li(kInterpreterBytecodeOffsetRegister,\n        Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));\n  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n\n  __ SmiTag(a5, kInterpreterBytecodeOffsetRegister);\n  __ StoreWord(\n      a5, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));\n\n  __ Branch(&after_stack_check_interrupt);\n\n#ifndef V8_JITLESS\n  __ bind(&flags_need_processing);\n  __ OptimizeCodeOrTailCallOptimizedCodeSlot(flags, feedback_vector);\n  __ bind(&is_baseline);\n  {\n    // Load the feedback vector from the closure.\n    __ LoadTaggedField(\n        feedback_vector,\n        FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n    __ LoadTaggedField(\n        feedback_vector,\n        FieldMemOperand(feedback_vector, FeedbackCell::kValueOffset));\n\n    Label install_baseline_code;\n    // Check if feedback vector is valid. If not, call prepare for baseline to\n    // allocate it.\n    __ LoadTaggedField(\n        t0, FieldMemOperand(feedback_vector, HeapObject::kMapOffset));\n    __ Lhu(t0, FieldMemOperand(t0, Map::kInstanceTypeOffset));\n    __ Branch(&install_baseline_code, ne, t0, Operand(FEEDBACK_VECTOR_TYPE));\n\n    // Check for an tiering state.\n    __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n        flags, feedback_vector, CodeKind::BASELINE, &flags_need_processing);\n\n    // Load the baseline code into the closure.\n    __ Move(a2, kInterpreterBytecodeArrayRegister);\n    static_assert(kJavaScriptCallCodeStartRegister == a2, \"ABI mismatch\");\n    __ ReplaceClosureCodeWithOptimizedCode(a2, closure);\n    __ JumpCodeObject(a2, kJSEntrypointTag);\n\n    __ bind(&install_baseline_code);\n    __ GenerateTailCallToReturnedCode(Runtime::kInstallBaselineCode);\n  }\n#endif  // !V8_JITLESS\n\n  __ bind(&compile_lazy);\n  __ GenerateTailCallToReturnedCode(Runtime::kCompileLazy);\n  // Unreachable code.\n  __ break_(0xCC);\n\n  __ bind(&stack_overflow);\n  __ CallRuntime(Runtime::kThrowStackOverflow);\n  // Unreachable code.\n  __ break_(0xCC);\n}", "name_and_para": "void Builtins::Generate_InterpreterEntryTrampoline(\n    MacroAssembler* masm, InterpreterEntryTrampolineMode mode) "}], [{"name": "Builtins::Generate_BaselineOutOfLinePrologueDeopt", "content": "void Builtins::Generate_BaselineOutOfLinePrologueDeopt(MacroAssembler* masm) {\n  // We're here because we got deopted during BaselineOutOfLinePrologue's stack\n  // check. Undo all its frame creation and call into the interpreter instead.\n\n  // Drop the feedback vector and the bytecode offset (was the feedback vector\n  // but got replaced during deopt).\n  __ Drop(2);\n\n  // Bytecode array, argc, Closure, Context.\n  __ Pop(padreg, kJavaScriptCallArgCountRegister, kJavaScriptCallTargetRegister,\n         kContextRegister);\n\n  // Drop frame pointer\n  __ LeaveFrame(StackFrame::BASELINE);\n\n  // Enter the interpreter.\n  __ TailCallBuiltin(Builtin::kInterpreterEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_BaselineOutOfLinePrologueDeopt(MacroAssembler* masm) "}, {"name": "Builtins::Generate_BaselineOutOfLinePrologueDeopt", "content": "void Builtins::Generate_BaselineOutOfLinePrologueDeopt(MacroAssembler* masm) {\n  // We're here because we got deopted during BaselineOutOfLinePrologue's stack\n  // check. Undo all its frame creation and call into the interpreter instead.\n\n  // Drop bytecode offset (was the feedback vector but got replaced during\n  // deopt) and bytecode array.\n  __ AddWord(sp, sp, Operand(3 * kSystemPointerSize));\n\n  // Context, closure, argc.\n  __ Pop(kContextRegister, kJavaScriptCallTargetRegister,\n         kJavaScriptCallArgCountRegister);\n\n  // Drop frame pointer\n  __ LeaveFrame(StackFrame::BASELINE);\n\n  // Enter the interpreter.\n  __ TailCallBuiltin(Builtin::kInterpreterEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_BaselineOutOfLinePrologueDeopt(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_BaselineOutOfLinePrologue", "content": "void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {\n  UseScratchRegisterScope temps(masm);\n  // Need a few extra registers\n  temps.Include(CPURegList(kXRegSizeInBits, {x14, x15, x22}));\n\n  auto descriptor =\n      Builtins::CallInterfaceDescriptorFor(Builtin::kBaselineOutOfLinePrologue);\n  Register closure = descriptor.GetRegisterParameter(\n      BaselineOutOfLinePrologueDescriptor::kClosure);\n  // Load the feedback cell and vector from the closure.\n  Register feedback_cell = temps.AcquireX();\n  Register feedback_vector = temps.AcquireX();\n  __ LoadTaggedField(feedback_cell,\n                     FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n  __ LoadTaggedField(\n      feedback_vector,\n      FieldMemOperand(feedback_cell, FeedbackCell::kValueOffset));\n  __ AssertFeedbackVector(feedback_vector, x4);\n\n  // Check the tiering state.\n  Label flags_need_processing;\n  Register flags = temps.AcquireW();\n  __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      flags, feedback_vector, CodeKind::BASELINE, &flags_need_processing);\n\n  {\n    UseScratchRegisterScope temps(masm);\n    ResetFeedbackVectorOsrUrgency(masm, feedback_vector, temps.AcquireW());\n  }\n\n  // Increment invocation count for the function.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register invocation_count = temps.AcquireW();\n    __ Ldr(invocation_count,\n           FieldMemOperand(feedback_vector,\n                           FeedbackVector::kInvocationCountOffset));\n    __ Add(invocation_count, invocation_count, Operand(1));\n    __ Str(invocation_count,\n           FieldMemOperand(feedback_vector,\n                           FeedbackVector::kInvocationCountOffset));\n  }\n\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Frame Setup\");\n    // Normally the first thing we'd do here is Push(lr, fp), but we already\n    // entered the frame in BaselineCompiler::Prologue, as we had to use the\n    // value lr before the call to this BaselineOutOfLinePrologue builtin.\n\n    Register callee_context = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kCalleeContext);\n    Register callee_js_function = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kClosure);\n    {\n      UseScratchRegisterScope temps(masm);\n      ResetJSFunctionAge(masm, callee_js_function, temps.AcquireX());\n    }\n    __ Push(callee_context, callee_js_function);\n    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);\n    DCHECK_EQ(callee_js_function, kJSFunctionRegister);\n\n    Register argc = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);\n    // We'll use the bytecode for both code age/OSR resetting, and pushing onto\n    // the frame, so load it into a register.\n    Register bytecode_array = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);\n    __ Push(argc, bytecode_array, feedback_cell, feedback_vector);\n    __ AssertFeedbackVector(feedback_vector, x4);\n  }\n\n  Label call_stack_guard;\n  Register frame_size = descriptor.GetRegisterParameter(\n      BaselineOutOfLinePrologueDescriptor::kStackFrameSize);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Stack/interrupt check\");\n    // Stack check. This folds the checks for both the interrupt stack limit\n    // check and the real stack limit into one by just checking for the\n    // interrupt limit. The interrupt limit is either equal to the real stack\n    // limit or tighter. By ensuring we have space until that limit after\n    // building the frame we can quickly precheck both at once.\n    UseScratchRegisterScope temps(masm);\n\n    Register sp_minus_frame_size = temps.AcquireX();\n    __ Sub(sp_minus_frame_size, sp, frame_size);\n    Register interrupt_limit = temps.AcquireX();\n    __ LoadStackLimit(interrupt_limit, StackLimitKind::kInterruptStackLimit);\n    __ Cmp(sp_minus_frame_size, interrupt_limit);\n    __ B(lo, &call_stack_guard);\n  }\n\n  // Do \"fast\" return to the caller pc in lr.\n  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n  __ Ret();\n\n  __ bind(&flags_need_processing);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Optimized marker check\");\n    // Drop the frame created by the baseline call.\n    __ Pop<MacroAssembler::kAuthLR>(fp, lr);\n    __ OptimizeCodeOrTailCallOptimizedCodeSlot(flags, feedback_vector);\n    __ Trap();\n  }\n\n  __ bind(&call_stack_guard);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Stack/interrupt call\");\n    Register new_target = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallNewTarget);\n\n    FrameScope frame_scope(masm, StackFrame::INTERNAL);\n    // Save incoming new target or generator\n    __ Push(padreg, new_target);\n    __ SmiTag(frame_size);\n    __ PushArgument(frame_size);\n    __ CallRuntime(Runtime::kStackGuardWithGap);\n    __ Pop(new_target, padreg);\n  }\n  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);\n  __ Ret();\n}", "name_and_para": "void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) "}, {"name": "Builtins::Generate_BaselineOutOfLinePrologue", "content": "void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {\n  UseScratchRegisterScope temps(masm);\n  temps.Include({kScratchReg, kScratchReg2, s1});\n  auto descriptor =\n      Builtins::CallInterfaceDescriptorFor(Builtin::kBaselineOutOfLinePrologue);\n  Register closure = descriptor.GetRegisterParameter(\n      BaselineOutOfLinePrologueDescriptor::kClosure);\n  // Load the feedback cell and vector from the closure.\n  Register feedback_cell = temps.Acquire();\n  Register feedback_vector = temps.Acquire();\n  __ LoadTaggedField(feedback_cell,\n                     FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));\n  __ LoadTaggedField(\n      feedback_vector,\n      FieldMemOperand(feedback_cell, FeedbackCell::kValueOffset));\n  {\n    UseScratchRegisterScope temp(masm);\n    Register type = temps.Acquire();\n    __ AssertFeedbackVector(feedback_vector, type);\n  }\n\n  // Check for an tiering state.\n  Label flags_need_processing;\n  Register flags = temps.Acquire();\n  __ LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      flags, feedback_vector, CodeKind::BASELINE, &flags_need_processing);\n  {\n    UseScratchRegisterScope temps(masm);\n    ResetFeedbackVectorOsrUrgency(masm, feedback_vector, temps.Acquire());\n  }\n  // Increment invocation count for the function.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register invocation_count = temps.Acquire();\n    __ Lw(invocation_count,\n          FieldMemOperand(feedback_vector,\n                          FeedbackVector::kInvocationCountOffset));\n    __ Add32(invocation_count, invocation_count, Operand(1));\n    __ Sw(invocation_count,\n          FieldMemOperand(feedback_vector,\n                          FeedbackVector::kInvocationCountOffset));\n  }\n\n  FrameScope frame_scope(masm, StackFrame::MANUAL);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Frame Setup\");\n    // Normally the first thing we'd do here is Push(lr, fp), but we already\n    // entered the frame in BaselineCompiler::Prologue, as we had to use the\n    // value lr before the call to this BaselineOutOfLinePrologue builtin.\n\n    Register callee_context = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kCalleeContext);\n    Register callee_js_function = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kClosure);\n    {\n      UseScratchRegisterScope temps(masm);\n      ResetJSFunctionAge(masm, callee_js_function, temps.Acquire());\n    }\n    __ Push(callee_context, callee_js_function);\n    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);\n    DCHECK_EQ(callee_js_function, kJSFunctionRegister);\n\n    Register argc = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);\n    // We'll use the bytecode for both code age/OSR resetting, and pushing onto\n    // the frame, so load it into a register.\n    Register bytecode_array = descriptor.GetRegisterParameter(\n        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);\n    __ Push(argc, bytecode_array, feedback_cell, feedback_vector);\n    // Baseline code frames store the feedback vector where interpreter would\n    // store the bytecode offset.\n    {\n      UseScratchRegisterScope temp(masm);\n      Register type = temps.Acquire();\n      __ AssertFeedbackVector(feedback_vector, type);\n    }\n  }\n\n  Label call_stack_guard;\n  Register frame_size = descriptor.GetRegisterParameter(\n      BaselineOutOfLinePrologueDescriptor::kStackFrameSize);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Stack/interrupt check\");\n    // Stack check. This folds the checks for both the interrupt stack limit\n    // check and the real stack limit into one by just checking for the\n    // interrupt limit. The interrupt limit is either equal to the real stack\n    // limit or tighter. By ensuring we have space until that limit after\n    // building the frame we can quickly precheck both at once.\n    UseScratchRegisterScope temps(masm);\n    Register sp_minus_frame_size = temps.Acquire();\n    __ SubWord(sp_minus_frame_size, sp, frame_size);\n    Register interrupt_limit = temps.Acquire();\n    __ LoadStackLimit(interrupt_limit,\n                      MacroAssembler::StackLimitKind::kInterruptStackLimit);\n    __ Branch(&call_stack_guard, Uless, sp_minus_frame_size,\n              Operand(interrupt_limit));\n  }\n\n  // Do \"fast\" return to the caller pc in lr.\n  // TODO(v8:11429): Document this frame setup better.\n  __ Ret();\n\n  __ bind(&flags_need_processing);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Optimized marker check\");\n    // Drop the frame created by the baseline call.\n    __ Pop(ra, fp);\n    __ OptimizeCodeOrTailCallOptimizedCodeSlot(flags, feedback_vector);\n    __ Trap();\n  }\n\n  __ bind(&call_stack_guard);\n  {\n    ASM_CODE_COMMENT_STRING(masm, \"Stack/interrupt call\");\n    FrameScope frame_scope(masm, StackFrame::INTERNAL);\n    // Save incoming new target or generator\n    __ Push(kJavaScriptCallNewTargetRegister);\n    __ SmiTag(frame_size);\n    __ Push(frame_size);\n    __ CallRuntime(Runtime::kStackGuardWithGap);\n    __ Pop(kJavaScriptCallNewTargetRegister);\n  }\n  __ Ret();\n  temps.Exclude({kScratchReg, kScratchReg2, s1});\n}", "name_and_para": "void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) "}], [{"name": "ResetFeedbackVectorOsrUrgency", "content": "void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,\n                                   Register feedback_vector, Register scratch) {\n  DCHECK(!AreAliased(feedback_vector, scratch));\n  __ Ldrb(scratch,\n          FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));\n  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));\n  __ Strb(scratch,\n          FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));\n}", "name_and_para": "void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,\n                                   Register feedback_vector, Register scratch) "}, {"name": "ResetFeedbackVectorOsrUrgency", "content": "void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,\n                                   Register feedback_vector, Register scratch) {\n  DCHECK(!AreAliased(feedback_vector, scratch));\n  __ Lbu(scratch,\n         FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));\n  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));\n  __ Sb(scratch,\n        FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));\n}", "name_and_para": "void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,\n                                   Register feedback_vector, Register scratch) "}], [{"name": "ResetJSFunctionAge", "content": "void ResetJSFunctionAge(MacroAssembler* masm, Register js_function,\n                        Register scratch) {\n  const Register shared_function_info(scratch);\n  __ LoadTaggedField(\n      shared_function_info,\n      FieldMemOperand(js_function, JSFunction::kSharedFunctionInfoOffset));\n  ResetSharedFunctionInfoAge(masm, shared_function_info);\n}", "name_and_para": "void ResetJSFunctionAge(MacroAssembler* masm, Register js_function,\n                        Register scratch) "}, {"name": "ResetJSFunctionAge", "content": "void ResetJSFunctionAge(MacroAssembler* masm, Register js_function,\n                        Register scratch) {\n  const Register shared_function_info(scratch);\n  __ LoadTaggedField(\n      shared_function_info,\n      FieldMemOperand(js_function, JSFunction::kSharedFunctionInfoOffset));\n  ResetSharedFunctionInfoAge(masm, shared_function_info);\n}", "name_and_para": "void ResetJSFunctionAge(MacroAssembler* masm, Register js_function,\n                        Register scratch) "}], [{"name": "ResetSharedFunctionInfoAge", "content": "void ResetSharedFunctionInfoAge(MacroAssembler* masm, Register sfi) {\n  __ Strh(wzr, FieldMemOperand(sfi, SharedFunctionInfo::kAgeOffset));\n}", "name_and_para": "void ResetSharedFunctionInfoAge(MacroAssembler* masm, Register sfi) "}, {"name": "ResetSharedFunctionInfoAge", "content": "void ResetSharedFunctionInfoAge(MacroAssembler* masm, Register sfi) {\n  __ Sh(zero_reg, FieldMemOperand(sfi, SharedFunctionInfo::kAgeOffset));\n}", "name_and_para": "void ResetSharedFunctionInfoAge(MacroAssembler* masm, Register sfi) "}], [{"name": "AdvanceBytecodeOffsetOrReturn", "content": "static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,\n                                          Register bytecode_array,\n                                          Register bytecode_offset,\n                                          Register bytecode, Register scratch1,\n                                          Register scratch2, Label* if_return) {\n  ASM_CODE_COMMENT(masm);\n  Register bytecode_size_table = scratch1;\n\n  // The bytecode offset value will be increased by one in wide and extra wide\n  // cases. In the case of having a wide or extra wide JumpLoop bytecode, we\n  // will restore the original bytecode. In order to simplify the code, we have\n  // a backup of it.\n  Register original_bytecode_offset = scratch2;\n  DCHECK(!AreAliased(bytecode_array, bytecode_offset, bytecode_size_table,\n                     bytecode, original_bytecode_offset));\n\n  __ Mov(bytecode_size_table, ExternalReference::bytecode_size_table_address());\n  __ Mov(original_bytecode_offset, bytecode_offset);\n\n  // Check if the bytecode is a Wide or ExtraWide prefix bytecode.\n  Label process_bytecode, extra_wide;\n  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));\n  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));\n  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));\n  static_assert(3 ==\n                static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));\n  __ Cmp(bytecode, Operand(0x3));\n  __ B(hi, &process_bytecode);\n  __ Tst(bytecode, Operand(0x1));\n  // The code to load the next bytecode is common to both wide and extra wide.\n  // We can hoist them up here since they do not modify the flags after Tst.\n  __ Add(bytecode_offset, bytecode_offset, Operand(1));\n  __ Ldrb(bytecode, MemOperand(bytecode_array, bytecode_offset));\n  __ B(ne, &extra_wide);\n\n  // Update table to the wide scaled table.\n  __ Add(bytecode_size_table, bytecode_size_table,\n         Operand(kByteSize * interpreter::Bytecodes::kBytecodeCount));\n  __ B(&process_bytecode);\n\n  __ Bind(&extra_wide);\n  // Update table to the extra wide scaled table.\n  __ Add(bytecode_size_table, bytecode_size_table,\n         Operand(2 * kByteSize * interpreter::Bytecodes::kBytecodeCount));\n\n  __ Bind(&process_bytecode);\n\n// Bailout to the return label if this is a return bytecode.\n#define JUMP_IF_EQUAL(NAME)                                              \\\n  __ Cmp(x1, Operand(static_cast<int>(interpreter::Bytecode::k##NAME))); \\\n  __ B(if_return, eq);\n  RETURN_BYTECODE_LIST(JUMP_IF_EQUAL)\n#undef JUMP_IF_EQUAL\n\n  // If this is a JumpLoop, re-execute it to perform the jump to the beginning\n  // of the loop.\n  Label end, not_jump_loop;\n  __ Cmp(bytecode, Operand(static_cast<int>(interpreter::Bytecode::kJumpLoop)));\n  __ B(ne, &not_jump_loop);\n  // We need to restore the original bytecode_offset since we might have\n  // increased it to skip the wide / extra-wide prefix bytecode.\n  __ Mov(bytecode_offset, original_bytecode_offset);\n  __ B(&end);\n\n  __ bind(&not_jump_loop);\n  // Otherwise, load the size of the current bytecode and advance the offset.\n  __ Ldrb(scratch1.W(), MemOperand(bytecode_size_table, bytecode));\n  __ Add(bytecode_offset, bytecode_offset, scratch1);\n\n  __ Bind(&end);\n}", "name_and_para": "static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,\n                                          Register bytecode_array,\n                                          Register bytecode_offset,\n                                          Register bytecode, Register scratch1,\n                                          Register scratch2, Label* if_return) "}, {"name": "AdvanceBytecodeOffsetOrReturn", "content": "static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,\n                                          Register bytecode_array,\n                                          Register bytecode_offset,\n                                          Register bytecode, Register scratch1,\n                                          Register scratch2, Register scratch3,\n                                          Label* if_return) {\n  ASM_CODE_COMMENT(masm);\n  Register bytecode_size_table = scratch1;\n\n  // The bytecode offset value will be increased by one in wide and extra wide\n  // cases. In the case of having a wide or extra wide JumpLoop bytecode, we\n  // will restore the original bytecode. In order to simplify the code, we have\n  // a backup of it.\n  Register original_bytecode_offset = scratch3;\n  DCHECK(!AreAliased(bytecode_array, bytecode_offset, bytecode,\n                     bytecode_size_table, original_bytecode_offset));\n  __ Move(original_bytecode_offset, bytecode_offset);\n  __ li(bytecode_size_table, ExternalReference::bytecode_size_table_address());\n\n  // Check if the bytecode is a Wide or ExtraWide prefix bytecode.\n  Label process_bytecode, extra_wide;\n  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));\n  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));\n  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));\n  static_assert(3 ==\n                static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));\n  __ Branch(&process_bytecode, Ugreater, bytecode, Operand(3),\n            Label::Distance::kNear);\n  __ And(scratch2, bytecode, Operand(1));\n  __ Branch(&extra_wide, ne, scratch2, Operand(zero_reg),\n            Label::Distance::kNear);\n\n  // Load the next bytecode and update table to the wide scaled table.\n  __ AddWord(bytecode_offset, bytecode_offset, Operand(1));\n  __ AddWord(scratch2, bytecode_array, bytecode_offset);\n  __ Lbu(bytecode, MemOperand(scratch2));\n  __ AddWord(bytecode_size_table, bytecode_size_table,\n             Operand(kByteSize * interpreter::Bytecodes::kBytecodeCount));\n  __ BranchShort(&process_bytecode);\n\n  __ bind(&extra_wide);\n  // Load the next bytecode and update table to the extra wide scaled table.\n  __ AddWord(bytecode_offset, bytecode_offset, Operand(1));\n  __ AddWord(scratch2, bytecode_array, bytecode_offset);\n  __ Lbu(bytecode, MemOperand(scratch2));\n  __ AddWord(bytecode_size_table, bytecode_size_table,\n             Operand(2 * kByteSize * interpreter::Bytecodes::kBytecodeCount));\n\n  __ bind(&process_bytecode);\n\n// Bailout to the return label if this is a return bytecode.\n#define JUMP_IF_EQUAL(NAME)          \\\n  __ Branch(if_return, eq, bytecode, \\\n            Operand(static_cast<int64_t>(interpreter::Bytecode::k##NAME)));\n  RETURN_BYTECODE_LIST(JUMP_IF_EQUAL)\n#undef JUMP_IF_EQUAL\n\n  // If this is a JumpLoop, re-execute it to perform the jump to the beginning\n  // of the loop.\n  Label end, not_jump_loop;\n  __ Branch(&not_jump_loop, ne, bytecode,\n            Operand(static_cast<int64_t>(interpreter::Bytecode::kJumpLoop)),\n            Label::Distance::kNear);\n  // We need to restore the original bytecode_offset since we might have\n  // increased it to skip the wide / extra-wide prefix bytecode.\n  __ Move(bytecode_offset, original_bytecode_offset);\n  __ BranchShort(&end);\n\n  __ bind(&not_jump_loop);\n  // Otherwise, load the size of the current bytecode and advance the offset.\n  __ AddWord(scratch2, bytecode_size_table, bytecode);\n  __ Lb(scratch2, MemOperand(scratch2));\n  __ AddWord(bytecode_offset, bytecode_offset, scratch2);\n\n  __ bind(&end);\n}", "name_and_para": "static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,\n                                          Register bytecode_array,\n                                          Register bytecode_offset,\n                                          Register bytecode, Register scratch1,\n                                          Register scratch2, Register scratch3,\n                                          Label* if_return) "}], [{"name": "LeaveInterpreterFrame", "content": "static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,\n                                  Register scratch2) {\n  ASM_CODE_COMMENT(masm);\n  Register params_size = scratch1;\n  // Get the size of the formal parameters + receiver (in bytes).\n  __ Ldr(params_size,\n         MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ Ldr(params_size.W(),\n         FieldMemOperand(params_size, BytecodeArray::kParameterSizeOffset));\n\n  Register actual_params_size = scratch2;\n  // Compute the size of the actual parameters + receiver (in bytes).\n  __ Ldr(actual_params_size,\n         MemOperand(fp, StandardFrameConstants::kArgCOffset));\n  __ lsl(actual_params_size, actual_params_size, kSystemPointerSizeLog2);\n\n  // If actual is bigger than formal, then we should use it to free up the stack\n  // arguments.\n  __ Cmp(params_size, actual_params_size);\n  __ Csel(params_size, actual_params_size, params_size, kLessThan);\n\n  // Leave the frame (also dropping the register file).\n  __ LeaveFrame(StackFrame::INTERPRETED);\n\n  // Drop receiver + arguments.\n  if (v8_flags.debug_code) {\n    __ Tst(params_size, kSystemPointerSize - 1);\n    __ Check(eq, AbortReason::kUnexpectedValue);\n  }\n  __ Lsr(params_size, params_size, kSystemPointerSizeLog2);\n  __ DropArguments(params_size);\n}", "name_and_para": "static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,\n                                  Register scratch2) "}, {"name": "LeaveInterpreterFrame", "content": "static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,\n                                  Register scratch2) {\n  ASM_CODE_COMMENT(masm);\n  Register params_size = scratch1;\n\n  // Get the size of the formal parameters + receiver (in bytes).\n  __ LoadWord(params_size,\n              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));\n  __ Lw(params_size,\n        FieldMemOperand(params_size, BytecodeArray::kParameterSizeOffset));\n\n  Register actual_params_size = scratch2;\n  Label L1;\n  // Compute the size of the actual parameters + receiver (in bytes).\n  __ LoadWord(actual_params_size,\n              MemOperand(fp, StandardFrameConstants::kArgCOffset));\n  __ SllWord(actual_params_size, actual_params_size, kSystemPointerSizeLog2);\n  // If actual is bigger than formal, then we should use it to free up the stack\n  // arguments.\n  __ Branch(&L1, le, actual_params_size, Operand(params_size),\n            Label::Distance::kNear);\n  __ Move(params_size, actual_params_size);\n  __ bind(&L1);\n\n  // Leave the frame (also dropping the register file).\n  __ LeaveFrame(StackFrame::INTERPRETED);\n\n  // Drop receiver + arguments.\n  __ DropArguments(params_size, MacroAssembler::kCountIsBytes,\n                   MacroAssembler::kCountIncludesReceiver);\n}", "name_and_para": "static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,\n                                  Register scratch2) "}], [{"name": "Builtins::Generate_RunMicrotasksTrampoline", "content": "void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {\n  // This expects two C++ function parameters passed by Invoke() in\n  // execution.cc.\n  //   x0: root_register_value\n  //   x1: microtask_queue\n\n  __ Mov(RunMicrotasksDescriptor::MicrotaskQueueRegister(), x1);\n  __ TailCallBuiltin(Builtin::kRunMicrotasks);\n}", "name_and_para": "void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_RunMicrotasksTrampoline", "content": "void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {\n  // a1: microtask_queue\n  __ Move(RunMicrotasksDescriptor::MicrotaskQueueRegister(), a1);\n  __ TailCallBuiltin(Builtin::kRunMicrotasks);\n}", "name_and_para": "void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSConstructEntryTrampoline", "content": "void Builtins::Generate_JSConstructEntryTrampoline(MacroAssembler* masm) {\n  Generate_JSEntryTrampolineHelper(masm, true);\n}", "name_and_para": "void Builtins::Generate_JSConstructEntryTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSConstructEntryTrampoline", "content": "void Builtins::Generate_JSConstructEntryTrampoline(MacroAssembler* masm) {\n  Generate_JSEntryTrampolineHelper(masm, true);\n}", "name_and_para": "void Builtins::Generate_JSConstructEntryTrampoline(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSEntryTrampoline", "content": "void Builtins::Generate_JSEntryTrampoline(MacroAssembler* masm) {\n  Generate_JSEntryTrampolineHelper(masm, false);\n}", "name_and_para": "void Builtins::Generate_JSEntryTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSEntryTrampoline", "content": "void Builtins::Generate_JSEntryTrampoline(MacroAssembler* masm) {\n  Generate_JSEntryTrampolineHelper(masm, false);\n}", "name_and_para": "void Builtins::Generate_JSEntryTrampoline(MacroAssembler* masm) "}], [{"name": "Generate_JSEntryTrampolineHelper", "content": "static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,\n                                             bool is_construct) {\n  Register new_target = x1;\n  Register function = x2;\n  Register receiver = x3;\n  Register argc = x4;\n  Register argv = x5;\n  Register scratch = x10;\n  Register slots_to_claim = x11;\n\n  {\n    // Enter an internal frame.\n    FrameScope scope(masm, StackFrame::INTERNAL);\n\n    // Setup the context (we need to use the caller context from the isolate).\n    __ Mov(scratch, ExternalReference::Create(IsolateAddressId::kContextAddress,\n                                              masm->isolate()));\n    __ Ldr(cp, MemOperand(scratch));\n\n    // Claim enough space for the arguments and the function, including an\n    // optional slot of padding.\n    constexpr int additional_slots = 2;\n    __ Add(slots_to_claim, argc, additional_slots);\n    __ Bic(slots_to_claim, slots_to_claim, 1);\n\n    // Check if we have enough stack space to push all arguments.\n    Label enough_stack_space, stack_overflow;\n    __ StackOverflowCheck(slots_to_claim, &stack_overflow);\n    __ B(&enough_stack_space);\n\n    __ Bind(&stack_overflow);\n    __ CallRuntime(Runtime::kThrowStackOverflow);\n    __ Unreachable();\n\n    __ Bind(&enough_stack_space);\n    __ Claim(slots_to_claim);\n\n    // Store padding (which might be overwritten).\n    __ SlotAddress(scratch, slots_to_claim);\n    __ Str(padreg, MemOperand(scratch, -kSystemPointerSize));\n\n    // Store receiver on the stack.\n    __ Poke(receiver, 0);\n    // Store function on the stack.\n    __ SlotAddress(scratch, argc);\n    __ Str(function, MemOperand(scratch));\n\n    // Copy arguments to the stack in a loop, in reverse order.\n    // x4: argc.\n    // x5: argv.\n    Label loop, done;\n\n    // Skip the argument set up if we have no arguments.\n    __ Cmp(argc, JSParameterCount(0));\n    __ B(eq, &done);\n\n    // scratch has been set to point to the location of the function, which\n    // marks the end of the argument copy.\n    __ SlotAddress(x0, 1);  // Skips receiver.\n    __ Bind(&loop);\n    // Load the handle.\n    __ Ldr(x11, MemOperand(argv, kSystemPointerSize, PostIndex));\n    // Dereference the handle.\n    __ Ldr(x11, MemOperand(x11));\n    // Poke the result into the stack.\n    __ Str(x11, MemOperand(x0, kSystemPointerSize, PostIndex));\n    // Loop if we've not reached the end of copy marker.\n    __ Cmp(x0, scratch);\n    __ B(lt, &loop);\n\n    __ Bind(&done);\n\n    __ Mov(x0, argc);\n    __ Mov(x3, new_target);\n    __ Mov(x1, function);\n    // x0: argc.\n    // x1: function.\n    // x3: new.target.\n\n    // Initialize all JavaScript callee-saved registers, since they will be seen\n    // by the garbage collector as part of handlers.\n    // The original values have been saved in JSEntry.\n    __ LoadRoot(x19, RootIndex::kUndefinedValue);\n    __ Mov(x20, x19);\n    __ Mov(x21, x19);\n    __ Mov(x22, x19);\n    __ Mov(x23, x19);\n    __ Mov(x24, x19);\n    __ Mov(x25, x19);\n#ifndef V8_COMPRESS_POINTERS\n    __ Mov(x28, x19);\n#endif\n    // Don't initialize the reserved registers.\n    // x26 : root register (kRootRegister).\n    // x27 : context pointer (cp).\n    // x28 : pointer cage base register (kPtrComprCageBaseRegister).\n    // x29 : frame pointer (fp).\n\n    Builtin builtin = is_construct ? Builtin::kConstruct : Builtins::Call();\n    __ CallBuiltin(builtin);\n\n    // Exit the JS internal frame and remove the parameters (except function),\n    // and return.\n  }\n\n  // Result is in x0. Return.\n  __ Ret();\n}", "name_and_para": "static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,\n                                             bool is_construct) "}, {"name": "Generate_JSEntryTrampolineHelper", "content": "static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,\n                                             bool is_construct) {\n  // ----------- S t a t e -------------\n  //  -- a1: new.target\n  //  -- a2: function\n  //  -- a3: receiver_pointer\n  //  -- a4: argc\n  //  -- a5: argv\n  // -----------------------------------\n\n  // Enter an internal frame.\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n\n    // Setup the context (we need to use the caller context from the isolate).\n    ExternalReference context_address = ExternalReference::Create(\n        IsolateAddressId::kContextAddress, masm->isolate());\n    __ li(cp, context_address);\n    __ LoadWord(cp, MemOperand(cp));\n\n    // Push the function onto the stack.\n    __ Push(a2);\n\n    // Check if we have enough stack space to push all arguments.\n    __ mv(a6, a4);\n    Generate_CheckStackOverflow(masm, a6, a0, s2);\n\n    // Copy arguments to the stack.\n    // a4: argc\n    // a5: argv, i.e. points to first arg\n    {\n      UseScratchRegisterScope temps(masm);\n      Generate_PushArguments(masm, a5, a4, temps.Acquire(), temps.Acquire(),\n                             ArgumentsElementType::kHandle);\n    }\n    // Push the receive.\n    __ Push(a3);\n\n    // a0: argc\n    // a1: function\n    // a3: new.target\n    __ Move(a3, a1);\n    __ Move(a1, a2);\n    __ Move(a0, a4);\n\n    // Initialize all JavaScript callee-saved registers, since they will be seen\n    // by the garbage collector as part of handlers.\n    __ LoadRoot(a4, RootIndex::kUndefinedValue);\n    __ Move(a5, a4);\n    __ Move(s1, a4);\n    __ Move(s2, a4);\n    __ Move(s3, a4);\n    __ Move(s4, a4);\n    __ Move(s5, a4);\n#ifndef V8_COMPRESS_POINTERS\n    __ Move(s11, a4);\n#endif\n    // s6 holds the root address. Do not clobber.\n    // s7 is cp. Do not init.\n    // s11 is pointer cage base register (kPointerCageBaseRegister).\n\n    // Invoke the code.\n    Builtin builtin = is_construct ? Builtin::kConstruct : Builtins::Call();\n    __ CallBuiltin(builtin);\n\n    // Leave internal frame.\n  }\n  __ Jump(ra);\n}", "name_and_para": "static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,\n                                             bool is_construct) "}], [{"name": "Builtins::Generate_JSRunMicrotasksEntry", "content": "void Builtins::Generate_JSRunMicrotasksEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::ENTRY,\n                          Builtin::kRunMicrotasksTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSRunMicrotasksEntry(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSRunMicrotasksEntry", "content": "void Builtins::Generate_JSRunMicrotasksEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::ENTRY,\n                          Builtin::kRunMicrotasksTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSRunMicrotasksEntry(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSConstructEntry", "content": "void Builtins::Generate_JSConstructEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::CONSTRUCT_ENTRY,\n                          Builtin::kJSConstructEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSConstructEntry(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSConstructEntry", "content": "void Builtins::Generate_JSConstructEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::CONSTRUCT_ENTRY,\n                          Builtin::kJSConstructEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSConstructEntry(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSEntry", "content": "void Builtins::Generate_JSEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::ENTRY, Builtin::kJSEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSEntry(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSEntry", "content": "void Builtins::Generate_JSEntry(MacroAssembler* masm) {\n  Generate_JSEntryVariant(masm, StackFrame::ENTRY, Builtin::kJSEntryTrampoline);\n}", "name_and_para": "void Builtins::Generate_JSEntry(MacroAssembler* masm) "}], [{"name": "Generate_JSEntryVariant", "content": "void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,\n                             Builtin entry_trampoline) {\n  Label invoke, handler_entry, exit;\n\n  {\n    NoRootArrayScope no_root_array(masm);\n\n#if defined(V8_OS_WIN)\n    // In order to allow Windows debugging tools to reconstruct a call stack, we\n    // must generate information describing how to recover at least fp, sp, and\n    // pc for the calling frame. Here, JSEntry registers offsets to\n    // xdata_encoder which then emits the offset values as part of the unwind\n    // data accordingly.\n    win64_unwindinfo::XdataEncoder* xdata_encoder = masm->GetXdataEncoder();\n    if (xdata_encoder) {\n      xdata_encoder->onFramePointerAdjustment(\n          EntryFrameConstants::kDirectCallerFPOffset,\n          EntryFrameConstants::kDirectCallerSPOffset);\n    }\n#endif\n\n    __ PushCalleeSavedRegisters();\n\n    // Set up the reserved register for 0.0.\n    __ Fmov(fp_zero, 0.0);\n\n    // Initialize the root register.\n    // C calling convention. The first argument is passed in x0.\n    __ Mov(kRootRegister, x0);\n\n#ifdef V8_COMPRESS_POINTERS\n    // Initialize the pointer cage base register.\n    __ LoadRootRelative(kPtrComprCageBaseRegister,\n                        IsolateData::cage_base_offset());\n#endif\n  }\n\n  // Set up fp. It points to the {fp, lr} pair pushed as the last step in\n  // PushCalleeSavedRegisters.\n  static_assert(\n      EntryFrameConstants::kCalleeSavedRegisterBytesPushedAfterFpLrPair == 0);\n  static_assert(EntryFrameConstants::kOffsetToCalleeSavedRegisters == 0);\n  __ Mov(fp, sp);\n\n  // Build an entry frame (see layout below).\n\n  // Push frame type markers.\n  __ Mov(x12, StackFrame::TypeToMarker(type));\n  __ Push(x12, xzr);\n\n  __ Mov(x11, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,\n                                        masm->isolate()));\n  __ Ldr(x10, MemOperand(x11));  // x10 = C entry FP.\n\n  // Clear c_entry_fp, now we've loaded its value to be pushed on the stack.\n  // If the c_entry_fp is not already zero and we don't clear it, the\n  // StackFrameIteratorForProfiler will assume we are executing C++ and miss the\n  // JS frames on top.\n  __ Str(xzr, MemOperand(x11));\n\n  // Set js_entry_sp if this is the outermost JS call.\n  Label done;\n  ExternalReference js_entry_sp = ExternalReference::Create(\n      IsolateAddressId::kJSEntrySPAddress, masm->isolate());\n  __ Mov(x12, js_entry_sp);\n  __ Ldr(x11, MemOperand(x12));  // x11 = previous JS entry SP.\n\n  // Select between the inner and outermost frame marker, based on the JS entry\n  // sp. We assert that the inner marker is zero, so we can use xzr to save a\n  // move instruction.\n  DCHECK_EQ(StackFrame::INNER_JSENTRY_FRAME, 0);\n  __ Cmp(x11, 0);  // If x11 is zero, this is the outermost frame.\n  // x11 = JS entry frame marker.\n  __ Csel(x11, xzr, StackFrame::OUTERMOST_JSENTRY_FRAME, ne);\n  __ B(ne, &done);\n  __ Str(fp, MemOperand(x12));\n\n  __ Bind(&done);\n\n  __ Mov(x9, ExternalReference::fast_c_call_caller_fp_address(masm->isolate()));\n  __ Ldr(x7, MemOperand(x9));\n  __ Str(xzr, MemOperand(x9));\n  __ Mov(x9, ExternalReference::fast_c_call_caller_pc_address(masm->isolate()));\n  __ Ldr(x8, MemOperand(x9));\n  __ Str(xzr, MemOperand(x9));\n  __ Push(x10, x11, x7, x8);\n\n  // The frame set up looks like this:\n  // sp[0] : fast api call pc.\n  // sp[1] : fast api call fp.\n  // sp[2] : JS entry frame marker.\n  // sp[3] : C entry FP.\n  // sp[4] : stack frame marker (0).\n  // sp[5] : stack frame marker (type).\n  // sp[6] : saved fp   <- fp points here.\n  // sp[7] : saved lr\n  // sp[8,26) : other saved registers\n\n  // Jump to a faked try block that does the invoke, with a faked catch\n  // block that sets the exception.\n  __ B(&invoke);\n\n  // Prevent the constant pool from being emitted between the record of the\n  // handler_entry position and the first instruction of the sequence here.\n  // There is no risk because Assembler::Emit() emits the instruction before\n  // checking for constant pool emission, but we do not want to depend on\n  // that.\n  {\n    Assembler::BlockPoolsScope block_pools(masm);\n\n    // Store the current pc as the handler offset. It's used later to create the\n    // handler table.\n    __ BindExceptionHandler(&handler_entry);\n    masm->isolate()->builtins()->SetJSEntryHandlerOffset(handler_entry.pos());\n\n    // Caught exception: Store result (exception) in the exception\n    // field in the JSEnv and return a failure sentinel. Coming in here the\n    // fp will be invalid because UnwindAndFindHandler sets it to 0 to\n    // signal the existence of the JSEntry frame.\n    __ Mov(x10, ExternalReference::Create(IsolateAddressId::kExceptionAddress,\n                                          masm->isolate()));\n  }\n  __ Str(x0, MemOperand(x10));\n  __ LoadRoot(x0, RootIndex::kException);\n  __ B(&exit);\n\n  // Invoke: Link this frame into the handler chain.\n  __ Bind(&invoke);\n\n  // Push new stack handler.\n  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize,\n                \"Unexpected offset for StackHandlerConstants::kSize\");\n  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize,\n                \"Unexpected offset for StackHandlerConstants::kNextOffset\");\n\n  // Link the current handler as the next handler.\n  __ Mov(x11, ExternalReference::Create(IsolateAddressId::kHandlerAddress,\n                                        masm->isolate()));\n  __ Ldr(x10, MemOperand(x11));\n  __ Push(padreg, x10);\n\n  // Set this new handler as the current one.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register scratch = temps.AcquireX();\n    __ Mov(scratch, sp);\n    __ Str(scratch, MemOperand(x11));\n  }\n\n  // If an exception not caught by another handler occurs, this handler\n  // returns control to the code after the B(&invoke) above, which\n  // restores all callee-saved registers (including cp and fp) to their\n  // saved values before returning a failure to C.\n  //\n  // Invoke the function by calling through JS entry trampoline builtin and\n  // pop the faked function when we return.\n  __ CallBuiltin(entry_trampoline);\n\n  // Pop the stack handler and unlink this frame from the handler chain.\n  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize,\n                \"Unexpected offset for StackHandlerConstants::kNextOffset\");\n  __ Pop(x10, padreg);\n  __ Mov(x11, ExternalReference::Create(IsolateAddressId::kHandlerAddress,\n                                        masm->isolate()));\n  __ Drop(StackHandlerConstants::kSlotCount - 2);\n  __ Str(x10, MemOperand(x11));\n\n  __ Bind(&exit);\n  // x0 holds the result.\n  // The stack pointer points to the top of the entry frame pushed on entry from\n  // C++ (at the beginning of this stub):\n  // sp[0] : fast api call pc.\n  // sp[1] : fast api call fp.\n  // sp[2] : JS entry frame marker.\n  // sp[3] : C entry FP.\n  // sp[4] : stack frame marker (0).\n  // sp[5] : stack frame marker (type).\n  // sp[6] : saved fp   <- fp points here.\n  // sp[7] : saved lr\n  // sp[8,26) : other saved registers\n\n  __ Pop(x10, x11);\n  __ Mov(x8, ExternalReference::fast_c_call_caller_pc_address(masm->isolate()));\n  __ Str(x10, MemOperand(x8));\n  __ Mov(x9, ExternalReference::fast_c_call_caller_fp_address(masm->isolate()));\n  __ Str(x11, MemOperand(x9));\n\n  // Check if the current stack frame is marked as the outermost JS frame.\n  Label non_outermost_js_2;\n  {\n    Register c_entry_fp = x11;\n    __ PeekPair(x10, c_entry_fp, 0);\n    __ Cmp(x10, StackFrame::OUTERMOST_JSENTRY_FRAME);\n    __ B(ne, &non_outermost_js_2);\n    __ Mov(x12, js_entry_sp);\n    __ Str(xzr, MemOperand(x12));\n    __ Bind(&non_outermost_js_2);\n\n    // Restore the top frame descriptors from the stack.\n    __ Mov(x12, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,\n                                          masm->isolate()));\n    __ Str(c_entry_fp, MemOperand(x12));\n  }\n\n  // Reset the stack to the callee saved registers.\n  static_assert(\n      EntryFrameConstants::kFixedFrameSize % (2 * kSystemPointerSize) == 0,\n      \"Size of entry frame is not a multiple of 16 bytes\");\n  // fast_c_call_caller_fp and fast_c_call_caller_pc have already been popped.\n  int drop_count =\n      (EntryFrameConstants::kFixedFrameSize / kSystemPointerSize) - 2;\n  __ Drop(drop_count);\n  // Restore the callee-saved registers and return.\n  __ PopCalleeSavedRegisters();\n  __ Ret();\n}", "name_and_para": "void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,\n                             Builtin entry_trampoline) "}, {"name": "Generate_JSEntryVariant", "content": "void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,\n                             Builtin entry_trampoline) {\n  Label invoke, handler_entry, exit;\n\n  {\n    NoRootArrayScope no_root_array(masm);\n\n    // TODO(plind): unify the ABI description here.\n    // Registers:\n    //  either\n    //   a0: root register value\n    //   a1: entry address\n    //   a2: function\n    //   a3: receiver\n    //   a4: argc\n    //   a5: argv\n    //  or\n    //   a0: root register value\n    //   a1: microtask_queue\n\n    // Save callee saved registers on the stack.\n    __ MultiPush(kCalleeSaved | ra);\n\n    // Save callee-saved FPU registers.\n    __ MultiPushFPU(kCalleeSavedFPU);\n    // Set up the reserved register for 0.0.\n    __ LoadFPRImmediate(kDoubleRegZero, 0.0);\n    __ LoadFPRImmediate(kSingleRegZero, 0.0f);\n\n    // Initialize the root register.\n    // C calling convention. The first argument is passed in a0.\n    __ Move(kRootRegister, a0);\n\n#ifdef V8_COMPRESS_POINTERS\n    // Initialize the pointer cage base register.\n    __ LoadRootRelative(kPtrComprCageBaseRegister,\n                        IsolateData::cage_base_offset());\n#endif\n  }\n\n  // a1: entry address\n  // a2: function\n  // a3: receiver\n  // a4: argc\n  // a5: argv\n\n  // We build an EntryFrame.\n  __ li(s1, Operand(-1));  // Push a bad frame pointer to fail if it is used.\n  __ li(s2, Operand(StackFrame::TypeToMarker(type)));\n  __ li(s3, Operand(StackFrame::TypeToMarker(type)));\n  ExternalReference c_entry_fp = ExternalReference::Create(\n      IsolateAddressId::kCEntryFPAddress, masm->isolate());\n  __ li(s5, c_entry_fp);\n  __ LoadWord(s4, MemOperand(s5));\n  __ Push(s1, s2, s3, s4);\n  // Clear c_entry_fp, now we've pushed its previous value to the stack.\n  // If the c_entry_fp is not already zero and we don't clear it, the\n  // StackFrameIteratorForProfiler will assume we are executing C++ and miss the\n  // JS frames on top.\n  __ StoreWord(zero_reg, MemOperand(s5));\n\n  __ li(s1, ExternalReference::fast_c_call_caller_fp_address(masm->isolate()));\n  __ LoadWord(s2, MemOperand(s1, 0));\n  __ StoreWord(zero_reg, MemOperand(s1, 0));\n  __ li(s1, ExternalReference::fast_c_call_caller_pc_address(masm->isolate()));\n  __ LoadWord(s3, MemOperand(s1, 0));\n  __ StoreWord(zero_reg, MemOperand(s1, 0));\n  __ Push(s2, s3);\n  // Set up frame pointer for the frame to be pushed.\n  __ AddWord(fp, sp, -EntryFrameConstants::kNextFastCallFramePCOffset);\n  // Registers:\n  //  either\n  //   a1: entry address\n  //   a2: function\n  //   a3: receiver\n  //   a4: argc\n  //   a5: argv\n  //  or\n  //   a1: microtask_queue\n  //\n  // Stack:\n  // fast api call pc\n  // fast api call fp\n  // caller fp          |\n  // function slot      | entry frame\n  // context slot       |\n  // bad fp (0xFF...F)  |\n  // callee saved registers + ra\n\n  // If this is the outermost JS call, set js_entry_sp value.\n  Label non_outermost_js;\n  ExternalReference js_entry_sp = ExternalReference::Create(\n      IsolateAddressId::kJSEntrySPAddress, masm->isolate());\n  __ li(s1, js_entry_sp);\n  __ LoadWord(s2, MemOperand(s1));\n  __ Branch(&non_outermost_js, ne, s2, Operand(zero_reg),\n            Label::Distance::kNear);\n  __ StoreWord(fp, MemOperand(s1));\n  __ li(s3, Operand(StackFrame::OUTERMOST_JSENTRY_FRAME));\n  Label cont;\n  __ Branch(&cont);\n  __ bind(&non_outermost_js);\n  __ li(s3, Operand(StackFrame::INNER_JSENTRY_FRAME));\n  __ bind(&cont);\n  __ push(s3);\n\n  // Jump to a faked try block that does the invoke, with a faked catch\n  // block that sets the exception.\n  __ BranchShort(&invoke);\n  __ bind(&handler_entry);\n\n  // Store the current pc as the handler offset. It's used later to create the\n  // handler table.\n  masm->isolate()->builtins()->SetJSEntryHandlerOffset(handler_entry.pos());\n\n  // Caught exception: Store result (exception) in the exception\n  // field in the JSEnv and return a failure sentinel.  Coming in here the\n  // fp will be invalid because the PushStackHandler below sets it to 0 to\n  // signal the existence of the JSEntry frame.\n  __ li(s1, ExternalReference::Create(IsolateAddressId::kExceptionAddress,\n                                      masm->isolate()));\n  __ StoreWord(a0,\n               MemOperand(s1));  // We come back from 'invoke'. result is in a0.\n  __ LoadRoot(a0, RootIndex::kException);\n  __ BranchShort(&exit);\n\n  // Invoke: Link this frame into the handler chain.\n  __ bind(&invoke);\n  __ PushStackHandler();\n  // If an exception not caught by another handler occurs, this handler\n  // returns control to the code after the bal(&invoke) above, which\n  // restores all kCalleeSaved registers (including cp and fp) to their\n  // saved values before returning a failure to C.\n  //\n  // Registers:\n  //  either\n  //   a0: root register value\n  //   a1: entry address\n  //   a2: function\n  //   a3: receiver\n  //   a4: argc\n  //   a5: argv\n  //  or\n  //   a0: root register value\n  //   a1: microtask_queue\n  //\n  // Stack:\n  // fast api call pc.\n  // fast api call fp.\n  // JS entry frame marker\n  // caller fp          |\n  // function slot      | entry frame\n  // context slot       |\n  // bad fp (0xFF...F)  |\n  // handler frame\n  // entry frame\n  // callee saved registers + ra\n  // [ O32: 4 args slots]\n  // args\n  //\n  // Invoke the function by calling through JS entry trampoline builtin and\n  // pop the faked function when we return.\n  __ CallBuiltin(entry_trampoline);\n\n  // Unlink this frame from the handler chain.\n  __ PopStackHandler();\n\n  __ bind(&exit);  // a0 holds result\n  // Check if the current stack frame is marked as the outermost JS frame.\n\n  Label non_outermost_js_2;\n  __ pop(a5);\n  __ Branch(&non_outermost_js_2, ne, a5,\n            Operand(StackFrame::OUTERMOST_JSENTRY_FRAME),\n            Label::Distance::kNear);\n  __ li(a5, js_entry_sp);\n  __ StoreWord(zero_reg, MemOperand(a5));\n  __ bind(&non_outermost_js_2);\n\n  __ Pop(s2, s3);\n  __ li(s1, ExternalReference::fast_c_call_caller_fp_address(masm->isolate()));\n  __ StoreWord(s2, MemOperand(s1, 0));\n  __ li(s1, ExternalReference::fast_c_call_caller_pc_address(masm->isolate()));\n  __ StoreWord(s3, MemOperand(s1, 0));\n  // Restore the top frame descriptors from the stack.\n  __ pop(a5);\n  __ li(a4, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,\n                                      masm->isolate()));\n  __ StoreWord(a5, MemOperand(a4));\n\n  // Reset the stack to the callee saved registers.\n  __ AddWord(sp, sp, -EntryFrameConstants::kNextExitFrameFPOffset);\n\n  // Restore callee-saved fpu registers.\n  __ MultiPopFPU(kCalleeSavedFPU);\n\n  // Restore callee saved registers from the stack.\n  __ MultiPop(kCalleeSaved | ra);\n  // Return.\n  __ Jump(ra);\n}", "name_and_para": "void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,\n                             Builtin entry_trampoline) "}], [{"name": "Builtins::Generate_ResumeGeneratorTrampoline", "content": "void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0 : the value to pass to the generator\n  //  -- x1 : the JSGeneratorObject to resume\n  //  -- lr : return address\n  // -----------------------------------\n\n  // Store input value into generator object.\n  __ StoreTaggedField(\n      x0, FieldMemOperand(x1, JSGeneratorObject::kInputOrDebugPosOffset));\n  __ RecordWriteField(x1, JSGeneratorObject::kInputOrDebugPosOffset, x0,\n                      kLRHasNotBeenSaved, SaveFPRegsMode::kIgnore);\n  // Check that x1 is still valid, RecordWrite might have clobbered it.\n  __ AssertGeneratorObject(x1);\n\n  // Load suspended function and context.\n  __ LoadTaggedField(x4,\n                     FieldMemOperand(x1, JSGeneratorObject::kFunctionOffset));\n  __ LoadTaggedField(cp, FieldMemOperand(x4, JSFunction::kContextOffset));\n\n  // Flood function if we are stepping.\n  Label prepare_step_in_if_stepping, prepare_step_in_suspended_generator;\n  Label stepping_prepared;\n  ExternalReference debug_hook =\n      ExternalReference::debug_hook_on_function_call_address(masm->isolate());\n  __ Mov(x10, debug_hook);\n  __ Ldrsb(x10, MemOperand(x10));\n  __ CompareAndBranch(x10, Operand(0), ne, &prepare_step_in_if_stepping);\n\n  // Flood function if we need to continue stepping in the suspended generator.\n  ExternalReference debug_suspended_generator =\n      ExternalReference::debug_suspended_generator_address(masm->isolate());\n  __ Mov(x10, debug_suspended_generator);\n  __ Ldr(x10, MemOperand(x10));\n  __ CompareAndBranch(x10, Operand(x1), eq,\n                      &prepare_step_in_suspended_generator);\n  __ Bind(&stepping_prepared);\n\n  // Check the stack for overflow. We are not trying to catch interruptions\n  // (i.e. debug break and preemption) here, so check the \"real stack limit\".\n  Label stack_overflow;\n  __ LoadStackLimit(x10, StackLimitKind::kRealStackLimit);\n  __ Cmp(sp, x10);\n  __ B(lo, &stack_overflow);\n\n  // Get number of arguments for generator function.\n  __ LoadTaggedField(\n      x10, FieldMemOperand(x4, JSFunction::kSharedFunctionInfoOffset));\n  __ Ldrh(w10, FieldMemOperand(\n                   x10, SharedFunctionInfo::kFormalParameterCountOffset));\n\n  __ Sub(x10, x10, kJSArgcReceiverSlots);\n  // Claim slots for arguments and receiver (rounded up to a multiple of two).\n  __ Add(x11, x10, 2);\n  __ Bic(x11, x11, 1);\n  __ Claim(x11);\n\n  // Store padding (which might be replaced by the last argument).\n  __ Sub(x11, x11, 1);\n  __ Poke(padreg, Operand(x11, LSL, kSystemPointerSizeLog2));\n\n  // Poke receiver into highest claimed slot.\n  __ LoadTaggedField(x5,\n                     FieldMemOperand(x1, JSGeneratorObject::kReceiverOffset));\n  __ Poke(x5, __ ReceiverOperand());\n\n  // ----------- S t a t e -------------\n  //  -- x1                       : the JSGeneratorObject to resume\n  //  -- x4                       : generator function\n  //  -- x10                      : argument count\n  //  -- cp                       : generator context\n  //  -- lr                       : return address\n  //  -- sp[0 .. arg count]       : claimed for receiver and args\n  // -----------------------------------\n\n  // Copy the function arguments from the generator object's register file.\n  __ LoadTaggedField(\n      x5,\n      FieldMemOperand(x1, JSGeneratorObject::kParametersAndRegistersOffset));\n  {\n    Label loop, done;\n    __ Cbz(x10, &done);\n    __ SlotAddress(x12, x10);\n    __ Add(x5, x5, Operand(x10, LSL, kTaggedSizeLog2));\n    __ Add(x5, x5, Operand(FixedArray::kHeaderSize - kHeapObjectTag));\n    __ Bind(&loop);\n    __ Sub(x10, x10, 1);\n    __ LoadTaggedField(x11, MemOperand(x5, -kTaggedSize, PreIndex));\n    __ Str(x11, MemOperand(x12, -kSystemPointerSize, PostIndex));\n    __ Cbnz(x10, &loop);\n    __ Bind(&done);\n  }\n\n  // Underlying function needs to have bytecode available.\n  if (v8_flags.debug_code) {\n    Label ok, is_baseline, is_unavailable;\n    Register sfi = x3;\n    Register bytecode = x3;\n    __ LoadTaggedField(\n        sfi, FieldMemOperand(x4, JSFunction::kSharedFunctionInfoOffset));\n    GetSharedFunctionInfoBytecodeOrBaseline(masm, sfi, bytecode, x0,\n                                            &is_baseline, &is_unavailable);\n    __ B(&ok);\n\n    __ Bind(&is_unavailable);\n    __ Abort(AbortReason::kMissingBytecodeArray);\n\n    __ Bind(&is_baseline);\n    __ IsObjectType(bytecode, x0, x0, CODE_TYPE);\n    __ Assert(eq, AbortReason::kMissingBytecodeArray);\n\n    __ Bind(&ok);\n  }\n\n  // Resume (Ignition/TurboFan) generator object.\n  {\n    __ LoadTaggedField(\n        x0, FieldMemOperand(x4, JSFunction::kSharedFunctionInfoOffset));\n    __ Ldrh(w0, FieldMemOperand(\n                    x0, SharedFunctionInfo::kFormalParameterCountOffset));\n    // We abuse new.target both to indicate that this is a resume call and to\n    // pass in the generator object.  In ordinary calls, new.target is always\n    // undefined because generator functions are non-constructable.\n    __ Mov(x3, x1);\n    __ Mov(x1, x4);\n    __ JumpJSFunction(x1);\n  }\n\n  __ Bind(&prepare_step_in_if_stepping);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    // Push hole as receiver since we do not use it for stepping.\n    __ LoadRoot(x5, RootIndex::kTheHoleValue);\n    __ Push(x1, padreg, x4, x5);\n    __ CallRuntime(Runtime::kDebugOnFunctionCall);\n    __ Pop(padreg, x1);\n    __ LoadTaggedField(x4,\n                       FieldMemOperand(x1, JSGeneratorObject::kFunctionOffset));\n  }\n  __ B(&stepping_prepared);\n\n  __ Bind(&prepare_step_in_suspended_generator);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(x1, padreg);\n    __ CallRuntime(Runtime::kDebugPrepareStepInSuspendedGenerator);\n    __ Pop(padreg, x1);\n    __ LoadTaggedField(x4,\n                       FieldMemOperand(x1, JSGeneratorObject::kFunctionOffset));\n  }\n  __ B(&stepping_prepared);\n\n  __ bind(&stack_overflow);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kThrowStackOverflow);\n    __ Unreachable();  // This should be unreachable.\n  }\n}", "name_and_para": "void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ResumeGeneratorTrampoline", "content": "void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0 : the value to pass to the generator\n  //  -- a1 : the JSGeneratorObject to resume\n  //  -- ra : return address\n  // -----------------------------------\n  // Store input value into generator object.\n  __ StoreTaggedField(\n      a0, FieldMemOperand(a1, JSGeneratorObject::kInputOrDebugPosOffset));\n  __ RecordWriteField(a1, JSGeneratorObject::kInputOrDebugPosOffset, a0,\n                      kRAHasNotBeenSaved, SaveFPRegsMode::kIgnore);\n  // Check that a1 is still valid, RecordWrite might have clobbered it.\n  __ AssertGeneratorObject(a1);\n\n  // Load suspended function and context.\n  __ LoadTaggedField(a4,\n                     FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));\n  __ LoadTaggedField(cp, FieldMemOperand(a4, JSFunction::kContextOffset));\n\n  // Flood function if we are stepping.\n  Label prepare_step_in_if_stepping, prepare_step_in_suspended_generator;\n  Label stepping_prepared;\n  ExternalReference debug_hook =\n      ExternalReference::debug_hook_on_function_call_address(masm->isolate());\n  __ li(a5, debug_hook);\n  __ Lb(a5, MemOperand(a5));\n  __ Branch(&prepare_step_in_if_stepping, ne, a5, Operand(zero_reg));\n\n  // Flood function if we need to continue stepping in the suspended generator.\n  ExternalReference debug_suspended_generator =\n      ExternalReference::debug_suspended_generator_address(masm->isolate());\n  __ li(a5, debug_suspended_generator);\n  __ LoadWord(a5, MemOperand(a5));\n  __ Branch(&prepare_step_in_suspended_generator, eq, a1, Operand(a5));\n  __ bind(&stepping_prepared);\n\n  // Check the stack for overflow. We are not trying to catch interruptions\n  // (i.e. debug break and preemption) here, so check the \"real stack limit\".\n  Label stack_overflow;\n  __ LoadStackLimit(kScratchReg,\n                    MacroAssembler::StackLimitKind::kRealStackLimit);\n  __ Branch(&stack_overflow, Uless, sp, Operand(kScratchReg));\n\n  // ----------- S t a t e -------------\n  //  -- a1    : the JSGeneratorObject to resume\n  //  -- a4    : generator function\n  //  -- cp    : generator context\n  //  -- ra    : return address\n  // -----------------------------------\n\n  // Push holes for arguments to generator function. Since the parser forced\n  // context allocation for any variables in generators, the actual argument\n  // values have already been copied into the context and these dummy values\n  // will never be used.\n  __ LoadTaggedField(\n      a3, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));\n  __ Lhu(a3,\n         FieldMemOperand(a3, SharedFunctionInfo::kFormalParameterCountOffset));\n  __ SubWord(a3, a3, Operand(kJSArgcReceiverSlots));\n  __ LoadTaggedField(\n      t1,\n      FieldMemOperand(a1, JSGeneratorObject::kParametersAndRegistersOffset));\n  {\n    Label done_loop, loop;\n    __ bind(&loop);\n    __ SubWord(a3, a3, Operand(1));\n    __ Branch(&done_loop, lt, a3, Operand(zero_reg), Label::Distance::kNear);\n    __ CalcScaledAddress(kScratchReg, t1, a3, kTaggedSizeLog2);\n    __ LoadTaggedField(kScratchReg,\n                       FieldMemOperand(kScratchReg, FixedArray::kHeaderSize));\n    __ Push(kScratchReg);\n    __ Branch(&loop);\n    __ bind(&done_loop);\n    // Push receiver.\n    __ LoadTaggedField(kScratchReg,\n                       FieldMemOperand(a1, JSGeneratorObject::kReceiverOffset));\n    __ Push(kScratchReg);\n  }\n\n  // Underlying function needs to have bytecode available.\n  if (v8_flags.debug_code) {\n    Label is_baseline;\n    Register sfi = a3;\n    Register bytecode = a3;\n    __ LoadTaggedField(\n        sfi, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));\n    GetSharedFunctionInfoBytecodeOrBaseline(masm, sfi, bytecode, t5,\n                                            &is_baseline);\n    __ GetObjectType(a3, a3, bytecode);\n    __ Assert(eq, AbortReason::kMissingBytecodeArray, bytecode,\n              Operand(BYTECODE_ARRAY_TYPE));\n    __ bind(&is_baseline);\n  }\n\n  // Resume (Ignition/TurboFan) generator object.\n  {\n    __ LoadTaggedField(\n        a0, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));\n    __ Lhu(a0, FieldMemOperand(\n                   a0, SharedFunctionInfo::kFormalParameterCountOffset));\n    // We abuse new.target both to indicate that this is a resume call and to\n    // pass in the generator object.  In ordinary calls, new.target is always\n    // undefined because generator functions are non-constructable.\n    __ Move(a3, a1);\n    __ Move(a1, a4);\n    __ JumpJSFunction(a1);\n  }\n\n  __ bind(&prepare_step_in_if_stepping);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(a1, a4);\n    // Push hole as receiver since we do not use it for stepping.\n    __ PushRoot(RootIndex::kTheHoleValue);\n    __ CallRuntime(Runtime::kDebugOnFunctionCall);\n    __ Pop(a1);\n  }\n  __ LoadTaggedField(a4,\n                     FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));\n  __ Branch(&stepping_prepared);\n\n  __ bind(&prepare_step_in_suspended_generator);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ Push(a1);\n    __ CallRuntime(Runtime::kDebugPrepareStepInSuspendedGenerator);\n    __ Pop(a1);\n  }\n  __ LoadTaggedField(a4,\n                     FieldMemOperand(a1, JSGeneratorObject::kFunctionOffset));\n  __ Branch(&stepping_prepared);\n\n  __ bind(&stack_overflow);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kThrowStackOverflow);\n    __ break_(0xCC);  // This should be unreachable.\n  }\n}", "name_and_para": "void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) "}], [{"name": "GetSharedFunctionInfoBytecodeOrBaseline", "content": "static void GetSharedFunctionInfoBytecodeOrBaseline(\n    MacroAssembler* masm, Register sfi, Register bytecode, Register scratch1,\n    Label* is_baseline, Label* is_unavailable) {\n  DCHECK(!AreAliased(bytecode, scratch1));\n  ASM_CODE_COMMENT(masm);\n  Label done;\n\n  Register data = bytecode;\n#ifdef V8_ENABLE_SANDBOX\n  // In this case, the bytecode array must be referenced via a trusted pointer.\n  // Loading it from the tagged function_data field would not be safe.\n  __ Ldr(scratch1.W(),\n         FieldMemOperand(sfi, SharedFunctionInfo::kTrustedFunctionDataOffset));\n\n  __ Cbz(scratch1.W(), is_unavailable);\n  __ ResolveIndirectPointerHandle(data, scratch1, kUnknownIndirectPointerTag);\n#else\n  __ LoadTaggedField(\n      data, FieldMemOperand(sfi, SharedFunctionInfo::kFunctionDataOffset));\n#endif  // V8_ENABLE_SANDBOX\n  if (V8_JITLESS_BOOL) {\n    __ IsObjectType(data, scratch1, scratch1, INTERPRETER_DATA_TYPE);\n    __ B(ne, &done);\n  } else {\n    CheckSharedFunctionInfoBytecodeOrBaseline(masm, data, scratch1, is_baseline,\n                                              &done);\n  }\n\n  __ LoadProtectedPointerField(\n      bytecode, FieldMemOperand(data, InterpreterData::kBytecodeArrayOffset));\n\n  __ Bind(&done);\n  __ IsObjectType(bytecode, scratch1, scratch1, BYTECODE_ARRAY_TYPE);\n  __ B(ne, is_unavailable);\n}", "name_and_para": "static void GetSharedFunctionInfoBytecodeOrBaseline(\n    MacroAssembler* masm, Register sfi, Register bytecode, Register scratch1,\n    Label* is_baseline, Label* is_unavailable) "}, {"name": "GetSharedFunctionInfoBytecodeOrBaseline", "content": "static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,\n                                                    Register sfi,\n                                                    Register bytecode,\n                                                    Register scratch1,\n                                                    Label* is_baseline) {\n  DCHECK(!AreAliased(bytecode, scratch1));\n  ASM_CODE_COMMENT(masm);\n  Label done;\n\n  Register data = bytecode;\n  GetSharedFunctionInfoData(masm, data, sfi, scratch1);\n  __ GetObjectType(data, scratch1, scratch1);\n\n#ifndef V8_JITLESS\n  if (v8_flags.debug_code) {\n    Label not_baseline;\n    __ Branch(&not_baseline, ne, scratch1, Operand(CODE_TYPE));\n    AssertCodeIsBaseline(masm, data, scratch1);\n    __ Branch(is_baseline);\n    __ bind(&not_baseline);\n  } else {\n    __ Branch(is_baseline, eq, scratch1, Operand(CODE_TYPE));\n  }\n#endif  // !V8_JITLESS\n\n  __ Branch(&done, ne, scratch1, Operand(INTERPRETER_DATA_TYPE));\n  __ LoadProtectedPointerField(\n      bytecode, FieldMemOperand(data, InterpreterData::kBytecodeArrayOffset));\n\n  __ bind(&done);\n}", "name_and_para": "static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,\n                                                    Register sfi,\n                                                    Register bytecode,\n                                                    Register scratch1,\n                                                    Label* is_baseline) "}], [{"name": "GetSharedFunctionInfoData", "content": "static void GetSharedFunctionInfoData(MacroAssembler* masm, Register data,\n                                      Register sfi, Register scratch) {\n#ifdef V8_ENABLE_SANDBOX\n  DCHECK(!AreAliased(data, scratch));\n  DCHECK(!AreAliased(sfi, scratch));\n  // Use trusted_function_data if non-empy, otherwise the regular function_data.\n  Label use_tagged_field, done;\n  __ Ldr(scratch.W(),\n         FieldMemOperand(sfi, SharedFunctionInfo::kTrustedFunctionDataOffset));\n\n  __ Cbz(scratch.W(), &use_tagged_field);\n  __ ResolveIndirectPointerHandle(data, scratch, kUnknownIndirectPointerTag);\n  __ B(&done);\n\n  __ Bind(&use_tagged_field);\n  __ LoadTaggedField(\n      data, FieldMemOperand(sfi, SharedFunctionInfo::kFunctionDataOffset));\n\n  __ Bind(&done);\n#else\n  __ LoadTaggedField(\n      data, FieldMemOperand(sfi, SharedFunctionInfo::kFunctionDataOffset));\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "static void GetSharedFunctionInfoData(MacroAssembler* masm, Register data,\n                                      Register sfi, Register scratch) "}, {"name": "GetSharedFunctionInfoData", "content": "static void GetSharedFunctionInfoData(MacroAssembler* masm, Register data,\n                                      Register sfi, Register scratch) {\n  ASM_CODE_COMMENT(masm);\n#ifdef V8_ENABLE_SANDBOX\n  DCHECK(!AreAliased(data, scratch));\n  DCHECK(!AreAliased(sfi, scratch));\n  // Use trusted_function_data if non-empy, otherwise the regular function_data.\n  Label use_tagged_field, done;\n  __ Lwu(scratch,\n         FieldMemOperand(sfi, SharedFunctionInfo::kTrustedFunctionDataOffset));\n  __ Branch(&use_tagged_field, eq, scratch, Operand(zero_reg));\n  __ ResolveIndirectPointerHandle(data, scratch, kUnknownIndirectPointerTag);\n  __ Branch(&done);\n  __ bind(&use_tagged_field);\n  __ LoadTaggedField(\n      data, FieldMemOperand(sfi, SharedFunctionInfo::kFunctionDataOffset));\n  __ bind(&done);\n#else\n  __ LoadTaggedField(\n      data, FieldMemOperand(sfi, SharedFunctionInfo::kFunctionDataOffset));\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "static void GetSharedFunctionInfoData(MacroAssembler* masm, Register data,\n                                      Register sfi, Register scratch) "}], [{"name": "AssertCodeIsBaseline", "content": "static void AssertCodeIsBaseline(MacroAssembler* masm, Register code,\n                                 Register scratch) {\n  DCHECK(!AreAliased(code, scratch));\n  return AssertCodeIsBaselineAllowClobber(masm, code, scratch);\n}", "name_and_para": "static void AssertCodeIsBaseline(MacroAssembler* masm, Register code,\n                                 Register scratch) "}, {"name": "AssertCodeIsBaseline", "content": "static void AssertCodeIsBaseline(MacroAssembler* masm, Register code,\n                                 Register scratch) {\n  DCHECK(!AreAliased(code, scratch));\n  // Verify that the code kind is baseline code via the CodeKind.\n  __ LoadWord(scratch, FieldMemOperand(code, Code::kFlagsOffset));\n  __ DecodeField<Code::KindField>(scratch);\n  __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,\n            Operand(static_cast<int64_t>(CodeKind::BASELINE)));\n}", "name_and_para": "static void AssertCodeIsBaseline(MacroAssembler* masm, Register code,\n                                 Register scratch) "}], [{"name": "Builtins::Generate_ConstructedNonConstructable", "content": "void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) {\n  FrameScope scope(masm, StackFrame::INTERNAL);\n  __ PushArgument(x1);\n  __ CallRuntime(Runtime::kThrowConstructedNonConstructable);\n  __ Unreachable();\n}", "name_and_para": "void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) "}, {"name": "Builtins::Generate_ConstructedNonConstructable", "content": "void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) {\n  FrameScope scope(masm, StackFrame::INTERNAL);\n  __ Push(a1);\n  __ CallRuntime(Runtime::kThrowConstructedNonConstructable);\n}", "name_and_para": "void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSBuiltinsConstructStub", "content": "void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {\n  Generate_JSBuiltinsConstructStubHelper(masm);\n}", "name_and_para": "void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSBuiltinsConstructStub", "content": "void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {\n  Generate_JSBuiltinsConstructStubHelper(masm);\n}", "name_and_para": "void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_JSConstructStubGeneric", "content": "void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0     : number of arguments\n  //  -- x1     : constructor function\n  //  -- x3     : new target\n  //  -- lr     : return address\n  //  -- cp     : context pointer\n  //  -- sp[...]: constructor arguments\n  // -----------------------------------\n\n  ASM_LOCATION(\"Builtins::Generate_JSConstructStubGeneric\");\n\n  FrameScope scope(masm, StackFrame::MANUAL);\n  // Enter a construct frame.\n  __ EnterFrame(StackFrame::CONSTRUCT);\n  Label post_instantiation_deopt_entry, not_create_implicit_receiver;\n\n  if (v8_flags.debug_code) {\n    // Check that FrameScope pushed the context on to the stack already.\n    __ Peek(x2, 0);\n    __ Cmp(x2, cp);\n    __ Check(eq, AbortReason::kUnexpectedValue);\n  }\n\n  // Preserve the incoming parameters on the stack.\n  __ SmiTag(x0);\n  __ Push(x0, x1, padreg, x3);\n\n  // ----------- S t a t e -------------\n  //  --        sp[0*kSystemPointerSize]: new target\n  //  --        sp[1*kSystemPointerSize]: padding\n  //  -- x1 and sp[2*kSystemPointerSize]: constructor function\n  //  --        sp[3*kSystemPointerSize]: number of arguments (tagged)\n  //  --        sp[4*kSystemPointerSize]: context (pushed by FrameScope)\n  // -----------------------------------\n\n  __ LoadTaggedField(\n      x4, FieldMemOperand(x1, JSFunction::kSharedFunctionInfoOffset));\n  __ Ldr(w4, FieldMemOperand(x4, SharedFunctionInfo::kFlagsOffset));\n  __ DecodeField<SharedFunctionInfo::FunctionKindBits>(w4);\n  __ JumpIfIsInRange(\n      w4, static_cast<uint32_t>(FunctionKind::kDefaultDerivedConstructor),\n      static_cast<uint32_t>(FunctionKind::kDerivedConstructor),\n      &not_create_implicit_receiver);\n\n  // If not derived class constructor: Allocate the new receiver object.\n  __ CallBuiltin(Builtin::kFastNewObject);\n\n  __ B(&post_instantiation_deopt_entry);\n\n  // Else: use TheHoleValue as receiver for constructor call\n  __ Bind(&not_create_implicit_receiver);\n  __ LoadRoot(x0, RootIndex::kTheHoleValue);\n\n  // ----------- S t a t e -------------\n  //  --                                x0: receiver\n  //  -- Slot 4 / sp[0*kSystemPointerSize]: new target\n  //  -- Slot 3 / sp[1*kSystemPointerSize]: padding\n  //  -- Slot 2 / sp[2*kSystemPointerSize]: constructor function\n  //  -- Slot 1 / sp[3*kSystemPointerSize]: number of arguments (tagged)\n  //  -- Slot 0 / sp[4*kSystemPointerSize]: context\n  // -----------------------------------\n  // Deoptimizer enters here.\n  masm->isolate()->heap()->SetConstructStubCreateDeoptPCOffset(\n      masm->pc_offset());\n\n  __ Bind(&post_instantiation_deopt_entry);\n\n  // Restore new target from the top of the stack.\n  __ Peek(x3, 0 * kSystemPointerSize);\n\n  // Restore constructor function and argument count.\n  __ Ldr(x1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));\n  __ SmiUntag(x12, MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n\n  // Copy arguments to the expression stack. The called function pops the\n  // receiver along with its arguments, so we need an extra receiver on the\n  // stack, in case we have to return it later.\n\n  // Overwrite the new target with a receiver.\n  __ Poke(x0, 0);\n\n  // Push two further copies of the receiver. One will be popped by the called\n  // function. The second acts as padding if the number of arguments plus\n  // receiver is odd - pushing receiver twice avoids branching. It also means\n  // that we don't have to handle the even and odd cases specially on\n  // InvokeFunction's return, as top of stack will be the receiver in either\n  // case.\n  __ Push(x0, x0);\n\n  // ----------- S t a t e -------------\n  //  --                              x3: new target\n  //  --                             x12: number of arguments (untagged)\n  //  --        sp[0*kSystemPointerSize]: implicit receiver (overwrite if argc\n  //  odd)\n  //  --        sp[1*kSystemPointerSize]: implicit receiver\n  //  --        sp[2*kSystemPointerSize]: implicit receiver\n  //  --        sp[3*kSystemPointerSize]: padding\n  //  -- x1 and sp[4*kSystemPointerSize]: constructor function\n  //  --        sp[5*kSystemPointerSize]: number of arguments (tagged)\n  //  --        sp[6*kSystemPointerSize]: context\n  // -----------------------------------\n\n  // Round the number of arguments down to the next even number, and claim\n  // slots for the arguments. If the number of arguments was odd, the last\n  // argument will overwrite one of the receivers pushed above.\n  Register argc_without_receiver = x11;\n  __ Sub(argc_without_receiver, x12, kJSArgcReceiverSlots);\n  __ Bic(x10, x12, 1);\n\n  // Check if we have enough stack space to push all arguments.\n  Label stack_overflow;\n  __ StackOverflowCheck(x10, &stack_overflow);\n  __ Claim(x10);\n\n  // TODO(victorgomes): When the arguments adaptor is completely removed, we\n  // should get the formal parameter count and copy the arguments in its\n  // correct position (including any undefined), instead of delaying this to\n  // InvokeFunction.\n\n  // Copy the arguments.\n  {\n    Register count = x2;\n    Register dst = x10;\n    Register src = x11;\n    __ Mov(count, argc_without_receiver);\n    __ Poke(x0, 0);          // Add the receiver.\n    __ SlotAddress(dst, 1);  // Skip receiver.\n    __ Add(src, fp,\n           StandardFrameConstants::kCallerSPOffset + kSystemPointerSize);\n    __ CopyDoubleWords(dst, src, count);\n  }\n\n  // Call the function.\n  __ Mov(x0, x12);\n  __ InvokeFunctionWithNewTarget(x1, x3, x0, InvokeType::kCall);\n\n  // If the result is an object (in the ECMA sense), we should get rid\n  // of the receiver and use the result; see ECMA-262 section 13.2.2-7\n  // on page 74.\n  Label use_receiver, do_throw, leave_and_return, check_receiver;\n\n  // If the result is undefined, we jump out to using the implicit receiver.\n  __ CompareRoot(x0, RootIndex::kUndefinedValue);\n  __ B(ne, &check_receiver);\n\n  // Throw away the result of the constructor invocation and use the\n  // on-stack receiver as the result.\n  __ Bind(&use_receiver);\n  __ Peek(x0, 0 * kSystemPointerSize);\n  __ CompareRoot(x0, RootIndex::kTheHoleValue);\n  __ B(eq, &do_throw);\n\n  __ Bind(&leave_and_return);\n  // Restore smi-tagged arguments count from the frame.\n  __ SmiUntag(x1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n  // Leave construct frame.\n  __ LeaveFrame(StackFrame::CONSTRUCT);\n  // Remove caller arguments from the stack and return.\n  __ DropArguments(x1, MacroAssembler::kCountIncludesReceiver);\n  __ Ret();\n\n  // Otherwise we do a smi check and fall through to check if the return value\n  // is a valid receiver.\n  __ bind(&check_receiver);\n\n  // If the result is a smi, it is *not* an object in the ECMA sense.\n  __ JumpIfSmi(x0, &use_receiver);\n\n  // Check if the type of the result is not an object in the ECMA sense.\n  __ JumpIfJSAnyIsNotPrimitive(x0, x4, &leave_and_return);\n  __ B(&use_receiver);\n\n  __ Bind(&do_throw);\n  // Restore the context from the frame.\n  __ Ldr(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);\n  __ Unreachable();\n\n  __ Bind(&stack_overflow);\n  // Restore the context from the frame.\n  __ Ldr(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowStackOverflow);\n  __ Unreachable();\n}", "name_and_para": "void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) "}, {"name": "Builtins::Generate_JSConstructStubGeneric", "content": "void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  --      a0: number of arguments (untagged)\n  //  --      a1: constructor function\n  //  --      a3: new target\n  //  --      cp: context\n  //  --      ra: return address\n  //  -- sp[...]: constructor arguments\n  // -----------------------------------\n  UseScratchRegisterScope temps(masm);\n  temps.Include(t0, t1);\n  // Enter a construct frame.\n  FrameScope scope(masm, StackFrame::MANUAL);\n  Label post_instantiation_deopt_entry, not_create_implicit_receiver;\n  __ EnterFrame(StackFrame::CONSTRUCT);\n\n  // Preserve the incoming parameters on the stack.\n  __ SmiTag(a0);\n  __ Push(cp, a0, a1);\n  __ PushRoot(RootIndex::kUndefinedValue);\n  __ Push(a3);\n\n  // ----------- S t a t e -------------\n  //  --        sp[0*kSystemPointerSize]: new target\n  //  --        sp[1*kSystemPointerSize]: padding\n  //  -- a1 and sp[2*kSystemPointerSize]: constructor function\n  //  --        sp[3*kSystemPointerSize]: number of arguments (tagged)\n  //  --        sp[4*kSystemPointerSize]: context\n  // -----------------------------------\n  {\n    UseScratchRegisterScope temps(masm);\n    Register func_info = temps.Acquire();\n    __ LoadTaggedField(\n        func_info, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));\n    __ Load32U(func_info,\n               FieldMemOperand(func_info, SharedFunctionInfo::kFlagsOffset));\n    __ DecodeField<SharedFunctionInfo::FunctionKindBits>(func_info);\n    __ JumpIfIsInRange(\n        func_info,\n        static_cast<uint32_t>(FunctionKind::kDefaultDerivedConstructor),\n        static_cast<uint32_t>(FunctionKind::kDerivedConstructor),\n        &not_create_implicit_receiver);\n    // If not derived class constructor: Allocate the new receiver object.\n    __ CallBuiltin(Builtin::kFastNewObject);\n    __ BranchShort(&post_instantiation_deopt_entry);\n\n    // Else: use TheHoleValue as receiver for constructor call\n    __ bind(&not_create_implicit_receiver);\n    __ LoadRoot(a0, RootIndex::kTheHoleValue);\n  }\n  // ----------- S t a t e -------------\n  //  --                          a0: receiver\n  //  -- Slot 4 / sp[0*kSystemPointerSize]: new target\n  //  -- Slot 3 / sp[1*kSystemPointerSize]: padding\n  //  -- Slot 2 / sp[2*kSystemPointerSize]: constructor function\n  //  -- Slot 1 / sp[3*kSystemPointerSize]: number of arguments (tagged)\n  //  -- Slot 0 / sp[4*kSystemPointerSize]: context\n  // -----------------------------------\n  // Deoptimizer enters here.\n  masm->isolate()->heap()->SetConstructStubCreateDeoptPCOffset(\n      masm->pc_offset());\n  __ bind(&post_instantiation_deopt_entry);\n\n  // Restore new target.\n  __ Pop(a3);\n\n  // Push the allocated receiver to the stack.\n  __ Push(a0);\n\n  // We need two copies because we may have to return the original one\n  // and the calling conventions dictate that the called function pops the\n  // receiver. The second copy is pushed after the arguments, we saved in a6\n  // since a0 will store the return value of callRuntime.\n  __ Move(a6, a0);\n\n  // Set up pointer to first argument (skip receiver)..\n  __ AddWord(\n      t2, fp,\n      Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));\n\n  // ----------- S t a t e -------------\n  //  --                 a3: new target\n  //  -- sp[0*kSystemPointerSize]: implicit receiver\n  //  -- sp[1*kSystemPointerSize]: implicit receiver\n  //  -- sp[2*kSystemPointerSize]: padding\n  //  -- sp[3*kSystemPointerSize]: constructor function\n  //  -- sp[4*kSystemPointerSize]: number of arguments (tagged)\n  //  -- sp[5*kSystemPointerSize]: context\n  // -----------------------------------\n\n  // Restore constructor function and argument count.\n  __ LoadWord(a1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));\n  __ LoadWord(a0, MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n  __ SmiUntag(a0);\n\n  Label stack_overflow;\n  {\n    UseScratchRegisterScope temps(masm);\n    __ StackOverflowCheck(a0, temps.Acquire(), temps.Acquire(),\n                          &stack_overflow);\n  }\n  // TODO(victorgomes): When the arguments adaptor is completely removed, we\n  // should get the formal parameter count and copy the arguments in its\n  // correct position (including any undefined), instead of delaying this to\n  // InvokeFunction.\n\n  // Copy arguments and receiver to the expression stack.\n  // t2: Pointer to start of argument.\n  // a0: Number of arguments.\n  {\n    UseScratchRegisterScope temps(masm);\n    Generate_PushArguments(masm, t2, a0, temps.Acquire(), temps.Acquire(),\n                           ArgumentsElementType::kRaw);\n  }\n  // We need two copies because we may have to return the original one\n  // and the calling conventions dictate that the called function pops the\n  // receiver. The second copy is pushed after the arguments,\n  __ Push(a6);\n\n  // Call the function.\n  __ InvokeFunctionWithNewTarget(a1, a3, a0, InvokeType::kCall);\n\n  // If the result is an object (in the ECMA sense), we should get rid\n  // of the receiver and use the result; see ECMA-262 section 13.2.2-7\n  // on page 74.\n  Label use_receiver, do_throw, leave_and_return, check_receiver;\n\n  // If the result is undefined, we jump out to using the implicit receiver.\n  __ JumpIfNotRoot(a0, RootIndex::kUndefinedValue, &check_receiver);\n\n  // Otherwise we do a smi check and fall through to check if the return value\n  // is a valid receiver.\n\n  // Throw away the result of the constructor invocation and use the\n  // on-stack receiver as the result.\n  __ bind(&use_receiver);\n  __ LoadWord(a0, MemOperand(sp, 0 * kSystemPointerSize));\n  __ JumpIfRoot(a0, RootIndex::kTheHoleValue, &do_throw);\n\n  __ bind(&leave_and_return);\n  // Restore smi-tagged arguments count from the frame.\n  __ LoadWord(a1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n  // Leave construct frame.\n  __ LeaveFrame(StackFrame::CONSTRUCT);\n\n  // Remove caller arguments from the stack and return.\n  __ DropArguments(a1, MacroAssembler::kCountIsSmi,\n                   MacroAssembler::kCountIncludesReceiver, a4);\n  __ Ret();\n\n  __ bind(&check_receiver);\n  __ JumpIfSmi(a0, &use_receiver);\n\n  // If the type of the result (stored in its map) is less than\n  // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.\n  {\n    UseScratchRegisterScope temps(masm);\n    Register map = temps.Acquire(), type = temps.Acquire();\n    __ GetObjectType(a0, map, type);\n\n    static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);\n    __ Branch(&leave_and_return, greater_equal, type,\n              Operand(FIRST_JS_RECEIVER_TYPE));\n    __ Branch(&use_receiver);\n  }\n  __ bind(&do_throw);\n  // Restore the context from the frame.\n  __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);\n  __ break_(0xCC);\n\n  __ bind(&stack_overflow);\n  // Restore the context from the frame.\n  __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n  __ CallRuntime(Runtime::kThrowStackOverflow);\n  __ break_(0xCC);\n}", "name_and_para": "void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) "}], [{"name": "Generate_JSBuiltinsConstructStubHelper", "content": "void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- x0     : number of arguments\n  //  -- x1     : constructor function\n  //  -- x3     : new target\n  //  -- cp     : context\n  //  -- lr     : return address\n  //  -- sp[...]: constructor arguments\n  // -----------------------------------\n\n  ASM_LOCATION(\"Builtins::Generate_JSConstructStubHelper\");\n  Label stack_overflow;\n\n  __ StackOverflowCheck(x0, &stack_overflow);\n\n  // Enter a construct frame.\n  {\n    FrameScope scope(masm, StackFrame::CONSTRUCT);\n    Label already_aligned;\n    Register argc = x0;\n\n    if (v8_flags.debug_code) {\n      // Check that FrameScope pushed the context on to the stack already.\n      __ Peek(x2, 0);\n      __ Cmp(x2, cp);\n      __ Check(eq, AbortReason::kUnexpectedValue);\n    }\n\n    // Push number of arguments.\n    __ SmiTag(x11, argc);\n    __ Push(x11, padreg);\n\n    // Round up to maintain alignment.\n    Register slot_count = x2;\n    Register slot_count_without_rounding = x12;\n    __ Add(slot_count_without_rounding, argc, 1);\n    __ Bic(slot_count, slot_count_without_rounding, 1);\n    __ Claim(slot_count);\n\n    // Preserve the incoming parameters on the stack.\n    __ LoadRoot(x4, RootIndex::kTheHoleValue);\n\n    // Compute a pointer to the slot immediately above the location on the\n    // stack to which arguments will be later copied.\n    __ SlotAddress(x2, argc);\n\n    // Store padding, if needed.\n    __ Tbnz(slot_count_without_rounding, 0, &already_aligned);\n    __ Str(padreg, MemOperand(x2));\n    __ Bind(&already_aligned);\n\n    // TODO(victorgomes): When the arguments adaptor is completely removed, we\n    // should get the formal parameter count and copy the arguments in its\n    // correct position (including any undefined), instead of delaying this to\n    // InvokeFunction.\n\n    // Copy arguments to the expression stack.\n    {\n      Register count = x2;\n      Register dst = x10;\n      Register src = x11;\n      __ SlotAddress(dst, 0);\n      // Poke the hole (receiver).\n      __ Str(x4, MemOperand(dst));\n      __ Add(dst, dst, kSystemPointerSize);  // Skip receiver.\n      __ Add(src, fp,\n             StandardFrameConstants::kCallerSPOffset +\n                 kSystemPointerSize);  // Skip receiver.\n      __ Sub(count, argc, kJSArgcReceiverSlots);\n      __ CopyDoubleWords(dst, src, count);\n    }\n\n    // ----------- S t a t e -------------\n    //  --                           x0: number of arguments (untagged)\n    //  --                           x1: constructor function\n    //  --                           x3: new target\n    // If argc is odd:\n    //  --     sp[0*kSystemPointerSize]: the hole (receiver)\n    //  --     sp[1*kSystemPointerSize]: argument 1\n    //  --             ...\n    //  -- sp[(n-1)*kSystemPointerSize]: argument (n - 1)\n    //  -- sp[(n+0)*kSystemPointerSize]: argument n\n    //  -- sp[(n+1)*kSystemPointerSize]: padding\n    //  -- sp[(n+2)*kSystemPointerSize]: padding\n    //  -- sp[(n+3)*kSystemPointerSize]: number of arguments (tagged)\n    //  -- sp[(n+4)*kSystemPointerSize]: context (pushed by FrameScope)\n    // If argc is even:\n    //  --     sp[0*kSystemPointerSize]: the hole (receiver)\n    //  --     sp[1*kSystemPointerSize]: argument 1\n    //  --             ...\n    //  -- sp[(n-1)*kSystemPointerSize]: argument (n - 1)\n    //  -- sp[(n+0)*kSystemPointerSize]: argument n\n    //  -- sp[(n+1)*kSystemPointerSize]: padding\n    //  -- sp[(n+2)*kSystemPointerSize]: number of arguments (tagged)\n    //  -- sp[(n+3)*kSystemPointerSize]: context (pushed by FrameScope)\n    // -----------------------------------\n\n    // Call the function.\n    __ InvokeFunctionWithNewTarget(x1, x3, argc, InvokeType::kCall);\n\n    // Restore the context from the frame.\n    __ Ldr(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n    // Restore smi-tagged arguments count from the frame. Use fp relative\n    // addressing to avoid the circular dependency between padding existence and\n    // argc parity.\n    __ SmiUntag(x1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n    // Leave construct frame.\n  }\n\n  // Remove caller arguments from the stack and return.\n  __ DropArguments(x1, MacroAssembler::kCountIncludesReceiver);\n  __ Ret();\n\n  __ Bind(&stack_overflow);\n  {\n    FrameScope scope(masm, StackFrame::INTERNAL);\n    __ CallRuntime(Runtime::kThrowStackOverflow);\n    __ Unreachable();\n  }\n}", "name_and_para": "void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) "}, {"name": "Generate_JSBuiltinsConstructStubHelper", "content": "void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {\n  // ----------- S t a t e -------------\n  //  -- a0     : number of arguments\n  //  -- a1     : constructor function\n  //  -- a3     : new target\n  //  -- cp     : context\n  //  -- ra     : return address\n  //  -- sp[...]: constructor arguments\n  // -----------------------------------\n\n  // Enter a construct frame.\n  {\n    FrameScope scope(masm, StackFrame::CONSTRUCT);\n\n    // Preserve the incoming parameters on the stack.\n    __ SmiTag(a0);\n    __ Push(cp, a0);\n    __ SmiUntag(a0);\n\n    // Set up pointer to first argument (skip receiver).\n    __ AddWord(\n        t2, fp,\n        Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));\n    // t2: Pointer to start of arguments.\n    // a0: Number of arguments.\n    {\n      UseScratchRegisterScope temps(masm);\n      temps.Include(t0);\n      Generate_PushArguments(masm, t2, a0, temps.Acquire(), temps.Acquire(),\n                             ArgumentsElementType::kRaw);\n    }\n    // The receiver for the builtin/api call.\n    __ PushRoot(RootIndex::kTheHoleValue);\n\n    // Call the function.\n    // a0: number of arguments (untagged)\n    // a1: constructor function\n    // a3: new target\n    __ InvokeFunctionWithNewTarget(a1, a3, a0, InvokeType::kCall);\n\n    // Restore context from the frame.\n    __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));\n    // Restore smi-tagged arguments count from the frame.\n    __ LoadWord(kScratchReg,\n                MemOperand(fp, ConstructFrameConstants::kLengthOffset));\n    // Leave construct frame.\n  }\n\n  // Remove caller arguments from the stack and return.\n  __ DropArguments(kScratchReg, MacroAssembler::kCountIsSmi,\n                   MacroAssembler::kCountIncludesReceiver, kScratchReg);\n  __ Ret();\n}", "name_and_para": "void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) "}], [{"name": "Builtins::Generate_Adaptor", "content": "void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) {\n  __ CodeEntry();\n\n  __ Mov(kJavaScriptCallExtraArg1Register, ExternalReference::Create(address));\n  __ TailCallBuiltin(Builtin::kAdaptorWithBuiltinExitFrame);\n}", "name_and_para": "void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) "}, {"name": "Builtins::Generate_Adaptor", "content": "void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) {\n  ASM_CODE_COMMENT(masm);\n  __ li(kJavaScriptCallExtraArg1Register, ExternalReference::Create(address));\n  __ TailCallBuiltin(Builtin::kAdaptorWithBuiltinExitFrame);\n}", "name_and_para": "void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) "}]]], [["./v8/src/codegen/riscv/constants-riscv.h", "./v8/src/codegen/arm64/constants-arm64.h"], -1, -1, []], [["./v8/src/codegen/riscv/assembler-riscv-inl.h", "./v8/src/codegen/arm64/assembler-arm64-inl.h"], 0.16312056737588654, 0.8518518518518519, [[{"name": "EnsureSpace::EnsureSpace", "content": "EnsureSpace::EnsureSpace(Assembler* assembler) : block_pools_scope_(assembler) {\n  assembler->CheckBufferSpace();\n}", "name_and_para": "EnsureSpace::EnsureSpace(Assembler* assembler) : block_pools_scope_(assembler) "}, {"name": "EnsureSpace::EnsureSpace", "content": "EnsureSpace::EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }", "name_and_para": "EnsureSpace::EnsureSpace(Assembler* assembler) "}], [{"name": "RelocInfo::target_off_heap_target", "content": "Address RelocInfo::target_off_heap_target() {\n  DCHECK(IsOffHeapTarget(rmode_));\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_off_heap_target() "}, {"name": "RelocInfo::target_off_heap_target", "content": "Address RelocInfo::target_off_heap_target() {\n  DCHECK(IsOffHeapTarget(rmode_));\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_off_heap_target() "}], [{"name": "RelocInfo::target_builtin_at", "content": "Builtin RelocInfo::target_builtin_at(Assembler* origin) {\n  DCHECK(IsNearBuiltinEntry(rmode_));\n  return Assembler::target_builtin_at(pc_);\n}", "name_and_para": "Builtin RelocInfo::target_builtin_at(Assembler* origin) "}, {"name": "RelocInfo::target_builtin_at", "content": "Builtin RelocInfo::target_builtin_at(Assembler* origin) {\n  DCHECK(IsNearBuiltinEntry(rmode_));\n  return Assembler::target_builtin_at(pc_);\n}", "name_and_para": "Builtin RelocInfo::target_builtin_at(Assembler* origin) "}], [{"name": "RelocInfo::target_internal_reference_address", "content": "Address RelocInfo::target_internal_reference_address() {\n  DCHECK(rmode_ == INTERNAL_REFERENCE);\n  return pc_;\n}", "name_and_para": "Address RelocInfo::target_internal_reference_address() "}, {"name": "RelocInfo::target_internal_reference_address", "content": "Address RelocInfo::target_internal_reference_address() {\n  DCHECK(IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_));\n  return pc_;\n}", "name_and_para": "Address RelocInfo::target_internal_reference_address() "}], [{"name": "RelocInfo::target_internal_reference", "content": "Address RelocInfo::target_internal_reference() {\n  DCHECK(rmode_ == INTERNAL_REFERENCE);\n  return ReadUnalignedValue<Address>(pc_);\n}", "name_and_para": "Address RelocInfo::target_internal_reference() "}, {"name": "RelocInfo::target_internal_reference", "content": "Address RelocInfo::target_internal_reference() {\n  if (IsInternalReference(rmode_)) {\n    return Memory<Address>(pc_);\n  } else {\n    // Encoded internal references are j/jal instructions.\n    DCHECK(IsInternalReferenceEncoded(rmode_));\n    DCHECK(Assembler::IsLui(Assembler::instr_at(pc_ + 0 * kInstrSize)));\n    Address address = Assembler::target_address_at(pc_);\n    return address;\n  }\n}", "name_and_para": "Address RelocInfo::target_internal_reference() "}], [{"name": "WritableRelocInfo::set_target_external_reference", "content": "void WritableRelocInfo::set_target_external_reference(\n    Address target, ICacheFlushMode icache_flush_mode) {\n  DCHECK(rmode_ == RelocInfo::EXTERNAL_REFERENCE);\n  Assembler::set_target_address_at(pc_, constant_pool_, target,\n                                   icache_flush_mode);\n}", "name_and_para": "void WritableRelocInfo::set_target_external_reference(\n    Address target, ICacheFlushMode icache_flush_mode) "}, {"name": "WritableRelocInfo::set_target_external_reference", "content": "void WritableRelocInfo::set_target_external_reference(\n    Address target, ICacheFlushMode icache_flush_mode) {\n  DCHECK(rmode_ == RelocInfo::EXTERNAL_REFERENCE);\n  Assembler::set_target_address_at(pc_, constant_pool_, target,\n                                   icache_flush_mode);\n}", "name_and_para": "void WritableRelocInfo::set_target_external_reference(\n    Address target, ICacheFlushMode icache_flush_mode) "}], [{"name": "RelocInfo::target_external_reference", "content": "Address RelocInfo::target_external_reference() {\n  DCHECK(rmode_ == EXTERNAL_REFERENCE);\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_external_reference() "}, {"name": "RelocInfo::target_external_reference", "content": "Address RelocInfo::target_external_reference() {\n  DCHECK(rmode_ == EXTERNAL_REFERENCE);\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_external_reference() "}], [{"name": "WritableRelocInfo::set_target_object", "content": "void WritableRelocInfo::set_target_object(Tagged<HeapObject> target,\n                                          ICacheFlushMode icache_flush_mode) {\n  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));\n  if (IsCompressedEmbeddedObject(rmode_)) {\n    DCHECK(COMPRESS_POINTERS_BOOL);\n    // We must not compress pointers to objects outside of the main pointer\n    // compression cage as we wouldn't be able to decompress them with the\n    // correct cage base.\n    DCHECK_IMPLIES(V8_ENABLE_SANDBOX_BOOL, !IsTrustedSpaceObject(target));\n    DCHECK_IMPLIES(V8_EXTERNAL_CODE_SPACE_BOOL, !IsCodeSpaceObject(target));\n    Assembler::set_target_compressed_address_at(\n        pc_, constant_pool_,\n        V8HeapCompressionScheme::CompressObject(target.ptr()),\n        icache_flush_mode);\n  } else {\n    DCHECK(IsFullEmbeddedObject(rmode_));\n    Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),\n                                     icache_flush_mode);\n  }\n}", "name_and_para": "void WritableRelocInfo::set_target_object(Tagged<HeapObject> target,\n                                          ICacheFlushMode icache_flush_mode) "}, {"name": "WritableRelocInfo::set_target_object", "content": "void WritableRelocInfo::set_target_object(Tagged<HeapObject> target,\n                                          ICacheFlushMode icache_flush_mode) {\n  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));\n  if (IsCompressedEmbeddedObject(rmode_)) {\n    DCHECK(COMPRESS_POINTERS_BOOL);\n    // We must not compress pointers to objects outside of the main pointer\n    // compression cage as we wouldn't be able to decompress them with the\n    // correct cage base.\n    DCHECK_IMPLIES(V8_ENABLE_SANDBOX_BOOL, !IsTrustedSpaceObject(target));\n    DCHECK_IMPLIES(V8_EXTERNAL_CODE_SPACE_BOOL, !IsCodeSpaceObject(target));\n    Assembler::set_target_compressed_address_at(\n        pc_, constant_pool_,\n        V8HeapCompressionScheme::CompressObject(target.ptr()),\n        icache_flush_mode);\n  } else {\n    DCHECK(IsFullEmbeddedObject(rmode_));\n    Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),\n                                     icache_flush_mode);\n  }\n}", "name_and_para": "void WritableRelocInfo::set_target_object(Tagged<HeapObject> target,\n                                          ICacheFlushMode icache_flush_mode) "}], [{"name": "RelocInfo::target_object_handle", "content": "Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {\n  if (IsEmbeddedObjectMode(rmode_)) {\n    return origin->target_object_handle_at(pc_);\n  } else {\n    DCHECK(IsCodeTarget(rmode_));\n    return origin->code_target_object_handle_at(pc_);\n  }\n}", "name_and_para": "Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) "}, {"name": "RelocInfo::target_object_handle", "content": "Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) {\n  if (IsCodeTarget(rmode_)) {\n    return Handle<HeapObject>::cast(\n        origin->code_target_object_handle_at(pc_, constant_pool_));\n  } else if (IsCompressedEmbeddedObject(rmode_)) {\n    return origin->compressed_embedded_object_handle_at(pc_, constant_pool_);\n  } else if (IsFullEmbeddedObject(rmode_)) {\n    return Handle<HeapObject>(reinterpret_cast<Address*>(\n        Assembler::target_address_at(pc_, constant_pool_)));\n  } else {\n    DCHECK(IsRelativeCodeTarget(rmode_));\n    return origin->relative_code_target_object_handle_at(pc_);\n  }\n}", "name_and_para": "Handle<HeapObject> RelocInfo::target_object_handle(Assembler* origin) "}], [{"name": "RelocInfo::target_object", "content": "Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {\n  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));\n  if (IsCompressedEmbeddedObject(rmode_)) {\n    Tagged_t compressed =\n        Assembler::target_compressed_address_at(pc_, constant_pool_);\n    DCHECK(!HAS_SMI_TAG(compressed));\n    Tagged<Object> obj(\n        V8HeapCompressionScheme::DecompressTagged(cage_base, compressed));\n    return HeapObject::cast(obj);\n  } else {\n    return HeapObject::cast(\n        Tagged<Object>(Assembler::target_address_at(pc_, constant_pool_)));\n  }\n}", "name_and_para": "Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) "}, {"name": "RelocInfo::target_object", "content": "Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) {\n  DCHECK(IsCodeTarget(rmode_) || IsEmbeddedObjectMode(rmode_));\n  if (IsCompressedEmbeddedObject(rmode_)) {\n    return HeapObject::cast(\n        Tagged<Object>(V8HeapCompressionScheme::DecompressTagged(\n            cage_base,\n            Assembler::target_compressed_address_at(pc_, constant_pool_))));\n  } else {\n    return HeapObject::cast(\n        Tagged<Object>(Assembler::target_address_at(pc_, constant_pool_)));\n  }\n}", "name_and_para": "Tagged<HeapObject> RelocInfo::target_object(PtrComprCageBase cage_base) "}], [{"name": "RelocInfo::constant_pool_entry_address", "content": "Address RelocInfo::constant_pool_entry_address() {\n  DCHECK(IsInConstantPool());\n  return Assembler::target_pointer_address_at(pc_);\n}", "name_and_para": "Address RelocInfo::constant_pool_entry_address() "}, {"name": "RelocInfo::constant_pool_entry_address", "content": "Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }", "name_and_para": "Address RelocInfo::constant_pool_entry_address() "}], [{"name": "RelocInfo::target_address_address", "content": "Address RelocInfo::target_address_address() {\n  DCHECK(HasTargetAddressAddress());\n  Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n  // Read the address of the word containing the target_address in an\n  // instruction stream.\n  // The only architecture-independent user of this function is the serializer.\n  // The serializer uses it to find out how many raw bytes of instruction to\n  // output before the next target.\n  // For an instruction like B/BL, where the target bits are mixed into the\n  // instruction bits, the size of the target will be zero, indicating that the\n  // serializer should not step forward in memory after a target is resolved\n  // and written.\n  // For LDR literal instructions, we can skip up to the constant pool entry\n  // address. We make sure that RelocInfo is ordered by the\n  // target_address_address so that we do not skip over any relocatable\n  // instruction sequences.\n  if (instr->IsLdrLiteralX()) {\n    return constant_pool_entry_address();\n  } else {\n    DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());\n    return pc_;\n  }\n}", "name_and_para": "Address RelocInfo::target_address_address() "}, {"name": "RelocInfo::target_address_address", "content": "Address RelocInfo::target_address_address() {\n  DCHECK(HasTargetAddressAddress());\n  // Read the address of the word containing the target_address in an\n  // instruction stream.\n  // The only architecture-independent user of this function is the serializer.\n  // The serializer uses it to find out how many raw bytes of instruction to\n  // output before the next target.\n  // For an instruction like LUI/ORI where the target bits are mixed into the\n  // instruction bits, the size of the target will be zero, indicating that the\n  // serializer should not step forward in memory after a target is resolved\n  // and written. In this case the target_address_address function should\n  // return the end of the instructions to be patched, allowing the\n  // deserializer to deserialize the instructions as raw bytes and put them in\n  // place, ready to be patched with the target. After jump optimization,\n  // that is the address of the instruction that follows J/JAL/JR/JALR\n  // instruction.\n#ifdef V8_TARGET_ARCH_RISCV64\n  return pc_ + Assembler::kInstructionsFor64BitConstant * kInstrSize;\n#elif defined(V8_TARGET_ARCH_RISCV32)\n  return pc_ + Assembler::kInstructionsFor32BitConstant * kInstrSize;\n#endif\n}", "name_and_para": "Address RelocInfo::target_address_address() "}], [{"name": "RelocInfo::target_address", "content": "Address RelocInfo::target_address() {\n  DCHECK(IsCodeTarget(rmode_) || IsNearBuiltinEntry(rmode_) ||\n         IsWasmCall(rmode_) || IsWasmStubCall(rmode_));\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_address() "}, {"name": "RelocInfo::target_address", "content": "Address RelocInfo::target_address() {\n  DCHECK(IsCodeTargetMode(rmode_) || IsWasmCall(rmode_) ||\n         IsNearBuiltinEntry(rmode_) || IsWasmStubCall(rmode_));\n  return Assembler::target_address_at(pc_, constant_pool_);\n}", "name_and_para": "Address RelocInfo::target_address() "}], [{"name": "RelocInfo::target_address_size", "content": "int RelocInfo::target_address_size() {\n  if (IsCodedSpecially()) {\n    return Assembler::kSpecialTargetSize;\n  } else {\n    Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n    DCHECK(instr->IsLdrLiteralX() || instr->IsLdrLiteralW());\n    return instr->IsLdrLiteralW() ? kTaggedSize : kSystemPointerSize;\n  }\n}", "name_and_para": "int RelocInfo::target_address_size() "}, {"name": "RelocInfo::target_address_size", "content": "int RelocInfo::target_address_size() {\n  if (IsCodedSpecially()) {\n    return Assembler::kSpecialTargetSize;\n  } else {\n    return kSystemPointerSize;\n  }\n}", "name_and_para": "int RelocInfo::target_address_size() "}], [{"name": "Assembler::set_target_compressed_address_at", "content": "void Assembler::set_target_compressed_address_at(\n    Address pc, Address constant_pool, Tagged_t target,\n    ICacheFlushMode icache_flush_mode) {\n  Instruction* instr = reinterpret_cast<Instruction*>(pc);\n  CHECK(instr->IsLdrLiteralW());\n  Memory<Tagged_t>(target_pointer_address_at(pc)) = target;\n}", "name_and_para": "void Assembler::set_target_compressed_address_at(\n    Address pc, Address constant_pool, Tagged_t target,\n    ICacheFlushMode icache_flush_mode) "}, {"name": "Assembler::set_target_compressed_address_at", "content": "void Assembler::set_target_compressed_address_at(\n    Address pc, Address constant_pool, Tagged_t target,\n    ICacheFlushMode icache_flush_mode) {\n  Assembler::set_target_address_at(\n      pc, constant_pool, static_cast<Address>(target), icache_flush_mode);\n}", "name_and_para": "void Assembler::set_target_compressed_address_at(\n    Address pc, Address constant_pool, Tagged_t target,\n    ICacheFlushMode icache_flush_mode) "}], [{"name": "Assembler::deserialization_set_target_internal_reference_at", "content": "void Assembler::deserialization_set_target_internal_reference_at(\n    Address pc, Address target, RelocInfo::Mode mode) {\n  WriteUnalignedValue<Address>(pc, target);\n}", "name_and_para": "void Assembler::deserialization_set_target_internal_reference_at(\n    Address pc, Address target, RelocInfo::Mode mode) "}, {"name": "Assembler::deserialization_set_target_internal_reference_at", "content": "void Assembler::deserialization_set_target_internal_reference_at(\n    Address pc, Address target, RelocInfo::Mode mode) {\n  if (RelocInfo::IsInternalReferenceEncoded(mode)) {\n    DCHECK(IsLui(instr_at(pc)));\n    set_target_internal_reference_encoded_at(pc, target);\n  } else {\n    DCHECK(RelocInfo::IsInternalReference(mode));\n    Memory<Address>(pc) = target;\n  }\n}", "name_and_para": "void Assembler::deserialization_set_target_internal_reference_at(\n    Address pc, Address target, RelocInfo::Mode mode) "}], [{"name": "Assembler::deserialization_set_special_target_at", "content": "void Assembler::deserialization_set_special_target_at(Address location,\n                                                      Tagged<Code> code,\n                                                      Address target) {\n  Instruction* instr = reinterpret_cast<Instruction*>(location);\n  if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {\n    if (target == 0) {\n      // We are simply wiping the target out for serialization. Set the offset\n      // to zero instead.\n      target = location;\n    }\n    instr->SetBranchImmTarget<UncondBranchType>(\n        reinterpret_cast<Instruction*>(target));\n    FlushInstructionCache(location, kInstrSize);\n  } else {\n    DCHECK_EQ(instr->InstructionBits(), 0);\n    Memory<Address>(location) = target;\n    // Intuitively, we would think it is necessary to always flush the\n    // instruction cache after patching a target address in the code. However,\n    // in this case, only the constant pool contents change. The instruction\n    // accessing the constant pool remains unchanged, so a flush is not\n    // required.\n  }\n}", "name_and_para": "void Assembler::deserialization_set_special_target_at(Address location,\n                                                      Tagged<Code> code,\n                                                      Address target) "}, {"name": "Assembler::deserialization_set_special_target_at", "content": "void Assembler::deserialization_set_special_target_at(\n    Address instruction_payload, Tagged<Code> code, Address target) {\n  set_target_address_at(instruction_payload,\n                        !code.is_null() ? code->constant_pool() : kNullAddress,\n                        target);\n}", "name_and_para": "void Assembler::deserialization_set_special_target_at(\n    Address instruction_payload, Tagged<Code> code, Address target) "}], [{"name": "Assembler::deserialization_special_target_size", "content": "int Assembler::deserialization_special_target_size(Address location) {\n  Instruction* instr = reinterpret_cast<Instruction*>(location);\n  if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {\n    return kSpecialTargetSize;\n  } else {\n    DCHECK_EQ(instr->InstructionBits(), 0);\n    return kSystemPointerSize;\n  }\n}", "name_and_para": "int Assembler::deserialization_special_target_size(Address location) "}, {"name": "Assembler::deserialization_special_target_size", "content": "int Assembler::deserialization_special_target_size(\n    Address instruction_payload) {\n  return kSpecialTargetSize;\n}", "name_and_para": "int Assembler::deserialization_special_target_size(\n    Address instruction_payload) "}], [{"name": "Assembler::target_builtin_at", "content": "Builtin Assembler::target_builtin_at(Address pc) {\n  Instruction* instr = reinterpret_cast<Instruction*>(pc);\n  DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());\n  DCHECK_EQ(instr->ImmPCOffset() % kInstrSize, 0);\n  int builtin_id = static_cast<int>(instr->ImmPCOffset() / kInstrSize);\n  DCHECK(Builtins::IsBuiltinId(builtin_id));\n  return static_cast<Builtin>(builtin_id);\n}", "name_and_para": "Builtin Assembler::target_builtin_at(Address pc) "}, {"name": "Assembler::target_builtin_at", "content": "Builtin Assembler::target_builtin_at(Address pc) {\n  Instr instr1 = Assembler::instr_at(pc);\n  Instr instr2 = Assembler::instr_at(pc + kInstrSize);\n  DCHECK(IsAuipc(instr1));\n  DCHECK(IsJalr(instr2));\n  int32_t builtin_id = BrachlongOffset(instr1, instr2);\n  DCHECK(Builtins::IsBuiltinId(builtin_id));\n  return static_cast<Builtin>(builtin_id);\n}", "name_and_para": "Builtin Assembler::target_builtin_at(Address pc) "}], [{"name": "Assembler::code_target_object_handle_at", "content": "Handle<Code> Assembler::code_target_object_handle_at(Address pc) {\n  Instruction* instr = reinterpret_cast<Instruction*>(pc);\n  if (instr->IsLdrLiteralX()) {\n    return Handle<Code>(reinterpret_cast<Address*>(\n        Assembler::target_address_at(pc, 0 /* unused */)));\n  } else {\n    DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());\n    DCHECK_EQ(instr->ImmPCOffset() % kInstrSize, 0);\n    return Handle<Code>::cast(\n        GetEmbeddedObject(instr->ImmPCOffset() >> kInstrSizeLog2));\n  }\n}", "name_and_para": "Handle<Code> Assembler::code_target_object_handle_at(Address pc) "}, {"name": "Assembler::code_target_object_handle_at", "content": "Handle<Object> Assembler::code_target_object_handle_at(Address pc,\n                                                       Address constant_pool) {\n  int index =\n      static_cast<int>(target_address_at(pc, constant_pool)) & 0xFFFFFFFF;\n  return GetCodeTarget(index);\n}", "name_and_para": "Handle<Object> Assembler::code_target_object_handle_at(Address pc,\n                                                       Address constant_pool) "}], [{"name": "Assembler::target_compressed_address_at", "content": "Tagged_t Assembler::target_compressed_address_at(Address pc,\n                                                 Address constant_pool) {\n  Instruction* instr = reinterpret_cast<Instruction*>(pc);\n  CHECK(instr->IsLdrLiteralW());\n  return Memory<Tagged_t>(target_pointer_address_at(pc));\n}", "name_and_para": "Tagged_t Assembler::target_compressed_address_at(Address pc,\n                                                 Address constant_pool) "}, {"name": "Assembler::target_compressed_address_at", "content": "Tagged_t Assembler::target_compressed_address_at(Address pc,\n                                                 Address constant_pool) {\n  return static_cast<Tagged_t>(target_address_at(pc, constant_pool));\n}", "name_and_para": "Tagged_t Assembler::target_compressed_address_at(Address pc,\n                                                 Address constant_pool) "}], [{"name": "WritableRelocInfo::apply", "content": "void WritableRelocInfo::apply(intptr_t delta) {\n  // On arm64 only internal references and immediate branches need extra work.\n  if (RelocInfo::IsInternalReference(rmode_)) {\n    // Absolute code pointer inside code object moves with the code object.\n    intptr_t internal_ref = ReadUnalignedValue<intptr_t>(pc_);\n    internal_ref += delta;  // Relocate entry.\n    WriteUnalignedValue<intptr_t>(pc_, internal_ref);\n  } else {\n    Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n    if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {\n      Address old_target =\n          reinterpret_cast<Address>(instr->ImmPCOffsetTarget());\n      Address new_target = old_target - delta;\n      instr->SetBranchImmTarget<UncondBranchType>(\n          reinterpret_cast<Instruction*>(new_target));\n    }\n  }\n}", "name_and_para": "void WritableRelocInfo::apply(intptr_t delta) "}, {"name": "WritableRelocInfo::apply", "content": "void WritableRelocInfo::apply(intptr_t delta) {\n  if (IsInternalReference(rmode_) || IsInternalReferenceEncoded(rmode_)) {\n    // Absolute code pointer inside code object moves with the code object.\n    Assembler::RelocateInternalReference(rmode_, pc_, delta);\n  } else {\n    DCHECK(IsRelativeCodeTarget(rmode_));\n    Assembler::RelocateRelativeReference(rmode_, pc_, delta);\n  }\n}", "name_and_para": "void WritableRelocInfo::apply(intptr_t delta) "}], [{"name": "CpuFeatures::SupportsOptimizer", "content": "bool CpuFeatures::SupportsOptimizer() { return true; }", "name_and_para": "bool CpuFeatures::SupportsOptimizer() "}, {"name": "CpuFeatures::SupportsOptimizer", "content": "bool CpuFeatures::SupportsOptimizer() { return IsSupported(FPU); }", "name_and_para": "bool CpuFeatures::SupportsOptimizer() "}]]], [["./v8/src/codegen/riscv/interface-descriptors-riscv-inl.h", "./v8/src/codegen/arm64/interface-descriptors-arm64-inl.h"], 0.96, 1.0, [[{"name": "WasmJSToWasmWrapperDescriptor::registers", "content": "constexpr auto WasmJSToWasmWrapperDescriptor::registers() {\n  // Arbitrarily picked register.\n  return RegisterArray(x8);\n}", "name_and_para": "constexpr auto WasmJSToWasmWrapperDescriptor::registers() "}, {"name": "WasmJSToWasmWrapperDescriptor::registers", "content": "constexpr auto WasmJSToWasmWrapperDescriptor::registers() {\n  // Arbitrarily picked register.\n  return RegisterArray(t0);\n}", "name_and_para": "constexpr auto WasmJSToWasmWrapperDescriptor::registers() "}], [{"name": "RunMicrotasksEntryDescriptor::registers", "content": "constexpr auto RunMicrotasksEntryDescriptor::registers() {\n  return RegisterArray(x0, x1);\n}", "name_and_para": "constexpr auto RunMicrotasksEntryDescriptor::registers() "}, {"name": "RunMicrotasksEntryDescriptor::registers", "content": "constexpr auto RunMicrotasksEntryDescriptor::registers() {\n  return RegisterArray(a0, a1);\n}", "name_and_para": "constexpr auto RunMicrotasksEntryDescriptor::registers() "}], [{"name": "ResumeGeneratorDescriptor::registers", "content": "constexpr auto ResumeGeneratorDescriptor::registers() {\n  return RegisterArray(x0,   // the value to pass to the generator\n                       x1);  // the JSGeneratorObject to resume\n}", "name_and_para": "constexpr auto ResumeGeneratorDescriptor::registers() "}, {"name": "ResumeGeneratorDescriptor::registers", "content": "constexpr auto ResumeGeneratorDescriptor::registers() {\n  return RegisterArray(a0,   // the value to pass to the generator\n                       a1);  // the JSGeneratorObject to resume\n}", "name_and_para": "constexpr auto ResumeGeneratorDescriptor::registers() "}], [{"name": "ConstructForwardAllArgsDescriptor::registers", "content": "constexpr auto ConstructForwardAllArgsDescriptor::registers() {\n  return RegisterArray(x1,   // constructor to call\n                       x3);  // new target\n}", "name_and_para": "constexpr auto ConstructForwardAllArgsDescriptor::registers() "}, {"name": "ConstructForwardAllArgsDescriptor::registers", "content": "constexpr auto ConstructForwardAllArgsDescriptor::registers() {\n  return RegisterArray(a1,   // constructor to call\n                       a3);  // new target\n}", "name_and_para": "constexpr auto ConstructForwardAllArgsDescriptor::registers() "}], [{"name": "InterpreterPushArgsThenConstructDescriptor::registers", "content": "constexpr auto InterpreterPushArgsThenConstructDescriptor::registers() {\n  return RegisterArray(\n      x0,   // argument count\n      x4,   // address of the first argument\n      x1,   // constructor to call\n      x3,   // new target\n      x2);  // allocation site feedback if available, undefined otherwise\n}", "name_and_para": "constexpr auto InterpreterPushArgsThenConstructDescriptor::registers() "}, {"name": "InterpreterPushArgsThenConstructDescriptor::registers", "content": "constexpr auto InterpreterPushArgsThenConstructDescriptor::registers() {\n  return RegisterArray(\n      a0,   // argument count\n      a4,   // address of the first argument\n      a1,   // constructor to call\n      a3,   // new target\n      a2);  // allocation site feedback if available, undefined otherwise\n}", "name_and_para": "constexpr auto InterpreterPushArgsThenConstructDescriptor::registers() "}], [{"name": "InterpreterPushArgsThenCallDescriptor::registers", "content": "constexpr auto InterpreterPushArgsThenCallDescriptor::registers() {\n  return RegisterArray(x0,   // argument count\n                       x2,   // address of first argument\n                       x1);  // the target callable to be call\n}", "name_and_para": "constexpr auto InterpreterPushArgsThenCallDescriptor::registers() "}, {"name": "InterpreterPushArgsThenCallDescriptor::registers", "content": "constexpr auto InterpreterPushArgsThenCallDescriptor::registers() {\n  return RegisterArray(a0,   // argument count\n                       a2,   // address of first argument\n                       a1);  // the target callable to be call\n}", "name_and_para": "constexpr auto InterpreterPushArgsThenCallDescriptor::registers() "}], [{"name": "InterpreterDispatchDescriptor::registers", "content": "constexpr auto InterpreterDispatchDescriptor::registers() {\n  return RegisterArray(\n      kInterpreterAccumulatorRegister, kInterpreterBytecodeOffsetRegister,\n      kInterpreterBytecodeArrayRegister, kInterpreterDispatchTableRegister);\n}", "name_and_para": "constexpr auto InterpreterDispatchDescriptor::registers() "}, {"name": "InterpreterDispatchDescriptor::registers", "content": "constexpr auto InterpreterDispatchDescriptor::registers() {\n  return RegisterArray(\n      kInterpreterAccumulatorRegister, kInterpreterBytecodeOffsetRegister,\n      kInterpreterBytecodeArrayRegister, kInterpreterDispatchTableRegister);\n}", "name_and_para": "constexpr auto InterpreterDispatchDescriptor::registers() "}], [{"name": "CallApiCallbackGenericDescriptor::HolderRegister", "content": "constexpr Register CallApiCallbackGenericDescriptor::HolderRegister() {\n  return x0;\n}", "name_and_para": "constexpr Register CallApiCallbackGenericDescriptor::HolderRegister() "}, {"name": "CallApiCallbackGenericDescriptor::HolderRegister", "content": "constexpr Register CallApiCallbackGenericDescriptor::HolderRegister() {\n  return a0;\n}", "name_and_para": "constexpr Register CallApiCallbackGenericDescriptor::HolderRegister() "}], [{"name": "CallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister() {\n  return x3;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister() "}, {"name": "CallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister() {\n  return a3;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::FunctionTemplateInfoRegister() "}], [{"name": "CallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister() {\n  return x1;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister() "}, {"name": "CallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister() {\n  return a1;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::TopmostScriptHavingContextRegister() "}], [{"name": "CallApiCallbackGenericDescriptor::ActualArgumentsCountRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::ActualArgumentsCountRegister() {\n  return x2;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::ActualArgumentsCountRegister() "}, {"name": "CallApiCallbackGenericDescriptor::ActualArgumentsCountRegister", "content": "constexpr Register\nCallApiCallbackGenericDescriptor::ActualArgumentsCountRegister() {\n  return a2;\n}", "name_and_para": "constexpr Register\nCallApiCallbackGenericDescriptor::ActualArgumentsCountRegister() "}], [{"name": "CallApiCallbackOptimizedDescriptor::HolderRegister", "content": "constexpr Register CallApiCallbackOptimizedDescriptor::HolderRegister() {\n  return x0;\n}", "name_and_para": "constexpr Register CallApiCallbackOptimizedDescriptor::HolderRegister() "}, {"name": "CallApiCallbackOptimizedDescriptor::HolderRegister", "content": "constexpr Register CallApiCallbackOptimizedDescriptor::HolderRegister() {\n  return a0;\n}", "name_and_para": "constexpr Register CallApiCallbackOptimizedDescriptor::HolderRegister() "}], [{"name": "CallApiCallbackOptimizedDescriptor::CallDataRegister", "content": "constexpr Register CallApiCallbackOptimizedDescriptor::CallDataRegister() {\n  return x3;\n}", "name_and_para": "constexpr Register CallApiCallbackOptimizedDescriptor::CallDataRegister() "}, {"name": "CallApiCallbackOptimizedDescriptor::CallDataRegister", "content": "constexpr Register CallApiCallbackOptimizedDescriptor::CallDataRegister() {\n  return a3;\n}", "name_and_para": "constexpr Register CallApiCallbackOptimizedDescriptor::CallDataRegister() "}], [{"name": "CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister", "content": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister() {\n  return x2;\n}", "name_and_para": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister() "}, {"name": "CallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister", "content": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister() {\n  return a2;\n}", "name_and_para": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ActualArgumentsCountRegister() "}], [{"name": "CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister", "content": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister() {\n  return x1;\n}", "name_and_para": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister() "}, {"name": "CallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister", "content": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister() {\n  return a1;\n}", "name_and_para": "constexpr Register\nCallApiCallbackOptimizedDescriptor::ApiFunctionAddressRegister() "}], [{"name": "BinarySmiOp_BaselineDescriptor::registers", "content": "constexpr auto BinarySmiOp_BaselineDescriptor::registers() {\n  // x0: left operand\n  // x1: right operand\n  // x2: feedback slot\n  return RegisterArray(x0, x1, x2);\n}", "name_and_para": "constexpr auto BinarySmiOp_BaselineDescriptor::registers() "}, {"name": "BinarySmiOp_BaselineDescriptor::registers", "content": "constexpr auto BinarySmiOp_BaselineDescriptor::registers() {\n  // a0: left operand\n  // a1: right operand\n  // a2: feedback slot\n  return RegisterArray(a0, a1, a2);\n}", "name_and_para": "constexpr auto BinarySmiOp_BaselineDescriptor::registers() "}], [{"name": "BinaryOp_BaselineDescriptor::registers", "content": "constexpr auto BinaryOp_BaselineDescriptor::registers() {\n  // x1: left operand\n  // x0: right operand\n  // x2: feedback slot\n  return RegisterArray(x1, x0, x2);\n}", "name_and_para": "constexpr auto BinaryOp_BaselineDescriptor::registers() "}, {"name": "BinaryOp_BaselineDescriptor::registers", "content": "constexpr auto BinaryOp_BaselineDescriptor::registers() {\n  // a1: left operand\n  // a0: right operand\n  // a2: feedback slot\n  return RegisterArray(a1, a0, a2);\n}", "name_and_para": "constexpr auto BinaryOp_BaselineDescriptor::registers() "}], [{"name": "BinaryOpDescriptor::registers", "content": "constexpr auto BinaryOpDescriptor::registers() {\n  // x1: left operand\n  // x0: right operand\n  return RegisterArray(x1, x0);\n}", "name_and_para": "constexpr auto BinaryOpDescriptor::registers() "}, {"name": "BinaryOpDescriptor::registers", "content": "constexpr auto BinaryOpDescriptor::registers() {\n  // a1: left operand\n  // a0: right operand\n  return RegisterArray(a1, a0);\n}", "name_and_para": "constexpr auto BinaryOpDescriptor::registers() "}], [{"name": "Compare_BaselineDescriptor::registers", "content": "constexpr auto Compare_BaselineDescriptor::registers() {\n  // x1: left operand\n  // x0: right operand\n  // x2: feedback slot\n  return RegisterArray(x1, x0, x2);\n}", "name_and_para": "constexpr auto Compare_BaselineDescriptor::registers() "}, {"name": "Compare_BaselineDescriptor::registers", "content": "constexpr auto Compare_BaselineDescriptor::registers() {\n  // a1: left operand\n  // a0: right operand\n  // a2: feedback slot\n  return RegisterArray(a1, a0, a2);\n}", "name_and_para": "constexpr auto Compare_BaselineDescriptor::registers() "}], [{"name": "CompareDescriptor::registers", "content": "constexpr auto CompareDescriptor::registers() {\n  // x1: left operand\n  // x0: right operand\n  return RegisterArray(x1, x0);\n}", "name_and_para": "constexpr auto CompareDescriptor::registers() "}, {"name": "CompareDescriptor::registers", "content": "constexpr auto CompareDescriptor::registers() {\n  // a1: left operand\n  // a0: right operand\n  return RegisterArray(a1, a0);\n}", "name_and_para": "constexpr auto CompareDescriptor::registers() "}], [{"name": "AbortDescriptor::registers", "content": "constexpr auto AbortDescriptor::registers() { return RegisterArray(x1); }", "name_and_para": "constexpr auto AbortDescriptor::registers() "}, {"name": "AbortDescriptor::registers", "content": "constexpr auto AbortDescriptor::registers() { return RegisterArray(a0); }", "name_and_para": "constexpr auto AbortDescriptor::registers() "}], [{"name": "ConstructStubDescriptor::registers", "content": "constexpr auto ConstructStubDescriptor::registers() {\n  // x3: new target\n  // x1: target\n  // x0: number of arguments\n  return RegisterArray(x1, x3, x0);\n}", "name_and_para": "constexpr auto ConstructStubDescriptor::registers() "}, {"name": "ConstructStubDescriptor::registers", "content": "constexpr auto ConstructStubDescriptor::registers() {\n  // a3: new target\n  // a1: target\n  // a0: number of arguments\n  return RegisterArray(a1, a3, a0);\n}", "name_and_para": "constexpr auto ConstructStubDescriptor::registers() "}], [{"name": "ConstructWithArrayLikeDescriptor::registers", "content": "constexpr auto ConstructWithArrayLikeDescriptor::registers() {\n  // x1 : the target to call\n  // x3 : the new target\n  // x2 : the arguments list\n  return RegisterArray(x1, x3, x2);\n}", "name_and_para": "constexpr auto ConstructWithArrayLikeDescriptor::registers() "}, {"name": "ConstructWithArrayLikeDescriptor::registers", "content": "constexpr auto ConstructWithArrayLikeDescriptor::registers() {\n  // a1 : the target to call\n  // a3 : the new target\n  // a2 : the arguments list\n  return RegisterArray(a1, a3, a2);\n}", "name_and_para": "constexpr auto ConstructWithArrayLikeDescriptor::registers() "}], [{"name": "ConstructWithSpreadDescriptor::registers", "content": "constexpr auto ConstructWithSpreadDescriptor::registers() {\n  // x0 : number of arguments (on the stack)\n  // x1 : the target to call\n  // x3 : the new target\n  // x2 : the object to spread\n  return RegisterArray(x1, x3, x0, x2);\n}", "name_and_para": "constexpr auto ConstructWithSpreadDescriptor::registers() "}, {"name": "ConstructWithSpreadDescriptor::registers", "content": "constexpr auto ConstructWithSpreadDescriptor::registers() {\n  // a0 : number of arguments (on the stack)\n  // a1 : the target to call\n  // a3 : the new target\n  // a2 : the object to spread\n  return RegisterArray(a1, a3, a0, a2);\n}", "name_and_para": "constexpr auto ConstructWithSpreadDescriptor::registers() "}], [{"name": "ConstructForwardVarargsDescriptor::registers", "content": "constexpr auto ConstructForwardVarargsDescriptor::registers() {\n  // x3: new target\n  // x1: target\n  // x0: number of arguments\n  // x2: start index (to supported rest parameters)\n  return RegisterArray(x1, x3, x0, x2);\n}", "name_and_para": "constexpr auto ConstructForwardVarargsDescriptor::registers() "}, {"name": "ConstructForwardVarargsDescriptor::registers", "content": "constexpr auto ConstructForwardVarargsDescriptor::registers() {\n  // a3: new target\n  // a1: target\n  // a0: number of arguments\n  // a2: start index (to supported rest parameters)\n  return RegisterArray(a1, a3, a0, a2);\n}", "name_and_para": "constexpr auto ConstructForwardVarargsDescriptor::registers() "}], [{"name": "ConstructVarargsDescriptor::registers", "content": "constexpr auto ConstructVarargsDescriptor::registers() {\n  // x0 : number of arguments (on the stack)\n  // x1 : the target to call\n  // x3 : the new target\n  // x4 : arguments list length (untagged)\n  // x2 : arguments list (FixedArray)\n  return RegisterArray(x1, x3, x0, x4, x2);\n}", "name_and_para": "constexpr auto ConstructVarargsDescriptor::registers() "}, {"name": "ConstructVarargsDescriptor::registers", "content": "constexpr auto ConstructVarargsDescriptor::registers() {\n  // a0 : number of arguments (on the stack)\n  // a1 : the target to call\n  // a3 : the new target\n  // a4 : arguments list length (untagged)\n  // a2 : arguments list (FixedArray)\n  return RegisterArray(a1, a3, a0, a4, a2);\n}", "name_and_para": "constexpr auto ConstructVarargsDescriptor::registers() "}], [{"name": "CallWithArrayLikeDescriptor::registers", "content": "constexpr auto CallWithArrayLikeDescriptor::registers() {\n  // x1 : the target to call\n  // x2 : the arguments list\n  return RegisterArray(x1, x2);\n}", "name_and_para": "constexpr auto CallWithArrayLikeDescriptor::registers() "}, {"name": "CallWithArrayLikeDescriptor::registers", "content": "constexpr auto CallWithArrayLikeDescriptor::registers() {\n  // a1 : the target to call\n  // a2 : the arguments list\n  return RegisterArray(a1, a2);\n}", "name_and_para": "constexpr auto CallWithArrayLikeDescriptor::registers() "}], [{"name": "CallWithSpreadDescriptor::registers", "content": "constexpr auto CallWithSpreadDescriptor::registers() {\n  // x0 : number of arguments (on the stack)\n  // x1 : the target to call\n  // x2 : the object to spread\n  return RegisterArray(x1, x0, x2);\n}", "name_and_para": "constexpr auto CallWithSpreadDescriptor::registers() "}, {"name": "CallWithSpreadDescriptor::registers", "content": "constexpr auto CallWithSpreadDescriptor::registers() {\n  // a0 : number of arguments (on the stack)\n  // a1 : the target to call\n  // a2 : the object to spread\n  return RegisterArray(a1, a0, a2);\n}", "name_and_para": "constexpr auto CallWithSpreadDescriptor::registers() "}], [{"name": "CallFunctionTemplateGenericDescriptor::registers", "content": "constexpr auto CallFunctionTemplateGenericDescriptor::registers() {\n  // x1 : function template info\n  // x2 : number of arguments (on the stack)\n  // x3 : topmost script-having context\n  return RegisterArray(x1, x2, x3);\n}", "name_and_para": "constexpr auto CallFunctionTemplateGenericDescriptor::registers() "}, {"name": "CallFunctionTemplateGenericDescriptor::registers", "content": "constexpr auto CallFunctionTemplateGenericDescriptor::registers() {\n  // a1 : function template info\n  // a2 : number of arguments (on the stack)\n  // a3 : topmost script-having context\n  return RegisterArray(a1, a2, a3);\n}", "name_and_para": "constexpr auto CallFunctionTemplateGenericDescriptor::registers() "}], [{"name": "CallFunctionTemplateDescriptor::registers", "content": "constexpr auto CallFunctionTemplateDescriptor::registers() {\n  // x1 : function template info\n  // x2 : number of arguments (on the stack)\n  return RegisterArray(x1, x2);\n}", "name_and_para": "constexpr auto CallFunctionTemplateDescriptor::registers() "}, {"name": "CallFunctionTemplateDescriptor::registers", "content": "constexpr auto CallFunctionTemplateDescriptor::registers() {\n  // a1 : function template info\n  // a0 : number of arguments (on the stack)\n  return RegisterArray(a1, a0);\n}", "name_and_para": "constexpr auto CallFunctionTemplateDescriptor::registers() "}], [{"name": "CallForwardVarargsDescriptor::registers", "content": "constexpr auto CallForwardVarargsDescriptor::registers() {\n  // x1: target\n  // x0: number of arguments\n  // x2: start index (to supported rest parameters)\n  return RegisterArray(x1, x0, x2);\n}", "name_and_para": "constexpr auto CallForwardVarargsDescriptor::registers() "}, {"name": "CallForwardVarargsDescriptor::registers", "content": "constexpr auto CallForwardVarargsDescriptor::registers() {\n  // a1: target\n  // a0: number of arguments\n  // a2: start index (to supported rest parameters)\n  return RegisterArray(a1, a0, a2);\n}", "name_and_para": "constexpr auto CallForwardVarargsDescriptor::registers() "}], [{"name": "CallVarargsDescriptor::registers", "content": "constexpr auto CallVarargsDescriptor::registers() {\n  // x0 : number of arguments (on the stack)\n  // x1 : the target to call\n  // x4 : arguments list length (untagged)\n  // x2 : arguments list (FixedArray)\n  return RegisterArray(x1, x0, x4, x2);\n}", "name_and_para": "constexpr auto CallVarargsDescriptor::registers() "}, {"name": "CallVarargsDescriptor::registers", "content": "constexpr auto CallVarargsDescriptor::registers() {\n  // a0 : number of arguments (on the stack)\n  // a1 : the target to call\n  // a4 : arguments list length (untagged)\n  // a2 : arguments list (FixedArray)\n  return RegisterArray(a1, a0, a4, a2);\n}", "name_and_para": "constexpr auto CallVarargsDescriptor::registers() "}], [{"name": "CopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers", "content": "constexpr auto\nCopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers() {\n  // r1 : the source\n  // r0 : the excluded property count\n  // x2 : the excluded property base\n  return RegisterArray(x1, x0, x2);\n}", "name_and_para": "constexpr auto\nCopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers() "}, {"name": "CopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers", "content": "constexpr auto\nCopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers() {\n  // a1 : the source\n  // a0 : the excluded property count\n  // a2 : the excluded property base\n  return RegisterArray(a1, a0, a2);\n}", "name_and_para": "constexpr auto\nCopyDataPropertiesWithExcludedPropertiesOnStackDescriptor::registers() "}], [{"name": "CopyDataPropertiesWithExcludedPropertiesDescriptor::registers", "content": "constexpr auto CopyDataPropertiesWithExcludedPropertiesDescriptor::registers() {\n  // r1 : the source\n  // r0 : the excluded property count\n  return RegisterArray(x1, x0);\n}", "name_and_para": "constexpr auto CopyDataPropertiesWithExcludedPropertiesDescriptor::registers() "}, {"name": "CopyDataPropertiesWithExcludedPropertiesDescriptor::registers", "content": "constexpr auto CopyDataPropertiesWithExcludedPropertiesDescriptor::registers() {\n  // a1 : the source\n  // a0 : the excluded property count\n  return RegisterArray(a1, a0);\n}", "name_and_para": "constexpr auto CopyDataPropertiesWithExcludedPropertiesDescriptor::registers() "}], [{"name": "CallTrampolineDescriptor::registers", "content": "constexpr auto CallTrampolineDescriptor::registers() {\n  // x1: target\n  // x0: number of arguments\n  return RegisterArray(x1, x0);\n}", "name_and_para": "constexpr auto CallTrampolineDescriptor::registers() "}, {"name": "CallTrampolineDescriptor::registers", "content": "constexpr auto CallTrampolineDescriptor::registers() {\n  // a1: target\n  // a0: number of arguments\n  return RegisterArray(a1, a0);\n}", "name_and_para": "constexpr auto CallTrampolineDescriptor::registers() "}], [{"name": "TypeofDescriptor::registers", "content": "constexpr auto TypeofDescriptor::registers() { return RegisterArray(x0); }", "name_and_para": "constexpr auto TypeofDescriptor::registers() "}, {"name": "TypeofDescriptor::registers", "content": "constexpr auto TypeofDescriptor::registers() { return RegisterArray(a0); }", "name_and_para": "constexpr auto TypeofDescriptor::registers() "}], [{"name": "TypeConversionDescriptor::ArgumentRegister", "content": "constexpr Register TypeConversionDescriptor::ArgumentRegister() { return x0; }", "name_and_para": "constexpr Register TypeConversionDescriptor::ArgumentRegister() "}, {"name": "TypeConversionDescriptor::ArgumentRegister", "content": "constexpr Register TypeConversionDescriptor::ArgumentRegister() { return a0; }", "name_and_para": "constexpr Register TypeConversionDescriptor::ArgumentRegister() "}], [{"name": "BaselineLeaveFrameDescriptor::WeightRegister", "content": "constexpr Register BaselineLeaveFrameDescriptor::WeightRegister() { return x4; }", "name_and_para": "constexpr Register BaselineLeaveFrameDescriptor::WeightRegister() "}, {"name": "BaselineLeaveFrameDescriptor::WeightRegister", "content": "constexpr Register BaselineLeaveFrameDescriptor::WeightRegister() { return a3; }", "name_and_para": "constexpr Register BaselineLeaveFrameDescriptor::WeightRegister() "}], [{"name": "BaselineLeaveFrameDescriptor::ParamsSizeRegister", "content": "constexpr Register BaselineLeaveFrameDescriptor::ParamsSizeRegister() {\n  return x3;\n}", "name_and_para": "constexpr Register BaselineLeaveFrameDescriptor::ParamsSizeRegister() "}, {"name": "BaselineLeaveFrameDescriptor::ParamsSizeRegister", "content": "constexpr Register BaselineLeaveFrameDescriptor::ParamsSizeRegister() {\n  return a2;\n}", "name_and_para": "constexpr Register BaselineLeaveFrameDescriptor::ParamsSizeRegister() "}], [{"name": "GrowArrayElementsDescriptor::KeyRegister", "content": "constexpr Register GrowArrayElementsDescriptor::KeyRegister() { return x3; }", "name_and_para": "constexpr Register GrowArrayElementsDescriptor::KeyRegister() "}, {"name": "GrowArrayElementsDescriptor::KeyRegister", "content": "constexpr Register GrowArrayElementsDescriptor::KeyRegister() { return a3; }", "name_and_para": "constexpr Register GrowArrayElementsDescriptor::KeyRegister() "}], [{"name": "GrowArrayElementsDescriptor::ObjectRegister", "content": "constexpr Register GrowArrayElementsDescriptor::ObjectRegister() { return x0; }", "name_and_para": "constexpr Register GrowArrayElementsDescriptor::ObjectRegister() "}, {"name": "GrowArrayElementsDescriptor::ObjectRegister", "content": "constexpr Register GrowArrayElementsDescriptor::ObjectRegister() { return a0; }", "name_and_para": "constexpr Register GrowArrayElementsDescriptor::ObjectRegister() "}], [{"name": "ApiGetterDescriptor::CallbackRegister", "content": "constexpr Register ApiGetterDescriptor::CallbackRegister() { return x3; }", "name_and_para": "constexpr Register ApiGetterDescriptor::CallbackRegister() "}, {"name": "ApiGetterDescriptor::CallbackRegister", "content": "constexpr Register ApiGetterDescriptor::CallbackRegister() { return a3; }", "name_and_para": "constexpr Register ApiGetterDescriptor::CallbackRegister() "}], [{"name": "ApiGetterDescriptor::HolderRegister", "content": "constexpr Register ApiGetterDescriptor::HolderRegister() { return x0; }", "name_and_para": "constexpr Register ApiGetterDescriptor::HolderRegister() "}, {"name": "ApiGetterDescriptor::HolderRegister", "content": "constexpr Register ApiGetterDescriptor::HolderRegister() { return a0; }", "name_and_para": "constexpr Register ApiGetterDescriptor::HolderRegister() "}], [{"name": "StoreTransitionDescriptor::MapRegister", "content": "constexpr Register StoreTransitionDescriptor::MapRegister() { return x5; }", "name_and_para": "constexpr Register StoreTransitionDescriptor::MapRegister() "}, {"name": "StoreTransitionDescriptor::MapRegister", "content": "constexpr Register StoreTransitionDescriptor::MapRegister() { return a5; }", "name_and_para": "constexpr Register StoreTransitionDescriptor::MapRegister() "}], [{"name": "DefineKeyedOwnDescriptor::FlagsRegister", "content": "constexpr Register DefineKeyedOwnDescriptor::FlagsRegister() { return x5; }", "name_and_para": "constexpr Register DefineKeyedOwnDescriptor::FlagsRegister() "}, {"name": "DefineKeyedOwnDescriptor::FlagsRegister", "content": "constexpr Register DefineKeyedOwnDescriptor::FlagsRegister() { return a5; }", "name_and_para": "constexpr Register DefineKeyedOwnDescriptor::FlagsRegister() "}], [{"name": "StoreWithVectorDescriptor::VectorRegister", "content": "constexpr Register StoreWithVectorDescriptor::VectorRegister() { return x3; }", "name_and_para": "constexpr Register StoreWithVectorDescriptor::VectorRegister() "}, {"name": "StoreWithVectorDescriptor::VectorRegister", "content": "constexpr Register StoreWithVectorDescriptor::VectorRegister() { return a3; }", "name_and_para": "constexpr Register StoreWithVectorDescriptor::VectorRegister() "}], [{"name": "StoreDescriptor::SlotRegister", "content": "constexpr Register StoreDescriptor::SlotRegister() { return x4; }", "name_and_para": "constexpr Register StoreDescriptor::SlotRegister() "}, {"name": "StoreDescriptor::SlotRegister", "content": "constexpr Register StoreDescriptor::SlotRegister() { return a4; }", "name_and_para": "constexpr Register StoreDescriptor::SlotRegister() "}], [{"name": "StoreDescriptor::ValueRegister", "content": "constexpr Register StoreDescriptor::ValueRegister() { return x0; }", "name_and_para": "constexpr Register StoreDescriptor::ValueRegister() "}, {"name": "StoreDescriptor::ValueRegister", "content": "constexpr Register StoreDescriptor::ValueRegister() { return a0; }", "name_and_para": "constexpr Register StoreDescriptor::ValueRegister() "}], [{"name": "StoreDescriptor::NameRegister", "content": "constexpr Register StoreDescriptor::NameRegister() { return x2; }", "name_and_para": "constexpr Register StoreDescriptor::NameRegister() "}, {"name": "StoreDescriptor::NameRegister", "content": "constexpr Register StoreDescriptor::NameRegister() { return a2; }", "name_and_para": "constexpr Register StoreDescriptor::NameRegister() "}], [{"name": "StoreDescriptor::ReceiverRegister", "content": "constexpr Register StoreDescriptor::ReceiverRegister() { return x1; }", "name_and_para": "constexpr Register StoreDescriptor::ReceiverRegister() "}, {"name": "StoreDescriptor::ReceiverRegister", "content": "constexpr Register StoreDescriptor::ReceiverRegister() { return a1; }", "name_and_para": "constexpr Register StoreDescriptor::ReceiverRegister() "}], [{"name": "LoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister", "content": "constexpr Register\nLoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() {\n  return x4;\n}", "name_and_para": "constexpr Register\nLoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() "}, {"name": "LoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister", "content": "constexpr Register\nLoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() {\n  return a4;\n}", "name_and_para": "constexpr Register\nLoadWithReceiverAndVectorDescriptor::LookupStartObjectRegister() "}], [{"name": "KeyedHasICWithVectorDescriptor::VectorRegister", "content": "constexpr Register KeyedHasICWithVectorDescriptor::VectorRegister() {\n  return x3;\n}", "name_and_para": "constexpr Register KeyedHasICWithVectorDescriptor::VectorRegister() "}, {"name": "KeyedHasICWithVectorDescriptor::VectorRegister", "content": "constexpr Register KeyedHasICWithVectorDescriptor::VectorRegister() {\n  return a3;\n}", "name_and_para": "constexpr Register KeyedHasICWithVectorDescriptor::VectorRegister() "}], [{"name": "KeyedHasICBaselineDescriptor::SlotRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::SlotRegister() { return x2; }", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::SlotRegister() "}, {"name": "KeyedHasICBaselineDescriptor::SlotRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::SlotRegister() { return a2; }", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::SlotRegister() "}], [{"name": "KeyedHasICBaselineDescriptor::NameRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::NameRegister() { return x1; }", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::NameRegister() "}, {"name": "KeyedHasICBaselineDescriptor::NameRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::NameRegister() { return a1; }", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::NameRegister() "}], [{"name": "KeyedHasICBaselineDescriptor::ReceiverRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::ReceiverRegister() {\n  return kInterpreterAccumulatorRegister;\n}", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::ReceiverRegister() "}, {"name": "KeyedHasICBaselineDescriptor::ReceiverRegister", "content": "constexpr Register KeyedHasICBaselineDescriptor::ReceiverRegister() {\n  return kInterpreterAccumulatorRegister;\n}", "name_and_para": "constexpr Register KeyedHasICBaselineDescriptor::ReceiverRegister() "}], [{"name": "EnumeratedKeyedLoadBaselineDescriptor::SlotRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::SlotRegister() {\n  return x2;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::SlotRegister() "}, {"name": "EnumeratedKeyedLoadBaselineDescriptor::SlotRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::SlotRegister() {\n  return a2;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::SlotRegister() "}], [{"name": "EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister() {\n  return x5;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister() "}, {"name": "EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister() {\n  return a5;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::CacheTypeRegister() "}], [{"name": "EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister() {\n  return x4;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister() "}, {"name": "EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister", "content": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister() {\n  return a4;\n}", "name_and_para": "constexpr Register EnumeratedKeyedLoadBaselineDescriptor::EnumIndexRegister() "}], [{"name": "KeyedLoadWithVectorDescriptor::VectorRegister", "content": "constexpr Register KeyedLoadWithVectorDescriptor::VectorRegister() {\n  return x3;\n}", "name_and_para": "constexpr Register KeyedLoadWithVectorDescriptor::VectorRegister() "}, {"name": "KeyedLoadWithVectorDescriptor::VectorRegister", "content": "constexpr Register KeyedLoadWithVectorDescriptor::VectorRegister() {\n  return a3;\n}", "name_and_para": "constexpr Register KeyedLoadWithVectorDescriptor::VectorRegister() "}], [{"name": "KeyedLoadBaselineDescriptor::SlotRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::SlotRegister() { return x2; }", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::SlotRegister() "}, {"name": "KeyedLoadBaselineDescriptor::SlotRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::SlotRegister() { return a2; }", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::SlotRegister() "}], [{"name": "KeyedLoadBaselineDescriptor::NameRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::NameRegister() {\n  return kInterpreterAccumulatorRegister;\n}", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::NameRegister() "}, {"name": "KeyedLoadBaselineDescriptor::NameRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::NameRegister() {\n  return kInterpreterAccumulatorRegister;\n}", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::NameRegister() "}], [{"name": "KeyedLoadBaselineDescriptor::ReceiverRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::ReceiverRegister() {\n  return x1;\n}", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::ReceiverRegister() "}, {"name": "KeyedLoadBaselineDescriptor::ReceiverRegister", "content": "constexpr Register KeyedLoadBaselineDescriptor::ReceiverRegister() {\n  return a1;\n}", "name_and_para": "constexpr Register KeyedLoadBaselineDescriptor::ReceiverRegister() "}], [{"name": "LoadWithVectorDescriptor::VectorRegister", "content": "constexpr Register LoadWithVectorDescriptor::VectorRegister() { return x3; }", "name_and_para": "constexpr Register LoadWithVectorDescriptor::VectorRegister() "}, {"name": "LoadWithVectorDescriptor::VectorRegister", "content": "constexpr Register LoadWithVectorDescriptor::VectorRegister() { return a3; }", "name_and_para": "constexpr Register LoadWithVectorDescriptor::VectorRegister() "}], [{"name": "LoadDescriptor::SlotRegister", "content": "constexpr Register LoadDescriptor::SlotRegister() { return x0; }", "name_and_para": "constexpr Register LoadDescriptor::SlotRegister() "}, {"name": "LoadDescriptor::SlotRegister", "content": "constexpr Register LoadDescriptor::SlotRegister() { return a0; }", "name_and_para": "constexpr Register LoadDescriptor::SlotRegister() "}], [{"name": "LoadDescriptor::NameRegister", "content": "constexpr Register LoadDescriptor::NameRegister() { return x2; }", "name_and_para": "constexpr Register LoadDescriptor::NameRegister() "}, {"name": "LoadDescriptor::NameRegister", "content": "constexpr Register LoadDescriptor::NameRegister() { return a2; }", "name_and_para": "constexpr Register LoadDescriptor::NameRegister() "}], [{"name": "LoadDescriptor::ReceiverRegister", "content": "constexpr Register LoadDescriptor::ReceiverRegister() { return x1; }", "name_and_para": "constexpr Register LoadDescriptor::ReceiverRegister() "}, {"name": "LoadDescriptor::ReceiverRegister", "content": "constexpr Register LoadDescriptor::ReceiverRegister() { return a1; }", "name_and_para": "constexpr Register LoadDescriptor::ReceiverRegister() "}], [{"name": "WriteBarrierDescriptor::registers", "content": "constexpr auto WriteBarrierDescriptor::registers() {\n  // TODO(leszeks): Remove x7 which is just there for padding.\n  return RegisterArray(x1, x5, x4, x2, x0, x3, kContextRegister, x7);\n}", "name_and_para": "constexpr auto WriteBarrierDescriptor::registers() "}, {"name": "WriteBarrierDescriptor::registers", "content": "constexpr auto WriteBarrierDescriptor::registers() {\n  // TODO(Yuxiang): Remove a7 which is just there for padding.\n  return RegisterArray(a1, a5, a4, a2, a0, a3, kContextRegister, a7);\n}", "name_and_para": "constexpr auto WriteBarrierDescriptor::registers() "}], [{"name": "StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount", "content": "void StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount(CallInterfaceDescriptorData* data, int argc) {\n  RegList allocatable_regs = data->allocatable_registers();\n  if (argc >= 1) DCHECK(allocatable_regs.has(x0));\n  if (argc >= 2) DCHECK(allocatable_regs.has(x1));\n  if (argc >= 3) DCHECK(allocatable_regs.has(x2));\n  if (argc >= 4) DCHECK(allocatable_regs.has(x3));\n  if (argc >= 5) DCHECK(allocatable_regs.has(x4));\n  if (argc >= 6) DCHECK(allocatable_regs.has(x5));\n  if (argc >= 7) DCHECK(allocatable_regs.has(x6));\n  if (argc >= 8) DCHECK(allocatable_regs.has(x7));\n}", "name_and_para": "void StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount(CallInterfaceDescriptorData* data, int argc) "}, {"name": "StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount", "content": "void StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount(CallInterfaceDescriptorData* data, int argc) {\n  RegList allocatable_regs = data->allocatable_registers();\n  if (argc >= 1) DCHECK(allocatable_regs.has(a0));\n  if (argc >= 2) DCHECK(allocatable_regs.has(a1));\n  if (argc >= 3) DCHECK(allocatable_regs.has(a2));\n  if (argc >= 4) DCHECK(allocatable_regs.has(a3));\n  if (argc >= 5) DCHECK(allocatable_regs.has(a4));\n  if (argc >= 6) DCHECK(allocatable_regs.has(a5));\n  if (argc >= 7) DCHECK(allocatable_regs.has(a6));\n  if (argc >= 8) DCHECK(allocatable_regs.has(a7));\n  // Additional arguments are passed on the stack.\n}", "name_and_para": "void StaticCallInterfaceDescriptor<DerivedDescriptor>::\n    VerifyArgumentRegisterCount(CallInterfaceDescriptorData* data, int argc) "}], [{"name": "CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray() {\n  // Padding to have as many double return registers as GP return registers.\n  auto registers = DoubleRegisterArray(kFPReturnRegister0, no_dreg, no_dreg);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray() "}, {"name": "CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray() {\n  // Padding to have as many double return registers as GP return registers.\n  auto registers = DoubleRegisterArray(kFPReturnRegister0, no_dreg, no_dreg);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultReturnDoubleRegisterArray() "}], [{"name": "CallInterfaceDescriptor::DefaultReturnRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultReturnRegisterArray() {\n  auto registers =\n      RegisterArray(kReturnRegister0, kReturnRegister1, kReturnRegister2);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultReturnRegisterArray() "}, {"name": "CallInterfaceDescriptor::DefaultReturnRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultReturnRegisterArray() {\n  auto registers =\n      RegisterArray(kReturnRegister0, kReturnRegister1, kReturnRegister2);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultReturnRegisterArray() "}], [{"name": "CallInterfaceDescriptor::DefaultDoubleRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultDoubleRegisterArray() {\n  auto registers = DoubleRegisterArray(d0, d1, d2, d3, d4, d5, d6);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultDoubleRegisterArray() "}, {"name": "CallInterfaceDescriptor::DefaultDoubleRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultDoubleRegisterArray() {\n  auto registers = DoubleRegisterArray(ft1, ft2, ft3, ft4, ft5, ft6, ft7);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultDoubleRegisterArray() "}], [{"name": "CallInterfaceDescriptor::DefaultRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {\n  auto registers = RegisterArray(x0, x1, x2, x3, x4);\n  static_assert(registers.size() == kMaxBuiltinRegisterParams);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() "}, {"name": "CallInterfaceDescriptor::DefaultRegisterArray", "content": "constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {\n  auto registers = RegisterArray(a0, a1, a2, a3, a4);\n  static_assert(registers.size() == kMaxBuiltinRegisterParams);\n  return registers;\n}", "name_and_para": "constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() "}]]], [["./v8/src/codegen/riscv/assembler-riscv.cc", "./v8/src/codegen/arm64/assembler-arm64.cc"], 0.04367816091954023, 0.24675324675324675, [[{"name": "Assembler::RecordRelocInfo", "content": "void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data,\n                                ConstantPoolMode constant_pool_mode) {\n  if (rmode == RelocInfo::INTERNAL_REFERENCE ||\n      rmode == RelocInfo::CONST_POOL || rmode == RelocInfo::VENEER_POOL ||\n      rmode == RelocInfo::DEOPT_SCRIPT_OFFSET ||\n      rmode == RelocInfo::DEOPT_INLINING_ID ||\n      rmode == RelocInfo::DEOPT_REASON || rmode == RelocInfo::DEOPT_ID ||\n      rmode == RelocInfo::DEOPT_NODE_ID) {\n    // Adjust code for new modes.\n    DCHECK(RelocInfo::IsDeoptReason(rmode) || RelocInfo::IsDeoptId(rmode) ||\n           RelocInfo::IsDeoptNodeId(rmode) ||\n           RelocInfo::IsDeoptPosition(rmode) ||\n           RelocInfo::IsInternalReference(rmode) ||\n           RelocInfo::IsConstPool(rmode) || RelocInfo::IsVeneerPool(rmode));\n    // These modes do not need an entry in the constant pool.\n  } else if (constant_pool_mode == NEEDS_POOL_ENTRY) {\n    if (RelocInfo::IsEmbeddedObjectMode(rmode)) {\n      Handle<HeapObject> handle(reinterpret_cast<Address*>(data));\n      data = AddEmbeddedObject(handle);\n    }\n    if (rmode == RelocInfo::COMPRESSED_EMBEDDED_OBJECT) {\n      if (constpool_.RecordEntry(static_cast<uint32_t>(data), rmode) ==\n          RelocInfoStatus::kMustOmitForDuplicate) {\n        return;\n      }\n    } else {\n      if (constpool_.RecordEntry(static_cast<uint64_t>(data), rmode) ==\n          RelocInfoStatus::kMustOmitForDuplicate) {\n        return;\n      }\n    }\n  }\n  // For modes that cannot use the constant pool, a different sequence of\n  // instructions will be emitted by this function's caller.\n\n  if (!ShouldRecordRelocInfo(rmode)) return;\n\n  // Callers should ensure that constant pool emission is blocked until the\n  // instruction the reloc info is associated with has been emitted.\n  DCHECK(constpool_.IsBlocked());\n\n  // We do not try to reuse pool constants.\n  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data);\n  DCHECK_GE(buffer_space(), kMaxRelocSize);  // too late to grow buffer here\n  reloc_info_writer.Write(&rinfo);\n}", "name_and_para": "void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data,\n                                ConstantPoolMode constant_pool_mode) "}, {"name": "Assembler::RecordRelocInfo", "content": "void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {\n  if (!ShouldRecordRelocInfo(rmode)) return;\n  // We do not try to reuse pool constants.\n  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data);\n  DCHECK_GE(buffer_space(), kMaxRelocSize);  // Too late to grow buffer here.\n  reloc_info_writer.Write(&rinfo);\n}", "name_and_para": "void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) "}], [{"name": "Assembler::GrowBuffer", "content": "void Assembler::GrowBuffer() {\n  // Compute new buffer size.\n  int old_size = buffer_->size();\n  int new_size = std::min(2 * old_size, old_size + 1 * MB);\n\n  // Some internal data structures overflow for very large buffers,\n  // they must ensure that kMaximalBufferSize is not too large.\n  if (new_size > kMaximalBufferSize) {\n    V8::FatalProcessOutOfMemory(nullptr, \"Assembler::GrowBuffer\");\n  }\n\n  // Set up new buffer.\n  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);\n  DCHECK_EQ(new_size, new_buffer->size());\n  uint8_t* new_start = new_buffer->start();\n\n  // Copy the data.\n  intptr_t pc_delta = new_start - buffer_start_;\n  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);\n  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();\n  memmove(new_start, buffer_start_, pc_offset());\n  memmove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),\n          reloc_size);\n\n  // Switch buffers.\n  buffer_ = std::move(new_buffer);\n  buffer_start_ = new_start;\n  pc_ += pc_delta;\n  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,\n                               reloc_info_writer.last_pc() + pc_delta);\n\n  // None of our relocation types are pc relative pointing outside the code\n  // buffer nor pc absolute pointing inside the code buffer, so there is no need\n  // to relocate any emitted relocation entries.\n\n  // Relocate internal references.\n  for (auto pos : internal_reference_positions_) {\n    Address address = reinterpret_cast<intptr_t>(buffer_start_) + pos;\n    intptr_t internal_ref = ReadUnalignedValue<intptr_t>(address);\n    internal_ref += pc_delta;\n    WriteUnalignedValue<intptr_t>(address, internal_ref);\n  }\n\n  // Pending relocation entries are also relative, no need to relocate.\n}", "name_and_para": "void Assembler::GrowBuffer() "}, {"name": "Assembler::GrowBuffer", "content": "void Assembler::GrowBuffer() {\n  DEBUG_PRINTF(\"GrowBuffer: %p -> \", buffer_start_);\n  // Compute new buffer size.\n  int old_size = buffer_->size();\n  int new_size = std::min(2 * old_size, old_size + 1 * MB);\n\n  // Some internal data structures overflow for very large buffers,\n  // they must ensure that kMaximalBufferSize is not too large.\n  if (new_size > kMaximalBufferSize) {\n    V8::FatalProcessOutOfMemory(nullptr, \"Assembler::GrowBuffer\");\n  }\n\n  // Set up new buffer.\n  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);\n  DCHECK_EQ(new_size, new_buffer->size());\n  uint8_t* new_start = new_buffer->start();\n\n  // Copy the data.\n  intptr_t pc_delta = new_start - buffer_start_;\n  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);\n  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();\n  MemMove(new_start, buffer_start_, pc_offset());\n  MemMove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),\n          reloc_size);\n\n  // Switch buffers.\n  buffer_ = std::move(new_buffer);\n  buffer_start_ = new_start;\n  DEBUG_PRINTF(\"%p\\n\", buffer_start_);\n  pc_ += pc_delta;\n  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,\n                               reloc_info_writer.last_pc() + pc_delta);\n\n  // Relocate runtime entries.\n  base::Vector<uint8_t> instructions{buffer_start_,\n                                     static_cast<size_t>(pc_offset())};\n  base::Vector<const uint8_t> reloc_info{reloc_info_writer.pos(), reloc_size};\n  for (RelocIterator it(instructions, reloc_info, 0); !it.done(); it.next()) {\n    RelocInfo::Mode rmode = it.rinfo()->rmode();\n    if (rmode == RelocInfo::INTERNAL_REFERENCE) {\n      RelocateInternalReference(rmode, it.rinfo()->pc(), pc_delta);\n    }\n  }\n\n  DCHECK(!overflow());\n}", "name_and_para": "void Assembler::GrowBuffer() "}], [{"name": "Operand::EmbeddedNumber", "content": "Operand Operand::EmbeddedNumber(double number) {\n  int32_t smi;\n  if (DoubleToSmiInteger(number, &smi)) {\n    return Operand(Immediate(Smi::FromInt(smi)));\n  }\n  return EmbeddedHeapNumber(number);\n}", "name_and_para": "Operand Operand::EmbeddedNumber(double number) "}, {"name": "Operand::EmbeddedNumber", "content": "Operand Operand::EmbeddedNumber(double value) {\n  int32_t smi;\n  if (DoubleToSmiInteger(value, &smi)) return Operand(Smi::FromInt(smi));\n  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);\n  result.is_heap_number_request_ = true;\n  result.value_.heap_number_request = HeapNumberRequest(value);\n  return result;\n}", "name_and_para": "Operand Operand::EmbeddedNumber(double value) "}], [{"name": "Assembler::nop", "content": "void Assembler::nop(NopMarkerTypes n) {\n  DCHECK((FIRST_NOP_MARKER <= n) && (n <= LAST_NOP_MARKER));\n  mov(Register::XRegFromCode(n), Register::XRegFromCode(n));\n}", "name_and_para": "void Assembler::nop(NopMarkerTypes n) "}, {"name": "Assembler::nop", "content": "void Assembler::nop() { addi(ToRegister(0), ToRegister(0), 0); }", "name_and_para": "void Assembler::nop() "}], [{"name": "Assembler::bind", "content": "void Assembler::bind(Label* label) {\n  // Bind label to the address at pc_. All instructions (most likely branches)\n  // that are linked to this label will be updated to point to the newly-bound\n  // label.\n\n  DCHECK(!label->is_near_linked());\n  DCHECK(!label->is_bound());\n\n  DeleteUnresolvedBranchInfoForLabel(label);\n\n  // If the label is linked, the link chain looks something like this:\n  //\n  // |--I----I-------I-------L\n  // |---------------------->| pc_offset\n  // |-------------->|         linkoffset = label->pos()\n  //         |<------|         link->ImmPCOffset()\n  // |------>|                 prevlinkoffset = linkoffset + link->ImmPCOffset()\n  //\n  // On each iteration, the last link is updated and then removed from the\n  // chain until only one remains. At that point, the label is bound.\n  //\n  // If the label is not linked, no preparation is required before binding.\n  while (label->is_linked()) {\n    int linkoffset = label->pos();\n    Instruction* link = InstructionAt(linkoffset);\n    int prevlinkoffset = linkoffset + static_cast<int>(link->ImmPCOffset());\n\n    CheckLabelLinkChain(label);\n\n    DCHECK_GE(linkoffset, 0);\n    DCHECK(linkoffset < pc_offset());\n    DCHECK((linkoffset > prevlinkoffset) ||\n           (linkoffset - prevlinkoffset == kStartOfLabelLinkChain));\n    DCHECK_GE(prevlinkoffset, 0);\n\n    // Update the link to point to the label.\n    if (link->IsUnresolvedInternalReference()) {\n      // Internal references do not get patched to an instruction but directly\n      // to an address.\n      internal_reference_positions_.push_back(linkoffset);\n      memcpy(link, &pc_, kSystemPointerSize);\n    } else {\n      link->SetImmPCOffsetTarget(options(),\n                                 reinterpret_cast<Instruction*>(pc_));\n\n      // Discard back edge data for this link.\n      branch_link_chain_back_edge_.erase(\n          static_cast<int>(InstructionOffset(link)));\n    }\n\n    // Link the label to the previous link in the chain.\n    if (linkoffset - prevlinkoffset == kStartOfLabelLinkChain) {\n      // We hit kStartOfLabelLinkChain, so the chain is fully processed.\n      label->Unuse();\n    } else {\n      // Update the label for the next iteration.\n      label->link_to(prevlinkoffset);\n    }\n  }\n  label->bind_to(pc_offset());\n\n  DCHECK(label->is_bound());\n  DCHECK(!label->is_linked());\n}", "name_and_para": "void Assembler::bind(Label* label) "}, {"name": "Assembler::bind", "content": "void Assembler::bind(Label* L) {\n  DCHECK(!L->is_bound());  // Label can only be bound once.\n  bind_to(L, pc_offset());\n}", "name_and_para": "void Assembler::bind(Label* L) "}], [{"name": "Assembler::CodeTargetAlign", "content": "void Assembler::CodeTargetAlign() {\n  // Preferred alignment of jump targets on some ARM chips.\n#if !defined(V8_TARGET_OS_MACOS)\n  Align(8);\n#endif\n}", "name_and_para": "void Assembler::CodeTargetAlign() "}, {"name": "Assembler::CodeTargetAlign", "content": "void Assembler::CodeTargetAlign() {\n  // No advantage to aligning branch/call targets to more than\n  // single instruction, that I am aware of.\n  Align(4);\n}", "name_and_para": "void Assembler::CodeTargetAlign() "}], [{"name": "Assembler::Align", "content": "void Assembler::Align(int m) {\n  // If not, the loop below won't terminate.\n  DCHECK(IsAligned(pc_offset(), kInstrSize));\n  DCHECK(m >= kInstrSize && base::bits::IsPowerOfTwo(m));\n  while ((pc_offset() & (m - 1)) != 0) {\n    nop();\n  }\n}", "name_and_para": "void Assembler::Align(int m) "}, {"name": "Assembler::Align", "content": "void Assembler::Align(int m) {\n  DCHECK(m >= 4 && base::bits::IsPowerOfTwo(m));\n  while ((pc_offset() & (m - 1)) != 0) {\n    NOP();\n  }\n}", "name_and_para": "void Assembler::Align(int m) "}], [{"name": "Assembler::GetCode", "content": "void Assembler::GetCode(Isolate* isolate, CodeDesc* desc) {\n  GetCode(isolate->main_thread_local_isolate(), desc);\n}", "name_and_para": "void Assembler::GetCode(Isolate* isolate, CodeDesc* desc) "}, {"name": "Assembler::GetCode", "content": "void Assembler::GetCode(LocalIsolate* isolate, CodeDesc* desc,\n                        SafepointTableBuilder* safepoint_table_builder,\n                        int handler_table_offset) {\n  // As a crutch to avoid having to add manual Align calls wherever we use a\n  // raw workflow to create InstructionStream objects (mostly in tests), add\n  // another Align call here. It does no harm - the end of the InstructionStream\n  // object is aligned to the (larger) kCodeAlignment anyways.\n  // TODO(jgruber): Consider moving responsibility for proper alignment to\n  // metadata table builders (safepoint, handler, constant pool, code\n  // comments).\n  DataAlign(InstructionStream::kMetadataAlignment);\n\n  ForceConstantPoolEmissionWithoutJump();\n\n  int code_comments_size = WriteCodeComments();\n\n  DCHECK(pc_ <= reloc_info_writer.pos());  // No overlap.\n\n  AllocateAndInstallRequestedHeapNumbers(isolate);\n\n  // Set up code descriptor.\n  // TODO(jgruber): Reconsider how these offsets and sizes are maintained up to\n  // this point to make CodeDesc initialization less fiddly.\n\n  static constexpr int kConstantPoolSize = 0;\n  const int instruction_size = pc_offset();\n  const int code_comments_offset = instruction_size - code_comments_size;\n  const int constant_pool_offset = code_comments_offset - kConstantPoolSize;\n  const int handler_table_offset2 = (handler_table_offset == kNoHandlerTable)\n                                        ? constant_pool_offset\n                                        : handler_table_offset;\n  const int safepoint_table_offset =\n      (safepoint_table_builder == kNoSafepointTable)\n          ? handler_table_offset2\n          : safepoint_table_builder->safepoint_table_offset();\n  const int reloc_info_offset =\n      static_cast<int>(reloc_info_writer.pos() - buffer_->start());\n  CodeDesc::Initialize(desc, this, safepoint_table_offset,\n                       handler_table_offset2, constant_pool_offset,\n                       code_comments_offset, reloc_info_offset);\n}", "name_and_para": "void Assembler::GetCode(LocalIsolate* isolate, CodeDesc* desc,\n                        SafepointTableBuilder* safepoint_table_builder,\n                        int handler_table_offset) "}], [{"name": "Assembler::AllocateAndInstallRequestedHeapNumbers", "content": "void Assembler::AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate) {\n  DCHECK_IMPLIES(isolate == nullptr, heap_number_requests_.empty());\n  for (auto& request : heap_number_requests_) {\n    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();\n    Handle<HeapObject> object =\n        isolate->factory()->NewHeapNumber<AllocationType::kOld>(\n            request.heap_number());\n    EmbeddedObjectIndex index = AddEmbeddedObject(object);\n    set_embedded_object_index_referenced_from(pc, index);\n  }\n}", "name_and_para": "void Assembler::AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate) "}, {"name": "Assembler::AllocateAndInstallRequestedHeapNumbers", "content": "void Assembler::AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate) {\n  DCHECK_IMPLIES(isolate == nullptr, heap_number_requests_.empty());\n  for (auto& request : heap_number_requests_) {\n    Handle<HeapObject> object =\n        isolate->factory()->NewHeapNumber<AllocationType::kOld>(\n            request.heap_number());\n    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();\n    set_target_value_at(pc, reinterpret_cast<uintptr_t>(object.location()));\n  }\n}", "name_and_para": "void Assembler::AllocateAndInstallRequestedHeapNumbers(LocalIsolate* isolate) "}], [{"name": "Assembler::AbortedCodeGeneration", "content": "void Assembler::AbortedCodeGeneration() { constpool_.Clear(); }", "name_and_para": "void Assembler::AbortedCodeGeneration() "}, {"name": "Assembler::AbortedCodeGeneration", "content": "void Assembler::AbortedCodeGeneration() { constpool_.Clear(); }", "name_and_para": "void Assembler::AbortedCodeGeneration() "}], [{"name": "Assembler::~Assembler", "content": "Assembler::~Assembler() {\n  DCHECK(constpool_.IsEmpty());\n  DCHECK_EQ(veneer_pool_blocked_nesting_, 0);\n}", "name_and_para": "Assembler::~Assembler() "}, {"name": "Assembler::~Assembler", "content": "Assembler::~Assembler() { CHECK(constpool_.IsEmpty()); }", "name_and_para": "Assembler::~Assembler() "}], [{"name": "Assembler::Assembler", "content": "Assembler::Assembler(const AssemblerOptions& options,\n                     std::unique_ptr<AssemblerBuffer> buffer)\n    : AssemblerBase(options, std::move(buffer)),\n      unresolved_branches_(),\n      constpool_(this) {\n  veneer_pool_blocked_nesting_ = 0;\n  Reset();\n\n#if defined(V8_OS_WIN)\n  if (options.collect_win64_unwind_info) {\n    xdata_encoder_ = std::make_unique<win64_unwindinfo::XdataEncoder>(*this);\n  }\n#endif\n}", "name_and_para": "Assembler::Assembler(const AssemblerOptions& options,\n                     std::unique_ptr<AssemblerBuffer> buffer)\n    : AssemblerBase(options, std::move(buffer)),\n      unresolved_branches_(),\n      constpool_(this) "}, {"name": "Assembler::Assembler", "content": "Assembler::Assembler(const AssemblerOptions& options,\n                     std::unique_ptr<AssemblerBuffer> buffer)\n    : AssemblerBase(options, std::move(buffer)),\n      VU(this),\n      scratch_register_list_({t3, t5}),\n      constpool_(this) {\n  reloc_info_writer.Reposition(buffer_start_ + buffer_->size(), pc_);\n\n  last_trampoline_pool_end_ = 0;\n  no_trampoline_pool_before_ = 0;\n  trampoline_pool_blocked_nesting_ = 0;\n  // We leave space (16 * kTrampolineSlotsSize)\n  // for BlockTrampolinePoolScope buffer.\n  next_buffer_check_ = v8_flags.force_long_branches\n                           ? kMaxInt\n                           : kMaxBranchOffset - kTrampolineSlotsSize * 16;\n  internal_trampoline_exception_ = false;\n  last_bound_pos_ = 0;\n\n  trampoline_emitted_ = v8_flags.force_long_branches;\n  unbound_labels_count_ = 0;\n  block_buffer_growth_ = false;\n}", "name_and_para": "Assembler::Assembler(const AssemblerOptions& options,\n                     std::unique_ptr<AssemblerBuffer> buffer)\n    : AssemblerBase(options, std::move(buffer)),\n      VU(this),\n      scratch_register_list_({t3, t5}),\n      constpool_(this) "}], [{"name": "RelocInfo::wasm_call_tag", "content": "uint32_t RelocInfo::wasm_call_tag() const {\n  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);\n  Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n  if (instr->IsLdrLiteralX()) {\n    return static_cast<uint32_t>(\n        Memory<Address>(Assembler::target_pointer_address_at(pc_)));\n  } else {\n    DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());\n    return static_cast<uint32_t>(instr->ImmPCOffset() / kInstrSize);\n  }\n}", "name_and_para": "uint32_t RelocInfo::wasm_call_tag() const "}, {"name": "RelocInfo::wasm_call_tag", "content": "uint32_t RelocInfo::wasm_call_tag() const {\n  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);\n  return static_cast<uint32_t>(\n      Assembler::target_address_at(pc_, constant_pool_));\n}", "name_and_para": "uint32_t RelocInfo::wasm_call_tag() const "}], [{"name": "RelocInfo::IsInConstantPool", "content": "bool RelocInfo::IsInConstantPool() {\n  Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n  DCHECK_IMPLIES(instr->IsLdrLiteralW(), COMPRESS_POINTERS_BOOL);\n  return instr->IsLdrLiteralX() ||\n         (COMPRESS_POINTERS_BOOL && instr->IsLdrLiteralW());\n}", "name_and_para": "bool RelocInfo::IsInConstantPool() "}, {"name": "RelocInfo::IsInConstantPool", "content": "bool RelocInfo::IsInConstantPool() { return false; }", "name_and_para": "bool RelocInfo::IsInConstantPool() "}], [{"name": "RelocInfo::IsCodedSpecially", "content": "bool RelocInfo::IsCodedSpecially() {\n  // The deserializer needs to know whether a pointer is specially coded. Being\n  // specially coded on ARM64 means that it is an immediate branch.\n  Instruction* instr = reinterpret_cast<Instruction*>(pc_);\n  if (instr->IsLdrLiteralX()) {\n    return false;\n  } else {\n    DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());\n    return true;\n  }\n}", "name_and_para": "bool RelocInfo::IsCodedSpecially() "}, {"name": "RelocInfo::IsCodedSpecially", "content": "bool RelocInfo::IsCodedSpecially() {\n  // The deserializer needs to know whether a pointer is specially coded.  Being\n  // specially coded on RISC-V means that it is a lui/addi instruction, and that\n  // is always the case inside code objects.\n  return true;\n}", "name_and_para": "bool RelocInfo::IsCodedSpecially() "}], [{"name": "CpuFeatures::PrintFeatures", "content": "void CpuFeatures::PrintFeatures() {}", "name_and_para": "void CpuFeatures::PrintFeatures() "}, {"name": "CpuFeatures::PrintFeatures", "content": "void CpuFeatures::PrintFeatures() {\n  printf(\"supports_wasm_simd_128=%d\\n\", CpuFeatures::SupportsWasmSimd128());\n  printf(\"zba=%d,zbb=%d,zbs=%d\\n\", CpuFeatures::IsSupported(ZBA),\n         CpuFeatures::IsSupported(ZBB), CpuFeatures::IsSupported(ZBS));\n}", "name_and_para": "void CpuFeatures::PrintFeatures() "}], [{"name": "CpuFeatures::PrintTarget", "content": "void CpuFeatures::PrintTarget() {}", "name_and_para": "void CpuFeatures::PrintTarget() "}, {"name": "CpuFeatures::PrintTarget", "content": "void CpuFeatures::PrintTarget() {}", "name_and_para": "void CpuFeatures::PrintTarget() "}], [{"name": "CpuFeatures::ProbeImpl", "content": "void CpuFeatures::ProbeImpl(bool cross_compile) {\n  // Only use statically determined features for cross compile (snapshot).\n  if (cross_compile) {\n    supported_ |= CpuFeaturesFromCompiler();\n    supported_ |= CpuFeaturesFromTargetOS();\n    return;\n  }\n\n  // We used to probe for coherent cache support, but on older CPUs it\n  // causes crashes (crbug.com/524337), and newer CPUs don't even have\n  // the feature any more.\n\n#ifdef USE_SIMULATOR\n  supported_ |= SimulatorFeaturesFromCommandLine();\n#else\n  // Probe for additional features at runtime.\n  base::CPU cpu;\n  unsigned runtime = 0;\n  if (cpu.has_jscvt()) {\n    runtime |= 1u << JSCVT;\n  }\n  if (cpu.has_dot_prod()) {\n    runtime |= 1u << DOTPROD;\n  }\n  if (cpu.has_lse()) {\n    runtime |= 1u << LSE;\n  }\n  if (cpu.has_pmull1q()) {\n    runtime |= 1u << PMULL1Q;\n  }\n\n  // Use the best of the features found by CPU detection and those inferred from\n  // the build system.\n  supported_ |= CpuFeaturesFromCompiler();\n  supported_ |= runtime;\n#endif  // USE_SIMULATOR\n\n  // Set a static value on whether Simd is supported.\n  // This variable is only used for certain archs to query SupportWasmSimd128()\n  // at runtime in builtins using an extern ref. Other callers should use\n  // CpuFeatures::SupportWasmSimd128().\n  CpuFeatures::supports_wasm_simd_128_ = CpuFeatures::SupportsWasmSimd128();\n}", "name_and_para": "void CpuFeatures::ProbeImpl(bool cross_compile) "}, {"name": "CpuFeatures::ProbeImpl", "content": "void CpuFeatures::ProbeImpl(bool cross_compile) {\n  supported_ |= CpuFeaturesImpliedByCompiler();\n  // Only use statically determined features for cross compile (snapshot).\n  if (cross_compile) return;\n  // Probe for additional features at runtime.\n\n#ifdef USE_SIMULATOR\n  supported_ |= SimulatorFeatures();\n#else\n  base::CPU cpu;\n  if (cpu.has_fpu()) supported_ |= 1u << FPU;\n  if (cpu.has_rvv()) supported_ |= 1u << RISCV_SIMD;\n#ifdef V8_COMPRESS_POINTERS\n  if (cpu.riscv_mmu() == base::CPU::RV_MMU_MODE::kRiscvSV57) {\n    FATAL(\"SV57 is not supported\");\n    UNIMPLEMENTED();\n  }\n#endif  // V8_COMPRESS_POINTERS\n#endif  // USE_SIMULATOR\n  // Set a static value on whether SIMD is supported.\n  // This variable is only used for certain archs to query SupportWasmSimd128()\n  // at runtime in builtins using an extern ref. Other callers should use\n  // CpuFeatures::SupportWasmSimd128().\n  CpuFeatures::supports_wasm_simd_128_ = CpuFeatures::SupportsWasmSimd128();\n}", "name_and_para": "void CpuFeatures::ProbeImpl(bool cross_compile) "}], [{"name": "CpuFeatures::SupportsWasmSimd128", "content": "bool CpuFeatures::SupportsWasmSimd128() { return true; }", "name_and_para": "bool CpuFeatures::SupportsWasmSimd128() "}, {"name": "CpuFeatures::SupportsWasmSimd128", "content": "bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(RISCV_SIMD); }", "name_and_para": "bool CpuFeatures::SupportsWasmSimd128() "}]]], [["./v8/src/codegen/riscv/macro-assembler-riscv.cc", "./v8/src/codegen/arm64/macro-assembler-arm64.cc"], 0.14798206278026907, 0.13095238095238096, [[{"name": "MacroAssembler::RecordWrite", "content": "void MacroAssembler::RecordWrite(Register object, Operand offset,\n                                 Register value, LinkRegisterStatus lr_status,\n                                 SaveFPRegsMode fp_mode, SmiCheck smi_check,\n                                 SlotDescriptor slot) {\n  ASM_CODE_COMMENT(this);\n  ASM_LOCATION_IN_ASSEMBLER(\"MacroAssembler::RecordWrite\");\n  DCHECK(!AreAliased(object, value));\n\n  if (v8_flags.debug_code) {\n    ASM_CODE_COMMENT_STRING(this, \"Verify slot_address\");\n    UseScratchRegisterScope temps(this);\n    Register temp = temps.AcquireX();\n    DCHECK(!AreAliased(object, value, temp));\n    Add(temp, object, offset);\n    if (slot.contains_indirect_pointer()) {\n      LoadIndirectPointerField(temp, MemOperand(temp),\n                               slot.indirect_pointer_tag());\n    } else {\n      DCHECK(slot.contains_direct_pointer());\n      LoadTaggedField(temp, MemOperand(temp));\n    }\n    Cmp(temp, value);\n    Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);\n  }\n\n  if (v8_flags.disable_write_barriers) {\n    return;\n  }\n\n  // First, check if a write barrier is even needed. The tests below\n  // catch stores of smis and stores into the young generation.\n  Label done;\n\n  if (smi_check == SmiCheck::kInline) {\n    DCHECK_EQ(0, kSmiTag);\n    JumpIfSmi(value, &done);\n  }\n\n  if (slot.contains_indirect_pointer()) {\n    // The indirect pointer write barrier is only enabled during marking.\n    JumpIfNotMarking(&done);\n  } else {\n    CheckPageFlag(value, MemoryChunk::kPointersToHereAreInterestingMask, eq,\n                  &done);\n\n    CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask, eq,\n                  &done);\n  }\n\n  // Record the actual write.\n  if (lr_status == kLRHasNotBeenSaved) {\n    Push<MacroAssembler::kSignLR>(padreg, lr);\n  }\n  Register slot_address = WriteBarrierDescriptor::SlotAddressRegister();\n  DCHECK(!AreAliased(object, slot_address, value));\n  if (slot.contains_direct_pointer()) {\n    // TODO(cbruni): Turn offset into int.\n    DCHECK(offset.IsImmediate());\n    Add(slot_address, object, offset);\n    CallRecordWriteStub(object, slot_address, fp_mode,\n                        StubCallMode::kCallBuiltinPointer);\n  } else {\n    DCHECK(slot.contains_indirect_pointer());\n    CallIndirectPointerBarrier(object, offset, fp_mode,\n                               slot.indirect_pointer_tag());\n  }\n  if (lr_status == kLRHasNotBeenSaved) {\n    Pop<MacroAssembler::kAuthLR>(lr, padreg);\n  }\n  if (v8_flags.debug_code) Mov(slot_address, Operand(kZapValue));\n\n  Bind(&done);\n}", "name_and_para": "void MacroAssembler::RecordWrite(Register object, Operand offset,\n                                 Register value, LinkRegisterStatus lr_status,\n                                 SaveFPRegsMode fp_mode, SmiCheck smi_check,\n                                 SlotDescriptor slot) "}, {"name": "MacroAssembler::RecordWrite", "content": "void MacroAssembler::RecordWrite(Register object, Operand offset,\n                                 Register value, RAStatus ra_status,\n                                 SaveFPRegsMode fp_mode, SmiCheck smi_check,\n                                 SlotDescriptor slot) {\n  DCHECK(!AreAliased(object, value));\n\n  if (v8_flags.debug_code) {\n    UseScratchRegisterScope temps(this);\n    Register temp = temps.Acquire();\n    DCHECK(!AreAliased(object, value, temp));\n    AddWord(temp, object, offset);\n#ifdef V8_TARGET_ARCH_RISCV64\n    if (slot.contains_indirect_pointer()) {\n      LoadIndirectPointerField(temp, MemOperand(temp, 0),\n                               slot.indirect_pointer_tag());\n    } else {\n      DCHECK(slot.contains_direct_pointer());\n      LoadTaggedField(temp, MemOperand(temp, 0));\n    }\n#else\n    LoadTaggedField(temp, MemOperand(temp));\n#endif\n    Assert(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite, temp,\n           Operand(value));\n  }\n\n  if (v8_flags.disable_write_barriers) {\n    return;\n  }\n\n  // First, check if a write barrier is even needed. The tests below\n  // catch stores of smis and stores into the young generation.\n  Label done;\n\n  if (smi_check == SmiCheck::kInline) {\n    DCHECK_EQ(0, kSmiTag);\n    JumpIfSmi(value, &done);\n  }\n\n  {\n    UseScratchRegisterScope temps(this);\n    CheckPageFlag(value, MemoryChunk::kPointersToHereAreInterestingMask,\n                  eq,  // In RISC-V, it uses cc for a comparison with 0, so if\n                       // no bits are set, and cc is eq, it will branch to done\n                  &done);\n\n    CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                  eq,  // In RISC-V, it uses cc for a comparison with 0, so if\n                       // no bits are set, and cc is eq, it will branch to done\n                  &done);\n  }\n  // Record the actual write.\n  if (ra_status == kRAHasNotBeenSaved) {\n    push(ra);\n  }\n  Register slot_address = WriteBarrierDescriptor::SlotAddressRegister();\n  DCHECK(!AreAliased(object, slot_address, value));\n  // TODO(cbruni): Turn offset into int.\n  if (slot.contains_direct_pointer()) {\n    DCHECK(offset.IsImmediate());\n    AddWord(slot_address, object, offset);\n    CallRecordWriteStub(object, slot_address, fp_mode,\n                        StubCallMode::kCallBuiltinPointer);\n  } else {\n    DCHECK(slot.contains_indirect_pointer());\n    CallIndirectPointerBarrier(object, offset, fp_mode,\n                               slot.indirect_pointer_tag());\n  }\n  if (ra_status == kRAHasNotBeenSaved) {\n    pop(ra);\n  }\n  if (v8_flags.debug_code) li(slot_address, Operand(kZapValue));\n\n  bind(&done);\n}", "name_and_para": "void MacroAssembler::RecordWrite(Register object, Operand offset,\n                                 Register value, RAStatus ra_status,\n                                 SaveFPRegsMode fp_mode, SmiCheck smi_check,\n                                 SlotDescriptor slot) "}], [{"name": "MacroAssembler::MoveObjectAndSlot", "content": "void MacroAssembler::MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                                       Register object, Operand offset) {\n  ASM_CODE_COMMENT(this);\n  DCHECK_NE(dst_object, dst_slot);\n  // If `offset` is a register, it cannot overlap with `object`.\n  DCHECK_IMPLIES(!offset.IsImmediate(), offset.reg() != object);\n\n  // If the slot register does not overlap with the object register, we can\n  // overwrite it.\n  if (dst_slot != object) {\n    Add(dst_slot, object, offset);\n    Mov(dst_object, object);\n    return;\n  }\n\n  DCHECK_EQ(dst_slot, object);\n\n  // If the destination object register does not overlap with the offset\n  // register, we can overwrite it.\n  if (offset.IsImmediate() || (offset.reg() != dst_object)) {\n    Mov(dst_object, dst_slot);\n    Add(dst_slot, dst_slot, offset);\n    return;\n  }\n\n  DCHECK_EQ(dst_object, offset.reg());\n\n  // We only have `dst_slot` and `dst_object` left as distinct registers so we\n  // have to swap them. We write this as a add+sub sequence to avoid using a\n  // scratch register.\n  Add(dst_slot, dst_slot, dst_object);\n  Sub(dst_object, dst_slot, dst_object);\n}", "name_and_para": "void MacroAssembler::MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                                       Register object, Operand offset) "}, {"name": "MacroAssembler::MoveObjectAndSlot", "content": "void MacroAssembler::MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                                       Register object, Operand offset) {\n  ASM_CODE_COMMENT(this);\n  DCHECK_NE(dst_object, dst_slot);\n  // If `offset` is a register, it cannot overlap with `object`.\n  DCHECK_IMPLIES(!offset.IsImmediate(), offset.rm() != object);\n\n  // If the slot register does not overlap with the object register, we can\n  // overwrite it.\n  if (dst_slot != object) {\n    AddWord(dst_slot, object, offset);\n    mv(dst_object, object);\n    return;\n  }\n\n  DCHECK_EQ(dst_slot, object);\n\n  // If the destination object register does not overlap with the offset\n  // register, we can overwrite it.\n  if (offset.IsImmediate() || (offset.rm() != dst_object)) {\n    mv(dst_object, dst_slot);\n    AddWord(dst_slot, dst_slot, offset);\n    return;\n  }\n\n  DCHECK_EQ(dst_object, offset.rm());\n\n  // We only have `dst_slot` and `dst_object` left as distinct registers so we\n  // have to swap them. We write this as a add+sub sequence to avoid using a\n  // scratch register.\n  AddWord(dst_slot, dst_slot, dst_object);\n  SubWord(dst_object, dst_slot, dst_object);\n}", "name_and_para": "void MacroAssembler::MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                                       Register object, Operand offset) "}], [{"name": "MacroAssembler::CallRecordWriteStubSaveRegisters", "content": "void MacroAssembler::CallRecordWriteStubSaveRegisters(Register object,\n                                                      Operand offset,\n                                                      SaveFPRegsMode fp_mode,\n                                                      StubCallMode mode) {\n  ASM_CODE_COMMENT(this);\n  RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  Register object_parameter = WriteBarrierDescriptor::ObjectRegister();\n  Register slot_address_parameter =\n      WriteBarrierDescriptor::SlotAddressRegister();\n  MoveObjectAndSlot(object_parameter, slot_address_parameter, object, offset);\n\n  CallRecordWriteStub(object_parameter, slot_address_parameter, fp_mode, mode);\n\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallRecordWriteStubSaveRegisters(Register object,\n                                                      Operand offset,\n                                                      SaveFPRegsMode fp_mode,\n                                                      StubCallMode mode) "}, {"name": "MacroAssembler::CallRecordWriteStubSaveRegisters", "content": "void MacroAssembler::CallRecordWriteStubSaveRegisters(Register object,\n                                                      Operand offset,\n                                                      SaveFPRegsMode fp_mode,\n                                                      StubCallMode mode) {\n  ASM_CODE_COMMENT(this);\n  RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  Register object_parameter = WriteBarrierDescriptor::ObjectRegister();\n  Register slot_address_parameter =\n      WriteBarrierDescriptor::SlotAddressRegister();\n\n  MoveObjectAndSlot(object_parameter, slot_address_parameter, object, offset);\n\n  CallRecordWriteStub(object_parameter, slot_address_parameter, fp_mode, mode);\n\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallRecordWriteStubSaveRegisters(Register object,\n                                                      Operand offset,\n                                                      SaveFPRegsMode fp_mode,\n                                                      StubCallMode mode) "}], [{"name": "MacroAssembler::CallIndirectPointerBarrier", "content": "void MacroAssembler::CallIndirectPointerBarrier(Register object, Operand offset,\n                                                SaveFPRegsMode fp_mode,\n                                                IndirectPointerTag tag) {\n  ASM_CODE_COMMENT(this);\n  RegList registers =\n      IndirectPointerWriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  MoveObjectAndSlot(\n      IndirectPointerWriteBarrierDescriptor::ObjectRegister(),\n      IndirectPointerWriteBarrierDescriptor::SlotAddressRegister(), object,\n      offset);\n  Mov(IndirectPointerWriteBarrierDescriptor::IndirectPointerTagRegister(),\n      Operand(tag));\n\n  CallBuiltin(Builtins::IndirectPointerBarrier(fp_mode));\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallIndirectPointerBarrier(Register object, Operand offset,\n                                                SaveFPRegsMode fp_mode,\n                                                IndirectPointerTag tag) "}, {"name": "MacroAssembler::CallIndirectPointerBarrier", "content": "void MacroAssembler::CallIndirectPointerBarrier(Register object, Operand offset,\n                                                SaveFPRegsMode fp_mode,\n                                                IndirectPointerTag tag) {\n  ASM_CODE_COMMENT(this);\n  RegList registers =\n      IndirectPointerWriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  MoveObjectAndSlot(\n      IndirectPointerWriteBarrierDescriptor::ObjectRegister(),\n      IndirectPointerWriteBarrierDescriptor::SlotAddressRegister(), object,\n      offset);\n  li(IndirectPointerWriteBarrierDescriptor::IndirectPointerTagRegister(),\n     Operand(tag));\n\n  CallBuiltin(Builtins::IndirectPointerBarrier(fp_mode));\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallIndirectPointerBarrier(Register object, Operand offset,\n                                                SaveFPRegsMode fp_mode,\n                                                IndirectPointerTag tag) "}], [{"name": "MacroAssembler::CallEphemeronKeyBarrier", "content": "void MacroAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,\n                                             SaveFPRegsMode fp_mode) {\n  ASM_CODE_COMMENT(this);\n  RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  MoveObjectAndSlot(WriteBarrierDescriptor::ObjectRegister(),\n                    WriteBarrierDescriptor::SlotAddressRegister(), object,\n                    offset);\n\n  CallBuiltin(Builtins::EphemeronKeyBarrier(fp_mode));\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,\n                                             SaveFPRegsMode fp_mode) "}, {"name": "MacroAssembler::CallEphemeronKeyBarrier", "content": "void MacroAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,\n                                             SaveFPRegsMode fp_mode) {\n  ASM_CODE_COMMENT(this);\n  RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);\n  MaybeSaveRegisters(registers);\n\n  Register object_parameter = WriteBarrierDescriptor::ObjectRegister();\n  Register slot_address_parameter =\n      WriteBarrierDescriptor::SlotAddressRegister();\n\n  MoveObjectAndSlot(object_parameter, slot_address_parameter, object, offset);\n\n  CallBuiltin(Builtins::EphemeronKeyBarrier(fp_mode));\n  MaybeRestoreRegisters(registers);\n}", "name_and_para": "void MacroAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,\n                                             SaveFPRegsMode fp_mode) "}], [{"name": "MacroAssembler::MaybeRestoreRegisters", "content": "void MacroAssembler::MaybeRestoreRegisters(RegList registers) {\n  if (registers.is_empty()) return;\n  ASM_CODE_COMMENT(this);\n  CPURegList regs(kXRegSizeInBits, registers);\n  // If we were saving LR, we might need to sign it.\n  DCHECK(!regs.IncludesAliasOf(lr));\n  regs.Align();\n  PopCPURegList(regs);\n}", "name_and_para": "void MacroAssembler::MaybeRestoreRegisters(RegList registers) "}, {"name": "MacroAssembler::MaybeRestoreRegisters", "content": "void MacroAssembler::MaybeRestoreRegisters(RegList registers) {\n  if (registers.is_empty()) return;\n  MultiPop(registers);\n}", "name_and_para": "void MacroAssembler::MaybeRestoreRegisters(RegList registers) "}], [{"name": "MacroAssembler::MaybeSaveRegisters", "content": "void MacroAssembler::MaybeSaveRegisters(RegList registers) {\n  if (registers.is_empty()) return;\n  ASM_CODE_COMMENT(this);\n  CPURegList regs(kXRegSizeInBits, registers);\n  // If we were saving LR, we might need to sign it.\n  DCHECK(!regs.IncludesAliasOf(lr));\n  regs.Align();\n  PushCPURegList(regs);\n}", "name_and_para": "void MacroAssembler::MaybeSaveRegisters(RegList registers) "}, {"name": "MacroAssembler::MaybeSaveRegisters", "content": "void MacroAssembler::MaybeSaveRegisters(RegList registers) {\n  if (registers.is_empty()) return;\n  MultiPush(registers);\n}", "name_and_para": "void MacroAssembler::MaybeSaveRegisters(RegList registers) "}], [{"name": "MacroAssembler::LoadCodeEntrypointViaCodePointer", "content": "void MacroAssembler::LoadCodeEntrypointViaCodePointer(Register destination,\n                                                      MemOperand field_operand,\n                                                      CodeEntrypointTag tag) {\n  DCHECK_NE(tag, kInvalidEntrypointTag);\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Mov(scratch, ExternalReference::code_pointer_table_address());\n  Ldr(destination.W(), field_operand);\n  // TODO(saelo): can the offset computation be done more efficiently?\n  Mov(destination, Operand(destination, LSR, kCodePointerHandleShift));\n  Mov(destination, Operand(destination, LSL, kCodePointerTableEntrySizeLog2));\n  Ldr(destination, MemOperand(scratch, destination));\n  if (tag != 0) {\n    Mov(scratch, Immediate(tag));\n    Eor(destination, destination, scratch);\n  }\n}", "name_and_para": "void MacroAssembler::LoadCodeEntrypointViaCodePointer(Register destination,\n                                                      MemOperand field_operand,\n                                                      CodeEntrypointTag tag) "}, {"name": "MacroAssembler::LoadCodeEntrypointViaCodePointer", "content": "void MacroAssembler::LoadCodeEntrypointViaCodePointer(Register destination,\n                                                      MemOperand field_operand,\n                                                      CodeEntrypointTag tag) {\n  DCHECK_NE(tag, kInvalidEntrypointTag);\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  li(scratch, ExternalReference::code_pointer_table_address());\n  Lwu(destination, field_operand);\n  SrlWord(destination, destination, kCodePointerHandleShift);\n  SllWord(destination, destination, kCodePointerTableEntrySizeLog2);\n  AddWord(scratch, scratch, destination);\n  LoadWord(destination, MemOperand(scratch, 0));\n  if (tag != 0) {\n    li(scratch, Operand(tag));\n    xor_(destination, destination, scratch);\n  }\n}", "name_and_para": "void MacroAssembler::LoadCodeEntrypointViaCodePointer(Register destination,\n                                                      MemOperand field_operand,\n                                                      CodeEntrypointTag tag) "}], [{"name": "MacroAssembler::ResolveCodePointerHandle", "content": "void MacroAssembler::ResolveCodePointerHandle(Register destination,\n                                              Register handle) {\n  DCHECK(!AreAliased(handle, destination));\n\n  Register table = destination;\n  Mov(table, ExternalReference::code_pointer_table_address());\n  Mov(handle, Operand(handle, LSR, kCodePointerHandleShift));\n  Add(destination, table, Operand(handle, LSL, kCodePointerTableEntrySizeLog2));\n  Ldr(destination,\n      MemOperand(destination,\n                 Immediate(kCodePointerTableEntryCodeObjectOffset)));\n  // The LSB is used as marking bit by the code pointer table, so here we have\n  // to set it using a bitwise OR as it may or may not be set.\n  Orr(destination, destination, Immediate(kHeapObjectTag));\n}", "name_and_para": "void MacroAssembler::ResolveCodePointerHandle(Register destination,\n                                              Register handle) "}, {"name": "MacroAssembler::ResolveCodePointerHandle", "content": "void MacroAssembler::ResolveCodePointerHandle(Register destination,\n                                              Register handle) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(handle, destination));\n\n  Register table = destination;\n  li(table, ExternalReference::code_pointer_table_address());\n  SrlWord(handle, handle, kCodePointerHandleShift);\n  CalcScaledAddress(destination, table, handle, kCodePointerTableEntrySizeLog2);\n  LoadWord(destination,\n           MemOperand(destination, kCodePointerTableEntryCodeObjectOffset));\n  // The LSB is used as marking bit by the code pointer table, so here we have\n  // to set it using a bitwise OR as it may or may not be set.\n  Or(destination, destination, Operand(kHeapObjectTag));\n}", "name_and_para": "void MacroAssembler::ResolveCodePointerHandle(Register destination,\n                                              Register handle) "}], [{"name": "MacroAssembler::ResolveTrustedPointerHandle", "content": "void MacroAssembler::ResolveTrustedPointerHandle(Register destination,\n                                                 Register handle,\n                                                 IndirectPointerTag tag) {\n  DCHECK_NE(tag, kCodeIndirectPointerTag);\n  DCHECK(!AreAliased(handle, destination));\n\n  Register table = destination;\n  DCHECK(root_array_available_);\n  Ldr(table,\n      MemOperand{kRootRegister, IsolateData::trusted_pointer_table_offset()});\n  Mov(handle, Operand(handle, LSR, kTrustedPointerHandleShift));\n  Ldr(destination,\n      MemOperand(table, handle, LSL, kTrustedPointerTableEntrySizeLog2));\n  // Untag the pointer and remove the marking bit in one operation.\n  Register tag_reg = handle;\n  Mov(tag_reg, Immediate(~(tag | kTrustedPointerTableMarkBit)));\n  And(destination, destination, tag_reg);\n}", "name_and_para": "void MacroAssembler::ResolveTrustedPointerHandle(Register destination,\n                                                 Register handle,\n                                                 IndirectPointerTag tag) "}, {"name": "MacroAssembler::ResolveTrustedPointerHandle", "content": "void MacroAssembler::ResolveTrustedPointerHandle(Register destination,\n                                                 Register handle,\n                                                 IndirectPointerTag tag) {\n  ASM_CODE_COMMENT(this);\n  DCHECK_NE(tag, kCodeIndirectPointerTag);\n  DCHECK(!AreAliased(handle, destination));\n\n  Register table = destination;\n  DCHECK(root_array_available_);\n  LoadWord(table, MemOperand{kRootRegister,\n                             IsolateData::trusted_pointer_table_offset()});\n  SrlWord(handle, handle, kTrustedPointerHandleShift);\n  CalcScaledAddress(destination, table, handle,\n                    kTrustedPointerTableEntrySizeLog2);\n  LoadWord(destination, MemOperand(destination, 0));\n  // The LSB is used as marking bit by the trusted pointer table, so here we\n  // have to set it using a bitwise OR as it may or may not be set.\n  // Untag the pointer and remove the marking bit in one operation.\n  Register tag_reg = handle;\n  li(tag_reg, Operand(~(tag | kTrustedPointerTableMarkBit)));\n  and_(destination, destination, tag_reg);\n}", "name_and_para": "void MacroAssembler::ResolveTrustedPointerHandle(Register destination,\n                                                 Register handle,\n                                                 IndirectPointerTag tag) "}], [{"name": "MacroAssembler::ResolveIndirectPointerHandle", "content": "void MacroAssembler::ResolveIndirectPointerHandle(Register destination,\n                                                  Register handle,\n                                                  IndirectPointerTag tag) {\n  // The tag implies which pointer table to use.\n  if (tag == kUnknownIndirectPointerTag) {\n    // In this case we have to rely on the handle marking to determine which\n    // pointer table to use.\n    Label is_trusted_pointer_handle, done;\n    constexpr int kCodePointerHandleMarkerBit = 0;\n    static_assert((1 << kCodePointerHandleMarkerBit) ==\n                  kCodePointerHandleMarker);\n    Tbz(handle, kCodePointerHandleMarkerBit, &is_trusted_pointer_handle);\n    ResolveCodePointerHandle(destination, handle);\n    B(&done);\n    Bind(&is_trusted_pointer_handle);\n    ResolveTrustedPointerHandle(destination, handle,\n                                kUnknownIndirectPointerTag);\n    Bind(&done);\n  } else if (tag == kCodeIndirectPointerTag) {\n    ResolveCodePointerHandle(destination, handle);\n  } else {\n    ResolveTrustedPointerHandle(destination, handle, tag);\n  }\n}", "name_and_para": "void MacroAssembler::ResolveIndirectPointerHandle(Register destination,\n                                                  Register handle,\n                                                  IndirectPointerTag tag) "}, {"name": "MacroAssembler::ResolveIndirectPointerHandle", "content": "void MacroAssembler::ResolveIndirectPointerHandle(Register destination,\n                                                  Register handle,\n                                                  IndirectPointerTag tag) {\n  ASM_CODE_COMMENT(this);\n  // The tag implies which pointer table to use.\n  if (tag == kUnknownIndirectPointerTag) {\n    // In this case we have to rely on the handle marking to determine which\n    // pointer table to use.\n    Label is_trusted_pointer_handle, done;\n    DCHECK(!AreAliased(destination, handle));\n    And(destination, handle, kCodePointerHandleMarker);\n    Branch(&is_trusted_pointer_handle, eq, destination, Operand(zero_reg));\n    ResolveCodePointerHandle(destination, handle);\n    Branch(&done);\n    bind(&is_trusted_pointer_handle);\n    ResolveTrustedPointerHandle(destination, handle,\n                                kUnknownIndirectPointerTag);\n    bind(&done);\n  } else if (tag == kCodeIndirectPointerTag) {\n    ResolveCodePointerHandle(destination, handle);\n  } else {\n    ResolveTrustedPointerHandle(destination, handle, tag);\n  }\n}", "name_and_para": "void MacroAssembler::ResolveIndirectPointerHandle(Register destination,\n                                                  Register handle,\n                                                  IndirectPointerTag tag) "}], [{"name": "MacroAssembler::StoreIndirectPointerField", "content": "void MacroAssembler::StoreIndirectPointerField(Register value,\n                                               MemOperand dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Ldr(scratch.W(),\n      FieldMemOperand(value, ExposedTrustedObject::kSelfIndirectPointerOffset));\n  Str(scratch.W(), dst_field_operand);\n#else\n  UNREACHABLE();\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::StoreIndirectPointerField(Register value,\n                                               MemOperand dst_field_operand) "}, {"name": "MacroAssembler::StoreIndirectPointerField", "content": "void MacroAssembler::StoreIndirectPointerField(Register value,\n                                               MemOperand dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  Lw(scratch,\n     FieldMemOperand(value, ExposedTrustedObject::kSelfIndirectPointerOffset));\n  Sw(scratch, dst_field_operand);\n#else\n  UNREACHABLE();\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::StoreIndirectPointerField(Register value,\n                                               MemOperand dst_field_operand) "}], [{"name": "MacroAssembler::LoadIndirectPointerField", "content": "void MacroAssembler::LoadIndirectPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              IndirectPointerTag tag) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n\n  Register handle = temps.AcquireX();\n  Ldr(handle.W(), field_operand);\n  ResolveIndirectPointerHandle(destination, handle, tag);\n#else\n  UNREACHABLE();\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::LoadIndirectPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              IndirectPointerTag tag) "}, {"name": "MacroAssembler::LoadIndirectPointerField", "content": "void MacroAssembler::LoadIndirectPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              IndirectPointerTag tag) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register handle = t6;\n  DCHECK_NE(handle, destination);\n  Lwu(handle, field_operand);\n\n  ResolveIndirectPointerHandle(destination, handle, tag);\n#else\n  UNREACHABLE();\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::LoadIndirectPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              IndirectPointerTag tag) "}], [{"name": "MacroAssembler::StoreTrustedPointerField", "content": "void MacroAssembler::StoreTrustedPointerField(Register value,\n                                              MemOperand dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  StoreIndirectPointerField(value, dst_field_operand);\n#else\n  StoreTaggedField(value, dst_field_operand);\n#endif\n}", "name_and_para": "void MacroAssembler::StoreTrustedPointerField(Register value,\n                                              MemOperand dst_field_operand) "}, {"name": "MacroAssembler::StoreTrustedPointerField", "content": "void MacroAssembler::StoreTrustedPointerField(Register value,\n                                              MemOperand dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  StoreIndirectPointerField(value, dst_field_operand);\n#else\n  StoreTaggedField(value, dst_field_operand);\n#endif\n}", "name_and_para": "void MacroAssembler::StoreTrustedPointerField(Register value,\n                                              MemOperand dst_field_operand) "}], [{"name": "MacroAssembler::LoadTrustedPointerField", "content": "void MacroAssembler::LoadTrustedPointerField(Register destination,\n                                             MemOperand field_operand,\n                                             IndirectPointerTag tag) {\n#ifdef V8_ENABLE_SANDBOX\n  LoadIndirectPointerField(destination, field_operand, tag);\n#else\n  LoadTaggedField(destination, field_operand);\n#endif\n}", "name_and_para": "void MacroAssembler::LoadTrustedPointerField(Register destination,\n                                             MemOperand field_operand,\n                                             IndirectPointerTag tag) "}, {"name": "MacroAssembler::LoadTrustedPointerField", "content": "void MacroAssembler::LoadTrustedPointerField(Register destination,\n                                             MemOperand field_operand,\n                                             IndirectPointerTag tag) {\n#ifdef V8_ENABLE_SANDBOX\n  LoadIndirectPointerField(destination, field_operand, tag);\n#else\n  LoadTaggedField(destination, field_operand);\n#endif\n}", "name_and_para": "void MacroAssembler::LoadTrustedPointerField(Register destination,\n                                             MemOperand field_operand,\n                                             IndirectPointerTag tag) "}], [{"name": "MacroAssembler::LoadExternalPointerField", "content": "void MacroAssembler::LoadExternalPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              ExternalPointerTag tag,\n                                              Register isolate_root) {\n  DCHECK(!AreAliased(destination, isolate_root));\n  ASM_CODE_COMMENT(this);\n#ifdef V8_ENABLE_SANDBOX\n  DCHECK_NE(tag, kExternalPointerNullTag);\n  DCHECK(!IsSharedExternalPointerType(tag));\n  UseScratchRegisterScope temps(this);\n  Register external_table = temps.AcquireX();\n  if (isolate_root == no_reg) {\n    DCHECK(root_array_available_);\n    isolate_root = kRootRegister;\n  }\n  Ldr(external_table,\n      MemOperand(isolate_root,\n                 IsolateData::external_pointer_table_offset() +\n                     Internals::kExternalPointerTableBasePointerOffset));\n  Ldr(destination.W(), field_operand);\n  Mov(destination, Operand(destination, LSR, kExternalPointerIndexShift));\n  Ldr(destination, MemOperand(external_table, destination, LSL,\n                              kExternalPointerTableEntrySizeLog2));\n  // We need another scratch register for the 64-bit tag constant. Instead of\n  // forcing the `And` to allocate a new temp register (which we may not have),\n  // reuse the temp register that we used for the external pointer table base.\n  Register tag_reg = external_table;\n  Mov(tag_reg, Immediate(~tag));\n  And(destination, destination, tag_reg);\n#else\n  Ldr(destination, field_operand);\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::LoadExternalPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              ExternalPointerTag tag,\n                                              Register isolate_root) "}, {"name": "MacroAssembler::LoadExternalPointerField", "content": "void MacroAssembler::LoadExternalPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              ExternalPointerTag tag,\n                                              Register isolate_root) {\n  DCHECK(!AreAliased(destination, isolate_root));\n  ASM_CODE_COMMENT(this);\n#ifdef V8_ENABLE_SANDBOX\n  DCHECK_NE(tag, kExternalPointerNullTag);\n  DCHECK(!IsSharedExternalPointerType(tag));\n  UseScratchRegisterScope temps(this);\n  Register external_table = temps.Acquire();\n  if (isolate_root == no_reg) {\n    DCHECK(root_array_available_);\n    isolate_root = kRootRegister;\n  }\n  LoadWord(external_table,\n           MemOperand(isolate_root,\n                      IsolateData::external_pointer_table_offset() +\n                          Internals::kExternalPointerTableBasePointerOffset));\n  Lwu(destination, field_operand);\n  srli(destination, destination, kExternalPointerIndexShift);\n  slli(destination, destination, kExternalPointerTableEntrySizeLog2);\n  AddWord(external_table, external_table, destination);\n  LoadWord(destination, MemOperand(external_table, 0));\n  And(destination, destination, Operand(~tag));\n#else\n  LoadWord(destination, field_operand);\n#endif  // V8_ENABLE_SANDBOX\n}", "name_and_para": "void MacroAssembler::LoadExternalPointerField(Register destination,\n                                              MemOperand field_operand,\n                                              ExternalPointerTag tag,\n                                              Register isolate_root) "}], [{"name": "MacroAssembler::StoreSandboxedPointerField", "content": "void MacroAssembler::StoreSandboxedPointerField(Register value,\n                                                MemOperand dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.AcquireX();\n  Sub(scratch, value, kPtrComprCageBaseRegister);\n  Mov(scratch, Operand(scratch, LSL, kSandboxedPointerShift));\n  Str(scratch, dst_field_operand);\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::StoreSandboxedPointerField(Register value,\n                                                MemOperand dst_field_operand) "}, {"name": "MacroAssembler::StoreSandboxedPointerField", "content": "void MacroAssembler::StoreSandboxedPointerField(\n    Register value, const MemOperand& dst_field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  SubWord(scratch, value, kPtrComprCageBaseRegister);\n  slli(scratch, scratch, kSandboxedPointerShift);\n  StoreWord(scratch, dst_field_operand);\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::StoreSandboxedPointerField(\n    Register value, const MemOperand& dst_field_operand) "}], [{"name": "MacroAssembler::LoadSandboxedPointerField", "content": "void MacroAssembler::LoadSandboxedPointerField(Register destination,\n                                               MemOperand field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  Ldr(destination, field_operand);\n  DecodeSandboxedPointer(destination);\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::LoadSandboxedPointerField(Register destination,\n                                               MemOperand field_operand) "}, {"name": "MacroAssembler::LoadSandboxedPointerField", "content": "void MacroAssembler::LoadSandboxedPointerField(\n    Register destination, const MemOperand& field_operand) {\n#ifdef V8_ENABLE_SANDBOX\n  ASM_CODE_COMMENT(this);\n  LoadWord(destination, field_operand);\n  DecodeSandboxedPointer(destination);\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::LoadSandboxedPointerField(\n    Register destination, const MemOperand& field_operand) "}], [{"name": "MacroAssembler::DecodeSandboxedPointer", "content": "void MacroAssembler::DecodeSandboxedPointer(Register value) {\n  ASM_CODE_COMMENT(this);\n#ifdef V8_ENABLE_SANDBOX\n  Add(value, kPtrComprCageBaseRegister,\n      Operand(value, LSR, kSandboxedPointerShift));\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::DecodeSandboxedPointer(Register value) "}, {"name": "MacroAssembler::DecodeSandboxedPointer", "content": "void MacroAssembler::DecodeSandboxedPointer(Register value) {\n  ASM_CODE_COMMENT(this);\n#ifdef V8_ENABLE_SANDBOX\n  srli(value, value, kSandboxedPointerShift);\n  AddWord(value, value, kPtrComprCageBaseRegister);\n#else\n  UNREACHABLE();\n#endif\n}", "name_and_para": "void MacroAssembler::DecodeSandboxedPointer(Register value) "}], [{"name": "MacroAssembler::RecordWriteField", "content": "void MacroAssembler::RecordWriteField(Register object, int offset,\n                                      Register value,\n                                      LinkRegisterStatus lr_status,\n                                      SaveFPRegsMode save_fp,\n                                      SmiCheck smi_check, SlotDescriptor slot) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(object, value));\n  // First, check if a write barrier is even needed. The tests below\n  // catch stores of Smis.\n  Label done;\n\n  // Skip the barrier if writing a smi.\n  if (smi_check == SmiCheck::kInline) {\n    JumpIfSmi(value, &done);\n  }\n\n  // Although the object register is tagged, the offset is relative to the start\n  // of the object, so offset must be a multiple of kTaggedSize.\n  DCHECK(IsAligned(offset, kTaggedSize));\n\n  if (v8_flags.debug_code) {\n    ASM_CODE_COMMENT_STRING(this, \"Verify slot_address\");\n    Label ok;\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.AcquireX();\n    DCHECK(!AreAliased(object, value, scratch));\n    Add(scratch, object, offset - kHeapObjectTag);\n    Tst(scratch, kTaggedSize - 1);\n    B(eq, &ok);\n    Abort(AbortReason::kUnalignedCellInWriteBarrier);\n    Bind(&ok);\n  }\n\n  RecordWrite(object, Operand(offset - kHeapObjectTag), value, lr_status,\n              save_fp, SmiCheck::kOmit, slot);\n\n  Bind(&done);\n}", "name_and_para": "void MacroAssembler::RecordWriteField(Register object, int offset,\n                                      Register value,\n                                      LinkRegisterStatus lr_status,\n                                      SaveFPRegsMode save_fp,\n                                      SmiCheck smi_check, SlotDescriptor slot) "}, {"name": "MacroAssembler::RecordWriteField", "content": "void MacroAssembler::RecordWriteField(Register object, int offset,\n                                      Register value, RAStatus ra_status,\n                                      SaveFPRegsMode save_fp,\n                                      SmiCheck smi_check, SlotDescriptor slot) {\n  DCHECK(!AreAliased(object, value));\n  // First, check if a write barrier is even needed. The tests below\n  // catch stores of Smis.\n  Label done;\n\n  // Skip the barrier if writing a smi.\n  if (smi_check == SmiCheck::kInline) {\n    JumpIfSmi(value, &done);\n  }\n\n  // Although the object register is tagged, the offset is relative to the start\n  // of the object, so offset must be a multiple of kTaggedSize.\n  DCHECK(IsAligned(offset, kTaggedSize));\n\n  if (v8_flags.debug_code) {\n    Label ok;\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.Acquire();\n    DCHECK(!AreAliased(object, value, scratch));\n    AddWord(scratch, object, offset - kHeapObjectTag);\n    And(scratch, scratch, Operand(kTaggedSize - 1));\n    BranchShort(&ok, eq, scratch, Operand(zero_reg));\n    Abort(AbortReason::kUnalignedCellInWriteBarrier);\n    bind(&ok);\n  }\n\n  RecordWrite(object, Operand(offset - kHeapObjectTag), value, ra_status,\n              save_fp, SmiCheck::kOmit, slot);\n\n  bind(&done);\n}", "name_and_para": "void MacroAssembler::RecordWriteField(Register object, int offset,\n                                      Register value, RAStatus ra_status,\n                                      SaveFPRegsMode save_fp,\n                                      SmiCheck smi_check, SlotDescriptor slot) "}], [{"name": "MacroAssembler::LoadRoot", "content": "void MacroAssembler::LoadRoot(Register destination, RootIndex index) {\n  ASM_CODE_COMMENT(this);\n  if (V8_STATIC_ROOTS_BOOL && RootsTable::IsReadOnly(index) &&\n      IsImmAddSub(ReadOnlyRootPtr(index))) {\n    DecompressTagged(destination, ReadOnlyRootPtr(index));\n    return;\n  }\n  // Many roots have addresses that are too large to fit into addition immediate\n  // operands. Evidence suggests that the extra instruction for decompression\n  // costs us more than the load.\n  Ldr(destination,\n      MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));\n}", "name_and_para": "void MacroAssembler::LoadRoot(Register destination, RootIndex index) "}, {"name": "MacroAssembler::LoadRoot", "content": "void MacroAssembler::LoadRoot(Register destination, RootIndex index) {\n#if V8_TARGET_ARCH_RISCV64\n  if (V8_STATIC_ROOTS_BOOL && RootsTable::IsReadOnly(index) &&\n      is_int12(ReadOnlyRootPtr(index))) {\n    DecompressTagged(destination, ReadOnlyRootPtr(index));\n    return;\n  }\n#endif\n  // Many roots have addresses that are too large to fit into addition immediate\n  // operands. Evidence suggests that the extra instruction for decompression\n  // costs us more than the load.\n  LoadWord(destination,\n           MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));\n}", "name_and_para": "void MacroAssembler::LoadRoot(Register destination, RootIndex index) "}], [{"name": "MacroAssembler::LoadTaggedRoot", "content": "void MacroAssembler::LoadTaggedRoot(Register destination, RootIndex index) {\n  ASM_CODE_COMMENT(this);\n  if (CanBeImmediate(index)) {\n    Mov(destination,\n        Immediate(ReadOnlyRootPtr(index), RelocInfo::Mode::NO_INFO));\n    return;\n  }\n  LoadRoot(destination, index);\n}", "name_and_para": "void MacroAssembler::LoadTaggedRoot(Register destination, RootIndex index) "}, {"name": "MacroAssembler::LoadTaggedRoot", "content": "void MacroAssembler::LoadTaggedRoot(Register destination, RootIndex index) {\n  if (V8_STATIC_ROOTS_BOOL && RootsTable::IsReadOnly(index) &&\n      is_int12(ReadOnlyRootPtr(index))) {\n    li(destination, (int32_t)ReadOnlyRootPtr(index));\n    return;\n  }\n  LoadWord(destination,\n           MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));\n}", "name_and_para": "void MacroAssembler::LoadTaggedRoot(Register destination, RootIndex index) "}], [{"name": "MacroAssembler::AssertUnreachable", "content": "void MacroAssembler::AssertUnreachable(AbortReason reason) {\n  if (v8_flags.debug_code) Abort(reason);\n}", "name_and_para": "void MacroAssembler::AssertUnreachable(AbortReason reason) "}, {"name": "MacroAssembler::AssertUnreachable", "content": "void MacroAssembler::AssertUnreachable(AbortReason reason) {\n  if (v8_flags.debug_code) Abort(reason);\n}", "name_and_para": "void MacroAssembler::AssertUnreachable(AbortReason reason) "}], [{"name": "MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot", "content": "void MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot(\n    Register flags, Register feedback_vector) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(flags, feedback_vector));\n  Label maybe_has_optimized_code, maybe_needs_logging;\n  // Check if optimized code is available.\n  TestAndBranchIfAllClear(flags,\n                          FeedbackVector::kFlagsTieringStateIsAnyRequested,\n                          &maybe_needs_logging);\n  GenerateTailCallToReturnedCode(Runtime::kCompileOptimized);\n\n  bind(&maybe_needs_logging);\n  TestAndBranchIfAllClear(flags, FeedbackVector::LogNextExecutionBit::kMask,\n                          &maybe_has_optimized_code);\n  GenerateTailCallToReturnedCode(Runtime::kFunctionLogNextExecution);\n\n  bind(&maybe_has_optimized_code);\n  Register optimized_code_entry = x7;\n  LoadTaggedField(optimized_code_entry,\n                  FieldMemOperand(feedback_vector,\n                                  FeedbackVector::kMaybeOptimizedCodeOffset));\n  TailCallOptimizedCodeSlot(this, optimized_code_entry, x4);\n}", "name_and_para": "void MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot(\n    Register flags, Register feedback_vector) "}, {"name": "MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot", "content": "void MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot(\n    Register flags, Register feedback_vector) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(flags, feedback_vector));\n  UseScratchRegisterScope temps(this);\n  temps.Include(t0, t1);\n  Label maybe_has_optimized_code, maybe_needs_logging;\n  // Check if optimized code is available.\n  {\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.Acquire();\n    And(scratch, flags,\n        Operand(FeedbackVector::kFlagsTieringStateIsAnyRequested));\n    Branch(&maybe_needs_logging, eq, scratch, Operand(zero_reg),\n           Label::Distance::kNear);\n  }\n  GenerateTailCallToReturnedCode(Runtime::kCompileOptimized);\n\n  bind(&maybe_needs_logging);\n  {\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.Acquire();\n    And(scratch, flags, Operand(FeedbackVector::LogNextExecutionBit::kMask));\n    Branch(&maybe_has_optimized_code, eq, scratch, Operand(zero_reg),\n           Label::Distance::kNear);\n  }\n\n  GenerateTailCallToReturnedCode(Runtime::kFunctionLogNextExecution);\n\n  bind(&maybe_has_optimized_code);\n  Register optimized_code_entry = flags;\n  LoadTaggedField(optimized_code_entry,\n                  FieldMemOperand(feedback_vector,\n                                  FeedbackVector::kMaybeOptimizedCodeOffset));\n  TailCallOptimizedCodeSlot(this, optimized_code_entry, temps.Acquire(),\n                            temps.Acquire());\n}", "name_and_para": "void MacroAssembler::OptimizeCodeOrTailCallOptimizedCodeSlot(\n    Register flags, Register feedback_vector) "}], [{"name": "MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing", "content": "void MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n    Register flags, Register feedback_vector, CodeKind current_code_kind,\n    Label* flags_need_processing) {\n  ASM_CODE_COMMENT(this);\n  B(LoadFeedbackVectorFlagsAndCheckIfNeedsProcessing(flags, feedback_vector,\n                                                     current_code_kind),\n    flags_need_processing);\n}", "name_and_para": "void MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n    Register flags, Register feedback_vector, CodeKind current_code_kind,\n    Label* flags_need_processing) "}, {"name": "MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing", "content": "void MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n    Register flags, Register feedback_vector, CodeKind current_code_kind,\n    Label* flags_need_processing) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(flags, feedback_vector));\n  DCHECK(CodeKindCanTierUp(current_code_kind));\n  UseScratchRegisterScope temps(this);\n  Register scratch = temps.Acquire();\n  Lhu(flags, FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));\n  uint32_t kFlagsMask = FeedbackVector::kFlagsTieringStateIsAnyRequested |\n                        FeedbackVector::kFlagsMaybeHasTurbofanCode |\n                        FeedbackVector::kFlagsLogNextExecution;\n  if (current_code_kind != CodeKind::MAGLEV) {\n    kFlagsMask |= FeedbackVector::kFlagsMaybeHasMaglevCode;\n  }\n  And(scratch, flags, Operand(kFlagsMask));\n  Branch(flags_need_processing, ne, scratch, Operand(zero_reg));\n}", "name_and_para": "void MacroAssembler::LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n    Register flags, Register feedback_vector, CodeKind current_code_kind,\n    Label* flags_need_processing) "}], [{"name": "MacroAssembler::GenerateTailCallToReturnedCode", "content": "void MacroAssembler::GenerateTailCallToReturnedCode(\n    Runtime::FunctionId function_id) {\n  ASM_CODE_COMMENT(this);\n  // ----------- S t a t e -------------\n  //  -- x0 : actual argument count\n  //  -- x1 : target function (preserved for callee)\n  //  -- x3 : new target (preserved for callee)\n  // -----------------------------------\n  {\n    FrameScope scope(this, StackFrame::INTERNAL);\n    // Push a copy of the target function, the new target and the actual\n    // argument count.\n    SmiTag(kJavaScriptCallArgCountRegister);\n    Push(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,\n         kJavaScriptCallArgCountRegister, padreg);\n    // Push another copy as a parameter to the runtime call.\n    PushArgument(kJavaScriptCallTargetRegister);\n\n    CallRuntime(function_id, 1);\n    Mov(x2, x0);\n\n    // Restore target function, new target and actual argument count.\n    Pop(padreg, kJavaScriptCallArgCountRegister,\n        kJavaScriptCallNewTargetRegister, kJavaScriptCallTargetRegister);\n    SmiUntag(kJavaScriptCallArgCountRegister);\n  }\n\n  static_assert(kJavaScriptCallCodeStartRegister == x2, \"ABI mismatch\");\n  JumpCodeObject(x2, kJSEntrypointTag);\n}", "name_and_para": "void MacroAssembler::GenerateTailCallToReturnedCode(\n    Runtime::FunctionId function_id) "}, {"name": "MacroAssembler::GenerateTailCallToReturnedCode", "content": "void MacroAssembler::GenerateTailCallToReturnedCode(\n    Runtime::FunctionId function_id) {\n  // ----------- S t a t e -------------\n  //  -- a0 : actual argument count\n  //  -- a1 : target function (preserved for callee)\n  //  -- a3 : new target (preserved for callee)\n  // -----------------------------------\n  {\n    FrameScope scope(this, StackFrame::INTERNAL);\n    // Push a copy of the target function, the new target and the actual\n    // argument count.\n    // Push function as parameter to the runtime call.\n    SmiTag(kJavaScriptCallArgCountRegister);\n    Push(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,\n         kJavaScriptCallArgCountRegister, kJavaScriptCallTargetRegister);\n\n    CallRuntime(function_id, 1);\n    // Use the return value before restoring a0\n    LoadCodeInstructionStart(a2, a0, kJSEntrypointTag);\n    // Restore target function, new target and actual argument count.\n    Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,\n        kJavaScriptCallArgCountRegister);\n    SmiUntag(kJavaScriptCallArgCountRegister);\n  }\n\n  static_assert(kJavaScriptCallCodeStartRegister == a2, \"ABI mismatch\");\n  Jump(a2);\n}", "name_and_para": "void MacroAssembler::GenerateTailCallToReturnedCode(\n    Runtime::FunctionId function_id) "}], [{"name": "MacroAssembler::ReplaceClosureCodeWithOptimizedCode", "content": "void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(\n    Register optimized_code, Register closure) {\n  ASM_CODE_COMMENT(this);\n  DCHECK(!AreAliased(optimized_code, closure));\n  // Store code entry in the closure.\n  AssertCode(optimized_code);\n  StoreCodePointerField(optimized_code,\n                        FieldMemOperand(closure, JSFunction::kCodeOffset));\n  RecordWriteField(closure, JSFunction::kCodeOffset, optimized_code,\n                   kLRHasNotBeenSaved, SaveFPRegsMode::kIgnore, SmiCheck::kOmit,\n                   SlotDescriptor::ForCodePointerSlot());\n}", "name_and_para": "void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(\n    Register optimized_code, Register closure) "}, {"name": "MacroAssembler::ReplaceClosureCodeWithOptimizedCode", "content": "void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(\n    Register optimized_code, Register closure) {\n  ASM_CODE_COMMENT(this);\n  StoreCodePointerField(optimized_code,\n                        FieldMemOperand(closure, JSFunction::kCodeOffset));\n  RecordWriteField(closure, JSFunction::kCodeOffset, optimized_code,\n                   kRAHasNotBeenSaved, SaveFPRegsMode::kIgnore, SmiCheck::kOmit,\n                   SlotDescriptor::ForCodePointerSlot());\n}", "name_and_para": "void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(\n    Register optimized_code, Register closure) "}], [{"name": "MacroAssembler::AssertFeedbackVector", "content": "void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {\n  if (v8_flags.debug_code) {\n    IsObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);\n    Assert(eq, AbortReason::kExpectedFeedbackVector);\n  }\n}", "name_and_para": "void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) "}, {"name": "MacroAssembler::AssertFeedbackVector", "content": "void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {\n  if (v8_flags.debug_code) {\n    GetObjectType(object, scratch, scratch);\n    Assert(eq, AbortReason::kExpectedFeedbackVector, scratch,\n           Operand(FEEDBACK_VECTOR_TYPE));\n  }\n}", "name_and_para": "void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) "}], [{"name": "MacroAssembler::AssertFeedbackCell", "content": "void MacroAssembler::AssertFeedbackCell(Register object, Register scratch) {\n  if (v8_flags.debug_code) {\n    IsObjectType(object, scratch, scratch, FEEDBACK_CELL_TYPE);\n    Assert(eq, AbortReason::kExpectedFeedbackCell);\n  }\n}", "name_and_para": "void MacroAssembler::AssertFeedbackCell(Register object, Register scratch) "}, {"name": "MacroAssembler::AssertFeedbackCell", "content": "void MacroAssembler::AssertFeedbackCell(Register object, Register scratch) {\n  if (v8_flags.debug_code) {\n    GetObjectType(object, scratch, scratch);\n    Assert(eq, AbortReason::kExpectedFeedbackCell, scratch,\n           Operand(FEEDBACK_CELL_TYPE));\n  }\n}", "name_and_para": "void MacroAssembler::AssertFeedbackCell(Register object, Register scratch) "}], [{"name": "TailCallOptimizedCodeSlot", "content": "void TailCallOptimizedCodeSlot(MacroAssembler* masm,\n                               Register optimized_code_entry,\n                               Register scratch) {\n  // ----------- S t a t e -------------\n  //  -- x0 : actual argument count\n  //  -- x3 : new target (preserved for callee if needed, and caller)\n  //  -- x1 : target function (preserved for callee if needed, and caller)\n  // -----------------------------------\n  ASM_CODE_COMMENT(masm);\n  DCHECK(!AreAliased(x1, x3, optimized_code_entry, scratch));\n\n  Register closure = x1;\n  Label heal_optimized_code_slot;\n\n  // If the optimized code is cleared, go to runtime to update the optimization\n  // marker field.\n  __ LoadWeakValue(optimized_code_entry, optimized_code_entry,\n                   &heal_optimized_code_slot);\n\n  // The entry references a CodeWrapper object. Unwrap it now.\n  __ LoadCodePointerField(\n      optimized_code_entry,\n      FieldMemOperand(optimized_code_entry, CodeWrapper::kCodeOffset));\n\n  // Check if the optimized code is marked for deopt. If it is, call the\n  // runtime to clear it.\n  __ AssertCode(optimized_code_entry);\n  __ JumpIfCodeIsMarkedForDeoptimization(optimized_code_entry, scratch,\n                                         &heal_optimized_code_slot);\n\n  // Optimized code is good, get it into the closure and link the closure into\n  // the optimized functions list, then tail call the optimized code.\n  __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure);\n  static_assert(kJavaScriptCallCodeStartRegister == x2, \"ABI mismatch\");\n  __ Move(x2, optimized_code_entry);\n  __ JumpCodeObject(x2, kJSEntrypointTag);\n\n  // Optimized code slot contains deoptimized code or code is cleared and\n  // optimized code marker isn't updated. Evict the code, update the marker\n  // and re-enter the closure's code.\n  __ bind(&heal_optimized_code_slot);\n  __ GenerateTailCallToReturnedCode(Runtime::kHealOptimizedCodeSlot);\n}", "name_and_para": "void TailCallOptimizedCodeSlot(MacroAssembler* masm,\n                               Register optimized_code_entry,\n                               Register scratch) "}, {"name": "TailCallOptimizedCodeSlot", "content": "static void TailCallOptimizedCodeSlot(MacroAssembler* masm,\n                                      Register optimized_code_entry,\n                                      Register scratch1, Register scratch2) {\n  // ----------- S t a t e -------------\n  //  -- a0 : actual argument count\n  //  -- a3 : new target (preserved for callee if needed, and caller)\n  //  -- a1 : target function (preserved for callee if needed, and caller)\n  // -----------------------------------\n  ASM_CODE_COMMENT(masm);\n  DCHECK(!AreAliased(optimized_code_entry, a1, a3, scratch1, scratch2));\n\n  Register closure = a1;\n  Label heal_optimized_code_slot;\n\n  // If the optimized code is cleared, go to runtime to update the optimization\n  // marker field.\n  __ LoadWeakValue(optimized_code_entry, optimized_code_entry,\n                   &heal_optimized_code_slot);\n\n  // The entry references a CodeWrapper object. Unwrap it now.\n  __ LoadCodePointerField(\n      optimized_code_entry,\n      FieldMemOperand(optimized_code_entry, CodeWrapper::kCodeOffset));\n\n  // Check if the optimized code is marked for deopt. If it is, call the\n  // runtime to clear it.\n  __ JumpIfCodeIsMarkedForDeoptimization(optimized_code_entry, scratch1,\n                                         &heal_optimized_code_slot);\n\n  // Optimized code is good, get it into the closure and link the closure into\n  // the optimized functions list, then tail call the optimized code.\n  // The feedback vector is no longer used, so re-use it as a scratch\n  // register.\n  __ ReplaceClosureCodeWithOptimizedCode(optimized_code_entry, closure);\n\n  static_assert(kJavaScriptCallCodeStartRegister == a2, \"ABI mismatch\");\n  __ LoadCodeInstructionStart(a2, optimized_code_entry, kJSEntrypointTag);\n  __ Jump(a2);\n\n  // Optimized code slot contains deoptimized code or code is cleared and\n  // optimized code marker isn't updated. Evict the code, update the marker\n  // and re-enter the closure's code.\n  __ bind(&heal_optimized_code_slot);\n  __ GenerateTailCallToReturnedCode(Runtime::kHealOptimizedCodeSlot);\n}", "name_and_para": "static void TailCallOptimizedCodeSlot(MacroAssembler* masm,\n                                      Register optimized_code_entry,\n                                      Register scratch1, Register scratch2) "}], [{"name": "MacroAssembler::PopCallerSaved", "content": "int MacroAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion) {\n  ASM_CODE_COMMENT(this);\n  int bytes = 0;\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    auto fp_list = CPURegList::GetCallerSavedV(kStackSavedSavedFPSizeInBits);\n    DCHECK_EQ(fp_list.Count() % 2, 0);\n    PopCPURegList(fp_list);\n    bytes += fp_list.TotalSizeInBytes();\n  }\n\n  auto list = kCallerSaved;\n  list.Remove(exclusion);\n  list.Align();\n\n  PopCPURegList(list);\n  bytes += list.TotalSizeInBytes();\n\n  return bytes;\n}", "name_and_para": "int MacroAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion) "}, {"name": "MacroAssembler::PopCallerSaved", "content": "int MacroAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,\n                                   Register exclusion2, Register exclusion3) {\n  int bytes = 0;\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    MultiPopFPU(kCallerSavedFPU);\n    bytes += kCallerSavedFPU.Count() * kDoubleSize;\n  }\n\n  RegList exclusions = {exclusion1, exclusion2, exclusion3};\n  RegList list = kJSCallerSaved - exclusions;\n  MultiPop(list);\n  bytes += list.Count() * kSystemPointerSize;\n\n  return bytes;\n}", "name_and_para": "int MacroAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,\n                                   Register exclusion2, Register exclusion3) "}], [{"name": "MacroAssembler::PushCallerSaved", "content": "int MacroAssembler::PushCallerSaved(SaveFPRegsMode fp_mode,\n                                    Register exclusion) {\n  ASM_CODE_COMMENT(this);\n  auto list = kCallerSaved;\n  list.Remove(exclusion);\n  list.Align();\n\n  PushCPURegList(list);\n\n  int bytes = list.TotalSizeInBytes();\n\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    auto fp_list = CPURegList::GetCallerSavedV(kStackSavedSavedFPSizeInBits);\n    DCHECK_EQ(fp_list.Count() % 2, 0);\n    PushCPURegList(fp_list);\n    bytes += fp_list.TotalSizeInBytes();\n  }\n  return bytes;\n}", "name_and_para": "int MacroAssembler::PushCallerSaved(SaveFPRegsMode fp_mode,\n                                    Register exclusion) "}, {"name": "MacroAssembler::PushCallerSaved", "content": "int MacroAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,\n                                    Register exclusion2, Register exclusion3) {\n  int bytes = 0;\n\n  RegList exclusions = {exclusion1, exclusion2, exclusion3};\n  RegList list = kJSCallerSaved - exclusions;\n  MultiPush(list);\n  bytes += list.Count() * kSystemPointerSize;\n\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    MultiPushFPU(kCallerSavedFPU);\n    bytes += kCallerSavedFPU.Count() * kDoubleSize;\n  }\n\n  return bytes;\n}", "name_and_para": "int MacroAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,\n                                    Register exclusion2, Register exclusion3) "}], [{"name": "MacroAssembler::RequiredStackSizeForCallerSaved", "content": "int MacroAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                                    Register exclusion) const {\n  auto list = kCallerSaved;\n  list.Remove(exclusion);\n  list.Align();\n\n  int bytes = list.TotalSizeInBytes();\n\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    auto fp_list = CPURegList::GetCallerSavedV(kStackSavedSavedFPSizeInBits);\n    DCHECK_EQ(fp_list.Count() % 2, 0);\n    bytes += fp_list.TotalSizeInBytes();\n  }\n  return bytes;\n}", "name_and_para": "int MacroAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                                    Register exclusion) const "}, {"name": "MacroAssembler::RequiredStackSizeForCallerSaved", "content": "int MacroAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                                    Register exclusion1,\n                                                    Register exclusion2,\n                                                    Register exclusion3) const {\n  int bytes = 0;\n\n  RegList exclusions = {exclusion1, exclusion2, exclusion3};\n  RegList list = kJSCallerSaved - exclusions;\n  bytes += list.Count() * kSystemPointerSize;\n\n  if (fp_mode == SaveFPRegsMode::kSave) {\n    bytes += kCallerSavedFPU.Count() * kDoubleSize;\n  }\n\n  return bytes;\n}", "name_and_para": "int MacroAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                                    Register exclusion1,\n                                                    Register exclusion2,\n                                                    Register exclusion3) const "}]]], [["./v8/src/codegen/riscv/cpu-riscv.cc", "./v8/src/codegen/arm64/cpu-arm64.cc"], 0.5, 1.0, [[{"name": "CpuFeatures::FlushICache", "content": "void CpuFeatures::FlushICache(void* address, size_t length) {\n#if defined(V8_HOST_ARCH_ARM64)\n#if defined(V8_OS_WIN)\n  ::FlushInstructionCache(GetCurrentProcess(), address, length);\n#elif defined(V8_OS_DARWIN)\n  sys_icache_invalidate(address, length);\n#else\n  // The code below assumes user space cache operations are allowed. The goal\n  // of this routine is to make sure the code generated is visible to the I\n  // side of the CPU.\n\n  uintptr_t start = reinterpret_cast<uintptr_t>(address);\n  // Sizes will be used to generate a mask big enough to cover a pointer.\n  CacheLineSizes sizes;\n  uintptr_t dsize = sizes.dcache_line_size();\n  uintptr_t isize = sizes.icache_line_size();\n  // Cache line sizes are always a power of 2.\n  DCHECK_EQ(CountSetBits(dsize, 64), 1);\n  DCHECK_EQ(CountSetBits(isize, 64), 1);\n  uintptr_t dstart = start & ~(dsize - 1);\n  uintptr_t istart = start & ~(isize - 1);\n  uintptr_t end = start + length;\n\n  __asm__ __volatile__(\n      // Clean every line of the D cache containing the target data.\n      \"0:                                \\n\\t\"\n      // dc       : Data Cache maintenance\n      //    c     : Clean\n      //     i    : Invalidate\n      //      va  : by (Virtual) Address\n      //        c : to the point of Coherency\n      // See ARM DDI 0406B page B2-12 for more information.\n      // We would prefer to use \"cvau\" (clean to the point of unification) here\n      // but we use \"civac\" to work around Cortex-A53 errata 819472, 826319,\n      // 827319 and 824069.\n      \"dc   civac, %[dline]               \\n\\t\"\n      \"add  %[dline], %[dline], %[dsize]  \\n\\t\"\n      \"cmp  %[dline], %[end]              \\n\\t\"\n      \"b.lt 0b                            \\n\\t\"\n      // Barrier to make sure the effect of the code above is visible to the\n      // rest of the world. dsb    : Data Synchronisation Barrier\n      //    ish : Inner SHareable domain\n      // The point of unification for an Inner Shareable shareability domain is\n      // the point by which the instruction and data caches of all the\n      // processors in that Inner Shareable shareability domain are guaranteed\n      // to see the same copy of a memory location.  See ARM DDI 0406B page\n      // B2-12 for more information.\n      \"dsb  ish                           \\n\\t\"\n      // Invalidate every line of the I cache containing the target data.\n      \"1:                                 \\n\\t\"\n      // ic      : instruction cache maintenance\n      //    i    : invalidate\n      //     va  : by address\n      //       u : to the point of unification\n      \"ic   ivau, %[iline]                \\n\\t\"\n      \"add  %[iline], %[iline], %[isize]  \\n\\t\"\n      \"cmp  %[iline], %[end]              \\n\\t\"\n      \"b.lt 1b                            \\n\\t\"\n      // Barrier to make sure the effect of the code above is visible to the\n      // rest of the world.\n      \"dsb  ish                           \\n\\t\"\n      // Barrier to ensure any prefetching which happened before this code is\n      // discarded.\n      // isb : Instruction Synchronisation Barrier\n      \"isb                                \\n\\t\"\n      : [dline] \"+r\"(dstart), [iline] \"+r\"(istart)\n      : [dsize] \"r\"(dsize), [isize] \"r\"(isize), [end] \"r\"(end)\n      // This code does not write to memory but without the dependency gcc might\n      // move this code before the code is generated.\n      : \"cc\", \"memory\");\n#endif  // V8_OS_WIN\n#endif  // V8_HOST_ARCH_ARM64\n}", "name_and_para": "void CpuFeatures::FlushICache(void* address, size_t length) "}, {"name": "CpuFeatures::FlushICache", "content": "void CpuFeatures::FlushICache(void* start, size_t size) {\n#if !defined(USE_SIMULATOR)\n  char* end = reinterpret_cast<char*>(start) + size;\n  // The definition of this syscall is equal to\n  // SYSCALL_DEFINE3(riscv_flush_icache, uintptr_t, start,\n  //                 uintptr_t, end, uintptr_t, flags)\n  // The flag here is set to be SYS_RISCV_FLUSH_ICACHE_LOCAL, which is\n  // defined as 1 in the Linux kernel.\n  // SYS_riscv_flush_icache is a symbolic constant used in user-space code to\n  // identify the flush_icache system call, while __NR_riscv_flush_icache is the\n  // corresponding system call number used in the kernel to dispatch the system\n  // call.\n  syscall(__NR_riscv_flush_icache, start, end, 1);\n#endif  // !USE_SIMULATOR.\n}", "name_and_para": "void CpuFeatures::FlushICache(void* start, size_t size) "}]]], [["./v8/src/codegen/riscv/register-riscv.h", "./v8/src/codegen/arm64/register-arm64.h"], 0.8, 1.0, [[{"name": "VRegister", "content": "class VRegister : public CPURegister {\n public:\n  static constexpr VRegister no_reg() {\n    return VRegister(CPURegister::no_reg(), 0);\n  }\n\n  static constexpr VRegister Create(int code, int size, int lane_count = 1) {\n    DCHECK(IsValidLaneCount(lane_count));\n    return VRegister(CPURegister::Create(code, size, CPURegister::kVRegister),\n                     lane_count);\n  }\n\n  static VRegister Create(int reg_code, VectorFormat format) {\n    int reg_size = RegisterSizeInBitsFromFormat(format);\n    int reg_count = IsVectorFormat(format) ? LaneCountFromFormat(format) : 1;\n    return VRegister::Create(reg_code, reg_size, reg_count);\n  }\n\n  static VRegister BRegFromCode(unsigned code);\n  static VRegister HRegFromCode(unsigned code);\n  static VRegister SRegFromCode(unsigned code);\n  static VRegister DRegFromCode(unsigned code);\n  static VRegister QRegFromCode(unsigned code);\n  static VRegister VRegFromCode(unsigned code);\n\n  VRegister V8B() const {\n    return VRegister::Create(code(), kDRegSizeInBits, 8);\n  }\n  VRegister V16B() const {\n    return VRegister::Create(code(), kQRegSizeInBits, 16);\n  }\n  VRegister V4H() const {\n    return VRegister::Create(code(), kDRegSizeInBits, 4);\n  }\n  VRegister V8H() const {\n    return VRegister::Create(code(), kQRegSizeInBits, 8);\n  }\n  VRegister V2S() const {\n    return VRegister::Create(code(), kDRegSizeInBits, 2);\n  }\n  VRegister V4S() const {\n    return VRegister::Create(code(), kQRegSizeInBits, 4);\n  }\n  VRegister V2D() const {\n    return VRegister::Create(code(), kQRegSizeInBits, 2);\n  }\n  VRegister V1D() const {\n    return VRegister::Create(code(), kDRegSizeInBits, 1);\n  }\n  VRegister V1Q() const {\n    return VRegister::Create(code(), kQRegSizeInBits, 1);\n  }\n\n  VRegister Format(VectorFormat f) const {\n    return VRegister::Create(code(), f);\n  }\n\n  bool Is8B() const { return (Is64Bits() && (lane_count_ == 8)); }\n  bool Is16B() const { return (Is128Bits() && (lane_count_ == 16)); }\n  bool Is4H() const { return (Is64Bits() && (lane_count_ == 4)); }\n  bool Is8H() const { return (Is128Bits() && (lane_count_ == 8)); }\n  bool Is2S() const { return (Is64Bits() && (lane_count_ == 2)); }\n  bool Is4S() const { return (Is128Bits() && (lane_count_ == 4)); }\n  bool Is1D() const { return (Is64Bits() && (lane_count_ == 1)); }\n  bool Is2D() const { return (Is128Bits() && (lane_count_ == 2)); }\n  bool Is1Q() const { return (Is128Bits() && (lane_count_ == 1)); }\n\n  // For consistency, we assert the number of lanes of these scalar registers,\n  // even though there are no vectors of equivalent total size with which they\n  // could alias.\n  bool Is1B() const {\n    DCHECK(!(Is8Bits() && IsVector()));\n    return Is8Bits();\n  }\n  bool Is1H() const {\n    DCHECK(!(Is16Bits() && IsVector()));\n    return Is16Bits();\n  }\n  bool Is1S() const {\n    DCHECK(!(Is32Bits() && IsVector()));\n    return Is32Bits();\n  }\n\n  bool IsLaneSizeB() const { return LaneSizeInBits() == kBRegSizeInBits; }\n  bool IsLaneSizeH() const { return LaneSizeInBits() == kHRegSizeInBits; }\n  bool IsLaneSizeS() const { return LaneSizeInBits() == kSRegSizeInBits; }\n  bool IsLaneSizeD() const { return LaneSizeInBits() == kDRegSizeInBits; }\n\n  bool IsScalar() const { return lane_count_ == 1; }\n  bool IsVector() const { return lane_count_ > 1; }\n\n  bool IsSameFormat(const VRegister& other) const {\n    return (reg_size_ == other.reg_size_) && (lane_count_ == other.lane_count_);\n  }\n\n  int LaneCount() const { return lane_count_; }\n\n  unsigned LaneSizeInBytes() const { return SizeInBytes() / lane_count_; }\n\n  unsigned LaneSizeInBits() const { return LaneSizeInBytes() * 8; }\n\n  static constexpr int kMaxNumRegisters = kNumberOfVRegisters;\n  static_assert(kMaxNumRegisters == kDoubleAfterLast);\n\n  static constexpr VRegister from_code(int code) {\n    // Always return a D register.\n    return VRegister::Create(code, kDRegSizeInBits);\n  }\n\n private:\n  int8_t lane_count_;\n\n  constexpr explicit VRegister(const CPURegister& r, int lane_count)\n      : CPURegister(r), lane_count_(lane_count) {}\n\n  static constexpr bool IsValidLaneCount(int lane_count) {\n    return base::bits::IsPowerOfTwo(lane_count) && lane_count <= 16;\n  }\n}", "name_and_para": ""}, {"name": "VRegister", "content": "class VRegister : public RegisterBase<VRegister, kVRAfterLast> {\n  friend class RegisterBase;\n\n public:\n  explicit constexpr VRegister(int code) : RegisterBase(code) {}\n}", "name_and_para": ""}], [{"name": "DoubleRegisterCode", "content": "enum DoubleRegisterCode {\n#define REGISTER_CODE(R) kDoubleCode_##R,\n  DOUBLE_REGISTERS(REGISTER_CODE)\n#undef REGISTER_CODE\n      kDoubleAfterLast\n}", "name_and_para": ""}, {"name": "DoubleRegisterCode", "content": "enum DoubleRegisterCode {\n#define REGISTER_CODE(R) kDoubleCode_##R,\n  DOUBLE_REGISTERS(REGISTER_CODE)\n#undef REGISTER_CODE\n      kDoubleAfterLast\n}", "name_and_para": ""}], [{"name": "ArgumentPaddingSlots", "content": "constexpr int ArgumentPaddingSlots(int argument_count) {\n  // Stack frames are aligned to 16 bytes.\n  constexpr int kStackFrameAlignment = 16;\n  constexpr int alignment_mask = kStackFrameAlignment / kSystemPointerSize - 1;\n  return argument_count & alignment_mask;\n}", "name_and_para": "constexpr int ArgumentPaddingSlots(int argument_count) "}, {"name": "ArgumentPaddingSlots", "content": "constexpr int ArgumentPaddingSlots(int argument_count) {\n  // No argument padding required.\n  return 0;\n}", "name_and_para": "constexpr int ArgumentPaddingSlots(int argument_count) "}], [{"name": "ReassignRegister", "content": "inline Register ReassignRegister(Register& source) {\n  Register result = source;\n  source = Register::no_reg();\n  return result;\n}", "name_and_para": "inline Register ReassignRegister(Register& source) "}, {"name": "ReassignRegister", "content": "inline Register ReassignRegister(Register& source) {\n  Register result = source;\n  source = Register::no_reg();\n  return result;\n}", "name_and_para": "inline Register ReassignRegister(Register& source) "}], [{"name": "Register", "content": "class Register : public CPURegister {\n public:\n  static constexpr Register no_reg() { return Register(CPURegister::no_reg()); }\n\n  static constexpr Register Create(int code, int size) {\n    return Register(CPURegister::Create(code, size, CPURegister::kRegister));\n  }\n\n  static Register XRegFromCode(unsigned code);\n  static Register WRegFromCode(unsigned code);\n\n  static constexpr Register from_code(int code) {\n    // Always return an X register.\n    return Register::Create(code, kXRegSizeInBits);\n  }\n\n  static const char* GetSpecialRegisterName(int code) {\n    return (code == kSPRegInternalCode) ? \"sp\" : \"UNKNOWN\";\n  }\n\n private:\n  constexpr explicit Register(const CPURegister& r) : CPURegister(r) {}\n}", "name_and_para": ""}, {"name": "Register", "content": "class Register : public RegisterBase<Register, kRegAfterLast> {\n public:\n#if defined(V8_TARGET_LITTLE_ENDIAN)\n  static constexpr int kMantissaOffset = 0;\n  static constexpr int kExponentOffset = 4;\n#elif defined(V8_TARGET_BIG_ENDIAN)\n  static constexpr int kMantissaOffset = 4;\n  static constexpr int kExponentOffset = 0;\n#else\n#error Unknown endianness\n#endif\n\n private:\n  friend class RegisterBase;\n  explicit constexpr Register(int code) : RegisterBase(code) {}\n}", "name_and_para": ""}], [{"name": "RegisterCode", "content": "enum RegisterCode {\n#define REGISTER_CODE(R) kRegCode_##R,\n  GENERAL_REGISTERS(REGISTER_CODE)\n#undef REGISTER_CODE\n      kRegAfterLast\n}", "name_and_para": ""}, {"name": "RegisterCode", "content": "enum RegisterCode {\n#define REGISTER_CODE(R) kRegCode_##R,\n  GENERAL_REGISTERS(REGISTER_CODE)\n#undef REGISTER_CODE\n      kRegAfterLast\n}", "name_and_para": ""}], [{"name": "VRegister", "content": "class VRegister", "name_and_para": ""}, {"name": "VRegister", "content": "class VRegister : public RegisterBase<VRegister, kVRAfterLast> {\n  friend class RegisterBase;\n\n public:\n  explicit constexpr VRegister(int code) : RegisterBase(code) {}\n}", "name_and_para": ""}], [{"name": "Register", "content": "class Register", "name_and_para": ""}, {"name": "Register", "content": "class Register : public RegisterBase<Register, kRegAfterLast> {\n public:\n#if defined(V8_TARGET_LITTLE_ENDIAN)\n  static constexpr int kMantissaOffset = 0;\n  static constexpr int kExponentOffset = 4;\n#elif defined(V8_TARGET_BIG_ENDIAN)\n  static constexpr int kMantissaOffset = 4;\n  static constexpr int kExponentOffset = 0;\n#else\n#error Unknown endianness\n#endif\n\n private:\n  friend class RegisterBase;\n  explicit constexpr Register(int code) : RegisterBase(code) {}\n}", "name_and_para": ""}]]], [["./v8/src/codegen/riscv/assembler-riscv.h", "./v8/src/codegen/arm64/assembler-arm64.h"], 0.5714285714285714, 0.08, [[{"name": "EnsureSpace", "content": "class EnsureSpace {\n public:\n  explicit V8_INLINE EnsureSpace(Assembler* assembler);\n\n private:\n  Assembler::BlockPoolsScope block_pools_scope_;\n}", "name_and_para": ""}, {"name": "EnsureSpace", "content": "class EnsureSpace {\n public:\n  explicit inline EnsureSpace(Assembler* assembler);\n}", "name_and_para": ""}], [{"name": "MemOperand", "content": "class MemOperand {\n public:\n  inline MemOperand();\n  inline explicit MemOperand(Register base, int64_t offset = 0,\n                             AddrMode addrmode = Offset);\n  inline explicit MemOperand(Register base, Register regoffset,\n                             Shift shift = LSL, unsigned shift_amount = 0);\n  inline explicit MemOperand(Register base, Register regoffset, Extend extend,\n                             unsigned shift_amount = 0);\n  inline explicit MemOperand(Register base, const Operand& offset,\n                             AddrMode addrmode = Offset);\n\n  const Register& base() const { return base_; }\n  const Register& regoffset() const { return regoffset_; }\n  int64_t offset() const { return offset_; }\n  AddrMode addrmode() const { return addrmode_; }\n  Shift shift() const { return shift_; }\n  Extend extend() const { return extend_; }\n  unsigned shift_amount() const { return shift_amount_; }\n  inline bool IsImmediateOffset() const;\n  inline bool IsRegisterOffset() const;\n  inline bool IsPreIndex() const;\n  inline bool IsPostIndex() const;\n\n private:\n  Register base_;\n  Register regoffset_;\n  int64_t offset_;\n  AddrMode addrmode_;\n  Shift shift_;\n  Extend extend_;\n  unsigned shift_amount_;\n}", "name_and_para": ""}, {"name": "MemOperand", "content": "class V8_EXPORT_PRIVATE MemOperand : public Operand {\n public:\n  // Immediate value attached to offset.\n  enum OffsetAddend { offset_minus_one = -1, offset_zero = 0 };\n\n  explicit MemOperand(Register rn, int32_t offset = 0);\n  explicit MemOperand(Register rn, int32_t unit, int32_t multiplier,\n                      OffsetAddend offset_addend = offset_zero);\n  int32_t offset() const { return offset_; }\n\n  void set_offset(int32_t offset) { offset_ = offset; }\n\n  bool OffsetIsInt12Encodable() const { return is_int12(offset_); }\n\n private:\n  int32_t offset_;\n\n  friend class Assembler;\n}", "name_and_para": "class V8_EXPORT_PRIVATE MemOperand : public Operand "}], [{"name": "Operand", "content": "class Operand {\n  // TODO(all): If necessary, study more in details which methods\n  // TODO(all): should be inlined or not.\n public:\n  // rm, {<shift> {#<shift_amount>}}\n  // where <shift> is one of {LSL, LSR, ASR, ROR}.\n  //       <shift_amount> is uint6_t.\n  // This is allowed to be an implicit constructor because Operand is\n  // a wrapper class that doesn't normally perform any type conversion.\n  inline Operand(Register reg, Shift shift = LSL,\n                 unsigned shift_amount = 0);  // NOLINT(runtime/explicit)\n\n  // rm, <extend> {#<shift_amount>}\n  // where <extend> is one of {UXTB, UXTH, UXTW, UXTX, SXTB, SXTH, SXTW, SXTX}.\n  //       <shift_amount> is uint2_t.\n  inline Operand(Register reg, Extend extend, unsigned shift_amount = 0);\n\n  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.\n  static Operand EmbeddedHeapNumber(double number);\n\n  inline bool IsHeapNumberRequest() const;\n  inline HeapNumberRequest heap_number_request() const;\n  inline Immediate immediate_for_heap_number_request() const;\n\n  // Implicit constructor for all int types, ExternalReference, and Smi.\n  template <typename T>\n  inline Operand(T t);  // NOLINT(runtime/explicit)\n\n  // Implicit constructor for int types.\n  template <typename T>\n  inline Operand(T t, RelocInfo::Mode rmode);\n\n  inline bool IsImmediate() const;\n  inline bool IsShiftedRegister() const;\n  inline bool IsExtendedRegister() const;\n  inline bool IsZero() const;\n\n  // This returns an LSL shift (<= 4) operand as an equivalent extend operand,\n  // which helps in the encoding of instructions that use the stack pointer.\n  inline Operand ToExtendedRegister() const;\n\n  // Returns new Operand adapted for using with W registers.\n  inline Operand ToW() const;\n\n  inline Immediate immediate() const;\n  inline int64_t ImmediateValue() const;\n  inline RelocInfo::Mode ImmediateRMode() const;\n  inline Register reg() const;\n  inline Shift shift() const;\n  inline Extend extend() const;\n  inline unsigned shift_amount() const;\n\n  // Relocation information.\n  bool NeedsRelocation(const Assembler* assembler) const;\n\n private:\n  base::Optional<HeapNumberRequest> heap_number_request_;\n  Immediate immediate_;\n  Register reg_;\n  Shift shift_;\n  Extend extend_;\n  unsigned shift_amount_;\n}", "name_and_para": ""}, {"name": "Operand", "content": "class Operand {\n public:\n  // Immediate.\n  V8_INLINE explicit Operand(intptr_t immediate,\n                             RelocInfo::Mode rmode = RelocInfo::NO_INFO)\n      : rm_(no_reg), rmode_(rmode) {\n    value_.immediate = immediate;\n  }\n\n  V8_INLINE explicit Operand(Tagged<Smi> value)\n      : Operand(static_cast<intptr_t>(value.ptr())) {}\n\n  V8_INLINE explicit Operand(const ExternalReference& f)\n      : rm_(no_reg), rmode_(RelocInfo::EXTERNAL_REFERENCE) {\n    value_.immediate = static_cast<intptr_t>(f.address());\n  }\n\n  explicit Operand(Handle<HeapObject> handle);\n\n  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.\n\n  // Register.\n  V8_INLINE explicit Operand(Register rm) : rm_(rm) {}\n\n  // Return true if this is a register operand.\n  V8_INLINE bool is_reg() const { return rm_.is_valid(); }\n  inline intptr_t immediate() const {\n    DCHECK(!is_reg());\n    DCHECK(!IsHeapNumberRequest());\n    return value_.immediate;\n  }\n\n  bool IsImmediate() const { return !rm_.is_valid(); }\n\n  HeapNumberRequest heap_number_request() const {\n    DCHECK(IsHeapNumberRequest());\n    return value_.heap_number_request;\n  }\n\n  bool IsHeapNumberRequest() const {\n    DCHECK_IMPLIES(is_heap_number_request_, IsImmediate());\n    DCHECK_IMPLIES(is_heap_number_request_,\n                   rmode_ == RelocInfo::FULL_EMBEDDED_OBJECT ||\n                       rmode_ == RelocInfo::CODE_TARGET);\n    return is_heap_number_request_;\n  }\n\n  Register rm() const { return rm_; }\n\n  RelocInfo::Mode rmode() const { return rmode_; }\n\n private:\n  Register rm_;\n  union Value {\n    Value() {}\n    HeapNumberRequest heap_number_request;  // if is_heap_number_request_\n    intptr_t immediate;                     // otherwise\n  } value_;                                 // valid if rm_ == no_reg\n  bool is_heap_number_request_ = false;\n  RelocInfo::Mode rmode_;\n\n  friend class Assembler;\n  friend class MacroAssembler;\n}", "name_and_para": ""}], [{"name": "SafepointTableBuilder", "content": "class SafepointTableBuilder", "name_and_para": ""}, {"name": "SafepointTableBuilder", "content": "class SafepointTableBuilder", "name_and_para": ""}]]], [["./v8/src/codegen/riscv/macro-assembler-riscv.h", "./v8/src/codegen/arm64/macro-assembler-arm64.h"], 0.2, 0.18181818181818182, [[{"name": "MoveCycleState", "content": "struct MoveCycleState {\n  // List of scratch registers reserved for pending moves in a move cycle, and\n  // which should therefore not be used as a temporary location by\n  // {MoveToTempLocation}.\n  RegList scratch_regs;\n  DoubleRegList scratch_fp_regs;\n  // Available scratch registers during the move cycle resolution scope.\n  base::Optional<UseScratchRegisterScope> temps;\n  // Scratch register picked by {MoveToTempLocation}.\n  base::Optional<CPURegister> scratch_reg;\n}", "name_and_para": ""}, {"name": "MoveCycleState", "content": "struct MoveCycleState {\n  // List of scratch registers reserved for pending moves in a move cycle, and\n  // which should therefore not be used as a temporary location by\n  // {MoveToTempLocation}.\n  RegList scratch_regs;\n  // Available scratch registers during the move cycle resolution scope.\n  base::Optional<UseScratchRegisterScope> temps;\n  // Scratch register picked by {MoveToTempLocation}.\n  base::Optional<Register> scratch_reg;\n}", "name_and_para": ""}], [{"name": "MacroAssemblerBase", "content": "class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {\n public:\n  using MacroAssemblerBase::MacroAssemblerBase;\n\n#if DEBUG\n  void set_allow_macro_instructions(bool value) {\n    allow_macro_instructions_ = value;\n  }\n  bool allow_macro_instructions() const { return allow_macro_instructions_; }\n#endif\n\n  // We should not use near calls or jumps for calls to external references,\n  // since the code spaces are not guaranteed to be close to each other.\n  bool CanUseNearCallOrJump(RelocInfo::Mode rmode) {\n    return rmode != RelocInfo::EXTERNAL_REFERENCE;\n  }\n\n  static bool IsNearCallOffset(int64_t offset);\n\n  // Activation support.\n  void EnterFrame(StackFrame::Type type);\n  void EnterFrame(StackFrame::Type type, bool load_constant_pool_pointer_reg) {\n    // Out-of-line constant pool not implemented on arm64.\n    UNREACHABLE();\n  }\n  void LeaveFrame(StackFrame::Type type);\n\n  inline void InitializeRootRegister();\n\n  void Mov(const Register& rd, const Operand& operand,\n           DiscardMoveMode discard_mode = kDontDiscardForSameWReg);\n  void Mov(const Register& rd, uint64_t imm);\n  void Mov(const VRegister& vd, int vd_index, const VRegister& vn,\n           int vn_index) {\n    DCHECK(allow_macro_instructions());\n    mov(vd, vd_index, vn, vn_index);\n  }\n  void Mov(const Register& rd, Tagged<Smi> smi);\n  void Mov(const VRegister& vd, const VRegister& vn, int index) {\n    DCHECK(allow_macro_instructions());\n    mov(vd, vn, index);\n  }\n  void Mov(const VRegister& vd, int vd_index, const Register& rn) {\n    DCHECK(allow_macro_instructions());\n    mov(vd, vd_index, rn);\n  }\n  void Mov(const Register& rd, const VRegister& vn, int vn_index) {\n    DCHECK(allow_macro_instructions());\n    mov(rd, vn, vn_index);\n  }\n\n  // These are required for compatibility with architecture independent code.\n  // Remove if not needed.\n  void Move(Register dst, Tagged<Smi> src);\n  void Move(Register dst, MemOperand src);\n  void Move(Register dst, Register src);\n\n  // Move src0 to dst0 and src1 to dst1, handling possible overlaps.\n  void MovePair(Register dst0, Register src0, Register dst1, Register src1);\n\n  // Register swap. Note that the register operands should be distinct.\n  void Swap(Register lhs, Register rhs);\n  void Swap(VRegister lhs, VRegister rhs);\n\n// NEON by element instructions.\n#define NEON_BYELEMENT_MACRO_LIST(V) \\\n  V(fmla, Fmla)                      \\\n  V(fmls, Fmls)                      \\\n  V(fmul, Fmul)                      \\\n  V(fmulx, Fmulx)                    \\\n  V(mul, Mul)                        \\\n  V(mla, Mla)                        \\\n  V(mls, Mls)                        \\\n  V(sqdmulh, Sqdmulh)                \\\n  V(sqrdmulh, Sqrdmulh)              \\\n  V(sqdmull, Sqdmull)                \\\n  V(sqdmull2, Sqdmull2)              \\\n  V(sqdmlal, Sqdmlal)                \\\n  V(sqdmlal2, Sqdmlal2)              \\\n  V(sqdmlsl, Sqdmlsl)                \\\n  V(sqdmlsl2, Sqdmlsl2)              \\\n  V(smull, Smull)                    \\\n  V(smull2, Smull2)                  \\\n  V(smlal, Smlal)                    \\\n  V(smlal2, Smlal2)                  \\\n  V(smlsl, Smlsl)                    \\\n  V(smlsl2, Smlsl2)                  \\\n  V(umull, Umull)                    \\\n  V(umull2, Umull2)                  \\\n  V(umlal, Umlal)                    \\\n  V(umlal2, Umlal2)                  \\\n  V(umlsl, Umlsl)                    \\\n  V(umlsl2, Umlsl2)\n\n#define DEFINE_MACRO_ASM_FUNC(ASM, MASM)                                   \\\n  void MASM(const VRegister& vd, const VRegister& vn, const VRegister& vm, \\\n            int vm_index) {                                                \\\n    DCHECK(allow_macro_instructions());                                    \\\n    ASM(vd, vn, vm, vm_index);                                             \\\n  }\n  NEON_BYELEMENT_MACRO_LIST(DEFINE_MACRO_ASM_FUNC)\n#undef DEFINE_MACRO_ASM_FUNC\n\n// NEON 2 vector register instructions.\n#define NEON_2VREG_MACRO_LIST(V) \\\n  V(abs, Abs)                    \\\n  V(addp, Addp)                  \\\n  V(addv, Addv)                  \\\n  V(cls, Cls)                    \\\n  V(clz, Clz)                    \\\n  V(cnt, Cnt)                    \\\n  V(faddp, Faddp)                \\\n  V(fcvtas, Fcvtas)              \\\n  V(fcvtau, Fcvtau)              \\\n  V(fcvtl, Fcvtl)                \\\n  V(fcvtms, Fcvtms)              \\\n  V(fcvtmu, Fcvtmu)              \\\n  V(fcvtn, Fcvtn)                \\\n  V(fcvtns, Fcvtns)              \\\n  V(fcvtnu, Fcvtnu)              \\\n  V(fcvtps, Fcvtps)              \\\n  V(fcvtpu, Fcvtpu)              \\\n  V(fmaxnmp, Fmaxnmp)            \\\n  V(fmaxnmv, Fmaxnmv)            \\\n  V(fmaxp, Fmaxp)                \\\n  V(fmaxv, Fmaxv)                \\\n  V(fminnmp, Fminnmp)            \\\n  V(fminnmv, Fminnmv)            \\\n  V(fminp, Fminp)                \\\n  V(fminv, Fminv)                \\\n  V(fneg, Fneg)                  \\\n  V(frecpe, Frecpe)              \\\n  V(frecpx, Frecpx)              \\\n  V(frinta, Frinta)              \\\n  V(frinti, Frinti)              \\\n  V(frintm, Frintm)              \\\n  V(frintn, Frintn)              \\\n  V(frintp, Frintp)              \\\n  V(frintx, Frintx)              \\\n  V(frintz, Frintz)              \\\n  V(frsqrte, Frsqrte)            \\\n  V(fsqrt, Fsqrt)                \\\n  V(mov, Mov)                    \\\n  V(mvn, Mvn)                    \\\n  V(neg, Neg)                    \\\n  V(not_, Not)                   \\\n  V(rbit, Rbit)                  \\\n  V(rev16, Rev16)                \\\n  V(rev32, Rev32)                \\\n  V(rev64, Rev64)                \\\n  V(sadalp, Sadalp)              \\\n  V(saddlp, Saddlp)              \\\n  V(saddlv, Saddlv)              \\\n  V(smaxv, Smaxv)                \\\n  V(sminv, Sminv)                \\\n  V(sqabs, Sqabs)                \\\n  V(sqneg, Sqneg)                \\\n  V(sqxtn2, Sqxtn2)              \\\n  V(sqxtn, Sqxtn)                \\\n  V(sqxtun2, Sqxtun2)            \\\n  V(sqxtun, Sqxtun)              \\\n  V(suqadd, Suqadd)              \\\n  V(sxtl2, Sxtl2)                \\\n  V(sxtl, Sxtl)                  \\\n  V(uadalp, Uadalp)              \\\n  V(uaddlp, Uaddlp)              \\\n  V(uaddlv, Uaddlv)              \\\n  V(umaxv, Umaxv)                \\\n  V(uminv, Uminv)                \\\n  V(uqxtn2, Uqxtn2)              \\\n  V(uqxtn, Uqxtn)                \\\n  V(urecpe, Urecpe)              \\\n  V(ursqrte, Ursqrte)            \\\n  V(usqadd, Usqadd)              \\\n  V(uxtl2, Uxtl2)                \\\n  V(uxtl, Uxtl)                  \\\n  V(xtn2, Xtn2)                  \\\n  V(xtn, Xtn)\n\n#define DEFINE_MACRO_ASM_FUNC(ASM, MASM)                \\\n  void MASM(const VRegister& vd, const VRegister& vn) { \\\n    DCHECK(allow_macro_instructions());                 \\\n    ASM(vd, vn);                                        \\\n  }\n  NEON_2VREG_MACRO_LIST(DEFINE_MACRO_ASM_FUNC)\n#undef DEFINE_MACRO_ASM_FUNC\n#undef NEON_2VREG_MACRO_LIST\n\n// NEON 2 vector register with immediate instructions.\n#define NEON_2VREG_FPIMM_MACRO_LIST(V) \\\n  V(fcmeq, Fcmeq)                      \\\n  V(fcmge, Fcmge)                      \\\n  V(fcmgt, Fcmgt)                      \\\n  V(fcmle, Fcmle)                      \\\n  V(fcmlt, Fcmlt)\n\n#define DEFINE_MACRO_ASM_FUNC(ASM, MASM)                            \\\n  void MASM(const VRegister& vd, const VRegister& vn, double imm) { \\\n    DCHECK(allow_macro_instructions());                             \\\n    ASM(vd, vn, imm);                                               \\\n  }\n  NEON_2VREG_FPIMM_MACRO_LIST(DEFINE_MACRO_ASM_FUNC)\n#undef DEFINE_MACRO_ASM_FUNC\n\n// NEON 3 vector register instructions.\n#define NEON_3VREG_MACRO_LIST(V) \\\n  V(add, Add)                    \\\n  V(addhn2, Addhn2)              \\\n  V(addhn, Addhn)                \\\n  V(addp, Addp)                  \\\n  V(and_, And)                   \\\n  V(bic, Bic)                    \\\n  V(bif, Bif)                    \\\n  V(bit, Bit)                    \\\n  V(bsl, Bsl)                    \\\n  V(cmeq, Cmeq)                  \\\n  V(cmge, Cmge)                  \\\n  V(cmgt, Cmgt)                  \\\n  V(cmhi, Cmhi)                  \\\n  V(cmhs, Cmhs)                  \\\n  V(cmtst, Cmtst)                \\\n  V(eor, Eor)                    \\\n  V(fabd, Fabd)                  \\\n  V(facge, Facge)                \\\n  V(facgt, Facgt)                \\\n  V(faddp, Faddp)                \\\n  V(fcmeq, Fcmeq)                \\\n  V(fcmge, Fcmge)                \\\n  V(fcmgt, Fcmgt)                \\\n  V(fmaxnmp, Fmaxnmp)            \\\n  V(fmaxp, Fmaxp)                \\\n  V(fminnmp, Fminnmp)            \\\n  V(fminp, Fminp)                \\\n  V(fmla, Fmla)                  \\\n  V(fmls, Fmls)                  \\\n  V(fmulx, Fmulx)                \\\n  V(fnmul, Fnmul)                \\\n  V(frecps, Frecps)              \\\n  V(frsqrts, Frsqrts)            \\\n  V(mla, Mla)                    \\\n  V(mls, Mls)                    \\\n  V(mul, Mul)                    \\\n  V(orn, Orn)                    \\\n  V(orr, Orr)                    \\\n  V(pmull2, Pmull2)              \\\n  V(pmull, Pmull)                \\\n  V(pmul, Pmul)                  \\\n  V(raddhn2, Raddhn2)            \\\n  V(raddhn, Raddhn)              \\\n  V(rsubhn2, Rsubhn2)            \\\n  V(rsubhn, Rsubhn)              \\\n  V(sabal2, Sabal2)              \\\n  V(sabal, Sabal)                \\\n  V(saba, Saba)                  \\\n  V(sabdl2, Sabdl2)              \\\n  V(sabdl, Sabdl)                \\\n  V(sabd, Sabd)                  \\\n  V(saddl2, Saddl2)              \\\n  V(saddl, Saddl)                \\\n  V(saddw2, Saddw2)              \\\n  V(saddw, Saddw)                \\\n  V(sdot, Sdot)                  \\\n  V(shadd, Shadd)                \\\n  V(shsub, Shsub)                \\\n  V(smaxp, Smaxp)                \\\n  V(smax, Smax)                  \\\n  V(sminp, Sminp)                \\\n  V(smin, Smin)                  \\\n  V(smlal2, Smlal2)              \\\n  V(smlal, Smlal)                \\\n  V(smlsl2, Smlsl2)              \\\n  V(smlsl, Smlsl)                \\\n  V(smull2, Smull2)              \\\n  V(smull, Smull)                \\\n  V(sqadd, Sqadd)                \\\n  V(sqdmlal2, Sqdmlal2)          \\\n  V(sqdmlal, Sqdmlal)            \\\n  V(sqdmlsl2, Sqdmlsl2)          \\\n  V(sqdmlsl, Sqdmlsl)            \\\n  V(sqdmulh, Sqdmulh)            \\\n  V(sqdmull2, Sqdmull2)          \\\n  V(sqdmull, Sqdmull)            \\\n  V(sqrdmulh, Sqrdmulh)          \\\n  V(sqrshl, Sqrshl)              \\\n  V(sqshl, Sqshl)                \\\n  V(sqsub, Sqsub)                \\\n  V(srhadd, Srhadd)              \\\n  V(srshl, Srshl)                \\\n  V(sshl, Sshl)                  \\\n  V(ssubl2, Ssubl2)              \\\n  V(ssubl, Ssubl)                \\\n  V(ssubw2, Ssubw2)              \\\n  V(ssubw, Ssubw)                \\\n  V(subhn2, Subhn2)              \\\n  V(subhn, Subhn)                \\\n  V(sub, Sub)                    \\\n  V(trn1, Trn1)                  \\\n  V(trn2, Trn2)                  \\\n  V(uabal2, Uabal2)              \\\n  V(uabal, Uabal)                \\\n  V(uaba, Uaba)                  \\\n  V(uabdl2, Uabdl2)              \\\n  V(uabdl, Uabdl)                \\\n  V(uabd, Uabd)                  \\\n  V(uaddl2, Uaddl2)              \\\n  V(uaddl, Uaddl)                \\\n  V(uaddw2, Uaddw2)              \\\n  V(uaddw, Uaddw)                \\\n  V(uhadd, Uhadd)                \\\n  V(uhsub, Uhsub)                \\\n  V(umaxp, Umaxp)                \\\n  V(umax, Umax)                  \\\n  V(uminp, Uminp)                \\\n  V(umin, Umin)                  \\\n  V(umlal2, Umlal2)              \\\n  V(umlal, Umlal)                \\\n  V(umlsl2, Umlsl2)              \\\n  V(umlsl, Umlsl)                \\\n  V(umull2, Umull2)              \\\n  V(umull, Umull)                \\\n  V(uqadd, Uqadd)                \\\n  V(uqrshl, Uqrshl)              \\\n  V(uqshl, Uqshl)                \\\n  V(uqsub, Uqsub)                \\\n  V(urhadd, Urhadd)              \\\n  V(urshl, Urshl)                \\\n  V(ushl, Ushl)                  \\\n  V(usubl2, Usubl2)              \\\n  V(usubl, Usubl)                \\\n  V(usubw2, Usubw2)              \\\n  V(usubw, Usubw)                \\\n  V(uzp1, Uzp1)                  \\\n  V(uzp2, Uzp2)                  \\\n  V(zip1, Zip1)                  \\\n  V(zip2, Zip2)\n\n#define DEFINE_MACRO_ASM_FUNC(ASM, MASM)                                     \\\n  void MASM(const VRegister& vd, const VRegister& vn, const VRegister& vm) { \\\n    DCHECK(allow_macro_instructions());                                      \\\n    ASM(vd, vn, vm);                                                         \\\n  }\n  NEON_3VREG_MACRO_LIST(DEFINE_MACRO_ASM_FUNC)\n#undef DEFINE_MACRO_ASM_FUNC\n\n  void Bic(const VRegister& vd, const int imm8, const int left_shift = 0) {\n    DCHECK(allow_macro_instructions());\n    bic(vd, imm8, left_shift);\n  }\n\n  // This is required for compatibility in architecture independent code.\n  inline void jmp(Label* L);\n\n  void B(Label* label, BranchType type, Register reg = NoReg, int bit = -1);\n  inline void B(Label* label);\n  inline void B(Condition cond, Label* label);\n  void B(Label* label, Condition cond);\n\n  void Tbnz(const Register& rt, unsigned bit_pos, Label* label);\n  void Tbz(const Register& rt, unsigned bit_pos, Label* label);\n\n  void Cbnz(const Register& rt, Label* label);\n  void Cbz(const Register& rt, Label* label);\n\n  void Pacibsp() {\n    DCHECK(allow_macro_instructions_);\n    pacibsp();\n  }\n  void Autibsp() {\n    DCHECK(allow_macro_instructions_);\n    autibsp();\n  }\n\n  // The 1716 pac and aut instructions encourage people to use x16 and x17\n  // directly, perhaps without realising that this is forbidden. For example:\n  //\n  //     UseScratchRegisterScope temps(&masm);\n  //     Register temp = temps.AcquireX();  // temp will be x16\n  //     __ Mov(x17, ptr);\n  //     __ Mov(x16, modifier);  // Will override temp!\n  //     __ Pacib1716();\n  //\n  // To work around this issue, you must exclude x16 and x17 from the scratch\n  // register list. You may need to replace them with other registers:\n  //\n  //     UseScratchRegisterScope temps(&masm);\n  //     temps.Exclude(x16, x17);\n  //     temps.Include(x10, x11);\n  //     __ Mov(x17, ptr);\n  //     __ Mov(x16, modifier);\n  //     __ Pacib1716();\n  void Pacib1716() {\n    DCHECK(allow_macro_instructions_);\n    DCHECK(!TmpList()->IncludesAliasOf(x16));\n    DCHECK(!TmpList()->IncludesAliasOf(x17));\n    pacib1716();\n  }\n  void Autib1716() {\n    DCHECK(allow_macro_instructions_);\n    DCHECK(!TmpList()->IncludesAliasOf(x16));\n    DCHECK(!TmpList()->IncludesAliasOf(x17));\n    autib1716();\n  }\n\n  inline void Dmb(BarrierDomain domain, BarrierType type);\n  inline void Dsb(BarrierDomain domain, BarrierType type);\n  inline void Isb();\n  inline void Csdb();\n\n  inline void SmiUntag(Register dst, Register src);\n  inline void SmiUntag(Register dst, const MemOperand& src);\n  inline void SmiUntag(Register smi);\n\n  inline void SmiTag(Register dst, Register src);\n  inline void SmiTag(Register smi);\n\n  inline void SmiToInt32(Register smi);\n  inline void SmiToInt32(Register dst, Register smi);\n\n  // Calls Abort(msg) if the condition cond is not satisfied.\n  // Use --debug_code to enable.\n  void Assert(Condition cond, AbortReason reason) NOOP_UNLESS_DEBUG_CODE;\n\n  // Like Assert(), but without condition.\n  // Use --debug_code to enable.\n  void AssertUnreachable(AbortReason reason) NOOP_UNLESS_DEBUG_CODE;\n\n  void AssertSmi(Register object,\n                 AbortReason reason = AbortReason::kOperandIsNotASmi)\n      NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is a smi, enabled via --debug-code.\n  void AssertNotSmi(Register object,\n                    AbortReason reason = AbortReason::kOperandIsASmi)\n      NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if a 64 bit register containing a 32 bit payload does\n  // not have zeros in the top 32 bits, enabled via --debug-code.\n  void AssertZeroExtended(Register int32_register) NOOP_UNLESS_DEBUG_CODE;\n\n  void AssertJSAny(Register object, Register map_tmp, Register tmp,\n                   AbortReason abort_reason) NOOP_UNLESS_DEBUG_CODE;\n\n  // Like Assert(), but always enabled.\n  void Check(Condition cond, AbortReason reason);\n\n  // Functions performing a check on a known or potential smi. Returns\n  // a condition that is satisfied if the check is successful.\n  Condition CheckSmi(Register src);\n\n  inline void Debug(const char* message, uint32_t code, Instr params = BREAK);\n\n  void Trap();\n  void DebugBreak();\n\n  // Print a message to stderr and abort execution.\n  void Abort(AbortReason reason);\n\n  // Like printf, but print at run-time from generated code.\n  //\n  // The caller must ensure that arguments for floating-point placeholders\n  // (such as %e, %f or %g) are VRegisters, and that arguments for integer\n  // placeholders are Registers.\n  //\n  // Format placeholders that refer to more than one argument, or to a specific\n  // argument, are not supported. This includes formats like \"%1$d\" or \"%.*d\".\n  //\n  // This function automatically preserves caller-saved registers so that\n  // calling code can use Printf at any point without having to worry about\n  // corruption. The preservation mechanism generates a lot of code. If this is\n  // a problem, preserve the important registers manually and then call\n  // PrintfNoPreserve. Callee-saved registers are not used by Printf, and are\n  // implicitly preserved.\n  void Printf(const char* format, CPURegister arg0 = NoCPUReg,\n              CPURegister arg1 = NoCPUReg, CPURegister arg2 = NoCPUReg,\n              CPURegister arg3 = NoCPUReg);\n\n  // Like Printf, but don't preserve any caller-saved registers, not even 'lr'.\n  //\n  // The return code from the system printf call will be returned in x0.\n  void PrintfNoPreserve(const char* format, const CPURegister& arg0 = NoCPUReg,\n                        const CPURegister& arg1 = NoCPUReg,\n                        const CPURegister& arg2 = NoCPUReg,\n                        const CPURegister& arg3 = NoCPUReg);\n\n  // Remaining instructions are simple pass-through calls to the assembler.\n  inline void Asr(const Register& rd, const Register& rn, unsigned shift);\n  inline void Asr(const Register& rd, const Register& rn, const Register& rm);\n\n  // Try to move an immediate into the destination register in a single\n  // instruction. Returns true for success, and updates the contents of dst.\n  // Returns false, otherwise.\n  bool TryOneInstrMoveImmediate(const Register& dst, int64_t imm);\n\n  inline void Bind(Label* label,\n                   BranchTargetIdentifier id = BranchTargetIdentifier::kNone);\n\n  // Control-flow integrity:\n\n  // Define a function entrypoint.\n  inline void CodeEntry();\n  // Define an exception handler.\n  inline void ExceptionHandler();\n  // Define an exception handler and bind a label.\n  inline void BindExceptionHandler(Label* label);\n\n  // Control-flow integrity:\n\n  // Define a jump (BR) target.\n  inline void JumpTarget();\n  // Define a jump (BR) target and bind a label.\n  inline void BindJumpTarget(Label* label);\n  // Define a call (BLR) target. The target also allows tail calls (via BR)\n  // when the target is x16 or x17.\n  inline void CallTarget();\n  // Define a jump/call target and bind a label.\n  inline void BindCallTarget(Label* label);\n  // Define a jump/call target.\n  inline void JumpOrCallTarget();\n  // Define a jump/call target and bind a label.\n  inline void BindJumpOrCallTarget(Label* label);\n\n  static unsigned CountSetHalfWords(uint64_t imm, unsigned reg_size);\n\n  CPURegList* TmpList() { return &tmp_list_; }\n  CPURegList* FPTmpList() { return &fptmp_list_; }\n\n  static CPURegList DefaultTmpList();\n  static CPURegList DefaultFPTmpList();\n\n  // Move macros.\n  inline void Mvn(const Register& rd, uint64_t imm);\n  void Mvn(const Register& rd, const Operand& operand);\n  static bool IsImmMovn(uint64_t imm, unsigned reg_size);\n  static bool IsImmMovz(uint64_t imm, unsigned reg_size);\n\n  void LogicalMacro(const Register& rd, const Register& rn,\n                    const Operand& operand, LogicalOp op);\n  void AddSubMacro(const Register& rd, const Register& rn,\n                   const Operand& operand, FlagsUpdate S, AddSubOp op);\n  inline void Orr(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  void Orr(const VRegister& vd, const int imm8, const int left_shift = 0) {\n    DCHECK(allow_macro_instructions());\n    orr(vd, imm8, left_shift);\n  }\n  inline void Orn(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Eor(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Eon(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void And(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Ands(const Register& rd, const Register& rn,\n                   const Operand& operand);\n  inline void Tst(const Register& rn, const Operand& operand);\n  inline void Bic(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Blr(const Register& xn);\n  inline void Cmp(const Register& rn, const Operand& operand);\n  inline void CmpTagged(const Register& rn, const Operand& operand);\n  inline void Subs(const Register& rd, const Register& rn,\n                   const Operand& operand);\n  void Csel(const Register& rd, const Register& rn, const Operand& operand,\n            Condition cond);\n  inline void Fcsel(const VRegister& fd, const VRegister& fn,\n                    const VRegister& fm, Condition cond);\n\n  // Emits a runtime assert that the stack pointer is aligned.\n  void AssertSpAligned() NOOP_UNLESS_DEBUG_CODE;\n\n  // Copy slot_count stack slots from the stack offset specified by src to\n  // the stack offset specified by dst. The offsets and count are expressed in\n  // slot-sized units. Offset dst must be less than src, or the gap between\n  // them must be greater than or equal to slot_count, otherwise the result is\n  // unpredictable. The function may corrupt its register arguments. The\n  // registers must not alias each other.\n  void CopySlots(int dst, Register src, Register slot_count);\n  void CopySlots(Register dst, Register src, Register slot_count);\n\n  // Copy count double words from the address in register src to the address\n  // in register dst. There are three modes for this function:\n  // 1) Address dst must be less than src, or the gap between them must be\n  //    greater than or equal to count double words, otherwise the result is\n  //    unpredictable. This is the default mode.\n  // 2) Address src must be less than dst, or the gap between them must be\n  //    greater than or equal to count double words, otherwise the result is\n  //    undpredictable. In this mode, src and dst specify the last (highest)\n  //    address of the regions to copy from and to.\n  // 3) The same as mode 1, but the words are copied in the reversed order.\n  // The case where src == dst is not supported.\n  // The function may corrupt its register arguments. The registers must not\n  // alias each other.\n  enum CopyDoubleWordsMode {\n    kDstLessThanSrc,\n    kSrcLessThanDst,\n    kDstLessThanSrcAndReverse\n  };\n  void CopyDoubleWords(Register dst, Register src, Register count,\n                       CopyDoubleWordsMode mode = kDstLessThanSrc);\n\n  // Calculate the address of a double word-sized slot at slot_offset from the\n  // stack pointer, and write it to dst. Positive slot_offsets are at addresses\n  // greater than sp, with slot zero at sp.\n  void SlotAddress(Register dst, int slot_offset);\n  void SlotAddress(Register dst, Register slot_offset);\n\n  // Load a literal from the inline constant pool.\n  inline void Ldr(const CPURegister& rt, const Operand& imm);\n\n  // Claim or drop stack space.\n  //\n  // On Windows, Claim will write a value every 4k, as is required by the stack\n  // expansion mechanism.\n  //\n  // The stack pointer must be aligned to 16 bytes and the size claimed or\n  // dropped must be a multiple of 16 bytes.\n  //\n  // Note that unit_size must be specified in bytes. For variants which take a\n  // Register count, the unit size must be a power of two.\n  inline void Claim(int64_t count, uint64_t unit_size = kXRegSize);\n  inline void Claim(const Register& count, uint64_t unit_size = kXRegSize,\n                    bool assume_sp_aligned = true);\n  inline void Drop(int64_t count, uint64_t unit_size = kXRegSize);\n  inline void Drop(const Register& count, uint64_t unit_size = kXRegSize);\n\n  // Drop 'count' arguments from the stack, rounded up to a multiple of two,\n  // without actually accessing memory.\n  // We assume the size of the arguments is the pointer size.\n  // An optional mode argument is passed, which can indicate we need to\n  // explicitly add the receiver to the count.\n  enum ArgumentsCountMode { kCountIncludesReceiver, kCountExcludesReceiver };\n  inline void DropArguments(const Register& count,\n                            ArgumentsCountMode mode = kCountIncludesReceiver);\n  inline void DropArguments(int64_t count,\n                            ArgumentsCountMode mode = kCountIncludesReceiver);\n\n  // Drop 'count' slots from stack, rounded up to a multiple of two, without\n  // actually accessing memory.\n  inline void DropSlots(int64_t count);\n\n  // Push a single argument, with padding, to the stack.\n  inline void PushArgument(const Register& arg);\n\n  // Add and sub macros.\n  inline void Add(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Adds(const Register& rd, const Register& rn,\n                   const Operand& operand);\n  inline void Sub(const Register& rd, const Register& rn,\n                  const Operand& operand);\n\n  // Abort execution if argument is not a positive or zero integer, enabled via\n  // --debug-code.\n  void AssertPositiveOrZero(Register value) NOOP_UNLESS_DEBUG_CODE;\n\n#define DECLARE_FUNCTION(FN, REGTYPE, REG, OP) \\\n  inline void FN(const REGTYPE REG, const MemOperand& addr);\n      LS_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n  // Caution: if {value} is a 32-bit negative int, it should be sign-extended\n  // to 64-bit before calling this function.\n  void Switch(Register scratch, Register value, int case_value_base,\n              Label** labels, int num_labels);\n\n  // Push or pop up to 4 registers of the same width to or from the stack.\n  //\n  // If an argument register is 'NoReg', all further arguments are also assumed\n  // to be 'NoReg', and are thus not pushed or popped.\n  //\n  // Arguments are ordered such that \"Push(a, b);\" is functionally equivalent\n  // to \"Push(a); Push(b);\".\n  //\n  // It is valid to push the same register more than once, and there is no\n  // restriction on the order in which registers are specified.\n  //\n  // It is not valid to pop into the same register more than once in one\n  // operation, not even into the zero register.\n  //\n  // The stack pointer must be aligned to 16 bytes on entry and the total size\n  // of the specified registers must also be a multiple of 16 bytes.\n  //\n  // Other than the registers passed into Pop, the stack pointer, (possibly)\n  // the system stack pointer and (possibly) the link register, these methods\n  // do not modify any other registers.\n  //\n  // Some of the methods take an optional LoadLRMode or StoreLRMode template\n  // argument, which specifies whether we need to sign the link register at the\n  // start of the operation, or authenticate it at the end of the operation,\n  // when control flow integrity measures are enabled.\n  // When the mode is kDontLoadLR or kDontStoreLR, LR must not be passed as an\n  // argument to the operation.\n  enum LoadLRMode { kAuthLR, kDontLoadLR };\n  enum StoreLRMode { kSignLR, kDontStoreLR };\n  template <StoreLRMode lr_mode = kDontStoreLR>\n  void Push(const CPURegister& src0, const CPURegister& src1 = NoReg,\n            const CPURegister& src2 = NoReg, const CPURegister& src3 = NoReg);\n  void Push(const CPURegister& src0, const CPURegister& src1,\n            const CPURegister& src2, const CPURegister& src3,\n            const CPURegister& src4, const CPURegister& src5 = NoReg,\n            const CPURegister& src6 = NoReg, const CPURegister& src7 = NoReg);\n  template <LoadLRMode lr_mode = kDontLoadLR>\n  void Pop(const CPURegister& dst0, const CPURegister& dst1 = NoReg,\n           const CPURegister& dst2 = NoReg, const CPURegister& dst3 = NoReg);\n  void Pop(const CPURegister& dst0, const CPURegister& dst1,\n           const CPURegister& dst2, const CPURegister& dst3,\n           const CPURegister& dst4, const CPURegister& dst5 = NoReg,\n           const CPURegister& dst6 = NoReg, const CPURegister& dst7 = NoReg);\n  template <StoreLRMode lr_mode = kDontStoreLR>\n  void Push(const Register& src0, const VRegister& src1);\n\n  void MaybeSaveRegisters(RegList registers);\n  void MaybeRestoreRegisters(RegList registers);\n\n  void CallEphemeronKeyBarrier(Register object, Operand offset,\n                               SaveFPRegsMode fp_mode);\n\n  void CallIndirectPointerBarrier(Register object, Operand offset,\n                                  SaveFPRegsMode fp_mode,\n                                  IndirectPointerTag tag);\n\n  void CallRecordWriteStubSaveRegisters(\n      Register object, Operand offset, SaveFPRegsMode fp_mode,\n      StubCallMode mode = StubCallMode::kCallBuiltinPointer);\n  void CallRecordWriteStub(\n      Register object, Register slot_address, SaveFPRegsMode fp_mode,\n      StubCallMode mode = StubCallMode::kCallBuiltinPointer);\n\n  // For a given |object| and |offset|:\n  //   - Move |object| to |dst_object|.\n  //   - Compute the address of the slot pointed to by |offset| in |object| and\n  //     write it to |dst_slot|.\n  // This method makes sure |object| and |offset| are allowed to overlap with\n  // the destination registers.\n  void MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                         Register object, Operand offset);\n\n  // Alternative forms of Push and Pop, taking a RegList or CPURegList that\n  // specifies the registers that are to be pushed or popped. Higher-numbered\n  // registers are associated with higher memory addresses (as in the A32 push\n  // and pop instructions).\n  //\n  // (Push|Pop)SizeRegList allow you to specify the register size as a\n  // parameter. Only kXRegSizeInBits, kWRegSizeInBits, kDRegSizeInBits and\n  // kSRegSizeInBits are supported.\n  //\n  // Otherwise, (Push|Pop)(CPU|X|W|D|S)RegList is preferred.\n  void PushCPURegList(CPURegList registers);\n  void PopCPURegList(CPURegList registers);\n\n  // Calculate how much stack space (in bytes) are required to store caller\n  // registers excluding those specified in the arguments.\n  int RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                      Register exclusion) const;\n\n  // Push caller saved registers on the stack, and return the number of bytes\n  // stack pointer is adjusted.\n  int PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion = no_reg);\n\n  // Restore caller saved registers from the stack, and return the number of\n  // bytes stack pointer is adjusted.\n  int PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion = no_reg);\n\n  // Move an immediate into register dst, and return an Operand object for use\n  // with a subsequent instruction that accepts a shift. The value moved into\n  // dst is not necessarily equal to imm; it may have had a shifting operation\n  // applied to it that will be subsequently undone by the shift applied in the\n  // Operand.\n  Operand MoveImmediateForShiftedOp(const Register& dst, int64_t imm,\n                                    PreShiftImmMode mode);\n\n  void CheckPageFlag(const Register& object, int mask, Condition cc,\n                     Label* condition_met);\n\n  void CheckPageFlag(const Register& object, Register scratch, int mask,\n                     Condition cc, Label* condition_met) {\n    CheckPageFlag(object, mask, cc, condition_met);\n  }\n\n  // Compare a register with an operand, and branch to label depending on the\n  // condition. May corrupt the status flags.\n  inline void CompareAndBranch(const Register& lhs, const Operand& rhs,\n                               Condition cond, Label* label);\n  inline void CompareTaggedAndBranch(const Register& lhs, const Operand& rhs,\n                                     Condition cond, Label* label);\n\n  // Test the bits of register defined by bit_pattern, and branch if ANY of\n  // those bits are set. May corrupt the status flags.\n  inline void TestAndBranchIfAnySet(const Register& reg,\n                                    const uint64_t bit_pattern, Label* label);\n\n  // Test the bits of register defined by bit_pattern, and branch if ALL of\n  // those bits are clear (ie. not set.) May corrupt the status flags.\n  inline void TestAndBranchIfAllClear(const Register& reg,\n                                      const uint64_t bit_pattern, Label* label);\n\n  inline void Brk(int code);\n\n  inline void JumpIfSmi(Register value, Label* smi_label,\n                        Label* not_smi_label = nullptr);\n\n  inline void JumpIfEqual(Register x, int32_t y, Label* dest);\n  inline void JumpIfLessThan(Register x, int32_t y, Label* dest);\n\n  void JumpIfMarking(Label* is_marking,\n                     Label::Distance condition_met_distance = Label::kFar);\n  void JumpIfNotMarking(Label* not_marking,\n                        Label::Distance condition_met_distance = Label::kFar);\n\n  void LoadMap(Register dst, Register object);\n  void LoadCompressedMap(Register dst, Register object);\n\n  void LoadFeedbackVector(Register dst, Register closure, Register scratch,\n                          Label* fbv_undef);\n\n  inline void Fmov(VRegister fd, VRegister fn);\n  inline void Fmov(VRegister fd, Register rn);\n  // Provide explicit double and float interfaces for FP immediate moves, rather\n  // than relying on implicit C++ casts. This allows signalling NaNs to be\n  // preserved when the immediate matches the format of fd. Most systems convert\n  // signalling NaNs to quiet NaNs when converting between float and double.\n  inline void Fmov(VRegister fd, double imm);\n  inline void Fmov(VRegister fd, float imm);\n  // Provide a template to allow other types to be converted automatically.\n  template <typename T>\n  void Fmov(VRegister fd, T imm) {\n    DCHECK(allow_macro_instructions());\n    Fmov(fd, static_cast<double>(imm));\n  }\n  inline void Fmov(Register rd, VRegister fn);\n\n  void Movi(const VRegister& vd, uint64_t imm, Shift shift = LSL,\n            int shift_amount = 0);\n  void Movi(const VRegister& vd, uint64_t hi, uint64_t lo);\n\n  void LoadFromConstantsTable(Register destination, int constant_index) final;\n  void LoadRootRegisterOffset(Register destination, intptr_t offset) final;\n  void LoadRootRelative(Register destination, int32_t offset) final;\n  void StoreRootRelative(int32_t offset, Register value) final;\n\n  // Operand pointing to an external reference.\n  // May emit code to set up the scratch register. The operand is\n  // only guaranteed to be correct as long as the scratch register\n  // isn't changed.\n  // If the operand is used more than once, use a scratch register\n  // that is guaranteed not to be clobbered.\n  MemOperand ExternalReferenceAsOperand(ExternalReference reference,\n                                        Register scratch);\n\n  void Jump(Register target, Condition cond = al);\n  void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al);\n  void Jump(Handle<Code> code, RelocInfo::Mode rmode, Condition cond = al);\n  void Jump(const ExternalReference& reference);\n\n  void Call(Register target);\n  void Call(Address target, RelocInfo::Mode rmode);\n  void Call(Handle<Code> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);\n  void Call(ExternalReference target);\n\n  // Generate an indirect call (for when a direct call's range is not adequate).\n  void IndirectCall(Address target, RelocInfo::Mode rmode);\n\n  // Load the builtin given by the Smi in |builtin| into |target|.\n  void LoadEntryFromBuiltinIndex(Register builtin, Register target);\n  void LoadEntryFromBuiltin(Builtin builtin, Register destination);\n  MemOperand EntryFromBuiltinAsOperand(Builtin builtin);\n  void CallBuiltinByIndex(Register builtin, Register target);\n  void CallBuiltin(Builtin builtin);\n  void TailCallBuiltin(Builtin builtin, Condition cond = al);\n\n  // Load code entry point from the Code object.\n  void LoadCodeInstructionStart(Register destination, Register code_object,\n                                CodeEntrypointTag tag);\n  void CallCodeObject(Register code_object, CodeEntrypointTag tag);\n  void JumpCodeObject(Register code_object, CodeEntrypointTag tag,\n                      JumpMode jump_mode = JumpMode::kJump);\n\n  // Convenience functions to call/jmp to the code of a JSFunction object.\n  void CallJSFunction(Register function_object);\n  void JumpJSFunction(Register function_object,\n                      JumpMode jump_mode = JumpMode::kJump);\n\n  // Generates an instruction sequence s.t. the return address points to the\n  // instruction following the call.\n  // The return address on the stack is used by frame iteration.\n  void StoreReturnAddressAndCall(Register target);\n\n  void BailoutIfDeoptimized();\n  void CallForDeoptimization(Builtin target, int deopt_id, Label* exit,\n                             DeoptimizeKind kind, Label* ret,\n                             Label* jump_deoptimization_entry_label);\n\n  // Calls a C function and cleans up the space for arguments allocated\n  // by PrepareCallCFunction. The called function is not allowed to trigger a\n  // garbage collection, since that might move the code and invalidate the\n  // return address (unless this is somehow accounted for by the called\n  // function).\n  int CallCFunction(\n      ExternalReference function, int num_reg_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  int CallCFunction(\n      ExternalReference function, int num_reg_arguments,\n      int num_double_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  int CallCFunction(\n      Register function, int num_reg_arguments, int num_double_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n\n  // Performs a truncating conversion of a floating point number as used by\n  // the JS bitwise operations. See ECMA-262 9.5: ToInt32.\n  // Exits with 'result' holding the answer.\n  void TruncateDoubleToI(Isolate* isolate, Zone* zone, Register result,\n                         DoubleRegister double_input, StubCallMode stub_mode,\n                         LinkRegisterStatus lr_status);\n\n  inline void Mul(const Register& rd, const Register& rn, const Register& rm);\n\n  inline void Fcvtzs(const Register& rd, const VRegister& fn);\n  void Fcvtzs(const VRegister& vd, const VRegister& vn, int fbits = 0) {\n    DCHECK(allow_macro_instructions());\n    fcvtzs(vd, vn, fbits);\n  }\n\n  void Fjcvtzs(const Register& rd, const VRegister& vn) {\n    DCHECK(allow_macro_instructions());\n    DCHECK(!rd.IsZero());\n    fjcvtzs(rd, vn);\n  }\n\n  inline void Fcvtzu(const Register& rd, const VRegister& fn);\n  void Fcvtzu(const VRegister& vd, const VRegister& vn, int fbits = 0) {\n    DCHECK(allow_macro_instructions());\n    fcvtzu(vd, vn, fbits);\n  }\n\n  inline void Madd(const Register& rd, const Register& rn, const Register& rm,\n                   const Register& ra);\n  inline void Mneg(const Register& rd, const Register& rn, const Register& rm);\n  inline void Sdiv(const Register& rd, const Register& rn, const Register& rm);\n  inline void Udiv(const Register& rd, const Register& rn, const Register& rm);\n  inline void Msub(const Register& rd, const Register& rn, const Register& rm,\n                   const Register& ra);\n\n  inline void Lsl(const Register& rd, const Register& rn, unsigned shift);\n  inline void Lsl(const Register& rd, const Register& rn, const Register& rm);\n  inline void Umull(const Register& rd, const Register& rn, const Register& rm);\n  inline void Umulh(const Register& rd, const Register& rn, const Register& rm);\n  inline void Smull(const Register& rd, const Register& rn, const Register& rm);\n  inline void Smulh(const Register& rd, const Register& rn, const Register& rm);\n\n  inline void Sxtb(const Register& rd, const Register& rn);\n  inline void Sxth(const Register& rd, const Register& rn);\n  inline void Sxtw(const Register& rd, const Register& rn);\n  inline void Ubfiz(const Register& rd, const Register& rn, unsigned lsb,\n                    unsigned width);\n  inline void Sbfiz(const Register& rd, const Register& rn, unsigned lsb,\n                    unsigned width);\n  inline void Ubfx(const Register& rd, const Register& rn, unsigned lsb,\n                   unsigned width);\n  inline void Lsr(const Register& rd, const Register& rn, unsigned shift);\n  inline void Lsr(const Register& rd, const Register& rn, const Register& rm);\n  inline void Ror(const Register& rd, const Register& rs, unsigned shift);\n  inline void Ror(const Register& rd, const Register& rn, const Register& rm);\n  inline void Cmn(const Register& rn, const Operand& operand);\n  inline void Fadd(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Fcmp(const VRegister& fn, const VRegister& fm);\n  inline void Fcmp(const VRegister& fn, double value);\n  inline void Fabs(const VRegister& fd, const VRegister& fn);\n  inline void Fmul(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Fsub(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Fdiv(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Fmax(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Fmin(const VRegister& fd, const VRegister& fn,\n                   const VRegister& fm);\n  inline void Rbit(const Register& rd, const Register& rn);\n  inline void Rev(const Register& rd, const Register& rn);\n\n  enum AdrHint {\n    // The target must be within the immediate range of adr.\n    kAdrNear,\n    // The target may be outside of the immediate range of adr. Additional\n    // instructions may be emitted.\n    kAdrFar\n  };\n  void Adr(const Register& rd, Label* label, AdrHint = kAdrNear);\n\n  // Add/sub with carry macros.\n  inline void Adc(const Register& rd, const Register& rn,\n                  const Operand& operand);\n\n  // Conditional macros.\n  inline void Ccmp(const Register& rn, const Operand& operand, StatusFlags nzcv,\n                   Condition cond);\n  inline void CcmpTagged(const Register& rn, const Operand& operand,\n                         StatusFlags nzcv, Condition cond);\n  inline void Ccmn(const Register& rn, const Operand& operand, StatusFlags nzcv,\n                   Condition cond);\n\n  inline void Clz(const Register& rd, const Register& rn);\n\n  // Poke 'src' onto the stack. The offset is in bytes. The stack pointer must\n  // be 16 byte aligned.\n  // When the optional template argument is kSignLR and control flow integrity\n  // measures are enabled, we sign the link register before poking it onto the\n  // stack. 'src' must be lr in this case.\n  template <StoreLRMode lr_mode = kDontStoreLR>\n  void Poke(const CPURegister& src, const Operand& offset);\n\n  // Peek at a value on the stack, and put it in 'dst'. The offset is in bytes.\n  // The stack pointer must be aligned to 16 bytes.\n  // When the optional template argument is kAuthLR and control flow integrity\n  // measures are enabled, we authenticate the link register after peeking the\n  // value. 'dst' must be lr in this case.\n  template <LoadLRMode lr_mode = kDontLoadLR>\n  void Peek(const CPURegister& dst, const Operand& offset);\n\n  // Poke 'src1' and 'src2' onto the stack. The values written will be adjacent\n  // with 'src2' at a higher address than 'src1'. The offset is in bytes. The\n  // stack pointer must be 16 byte aligned.\n  void PokePair(const CPURegister& src1, const CPURegister& src2, int offset);\n\n  inline void Sbfx(const Register& rd, const Register& rn, unsigned lsb,\n                   unsigned width);\n\n  inline void Bfi(const Register& rd, const Register& rn, unsigned lsb,\n                  unsigned width);\n\n  inline void Scvtf(const VRegister& fd, const Register& rn,\n                    unsigned fbits = 0);\n  void Scvtf(const VRegister& vd, const VRegister& vn, int fbits = 0) {\n    DCHECK(allow_macro_instructions());\n    scvtf(vd, vn, fbits);\n  }\n  inline void Ucvtf(const VRegister& fd, const Register& rn,\n                    unsigned fbits = 0);\n  void Ucvtf(const VRegister& vd, const VRegister& vn, int fbits = 0) {\n    DCHECK(allow_macro_instructions());\n    ucvtf(vd, vn, fbits);\n  }\n\n  void AssertFPCRState(Register fpcr = NoReg) NOOP_UNLESS_DEBUG_CODE;\n  void CanonicalizeNaN(const VRegister& dst, const VRegister& src);\n  void CanonicalizeNaN(const VRegister& reg) { CanonicalizeNaN(reg, reg); }\n\n  inline void CmovX(const Register& rd, const Register& rn, Condition cond);\n  inline void Cset(const Register& rd, Condition cond);\n  inline void Csetm(const Register& rd, Condition cond);\n  inline void Fccmp(const VRegister& fn, const VRegister& fm, StatusFlags nzcv,\n                    Condition cond);\n  inline void Fccmp(const VRegister& fn, const double value, StatusFlags nzcv,\n                    Condition cond);\n  inline void Csinc(const Register& rd, const Register& rn, const Register& rm,\n                    Condition cond);\n\n  inline void Fcvt(const VRegister& fd, const VRegister& fn);\n\n  int ActivationFrameAlignment();\n\n  void Ins(const VRegister& vd, int vd_index, const VRegister& vn,\n           int vn_index) {\n    DCHECK(allow_macro_instructions());\n    ins(vd, vd_index, vn, vn_index);\n  }\n  void Ins(const VRegister& vd, int vd_index, const Register& rn) {\n    DCHECK(allow_macro_instructions());\n    ins(vd, vd_index, rn);\n  }\n\n  inline void Bl(Label* label);\n  inline void Br(const Register& xn);\n\n  inline void Uxtb(const Register& rd, const Register& rn);\n  inline void Uxth(const Register& rd, const Register& rn);\n  inline void Uxtw(const Register& rd, const Register& rn);\n\n  void Dup(const VRegister& vd, const VRegister& vn, int index) {\n    DCHECK(allow_macro_instructions());\n    dup(vd, vn, index);\n  }\n  void Dup(const VRegister& vd, const Register& rn) {\n    DCHECK(allow_macro_instructions());\n    dup(vd, rn);\n  }\n\n#define DECLARE_FUNCTION(FN, REGTYPE, REG, REG2, OP) \\\n  inline void FN(const REGTYPE REG, const REGTYPE REG2, const MemOperand& addr);\n  LSPAIR_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n  void St1(const VRegister& vt, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st1(vt, dst);\n  }\n  void St1(const VRegister& vt, const VRegister& vt2, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st1(vt, vt2, dst);\n  }\n  void St1(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st1(vt, vt2, vt3, dst);\n  }\n  void St1(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st1(vt, vt2, vt3, vt4, dst);\n  }\n  void St1(const VRegister& vt, int lane, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st1(vt, lane, dst);\n  }\n\n#define NEON_2VREG_SHIFT_MACRO_LIST(V) \\\n  V(rshrn, Rshrn)                      \\\n  V(rshrn2, Rshrn2)                    \\\n  V(shl, Shl)                          \\\n  V(shll, Shll)                        \\\n  V(shll2, Shll2)                      \\\n  V(shrn, Shrn)                        \\\n  V(shrn2, Shrn2)                      \\\n  V(sli, Sli)                          \\\n  V(sqrshrn, Sqrshrn)                  \\\n  V(sqrshrn2, Sqrshrn2)                \\\n  V(sqrshrun, Sqrshrun)                \\\n  V(sqrshrun2, Sqrshrun2)              \\\n  V(sqshl, Sqshl)                      \\\n  V(sqshlu, Sqshlu)                    \\\n  V(sqshrn, Sqshrn)                    \\\n  V(sqshrn2, Sqshrn2)                  \\\n  V(sqshrun, Sqshrun)                  \\\n  V(sqshrun2, Sqshrun2)                \\\n  V(sri, Sri)                          \\\n  V(srshr, Srshr)                      \\\n  V(srsra, Srsra)                      \\\n  V(sshll, Sshll)                      \\\n  V(sshll2, Sshll2)                    \\\n  V(sshr, Sshr)                        \\\n  V(ssra, Ssra)                        \\\n  V(uqrshrn, Uqrshrn)                  \\\n  V(uqrshrn2, Uqrshrn2)                \\\n  V(uqshl, Uqshl)                      \\\n  V(uqshrn, Uqshrn)                    \\\n  V(uqshrn2, Uqshrn2)                  \\\n  V(urshr, Urshr)                      \\\n  V(ursra, Ursra)                      \\\n  V(ushll, Ushll)                      \\\n  V(ushll2, Ushll2)                    \\\n  V(ushr, Ushr)                        \\\n  V(usra, Usra)\n\n#define DEFINE_MACRO_ASM_FUNC(ASM, MASM)                           \\\n  void MASM(const VRegister& vd, const VRegister& vn, int shift) { \\\n    DCHECK(allow_macro_instructions());                            \\\n    ASM(vd, vn, shift);                                            \\\n  }\n  NEON_2VREG_SHIFT_MACRO_LIST(DEFINE_MACRO_ASM_FUNC)\n#undef DEFINE_MACRO_ASM_FUNC\n\n  void Umov(const Register& rd, const VRegister& vn, int vn_index) {\n    DCHECK(allow_macro_instructions());\n    umov(rd, vn, vn_index);\n  }\n  void Tbl(const VRegister& vd, const VRegister& vn, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbl(vd, vn, vm);\n  }\n  void Tbl(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbl(vd, vn, vn2, vm);\n  }\n  void Tbl(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vn3, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbl(vd, vn, vn2, vn3, vm);\n  }\n  void Tbl(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vn3, const VRegister& vn4, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbl(vd, vn, vn2, vn3, vn4, vm);\n  }\n  void Ext(const VRegister& vd, const VRegister& vn, const VRegister& vm,\n           int index) {\n    DCHECK(allow_macro_instructions());\n    ext(vd, vn, vm, index);\n  }\n\n  void Smov(const Register& rd, const VRegister& vn, int vn_index) {\n    DCHECK(allow_macro_instructions());\n    smov(rd, vn, vn_index);\n  }\n\n// Load-acquire/store-release macros.\n#define DECLARE_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rt, const Register& rn);\n  LDA_STL_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n#define DECLARE_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rs, const Register& rt, const MemOperand& src);\n  CAS_SINGLE_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n#define DECLARE_FUNCTION(FN, OP)                                              \\\n  inline void FN(const Register& rs, const Register& rs2, const Register& rt, \\\n                 const Register& rt2, const MemOperand& src);\n  CAS_PAIR_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n#define DECLARE_LOAD_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rs, const Register& rt, const MemOperand& src);\n#define DECLARE_STORE_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rs, const MemOperand& src);\n\n  ATOMIC_MEMORY_SIMPLE_MACRO_LIST(ATOMIC_MEMORY_LOAD_MACRO_MODES,\n                                  DECLARE_LOAD_FUNCTION, Ld, ld)\n  ATOMIC_MEMORY_SIMPLE_MACRO_LIST(ATOMIC_MEMORY_STORE_MACRO_MODES,\n                                  DECLARE_STORE_FUNCTION, St, st)\n\n#define DECLARE_SWP_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rs, const Register& rt, const MemOperand& src);\n\n  ATOMIC_MEMORY_LOAD_MACRO_MODES(DECLARE_SWP_FUNCTION, Swp, swp)\n\n#undef DECLARE_LOAD_FUNCTION\n#undef DECLARE_STORE_FUNCTION\n#undef DECLARE_SWP_FUNCTION\n\n  // Load an object from the root table.\n  void LoadRoot(Register destination, RootIndex index) final;\n  void LoadTaggedRoot(Register destination, RootIndex index);\n  void PushRoot(RootIndex index);\n\n  inline void Ret(const Register& xn = lr);\n\n  // Perform a conversion from a double to a signed int64. If the input fits in\n  // range of the 64-bit result, execution branches to done. Otherwise,\n  // execution falls through, and the sign of the result can be used to\n  // determine if overflow was towards positive or negative infinity.\n  //\n  // On successful conversion, the least significant 32 bits of the result are\n  // equivalent to the ECMA-262 operation \"ToInt32\".\n  void TryConvertDoubleToInt64(Register result, DoubleRegister input,\n                               Label* done);\n\n  inline void Mrs(const Register& rt, SystemRegister sysreg);\n  inline void Msr(SystemRegister sysreg, const Register& rt);\n\n  // Prologue claims an extra slot due to arm64's alignement constraints.\n  static constexpr int kExtraSlotClaimedByPrologue = 1;\n  // Generates function prologue code.\n  void Prologue();\n\n  void Cmgt(const VRegister& vd, const VRegister& vn, int imm) {\n    DCHECK(allow_macro_instructions());\n    cmgt(vd, vn, imm);\n  }\n  void Cmge(const VRegister& vd, const VRegister& vn, int imm) {\n    DCHECK(allow_macro_instructions());\n    cmge(vd, vn, imm);\n  }\n  void Cmeq(const VRegister& vd, const VRegister& vn, int imm) {\n    DCHECK(allow_macro_instructions());\n    cmeq(vd, vn, imm);\n  }\n  void Cmlt(const VRegister& vd, const VRegister& vn, int imm) {\n    DCHECK(allow_macro_instructions());\n    cmlt(vd, vn, imm);\n  }\n  void Cmle(const VRegister& vd, const VRegister& vn, int imm) {\n    DCHECK(allow_macro_instructions());\n    cmle(vd, vn, imm);\n  }\n\n  inline void Neg(const Register& rd, const Operand& operand);\n  inline void Negs(const Register& rd, const Operand& operand);\n\n  // Compute rd = abs(rm).\n  // This function clobbers the condition flags. On output the overflow flag is\n  // set iff the negation overflowed.\n  //\n  // If rm is the minimum representable value, the result is not representable.\n  // Handlers for each case can be specified using the relevant labels.\n  void Abs(const Register& rd, const Register& rm,\n           Label* is_not_representable = nullptr,\n           Label* is_representable = nullptr);\n\n  inline void Cls(const Register& rd, const Register& rn);\n  inline void Cneg(const Register& rd, const Register& rn, Condition cond);\n  inline void Rev16(const Register& rd, const Register& rn);\n  inline void Rev32(const Register& rd, const Register& rn);\n  inline void Fcvtns(const Register& rd, const VRegister& fn);\n  inline void Fcvtnu(const Register& rd, const VRegister& fn);\n  inline void Fcvtms(const Register& rd, const VRegister& fn);\n  inline void Fcvtmu(const Register& rd, const VRegister& fn);\n  inline void Fcvtas(const Register& rd, const VRegister& fn);\n  inline void Fcvtau(const Register& rd, const VRegister& fn);\n\n  // Compute the start of the generated instruction stream from the current PC.\n  // This is an alternative to embedding the {CodeObject} handle as a reference.\n  void ComputeCodeStartAddress(const Register& rd);\n\n  // ---------------------------------------------------------------------------\n  // Pointer compression Support\n\n  // Loads a field containing any tagged value and decompresses it if necessary.\n  void LoadTaggedField(const Register& destination,\n                       const MemOperand& field_operand);\n\n  // Loads a field containing any tagged value but never decompresses it.\n  void LoadTaggedFieldWithoutDecompressing(const Register& destination,\n                                           const MemOperand& field_operand);\n\n  // Loads a field containing a tagged signed value and decompresses it if\n  // necessary.\n  void LoadTaggedSignedField(const Register& destination,\n                             const MemOperand& field_operand);\n\n  // Loads a field containing smi value and untags it.\n  void SmiUntagField(Register dst, const MemOperand& src);\n\n  // Compresses and stores tagged value to given on-heap location.\n  void StoreTaggedField(const Register& value,\n                        const MemOperand& dst_field_operand);\n  void StoreTwoTaggedFields(const Register& value,\n                            const MemOperand& dst_field_operand);\n\n  // For compatibility with platform-independent code.\n  void StoreTaggedField(const MemOperand& dst_field_operand,\n                        const Register& value) {\n    StoreTaggedField(value, dst_field_operand);\n  }\n\n  void AtomicStoreTaggedField(const Register& value, const Register& dst_base,\n                              const Register& dst_index, const Register& temp);\n\n  void DecompressTaggedSigned(const Register& destination,\n                              const MemOperand& field_operand);\n  void DecompressTagged(const Register& destination,\n                        const MemOperand& field_operand);\n  void DecompressTagged(const Register& destination, const Register& source);\n  void DecompressTagged(const Register& destination, Tagged_t immediate);\n  void DecompressProtected(const Register& destination,\n                           const MemOperand& field_operand);\n\n  void AtomicDecompressTaggedSigned(const Register& destination,\n                                    const Register& base, const Register& index,\n                                    const Register& temp);\n  void AtomicDecompressTagged(const Register& destination, const Register& base,\n                              const Register& index, const Register& temp);\n\n  // Restore FP and LR from the values stored in the current frame. This will\n  // authenticate the LR when pointer authentication is enabled.\n  void RestoreFPAndLR();\n\n#if V8_ENABLE_WEBASSEMBLY\n  void StoreReturnAddressInWasmExitFrame(Label* return_location);\n#endif  // V8_ENABLE_WEBASSEMBLY\n\n  // Wasm helpers. These instructions don't have direct lowering\n  // to native instructions. These helpers allow us to define the optimal code\n  // sequence, and be used in both TurboFan and Liftoff.\n  void PopcntHelper(Register dst, Register src);\n  void I8x16BitMask(Register dst, VRegister src, VRegister temp = NoVReg);\n  void I16x8BitMask(Register dst, VRegister src);\n  void I32x4BitMask(Register dst, VRegister src);\n  void I64x2BitMask(Register dst, VRegister src);\n  void I64x2AllTrue(Register dst, VRegister src);\n\n  // ---------------------------------------------------------------------------\n  // V8 Sandbox support\n\n  // Transform a SandboxedPointer from/to its encoded form, which is used when\n  // the pointer is stored on the heap and ensures that the pointer will always\n  // point into the sandbox.\n  void DecodeSandboxedPointer(Register value);\n  void LoadSandboxedPointerField(Register destination,\n                                 MemOperand field_operand);\n  void StoreSandboxedPointerField(Register value, MemOperand dst_field_operand);\n\n  // Loads a field containing an off-heap (\"external\") pointer and does\n  // necessary decoding if the sandbox is enabled.\n  void LoadExternalPointerField(Register destination, MemOperand field_operand,\n                                ExternalPointerTag tag,\n                                Register isolate_root = Register::no_reg());\n\n  // Load a trusted pointer field.\n  // When the sandbox is enabled, these are indirect pointers using the trusted\n  // pointer table. Otherwise they are regular tagged fields.\n  void LoadTrustedPointerField(Register destination, MemOperand field_operand,\n                               IndirectPointerTag tag);\n  // Store a trusted pointer field.\n  void StoreTrustedPointerField(Register value, MemOperand dst_field_operand);\n\n  // Load a code pointer field.\n  // These are special versions of trusted pointers that, when the sandbox is\n  // enabled, reference code objects through the code pointer table.\n  void LoadCodePointerField(Register destination, MemOperand field_operand) {\n    LoadTrustedPointerField(destination, field_operand,\n                            kCodeIndirectPointerTag);\n  }\n  // Store a code pointer field.\n  void StoreCodePointerField(Register value, MemOperand dst_field_operand) {\n    StoreTrustedPointerField(value, dst_field_operand);\n  }\n\n  // Load an indirect pointer field.\n  // Only available when the sandbox is enabled, but always visible to avoid\n  // having to place the #ifdefs into the caller.\n  void LoadIndirectPointerField(Register destination, MemOperand field_operand,\n                                IndirectPointerTag tag);\n\n  // Store an indirect pointer field.\n  // Only available when the sandbox is enabled, but always visible to avoid\n  // having to place the #ifdefs into the caller.\n  void StoreIndirectPointerField(Register value, MemOperand dst_field_operand);\n\n#ifdef V8_ENABLE_SANDBOX\n  // Retrieve the heap object referenced by the given indirect pointer handle,\n  // which can either be a trusted pointer handle or a code pointer handle.\n  void ResolveIndirectPointerHandle(Register destination, Register handle,\n                                    IndirectPointerTag tag);\n\n  // Retrieve the heap object referenced by the given trusted pointer handle.\n  void ResolveTrustedPointerHandle(Register destination, Register handle,\n                                   IndirectPointerTag tag);\n\n  // Retrieve the Code object referenced by the given code pointer handle.\n  void ResolveCodePointerHandle(Register destination, Register handle);\n\n  // Load the pointer to a Code's entrypoint via a code pointer.\n  // Only available when the sandbox is enabled as it requires the code pointer\n  // table.\n  void LoadCodeEntrypointViaCodePointer(Register destination,\n                                        MemOperand field_operand,\n                                        CodeEntrypointTag tag);\n#endif\n\n  // Load a protected pointer field.\n  void LoadProtectedPointerField(Register destination,\n                                 MemOperand field_operand);\n\n  // Instruction set functions ------------------------------------------------\n  // Logical macros.\n  inline void Bics(const Register& rd, const Register& rn,\n                   const Operand& operand);\n\n  inline void Adcs(const Register& rd, const Register& rn,\n                   const Operand& operand);\n  inline void Sbc(const Register& rd, const Register& rn,\n                  const Operand& operand);\n  inline void Sbcs(const Register& rd, const Register& rn,\n                   const Operand& operand);\n  inline void Ngc(const Register& rd, const Operand& operand);\n  inline void Ngcs(const Register& rd, const Operand& operand);\n\n#define DECLARE_FUNCTION(FN, OP) \\\n  inline void FN(const Register& rs, const Register& rt, const Register& rn);\n  STLX_MACRO_LIST(DECLARE_FUNCTION)\n#undef DECLARE_FUNCTION\n\n  // Branch type inversion relies on these relations.\n  static_assert((reg_zero == (reg_not_zero ^ 1)) &&\n                (reg_bit_clear == (reg_bit_set ^ 1)) &&\n                (always == (never ^ 1)));\n\n  inline void Bfxil(const Register& rd, const Register& rn, unsigned lsb,\n                    unsigned width);\n  inline void Cinc(const Register& rd, const Register& rn, Condition cond);\n  inline void Cinv(const Register& rd, const Register& rn, Condition cond);\n  inline void CzeroX(const Register& rd, Condition cond);\n  inline void Csinv(const Register& rd, const Register& rn, const Register& rm,\n                    Condition cond);\n  inline void Csneg(const Register& rd, const Register& rn, const Register& rm,\n                    Condition cond);\n  inline void Extr(const Register& rd, const Register& rn, const Register& rm,\n                   unsigned lsb);\n  void Fcvtl2(const VRegister& vd, const VRegister& vn) {\n    DCHECK(allow_macro_instructions());\n    fcvtl2(vd, vn);\n  }\n  void Fcvtn2(const VRegister& vd, const VRegister& vn) {\n    DCHECK(allow_macro_instructions());\n    fcvtn2(vd, vn);\n  }\n  void Fcvtxn(const VRegister& vd, const VRegister& vn) {\n    DCHECK(allow_macro_instructions());\n    fcvtxn(vd, vn);\n  }\n  void Fcvtxn2(const VRegister& vd, const VRegister& vn) {\n    DCHECK(allow_macro_instructions());\n    fcvtxn2(vd, vn);\n  }\n  inline void Fmadd(const VRegister& fd, const VRegister& fn,\n                    const VRegister& fm, const VRegister& fa);\n  inline void Fmaxnm(const VRegister& fd, const VRegister& fn,\n                     const VRegister& fm);\n  inline void Fminnm(const VRegister& fd, const VRegister& fn,\n                     const VRegister& fm);\n  inline void Fmsub(const VRegister& fd, const VRegister& fn,\n                    const VRegister& fm, const VRegister& fa);\n  inline void Fnmadd(const VRegister& fd, const VRegister& fn,\n                     const VRegister& fm, const VRegister& fa);\n  inline void Fnmsub(const VRegister& fd, const VRegister& fn,\n                     const VRegister& fm, const VRegister& fa);\n  inline void Hint(SystemHint code);\n  inline void Hlt(int code);\n  inline void Ldnp(const CPURegister& rt, const CPURegister& rt2,\n                   const MemOperand& src);\n  inline void Movk(const Register& rd, uint64_t imm, int shift = -1);\n  inline void Nop() { nop(); }\n  void Mvni(const VRegister& vd, const int imm8, Shift shift = LSL,\n            const int shift_amount = 0) {\n    DCHECK(allow_macro_instructions());\n    mvni(vd, imm8, shift, shift_amount);\n  }\n  inline void Smaddl(const Register& rd, const Register& rn, const Register& rm,\n                     const Register& ra);\n  inline void Smsubl(const Register& rd, const Register& rn, const Register& rm,\n                     const Register& ra);\n  inline void Stnp(const CPURegister& rt, const CPURegister& rt2,\n                   const MemOperand& dst);\n  inline void Umaddl(const Register& rd, const Register& rn, const Register& rm,\n                     const Register& ra);\n  inline void Umsubl(const Register& rd, const Register& rn, const Register& rm,\n                     const Register& ra);\n\n  void Ld1(const VRegister& vt, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1(vt, src);\n  }\n  void Ld1(const VRegister& vt, const VRegister& vt2, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1(vt, vt2, src);\n  }\n  void Ld1(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1(vt, vt2, vt3, src);\n  }\n  void Ld1(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1(vt, vt2, vt3, vt4, src);\n  }\n  void Ld1(const VRegister& vt, int lane, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1(vt, lane, src);\n  }\n  void Ld1r(const VRegister& vt, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld1r(vt, src);\n  }\n  void Ld2(const VRegister& vt, const VRegister& vt2, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld2(vt, vt2, src);\n  }\n  void Ld2(const VRegister& vt, const VRegister& vt2, int lane,\n           const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld2(vt, vt2, lane, src);\n  }\n  void Ld2r(const VRegister& vt, const VRegister& vt2, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld2r(vt, vt2, src);\n  }\n  void Ld3(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld3(vt, vt2, vt3, src);\n  }\n  void Ld3(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           int lane, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld3(vt, vt2, vt3, lane, src);\n  }\n  void Ld3r(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n            const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld3r(vt, vt2, vt3, src);\n  }\n  void Ld4(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld4(vt, vt2, vt3, vt4, src);\n  }\n  void Ld4(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, int lane, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld4(vt, vt2, vt3, vt4, lane, src);\n  }\n  void Ld4r(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n            const VRegister& vt4, const MemOperand& src) {\n    DCHECK(allow_macro_instructions());\n    ld4r(vt, vt2, vt3, vt4, src);\n  }\n  void St2(const VRegister& vt, const VRegister& vt2, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st2(vt, vt2, dst);\n  }\n  void St3(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st3(vt, vt2, vt3, dst);\n  }\n  void St4(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st4(vt, vt2, vt3, vt4, dst);\n  }\n  void St2(const VRegister& vt, const VRegister& vt2, int lane,\n           const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st2(vt, vt2, lane, dst);\n  }\n  void St3(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           int lane, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st3(vt, vt2, vt3, lane, dst);\n  }\n  void St4(const VRegister& vt, const VRegister& vt2, const VRegister& vt3,\n           const VRegister& vt4, int lane, const MemOperand& dst) {\n    DCHECK(allow_macro_instructions());\n    st4(vt, vt2, vt3, vt4, lane, dst);\n  }\n  void Tbx(const VRegister& vd, const VRegister& vn, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbx(vd, vn, vm);\n  }\n  void Tbx(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbx(vd, vn, vn2, vm);\n  }\n  void Tbx(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vn3, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbx(vd, vn, vn2, vn3, vm);\n  }\n  void Tbx(const VRegister& vd, const VRegister& vn, const VRegister& vn2,\n           const VRegister& vn3, const VRegister& vn4, const VRegister& vm) {\n    DCHECK(allow_macro_instructions());\n    tbx(vd, vn, vn2, vn3, vn4, vm);\n  }\n\n  inline void PushSizeRegList(RegList registers, unsigned reg_size) {\n    PushCPURegList(CPURegList(reg_size, registers));\n  }\n  inline void PushSizeRegList(DoubleRegList registers, unsigned reg_size) {\n    PushCPURegList(CPURegList(reg_size, registers));\n  }\n  inline void PopSizeRegList(RegList registers, unsigned reg_size) {\n    PopCPURegList(CPURegList(reg_size, registers));\n  }\n  inline void PopSizeRegList(DoubleRegList registers, unsigned reg_size) {\n    PopCPURegList(CPURegList(reg_size, registers));\n  }\n  inline void PushXRegList(RegList regs) {\n    PushSizeRegList(regs, kXRegSizeInBits);\n  }\n  inline void PopXRegList(RegList regs) {\n    PopSizeRegList(regs, kXRegSizeInBits);\n  }\n  inline void PushWRegList(RegList regs) {\n    PushSizeRegList(regs, kWRegSizeInBits);\n  }\n  inline void PopWRegList(RegList regs) {\n    PopSizeRegList(regs, kWRegSizeInBits);\n  }\n  inline void PushQRegList(DoubleRegList regs) {\n    PushSizeRegList(regs, kQRegSizeInBits);\n  }\n  inline void PopQRegList(DoubleRegList regs) {\n    PopSizeRegList(regs, kQRegSizeInBits);\n  }\n  inline void PushDRegList(DoubleRegList regs) {\n    PushSizeRegList(regs, kDRegSizeInBits);\n  }\n  inline void PopDRegList(DoubleRegList regs) {\n    PopSizeRegList(regs, kDRegSizeInBits);\n  }\n  inline void PushSRegList(DoubleRegList regs) {\n    PushSizeRegList(regs, kSRegSizeInBits);\n  }\n  inline void PopSRegList(DoubleRegList regs) {\n    PopSizeRegList(regs, kSRegSizeInBits);\n  }\n\n  // These PushAll/PopAll respect the order of the registers in the stack from\n  // low index to high.\n  void PushAll(RegList registers);\n  void PopAll(RegList registers);\n\n  inline void PushAll(DoubleRegList registers,\n                      int stack_slot_size = kDoubleSize) {\n    if (registers.Count() % 2 != 0) {\n      DCHECK(!registers.has(fp_zero));\n      registers.set(fp_zero);\n    }\n    PushDRegList(registers);\n  }\n  inline void PopAll(DoubleRegList registers,\n                     int stack_slot_size = kDoubleSize) {\n    if (registers.Count() % 2 != 0) {\n      DCHECK(!registers.has(fp_zero));\n      registers.set(fp_zero);\n    }\n    PopDRegList(registers);\n  }\n\n  // Push the specified register 'count' times.\n  void PushMultipleTimes(CPURegister src, Register count);\n\n  // Peek at two values on the stack, and put them in 'dst1' and 'dst2'. The\n  // values peeked will be adjacent, with the value in 'dst2' being from a\n  // higher address than 'dst1'. The offset is in bytes. The stack pointer must\n  // be aligned to 16 bytes.\n  void PeekPair(const CPURegister& dst1, const CPURegister& dst2, int offset);\n\n  // Preserve the callee-saved registers (as defined by AAPCS64).\n  //\n  // Higher-numbered registers are pushed before lower-numbered registers, and\n  // thus get higher addresses.\n  // Floating-point registers are pushed before general-purpose registers, and\n  // thus get higher addresses.\n  //\n  // When control flow integrity measures are enabled, this method signs the\n  // link register before pushing it.\n  //\n  // Note that registers are not checked for invalid values. Use this method\n  // only if you know that the GC won't try to examine the values on the stack.\n  void PushCalleeSavedRegisters();\n\n  // Restore the callee-saved registers (as defined by AAPCS64).\n  //\n  // Higher-numbered registers are popped after lower-numbered registers, and\n  // thus come from higher addresses.\n  // Floating-point registers are popped after general-purpose registers, and\n  // thus come from higher addresses.\n  //\n  // When control flow integrity measures are enabled, this method\n  // authenticates the link register after popping it.\n  void PopCalleeSavedRegisters();\n\n  // Tiering support.\n  void AssertFeedbackCell(Register object,\n                          Register scratch) NOOP_UNLESS_DEBUG_CODE;\n  inline void AssertFeedbackVector(Register object);\n  void AssertFeedbackVector(Register object,\n                            Register scratch) NOOP_UNLESS_DEBUG_CODE;\n  void ReplaceClosureCodeWithOptimizedCode(Register optimized_code,\n                                           Register closure);\n  void GenerateTailCallToReturnedCode(Runtime::FunctionId function_id);\n  Condition LoadFeedbackVectorFlagsAndCheckIfNeedsProcessing(\n      Register flags, Register feedback_vector, CodeKind current_code_kind);\n  void LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      Register flags, Register feedback_vector, CodeKind current_code_kind,\n      Label* flags_need_processing);\n  void OptimizeCodeOrTailCallOptimizedCodeSlot(Register flags,\n                                               Register feedback_vector);\n\n  // Helpers ------------------------------------------------------------------\n\n  template <typename Field>\n  void DecodeField(Register dst, Register src) {\n    static const int shift = Field::kShift;\n    static const int setbits = CountSetBits(Field::kMask, 32);\n    Ubfx(dst, src, shift, setbits);\n  }\n\n  template <typename Field>\n  void DecodeField(Register reg) {\n    DecodeField<Field>(reg, reg);\n  }\n\n  void JumpIfCodeIsMarkedForDeoptimization(Register code, Register scratch,\n                                           Label* if_marked_for_deoptimization);\n  void JumpIfCodeIsTurbofanned(Register code, Register scratch,\n                               Label* if_marked_for_deoptimization);\n  Operand ClearedValue() const;\n\n  Operand ReceiverOperand();\n\n  // ---- SMI and Number Utilities ----\n\n  inline void JumpIfNotSmi(Register value, Label* not_smi_label);\n\n  // Abort execution if argument is not a Map, enabled via\n  // --debug-code.\n  void AssertMap(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a Code, enabled via\n  // --debug-code.\n  void AssertCode(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a Constructor, enabled via\n  // --debug-code.\n  void AssertConstructor(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a JSFunction, enabled via\n  // --debug-code.\n  void AssertFunction(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a callable JSFunction, enabled via\n  // --debug-code.\n  void AssertCallableFunction(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a JSGeneratorObject (or subclass),\n  // enabled via --debug-code.\n  void AssertGeneratorObject(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not a JSBoundFunction,\n  // enabled via --debug-code.\n  void AssertBoundFunction(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not undefined or an AllocationSite,\n  // enabled via --debug-code.\n  void AssertUndefinedOrAllocationSite(Register object) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not smi nor in the pointer compresssion\n  // cage, enabled via --debug-code.\n  void AssertSmiOrHeapObjectInMainCompressionCage(Register object)\n      NOOP_UNLESS_DEBUG_CODE;\n\n  // ---- Calling / Jumping helpers ----\n\n  void CallRuntime(const Runtime::Function* f, int num_arguments);\n\n  // Convenience function: Same as above, but takes the fid instead.\n  void CallRuntime(Runtime::FunctionId fid, int num_arguments) {\n    CallRuntime(Runtime::FunctionForId(fid), num_arguments);\n  }\n\n  // Convenience function: Same as above, but takes the fid instead.\n  void CallRuntime(Runtime::FunctionId fid) {\n    const Runtime::Function* function = Runtime::FunctionForId(fid);\n    CallRuntime(function, function->nargs);\n  }\n\n  void TailCallRuntime(Runtime::FunctionId fid);\n\n  // Jump to a runtime routine.\n  void JumpToExternalReference(const ExternalReference& builtin,\n                               bool builtin_exit_frame = false);\n\n  // Registers used through the invocation chain are hard-coded.\n  // We force passing the parameters to ensure the contracts are correctly\n  // honoured by the caller.\n  // 'function' must be x1.\n  // 'actual' must use an immediate or x0.\n  // 'expected' must use an immediate or x2.\n  // 'call_kind' must be x5.\n  void InvokePrologue(Register expected_parameter_count,\n                      Register actual_parameter_count, Label* done,\n                      InvokeType type);\n\n  // On function call, call into the debugger.\n  void CallDebugOnFunctionCall(Register fun, Register new_target,\n                               Register expected_parameter_count,\n                               Register actual_parameter_count);\n  void InvokeFunctionCode(Register function, Register new_target,\n                          Register expected_parameter_count,\n                          Register actual_parameter_count, InvokeType type);\n  // Invoke the JavaScript function in the given register.\n  // Changes the current context to the context in the function before invoking.\n  void InvokeFunctionWithNewTarget(Register function, Register new_target,\n                                   Register actual_parameter_count,\n                                   InvokeType type);\n  void InvokeFunction(Register function, Register expected_parameter_count,\n                      Register actual_parameter_count, InvokeType type);\n\n  // ---- InstructionStream generation helpers ----\n\n  // ---------------------------------------------------------------------------\n  // Support functions.\n\n  // Compare object type for heap object.  heap_object contains a non-Smi\n  // whose object type should be compared with the given type.  This both\n  // sets the flags and leaves the object type in the type_reg register.\n  // It leaves the map in the map register (unless the type_reg and map register\n  // are the same register).  It leaves the heap object in the heap_object\n  // register unless the heap_object register is the same register as one of the\n  // other registers.\n  void CompareObjectType(Register heap_object, Register map, Register type_reg,\n                         InstanceType type);\n  // Variant of the above, which only guarantees to set the correct eq/ne flag.\n  // Neither map, nor type_reg might be set to any particular value.\n  void IsObjectType(Register heap_object, Register scratch1, Register scratch2,\n                    InstanceType type);\n#if V8_STATIC_ROOTS_BOOL\n  // Fast variant which is guaranteed to not actually load the instance type\n  // from the map.\n  void IsObjectTypeFast(Register heap_object, Register compressed_map_scratch,\n                        InstanceType type);\n  void CompareInstanceTypeWithUniqueCompressedMap(Register map,\n                                                  Register scratch,\n                                                  InstanceType type);\n#endif  // V8_STATIC_ROOTS_BOOL\n\n  // Compare object type for heap object, and branch if equal (or not.)\n  // heap_object contains a non-Smi whose object type should be compared with\n  // the given type.  This both sets the flags and leaves the object type in\n  // the type_reg register. It leaves the map in the map register (unless the\n  // type_reg and map register are the same register).  It leaves the heap\n  // object in the heap_object register unless the heap_object register is the\n  // same register as one of the other registers.\n  void JumpIfObjectType(Register object, Register map, Register type_reg,\n                        InstanceType type, Label* if_cond_pass,\n                        Condition cond = eq);\n\n  // Fast check if the object is a js receiver type. Assumes only primitive\n  // objects or js receivers are passed.\n  void JumpIfJSAnyIsNotPrimitive(\n      Register heap_object, Register scratch, Label* target,\n      Label::Distance distance = Label::kFar,\n      Condition condition = Condition::kUnsignedGreaterThanEqual);\n  void JumpIfJSAnyIsPrimitive(Register heap_object, Register scratch,\n                              Label* target,\n                              Label::Distance distance = Label::kFar) {\n    return JumpIfJSAnyIsNotPrimitive(heap_object, scratch, target, distance,\n                                     Condition::kUnsignedLessThan);\n  }\n\n  // Compare instance type in a map.  map contains a valid map object whose\n  // object type should be compared with the given type.  This both\n  // sets the flags and leaves the object type in the type_reg register.\n  void CompareInstanceType(Register map, Register type_reg, InstanceType type);\n\n  // Compare instance type ranges for a map (lower_limit and higher_limit\n  // inclusive).\n  //\n  // Always use unsigned comparisons: ls for a positive result.\n  void CompareInstanceTypeRange(Register map, Register type_reg,\n                                InstanceType lower_limit,\n                                InstanceType higher_limit);\n\n  // Load the elements kind field from a map, and return it in the result\n  // register.\n  void LoadElementsKindFromMap(Register result, Register map);\n\n  // Compare the object in a register to a value from the root list.\n  void CompareRoot(const Register& obj, RootIndex index,\n                   ComparisonMode mode = ComparisonMode::kDefault);\n  void CompareTaggedRoot(const Register& with, RootIndex index);\n\n  // Compare the object in a register to a value and jump if they are equal.\n  void JumpIfRoot(const Register& obj, RootIndex index, Label* if_equal);\n\n  // Compare the object in a register to a value and jump if they are not equal.\n  void JumpIfNotRoot(const Register& obj, RootIndex index, Label* if_not_equal);\n\n  // Checks if value is in range [lower_limit, higher_limit] using a single\n  // comparison.\n  void JumpIfIsInRange(const Register& value, unsigned lower_limit,\n                       unsigned higher_limit, Label* on_in_range);\n\n  // ---------------------------------------------------------------------------\n  // Frames.\n\n  // Enter exit frame. Exit frames are used when calling C code from generated\n  // (JavaScript) code.\n  //\n  // The only registers modified by this function are the provided scratch\n  // register, the frame pointer and the stack pointer.\n  //\n  // The 'extra_space' argument can be used to allocate some space in the exit\n  // frame that will be ignored by the GC. This space will be reserved in the\n  // bottom of the frame immediately above the return address slot.\n  //\n  // Set up a stack frame and registers as follows:\n  //         fp[8]: CallerPC (lr)\n  //   fp -> fp[0]: CallerFP (old fp)\n  //         fp[-8]: SPOffset (new sp)\n  //         fp[-16]: CodeObject()\n  //         fp[-16 - fp-size]: Saved doubles, if saved_doubles is true.\n  //         sp[8]: Memory reserved for the caller if extra_space != 0.\n  //                 Alignment padding, if necessary.\n  //   sp -> sp[0]: Space reserved for the return address.\n  //\n  // This function also stores the new frame information in the top frame, so\n  // that the new frame becomes the current frame.\n  void EnterExitFrame(const Register& scratch, int extra_space,\n                      StackFrame::Type frame_type);\n\n  // Leave the current exit frame, after a C function has returned to generated\n  // (JavaScript) code.\n  //\n  // This effectively unwinds the operation of EnterExitFrame:\n  //  * The frame information is removed from the top frame.\n  //  * The exit frame is dropped.\n  void LeaveExitFrame(const Register& scratch, const Register& scratch2);\n\n  // Load the global proxy from the current context.\n  void LoadGlobalProxy(Register dst);\n\n  // ---------------------------------------------------------------------------\n  // In-place weak references.\n  void LoadWeakValue(Register out, Register in, Label* target_if_cleared);\n\n  // ---------------------------------------------------------------------------\n  // StatsCounter support\n\n  void IncrementCounter(StatsCounter* counter, int value, Register scratch1,\n                        Register scratch2) {\n    if (!v8_flags.native_code_counters) return;\n    EmitIncrementCounter(counter, value, scratch1, scratch2);\n  }\n  void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,\n                            Register scratch2);\n  void DecrementCounter(StatsCounter* counter, int value, Register scratch1,\n                        Register scratch2) {\n    if (!v8_flags.native_code_counters) return;\n    EmitIncrementCounter(counter, -value, scratch1, scratch2);\n  }\n\n  // ---------------------------------------------------------------------------\n  // Stack limit utilities\n  void LoadStackLimit(Register destination, StackLimitKind kind);\n  void StackOverflowCheck(Register num_args, Label* stack_overflow);\n\n  // ---------------------------------------------------------------------------\n  // Garbage collector support (GC).\n\n  // Notify the garbage collector that we wrote a pointer into an object.\n  // |object| is the object being stored into, |value| is the object being\n  // stored.\n  // The offset is the offset from the start of the object, not the offset from\n  // the tagged HeapObject pointer.  For use with FieldMemOperand(reg, off).\n  void RecordWriteField(\n      Register object, int offset, Register value, LinkRegisterStatus lr_status,\n      SaveFPRegsMode save_fp, SmiCheck smi_check = SmiCheck::kInline,\n      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());\n\n  // For a given |object| notify the garbage collector that the slot at |offset|\n  // has been written. |value| is the object being stored.\n  void RecordWrite(\n      Register object, Operand offset, Register value,\n      LinkRegisterStatus lr_status, SaveFPRegsMode save_fp,\n      SmiCheck smi_check = SmiCheck::kInline,\n      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());\n\n  // ---------------------------------------------------------------------------\n  // Debugging.\n\n  void LoadNativeContextSlot(Register dst, int index);\n\n  // Falls through and sets scratch_and_result to 0 on failure, jumps to\n  // on_result on success.\n  void TryLoadOptimizedOsrCode(Register scratch_and_result,\n                               CodeKind min_opt_level, Register feedback_vector,\n                               FeedbackSlot slot, Label* on_result,\n                               Label::Distance distance);\n\n protected:\n  // The actual Push and Pop implementations. These don't generate any code\n  // other than that required for the push or pop. This allows\n  // (Push|Pop)CPURegList to bundle together run-time assertions for a large\n  // block of registers.\n  //\n  // Note that size is per register, and is specified in bytes.\n  void PushHelper(int count, int size, const CPURegister& src0,\n                  const CPURegister& src1, const CPURegister& src2,\n                  const CPURegister& src3);\n  void PopHelper(int count, int size, const CPURegister& dst0,\n                 const CPURegister& dst1, const CPURegister& dst2,\n                 const CPURegister& dst3);\n\n  void ConditionalCompareMacro(const Register& rn, const Operand& operand,\n                               StatusFlags nzcv, Condition cond,\n                               ConditionalCompareOp op);\n\n  void AddSubWithCarryMacro(const Register& rd, const Register& rn,\n                            const Operand& operand, FlagsUpdate S,\n                            AddSubWithCarryOp op);\n\n  // Call Printf. On a native build, a simple call will be generated, but if the\n  // simulator is being used then a suitable pseudo-instruction is used. The\n  // arguments and stack must be prepared by the caller as for a normal AAPCS64\n  // call to 'printf'.\n  //\n  // The 'args' argument should point to an array of variable arguments in their\n  // proper PCS registers (and in calling order). The argument registers can\n  // have mixed types. The format string (x0) should not be included.\n  void CallPrintf(int arg_count = 0, const CPURegister* args = nullptr);\n\n private:\n#if DEBUG\n  // Tell whether any of the macro instruction can be used. When false the\n  // MacroAssembler will assert if a method which can emit a variable number\n  // of instructions is called.\n  bool allow_macro_instructions_ = true;\n#endif\n\n  // Scratch registers available for use by the MacroAssembler.\n  CPURegList tmp_list_ = DefaultTmpList();\n  CPURegList fptmp_list_ = DefaultFPTmpList();\n\n  // Helps resolve branching to labels potentially out of range.\n  // If the label is not bound, it registers the information necessary to later\n  // be able to emit a veneer for this branch if necessary.\n  // If the label is bound, it returns true if the label (or the previous link\n  // in the label chain) is out of range. In that case the caller is responsible\n  // for generating appropriate code.\n  // Otherwise it returns false.\n  // This function also checks wether veneers need to be emitted.\n  template <ImmBranchType branch_type>\n  bool NeedExtraInstructionsOrRegisterBranch(Label* label) {\n    static_assert((branch_type == CondBranchType) ||\n                  (branch_type == CompareBranchType) ||\n                  (branch_type == TestBranchType));\n\n    bool need_longer_range = false;\n    // There are two situations in which we care about the offset being out of\n    // range:\n    //  - The label is bound but too far away.\n    //  - The label is not bound but linked, and the previous branch\n    //    instruction in the chain is too far away.\n    if (label->is_bound() || label->is_linked()) {\n      need_longer_range = !Instruction::IsValidImmPCOffset(\n          branch_type, label->pos() - pc_offset());\n    }\n    if (!need_longer_range && !label->is_bound()) {\n      int max_reachable_pc =\n          pc_offset() + Instruction::ImmBranchRange(branch_type);\n\n      // Use the LSB of the max_reachable_pc (always four-byte aligned) to\n      // encode the branch type. We need only distinguish between TB[N]Z and\n      // CB[N]Z/conditional branch, as the ranges for the latter are the same.\n      int branch_type_tag = (branch_type == TestBranchType) ? 1 : 0;\n\n      unresolved_branches_.insert(\n          std::pair<int, Label*>(max_reachable_pc + branch_type_tag, label));\n      // Also maintain the next pool check.\n      next_veneer_pool_check_ =\n          std::min(next_veneer_pool_check_,\n                   max_reachable_pc - kVeneerDistanceCheckMargin);\n    }\n    return need_longer_range;\n  }\n\n  void Movi16bitHelper(const VRegister& vd, uint64_t imm);\n  void Movi32bitHelper(const VRegister& vd, uint64_t imm);\n  void Movi64bitHelper(const VRegister& vd, uint64_t imm);\n\n  void LoadStoreMacro(const CPURegister& rt, const MemOperand& addr,\n                      LoadStoreOp op);\n  void LoadStoreMacroComplex(const CPURegister& rt, const MemOperand& addr,\n                             LoadStoreOp op);\n\n  void LoadStorePairMacro(const CPURegister& rt, const CPURegister& rt2,\n                          const MemOperand& addr, LoadStorePairOp op);\n\n  int64_t CalculateTargetOffset(Address target, RelocInfo::Mode rmode,\n                                uint8_t* pc);\n\n  void JumpHelper(int64_t offset, RelocInfo::Mode rmode, Condition cond = al);\n\n  DISALLOW_IMPLICIT_CONSTRUCTORS(MacroAssembler);\n}", "name_and_para": "class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase "}, {"name": "MacroAssemblerBase", "content": "class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {\n public:\n  using MacroAssemblerBase::MacroAssemblerBase;\n\n  // Activation support.\n  void EnterFrame(StackFrame::Type type);\n  void EnterFrame(StackFrame::Type type, bool load_constant_pool_pointer_reg) {\n    // Out-of-line constant pool not implemented on RISC-V.\n    UNREACHABLE();\n  }\n  void LeaveFrame(StackFrame::Type type);\n\n  // Generates function and stub prologue code.\n  void StubPrologue(StackFrame::Type type);\n  void Prologue();\n\n  void InitializeRootRegister() {\n    ExternalReference isolate_root = ExternalReference::isolate_root(isolate());\n    li(kRootRegister, Operand(isolate_root));\n#ifdef V8_COMPRESS_POINTERS\n    LoadRootRelative(kPtrComprCageBaseRegister,\n                     IsolateData::cage_base_offset());\n#endif\n  }\n\n  // Jump unconditionally to given label.\n  void jmp(Label* L, Label::Distance distance = Label::kFar) {\n    Branch(L, distance);\n  }\n\n  // -------------------------------------------------------------------------\n  // Debugging.\n\n  void Trap();\n  void DebugBreak();\n#ifdef USE_SIMULATOR\n  // See src/codegen/riscv/base-constants-riscv.h DebugParameters.\n  void Debug(uint32_t parameters) { break_(parameters, false); }\n#endif\n  // Calls Abort(msg) if the condition cc is not satisfied.\n  // Use --debug_code to enable.\n  void Assert(Condition cc, AbortReason reason, Register rs, Operand rt);\n\n  void AssertJSAny(Register object, Register map_tmp, Register tmp,\n                   AbortReason abort_reason);\n\n  // Like Assert(), but always enabled.\n  void Check(Condition cc, AbortReason reason, Register rs, Operand rt);\n\n  // Print a message to stdout and abort execution.\n  void Abort(AbortReason msg);\n\n  // Arguments macros.\n#define COND_TYPED_ARGS Condition cond, Register r1, const Operand &r2\n#define COND_ARGS cond, r1, r2\n\n  // Cases when relocation is not needed.\n#define DECLARE_NORELOC_PROTOTYPE(Name, target_type) \\\n  void Name(target_type target);                     \\\n  void Name(target_type target, COND_TYPED_ARGS);\n\n#define DECLARE_BRANCH_PROTOTYPES(Name)   \\\n  DECLARE_NORELOC_PROTOTYPE(Name, Label*) \\\n  DECLARE_NORELOC_PROTOTYPE(Name, int32_t)\n\n  DECLARE_BRANCH_PROTOTYPES(BranchAndLink)\n  DECLARE_BRANCH_PROTOTYPES(BranchShort)\n\n  void Branch(Label* target);\n  void Branch(int32_t target);\n  void BranchLong(Label* L);\n  void Branch(Label* target, Condition cond, Register r1, const Operand& r2,\n              Label::Distance distance = Label::kFar);\n  void Branch(Label* target, Label::Distance distance) {\n    Branch(target, cc_always, zero_reg, Operand(zero_reg), distance);\n  }\n  void Branch(int32_t target, Condition cond, Register r1, const Operand& r2,\n              Label::Distance distance = Label::kFar);\n  void Branch(Label* L, Condition cond, Register rj, RootIndex index,\n              Label::Distance distance = Label::kFar);\n#undef DECLARE_BRANCH_PROTOTYPES\n#undef COND_TYPED_ARGS\n#undef COND_ARGS\n\n  void AllocateStackSpace(Register bytes) { SubWord(sp, sp, bytes); }\n\n  void AllocateStackSpace(int bytes) {\n    DCHECK_GE(bytes, 0);\n    if (bytes == 0) return;\n    SubWord(sp, sp, Operand(bytes));\n  }\n\n  inline void NegateBool(Register rd, Register rs) { Xor(rd, rs, 1); }\n\n  // Compare float, if any operand is NaN, result is false except for NE\n  void CompareF32(Register rd, FPUCondition cc, FPURegister cmp1,\n                  FPURegister cmp2);\n  // Compare double, if any operand is NaN, result is false except for NE\n  void CompareF64(Register rd, FPUCondition cc, FPURegister cmp1,\n                  FPURegister cmp2);\n  void CompareIsNotNanF32(Register rd, FPURegister cmp1, FPURegister cmp2);\n  void CompareIsNotNanF64(Register rd, FPURegister cmp1, FPURegister cmp2);\n  void CompareIsNanF32(Register rd, FPURegister cmp1, FPURegister cmp2);\n  void CompareIsNanF64(Register rd, FPURegister cmp1, FPURegister cmp2);\n\n  // Floating point branches\n  void BranchTrueShortF(Register rs, Label* target);\n  void BranchFalseShortF(Register rs, Label* target);\n\n  void BranchTrueF(Register rs, Label* target);\n  void BranchFalseF(Register rs, Label* target);\n\n  void CompareTaggedAndBranch(Label* label, Condition cond, Register r1,\n                              const Operand& r2, bool need_link = false);\n  static int InstrCountForLi64Bit(int64_t value);\n  inline void LiLower32BitHelper(Register rd, Operand j);\n  void li_optimized(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);\n  // Load int32 in the rd register.\n  void li(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);\n  inline void li(Register rd, intptr_t j, LiFlags mode = OPTIMIZE_SIZE) {\n    li(rd, Operand(j), mode);\n  }\n\n  inline void Move(Register output, MemOperand operand) {\n    LoadWord(output, operand);\n  }\n  void li(Register dst, Handle<HeapObject> value,\n          RelocInfo::Mode rmode = RelocInfo::FULL_EMBEDDED_OBJECT);\n  void li(Register dst, ExternalReference value, LiFlags mode = OPTIMIZE_SIZE);\n\n  void LoadFromConstantsTable(Register destination, int constant_index) final;\n  void LoadRootRegisterOffset(Register destination, intptr_t offset) final;\n  void LoadRootRelative(Register destination, int32_t offset) final;\n  void StoreRootRelative(int32_t offset, Register value) final;\n\n  // Operand pointing to an external reference.\n  // May emit code to set up the scratch register. The operand is\n  // only guaranteed to be correct as long as the scratch register\n  // isn't changed.\n  // If the operand is used more than once, use a scratch register\n  // that is guaranteed not to be clobbered.\n  MemOperand ExternalReferenceAsOperand(ExternalReference reference,\n                                        Register scratch);\n\n  inline void GenPCRelativeJump(Register rd, int32_t imm32) {\n    BlockTrampolinePoolScope block_trampoline_pool(this);\n    DCHECK(is_int32(imm32 + 0x800));\n    int32_t Hi20 = ((imm32 + 0x800) >> 12);\n    int32_t Lo12 = imm32 << 20 >> 20;\n    auipc(rd, Hi20);  // Read PC + Hi20 into scratch.\n    jr(rd, Lo12);     // jump PC + Hi20 + Lo12\n  }\n\n  inline void GenPCRelativeJumpAndLink(Register rd, int32_t imm32) {\n    BlockTrampolinePoolScope block_trampoline_pool(this);\n    DCHECK(is_int32(imm32 + 0x800));\n    int32_t Hi20 = ((imm32 + 0x800) >> 12);\n    int32_t Lo12 = imm32 << 20 >> 20;\n    auipc(rd, Hi20);  // Read PC + Hi20 into scratch.\n    jalr(rd, Lo12);   // jump PC + Hi20 + Lo12\n  }\n\n  // Generate a B immediate instruction with the corresponding relocation info.\n  // 'offset' is the immediate to encode in the B instruction (so it is the\n  // difference between the target and the PC of the instruction, divided by\n  // the instruction size).\n  void near_jump(int offset, RelocInfo::Mode rmode) {\n    UseScratchRegisterScope temps(this);\n    Register temp = temps.Acquire();\n    if (!RelocInfo::IsNoInfo(rmode)) RecordRelocInfo(rmode, offset);\n    GenPCRelativeJump(temp, offset);\n  }\n  // Generate a BL immediate instruction with the corresponding relocation info.\n  // As for near_jump, 'offset' is the immediate to encode in the BL\n  // instruction.\n  void near_call(int offset, RelocInfo::Mode rmode) {\n    UseScratchRegisterScope temps(this);\n    Register temp = temps.Acquire();\n    if (!RelocInfo::IsNoInfo(rmode)) RecordRelocInfo(rmode, offset);\n    GenPCRelativeJumpAndLink(temp, offset);\n  }\n  // Generate a BL immediate instruction with the corresponding relocation info\n  // for the input HeapNumberRequest.\n  void near_call(HeapNumberRequest request) { UNIMPLEMENTED(); }\n\n// Jump, Call, and Ret pseudo instructions implementing inter-working.\n#define COND_ARGS                              \\\n  Condition cond = al, Register rs = zero_reg, \\\n            const Operand &rt = Operand(zero_reg)\n\n  void Jump(Register target, COND_ARGS);\n  void Jump(intptr_t target, RelocInfo::Mode rmode, COND_ARGS);\n  void Jump(Address target, RelocInfo::Mode rmode, COND_ARGS);\n  // Deffer from li, this method save target to the memory, and then load\n  // it to register use ld, it can be used in wasm jump table for concurrent\n  // patching.\n\n  // We should not use near calls or jumps for calls to external references,\n  // since the code spaces are not guaranteed to be close to each other.\n  bool CanUseNearCallOrJump(RelocInfo::Mode rmode) {\n    return rmode != RelocInfo::EXTERNAL_REFERENCE;\n  }\n  void PatchAndJump(Address target);\n  void Jump(Handle<Code> code, RelocInfo::Mode rmode, COND_ARGS);\n  void Jump(const ExternalReference& reference);\n  void Call(Register target, COND_ARGS);\n  void Call(Address target, RelocInfo::Mode rmode, COND_ARGS);\n  void Call(Handle<Code> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,\n            COND_ARGS);\n  void Call(Label* target);\n  void LoadAddress(\n      Register dst, Label* target,\n      RelocInfo::Mode rmode = RelocInfo::INTERNAL_REFERENCE_ENCODED);\n\n  // Load the code entry point from the Code object.\n  void LoadCodeInstructionStart(\n      Register destination, Register code_object,\n      CodeEntrypointTag tag = kDefaultCodeEntrypointTag);\n  void CallCodeObject(Register code_object, CodeEntrypointTag tag);\n  void JumpCodeObject(Register code_object, CodeEntrypointTag tag,\n                      JumpMode jump_mode = JumpMode::kJump);\n\n  // Convenience functions to call/jmp to the code of a JSFunction object.\n  void CallJSFunction(Register function_object);\n  void JumpJSFunction(Register function_object,\n                      JumpMode jump_mode = JumpMode::kJump);\n\n  // Load the builtin given by the Smi in |builtin| into the same\n  // register.\n  // Load the builtin given by the Smi in |builtin_index| into |target|.\n  void LoadEntryFromBuiltinIndex(Register builtin_index, Register target);\n  void LoadEntryFromBuiltin(Builtin builtin, Register destination);\n  MemOperand EntryFromBuiltinAsOperand(Builtin builtin);\n  void CallBuiltinByIndex(Register builtin_index, Register target);\n  void CallBuiltin(Builtin builtin);\n  void TailCallBuiltin(Builtin builtin);\n  void TailCallBuiltin(Builtin builtin, Condition cond, Register type,\n                       Operand range);\n\n  // Generates an instruction sequence s.t. the return address points to the\n  // instruction following the call.\n  // The return address on the stack is used by frame iteration.\n  void StoreReturnAddressAndCall(Register target);\n\n  void CallForDeoptimization(Builtin target, int deopt_id, Label* exit,\n                             DeoptimizeKind kind, Label* ret,\n                             Label* jump_deoptimization_entry_label);\n\n  void Ret(COND_ARGS);\n\n  // Emit code to discard a non-negative number of pointer-sized elements\n  // from the stack, clobbering only the sp register.\n  void Drop(int count, Condition cond = cc_always, Register reg = no_reg,\n            const Operand& op = Operand(no_reg));\n\n  // Trivial case of DropAndRet that only emits 2 instructions.\n  void DropAndRet(int drop);\n\n  void DropAndRet(int drop, Condition cond, Register reg, const Operand& op);\n\n  void push(Register src) {\n    AddWord(sp, sp, Operand(-kSystemPointerSize));\n    StoreWord(src, MemOperand(sp, 0));\n  }\n  void Push(Register src) { push(src); }\n  void Push(Handle<HeapObject> handle);\n  void Push(Tagged<Smi> smi);\n\n  // Push two registers. Pushes leftmost register first (to highest address).\n  void Push(Register src1, Register src2) {\n    SubWord(sp, sp, Operand(2 * kSystemPointerSize));\n    StoreWord(src1, MemOperand(sp, 1 * kSystemPointerSize));\n    StoreWord(src2, MemOperand(sp, 0 * kSystemPointerSize));\n  }\n\n  // Push three registers. Pushes leftmost register first (to highest address).\n  void Push(Register src1, Register src2, Register src3) {\n    SubWord(sp, sp, Operand(3 * kSystemPointerSize));\n    StoreWord(src1, MemOperand(sp, 2 * kSystemPointerSize));\n    StoreWord(src2, MemOperand(sp, 1 * kSystemPointerSize));\n    StoreWord(src3, MemOperand(sp, 0 * kSystemPointerSize));\n  }\n\n  // Push four registers. Pushes leftmost register first (to highest address).\n  void Push(Register src1, Register src2, Register src3, Register src4) {\n    SubWord(sp, sp, Operand(4 * kSystemPointerSize));\n    StoreWord(src1, MemOperand(sp, 3 * kSystemPointerSize));\n    StoreWord(src2, MemOperand(sp, 2 * kSystemPointerSize));\n    StoreWord(src3, MemOperand(sp, 1 * kSystemPointerSize));\n    StoreWord(src4, MemOperand(sp, 0 * kSystemPointerSize));\n  }\n\n  // Push five registers. Pushes leftmost register first (to highest address).\n  void Push(Register src1, Register src2, Register src3, Register src4,\n            Register src5) {\n    SubWord(sp, sp, Operand(5 * kSystemPointerSize));\n    StoreWord(src1, MemOperand(sp, 4 * kSystemPointerSize));\n    StoreWord(src2, MemOperand(sp, 3 * kSystemPointerSize));\n    StoreWord(src3, MemOperand(sp, 2 * kSystemPointerSize));\n    StoreWord(src4, MemOperand(sp, 1 * kSystemPointerSize));\n    StoreWord(src5, MemOperand(sp, 0 * kSystemPointerSize));\n  }\n\n  void Push(Register src, Condition cond, Register tst1, Register tst2) {\n    // Since we don't have conditional execution we use a Branch.\n    Branch(3, cond, tst1, Operand(tst2));\n    SubWord(sp, sp, Operand(kSystemPointerSize));\n    StoreWord(src, MemOperand(sp, 0));\n  }\n\n  enum PushArrayOrder { kNormal, kReverse };\n  void PushArray(Register array, Register size, PushArrayOrder order = kNormal);\n\n  void MaybeSaveRegisters(RegList registers);\n  void MaybeRestoreRegisters(RegList registers);\n\n  void CallEphemeronKeyBarrier(Register object, Operand offset,\n                               SaveFPRegsMode fp_mode);\n  void CallIndirectPointerBarrier(Register object, Operand offset,\n                                  SaveFPRegsMode fp_mode,\n                                  IndirectPointerTag tag);\n  void CallRecordWriteStubSaveRegisters(\n      Register object, Operand offset, SaveFPRegsMode fp_mode,\n      StubCallMode mode = StubCallMode::kCallBuiltinPointer);\n  void CallRecordWriteStub(\n      Register object, Register slot_address, SaveFPRegsMode fp_mode,\n      StubCallMode mode = StubCallMode::kCallBuiltinPointer);\n\n  // For a given |object| and |offset|:\n  //   - Move |object| to |dst_object|.\n  //   - Compute the address of the slot pointed to by |offset| in |object| and\n  //     write it to |dst_slot|.\n  // This method makes sure |object| and |offset| are allowed to overlap with\n  // the destination registers.\n  void MoveObjectAndSlot(Register dst_object, Register dst_slot,\n                         Register object, Operand offset);\n  // Push multiple registers on the stack.\n  // Registers are saved in numerical order, with higher numbered registers\n  // saved in higher memory addresses.\n  void MultiPush(RegList regs);\n  void MultiPushFPU(DoubleRegList regs);\n\n  // Calculate how much stack space (in bytes) are required to store caller\n  // registers excluding those specified in the arguments.\n  int RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,\n                                      Register exclusion1 = no_reg,\n                                      Register exclusion2 = no_reg,\n                                      Register exclusion3 = no_reg) const;\n\n  // Push caller saved registers on the stack, and return the number of bytes\n  // stack pointer is adjusted.\n  int PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1 = no_reg,\n                      Register exclusion2 = no_reg,\n                      Register exclusion3 = no_reg);\n  // Restore caller saved registers from the stack, and return the number of\n  // bytes stack pointer is adjusted.\n  int PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1 = no_reg,\n                     Register exclusion2 = no_reg,\n                     Register exclusion3 = no_reg);\n\n  void pop(Register dst) {\n    LoadWord(dst, MemOperand(sp, 0));\n    AddWord(sp, sp, Operand(kSystemPointerSize));\n  }\n  void Pop(Register dst) { pop(dst); }\n\n  // Pop two registers. Pops rightmost register first (from lower address).\n  void Pop(Register src1, Register src2) {\n    DCHECK(src1 != src2);\n    LoadWord(src2, MemOperand(sp, 0 * kSystemPointerSize));\n    LoadWord(src1, MemOperand(sp, 1 * kSystemPointerSize));\n    AddWord(sp, sp, 2 * kSystemPointerSize);\n  }\n\n  // Pop three registers. Pops rightmost register first (from lower address).\n  void Pop(Register src1, Register src2, Register src3) {\n    LoadWord(src3, MemOperand(sp, 0 * kSystemPointerSize));\n    LoadWord(src2, MemOperand(sp, 1 * kSystemPointerSize));\n    LoadWord(src1, MemOperand(sp, 2 * kSystemPointerSize));\n    AddWord(sp, sp, 3 * kSystemPointerSize);\n  }\n\n  void Pop(uint32_t count = 1) {\n    AddWord(sp, sp, Operand(count * kSystemPointerSize));\n  }\n\n  // Pops multiple values from the stack and load them in the\n  // registers specified in regs. Pop order is the opposite as in MultiPush.\n  void MultiPop(RegList regs);\n  void MultiPopFPU(DoubleRegList regs);\n\n#define DEFINE_INSTRUCTION(instr)                          \\\n  void instr(Register rd, Register rs, const Operand& rt); \\\n  void instr(Register rd, Register rs, Register rt) {      \\\n    instr(rd, rs, Operand(rt));                            \\\n  }                                                        \\\n  void instr(Register rs, Register rt, int32_t j) { instr(rs, rt, Operand(j)); }\n\n#define DEFINE_INSTRUCTION2(instr)                                 \\\n  void instr(Register rs, const Operand& rt);                      \\\n  void instr(Register rs, Register rt) { instr(rs, Operand(rt)); } \\\n  void instr(Register rs, int32_t j) { instr(rs, Operand(j)); }\n\n#define DEFINE_INSTRUCTION3(instr) void instr(Register rd, intptr_t imm);\n\n  DEFINE_INSTRUCTION(AddWord)\n  DEFINE_INSTRUCTION(SubWord)\n  DEFINE_INSTRUCTION(SllWord)\n  DEFINE_INSTRUCTION(SrlWord)\n  DEFINE_INSTRUCTION(SraWord)\n#if V8_TARGET_ARCH_RISCV64\n  DEFINE_INSTRUCTION(Add32)\n  DEFINE_INSTRUCTION(Add64)\n  DEFINE_INSTRUCTION(Div32)\n  DEFINE_INSTRUCTION(Divu32)\n  DEFINE_INSTRUCTION(Divu64)\n  DEFINE_INSTRUCTION(Mod32)\n  DEFINE_INSTRUCTION(Modu32)\n  DEFINE_INSTRUCTION(Div64)\n  DEFINE_INSTRUCTION(Sub32)\n  DEFINE_INSTRUCTION(Sub64)\n  DEFINE_INSTRUCTION(Mod64)\n  DEFINE_INSTRUCTION(Modu64)\n  DEFINE_INSTRUCTION(Mul32)\n  DEFINE_INSTRUCTION(Mulh32)\n  DEFINE_INSTRUCTION(Mul64)\n  DEFINE_INSTRUCTION(Mulh64)\n  DEFINE_INSTRUCTION(Mulhu64)\n  DEFINE_INSTRUCTION2(Div32)\n  DEFINE_INSTRUCTION2(Div64)\n  DEFINE_INSTRUCTION2(Divu32)\n  DEFINE_INSTRUCTION2(Divu64)\n  DEFINE_INSTRUCTION(Sll64)\n  DEFINE_INSTRUCTION(Sra64)\n  DEFINE_INSTRUCTION(Srl64)\n  DEFINE_INSTRUCTION(Dror)\n#elif V8_TARGET_ARCH_RISCV32\n  DEFINE_INSTRUCTION(Add32)\n  DEFINE_INSTRUCTION(Div)\n  DEFINE_INSTRUCTION(Divu)\n  DEFINE_INSTRUCTION(Mod)\n  DEFINE_INSTRUCTION(Modu)\n  DEFINE_INSTRUCTION(Sub32)\n  DEFINE_INSTRUCTION(Mul)\n  DEFINE_INSTRUCTION(Mul32)\n  DEFINE_INSTRUCTION(Mulh)\n  DEFINE_INSTRUCTION2(Div)\n  DEFINE_INSTRUCTION2(Divu)\n#endif\n  DEFINE_INSTRUCTION(And)\n  DEFINE_INSTRUCTION(Or)\n  DEFINE_INSTRUCTION(Xor)\n  DEFINE_INSTRUCTION(Nor)\n  DEFINE_INSTRUCTION2(Neg)\n\n  DEFINE_INSTRUCTION(Slt)\n  DEFINE_INSTRUCTION(Sltu)\n  DEFINE_INSTRUCTION(Sle)\n  DEFINE_INSTRUCTION(Sleu)\n  DEFINE_INSTRUCTION(Sgt)\n  DEFINE_INSTRUCTION(Sgtu)\n  DEFINE_INSTRUCTION(Sge)\n  DEFINE_INSTRUCTION(Sgeu)\n  DEFINE_INSTRUCTION(Seq)\n  DEFINE_INSTRUCTION(Sne)\n  DEFINE_INSTRUCTION(Sll32)\n  DEFINE_INSTRUCTION(Sra32)\n  DEFINE_INSTRUCTION(Srl32)\n\n  DEFINE_INSTRUCTION2(Seqz)\n  DEFINE_INSTRUCTION2(Snez)\n\n  DEFINE_INSTRUCTION(Ror)\n\n  DEFINE_INSTRUCTION3(Li)\n  DEFINE_INSTRUCTION2(Mv)\n\n#undef DEFINE_INSTRUCTION\n#undef DEFINE_INSTRUCTION2\n#undef DEFINE_INSTRUCTION3\n\n  void Amosub_w(bool aq, bool rl, Register rd, Register rs1, Register rs2) {\n    UseScratchRegisterScope temps(this);\n    Register temp = temps.Acquire();\n    sub(temp, zero_reg, rs2);\n    amoadd_w(aq, rl, rd, rs1, temp);\n  }\n\n  void SmiUntag(Register dst, const MemOperand& src);\n  void SmiUntag(Register dst, Register src) {\n#if V8_TARGET_ARCH_RISCV64\n    DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());\n    if (COMPRESS_POINTERS_BOOL) {\n      sraiw(dst, src, kSmiShift);\n    } else {\n      srai(dst, src, kSmiShift);\n    }\n#elif V8_TARGET_ARCH_RISCV32\n    DCHECK(SmiValuesAre31Bits());\n    srai(dst, src, kSmiShift);\n#endif\n  }\n\n  void SmiUntag(Register reg) { SmiUntag(reg, reg); }\n  void SmiToInt32(Register smi);\n\n  // Enabled via --debug-code.\n  void AssertNotSmi(Register object,\n                    AbortReason reason = AbortReason::kOperandIsASmi);\n  void AssertSmi(Register object,\n                 AbortReason reason = AbortReason::kOperandIsASmi);\n\n  int CalculateStackPassedDWords(int num_gp_arguments, int num_fp_arguments);\n\n  // Before calling a C-function from generated code, align arguments on stack.\n  // After aligning the frame, non-register arguments must be stored on the\n  // stack, using helper: CFunctionArgumentOperand().\n  // The argument count assumes all arguments are word sized.\n  // Some compilers/platforms require the stack to be aligned when calling\n  // C++ code.\n  // Needs a scratch register to do some arithmetic. This register will be\n  // trashed.\n  void PrepareCallCFunction(int num_reg_arguments, int num_double_registers,\n                            Register scratch);\n  void PrepareCallCFunction(int num_reg_arguments, Register scratch);\n\n  // Arguments 1-8 are placed in registers a0 through a7 respectively.\n  // Arguments 9..n are stored to stack\n\n  // Calls a C function and cleans up the space for arguments allocated\n  // by PrepareCallCFunction. The called function is not allowed to trigger a\n  // garbage collection, since that might move the code and invalidate the\n  // return address (unless this is somehow accounted for by the called\n  // function).\n  int CallCFunction(\n      ExternalReference function, int num_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  int CallCFunction(\n      Register function, int num_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  int CallCFunction(\n      ExternalReference function, int num_reg_arguments,\n      int num_double_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  int CallCFunction(\n      Register function, int num_reg_arguments, int num_double_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n  void MovFromFloatResult(DoubleRegister dst);\n  void MovFromFloatParameter(DoubleRegister dst);\n\n  // These functions abstract parameter passing for the three different ways\n  // we call C functions from generated code.\n  void MovToFloatParameter(DoubleRegister src);\n  void MovToFloatParameters(DoubleRegister src1, DoubleRegister src2);\n  void MovToFloatResult(DoubleRegister src);\n\n  // See comments at the beginning of Builtins::Generate_CEntry.\n  inline void PrepareCEntryArgs(int num_args) { li(a0, num_args); }\n  inline void PrepareCEntryFunction(const ExternalReference& ref) {\n    li(a1, ref);\n  }\n\n  void CheckPageFlag(Register object, int mask, Condition cc,\n                     Label* condition_met);\n#undef COND_ARGS\n\n  // Performs a truncating conversion of a floating point number as used by\n  // the JS bitwise operations. See ECMA-262 9.5: ToInt32.\n  // Exits with 'result' holding the answer.\n  void TruncateDoubleToI(Isolate* isolate, Zone* zone, Register result,\n                         DoubleRegister double_input, StubCallMode stub_mode);\n\n  void CompareI(Register rd, Register rs, const Operand& rt, Condition cond);\n\n  void LoadZeroIfConditionNotZero(Register dest, Register condition);\n  void LoadZeroIfConditionZero(Register dest, Register condition);\n\n  void SignExtendByte(Register rd, Register rs) {\n    if (CpuFeatures::IsSupported(ZBB)) {\n      sextb(rd, rs);\n    } else {\n      slli(rd, rs, xlen - 8);\n      srai(rd, rd, xlen - 8);\n    }\n  }\n\n  void SignExtendShort(Register rd, Register rs) {\n    if (CpuFeatures::IsSupported(ZBB)) {\n      sexth(rd, rs);\n    } else {\n      slli(rd, rs, xlen - 16);\n      srai(rd, rd, xlen - 16);\n    }\n  }\n\n  void Clz32(Register rd, Register rs);\n  void Ctz32(Register rd, Register rs);\n  void Popcnt32(Register rd, Register rs, Register scratch);\n\n#if V8_TARGET_ARCH_RISCV64\n  void SignExtendWord(Register rd, Register rs) { sext_w(rd, rs); }\n  void ZeroExtendWord(Register rd, Register rs) {\n    if (CpuFeatures::IsSupported(ZBA)) {\n      zextw(rd, rs);\n    } else {\n      slli(rd, rs, 32);\n      srli(rd, rd, 32);\n    }\n  }\n  void Popcnt64(Register rd, Register rs, Register scratch);\n  void Ctz64(Register rd, Register rs);\n  void Clz64(Register rd, Register rs);\n#elif V8_TARGET_ARCH_RISCV32\n  void AddPair(Register dst_low, Register dst_high, Register left_low,\n               Register left_high, Register right_low, Register right_high,\n               Register scratch1, Register scratch2);\n\n  void SubPair(Register dst_low, Register dst_high, Register left_low,\n               Register left_high, Register right_low, Register right_high,\n               Register scratch1, Register scratch2);\n\n  void AndPair(Register dst_low, Register dst_high, Register left_low,\n               Register left_high, Register right_low, Register right_high);\n\n  void OrPair(Register dst_low, Register dst_high, Register left_low,\n              Register left_high, Register right_low, Register right_high);\n\n  void XorPair(Register dst_low, Register dst_high, Register left_low,\n               Register left_high, Register right_low, Register right_high);\n\n  void MulPair(Register dst_low, Register dst_high, Register left_low,\n               Register left_high, Register right_low, Register right_high,\n               Register scratch1, Register scratch2);\n\n  void ShlPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, Register shift, Register scratch1,\n               Register scratch2);\n  void ShlPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, int32_t shift, Register scratch1,\n               Register scratch2);\n\n  void ShrPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, Register shift, Register scratch1,\n               Register scratch2);\n\n  void ShrPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, int32_t shift, Register scratch1,\n               Register scratch2);\n\n  void SarPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, Register shift, Register scratch1,\n               Register scratch2);\n  void SarPair(Register dst_low, Register dst_high, Register src_low,\n               Register src_high, int32_t shift, Register scratch1,\n               Register scratch2);\n#endif\n\n  // Bit field starts at bit pos and extending for size bits is extracted from\n  // rs and stored zero/sign-extended and right-justified in rt\n  void ExtractBits(Register rt, Register rs, uint16_t pos, uint16_t size,\n                   bool sign_extend = false);\n  void ExtractBits(Register dest, Register source, Register pos, int size,\n                   bool sign_extend = false) {\n    sra(dest, source, pos);\n    ExtractBits(dest, dest, 0, size, sign_extend);\n  }\n\n  // Insert bits [0, size) of source to bits [pos, pos+size) of dest\n  void InsertBits(Register dest, Register source, Register pos, int size);\n\n  void Neg_s(FPURegister fd, FPURegister fs);\n  void Neg_d(FPURegister fd, FPURegister fs);\n\n  // Change endianness\n  void ByteSwap(Register dest, Register src, int operand_size,\n                Register scratch);\n\n  void Clear_if_nan_d(Register rd, FPURegister fs);\n  void Clear_if_nan_s(Register rd, FPURegister fs);\n  // Convert single to unsigned word.\n  void Trunc_uw_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // helper functions for unaligned load/store\n  template <int NBYTES, bool IS_SIGNED>\n  void UnalignedLoadHelper(Register rd, const MemOperand& rs);\n  template <int NBYTES>\n  void UnalignedStoreHelper(Register rd, const MemOperand& rs,\n                            Register scratch_other = no_reg);\n\n  template <int NBYTES>\n  void UnalignedFLoadHelper(FPURegister frd, const MemOperand& rs,\n                            Register scratch);\n  template <int NBYTES>\n  void UnalignedFStoreHelper(FPURegister frd, const MemOperand& rs,\n                             Register scratch);\n#if V8_TARGET_ARCH_RISCV32\n  void UnalignedDoubleHelper(FPURegister frd, const MemOperand& rs,\n                             Register scratch_base);\n  void UnalignedDStoreHelper(FPURegister frd, const MemOperand& rs,\n                             Register scratch);\n#endif\n\n  template <typename Reg_T, typename Func>\n  void AlignedLoadHelper(Reg_T target, const MemOperand& rs, Func generator);\n  template <typename Reg_T, typename Func>\n  void AlignedStoreHelper(Reg_T value, const MemOperand& rs, Func generator);\n\n  template <int NBYTES, bool LOAD_SIGNED>\n  void LoadNBytes(Register rd, const MemOperand& rs, Register scratch);\n  template <int NBYTES, bool LOAD_SIGNED>\n  void LoadNBytesOverwritingBaseReg(const MemOperand& rs, Register scratch0,\n                                    Register scratch1);\n  // load/store macros\n  void Ulh(Register rd, const MemOperand& rs);\n  void Ulhu(Register rd, const MemOperand& rs);\n  void Ush(Register rd, const MemOperand& rs);\n\n  void Ulw(Register rd, const MemOperand& rs);\n  void Usw(Register rd, const MemOperand& rs);\n\n  void Uld(Register rd, const MemOperand& rs);\n  void Usd(Register rd, const MemOperand& rs);\n\n  void ULoadFloat(FPURegister fd, const MemOperand& rs, Register scratch);\n  void UStoreFloat(FPURegister fd, const MemOperand& rs, Register scratch);\n\n  void ULoadDouble(FPURegister fd, const MemOperand& rs, Register scratch);\n  void UStoreDouble(FPURegister fd, const MemOperand& rs, Register scratch);\n\n  void Lb(Register rd, const MemOperand& rs);\n  void Lbu(Register rd, const MemOperand& rs);\n  void Sb(Register rd, const MemOperand& rs);\n\n  void Lh(Register rd, const MemOperand& rs);\n  void Lhu(Register rd, const MemOperand& rs);\n  void Sh(Register rd, const MemOperand& rs);\n\n  void Lw(Register rd, const MemOperand& rs);\n  void Sw(Register rd, const MemOperand& rs);\n\n#if V8_TARGET_ARCH_RISCV64\n  void Ulwu(Register rd, const MemOperand& rs);\n  void Lwu(Register rd, const MemOperand& rs);\n  void Ld(Register rd, const MemOperand& rs);\n  void Sd(Register rd, const MemOperand& rs);\n  void Lld(Register rd, const MemOperand& rs);\n  void Scd(Register rd, const MemOperand& rs);\n\n  inline void Load32U(Register rd, const MemOperand& rs) { Lwu(rd, rs); }\n  inline void LoadWord(Register rd, const MemOperand& rs) { Ld(rd, rs); }\n  inline void StoreWord(Register rd, const MemOperand& rs) { Sd(rd, rs); }\n#elif V8_TARGET_ARCH_RISCV32\n  inline void Load32U(Register rd, const MemOperand& rs) { Lw(rd, rs); }\n  inline void LoadWord(Register rd, const MemOperand& rs) { Lw(rd, rs); }\n  inline void StoreWord(Register rd, const MemOperand& rs) { Sw(rd, rs); }\n#endif\n  void LoadFloat(FPURegister fd, const MemOperand& src);\n  void StoreFloat(FPURegister fs, const MemOperand& dst);\n\n  void LoadDouble(FPURegister fd, const MemOperand& src);\n  void StoreDouble(FPURegister fs, const MemOperand& dst);\n\n  void Ll(Register rd, const MemOperand& rs);\n  void Sc(Register rd, const MemOperand& rs);\n\n  void Float32Max(FPURegister dst, FPURegister src1, FPURegister src2);\n  void Float32Min(FPURegister dst, FPURegister src1, FPURegister src2);\n  void Float64Max(FPURegister dst, FPURegister src1, FPURegister src2);\n  void Float64Min(FPURegister dst, FPURegister src1, FPURegister src2);\n  template <typename F>\n  void FloatMinMaxHelper(FPURegister dst, FPURegister src1, FPURegister src2,\n                         MaxMinKind kind);\n\n  bool IsDoubleZeroRegSet() { return has_double_zero_reg_set_; }\n  bool IsSingleZeroRegSet() { return has_single_zero_reg_set_; }\n\n  inline void Move(Register dst, Tagged<Smi> smi) { li(dst, Operand(smi)); }\n\n  inline void Move(Register dst, Register src) {\n    if (dst != src) {\n      mv(dst, src);\n    }\n  }\n\n  inline void MoveDouble(FPURegister dst, FPURegister src) {\n    if (dst != src) fmv_d(dst, src);\n  }\n\n  inline void MoveFloat(FPURegister dst, FPURegister src) {\n    if (dst != src) fmv_s(dst, src);\n  }\n\n  inline void Move(FPURegister dst, FPURegister src) { MoveDouble(dst, src); }\n\n#if V8_TARGET_ARCH_RISCV64\n  inline void Move(Register dst_low, Register dst_high, FPURegister src) {\n    fmv_x_d(dst_high, src);\n    fmv_x_w(dst_low, src);\n    srli(dst_high, dst_high, 32);\n  }\n\n  inline void Move(Register dst, FPURegister src) { fmv_x_d(dst, src); }\n\n  inline void Move(FPURegister dst, Register src) { fmv_d_x(dst, src); }\n#elif V8_TARGET_ARCH_RISCV32\n  inline void Move(Register dst, FPURegister src) { fmv_x_w(dst, src); }\n\n  inline void Move(FPURegister dst, Register src) { fmv_w_x(dst, src); }\n#endif\n\n  // Extract sign-extended word from high-half of FPR to GPR\n  inline void ExtractHighWordFromF64(Register dst_high, FPURegister src) {\n#if V8_TARGET_ARCH_RISCV64\n    fmv_x_d(dst_high, src);\n    srai(dst_high, dst_high, 32);\n#elif V8_TARGET_ARCH_RISCV32\n    // todo(riscv32): delete storedouble\n    AddWord(sp, sp, Operand(-8));\n    StoreDouble(src, MemOperand(sp, 0));\n    Lw(dst_high, MemOperand(sp, 4));\n    AddWord(sp, sp, Operand(8));\n#endif\n  }\n\n  // Insert low-word from GPR (src_high) to the high-half of FPR (dst)\n  void InsertHighWordF64(FPURegister dst, Register src_high);\n\n  // Extract sign-extended word from low-half of FPR to GPR\n  inline void ExtractLowWordFromF64(Register dst_low, FPURegister src) {\n    fmv_x_w(dst_low, src);\n  }\n\n  // Insert low-word from GPR (src_high) to the low-half of FPR (dst)\n  void InsertLowWordF64(FPURegister dst, Register src_low);\n\n  void LoadFPRImmediate(FPURegister dst, float imm) {\n    LoadFPRImmediate(dst, base::bit_cast<uint32_t>(imm));\n  }\n  void LoadFPRImmediate(FPURegister dst, double imm) {\n    LoadFPRImmediate(dst, base::bit_cast<uint64_t>(imm));\n  }\n  void LoadFPRImmediate(FPURegister dst, uint32_t src);\n  void LoadFPRImmediate(FPURegister dst, uint64_t src);\n#if V8_TARGET_ARCH_RISCV64\n  // AddOverflow64 sets overflow register to a negative value if\n  // overflow occured, otherwise it is zero or positive\n  void AddOverflow64(Register dst, Register left, const Operand& right,\n                     Register overflow);\n  // SubOverflow64 sets overflow register to a negative value if\n  // overflow occured, otherwise it is zero or positive\n  void SubOverflow64(Register dst, Register left, const Operand& right,\n                     Register overflow);\n  // MIPS-style 32-bit unsigned mulh\n  void Mulhu32(Register dst, Register left, const Operand& right,\n               Register left_zero, Register right_zero);\n#elif V8_TARGET_ARCH_RISCV32\n  // AddOverflow sets overflow register to a negative value if\n  // overflow occured, otherwise it is zero or positive\n  void AddOverflow(Register dst, Register left, const Operand& right,\n                   Register overflow);\n  // SubOverflow sets overflow register to a negative value if\n  // overflow occured, otherwise it is zero or positive\n  void SubOverflow(Register dst, Register left, const Operand& right,\n                   Register overflow);\n  // MIPS-style 32-bit unsigned mulh\n  void Mulhu(Register dst, Register left, const Operand& right,\n             Register left_zero, Register right_zero);\n#endif\n  // MulOverflow32 sets overflow register to zero if no overflow occured\n  void MulOverflow32(Register dst, Register left, const Operand& right,\n                     Register overflow);\n  // MulOverflow64 sets overflow register to zero if no overflow occured\n  void MulOverflow64(Register dst, Register left, const Operand& right,\n                     Register overflow);\n  // Number of instructions needed for calculation of switch table entry address\n  static const int kSwitchTablePrologueSize = 6;\n\n  // GetLabelFunction must be lambda '[](size_t index) -> Label*' or a\n  // functor/function with 'Label *func(size_t index)' declaration.\n  template <typename Func>\n  void GenerateSwitchTable(Register index, size_t case_count,\n                           Func GetLabelFunction);\n\n  // Load an object from the root table.\n  void LoadRoot(Register destination, RootIndex index) final;\n  void LoadTaggedRoot(Register destination, RootIndex index);\n\n  void LoadMap(Register destination, Register object);\n\n  void LoadFeedbackVector(Register dst, Register closure, Register scratch,\n                          Label* fbv_undef);\n  void LoadCompressedMap(Register dst, Register object);\n\n  // If the value is a NaN, canonicalize the value else, do nothing.\n  void FPUCanonicalizeNaN(const DoubleRegister dst, const DoubleRegister src);\n\n  // ---------------------------------------------------------------------------\n  // FPU macros. These do not handle special cases like NaN or +- inf.\n\n  // Convert unsigned word to double.\n  void Cvt_d_uw(FPURegister fd, Register rs);\n\n  // convert signed word to double.\n  void Cvt_d_w(FPURegister fd, Register rs);\n\n  // Convert unsigned long to double.\n  void Cvt_d_ul(FPURegister fd, Register rs);\n\n  // Convert unsigned word to float.\n  void Cvt_s_uw(FPURegister fd, Register rs);\n\n  // convert signed word to float.\n  void Cvt_s_w(FPURegister fd, Register rs);\n\n  // Convert unsigned long to float.\n  void Cvt_s_ul(FPURegister fd, Register rs);\n\n  // Convert double to unsigned word.\n  void Trunc_uw_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Convert double to signed word.\n  void Trunc_w_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Convert single to signed word.\n  void Trunc_w_s(Register rd, FPURegister fs, Register result = no_reg);\n#if V8_TARGET_ARCH_RISCV64\n  // Convert double to unsigned long.\n  void Trunc_ul_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Convert singled to signed long.\n  void Trunc_l_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Convert single to unsigned long.\n  void Trunc_ul_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Convert singled to signed long.\n  void Trunc_l_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Round double functions\n  void Trunc_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Round_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Floor_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Ceil_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n#endif\n  // Round single to signed word.\n  void Round_w_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Round double to signed word.\n  void Round_w_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Ceil single to signed word.\n  void Ceil_w_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Ceil double to signed word.\n  void Ceil_w_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Floor single to signed word.\n  void Floor_w_s(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Floor double to signed word.\n  void Floor_w_d(Register rd, FPURegister fs, Register result = no_reg);\n\n  // Round float functions\n  void Trunc_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Round_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Floor_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n  void Ceil_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);\n\n  void Ceil_f(VRegister dst, VRegister src, Register scratch,\n              VRegister v_scratch);\n\n  void Ceil_d(VRegister dst, VRegister src, Register scratch,\n              VRegister v_scratch);\n\n  void Floor_f(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  void Floor_d(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  void Trunc_f(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  void Trunc_d(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  void Round_f(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  void Round_d(VRegister dst, VRegister src, Register scratch,\n               VRegister v_scratch);\n  // -------------------------------------------------------------------------\n  // Smi utilities.\n\n  void SmiTag(Register dst, Register src) {\n    static_assert(kSmiTag == 0);\n#if V8_TARGET_ARCH_RISCV64\n    if (SmiValuesAre32Bits()) {\n      // Smi goes to upper 32\n      slli(dst, src, 32);\n    } else {\n      DCHECK(SmiValuesAre31Bits());\n      // Smi is shifted left by 1\n      Add32(dst, src, src);\n    }\n#elif V8_TARGET_ARCH_RISCV32\n\n    DCHECK(SmiValuesAre31Bits());\n    // Smi is shifted left by 1\n    slli(dst, src, kSmiShift);\n#endif\n  }\n\n  void SmiTag(Register reg) { SmiTag(reg, reg); }\n\n  // Jump the register contains a smi.\n  void JumpIfSmi(Register value, Label* smi_label,\n                 Label::Distance distance = Label::kFar);\n\n  // AssembleArchBinarySearchSwitchRange Use JumpIfEqual and JumpIfLessThan.\n  // In V8_COMPRESS_POINTERS, the compare is done with the lower 32 bits of the\n  // input.\n  void JumpIfEqual(Register a, int32_t b, Label* dest) {\n#ifdef V8_COMPRESS_POINTERS\n    Sll32(a, a, 0);\n#endif\n    Branch(dest, eq, a, Operand(b));\n  }\n\n  void JumpIfLessThan(Register a, int32_t b, Label* dest) {\n#ifdef V8_COMPRESS_POINTERS\n    Sll32(a, a, 0);\n#endif\n    Branch(dest, lt, a, Operand(b));\n  }\n\n  // Push a standard frame, consisting of ra, fp, context and JS function.\n  void PushStandardFrame(Register function_reg);\n\n  // Get the actual activation frame alignment for target environment.\n  static int ActivationFrameAlignment();\n\n  // Calculated scaled address (rd) as rt + rs << sa\n  void CalcScaledAddress(Register rd, Register rt, Register rs, uint8_t sa);\n\n  // Compute the start of the generated instruction stream from the current PC.\n  // This is an alternative to embedding the {CodeObject} handle as a reference.\n  void ComputeCodeStartAddress(Register dst);\n\n  // Load a trusted pointer field.\n  // When the sandbox is enabled, these are indirect pointers using the trusted\n  // pointer table. Otherwise they are regular tagged fields.\n  void LoadTrustedPointerField(Register destination, MemOperand field_operand,\n                               IndirectPointerTag tag);\n  // Store a trusted pointer field.\n  void StoreTrustedPointerField(Register value, MemOperand dst_field_operand);\n  // Load a code pointer field.\n  // These are special versions of trusted pointers that, when the sandbox is\n  // enabled, reference code objects through the code pointer table.\n  void LoadCodePointerField(Register destination, MemOperand field_operand) {\n    LoadTrustedPointerField(destination, field_operand,\n                            kCodeIndirectPointerTag);\n  }\n  // Store a code pointer field.\n  void StoreCodePointerField(Register value, MemOperand dst_field_operand) {\n    StoreTrustedPointerField(value, dst_field_operand);\n  }\n\n  // Loads a field containing an off-heap (\"external\") pointer and does\n  // necessary decoding if sandbox is enabled.\n  void LoadExternalPointerField(Register destination, MemOperand field_operand,\n                                ExternalPointerTag tag,\n                                Register isolate_root = no_reg);\n\n#if V8_TARGET_ARCH_RISCV64\n  // ---------------------------------------------------------------------------\n  // Pointer compression Support\n\n  // Loads a field containing any tagged value and decompresses it if necessary.\n  void LoadTaggedField(const Register& destination,\n                       const MemOperand& field_operand);\n\n  // Loads a field containing a tagged signed value and decompresses it if\n  // necessary.\n  void LoadTaggedSignedField(const Register& destination,\n                             const MemOperand& field_operand);\n\n  // Loads a field containing smi value and untags it.\n  void SmiUntagField(Register dst, const MemOperand& src);\n\n  // Compresses and stores tagged value to given on-heap location.\n  void StoreTaggedField(const Register& value,\n                        const MemOperand& dst_field_operand);\n  void AtomicStoreTaggedField(Register dst, const MemOperand& src);\n\n  void DecompressTaggedSigned(const Register& destination,\n                              const MemOperand& field_operand);\n  void DecompressTagged(const Register& destination,\n                        const MemOperand& field_operand);\n  void DecompressTagged(const Register& destination, const Register& source);\n  void DecompressTagged(Register dst, Tagged_t immediate);\n  void DecompressProtected(const Register& destination,\n                           const MemOperand& field_operand);\n\n  // ---------------------------------------------------------------------------\n  // V8 Sandbox support\n\n  // Transform a SandboxedPointer from/to its encoded form, which is used when\n  // the pointer is stored on the heap and ensures that the pointer will always\n  // point into the sandbox.\n  void DecodeSandboxedPointer(Register value);\n  void LoadSandboxedPointerField(Register destination,\n                                 const MemOperand& field_operand);\n  void StoreSandboxedPointerField(Register value,\n                                  const MemOperand& dst_field_operand);\n\n  // Loads an indirect pointer field.\n  // Only available when the sandbox is enabled, but always visible to avoid\n  // having to place the #ifdefs into the caller.\n  void LoadIndirectPointerField(Register destination, MemOperand field_operand,\n                                IndirectPointerTag tag);\n  // Store an indirect pointer field.\n  // Only available when the sandbox is enabled, but always visible to avoid\n  // having to place the #ifdefs into the caller.\n  void StoreIndirectPointerField(Register value, MemOperand dst_field_operand);\n\n#ifdef V8_ENABLE_SANDBOX\n  // Retrieve the heap object referenced by the given indirect pointer handle,\n  // which can either be a trusted pointer handle or a code pointer handle.\n  void ResolveIndirectPointerHandle(Register destination, Register handle,\n                                    IndirectPointerTag tag);\n\n  // Retrieve the heap object referenced by the given trusted pointer handle.\n  void ResolveTrustedPointerHandle(Register destination, Register handle,\n                                   IndirectPointerTag tag);\n  // Retrieve the Code object referenced by the given code pointer handle.\n  void ResolveCodePointerHandle(Register destination, Register handle);\n\n  // Load the pointer to a Code's entrypoint via a code pointer.\n  // Only available when the sandbox is enabled as it requires the code pointer\n  // table.\n  void LoadCodeEntrypointViaCodePointer(Register destination,\n                                        MemOperand field_operand,\n                                        CodeEntrypointTag tag);\n#endif\n\n  void AtomicDecompressTaggedSigned(Register dst, const MemOperand& src);\n  void AtomicDecompressTagged(Register dst, const MemOperand& src);\n\n  void CmpTagged(const Register& rd, const Register& rs1, const Register& rs2) {\n    if (COMPRESS_POINTERS_BOOL) {\n      Sub32(rd, rs1, rs2);\n    } else {\n      SubWord(rd, rs1, rs2);\n    }\n  }\n#elif V8_TARGET_ARCH_RISCV32\n  // ---------------------------------------------------------------------------\n  // Pointer compression Support\n  // rv32 don't support Pointer compression. Defines these functions for\n  // simplify builtins.\n  inline void LoadTaggedField(const Register& destination,\n                              const MemOperand& field_operand) {\n    Lw(destination, field_operand);\n  }\n  inline void LoadTaggedSignedField(const Register& destination,\n                                    const MemOperand& field_operand) {\n    Lw(destination, field_operand);\n  }\n\n  inline void SmiUntagField(Register dst, const MemOperand& src) {\n    SmiUntag(dst, src);\n  }\n\n  // Compresses and stores tagged value to given on-heap location.\n  void StoreTaggedField(const Register& value,\n                        const MemOperand& dst_field_operand) {\n    Sw(value, dst_field_operand);\n  }\n\n  void AtomicStoreTaggedField(Register src, const MemOperand& dst) {\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.Acquire();\n    AddWord(scratch, dst.rm(), dst.offset());\n    amoswap_w(true, true, zero_reg, src, scratch);\n  }\n#endif\n  // Control-flow integrity:\n\n  // Define a function entrypoint. This doesn't emit any code for this\n  // architecture, as control-flow integrity is not supported for it.\n  void CodeEntry() {}\n  // Define an exception handler.\n  void ExceptionHandler() {}\n  // Define an exception handler and bind a label.\n  void BindExceptionHandler(Label* label) { bind(label); }\n  // Wasm into RVV\n  void WasmRvvExtractLane(Register dst, VRegister src, int8_t idx, VSew sew,\n                          Vlmul lmul) {\n    VU.set(kScratchReg, sew, lmul);\n    VRegister Vsrc = idx != 0 ? kSimd128ScratchReg : src;\n    if (idx != 0) {\n      vslidedown_vi(kSimd128ScratchReg, src, idx);\n    }\n    vmv_xs(dst, Vsrc);\n  }\n\n  void WasmRvvEq(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                 Vlmul lmul);\n  void WasmRvvNe(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                 Vlmul lmul);\n  void WasmRvvGeS(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                  Vlmul lmul);\n  void WasmRvvGeU(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                  Vlmul lmul);\n  void WasmRvvGtS(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                  Vlmul lmul);\n  void WasmRvvGtU(VRegister dst, VRegister lhs, VRegister rhs, VSew sew,\n                  Vlmul lmul);\n\n  void WasmRvvS128const(VRegister dst, const uint8_t imms[16]);\n\n  void LoadLane(int sz, VRegister dst, uint8_t laneidx, MemOperand src);\n  void StoreLane(int sz, VRegister src, uint8_t laneidx, MemOperand dst);\n\n  // It assumes that the arguments are located below the stack pointer.\n  void LoadReceiver(Register dest) { LoadWord(dest, MemOperand(sp, 0)); }\n  void StoreReceiver(Register rec) { StoreWord(rec, MemOperand(sp, 0)); }\n\n  bool IsNear(Label* L, Condition cond, int rs_reg);\n\n  // Swap two registers.  If the scratch register is omitted then a slightly\n  // less efficient form using xor instead of mov is emitted.\n  void Swap(Register reg1, Register reg2, Register scratch = no_reg);\n\n  void PushRoot(RootIndex index) {\n    UseScratchRegisterScope temps(this);\n    Register scratch = temps.Acquire();\n    LoadRoot(scratch, index);\n    Push(scratch);\n  }\n\n  // Compare the object in a register to a value from the root list.\n  void CompareRootAndBranch(const Register& obj, RootIndex index, Condition cc,\n                            Label* target,\n                            ComparisonMode mode = ComparisonMode::kDefault);\n  void CompareTaggedRootAndBranch(const Register& with, RootIndex index,\n                                  Condition cc, Label* target);\n  // Compare the object in a register to a value and jump if they are equal.\n  void JumpIfRoot(Register with, RootIndex index, Label* if_equal,\n                  Label::Distance distance = Label::kFar) {\n    Branch(if_equal, eq, with, index, distance);\n  }\n\n  // Compare the object in a register to a value and jump if they are not equal.\n  void JumpIfNotRoot(Register with, RootIndex index, Label* if_not_equal,\n                     Label::Distance distance = Label::kFar) {\n    Branch(if_not_equal, ne, with, index, distance);\n  }\n\n  // Checks if value is in range [lower_limit, higher_limit] using a single\n  // comparison.\n  void JumpIfIsInRange(Register value, unsigned lower_limit,\n                       unsigned higher_limit, Label* on_in_range);\n  void JumpIfObjectType(Label* target, Condition cc, Register object,\n                        InstanceType instance_type, Register scratch = no_reg);\n  // Fast check if the object is a js receiver type. Assumes only primitive\n  // objects or js receivers are passed.\n  void JumpIfJSAnyIsNotPrimitive(\n      Register heap_object, Register scratch, Label* target,\n      Label::Distance distance = Label::kFar,\n      Condition condition = Condition::kUnsignedGreaterThanEqual);\n  void JumpIfJSAnyIsPrimitive(Register heap_object, Register scratch,\n                              Label* target,\n                              Label::Distance distance = Label::kFar) {\n    return JumpIfJSAnyIsNotPrimitive(heap_object, scratch, target, distance,\n                                     Condition::kUnsignedLessThan);\n  }\n  // ---------------------------------------------------------------------------\n  // GC Support\n\n  // Notify the garbage collector that we wrote a pointer into an object.\n  // |object| is the object being stored into, |value| is the object being\n  // stored.  value and scratch registers are clobbered by the operation.\n  // The offset is the offset from the start of the object, not the offset from\n  // the tagged HeapObject pointer.  For use with FieldOperand(reg, off).\n  void RecordWriteField(\n      Register object, int offset, Register value, RAStatus ra_status,\n      SaveFPRegsMode save_fp, SmiCheck smi_check = SmiCheck::kInline,\n      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());\n\n  // For a given |object| notify the garbage collector that the slot |address|\n  // has been written.  |value| is the object being stored. The value and\n  // address registers are clobbered by the operation.\n  void RecordWrite(\n      Register object, Operand offset, Register value, RAStatus ra_status,\n      SaveFPRegsMode save_fp, SmiCheck smi_check = SmiCheck::kInline,\n      SlotDescriptor slot = SlotDescriptor::ForDirectPointerSlot());\n\n  // void Pref(int32_t hint, const MemOperand& rs);\n\n  // ---------------------------------------------------------------------------\n  // Pseudo-instructions.\n\n  void LoadWordPair(Register rd, const MemOperand& rs);\n  void StoreWordPair(Register rd, const MemOperand& rs);\n\n  void Madd_s(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft);\n  void Madd_d(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft);\n  void Msub_s(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft);\n  void Msub_d(FPURegister fd, FPURegister fr, FPURegister fs, FPURegister ft);\n\n  // Enter exit frame.\n  // argc - argument count to be dropped by LeaveExitFrame.\n  // save_doubles - saves FPU registers on stack.\n  // stack_space - extra stack space.\n  void EnterExitFrame(int stack_space = 0,\n                      StackFrame::Type frame_type = StackFrame::EXIT);\n\n  // Leave the current exit frame.\n  void LeaveExitFrame(Register arg_count, bool do_return = NO_EMIT_RETURN,\n                      bool argument_count_is_length = false);\n\n  // Make sure the stack is aligned. Only emits code in debug mode.\n  void AssertStackIsAligned();\n\n  // Load the global proxy from the current context.\n  void LoadGlobalProxy(Register dst) {\n    LoadNativeContextSlot(dst, Context::GLOBAL_PROXY_INDEX);\n  }\n\n  void LoadNativeContextSlot(Register dst, int index);\n\n  // Load the initial map from the global function. The registers\n  // function and map can be the same, function is then overwritten.\n  void LoadGlobalFunctionInitialMap(Register function, Register map,\n                                    Register scratch);\n\n  // -------------------------------------------------------------------------\n  // JavaScript invokes.\n\n  // Invoke the JavaScript function code by either calling or jumping.\n  void InvokeFunctionCode(Register function, Register new_target,\n                          Register expected_parameter_count,\n                          Register actual_parameter_count, InvokeType type);\n\n  // On function call, call into the debugger if necessary.\n  void CheckDebugHook(Register fun, Register new_target,\n                      Register expected_parameter_count,\n                      Register actual_parameter_count);\n\n  // Invoke the JavaScript function in the given register. Changes the\n  // current context to the context in the function before invoking.\n  void InvokeFunctionWithNewTarget(Register function, Register new_target,\n                                   Register actual_parameter_count,\n                                   InvokeType type);\n  void InvokeFunction(Register function, Register expected_parameter_count,\n                      Register actual_parameter_count, InvokeType type);\n\n  // Exception handling.\n\n  // Push a new stack handler and link into stack handler chain.\n  void PushStackHandler();\n\n  // Unlink the stack handler on top of the stack from the stack handler chain.\n  // Must preserve the result register.\n  void PopStackHandler();\n\n  // Tiering support.\n  void AssertFeedbackCell(Register object,\n                          Register scratch) NOOP_UNLESS_DEBUG_CODE;\n  void AssertFeedbackVector(Register object,\n                            Register scratch) NOOP_UNLESS_DEBUG_CODE;\n  void ReplaceClosureCodeWithOptimizedCode(Register optimized_code,\n                                           Register closure);\n  void GenerateTailCallToReturnedCode(Runtime::FunctionId function_id);\n  void LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing(\n      Register flags, Register feedback_vector, CodeKind current_code_kind,\n      Label* flags_need_processing);\n  void OptimizeCodeOrTailCallOptimizedCodeSlot(Register flags,\n                                               Register feedback_vector);\n\n  // -------------------------------------------------------------------------\n  // Support functions.\n\n  void GetObjectType(Register function, Register map, Register type_reg);\n\n  void GetInstanceTypeRange(Register map, Register type_reg,\n                            InstanceType lower_limit, Register range);\n\n  // -------------------------------------------------------------------------\n  // Runtime calls.\n\n  // Call a runtime routine.\n  void CallRuntime(const Runtime::Function* f, int num_arguments);\n\n  // Convenience function: Same as above, but takes the fid instead.\n  void CallRuntime(Runtime::FunctionId fid) {\n    const Runtime::Function* function = Runtime::FunctionForId(fid);\n    CallRuntime(function, function->nargs);\n  }\n\n  // Convenience function: Same as above, but takes the fid instead.\n  void CallRuntime(Runtime::FunctionId fid, int num_arguments) {\n    CallRuntime(Runtime::FunctionForId(fid), num_arguments);\n  }\n\n  // Convenience function: tail call a runtime routine (jump).\n  void TailCallRuntime(Runtime::FunctionId fid);\n\n  // Jump to the builtin routine.\n  void JumpToExternalReference(const ExternalReference& builtin,\n                               bool builtin_exit_frame = false);\n  // ---------------------------------------------------------------------------\n  // In-place weak references.\n  void LoadWeakValue(Register out, Register in, Label* target_if_cleared);\n\n  // -------------------------------------------------------------------------\n  // StatsCounter support.\n\n  void IncrementCounter(StatsCounter* counter, int value, Register scratch1,\n                        Register scratch2) {\n    if (!v8_flags.native_code_counters) return;\n    EmitIncrementCounter(counter, value, scratch1, scratch2);\n  }\n  void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,\n                            Register scratch2);\n  void DecrementCounter(StatsCounter* counter, int value, Register scratch1,\n                        Register scratch2) {\n    if (!v8_flags.native_code_counters) return;\n    EmitDecrementCounter(counter, value, scratch1, scratch2);\n  }\n  void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,\n                            Register scratch2);\n\n  // -------------------------------------------------------------------------\n  // Stack limit utilities\n\n  enum StackLimitKind { kInterruptStackLimit, kRealStackLimit };\n  void LoadStackLimit(Register destination, StackLimitKind kind);\n  void StackOverflowCheck(Register num_args, Register scratch1,\n                          Register scratch2, Label* stack_overflow,\n                          Label* done = nullptr);\n\n  // Left-shifted from int32 equivalent of Smi.\n  void SmiScale(Register dst, Register src, int scale) {\n#if V8_TARGET_ARCH_RISCV64\n    if (SmiValuesAre32Bits()) {\n      // The int portion is upper 32-bits of 64-bit word.\n      srai(dst, src, (kSmiShift - scale) & 0x3F);\n    } else {\n      DCHECK(SmiValuesAre31Bits());\n      DCHECK_GE(scale, kSmiTagSize);\n      slliw(dst, src, scale - kSmiTagSize);\n    }\n#elif V8_TARGET_ARCH_RISCV32\n    DCHECK(SmiValuesAre31Bits());\n    DCHECK_GE(scale, kSmiTagSize);\n    slli(dst, src, scale - kSmiTagSize);\n#endif\n  }\n\n  // Test if the register contains a smi.\n  inline void SmiTst(Register value, Register scratch) {\n    And(scratch, value, Operand(kSmiTagMask));\n  }\n\n  enum ArgumentsCountMode { kCountIncludesReceiver, kCountExcludesReceiver };\n  enum ArgumentsCountType { kCountIsInteger, kCountIsSmi, kCountIsBytes };\n  void DropArguments(Register count, ArgumentsCountType type,\n                     ArgumentsCountMode mode, Register scratch = no_reg);\n  void DropArgumentsAndPushNewReceiver(Register argc, Register receiver,\n                                       ArgumentsCountType type,\n                                       ArgumentsCountMode mode,\n                                       Register scratch = no_reg);\n  void JumpIfCodeIsMarkedForDeoptimization(Register code, Register scratch,\n                                           Label* if_marked_for_deoptimization);\n  Operand ClearedValue() const;\n\n  // Jump if the register contains a non-smi.\n  void JumpIfNotSmi(Register value, Label* not_smi_label,\n                    Label::Distance dist = Label::kFar);\n  // Abort execution if argument is not a Constructor, enabled via --debug-code.\n  void AssertConstructor(Register object);\n\n  // Abort execution if argument is not a JSFunction, enabled via --debug-code.\n  void AssertFunction(Register object);\n\n  // Abort execution if argument is not a callable JSFunction, enabled via\n  // --debug-code.\n  void AssertCallableFunction(Register object);\n\n  // Abort execution if argument is not a JSBoundFunction,\n  // enabled via --debug-code.\n  void AssertBoundFunction(Register object);\n\n  // Abort execution if argument is not a JSGeneratorObject (or subclass),\n  // enabled via --debug-code.\n  void AssertGeneratorObject(Register object);\n\n  // Like Assert(), but without condition.\n  // Use --debug_code to enable.\n  void AssertUnreachable(AbortReason reason) NOOP_UNLESS_DEBUG_CODE;\n\n  // Abort execution if argument is not undefined or an AllocationSite, enabled\n  // via --debug-code.\n  void AssertUndefinedOrAllocationSite(Register object, Register scratch);\n\n  template <typename Field>\n  void DecodeField(Register dst, Register src) {\n    ExtractBits(dst, src, Field::kShift, Field::kSize);\n  }\n\n  template <typename Field>\n  void DecodeField(Register reg) {\n    DecodeField<Field>(reg, reg);\n  }\n  // Load a protected pointer field.\n  void LoadProtectedPointerField(Register destination,\n                                 MemOperand field_operand);\n\n protected:\n  inline Register GetRtAsRegisterHelper(const Operand& rt, Register scratch);\n  inline int32_t GetOffset(int32_t offset, Label* L, OffsetSize bits);\n\n private:\n  bool has_double_zero_reg_set_ = false;\n  bool has_single_zero_reg_set_ = false;\n  // Performs a truncating conversion of a floating point number as used by\n  // the JS bitwise operations. See ECMA-262 9.5: ToInt32. Goes to 'done' if it\n  // succeeds, otherwise falls through if result is saturated. On return\n  // 'result' either holds answer, or is clobbered on fall through.\n  void TryInlineTruncateDoubleToI(Register result, DoubleRegister input,\n                                  Label* done);\n\n  int CallCFunctionHelper(\n      Register function, int num_reg_arguments, int num_double_arguments,\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes,\n      Label* return_location = nullptr);\n\n  // TODO(RISCV) Reorder parameters so out parameters come last.\n  bool CalculateOffset(Label* L, int32_t* offset, OffsetSize bits);\n  bool CalculateOffset(Label* L, int32_t* offset, OffsetSize bits,\n                       Register* scratch, const Operand& rt);\n\n  void BranchShortHelper(int32_t offset, Label* L);\n  bool BranchShortHelper(int32_t offset, Label* L, Condition cond, Register rs,\n                         const Operand& rt);\n  bool BranchShortCheck(int32_t offset, Label* L, Condition cond, Register rs,\n                        const Operand& rt);\n\n  void BranchAndLinkShortHelper(int32_t offset, Label* L);\n  void BranchAndLinkShort(int32_t offset);\n  void BranchAndLinkShort(Label* L);\n  bool BranchAndLinkShortHelper(int32_t offset, Label* L, Condition cond,\n                                Register rs, const Operand& rt);\n  bool BranchAndLinkShortCheck(int32_t offset, Label* L, Condition cond,\n                               Register rs, const Operand& rt);\n  void BranchAndLinkLong(Label* L);\n#if V8_TARGET_ARCH_RISCV64\n  template <typename F_TYPE>\n  void RoundHelper(FPURegister dst, FPURegister src, FPURegister fpu_scratch,\n                   FPURoundingMode mode);\n#elif V8_TARGET_ARCH_RISCV32\n  void RoundDouble(FPURegister dst, FPURegister src, FPURegister fpu_scratch,\n                   FPURoundingMode mode);\n\n  void RoundFloat(FPURegister dst, FPURegister src, FPURegister fpu_scratch,\n                  FPURoundingMode mode);\n#endif\n  template <typename F>\n  void RoundHelper(VRegister dst, VRegister src, Register scratch,\n                   VRegister v_scratch, FPURoundingMode frm,\n                   bool keep_nan_same = true);\n\n  template <typename TruncFunc>\n  void RoundFloatingPointToInteger(Register rd, FPURegister fs, Register result,\n                                   TruncFunc trunc);\n\n  // Push a fixed frame, consisting of ra, fp.\n  void PushCommonFrame(Register marker_reg = no_reg);\n\n  // Helper functions for generating invokes.\n  void InvokePrologue(Register expected_parameter_count,\n                      Register actual_parameter_count, Label* done,\n                      InvokeType type);\n\n  // Compute memory operands for safepoint stack slots.\n  static int SafepointRegisterStackIndex(int reg_code);\n\n  // Needs access to SafepointRegisterStackIndex for compiled frame\n  // traversal.\n  friend class CommonFrame;\n\n  DISALLOW_IMPLICIT_CONSTRUCTORS(MacroAssembler);\n}", "name_and_para": "class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase "}]]], [["./v8/src/codegen/riscv/reglist-riscv.h", "./v8/src/codegen/arm64/reglist-arm64.h"], -1, -1, []], [["./v8/src/heap/base/asm/riscv/push_registers_asm.cc", "./v8/src/heap/base/asm/arm64/push_registers_asm.cc"], -1, -1, []], [["./v8/src/execution/riscv/simulator-riscv.h", "./v8/src/execution/arm64/simulator-arm64.h"], 0.2, 0.09523809523809523, [[{"name": "Simulator", "content": "class Simulator : public DecoderVisitor, public SimulatorBase {\n public:\n  static void SetRedirectInstruction(Instruction* instruction);\n  static bool ICacheMatch(void* one, void* two) { return false; }\n  static void FlushICache(base::CustomMatcherHashMap* i_cache, void* start,\n                          size_t size) {\n    USE(i_cache);\n    USE(start);\n    USE(size);\n  }\n\n  V8_EXPORT_PRIVATE explicit Simulator(\n      Decoder<DispatchingDecoderVisitor>* decoder, Isolate* isolate = nullptr,\n      FILE* stream = stderr);\n  Simulator();\n  V8_EXPORT_PRIVATE ~Simulator();\n\n  // System functions.\n\n  V8_EXPORT_PRIVATE static Simulator* current(v8::internal::Isolate* isolate);\n\n  // A wrapper class that stores an argument for one of the above Call\n  // functions.\n  //\n  // Only arguments up to 64 bits in size are supported.\n  class CallArgument {\n   public:\n    template <typename T>\n    explicit CallArgument(T argument) {\n      bits_ = 0;\n      DCHECK(sizeof(argument) <= sizeof(bits_));\n      memcpy(&bits_, &argument, sizeof(argument));\n      type_ = X_ARG;\n    }\n\n    explicit CallArgument(double argument) {\n      DCHECK(sizeof(argument) == sizeof(bits_));\n      memcpy(&bits_, &argument, sizeof(argument));\n      type_ = D_ARG;\n    }\n\n    explicit CallArgument(float argument) {\n      // TODO(all): CallArgument(float) is untested, remove this check once\n      //            tested.\n      UNIMPLEMENTED();\n      // Make the D register a NaN to try to trap errors if the callee expects a\n      // double. If it expects a float, the callee should ignore the top word.\n      DCHECK(sizeof(kFP64SignallingNaN) == sizeof(bits_));\n      memcpy(&bits_, &kFP64SignallingNaN, sizeof(kFP64SignallingNaN));\n      // Write the float payload to the S register.\n      DCHECK(sizeof(argument) <= sizeof(bits_));\n      memcpy(&bits_, &argument, sizeof(argument));\n      type_ = D_ARG;\n    }\n\n    // This indicates the end of the arguments list, so that CallArgument\n    // objects can be passed into varargs functions.\n    static CallArgument End() { return CallArgument(); }\n\n    int64_t bits() const { return bits_; }\n    bool IsEnd() const { return type_ == NO_ARG; }\n    bool IsX() const { return type_ == X_ARG; }\n    bool IsD() const { return type_ == D_ARG; }\n\n   private:\n    enum CallArgumentType { X_ARG, D_ARG, NO_ARG };\n\n    // All arguments are aligned to at least 64 bits and we don't support\n    // passing bigger arguments, so the payload size can be fixed at 64 bits.\n    int64_t bits_;\n    CallArgumentType type_;\n\n    CallArgument() { type_ = NO_ARG; }\n  };\n\n  // Call an arbitrary function taking an arbitrary number of arguments.\n  template <typename Return, typename... Args>\n  Return Call(Address entry, Args... args) {\n    // Convert all arguments to CallArgument.\n    CallArgument call_args[] = {CallArgument(args)..., CallArgument::End()};\n    CallImpl(entry, call_args);\n    return ReadReturn<Return>();\n  }\n\n  // Start the debugging command line.\n  void Debug();\n\n  // Executes a single debug command. Takes ownership of the command (so that it\n  // can store it for repeat executions), and returns true if the debugger\n  // should resume execution after this command completes.\n  bool ExecDebugCommand(ArrayUniquePtr<char> command);\n\n  bool GetValue(const char* desc, int64_t* value);\n\n  bool PrintValue(const char* desc);\n\n  // Push an address onto the JS stack.\n  uintptr_t PushAddress(uintptr_t address);\n\n  // Pop an address from the JS stack.\n  uintptr_t PopAddress();\n\n  // Accessor to the internal simulator stack area. Adds a safety\n  // margin to prevent overflows (kAdditionalStackMargin).\n  uintptr_t StackLimit(uintptr_t c_limit) const;\n  // Return current stack view, without additional safety margins.\n  // Users, for example wasm::StackMemory, can add their own.\n  base::Vector<uint8_t> GetCurrentStackView() const;\n\n  V8_EXPORT_PRIVATE void ResetState();\n\n  void DoRuntimeCall(Instruction* instr);\n\n  // Run the simulator.\n  static const Instruction* kEndOfSimAddress;\n  void DecodeInstruction();\n  void Run();\n  V8_EXPORT_PRIVATE void RunFrom(Instruction* start);\n\n  // Simulation helpers.\n  template <typename T>\n  void set_pc(T new_pc) {\n    static_assert(sizeof(T) == sizeof(pc_));\n    memcpy(&pc_, &new_pc, sizeof(T));\n    pc_modified_ = true;\n  }\n  Instruction* pc() { return pc_; }\n\n  void increment_pc() {\n    if (!pc_modified_) {\n      pc_ = pc_->following();\n    }\n\n    pc_modified_ = false;\n  }\n\n  virtual void Decode(Instruction* instr) { decoder_->Decode(instr); }\n\n  // Branch Target Identification (BTI)\n  //\n  // Executing an instruction updates PSTATE.BTYPE, as described in the table\n  // below. Execution of an instruction on a guarded page is allowed if either:\n  // * PSTATE.BTYPE is 00, or\n  // * it is a BTI or PACI[AB]SP instruction that accepts the current value of\n  //   PSTATE.BTYPE (as described in the table below), or\n  // * it is BRK or HLT instruction that causes some higher-priority exception.\n  //\n  //  --------------------------------------------------------------------------\n  //  | Last-executed instruction    | Sets     | Accepted by                  |\n  //  |                              | BTYPE to | BTI | BTI j | BTI c | BTI jc |\n  //  --------------------------------------------------------------------------\n  //  | - BR from an unguarded page. |          |     |       |       |        |\n  //  | - BR from guarded page,      |          |     |       |       |        |\n  //  |   to x16 or x17.             |    01    |     |   X   |   X   |   X    |\n  //  --------------------------------------------------------------------------\n  //  | BR from guarded page,        |          |     |       |       |        |\n  //  | not to x16 or x17.           |    11    |     |   X   |       |   X    |\n  //  --------------------------------------------------------------------------\n  //  | BLR                          |    10    |     |       |   X   |   X    |\n  //  --------------------------------------------------------------------------\n  //  | Any other instruction        |          |     |       |       |        |\n  //  |(including RET).              |    00    |  X  |   X   |   X   |   X    |\n  //  --------------------------------------------------------------------------\n  //\n  // PACI[AB]SP is treated either like \"BTI c\" or \"BTI jc\", according to the\n  // value of SCTLR_EL1.BT0. Details available in ARM DDI 0487E.a, D5-2580.\n\n  enum BType {\n    // Set when executing any instruction, except those cases listed below.\n    DefaultBType = 0,\n\n    // Set when an indirect branch is taken from an unguarded page, or from a\n    // guarded page to ip0 or ip1 (x16 or x17), eg \"br ip0\".\n    BranchFromUnguardedOrToIP = 1,\n\n    // Set when an indirect branch and link (call) is taken, eg. \"blr x0\".\n    BranchAndLink = 2,\n\n    // Set when an indirect branch is taken from a guarded page to a register\n    // that is not ip0 or ip1 (x16 or x17), eg, \"br x0\".\n    BranchFromGuardedNotToIP = 3\n  };\n\n  BType btype() const { return btype_; }\n  void ResetBType() { btype_ = DefaultBType; }\n  void set_btype(BType btype) { btype_ = btype; }\n\n  // Helper function to determine BType for branches.\n  BType GetBTypeFromInstruction(const Instruction* instr) const;\n\n  bool PcIsInGuardedPage() const { return guard_pages_; }\n  void SetGuardedPages(bool guard_pages) { guard_pages_ = guard_pages; }\n\n  void CheckBTypeForPAuth() {\n    DCHECK(pc_->IsPAuth());\n    Instr instr = pc_->Mask(SystemPAuthMask);\n    // Only PACI[AB]SP allowed here, and we only support PACIBSP.\n    CHECK(instr == PACIBSP);\n    // Check BType allows PACI[AB]SP instructions.\n    switch (btype()) {\n      case BranchFromGuardedNotToIP:\n        // This case depends on the value of SCTLR_EL1.BT0, which we assume\n        // here to be set. This makes PACI[AB]SP behave like \"BTI c\",\n        // disallowing its execution when BTYPE is BranchFromGuardedNotToIP\n        // (0b11).\n        FATAL(\"Executing PACIBSP with wrong BType.\");\n      case BranchFromUnguardedOrToIP:\n      case BranchAndLink:\n        break;\n      case DefaultBType:\n        UNREACHABLE();\n    }\n  }\n\n  void CheckBTypeForBti() {\n    DCHECK(pc_->IsBti());\n    switch (pc_->ImmHint()) {\n      case BTI_jc:\n        break;\n      case BTI: {\n        DCHECK(btype() != DefaultBType);\n        FATAL(\"Executing BTI with wrong BType (expected 0, got %d).\", btype());\n        break;\n      }\n      case BTI_c:\n        if (btype() == BranchFromGuardedNotToIP) {\n          FATAL(\"Executing BTI c with wrong BType (3).\");\n        }\n        break;\n      case BTI_j:\n        if (btype() == BranchAndLink) {\n          FATAL(\"Executing BTI j with wrong BType (2).\");\n        }\n        break;\n      default:\n        UNIMPLEMENTED();\n    }\n  }\n\n  void CheckBType() {\n    // On guarded pages, if BType is not zero, take an exception on any\n    // instruction other than BTI, PACI[AB]SP, HLT or BRK.\n    if (PcIsInGuardedPage() && (btype() != DefaultBType)) {\n      if (pc_->IsPAuth()) {\n        CheckBTypeForPAuth();\n      } else if (pc_->IsBti()) {\n        CheckBTypeForBti();\n      } else if (!pc_->IsException()) {\n        FATAL(\"Executing non-BTI instruction with wrong BType.\");\n      }\n    }\n  }\n\n  void ExecuteInstruction() {\n    DCHECK(IsAligned(reinterpret_cast<uintptr_t>(pc_), kInstrSize));\n    CheckBType();\n    ResetBType();\n    CheckBreakNext();\n    Decode(pc_);\n    increment_pc();\n    LogAllWrittenRegisters();\n    CheckBreakpoints();\n  }\n\n// Declare all Visitor functions.\n#define DECLARE(A) void Visit##A(Instruction* instr);\n  VISITOR_LIST(DECLARE)\n#undef DECLARE\n  void VisitNEON3SameFP(NEON3SameOp op, VectorFormat vf, SimVRegister& rd,\n                        SimVRegister& rn, SimVRegister& rm);\n\n  bool IsZeroRegister(unsigned code, Reg31Mode r31mode) const {\n    return ((code == 31) && (r31mode == Reg31IsZeroRegister));\n  }\n\n  // Register accessors.\n  // Return 'size' bits of the value of an integer register, as the specified\n  // type. The value is zero-extended to fill the result.\n  //\n  template <typename T>\n  T reg(unsigned code, Reg31Mode r31mode = Reg31IsZeroRegister) const {\n    DCHECK_LT(code, static_cast<unsigned>(kNumberOfRegisters));\n    if (IsZeroRegister(code, r31mode)) {\n      return 0;\n    }\n    return registers_[code].Get<T>();\n  }\n\n  // Common specialized accessors for the reg() template.\n  int32_t wreg(unsigned code, Reg31Mode r31mode = Reg31IsZeroRegister) const {\n    return reg<int32_t>(code, r31mode);\n  }\n\n  int64_t xreg(unsigned code, Reg31Mode r31mode = Reg31IsZeroRegister) const {\n    return reg<int64_t>(code, r31mode);\n  }\n\n  enum RegLogMode { LogRegWrites, NoRegLog };\n\n  // Write 'value' into an integer register. The value is zero-extended. This\n  // behaviour matches AArch64 register writes.\n  template <typename T>\n  void set_reg(unsigned code, T value,\n               Reg31Mode r31mode = Reg31IsZeroRegister) {\n    set_reg_no_log(code, value, r31mode);\n    LogRegister(code, r31mode);\n  }\n\n  // Common specialized accessors for the set_reg() template.\n  void set_wreg(unsigned code, int32_t value,\n                Reg31Mode r31mode = Reg31IsZeroRegister) {\n    set_reg(code, value, r31mode);\n  }\n\n  void set_xreg(unsigned code, int64_t value,\n                Reg31Mode r31mode = Reg31IsZeroRegister) {\n    set_reg(code, value, r31mode);\n  }\n\n  // As above, but don't automatically log the register update.\n  template <typename T>\n  void set_reg_no_log(unsigned code, T value,\n                      Reg31Mode r31mode = Reg31IsZeroRegister) {\n    DCHECK_LT(code, static_cast<unsigned>(kNumberOfRegisters));\n    if (!IsZeroRegister(code, r31mode)) {\n      registers_[code].Set(value);\n    }\n  }\n\n  void set_wreg_no_log(unsigned code, int32_t value,\n                       Reg31Mode r31mode = Reg31IsZeroRegister) {\n    set_reg_no_log(code, value, r31mode);\n  }\n\n  void set_xreg_no_log(unsigned code, int64_t value,\n                       Reg31Mode r31mode = Reg31IsZeroRegister) {\n    set_reg_no_log(code, value, r31mode);\n  }\n\n  // Commonly-used special cases.\n  template <typename T>\n  void set_lr(T value) {\n    DCHECK_EQ(sizeof(T), static_cast<unsigned>(kSystemPointerSize));\n    set_reg(kLinkRegCode, value);\n  }\n\n  template <typename T>\n  void set_sp(T value) {\n    DCHECK_EQ(sizeof(T), static_cast<unsigned>(kSystemPointerSize));\n    set_reg(31, value, Reg31IsStackPointer);\n  }\n\n  // Vector register accessors.\n  // These are equivalent to the integer register accessors, but for vector\n  // registers.\n\n  // A structure for representing a 128-bit Q register.\n  struct qreg_t {\n    uint8_t val[kQRegSize];\n  };\n\n  // Basic accessor: read the register as the specified type.\n  template <typename T>\n  T vreg(unsigned code) const {\n    static_assert((sizeof(T) == kBRegSize) || (sizeof(T) == kHRegSize) ||\n                      (sizeof(T) == kSRegSize) || (sizeof(T) == kDRegSize) ||\n                      (sizeof(T) == kQRegSize),\n                  \"Template type must match size of register.\");\n    DCHECK_LT(code, static_cast<unsigned>(kNumberOfVRegisters));\n\n    return vregisters_[code].Get<T>();\n  }\n\n  inline SimVRegister& vreg(unsigned code) { return vregisters_[code]; }\n\n  int64_t sp() { return xreg(31, Reg31IsStackPointer); }\n  int64_t fp() { return xreg(kFramePointerRegCode, Reg31IsStackPointer); }\n  Instruction* lr() { return reg<Instruction*>(kLinkRegCode); }\n\n  Address get_sp() const { return reg<Address>(31, Reg31IsStackPointer); }\n\n  // Common specialized accessors for the vreg() template.\n  uint8_t breg(unsigned code) const { return vreg<uint8_t>(code); }\n\n  float hreg(unsigned code) const { return vreg<uint16_t>(code); }\n\n  float sreg(unsigned code) const { return vreg<float>(code); }\n\n  uint32_t sreg_bits(unsigned code) const { return vreg<uint32_t>(code); }\n\n  double dreg(unsigned code) const { return vreg<double>(code); }\n\n  uint64_t dreg_bits(unsigned code) const { return vreg<uint64_t>(code); }\n\n  qreg_t qreg(unsigned code) const { return vreg<qreg_t>(code); }\n\n  // As above, with parameterized size and return type. The value is\n  // either zero-extended or truncated to fit, as required.\n  template <typename T>\n  T vreg(unsigned size, unsigned code) const {\n    uint64_t raw = 0;\n    T result;\n\n    switch (size) {\n      case kSRegSize:\n        raw = vreg<uint32_t>(code);\n        break;\n      case kDRegSize:\n        raw = vreg<uint64_t>(code);\n        break;\n      default:\n        UNREACHABLE();\n    }\n\n    static_assert(sizeof(result) <= sizeof(raw),\n                  \"Template type must be <= 64 bits.\");\n    // Copy the result and truncate to fit. This assumes a little-endian host.\n    memcpy(&result, &raw, sizeof(result));\n    return result;\n  }\n\n  // Write 'value' into a floating-point register. The value is zero-extended.\n  // This behaviour matches AArch64 register writes.\n  template <typename T>\n  void set_vreg(unsigned code, T value, RegLogMode log_mode = LogRegWrites) {\n    static_assert(\n        (sizeof(value) == kBRegSize) || (sizeof(value) == kHRegSize) ||\n            (sizeof(value) == kSRegSize) || (sizeof(value) == kDRegSize) ||\n            (sizeof(value) == kQRegSize),\n        \"Template type must match size of register.\");\n    DCHECK_LT(code, static_cast<unsigned>(kNumberOfVRegisters));\n    vregisters_[code].Set(value);\n\n    if (log_mode == LogRegWrites) {\n      LogVRegister(code, GetPrintRegisterFormat(value));\n    }\n  }\n\n  // Common specialized accessors for the set_vreg() template.\n  void set_breg(unsigned code, int8_t value,\n                RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_hreg(unsigned code, int16_t value,\n                RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_sreg(unsigned code, float value,\n                RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_sreg_bits(unsigned code, uint32_t value,\n                     RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_dreg(unsigned code, double value,\n                RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_dreg_bits(unsigned code, uint64_t value,\n                     RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  void set_qreg(unsigned code, qreg_t value,\n                RegLogMode log_mode = LogRegWrites) {\n    set_vreg(code, value, log_mode);\n  }\n\n  // As above, but don't automatically log the register update.\n  template <typename T>\n  void set_vreg_no_log(unsigned code, T value) {\n    static_assert((sizeof(value) == kBRegSize) ||\n                  (sizeof(value) == kHRegSize) ||\n                  (sizeof(value) == kSRegSize) ||\n                  (sizeof(value) == kDRegSize) || (sizeof(value) == kQRegSize));\n    DCHECK_LT(code, static_cast<unsigned>(kNumberOfVRegisters));\n    vregisters_[code].Set(value);\n  }\n\n  void set_breg_no_log(unsigned code, uint8_t value) {\n    set_vreg_no_log(code, value);\n  }\n\n  void set_hreg_no_log(unsigned code, uint16_t value) {\n    set_vreg_no_log(code, value);\n  }\n\n  void set_sreg_no_log(unsigned code, float value) {\n    set_vreg_no_log(code, value);\n  }\n\n  void set_dreg_no_log(unsigned code, double value) {\n    set_vreg_no_log(code, value);\n  }\n\n  void set_qreg_no_log(unsigned code, qreg_t value) {\n    set_vreg_no_log(code, value);\n  }\n\n  SimSystemRegister& nzcv() { return nzcv_; }\n  SimSystemRegister& fpcr() { return fpcr_; }\n  FPRounding RMode() { return static_cast<FPRounding>(fpcr_.RMode()); }\n  bool DN() { return fpcr_.DN() != 0; }\n\n  // Debug helpers\n\n  // Simulator breakpoints.\n  struct Breakpoint {\n    Instruction* location;\n    bool enabled;\n  };\n  std::vector<Breakpoint> breakpoints_;\n  void SetBreakpoint(Instruction* breakpoint);\n  void ListBreakpoints();\n  void CheckBreakpoints();\n\n  // Helpers for the 'next' command.\n  // When this is set, the Simulator will insert a breakpoint after the next BL\n  // instruction it meets.\n  bool break_on_next_;\n  // Check if the Simulator should insert a break after the current instruction\n  // for the 'next' command.\n  void CheckBreakNext();\n\n  // Disassemble instruction at the given address.\n  void PrintInstructionsAt(Instruction* pc, uint64_t count);\n\n  // Print all registers of the specified types.\n  void PrintRegisters();\n  void PrintVRegisters();\n  void PrintSystemRegisters();\n\n  // As above, but only print the registers that have been updated.\n  void PrintWrittenRegisters();\n  void PrintWrittenVRegisters();\n\n  // As above, but respect LOG_REG and LOG_VREG.\n  void LogWrittenRegisters() {\n    if (log_parameters() & LOG_REGS) PrintWrittenRegisters();\n  }\n  void LogWrittenVRegisters() {\n    if (log_parameters() & LOG_VREGS) PrintWrittenVRegisters();\n  }\n  void LogAllWrittenRegisters() {\n    LogWrittenRegisters();\n    LogWrittenVRegisters();\n  }\n\n  // Specify relevant register formats for Print(V)Register and related helpers.\n  enum PrintRegisterFormat {\n    // The lane size.\n    kPrintRegLaneSizeB = 0 << 0,\n    kPrintRegLaneSizeH = 1 << 0,\n    kPrintRegLaneSizeS = 2 << 0,\n    kPrintRegLaneSizeW = kPrintRegLaneSizeS,\n    kPrintRegLaneSizeD = 3 << 0,\n    kPrintRegLaneSizeX = kPrintRegLaneSizeD,\n    kPrintRegLaneSizeQ = 4 << 0,\n\n    kPrintRegLaneSizeOffset = 0,\n    kPrintRegLaneSizeMask = 7 << 0,\n\n    // The lane count.\n    kPrintRegAsScalar = 0,\n    kPrintRegAsDVector = 1 << 3,\n    kPrintRegAsQVector = 2 << 3,\n\n    kPrintRegAsVectorMask = 3 << 3,\n\n    // Indicate floating-point format lanes. (This flag is only supported for S-\n    // and D-sized lanes.)\n    kPrintRegAsFP = 1 << 5,\n\n    // Supported combinations.\n\n    kPrintXReg = kPrintRegLaneSizeX | kPrintRegAsScalar,\n    kPrintWReg = kPrintRegLaneSizeW | kPrintRegAsScalar,\n    kPrintSReg = kPrintRegLaneSizeS | kPrintRegAsScalar | kPrintRegAsFP,\n    kPrintDReg = kPrintRegLaneSizeD | kPrintRegAsScalar | kPrintRegAsFP,\n\n    kPrintReg1B = kPrintRegLaneSizeB | kPrintRegAsScalar,\n    kPrintReg8B = kPrintRegLaneSizeB | kPrintRegAsDVector,\n    kPrintReg16B = kPrintRegLaneSizeB | kPrintRegAsQVector,\n    kPrintReg1H = kPrintRegLaneSizeH | kPrintRegAsScalar,\n    kPrintReg4H = kPrintRegLaneSizeH | kPrintRegAsDVector,\n    kPrintReg8H = kPrintRegLaneSizeH | kPrintRegAsQVector,\n    kPrintReg1S = kPrintRegLaneSizeS | kPrintRegAsScalar,\n    kPrintReg2S = kPrintRegLaneSizeS | kPrintRegAsDVector,\n    kPrintReg4S = kPrintRegLaneSizeS | kPrintRegAsQVector,\n    kPrintReg1SFP = kPrintRegLaneSizeS | kPrintRegAsScalar | kPrintRegAsFP,\n    kPrintReg2SFP = kPrintRegLaneSizeS | kPrintRegAsDVector | kPrintRegAsFP,\n    kPrintReg4SFP = kPrintRegLaneSizeS | kPrintRegAsQVector | kPrintRegAsFP,\n    kPrintReg1D = kPrintRegLaneSizeD | kPrintRegAsScalar,\n    kPrintReg2D = kPrintRegLaneSizeD | kPrintRegAsQVector,\n    kPrintReg1DFP = kPrintRegLaneSizeD | kPrintRegAsScalar | kPrintRegAsFP,\n    kPrintReg2DFP = kPrintRegLaneSizeD | kPrintRegAsQVector | kPrintRegAsFP,\n    kPrintReg1Q = kPrintRegLaneSizeQ | kPrintRegAsScalar\n  };\n\n  unsigned GetPrintRegLaneSizeInBytesLog2(PrintRegisterFormat format) {\n    return (format & kPrintRegLaneSizeMask) >> kPrintRegLaneSizeOffset;\n  }\n\n  unsigned GetPrintRegLaneSizeInBytes(PrintRegisterFormat format) {\n    return 1 << GetPrintRegLaneSizeInBytesLog2(format);\n  }\n\n  unsigned GetPrintRegSizeInBytesLog2(PrintRegisterFormat format) {\n    if (format & kPrintRegAsDVector) return kDRegSizeLog2;\n    if (format & kPrintRegAsQVector) return kQRegSizeLog2;\n\n    // Scalar types.\n    return GetPrintRegLaneSizeInBytesLog2(format);\n  }\n\n  unsigned GetPrintRegSizeInBytes(PrintRegisterFormat format) {\n    return 1 << GetPrintRegSizeInBytesLog2(format);\n  }\n\n  unsigned GetPrintRegLaneCount(PrintRegisterFormat format) {\n    unsigned reg_size_log2 = GetPrintRegSizeInBytesLog2(format);\n    unsigned lane_size_log2 = GetPrintRegLaneSizeInBytesLog2(format);\n    DCHECK_GE(reg_size_log2, lane_size_log2);\n    return 1 << (reg_size_log2 - lane_size_log2);\n  }\n\n  template <typename T>\n  PrintRegisterFormat GetPrintRegisterFormat(T value) {\n    return GetPrintRegisterFormatForSize(sizeof(value));\n  }\n\n  PrintRegisterFormat GetPrintRegisterFormat(double value) {\n    static_assert(sizeof(value) == kDRegSize,\n                  \"D register must be size of double.\");\n    return GetPrintRegisterFormatForSizeFP(sizeof(value));\n  }\n\n  PrintRegisterFormat GetPrintRegisterFormat(float value) {\n    static_assert(sizeof(value) == kSRegSize,\n                  \"S register must be size of float.\");\n    return GetPrintRegisterFormatForSizeFP(sizeof(value));\n  }\n\n  PrintRegisterFormat GetPrintRegisterFormat(VectorFormat vform);\n  PrintRegisterFormat GetPrintRegisterFormatFP(VectorFormat vform);\n\n  PrintRegisterFormat GetPrintRegisterFormatForSize(size_t reg_size,\n                                                    size_t lane_size);\n\n  PrintRegisterFormat GetPrintRegisterFormatForSize(size_t size) {\n    return GetPrintRegisterFormatForSize(size, size);\n  }\n\n  PrintRegisterFormat GetPrintRegisterFormatForSizeFP(size_t size) {\n    switch (size) {\n      default:\n        UNREACHABLE();\n      case kDRegSize:\n        return kPrintDReg;\n      case kSRegSize:\n        return kPrintSReg;\n    }\n  }\n\n  PrintRegisterFormat GetPrintRegisterFormatTryFP(PrintRegisterFormat format) {\n    if ((GetPrintRegLaneSizeInBytes(format) == kSRegSize) ||\n        (GetPrintRegLaneSizeInBytes(format) == kDRegSize)) {\n      return static_cast<PrintRegisterFormat>(format | kPrintRegAsFP);\n    }\n    return format;\n  }\n\n  // Print individual register values (after update).\n  void PrintRegister(unsigned code, Reg31Mode r31mode = Reg31IsStackPointer);\n  void PrintVRegister(unsigned code, PrintRegisterFormat sizes);\n  void PrintSystemRegister(SystemRegister id);\n\n  // Like Print* (above), but respect log_parameters().\n  void LogRegister(unsigned code, Reg31Mode r31mode = Reg31IsStackPointer) {\n    if (log_parameters() & LOG_REGS) PrintRegister(code, r31mode);\n  }\n  void LogVRegister(unsigned code, PrintRegisterFormat format) {\n    if (log_parameters() & LOG_VREGS) PrintVRegister(code, format);\n  }\n  void LogSystemRegister(SystemRegister id) {\n    if (log_parameters() & LOG_SYS_REGS) PrintSystemRegister(id);\n  }\n\n  // Print memory accesses.\n  void PrintRead(uintptr_t address, unsigned reg_code,\n                 PrintRegisterFormat format);\n  void PrintWrite(uintptr_t address, unsigned reg_code,\n                  PrintRegisterFormat format);\n  void PrintVRead(uintptr_t address, unsigned reg_code,\n                  PrintRegisterFormat format, unsigned lane);\n  void PrintVWrite(uintptr_t address, unsigned reg_code,\n                   PrintRegisterFormat format, unsigned lane);\n\n  // Like Print* (above), but respect log_parameters().\n  void LogRead(uintptr_t address, unsigned reg_code,\n               PrintRegisterFormat format) {\n    if (log_parameters() & LOG_REGS) PrintRead(address, reg_code, format);\n  }\n  void LogWrite(uintptr_t address, unsigned reg_code,\n                PrintRegisterFormat format) {\n    if (log_parameters() & LOG_WRITE) PrintWrite(address, reg_code, format);\n  }\n  void LogVRead(uintptr_t address, unsigned reg_code,\n                PrintRegisterFormat format, unsigned lane = 0) {\n    if (log_parameters() & LOG_VREGS) {\n      PrintVRead(address, reg_code, format, lane);\n    }\n  }\n  void LogVWrite(uintptr_t address, unsigned reg_code,\n                 PrintRegisterFormat format, unsigned lane = 0) {\n    if (log_parameters() & LOG_WRITE) {\n      PrintVWrite(address, reg_code, format, lane);\n    }\n  }\n\n  int log_parameters() { return log_parameters_; }\n  void set_log_parameters(int new_parameters) {\n    log_parameters_ = new_parameters;\n    if (!decoder_) {\n      if (new_parameters & LOG_DISASM) {\n        PrintF(\"Run --debug-sim to dynamically turn on disassembler\\n\");\n      }\n      return;\n    }\n    if (new_parameters & LOG_DISASM) {\n      decoder_->InsertVisitorBefore(print_disasm_, this);\n    } else {\n      decoder_->RemoveVisitor(print_disasm_);\n    }\n  }\n\n  // Helper functions for register tracing.\n  void PrintRegisterRawHelper(unsigned code, Reg31Mode r31mode,\n                              int size_in_bytes = kXRegSize);\n  void PrintVRegisterRawHelper(unsigned code, int bytes = kQRegSize,\n                               int lsb = 0);\n  void PrintVRegisterFPHelper(unsigned code, unsigned lane_size_in_bytes,\n                              int lane_count = 1, int rightmost_lane = 0);\n\n  static inline const char* WRegNameForCode(\n      unsigned code, Reg31Mode mode = Reg31IsZeroRegister);\n  static inline const char* XRegNameForCode(\n      unsigned code, Reg31Mode mode = Reg31IsZeroRegister);\n  static inline const char* SRegNameForCode(unsigned code);\n  static inline const char* DRegNameForCode(unsigned code);\n  static inline const char* VRegNameForCode(unsigned code);\n  static inline int CodeFromName(const char* name);\n\n  enum PointerType { kDataPointer, kInstructionPointer };\n\n  struct PACKey {\n    uint64_t high;\n    uint64_t low;\n    int number;\n  };\n\n  static V8_EXPORT_PRIVATE const PACKey kPACKeyIB;\n\n  // Current implementation is that all pointers are tagged.\n  static bool HasTBI(uint64_t ptr, PointerType type) {\n    USE(ptr, type);\n    return true;\n  }\n\n  // Current implementation uses 48-bit virtual addresses.\n  static int GetBottomPACBit(uint64_t ptr, int ttbr) {\n    USE(ptr, ttbr);\n    DCHECK((ttbr == 0) || (ttbr == 1));\n    return 48;\n  }\n\n  // The top PAC bit is 55 for the purposes of relative bit fields with TBI,\n  // however bit 55 is the TTBR bit regardless of TBI so isn't part of the PAC\n  // codes in pointers.\n  static int GetTopPACBit(uint64_t ptr, PointerType type) {\n    return HasTBI(ptr, type) ? 55 : 63;\n  }\n\n  // Armv8.3 Pointer authentication helpers.\n  V8_EXPORT_PRIVATE static uint64_t CalculatePACMask(uint64_t ptr,\n                                                     PointerType type,\n                                                     int ext_bit);\n  V8_EXPORT_PRIVATE static uint64_t ComputePAC(uint64_t data, uint64_t context,\n                                               PACKey key);\n  V8_EXPORT_PRIVATE static uint64_t AuthPAC(uint64_t ptr, uint64_t context,\n                                            PACKey key, PointerType type);\n  V8_EXPORT_PRIVATE static uint64_t AddPAC(uint64_t ptr, uint64_t context,\n                                           PACKey key, PointerType type);\n  V8_EXPORT_PRIVATE static uint64_t StripPAC(uint64_t ptr, PointerType type);\n\n protected:\n  // Simulation helpers ------------------------------------\n  bool ConditionPassed(Condition cond) {\n    SimSystemRegister& flags = nzcv();\n    switch (cond) {\n      case eq:\n        return flags.Z();\n      case ne:\n        return !flags.Z();\n      case hs:\n        return flags.C();\n      case lo:\n        return !flags.C();\n      case mi:\n        return flags.N();\n      case pl:\n        return !flags.N();\n      case vs:\n        return flags.V();\n      case vc:\n        return !flags.V();\n      case hi:\n        return flags.C() && !flags.Z();\n      case ls:\n        return !(flags.C() && !flags.Z());\n      case ge:\n        return flags.N() == flags.V();\n      case lt:\n        return flags.N() != flags.V();\n      case gt:\n        return !flags.Z() && (flags.N() == flags.V());\n      case le:\n        return !(!flags.Z() && (flags.N() == flags.V()));\n      case nv:  // Fall through.\n      case al:\n        return true;\n      default:\n        UNREACHABLE();\n    }\n  }\n\n  bool ConditionFailed(Condition cond) { return !ConditionPassed(cond); }\n\n  template <typename T>\n  void AddSubHelper(Instruction* instr, T op2);\n  template <typename T>\n  T AddWithCarry(bool set_flags, T left, T right, int carry_in = 0);\n  template <typename T>\n  void AddSubWithCarry(Instruction* instr);\n  template <typename T>\n  void LogicalHelper(Instruction* instr, T op2);\n  template <typename T>\n  void ConditionalCompareHelper(Instruction* instr, T op2);\n  void LoadStoreHelper(Instruction* instr, int64_t offset, AddrMode addrmode);\n  void LoadStorePairHelper(Instruction* instr, AddrMode addrmode);\n  template <typename T>\n  void CompareAndSwapHelper(const Instruction* instr);\n  template <typename T>\n  void CompareAndSwapPairHelper(const Instruction* instr);\n  template <typename T>\n  void AtomicMemorySimpleHelper(const Instruction* instr);\n  template <typename T>\n  void AtomicMemorySwapHelper(const Instruction* instr);\n  uintptr_t LoadStoreAddress(unsigned addr_reg, int64_t offset,\n                             AddrMode addrmode);\n  void LoadStoreWriteBack(unsigned addr_reg, int64_t offset, AddrMode addrmode);\n  void NEONLoadStoreMultiStructHelper(const Instruction* instr,\n                                      AddrMode addr_mode);\n  void NEONLoadStoreSingleStructHelper(const Instruction* instr,\n                                       AddrMode addr_mode);\n  void CheckMemoryAccess(uintptr_t address, uintptr_t stack);\n\n  // \"Probe\" if an address range can be read. This is currently implemented\n  // by doing a 1-byte read of the last accessed byte, since the assumption is\n  // that if the last byte is accessible, also all lower bytes are accessible\n  // (which holds true for Wasm).\n  // Returns true if the access was successful, false if the access raised a\n  // signal which was then handled by the trap handler (also see\n  // {trap_handler::ProbeMemory}). If the access raises a signal which is not\n  // handled by the trap handler (e.g. because the current PC is not registered\n  // as a protected instruction), the signal will propagate and make the process\n  // crash. If no trap handler is available, this always returns true.\n  bool ProbeMemory(uintptr_t address, uintptr_t access_size);\n\n  // Memory read helpers.\n  template <typename T, typename A>\n  T MemoryRead(A address) {\n    T value;\n    static_assert((sizeof(value) == 1) || (sizeof(value) == 2) ||\n                  (sizeof(value) == 4) || (sizeof(value) == 8) ||\n                  (sizeof(value) == 16));\n    memcpy(&value, reinterpret_cast<const void*>(address), sizeof(value));\n    return value;\n  }\n\n  // Memory write helpers.\n  template <typename T, typename A>\n  void MemoryWrite(A address, T value) {\n    static_assert((sizeof(value) == 1) || (sizeof(value) == 2) ||\n                  (sizeof(value) == 4) || (sizeof(value) == 8) ||\n                  (sizeof(value) == 16));\n    memcpy(reinterpret_cast<void*>(address), &value, sizeof(value));\n  }\n\n  template <typename T>\n  T ShiftOperand(T value, Shift shift_type, unsigned amount);\n  template <typename T>\n  T ExtendValue(T value, Extend extend_type, unsigned left_shift = 0);\n  template <typename T>\n  void Extract(Instruction* instr);\n  template <typename T>\n  void DataProcessing2Source(Instruction* instr);\n  template <typename T>\n  void BitfieldHelper(Instruction* instr);\n  uint16_t PolynomialMult(uint8_t op1, uint8_t op2);\n  sim_uint128_t PolynomialMult128(uint64_t op1, uint64_t op2,\n                                  int lane_size_in_bits) const;\n  sim_uint128_t Lsl128(sim_uint128_t x, unsigned shift) const;\n  sim_uint128_t Eor128(sim_uint128_t x, sim_uint128_t y) const;\n\n  void ld1(VectorFormat vform, LogicVRegister dst, uint64_t addr);\n  void ld1(VectorFormat vform, LogicVRegister dst, int index, uint64_t addr);\n  void ld1r(VectorFormat vform, LogicVRegister dst, uint64_t addr);\n  void ld2(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           uint64_t addr);\n  void ld2(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           int index, uint64_t addr);\n  void ld2r(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n            uint64_t addr);\n  void ld3(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           LogicVRegister dst3, uint64_t addr);\n  void ld3(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           LogicVRegister dst3, int index, uint64_t addr);\n  void ld3r(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n            LogicVRegister dst3, uint64_t addr);\n  void ld4(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           LogicVRegister dst3, LogicVRegister dst4, uint64_t addr);\n  void ld4(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n           LogicVRegister dst3, LogicVRegister dst4, int index, uint64_t addr);\n  void ld4r(VectorFormat vform, LogicVRegister dst1, LogicVRegister dst2,\n            LogicVRegister dst3, LogicVRegister dst4, uint64_t addr);\n  void st1(VectorFormat vform, LogicVRegister src, uint64_t addr);\n  void st1(VectorFormat vform, LogicVRegister src, int index, uint64_t addr);\n  void st2(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           uint64_t addr);\n  void st2(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           int index, uint64_t addr);\n  void st3(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           LogicVRegister src3, uint64_t addr);\n  void st3(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           LogicVRegister src3, int index, uint64_t addr);\n  void st4(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           LogicVRegister src3, LogicVRegister src4, uint64_t addr);\n  void st4(VectorFormat vform, LogicVRegister src, LogicVRegister src2,\n           LogicVRegister src3, LogicVRegister src4, int index, uint64_t addr);\n  LogicVRegister cmp(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     Condition cond);\n  LogicVRegister cmp(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, int imm, Condition cond);\n  LogicVRegister cmptst(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister add(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister addp(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister mla(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister mls(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister mul(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister mul(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     int index);\n  LogicVRegister mla(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     int index);\n  LogicVRegister mls(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     int index);\n  LogicVRegister pmul(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n\n  using ByElementOp = LogicVRegister (Simulator::*)(VectorFormat vform,\n                                                    LogicVRegister dst,\n                                                    const LogicVRegister& src1,\n                                                    const LogicVRegister& src2,\n                                                    int index);\n  LogicVRegister fmul(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2,\n                      int index);\n  LogicVRegister fmla(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2,\n                      int index);\n  LogicVRegister fmls(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2,\n                      int index);\n  LogicVRegister fmulx(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister smull(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister smull2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister umull(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister umull2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister smlal(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister smlal2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister umlal(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister umlal2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister smlsl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister smlsl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister umlsl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2,\n                       int index);\n  LogicVRegister umlsl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2,\n                        int index);\n  LogicVRegister sqdmull(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         int index);\n  LogicVRegister sqdmull2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, int index);\n  LogicVRegister sqdmlal(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         int index);\n  LogicVRegister sqdmlal2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, int index);\n  LogicVRegister sqdmlsl(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         int index);\n  LogicVRegister sqdmlsl2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, int index);\n  LogicVRegister sqdmulh(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         int index);\n  LogicVRegister sqrdmulh(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, int index);\n  LogicVRegister sub(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister and_(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister orr(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister orn(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister eor(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister bic(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister bic(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, uint64_t imm);\n  LogicVRegister bif(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister bit(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister bsl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister cls(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister clz(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister cnt(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister not_(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister rbit(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister rev(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, int revSize);\n  LogicVRegister rev16(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister rev32(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister rev64(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister addlp(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, bool is_signed,\n                       bool do_accumulate);\n  LogicVRegister saddlp(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister uaddlp(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister sadalp(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister uadalp(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister ext(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     int index);\n  LogicVRegister ins_element(VectorFormat vform, LogicVRegister dst,\n                             int dst_index, const LogicVRegister& src,\n                             int src_index);\n  LogicVRegister ins_immediate(VectorFormat vform, LogicVRegister dst,\n                               int dst_index, uint64_t imm);\n  LogicVRegister dup_element(VectorFormat vform, LogicVRegister dst,\n                             const LogicVRegister& src, int src_index);\n  LogicVRegister dup_immediate(VectorFormat vform, LogicVRegister dst,\n                               uint64_t imm);\n  LogicVRegister movi(VectorFormat vform, LogicVRegister dst, uint64_t imm);\n  LogicVRegister mvni(VectorFormat vform, LogicVRegister dst, uint64_t imm);\n  LogicVRegister orr(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, uint64_t imm);\n  LogicVRegister sshl(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister ushl(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister SMinMax(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         bool max);\n  LogicVRegister smax(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister smin(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister SMinMaxP(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, bool max);\n  LogicVRegister smaxp(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister sminp(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister addp(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister addv(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister uaddlv(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister saddlv(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister SMinMaxV(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, bool max);\n  LogicVRegister smaxv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister sminv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister uxtl(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister uxtl2(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister sxtl(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister sxtl2(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister Table(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& ind, bool zero_out_of_bounds,\n                       const LogicVRegister* tab1,\n                       const LogicVRegister* tab2 = nullptr,\n                       const LogicVRegister* tab3 = nullptr,\n                       const LogicVRegister* tab4 = nullptr);\n  LogicVRegister tbl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& ind);\n  LogicVRegister tbl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& ind);\n  LogicVRegister tbl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& tab3, const LogicVRegister& ind);\n  LogicVRegister tbl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& tab3, const LogicVRegister& tab4,\n                     const LogicVRegister& ind);\n  LogicVRegister tbx(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& ind);\n  LogicVRegister tbx(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& ind);\n  LogicVRegister tbx(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& tab3, const LogicVRegister& ind);\n  LogicVRegister tbx(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& tab, const LogicVRegister& tab2,\n                     const LogicVRegister& tab3, const LogicVRegister& tab4,\n                     const LogicVRegister& ind);\n  LogicVRegister uaddl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uaddl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uaddw(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uaddw2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister saddl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister saddl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister saddw(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister saddw2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister usubl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister usubl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister usubw(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister usubw2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister ssubl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister ssubl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister ssubw(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister ssubw2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister UMinMax(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         bool max);\n  LogicVRegister umax(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister umin(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister UMinMaxP(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, bool max);\n  LogicVRegister umaxp(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uminp(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister UMinMaxV(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, bool max);\n  LogicVRegister umaxv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister uminv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister trn1(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister trn2(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister zip1(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister zip2(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uzp1(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uzp2(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister shl(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, int shift);\n  LogicVRegister scvtf(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int fbits,\n                       FPRounding rounding_mode);\n  LogicVRegister ucvtf(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int fbits,\n                       FPRounding rounding_mode);\n  LogicVRegister sshll(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister sshll2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister shll(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister shll2(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister ushll(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister ushll2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister sli(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, int shift);\n  LogicVRegister sri(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src, int shift);\n  LogicVRegister sshr(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src, int shift);\n  LogicVRegister ushr(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src, int shift);\n  LogicVRegister ssra(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src, int shift);\n  LogicVRegister usra(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src, int shift);\n  LogicVRegister srsra(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister ursra(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister suqadd(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister usqadd(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister sqshl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister uqshl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister sqshlu(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister abs(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister neg(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister ExtractNarrow(VectorFormat vform, LogicVRegister dst,\n                               bool dstIsSigned, const LogicVRegister& src,\n                               bool srcIsSigned);\n  LogicVRegister xtn(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src);\n  LogicVRegister sqxtn(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister uqxtn(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister sqxtun(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister AbsDiff(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         bool issigned);\n  LogicVRegister saba(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister uaba(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister shrn(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src, int shift);\n  LogicVRegister shrn2(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister rshrn(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, int shift);\n  LogicVRegister rshrn2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister uqshrn(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister uqshrn2(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src, int shift);\n  LogicVRegister uqrshrn(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src, int shift);\n  LogicVRegister uqrshrn2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, int shift);\n  LogicVRegister sqshrn(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, int shift);\n  LogicVRegister sqshrn2(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src, int shift);\n  LogicVRegister sqrshrn(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src, int shift);\n  LogicVRegister sqrshrn2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, int shift);\n  LogicVRegister sqshrun(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src, int shift);\n  LogicVRegister sqshrun2(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, int shift);\n  LogicVRegister sqrshrun(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, int shift);\n  LogicVRegister sqrshrun2(VectorFormat vform, LogicVRegister dst,\n                           const LogicVRegister& src, int shift);\n  LogicVRegister sqrdmulh(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src1,\n                          const LogicVRegister& src2, bool round = true);\n  LogicVRegister dot(VectorFormat vform, LogicVRegister dst,\n                     const LogicVRegister& src1, const LogicVRegister& src2,\n                     bool is_src1_signed, bool is_src2_signed);\n  LogicVRegister sdot(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister sqdmulh(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1,\n                         const LogicVRegister& src2);\n#define NEON_3VREG_LOGIC_LIST(V) \\\n  V(addhn)                       \\\n  V(addhn2)                      \\\n  V(raddhn)                      \\\n  V(raddhn2)                     \\\n  V(subhn)                       \\\n  V(subhn2)                      \\\n  V(rsubhn)                      \\\n  V(rsubhn2)                     \\\n  V(pmull)                       \\\n  V(pmull2)                      \\\n  V(sabal)                       \\\n  V(sabal2)                      \\\n  V(uabal)                       \\\n  V(uabal2)                      \\\n  V(sabdl)                       \\\n  V(sabdl2)                      \\\n  V(uabdl)                       \\\n  V(uabdl2)                      \\\n  V(smull)                       \\\n  V(smull2)                      \\\n  V(umull)                       \\\n  V(umull2)                      \\\n  V(smlal)                       \\\n  V(smlal2)                      \\\n  V(umlal)                       \\\n  V(umlal2)                      \\\n  V(smlsl)                       \\\n  V(smlsl2)                      \\\n  V(umlsl)                       \\\n  V(umlsl2)                      \\\n  V(sqdmlal)                     \\\n  V(sqdmlal2)                    \\\n  V(sqdmlsl)                     \\\n  V(sqdmlsl2)                    \\\n  V(sqdmull)                     \\\n  V(sqdmull2)\n\n#define DEFINE_LOGIC_FUNC(FXN)                               \\\n  LogicVRegister FXN(VectorFormat vform, LogicVRegister dst, \\\n                     const LogicVRegister& src1, const LogicVRegister& src2);\n  NEON_3VREG_LOGIC_LIST(DEFINE_LOGIC_FUNC)\n#undef DEFINE_LOGIC_FUNC\n\n#define NEON_FP3SAME_LIST(V) \\\n  V(fadd, FPAdd, false)      \\\n  V(fsub, FPSub, true)       \\\n  V(fmul, FPMul, true)       \\\n  V(fmulx, FPMulx, true)     \\\n  V(fdiv, FPDiv, true)       \\\n  V(fmax, FPMax, false)      \\\n  V(fmin, FPMin, false)      \\\n  V(fmaxnm, FPMaxNM, false)  \\\n  V(fminnm, FPMinNM, false)\n\n#define DECLARE_NEON_FP_VECTOR_OP(FN, OP, PROCNAN)                           \\\n  template <typename T>                                                      \\\n  LogicVRegister FN(VectorFormat vform, LogicVRegister dst,                  \\\n                    const LogicVRegister& src1, const LogicVRegister& src2); \\\n  LogicVRegister FN(VectorFormat vform, LogicVRegister dst,                  \\\n                    const LogicVRegister& src1, const LogicVRegister& src2);\n  NEON_FP3SAME_LIST(DECLARE_NEON_FP_VECTOR_OP)\n#undef DECLARE_NEON_FP_VECTOR_OP\n\n#define NEON_FPPAIRWISE_LIST(V) \\\n  V(faddp, fadd, FPAdd)         \\\n  V(fmaxp, fmax, FPMax)         \\\n  V(fmaxnmp, fmaxnm, FPMaxNM)   \\\n  V(fminp, fmin, FPMin)         \\\n  V(fminnmp, fminnm, FPMinNM)\n\n#define DECLARE_NEON_FP_PAIR_OP(FNP, FN, OP)                                  \\\n  LogicVRegister FNP(VectorFormat vform, LogicVRegister dst,                  \\\n                     const LogicVRegister& src1, const LogicVRegister& src2); \\\n  LogicVRegister FNP(VectorFormat vform, LogicVRegister dst,                  \\\n                     const LogicVRegister& src);\n  NEON_FPPAIRWISE_LIST(DECLARE_NEON_FP_PAIR_OP)\n#undef DECLARE_NEON_FP_PAIR_OP\n\n  template <typename T>\n  LogicVRegister frecps(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister frecps(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src1, const LogicVRegister& src2);\n  template <typename T>\n  LogicVRegister frsqrts(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1,\n                         const LogicVRegister& src2);\n  LogicVRegister frsqrts(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1,\n                         const LogicVRegister& src2);\n  template <typename T>\n  LogicVRegister fmla(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister fmla(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  template <typename T>\n  LogicVRegister fmls(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister fmls(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister fnmul(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src1, const LogicVRegister& src2);\n\n  template <typename T>\n  LogicVRegister fcmp(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2,\n                      Condition cond);\n  LogicVRegister fcmp(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2,\n                      Condition cond);\n  LogicVRegister fabscmp(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src1, const LogicVRegister& src2,\n                         Condition cond);\n  LogicVRegister fcmp_zero(VectorFormat vform, LogicVRegister dst,\n                           const LogicVRegister& src, Condition cond);\n\n  template <typename T>\n  LogicVRegister fneg(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  LogicVRegister fneg(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src);\n  template <typename T>\n  LogicVRegister frecpx(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister frecpx(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  template <typename T>\n  LogicVRegister fabs_(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fabs_(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fabd(VectorFormat vform, LogicVRegister dst,\n                      const LogicVRegister& src1, const LogicVRegister& src2);\n  LogicVRegister frint(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, FPRounding rounding_mode,\n                       bool inexact_exception = false);\n  LogicVRegister fcvts(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, FPRounding rounding_mode,\n                       int fbits = 0);\n  LogicVRegister fcvtu(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src, FPRounding rounding_mode,\n                       int fbits = 0);\n  LogicVRegister fcvtl(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fcvtl2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister fcvtn(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fcvtn2(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister fcvtxn(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n  LogicVRegister fcvtxn2(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src);\n  LogicVRegister fsqrt(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister frsqrte(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src);\n  LogicVRegister frecpe(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src, FPRounding rounding);\n  LogicVRegister ursqrte(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src);\n  LogicVRegister urecpe(VectorFormat vform, LogicVRegister dst,\n                        const LogicVRegister& src);\n\n  using FPMinMaxOp = float (Simulator::*)(float a, float b);\n\n  LogicVRegister FMinMaxV(VectorFormat vform, LogicVRegister dst,\n                          const LogicVRegister& src, FPMinMaxOp Op);\n\n  LogicVRegister fminv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fmaxv(VectorFormat vform, LogicVRegister dst,\n                       const LogicVRegister& src);\n  LogicVRegister fminnmv(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src);\n  LogicVRegister fmaxnmv(VectorFormat vform, LogicVRegister dst,\n                         const LogicVRegister& src);\n\n  template <typename T>\n  T FPRecipSqrtEstimate(T op);\n  template <typename T>\n  T FPRecipEstimate(T op, FPRounding rounding);\n  template <typename T, typename R>\n  R FPToFixed(T op, int fbits, bool is_signed, FPRounding rounding);\n\n  void FPCompare(double val0, double val1);\n  double FPRoundInt(double value, FPRounding round_mode);\n  double FPToDouble(float value);\n  float FPToFloat(double value, FPRounding round_mode);\n  float FPToFloat(float16 value);\n  float16 FPToFloat16(float value, FPRounding round_mode);\n  float16 FPToFloat16(double value, FPRounding round_mode);\n  double recip_sqrt_estimate(double a);\n  double recip_estimate(double a);\n  double FPRecipSqrtEstimate(double a);\n  double FPRecipEstimate(double a);\n  double FixedToDouble(int64_t src, int fbits, FPRounding round_mode);\n  double UFixedToDouble(uint64_t src, int fbits, FPRounding round_mode);\n  float FixedToFloat(int64_t src, int fbits, FPRounding round_mode);\n  float UFixedToFloat(uint64_t src, int fbits, FPRounding round_mode);\n  float16 FixedToFloat16(int64_t src, int fbits, FPRounding round_mode);\n  float16 UFixedToFloat16(uint64_t src, int fbits, FPRounding round_mode);\n  int16_t FPToInt16(double value, FPRounding rmode);\n  int32_t FPToInt32(double value, FPRounding rmode);\n  int64_t FPToInt64(double value, FPRounding rmode);\n  uint16_t FPToUInt16(double value, FPRounding rmode);\n  uint32_t FPToUInt32(double value, FPRounding rmode);\n  uint64_t FPToUInt64(double value, FPRounding rmode);\n  int32_t FPToFixedJS(double value);\n\n  template <typename T>\n  T FPAdd(T op1, T op2);\n\n  template <typename T>\n  T FPDiv(T op1, T op2);\n\n  template <typename T>\n  T FPMax(T a, T b);\n\n  template <typename T>\n  T FPMaxNM(T a, T b);\n\n  template <typename T>\n  T FPMin(T a, T b);\n\n  template <typename T>\n  T FPMinNM(T a, T b);\n\n  template <typename T>\n  T FPMul(T op1, T op2);\n\n  template <typename T>\n  T FPMulx(T op1, T op2);\n\n  template <typename T>\n  T FPMulAdd(T a, T op1, T op2);\n\n  template <typename T>\n  T FPSqrt(T op);\n\n  template <typename T>\n  T FPSub(T op1, T op2);\n\n  template <typename T>\n  T FPRecipStepFused(T op1, T op2);\n\n  template <typename T>\n  T FPRSqrtStepFused(T op1, T op2);\n\n  // This doesn't do anything at the moment. We'll need it if we want support\n  // for cumulative exception bits or floating-point exceptions.\n  void FPProcessException() {}\n\n  // Standard NaN processing.\n  bool FPProcessNaNs(Instruction* instr);\n\n  void CheckStackAlignment();\n\n  inline void CheckPCSComplianceAndRun();\n\n#ifdef DEBUG\n  // Corruption values should have their least significant byte cleared to\n  // allow the code of the register being corrupted to be inserted.\n  static const uint64_t kCallerSavedRegisterCorruptionValue =\n      0xca11edc0de000000UL;\n  // This value is a NaN in both 32-bit and 64-bit FP.\n  static const uint64_t kCallerSavedVRegisterCorruptionValue =\n      0x7ff000007f801000UL;\n  // This value is a mix of 32/64-bits NaN and \"verbose\" immediate.\n  static const uint64_t kDefaultCPURegisterCorruptionValue =\n      0x7ffbad007f8bad00UL;\n\n  void CorruptRegisters(CPURegList* list,\n                        uint64_t value = kDefaultCPURegisterCorruptionValue);\n  void CorruptAllCallerSavedCPURegisters();\n#endif\n\n  // Pseudo Printf instruction\n  void DoPrintf(Instruction* instr);\n\n  // Pseudo instruction for switching stack limit\n  void DoSwitchStackLimit(Instruction* instr);\n\n  // Processor state ---------------------------------------\n\n  // Output stream.\n  FILE* stream_;\n  PrintDisassembler* print_disasm_;\n  void PRINTF_FORMAT(2, 3) TraceSim(const char* format, ...);\n\n  // General purpose registers. Register 31 is the stack pointer.\n  SimRegister registers_[kNumberOfRegisters];\n\n  // Floating point registers\n  SimVRegister vregisters_[kNumberOfVRegisters];\n\n  // Processor state\n  // bits[31, 27]: Condition flags N, Z, C, and V.\n  //               (Negative, Zero, Carry, Overflow)\n  SimSystemRegister nzcv_;\n\n  // Floating-Point Control Register\n  SimSystemRegister fpcr_;\n\n  // Only a subset of FPCR features are supported by the simulator. This helper\n  // checks that the FPCR settings are supported.\n  //\n  // This is checked when floating-point instructions are executed, not when\n  // FPCR is set. This allows generated code to modify FPCR for external\n  // functions, or to save and restore it when entering and leaving generated\n  // code.\n  void AssertSupportedFPCR() {\n    DCHECK_EQ(fpcr().FZ(), 0);            // No flush-to-zero support.\n    DCHECK(fpcr().RMode() == FPTieEven);  // Ties-to-even rounding only.\n\n    // The simulator does not support half-precision operations so fpcr().AHP()\n    // is irrelevant, and is not checked here.\n  }\n\n  template <typename T>\n  static int CalcNFlag(T result) {\n    return (result >> (sizeof(T) * 8 - 1)) & 1;\n  }\n\n  static int CalcZFlag(uint64_t result) { return result == 0; }\n\n  static const uint32_t kConditionFlagsMask = 0xf0000000;\n\n  // Stack\n  uintptr_t stack_;\n  static const size_t kStackProtectionSize = KB;\n  // This includes a protection margin at each end of the stack area.\n  static size_t AllocatedStackSize() {\n    return (v8_flags.sim_stack_size * KB) + (2 * kStackProtectionSize);\n  }\n  static size_t UsableStackSize() { return v8_flags.sim_stack_size * KB; }\n  uintptr_t stack_limit_;\n  // Added in Simulator::StackLimit()\n  static const int kAdditionalStackMargin = 4 * KB;\n\n  Decoder<DispatchingDecoderVisitor>* decoder_;\n  Decoder<DispatchingDecoderVisitor>* disassembler_decoder_;\n\n  // Indicates if the pc has been modified by the instruction and should not be\n  // automatically incremented.\n  bool pc_modified_;\n  Instruction* pc_;\n\n  // Branch type register, used for branch target identification.\n  BType btype_;\n\n  // Global flag for enabling guarded pages.\n  // TODO(arm64): implement guarding at page granularity, rather than globally.\n  bool guard_pages_;\n\n  static const char* xreg_names[];\n  static const char* wreg_names[];\n  static const char* sreg_names[];\n  static const char* dreg_names[];\n  static const char* vreg_names[];\n\n  // Debugger input.\n  void set_last_debugger_input(ArrayUniquePtr<char> input) {\n    last_debugger_input_ = std::move(input);\n  }\n  const char* last_debugger_input() { return last_debugger_input_.get(); }\n  ArrayUniquePtr<char> last_debugger_input_;\n\n  // Synchronization primitives. See ARM DDI 0487A.a, B2.10. Pair types not\n  // implemented.\n  enum class MonitorAccess {\n    Open,\n    Exclusive,\n  };\n\n  enum class TransactionSize {\n    None = 0,\n    Byte = 1,\n    HalfWord = 2,\n    Word = 4,\n    DoubleWord = 8,\n  };\n\n  TransactionSize get_transaction_size(unsigned size);\n\n  // The least-significant bits of the address are ignored. The number of bits\n  // is implementation-defined, between 3 and 11. See ARM DDI 0487A.a, B2.10.3.\n  static const uintptr_t kExclusiveTaggedAddrMask = ~((1 << 11) - 1);\n\n  class LocalMonitor {\n   public:\n    LocalMonitor();\n\n    // These functions manage the state machine for the local monitor, but do\n    // not actually perform loads and stores. NotifyStoreExcl only returns\n    // true if the exclusive store is allowed; the global monitor will still\n    // have to be checked to see whether the memory should be updated.\n    void NotifyLoad();\n    void NotifyLoadExcl(uintptr_t addr, TransactionSize size);\n    void NotifyStore();\n    bool NotifyStoreExcl(uintptr_t addr, TransactionSize size);\n\n   private:\n    void Clear();\n\n    MonitorAccess access_state_;\n    uintptr_t tagged_addr_;\n    TransactionSize size_;\n  };\n\n  class GlobalMonitor {\n   public:\n    class Processor {\n     public:\n      Processor();\n\n     private:\n      friend class GlobalMonitor;\n      // These functions manage the state machine for the global monitor, but do\n      // not actually perform loads and stores.\n      void Clear_Locked();\n      void NotifyLoadExcl_Locked(uintptr_t addr);\n      void NotifyStore_Locked(bool is_requesting_processor);\n      bool NotifyStoreExcl_Locked(uintptr_t addr, bool is_requesting_processor);\n\n      MonitorAccess access_state_;\n      uintptr_t tagged_addr_;\n      Processor* next_;\n      Processor* prev_;\n      // A stxr can fail due to background cache evictions. Rather than\n      // simulating this, we'll just occasionally introduce cases where an\n      // exclusive store fails. This will happen once after every\n      // kMaxFailureCounter exclusive stores.\n      static const int kMaxFailureCounter = 5;\n      int failure_counter_;\n    };\n\n    // Exposed so it can be accessed by Simulator::{Read,Write}Ex*.\n    base::Mutex mutex;\n\n    void NotifyLoadExcl_Locked(uintptr_t addr, Processor* processor);\n    void NotifyStore_Locked(Processor* processor);\n    bool NotifyStoreExcl_Locked(uintptr_t addr, Processor* processor);\n\n    // Called when the simulator is destroyed.\n    void RemoveProcessor(Processor* processor);\n\n    static GlobalMonitor* Get();\n\n   private:\n    // Private constructor. Call {GlobalMonitor::Get()} to get the singleton.\n    GlobalMonitor() = default;\n    friend class base::LeakyObject<GlobalMonitor>;\n\n    bool IsProcessorInLinkedList_Locked(Processor* processor) const;\n    void PrependProcessor_Locked(Processor* processor);\n\n    Processor* head_ = nullptr;\n  };\n\n  LocalMonitor local_monitor_;\n  GlobalMonitor::Processor global_monitor_processor_;\n\n private:\n  void Init(FILE* stream);\n\n  V8_EXPORT_PRIVATE void CallImpl(Address entry, CallArgument* args);\n\n  void CallAnyCTypeFunction(Address target_address,\n                            const EncodedCSignature& signature);\n\n  // Read floating point return values.\n  template <typename T>\n  typename std::enable_if<std::is_floating_point<T>::value, T>::type\n  ReadReturn() {\n    return static_cast<T>(dreg(0));\n  }\n  // Read non-float return values.\n  template <typename T>\n  typename std::enable_if<!std::is_floating_point<T>::value, T>::type\n  ReadReturn() {\n    return ConvertReturn<T>(xreg(0));\n  }\n\n  template <typename T>\n  static T FPDefaultNaN();\n\n  template <typename T>\n  T FPProcessNaN(T op) {\n    DCHECK(std::isnan(op));\n    return fpcr().DN() ? FPDefaultNaN<T>() : ToQuietNaN(op);\n  }\n\n  template <typename T>\n  T FPProcessNaNs(T op1, T op2) {\n    if (IsSignallingNaN(op1)) {\n      return FPProcessNaN(op1);\n    } else if (IsSignallingNaN(op2)) {\n      return FPProcessNaN(op2);\n    } else if (std::isnan(op1)) {\n      DCHECK(IsQuietNaN(op1));\n      return FPProcessNaN(op1);\n    } else if (std::isnan(op2)) {\n      DCHECK(IsQuietNaN(op2));\n      return FPProcessNaN(op2);\n    } else {\n      return 0.0;\n    }\n  }\n\n  template <typename T>\n  T FPProcessNaNs3(T op1, T op2, T op3) {\n    if (IsSignallingNaN(op1)) {\n      return FPProcessNaN(op1);\n    } else if (IsSignallingNaN(op2)) {\n      return FPProcessNaN(op2);\n    } else if (IsSignallingNaN(op3)) {\n      return FPProcessNaN(op3);\n    } else if (std::isnan(op1)) {\n      DCHECK(IsQuietNaN(op1));\n      return FPProcessNaN(op1);\n    } else if (std::isnan(op2)) {\n      DCHECK(IsQuietNaN(op2));\n      return FPProcessNaN(op2);\n    } else if (std::isnan(op3)) {\n      DCHECK(IsQuietNaN(op3));\n      return FPProcessNaN(op3);\n    } else {\n      return 0.0;\n    }\n  }\n\n  int log_parameters_;\n  // Instruction counter only valid if v8_flags.stop_sim_at isn't 0.\n  int icount_for_stop_sim_at_;\n  Isolate* isolate_;\n}", "name_and_para": ""}, {"name": "Simulator", "content": "class Simulator : public SimulatorBase {\n public:\n  friend class RiscvDebugger;\n\n  // Registers are declared in order. See SMRL chapter 2.\n  enum Register {\n    no_reg = -1,\n    zero_reg = 0,\n    ra,\n    sp,\n    gp,\n    tp,\n    t0,\n    t1,\n    t2,\n    s0,\n    s1,\n    a0,\n    a1,\n    a2,\n    a3,\n    a4,\n    a5,\n    a6,\n    a7,\n    s2,\n    s3,\n    s4,\n    s5,\n    s6,\n    s7,\n    s8,\n    s9,\n    s10,\n    s11,\n    t3,\n    t4,\n    t5,\n    t6,\n    pc,  // pc must be the last register.\n    kNumSimuRegisters,\n    // aliases\n    fp = s0\n  };\n\n  // Coprocessor registers.\n  // Generated code will always use doubles. So we will only use even registers.\n  enum FPURegister {\n    ft0,\n    ft1,\n    ft2,\n    ft3,\n    ft4,\n    ft5,\n    ft6,\n    ft7,\n    fs0,\n    fs1,\n    fa0,\n    fa1,\n    fa2,\n    fa3,\n    fa4,\n    fa5,\n    fa6,\n    fa7,\n    fs2,\n    fs3,\n    fs4,\n    fs5,\n    fs6,\n    fs7,\n    fs8,\n    fs9,\n    fs10,\n    fs11,\n    ft8,\n    ft9,\n    ft10,\n    ft11,\n    kNumFPURegisters\n  };\n\n  enum VRegister {\n    v0,\n    v1,\n    v2,\n    v3,\n    v4,\n    v5,\n    v6,\n    v7,\n    v8,\n    v9,\n    v10,\n    v11,\n    v12,\n    v13,\n    v14,\n    v15,\n    v16,\n    v17,\n    v18,\n    v19,\n    v20,\n    v21,\n    v22,\n    v23,\n    v24,\n    v25,\n    v26,\n    v27,\n    v28,\n    v29,\n    v30,\n    v31,\n    kNumVRegisters\n  };\n\n  explicit Simulator(Isolate* isolate);\n  ~Simulator();\n\n  // The currently executing Simulator instance. Potentially there can be one\n  // for each native thread.\n  V8_EXPORT_PRIVATE static Simulator* current(v8::internal::Isolate* isolate);\n\n  // Accessors for register state. Reading the pc value adheres to the RISC-V\n  // architecture specification and is off by a 8 from the currently executing\n  // instruction.\n  void set_register(int reg, sreg_t value);\n  void set_register_word(int reg, int32_t value);\n  V8_EXPORT_PRIVATE sreg_t get_register(int reg) const;\n  double get_double_from_register_pair(int reg);\n\n  // Same for FPURegisters.\n  void set_fpu_register(int fpureg, int64_t value);\n  void set_fpu_register_word(int fpureg, int32_t value);\n  void set_fpu_register_hi_word(int fpureg, int32_t value);\n  void set_fpu_register_float(int fpureg, float value);\n  void set_fpu_register_float(int fpureg, Float32 value);\n  void set_fpu_register_double(int fpureg, double value);\n  void set_fpu_register_double(int fpureg, Float64 value);\n\n  int64_t get_fpu_register(int fpureg) const;\n  int32_t get_fpu_register_word(int fpureg) const;\n  int32_t get_fpu_register_signed_word(int fpureg) const;\n  int32_t get_fpu_register_hi_word(int fpureg) const;\n  float get_fpu_register_float(int fpureg) const;\n  Float32 get_fpu_register_Float32(int fpureg, bool check_nanbox = true) const;\n  double get_fpu_register_double(int fpureg) const;\n  Float64 get_fpu_register_Float64(int fpureg) const;\n\n  // RV CSR manipulation\n  uint32_t read_csr_value(uint32_t csr);\n  void write_csr_value(uint32_t csr, reg_t value);\n  void set_csr_bits(uint32_t csr, reg_t flags);\n  void clear_csr_bits(uint32_t csr, reg_t flags);\n\n  void set_fflags(uint32_t flags) { set_csr_bits(csr_fflags, flags); }\n  void clear_fflags(int32_t flags) { clear_csr_bits(csr_fflags, flags); }\n\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  // RVV CSR\n  __int128_t get_vregister(int vreg) const;\n  inline uint64_t rvv_vlen() const { return kRvvVLEN; }\n  inline uint64_t rvv_vtype() const { return vtype_; }\n  inline uint64_t rvv_vl() const { return vl_; }\n  inline uint64_t rvv_vstart() const { return vstart_; }\n  inline uint64_t rvv_vxsat() const { return vxsat_; }\n  inline uint64_t rvv_vxrm() const { return vxrm_; }\n  inline uint64_t rvv_vcsr() const { return vcsr_; }\n  inline uint64_t rvv_vlenb() const { return vlenb_; }\n  inline uint32_t rvv_zimm() const { return instr_.Rvvzimm(); }\n  inline uint32_t rvv_vlmul() const { return (rvv_vtype() & 0x7); }\n  inline float rvv_vflmul() const {\n    if ((rvv_vtype() & 0b100) == 0) {\n      return static_cast<float>(0x1 << (rvv_vtype() & 0x7));\n    } else {\n      return 1.0 / static_cast<float>(0x1 << (4 - rvv_vtype() & 0x3));\n    }\n  }\n  inline uint32_t rvv_vsew() const { return ((rvv_vtype() >> 3) & 0x7); }\n\n  inline const char* rvv_sew_s() const {\n    uint32_t vsew = rvv_vsew();\n    switch (vsew) {\n#define CAST_VSEW(name) \\\n  case name:            \\\n    return #name;\n      RVV_SEW(CAST_VSEW)\n      default:\n        return \"unknown\";\n#undef CAST_VSEW\n    }\n  }\n\n  inline const char* rvv_lmul_s() const {\n    uint32_t vlmul = rvv_vlmul();\n    switch (vlmul) {\n#define CAST_VLMUL(name) \\\n  case name:             \\\n    return #name;\n      RVV_LMUL(CAST_VLMUL)\n      default:\n        return \"unknown\";\n#undef CAST_VLMUL\n    }\n  }\n\n  // return size of lane.8 16 32 64\n  inline uint32_t rvv_sew() const {\n    DCHECK_EQ(rvv_vsew() & (~0x7), 0x0);\n    return (0x1 << rvv_vsew()) * 8;\n  }\n  inline uint64_t rvv_vlmax() const {\n    if ((rvv_vlmul() & 0b100) != 0) {\n      return (rvv_vlen() / rvv_sew()) >> (4 - (rvv_vlmul() & 0b11));\n    } else {\n      return ((rvv_vlen() << rvv_vlmul()) / rvv_sew());\n    }\n  }\n#endif\n\n  inline uint32_t get_dynamic_rounding_mode();\n  inline bool test_fflags_bits(uint32_t mask);\n\n  float RoundF2FHelper(float input_val, int rmode);\n  double RoundF2FHelper(double input_val, int rmode);\n  template <typename I_TYPE, typename F_TYPE>\n  I_TYPE RoundF2IHelper(F_TYPE original, int rmode);\n\n  template <typename T>\n  T FMaxMinHelper(T a, T b, MaxMinKind kind);\n\n  template <typename T>\n  bool CompareFHelper(T input1, T input2, FPUCondition cc);\n\n  // Special case of set_register and get_register to access the raw PC value.\n  void set_pc(sreg_t value);\n  V8_EXPORT_PRIVATE sreg_t get_pc() const;\n\n  Address get_sp() const { return static_cast<Address>(get_register(sp)); }\n\n  // Accessor to the internal simulator stack area. Adds a safety\n  // margin to prevent overflows (kAdditionalStackMargin).\n  uintptr_t StackLimit(uintptr_t c_limit) const;\n  // Return current stack view, without additional safety margins.\n  // Users, for example wasm::StackMemory, can add their own.\n  base::Vector<uint8_t> GetCurrentStackView() const;\n\n  // Executes RISC-V instructions until the PC reaches end_sim_pc.\n  void Execute();\n\n  // Only arguments up to 64 bits in size are supported.\n  class CallArgument {\n   public:\n    template <typename T>\n    explicit CallArgument(T argument) {\n      bits_ = 0;\n      DCHECK(sizeof(argument) <= sizeof(bits_));\n      bits_ = ConvertArg(argument);\n      type_ = GP_ARG;\n    }\n    explicit CallArgument(double argument) {\n      DCHECK(sizeof(argument) == sizeof(bits_));\n      memcpy(&bits_, &argument, sizeof(argument));\n      type_ = FP_ARG;\n    }\n    explicit CallArgument(float argument) {\n      // TODO(all): CallArgument(float) is untested.\n      UNIMPLEMENTED();\n    }\n    // This indicates the end of the arguments list, so that CallArgument\n    // objects can be passed into varargs functions.\n    static CallArgument End() { return CallArgument(); }\n    int64_t bits() const { return bits_; }\n    bool IsEnd() const { return type_ == NO_ARG; }\n    bool IsGP() const { return type_ == GP_ARG; }\n    bool IsFP() const { return type_ == FP_ARG; }\n\n   private:\n    enum CallArgumentType { GP_ARG, FP_ARG, NO_ARG };\n    // All arguments are aligned to at least 64 bits and we don't support\n    // passing bigger arguments, so the payload size can be fixed at 64 bits.\n    int64_t bits_;\n    CallArgumentType type_;\n    CallArgument() { type_ = NO_ARG; }\n  };\n\n  template <typename Return, typename... Args>\n  Return Call(Address entry, Args... args) {\n#ifdef V8_TARGET_ARCH_RISCV64\n    // Convert all arguments to CallArgument.\n    CallArgument call_args[] = {CallArgument(args)..., CallArgument::End()};\n    CallImpl(entry, call_args);\n    return ReadReturn<Return>();\n#else\n    return VariadicCall<Return>(this, &Simulator::CallImpl, entry, args...);\n#endif\n  }\n  // Alternative: call a 2-argument double function.\n  double CallFP(Address entry, double d0, double d1);\n\n  // Push an address onto the JS stack.\n  uintptr_t PushAddress(uintptr_t address);\n\n  // Pop an address from the JS stack.\n  uintptr_t PopAddress();\n\n  // Debugger input.\n  void set_last_debugger_input(char* input);\n  char* last_debugger_input() { return last_debugger_input_; }\n\n  // Redirection support.\n  static void SetRedirectInstruction(Instruction* instruction);\n\n  // ICache checking.\n  static bool ICacheMatch(void* one, void* two);\n  static void FlushICache(base::CustomMatcherHashMap* i_cache, void* start,\n                          size_t size);\n\n  // Returns true if pc register contains one of the 'special_values' defined\n  // below (bad_ra, end_sim_pc).\n  bool has_bad_pc() const;\n\n private:\n  enum special_values {\n    // Known bad pc value to ensure that the simulator does not execute\n    // without being properly setup.\n    bad_ra = -1,\n    // A pc value used to signal the simulator to stop execution.  Generally\n    // the ra is set to this value on transition from native C code to\n    // simulated execution, so that the simulator can \"return\" to the native\n    // C code.\n    end_sim_pc = -2,\n    // Unpredictable value.\n    Unpredictable = 0xbadbeaf\n  };\n\n#ifdef V8_TARGET_ARCH_RISCV64\n  V8_EXPORT_PRIVATE void CallImpl(Address entry, CallArgument* args);\n  void CallAnyCTypeFunction(Address target_address,\n                            const EncodedCSignature& signature);\n  // Read floating point return values.\n  template <typename T>\n  typename std::enable_if<std::is_floating_point<T>::value, T>::type\n  ReadReturn() {\n    return static_cast<T>(get_fpu_register_double(fa0));\n  }\n  // Read non-float return values.\n  template <typename T>\n  typename std::enable_if<!std::is_floating_point<T>::value, T>::type\n  ReadReturn() {\n    return ConvertReturn<T>(get_register(a0));\n  }\n#else\n  V8_EXPORT_PRIVATE intptr_t CallImpl(Address entry, int argument_count,\n                                      const intptr_t* arguments);\n#endif\n  // Unsupported instructions use Format to print an error and stop execution.\n  void Format(Instruction* instr, const char* format);\n\n  // Helpers for data value tracing.\n  enum TraceType {\n    BYTE,\n    HALF,\n    WORD,\n#if V8_TARGET_ARCH_RISCV64\n    DWORD,\n#endif\n    FLOAT,\n    DOUBLE,\n    // FLOAT_DOUBLE,\n    // WORD_DWORD\n  };\n\n  // \"Probe\" if an address range can be read. This is currently implemented\n  // by doing a 1-byte read of the last accessed byte, since the assumption is\n  // that if the last byte is accessible, also all lower bytes are accessible\n  // (which holds true for Wasm).\n  // Returns true if the access was successful, false if the access raised a\n  // signal which was then handled by the trap handler (also see\n  // {trap_handler::ProbeMemory}). If the access raises a signal which is not\n  // handled by the trap handler (e.g. because the current PC is not registered\n  // as a protected instruction), the signal will propagate and make the process\n  // crash. If no trap handler is available, this always returns true.\n  bool ProbeMemory(uintptr_t address, uintptr_t access_size);\n\n  // RISCV Memory read/write methods\n  template <typename T>\n  T ReadMem(sreg_t addr, Instruction* instr);\n  template <typename T>\n  void WriteMem(sreg_t addr, T value, Instruction* instr);\n  template <typename T, typename OP>\n  T amo(sreg_t addr, OP f, Instruction* instr, TraceType t) {\n    auto lhs = ReadMem<T>(addr, instr);\n    // TODO(RISCV): trace memory read for AMO\n    WriteMem<T>(addr, (T)f(lhs), instr);\n    return lhs;\n  }\n\n  // Helper for debugging memory access.\n  inline void DieOrDebug();\n\n#if V8_TARGET_ARCH_RISCV32\n  template <typename T>\n  void TraceRegWr(T value, TraceType t = WORD);\n#elif V8_TARGET_ARCH_RISCV64\n  void TraceRegWr(sreg_t value, TraceType t = DWORD);\n#endif\n  void TraceMemWr(sreg_t addr, sreg_t value, TraceType t);\n  template <typename T>\n  void TraceMemRd(sreg_t addr, T value, sreg_t reg_value);\n  void TraceMemRdDouble(sreg_t addr, double value, int64_t reg_value);\n  void TraceMemRdDouble(sreg_t addr, Float64 value, int64_t reg_value);\n  void TraceMemRdFloat(sreg_t addr, Float32 value, int64_t reg_value);\n\n  template <typename T>\n  void TraceMemWr(sreg_t addr, T value);\n  void TraceMemWrDouble(sreg_t addr, double value);\n\n  SimInstruction instr_;\n\n  // RISCV utlity API to access register value\n  inline int32_t rs1_reg() const { return instr_.Rs1Value(); }\n  inline sreg_t rs1() const { return get_register(rs1_reg()); }\n  inline float frs1() const { return get_fpu_register_float(rs1_reg()); }\n  inline double drs1() const { return get_fpu_register_double(rs1_reg()); }\n  inline Float32 frs1_boxed() const {\n    return get_fpu_register_Float32(rs1_reg());\n  }\n  inline Float64 drs1_boxed() const {\n    return get_fpu_register_Float64(rs1_reg());\n  }\n  inline int32_t rs2_reg() const { return instr_.Rs2Value(); }\n  inline sreg_t rs2() const { return get_register(rs2_reg()); }\n  inline float frs2() const { return get_fpu_register_float(rs2_reg()); }\n  inline double drs2() const { return get_fpu_register_double(rs2_reg()); }\n  inline Float32 frs2_boxed() const {\n    return get_fpu_register_Float32(rs2_reg());\n  }\n  inline Float64 drs2_boxed() const {\n    return get_fpu_register_Float64(rs2_reg());\n  }\n  inline int32_t rs3_reg() const { return instr_.Rs3Value(); }\n  inline sreg_t rs3() const { return get_register(rs3_reg()); }\n  inline float frs3() const { return get_fpu_register_float(rs3_reg()); }\n  inline double drs3() const { return get_fpu_register_double(rs3_reg()); }\n  inline Float32 frs3_boxed() const {\n    return get_fpu_register_Float32(rs3_reg());\n  }\n  inline Float64 drs3_boxed() const {\n    return get_fpu_register_Float64(rs3_reg());\n  }\n  inline int32_t rd_reg() const { return instr_.RdValue(); }\n  inline int32_t frd_reg() const { return instr_.RdValue(); }\n  inline int32_t rvc_rs1_reg() const { return instr_.RvcRs1Value(); }\n  inline sreg_t rvc_rs1() const { return get_register(rvc_rs1_reg()); }\n  inline int32_t rvc_rs2_reg() const { return instr_.RvcRs2Value(); }\n  inline sreg_t rvc_rs2() const { return get_register(rvc_rs2_reg()); }\n  inline double rvc_drs2() const {\n    return get_fpu_register_double(rvc_rs2_reg());\n  }\n  inline int32_t rvc_rs1s_reg() const { return instr_.RvcRs1sValue(); }\n  inline sreg_t rvc_rs1s() const { return get_register(rvc_rs1s_reg()); }\n  inline int32_t rvc_rs2s_reg() const { return instr_.RvcRs2sValue(); }\n  inline sreg_t rvc_rs2s() const { return get_register(rvc_rs2s_reg()); }\n  inline double rvc_drs2s() const {\n    return get_fpu_register_double(rvc_rs2s_reg());\n  }\n  inline int32_t rvc_rd_reg() const { return instr_.RvcRdValue(); }\n  inline int32_t rvc_frd_reg() const { return instr_.RvcRdValue(); }\n  inline int16_t boffset() const { return instr_.BranchOffset(); }\n  inline int16_t imm12() const { return instr_.Imm12Value(); }\n  inline int32_t imm20J() const { return instr_.Imm20JValue(); }\n  inline int32_t imm5CSR() const { return instr_.Rs1Value(); }\n  inline int16_t csr_reg() const { return instr_.CsrValue(); }\n  inline int16_t rvc_imm6() const { return instr_.RvcImm6Value(); }\n  inline int16_t rvc_imm6_addi16sp() const {\n    return instr_.RvcImm6Addi16spValue();\n  }\n  inline int16_t rvc_imm8_addi4spn() const {\n    return instr_.RvcImm8Addi4spnValue();\n  }\n  inline int16_t rvc_imm6_lwsp() const { return instr_.RvcImm6LwspValue(); }\n  inline int16_t rvc_imm6_ldsp() const { return instr_.RvcImm6LdspValue(); }\n  inline int16_t rvc_imm6_swsp() const { return instr_.RvcImm6SwspValue(); }\n  inline int16_t rvc_imm6_sdsp() const { return instr_.RvcImm6SdspValue(); }\n  inline int16_t rvc_imm5_w() const { return instr_.RvcImm5WValue(); }\n  inline int16_t rvc_imm5_d() const { return instr_.RvcImm5DValue(); }\n  inline int16_t rvc_imm8_b() const { return instr_.RvcImm8BValue(); }\n\n  inline void set_rd(sreg_t value, bool trace = true) {\n    set_register(rd_reg(), value);\n#if V8_TARGET_ARCH_RISCV64\n    if (trace) TraceRegWr(get_register(rd_reg()), DWORD);\n#elif V8_TARGET_ARCH_RISCV32\n    if (trace) TraceRegWr(get_register(rd_reg()), WORD);\n#endif\n  }\n  inline void set_frd(float value, bool trace = true) {\n    set_fpu_register_float(rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register_word(rd_reg()), FLOAT);\n  }\n  inline void set_frd(Float32 value, bool trace = true) {\n    set_fpu_register_float(rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register_word(rd_reg()), FLOAT);\n  }\n  inline void set_drd(double value, bool trace = true) {\n    set_fpu_register_double(rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rd_reg()), DOUBLE);\n  }\n  inline void set_drd(Float64 value, bool trace = true) {\n    set_fpu_register_double(rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rd_reg()), DOUBLE);\n  }\n  inline void set_rvc_rd(sreg_t value, bool trace = true) {\n    set_register(rvc_rd_reg(), value);\n#if V8_TARGET_ARCH_RISCV64\n    if (trace) TraceRegWr(get_register(rvc_rd_reg()), DWORD);\n#elif V8_TARGET_ARCH_RISCV32\n    if (trace) TraceRegWr(get_register(rvc_rd_reg()), WORD);\n#endif\n  }\n  inline void set_rvc_rs1s(sreg_t value, bool trace = true) {\n    set_register(rvc_rs1s_reg(), value);\n#if V8_TARGET_ARCH_RISCV64\n    if (trace) TraceRegWr(get_register(rvc_rs1s_reg()), DWORD);\n#elif V8_TARGET_ARCH_RISCV32\n    if (trace) TraceRegWr(get_register(rvc_rs1s_reg()), WORD);\n#endif\n  }\n  inline void set_rvc_rs2(sreg_t value, bool trace = true) {\n    set_register(rvc_rs2_reg(), value);\n#if V8_TARGET_ARCH_RISCV64\n    if (trace) TraceRegWr(get_register(rvc_rs2_reg()), DWORD);\n#elif V8_TARGET_ARCH_RISCV32\n    if (trace) TraceRegWr(get_register(rvc_rs2_reg()), WORD);\n#endif\n  }\n  inline void set_rvc_drd(double value, bool trace = true) {\n    set_fpu_register_double(rvc_rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);\n  }\n  inline void set_rvc_drd(Float64 value, bool trace = true) {\n    set_fpu_register_double(rvc_rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);\n  }\n  inline void set_rvc_frd(Float32 value, bool trace = true) {\n    set_fpu_register_float(rvc_rd_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);\n  }\n  inline void set_rvc_rs2s(sreg_t value, bool trace = true) {\n    set_register(rvc_rs2s_reg(), value);\n#if V8_TARGET_ARCH_RISCV64\n    if (trace) TraceRegWr(get_register(rvc_rs2s_reg()), DWORD);\n#elif V8_TARGET_ARCH_RISCV32\n    if (trace) TraceRegWr(get_register(rvc_rs2s_reg()), WORD);\n#endif\n  }\n  inline void set_rvc_drs2s(double value, bool trace = true) {\n    set_fpu_register_double(rvc_rs2s_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), DOUBLE);\n  }\n  inline void set_rvc_drs2s(Float64 value, bool trace = true) {\n    set_fpu_register_double(rvc_rs2s_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), DOUBLE);\n  }\n\n  inline void set_rvc_frs2s(Float32 value, bool trace = true) {\n    set_fpu_register_float(rvc_rs2s_reg(), value);\n    if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), FLOAT);\n  }\n  inline int16_t shamt6() const { return (imm12() & 0x3F); }\n  inline int16_t shamt5() const { return (imm12() & 0x1F); }\n  inline int16_t rvc_shamt6() const { return instr_.RvcShamt6(); }\n  inline int32_t s_imm12() const { return instr_.StoreOffset(); }\n  inline int32_t u_imm20() const { return instr_.Imm20UValue() << 12; }\n  inline int32_t rvc_u_imm6() const { return instr_.RvcImm6Value() << 12; }\n  inline void require(bool check) {\n    if (!check) {\n      SignalException(kIllegalInstruction);\n    }\n  }\n\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  inline void rvv_trace_vd() {\n    if (v8_flags.trace_sim) {\n      __int128_t value = Vregister_[rvv_vd_reg()];\n      SNPrintF(trace_buf_, \"%016\" PRIx64 \"%016\" PRIx64 \" (%\" PRId64 \")\",\n               *(reinterpret_cast<int64_t*>(&value) + 1),\n               *reinterpret_cast<int64_t*>(&value), icount_);\n    }\n  }\n\n  inline void rvv_trace_vs1() {\n    if (v8_flags.trace_sim) {\n      PrintF(\"\\t%s:0x%016\" PRIx64 \"%016\" PRIx64 \"\\n\",\n             v8::internal::VRegisters::Name(static_cast<int>(rvv_vs1_reg())),\n             (uint64_t)(get_vregister(static_cast<int>(rvv_vs1_reg())) >> 64),\n             (uint64_t)get_vregister(static_cast<int>(rvv_vs1_reg())));\n    }\n  }\n\n  inline void rvv_trace_vs2() {\n    if (v8_flags.trace_sim) {\n      PrintF(\"\\t%s:0x%016\" PRIx64 \"%016\" PRIx64 \"\\n\",\n             v8::internal::VRegisters::Name(static_cast<int>(rvv_vs2_reg())),\n             (uint64_t)(get_vregister(static_cast<int>(rvv_vs2_reg())) >> 64),\n             (uint64_t)get_vregister(static_cast<int>(rvv_vs2_reg())));\n    }\n  }\n  inline void rvv_trace_v0() {\n    if (v8_flags.trace_sim) {\n      PrintF(\"\\t%s:0x%016\" PRIx64 \"%016\" PRIx64 \"\\n\",\n             v8::internal::VRegisters::Name(v0),\n             (uint64_t)(get_vregister(v0) >> 64), (uint64_t)get_vregister(v0));\n    }\n  }\n\n  inline void rvv_trace_rs1() {\n    if (v8_flags.trace_sim) {\n      PrintF(\"\\t%s:0x%016\" PRIx64 \"\\n\",\n             v8::internal::Registers::Name(static_cast<int>(rs1_reg())),\n             (uint64_t)(get_register(rs1_reg())));\n    }\n  }\n\n  inline void rvv_trace_status() {\n    if (v8_flags.trace_sim) {\n      int i = 0;\n      for (; i < trace_buf_.length(); i++) {\n        if (trace_buf_[i] == '\\0') break;\n      }\n      SNPrintF(trace_buf_.SubVector(i, trace_buf_.length()),\n               \"  sew:%s lmul:%s vstart:%\" PRId64 \"vl:%\" PRId64, rvv_sew_s(),\n               rvv_lmul_s(), rvv_vstart(), rvv_vl());\n    }\n  }\n\n  template <class T>\n  T& Rvvelt(reg_t vReg, uint64_t n, bool is_write = false) {\n    CHECK_NE(rvv_sew(), 0);\n    CHECK_GT((rvv_vlen() >> 3) / sizeof(T), 0);\n    reg_t elts_per_reg = (rvv_vlen() >> 3) / (sizeof(T));\n    vReg += n / elts_per_reg;\n    n = n % elts_per_reg;\n    T* regStart = reinterpret_cast<T*>(reinterpret_cast<char*>(Vregister_) +\n                                       vReg * (rvv_vlen() >> 3));\n    return regStart[n];\n  }\n\n  inline int32_t rvv_vs1_reg() { return instr_.Vs1Value(); }\n  inline reg_t rvv_vs1() { UNIMPLEMENTED(); }\n  inline int32_t rvv_vs2_reg() { return instr_.Vs2Value(); }\n  inline reg_t rvv_vs2() { UNIMPLEMENTED(); }\n  inline int32_t rvv_vd_reg() { return instr_.VdValue(); }\n  inline int32_t rvv_vs3_reg() { return instr_.VdValue(); }\n  inline reg_t rvv_vd() { UNIMPLEMENTED(); }\n  inline int32_t rvv_nf() {\n    return (instr_.InstructionBits() & kRvvNfMask) >> kRvvNfShift;\n  }\n\n  inline void set_vrd() { UNIMPLEMENTED(); }\n\n  inline void set_rvv_vtype(uint64_t value, bool trace = true) {\n    vtype_ = value;\n  }\n  inline void set_rvv_vl(uint64_t value, bool trace = true) { vl_ = value; }\n  inline void set_rvv_vstart(uint64_t value, bool trace = true) {\n    vstart_ = value;\n  }\n  inline void set_rvv_vxsat(uint64_t value, bool trace = true) {\n    vxsat_ = value;\n  }\n  inline void set_rvv_vxrm(uint64_t value, bool trace = true) { vxrm_ = value; }\n  inline void set_rvv_vcsr(uint64_t value, bool trace = true) { vcsr_ = value; }\n  inline void set_rvv_vlenb(uint64_t value, bool trace = true) {\n    vlenb_ = value;\n  }\n#endif\n\n  template <typename T, typename Func>\n  inline T CanonicalizeFPUOpFMA(Func fn, T dst, T src1, T src2) {\n    static_assert(std::is_floating_point<T>::value);\n    auto alu_out = fn(dst, src1, src2);\n    // if any input or result is NaN, the result is quiet_NaN\n    if (std::isnan(alu_out) || std::isnan(src1) || std::isnan(src2) ||\n        std::isnan(dst)) {\n      // signaling_nan sets kInvalidOperation bit\n      if (isSnan(alu_out) || isSnan(src1) || isSnan(src2) || isSnan(dst))\n        set_fflags(kInvalidOperation);\n      alu_out = std::numeric_limits<T>::quiet_NaN();\n    }\n    return alu_out;\n  }\n\n  template <typename T, typename Func>\n  inline T CanonicalizeFPUOp3(Func fn) {\n    static_assert(std::is_floating_point<T>::value);\n    T src1 = std::is_same<float, T>::value ? frs1() : drs1();\n    T src2 = std::is_same<float, T>::value ? frs2() : drs2();\n    T src3 = std::is_same<float, T>::value ? frs3() : drs3();\n    auto alu_out = fn(src1, src2, src3);\n    // if any input or result is NaN, the result is quiet_NaN\n    if (std::isnan(alu_out) || std::isnan(src1) || std::isnan(src2) ||\n        std::isnan(src3)) {\n      // signaling_nan sets kInvalidOperation bit\n      if (isSnan(alu_out) || isSnan(src1) || isSnan(src2) || isSnan(src3))\n        set_fflags(kInvalidOperation);\n      alu_out = std::numeric_limits<T>::quiet_NaN();\n    }\n    return alu_out;\n  }\n\n  template <typename T, typename Func>\n  inline T CanonicalizeFPUOp2(Func fn) {\n    static_assert(std::is_floating_point<T>::value);\n    T src1 = std::is_same<float, T>::value ? frs1() : drs1();\n    T src2 = std::is_same<float, T>::value ? frs2() : drs2();\n    auto alu_out = fn(src1, src2);\n    // if any input or result is NaN, the result is quiet_NaN\n    if (std::isnan(alu_out) || std::isnan(src1) || std::isnan(src2)) {\n      // signaling_nan sets kInvalidOperation bit\n      if (isSnan(alu_out) || isSnan(src1) || isSnan(src2))\n        set_fflags(kInvalidOperation);\n      alu_out = std::numeric_limits<T>::quiet_NaN();\n    }\n    return alu_out;\n  }\n\n  template <typename T, typename Func>\n  inline T CanonicalizeFPUOp1(Func fn) {\n    static_assert(std::is_floating_point<T>::value);\n    T src1 = std::is_same<float, T>::value ? frs1() : drs1();\n    auto alu_out = fn(src1);\n    // if any input or result is NaN, the result is quiet_NaN\n    if (std::isnan(alu_out) || std::isnan(src1)) {\n      // signaling_nan sets kInvalidOperation bit\n      if (isSnan(alu_out) || isSnan(src1)) set_fflags(kInvalidOperation);\n      alu_out = std::numeric_limits<T>::quiet_NaN();\n    }\n    return alu_out;\n  }\n\n  template <typename Func>\n  inline float CanonicalizeDoubleToFloatOperation(Func fn) {\n    float alu_out = fn(drs1());\n    if (std::isnan(alu_out) || std::isnan(drs1()))\n      alu_out = std::numeric_limits<float>::quiet_NaN();\n    return alu_out;\n  }\n\n  template <typename Func>\n  inline float CanonicalizeDoubleToFloatOperation(Func fn, double frs) {\n    float alu_out = fn(frs);\n    if (std::isnan(alu_out) || std::isnan(drs1()))\n      alu_out = std::numeric_limits<float>::quiet_NaN();\n    return alu_out;\n  }\n\n  template <typename Func>\n  inline float CanonicalizeFloatToDoubleOperation(Func fn, float frs) {\n    double alu_out = fn(frs);\n    if (std::isnan(alu_out) || std::isnan(frs1()))\n      alu_out = std::numeric_limits<double>::quiet_NaN();\n    return alu_out;\n  }\n\n  template <typename Func>\n  inline float CanonicalizeFloatToDoubleOperation(Func fn) {\n    double alu_out = fn(frs1());\n    if (std::isnan(alu_out) || std::isnan(frs1()))\n      alu_out = std::numeric_limits<double>::quiet_NaN();\n    return alu_out;\n  }\n\n  Builtin LookUp(Address pc);\n  // RISCV decoding routine\n  void DecodeRVRType();\n  void DecodeRVR4Type();\n  void DecodeRVRFPType();  // Special routine for R/OP_FP type\n  void DecodeRVRAType();   // Special routine for R/AMO type\n  void DecodeRVIType();\n  void DecodeRVSType();\n  void DecodeRVBType();\n  void DecodeRVUType();\n  void DecodeRVJType();\n  void DecodeCRType();\n  void DecodeCAType();\n  void DecodeCIType();\n  void DecodeCIWType();\n  void DecodeCSSType();\n  void DecodeCLType();\n  void DecodeCSType();\n  void DecodeCJType();\n  void DecodeCBType();\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  void DecodeVType();\n  void DecodeRvvIVV();\n  void DecodeRvvIVI();\n  void DecodeRvvIVX();\n  void DecodeRvvMVV();\n  void DecodeRvvMVX();\n  void DecodeRvvFVV();\n  void DecodeRvvFVF();\n  bool DecodeRvvVL();\n  bool DecodeRvvVS();\n#endif\n\n  // Used for breakpoints and traps.\n  void SoftwareInterrupt();\n\n  // Debug helpers\n\n  // Simulator breakpoints.\n  struct Breakpoint {\n    Instruction* location;\n    bool enabled;\n    bool is_tbreak;\n  };\n  std::vector<Breakpoint> breakpoints_;\n  void SetBreakpoint(Instruction* breakpoint, bool is_tbreak);\n  void ListBreakpoints();\n  void CheckBreakpoints();\n\n  // Stop helper functions.\n  bool IsWatchpoint(reg_t code);\n  bool IsTracepoint(reg_t code);\n  void PrintWatchpoint(reg_t code);\n  void HandleStop(reg_t code);\n  bool IsStopInstruction(Instruction* instr);\n  bool IsEnabledStop(reg_t code);\n  void EnableStop(reg_t code);\n  void DisableStop(reg_t code);\n  void IncreaseStopCounter(reg_t code);\n  void PrintStopInfo(reg_t code);\n\n  // Executes one instruction.\n  void InstructionDecode(Instruction* instr);\n\n  // ICache.\n  static void CheckICache(base::CustomMatcherHashMap* i_cache,\n                          Instruction* instr);\n  static void FlushOnePage(base::CustomMatcherHashMap* i_cache, intptr_t start,\n                           size_t size);\n  static CachePage* GetCachePage(base::CustomMatcherHashMap* i_cache,\n                                 void* page);\n\n  enum Exception {\n    none,\n    kIntegerOverflow,\n    kIntegerUnderflow,\n    kDivideByZero,\n    kNumExceptions,\n    // RISCV illegual instruction exception\n    kIllegalInstruction,\n  };\n\n  // Exceptions.\n  void SignalException(Exception e);\n\n  // Handle arguments and return value for runtime FP functions.\n  void GetFpArgs(double* x, double* y, int32_t* z);\n  void SetFpResult(const double& result);\n\n  void CallInternal(Address entry);\n\n  // Architecture state.\n  // Registers.\n  sreg_t registers_[kNumSimuRegisters];\n  // Coprocessor Registers.\n  sfreg_t FPUregisters_[kNumFPURegisters];\n  // Floating-point control and status register.\n  uint32_t FCSR_;\n\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  // RVV registers\n  __int128_t Vregister_[kNumVRegisters];\n  static_assert(sizeof(__int128_t) == kRvvVLEN / 8, \"unmatch vlen\");\n  uint64_t vstart_, vxsat_, vxrm_, vcsr_, vtype_, vl_, vlenb_;\n#endif\n  // Simulator support.\n  // Allocate 1MB for stack.\n  uint8_t* stack_;\n  static const size_t kStackProtectionSize = 256 * kSystemPointerSize;\n  // This includes a protection margin at each end of the stack area.\n  static size_t AllocatedStackSize() {\n#if V8_TARGET_ARCH_PPC64\n    size_t stack_size = v8_flags.sim_stack_size * KB;\n#else\n    size_t stack_size = 1 * MB;  // allocate 1MB for stack\n#endif\n    return stack_size + (2 * kStackProtectionSize);\n  }\n  static size_t UsableStackSize() {\n    return AllocatedStackSize() - kStackProtectionSize;\n  }\n\n  bool pc_modified_;\n  int64_t icount_;\n  sreg_t* watch_address_ = nullptr;\n  sreg_t watch_value_ = 0;\n  int break_count_;\n  base::EmbeddedVector<char, 256> trace_buf_;\n\n  // Debugger input.\n  char* last_debugger_input_;\n\n  v8::internal::Isolate* isolate_;\n  v8::internal::Builtins builtins_;\n\n  // Stop is disabled if bit 31 is set.\n  static const uint32_t kStopDisabledBit = 1 << 31;\n\n  // A stop is enabled, meaning the simulator will stop when meeting the\n  // instruction, if bit 31 of watched_stops_[code].count is unset.\n  // The value watched_stops_[code].count & ~(1 << 31) indicates how many times\n  // the breakpoint was hit or gone through.\n  struct StopCountAndDesc {\n    uint32_t count;\n    char* desc;\n  };\n  StopCountAndDesc watched_stops_[kMaxStopCode + 1];\n\n  // Synchronization primitives.\n  enum class MonitorAccess {\n    Open,\n    RMW,\n  };\n\n  enum class TransactionSize {\n    None = 0,\n    Word = 4,\n    DoubleWord = 8,\n  };\n\n  // The least-significant bits of the address are ignored. The number of bits\n  // is implementation-defined, between 3 and minimum page size.\n  static const uintptr_t kExclusiveTaggedAddrMask = ~((1 << 3) - 1);\n\n  class LocalMonitor {\n   public:\n    LocalMonitor();\n\n    // These functions manage the state machine for the local monitor, but do\n    // not actually perform loads and stores. NotifyStoreConditional only\n    // returns true if the store conditional is allowed; the global monitor will\n    // still have to be checked to see whether the memory should be updated.\n    void NotifyLoad();\n    void NotifyLoadLinked(uintptr_t addr, TransactionSize size);\n    void NotifyStore();\n    bool NotifyStoreConditional(uintptr_t addr, TransactionSize size);\n\n   private:\n    void Clear();\n\n    MonitorAccess access_state_;\n    uintptr_t tagged_addr_;\n    TransactionSize size_;\n  };\n\n  class GlobalMonitor {\n   public:\n    class LinkedAddress {\n     public:\n      LinkedAddress();\n\n     private:\n      friend class GlobalMonitor;\n      // These functions manage the state machine for the global monitor, but do\n      // not actually perform loads and stores.\n      void Clear_Locked();\n      void NotifyLoadLinked_Locked(uintptr_t addr);\n      void NotifyStore_Locked();\n      bool NotifyStoreConditional_Locked(uintptr_t addr,\n                                         bool is_requesting_thread);\n\n      MonitorAccess access_state_;\n      uintptr_t tagged_addr_;\n      LinkedAddress* next_;\n      LinkedAddress* prev_;\n      // A scd can fail due to background cache evictions. Rather than\n      // simulating this, we'll just occasionally introduce cases where an\n      // store conditional fails. This will happen once after every\n      // kMaxFailureCounter exclusive stores.\n      static const int kMaxFailureCounter = 5;\n      int failure_counter_;\n    };\n\n    // Exposed so it can be accessed by Simulator::{Read,Write}Ex*.\n    base::Mutex mutex;\n\n    void NotifyLoadLinked_Locked(uintptr_t addr, LinkedAddress* linked_address);\n    void NotifyStore_Locked(LinkedAddress* linked_address);\n    bool NotifyStoreConditional_Locked(uintptr_t addr,\n                                       LinkedAddress* linked_address);\n\n    // Called when the simulator is destroyed.\n    void RemoveLinkedAddress(LinkedAddress* linked_address);\n\n    static GlobalMonitor* Get();\n\n   private:\n    // Private constructor. Call {GlobalMonitor::Get()} to get the singleton.\n    GlobalMonitor() = default;\n    friend class base::LeakyObject<GlobalMonitor>;\n\n    bool IsProcessorInLinkedList_Locked(LinkedAddress* linked_address) const;\n    void PrependProcessor_Locked(LinkedAddress* linked_address);\n\n    LinkedAddress* head_ = nullptr;\n  };\n\n  LocalMonitor local_monitor_;\n  GlobalMonitor::LinkedAddress global_monitor_thread_;\n}", "name_and_para": ""}], [{"name": "CachePage", "content": "class CachePage {\n  // TODO(all): Simulate instruction cache.\n}", "name_and_para": ""}, {"name": "CachePage", "content": "class CachePage {\n public:\n  static const int LINE_VALID = 0;\n  static const int LINE_INVALID = 1;\n\n  static const int kPageShift = 12;\n  static const int kPageSize = 1 << kPageShift;\n  static const int kPageMask = kPageSize - 1;\n  static const int kLineShift = 2;  // The cache line is only 4 bytes right now.\n  static const int kLineLength = 1 << kLineShift;\n  static const int kLineMask = kLineLength - 1;\n\n  CachePage() { memset(&validity_map_, LINE_INVALID, sizeof(validity_map_)); }\n\n  char* ValidityByte(int offset) {\n    return &validity_map_[offset >> kLineShift];\n  }\n\n  char* CachedData(int offset) { return &data_[offset]; }\n\n private:\n  char data_[kPageSize];  // The cached data.\n  static const int kValidityMapSize = kPageSize >> kLineShift;\n  char validity_map_[kValidityMapSize];  // One byte per line.\n}", "name_and_para": ""}]]], [["./v8/src/execution/riscv/simulator-riscv.cc", "./v8/src/execution/arm64/simulator-arm64.cc"], 0.12359550561797752, 0.1286549707602339, [[{"name": "Simulator::GlobalMonitor::PrependProcessor_Locked", "content": "void Simulator::GlobalMonitor::PrependProcessor_Locked(Processor* processor) {\n  if (IsProcessorInLinkedList_Locked(processor)) {\n    return;\n  }\n\n  if (head_) {\n    head_->prev_ = processor;\n  }\n  processor->prev_ = nullptr;\n  processor->next_ = head_;\n  head_ = processor;\n}", "name_and_para": "void Simulator::GlobalMonitor::PrependProcessor_Locked(Processor* processor) "}, {"name": "Simulator::GlobalMonitor::PrependProcessor_Locked", "content": "void Simulator::GlobalMonitor::PrependProcessor_Locked(\n    LinkedAddress* linked_address) {\n  if (IsProcessorInLinkedList_Locked(linked_address)) {\n    return;\n  }\n\n  if (head_) {\n    head_->prev_ = linked_address;\n  }\n  linked_address->prev_ = nullptr;\n  linked_address->next_ = head_;\n  head_ = linked_address;\n}", "name_and_para": "void Simulator::GlobalMonitor::PrependProcessor_Locked(\n    LinkedAddress* linked_address) "}], [{"name": "Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked", "content": "bool Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked(\n    Processor* processor) const {\n  return head_ == processor || processor->next_ || processor->prev_;\n}", "name_and_para": "bool Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked(\n    Processor* processor) const "}, {"name": "Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked", "content": "bool Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked(\n    LinkedAddress* linked_address) const {\n  return head_ == linked_address || linked_address->next_ ||\n         linked_address->prev_;\n}", "name_and_para": "bool Simulator::GlobalMonitor::IsProcessorInLinkedList_Locked(\n    LinkedAddress* linked_address) const "}], [{"name": "Simulator::GlobalMonitor::NotifyStore_Locked", "content": "void Simulator::GlobalMonitor::NotifyStore_Locked(Processor* processor) {\n  // Notify each processor of the store operation.\n  for (Processor* iter = head_; iter; iter = iter->next_) {\n    bool is_requesting_processor = iter == processor;\n    iter->NotifyStore_Locked(is_requesting_processor);\n  }\n}", "name_and_para": "void Simulator::GlobalMonitor::NotifyStore_Locked(Processor* processor) "}, {"name": "Simulator::GlobalMonitor::NotifyStore_Locked", "content": "void Simulator::GlobalMonitor::NotifyStore_Locked(\n    LinkedAddress* linked_address) {\n  // Notify each thread of the store operation.\n  for (LinkedAddress* iter = head_; iter; iter = iter->next_) {\n    iter->NotifyStore_Locked();\n  }\n}", "name_and_para": "void Simulator::GlobalMonitor::NotifyStore_Locked(\n    LinkedAddress* linked_address) "}], [{"name": "Simulator::LocalMonitor::NotifyStore", "content": "void Simulator::LocalMonitor::NotifyStore() {\n  if (access_state_ == MonitorAccess::Exclusive) {\n    // A non exclusive store could clear the local monitor. As a result, it's\n    // most strict to unconditionally clear the local monitor on store.\n    Clear();\n  }\n}", "name_and_para": "void Simulator::LocalMonitor::NotifyStore() "}, {"name": "Simulator::LocalMonitor::NotifyStore", "content": "void Simulator::LocalMonitor::NotifyStore() {\n  if (access_state_ == MonitorAccess::RMW) {\n    // A non exclusive store could clear the local monitor. As a result, it's\n    // most strict to unconditionally clear the local monitor on store.\n    Clear();\n  }\n}", "name_and_para": "void Simulator::LocalMonitor::NotifyStore() "}], [{"name": "Simulator::LocalMonitor::NotifyLoad", "content": "void Simulator::LocalMonitor::NotifyLoad() {\n  if (access_state_ == MonitorAccess::Exclusive) {\n    // A non exclusive load could clear the local monitor. As a result, it's\n    // most strict to unconditionally clear the local monitor on load.\n    Clear();\n  }\n}", "name_and_para": "void Simulator::LocalMonitor::NotifyLoad() "}, {"name": "Simulator::LocalMonitor::NotifyLoad", "content": "void Simulator::LocalMonitor::NotifyLoad() {\n  if (access_state_ == MonitorAccess::RMW) {\n    // A non linked load could clear the local monitor. As a result, it's\n    // most strict to unconditionally clear the local monitor on load.\n    Clear();\n  }\n}", "name_and_para": "void Simulator::LocalMonitor::NotifyLoad() "}], [{"name": "Simulator::LocalMonitor::Clear", "content": "void Simulator::LocalMonitor::Clear() {\n  access_state_ = MonitorAccess::Open;\n  tagged_addr_ = 0;\n  size_ = TransactionSize::None;\n}", "name_and_para": "void Simulator::LocalMonitor::Clear() "}, {"name": "Simulator::LocalMonitor::Clear", "content": "void Simulator::LocalMonitor::Clear() {\n  access_state_ = MonitorAccess::Open;\n  tagged_addr_ = 0;\n  size_ = TransactionSize::None;\n}", "name_and_para": "void Simulator::LocalMonitor::Clear() "}], [{"name": "Simulator::LocalMonitor::LocalMonitor", "content": "Simulator::LocalMonitor::LocalMonitor()\n    : access_state_(MonitorAccess::Open),\n      tagged_addr_(0),\n      size_(TransactionSize::None) {}", "name_and_para": "Simulator::LocalMonitor::LocalMonitor()\n    : access_state_(MonitorAccess::Open),\n      tagged_addr_(0),\n      size_(TransactionSize::None) "}, {"name": "Simulator::LocalMonitor::LocalMonitor", "content": "Simulator::LocalMonitor::LocalMonitor()\n    : access_state_(MonitorAccess::Open),\n      tagged_addr_(0),\n      size_(TransactionSize::None) {}", "name_and_para": "Simulator::LocalMonitor::LocalMonitor()\n    : access_state_(MonitorAccess::Open),\n      tagged_addr_(0),\n      size_(TransactionSize::None) "}], [{"name": "Simulator::CheckBreakpoints", "content": "void Simulator::CheckBreakpoints() {\n  bool hit_a_breakpoint = false;\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    if ((breakpoints_.at(i).location == pc_) && breakpoints_.at(i).enabled) {\n      hit_a_breakpoint = true;\n      // Disable this breakpoint.\n      breakpoints_.at(i).enabled = false;\n    }\n  }\n  if (hit_a_breakpoint) {\n    PrintF(stream_, \"Hit and disabled a breakpoint at %p.\\n\",\n           reinterpret_cast<void*>(pc_));\n    Debug();\n  }\n}", "name_and_para": "void Simulator::CheckBreakpoints() "}, {"name": "Simulator::CheckBreakpoints", "content": "void Simulator::CheckBreakpoints() {\n  bool hit_a_breakpoint = false;\n  bool is_tbreak = false;\n  Instruction* pc_ = reinterpret_cast<Instruction*>(get_pc());\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    if ((breakpoints_.at(i).location == pc_) && breakpoints_.at(i).enabled) {\n      hit_a_breakpoint = true;\n      if (breakpoints_.at(i).is_tbreak) {\n        // Disable a temporary breakpoint.\n        is_tbreak = true;\n        breakpoints_.at(i).enabled = false;\n      }\n      break;\n    }\n  }\n  if (hit_a_breakpoint) {\n    PrintF(\"Hit %sa breakpoint at %p.\\n\", is_tbreak ? \"and disabled \" : \"\",\n           reinterpret_cast<void*>(pc_));\n    RiscvDebugger dbg(this);\n    dbg.Debug();\n  }\n}", "name_and_para": "void Simulator::CheckBreakpoints() "}], [{"name": "Simulator::ListBreakpoints", "content": "void Simulator::ListBreakpoints() {\n  PrintF(stream_, \"Breakpoints:\\n\");\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    PrintF(stream_, \"%p  : %s\\n\",\n           reinterpret_cast<void*>(breakpoints_.at(i).location),\n           breakpoints_.at(i).enabled ? \"enabled\" : \"disabled\");\n  }\n}", "name_and_para": "void Simulator::ListBreakpoints() "}, {"name": "Simulator::ListBreakpoints", "content": "void Simulator::ListBreakpoints() {\n  PrintF(\"Breakpoints:\\n\");\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    PrintF(\"%p  : %s %s\\n\",\n           reinterpret_cast<void*>(breakpoints_.at(i).location),\n           breakpoints_.at(i).enabled ? \"enabled\" : \"disabled\",\n           breakpoints_.at(i).is_tbreak ? \": temporary\" : \"\");\n  }\n}", "name_and_para": "void Simulator::ListBreakpoints() "}], [{"name": "Simulator::SetBreakpoint", "content": "void Simulator::SetBreakpoint(Instruction* location) {\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    if (breakpoints_.at(i).location == location) {\n      PrintF(stream_, \"Existing breakpoint at %p was %s\\n\",\n             reinterpret_cast<void*>(location),\n             breakpoints_.at(i).enabled ? \"disabled\" : \"enabled\");\n      breakpoints_.at(i).enabled = !breakpoints_.at(i).enabled;\n      return;\n    }\n  }\n  Breakpoint new_breakpoint = {location, true};\n  breakpoints_.push_back(new_breakpoint);\n  PrintF(stream_, \"Set a breakpoint at %p\\n\",\n         reinterpret_cast<void*>(location));\n}", "name_and_para": "void Simulator::SetBreakpoint(Instruction* location) "}, {"name": "Simulator::SetBreakpoint", "content": "void Simulator::SetBreakpoint(Instruction* location, bool is_tbreak) {\n  for (unsigned i = 0; i < breakpoints_.size(); i++) {\n    if (breakpoints_.at(i).location == location) {\n      if (breakpoints_.at(i).is_tbreak != is_tbreak) {\n        PrintF(\"Change breakpoint at %p to %s breakpoint\\n\",\n               reinterpret_cast<void*>(location),\n               is_tbreak ? \"temporary\" : \"regular\");\n        breakpoints_.at(i).is_tbreak = is_tbreak;\n        return;\n      }\n      PrintF(\"Existing breakpoint at %p was %s\\n\",\n             reinterpret_cast<void*>(location),\n             breakpoints_.at(i).enabled ? \"disabled\" : \"enabled\");\n      breakpoints_.at(i).enabled = !breakpoints_.at(i).enabled;\n      return;\n    }\n  }\n  Breakpoint new_breakpoint = {location, true, is_tbreak};\n  breakpoints_.push_back(new_breakpoint);\n  PrintF(\"Set a %sbreakpoint at %p\\n\", is_tbreak ? \"temporary \" : \"\",\n         reinterpret_cast<void*>(location));\n}", "name_and_para": "void Simulator::SetBreakpoint(Instruction* location, bool is_tbreak) "}], [{"name": "Simulator::CallAnyCTypeFunction", "content": "void Simulator::CallAnyCTypeFunction(Address target_address,\n                                     const EncodedCSignature& signature) {\n  TraceSim(\"Type: mixed types BUILTIN_CALL\\n\");\n\n  const int64_t* stack_pointer = reinterpret_cast<int64_t*>(sp());\n  const double* double_stack_pointer = reinterpret_cast<double*>(sp());\n  int num_gp_params = 0, num_fp_params = 0, num_stack_params = 0;\n\n  CHECK_LE(signature.ParameterCount(), kMaxCParameters);\n  static_assert(sizeof(AnyCType) == 8, \"AnyCType is assumed to be 64-bit.\");\n  AnyCType args[kMaxCParameters];\n  // The first 8 parameters of each type (GP or FP) are placed in corresponding\n  // registers. The rest are expected to be on the stack, where each parameter\n  // type counts on its own. For example a function like:\n  // foo(int i1, ..., int i9, float f1, float f2) will use up all 8 GP\n  // registers, place i9 on the stack, and place f1 and f2 in FP registers.\n  // Source: https://developer.arm.com/documentation/ihi0055/d/, section\n  // \"Parameter Passing\".\n  for (int i = 0; i < signature.ParameterCount(); ++i) {\n    if (signature.IsFloat(i)) {\n      if (num_fp_params < 8) {\n        args[i].double_value = dreg(num_fp_params++);\n      } else {\n        args[i].double_value = double_stack_pointer[num_stack_params++];\n      }\n    } else {\n      if (num_gp_params < 8) {\n        args[i].int64_value = xreg(num_gp_params++);\n      } else {\n        args[i].int64_value = stack_pointer[num_stack_params++];\n      }\n    }\n  }\n  AnyCType result;\n  GEN_MAX_PARAM_COUNT(CALL_TARGET_VARARG)\n  /* else */ {\n    UNREACHABLE();\n  }\n  static_assert(20 == kMaxCParameters,\n                \"If you've changed kMaxCParameters, please change the \"\n                \"GEN_MAX_PARAM_COUNT macro.\");\n\n#undef CALL_TARGET_VARARG\n#undef CALL_ARGS\n#undef GEN_MAX_PARAM_COUNT\n\n#ifdef DEBUG\n  CorruptAllCallerSavedCPURegisters();\n#endif\n\n  if (signature.IsReturnFloat()) {\n    set_dreg(0, result.double_value);\n  } else {\n    set_xreg(0, result.int64_value);\n  }\n}", "name_and_para": "void Simulator::CallAnyCTypeFunction(Address target_address,\n                                     const EncodedCSignature& signature) "}, {"name": "Simulator::CallAnyCTypeFunction", "content": "void Simulator::CallAnyCTypeFunction(Address target_address,\n                                     const EncodedCSignature& signature) {\n  const int64_t* stack_pointer = reinterpret_cast<int64_t*>(get_register(sp));\n  const double* double_stack_pointer =\n      reinterpret_cast<double*>(get_register(sp));\n  const Register kParamRegisters[] = {PARAM_REGISTERS};\n  const FPURegister kFPParamRegisters[] = {FP_PARAM_REGISTERS};\n  CHECK_LE(signature.ParameterCount(), kMaxCParameters);\n  static_assert(sizeof(AnyCType) == 8, \"AnyCType is assumed to be 64-bit.\");\n  AnyCType args[kMaxCParameters];\n  int num_gp_params = 0, num_fp_params = 0, num_stack_params = 0;\n  for (int i = 0; i < signature.ParameterCount(); ++i) {\n    if (signature.IsFloat(i)) {\n      if (num_fp_params < 8) {\n        args[i].double_value =\n            get_fpu_register_double(kFPParamRegisters[num_fp_params++]);\n      } else {\n        args[i].double_value = double_stack_pointer[num_stack_params++];\n      }\n    } else {\n      if (num_gp_params < 8) {\n        args[i].int64_value = get_register(kParamRegisters[num_gp_params++]);\n      } else {\n        args[i].int64_value = stack_pointer[num_stack_params++];\n      }\n    }\n  }\n  AnyCType result;\n  GEN_MAX_PARAM_COUNT(CALL_TARGET_VARARG)\n  /* else */ {\n    UNREACHABLE();\n  }\n  static_assert(20 == kMaxCParameters,\n                \"If you've changed kMaxCParameters, please change the \"\n                \"GEN_MAX_PARAM_COUNT macro.\");\n  if (v8_flags.trace_sim) {\n    printf(\"CallAnyCTypeFunction end result \\n\");\n  }\n#undef CALL_TARGET_VARARG\n#undef CALL_ARGS\n#undef GEN_MAX_PARAM_COUNT\n  if (signature.IsReturnFloat()) {\n    if (signature.IsReturnFloat64()) {\n      set_fpu_register_double(FP_RETURN_REGISTER, result.double_value);\n    } else {\n      set_fpu_register_float(FP_RETURN_REGISTER, result.float_value);\n    }\n  } else {\n    set_register(RETURN_REGISTER, result.int64_value);\n  }\n}", "name_and_para": "void Simulator::CallAnyCTypeFunction(Address target_address,\n                                     const EncodedCSignature& signature) "}], [{"name": "Simulator::~Simulator", "content": "Simulator::~Simulator() {\n  GlobalMonitor::Get()->RemoveProcessor(&global_monitor_processor_);\n  delete[] reinterpret_cast<uint8_t*>(stack_);\n  delete disassembler_decoder_;\n  delete print_disasm_;\n  delete decoder_;\n}", "name_and_para": "Simulator::~Simulator() "}, {"name": "Simulator::~Simulator", "content": "Simulator::~Simulator() {\n  GlobalMonitor::Get()->RemoveLinkedAddress(&global_monitor_thread_);\n  free(stack_);\n}", "name_and_para": "Simulator::~Simulator() "}], [{"name": "Simulator::Simulator", "content": "Simulator::Simulator()\n    : decoder_(nullptr),\n      guard_pages_(ENABLE_CONTROL_FLOW_INTEGRITY_BOOL),\n      last_debugger_input_(nullptr),\n      log_parameters_(NO_PARAM),\n      isolate_(nullptr) {\n  Init(stdout);\n  CHECK(!v8_flags.trace_sim);\n}", "name_and_para": "Simulator::Simulator()\n    : decoder_(nullptr),\n      guard_pages_(ENABLE_CONTROL_FLOW_INTEGRITY_BOOL),\n      last_debugger_input_(nullptr),\n      log_parameters_(NO_PARAM),\n      isolate_(nullptr) "}, {"name": "Simulator::Simulator", "content": "Simulator::Simulator(Isolate* isolate) : isolate_(isolate), builtins_(isolate) {\n  // Set up simulator support first. Some of this information is needed to\n  // setup the architecture state.\n  stack_ = reinterpret_cast<uint8_t*>(base::Malloc(AllocatedStackSize()));\n  pc_modified_ = false;\n  icount_ = 0;\n  break_count_ = 0;\n  // Reset debug helpers.\n  breakpoints_.clear();\n  // TODO(riscv): 'next' command\n  // break_on_next_ = false;\n\n  // Set up architecture state.\n  // All registers are initialized to zero to start with.\n  for (int i = 0; i < kNumSimuRegisters; i++) {\n    registers_[i] = 0;\n  }\n\n  for (int i = 0; i < kNumFPURegisters; i++) {\n    FPUregisters_[i] = 0;\n  }\n\n  FCSR_ = 0;\n\n  // The sp is initialized to point to the bottom (high address) of the\n  // allocated stack area. To be safe in potential stack underflows we leave\n  // some buffer below.\n  registers_[sp] = reinterpret_cast<intptr_t>(stack_) + UsableStackSize();\n  // The ra and pc are initialized to a known bad value that will cause an\n  // access violation if the simulator ever tries to execute it.\n  registers_[pc] = bad_ra;\n  registers_[ra] = bad_ra;\n\n  last_debugger_input_ = nullptr;\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  for (int i = 0; i < kNumVRegisters; ++i) {\n    Vregister_[i] = 0;\n  }\n  vxrm_ = 0;\n  vstart_ = 0;\n  vxsat_ = 0;\n  vxrm_ = 0;\n  vcsr_ = 0;\n  vtype_ = 0;\n  vl_ = 0;\n  vlenb_ = 0;\n#endif\n}", "name_and_para": "Simulator::Simulator(Isolate* isolate) : isolate_(isolate), builtins_(isolate) "}], [{"name": "Simulator::Simulator", "content": "Simulator::Simulator(Decoder<DispatchingDecoderVisitor>* decoder,\n                     Isolate* isolate, FILE* stream)\n    : decoder_(decoder),\n      guard_pages_(ENABLE_CONTROL_FLOW_INTEGRITY_BOOL),\n      last_debugger_input_(nullptr),\n      log_parameters_(NO_PARAM),\n      icount_for_stop_sim_at_(0),\n      isolate_(isolate) {\n  // Setup the decoder.\n  decoder_->AppendVisitor(this);\n\n  Init(stream);\n\n  if (v8_flags.trace_sim) {\n    decoder_->InsertVisitorBefore(print_disasm_, this);\n    log_parameters_ = LOG_ALL;\n  }\n}", "name_and_para": "Simulator::Simulator(Decoder<DispatchingDecoderVisitor>* decoder,\n                     Isolate* isolate, FILE* stream)\n    : decoder_(decoder),\n      guard_pages_(ENABLE_CONTROL_FLOW_INTEGRITY_BOOL),\n      last_debugger_input_(nullptr),\n      log_parameters_(NO_PARAM),\n      icount_for_stop_sim_at_(0),\n      isolate_(isolate) "}, {"name": "Simulator::Simulator", "content": "Simulator::Simulator(Isolate* isolate) : isolate_(isolate), builtins_(isolate) {\n  // Set up simulator support first. Some of this information is needed to\n  // setup the architecture state.\n  stack_ = reinterpret_cast<uint8_t*>(base::Malloc(AllocatedStackSize()));\n  pc_modified_ = false;\n  icount_ = 0;\n  break_count_ = 0;\n  // Reset debug helpers.\n  breakpoints_.clear();\n  // TODO(riscv): 'next' command\n  // break_on_next_ = false;\n\n  // Set up architecture state.\n  // All registers are initialized to zero to start with.\n  for (int i = 0; i < kNumSimuRegisters; i++) {\n    registers_[i] = 0;\n  }\n\n  for (int i = 0; i < kNumFPURegisters; i++) {\n    FPUregisters_[i] = 0;\n  }\n\n  FCSR_ = 0;\n\n  // The sp is initialized to point to the bottom (high address) of the\n  // allocated stack area. To be safe in potential stack underflows we leave\n  // some buffer below.\n  registers_[sp] = reinterpret_cast<intptr_t>(stack_) + UsableStackSize();\n  // The ra and pc are initialized to a known bad value that will cause an\n  // access violation if the simulator ever tries to execute it.\n  registers_[pc] = bad_ra;\n  registers_[ra] = bad_ra;\n\n  last_debugger_input_ = nullptr;\n#ifdef CAN_USE_RVV_INSTRUCTIONS\n  for (int i = 0; i < kNumVRegisters; ++i) {\n    Vregister_[i] = 0;\n  }\n  vxrm_ = 0;\n  vstart_ = 0;\n  vxsat_ = 0;\n  vxrm_ = 0;\n  vcsr_ = 0;\n  vtype_ = 0;\n  vl_ = 0;\n  vlenb_ = 0;\n#endif\n}", "name_and_para": "Simulator::Simulator(Isolate* isolate) : isolate_(isolate), builtins_(isolate) "}], [{"name": "Simulator::SetRedirectInstruction", "content": "void Simulator::SetRedirectInstruction(Instruction* instruction) {\n  instruction->SetInstructionBits(\n      HLT | Assembler::ImmException(kImmExceptionIsRedirectedCall));\n}", "name_and_para": "void Simulator::SetRedirectInstruction(Instruction* instruction) "}, {"name": "Simulator::SetRedirectInstruction", "content": "void Simulator::SetRedirectInstruction(Instruction* instruction) {\n  instruction->SetInstructionBits(rtCallRedirInstr);\n}", "name_and_para": "void Simulator::SetRedirectInstruction(Instruction* instruction) "}], [{"name": "base::Vector<uint8_t>", "content": "base::Vector<uint8_t> Simulator::GetCurrentStackView() const {\n  // We do not add an additional safety margin as above in\n  // Simulator::StackLimit, as users of this method are expected to add their\n  // own margin.\n  return base::VectorOf(reinterpret_cast<uint8_t*>(stack_limit_),\n                        UsableStackSize());\n}", "name_and_para": "base::Vector<uint8_t> Simulator::GetCurrentStackView() const "}, {"name": "base::Vector<uint8_t>", "content": "base::Vector<uint8_t> Simulator::GetCurrentStackView() const {\n  // We do not add an additional safety margin as above in\n  // Simulator::StackLimit, as this is currently only used in wasm::StackMemory,\n  // which adds its own margin.\n  return base::VectorOf(stack_, UsableStackSize());\n}", "name_and_para": "base::Vector<uint8_t> Simulator::GetCurrentStackView() const "}], [{"name": "Simulator::StackLimit", "content": "uintptr_t Simulator::StackLimit(uintptr_t c_limit) const {\n  // The simulator uses a separate JS stack. If we have exhausted the C stack,\n  // we also drop down the JS limit to reflect the exhaustion on the JS stack.\n  if (base::Stack::GetCurrentStackPosition() < c_limit) {\n    return get_sp();\n  }\n\n  // Otherwise the limit is the JS stack. Leave a safety margin to prevent\n  // overrunning the stack when pushing values.\n  return stack_limit_ + kAdditionalStackMargin;\n}", "name_and_para": "uintptr_t Simulator::StackLimit(uintptr_t c_limit) const "}, {"name": "Simulator::StackLimit", "content": "uintptr_t Simulator::StackLimit(uintptr_t c_limit) const {\n  // The simulator uses a separate JS stack. If we have exhausted the C stack,\n  // we also drop down the JS limit to reflect the exhaustion on the JS stack.\n  if (GetCurrentStackPosition() < c_limit) {\n    return reinterpret_cast<uintptr_t>(get_sp());\n  }\n\n  // Otherwise the limit is the JS stack. Leave a safety margin to prevent\n  // overrunning the stack when pushing values.\n  return reinterpret_cast<uintptr_t>(stack_) + kStackProtectionSize;\n}", "name_and_para": "uintptr_t Simulator::StackLimit(uintptr_t c_limit) const "}], [{"name": "Simulator::PopAddress", "content": "uintptr_t Simulator::PopAddress() {\n  intptr_t current_sp = sp();\n  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(current_sp);\n  uintptr_t address = *stack_slot;\n  DCHECK_LT(sizeof(uintptr_t), 2 * kXRegSize);\n  set_sp(current_sp + 2 * kXRegSize);\n  return address;\n}", "name_and_para": "uintptr_t Simulator::PopAddress() "}, {"name": "Simulator::PopAddress", "content": "uintptr_t Simulator::PopAddress() {\n  int64_t current_sp = get_register(sp);\n  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(current_sp);\n  uintptr_t address = *stack_slot;\n  set_register(sp, current_sp + sizeof(uintptr_t));\n  return address;\n}", "name_and_para": "uintptr_t Simulator::PopAddress() "}], [{"name": "Simulator::PushAddress", "content": "uintptr_t Simulator::PushAddress(uintptr_t address) {\n  DCHECK(sizeof(uintptr_t) < 2 * kXRegSize);\n  intptr_t new_sp = sp() - 2 * kXRegSize;\n  uintptr_t* alignment_slot = reinterpret_cast<uintptr_t*>(new_sp + kXRegSize);\n  memcpy(alignment_slot, &kSlotsZapValue, kSystemPointerSize);\n  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(new_sp);\n  memcpy(stack_slot, &address, kSystemPointerSize);\n  set_sp(new_sp);\n  return new_sp;\n}", "name_and_para": "uintptr_t Simulator::PushAddress(uintptr_t address) "}, {"name": "Simulator::PushAddress", "content": "uintptr_t Simulator::PushAddress(uintptr_t address) {\n  int64_t new_sp = get_register(sp) - sizeof(uintptr_t);\n  uintptr_t* stack_slot = reinterpret_cast<uintptr_t*>(new_sp);\n  *stack_slot = address;\n  set_register(sp, new_sp);\n  return new_sp;\n}", "name_and_para": "uintptr_t Simulator::PushAddress(uintptr_t address) "}], [{"name": "Simulator::CallImpl", "content": "void Simulator::CallImpl(Address entry, CallArgument* args) {\n  int index_x = 0;\n  int index_d = 0;\n\n  std::vector<int64_t> stack_args(0);\n  for (int i = 0; !args[i].IsEnd(); i++) {\n    CallArgument arg = args[i];\n    if (arg.IsX() && (index_x < 8)) {\n      set_xreg(index_x++, arg.bits());\n    } else if (arg.IsD() && (index_d < 8)) {\n      set_dreg_bits(index_d++, arg.bits());\n    } else {\n      DCHECK(arg.IsD() || arg.IsX());\n      stack_args.push_back(arg.bits());\n    }\n  }\n\n  // Process stack arguments, and make sure the stack is suitably aligned.\n  uintptr_t original_stack = sp();\n  uintptr_t entry_stack =\n      original_stack - stack_args.size() * sizeof(stack_args[0]);\n  if (base::OS::ActivationFrameAlignment() != 0) {\n    entry_stack &= -base::OS::ActivationFrameAlignment();\n  }\n  char* stack = reinterpret_cast<char*>(entry_stack);\n  std::vector<int64_t>::const_iterator it;\n  for (it = stack_args.begin(); it != stack_args.end(); it++) {\n    memcpy(stack, &(*it), sizeof(*it));\n    stack += sizeof(*it);\n  }\n\n  DCHECK(reinterpret_cast<uintptr_t>(stack) <= original_stack);\n  set_sp(entry_stack);\n\n  // Call the generated code.\n  set_pc(entry);\n  set_lr(kEndOfSimAddress);\n  CheckPCSComplianceAndRun();\n\n  set_sp(original_stack);\n}", "name_and_para": "void Simulator::CallImpl(Address entry, CallArgument* args) "}, {"name": "Simulator::CallImpl", "content": "intptr_t Simulator::CallImpl(Address entry, int argument_count,\n                             const intptr_t* arguments) {\n  constexpr int kRegisterPassedArguments = 8;\n  // Set up arguments.\n  // RISC-V 64G ISA has a0-a7 for passing arguments\n  int reg_arg_count = std::min(kRegisterPassedArguments, argument_count);\n  if (reg_arg_count > 0) set_register(a0, arguments[0]);\n  if (reg_arg_count > 1) set_register(a1, arguments[1]);\n  if (reg_arg_count > 2) set_register(a2, arguments[2]);\n  if (reg_arg_count > 3) set_register(a3, arguments[3]);\n  if (reg_arg_count > 4) set_register(a4, arguments[4]);\n  if (reg_arg_count > 5) set_register(a5, arguments[5]);\n  if (reg_arg_count > 6) set_register(a6, arguments[6]);\n  if (reg_arg_count > 7) set_register(a7, arguments[7]);\n  if (v8_flags.trace_sim) {\n    std::cout << \"CallImpl: reg_arg_count = \" << reg_arg_count << std::hex\n              << \" entry-pc (JSEntry) = 0x\" << entry\n              << \" a0 (Isolate-root) = 0x\" << get_register(a0)\n              << \" a1 (orig_func/new_target) = 0x\" << get_register(a1)\n              << \" a2 (func/target) = 0x\" << get_register(a2)\n              << \" a3 (receiver) = 0x\" << get_register(a3) << \" a4 (argc) = 0x\"\n              << get_register(a4) << \" a5 (argv) = 0x\" << get_register(a5)\n              << std::endl;\n  }\n  // Remaining arguments passed on stack.\n  sreg_t original_stack = get_register(sp);\n  // Compute position of stack on entry to generated code.\n  int stack_args_count = argument_count - reg_arg_count;\n  int stack_args_size = stack_args_count * sizeof(*arguments) + kCArgsSlotsSize;\n  sreg_t entry_stack = original_stack - stack_args_size;\n  if (base::OS::ActivationFrameAlignment() != 0) {\n    entry_stack &= -base::OS::ActivationFrameAlignment();\n  }\n  // Store remaining arguments on stack, from low to high memory.\n  intptr_t* stack_argument = reinterpret_cast<intptr_t*>(entry_stack);\n  memcpy(stack_argument + kCArgSlotCount, arguments + reg_arg_count,\n         stack_args_count * sizeof(*arguments));\n  set_register(sp, entry_stack);\n  CallInternal(entry);\n  // Pop stack passed arguments.\n  CHECK_EQ(entry_stack, get_register(sp));\n  set_register(sp, original_stack);\n  // return get_register(a0);\n  // RISCV uses a0 to return result\n  return get_register(a0);\n}", "name_and_para": "intptr_t Simulator::CallImpl(Address entry, int argument_count,\n                             const intptr_t* arguments) "}], [{"name": "Simulator::current", "content": "Simulator* Simulator::current(Isolate* isolate) {\n  Isolate::PerIsolateThreadData* isolate_data =\n      isolate->FindOrAllocatePerThreadDataForThisThread();\n  DCHECK_NOT_NULL(isolate_data);\n\n  Simulator* sim = isolate_data->simulator();\n  if (sim == nullptr) {\n    if (v8_flags.trace_sim || v8_flags.debug_sim) {\n      sim = new Simulator(new Decoder<DispatchingDecoderVisitor>(), isolate);\n    } else {\n      sim = new Decoder<Simulator>();\n      sim->isolate_ = isolate;\n    }\n    isolate_data->set_simulator(sim);\n  }\n  return sim;\n}", "name_and_para": "Simulator* Simulator::current(Isolate* isolate) "}, {"name": "Simulator::current", "content": "Simulator* Simulator::current(Isolate* isolate) {\n  v8::internal::Isolate::PerIsolateThreadData* isolate_data =\n      isolate->FindOrAllocatePerThreadDataForThisThread();\n  DCHECK_NOT_NULL(isolate_data);\n\n  Simulator* sim = isolate_data->simulator();\n  if (sim == nullptr) {\n    // TODO(146): delete the simulator object when a thread/isolate goes away.\n    sim = new Simulator(isolate);\n    isolate_data->set_simulator(sim);\n  }\n  return sim;\n}", "name_and_para": "Simulator* Simulator::current(Isolate* isolate) "}], [{"name": "Simulator::ProbeMemory", "content": "bool Simulator::ProbeMemory(uintptr_t address, uintptr_t access_size) {\n#if V8_ENABLE_WEBASSEMBLY && V8_TRAP_HANDLER_SUPPORTED\n  uintptr_t last_accessed_byte = address + access_size - 1;\n  uintptr_t current_pc = reinterpret_cast<uintptr_t>(pc_);\n  uintptr_t landing_pad =\n      trap_handler::ProbeMemory(last_accessed_byte, current_pc);\n  if (!landing_pad) return true;\n  set_pc(landing_pad);\n  set_reg(kWasmTrapHandlerFaultAddressRegister.code(), current_pc);\n  return false;\n#else\n  return true;\n#endif\n}", "name_and_para": "bool Simulator::ProbeMemory(uintptr_t address, uintptr_t access_size) "}, {"name": "Simulator::ProbeMemory", "content": "bool Simulator::ProbeMemory(uintptr_t address, uintptr_t access_size) {\n#if V8_ENABLE_WEBASSEMBLY && V8_TRAP_HANDLER_SUPPORTED\n  uintptr_t last_accessed_byte = address + access_size - 1;\n  uintptr_t current_pc = registers_[pc];\n  uintptr_t landing_pad =\n      trap_handler::ProbeMemory(last_accessed_byte, current_pc);\n  if (!landing_pad) return true;\n  set_pc(landing_pad);\n  set_register(kWasmTrapHandlerFaultAddressRegister.code(), current_pc);\n  return false;\n#else\n  return true;\n#endif\n}", "name_and_para": "bool Simulator::ProbeMemory(uintptr_t address, uintptr_t access_size) "}]]], [["./v8/src/execution/riscv/frame-constants-riscv.cc", "./v8/src/execution/arm64/frame-constants-arm64.cc"], 1.0, 1.0, [[{"name": "MaglevFrame::StackGuardFrameSize", "content": "intptr_t MaglevFrame::StackGuardFrameSize(int register_input_count) {\n  // Include any paddings from kFixedFrameSizeFromFp, an extra slot + padding\n  // for the single argument into StackGuardWithGap and finally padded register\n  // input count.\n  int slot_count = RoundUp(StandardFrameConstants::kFixedSlotCountFromFp, 2) +\n                   2 /* argument */ + RoundUp(register_input_count, 2);\n  return slot_count * kSystemPointerSize;\n}", "name_and_para": "intptr_t MaglevFrame::StackGuardFrameSize(int register_input_count) "}, {"name": "MaglevFrame::StackGuardFrameSize", "content": "intptr_t MaglevFrame::StackGuardFrameSize(int register_input_count) {\n  USE(register_input_count);\n  UNREACHABLE();\n}", "name_and_para": "intptr_t MaglevFrame::StackGuardFrameSize(int register_input_count) "}], [{"name": "BuiltinContinuationFrameConstants::PaddingSlotCount", "content": "int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) {\n  // Round the total slot count up to a multiple of two, to make the frame a\n  // multiple of 16 bytes.\n  int slot_count = kFixedSlotCount + register_count;\n  int rounded_slot_count = RoundUp(slot_count, 2);\n  return rounded_slot_count - slot_count;\n}", "name_and_para": "int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) "}, {"name": "BuiltinContinuationFrameConstants::PaddingSlotCount", "content": "int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) {\n  USE(register_count);\n  return 0;\n}", "name_and_para": "int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) "}], [{"name": "UnoptimizedFrameConstants::RegisterStackSlotCount", "content": "int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) {\n  static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 0);\n  // Round up to a multiple of two, to make the frame a multiple of 16 bytes.\n  return RoundUp(register_count, 2);\n}", "name_and_para": "int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) "}, {"name": "UnoptimizedFrameConstants::RegisterStackSlotCount", "content": "int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) {\n  return register_count;\n}", "name_and_para": "int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) "}], [{"name": "JavaScriptFrame::constant_pool_pointer_register", "content": "Register JavaScriptFrame::constant_pool_pointer_register() { UNREACHABLE(); }", "name_and_para": "Register JavaScriptFrame::constant_pool_pointer_register() "}, {"name": "JavaScriptFrame::constant_pool_pointer_register", "content": "Register JavaScriptFrame::constant_pool_pointer_register() { UNREACHABLE(); }", "name_and_para": "Register JavaScriptFrame::constant_pool_pointer_register() "}], [{"name": "JavaScriptFrame::context_register", "content": "Register JavaScriptFrame::context_register() { return cp; }", "name_and_para": "Register JavaScriptFrame::context_register() "}, {"name": "JavaScriptFrame::context_register", "content": "Register JavaScriptFrame::context_register() { return cp; }", "name_and_para": "Register JavaScriptFrame::context_register() "}], [{"name": "JavaScriptFrame::fp_register", "content": "Register JavaScriptFrame::fp_register() { return v8::internal::fp; }", "name_and_para": "Register JavaScriptFrame::fp_register() "}, {"name": "JavaScriptFrame::fp_register", "content": "Register JavaScriptFrame::fp_register() { return v8::internal::fp; }", "name_and_para": "Register JavaScriptFrame::fp_register() "}]]], [["./v8/src/execution/riscv/frame-constants-riscv.h", "./v8/src/execution/arm64/frame-constants-arm64.h"], 1.0, 1.0, [[{"name": "WasmDebugBreakFrameConstants", "content": "class WasmDebugBreakFrameConstants : public TypedFrameConstants {\n public:\n  // x16: ip0, x17: ip1, x18: platform register, x26: root, x28: base, x29: fp,\n  // x30: lr, x31: xzr.\n  static constexpr RegList kPushedGpRegs = {\n      x0,  x1,  x2,  x3,  x4,  x5,  x6,  x7,  x8,  x9,  x10, x11,\n      x12, x13, x14, x15, x19, x20, x21, x22, x23, x24, x25, x27};\n\n  // We push FpRegs as 128-bit SIMD registers, so 16-byte frame alignment\n  // is guaranteed regardless of register count.\n  static constexpr DoubleRegList kPushedFpRegs = {\n      d0,  d1,  d2,  d3,  d4,  d5,  d6,  d7,  d8,  d9,  d10, d11, d12, d13, d14,\n      d16, d17, d18, d19, d20, d21, d22, d23, d24, d25, d26, d27, d28, d29};\n\n  static constexpr int kNumPushedGpRegisters = kPushedGpRegs.Count();\n  static_assert(kNumPushedGpRegisters % 2 == 0,\n                \"stack frames need to be 16-byte aligned\");\n\n  static constexpr int kNumPushedFpRegisters = kPushedFpRegs.Count();\n\n  static constexpr int kLastPushedGpRegisterOffset =\n      // Header is padded to 16 byte (see {MacroAssembler::EnterFrame}).\n      -RoundUp<16>(TypedFrameConstants::kFixedFrameSizeFromFp) -\n      kSystemPointerSize * kNumPushedGpRegisters;\n  static constexpr int kLastPushedFpRegisterOffset =\n      kLastPushedGpRegisterOffset - kSimd128Size * kNumPushedFpRegisters;\n\n  // Offsets are fp-relative.\n  static int GetPushedGpRegisterOffset(int reg_code) {\n    DCHECK_NE(0, kPushedGpRegs.bits() & (1 << reg_code));\n    uint32_t lower_regs =\n        kPushedGpRegs.bits() & ((uint32_t{1} << reg_code) - 1);\n    return kLastPushedGpRegisterOffset +\n           base::bits::CountPopulation(lower_regs) * kSystemPointerSize;\n  }\n\n  static int GetPushedFpRegisterOffset(int reg_code) {\n    DCHECK_NE(0, kPushedFpRegs.bits() & (1 << reg_code));\n    uint32_t lower_regs =\n        kPushedFpRegs.bits() & ((uint32_t{1} << reg_code) - 1);\n    return kLastPushedFpRegisterOffset +\n           base::bits::CountPopulation(lower_regs) * kSimd128Size;\n  }\n}", "name_and_para": ""}, {"name": "WasmDebugBreakFrameConstants", "content": "class WasmDebugBreakFrameConstants : public TypedFrameConstants {\n public:\n  static constexpr RegList kPushedGpRegs = wasm::kLiftoffAssemblerGpCacheRegs;\n\n  static constexpr DoubleRegList kPushedFpRegs =\n      wasm::kLiftoffAssemblerFpCacheRegs;\n\n  static constexpr int kNumPushedGpRegisters = kPushedGpRegs.Count();\n  static constexpr int kNumPushedFpRegisters = kPushedFpRegs.Count();\n\n  static constexpr int kLastPushedGpRegisterOffset =\n      -kFixedFrameSizeFromFp - kNumPushedGpRegisters * kSystemPointerSize;\n  static constexpr int kLastPushedFpRegisterOffset =\n      kLastPushedGpRegisterOffset - kNumPushedFpRegisters * kDoubleSize;\n\n  // Offsets are fp-relative.\n  static int GetPushedGpRegisterOffset(int reg_code) {\n    DCHECK_NE(0, kPushedGpRegs.bits() & (1 << reg_code));\n    uint32_t lower_regs =\n        kPushedGpRegs.bits() & ((uint32_t{1} << reg_code) - 1);\n    return kLastPushedGpRegisterOffset +\n           base::bits::CountPopulation(lower_regs) * kSystemPointerSize;\n  }\n\n  static int GetPushedFpRegisterOffset(int reg_code) {\n    DCHECK_NE(0, kPushedFpRegs.bits() & (1 << reg_code));\n    uint32_t lower_regs =\n        kPushedFpRegs.bits() & ((uint32_t{1} << reg_code) - 1);\n    return kLastPushedFpRegisterOffset +\n           base::bits::CountPopulation(lower_regs) * kDoubleSize;\n  }\n}", "name_and_para": ""}], [{"name": "WasmLiftoffFrameConstants", "content": "class WasmLiftoffFrameConstants : public TypedFrameConstants {\n public:\n  static constexpr int kFeedbackVectorOffset = 3 * kSystemPointerSize;\n  static constexpr int kInstanceDataOffset = 2 * kSystemPointerSize;\n}", "name_and_para": ""}, {"name": "WasmLiftoffFrameConstants", "content": "class WasmLiftoffFrameConstants : public TypedFrameConstants {\n public:\n  static constexpr int kFeedbackVectorOffset = 3 * kSystemPointerSize;\n  static constexpr int kInstanceDataOffset = 2 * kSystemPointerSize;\n}", "name_and_para": ""}], [{"name": "WasmLiftoffSetupFrameConstants", "content": "class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {\n public:\n  // Number of gp parameters, without the instance.\n  static constexpr int kNumberOfSavedGpParamRegs = 6;\n  static constexpr int kNumberOfSavedFpParamRegs = 8;\n\n  // On arm, spilled registers are implicitly sorted backwards by number.\n  // We spill:\n  //   x0, x2, x3, x4, x5, x6: param1, param2, ..., param6\n  // in the following FP-relative order: [x6, x5, x4, x3, x2, x0].\n  // The instance slot is in position '0', the first spill slot is at '1'.\n  static constexpr int kInstanceSpillOffset =\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);\n\n  static constexpr int kParameterSpillsOffset[] = {\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(6), TYPED_FRAME_PUSHED_VALUE_OFFSET(5),\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(4), TYPED_FRAME_PUSHED_VALUE_OFFSET(3),\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(2), TYPED_FRAME_PUSHED_VALUE_OFFSET(1)};\n\n  // SP-relative.\n  static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;\n  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;\n  static constexpr int kNativeModuleOffset = 0;\n}", "name_and_para": ""}, {"name": "WasmLiftoffSetupFrameConstants", "content": "class WasmLiftoffSetupFrameConstants : public TypedFrameConstants {\n public:\n  // Number of gp parameters, without the instance.\n  // Note that {kNumberOfSavedGpParamRegs} = arraysize(wasm::kGpParamRegisters)\n  // - 1, {kNumberOfSavedFpParamRegs} = arraysize(wasm::kFpParamRegisters). Here\n  // we use immediate values instead to avoid circular references (introduced by\n  // linkage_location.h, issue: v8:14035) and resultant compilation errors.\n  static constexpr int kNumberOfSavedGpParamRegs = 6;\n  static constexpr int kNumberOfSavedFpParamRegs = 8;\n  static constexpr int kNumberOfSavedAllParamRegs =\n      kNumberOfSavedGpParamRegs + kNumberOfSavedFpParamRegs;\n  static constexpr int kInstanceSpillOffset =\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(0);\n  static constexpr int kParameterSpillsOffset[] = {\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(1), TYPED_FRAME_PUSHED_VALUE_OFFSET(2),\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(3), TYPED_FRAME_PUSHED_VALUE_OFFSET(4),\n      TYPED_FRAME_PUSHED_VALUE_OFFSET(5), TYPED_FRAME_PUSHED_VALUE_OFFSET(6)};\n\n  // SP-relative.\n  static constexpr int kWasmInstanceOffset = 2 * kSystemPointerSize;\n  static constexpr int kDeclaredFunctionIndexOffset = 1 * kSystemPointerSize;\n  static constexpr int kNativeModuleOffset = 0;\n}", "name_and_para": ""}], [{"name": "EntryFrameConstants", "content": "class EntryFrameConstants : public AllStatic {\n public:\n  // This is the offset to where JSEntry pushes the current value of\n  // Isolate::c_entry_fp onto the stack.\n  static constexpr int kNextExitFrameFPOffset = -3 * kSystemPointerSize;\n  // The offsets for storing the FP and PC of fast API calls.\n  static constexpr int kNextFastCallFrameFPOffset = -5 * kSystemPointerSize;\n  static constexpr int kNextFastCallFramePCOffset = -6 * kSystemPointerSize;\n\n  static constexpr int kFixedFrameSize = 6 * kSystemPointerSize;\n\n  // The following constants are defined so we can static-assert their values\n  // near the relevant JSEntry assembly code, not because they're actually very\n  // useful.\n  static constexpr int kCalleeSavedRegisterBytesPushedBeforeFpLrPair =\n      18 * kSystemPointerSize;\n  static constexpr int kCalleeSavedRegisterBytesPushedAfterFpLrPair = 0;\n  static constexpr int kOffsetToCalleeSavedRegisters = 0;\n\n  // These offsets refer to the immediate caller (a native frame), not to the\n  // previous JS exit frame like kCallerFPOffset above.\n  static constexpr int kDirectCallerFPOffset =\n      kCalleeSavedRegisterBytesPushedAfterFpLrPair +\n      kOffsetToCalleeSavedRegisters;\n  static constexpr int kDirectCallerPCOffset =\n      kDirectCallerFPOffset + 1 * kSystemPointerSize;\n  static constexpr int kDirectCallerSPOffset =\n      kDirectCallerPCOffset + 1 * kSystemPointerSize +\n      kCalleeSavedRegisterBytesPushedBeforeFpLrPair;\n}", "name_and_para": ""}, {"name": "EntryFrameConstants", "content": "class EntryFrameConstants : public AllStatic {\n public:\n  // This is the offset to where JSEntry pushes the current value of\n  // Isolate::c_entry_fp onto the stack.\n  static constexpr int kNextExitFrameFPOffset = -3 * kSystemPointerSize;\n  // The offsets for storing the FP and PC of fast API calls.\n  static constexpr int kNextFastCallFrameFPOffset =\n      kNextExitFrameFPOffset - kSystemPointerSize;\n  static constexpr int kNextFastCallFramePCOffset =\n      kNextFastCallFrameFPOffset - kSystemPointerSize;\n}", "name_and_para": ""}]]], [["./v8/src/diagnostics/riscv/disasm-riscv.cc", "./v8/src/diagnostics/arm64/disasm-arm64.cc"], 0.08571428571428572, 0.09375, [[{"name": "Disassembler::Disassemble", "content": "void Disassembler::Disassemble(FILE* file, uint8_t* start, uint8_t* end,\n                               UnimplementedOpcodeAction) {\n  v8::internal::Decoder<v8::internal::DispatchingDecoderVisitor> decoder;\n  v8::internal::PrintDisassembler disasm(file);\n  decoder.AppendVisitor(&disasm);\n\n  for (uint8_t* pc = start; pc < end; pc += v8::internal::kInstrSize) {\n    decoder.Decode(reinterpret_cast<v8::internal::Instruction*>(pc));\n  }\n}", "name_and_para": "void Disassembler::Disassemble(FILE* file, uint8_t* start, uint8_t* end,\n                               UnimplementedOpcodeAction) "}, {"name": "Disassembler::Disassemble", "content": "void Disassembler::Disassemble(FILE* f, uint8_t* begin, uint8_t* end,\n                               UnimplementedOpcodeAction unimplemented_action) {\n  NameConverter converter;\n  Disassembler d(converter, unimplemented_action);\n  for (uint8_t* pc = begin; pc < end;) {\n    v8::base::EmbeddedVector<char, 128> buffer;\n    buffer[0] = '\\0';\n    uint8_t* prev_pc = pc;\n    pc += d.InstructionDecode(buffer, pc);\n    v8::internal::PrintF(f, \"%p    %08x      %s\\n\", static_cast<void*>(prev_pc),\n                         *reinterpret_cast<uint32_t*>(prev_pc), buffer.begin());\n  }\n}", "name_and_para": "void Disassembler::Disassemble(FILE* f, uint8_t* begin, uint8_t* end,\n                               UnimplementedOpcodeAction unimplemented_action) "}], [{"name": "Disassembler::ConstantPoolSizeAt", "content": "int Disassembler::ConstantPoolSizeAt(uint8_t* instr) {\n  return v8::internal::Assembler::ConstantPoolSizeAt(\n      reinterpret_cast<v8::internal::Instruction*>(instr));\n}", "name_and_para": "int Disassembler::ConstantPoolSizeAt(uint8_t* instr) "}, {"name": "Disassembler::ConstantPoolSizeAt", "content": "int Disassembler::ConstantPoolSizeAt(uint8_t* instruction) {\n  return v8::internal::Assembler::ConstantPoolSizeAt(\n      reinterpret_cast<v8::internal::Instruction*>(instruction));\n}", "name_and_para": "int Disassembler::ConstantPoolSizeAt(uint8_t* instruction) "}], [{"name": "Disassembler::InstructionDecode", "content": "int Disassembler::InstructionDecode(v8::base::Vector<char> buffer,\n                                    uint8_t* instr) {\n  USE(converter_);  // avoid unused field warning\n  v8::internal::Decoder<v8::internal::DispatchingDecoderVisitor> decoder;\n  BufferDisassembler disasm(buffer);\n  decoder.AppendVisitor(&disasm);\n\n  decoder.Decode(reinterpret_cast<v8::internal::Instruction*>(instr));\n  return v8::internal::kInstrSize;\n}", "name_and_para": "int Disassembler::InstructionDecode(v8::base::Vector<char> buffer,\n                                    uint8_t* instr) "}, {"name": "Disassembler::InstructionDecode", "content": "int Disassembler::InstructionDecode(v8::base::Vector<char> buffer,\n                                    uint8_t* instruction) {\n  v8::internal::Decoder d(converter_, buffer);\n  return d.InstructionDecode(instruction);\n}", "name_and_para": "int Disassembler::InstructionDecode(v8::base::Vector<char> buffer,\n                                    uint8_t* instruction) "}], [{"name": "NameConverter::NameInCode", "content": "const char* NameConverter::NameInCode(uint8_t* addr) const {\n  // The default name converter is called for unknown code, so we will not try\n  // to access any memory.\n  return \"\";\n}", "name_and_para": "const char* NameConverter::NameInCode(uint8_t* addr) const "}, {"name": "NameConverter::NameInCode", "content": "const char* NameConverter::NameInCode(uint8_t* addr) const {\n  // The default name converter is called for unknown code. So we will not try\n  // to access any memory.\n  return \"\";\n}", "name_and_para": "const char* NameConverter::NameInCode(uint8_t* addr) const "}], [{"name": "NameConverter::NameOfXMMRegister", "content": "const char* NameConverter::NameOfXMMRegister(int reg) const {\n  UNREACHABLE();  // ARM64 does not have any XMM registers\n}", "name_and_para": "const char* NameConverter::NameOfXMMRegister(int reg) const "}, {"name": "NameConverter::NameOfXMMRegister", "content": "const char* NameConverter::NameOfXMMRegister(int reg) const {\n  return v8::internal::FPURegisters::Name(reg);\n}", "name_and_para": "const char* NameConverter::NameOfXMMRegister(int reg) const "}], [{"name": "NameConverter::NameOfByteCPURegister", "content": "const char* NameConverter::NameOfByteCPURegister(int reg) const {\n  UNREACHABLE();  // ARM64 does not have the concept of a byte register\n}", "name_and_para": "const char* NameConverter::NameOfByteCPURegister(int reg) const "}, {"name": "NameConverter::NameOfByteCPURegister", "content": "const char* NameConverter::NameOfByteCPURegister(int reg) const {\n  UNREACHABLE();  // RISC-V does not have the concept of a byte register.\n  // return \"nobytereg\";\n}", "name_and_para": "const char* NameConverter::NameOfByteCPURegister(int reg) const "}], [{"name": "NameConverter::NameOfCPURegister", "content": "const char* NameConverter::NameOfCPURegister(int reg) const {\n  unsigned ureg = reg;  // Avoid warnings about signed/unsigned comparisons.\n  if (ureg >= v8::internal::kNumberOfRegisters) {\n    return \"noreg\";\n  }\n  if (ureg == v8::internal::kZeroRegCode) {\n    return \"xzr\";\n  }\n  v8::base::SNPrintF(tmp_buffer_, \"x%u\", ureg);\n  return tmp_buffer_.begin();\n}", "name_and_para": "const char* NameConverter::NameOfCPURegister(int reg) const "}, {"name": "NameConverter::NameOfCPURegister", "content": "const char* NameConverter::NameOfCPURegister(int reg) const {\n  return v8::internal::Registers::Name(reg);\n}", "name_and_para": "const char* NameConverter::NameOfCPURegister(int reg) const "}], [{"name": "NameConverter::NameOfConstant", "content": "const char* NameConverter::NameOfConstant(uint8_t* addr) const {\n  return NameOfAddress(addr);\n}", "name_and_para": "const char* NameConverter::NameOfConstant(uint8_t* addr) const "}, {"name": "NameConverter::NameOfConstant", "content": "const char* NameConverter::NameOfConstant(uint8_t* addr) const {\n  return NameOfAddress(addr);\n}", "name_and_para": "const char* NameConverter::NameOfConstant(uint8_t* addr) const "}], [{"name": "NameConverter::NameOfAddress", "content": "const char* NameConverter::NameOfAddress(uint8_t* addr) const {\n  v8::base::SNPrintF(tmp_buffer_, \"%p\", static_cast<void*>(addr));\n  return tmp_buffer_.begin();\n}", "name_and_para": "const char* NameConverter::NameOfAddress(uint8_t* addr) const "}, {"name": "NameConverter::NameOfAddress", "content": "const char* NameConverter::NameOfAddress(uint8_t* addr) const {\n  v8::base::SNPrintF(tmp_buffer_, \"%p\", static_cast<void*>(addr));\n  return tmp_buffer_.begin();\n}", "name_and_para": "const char* NameConverter::NameOfAddress(uint8_t* addr) const "}]]], [["./v8/src/diagnostics/riscv/unwinder-riscv.cc", "./v8/src/diagnostics/arm64/unwinder-arm64.cc"], 1.0, 1.0, [[{"name": "GetCalleeSavedRegistersFromEntryFrame", "content": "void GetCalleeSavedRegistersFromEntryFrame(void* fp,\n                                           RegisterState* register_state) {}", "name_and_para": "void GetCalleeSavedRegistersFromEntryFrame(void* fp,\n                                           RegisterState* register_state) "}, {"name": "GetCalleeSavedRegistersFromEntryFrame", "content": "void GetCalleeSavedRegistersFromEntryFrame(void* fp,\n                                           RegisterState* register_state) {}", "name_and_para": "void GetCalleeSavedRegistersFromEntryFrame(void* fp,\n                                           RegisterState* register_state) "}], [{"name": "RegisterState", "content": "struct RegisterState", "name_and_para": ""}, {"name": "RegisterState", "content": "struct RegisterState", "name_and_para": ""}]]], [["./v8/src/compiler/backend/riscv/instruction-selector-riscv32.cc", "./v8/src/compiler/backend/riscv/instruction-selector-riscv64.cc", "./v8/src/compiler/backend/arm64/instruction-selector-arm64.cc"], 0.3069620253164557, 0.34397163120567376, [[{"name": "EXPORT_TEMPLATE_DEFINE", "content": "class EXPORT_TEMPLATE_DEFINE", "name_and_para": ""}, {"name": "EXPORT_TEMPLATE_DEFINE", "content": "class EXPORT_TEMPLATE_DEFINE", "name_and_para": ""}], [{"name": "EXPORT_TEMPLATE_DEFINE", "content": "class EXPORT_TEMPLATE_DEFINE", "name_and_para": ""}, {"name": "EXPORT_TEMPLATE_DEFINE", "content": "class EXPORT_TEMPLATE_DEFINE", "name_and_para": ""}], [{"name": "MachineOperatorBuilder::Flags", "content": "MachineOperatorBuilder::Flags\nInstructionSelector::SupportedMachineOperatorFlags() {\n  return MachineOperatorBuilder::kFloat32RoundDown |\n         MachineOperatorBuilder::kFloat64RoundDown |\n         MachineOperatorBuilder::kFloat32RoundUp |\n         MachineOperatorBuilder::kFloat64RoundUp |\n         MachineOperatorBuilder::kFloat32RoundTruncate |\n         MachineOperatorBuilder::kFloat64RoundTruncate |\n         MachineOperatorBuilder::kFloat64RoundTiesAway |\n         MachineOperatorBuilder::kFloat32RoundTiesEven |\n         MachineOperatorBuilder::kFloat64RoundTiesEven |\n         MachineOperatorBuilder::kWord32Popcnt |\n         MachineOperatorBuilder::kWord64Popcnt |\n         MachineOperatorBuilder::kWord32ShiftIsSafe |\n         MachineOperatorBuilder::kInt32DivIsSafe |\n         MachineOperatorBuilder::kUint32DivIsSafe |\n         MachineOperatorBuilder::kWord32ReverseBits |\n         MachineOperatorBuilder::kWord64ReverseBits |\n         MachineOperatorBuilder::kSatConversionIsSafe |\n         MachineOperatorBuilder::kFloat32Select |\n         MachineOperatorBuilder::kFloat64Select |\n         MachineOperatorBuilder::kWord32Select |\n         MachineOperatorBuilder::kWord64Select |\n         MachineOperatorBuilder::kLoadStorePairs;\n}", "name_and_para": "MachineOperatorBuilder::Flags\nInstructionSelector::SupportedMachineOperatorFlags() "}, {"name": "MachineOperatorBuilder::Flags", "content": "MachineOperatorBuilder::Flags\nInstructionSelector::SupportedMachineOperatorFlags() {\n  MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;\n  return flags | MachineOperatorBuilder::kWord32Ctz |\n         MachineOperatorBuilder::kWord32Ctz |\n         MachineOperatorBuilder::kWord32Popcnt |\n         MachineOperatorBuilder::kInt32DivIsSafe |\n         MachineOperatorBuilder::kUint32DivIsSafe |\n         MachineOperatorBuilder::kFloat32RoundDown |\n         MachineOperatorBuilder::kFloat32RoundUp |\n         MachineOperatorBuilder::kFloat32RoundTruncate |\n         MachineOperatorBuilder::kFloat32RoundTiesEven;\n}", "name_and_para": "MachineOperatorBuilder::Flags\nInstructionSelector::SupportedMachineOperatorFlags() "}], [{"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(node_t node) {\n  VisitRR(this, kArm64Sxtw, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    EmitSignExtendWord(this, node);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(node_t node) {\n  VisitRR(this, kArm64Sxth, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Emit(kRiscvSignExtendShort, g.DefineAsRegister(node),\n         g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(node_t node) {\n  VisitRR(this, kArm64Sxtb, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),\n         g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(\n    node_t node, ArchOpcode uint8_op, ArchOpcode uint16_op,\n    ArchOpcode uint32_op, ArchOpcode uint64_op) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n    ArchOpcode opcode;\n    if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n      opcode = uint8_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n      opcode = uint16_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n      opcode = uint32_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Uint64()) {\n      opcode = uint64_op;\n    } else {\n      UNREACHABLE();\n    }\n    VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord64,\n                     atomic_op.memory_access_kind);\n  } else {\n    ArchOpcode opcode;\n    AtomicOpParameters params = AtomicOpParametersOf(node->op());\n    if (params.type() == MachineType::Uint8()) {\n      opcode = uint8_op;\n    } else if (params.type() == MachineType::Uint16()) {\n      opcode = uint16_op;\n    } else if (params.type() == MachineType::Uint32()) {\n      opcode = uint32_op;\n    } else if (params.type() == MachineType::Uint64()) {\n      opcode = uint64_op;\n    } else {\n      UNREACHABLE();\n    }\n    VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord64, params.kind());\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(\n    node_t node, ArchOpcode uint8_op, ArchOpcode uint16_op,\n    ArchOpcode uint32_op, ArchOpcode uint64_op) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(\n    node_t node, ArchOpcode uint8_op, ArchOpcode uint16_op,\n    ArchOpcode uint32_op, ArchOpcode uint64_op) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    ArchOpcode opcode;\n    AtomicOpParameters params = AtomicOpParametersOf(node->op());\n    if (params.type() == MachineType::Uint8()) {\n      opcode = uint8_op;\n    } else if (params.type() == MachineType::Uint16()) {\n      opcode = uint16_op;\n    } else if (params.type() == MachineType::Uint32()) {\n      opcode = uint32_op;\n    } else if (params.type() == MachineType::Uint64()) {\n      opcode = uint64_op;\n    } else {\n      UNREACHABLE();\n    }\n    VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord64, params.kind());\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(\n    node_t node, ArchOpcode uint8_op, ArchOpcode uint16_op,\n    ArchOpcode uint32_op, ArchOpcode uint64_op) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(\n    node_t node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,\n    ArchOpcode uint16_op, ArchOpcode word32_op) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n    ArchOpcode opcode;\n    if (atomic_op.memory_rep == MemoryRepresentation::Int8()) {\n      opcode = int8_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n      opcode = uint8_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Int16()) {\n      opcode = int16_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n      opcode = uint16_op;\n    } else if (atomic_op.memory_rep == MemoryRepresentation::Int32() ||\n               atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n      opcode = word32_op;\n    } else {\n      UNREACHABLE();\n    }\n    VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord32,\n                     atomic_op.memory_access_kind);\n  } else {\n    ArchOpcode opcode;\n    AtomicOpParameters params = AtomicOpParametersOf(node->op());\n    if (params.type() == MachineType::Int8()) {\n      opcode = int8_op;\n    } else if (params.type() == MachineType::Uint8()) {\n      opcode = uint8_op;\n    } else if (params.type() == MachineType::Int16()) {\n      opcode = int16_op;\n    } else if (params.type() == MachineType::Uint16()) {\n      opcode = uint16_op;\n    } else if (params.type() == MachineType::Int32() ||\n               params.type() == MachineType::Uint32()) {\n      opcode = word32_op;\n    } else {\n      UNREACHABLE();\n    }\n    VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord32, params.kind());\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(\n    node_t node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,\n    ArchOpcode uint16_op, ArchOpcode word32_op) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(\n    node_t node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,\n    ArchOpcode uint16_op, ArchOpcode word32_op) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    ArchOpcode opcode;\n    MachineType type = AtomicOpType(node->op());\n    if (type == MachineType::Int8()) {\n      opcode = int8_op;\n    } else if (type == MachineType::Uint8()) {\n      opcode = uint8_op;\n    } else if (type == MachineType::Int16()) {\n      opcode = int16_op;\n    } else if (type == MachineType::Uint16()) {\n      opcode = uint16_op;\n    } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {\n      opcode = word32_op;\n    } else {\n      UNREACHABLE();\n    }\n\n    VisitAtomicBinop(this, node, opcode);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(\n    node_t node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,\n    ArchOpcode uint16_op, ArchOpcode word32_op) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (params.type() == MachineType::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else if (params.type() == MachineType::Uint64()) {\n    opcode = kArm64Word64AtomicCompareExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord64,\n                             params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange(\n    Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (params.type() == MachineType::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else if (params.type() == MachineType::Uint64()) {\n    opcode = kRiscvWord64AtomicCompareExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord64,\n                             params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicCompareExchange(\n    Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n  ArchOpcode opcode;\n  if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint64()) {\n    opcode = kArm64Word64AtomicCompareExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord64,\n                             atomic_op.memory_access_kind);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange(\n    node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange(\n    node_t node) {\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicCompareExchange(\n    node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Int8()) {\n    opcode = kAtomicCompareExchangeInt8;\n  } else if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (params.type() == MachineType::Int16()) {\n    opcode = kAtomicCompareExchangeInt16;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (params.type() == MachineType::Int32()\n    || params.type() == MachineType::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord32,\n                             params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange(\n    Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Int8()) {\n    opcode = kAtomicCompareExchangeInt8;\n  } else if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (params.type() == MachineType::Int16()) {\n    opcode = kAtomicCompareExchangeInt16;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (params.type() == MachineType::Int32() ||\n             params.type() == MachineType::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord32,\n                             params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicCompareExchange(\n    Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n  ArchOpcode opcode;\n  if (atomic_op.memory_rep == MemoryRepresentation::Int8()) {\n    opcode = kAtomicCompareExchangeInt8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n    opcode = kAtomicCompareExchangeUint8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Int16()) {\n    opcode = kAtomicCompareExchangeInt16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n    opcode = kAtomicCompareExchangeUint16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Int32() ||\n             atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n    opcode = kAtomicCompareExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord32,\n                             atomic_op.memory_access_kind);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange(\n    node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange(\n    node_t node) {\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicCompareExchange(\n    node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (params.type() == MachineType::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else if (params.type() == MachineType::Uint64()) {\n    opcode = kArm64Word64AtomicExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64, params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange(\n    Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (params.type() == MachineType::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else if (params.type() == MachineType::Uint64()) {\n    opcode = kRiscvWord64AtomicExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64, params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord64AtomicExchange(\n    Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n  ArchOpcode opcode;\n  if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint64()) {\n    opcode = kArm64Word64AtomicExchangeUint64;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64,\n                      atomic_op.memory_access_kind);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange(\n    node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange(\n    node_t node) {\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64AtomicExchange(\n    node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Int8()) {\n    opcode = kAtomicExchangeInt8;\n  } else if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (params.type() == MachineType::Int16()) {\n    opcode = kAtomicExchangeInt16;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (params.type() == MachineType::Int32()\n    || params.type() == MachineType::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32, params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange(\n    Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange(\n    Node* node) {\n  ArchOpcode opcode;\n  AtomicOpParameters params = AtomicOpParametersOf(node->op());\n  if (params.type() == MachineType::Int8()) {\n    opcode = kAtomicExchangeInt8;\n  } else if (params.type() == MachineType::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (params.type() == MachineType::Int16()) {\n    opcode = kAtomicExchangeInt16;\n  } else if (params.type() == MachineType::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (params.type() == MachineType::Int32() ||\n             params.type() == MachineType::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32, params.kind());\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWord32AtomicExchange(\n    Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const AtomicRMWOp& atomic_op = this->Get(node).template Cast<AtomicRMWOp>();\n  ArchOpcode opcode;\n  if (atomic_op.memory_rep == MemoryRepresentation::Int8()) {\n    opcode = kAtomicExchangeInt8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint8()) {\n    opcode = kAtomicExchangeUint8;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Int16()) {\n    opcode = kAtomicExchangeInt16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Uint16()) {\n    opcode = kAtomicExchangeUint16;\n  } else if (atomic_op.memory_rep == MemoryRepresentation::Int32() ||\n             atomic_op.memory_rep == MemoryRepresentation::Uint32()) {\n    opcode = kAtomicExchangeWord32;\n  } else {\n    UNREACHABLE();\n  }\n  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32,\n                      atomic_op.memory_access_kind);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange(\n    node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange(\n    node_t node) {\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32AtomicExchange(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicStore", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(node_t node) {\n  VisitAtomicStore(this, node, AtomicWidth::kWord64);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicStore", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(node_t node) {\n    VisitAtomicStore(this, node, AtomicWidth::kWord64);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicStore", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(node_t node) {\n  VisitAtomicStore(this, node, AtomicWidth::kWord32);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicStore", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    AtomicStoreParameters store_params = AtomicStoreParametersOf(node->op());\n    MachineRepresentation rep = store_params.representation();\n    ArchOpcode opcode;\n    switch (rep) {\n      case MachineRepresentation::kWord8:\n        opcode = kAtomicStoreWord8;\n        break;\n      case MachineRepresentation::kWord16:\n        opcode = kAtomicStoreWord16;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:\n      case MachineRepresentation::kWord32:\n        opcode = kAtomicStoreWord32;\n        break;\n      default:\n        UNREACHABLE();\n    }\n\n    VisitAtomicStore(this, node, opcode, AtomicWidth::kWord32);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicLoad", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(node_t node) {\n  VisitAtomicLoad(this, node, AtomicWidth::kWord64);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64AtomicLoad", "content": "void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(node_t node) {\n    VisitAtomicLoad(this, node, AtomicWidth::kWord64);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicLoad", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(node_t node) {\n  VisitAtomicLoad(this, node, AtomicWidth::kWord32);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32AtomicLoad", "content": "void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    AtomicLoadParameters atomic_load_params =\n        AtomicLoadParametersOf(node->op());\n    LoadRepresentation load_rep = atomic_load_params.representation();\n    ArchOpcode opcode;\n    switch (load_rep.representation()) {\n      case MachineRepresentation::kWord8:\n        opcode = load_rep.IsSigned() ? kAtomicLoadInt8 : kAtomicLoadUint8;\n        break;\n      case MachineRepresentation::kWord16:\n        opcode = load_rep.IsSigned() ? kAtomicLoadInt16 : kAtomicLoadUint16;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:\n      case MachineRepresentation::kWord32:\n        opcode = kAtomicLoadWord32;\n        break;\n      default:\n        UNREACHABLE();\n    }\n    VisitAtomicLoad(this, node, opcode, AtomicWidth::kWord32);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitFloat64Neg", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Neg(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex input = this->Get(node).input(0);\n    const Operation& input_op = this->Get(input);\n    if (input_op.Is<Opmask::kFloat64Mul>() && CanCover(node, input)) {\n      const FloatBinopOp& mul = input_op.Cast<FloatBinopOp>();\n      Emit(kArm64Float64Fnmul, g.DefineAsRegister(node),\n           g.UseRegister(mul.left()), g.UseRegister(mul.right()));\n      return;\n    }\n    VisitRR(this, kArm64Float64Neg, node);\n  } else {\n    Node* in = node->InputAt(0);\n    if (in->opcode() == IrOpcode::kFloat64Mul && CanCover(node, in)) {\n      Float64BinopMatcher m(in);\n      Emit(kArm64Float64Fnmul, g.DefineAsRegister(node),\n           g.UseRegister(m.left().node()), g.UseRegister(m.right().node()));\n      return;\n    }\n    VisitRR(this, kArm64Float64Neg, node);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Neg(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitFloat64Neg", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Neg(node_t node) {\n  VisitRR(this, kRiscvNegD, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Neg(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitFloat32Neg", "content": "void InstructionSelectorT<Adapter>::VisitFloat32Neg(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex input = this->Get(node).input(0);\n    const Operation& input_op = this->Get(input);\n    if (input_op.Is<Opmask::kFloat32Mul>() && CanCover(node, input)) {\n      const FloatBinopOp& mul = input_op.Cast<FloatBinopOp>();\n      Emit(kArm64Float32Fnmul, g.DefineAsRegister(node),\n           g.UseRegister(mul.left()), g.UseRegister(mul.right()));\n      return;\n    }\n    VisitRR(this, kArm64Float32Neg, node);\n\n  } else {\n    Node* in = node->InputAt(0);\n    if (in->opcode() == IrOpcode::kFloat32Mul && CanCover(node, in)) {\n      Float32BinopMatcher m(in);\n      Emit(kArm64Float32Fnmul, g.DefineAsRegister(node),\n           g.UseRegister(m.left().node()), g.UseRegister(m.right().node()));\n      return;\n    }\n    VisitRR(this, kArm64Float32Neg, node);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat32Neg(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitFloat32Neg", "content": "void InstructionSelectorT<Adapter>::VisitFloat32Neg(node_t node) {\n  VisitRR(this, kRiscvNegS, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat32Neg(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);\n  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(node_t node) {\n    FlagsContinuation cont =\n        FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);\n    VisitWord64Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint64LessThan", "content": "void InstructionSelectorT<Adapter>::VisitUint64LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);\n  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64LessThan(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint64LessThan", "content": "void InstructionSelectorT<Adapter>::VisitUint64LessThan(node_t node) {\n    FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);\n    VisitWord64Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64LessThan(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);\n  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(node_t node) {\n    FlagsContinuation cont =\n        FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);\n    VisitWord64Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64LessThan", "content": "void InstructionSelectorT<Adapter>::VisitInt64LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);\n  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64LessThan(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64LessThan", "content": "void InstructionSelectorT<Adapter>::VisitInt64LessThan(node_t node) {\n    FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);\n    VisitWord64Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64LessThan(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(node_t node) {\n  node_t ovf = FindProjection(node, 1);\n  if (this->valid(ovf)) {\n    // ARM64 doesn't set the overflow flag for multiplication, so we need to\n    // test on kNotEqual. Here is the code sequence used:\n    //   mul result, left, right\n    //   smulh high, left, right\n    //   cmp high, result, asr 63\n    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);\n    return EmitInt64MulWithOverflow(this, node, &cont);\n  }\n  FlagsContinuation cont;\n  EmitInt64MulWithOverflow(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      // RISCV64 doesn't set the overflow flag for multiplication, so we need to\n      // test on kNotEqual. Here is the code sequence used:\n      //   mulh rdh, left, right\n      //   mul rdl, left, right\n      //   srai temp, rdl, 63\n      //   xor overflow, rdl, temp\n      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);\n      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvMulOvf64,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvMulOvf64, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex ovf = FindProjection(node, 1);\n    if (ovf.valid()) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop(this, node, RegisterRepresentation::Word64(), kArm64Sub,\n                        kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop(this, node, RegisterRepresentation::Word64(), kArm64Sub,\n               kArithmeticImm, &cont);\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,\n                                                    kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,\n                                           kArithmeticImm, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvSubOvf64,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvSubOvf64, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex ovf = FindProjection(node, 1);\n    if (ovf.valid()) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop(this, node, RegisterRepresentation::Word64(), kArm64Add,\n                        kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop(this, node, RegisterRepresentation::Word64(), kArm64Add,\n               kArithmeticImm, &cont);\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,\n                                                    kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,\n                                           kArithmeticImm, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvAddOvf64,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvAddOvf64, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(node_t node) {\n  node_t ovf = FindProjection(node, 1);\n  if (this->valid(ovf)) {\n    // ARM64 doesn't set the overflow flag for multiplication, so we need to\n    // test on kNotEqual. Here is the code sequence used:\n    //   smull result, left, right\n    //   cmp result.X(), Operand(result, SXTW)\n    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);\n    return EmitInt32MulWithOverflow(this, node, &cont);\n  }\n  FlagsContinuation cont;\n  EmitInt32MulWithOverflow(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvMulOvf32,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvMulOvf32, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex ovf = FindProjection(node, 1);\n    if (ovf.valid()) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop(this, node, RegisterRepresentation::Word32(),\n                        kArm64Sub32, kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop(this, node, RegisterRepresentation::Word32(), kArm64Sub32,\n               kArithmeticImm, &cont);\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,\n                                                    kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,\n                                           kArithmeticImm, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvSubOvf,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvSubOvf, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    OpIndex ovf = FindProjection(node, 1);\n    if (ovf.valid() && IsUsed(ovf)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop(this, node, RegisterRepresentation::Word32(),\n                        kArm64Add32, kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop(this, node, RegisterRepresentation::Word32(), kArm64Add32,\n               kArithmeticImm, &cont);\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,\n                                                    kArithmeticImm, &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,\n                                           kArithmeticImm, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow", "content": "void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {\n      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);\n      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvAddOvf,\n                                                    &cont);\n    }\n    FlagsContinuation cont;\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvAddOvf, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Equal", "content": "void InstructionSelectorT<Adapter>::VisitWord64Equal(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ComparisonOp& equal = this->Get(node).template Cast<ComparisonOp>();\n    DCHECK_EQ(equal.kind, ComparisonOp::Kind::kEqual);\n    if (this->MatchIntegralZero(equal.right()) &&\n        CanCover(node, equal.left())) {\n      if (this->Get(equal.left()).template Is<Opmask::kWord64BitwiseAnd>()) {\n        return VisitWordCompare(this, equal.left(), kArm64Tst, &cont,\n                                kLogical64Imm);\n      }\n      return VisitWord64Test(this, equal.left(), &cont);\n    }\n    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n  } else {\n    Node* const user = node;\n    Int64BinopMatcher m(user);\n    if (m.right().Is(0)) {\n      Node* const value = m.left().node();\n      if (CanCover(user, value)) {\n        switch (value->opcode()) {\n          case IrOpcode::kWord64And:\n            return VisitWordCompare(this, value, kArm64Tst, &cont,\n                                    kLogical64Imm);\n          default:\n            break;\n        }\n        return VisitWord64Test(this, value, &cont);\n      }\n    }\n    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Equal(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Equal", "content": "void InstructionSelectorT<Adapter>::VisitWord64Equal(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);\n    const ComparisonOp& equal = this->Get(node).template Cast<ComparisonOp>();\n    DCHECK_EQ(equal.kind, ComparisonOp::Kind::kEqual);\n    if (this->MatchIntegralZero(equal.right())) {\n      return VisitWordCompareZero(node, equal.left(), &cont);\n    }\n    VisitWord64Compare(this, node, &cont);\n  } else {\n    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);\n    Int64BinopMatcher m(node);\n    if (m.right().Is(0)) {\n      return VisitWordCompareZero(m.node(), m.left().node(), &cont);\n    }\n\n    VisitWord64Compare(this, node, &cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Equal(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);\n  VisitWord32Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);\n  VisitWordCompare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint32LessThan", "content": "void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);\n  VisitWord32Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint32LessThan", "content": "void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);\n  VisitWordCompare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);\n  VisitWord32Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual", "content": "void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {\n  FlagsContinuation cont =\n      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);\n  VisitWordCompare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32LessThan", "content": "void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);\n  VisitWord32Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32LessThan", "content": "void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {\n  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);\n  VisitWordCompare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const Operation& equal = Get(node);\n  DCHECK(equal.Is<ComparisonOp>());\n  OpIndex left = equal.input(0);\n  OpIndex right = equal.input(1);\n  OpIndex user = node;\n  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);\n\n  if (MatchZero(right)) {\n    OpIndex value = left;\n    if (CanCover(user, value)) {\n      const Operation& value_op = Get(value);\n      if (value_op.Is<Opmask::kWord32Add>() ||\n          value_op.Is<Opmask::kWord32BitwiseAnd>()) {\n        return VisitWord32Compare(this, node, &cont);\n      }\n      if (value_op.Is<Opmask::kWord32Sub>()) {\n        return VisitWordCompare(this, value, kArm64Cmp32, &cont,\n                                kArithmeticImm);\n      }\n      if (value_op.Is<Opmask::kWord32Equal>()) {\n        // Word32Equal(Word32Equal(x, y), 0) => Word32Compare(x, y, ne).\n        // A new FlagsContinuation is needed as instead of generating the result\n        // for {node}, it is generated for {value}.\n        FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, value);\n        cont.Negate();\n        VisitWord32Compare(this, value, &cont);\n        EmitIdentity(node);\n        return;\n      }\n      return VisitWord32Test(this, value, &cont);\n    }\n  }\n\n  if (isolate() && (V8_STATIC_ROOTS_BOOL ||\n                    (COMPRESS_POINTERS_BOOL && !isolate()->bootstrapper()))) {\n    Arm64OperandGeneratorT<TurboshaftAdapter> g(this);\n    const RootsTable& roots_table = isolate()->roots_table();\n    RootIndex root_index;\n    Handle<HeapObject> right;\n    // HeapConstants and CompressedHeapConstants can be treated the same when\n    // using them as an input to a 32-bit comparison. Check whether either is\n    // present.\n    if (MatchTaggedConstant(node, &right) && !right.is_null() &&\n        roots_table.IsRootHandle(right, &root_index)) {\n      if (RootsTable::IsReadOnly(root_index)) {\n        Tagged_t ptr =\n            MacroAssemblerBase::ReadOnlyRootPtr(root_index, isolate());\n        if (g.CanBeImmediate(ptr, ImmediateMode::kArithmeticImm)) {\n          return VisitCompare(this, kArm64Cmp32, g.UseRegister(left),\n                              g.TempImmediate(ptr), &cont);\n        }\n      }\n    }\n  }\n  VisitWord32Compare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal(node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const Operation& equal = Get(node);\n  DCHECK(equal.Is<ComparisonOp>());\n  OpIndex left = equal.input(0);\n  OpIndex right = equal.input(1);\n  OpIndex user = node;\n  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);\n\n  if (MatchZero(right)) {\n    return VisitWordCompareZero(user, left, &cont);\n  }\n  VisitWordCompare(this, node, &cont);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord32Equal(node_t node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  Arm64OperandGeneratorT<TurboshaftAdapter> g(this);\n  // Try to combine with comparisons against 0 by simply inverting the branch.\n  ConsumeEqualZero(&user, &value, cont);\n\n  // Remove Word64->Word32 truncation.\n  if (this->is_truncate_word64_to_word32(value) && CanCover(user, value)) {\n    user = value;\n    value = this->remove_truncate_word64_to_word32(value);\n  }\n\n  // Try to match bit checks to create TBZ/TBNZ instructions.\n  // Unlike the switch below, CanCover check is not needed here.\n  // If there are several uses of the given operation, we will generate a TBZ\n  // instruction for each. This is useful even if there are other uses of the\n  // arithmetic result, because it moves dependencies further back.\n  const Operation& value_op = Get(value);\n\n  if (cont->IsBranch()) {\n    if (value_op.Is<Opmask::kWord64Equal>()) {\n      const ComparisonOp& equal = value_op.Cast<ComparisonOp>();\n      if (MatchIntegralZero(equal.right())) {\n        const WordBinopOp* left_binop =\n            Get(equal.left()).TryCast<WordBinopOp>();\n        if (left_binop) {\n          TestAndBranchMatcherTurboshaft matcher(this, *left_binop);\n          if (matcher.Matches()) {\n            // If the mask has only one bit set, we can use tbz/tbnz.\n            DCHECK((cont->condition() == kEqual) ||\n                   (cont->condition() == kNotEqual));\n            Arm64OperandGeneratorT<TurboshaftAdapter> gen(this);\n            cont->OverwriteAndNegateIfEqual(kEqual);\n            EmitWithContinuation(kArm64TestAndBranch,\n                                 gen.UseRegister(left_binop->left()),\n                                 gen.TempImmediate(matcher.bit()), cont);\n            return;\n          }\n        }\n      }\n    }\n\n    if (const WordBinopOp* value_binop = value_op.TryCast<WordBinopOp>()) {\n      TestAndBranchMatcherTurboshaft matcher(this, *value_binop);\n      if (matcher.Matches()) {\n        // If the mask has only one bit set, we can use tbz/tbnz.\n        DCHECK((cont->condition() == kEqual) ||\n               (cont->condition() == kNotEqual));\n        InstructionCode opcode = value_binop->rep.MapTaggedToWord() ==\n                                         RegisterRepresentation::Word32()\n                                     ? kArm64TestAndBranch32\n                                     : kArm64TestAndBranch;\n        Arm64OperandGeneratorT<TurboshaftAdapter> gen(this);\n        EmitWithContinuation(opcode, gen.UseRegister(value_binop->left()),\n                             gen.TempImmediate(matcher.bit()), cont);\n        return;\n      }\n    }\n  }\n\n  if (CanCover(user, value)) {\n    if (const ComparisonOp* comparison = value_op.TryCast<ComparisonOp>()) {\n      switch (comparison->rep.MapTaggedToWord().value()) {\n        case RegisterRepresentation::Word32():\n          cont->OverwriteAndNegateIfEqual(\n              GetComparisonFlagCondition(*comparison));\n          return VisitWord32Compare(this, value, cont);\n\n        case RegisterRepresentation::Word64():\n          cont->OverwriteAndNegateIfEqual(\n              GetComparisonFlagCondition(*comparison));\n\n          if (comparison->kind == ComparisonOp::Kind::kEqual) {\n            const Operation& left_op = Get(comparison->left());\n            if (MatchIntegralZero(comparison->right()) &&\n                left_op.Is<Opmask::kWord64BitwiseAnd>() &&\n                CanCover(value, comparison->left())) {\n              return VisitWordCompare(this, comparison->left(), kArm64Tst, cont,\n                                      kLogical64Imm);\n            }\n          }\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n\n        case RegisterRepresentation::Float32():\n          switch (comparison->kind) {\n            case ComparisonOp::Kind::kEqual:\n              cont->OverwriteAndNegateIfEqual(kEqual);\n              return VisitFloat32Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThan:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n              return VisitFloat32Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThanOrEqual:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n              return VisitFloat32Compare(this, value, cont);\n            default:\n              UNREACHABLE();\n          }\n\n        case RegisterRepresentation::Float64():\n          switch (comparison->kind) {\n            case ComparisonOp::Kind::kEqual:\n              cont->OverwriteAndNegateIfEqual(kEqual);\n              return VisitFloat64Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThan:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n              return VisitFloat64Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThanOrEqual:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n              return VisitFloat64Compare(this, value, cont);\n            default:\n              UNREACHABLE();\n          }\n\n        default:\n          break;\n      }\n    } else if (const ProjectionOp* projection =\n                   value_op.TryCast<ProjectionOp>()) {\n      // Check if this is the overflow output projection of an\n      // <Operation>WithOverflow node.\n      if (projection->index == 1u) {\n        // We cannot combine the <Operation>WithOverflow with this branch\n        // unless the 0th projection (the use of the actual value of the\n        // <Operation> is either nullptr, which means there's no use of the\n        // actual value, or was already defined, which means it is scheduled\n        // *AFTER* this branch).\n        OpIndex node = projection->input();\n        OpIndex result = FindProjection(node, 0);\n        if (!result.valid() || IsDefined(result)) {\n          if (const OverflowCheckedBinopOp* binop =\n                  TryCast<OverflowCheckedBinopOp>(node)) {\n            const bool is64 = binop->rep == WordRepresentation::Word64();\n            switch (binop->kind) {\n              case OverflowCheckedBinopOp::Kind::kSignedAdd:\n                cont->OverwriteAndNegateIfEqual(kOverflow);\n                return VisitBinop(this, node, binop->rep,\n                                  is64 ? kArm64Add : kArm64Add32,\n                                  kArithmeticImm, cont);\n              case OverflowCheckedBinopOp::Kind::kSignedSub:\n                cont->OverwriteAndNegateIfEqual(kOverflow);\n                return VisitBinop(this, node, binop->rep,\n                                  is64 ? kArm64Sub : kArm64Sub32,\n                                  kArithmeticImm, cont);\n              case OverflowCheckedBinopOp::Kind::kSignedMul:\n                if (is64) {\n                  // ARM64 doesn't set the overflow flag for multiplication, so\n                  // we need to test on kNotEqual. Here is the code sequence\n                  // used:\n                  //   mul result, left, right\n                  //   smulh high, left, right\n                  //   cmp high, result, asr 63\n                  cont->OverwriteAndNegateIfEqual(kNotEqual);\n                  return EmitInt64MulWithOverflow(this, node, cont);\n                } else {\n                  // ARM64 doesn't set the overflow flag for multiplication, so\n                  // we need to test on kNotEqual. Here is the code sequence\n                  // used:\n                  //   smull result, left, right\n                  //   cmp result.X(), Operand(result, SXTW)\n                  cont->OverwriteAndNegateIfEqual(kNotEqual);\n                  return EmitInt32MulWithOverflow(this, node, cont);\n                }\n            }\n          }\n        }\n      }\n    } else if (value_op.Is<Opmask::kWord32Add>()) {\n      return VisitWordCompare(this, value, kArm64Cmn32, cont, kArithmeticImm);\n    } else if (value_op.Is<Opmask::kWord32Sub>()) {\n      return VisitWord32Compare(this, value, cont);\n    } else if (value_op.Is<Opmask::kWord32BitwiseAnd>()) {\n      if (TryMatchConditionalCompareChainBranch(this, zone(), value, cont)) {\n        return;\n      }\n      return VisitWordCompare(this, value, kArm64Tst32, cont, kLogical32Imm);\n    } else if (value_op.Is<Opmask::kWord64BitwiseAnd>()) {\n      return VisitWordCompare(this, value, kArm64Tst, cont, kLogical64Imm);\n    } else if (value_op.Is<Opmask::kWord32BitwiseOr>()) {\n      if (TryMatchConditionalCompareChainBranch(this, zone(), value, cont)) {\n        return;\n      }\n    } else if (value_op.Is<StackPointerGreaterThanOp>()) {\n      cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);\n      return VisitStackPointerGreaterThan(value, cont);\n    }\n  }\n\n  // Branch could not be combined with a compare, compare against 0 and\n  // branch.\n  if (cont->IsBranch()) {\n    Emit(cont->Encode(kArm64CompareAndBranch32), g.NoOutput(),\n         g.UseRegister(value), g.Label(cont->true_block()),\n         g.Label(cont->false_block()));\n  } else {\n    VisitCompare(this, cont->Encode(kArm64Tst32), g.UseRegister(value),\n                 g.UseRegister(value), cont);\n  }\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  // Try to combine with comparisons against 0 by simply inverting the branch.\n  while (const ComparisonOp* equal =\n             this->TryCast<Opmask::kWord32Equal>(value)) {\n    if (!CanCover(user, value)) break;\n    if (!MatchIntegralZero(equal->right())) break;\n\n    user = value;\n    value = equal->left();\n    cont->Negate();\n  }\n\n  const Operation& value_op = Get(value);\n  if (CanCover(user, value)) {\n    if (const ComparisonOp* comparison = value_op.TryCast<ComparisonOp>()) {\n      switch (comparison->rep.value()) {\n        case RegisterRepresentation::Word32():\n          cont->OverwriteAndNegateIfEqual(\n              GetComparisonFlagCondition(*comparison));\n          return VisitWordCompare(this, value, cont);\n        case RegisterRepresentation::Float32():\n          switch (comparison->kind) {\n            case ComparisonOp::Kind::kEqual:\n              cont->OverwriteAndNegateIfEqual(kEqual);\n              return VisitFloat32Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThan:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n              return VisitFloat32Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThanOrEqual:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n              return VisitFloat32Compare(this, value, cont);\n            default:\n              UNREACHABLE();\n          }\n        case RegisterRepresentation::Float64():\n          switch (comparison->kind) {\n            case ComparisonOp::Kind::kEqual:\n              cont->OverwriteAndNegateIfEqual(kEqual);\n              return VisitFloat64Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThan:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n              return VisitFloat64Compare(this, value, cont);\n            case ComparisonOp::Kind::kSignedLessThanOrEqual:\n              cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n              return VisitFloat64Compare(this, value, cont);\n            default:\n              UNREACHABLE();\n          }\n        default:\n          break;\n      }\n    } else if (const ProjectionOp* projection =\n                   value_op.TryCast<ProjectionOp>()) {\n      // Check if this is the overflow output projection of an\n      // <Operation>WithOverflow node.\n      if (projection->index == 1u) {\n        // We cannot combine the <Operation>WithOverflow with this branch\n        // unless the 0th projection (the use of the actual value of the\n        // <Operation> is either nullptr, which means there's no use of the\n        // actual value, or was already defined, which means it is scheduled\n        // *AFTER* this branch).\n        OpIndex node = projection->input();\n        OpIndex result = FindProjection(node, 0);\n        if (!result.valid() || IsDefined(result)) {\n          if (const OverflowCheckedBinopOp* binop =\n                  TryCast<OverflowCheckedBinopOp>(node)) {\n            const bool is64 = binop->rep == WordRepresentation::Word64();\n            if (is64) {\n              TRACE_UNIMPL();\n            } else {\n              switch (binop->kind) {\n                case OverflowCheckedBinopOp::Kind::kSignedAdd:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurboshaftAdapter, Int32BinopMatcher>(\n                      this, node, kRiscvAddOvf, cont);\n                case OverflowCheckedBinopOp::Kind::kSignedSub:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurboshaftAdapter, Int32BinopMatcher>(\n                      this, node, kRiscvSubOvf, cont);\n                case OverflowCheckedBinopOp::Kind::kSignedMul:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurboshaftAdapter, Int32BinopMatcher>(\n                      this, node, kRiscvMulOvf32, cont);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n\n  // Continuation could not be combined with a compare, emit compare against\n  // 0.\n  EmitWordCompareZero(this, value, cont);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) {\n  {\n    Arm64OperandGeneratorT<TurbofanAdapter> g(this);\n    // Try to combine with comparisons against 0 by simply inverting the branch.\n    while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {\n      Int32BinopMatcher m(value);\n      if (!m.right().Is(0)) break;\n\n      user = value;\n      value = m.left().node();\n      cont->Negate();\n    }\n\n    // Try to match bit checks to create TBZ/TBNZ instructions.\n    // Unlike the switch below, CanCover check is not needed here.\n    // If there are several uses of the given operation, we will generate a TBZ\n    // instruction for each. This is useful even if there are other uses of the\n    // arithmetic result, because it moves dependencies further back.\n    switch (value->opcode()) {\n      case IrOpcode::kWord64Equal: {\n        Int64BinopMatcher m(value);\n        if (m.right().Is(0)) {\n          Node* const left = m.left().node();\n          if (left->opcode() == IrOpcode::kWord64And) {\n            // Attempt to merge the Word64Equal(Word64And(x, y), 0) comparison\n            // into a tbz/tbnz instruction.\n            TestAndBranchMatcher<TurbofanAdapter, Uint64BinopMatcher> tbm(left,\n                                                                          cont);\n            if (tbm.Matches()) {\n              Arm64OperandGeneratorT<TurbofanAdapter> gen(this);\n              cont->OverwriteAndNegateIfEqual(kEqual);\n              this->EmitWithContinuation(kArm64TestAndBranch,\n                                         gen.UseRegister(tbm.input()),\n                                         gen.TempImmediate(tbm.bit()), cont);\n              return;\n            }\n          }\n        }\n        break;\n      }\n      case IrOpcode::kWord32And: {\n        TestAndBranchMatcher<TurbofanAdapter, Uint32BinopMatcher> tbm(value,\n                                                                      cont);\n        if (tbm.Matches()) {\n          Arm64OperandGeneratorT<TurbofanAdapter> gen(this);\n          this->EmitWithContinuation(kArm64TestAndBranch32,\n                                     gen.UseRegister(tbm.input()),\n                                     gen.TempImmediate(tbm.bit()), cont);\n          return;\n        }\n        break;\n      }\n      case IrOpcode::kWord64And: {\n        TestAndBranchMatcher<TurbofanAdapter, Uint64BinopMatcher> tbm(value,\n                                                                      cont);\n        if (tbm.Matches()) {\n          Arm64OperandGeneratorT<TurbofanAdapter> gen(this);\n          this->EmitWithContinuation(kArm64TestAndBranch,\n                                     gen.UseRegister(tbm.input()),\n                                     gen.TempImmediate(tbm.bit()), cont);\n          return;\n        }\n        break;\n      }\n      default:\n        break;\n    }\n\n    if (CanCover(user, value)) {\n      switch (value->opcode()) {\n        case IrOpcode::kWord32Equal:\n          cont->OverwriteAndNegateIfEqual(kEqual);\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kInt32LessThan:\n          cont->OverwriteAndNegateIfEqual(kSignedLessThan);\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kInt32LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kUint32LessThan:\n          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kUint32LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kWord64Equal: {\n          cont->OverwriteAndNegateIfEqual(kEqual);\n          Int64BinopMatcher m(value);\n          if (m.right().Is(0)) {\n            Node* const left = m.left().node();\n            if (CanCover(value, left) &&\n                left->opcode() == IrOpcode::kWord64And) {\n              return VisitWordCompare(this, left, kArm64Tst, cont,\n                                      kLogical64Imm);\n            }\n          }\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n        }\n        case IrOpcode::kInt64LessThan:\n          cont->OverwriteAndNegateIfEqual(kSignedLessThan);\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n        case IrOpcode::kInt64LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n        case IrOpcode::kUint64LessThan:\n          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n        case IrOpcode::kUint64LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);\n          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);\n        case IrOpcode::kFloat32Equal:\n          cont->OverwriteAndNegateIfEqual(kEqual);\n          return VisitFloat32Compare(this, value, cont);\n        case IrOpcode::kFloat32LessThan:\n          cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n          return VisitFloat32Compare(this, value, cont);\n        case IrOpcode::kFloat32LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n          return VisitFloat32Compare(this, value, cont);\n        case IrOpcode::kFloat64Equal:\n          cont->OverwriteAndNegateIfEqual(kEqual);\n          return VisitFloat64Compare(this, value, cont);\n        case IrOpcode::kFloat64LessThan:\n          cont->OverwriteAndNegateIfEqual(kFloatLessThan);\n          return VisitFloat64Compare(this, value, cont);\n        case IrOpcode::kFloat64LessThanOrEqual:\n          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);\n          return VisitFloat64Compare(this, value, cont);\n        case IrOpcode::kProjection:\n          // Check if this is the overflow output projection of an\n          // <Operation>WithOverflow node.\n          if (ProjectionIndexOf(value->op()) == 1u) {\n            // We cannot combine the <Operation>WithOverflow with this branch\n            // unless the 0th projection (the use of the actual value of the\n            // <Operation> is either nullptr, which means there's no use of the\n            // actual value, or was already defined, which means it is scheduled\n            // *AFTER* this branch).\n            Node* const node = value->InputAt(0);\n            Node* const result = NodeProperties::FindProjection(node, 0);\n            if (result == nullptr || IsDefined(result)) {\n              switch (node->opcode()) {\n                case IrOpcode::kInt32AddWithOverflow:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurbofanAdapter, Int32BinopMatcher>(\n                      this, node, kArm64Add32, kArithmeticImm, cont);\n                case IrOpcode::kInt32SubWithOverflow:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurbofanAdapter, Int32BinopMatcher>(\n                      this, node, kArm64Sub32, kArithmeticImm, cont);\n                case IrOpcode::kInt32MulWithOverflow:\n                  // ARM64 doesn't set the overflow flag for multiplication, so\n                  // we need to test on kNotEqual. Here is the code sequence\n                  // used:\n                  //   smull result, left, right\n                  //   cmp result.X(), Operand(result, SXTW)\n                  cont->OverwriteAndNegateIfEqual(kNotEqual);\n                  return EmitInt32MulWithOverflow(this, node, cont);\n                case IrOpcode::kInt64AddWithOverflow:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurbofanAdapter, Int64BinopMatcher>(\n                      this, node, kArm64Add, kArithmeticImm, cont);\n                case IrOpcode::kInt64SubWithOverflow:\n                  cont->OverwriteAndNegateIfEqual(kOverflow);\n                  return VisitBinop<TurbofanAdapter, Int64BinopMatcher>(\n                      this, node, kArm64Sub, kArithmeticImm, cont);\n                case IrOpcode::kInt64MulWithOverflow:\n                  // ARM64 doesn't set the overflow flag for multiplication, so\n                  // we need to test on kNotEqual. Here is the code sequence\n                  // used:\n                  //   mul result, left, right\n                  //   smulh high, left, right\n                  //   cmp high, result, asr 63\n                  cont->OverwriteAndNegateIfEqual(kNotEqual);\n                  return EmitInt64MulWithOverflow(this, node, cont);\n                default:\n                  break;\n              }\n            }\n          }\n          break;\n        case IrOpcode::kInt32Add:\n          return VisitWordCompare(this, value, kArm64Cmn32, cont,\n                                  kArithmeticImm);\n        case IrOpcode::kInt32Sub:\n          return VisitWord32Compare(this, value, cont);\n        case IrOpcode::kWord32And:\n          return VisitWordCompare(this, value, kArm64Tst32, cont,\n                                  kLogical32Imm);\n        case IrOpcode::kWord64And:\n          return VisitWordCompare(this, value, kArm64Tst, cont, kLogical64Imm);\n        case IrOpcode::kStackPointerGreaterThan:\n          cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);\n          return VisitStackPointerGreaterThan(value, cont);\n        default:\n          break;\n      }\n    }\n\n    // Branch could not be combined with a compare, compare against 0 and\n    // branch.\n    if (cont->IsBranch()) {\n      Emit(cont->Encode(kArm64CompareAndBranch32), g.NoOutput(),\n           g.UseRegister(value), g.Label(cont->true_block()),\n           g.Label(cont->false_block()));\n    } else {\n      VisitCompare(this, cont->Encode(kArm64Tst32), g.UseRegister(value),\n                   g.UseRegister(value), cont);\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuation* cont) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuationT<TurbofanAdapter>* cont) {\n  // Try to combine with comparisons against 0 by simply inverting the branch.\n  while (CanCover(user, value)) {\n    if (value->opcode() == IrOpcode::kWord32Equal) {\n      Int32BinopMatcher m(value);\n      if (!m.right().Is(0)) break;\n      user = value;\n      value = m.left().node();\n    } else if (value->opcode() == IrOpcode::kWord64Equal) {\n      Int64BinopMatcher m(value);\n      if (!m.right().Is(0)) break;\n      user = value;\n      value = m.left().node();\n    } else {\n      break;\n    }\n\n    cont->Negate();\n  }\n\n  if (CanCover(user, value)) {\n    switch (value->opcode()) {\n      case IrOpcode::kWord32Equal:\n        cont->OverwriteAndNegateIfEqual(kEqual);\n        return VisitWordCompare(this, value, cont);\n      case IrOpcode::kInt32LessThan:\n        cont->OverwriteAndNegateIfEqual(kSignedLessThan);\n        return VisitWordCompare(this, value, cont);\n      case IrOpcode::kInt32LessThanOrEqual:\n        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);\n        return VisitWordCompare(this, value, cont);\n      case IrOpcode::kUint32LessThan:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);\n        return VisitWordCompare(this, value, cont);\n      case IrOpcode::kUint32LessThanOrEqual:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);\n        return VisitWordCompare(this, value, cont);\n      case IrOpcode::kFloat32Equal:\n        cont->OverwriteAndNegateIfEqual(kEqual);\n        return VisitFloat32Compare(this, value, cont);\n      case IrOpcode::kFloat32LessThan:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);\n        return VisitFloat32Compare(this, value, cont);\n      case IrOpcode::kFloat32LessThanOrEqual:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);\n        return VisitFloat32Compare(this, value, cont);\n      case IrOpcode::kFloat64Equal:\n        cont->OverwriteAndNegateIfEqual(kEqual);\n        return VisitFloat64Compare(this, value, cont);\n      case IrOpcode::kFloat64LessThan:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);\n        return VisitFloat64Compare(this, value, cont);\n      case IrOpcode::kFloat64LessThanOrEqual:\n        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);\n        return VisitFloat64Compare(this, value, cont);\n      case IrOpcode::kProjection:\n        // Check if this is the overflow output projection of an\n        // <Operation>WithOverflow node.\n        if (ProjectionIndexOf(value->op()) == 1u) {\n          // We cannot combine the <Operation>WithOverflow with this branch\n          // unless the 0th projection (the use of the actual value of the\n          // <Operation> is either nullptr, which means there's no use of the\n          // actual value, or was already defined, which means it is scheduled\n          // *AFTER* this branch).\n          Node* const node = value->InputAt(0);\n          Node* const result = NodeProperties::FindProjection(node, 0);\n          if (result == nullptr || IsDefined(result)) {\n            switch (node->opcode()) {\n              case IrOpcode::kInt32AddWithOverflow:\n                cont->OverwriteAndNegateIfEqual(kOverflow);\n                return VisitBinop<TurbofanAdapter, Int32BinopMatcher>(\n                    this, node, kRiscvAddOvf, cont);\n              case IrOpcode::kInt32SubWithOverflow:\n                cont->OverwriteAndNegateIfEqual(kOverflow);\n                return VisitBinop<TurbofanAdapter, Int32BinopMatcher>(\n                    this, node, kRiscvSubOvf, cont);\n              case IrOpcode::kInt32MulWithOverflow:\n                cont->OverwriteAndNegateIfEqual(kOverflow);\n                return VisitBinop<TurbofanAdapter, Int32BinopMatcher>(\n                    this, node, kRiscvMulOvf32, cont);\n              case IrOpcode::kInt64AddWithOverflow:\n              case IrOpcode::kInt64SubWithOverflow:\n                TRACE_UNIMPL();\n                break;\n              default:\n                break;\n            }\n          }\n        }\n        break;\n      case IrOpcode::kWord32And:\n        return VisitWordCompare(this, value, kRiscvTst32, cont, true);\n      case IrOpcode::kStackPointerGreaterThan:\n        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);\n        return VisitStackPointerGreaterThan(value, cont);\n      default:\n        break;\n    }\n  }\n\n  // Continuation could not be combined with a compare, emit compare against\n  // 0.\n  EmitWordCompareZero(this, value, cont);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(\n    node_t user, node_t value, FlagsContinuationT<TurbofanAdapter>* cont) "}], [{"name": "VisitAtomicStore", "content": "void VisitAtomicStore(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node, AtomicWidth width) {\n  using node_t = typename Adapter::node_t;\n  Arm64OperandGeneratorT<Adapter> g(selector);\n  auto store = selector->store_view(node);\n  node_t base = store.base();\n  node_t index = selector->value(store.index());\n  node_t value = store.value();\n  DCHECK_EQ(store.displacement(), 0);\n\n  // The memory order is ignored as both release and sequentially consistent\n  // stores can emit STLR.\n  // https://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html\n  AtomicStoreParameters store_params = AtomicStoreParametersOf(selector, node);\n  WriteBarrierKind write_barrier_kind = store_params.write_barrier_kind();\n  MachineRepresentation rep = store_params.representation();\n\n  if (v8_flags.enable_unconditional_write_barriers &&\n      CanBeTaggedOrCompressedPointer(rep)) {\n    write_barrier_kind = kFullWriteBarrier;\n  }\n\n  InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index),\n                                 g.UseUniqueRegister(value)};\n  InstructionOperand temps[] = {g.TempRegister()};\n  InstructionCode code;\n\n  if (write_barrier_kind != kNoWriteBarrier &&\n      !v8_flags.disable_write_barriers) {\n    DCHECK(CanBeTaggedOrCompressedPointer(rep));\n    DCHECK_EQ(AtomicWidthSize(width), kTaggedSize);\n\n    RecordWriteMode record_write_mode =\n        WriteBarrierKindToRecordWriteMode(write_barrier_kind);\n    code = kArchAtomicStoreWithWriteBarrier;\n    code |= RecordWriteModeField::encode(record_write_mode);\n  } else {\n    switch (rep) {\n      case MachineRepresentation::kWord8:\n        code = kAtomicStoreWord8;\n        break;\n      case MachineRepresentation::kWord16:\n        code = kAtomicStoreWord16;\n        break;\n      case MachineRepresentation::kWord32:\n        code = kAtomicStoreWord32;\n        break;\n      case MachineRepresentation::kWord64:\n        DCHECK_EQ(width, AtomicWidth::kWord64);\n        code = kArm64Word64AtomicStoreWord64;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:\n        DCHECK_EQ(AtomicWidthSize(width), kTaggedSize);\n        code = kArm64StlrCompressTagged;\n        break;\n      case MachineRepresentation::kCompressedPointer:  // Fall through.\n      case MachineRepresentation::kCompressed:\n        CHECK(COMPRESS_POINTERS_BOOL);\n        DCHECK_EQ(width, AtomicWidth::kWord32);\n        code = kArm64StlrCompressTagged;\n        break;\n      default:\n        UNREACHABLE();\n    }\n    code |= AtomicWidthField::encode(width);\n  }\n\n  if (store_params.kind() == MemoryAccessKind::kProtected) {\n    code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);\n  }\n\n  code |= AddressingModeField::encode(kMode_MRR);\n  selector->Emit(code, 0, nullptr, arraysize(inputs), inputs, arraysize(temps),\n                 temps);\n}", "name_and_para": "void VisitAtomicStore(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node, AtomicWidth width) "}, {"name": "VisitAtomicStore", "content": "void VisitAtomicStore(InstructionSelectorT<Adapter>* selector, Node* node,\n                      ArchOpcode opcode, AtomicWidth width) {\n  RiscvOperandGeneratorT<Adapter> g(selector);\n  Node* base = node->InputAt(0);\n  Node* index = node->InputAt(1);\n  Node* value = node->InputAt(2);\n\n  if (g.CanBeImmediate(index, opcode)) {\n    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |\n                       AtomicWidthField::encode(width),\n                   g.NoOutput(), g.UseRegisterOrImmediateZero(value),\n                   g.UseRegister(base), g.UseImmediate(index));\n  } else {\n    InstructionOperand addr_reg = g.TempRegister();\n    selector->Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None),\n                   addr_reg, g.UseRegister(index), g.UseRegister(base));\n    // Emit desired store opcode, using temp addr_reg.\n    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |\n                       AtomicWidthField::encode(width),\n                   g.NoOutput(), g.UseRegisterOrImmediateZero(value), addr_reg,\n                   g.TempImmediate(0));\n  }\n}", "name_and_para": "void VisitAtomicStore(InstructionSelectorT<Adapter>* selector, Node* node,\n                      ArchOpcode opcode, AtomicWidth width) "}], [{"name": "AtomicStoreParametersOf", "content": "AtomicStoreParameters AtomicStoreParametersOf(\n    InstructionSelectorT<Adapter>* selector, typename Adapter::node_t node) {\n  auto store = selector->store_view(node);\n  return AtomicStoreParameters(store.stored_rep().representation(),\n                               store.stored_rep().write_barrier_kind(),\n                               store.memory_order().value(),\n                               store.access_kind());\n}", "name_and_para": "AtomicStoreParameters AtomicStoreParametersOf(\n    InstructionSelectorT<Adapter>* selector, typename Adapter::node_t node) "}, {"name": "AtomicStoreParametersOf", "content": "AtomicStoreParameters AtomicStoreParametersOf(\n    InstructionSelectorT<Adapter>* selector, typename Adapter::node_t node) {\n  auto store = selector->store_view(node);\n  return AtomicStoreParameters(store.stored_rep().representation(),\n                               store.stored_rep().write_barrier_kind(),\n                               store.memory_order().value(),\n                               store.access_kind());\n}", "name_and_para": "AtomicStoreParameters AtomicStoreParametersOf(\n    InstructionSelectorT<Adapter>* selector, typename Adapter::node_t node) "}], [{"name": "VisitWordCompare", "content": "void VisitWordCompare(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node, InstructionCode opcode,\n                      FlagsContinuationT<Adapter>* cont,\n                      ImmediateMode immediate_mode) {\n  Arm64OperandGeneratorT<Adapter> g(selector);\n  DCHECK_EQ(selector->value_input_count(node), 2);\n  auto left = selector->input_at(node, 0);\n  auto right = selector->input_at(node, 1);\n\n  // If one of the two inputs is an immediate, make sure it's on the right.\n  if (!g.CanBeImmediate(right, immediate_mode) &&\n      g.CanBeImmediate(left, immediate_mode)) {\n    cont->Commute();\n    std::swap(left, right);\n  }\n\n  if (opcode == kArm64Cmp && selector->is_constant(right)) {\n    auto constant = selector->constant_view(right);\n    if (g.IsIntegerConstant(constant)) {\n      if (TryEmitCbzOrTbz<Adapter, 64>(selector, left,\n                                       g.GetIntegerConstantValue(constant),\n                                       node, cont->condition(), cont)) {\n        return;\n      }\n    }\n  }\n\n  VisitCompare(selector, opcode, g.UseRegister(left),\n               g.UseOperand(right, immediate_mode), cont);\n}", "name_and_para": "void VisitWordCompare(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node, InstructionCode opcode,\n                      FlagsContinuationT<Adapter>* cont,\n                      ImmediateMode immediate_mode) "}, {"name": "VisitWordCompare", "content": "void VisitWordCompare(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node,\n                      FlagsContinuationT<Adapter>* cont) {\n  VisitWordCompare(selector, node, kRiscvCmp, cont, false);\n}", "name_and_para": "void VisitWordCompare(InstructionSelectorT<Adapter>* selector,\n                      typename Adapter::node_t node,\n                      FlagsContinuationT<Adapter>* cont) "}], [{"name": "InstructionSelectorT<Adapter>::EmitPrepareArguments", "content": "void InstructionSelectorT<Adapter>::EmitPrepareArguments(\n    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  // `arguments` includes alignment \"holes\". This means that slots bigger than\n  // kSystemPointerSize, e.g. Simd128, will span across multiple arguments.\n  int claim_count = static_cast<int>(arguments->size());\n  bool needs_padding = claim_count % 2 != 0;\n  int slot = claim_count - 1;\n  claim_count = RoundUp(claim_count, 2);\n  // Bump the stack pointer.\n  if (claim_count > 0) {\n    // TODO(titzer): claim and poke probably take small immediates.\n    // TODO(titzer): it would be better to bump the sp here only\n    //               and emit paired stores with increment for non c frames.\n    Emit(kArm64Claim, g.NoOutput(), g.TempImmediate(claim_count));\n\n    if (needs_padding) {\n      Emit(kArm64Poke, g.NoOutput(), g.UseImmediate(0),\n           g.TempImmediate(claim_count - 1));\n    }\n  }\n\n  // Poke the arguments into the stack.\n  while (slot >= 0) {\n    PushParameter input0 = (*arguments)[slot];\n    // Skip holes in the param array. These represent both extra slots for\n    // multi-slot values and padding slots for alignment.\n    if (!this->valid(input0.node)) {\n      slot--;\n      continue;\n    }\n    PushParameter input1 = slot > 0 ? (*arguments)[slot - 1] : PushParameter();\n    // Emit a poke-pair if consecutive parameters have the same type.\n    // TODO(arm): Support consecutive Simd128 parameters.\n    if (this->valid(input1.node) &&\n        input0.location.GetType() == input1.location.GetType()) {\n      Emit(kArm64PokePair, g.NoOutput(), g.UseRegister(input0.node),\n           g.UseRegister(input1.node), g.TempImmediate(slot));\n      slot -= 2;\n    } else {\n      Emit(kArm64Poke, g.NoOutput(), g.UseRegister(input0.node),\n           g.TempImmediate(slot));\n      slot--;\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::EmitPrepareArguments(\n    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::EmitPrepareArguments", "content": "void InstructionSelectorT<Adapter>::EmitPrepareArguments(\n    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n\n    // Prepare for C function call.\n    if (call_descriptor->IsCFunctionCall()) {\n      Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(\n                                           call_descriptor->ParameterCount())),\n           0, nullptr, 0, nullptr);\n\n      // Poke any stack arguments.\n      int slot = kCArgSlotCount;\n      for (PushParameter input : (*arguments)) {\n        Emit(kRiscvStoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),\n             g.TempImmediate(slot << kSystemPointerSizeLog2));\n        ++slot;\n      }\n    } else {\n      int push_count = static_cast<int>(call_descriptor->ParameterSlotCount());\n      if (push_count > 0) {\n        Emit(kRiscvStackClaim, g.NoOutput(),\n             g.TempImmediate(arguments->size() << kSystemPointerSizeLog2));\n      }\n      for (size_t n = 0; n < arguments->size(); ++n) {\n        PushParameter input = (*arguments)[n];\n        if (input.node) {\n          Emit(kRiscvStoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),\n               g.TempImmediate(static_cast<int>(n << kSystemPointerSizeLog2)));\n        }\n      }\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::EmitPrepareArguments(\n    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(\n    node_t node, InstructionCode opcode) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  Emit(opcode, g.DefineAsFixed(node, d0),\n       g.UseFixed(this->input_at(node, 0), d0))\n      ->MarkAsCall();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(\n    node_t node, InstructionCode opcode) "}, {"name": "InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(\n    node_t node, InstructionCode opcode) {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  Emit(opcode, g.DefineAsFixed(node, fa0),\n       g.UseFixed(this->input_at(node, 0), fa1))\n      ->MarkAsCall();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(\n    node_t node, InstructionCode opcode) "}], [{"name": "InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(\n    node_t node, InstructionCode opcode) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  Emit(opcode, g.DefineAsFixed(node, d0),\n       g.UseFixed(this->input_at(node, 0), d0),\n       g.UseFixed(this->input_at(node, 1), d1))\n      ->MarkAsCall();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(\n    node_t node, InstructionCode opcode) "}, {"name": "InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop", "content": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(\n    node_t node, InstructionCode opcode) {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Emit(opcode, g.DefineAsFixed(node, fa0),\n         g.UseFixed(this->input_at(node, 0), fa0),\n         g.UseFixed(this->input_at(node, 1), fa1))\n        ->MarkAsCall();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(\n    node_t node, InstructionCode opcode) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  // The top 32 bits in the 64-bit register will be undefined, and\n  // must not be used by a dependent node.\n  EmitIdentity(node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(node_t node) {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Node* value = node->InputAt(0);\n    if (CanCover(node, value)) {\n      switch (value->opcode()) {\n        case IrOpcode::kWord64Sar: {\n          if (CanCover(value, value->InputAt(0)) &&\n              TryEmitExtendingLoad(this, value, node)) {\n            return;\n          } else {\n            Int64BinopMatcher m(value);\n            if (m.right().IsInRange(32, 63)) {\n              // After smi untagging no need for truncate. Combine sequence.\n              Emit(kRiscvSar64, g.DefineSameAsFirst(node),\n                   g.UseRegister(m.left().node()),\n                   g.UseImmediate(m.right().node()));\n              return;\n            }\n          }\n          break;\n        }\n        default:\n          break;\n      }\n    }\n    // Semantics of this machine IR is not clear. For example, x86 zero-extend\n    // the truncated value; arm treats it as nop thus the upper 32-bit as\n    // undefined; Riscv emits ext instruction which zero-extend the 32-bit\n    // value; for riscv, we do sign-extension of the truncated value\n    Emit(kRiscvSignExtendWord, g.DefineAsRegister(node),\n         g.UseRegister(node->InputAt(0)), g.TempImmediate(0));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  node_t value = this->input_at(node, 0);\n  if (ZeroExtendsWord32ToWord64(value)) {\n    return EmitIdentity(node);\n  }\n  Emit(kArm64Mov32, g.DefineAsRegister(node), g.UseRegister(value));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    node_t value = this->input_at(node, 0);\n    if (ZeroExtendsWord32ToWord64(value)) {\n      Emit(kArchNop, g.DefineSameAsFirst(node), g.Use(value));\n      return;\n    }\n    Emit(kRiscvZeroExtendWord, g.DefineAsRegister(node), g.UseRegister(value));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis", "content": "bool InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  DCHECK(!this->Get(node).Is<PhiOp>());\n  const Operation& op = this->Get(node);\n  // 32-bit operations will write their result in a W register (implicitly\n  // clearing the top 32-bit of the corresponding X register) so the\n  // zero-extension is a no-op.\n  switch (op.opcode) {\n    case Opcode::kWordBinop:\n      return op.Cast<WordBinopOp>().rep == WordRepresentation::Word32();\n    case Opcode::kShift:\n      return op.Cast<ShiftOp>().rep == WordRepresentation::Word32();\n    case Opcode::kComparison:\n      return op.Cast<ComparisonOp>().rep == RegisterRepresentation::Word32();\n    case Opcode::kOverflowCheckedBinop:\n      return op.Cast<OverflowCheckedBinopOp>().rep ==\n             WordRepresentation::Word32();\n    case Opcode::kProjection:\n      return ZeroExtendsWord32ToWord64NoPhis(op.Cast<ProjectionOp>().input());\n    case Opcode::kLoad: {\n      RegisterRepresentation rep =\n          op.Cast<LoadOp>().loaded_rep.ToRegisterRepresentation();\n      return rep == RegisterRepresentation::Word32();\n    }\n    default:\n      return false;\n  }\n}", "name_and_para": "bool InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis(\n    node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis", "content": "bool InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis(\n    node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  DCHECK(!this->Get(node).Is<PhiOp>());\n  const Operation& op = this->Get(node);\n  if (op.opcode == Opcode::kLoad) {\n    auto load = this->load_view(node);\n    LoadRepresentation load_rep = load.loaded_rep();\n    if (load_rep.IsUnsigned()) {\n      switch (load_rep.representation()) {\n        case MachineRepresentation::kWord8:\n        case MachineRepresentation::kWord16:\n          return true;\n        default:\n          return false;\n      }\n    }\n  }\n  // All other 32-bit operations sign-extend to the upper 32 bits\n  return false;\n}", "name_and_para": "bool InstructionSelectorT<TurboshaftAdapter>::ZeroExtendsWord32ToWord64NoPhis(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ChangeOp& change_op = this->Get(node).template Cast<ChangeOp>();\n    const Operation& input_op = this->Get(change_op.input());\n    if (input_op.Is<LoadOp>() && CanCover(node, change_op.input())) {\n      // Generate sign-extending load.\n      LoadRepresentation load_rep =\n          this->load_view(change_op.input()).loaded_rep();\n      MachineRepresentation rep = load_rep.representation();\n      InstructionCode opcode = kArchNop;\n      ImmediateMode immediate_mode = kNoImmediate;\n      switch (rep) {\n        case MachineRepresentation::kBit:  // Fall through.\n        case MachineRepresentation::kWord8:\n          opcode = load_rep.IsSigned() ? kArm64Ldrsb : kArm64Ldrb;\n          immediate_mode = kLoadStoreImm8;\n          break;\n        case MachineRepresentation::kWord16:\n          opcode = load_rep.IsSigned() ? kArm64Ldrsh : kArm64Ldrh;\n          immediate_mode = kLoadStoreImm16;\n          break;\n        case MachineRepresentation::kWord32:\n        case MachineRepresentation::kWord64:\n          // Since BitcastElider may remove nodes of\n          // IrOpcode::kTruncateInt64ToInt32 and directly use the inputs, values\n          // with kWord64 can also reach this line.\n        case MachineRepresentation::kTaggedSigned:\n        case MachineRepresentation::kTagged:\n        case MachineRepresentation::kTaggedPointer:\n          opcode = kArm64Ldrsw;\n          immediate_mode = kLoadStoreImm32;\n          break;\n        default:\n          UNREACHABLE();\n      }\n      EmitLoad(this, change_op.input(), opcode, immediate_mode, rep, node);\n      return;\n    }\n    if ((input_op.Is<Opmask::kWord32ShiftRightArithmetic>() ||\n         input_op.Is<Opmask::kWord32ShiftRightArithmeticShiftOutZeros>()) &&\n        CanCover(node, change_op.input())) {\n      const ShiftOp& sar = input_op.Cast<ShiftOp>();\n      if (this->is_integer_constant(sar.right())) {\n        Arm64OperandGeneratorT<Adapter> g(this);\n        // Mask the shift amount, to keep the same semantics as Word32Sar.\n        int right = this->integer_constant(sar.right()) & 0x1F;\n        Emit(kArm64Sbfx, g.DefineAsRegister(node), g.UseRegister(sar.left()),\n             g.TempImmediate(right), g.TempImmediate(32 - right));\n        return;\n      }\n    }\n    VisitRR(this, kArm64Sxtw, node);\n  } else {\n    Node* value = node->InputAt(0);\n    if ((value->opcode() == IrOpcode::kLoad ||\n         value->opcode() == IrOpcode::kLoadImmutable) &&\n        CanCover(node, value)) {\n      // Generate sign-extending load.\n      LoadRepresentation load_rep = LoadRepresentationOf(value->op());\n      MachineRepresentation rep = load_rep.representation();\n      InstructionCode opcode = kArchNop;\n      ImmediateMode immediate_mode = kNoImmediate;\n      switch (rep) {\n        case MachineRepresentation::kBit:  // Fall through.\n        case MachineRepresentation::kWord8:\n          opcode = load_rep.IsSigned() ? kArm64Ldrsb : kArm64Ldrb;\n          immediate_mode = kLoadStoreImm8;\n          break;\n        case MachineRepresentation::kWord16:\n          opcode = load_rep.IsSigned() ? kArm64Ldrsh : kArm64Ldrh;\n          immediate_mode = kLoadStoreImm16;\n          break;\n        case MachineRepresentation::kWord32:\n        case MachineRepresentation::kWord64:\n          // Since BitcastElider may remove nodes of\n          // IrOpcode::kTruncateInt64ToInt32 and directly use the inputs, values\n          // with kWord64 can also reach this line.\n        case MachineRepresentation::kTaggedSigned:\n        case MachineRepresentation::kTagged:\n        case MachineRepresentation::kTaggedPointer:\n          opcode = kArm64Ldrsw;\n          immediate_mode = kLoadStoreImm32;\n          break;\n        default:\n          UNREACHABLE();\n      }\n      EmitLoad(this, value, opcode, immediate_mode, rep, node);\n      return;\n    }\n\n    if (value->opcode() == IrOpcode::kWord32Sar && CanCover(node, value)) {\n      Int32BinopMatcher m(value);\n      if (m.right().HasResolvedValue()) {\n        Arm64OperandGeneratorT<Adapter> g(this);\n        // Mask the shift amount, to keep the same semantics as Word32Sar.\n        int right = m.right().ResolvedValue() & 0x1F;\n        Emit(kArm64Sbfx, g.DefineAsRegister(node),\n             g.UseRegister(m.left().node()), g.TempImmediate(right),\n             g.TempImmediate(32 - right));\n        return;\n      }\n    }\n\n    VisitRR(this, kArm64Sxtw, node);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ChangeOp& change_op = this->Get(node).template Cast<ChangeOp>();\n    const Operation& input_op = this->Get(change_op.input());\n    if (input_op.Is<LoadOp>() && CanCover(node, change_op.input())) {\n      // Generate sign-extending load.\n      LoadRepresentation load_rep =\n          this->load_view(change_op.input()).loaded_rep();\n      MachineRepresentation rep = load_rep.representation();\n      InstructionCode opcode = kArchNop;\n      switch (rep) {\n        case MachineRepresentation::kBit:  // Fall through.\n        case MachineRepresentation::kWord8:\n          opcode = load_rep.IsSigned() ? kRiscvLbu : kRiscvLb;\n          break;\n        case MachineRepresentation::kWord16:\n          opcode = load_rep.IsSigned() ? kRiscvLhu : kRiscvLh;\n          break;\n        case MachineRepresentation::kWord32:\n        case MachineRepresentation::kWord64:\n          // Since BitcastElider may remove nodes of\n          // IrOpcode::kTruncateInt64ToInt32 and directly use the inputs, values\n          // with kWord64 can also reach this line.\n        case MachineRepresentation::kTaggedSigned:\n        case MachineRepresentation::kTagged:\n          opcode = kRiscvLw;\n          break;\n        default:\n          UNREACHABLE();\n      }\n      EmitLoad(this, change_op.input(), opcode, node);\n      return;\n    }\n    EmitSignExtendWord(this, node);\n  } else {\n    Node* value = node->InputAt(0);\n    if ((value->opcode() == IrOpcode::kLoad ||\n         value->opcode() == IrOpcode::kLoadImmutable) &&\n        CanCover(node, value)) {\n      // Generate sign-extending load.\n      LoadRepresentation load_rep = LoadRepresentationOf(value->op());\n      InstructionCode opcode = kArchNop;\n      switch (load_rep.representation()) {\n        case MachineRepresentation::kBit:  // Fall through.\n        case MachineRepresentation::kWord8:\n          opcode = load_rep.IsUnsigned() ? kRiscvLbu : kRiscvLb;\n          break;\n        case MachineRepresentation::kWord16:\n          opcode = load_rep.IsUnsigned() ? kRiscvLhu : kRiscvLh;\n          break;\n        case MachineRepresentation::kWord32:\n        case MachineRepresentation::kWord64:\n          // Since BitcastElider may remove nodes of\n          // IrOpcode::kTruncateInt64ToInt32 and directly use the inputs, values\n          // with kWord64 can also reach this line.\n          // For RV64, the lw loads a 32 bit value from memory and sign-extend\n          // it to 64 bits before storing it in rd register\n        case MachineRepresentation::kTaggedSigned:\n        case MachineRepresentation::kTagged:\n          opcode = kRiscvLw;\n          break;\n        default:\n          UNREACHABLE();\n      }\n      EmitLoad(this, value, opcode, node);\n    } else {\n      EmitSignExtendWord(this, node);\n      return;\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64", "content": "void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(node_t node) {\n  DCHECK(SmiValuesAre31Bits());\n  DCHECK(COMPRESS_POINTERS_BOOL);\n  EmitIdentity(node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64", "content": "void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    DCHECK(SmiValuesAre31Bits());\n    DCHECK(COMPRESS_POINTERS_BOOL);\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Emit(kRiscvZeroExtendWord, g.DefineAsRegister(node),\n         g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  node_t success_output = FindProjection(node, 1);\n  if (this->valid(success_output)) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kArm64Float64ToUint32, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  Node* success_output = NodeProperties::FindProjection(node, 1);\n  if (success_output) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kRiscvTruncUwD, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  node_t success_output = FindProjection(node, 1);\n  if (this->valid(success_output)) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kArm64Float64ToInt32, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n    InstructionOperand outputs[2];\n    size_t output_count = 0;\n    outputs[output_count++] = g.DefineAsRegister(node);\n\n    Node* success_output = NodeProperties::FindProjection(node, 1);\n    if (success_output) {\n      outputs[output_count++] = g.DefineAsRegister(success_output);\n    }\n\n    this->Emit(kRiscvTruncWD, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  node_t success_output = FindProjection(node, 1);\n  if (this->valid(success_output)) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kArm64Float64ToUint64, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n  RiscvOperandGeneratorT<Adapter> g(this);\n\n  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  Node* success_output = NodeProperties::FindProjection(node, 1);\n  if (success_output) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kRiscvTruncUlD, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  node_t success_output = FindProjection(node, 1);\n  if (this->valid(success_output)) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kArm64Float32ToUint64, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  Node* success_output = NodeProperties::FindProjection(node, 1);\n  if (success_output) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kRiscvTruncUlS, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(\n    node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  node_t success_output = FindProjection(node, 1);\n  if (this->valid(success_output)) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kArm64Float64ToInt64, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  Node* success_output = NodeProperties::FindProjection(node, 1);\n  if (success_output) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  Emit(kRiscvTruncLD, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    InstructionCode opcode = kArm64Float64ToInt64;\n    const Operation& op = this->Get(node);\n    if (op.Is<Opmask::kTruncateFloat64ToInt64OverflowToMin>()) {\n      opcode |= MiscField::encode(true);\n    }\n\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(op.input(0)));\n  } else {\n    InstructionCode opcode = kArm64Float64ToInt64;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    if (kind == TruncateKind::kSetOverflowToMin) {\n      opcode |= MiscField::encode(true);\n    }\n\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    InstructionCode opcode = kRiscvTruncLD;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    if (kind == TruncateKind::kSetOverflowToMin) {\n      opcode |= MiscField::encode(true);\n    }\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(\n    node_t node) {\n    Arm64OperandGeneratorT<Adapter> g(this);\n\n    InstructionOperand inputs[] = {g.UseRegister(this->input_at(node, 0))};\n    InstructionOperand outputs[2];\n    size_t output_count = 0;\n    outputs[output_count++] = g.DefineAsRegister(node);\n\n    node_t success_output = FindProjection(node, 1);\n    if (this->valid(success_output)) {\n      outputs[output_count++] = g.DefineAsRegister(success_output);\n    }\n\n    Emit(kArm64Float32ToInt64, output_count, outputs, 1, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(\n    node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64", "content": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(\n    node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};\n  InstructionOperand outputs[2];\n  size_t output_count = 0;\n  outputs[output_count++] = g.DefineAsRegister(node);\n\n  Node* success_output = NodeProperties::FindProjection(node, 1);\n  if (success_output) {\n    outputs[output_count++] = g.DefineAsRegister(success_output);\n  }\n\n  this->Emit(kRiscvTruncLS, output_count, outputs, 1, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(\n    node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const Operation& op = this->Get(node);\n    InstructionCode opcode = kArm64Float32ToUint32;\n    if (op.Is<Opmask::kTruncateFloat32ToUint32OverflowToMin>()) {\n      opcode |= MiscField::encode(true);\n    }\n\n    Emit(opcode, g.DefineAsRegister(node),\n         g.UseRegister(this->input_at(node, 0)));\n\n  } else {\n    InstructionCode opcode = kArm64Float32ToUint32;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    if (kind == TruncateKind::kSetOverflowToMin) {\n      opcode |= MiscField::encode(true);\n    }\n\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    InstructionCode opcode = kRiscvTruncUwS;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    if (kind == TruncateKind::kSetOverflowToMin) {\n      opcode |= MiscField::encode(true);\n    }\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const Operation& op = this->Get(node);\n    InstructionCode opcode = kArm64Float32ToInt32;\n    opcode |= MiscField::encode(\n        op.Is<Opmask::kTruncateFloat32ToInt32OverflowToMin>());\n    Emit(opcode, g.DefineAsRegister(node),\n         g.UseRegister(this->input_at(node, 0)));\n  } else {\n    InstructionCode opcode = kArm64Float32ToInt32;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    opcode |= MiscField::encode(kind == TruncateKind::kSetOverflowToMin);\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32", "content": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    InstructionCode opcode = kRiscvTruncWS;\n    TruncateKind kind = OpParameter<TruncateKind>(node->op());\n    if (kind == TruncateKind::kSetOverflowToMin) {\n      opcode |= MiscField::encode(true);\n    }\n    Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint64MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitUint64MulHigh(node_t node) {\n  return VisitRRR(this, kArm64Umulh, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64MulHigh(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint64MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitUint64MulHigh(node_t node) {\n    VisitRRR(this, kRiscvMulHighU64, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint64MulHigh(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUint32MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitUint32MulHigh(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  InstructionOperand const smull_operand = g.TempRegister();\n  Emit(kArm64Umull, smull_operand, g.UseRegister(this->input_at(node, 0)),\n       g.UseRegister(this->input_at(node, 1)));\n  Emit(kArm64Lsr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32MulHigh(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUint32MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitUint32MulHigh(node_t node) {\n  VisitRRR(this, kRiscvMulHighU32, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUint32MulHigh(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitInt64MulHigh(node_t node) {\n  return VisitRRR(this, kArm64Smulh, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64MulHigh(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitInt64MulHigh(node_t node) {\n    return VisitRRR(this, kRiscvMulHigh64, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64MulHigh(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitInt32MulHigh(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  InstructionOperand const smull_operand = g.TempRegister();\n  Emit(kArm64Smull, smull_operand, g.UseRegister(this->input_at(node, 0)),\n       g.UseRegister(this->input_at(node, 1)));\n  Emit(kArm64Asr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32MulHigh(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32MulHigh", "content": "void InstructionSelectorT<Adapter>::VisitInt32MulHigh(node_t node) {\n  VisitRRR(this, kRiscvMulHigh32, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32MulHigh(node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul(Node* node) {\n  Arm64OperandGeneratorT<TurbofanAdapter> g(this);\n  Int64BinopMatcher m(node);\n\n  // First, try to reduce the multiplication to addition with left shift.\n  // x * (2^k + 1) -> x + (x << k)\n  int32_t shift = LeftShiftForReducedMultiply(&m);\n  if (shift > 0) {\n    Emit(kArm64Add | AddressingModeField::encode(kMode_Operand2_R_LSL_I),\n         g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n         g.UseRegister(m.left().node()), g.TempImmediate(shift));\n    return;\n  }\n\n  if (m.left().IsInt64Sub() && CanCover(node, m.left().node())) {\n    Int64BinopMatcher mleft(m.left().node());\n\n    // Select Mneg(x, y) for Mul(Sub(0, x), y).\n    if (mleft.left().Is(0)) {\n      Emit(kArm64Mneg, g.DefineAsRegister(node),\n           g.UseRegister(mleft.right().node()),\n           g.UseRegister(m.right().node()));\n      return;\n    }\n  }\n\n  if (m.right().IsInt64Sub() && CanCover(node, m.right().node())) {\n    Int64BinopMatcher mright(m.right().node());\n\n    // Select Mneg(x, y) for Mul(x, Sub(0, y)).\n    if (mright.left().Is(0)) {\n      Emit(kArm64Mneg, g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n           g.UseRegister(mright.right().node()));\n      return;\n    }\n  }\n\n  VisitRRR(this, kArm64Mul, node);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul(Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul(Node* node) {\n  RiscvOperandGeneratorT<TurbofanAdapter> g(this);\n  Int64BinopMatcher m(node);\n  // TODO(dusmil): Add optimization for shifts larger than 32.\n  if (m.right().HasResolvedValue() && m.right().ResolvedValue() > 0) {\n    uint64_t value = static_cast<uint64_t>(m.right().ResolvedValue());\n    if (base::bits::IsPowerOfTwo(value)) {\n      Emit(kRiscvShl64 | AddressingModeField::encode(kMode_None),\n           g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n           g.TempImmediate(base::bits::WhichPowerOfTwo(value)));\n      return;\n    }\n    if (base::bits::IsPowerOfTwo(value + 1)) {\n      InstructionOperand temp = g.TempRegister();\n      Emit(kRiscvShl64 | AddressingModeField::encode(kMode_None), temp,\n           g.UseRegister(m.left().node()),\n           g.TempImmediate(base::bits::WhichPowerOfTwo(value + 1)));\n      Emit(kRiscvSub64 | AddressingModeField::encode(kMode_None),\n           g.DefineAsRegister(node), temp, g.UseRegister(m.left().node()));\n      return;\n    }\n  }\n  Emit(kRiscvMul64, g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n       g.UseRegister(m.right().node()));\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt64Mul(Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  Arm64OperandGeneratorT<TurboshaftAdapter> g(this);\n  const WordBinopOp& mul = Get(node).Cast<WordBinopOp>();\n  DCHECK(mul.Is<Opmask::kWord64Mul>());\n\n  // First, try to reduce the multiplication to addition with left shift.\n  // x * (2^k + 1) -> x + (x << k)\n  int32_t shift = LeftShiftForReducedMultiply(this, mul.right());\n  if (shift > 0) {\n    Emit(kArm64Add | AddressingModeField::encode(kMode_Operand2_R_LSL_I),\n         g.DefineAsRegister(node), g.UseRegister(mul.left()),\n         g.UseRegister(mul.left()), g.TempImmediate(shift));\n    return;\n  }\n\n  // Select Mneg(x, y) for Mul(Sub(0, x), y) or Mul(y, Sub(0, x)).\n  if (TryEmitMultiplyNegateInt64(this, node, mul.left(), mul.right()) ||\n      TryEmitMultiplyNegateInt64(this, node, mul.right(), mul.left())) {\n    return;\n  }\n\n  VisitRRR(this, kArm64Mul, node);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul(node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul(node_t node) {\n  VisitRRR(this, kRiscvMul64, node);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt64Mul(node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul(Node* node) {\n  Arm64OperandGeneratorT<TurbofanAdapter> g(this);\n  Int32BinopMatcher m(node);\n\n  // First, try to reduce the multiplication to addition with left shift.\n  // x * (2^k + 1) -> x + (x << k)\n  int32_t shift = LeftShiftForReducedMultiply(&m);\n  if (shift > 0) {\n    Emit(kArm64Add32 | AddressingModeField::encode(kMode_Operand2_R_LSL_I),\n         g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n         g.UseRegister(m.left().node()), g.TempImmediate(shift));\n    return;\n  }\n\n  if (m.left().IsInt32Sub() && CanCover(node, m.left().node())) {\n    Int32BinopMatcher mleft(m.left().node());\n\n    // Select Mneg(x, y) for Mul(Sub(0, x), y).\n    if (mleft.left().Is(0)) {\n      Emit(kArm64Mneg32, g.DefineAsRegister(node),\n           g.UseRegister(mleft.right().node()),\n           g.UseRegister(m.right().node()));\n      return;\n    }\n  }\n\n  if (m.right().IsInt32Sub() && CanCover(node, m.right().node())) {\n    Int32BinopMatcher mright(m.right().node());\n\n    // Select Mneg(x, y) for Mul(x, Sub(0, y)).\n    if (mright.left().Is(0)) {\n      Emit(kArm64Mneg32, g.DefineAsRegister(node),\n           g.UseRegister(m.left().node()),\n           g.UseRegister(mright.right().node()));\n      return;\n    }\n  }\n\n  VisitRRR(this, kArm64Mul32, node);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul(Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul(Node* node) {\n  RiscvOperandGeneratorT<TurbofanAdapter> g(this);\n  Int32BinopMatcher m(node);\n  if (m.right().HasResolvedValue() && m.right().ResolvedValue() > 0) {\n    uint32_t value = static_cast<uint32_t>(m.right().ResolvedValue());\n    if (base::bits::IsPowerOfTwo(value)) {\n      Emit(kRiscvShl32 | AddressingModeField::encode(kMode_None),\n           g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n           g.TempImmediate(base::bits::WhichPowerOfTwo(value)));\n      return;\n    }\n    if (base::bits::IsPowerOfTwo(value + 1)) {\n      InstructionOperand temp = g.TempRegister();\n      Emit(kRiscvShl32 | AddressingModeField::encode(kMode_None), temp,\n           g.UseRegister(m.left().node()),\n           g.TempImmediate(base::bits::WhichPowerOfTwo(value + 1)));\n      Emit(kRiscvSub32 | AddressingModeField::encode(kMode_None),\n           g.DefineAsRegister(node), temp, g.UseRegister(m.left().node()));\n      return;\n    }\n  }\n\n  VisitRRR(this, kRiscvMul32, node);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Mul(Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  Arm64OperandGeneratorT<TurboshaftAdapter> g(this);\n  const WordBinopOp& mul = Get(node).Cast<WordBinopOp>();\n  DCHECK(mul.Is<Opmask::kWord32Mul>());\n\n  // First, try to reduce the multiplication to addition with left shift.\n  // x * (2^k + 1) -> x + (x << k)\n  int32_t shift = LeftShiftForReducedMultiply(this, mul.right());\n  if (shift > 0) {\n    Emit(kArm64Add32 | AddressingModeField::encode(kMode_Operand2_R_LSL_I),\n         g.DefineAsRegister(node), g.UseRegister(mul.left()),\n         g.UseRegister(mul.left()), g.TempImmediate(shift));\n    return;\n  }\n\n  // Select Mneg(x, y) for Mul(Sub(0, x), y) or Mul(y, Sub(0, x)).\n  if (TryEmitMultiplyNegateInt32(this, node, mul.left(), mul.right()) ||\n      TryEmitMultiplyNegateInt32(this, node, mul.right(), mul.left())) {\n    return;\n  }\n\n  VisitRRR(this, kArm64Mul32, node);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul(node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul(node_t node) {\n  VisitRRR(this, kRiscvMul32, node);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Mul(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt64Sub", "content": "void InstructionSelectorT<Adapter>::VisitInt64Sub(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  Int64BinopMatcher m(node);\n\n  // Select Msub(x, y, a) for Sub(a, Mul(x, y)).\n  if (m.right().IsInt64Mul() && CanCover(node, m.right().node())) {\n    Int64BinopMatcher mright(m.right().node());\n    // Check multiply can't be later reduced to addition with shift.\n    if (LeftShiftForReducedMultiply(&mright) == 0) {\n      Emit(kArm64Msub, g.DefineAsRegister(node),\n           g.UseRegister(mright.left().node()),\n           g.UseRegister(mright.right().node()),\n           g.UseRegister(m.left().node()));\n      return;\n    }\n  }\n\n  VisitAddSub<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArm64Add);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64Sub(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt64Sub", "content": "void InstructionSelectorT<Adapter>::VisitInt64Sub(node_t node) {\n  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvSub64);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt64Sub(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitInt32Sub", "content": "void InstructionSelectorT<Adapter>::VisitInt32Sub(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  Int32BinopMatcher m(node);\n\n  // Select Msub(x, y, a) for Sub(a, Mul(x, y)).\n  if (m.right().IsInt32Mul() && CanCover(node, m.right().node())) {\n    Int32BinopMatcher mright(m.right().node());\n    // Check multiply can't be later reduced to addition with shift.\n    if (LeftShiftForReducedMultiply(&mright) == 0) {\n      Emit(kArm64Msub32, g.DefineAsRegister(node),\n           g.UseRegister(mright.left().node()),\n           g.UseRegister(mright.right().node()),\n           g.UseRegister(m.left().node()));\n      return;\n    }\n  }\n\n  VisitAddSub<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32, kArm64Add32);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32Sub(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitInt32Sub", "content": "void InstructionSelectorT<Adapter>::VisitInt32Sub(node_t node) {\n  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvSub32);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitInt32Sub(node_t node) "}], [{"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt32Add", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Add(Node* node) {\n  Arm64OperandGeneratorT<TurbofanAdapter> g(this);\n  Int32BinopMatcher m(node);\n  // Select Madd(x, y, z) for Add(Mul(x, y), z).\n  if (m.left().IsInt32Mul() && CanCover(node, m.left().node())) {\n    Int32BinopMatcher mleft(m.left().node());\n    // Check multiply can't be later reduced to addition with shift.\n    if (LeftShiftForReducedMultiply(&mleft) == 0) {\n      Emit(kArm64Madd32, g.DefineAsRegister(node),\n           g.UseRegister(mleft.left().node()),\n           g.UseRegister(mleft.right().node()),\n           g.UseRegister(m.right().node()));\n      return;\n    }\n  }\n  // Select Madd(x, y, z) for Add(z, Mul(x, y)).\n  if (m.right().IsInt32Mul() && CanCover(node, m.right().node())) {\n    Int32BinopMatcher mright(m.right().node());\n    // Check multiply can't be later reduced to addition with shift.\n    if (LeftShiftForReducedMultiply(&mright) == 0) {\n      Emit(kArm64Madd32, g.DefineAsRegister(node),\n           g.UseRegister(mright.left().node()),\n           g.UseRegister(mright.right().node()),\n           g.UseRegister(m.left().node()));\n      return;\n    }\n  }\n  VisitAddSub<TurbofanAdapter, Int32BinopMatcher>(this, node, kArm64Add32,\n                                                  kArm64Sub32);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Add(Node* node) "}, {"name": "InstructionSelectorT<TurbofanAdapter>::VisitInt32Add", "content": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Add(Node* node) {\n  VisitBinop<TurbofanAdapter, Int32BinopMatcher>(this, node, kRiscvAdd32, true,\n                                         kRiscvAdd32);\n}", "name_and_para": "void InstructionSelectorT<TurbofanAdapter>::VisitInt32Add(Node* node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  const WordBinopOp& add = this->Get(node).Cast<WordBinopOp>();\n  DCHECK(add.Is<Opmask::kWord32Add>());\n  V<Word32> left = add.left<Word32>();\n  V<Word32> right = add.right<Word32>();\n  // Select Madd(x, y, z) for Add(Mul(x, y), z) or Add(z, Mul(x, y)).\n  if (TryEmitMultiplyAddInt32(this, node, left, right) ||\n      TryEmitMultiplyAddInt32(this, node, right, left)) {\n    return;\n  }\n  VisitAddSub(this, node, kArm64Add32, kArm64Sub32);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add(node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add(node_t node) {\n  VisitBinop<TurboshaftAdapter, Int32BinopMatcher>(this, node, kRiscvAdd32,\n                                                   true, kRiscvAdd32);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitInt32Add(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Ctz", "content": "void InstructionSelectorT<Adapter>::VisitWord64Ctz(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Ctz(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Ctz", "content": "void InstructionSelectorT<Adapter>::VisitWord64Ctz(node_t node) {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Emit(kRiscvCtz64, g.DefineAsRegister(node),\n         g.UseRegister(this->input_at(node, 0)));\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Ctz(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Ror", "content": "void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) {\n  VisitRRO(this, kArm64Ror, node, kShift64Imm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Ror", "content": "void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) {\n    VisitRRO(this, kRiscvRor64, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32Ror", "content": "void InstructionSelectorT<Adapter>::VisitWord32Ror(node_t node) {\n  VisitRRO(this, kArm64Ror32, node, kShift32Imm);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Ror(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32Ror", "content": "void InstructionSelectorT<Adapter>::VisitWord32Ror(node_t node) {\n    VisitRRO(this, kRiscvRor32, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Ror(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Rol", "content": "void InstructionSelectorT<Adapter>::VisitWord64Rol(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Rol(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Rol", "content": "void InstructionSelectorT<Adapter>::VisitWord64Rol(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Rol(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32Rol", "content": "void InstructionSelectorT<Adapter>::VisitWord32Rol(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Rol(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32Rol", "content": "void InstructionSelectorT<Adapter>::VisitWord32Rol(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    UNREACHABLE();\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Rol(node_t node) "}], [{"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  if (TryEmitExtendingLoad(this, node)) return;\n\n  // Select Sbfx(x, imm, 32-imm) for Word64Sar(ChangeInt32ToInt64(x), imm)\n  // where possible\n  const ShiftOp& shiftop = Get(node).Cast<ShiftOp>();\n  const Operation& lhs = Get(shiftop.left());\n\n  int64_t constant_rhs;\n  if (lhs.Is<Opmask::kChangeInt32ToInt64>() &&\n      MatchIntegralWord64Constant(shiftop.right(), &constant_rhs) &&\n      is_uint5(constant_rhs) && CanCover(node, shiftop.left())) {\n    // Don't select Sbfx here if Asr(Ldrsw(x), imm) can be selected for\n    // Word64Sar(ChangeInt32ToInt64(Load(x)), imm)\n    OpIndex input = lhs.Cast<ChangeOp>().input();\n    if (!Get(input).Is<LoadOp>() || !CanCover(shiftop.left(), input)) {\n      Arm64OperandGeneratorT<TurboshaftAdapter> g(this);\n      int right = static_cast<int>(constant_rhs);\n      Emit(kArm64Sbfx, g.DefineAsRegister(node), g.UseRegister(input),\n           g.UseImmediate(right), g.UseImmediate(32 - right));\n      return;\n    }\n  }\n\n  VisitRRO(this, kArm64Asr, node, kShift64Imm);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar(node_t node) "}, {"name": "InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar", "content": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar(node_t node) {\n  using namespace turboshaft;  // NOLINT(build/namespaces)\n  if (TryEmitExtendingLoad(this, node, node)) return;\n  // Select Sbfx(x, imm, 32-imm) for Word64Sar(ChangeInt32ToInt64(x), imm)\n  // where possible\n  const ShiftOp& shiftop = Get(node).Cast<ShiftOp>();\n  const Operation& lhs = Get(shiftop.left());\n\n  int64_t constant_rhs;\n  if (lhs.Is<Opmask::kChangeInt32ToInt64>() &&\n      MatchIntegralWord64Constant(shiftop.right(), &constant_rhs) &&\n      is_uint5(constant_rhs) && CanCover(node, shiftop.left())) {\n    // Don't select Sbfx here if Asr(Ldrsw(x), imm) can be selected for\n    // Word64Sar(ChangeInt32ToInt64(Load(x)), imm)\n    OpIndex input = lhs.Cast<ChangeOp>().input();\n    if (!Get(input).Is<LoadOp>() || !CanCover(shiftop.left(), input)) {\n      RiscvOperandGeneratorT<TurboshaftAdapter> g(this);\n      int right = static_cast<int>(constant_rhs);\n      Emit(kRiscvSar32, g.DefineAsRegister(node), g.UseRegister(input),\n           g.UseImmediate(right));\n      return;\n    }\n  }\n  VisitRRO(this, kRiscvSar64, node);\n}", "name_and_para": "void InstructionSelectorT<TurboshaftAdapter>::VisitWord64Sar(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Sar", "content": "void InstructionSelectorT<Adapter>::VisitWord64Sar(node_t node) {\n  {\n    if (TryEmitExtendingLoad(this, node)) return;\n\n    // Select Sbfx(x, imm, 32-imm) for Word64Sar(ChangeInt32ToInt64(x), imm)\n    // where possible\n    Int64BinopMatcher m(node);\n    if (m.left().IsChangeInt32ToInt64() && m.right().HasResolvedValue() &&\n        is_uint5(m.right().ResolvedValue()) &&\n        CanCover(node, m.left().node())) {\n      // Don't select Sbfx here if Asr(Ldrsw(x), imm) can be selected for\n      // Word64Sar(ChangeInt32ToInt64(Load(x)), imm)\n      if ((m.left().InputAt(0)->opcode() != IrOpcode::kLoad &&\n           m.left().InputAt(0)->opcode() != IrOpcode::kLoadImmutable) ||\n          !CanCover(m.left().node(), m.left().InputAt(0))) {\n        Arm64OperandGeneratorT<Adapter> g(this);\n        int right = static_cast<int>(m.right().ResolvedValue());\n        Emit(kArm64Sbfx, g.DefineAsRegister(node),\n             g.UseRegister(m.left().node()->InputAt(0)),\n             g.UseImmediate(m.right().node()), g.UseImmediate(32 - right));\n        return;\n      }\n    }\n\n    VisitRRO(this, kArm64Asr, node, kShift64Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Sar(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Sar", "content": "void InstructionSelectorT<Adapter>::VisitWord64Sar(node_t node) {\n    if (TryEmitExtendingLoad(this, node, node)) return;\n    Int64BinopMatcher m(node);\n    if (m.left().IsChangeInt32ToInt64() && m.right().HasResolvedValue() &&\n        is_uint5(m.right().ResolvedValue()) &&\n        CanCover(node, m.left().node())) {\n      if ((m.left().InputAt(0)->opcode() != IrOpcode::kLoad &&\n           m.left().InputAt(0)->opcode() != IrOpcode::kLoadImmutable) ||\n          !CanCover(m.left().node(), m.left().InputAt(0))) {\n        RiscvOperandGeneratorT<Adapter> g(this);\n        Emit(kRiscvSar32, g.DefineAsRegister(node),\n             g.UseRegister(m.left().node()->InputAt(0)),\n             g.UseImmediate(m.right().node()));\n        return;\n      }\n    }\n    VisitRRO(this, kRiscvSar64, node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Sar(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan", "content": "void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(\n    node_t node, FlagsContinuationT<Adapter>* cont) {\n  StackCheckKind kind;\n  node_t value;\n  if constexpr (Adapter::IsTurboshaft) {\n    const auto& op =\n        this->turboshaft_graph()\n            ->Get(node)\n            .template Cast<turboshaft::StackPointerGreaterThanOp>();\n    kind = op.kind;\n    value = op.stack_limit();\n  } else {\n    kind = StackCheckKindOf(node->op());\n    value = node->InputAt(0);\n  }\n  InstructionCode opcode =\n      kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));\n\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  // No outputs.\n  InstructionOperand* const outputs = nullptr;\n  const int output_count = 0;\n\n  // Applying an offset to this stack check requires a temp register. Offsets\n  // are only applied to the first stack check. If applying an offset, we must\n  // ensure the input and temp registers do not alias, thus kUniqueRegister.\n  InstructionOperand temps[] = {g.TempRegister()};\n  const int temp_count = (kind == StackCheckKind::kJSFunctionEntry) ? 1 : 0;\n  const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)\n                                 ? OperandGenerator::kUniqueRegister\n                                 : OperandGenerator::kRegister;\n\n  InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};\n  static constexpr int input_count = arraysize(inputs);\n\n  EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,\n                       temp_count, temps, cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(\n    node_t node, FlagsContinuationT<Adapter>* cont) "}, {"name": "InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan", "content": "void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(\n    node_t node, FlagsContinuationT<Adapter>* cont) {\n  StackCheckKind kind;\n  node_t value;\n  if constexpr (Adapter::IsTurboshaft) {\n    const auto& op =\n        this->turboshaft_graph()\n            ->Get(node)\n            .template Cast<turboshaft::StackPointerGreaterThanOp>();\n    kind = op.kind;\n    value = op.stack_limit();\n  } else {\n    kind = StackCheckKindOf(node->op());\n    value = node->InputAt(0);\n  }\n  InstructionCode opcode =\n      kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));\n\n  RiscvOperandGeneratorT<Adapter> g(this);\n\n  // No outputs.\n  InstructionOperand* const outputs = nullptr;\n  const int output_count = 0;\n\n  // Applying an offset to this stack check requires a temp register. Offsets\n  // are only applied to the first stack check. If applying an offset, we must\n  // ensure the input and temp registers do not alias, thus kUniqueRegister.\n  InstructionOperand temps[] = {g.TempRegister()};\n  const int temp_count = (kind == StackCheckKind::kJSFunctionEntry ? 1 : 0);\n  const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)\n                                 ? OperandGenerator::kUniqueRegister\n                                 : OperandGenerator::kRegister;\n\n  InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};\n  static constexpr int input_count = arraysize(inputs);\n\n  EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,\n                       temp_count, temps, cont);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(\n    node_t node, FlagsContinuationT<Adapter>* cont) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Shl", "content": "void InstructionSelectorT<Adapter>::VisitWord64Shl(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ShiftOp& shift_op = this->Get(node).template Cast<ShiftOp>();\n    const Operation& lhs = this->Get(shift_op.left());\n    const Operation& rhs = this->Get(shift_op.right());\n    if ((lhs.Is<Opmask::kChangeInt32ToInt64>() ||\n         lhs.Is<Opmask::kChangeUint32ToUint64>()) &&\n        rhs.Is<Opmask::kWord32Constant>()) {\n      int64_t shift_by = rhs.Cast<ConstantOp>().signed_integral();\n      if (base::IsInRange(shift_by, 32, 63) &&\n          CanCover(node, shift_op.left())) {\n        // There's no need to sign/zero-extend to 64-bit if we shift out the\n        // upper 32 bits anyway.\n        Emit(kArm64Lsl, g.DefineAsRegister(node),\n             g.UseRegister(lhs.Cast<ChangeOp>().input()),\n             g.UseImmediate64(shift_by));\n        return;\n      }\n    }\n    VisitRRO(this, kArm64Lsl, node, kShift64Imm);\n  } else {\n    Int64BinopMatcher m(node);\n    if ((m.left().IsChangeInt32ToInt64() ||\n         m.left().IsChangeUint32ToUint64()) &&\n        m.right().IsInRange(32, 63) && CanCover(node, m.left().node())) {\n      // There's no need to sign/zero-extend to 64-bit if we shift out the upper\n      // 32 bits anyway.\n      Emit(kArm64Lsl, g.DefineAsRegister(node),\n           g.UseRegister(m.left().node()->InputAt(0)),\n           g.UseImmediate(m.right().node()));\n      return;\n    }\n    VisitRRO(this, kArm64Lsl, node, kShift64Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Shl(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Shl", "content": "void InstructionSelectorT<Adapter>::VisitWord64Shl(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ShiftOp& shift_op = this->Get(node).template Cast<ShiftOp>();\n    const Operation& lhs = this->Get(shift_op.left());\n    const Operation& rhs = this->Get(shift_op.right());\n    if ((lhs.Is<Opmask::kChangeInt32ToInt64>() ||\n         lhs.Is<Opmask::kChangeUint32ToUint64>()) &&\n        rhs.Is<Opmask::kWord32Constant>()) {\n      int64_t shift_by = rhs.Cast<ConstantOp>().signed_integral();\n      if (base::IsInRange(shift_by, 32, 63) &&\n          CanCover(node, shift_op.left())) {\n        RiscvOperandGeneratorT<Adapter> g(this);\n        // There's no need to sign/zero-extend to 64-bit if we shift out the\n        // upper 32 bits anyway.\n        Emit(kRiscvShl64, g.DefineSameAsFirst(node),\n             g.UseRegister(lhs.Cast<ChangeOp>().input()),\n             g.UseImmediate64(shift_by));\n        return;\n      }\n    }\n    VisitRRO(this, kRiscvShl64, node);\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Int64BinopMatcher m(node);\n    if ((m.left().IsChangeInt32ToInt64() ||\n         m.left().IsChangeUint32ToUint64()) &&\n        m.right().IsInRange(32, 63) && CanCover(node, m.left().node())) {\n      // There's no need to sign/zero-extend to 64-bit if we shift out the upper\n      // 32 bits anyway.\n      Emit(kRiscvShl64, g.DefineSameAsFirst(node),\n           g.UseRegister(m.left().node()->InputAt(0)),\n           g.UseImmediate(m.right().node()));\n      return;\n    }\n    if (m.left().IsWord64And() && CanCover(node, m.left().node()) &&\n        m.right().IsInRange(1, 63)) {\n      // Match Word64Shl(Word64And(x, mask), imm) to Dshl where the mask is\n      // contiguous, and the shift immediate non-zero.\n      Int64BinopMatcher mleft(m.left().node());\n      if (mleft.right().HasResolvedValue()) {\n        uint64_t mask = mleft.right().ResolvedValue();\n        uint32_t mask_width = base::bits::CountPopulation(mask);\n        uint32_t mask_msb = base::bits::CountLeadingZeros64(mask);\n        if ((mask_width != 0) && (mask_msb + mask_width == 64)) {\n          uint64_t shift = m.right().ResolvedValue();\n          DCHECK_EQ(0u, base::bits::CountTrailingZeros64(mask));\n          DCHECK_NE(0u, shift);\n\n          if ((shift + mask_width) >= 64) {\n            // If the mask is contiguous and reaches or extends beyond the top\n            // bit, only the shift is needed.\n            Emit(kRiscvShl64, g.DefineAsRegister(node),\n                 g.UseRegister(mleft.left().node()),\n                 g.UseImmediate(m.right().node()));\n            return;\n          }\n        }\n      }\n    }\n    VisitRRO(this, kRiscvShl64, node);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Shl(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Xor", "content": "void InstructionSelectorT<Adapter>::VisitWord64Xor(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const WordBinopOp& op = this->Get(node).template Cast<WordBinopOp>();\n    VisitLogical(this, zone(), node, op.rep, kArm64Eor,\n                 CanCover(node, op.left()), CanCover(node, op.right()),\n                 kLogical64Imm);\n  } else {\n    Int64BinopMatcher m(node);\n    VisitLogical<Adapter, Int64BinopMatcher>(\n        this, node, &m, kArm64Eor, CanCover(node, m.left().node()),\n        CanCover(node, m.right().node()), kLogical64Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Xor(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Xor", "content": "void InstructionSelectorT<Adapter>::VisitWord64Xor(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    // const WordBinopOp& op = this->Get(node).template Cast<WordBinopOp>();\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvXor, true,\n                                           kRiscvXor);\n  } else {\n    Int64BinopMatcher m(node);\n    if (m.left().IsWord64Or() && CanCover(node, m.left().node()) &&\n        m.right().Is(-1)) {\n      Int64BinopMatcher mleft(m.left().node());\n      if (!mleft.right().HasResolvedValue()) {\n        RiscvOperandGeneratorT<Adapter> g(this);\n        Emit(kRiscvNor, g.DefineAsRegister(node),\n             g.UseRegister(mleft.left().node()),\n             g.UseRegister(mleft.right().node()));\n        return;\n      }\n    }\n    if (m.right().Is(-1)) {\n      // Use Nor for bit negation and eliminate constant loading for xori.\n      RiscvOperandGeneratorT<Adapter> g(this);\n      Emit(kRiscvNor, g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n           g.TempImmediate(0));\n      return;\n    }\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvXor, true,\n                                           kRiscvXor);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Xor(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32Xor", "content": "void InstructionSelectorT<Adapter>::VisitWord32Xor(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const WordBinopOp& op = this->Get(node).template Cast<WordBinopOp>();\n    VisitLogical(this, zone(), node, op.rep, kArm64Eor32,\n                 CanCover(node, op.left()), CanCover(node, op.right()),\n                 kLogical32Imm);\n  } else {\n    Int32BinopMatcher m(node);\n    VisitLogical<Adapter, Int32BinopMatcher>(\n        this, node, &m, kArm64Eor32, CanCover(node, m.left().node()),\n        CanCover(node, m.right().node()), kLogical32Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Xor(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32Xor", "content": "void InstructionSelectorT<Adapter>::VisitWord32Xor(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvXor, true,\n                                           kRiscvXor);\n  } else {\n    Int32BinopMatcher m(node);\n    if (m.left().IsWord32Or() && CanCover(node, m.left().node()) &&\n        m.right().Is(-1)) {\n      Int32BinopMatcher mleft(m.left().node());\n      if (!mleft.right().HasResolvedValue()) {\n        RiscvOperandGeneratorT<Adapter> g(this);\n        Emit(kRiscvNor, g.DefineAsRegister(node),\n             g.UseRegister(mleft.left().node()),\n             g.UseRegister(mleft.right().node()));\n        return;\n      }\n    }\n    if (m.right().Is(-1)) {\n      // Use Nor for bit negation and eliminate constant loading for xori.\n      RiscvOperandGeneratorT<Adapter> g(this);\n      Emit(kRiscvNor, g.DefineAsRegister(node), g.UseRegister(m.left().node()),\n           g.TempImmediate(0));\n      return;\n    }\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvXor, true,\n                                           kRiscvXor);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Xor(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord64Or", "content": "void InstructionSelectorT<Adapter>::VisitWord64Or(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const WordBinopOp& op = this->Get(node).template Cast<WordBinopOp>();\n    VisitLogical(this, zone(), node, op.rep, kArm64Or,\n                 CanCover(node, op.left()), CanCover(node, op.right()),\n                 kLogical64Imm);\n  } else {\n    Int64BinopMatcher m(node);\n    VisitLogical<Adapter, Int64BinopMatcher>(\n        this, node, &m, kArm64Or, CanCover(node, m.left().node()),\n        CanCover(node, m.right().node()), kLogical64Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Or(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord64Or", "content": "void InstructionSelectorT<Adapter>::VisitWord64Or(node_t node) {\n    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kRiscvOr, true,\n                                           kRiscvOr);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord64Or(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitWord32Or", "content": "void InstructionSelectorT<Adapter>::VisitWord32Or(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const WordBinopOp& op = this->Get(node).template Cast<WordBinopOp>();\n    VisitLogical(this, zone(), node, op.rep, kArm64Or32,\n                 CanCover(node, op.left()), CanCover(node, op.right()),\n                 kLogical32Imm);\n  } else {\n    Int32BinopMatcher m(node);\n    VisitLogical<Adapter, Int32BinopMatcher>(\n        this, node, &m, kArm64Or32, CanCover(node, m.left().node()),\n        CanCover(node, m.right().node()), kLogical32Imm);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Or(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitWord32Or", "content": "void InstructionSelectorT<Adapter>::VisitWord32Or(node_t node) {\n    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kRiscvOr, true,\n                                           kRiscvOr);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitWord32Or(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUnalignedStore", "content": "void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUnalignedStore", "content": "void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Node* base = node->InputAt(0);\n    Node* index = node->InputAt(1);\n    Node* value = node->InputAt(2);\n\n    UnalignedStoreRepresentation rep =\n        UnalignedStoreRepresentationOf(node->op());\n    ArchOpcode opcode;\n    switch (rep) {\n      case MachineRepresentation::kFloat32:\n        opcode = kRiscvUStoreFloat;\n        break;\n      case MachineRepresentation::kFloat64:\n        opcode = kRiscvUStoreDouble;\n        break;\n      case MachineRepresentation::kWord8:\n        opcode = kRiscvSb;\n        break;\n      case MachineRepresentation::kWord16:\n        opcode = kRiscvUsh;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:         // Fall through.\n      case MachineRepresentation::kWord32:\n        opcode = kRiscvUsw;\n        break;\n      case MachineRepresentation::kSimd128:\n        opcode = kRiscvRvvSt;\n        break;\n      case MachineRepresentation::kSimd256:            // Fall through.\n      case MachineRepresentation::kBit:                // Fall through.\n      case MachineRepresentation::kCompressedPointer:  // Fall through.\n      case MachineRepresentation::kCompressed:         // Fall through.\n      case MachineRepresentation::kSandboxedPointer:\n      case MachineRepresentation::kMapWord:  // Fall through.\n      case MachineRepresentation::kProtectedPointer:  // Fall through.\n      case MachineRepresentation::kNone:\n      case MachineRepresentation::kWord64:\n      case MachineRepresentation::kIndirectPointer:\n        UNREACHABLE();\n    }\n\n    if (g.CanBeImmediate(index, opcode)) {\n      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),\n           g.UseRegister(base), g.UseImmediate(index),\n           g.UseRegisterOrImmediateZero(value));\n    } else {\n      InstructionOperand addr_reg = g.TempRegister();\n      Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None), addr_reg,\n           g.UseRegister(index), g.UseRegister(base));\n      // Emit desired store opcode, using temp addr_reg.\n      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),\n           addr_reg, g.TempImmediate(0), g.UseRegisterOrImmediateZero(value));\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitUnalignedLoad", "content": "void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitUnalignedLoad", "content": "void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {\n  if constexpr (Adapter::IsTurboshaft) {\n    UNIMPLEMENTED();\n  } else {\n    LoadRepresentation load_rep = LoadRepresentationOf(node->op());\n    RiscvOperandGeneratorT<Adapter> g(this);\n    Node* base = node->InputAt(0);\n    Node* index = node->InputAt(1);\n\n    ArchOpcode opcode;\n    switch (load_rep.representation()) {\n      case MachineRepresentation::kFloat32:\n        opcode = kRiscvULoadFloat;\n        break;\n      case MachineRepresentation::kFloat64:\n        opcode = kRiscvULoadDouble;\n        break;\n      case MachineRepresentation::kWord8:\n        opcode = load_rep.IsUnsigned() ? kRiscvLbu : kRiscvLb;\n        break;\n      case MachineRepresentation::kWord16:\n        opcode = load_rep.IsUnsigned() ? kRiscvUlhu : kRiscvUlh;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:         // Fall through.\n      case MachineRepresentation::kWord32:\n        opcode = kRiscvUlw;\n        break;\n      case MachineRepresentation::kSimd128:\n        opcode = kRiscvRvvLd;\n        break;\n      case MachineRepresentation::kSimd256:            // Fall through.\n      case MachineRepresentation::kBit:                // Fall through.\n      case MachineRepresentation::kCompressedPointer:  // Fall through.\n      case MachineRepresentation::kCompressed:         // Fall through.\n      case MachineRepresentation::kSandboxedPointer:   // Fall through.\n      case MachineRepresentation::kMapWord:            // Fall through.\n      case MachineRepresentation::kProtectedPointer:   // Fall through.\n      case MachineRepresentation::kWord64:\n      case MachineRepresentation::kNone:\n      case MachineRepresentation::kIndirectPointer:\n        UNREACHABLE();\n    }\n\n    if (g.CanBeImmediate(index, opcode)) {\n      Emit(opcode | AddressingModeField::encode(kMode_MRI),\n           g.DefineAsRegister(node), g.UseRegister(base),\n           g.UseImmediate(index));\n    } else {\n      InstructionOperand addr_reg = g.TempRegister();\n      Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None), addr_reg,\n           g.UseRegister(index), g.UseRegister(base));\n      // Emit desired load opcode, using temp addr_reg.\n      Emit(opcode | AddressingModeField::encode(kMode_MRI),\n           g.DefineAsRegister(node), addr_reg, g.TempImmediate(0));\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes", "content": "void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes", "content": "void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitProtectedStore", "content": "void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) {\n  VisitStore(node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitProtectedStore", "content": "void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) {\n  // TODO(eholk)\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitStore", "content": "void InstructionSelectorT<Adapter>::VisitStore(typename Adapter::node_t node) {\n  typename Adapter::StoreView store_view = this->store_view(node);\n  DCHECK_EQ(store_view.displacement(), 0);\n  WriteBarrierKind write_barrier_kind =\n      store_view.stored_rep().write_barrier_kind();\n  MachineRepresentation representation =\n      store_view.stored_rep().representation();\n\n  Arm64OperandGeneratorT<Adapter> g(this);\n\n  // TODO(arm64): I guess this could be done in a better way.\n  if (write_barrier_kind != kNoWriteBarrier &&\n      !v8_flags.disable_write_barriers) {\n    DCHECK(CanBeTaggedOrCompressedOrIndirectPointer(representation));\n    AddressingMode addressing_mode;\n    InstructionOperand inputs[4];\n    size_t input_count = 0;\n    inputs[input_count++] = g.UseUniqueRegister(store_view.base());\n    // OutOfLineRecordWrite uses the index in an add or sub instruction, but we\n    // can trust the assembler to generate extra instructions if the index does\n    // not fit into add or sub. So here only check the immediate for a store.\n    node_t index = this->value(store_view.index());\n    if (g.CanBeImmediate(index, COMPRESS_POINTERS_BOOL ? kLoadStoreImm32\n                                                       : kLoadStoreImm64)) {\n      inputs[input_count++] = g.UseImmediate(index);\n      addressing_mode = kMode_MRI;\n    } else {\n      inputs[input_count++] = g.UseUniqueRegister(index);\n      addressing_mode = kMode_MRR;\n    }\n    inputs[input_count++] = g.UseUniqueRegister(store_view.value());\n    RecordWriteMode record_write_mode =\n        WriteBarrierKindToRecordWriteMode(write_barrier_kind);\n    InstructionCode code;\n    if (representation == MachineRepresentation::kIndirectPointer) {\n      DCHECK_EQ(write_barrier_kind, kIndirectPointerWriteBarrier);\n      // In this case we need to add the IndirectPointerTag as additional input.\n      code = kArchStoreIndirectWithWriteBarrier;\n      IndirectPointerTag tag = store_view.indirect_pointer_tag();\n      inputs[input_count++] = g.UseImmediate64(static_cast<int64_t>(tag));\n    } else {\n      code = kArchStoreWithWriteBarrier;\n    }\n    code |= AddressingModeField::encode(addressing_mode);\n    code |= RecordWriteModeField::encode(record_write_mode);\n    if (store_view.is_store_trap_on_null()) {\n      code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);\n    }\n    Emit(code, 0, nullptr, input_count, inputs);\n    return;\n  }\n\n  InstructionOperand inputs[4];\n  size_t input_count = 0;\n\n  MachineRepresentation approx_rep = representation;\n  auto info = GetStoreOpcodeAndImmediate(approx_rep, false);\n  InstructionCode opcode = std::get<InstructionCode>(info);\n  ImmediateMode immediate_mode = std::get<ImmediateMode>(info);\n\n  if (v8_flags.enable_unconditional_write_barriers) {\n    if (CanBeTaggedOrCompressedPointer(representation)) {\n      write_barrier_kind = kFullWriteBarrier;\n    }\n  }\n\n  base::Optional<ExternalReference> external_base;\n  if constexpr (Adapter::IsTurboshaft) {\n    ExternalReference value;\n    if (this->MatchExternalConstant(store_view.base(), &value)) {\n      external_base = value;\n    }\n  } else {\n    ExternalReferenceMatcher m(store_view.base());\n    if (m.HasResolvedValue()) {\n      external_base = m.ResolvedValue();\n    }\n  }\n\n  base::Optional<int64_t> constant_index;\n  if (this->valid(store_view.index())) {\n    node_t index = this->value(store_view.index());\n    constant_index = g.GetOptionalIntegerConstant(index);\n  }\n  if (external_base.has_value() && constant_index.has_value() &&\n      CanAddressRelativeToRootsRegister(*external_base)) {\n    ptrdiff_t const delta =\n        *constant_index +\n        MacroAssemblerBase::RootRegisterOffsetForExternalReference(\n            isolate(), *external_base);\n    if (is_int32(delta)) {\n      input_count = 2;\n      InstructionOperand inputs[2];\n      inputs[0] = g.UseRegister(store_view.value());\n      inputs[1] = g.UseImmediate(static_cast<int32_t>(delta));\n      opcode |= AddressingModeField::encode(kMode_Root);\n      Emit(opcode, 0, nullptr, input_count, inputs);\n      return;\n    }\n  }\n\n  node_t base = store_view.base();\n  node_t index = this->value(store_view.index());\n\n  inputs[input_count++] = g.UseRegisterOrImmediateZero(store_view.value());\n\n  if (this->is_load_root_register(base)) {\n    inputs[input_count++] = g.UseImmediate(index);\n    opcode |= AddressingModeField::encode(kMode_Root);\n    Emit(opcode, 0, nullptr, input_count, inputs);\n    return;\n  }\n\n  inputs[input_count++] = g.UseRegister(base);\n\n  if (g.CanBeImmediate(index, immediate_mode)) {\n    inputs[input_count++] = g.UseImmediate(index);\n    opcode |= AddressingModeField::encode(kMode_MRI);\n  } else if (TryMatchLoadStoreShift(&g, this, approx_rep, node, index,\n                                    &inputs[input_count],\n                                    &inputs[input_count + 1])) {\n    input_count += 2;\n    opcode |= AddressingModeField::encode(kMode_Operand2_R_LSL_I);\n  } else {\n    inputs[input_count++] = g.UseRegister(index);\n    opcode |= AddressingModeField::encode(kMode_MRR);\n  }\n\n  if (store_view.is_store_trap_on_null()) {\n    opcode |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);\n  } else if (store_view.access_kind() == MemoryAccessKind::kProtected) {\n    opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);\n  }\n\n  Emit(opcode, 0, nullptr, input_count, inputs);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStore(typename Adapter::node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitStore", "content": "void InstructionSelectorT<Adapter>::VisitStore(typename Adapter::node_t node) {\n  RiscvOperandGeneratorT<Adapter> g(this);\n  typename Adapter::StoreView store_view = this->store_view(node);\n  DCHECK_EQ(store_view.displacement(), 0);\n  node_t base = store_view.base();\n  node_t index = this->value(store_view.index());\n  node_t value = store_view.value();\n\n  WriteBarrierKind write_barrier_kind =\n      store_view.stored_rep().write_barrier_kind();\n  MachineRepresentation rep = store_view.stored_rep().representation();\n\n  // TODO(riscv): I guess this could be done in a better way.\n  if (write_barrier_kind != kNoWriteBarrier &&\n      V8_LIKELY(!v8_flags.disable_write_barriers)) {\n    DCHECK(CanBeTaggedOrCompressedOrIndirectPointer(rep));\n    InstructionOperand inputs[4];\n    size_t input_count = 0;\n    inputs[input_count++] = g.UseUniqueRegister(base);\n    inputs[input_count++] = g.UseUniqueRegister(index);\n    inputs[input_count++] = g.UseUniqueRegister(value);\n    RecordWriteMode record_write_mode =\n        WriteBarrierKindToRecordWriteMode(write_barrier_kind);\n    InstructionOperand temps[] = {g.TempRegister(), g.TempRegister()};\n    size_t const temp_count = arraysize(temps);\n    InstructionCode code;\n    if (rep == MachineRepresentation::kIndirectPointer) {\n      DCHECK_EQ(write_barrier_kind, kIndirectPointerWriteBarrier);\n      // In this case we need to add the IndirectPointerTag as additional input.\n      code = kArchStoreIndirectWithWriteBarrier;\n      IndirectPointerTag tag = store_view.indirect_pointer_tag();\n      inputs[input_count++] = g.UseImmediate64(static_cast<int64_t>(tag));\n    } else {\n      code = kArchStoreWithWriteBarrier;\n    }\n    code |= RecordWriteModeField::encode(record_write_mode);\n    if (store_view.is_store_trap_on_null()) {\n      code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);\n    }\n    Emit(code, 0, nullptr, input_count, inputs, temp_count, temps);\n  } else {\n    InstructionCode code;\n    switch (rep) {\n      case MachineRepresentation::kFloat32:\n        code = kRiscvStoreFloat;\n        break;\n      case MachineRepresentation::kFloat64:\n        code = kRiscvStoreDouble;\n        break;\n      case MachineRepresentation::kBit:  // Fall through.\n      case MachineRepresentation::kWord8:\n        code = kRiscvSb;\n        break;\n      case MachineRepresentation::kWord16:\n        code = kRiscvSh;\n        break;\n      case MachineRepresentation::kWord32:\n        code = kRiscvSw;\n        break;\n      case MachineRepresentation::kTaggedSigned:   // Fall through.\n      case MachineRepresentation::kTaggedPointer:  // Fall through.\n      case MachineRepresentation::kTagged:\n#ifdef V8_COMPRESS_POINTERS\n        code = kRiscvStoreCompressTagged;\n        break;\n#endif\n      case MachineRepresentation::kWord64:\n        code = kRiscvSd;\n        break;\n      case MachineRepresentation::kSimd128:\n        code = kRiscvRvvSt;\n        break;\n      case MachineRepresentation::kCompressedPointer:  // Fall through.\n      case MachineRepresentation::kCompressed:\n#ifdef V8_COMPRESS_POINTERS\n        code = kRiscvStoreCompressTagged;\n        break;\n#else\n        UNREACHABLE();\n#endif\n      case MachineRepresentation::kSandboxedPointer:\n        code = kRiscvStoreEncodeSandboxedPointer;\n        break;\n      case MachineRepresentation::kIndirectPointer:\n        code = kRiscvStoreIndirectPointer;\n        break;\n      case MachineRepresentation::kSimd256:  // Fall through.\n      case MachineRepresentation::kMapWord:  // Fall through.\n      case MachineRepresentation::kNone:\n      case MachineRepresentation::kProtectedPointer:\n        UNREACHABLE();\n    }\n\n    if (this->is_load_root_register(base)) {\n      Emit(code | AddressingModeField::encode(kMode_Root), g.NoOutput(),\n           g.UseRegisterOrImmediateZero(value), g.UseImmediate(index));\n      return;\n    }\n\n    if (store_view.is_store_trap_on_null()) {\n      code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);\n    } else if (store_view.access_kind() == MemoryAccessKind::kProtected) {\n      code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);\n    }\n\n    if (g.CanBeImmediate(index, code)) {\n      Emit(code | AddressingModeField::encode(kMode_MRI), g.NoOutput(),\n           g.UseRegisterOrImmediateZero(value), g.UseRegister(base),\n           g.UseImmediate(index));\n    } else {\n      InstructionOperand addr_reg = g.TempRegister();\n      Emit(kRiscvAdd64 | AddressingModeField::encode(kMode_None), addr_reg,\n           g.UseRegister(index), g.UseRegister(base));\n      // Emit desired store opcode, using temp addr_reg.\n      Emit(code | AddressingModeField::encode(kMode_MRI), g.NoOutput(),\n           g.UseRegisterOrImmediateZero(value), addr_reg, g.TempImmediate(0));\n    }\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStore(typename Adapter::node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitStorePair", "content": "void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {\n  Arm64OperandGeneratorT<Adapter> g(this);\n  if constexpr (Adapter::IsTurboshaft) {\n  UNIMPLEMENTED();\n  } else {\n    auto rep_pair = StorePairRepresentationOf(node->op());\n    CHECK_EQ(rep_pair.first.write_barrier_kind(), kNoWriteBarrier);\n    CHECK_EQ(rep_pair.second.write_barrier_kind(),\n             rep_pair.first.write_barrier_kind());\n    DCHECK(!v8_flags.enable_unconditional_write_barriers);\n\n    InstructionOperand inputs[4];\n    size_t input_count = 0;\n\n    MachineRepresentation approx_rep;\n    auto info1 =\n        GetStoreOpcodeAndImmediate(rep_pair.first.representation(), true);\n    auto info2 =\n        GetStoreOpcodeAndImmediate(rep_pair.second.representation(), true);\n    CHECK_EQ(ElementSizeLog2Of(rep_pair.first.representation()),\n             ElementSizeLog2Of(rep_pair.second.representation()));\n    switch (ElementSizeLog2Of(rep_pair.first.representation())) {\n      case 2:\n        approx_rep = MachineRepresentation::kWord32;\n        break;\n      case 3:\n        approx_rep = MachineRepresentation::kWord64;\n        break;\n      default:\n        UNREACHABLE();\n    }\n    InstructionCode opcode = std::get<InstructionCode>(info1);\n    ImmediateMode immediate_mode = std::get<ImmediateMode>(info1);\n    CHECK_EQ(opcode, std::get<InstructionCode>(info2));\n    CHECK_EQ(immediate_mode, std::get<ImmediateMode>(info2));\n\n    node_t base = this->input_at(node, 0);\n    node_t index = this->input_at(node, 1);\n    node_t value = this->input_at(node, 2);\n\n    inputs[input_count++] = g.UseRegisterOrImmediateZero(value);\n    inputs[input_count++] =\n        g.UseRegisterOrImmediateZero(this->input_at(node, 3));\n\n    if (this->is_load_root_register(base)) {\n      inputs[input_count++] = g.UseImmediate(index);\n      opcode |= AddressingModeField::encode(kMode_Root);\n      Emit(opcode, 0, nullptr, input_count, inputs);\n      return;\n    }\n\n    inputs[input_count++] = g.UseRegister(base);\n\n    if (g.CanBeImmediate(index, immediate_mode)) {\n      inputs[input_count++] = g.UseImmediate(index);\n      opcode |= AddressingModeField::encode(kMode_MRI);\n    } else if (TryMatchLoadStoreShift(&g, this, approx_rep, node, index,\n                                      &inputs[input_count],\n                                      &inputs[input_count + 1])) {\n      input_count += 2;\n      opcode |= AddressingModeField::encode(kMode_Operand2_R_LSL_I);\n    } else {\n      inputs[input_count++] = g.UseRegister(index);\n      opcode |= AddressingModeField::encode(kMode_MRR);\n    }\n\n    Emit(opcode, 0, nullptr, input_count, inputs);\n  }\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitStorePair", "content": "void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {\n  UNREACHABLE();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitProtectedLoad", "content": "void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {\n  VisitLoad(node);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitProtectedLoad", "content": "void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {\n  // TODO(eholk)\n  UNIMPLEMENTED();\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) "}], [{"name": "InstructionSelectorT<Adapter>::VisitLoad", "content": "void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {\n  InstructionCode opcode = kArchNop;\n  ImmediateMode immediate_mode = kNoImmediate;\n  auto load = this->load_view(node);\n  LoadRepresentation load_rep = load.loaded_rep();\n  MachineRepresentation rep = load_rep.representation();\n  switch (rep) {\n    case MachineRepresentation::kFloat32:\n      opcode = kArm64LdrS;\n      immediate_mode = kLoadStoreImm32;\n      break;\n    case MachineRepresentation::kFloat64:\n      opcode = kArm64LdrD;\n      immediate_mode = kLoadStoreImm64;\n      break;\n    case MachineRepresentation::kBit:  // Fall through.\n    case MachineRepresentation::kWord8:\n      opcode = load_rep.IsUnsigned()                            ? kArm64Ldrb\n               : load_rep.semantic() == MachineSemantic::kInt32 ? kArm64LdrsbW\n                                                                : kArm64Ldrsb;\n      immediate_mode = kLoadStoreImm8;\n      break;\n    case MachineRepresentation::kWord16:\n      opcode = load_rep.IsUnsigned()                            ? kArm64Ldrh\n               : load_rep.semantic() == MachineSemantic::kInt32 ? kArm64LdrshW\n                                                                : kArm64Ldrsh;\n      immediate_mode = kLoadStoreImm16;\n      break;\n    case MachineRepresentation::kWord32:\n      opcode = kArm64LdrW;\n      immediate_mode = kLoadStoreImm32;\n      break;\n    case MachineRepresentation::kCompressedPointer:  // Fall through.\n    case MachineRepresentation::kCompressed:\n#ifdef V8_COMPRESS_POINTERS\n      opcode = kArm64LdrW;\n      immediate_mode = kLoadStoreImm32;\n      break;\n#else\n      UNREACHABLE();\n#endif\n#ifdef V8_COMPRESS_POINTERS\n    case MachineRepresentation::kTaggedSigned:\n      opcode = kArm64LdrDecompressTaggedSigned;\n      immediate_mode = kLoadStoreImm32;\n      break;\n    case MachineRepresentation::kTaggedPointer:\n    case MachineRepresentation::kTagged:\n      opcode = kArm64LdrDecompressTagged;\n      immediate_mode = kLoadStoreImm32;\n      break;\n#else\n    case MachineRepresentation::kTaggedSigned:   // Fall through.\n    case MachineRepresentation::kTaggedPointer:  // Fall through.\n    case MachineRepresentation::kTagged:         // Fall through.\n#endif\n    case MachineRepresentation::kWord64:\n      opcode = kArm64Ldr;\n      immediate_mode = kLoadStoreImm64;\n      break;\n    case MachineRepresentation::kProtectedPointer:\n      CHECK(V8_ENABLE_SANDBOX_BOOL);\n      opcode = kArm64LdrDecompressProtected;\n      break;\n    case MachineRepresentation::kSandboxedPointer:\n      opcode = kArm64LdrDecodeSandboxedPointer;\n      immediate_mode = kLoadStoreImm64;\n      break;\n    case MachineRepresentation::kSimd128:\n      opcode = kArm64LdrQ;\n      immediate_mode = kNoImmediate;\n      break;\n    case MachineRepresentation::kSimd256:  // Fall through.\n    case MachineRepresentation::kMapWord:  // Fall through.\n    case MachineRepresentation::kIndirectPointer:  // Fall through.\n    case MachineRepresentation::kNone:\n      UNREACHABLE();\n  }\n  bool traps_on_null;\n  if (load.is_protected(&traps_on_null)) {\n    if (traps_on_null) {\n      opcode |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);\n    } else {\n      opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);\n    }\n  }\n  EmitLoad(this, node, opcode, immediate_mode, rep);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitLoad(node_t node) "}, {"name": "InstructionSelectorT<Adapter>::VisitLoad", "content": "void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {\n  auto load = this->load_view(node);\n  LoadRepresentation load_rep = load.loaded_rep();\n  InstructionCode opcode = kArchNop;\n  switch (load_rep.representation()) {\n    case MachineRepresentation::kFloat32:\n      opcode = kRiscvLoadFloat;\n      break;\n    case MachineRepresentation::kFloat64:\n      opcode = kRiscvLoadDouble;\n      break;\n    case MachineRepresentation::kBit:  // Fall through.\n    case MachineRepresentation::kWord8:\n      opcode = load_rep.IsUnsigned() ? kRiscvLbu : kRiscvLb;\n      break;\n    case MachineRepresentation::kWord16:\n      opcode = load_rep.IsUnsigned() ? kRiscvLhu : kRiscvLh;\n      break;\n    case MachineRepresentation::kTaggedSigned:   // Fall through.\n    case MachineRepresentation::kTaggedPointer:  // Fall through.\n    case MachineRepresentation::kTagged:         // Fall through.\n    case MachineRepresentation::kWord32:\n      opcode = kRiscvLw;\n      break;\n    case MachineRepresentation::kSimd128:\n      opcode = kRiscvRvvLd;\n      break;\n    case MachineRepresentation::kCompressedPointer:\n    case MachineRepresentation::kCompressed:\n    case MachineRepresentation::kSandboxedPointer:\n    case MachineRepresentation::kMapWord:  // Fall through.\n    case MachineRepresentation::kWord64:\n    case MachineRepresentation::kNone:\n    case MachineRepresentation::kSimd256:  // Fall through.\n    case MachineRepresentation::kProtectedPointer:  // Fall through.\n    case MachineRepresentation::kIndirectPointer:\n      UNREACHABLE();\n    }\n\n    EmitLoad(this, node, opcode);\n}", "name_and_para": "void InstructionSelectorT<Adapter>::VisitLoad(node_t node) "}], [{"name": "TryEmitExtendingLoad", "content": "bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector,\n                          typename Adapter::node_t node) {\n  ExtendingLoadMatcher<Adapter> m(node, selector);\n  Arm64OperandGeneratorT<Adapter> g(selector);\n  if (m.Matches()) {\n    InstructionOperand inputs[2];\n    inputs[0] = g.UseRegister(m.base());\n    InstructionCode opcode =\n        m.opcode() | AddressingModeField::encode(kMode_MRI);\n    DCHECK(is_int32(m.immediate()));\n    inputs[1] = g.TempImmediate(static_cast<int32_t>(m.immediate()));\n    InstructionOperand outputs[] = {g.DefineAsRegister(node)};\n    selector->Emit(opcode, arraysize(outputs), outputs, arraysize(inputs),\n                   inputs);\n    return true;\n  }\n  return false;\n}", "name_and_para": "bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector,\n                          typename Adapter::node_t node) "}, {"name": "TryEmitExtendingLoad", "content": "bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector,\n                          typename Adapter::node_t node,\n                          typename Adapter::node_t output_node) {\n  ExtendingLoadMatcher<Adapter> m(node, selector);\n  RiscvOperandGeneratorT<Adapter> g(selector);\n  if (m.Matches()) {\n    InstructionOperand inputs[2];\n    inputs[0] = g.UseRegister(m.base());\n    InstructionCode opcode =\n        m.opcode() | AddressingModeField::encode(kMode_MRI);\n    DCHECK(is_int32(m.immediate()));\n    inputs[1] = g.TempImmediate(static_cast<int32_t>(m.immediate()));\n    InstructionOperand outputs[] = {g.DefineAsRegister(output_node)};\n    selector->Emit(opcode, arraysize(outputs), outputs, arraysize(inputs),\n                   inputs);\n    return true;\n  }\n  return false;\n}", "name_and_para": "bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector,\n                          typename Adapter::node_t node,\n                          typename Adapter::node_t output_node) "}], [{"name": "ExtendingLoadMatcher", "content": "struct ExtendingLoadMatcher {\n  ExtendingLoadMatcher(typename Adapter::node_t node,\n                       InstructionSelectorT<Adapter>* selector)\n      : matches_(false), selector_(selector), immediate_(0) {\n    Initialize(node);\n  }\n\n  bool Matches() const { return matches_; }\n\n  typename Adapter::node_t base() const {\n    DCHECK(Matches());\n    return base_;\n  }\n  int64_t immediate() const {\n    DCHECK(Matches());\n    return immediate_;\n  }\n  ArchOpcode opcode() const {\n    DCHECK(Matches());\n    return opcode_;\n  }\n\n private:\n  bool matches_;\n  InstructionSelectorT<Adapter>* selector_;\n  typename Adapter::node_t base_{};\n  int64_t immediate_;\n  ArchOpcode opcode_;\n\n  void Initialize(Node* node) {\n    Int64BinopMatcher m(node);\n    // When loading a 64-bit value and shifting by 32, we should\n    // just load and sign-extend the interesting 4 bytes instead.\n    // This happens, for example, when we're loading and untagging SMIs.\n    DCHECK(m.IsWord64Sar());\n    if (m.left().IsLoad() && m.right().Is(32) &&\n        selector_->CanCover(m.node(), m.left().node())) {\n      Arm64OperandGeneratorT<Adapter> g(selector_);\n      Node* load = m.left().node();\n      Node* offset = load->InputAt(1);\n      base_ = load->InputAt(0);\n      opcode_ = kArm64Ldrsw;\n      if (g.IsIntegerConstant(offset)) {\n        immediate_ = g.GetIntegerConstantValue(offset) + 4;\n        matches_ = g.CanBeImmediate(immediate_, kLoadStoreImm32);\n      }\n    }\n  }\n\n  void Initialize(turboshaft::OpIndex node) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ShiftOp& shift = selector_->Get(node).template Cast<ShiftOp>();\n    DCHECK(shift.kind == ShiftOp::Kind::kShiftRightArithmetic ||\n           shift.kind == ShiftOp::Kind::kShiftRightArithmeticShiftOutZeros);\n    // When loading a 64-bit value and shifting by 32, we should\n    // just load and sign-extend the interesting 4 bytes instead.\n    // This happens, for example, when we're loading and untagging SMIs.\n    const Operation& lhs = selector_->Get(shift.left());\n    int64_t constant_rhs;\n\n    if (lhs.Is<LoadOp>() &&\n        selector_->MatchIntegralWord64Constant(shift.right(), &constant_rhs) &&\n        constant_rhs == 32 && selector_->CanCover(node, shift.left())) {\n      Arm64OperandGeneratorT<Adapter> g(selector_);\n      const LoadOp& load = lhs.Cast<LoadOp>();\n      base_ = load.base();\n      opcode_ = kArm64Ldrsw;\n      if (load.index().has_value()) {\n        int64_t index_constant;\n        if (selector_->MatchIntegralWord64Constant(load.index().value(),\n                                                   &index_constant)) {\n          DCHECK_EQ(load.element_size_log2, 0);\n          immediate_ = index_constant + 4;\n          matches_ = g.CanBeImmediate(immediate_, kLoadStoreImm32);\n        }\n      } else {\n        immediate_ = load.offset + 4;\n        matches_ = g.CanBeImmediate(immediate_, kLoadStoreImm32);\n      }\n    }\n  }\n}", "name_and_para": ""}, {"name": "ExtendingLoadMatcher", "content": "struct ExtendingLoadMatcher {\n  ExtendingLoadMatcher(typename Adapter::node_t node,\n                       InstructionSelectorT<Adapter>* selector)\n      : matches_(false), selector_(selector), immediate_(0) {\n    Initialize(node);\n  }\n\n  bool Matches() const { return matches_; }\n\n  typename Adapter::node_t base() const {\n    DCHECK(Matches());\n    return base_;\n  }\n  int64_t immediate() const {\n    DCHECK(Matches());\n    return immediate_;\n  }\n  ArchOpcode opcode() const {\n    DCHECK(Matches());\n    return opcode_;\n  }\n\n private:\n  bool matches_;\n  InstructionSelectorT<Adapter>* selector_;\n  typename Adapter::node_t base_{};\n  int64_t immediate_;\n  ArchOpcode opcode_;\n\n  void Initialize(Node* node) {\n    Int64BinopMatcher m(node);\n    // When loading a 64-bit value and shifting by 32, we should\n    // just load and sign-extend the interesting 4 bytes instead.\n    // This happens, for example, when we're loading and untagging SMIs.\n    DCHECK(m.IsWord64Sar());\n    if (m.left().IsLoad() && m.right().Is(32) &&\n        selector_->CanCover(m.node(), m.left().node())) {\n      DCHECK_EQ(selector_->GetEffectLevel(node),\n                selector_->GetEffectLevel(m.left().node()));\n      MachineRepresentation rep =\n          LoadRepresentationOf(m.left().node()->op()).representation();\n      DCHECK_EQ(3, ElementSizeLog2Of(rep));\n      if (rep != MachineRepresentation::kTaggedSigned &&\n          rep != MachineRepresentation::kTaggedPointer &&\n          rep != MachineRepresentation::kTagged &&\n          rep != MachineRepresentation::kWord64) {\n        return;\n      }\n\n      RiscvOperandGeneratorT<Adapter> g(selector_);\n      Node* load = m.left().node();\n      Node* offset = load->InputAt(1);\n      base_ = load->InputAt(0);\n      opcode_ = kRiscvLw;\n      if (g.CanBeImmediate(offset, opcode_)) {\n#if defined(V8_TARGET_LITTLE_ENDIAN)\n        immediate_ = g.GetIntegerConstantValue(offset) + 4;\n#elif defined(V8_TARGET_BIG_ENDIAN)\n        immediate_ = g.GetIntegerConstantValue(offset);\n#endif\n        matches_ = g.CanBeImmediate(immediate_, kRiscvLw);\n      }\n    }\n  }\n\n  void Initialize(turboshaft::OpIndex node) {\n    using namespace turboshaft;  // NOLINT(build/namespaces)\n    const ShiftOp& shift = selector_->Get(node).template Cast<ShiftOp>();\n    DCHECK(shift.kind == ShiftOp::Kind::kShiftRightArithmetic ||\n           shift.kind == ShiftOp::Kind::kShiftRightArithmeticShiftOutZeros);\n    // When loading a 64-bit value and shifting by 32, we should\n    // just load and sign-extend the interesting 4 bytes instead.\n    // This happens, for example, when we're loading and untagging SMIs.\n    const Operation& lhs = selector_->Get(shift.left());\n    int64_t constant_rhs;\n\n    if (lhs.Is<LoadOp>() &&\n        selector_->MatchIntegralWord64Constant(shift.right(), &constant_rhs) &&\n        constant_rhs == 32 && selector_->CanCover(node, shift.left())) {\n      RiscvOperandGeneratorT<Adapter> g(selector_);\n      const LoadOp& load = lhs.Cast<LoadOp>();\n      base_ = load.base();\n      opcode_ = kRiscvLw;\n      if (load.index().has_value()) {\n        int64_t index_constant;\n        if (selector_->MatchIntegralWord64Constant(load.index().value(),\n                                                   &index_constant)) {\n          DCHECK_EQ(load.element_size_log2, 0);\n          immediate_ = index_constant + 4;\n          matches_ = g.CanBeImmediate(immediate_, kRiscvLw);\n        }\n      } else {\n        immediate_ = load.offset + 4;\n        matches_ = g.CanBeImmediate(immediate_, kRiscvLw);\n      }\n    }\n  }\n}", "name_and_para": ""}]]], [["./v8/src/compiler/backend/riscv/instruction-codes-riscv.h", "./v8/src/compiler/backend/arm64/instruction-codes-arm64.h"], -1, -1, []], [["./v8/src/compiler/backend/riscv/code-generator-riscv.cc", "./v8/src/compiler/backend/arm64/code-generator-arm64.cc"], 0.8780487804878049, 0.8571428571428571, [[{"name": "CodeGenerator::AssembleJumpTable", "content": "void CodeGenerator::AssembleJumpTable(Label** targets, size_t target_count) {\n  // On 64-bit ARM we emit the jump tables inline.\n  UNREACHABLE();\n}", "name_and_para": "void CodeGenerator::AssembleJumpTable(Label** targets, size_t target_count) "}, {"name": "CodeGenerator::AssembleJumpTable", "content": "void CodeGenerator::AssembleJumpTable(Label** targets, size_t target_count) {\n  // On 64-bit RISC-V we emit the jump tables inline.\n  UNREACHABLE();\n}", "name_and_para": "void CodeGenerator::AssembleJumpTable(Label** targets, size_t target_count) "}], [{"name": "CodeGenerator::AssembleSwap", "content": "void CodeGenerator::AssembleSwap(InstructionOperand* source,\n                                 InstructionOperand* destination) {\n  Arm64OperandConverter g(this, nullptr);\n  switch (MoveType::InferSwap(source, destination)) {\n    case MoveType::kRegisterToRegister:\n      if (source->IsRegister()) {\n        __ Swap(g.ToRegister(source), g.ToRegister(destination));\n      } else {\n        VRegister src = g.ToDoubleRegister(source);\n        VRegister dst = g.ToDoubleRegister(destination);\n        if (source->IsFloatRegister() || source->IsDoubleRegister()) {\n          __ Swap(src, dst);\n        } else {\n          DCHECK(source->IsSimd128Register());\n          __ Swap(src.Q(), dst.Q());\n        }\n      }\n      return;\n    case MoveType::kRegisterToStack: {\n      UseScratchRegisterScope scope(masm());\n      MemOperand dst = g.ToMemOperand(destination, masm());\n      if (source->IsRegister()) {\n        Register temp = scope.AcquireX();\n        Register src = g.ToRegister(source);\n        __ Mov(temp, src);\n        __ Ldr(src, dst);\n        __ Str(temp, dst);\n      } else {\n        UseScratchRegisterScope scope(masm());\n        VRegister src = g.ToDoubleRegister(source);\n        if (source->IsFloatRegister() || source->IsDoubleRegister()) {\n          VRegister temp = scope.AcquireD();\n          __ Mov(temp, src);\n          __ Ldr(src, dst);\n          __ Str(temp, dst);\n        } else {\n          DCHECK(source->IsSimd128Register());\n          VRegister temp = scope.AcquireQ();\n          __ Mov(temp, src.Q());\n          __ Ldr(src.Q(), dst);\n          __ Str(temp, dst);\n        }\n      }\n      return;\n    }\n    case MoveType::kStackToStack: {\n      UseScratchRegisterScope scope(masm());\n      MemOperand src = g.ToMemOperand(source, masm());\n      MemOperand dst = g.ToMemOperand(destination, masm());\n      VRegister temp_0 = scope.AcquireD();\n      VRegister temp_1 = scope.AcquireD();\n      if (source->IsSimd128StackSlot()) {\n        __ Ldr(temp_0.Q(), src);\n        __ Ldr(temp_1.Q(), dst);\n        __ Str(temp_0.Q(), dst);\n        __ Str(temp_1.Q(), src);\n      } else {\n        __ Ldr(temp_0, src);\n        __ Ldr(temp_1, dst);\n        __ Str(temp_0, dst);\n        __ Str(temp_1, src);\n      }\n      return;\n    }\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void CodeGenerator::AssembleSwap(InstructionOperand* source,\n                                 InstructionOperand* destination) "}, {"name": "CodeGenerator::AssembleSwap", "content": "void CodeGenerator::AssembleSwap(InstructionOperand* source,\n                                 InstructionOperand* destination) {\n  RiscvOperandConverter g(this, nullptr);\n  switch (MoveType::InferSwap(source, destination)) {\n    case MoveType::kRegisterToRegister:\n      if (source->IsRegister()) {\n        Register temp = kScratchReg;\n        Register src = g.ToRegister(source);\n        Register dst = g.ToRegister(destination);\n        __ Move(temp, src);\n        __ Move(src, dst);\n        __ Move(dst, temp);\n      } else {\n        if (source->IsFloatRegister() || source->IsDoubleRegister()) {\n          FPURegister temp = kScratchDoubleReg;\n          FPURegister src = g.ToDoubleRegister(source);\n          FPURegister dst = g.ToDoubleRegister(destination);\n          __ Move(temp, src);\n          __ Move(src, dst);\n          __ Move(dst, temp);\n        } else {\n          DCHECK(source->IsSimd128Register());\n          VRegister src = g.ToDoubleRegister(source).toV();\n          VRegister dst = g.ToDoubleRegister(destination).toV();\n          VRegister temp = kSimd128ScratchReg;\n          __ VU.set(kScratchReg, E8, m1);\n          __ vmv_vv(temp, src);\n          __ vmv_vv(src, dst);\n          __ vmv_vv(dst, temp);\n        }\n      }\n      break;\n    case MoveType::kRegisterToStack: {\n      MemOperand dst = g.ToMemOperand(destination);\n      if (source->IsRegister()) {\n        Register temp = kScratchReg;\n        Register src = g.ToRegister(source);\n        __ mv(temp, src);\n        __ LoadWord(src, dst);\n        __ StoreWord(temp, dst);\n      } else {\n        MemOperand dst = g.ToMemOperand(destination);\n        if (source->IsFloatRegister()) {\n          DoubleRegister src = g.ToDoubleRegister(source);\n          DoubleRegister temp = kScratchDoubleReg;\n          __ fmv_s(temp, src);\n          __ LoadFloat(src, dst);\n          __ StoreFloat(temp, dst);\n        } else if (source->IsDoubleRegister()) {\n          DoubleRegister src = g.ToDoubleRegister(source);\n          DoubleRegister temp = kScratchDoubleReg;\n          __ fmv_d(temp, src);\n          __ LoadDouble(src, dst);\n          __ StoreDouble(temp, dst);\n        } else {\n          DCHECK(source->IsSimd128Register());\n          VRegister src = g.ToDoubleRegister(source).toV();\n          VRegister temp = kSimd128ScratchReg;\n          __ VU.set(kScratchReg, E8, m1);\n          __ vmv_vv(temp, src);\n          Register dst_v = dst.rm();\n          if (dst.offset() != 0) {\n            dst_v = kScratchReg2;\n            __ AddWord(dst_v, dst.rm(), Operand(dst.offset()));\n          }\n          __ vl(src, dst_v, 0, E8);\n          __ vs(temp, dst_v, 0, E8);\n        }\n      }\n    } break;\n    case MoveType::kStackToStack: {\n      MemOperand src = g.ToMemOperand(source);\n      MemOperand dst = g.ToMemOperand(destination);\n      if (source->IsSimd128StackSlot()) {\n        __ VU.set(kScratchReg, E8, m1);\n        Register src_v = src.rm();\n        Register dst_v = dst.rm();\n        if (src.offset() != 0) {\n          src_v = kScratchReg;\n          __ AddWord(src_v, src.rm(), Operand(src.offset()));\n        }\n        if (dst.offset() != 0) {\n          dst_v = kScratchReg2;\n          __ AddWord(dst_v, dst.rm(), Operand(dst.offset()));\n        }\n        __ vl(kSimd128ScratchReg, src_v, 0, E8);\n        __ vl(kSimd128ScratchReg2, dst_v, 0, E8);\n        __ vs(kSimd128ScratchReg, dst_v, 0, E8);\n        __ vs(kSimd128ScratchReg2, src_v, 0, E8);\n      } else {\n#if V8_TARGET_ARCH_RISCV32\n        if (source->IsFPStackSlot()) {\n          DCHECK(destination->IsFPStackSlot());\n          MachineRepresentation rep =\n              LocationOperand::cast(source)->representation();\n          if (rep == MachineRepresentation::kFloat64) {\n            FPURegister temp_double = kScratchDoubleReg;\n            Register temp_word32 = kScratchReg;\n            MemOperand src_hi(src.rm(), src.offset() + kSystemPointerSize);\n            MemOperand dst_hi(dst.rm(), dst.offset() + kSystemPointerSize);\n            __ LoadDouble(temp_double, src);\n            __ Lw(temp_word32, dst);\n            __ Sw(temp_word32, src);\n            __ Lw(temp_word32, dst_hi);\n            __ Sw(temp_word32, src_hi);\n            __ StoreDouble(temp_double, dst);\n            break;\n          }\n        }\n#endif\n        UseScratchRegisterScope scope(masm());\n        Register temp_0 = kScratchReg;\n        Register temp_1 = kScratchReg2;\n        __ LoadWord(temp_0, src);\n        __ LoadWord(temp_1, dst);\n        __ StoreWord(temp_0, dst);\n        __ StoreWord(temp_1, src);\n      }\n    } break;\n    default:\n      UNREACHABLE();\n  }\n}", "name_and_para": "void CodeGenerator::AssembleSwap(InstructionOperand* source,\n                                 InstructionOperand* destination) "}], [{"name": "CodeGenerator::AssembleMove", "content": "void CodeGenerator::AssembleMove(InstructionOperand* source,\n                                 InstructionOperand* destination) {\n  Arm64OperandConverter g(this, nullptr);\n  // Helper function to write the given constant to the dst register.\n  auto MoveConstantToRegister = [&](Register dst, Constant src) {\n    if (src.type() == Constant::kHeapObject) {\n      Handle<HeapObject> src_object = src.ToHeapObject();\n      RootIndex index;\n      if (IsMaterializableFromRoot(src_object, &index)) {\n        __ LoadRoot(dst, index);\n      } else {\n        __ Mov(dst, src_object);\n      }\n    } else if (src.type() == Constant::kCompressedHeapObject) {\n      Handle<HeapObject> src_object = src.ToHeapObject();\n      RootIndex index;\n      if (IsMaterializableFromRoot(src_object, &index)) {\n        __ LoadTaggedRoot(dst, index);\n      } else {\n        // TODO(v8:8977): Even though this mov happens on 32 bits (Note the\n        // .W()) and we are passing along the RelocInfo, we still haven't made\n        // the address embedded in the code-stream actually be compressed.\n        __ Mov(dst.W(),\n               Immediate(src_object, RelocInfo::COMPRESSED_EMBEDDED_OBJECT));\n      }\n    } else {\n      __ Mov(dst, g.ToImmediate(source));\n    }\n  };\n  switch (MoveType::InferMove(source, destination)) {\n    case MoveType::kRegisterToRegister:\n      if (source->IsRegister()) {\n        __ Mov(g.ToRegister(destination), g.ToRegister(source));\n      } else {\n        DCHECK(source->IsSimd128Register() || source->IsFloatRegister() ||\n               source->IsDoubleRegister());\n        __ Mov(g.ToDoubleRegister(destination).Q(),\n               g.ToDoubleRegister(source).Q());\n      }\n      return;\n    case MoveType::kRegisterToStack: {\n      MemOperand dst = g.ToMemOperand(destination, masm());\n      if (source->IsRegister()) {\n        __ Str(g.ToRegister(source), dst);\n      } else {\n        VRegister src = g.ToDoubleRegister(source);\n        if (source->IsFloatRegister() || source->IsDoubleRegister()) {\n          __ Str(src, dst);\n        } else {\n          DCHECK(source->IsSimd128Register());\n          __ Str(src.Q(), dst);\n        }\n      }\n      return;\n    }\n    case MoveType::kStackToRegister: {\n      MemOperand src = g.ToMemOperand(source, masm());\n      if (destination->IsRegister()) {\n        __ Ldr(g.ToRegister(destination), src);\n      } else {\n        VRegister dst = g.ToDoubleRegister(destination);\n        if (destination->IsFloatRegister() || destination->IsDoubleRegister()) {\n          __ Ldr(dst, src);\n        } else {\n          DCHECK(destination->IsSimd128Register());\n          __ Ldr(dst.Q(), src);\n        }\n      }\n      return;\n    }\n    case MoveType::kStackToStack: {\n      MemOperand src = g.ToMemOperand(source, masm());\n      MemOperand dst = g.ToMemOperand(destination, masm());\n      if (source->IsSimd128StackSlot()) {\n        UseScratchRegisterScope scope(masm());\n        VRegister temp = scope.AcquireQ();\n        __ Ldr(temp, src);\n        __ Str(temp, dst);\n      } else {\n        UseScratchRegisterScope scope(masm());\n        Register temp = scope.AcquireX();\n        __ Ldr(temp, src);\n        __ Str(temp, dst);\n      }\n      return;\n    }\n    case MoveType::kConstantToRegister: {\n      Constant src = g.ToConstant(source);\n      if (destination->IsRegister()) {\n        MoveConstantToRegister(g.ToRegister(destination), src);\n      } else {\n        VRegister dst = g.ToDoubleRegister(destination);\n        if (destination->IsFloatRegister()) {\n          __ Fmov(dst.S(), src.ToFloat32());\n        } else {\n          DCHECK(destination->IsDoubleRegister());\n          __ Fmov(dst, src.ToFloat64().value());\n        }\n      }\n      return;\n    }\n    case MoveType::kConstantToStack: {\n      Constant src = g.ToConstant(source);\n      MemOperand dst = g.ToMemOperand(destination, masm());\n      if (destination->IsStackSlot()) {\n        UseScratchRegisterScope scope(masm());\n        Register temp = scope.AcquireX();\n        MoveConstantToRegister(temp, src);\n        __ Str(temp, dst);\n      } else if (destination->IsFloatStackSlot()) {\n        if (base::bit_cast<int32_t>(src.ToFloat32()) == 0) {\n          __ Str(wzr, dst);\n        } else {\n          UseScratchRegisterScope scope(masm());\n          VRegister temp = scope.AcquireS();\n          __ Fmov(temp, src.ToFloat32());\n          __ Str(temp, dst);\n        }\n      } else {\n        DCHECK(destination->IsDoubleStackSlot());\n        if (src.ToFloat64().AsUint64() == 0) {\n          __ Str(xzr, dst);\n        } else {\n          UseScratchRegisterScope scope(masm());\n          VRegister temp = scope.AcquireD();\n          __ Fmov(temp, src.ToFloat64().value());\n          __ Str(temp, dst);\n        }\n      }\n      return;\n    }\n  }\n  UNREACHABLE();\n}", "name_and_para": "void CodeGenerator::AssembleMove(InstructionOperand* source,\n                                 InstructionOperand* destination) "}, {"name": "CodeGenerator::AssembleMove", "content": "void CodeGenerator::AssembleMove(InstructionOperand* source,\n                                 InstructionOperand* destination) {\n  RiscvOperandConverter g(this, nullptr);\n  // Dispatch on the source and destination operand kinds.  Not all\n  // combinations are possible.\n  if (source->IsRegister()) {\n    DCHECK(destination->IsRegister() || destination->IsStackSlot());\n    Register src = g.ToRegister(source);\n    if (destination->IsRegister()) {\n      __ Move(g.ToRegister(destination), src);\n    } else {\n      __ StoreWord(src, g.ToMemOperand(destination));\n    }\n  } else if (source->IsStackSlot()) {\n    DCHECK(destination->IsRegister() || destination->IsStackSlot());\n    MemOperand src = g.ToMemOperand(source);\n    if (destination->IsRegister()) {\n      __ LoadWord(g.ToRegister(destination), src);\n    } else {\n      Register temp = kScratchReg;\n      __ LoadWord(temp, src);\n      __ StoreWord(temp, g.ToMemOperand(destination));\n    }\n  } else if (source->IsConstant()) {\n    Constant src = g.ToConstant(source);\n    if (destination->IsRegister() || destination->IsStackSlot()) {\n      Register dst =\n          destination->IsRegister() ? g.ToRegister(destination) : kScratchReg;\n      switch (src.type()) {\n        case Constant::kInt32:\n          if (RelocInfo::IsWasmReference(src.rmode())) {\n            __ li(dst, Operand(src.ToInt32(), src.rmode()));\n          } else if (src.ToInt32() == 0 && destination->IsStackSlot()) {\n            dst = zero_reg;\n          } else {\n            __ li(dst, Operand(src.ToInt32()));\n          }\n          break;\n        case Constant::kFloat32:\n          __ li(dst, Operand::EmbeddedNumber(src.ToFloat32()));\n          break;\n        case Constant::kInt64:\n          if (RelocInfo::IsWasmReference(src.rmode())) {\n            __ li(dst, Operand(src.ToInt64(), src.rmode()));\n          } else {\n            if (src.ToInt64() == 0 && destination->IsStackSlot()) {\n              dst = zero_reg;\n            } else {\n              __ li(dst, Operand(src.ToInt64()));\n            }\n          }\n          break;\n        case Constant::kFloat64:\n          __ li(dst, Operand::EmbeddedNumber(src.ToFloat64().value()));\n          break;\n        case Constant::kExternalReference:\n          __ li(dst, src.ToExternalReference());\n          break;\n        case Constant::kHeapObject: {\n          Handle<HeapObject> src_object = src.ToHeapObject();\n          RootIndex index;\n          if (IsMaterializableFromRoot(src_object, &index)) {\n            __ LoadRoot(dst, index);\n          } else {\n            __ li(dst, src_object);\n          }\n          break;\n        }\n        case Constant::kCompressedHeapObject: {\n          Handle<HeapObject> src_object = src.ToHeapObject();\n          RootIndex index;\n          if (IsMaterializableFromRoot(src_object, &index)) {\n            __ LoadTaggedRoot(dst, index);\n          } else {\n            __ li(dst, src_object, RelocInfo::COMPRESSED_EMBEDDED_OBJECT);\n          }\n          break;\n        }\n        case Constant::kRpoNumber:\n          UNREACHABLE();  // TODO(titzer): loading RPO numbers\n      }\n      if (destination->IsStackSlot())\n        __ StoreWord(dst, g.ToMemOperand(destination));\n    } else if (src.type() == Constant::kFloat32) {\n      if (destination->IsFPStackSlot()) {\n        MemOperand dst = g.ToMemOperand(destination);\n        if (base::bit_cast<int32_t>(src.ToFloat32()) == 0) {\n          __ Sw(zero_reg, dst);\n        } else {\n          __ li(kScratchReg, Operand(base::bit_cast<int32_t>(src.ToFloat32())));\n          __ Sw(kScratchReg, dst);\n        }\n      } else {\n        DCHECK(destination->IsFPRegister());\n        FloatRegister dst = g.ToSingleRegister(destination);\n        __ LoadFPRImmediate(dst, src.ToFloat32());\n      }\n    } else {\n      DCHECK_EQ(Constant::kFloat64, src.type());\n      DoubleRegister dst = destination->IsFPRegister()\n                               ? g.ToDoubleRegister(destination)\n                               : kScratchDoubleReg;\n      __ LoadFPRImmediate(dst, src.ToFloat64().value());\n      if (destination->IsFPStackSlot()) {\n        __ StoreDouble(dst, g.ToMemOperand(destination));\n      }\n    }\n  } else if (source->IsFPRegister()) {\n    MachineRepresentation rep = LocationOperand::cast(source)->representation();\n    if (rep == MachineRepresentation::kSimd128) {\n      VRegister src = g.ToSimd128Register(source);\n      if (destination->IsSimd128Register()) {\n        VRegister dst = g.ToSimd128Register(destination);\n        __ VU.set(kScratchReg, E8, m1);\n        __ vmv_vv(dst, src);\n      } else {\n        DCHECK(destination->IsSimd128StackSlot());\n        __ VU.set(kScratchReg, E8, m1);\n        MemOperand dst = g.ToMemOperand(destination);\n        Register dst_r = dst.rm();\n        if (dst.offset() != 0) {\n          dst_r = kScratchReg;\n          __ AddWord(dst_r, dst.rm(), dst.offset());\n        }\n        __ vs(src, dst_r, 0, E8);\n      }\n    } else {\n      FPURegister src = g.ToDoubleRegister(source);\n      if (destination->IsFPRegister()) {\n        FPURegister dst = g.ToDoubleRegister(destination);\n        if (rep == MachineRepresentation::kFloat32) {\n          // In src/builtins/wasm-to-js.tq:193\n          //*toRef =\n          //Convert<intptr>(Bitcast<uint32>(WasmTaggedToFloat32(retVal))); so\n          // high 32 of src is 0. fmv.s can't NaNBox src.\n          __ fmv_x_w(kScratchReg, src);\n          __ fmv_w_x(dst, kScratchReg);\n        } else {\n          __ MoveDouble(dst, src);\n        }\n      } else {\n        DCHECK(destination->IsFPStackSlot());\n        if (rep == MachineRepresentation::kFloat32) {\n          __ StoreFloat(src, g.ToMemOperand(destination));\n        } else {\n          DCHECK_EQ(rep, MachineRepresentation::kFloat64);\n          __ StoreDouble(src, g.ToMemOperand(destination));\n        }\n      }\n    }\n  } else if (source->IsFPStackSlot()) {\n    DCHECK(destination->IsFPRegister() || destination->IsFPStackSlot());\n    MemOperand src = g.ToMemOperand(source);\n    MachineRepresentation rep = LocationOperand::cast(source)->representation();\n    if (rep == MachineRepresentation::kSimd128) {\n      __ VU.set(kScratchReg, E8, m1);\n      Register src_r = src.rm();\n      if (src.offset() != 0) {\n        src_r = kScratchReg;\n        __ AddWord(src_r, src.rm(), src.offset());\n      }\n      if (destination->IsSimd128Register()) {\n        __ vl(g.ToSimd128Register(destination), src_r, 0, E8);\n      } else {\n        DCHECK(destination->IsSimd128StackSlot());\n        VRegister temp = kSimd128ScratchReg;\n        MemOperand dst = g.ToMemOperand(destination);\n        Register dst_r = dst.rm();\n        if (dst.offset() != 0) {\n          dst_r = kScratchReg2;\n          __ AddWord(dst_r, dst.rm(), dst.offset());\n        }\n        __ vl(temp, src_r, 0, E8);\n        __ vs(temp, dst_r, 0, E8);\n      }\n    } else {\n      if (destination->IsFPRegister()) {\n        if (rep == MachineRepresentation::kFloat32) {\n          __ LoadFloat(g.ToDoubleRegister(destination), src);\n        } else {\n          DCHECK_EQ(rep, MachineRepresentation::kFloat64);\n          __ LoadDouble(g.ToDoubleRegister(destination), src);\n        }\n      } else {\n        DCHECK(destination->IsFPStackSlot());\n        FPURegister temp = kScratchDoubleReg;\n        if (rep == MachineRepresentation::kFloat32) {\n          __ LoadFloat(temp, src);\n          __ StoreFloat(temp, g.ToMemOperand(destination));\n        } else {\n          DCHECK_EQ(rep, MachineRepresentation::kFloat64);\n          __ LoadDouble(temp, src);\n          __ StoreDouble(temp, g.ToMemOperand(destination));\n        }\n      }\n    }\n  } else {\n    UNREACHABLE();\n  }\n}", "name_and_para": "void CodeGenerator::AssembleMove(InstructionOperand* source,\n                                 InstructionOperand* destination) "}], [{"name": "CodeGenerator::SetPendingMove", "content": "void CodeGenerator::SetPendingMove(MoveOperands* move) {\n  auto move_type = MoveType::InferMove(&move->source(), &move->destination());\n  if (move_type == MoveType::kStackToStack) {\n    Arm64OperandConverter g(this, nullptr);\n    MemOperand src = g.ToMemOperand(&move->source(), masm());\n    MemOperand dst = g.ToMemOperand(&move->destination(), masm());\n    UseScratchRegisterScope temps(masm());\n    if (move->source().IsSimd128StackSlot()) {\n      VRegister temp = temps.AcquireQ();\n      move_cycle_.scratch_fp_regs.set(temp);\n    } else {\n      Register temp = temps.AcquireX();\n      move_cycle_.scratch_regs.set(temp);\n    }\n    int64_t src_offset = src.offset();\n    unsigned src_size_log2 = CalcLSDataSizeLog2(LDR_x);\n    int64_t dst_offset = dst.offset();\n    unsigned dst_size_log2 = CalcLSDataSizeLog2(STR_x);\n    // Offset doesn't fit into the immediate field so the assembler will emit\n    // two instructions and use a second temp register.\n    if ((src.IsImmediateOffset() &&\n         !masm()->IsImmLSScaled(src_offset, src_size_log2) &&\n         !masm()->IsImmLSUnscaled(src_offset)) ||\n        (dst.IsImmediateOffset() &&\n         !masm()->IsImmLSScaled(dst_offset, dst_size_log2) &&\n         !masm()->IsImmLSUnscaled(dst_offset))) {\n      Register temp = temps.AcquireX();\n      move_cycle_.scratch_regs.set(temp);\n    }\n  }\n}", "name_and_para": "void CodeGenerator::SetPendingMove(MoveOperands* move) "}, {"name": "CodeGenerator::SetPendingMove", "content": "void CodeGenerator::SetPendingMove(MoveOperands* move) {\n  InstructionOperand* src = &move->source();\n  InstructionOperand* dst = &move->destination();\n  UseScratchRegisterScope temps(masm());\n  if (src->IsConstant() && dst->IsFPLocationOperand()) {\n    Register temp = temps.Acquire();\n    move_cycle_.scratch_regs.set(temp);\n  } else if (src->IsAnyStackSlot() || dst->IsAnyStackSlot()) {\n    RiscvOperandConverter g(this, nullptr);\n    bool src_need_scratch = false;\n    bool dst_need_scratch = false;\n    if (src->IsAnyStackSlot()) {\n      MemOperand src_mem = g.ToMemOperand(src);\n      src_need_scratch =\n          (!is_int16(src_mem.offset())) || (((src_mem.offset() & 0b111) != 0) &&\n                                            !is_int16(src_mem.offset() + 4));\n    }\n    if (dst->IsAnyStackSlot()) {\n      MemOperand dst_mem = g.ToMemOperand(dst);\n      dst_need_scratch =\n          (!is_int16(dst_mem.offset())) || (((dst_mem.offset() & 0b111) != 0) &&\n                                            !is_int16(dst_mem.offset() + 4));\n    }\n    if (src_need_scratch || dst_need_scratch) {\n      Register temp = temps.Acquire();\n      move_cycle_.scratch_regs.set(temp);\n    }\n  }\n}", "name_and_para": "void CodeGenerator::SetPendingMove(MoveOperands* move) "}], [{"name": "CodeGenerator::MoveTempLocationTo", "content": "void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,\n                                       MachineRepresentation rep) {\n  if (move_cycle_.scratch_reg.has_value()) {\n    auto& scratch_reg = *move_cycle_.scratch_reg;\n    if (!IsFloatingPoint(rep) && scratch_reg.IsD()) {\n      // We used a D register to move a non-FP operand, change the\n      // representation to correctly interpret the InstructionOperand's code.\n      AllocatedOperand scratch(LocationOperand::REGISTER,\n                               MachineRepresentation::kFloat64,\n                               move_cycle_.scratch_reg->code());\n      Arm64OperandConverter g(this, nullptr);\n      if (dest->IsStackSlot()) {\n        __ Str(g.ToDoubleRegister(&scratch), g.ToMemOperand(dest, masm()));\n      } else {\n        DCHECK(dest->IsRegister());\n        __ fmov(g.ToRegister(dest), g.ToDoubleRegister(&scratch));\n      }\n    } else {\n      AllocatedOperand scratch(LocationOperand::REGISTER, rep,\n                               move_cycle_.scratch_reg->code());\n      AssembleMove(&scratch, dest);\n    }\n  } else {\n    Pop(dest, rep);\n  }\n  // Restore the default state to release the {UseScratchRegisterScope} and to\n  // prepare for the next cycle.\n  move_cycle_ = MoveCycleState();\n}", "name_and_para": "void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,\n                                       MachineRepresentation rep) "}, {"name": "CodeGenerator::MoveTempLocationTo", "content": "void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,\n                                       MachineRepresentation rep) {\n  if (move_cycle_.scratch_reg.has_value()) {\n    // auto& scratch_reg = *move_cycle_.scratch_reg;\n    AllocatedOperand scratch(LocationOperand::REGISTER, rep,\n                             move_cycle_.scratch_reg->code());\n    AssembleMove(&scratch, dest);\n  } else {\n    Pop(dest, rep);\n  }\n  // Restore the default state to release the {UseScratchRegisterScope} and to\n  // prepare for the next cycle.\n  move_cycle_ = MoveCycleState();\n}", "name_and_para": "void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,\n                                       MachineRepresentation rep) "}], [{"name": "CodeGenerator::MoveToTempLocation", "content": "void CodeGenerator::MoveToTempLocation(InstructionOperand* source,\n                                       MachineRepresentation rep) {\n  // Must be kept in sync with {MoveTempLocationTo}.\n  DCHECK(!source->IsImmediate());\n  move_cycle_.temps.emplace(masm());\n  auto& temps = *move_cycle_.temps;\n  // Temporarily exclude the reserved scratch registers while we pick one to\n  // resolve the move cycle. Re-include them immediately afterwards as they\n  // might be needed for the move to the temp location.\n  temps.Exclude(CPURegList(64, move_cycle_.scratch_regs));\n  temps.ExcludeFP(CPURegList(64, move_cycle_.scratch_fp_regs));\n  if (!IsFloatingPoint(rep)) {\n    if (temps.CanAcquire()) {\n      Register scratch = move_cycle_.temps->AcquireX();\n      move_cycle_.scratch_reg.emplace(scratch);\n    } else if (temps.CanAcquireFP()) {\n      // Try to use an FP register if no GP register is available for non-FP\n      // moves.\n      DoubleRegister scratch = move_cycle_.temps->AcquireD();\n      move_cycle_.scratch_reg.emplace(scratch);\n    }\n  } else if (rep == MachineRepresentation::kFloat32) {\n    VRegister scratch = move_cycle_.temps->AcquireS();\n    move_cycle_.scratch_reg.emplace(scratch);\n  } else if (rep == MachineRepresentation::kFloat64) {\n    VRegister scratch = move_cycle_.temps->AcquireD();\n    move_cycle_.scratch_reg.emplace(scratch);\n  } else if (rep == MachineRepresentation::kSimd128) {\n    VRegister scratch = move_cycle_.temps->AcquireQ();\n    move_cycle_.scratch_reg.emplace(scratch);\n  }\n  temps.Include(CPURegList(64, move_cycle_.scratch_regs));\n  temps.IncludeFP(CPURegList(64, move_cycle_.scratch_fp_regs));\n  if (move_cycle_.scratch_reg.has_value()) {\n    // A scratch register is available for this rep.\n    auto& scratch_reg = *move_cycle_.scratch_reg;\n    if (scratch_reg.IsD() && !IsFloatingPoint(rep)) {\n      AllocatedOperand scratch(LocationOperand::REGISTER,\n                               MachineRepresentation::kFloat64,\n                               scratch_reg.code());\n      Arm64OperandConverter g(this, nullptr);\n      if (source->IsStackSlot()) {\n        __ Ldr(g.ToDoubleRegister(&scratch), g.ToMemOperand(source, masm()));\n      } else {\n        DCHECK(source->IsRegister());\n        __ fmov(g.ToDoubleRegister(&scratch), g.ToRegister(source));\n      }\n    } else {\n      AllocatedOperand scratch(LocationOperand::REGISTER, rep,\n                               move_cycle_.scratch_reg->code());\n      AssembleMove(source, &scratch);\n    }\n  } else {\n    // The scratch registers are blocked by pending moves. Use the stack\n    // instead.\n    Push(source);\n  }\n}", "name_and_para": "void CodeGenerator::MoveToTempLocation(InstructionOperand* source,\n                                       MachineRepresentation rep) "}, {"name": "CodeGenerator::MoveToTempLocation", "content": "void CodeGenerator::MoveToTempLocation(InstructionOperand* source,\n                                       MachineRepresentation rep) {\n  // Must be kept in sync with {MoveTempLocationTo}.\n  DCHECK(!source->IsImmediate());\n  move_cycle_.temps.emplace(masm());\n  auto& temps = *move_cycle_.temps;\n  // Temporarily exclude the reserved scratch registers while we pick one to\n  // resolve the move cycle. Re-include them immediately afterwards as they\n  // might be needed for the move to the temp location.\n  temps.Exclude(move_cycle_.scratch_regs);\n  if (!IsFloatingPoint(rep)) {\n    if (temps.hasAvailable()) {\n      Register scratch = move_cycle_.temps->Acquire();\n      move_cycle_.scratch_reg.emplace(scratch);\n    }\n  }\n\n  temps.Include(move_cycle_.scratch_regs);\n\n  if (move_cycle_.scratch_reg.has_value()) {\n    // A scratch register is available for this rep.\n    // auto& scratch_reg = *move_cycle_.scratch_reg;\n    AllocatedOperand scratch(LocationOperand::REGISTER, rep,\n                             move_cycle_.scratch_reg->code());\n    AssembleMove(source, &scratch);\n  } else {\n    // The scratch registers are blocked by pending moves. Use the stack\n    // instead.\n    Push(source);\n  }\n}", "name_and_para": "void CodeGenerator::MoveToTempLocation(InstructionOperand* source,\n                                       MachineRepresentation rep) "}], [{"name": "CodeGenerator::PopTempStackSlots", "content": "void CodeGenerator::PopTempStackSlots() {\n  if (temp_slots_ > 0) {\n    frame_access_state()->IncreaseSPDelta(-temp_slots_);\n    __ add(sp, sp, Operand(temp_slots_ * kSystemPointerSize));\n    temp_slots_ = 0;\n  }\n}", "name_and_para": "void CodeGenerator::PopTempStackSlots() "}, {"name": "CodeGenerator::PopTempStackSlots", "content": "void CodeGenerator::PopTempStackSlots() {\n  if (temp_slots_ > 0) {\n    frame_access_state()->IncreaseSPDelta(-temp_slots_);\n    __ AddWord(sp, sp, Operand(temp_slots_ * kSystemPointerSize));\n    temp_slots_ = 0;\n  }\n}", "name_and_para": "void CodeGenerator::PopTempStackSlots() "}], [{"name": "CodeGenerator::Pop", "content": "void CodeGenerator::Pop(InstructionOperand* dest, MachineRepresentation rep) {\n  int dropped_slots = RoundUp<2>(ElementSizeInPointers(rep));\n  Arm64OperandConverter g(this, nullptr);\n  if (dest->IsRegister()) {\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    __ Pop(g.ToRegister(dest), padreg);\n  } else if (dest->IsStackSlot()) {\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    UseScratchRegisterScope temps(masm());\n    Register scratch = temps.AcquireX();\n    __ Pop(scratch, padreg);\n    __ Str(scratch, g.ToMemOperand(dest, masm()));\n  } else {\n    int last_frame_slot_id =\n        frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;\n    int sp_delta = frame_access_state_->sp_delta();\n    int slot_id = last_frame_slot_id + sp_delta;\n    AllocatedOperand stack_slot(LocationOperand::STACK_SLOT, rep, slot_id);\n    AssembleMove(&stack_slot, dest);\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    __ Add(sp, sp, Operand(dropped_slots * kSystemPointerSize));\n  }\n  temp_slots_ -= dropped_slots;\n}", "name_and_para": "void CodeGenerator::Pop(InstructionOperand* dest, MachineRepresentation rep) "}, {"name": "CodeGenerator::Pop", "content": "void CodeGenerator::Pop(InstructionOperand* dest, MachineRepresentation rep) {\n  int dropped_slots = ElementSizeInPointers(rep);\n  RiscvOperandConverter g(this, nullptr);\n  if (dest->IsRegister()) {\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    __ Pop(g.ToRegister(dest));\n  } else if (dest->IsStackSlot()) {\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    UseScratchRegisterScope temps(masm());\n    Register scratch = temps.Acquire();\n    __ Pop(scratch);\n    __ StoreWord(scratch, g.ToMemOperand(dest));\n  } else {\n    int last_frame_slot_id =\n        frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;\n    int sp_delta = frame_access_state_->sp_delta();\n    int slot_id = last_frame_slot_id + sp_delta;\n    AllocatedOperand stack_slot(LocationOperand::STACK_SLOT, rep, slot_id);\n    AssembleMove(&stack_slot, dest);\n    frame_access_state()->IncreaseSPDelta(-dropped_slots);\n    __ AddWord(sp, sp, Operand(dropped_slots * kSystemPointerSize));\n  }\n  temp_slots_ -= dropped_slots;\n}", "name_and_para": "void CodeGenerator::Pop(InstructionOperand* dest, MachineRepresentation rep) "}], [{"name": "CodeGenerator::Push", "content": "AllocatedOperand CodeGenerator::Push(InstructionOperand* source) {\n  auto rep = LocationOperand::cast(source)->representation();\n  int new_slots = RoundUp<2>(ElementSizeInPointers(rep));\n  Arm64OperandConverter g(this, nullptr);\n  int last_frame_slot_id =\n      frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;\n  int sp_delta = frame_access_state_->sp_delta();\n  int slot_id = last_frame_slot_id + sp_delta + new_slots;\n  AllocatedOperand stack_slot(LocationOperand::STACK_SLOT, rep, slot_id);\n  if (source->IsRegister()) {\n    __ Push(padreg, g.ToRegister(source));\n    frame_access_state()->IncreaseSPDelta(new_slots);\n  } else if (source->IsStackSlot()) {\n    UseScratchRegisterScope temps(masm());\n    Register scratch = temps.AcquireX();\n    __ Ldr(scratch, g.ToMemOperand(source, masm()));\n    __ Push(padreg, scratch);\n    frame_access_state()->IncreaseSPDelta(new_slots);\n  } else {\n    // No push instruction for this operand type. Bump the stack pointer and\n    // assemble the move.\n    __ Sub(sp, sp, Operand(new_slots * kSystemPointerSize));\n    frame_access_state()->IncreaseSPDelta(new_slots);\n    AssembleMove(source, &stack_slot);\n  }\n  temp_slots_ += new_slots;\n  return stack_slot;\n}", "name_and_para": "AllocatedOperand CodeGenerator::Push(InstructionOperand* source) "}, {"name": "CodeGenerator::Push", "content": "AllocatedOperand CodeGenerator::Push(InstructionOperand* source) {\n  auto rep = LocationOperand::cast(source)->representation();\n  int new_slots = ElementSizeInPointers(rep);\n  RiscvOperandConverter g(this, nullptr);\n  int last_frame_slot_id =\n      frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;\n  int sp_delta = frame_access_state_->sp_delta();\n  int slot_id = last_frame_slot_id + sp_delta + new_slots;\n  AllocatedOperand stack_slot(LocationOperand::STACK_SLOT, rep, slot_id);\n  if (source->IsRegister()) {\n    __ Push(g.ToRegister(source));\n    frame_access_state()->IncreaseSPDelta(new_slots);\n  } else if (source->IsStackSlot()) {\n    UseScratchRegisterScope temps(masm());\n    Register scratch = temps.Acquire();\n    __ LoadWord(scratch, g.ToMemOperand(source));\n    __ Push(scratch);\n    frame_access_state()->IncreaseSPDelta(new_slots);\n  } else {\n    // No push instruction for this operand type. Bump the stack pointer and\n    // assemble the move.\n    __ SubWord(sp, sp, Operand(new_slots * kSystemPointerSize));\n    frame_access_state()->IncreaseSPDelta(new_slots);\n    AssembleMove(source, &stack_slot);\n  }\n  temp_slots_ += new_slots;\n  return stack_slot;\n}", "name_and_para": "AllocatedOperand CodeGenerator::Push(InstructionOperand* source) "}], [{"name": "CodeGenerator::PrepareForDeoptimizationExits", "content": "void CodeGenerator::PrepareForDeoptimizationExits(\n    ZoneDeque<DeoptimizationExit*>* exits) {\n  __ ForceConstantPoolEmissionWithoutJump();\n  // We are conservative here, reserving sufficient space for the largest deopt\n  // kind.\n  DCHECK_GE(Deoptimizer::kLazyDeoptExitSize, Deoptimizer::kEagerDeoptExitSize);\n  __ CheckVeneerPool(\n      false, false,\n      static_cast<int>(exits->size()) * Deoptimizer::kLazyDeoptExitSize);\n\n  // Check which deopt kinds exist in this InstructionStream object, to avoid\n  // emitting jumps to unused entries.\n  bool saw_deopt_kind[kDeoptimizeKindCount] = {false};\n  for (auto exit : *exits) {\n    saw_deopt_kind[static_cast<int>(exit->kind())] = true;\n  }\n\n  // Emit the jumps to deoptimization entries.\n  UseScratchRegisterScope scope(masm());\n  Register scratch = scope.AcquireX();\n  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);\n  for (int i = 0; i < kDeoptimizeKindCount; i++) {\n    if (!saw_deopt_kind[i]) continue;\n    DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);\n    __ bind(&jump_deoptimization_entry_labels_[i]);\n    __ LoadEntryFromBuiltin(Deoptimizer::GetDeoptimizationEntry(kind), scratch);\n    __ Jump(scratch);\n  }\n}", "name_and_para": "void CodeGenerator::PrepareForDeoptimizationExits(\n    ZoneDeque<DeoptimizationExit*>* exits) "}, {"name": "CodeGenerator::PrepareForDeoptimizationExits", "content": "void CodeGenerator::PrepareForDeoptimizationExits(\n    ZoneDeque<DeoptimizationExit*>* exits) {\n  __ ForceConstantPoolEmissionWithoutJump();\n  int total_size = 0;\n  for (DeoptimizationExit* exit : deoptimization_exits_) {\n    total_size += (exit->kind() == DeoptimizeKind::kLazy)\n                      ? Deoptimizer::kLazyDeoptExitSize\n                      : Deoptimizer::kEagerDeoptExitSize;\n  }\n\n  __ CheckTrampolinePoolQuick(total_size);\n}", "name_and_para": "void CodeGenerator::PrepareForDeoptimizationExits(\n    ZoneDeque<DeoptimizationExit*>* exits) "}], [{"name": "CodeGenerator::FinishCode", "content": "void CodeGenerator::FinishCode() { __ ForceConstantPoolEmissionWithoutJump(); }", "name_and_para": "void CodeGenerator::FinishCode() "}, {"name": "CodeGenerator::FinishCode", "content": "void CodeGenerator::FinishCode() { __ ForceConstantPoolEmissionWithoutJump(); }", "name_and_para": "void CodeGenerator::FinishCode() "}], [{"name": "CodeGenerator::AssembleReturn", "content": "void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n\n  const int returns = RoundUp(frame()->GetReturnSlotCount(), 2);\n  if (returns != 0) {\n    __ Drop(returns);\n  }\n\n  // Restore registers.\n  CPURegList saves =\n      CPURegList(kXRegSizeInBits, call_descriptor->CalleeSavedRegisters());\n  __ PopCPURegList(saves);\n\n  // Restore fp registers.\n  CPURegList saves_fp =\n      CPURegList(kDRegSizeInBits, call_descriptor->CalleeSavedFPRegisters());\n  __ PopCPURegList(saves_fp);\n\n  unwinding_info_writer_.MarkBlockWillExit();\n\n  const int parameter_slots =\n      static_cast<int>(call_descriptor->ParameterSlotCount());\n  Arm64OperandConverter g(this, nullptr);\n\n  // {aditional_pop_count} is only greater than zero if {parameter_slots = 0}.\n  // Check RawMachineAssembler::PopAndReturn.\n  if (parameter_slots != 0) {\n    if (additional_pop_count->IsImmediate()) {\n      DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);\n    } else if (v8_flags.debug_code) {\n      __ cmp(g.ToRegister(additional_pop_count), Operand(0));\n      __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue);\n    }\n  }\n\n  Register argc_reg = x3;\n  // Functions with JS linkage have at least one parameter (the receiver).\n  // If {parameter_slots} == 0, it means it is a builtin with\n  // kDontAdaptArgumentsSentinel, which takes care of JS arguments popping\n  // itself.\n  const bool drop_jsargs = parameter_slots != 0 &&\n                           frame_access_state()->has_frame() &&\n                           call_descriptor->IsJSFunctionCall();\n  if (call_descriptor->IsCFunctionCall()) {\n    AssembleDeconstructFrame();\n  } else if (frame_access_state()->has_frame()) {\n    // Canonicalize JSFunction return sites for now unless they have an variable\n    // number of stack slot pops.\n    if (additional_pop_count->IsImmediate() &&\n        g.ToConstant(additional_pop_count).ToInt32() == 0) {\n      if (return_label_.is_bound()) {\n        __ B(&return_label_);\n        return;\n      } else {\n        __ Bind(&return_label_);\n      }\n    }\n    if (drop_jsargs) {\n      // Get the actual argument count.\n      DCHECK(!call_descriptor->CalleeSavedRegisters().has(argc_reg));\n      __ Ldr(argc_reg, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n    }\n    AssembleDeconstructFrame();\n  }\n\n  if (drop_jsargs) {\n    // We must pop all arguments from the stack (including the receiver). This\n    // number of arguments is given by max(1 + argc_reg, parameter_slots).\n    Label argc_reg_has_final_count;\n    DCHECK(!call_descriptor->CalleeSavedRegisters().has(argc_reg));\n    if (parameter_slots > 1) {\n      __ Cmp(argc_reg, Operand(parameter_slots));\n      __ B(&argc_reg_has_final_count, ge);\n      __ Mov(argc_reg, Operand(parameter_slots));\n      __ Bind(&argc_reg_has_final_count);\n    }\n    __ DropArguments(argc_reg);\n  } else if (additional_pop_count->IsImmediate()) {\n    int additional_count = g.ToConstant(additional_pop_count).ToInt32();\n    __ DropArguments(parameter_slots + additional_count);\n  } else if (parameter_slots == 0) {\n    __ DropArguments(g.ToRegister(additional_pop_count));\n  } else {\n    // {additional_pop_count} is guaranteed to be zero if {parameter_slots !=\n    // 0}. Check RawMachineAssembler::PopAndReturn.\n    __ DropArguments(parameter_slots);\n  }\n  __ AssertSpAligned();\n  __ Ret();\n}", "name_and_para": "void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) "}, {"name": "CodeGenerator::AssembleReturn", "content": "void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n\n  const int returns = frame()->GetReturnSlotCount();\n  if (returns != 0) {\n    __ AddWord(sp, sp, Operand(returns * kSystemPointerSize));\n  }\n\n  // Restore GP registers.\n  const RegList saves = call_descriptor->CalleeSavedRegisters();\n  if (!saves.is_empty()) {\n    __ MultiPop(saves);\n  }\n\n  // Restore FPU registers.\n  const DoubleRegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();\n  if (!saves_fpu.is_empty()) {\n    __ MultiPopFPU(saves_fpu);\n  }\n\n  RiscvOperandConverter g(this, nullptr);\n\n  const int parameter_slots =\n      static_cast<int>(call_descriptor->ParameterSlotCount());\n\n  // {aditional_pop_count} is only greater than zero if {parameter_slots = 0}.\n  // Check RawMachineAssembler::PopAndReturn.\n  if (parameter_slots != 0) {\n    if (additional_pop_count->IsImmediate()) {\n      DCHECK_EQ(g.ToConstant(additional_pop_count).ToInt32(), 0);\n    } else if (v8_flags.debug_code) {\n      __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue,\n                g.ToRegister(additional_pop_count),\n                Operand(static_cast<intptr_t>(0)));\n    }\n  }\n\n  // Functions with JS linkage have at least one parameter (the receiver).\n  // If {parameter_slots} == 0, it means it is a builtin with\n  // kDontAdaptArgumentsSentinel, which takes care of JS arguments popping\n  // itself.\n  const bool drop_jsargs = frame_access_state()->has_frame() &&\n                           call_descriptor->IsJSFunctionCall() &&\n                           parameter_slots != 0;\n\n  if (call_descriptor->IsCFunctionCall()) {\n    AssembleDeconstructFrame();\n  } else if (frame_access_state()->has_frame()) {\n    // Canonicalize JSFunction return sites for now unless they have an variable\n    // number of stack slot pops.\n    if (additional_pop_count->IsImmediate() &&\n        g.ToConstant(additional_pop_count).ToInt32() == 0) {\n      if (return_label_.is_bound()) {\n        __ Branch(&return_label_);\n        return;\n      } else {\n        __ bind(&return_label_);\n      }\n    }\n    if (drop_jsargs) {\n      // Get the actual argument count\n      __ LoadWord(t0, MemOperand(fp, StandardFrameConstants::kArgCOffset));\n    }\n    AssembleDeconstructFrame();\n  }\n  if (drop_jsargs) {\n    // We must pop all arguments from the stack (including the receiver). This\n    // number of arguments is given by max(1 + argc_reg, parameter_slots).\n    if (parameter_slots > 1) {\n      Label done;\n      __ li(kScratchReg, parameter_slots);\n      __ BranchShort(&done, ge, t0, Operand(kScratchReg));\n      __ Move(t0, kScratchReg);\n      __ bind(&done);\n    }\n    __ SllWord(t0, t0, kSystemPointerSizeLog2);\n    __ AddWord(sp, sp, t0);\n  } else if (additional_pop_count->IsImmediate()) {\n    // it should be a kInt32 or a kInt64\n    DCHECK_LE(g.ToConstant(additional_pop_count).type(), Constant::kInt64);\n    int additional_count = g.ToConstant(additional_pop_count).ToInt32();\n    __ Drop(parameter_slots + additional_count);\n  } else {\n    Register pop_reg = g.ToRegister(additional_pop_count);\n    __ Drop(parameter_slots);\n    __ SllWord(pop_reg, pop_reg, kSystemPointerSizeLog2);\n    __ AddWord(sp, sp, pop_reg);\n  }\n  __ Ret();\n}", "name_and_para": "void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) "}], [{"name": "CodeGenerator::AssembleConstructFrame", "content": "void CodeGenerator::AssembleConstructFrame() {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n  __ AssertSpAligned();\n\n  // The frame has been previously padded in CodeGenerator::FinishFrame().\n  DCHECK_EQ(frame()->GetTotalFrameSlotCount() % 2, 0);\n  int required_slots =\n      frame()->GetTotalFrameSlotCount() - frame()->GetFixedSlotCount();\n\n  CPURegList saves =\n      CPURegList(kXRegSizeInBits, call_descriptor->CalleeSavedRegisters());\n  DCHECK_EQ(saves.Count() % 2, 0);\n  CPURegList saves_fp =\n      CPURegList(kDRegSizeInBits, call_descriptor->CalleeSavedFPRegisters());\n  DCHECK_EQ(saves_fp.Count() % 2, 0);\n  // The number of return slots should be even after aligning the Frame.\n  const int returns = frame()->GetReturnSlotCount();\n  DCHECK_EQ(returns % 2, 0);\n\n  if (frame_access_state()->has_frame()) {\n    // Link the frame\n    if (call_descriptor->IsJSFunctionCall()) {\n      static_assert(StandardFrameConstants::kFixedFrameSize % 16 == 8);\n      DCHECK_EQ(required_slots % 2, 1);\n      __ Prologue();\n      // Update required_slots count since we have just claimed one extra slot.\n      static_assert(MacroAssembler::kExtraSlotClaimedByPrologue == 1);\n      required_slots -= MacroAssembler::kExtraSlotClaimedByPrologue;\n#if V8_ENABLE_WEBASSEMBLY\n    } else if (call_descriptor->IsWasmFunctionCall() ||\n               call_descriptor->IsWasmCapiFunction() ||\n               call_descriptor->IsWasmImportWrapper() ||\n               (call_descriptor->IsCFunctionCall() &&\n                info()->GetOutputStackFrameType() ==\n                    StackFrame::C_WASM_ENTRY)) {\n      UseScratchRegisterScope temps(masm());\n      Register scratch = temps.AcquireX();\n      __ Mov(scratch,\n             StackFrame::TypeToMarker(info()->GetOutputStackFrameType()));\n      __ Push<MacroAssembler::kSignLR>(lr, fp, scratch, kWasmInstanceRegister);\n      static constexpr int kSPToFPDelta = 2 * kSystemPointerSize;\n      __ Add(fp, sp, kSPToFPDelta);\n      if (call_descriptor->IsWasmCapiFunction()) {\n        // The C-API function has one extra slot for the PC.\n        required_slots++;\n      } else if (call_descriptor->IsWasmImportWrapper()) {\n        // If the wrapper is running on a secondary stack, it will switch to the\n        // central stack and fill these slots with the central stack pointer and\n        // secondary stack limit. Otherwise the slots remain empty.\n        static_assert(WasmImportWrapperFrameConstants::kCentralStackSPOffset ==\n                      -24);\n        static_assert(\n            WasmImportWrapperFrameConstants::kSecondaryStackLimitOffset == -32);\n        __ Push(xzr, xzr);\n      }\n#endif  // V8_ENABLE_WEBASSEMBLY\n    } else if (call_descriptor->kind() == CallDescriptor::kCallCodeObject) {\n      UseScratchRegisterScope temps(masm());\n      Register scratch = temps.AcquireX();\n      __ Mov(scratch,\n             StackFrame::TypeToMarker(info()->GetOutputStackFrameType()));\n      __ Push<MacroAssembler::kSignLR>(lr, fp, scratch, padreg);\n      static constexpr int kSPToFPDelta = 2 * kSystemPointerSize;\n      __ Add(fp, sp, kSPToFPDelta);\n      // One of the extra slots has just been claimed when pushing the padreg.\n      // We also know that we have at least one slot to claim here, as the typed\n      // frame has an odd number of fixed slots, and all other parts of the\n      // total frame slots are even, leaving {required_slots} to be odd.\n      DCHECK_GE(required_slots, 1);\n      required_slots--;\n    } else {\n      __ Push<MacroAssembler::kSignLR>(lr, fp);\n      __ Mov(fp, sp);\n    }\n    unwinding_info_writer_.MarkFrameConstructed(__ pc_offset());\n\n    // Create OSR entry if applicable\n    if (info()->is_osr()) {\n      // TurboFan OSR-compiled functions cannot be entered directly.\n      __ Abort(AbortReason::kShouldNotDirectlyEnterOsrFunction);\n\n      // Unoptimized code jumps directly to this entrypoint while the\n      // unoptimized frame is still on the stack. Optimized code uses OSR values\n      // directly from the unoptimized frame. Thus, all that needs to be done is\n      // to allocate the remaining stack slots.\n      __ RecordComment(\"-- OSR entrypoint --\");\n      osr_pc_offset_ = __ pc_offset();\n      __ CodeEntry();\n      size_t unoptimized_frame_slots = osr_helper()->UnoptimizedFrameSlots();\n      DCHECK(call_descriptor->IsJSFunctionCall());\n      DCHECK_EQ(unoptimized_frame_slots % 2, 1);\n      // One unoptimized frame slot has already been claimed when the actual\n      // arguments count was pushed.\n      required_slots -=\n          unoptimized_frame_slots - MacroAssembler::kExtraSlotClaimedByPrologue;\n    }\n\n#if V8_ENABLE_WEBASSEMBLY\n    if (info()->IsWasm() && required_slots * kSystemPointerSize > 4 * KB) {\n      // For WebAssembly functions with big frames we have to do the stack\n      // overflow check before we construct the frame. Otherwise we may not\n      // have enough space on the stack to call the runtime for the stack\n      // overflow.\n      Label done;\n      // If the frame is bigger than the stack, we throw the stack overflow\n      // exception unconditionally. Thereby we can avoid the integer overflow\n      // check in the condition code.\n      if (required_slots * kSystemPointerSize < v8_flags.stack_size * KB) {\n        UseScratchRegisterScope temps(masm());\n        Register stack_limit = temps.AcquireX();\n        __ LoadStackLimit(stack_limit, StackLimitKind::kRealStackLimit);\n        __ Add(stack_limit, stack_limit, required_slots * kSystemPointerSize);\n        __ Cmp(sp, stack_limit);\n        __ B(hs, &done);\n      }\n\n      __ Call(static_cast<intptr_t>(Builtin::kWasmStackOverflow),\n              RelocInfo::WASM_STUB_CALL);\n      // The call does not return, hence we can ignore any references and just\n      // define an empty safepoint.\n      ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());\n      RecordSafepoint(reference_map);\n      if (v8_flags.debug_code) __ Brk(0);\n      __ Bind(&done);\n    }\n#endif  // V8_ENABLE_WEBASSEMBLY\n\n    // Skip callee-saved slots, which are pushed below.\n    required_slots -= saves.Count();\n    required_slots -= saves_fp.Count();\n    required_slots -= returns;\n\n    __ Claim(required_slots);\n  }\n\n  // Save FP registers.\n  DCHECK_IMPLIES(saves_fp.Count() != 0,\n                 saves_fp.bits() == CPURegList::GetCalleeSavedV().bits());\n  __ PushCPURegList(saves_fp);\n\n  // Save registers.\n  __ PushCPURegList(saves);\n\n  if (returns != 0) {\n    __ Claim(returns);\n  }\n\n  for (int spill_slot : frame()->tagged_slots()) {\n    FrameOffset offset = frame_access_state()->GetFrameOffset(spill_slot);\n    DCHECK(offset.from_frame_pointer());\n    __ Str(xzr, MemOperand(fp, offset.offset()));\n  }\n}", "name_and_para": "void CodeGenerator::AssembleConstructFrame() "}, {"name": "CodeGenerator::AssembleConstructFrame", "content": "void CodeGenerator::AssembleConstructFrame() {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n\n  if (frame_access_state()->has_frame()) {\n    if (call_descriptor->IsCFunctionCall()) {\n      if (info()->GetOutputStackFrameType() == StackFrame::C_WASM_ENTRY) {\n        __ StubPrologue(StackFrame::C_WASM_ENTRY);\n        // Reserve stack space for saving the c_entry_fp later.\n        __ SubWord(sp, sp, Operand(kSystemPointerSize));\n      } else {\n        __ Push(ra, fp);\n        __ Move(fp, sp);\n      }\n    } else if (call_descriptor->IsJSFunctionCall()) {\n      __ Prologue();\n    } else {\n      __ StubPrologue(info()->GetOutputStackFrameType());\n      if (call_descriptor->IsWasmFunctionCall() ||\n          call_descriptor->IsWasmImportWrapper() ||\n          call_descriptor->IsWasmCapiFunction()) {\n        __ Push(kWasmInstanceRegister);\n      }\n      if (call_descriptor->IsWasmImportWrapper()) {\n        // If the wrapper is running on a secondary stack, it will switch to the\n        // central stack and fill these slots with the central stack pointer and\n        // secondary stack limit. Otherwise the slots remain empty.\n#if V8_TARGET_ARCH_RISCV64\n        static_assert(WasmImportWrapperFrameConstants::kCentralStackSPOffset ==\n                      -24);\n        static_assert(\n            WasmImportWrapperFrameConstants::kSecondaryStackLimitOffset == -32);\n#elif V8_TARGET_ARCH_RISCV32\n        static_assert(WasmImportWrapperFrameConstants::kCentralStackSPOffset ==\n                      -12);\n        static_assert(\n            WasmImportWrapperFrameConstants::kSecondaryStackLimitOffset == -16);\n#endif\n        __ push(zero_reg);\n        __ push(zero_reg);\n      } else if (call_descriptor->IsWasmCapiFunction()) {\n        // Reserve space for saving the PC later.\n        __ SubWord(sp, sp, Operand(kSystemPointerSize));\n      }\n    }\n  }\n\n  int required_slots =\n      frame()->GetTotalFrameSlotCount() - frame()->GetFixedSlotCount();\n\n  if (info()->is_osr()) {\n    // TurboFan OSR-compiled functions cannot be entered directly.\n    __ Abort(AbortReason::kShouldNotDirectlyEnterOsrFunction);\n\n    // Unoptimized code jumps directly to this entrypoint while the unoptimized\n    // frame is still on the stack. Optimized code uses OSR values directly from\n    // the unoptimized frame. Thus, all that needs to be done is to allocate the\n    // remaining stack slots.\n    __ RecordComment(\"-- OSR entrypoint --\");\n    osr_pc_offset_ = __ pc_offset();\n    required_slots -= osr_helper()->UnoptimizedFrameSlots();\n  }\n\n  const RegList saves = call_descriptor->CalleeSavedRegisters();\n  const DoubleRegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();\n\n  if (required_slots > 0) {\n    DCHECK(frame_access_state()->has_frame());\n    if (info()->IsWasm() && required_slots > 128) {\n      // For WebAssembly functions with big frames we have to do the stack\n      // overflow check before we construct the frame. Otherwise we may not\n      // have enough space on the stack to call the runtime for the stack\n      // overflow.\n      Label done;\n\n      // If the frame is bigger than the stack, we throw the stack overflow\n      // exception unconditionally. Thereby we can avoid the integer overflow\n      // check in the condition code.\n      if ((required_slots * kSystemPointerSize) <\n          (v8_flags.stack_size * KB)) {\n        UseScratchRegisterScope temps(masm());\n        Register stack_limit = temps.Acquire();\n        __ LoadStackLimit(stack_limit,\n                          MacroAssembler::StackLimitKind::kRealStackLimit);\n        __ AddWord(stack_limit, stack_limit,\n                 Operand(required_slots * kSystemPointerSize));\n        __ Branch(&done, uge, sp, Operand(stack_limit));\n      }\n\n      __ Call(static_cast<intptr_t>(Builtin::kWasmStackOverflow),\n              RelocInfo::WASM_STUB_CALL);\n      // We come from WebAssembly, there are no references for the GC.\n      ReferenceMap* reference_map = zone()->New<ReferenceMap>(zone());\n      RecordSafepoint(reference_map);\n      if (v8_flags.debug_code) {\n        __ stop();\n      }\n\n      __ bind(&done);\n    }\n  }\n\n  const int returns = frame()->GetReturnSlotCount();\n\n  // Skip callee-saved and return slots, which are pushed below.\n  required_slots -= saves.Count();\n  required_slots -= saves_fpu.Count() * (kDoubleSize / kSystemPointerSize);\n  required_slots -= returns;\n  if (required_slots > 0) {\n    __ SubWord(sp, sp, Operand(required_slots * kSystemPointerSize));\n  }\n\n  if (!saves_fpu.is_empty()) {\n    // Save callee-saved FPU registers.\n    __ MultiPushFPU(saves_fpu);\n    DCHECK_EQ(kNumCalleeSavedFPU, saves_fpu.Count());\n  }\n\n  if (!saves.is_empty()) {\n    // Save callee-saved registers.\n    __ MultiPush(saves);\n  }\n\n  if (returns != 0) {\n    // Create space for returns.\n    __ SubWord(sp, sp, Operand(returns * kSystemPointerSize));\n  }\n\n  for (int spill_slot : frame()->tagged_slots()) {\n    FrameOffset offset = frame_access_state()->GetFrameOffset(spill_slot);\n    DCHECK(offset.from_frame_pointer());\n    __ StoreWord(zero_reg, MemOperand(fp, offset.offset()));\n  }\n}", "name_and_para": "void CodeGenerator::AssembleConstructFrame() "}], [{"name": "CodeGenerator::FinishFrame", "content": "void CodeGenerator::FinishFrame(Frame* frame) {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n\n  // Save FP registers.\n  CPURegList saves_fp =\n      CPURegList(kDRegSizeInBits, call_descriptor->CalleeSavedFPRegisters());\n  int saved_count = saves_fp.Count();\n  if (saved_count != 0) {\n    DCHECK(saves_fp.bits() == CPURegList::GetCalleeSavedV().bits());\n    frame->AllocateSavedCalleeRegisterSlots(saved_count *\n                                            (kDoubleSize / kSystemPointerSize));\n  }\n\n  CPURegList saves =\n      CPURegList(kXRegSizeInBits, call_descriptor->CalleeSavedRegisters());\n  saved_count = saves.Count();\n  if (saved_count != 0) {\n    frame->AllocateSavedCalleeRegisterSlots(saved_count);\n  }\n  frame->AlignFrame(16);\n}", "name_and_para": "void CodeGenerator::FinishFrame(Frame* frame) "}, {"name": "CodeGenerator::FinishFrame", "content": "void CodeGenerator::FinishFrame(Frame* frame) {\n  auto call_descriptor = linkage()->GetIncomingDescriptor();\n\n  const DoubleRegList saves_fpu = call_descriptor->CalleeSavedFPRegisters();\n  if (!saves_fpu.is_empty()) {\n    int count = saves_fpu.Count();\n    DCHECK_EQ(kNumCalleeSavedFPU, count);\n    frame->AllocateSavedCalleeRegisterSlots(count *\n                                            (kDoubleSize / kSystemPointerSize));\n  }\n\n  const RegList saves = call_descriptor->CalleeSavedRegisters();\n  if (!saves.is_empty()) {\n    int count = saves.Count();\n    frame->AllocateSavedCalleeRegisterSlots(count);\n  }\n}", "name_and_para": "void CodeGenerator::FinishFrame(Frame* frame) "}], [{"name": "CodeGenerator::AssembleArchTableSwitch", "content": "void CodeGenerator::AssembleArchTableSwitch(Instruction* instr) {\n  Arm64OperandConverter i(this, instr);\n  UseScratchRegisterScope scope(masm());\n  Register input = i.InputRegister32(0);\n  Register temp = scope.AcquireX();\n  size_t const case_count = instr->InputCount() - 2;\n  Label table;\n  __ Cmp(input, case_count);\n  __ B(hs, GetLabel(i.InputRpo(1)));\n  __ Adr(temp, &table);\n  int entry_size_log2 = 2;\n#ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY\n  ++entry_size_log2;  // Account for BTI.\n  constexpr int instructions_per_jump_target = 1;\n#else\n  constexpr int instructions_per_jump_target = 0;\n#endif\n  constexpr int instructions_per_case = 1 + instructions_per_jump_target;\n  __ Add(temp, temp, Operand(input, UXTW, entry_size_log2));\n  __ Br(temp);\n  {\n    const size_t instruction_count =\n        case_count * instructions_per_case + instructions_per_jump_target;\n    MacroAssembler::BlockPoolsScope block_pools(masm(),\n                                                instruction_count * kInstrSize);\n    __ Bind(&table);\n    for (size_t index = 0; index < case_count; ++index) {\n      __ JumpTarget();\n      __ B(GetLabel(i.InputRpo(index + 2)));\n    }\n    __ JumpTarget();\n  }\n}", "name_and_para": "void CodeGenerator::AssembleArchTableSwitch(Instruction* instr) "}, {"name": "CodeGenerator::AssembleArchTableSwitch", "content": "void CodeGenerator::AssembleArchTableSwitch(Instruction* instr) {\n  RiscvOperandConverter i(this, instr);\n  Register input = i.InputRegister(0);\n  size_t const case_count = instr->InputCount() - 2;\n\n  __ Branch(GetLabel(i.InputRpo(1)), Ugreater_equal, input,\n            Operand(case_count));\n  __ GenerateSwitchTable(input, case_count, [&i, this](size_t index) {\n    return GetLabel(i.InputRpo(index + 2));\n  });\n}", "name_and_para": "void CodeGenerator::AssembleArchTableSwitch(Instruction* instr) "}], [{"name": "CodeGenerator::AssembleArchBinarySearchSwitch", "content": "void CodeGenerator::AssembleArchBinarySearchSwitch(Instruction* instr) {\n  Arm64OperandConverter i(this, instr);\n  Register input = i.InputRegister32(0);\n  std::vector<std::pair<int32_t, Label*>> cases;\n  for (size_t index = 2; index < instr->InputCount(); index += 2) {\n    cases.push_back({i.InputInt32(index + 0), GetLabel(i.InputRpo(index + 1))});\n  }\n  AssembleArchBinarySearchSwitchRange(input, i.InputRpo(1), cases.data(),\n                                      cases.data() + cases.size());\n}", "name_and_para": "void CodeGenerator::AssembleArchBinarySearchSwitch(Instruction* instr) "}, {"name": "CodeGenerator::AssembleArchBinarySearchSwitch", "content": "void CodeGenerator::AssembleArchBinarySearchSwitch(Instruction* instr) {\n  RiscvOperandConverter i(this, instr);\n  Register input = i.InputRegister(0);\n  std::vector<std::pair<int32_t, Label*>> cases;\n  for (size_t index = 2; index < instr->InputCount(); index += 2) {\n    cases.push_back({i.InputInt32(index + 0), GetLabel(i.InputRpo(index + 1))});\n  }\n  AssembleArchBinarySearchSwitchRange(input, i.InputRpo(1), cases.data(),\n                                      cases.data() + cases.size());\n}", "name_and_para": "void CodeGenerator::AssembleArchBinarySearchSwitch(Instruction* instr) "}], [{"name": "CodeGenerator::AssembleArchSelect", "content": "void CodeGenerator::AssembleArchSelect(Instruction* instr,\n                                       FlagsCondition condition) {\n  Arm64OperandConverter i(this, instr);\n  // The result register is always the last output of the instruction.\n  size_t output_index = instr->OutputCount() - 1;\n  MachineRepresentation rep =\n      LocationOperand::cast(instr->OutputAt(output_index))->representation();\n  Condition cc = FlagsConditionToCondition(condition);\n  // We don't now how many inputs were consumed by the condition, so we have to\n  // calculate the indices of the last two inputs.\n  DCHECK_GE(instr->InputCount(), 2);\n  size_t true_value_index = instr->InputCount() - 2;\n  size_t false_value_index = instr->InputCount() - 1;\n  if (rep == MachineRepresentation::kFloat32) {\n    __ Fcsel(i.OutputFloat32Register(output_index),\n             i.InputFloat32OrFPZeroRegister(true_value_index),\n             i.InputFloat32OrFPZeroRegister(false_value_index), cc);\n  } else if (rep == MachineRepresentation::kFloat64) {\n    __ Fcsel(i.OutputFloat64Register(output_index),\n             i.InputFloat64OrFPZeroRegister(true_value_index),\n             i.InputFloat64OrFPZeroRegister(false_value_index), cc);\n  } else if (rep == MachineRepresentation::kWord32) {\n    __ Csel(i.OutputRegister32(output_index),\n            i.InputOrZeroRegister32(true_value_index),\n            i.InputOrZeroRegister32(false_value_index), cc);\n  } else {\n    DCHECK_EQ(rep, MachineRepresentation::kWord64);\n    __ Csel(i.OutputRegister64(output_index),\n            i.InputOrZeroRegister64(true_value_index),\n            i.InputOrZeroRegister64(false_value_index), cc);\n  }\n}", "name_and_para": "void CodeGenerator::AssembleArchSelect(Instruction* instr,\n                                       FlagsCondition condition) "}, {"name": "CodeGenerator::AssembleArchSelect", "content": "void CodeGenerator::AssembleArchSelect(Instruction* instr,\n                                       FlagsCondition condition) {\n  UNIMPLEMENTED();\n}", "name_and_para": "void CodeGenerator::AssembleArchSelect(Instruction* instr,\n                                       FlagsCondition condition) "}], [{"name": "CodeGenerator::AssembleArchConditionalBranch", "content": "void CodeGenerator::AssembleArchConditionalBranch(Instruction* instr,\n                                                  BranchInfo* branch) {\n  DCHECK_GE(instr->InputCount(), 6);\n  Arm64OperandConverter i(this, instr);\n  // Input ordering:\n  // > InputCount - 1: false block.\n  // > InputCount - 2: true block.\n  // > InputCount - 3: number of ccmps.\n  // > InputCount - 4: branch condition.\n  size_t num_ccmps_index =\n      instr->InputCount() - kConditionalBranchEndOffsetOfNumCcmps;\n  int64_t num_ccmps = i.ToConstant(instr->InputAt(num_ccmps_index)).ToInt64();\n  size_t ccmp_base_index = instr->InputCount() -\n                           kConditionalBranchEndOffsetOfCondition -\n                           kNumCcmpOperands * num_ccmps;\n  AssembleConditionalCompareChain(instr, num_ccmps, ccmp_base_index, this);\n  Condition cc = FlagsConditionToCondition(branch->condition);\n  __ B(cc, branch->true_label);\n  if (!branch->fallthru) __ B(branch->false_label);\n}", "name_and_para": "void CodeGenerator::AssembleArchConditionalBranch(Instruction* instr,\n                                                  BranchInfo* branch) "}, {"name": "CodeGenerator::AssembleArchConditionalBranch", "content": "void CodeGenerator::AssembleArchConditionalBranch(Instruction* instr,\n                                                  BranchInfo* branch) {\n  UNREACHABLE();\n}", "name_and_para": "void CodeGenerator::AssembleArchConditionalBranch(Instruction* instr,\n                                                  BranchInfo* branch) "}], [{"name": "CodeGenerator::AssembleArchConditionalBoolean", "content": "void CodeGenerator::AssembleArchConditionalBoolean(Instruction* instr) {\n  // Materialize a full 64-bit 1 or 0 value. The result register is always the\n  // last output of the instruction.\n  DCHECK_NE(0u, instr->OutputCount());\n  Arm64OperandConverter i(this, instr);\n  Register reg = i.OutputRegister(instr->OutputCount() - 1);\n  DCHECK_GE(instr->InputCount(), 6);\n\n  // Input ordering:\n  // > InputCount - 1: number of ccmps.\n  // > InputCount - 2: branch condition.\n  size_t num_ccmps_index =\n      instr->InputCount() - kConditionalSetEndOffsetOfNumCcmps;\n  size_t set_condition_index =\n      instr->InputCount() - kConditionalSetEndOffsetOfCondition;\n  int64_t num_ccmps = i.ToConstant(instr->InputAt(num_ccmps_index)).ToInt64();\n  size_t ccmp_base_index = set_condition_index - kNumCcmpOperands * num_ccmps;\n  AssembleConditionalCompareChain(instr, num_ccmps, ccmp_base_index, this);\n\n  FlagsCondition set_condition = static_cast<FlagsCondition>(\n      i.ToConstant(instr->InputAt(set_condition_index)).ToInt64());\n  __ Cset(reg, FlagsConditionToCondition(set_condition));\n}", "name_and_para": "void CodeGenerator::AssembleArchConditionalBoolean(Instruction* instr) "}, {"name": "CodeGenerator::AssembleArchConditionalBoolean", "content": "void CodeGenerator::AssembleArchConditionalBoolean(Instruction* instr) {\n  UNREACHABLE();\n}", "name_and_para": "void CodeGenerator::AssembleArchConditionalBoolean(Instruction* instr) "}], [{"name": "CodeGenerator::AssembleArchTrap", "content": "void CodeGenerator::AssembleArchTrap(Instruction* instr,\n                                     FlagsCondition condition) {\n  auto ool = zone()->New<WasmOutOfLineTrap>(this, instr);\n  Label* tlabel = ool->entry();\n  Condition cc = FlagsConditionToCondition(condition);\n  __ B(cc, tlabel);\n}", "name_and_para": "void CodeGenerator::AssembleArchTrap(Instruction* instr,\n                                     FlagsCondition condition) "}, {"name": "CodeGenerator::AssembleArchTrap", "content": "void CodeGenerator::AssembleArchTrap(Instruction* instr,\n                                     FlagsCondition condition) {\n  class OutOfLineTrap final : public OutOfLineCode {\n   public:\n    OutOfLineTrap(CodeGenerator* gen, Instruction* instr)\n        : OutOfLineCode(gen), instr_(instr), gen_(gen) {}\n    void Generate() override {\n      RiscvOperandConverter i(gen_, instr_);\n      TrapId trap_id =\n          static_cast<TrapId>(i.InputInt32(instr_->InputCount() - 1));\n      GenerateCallToTrap(trap_id);\n    }\n   private:\n    void GenerateCallToTrap(TrapId trap_id) {\n      gen_->AssembleSourcePosition(instr_);\n      // A direct call to a wasm runtime stub defined in this module.\n      // Just encode the stub index. This will be patched when the code\n      // is added to the native module and copied into wasm code space.\n      __ Call(static_cast<Address>(trap_id), RelocInfo::WASM_STUB_CALL);\n      ReferenceMap* reference_map =\n          gen_->zone()->New<ReferenceMap>(gen_->zone());\n      gen_->RecordSafepoint(reference_map);\n      if (v8_flags.debug_code) {\n        __ stop();\n      }\n    }\n    Instruction* instr_;\n    CodeGenerator* gen_;\n  };\n  auto ool = zone()->New<OutOfLineTrap>(this, instr);\n  Label* tlabel = ool->entry();\n  AssembleBranchToLabels(this, masm(), instr, condition, tlabel, nullptr, true);\n}", "name_and_para": "void CodeGenerator::AssembleArchTrap(Instruction* instr,\n                                     FlagsCondition condition) "}], [{"name": "CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder", "content": "void CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder(\n    RpoNumber target) {\n  __ B(GetLabel(target));\n}", "name_and_para": "void CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder(\n    RpoNumber target) "}, {"name": "CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder", "content": "void CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder(\n    RpoNumber target) {\n  __ Branch(GetLabel(target));\n}", "name_and_para": "void CodeGenerator::AssembleArchJumpRegardlessOfAssemblyOrder(\n    RpoNumber target) "}], [{"name": "CodeGenerator::AssembleArchDeoptBranch", "content": "void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,\n                                            BranchInfo* branch) {\n  AssembleArchBranch(instr, branch);\n}", "name_and_para": "void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,\n                                            BranchInfo* branch) "}, {"name": "CodeGenerator::AssembleArchDeoptBranch", "content": "void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,\n                                            BranchInfo* branch) {\n  AssembleArchBranch(instr, branch);\n}", "name_and_para": "void CodeGenerator::AssembleArchDeoptBranch(Instruction* instr,\n                                            BranchInfo* branch) "}], [{"name": "CodeGenerator::AssembleArchBranch", "content": "void CodeGenerator::AssembleArchBranch(Instruction* instr, BranchInfo* branch) {\n  Arm64OperandConverter i(this, instr);\n  Label* tlabel = branch->true_label;\n  Label* flabel = branch->false_label;\n  FlagsCondition condition = branch->condition;\n  ArchOpcode opcode = instr->arch_opcode();\n\n  if (opcode == kArm64CompareAndBranch32) {\n    switch (condition) {\n      case kEqual:\n        __ Cbz(i.InputRegister32(0), tlabel);\n        break;\n      case kNotEqual:\n        __ Cbnz(i.InputRegister32(0), tlabel);\n        break;\n      default:\n        UNREACHABLE();\n    }\n  } else if (opcode == kArm64CompareAndBranch) {\n    switch (condition) {\n      case kEqual:\n        __ Cbz(i.InputRegister64(0), tlabel);\n        break;\n      case kNotEqual:\n        __ Cbnz(i.InputRegister64(0), tlabel);\n        break;\n      default:\n        UNREACHABLE();\n    }\n  } else if (opcode == kArm64TestAndBranch32) {\n    switch (condition) {\n      case kEqual:\n        __ Tbz(i.InputRegister32(0), i.InputInt5(1), tlabel);\n        break;\n      case kNotEqual:\n        __ Tbnz(i.InputRegister32(0), i.InputInt5(1), tlabel);\n        break;\n      default:\n        UNREACHABLE();\n    }\n  } else if (opcode == kArm64TestAndBranch) {\n    switch (condition) {\n      case kEqual:\n        __ Tbz(i.InputRegister64(0), i.InputInt6(1), tlabel);\n        break;\n      case kNotEqual:\n        __ Tbnz(i.InputRegister64(0), i.InputInt6(1), tlabel);\n        break;\n      default:\n        UNREACHABLE();\n    }\n  } else {\n    Condition cc = FlagsConditionToCondition(condition);\n    __ B(cc, tlabel);\n  }\n  if (!branch->fallthru) __ B(flabel);  // no fallthru to flabel.\n}", "name_and_para": "void CodeGenerator::AssembleArchBranch(Instruction* instr, BranchInfo* branch) "}, {"name": "CodeGenerator::AssembleArchBranch", "content": "void CodeGenerator::AssembleArchBranch(Instruction* instr, BranchInfo* branch) {\n  Label* tlabel = branch->true_label;\n  Label* flabel = branch->false_label;\n\n  AssembleBranchToLabels(this, masm(), instr, branch->condition, tlabel, flabel,\n                         branch->fallthru);\n}", "name_and_para": "void CodeGenerator::AssembleArchBranch(Instruction* instr, BranchInfo* branch) "}], [{"name": "CodeGenerator::CodeGenResult", "content": "CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(\n    Instruction* instr) {\n  Arm64OperandConverter i(this, instr);\n  InstructionCode opcode = instr->opcode();\n  ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode);\n  switch (arch_opcode) {\n    case kArchCallCodeObject: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        __ Call(i.InputCode(0), RelocInfo::CODE_TARGET);\n      } else {\n        Register reg = i.InputRegister(0);\n        CodeEntrypointTag tag =\n            i.InputCodeEntrypointTag(instr->CodeEnrypointTagInputIndex());\n        DCHECK_IMPLIES(\n            instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n            reg == kJavaScriptCallCodeStartRegister);\n        __ CallCodeObject(reg, tag);\n      }\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchCallBuiltinPointer: {\n      DCHECK(!instr->InputAt(0)->IsImmediate());\n      Register builtin_index = i.InputRegister(0);\n      Register target =\n          instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister)\n              ? kJavaScriptCallCodeStartRegister\n              : builtin_index;\n      __ CallBuiltinByIndex(builtin_index, target);\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n#if V8_ENABLE_WEBASSEMBLY\n    case kArchCallWasmFunction: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        Constant constant = i.ToConstant(instr->InputAt(0));\n        Address wasm_code = static_cast<Address>(constant.ToInt64());\n        __ Call(wasm_code, constant.rmode());\n      } else {\n        Register target = i.InputRegister(0);\n        __ Call(target);\n      }\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchTailCallWasm: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        Constant constant = i.ToConstant(instr->InputAt(0));\n        Address wasm_code = static_cast<Address>(constant.ToInt64());\n        __ Jump(wasm_code, constant.rmode());\n      } else {\n        Register target = i.InputRegister(0);\n        UseScratchRegisterScope temps(masm());\n        temps.Exclude(x17);\n        __ Mov(x17, target);\n        __ Jump(x17);\n      }\n      unwinding_info_writer_.MarkBlockWillExit();\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n#endif  // V8_ENABLE_WEBASSEMBLY\n    case kArchTailCallCodeObject: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        __ Jump(i.InputCode(0), RelocInfo::CODE_TARGET);\n      } else {\n        Register reg = i.InputRegister(0);\n        CodeEntrypointTag tag =\n            i.InputCodeEntrypointTag(instr->CodeEnrypointTagInputIndex());\n        DCHECK_IMPLIES(\n            instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n            reg == kJavaScriptCallCodeStartRegister);\n        __ JumpCodeObject(reg, tag);\n      }\n      unwinding_info_writer_.MarkBlockWillExit();\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n    case kArchTailCallAddress: {\n      CHECK(!instr->InputAt(0)->IsImmediate());\n      Register reg = i.InputRegister(0);\n      DCHECK_IMPLIES(\n          instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n          reg == kJavaScriptCallCodeStartRegister);\n      UseScratchRegisterScope temps(masm());\n      temps.Exclude(x17);\n      __ Mov(x17, reg);\n      __ Jump(x17);\n      unwinding_info_writer_.MarkBlockWillExit();\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n    case kArchCallJSFunction: {\n      Register func = i.InputRegister(0);\n      if (v8_flags.debug_code) {\n        // Check the function's context matches the context argument.\n        UseScratchRegisterScope scope(masm());\n        Register temp = scope.AcquireX();\n        __ LoadTaggedField(temp,\n                           FieldMemOperand(func, JSFunction::kContextOffset));\n        __ cmp(cp, temp);\n        __ Assert(eq, AbortReason::kWrongFunctionContext);\n      }\n      __ CallJSFunction(func);\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchPrepareCallCFunction:\n      // We don't need kArchPrepareCallCFunction on arm64 as the instruction\n      // selector has already performed a Claim to reserve space on the stack.\n      // Frame alignment is always 16 bytes, and the stack pointer is already\n      // 16-byte aligned, therefore we do not need to align the stack pointer\n      // by an unknown value, and it is safe to continue accessing the frame\n      // via the stack pointer.\n      UNREACHABLE();\n    case kArchSaveCallerRegisters: {\n      fp_mode_ =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      DCHECK(fp_mode_ == SaveFPRegsMode::kIgnore ||\n             fp_mode_ == SaveFPRegsMode::kSave);\n      // kReturnRegister0 should have been saved before entering the stub.\n      int bytes = __ PushCallerSaved(fp_mode_, kReturnRegister0);\n      DCHECK(IsAligned(bytes, kSystemPointerSize));\n      DCHECK_EQ(0, frame_access_state()->sp_delta());\n      frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);\n      DCHECK(!caller_registers_saved_);\n      caller_registers_saved_ = true;\n      break;\n    }\n    case kArchRestoreCallerRegisters: {\n      DCHECK(fp_mode_ ==\n             static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode())));\n      DCHECK(fp_mode_ == SaveFPRegsMode::kIgnore ||\n             fp_mode_ == SaveFPRegsMode::kSave);\n      // Don't overwrite the returned value.\n      int bytes = __ PopCallerSaved(fp_mode_, kReturnRegister0);\n      frame_access_state()->IncreaseSPDelta(-(bytes / kSystemPointerSize));\n      DCHECK_EQ(0, frame_access_state()->sp_delta());\n      DCHECK(caller_registers_saved_);\n      caller_registers_saved_ = false;\n      break;\n    }\n    case kArchPrepareTailCall:\n      AssemblePrepareTailCall();\n      break;\n    case kArchCallCFunctionWithFrameState:\n    case kArchCallCFunction: {\n      int const num_gp_parameters = ParamField::decode(instr->opcode());\n      int const num_fp_parameters = FPParamField::decode(instr->opcode());\n      Label return_location;\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes;\n#if V8_ENABLE_WEBASSEMBLY\n      if (linkage()->GetIncomingDescriptor()->IsWasmCapiFunction()) {\n        // Put the return address in a stack slot.\n        __ StoreReturnAddressInWasmExitFrame(&return_location);\n        set_isolate_data_slots = SetIsolateDataSlots::kNo;\n      }\n#endif  // V8_ENABLE_WEBASSEMBLY\n      int pc_offset;\n      if (instr->InputAt(0)->IsImmediate()) {\n        ExternalReference ref = i.InputExternalReference(0);\n        pc_offset = __ CallCFunction(ref, num_gp_parameters, num_fp_parameters,\n                                     set_isolate_data_slots, &return_location);\n      } else {\n        Register func = i.InputRegister(0);\n        pc_offset = __ CallCFunction(func, num_gp_parameters, num_fp_parameters,\n                                     set_isolate_data_slots, &return_location);\n      }\n      RecordSafepoint(instr->reference_map(), pc_offset);\n\n      bool const needs_frame_state =\n          (arch_opcode == kArchCallCFunctionWithFrameState);\n      if (needs_frame_state) {\n        RecordDeoptInfo(instr, pc_offset);\n      }\n\n      frame_access_state()->SetFrameAccessToDefault();\n      // Ideally, we should decrement SP delta to match the change of stack\n      // pointer in CallCFunction. However, for certain architectures (e.g.\n      // ARM), there may be more strict alignment requirement, causing old SP\n      // to be saved on the stack. In those cases, we can not calculate the SP\n      // delta statically.\n      frame_access_state()->ClearSPDelta();\n      if (caller_registers_saved_) {\n        // Need to re-sync SP delta introduced in kArchSaveCallerRegisters.\n        // Here, we assume the sequence to be:\n        //   kArchSaveCallerRegisters;\n        //   kArchCallCFunction;\n        //   kArchRestoreCallerRegisters;\n        int bytes =\n            __ RequiredStackSizeForCallerSaved(fp_mode_, kReturnRegister0);\n        frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);\n      }\n      break;\n    }\n    case kArchJmp:\n      AssembleArchJump(i.InputRpo(0));\n      break;\n    case kArchTableSwitch:\n      AssembleArchTableSwitch(instr);\n      break;\n    case kArchBinarySearchSwitch:\n      AssembleArchBinarySearchSwitch(instr);\n      break;\n    case kArchAbortCSADcheck:\n      DCHECK_EQ(i.InputRegister(0), x1);\n      {\n        // We don't actually want to generate a pile of code for this, so just\n        // claim there is a stack frame, without generating one.\n        FrameScope scope(masm(), StackFrame::NO_FRAME_TYPE);\n        __ CallBuiltin(Builtin::kAbortCSADcheck);\n      }\n      __ Debug(\"kArchAbortCSADcheck\", 0, BREAK);\n      unwinding_info_writer_.MarkBlockWillExit();\n      break;\n    case kArchDebugBreak:\n      __ DebugBreak();\n      break;\n    case kArchComment:\n      __ RecordComment(reinterpret_cast<const char*>(i.InputInt64(0)));\n      break;\n    case kArchThrowTerminator:\n      unwinding_info_writer_.MarkBlockWillExit();\n      break;\n    case kArchNop:\n      // don't emit code for nops.\n      break;\n    case kArchDeoptimize: {\n      DeoptimizationExit* exit =\n          BuildTranslation(instr, -1, 0, 0, OutputFrameStateCombine::Ignore());\n      __ B(exit->label());\n      break;\n    }\n    case kArchRet:\n      AssembleReturn(instr->InputAt(0));\n      break;\n    case kArchFramePointer:\n      __ mov(i.OutputRegister(), fp);\n      break;\n    case kArchParentFramePointer:\n      if (frame_access_state()->has_frame()) {\n        __ ldr(i.OutputRegister(), MemOperand(fp, 0));\n      } else {\n        __ mov(i.OutputRegister(), fp);\n      }\n      break;\n#if V8_ENABLE_WEBASSEMBLY\n    case kArchStackPointer:\n      // The register allocator expects an allocatable register for the output,\n      // we cannot use sp directly.\n      __ mov(i.OutputRegister(), sp);\n      break;\n    case kArchSetStackPointer: {\n      DCHECK(instr->InputAt(0)->IsRegister());\n      if (masm()->options().enable_simulator_code) {\n        __ RecordComment(\"-- Set simulator stack limit --\");\n        DCHECK(__ TmpList()->IncludesAliasOf(kSimulatorHltArgument));\n        __ LoadStackLimit(kSimulatorHltArgument,\n                          StackLimitKind::kRealStackLimit);\n        __ hlt(kImmExceptionIsSwitchStackLimit);\n      }\n      __ Mov(sp, i.InputRegister(0));\n      break;\n    }\n#endif  // V8_ENABLE_WEBASSEMBLY\n    case kArchStackPointerGreaterThan: {\n      // Potentially apply an offset to the current stack pointer before the\n      // comparison to consider the size difference of an optimized frame versus\n      // the contained unoptimized frames.\n\n      Register lhs_register = sp;\n      uint32_t offset;\n\n      if (ShouldApplyOffsetToStackCheck(instr, &offset)) {\n        lhs_register = i.TempRegister(0);\n        __ Sub(lhs_register, sp, offset);\n      }\n\n      constexpr size_t kValueIndex = 0;\n      DCHECK(instr->InputAt(kValueIndex)->IsRegister());\n      __ Cmp(lhs_register, i.InputRegister(kValueIndex));\n      break;\n    }\n    case kArchStackCheckOffset:\n      __ Move(i.OutputRegister(), Smi::FromInt(GetStackCheckOffset()));\n      break;\n    case kArchTruncateDoubleToI:\n      __ TruncateDoubleToI(isolate(), zone(), i.OutputRegister(),\n                           i.InputDoubleRegister(0), DetermineStubCallMode(),\n                           frame_access_state()->has_frame()\n                               ? kLRHasBeenSaved\n                               : kLRHasNotBeenSaved);\n\n      break;\n    case kArchStoreWithWriteBarrier: {\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      // Indirect pointer writes must use a different opcode.\n      DCHECK_NE(mode, RecordWriteMode::kValueIsIndirectPointer);\n      AddressingMode addressing_mode =\n          AddressingModeField::decode(instr->opcode());\n      Register object = i.InputRegister(0);\n      Operand offset(0);\n      if (addressing_mode == kMode_MRI) {\n        offset = Operand(i.InputInt64(1));\n      } else {\n        DCHECK_EQ(addressing_mode, kMode_MRR);\n        offset = Operand(i.InputRegister(1));\n      }\n      Register value = i.InputRegister(2);\n\n      if (v8_flags.debug_code) {\n        // Checking that |value| is not a cleared weakref: our write barrier\n        // does not support that for now.\n        __ cmp(value, Operand(kClearedWeakHeapObjectLower32));\n        __ Check(ne, AbortReason::kOperandIsCleared);\n      }\n\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, offset, value, mode, DetermineStubCallMode(),\n          &unwinding_info_writer_);\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ StoreTaggedField(value, MemOperand(object, offset));\n      if (mode > RecordWriteMode::kValueIsIndirectPointer) {\n        __ JumpIfSmi(value, ool->exit());\n      }\n      __ CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                       ne, ool->entry());\n      __ Bind(ool->exit());\n      break;\n    }\n    case kArchAtomicStoreWithWriteBarrier: {\n      DCHECK_EQ(AddressingModeField::decode(instr->opcode()), kMode_MRR);\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      // Indirect pointer writes must use a different opcode.\n      DCHECK_NE(mode, RecordWriteMode::kValueIsIndirectPointer);\n      Register object = i.InputRegister(0);\n      Register offset = i.InputRegister(1);\n      Register value = i.InputRegister(2);\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, offset, value, mode, DetermineStubCallMode(),\n          &unwinding_info_writer_);\n      __ AtomicStoreTaggedField(value, object, offset, i.TempRegister(0));\n      // Skip the write barrier if the value is a Smi. However, this is only\n      // valid if the value isn't an indirect pointer. Otherwise the value will\n      // be a pointer table index, which will always look like a Smi (but\n      // actually reference a pointer in the pointer table).\n      if (mode > RecordWriteMode::kValueIsIndirectPointer) {\n        __ JumpIfSmi(value, ool->exit());\n      }\n      __ CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                       ne, ool->entry());\n      __ Bind(ool->exit());\n      break;\n    }\n    case kArchStoreIndirectWithWriteBarrier: {\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      DCHECK_EQ(mode, RecordWriteMode::kValueIsIndirectPointer);\n      AddressingMode addressing_mode =\n          AddressingModeField::decode(instr->opcode());\n      Register object = i.InputRegister(0);\n      Operand offset(0);\n      if (addressing_mode == kMode_MRI) {\n        offset = Operand(i.InputInt64(1));\n      } else {\n        DCHECK_EQ(addressing_mode, kMode_MRR);\n        offset = Operand(i.InputRegister(1));\n      }\n      Register value = i.InputRegister(2);\n      IndirectPointerTag tag = static_cast<IndirectPointerTag>(i.InputInt64(3));\n      DCHECK(IsValidIndirectPointerTag(tag));\n\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, offset, value, mode, DetermineStubCallMode(),\n          &unwinding_info_writer_, tag);\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ StoreIndirectPointerField(value, MemOperand(object, offset));\n      __ JumpIfMarking(ool->entry());\n      __ Bind(ool->exit());\n      break;\n    }\n    case kArchStackSlot: {\n      FrameOffset offset =\n          frame_access_state()->GetFrameOffset(i.InputInt32(0));\n      Register base = offset.from_stack_pointer() ? sp : fp;\n      __ Add(i.OutputRegister(0), base, Operand(offset.offset()));\n      break;\n    }\n    case kIeee754Float64Acos:\n      ASSEMBLE_IEEE754_UNOP(acos);\n      break;\n    case kIeee754Float64Acosh:\n      ASSEMBLE_IEEE754_UNOP(acosh);\n      break;\n    case kIeee754Float64Asin:\n      ASSEMBLE_IEEE754_UNOP(asin);\n      break;\n    case kIeee754Float64Asinh:\n      ASSEMBLE_IEEE754_UNOP(asinh);\n      break;\n    case kIeee754Float64Atan:\n      ASSEMBLE_IEEE754_UNOP(atan);\n      break;\n    case kIeee754Float64Atanh:\n      ASSEMBLE_IEEE754_UNOP(atanh);\n      break;\n    case kIeee754Float64Atan2:\n      ASSEMBLE_IEEE754_BINOP(atan2);\n      break;\n    case kIeee754Float64Cos:\n      ASSEMBLE_IEEE754_UNOP(cos);\n      break;\n    case kIeee754Float64Cosh:\n      ASSEMBLE_IEEE754_UNOP(cosh);\n      break;\n    case kIeee754Float64Cbrt:\n      ASSEMBLE_IEEE754_UNOP(cbrt);\n      break;\n    case kIeee754Float64Exp:\n      ASSEMBLE_IEEE754_UNOP(exp);\n      break;\n    case kIeee754Float64Expm1:\n      ASSEMBLE_IEEE754_UNOP(expm1);\n      break;\n    case kIeee754Float64Log:\n      ASSEMBLE_IEEE754_UNOP(log);\n      break;\n    case kIeee754Float64Log1p:\n      ASSEMBLE_IEEE754_UNOP(log1p);\n      break;\n    case kIeee754Float64Log2:\n      ASSEMBLE_IEEE754_UNOP(log2);\n      break;\n    case kIeee754Float64Log10:\n      ASSEMBLE_IEEE754_UNOP(log10);\n      break;\n    case kIeee754Float64Pow:\n      ASSEMBLE_IEEE754_BINOP(pow);\n      break;\n    case kIeee754Float64Sin:\n      ASSEMBLE_IEEE754_UNOP(sin);\n      break;\n    case kIeee754Float64Sinh:\n      ASSEMBLE_IEEE754_UNOP(sinh);\n      break;\n    case kIeee754Float64Tan:\n      ASSEMBLE_IEEE754_UNOP(tan);\n      break;\n    case kIeee754Float64Tanh:\n      ASSEMBLE_IEEE754_UNOP(tanh);\n      break;\n    case kArm64Float32RoundDown:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintm, instr, i, kFormatS,\n                       kFormat4S);\n      break;\n    case kArm64Float64RoundDown:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintm, instr, i, kFormatD,\n                       kFormat2D);\n      break;\n    case kArm64Float32RoundUp:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintp, instr, i, kFormatS,\n                       kFormat4S);\n      break;\n    case kArm64Float64RoundUp:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintp, instr, i, kFormatD,\n                       kFormat2D);\n      break;\n    case kArm64Float64RoundTiesAway:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frinta, instr, i, kFormatD,\n                       kFormat2D);\n      break;\n    case kArm64Float32RoundTruncate:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintz, instr, i, kFormatS,\n                       kFormat4S);\n      break;\n    case kArm64Float64RoundTruncate:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintz, instr, i, kFormatD,\n                       kFormat2D);\n      break;\n    case kArm64Float32RoundTiesEven:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintn, instr, i, kFormatS,\n                       kFormat4S);\n      break;\n    case kArm64Float64RoundTiesEven:\n      EmitFpOrNeonUnop(masm(), &MacroAssembler::Frintn, instr, i, kFormatD,\n                       kFormat2D);\n      break;\n    case kArm64Add:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        __ Adds(i.OutputRegister(), i.InputOrZeroRegister64(0),\n                i.InputOperand2_64(1));\n      } else {\n        __ Add(i.OutputRegister(), i.InputOrZeroRegister64(0),\n               i.InputOperand2_64(1));\n      }\n      break;\n    case kArm64Add32:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        __ Adds(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n                i.InputOperand2_32(1));\n      } else {\n        __ Add(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n               i.InputOperand2_32(1));\n      }\n      break;\n    case kArm64And:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        // The ands instruction only sets N and Z, so only the following\n        // conditions make sense.\n        DCHECK(FlagsConditionField::decode(opcode) == kEqual ||\n               FlagsConditionField::decode(opcode) == kNotEqual ||\n               FlagsConditionField::decode(opcode) == kPositiveOrZero ||\n               FlagsConditionField::decode(opcode) == kNegative);\n        __ Ands(i.OutputRegister(), i.InputOrZeroRegister64(0),\n                i.InputOperand2_64(1));\n      } else {\n        __ And(i.OutputRegister(), i.InputOrZeroRegister64(0),\n               i.InputOperand2_64(1));\n      }\n      break;\n    case kArm64And32:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        // The ands instruction only sets N and Z, so only the following\n        // conditions make sense.\n        DCHECK(FlagsConditionField::decode(opcode) == kEqual ||\n               FlagsConditionField::decode(opcode) == kNotEqual ||\n               FlagsConditionField::decode(opcode) == kPositiveOrZero ||\n               FlagsConditionField::decode(opcode) == kNegative);\n        __ Ands(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n                i.InputOperand2_32(1));\n      } else {\n        __ And(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n               i.InputOperand2_32(1));\n      }\n      break;\n    case kArm64Bic:\n      __ Bic(i.OutputRegister(), i.InputOrZeroRegister64(0),\n             i.InputOperand2_64(1));\n      break;\n    case kArm64Bic32:\n      __ Bic(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n             i.InputOperand2_32(1));\n      break;\n    case kArm64Mul:\n      __ Mul(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Smulh:\n      __ Smulh(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Umulh:\n      __ Umulh(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Mul32:\n      __ Mul(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1));\n      break;\n#if V8_ENABLE_WEBASSEMBLY\n    case kArm64Sadalp: {\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Sadalp(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(1).Format(src_f));\n      break;\n    }\n    case kArm64Saddlp: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Saddlp(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(0).Format(src_f));\n      break;\n    }\n    case kArm64Uadalp: {\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Uadalp(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(1).Format(src_f));\n      break;\n    }\n    case kArm64Uaddlp: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Uaddlp(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(0).Format(src_f));\n      break;\n    }\n    case kArm64ISplat: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      Register src = LaneSizeField::decode(opcode) == 64 ? i.InputRegister64(0)\n                                                         : i.InputRegister32(0);\n      __ Dup(i.OutputSimd128Register().Format(f), src);\n      break;\n    }\n    case kArm64FSplat: {\n      VectorFormat src_f =\n          ScalarFormatFromLaneSize(LaneSizeField::decode(opcode));\n      VectorFormat dst_f = VectorFormatFillQ(src_f);\n      __ Dup(i.OutputSimd128Register().Format(dst_f),\n             i.InputSimd128Register(0).Format(src_f), 0);\n      break;\n    }\n    case kArm64Smlal: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidth(dst_f);\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ Smlal(i.OutputSimd128Register().Format(dst_f),\n               i.InputSimd128Register(1).Format(src_f),\n               i.InputSimd128Register(2).Format(src_f));\n      break;\n    }\n    case kArm64Smlal2: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ Smlal2(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(1).Format(src_f),\n                i.InputSimd128Register(2).Format(src_f));\n      break;\n    }\n    case kArm64Umlal: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidth(dst_f);\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ Umlal(i.OutputSimd128Register().Format(dst_f),\n               i.InputSimd128Register(1).Format(src_f),\n               i.InputSimd128Register(2).Format(src_f));\n      break;\n    }\n    case kArm64Umlal2: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ Umlal2(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(1).Format(src_f),\n                i.InputSimd128Register(2).Format(src_f));\n      break;\n    }\n#endif  // V8_ENABLE_WEBASSEMBLY\n    case kArm64Smull: {\n      if (instr->InputAt(0)->IsRegister()) {\n        __ Smull(i.OutputRegister(), i.InputRegister32(0),\n                 i.InputRegister32(1));\n      } else {\n        DCHECK(instr->InputAt(0)->IsSimd128Register());\n        VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n        VectorFormat src_f = VectorFormatHalfWidth(dst_f);\n        __ Smull(i.OutputSimd128Register().Format(dst_f),\n                 i.InputSimd128Register(0).Format(src_f),\n                 i.InputSimd128Register(1).Format(src_f));\n      }\n      break;\n    }\n    case kArm64Smull2: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Smull2(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(0).Format(src_f),\n                i.InputSimd128Register(1).Format(src_f));\n      break;\n    }\n    case kArm64Umull: {\n      if (instr->InputAt(0)->IsRegister()) {\n        __ Umull(i.OutputRegister(), i.InputRegister32(0),\n                 i.InputRegister32(1));\n      } else {\n        DCHECK(instr->InputAt(0)->IsSimd128Register());\n        VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n        VectorFormat src_f = VectorFormatHalfWidth(dst_f);\n        __ Umull(i.OutputSimd128Register().Format(dst_f),\n                 i.InputSimd128Register(0).Format(src_f),\n                 i.InputSimd128Register(1).Format(src_f));\n      }\n      break;\n    }\n    case kArm64Umull2: {\n      VectorFormat dst_f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatHalfWidthDoubleLanes(dst_f);\n      __ Umull2(i.OutputSimd128Register().Format(dst_f),\n                i.InputSimd128Register(0).Format(src_f),\n                i.InputSimd128Register(1).Format(src_f));\n      break;\n    }\n    case kArm64Madd:\n      __ Madd(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1),\n              i.InputRegister(2));\n      break;\n    case kArm64Madd32:\n      __ Madd(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1),\n              i.InputRegister32(2));\n      break;\n    case kArm64Msub:\n      __ Msub(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1),\n              i.InputRegister(2));\n      break;\n    case kArm64Msub32:\n      __ Msub(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1),\n              i.InputRegister32(2));\n      break;\n    case kArm64Mneg:\n      __ Mneg(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Mneg32:\n      __ Mneg(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1));\n      break;\n    case kArm64Idiv:\n      __ Sdiv(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Idiv32:\n      __ Sdiv(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1));\n      break;\n    case kArm64Udiv:\n      __ Udiv(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kArm64Udiv32:\n      __ Udiv(i.OutputRegister32(), i.InputRegister32(0), i.InputRegister32(1));\n      break;\n    case kArm64Imod: {\n      UseScratchRegisterScope scope(masm());\n      Register temp = scope.AcquireX();\n      __ Sdiv(temp, i.InputRegister(0), i.InputRegister(1));\n      __ Msub(i.OutputRegister(), temp, i.InputRegister(1), i.InputRegister(0));\n      break;\n    }\n    case kArm64Imod32: {\n      UseScratchRegisterScope scope(masm());\n      Register temp = scope.AcquireW();\n      __ Sdiv(temp, i.InputRegister32(0), i.InputRegister32(1));\n      __ Msub(i.OutputRegister32(), temp, i.InputRegister32(1),\n              i.InputRegister32(0));\n      break;\n    }\n    case kArm64Umod: {\n      UseScratchRegisterScope scope(masm());\n      Register temp = scope.AcquireX();\n      __ Udiv(temp, i.InputRegister(0), i.InputRegister(1));\n      __ Msub(i.OutputRegister(), temp, i.InputRegister(1), i.InputRegister(0));\n      break;\n    }\n    case kArm64Umod32: {\n      UseScratchRegisterScope scope(masm());\n      Register temp = scope.AcquireW();\n      __ Udiv(temp, i.InputRegister32(0), i.InputRegister32(1));\n      __ Msub(i.OutputRegister32(), temp, i.InputRegister32(1),\n              i.InputRegister32(0));\n      break;\n    }\n    case kArm64Not:\n      __ Mvn(i.OutputRegister(), i.InputOperand(0));\n      break;\n    case kArm64Not32:\n      __ Mvn(i.OutputRegister32(), i.InputOperand32(0));\n      break;\n    case kArm64Or:\n      __ Orr(i.OutputRegister(), i.InputOrZeroRegister64(0),\n             i.InputOperand2_64(1));\n      break;\n    case kArm64Or32:\n      __ Orr(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n             i.InputOperand2_32(1));\n      break;\n    case kArm64Orn:\n      __ Orn(i.OutputRegister(), i.InputOrZeroRegister64(0),\n             i.InputOperand2_64(1));\n      break;\n    case kArm64Orn32:\n      __ Orn(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n             i.InputOperand2_32(1));\n      break;\n    case kArm64Eor:\n      __ Eor(i.OutputRegister(), i.InputOrZeroRegister64(0),\n             i.InputOperand2_64(1));\n      break;\n    case kArm64Eor32:\n      __ Eor(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n             i.InputOperand2_32(1));\n      break;\n    case kArm64Eon:\n      __ Eon(i.OutputRegister(), i.InputOrZeroRegister64(0),\n             i.InputOperand2_64(1));\n      break;\n    case kArm64Eon32:\n      __ Eon(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n             i.InputOperand2_32(1));\n      break;\n    case kArm64Sub:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        __ Subs(i.OutputRegister(), i.InputOrZeroRegister64(0),\n                i.InputOperand2_64(1));\n      } else {\n        __ Sub(i.OutputRegister(), i.InputOrZeroRegister64(0),\n               i.InputOperand2_64(1));\n      }\n      break;\n    case kArm64Sub32:\n      if (FlagsModeField::decode(opcode) != kFlags_none) {\n        __ Subs(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n                i.InputOperand2_32(1));\n      } else {\n        __ Sub(i.OutputRegister32(), i.InputOrZeroRegister32(0),\n               i.InputOperand2_32(1));\n      }\n      break;\n    case kArm64Lsl:\n      ASSEMBLE_SHIFT(Lsl, 64);\n      break;\n    case kArm64Lsl32:\n      ASSEMBLE_SHIFT(Lsl, 32);\n      break;\n    case kArm64Lsr:\n      ASSEMBLE_SHIFT(Lsr, 64);\n      break;\n    case kArm64Lsr32:\n      ASSEMBLE_SHIFT(Lsr, 32);\n      break;\n    case kArm64Asr:\n      ASSEMBLE_SHIFT(Asr, 64);\n      break;\n    case kArm64Asr32:\n      ASSEMBLE_SHIFT(Asr, 32);\n      break;\n    case kArm64Ror:\n      ASSEMBLE_SHIFT(Ror, 64);\n      break;\n    case kArm64Ror32:\n      ASSEMBLE_SHIFT(Ror, 32);\n      break;\n    case kArm64Mov32:\n      __ Mov(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Sxtb32:\n      __ Sxtb(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Sxth32:\n      __ Sxth(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Sxtb:\n      __ Sxtb(i.OutputRegister(), i.InputRegister32(0));\n      break;\n    case kArm64Sxth:\n      __ Sxth(i.OutputRegister(), i.InputRegister32(0));\n      break;\n    case kArm64Sxtw:\n      __ Sxtw(i.OutputRegister(), i.InputRegister32(0));\n      break;\n    case kArm64Sbfx:\n      __ Sbfx(i.OutputRegister(), i.InputRegister(0), i.InputInt6(1),\n              i.InputInt6(2));\n      break;\n    case kArm64Sbfx32:\n      __ Sbfx(i.OutputRegister32(), i.InputRegister32(0), i.InputInt5(1),\n              i.InputInt5(2));\n      break;\n    case kArm64Ubfx:\n      __ Ubfx(i.OutputRegister(), i.InputRegister(0), i.InputInt6(1),\n              i.InputInt32(2));\n      break;\n    case kArm64Ubfx32:\n      __ Ubfx(i.OutputRegister32(), i.InputRegister32(0), i.InputInt5(1),\n              i.InputInt32(2));\n      break;\n    case kArm64Ubfiz32:\n      __ Ubfiz(i.OutputRegister32(), i.InputRegister32(0), i.InputInt5(1),\n               i.InputInt5(2));\n      break;\n    case kArm64Sbfiz:\n      __ Sbfiz(i.OutputRegister(), i.InputRegister(0), i.InputInt6(1),\n               i.InputInt6(2));\n      break;\n    case kArm64Bfi:\n      __ Bfi(i.OutputRegister(), i.InputRegister(1), i.InputInt6(2),\n             i.InputInt6(3));\n      break;\n    case kArm64TestAndBranch32:\n    case kArm64TestAndBranch:\n      // Pseudo instructions turned into tbz/tbnz in AssembleArchBranch.\n      break;\n    case kArm64CompareAndBranch32:\n    case kArm64CompareAndBranch:\n      // Pseudo instruction handled in AssembleArchBranch.\n      break;\n    case kArm64Claim: {\n      int count = i.InputInt32(0);\n      DCHECK_EQ(count % 2, 0);\n      __ AssertSpAligned();\n      if (count > 0) {\n        __ Claim(count);\n        frame_access_state()->IncreaseSPDelta(count);\n      }\n      break;\n    }\n    case kArm64Poke: {\n      Operand operand(i.InputInt32(1) * kSystemPointerSize);\n      if (instr->InputAt(0)->IsSimd128Register()) {\n        __ Poke(i.InputSimd128Register(0), operand);\n      } else if (instr->InputAt(0)->IsFPRegister()) {\n        __ Poke(i.InputFloat64Register(0), operand);\n      } else {\n        __ Poke(i.InputOrZeroRegister64(0), operand);\n      }\n      break;\n    }\n    case kArm64PokePair: {\n      int slot = i.InputInt32(2) - 1;\n      if (instr->InputAt(0)->IsFPRegister()) {\n        __ PokePair(i.InputFloat64Register(1), i.InputFloat64Register(0),\n                    slot * kSystemPointerSize);\n      } else {\n        __ PokePair(i.InputRegister(1), i.InputRegister(0),\n                    slot * kSystemPointerSize);\n      }\n      break;\n    }\n    case kArm64Peek: {\n      int reverse_slot = i.InputInt32(0);\n      int offset =\n          FrameSlotToFPOffset(frame()->GetTotalFrameSlotCount() - reverse_slot);\n      if (instr->OutputAt(0)->IsFPRegister()) {\n        LocationOperand* op = LocationOperand::cast(instr->OutputAt(0));\n        if (op->representation() == MachineRepresentation::kFloat64) {\n          __ Ldr(i.OutputDoubleRegister(), MemOperand(fp, offset));\n        } else if (op->representation() == MachineRepresentation::kFloat32) {\n          __ Ldr(i.OutputFloatRegister(), MemOperand(fp, offset));\n        } else {\n          DCHECK_EQ(MachineRepresentation::kSimd128, op->representation());\n          __ Ldr(i.OutputSimd128Register(), MemOperand(fp, offset));\n        }\n      } else {\n        __ Ldr(i.OutputRegister(), MemOperand(fp, offset));\n      }\n      break;\n    }\n    case kArm64Clz:\n      __ Clz(i.OutputRegister64(), i.InputRegister64(0));\n      break;\n    case kArm64Clz32:\n      __ Clz(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Rbit:\n      __ Rbit(i.OutputRegister64(), i.InputRegister64(0));\n      break;\n    case kArm64Rbit32:\n      __ Rbit(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Rev:\n      __ Rev(i.OutputRegister64(), i.InputRegister64(0));\n      break;\n    case kArm64Rev32:\n      __ Rev(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    case kArm64Cmp:\n      __ Cmp(i.InputOrZeroRegister64(0), i.InputOperand2_64(1));\n      break;\n    case kArm64Cmp32:\n      __ Cmp(i.InputOrZeroRegister32(0), i.InputOperand2_32(1));\n      break;\n    case kArm64Cmn:\n      __ Cmn(i.InputOrZeroRegister64(0), i.InputOperand2_64(1));\n      break;\n    case kArm64Cmn32:\n      __ Cmn(i.InputOrZeroRegister32(0), i.InputOperand2_32(1));\n      break;\n    case kArm64Cnt32: {\n      __ PopcntHelper(i.OutputRegister32(), i.InputRegister32(0));\n      break;\n    }\n    case kArm64Cnt64: {\n      __ PopcntHelper(i.OutputRegister64(), i.InputRegister64(0));\n      break;\n    }\n    case kArm64Cnt: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      __ Cnt(i.OutputSimd128Register().Format(f),\n             i.InputSimd128Register(0).Format(f));\n      break;\n    }\n    case kArm64Tst:\n      __ Tst(i.InputOrZeroRegister64(0), i.InputOperand2_64(1));\n      break;\n    case kArm64Tst32:\n      __ Tst(i.InputOrZeroRegister32(0), i.InputOperand2_32(1));\n      break;\n    case kArm64Float32Cmp:\n      if (instr->InputAt(1)->IsFPRegister()) {\n        __ Fcmp(i.InputFloat32Register(0), i.InputFloat32Register(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        // 0.0 is the only immediate supported by fcmp instructions.\n        DCHECK_EQ(0.0f, i.InputFloat32(1));\n        __ Fcmp(i.InputFloat32Register(0), i.InputFloat32(1));\n      }\n      break;\n    case kArm64Float32Add:\n      __ Fadd(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    case kArm64Float32Sub:\n      __ Fsub(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    case kArm64Float32Mul:\n      __ Fmul(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    case kArm64Float32Div:\n      __ Fdiv(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    case kArm64Float32Abs:\n      __ Fabs(i.OutputFloat32Register(), i.InputFloat32Register(0));\n      break;\n    case kArm64Float32Abd:\n      __ Fabd(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    case kArm64Float32Neg:\n      __ Fneg(i.OutputFloat32Register(), i.InputFloat32Register(0));\n      break;\n    case kArm64Float32Sqrt:\n      __ Fsqrt(i.OutputFloat32Register(), i.InputFloat32Register(0));\n      break;\n    case kArm64Float32Fnmul: {\n      __ Fnmul(i.OutputFloat32Register(), i.InputFloat32Register(0),\n               i.InputFloat32Register(1));\n      break;\n    }\n    case kArm64Float64Cmp:\n      if (instr->InputAt(1)->IsFPRegister()) {\n        __ Fcmp(i.InputDoubleRegister(0), i.InputDoubleRegister(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        // 0.0 is the only immediate supported by fcmp instructions.\n        DCHECK_EQ(0.0, i.InputDouble(1));\n        __ Fcmp(i.InputDoubleRegister(0), i.InputDouble(1));\n      }\n      break;\n    case kArm64Float64Add:\n      __ Fadd(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    case kArm64Float64Sub:\n      __ Fsub(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    case kArm64Float64Mul:\n      __ Fmul(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    case kArm64Float64Div:\n      __ Fdiv(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    case kArm64Float64Mod: {\n      // TODO(turbofan): implement directly.\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      DCHECK_EQ(d0, i.InputDoubleRegister(0));\n      DCHECK_EQ(d1, i.InputDoubleRegister(1));\n      DCHECK_EQ(d0, i.OutputDoubleRegister());\n      // TODO(turbofan): make sure this saves all relevant registers.\n      __ CallCFunction(ExternalReference::mod_two_doubles_operation(), 0, 2);\n      break;\n    }\n    case kArm64Float32Max: {\n      __ Fmax(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    }\n    case kArm64Float64Max: {\n      __ Fmax(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    }\n    case kArm64Float32Min: {\n      __ Fmin(i.OutputFloat32Register(), i.InputFloat32Register(0),\n              i.InputFloat32Register(1));\n      break;\n    }\n    case kArm64Float64Min: {\n      __ Fmin(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    }\n    case kArm64Float64Abs:\n      __ Fabs(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kArm64Float64Abd:\n      __ Fabd(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n              i.InputDoubleRegister(1));\n      break;\n    case kArm64Float64Neg:\n      __ Fneg(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kArm64Float64Sqrt:\n      __ Fsqrt(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kArm64Float64Fnmul:\n      __ Fnmul(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n               i.InputDoubleRegister(1));\n      break;\n    case kArm64Float32ToFloat64:\n      __ Fcvt(i.OutputDoubleRegister(), i.InputDoubleRegister(0).S());\n      break;\n    case kArm64Float64ToFloat32:\n      __ Fcvt(i.OutputDoubleRegister().S(), i.InputDoubleRegister(0));\n      break;\n    case kArm64Float32ToInt32: {\n      __ Fcvtzs(i.OutputRegister32(), i.InputFloat32Register(0));\n      bool set_overflow_to_min_i32 = MiscField::decode(instr->opcode());\n      if (set_overflow_to_min_i32) {\n        // Avoid INT32_MAX as an overflow indicator and use INT32_MIN instead,\n        // because INT32_MIN allows easier out-of-bounds detection.\n        __ Cmn(i.OutputRegister32(), 1);\n        __ Csinc(i.OutputRegister32(), i.OutputRegister32(),\n                 i.OutputRegister32(), vc);\n      }\n      break;\n    }\n    case kArm64Float64ToInt32:\n      __ Fcvtzs(i.OutputRegister32(), i.InputDoubleRegister(0));\n      if (i.OutputCount() > 1) {\n        // Check for inputs below INT32_MIN and NaN.\n        __ Fcmp(i.InputDoubleRegister(0), static_cast<double>(INT32_MIN));\n        __ Cset(i.OutputRegister(1).W(), ge);\n        __ Fcmp(i.InputDoubleRegister(0), static_cast<double>(INT32_MAX) + 1);\n        __ CmovX(i.OutputRegister(1), xzr, ge);\n      }\n      break;\n    case kArm64Float32ToUint32: {\n      __ Fcvtzu(i.OutputRegister32(), i.InputFloat32Register(0));\n      bool set_overflow_to_min_u32 = MiscField::decode(instr->opcode());\n      if (set_overflow_to_min_u32) {\n        // Avoid UINT32_MAX as an overflow indicator and use 0 instead,\n        // because 0 allows easier out-of-bounds detection.\n        __ Cmn(i.OutputRegister32(), 1);\n        __ Adc(i.OutputRegister32(), i.OutputRegister32(), Operand(0));\n      }\n      break;\n    }\n    case kArm64Float64ToUint32:\n      __ Fcvtzu(i.OutputRegister32(), i.InputDoubleRegister(0));\n      if (i.OutputCount() > 1) {\n        __ Fcmp(i.InputDoubleRegister(0), -1.0);\n        __ Cset(i.OutputRegister(1).W(), gt);\n        __ Fcmp(i.InputDoubleRegister(0), static_cast<double>(UINT32_MAX) + 1);\n        __ CmovX(i.OutputRegister(1), xzr, ge);\n      }\n      break;\n    case kArm64Float32ToInt64:\n      __ Fcvtzs(i.OutputRegister64(), i.InputFloat32Register(0));\n      if (i.OutputCount() > 1) {\n        // Check for inputs below INT64_MIN and NaN.\n        __ Fcmp(i.InputFloat32Register(0), static_cast<float>(INT64_MIN));\n        // Check overflow.\n        // -1 value is used to indicate a possible overflow which will occur\n        // when subtracting (-1) from the provided INT64_MAX operand.\n        // OutputRegister(1) is set to 0 if the input was out of range or NaN.\n        __ Ccmp(i.OutputRegister(0), -1, VFlag, ge);\n        __ Cset(i.OutputRegister(1), vc);\n      }\n      break;\n    case kArm64Float64ToInt64: {\n      __ Fcvtzs(i.OutputRegister(0), i.InputDoubleRegister(0));\n      bool set_overflow_to_min_i64 = MiscField::decode(instr->opcode());\n      DCHECK_IMPLIES(set_overflow_to_min_i64, i.OutputCount() == 1);\n      if (set_overflow_to_min_i64) {\n        // Avoid INT64_MAX as an overflow indicator and use INT64_MIN instead,\n        // because INT64_MIN allows easier out-of-bounds detection.\n        __ Cmn(i.OutputRegister64(), 1);\n        __ Csinc(i.OutputRegister64(), i.OutputRegister64(),\n                 i.OutputRegister64(), vc);\n      } else if (i.OutputCount() > 1) {\n        // See kArm64Float32ToInt64 for a detailed description.\n        __ Fcmp(i.InputDoubleRegister(0), static_cast<double>(INT64_MIN));\n        __ Ccmp(i.OutputRegister(0), -1, VFlag, ge);\n        __ Cset(i.OutputRegister(1), vc);\n      }\n      break;\n    }\n    case kArm64Float32ToUint64:\n      __ Fcvtzu(i.OutputRegister64(), i.InputFloat32Register(0));\n      if (i.OutputCount() > 1) {\n        // See kArm64Float32ToInt64 for a detailed description.\n        __ Fcmp(i.InputFloat32Register(0), -1.0);\n        __ Ccmp(i.OutputRegister(0), -1, ZFlag, gt);\n        __ Cset(i.OutputRegister(1), ne);\n      }\n      break;\n    case kArm64Float64ToUint64:\n      __ Fcvtzu(i.OutputRegister64(), i.InputDoubleRegister(0));\n      if (i.OutputCount() > 1) {\n        // See kArm64Float32ToInt64 for a detailed description.\n        __ Fcmp(i.InputDoubleRegister(0), -1.0);\n        __ Ccmp(i.OutputRegister(0), -1, ZFlag, gt);\n        __ Cset(i.OutputRegister(1), ne);\n      }\n      break;\n    case kArm64Int32ToFloat32:\n      __ Scvtf(i.OutputFloat32Register(), i.InputRegister32(0));\n      break;\n    case kArm64Int32ToFloat64:\n      __ Scvtf(i.OutputDoubleRegister(), i.InputRegister32(0));\n      break;\n    case kArm64Int64ToFloat32:\n      __ Scvtf(i.OutputDoubleRegister().S(), i.InputRegister64(0));\n      break;\n    case kArm64Int64ToFloat64:\n      __ Scvtf(i.OutputDoubleRegister(), i.InputRegister64(0));\n      break;\n    case kArm64Uint32ToFloat32:\n      __ Ucvtf(i.OutputFloat32Register(), i.InputRegister32(0));\n      break;\n    case kArm64Uint32ToFloat64:\n      __ Ucvtf(i.OutputDoubleRegister(), i.InputRegister32(0));\n      break;\n    case kArm64Uint64ToFloat32:\n      __ Ucvtf(i.OutputDoubleRegister().S(), i.InputRegister64(0));\n      break;\n    case kArm64Uint64ToFloat64:\n      __ Ucvtf(i.OutputDoubleRegister(), i.InputRegister64(0));\n      break;\n    case kArm64Float64ExtractLowWord32:\n      __ Fmov(i.OutputRegister32(), i.InputFloat32Register(0));\n      break;\n    case kArm64Float64ExtractHighWord32:\n      __ Umov(i.OutputRegister32(), i.InputFloat64Register(0).V2S(), 1);\n      break;\n    case kArm64Float64InsertLowWord32:\n      DCHECK_EQ(i.OutputFloat64Register(), i.InputFloat64Register(0));\n      __ Ins(i.OutputFloat64Register().V2S(), 0, i.InputRegister32(1));\n      break;\n    case kArm64Float64InsertHighWord32:\n      DCHECK_EQ(i.OutputFloat64Register(), i.InputFloat64Register(0));\n      __ Ins(i.OutputFloat64Register().V2S(), 1, i.InputRegister32(1));\n      break;\n    case kArm64Float64MoveU64:\n      __ Fmov(i.OutputFloat64Register(), i.InputRegister(0));\n      break;\n    case kArm64Float64SilenceNaN:\n      __ CanonicalizeNaN(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kArm64U64MoveFloat64:\n      __ Fmov(i.OutputRegister(), i.InputDoubleRegister(0));\n      break;\n    case kArm64Ldrb:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrb(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64Ldrsb:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrsb(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrsbW:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrsb(i.OutputRegister32(), i.MemoryOperand());\n      break;\n    case kArm64Strb:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Strb(i.InputOrZeroRegister64(0), i.MemoryOperand(1));\n      break;\n    case kArm64Ldrh:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrh(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64Ldrsh:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrsh(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrshW:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrsh(i.OutputRegister32(), i.MemoryOperand());\n      break;\n    case kArm64Strh:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Strh(i.InputOrZeroRegister64(0), i.MemoryOperand(1));\n      break;\n    case kArm64Ldrsw:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldrsw(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrW:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputRegister32(), i.MemoryOperand());\n      break;\n    case kArm64StrW:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Str(i.InputOrZeroRegister32(0), i.MemoryOperand(1));\n      break;\n    case kArm64StrWPair:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Stp(i.InputOrZeroRegister32(0), i.InputOrZeroRegister32(1),\n             i.MemoryOperand(2));\n      break;\n    case kArm64Ldr:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrDecompressTaggedSigned:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ DecompressTaggedSigned(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrDecompressTagged:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ DecompressTagged(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdrDecompressProtected:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ DecompressProtected(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64LdarDecompressTaggedSigned:\n      __ AtomicDecompressTaggedSigned(i.OutputRegister(), i.InputRegister(0),\n                                      i.InputRegister(1), i.TempRegister(0));\n      break;\n    case kArm64LdarDecompressTagged:\n      __ AtomicDecompressTagged(i.OutputRegister(), i.InputRegister(0),\n                                i.InputRegister(1), i.TempRegister(0));\n      break;\n    case kArm64LdrDecodeSandboxedPointer:\n      __ LoadSandboxedPointerField(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kArm64Str:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Str(i.InputOrZeroRegister64(0), i.MemoryOperand(1));\n      break;\n    case kArm64StrPair:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Stp(i.InputOrZeroRegister64(0), i.InputOrZeroRegister64(1),\n             i.MemoryOperand(2));\n      break;\n    case kArm64StrCompressTagged:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ StoreTaggedField(i.InputOrZeroRegister64(0), i.MemoryOperand(1));\n      break;\n    case kArm64StlrCompressTagged:\n      // To be consistent with other STLR instructions, the value is stored at\n      // the 3rd input register instead of the 1st.\n      __ AtomicStoreTaggedField(i.InputRegister(2), i.InputRegister(0),\n                                i.InputRegister(1), i.TempRegister(0));\n      break;\n    case kArm64StrIndirectPointer:\n      __ StoreIndirectPointerField(i.InputOrZeroRegister64(0),\n                                   i.MemoryOperand(1));\n      break;\n    case kArm64StrEncodeSandboxedPointer:\n      __ StoreSandboxedPointerField(i.InputOrZeroRegister64(0),\n                                    i.MemoryOperand(1));\n      break;\n    case kArm64LdrS:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputDoubleRegister().S(), i.MemoryOperand());\n      break;\n    case kArm64StrS:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Str(i.InputFloat32OrZeroRegister(0), i.MemoryOperand(1));\n      break;\n    case kArm64LdrD:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputDoubleRegister(), i.MemoryOperand());\n      break;\n    case kArm64StrD:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Str(i.InputFloat64OrZeroRegister(0), i.MemoryOperand(1));\n      break;\n    case kArm64LdrQ:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register(), i.MemoryOperand());\n      break;\n    case kArm64StrQ:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Str(i.InputSimd128Register(0), i.MemoryOperand(1));\n      break;\n    case kArm64DmbIsh:\n      __ Dmb(InnerShareable, BarrierAll);\n      break;\n    case kArm64DsbIsb:\n      __ Dsb(FullSystem, BarrierAll);\n      __ Isb();\n      break;\n    case kAtomicLoadInt8:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldarb, Register32);\n      __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicLoadUint8:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldarb, Register32);\n      break;\n    case kAtomicLoadInt16:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldarh, Register32);\n      __ Sxth(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicLoadUint16:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldarh, Register32);\n      break;\n    case kAtomicLoadWord32:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldar, Register32);\n      break;\n    case kArm64Word64AtomicLoadUint64:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ldar, Register);\n      break;\n    case kAtomicStoreWord8:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Stlrb, Register32);\n      break;\n    case kAtomicStoreWord16:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Stlrh, Register32);\n      break;\n    case kAtomicStoreWord32:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Stlr, Register32);\n      break;\n    case kArm64Word64AtomicStoreWord64:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Stlr, Register);\n      break;\n    case kAtomicExchangeInt8:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(b, Register32);\n      __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicExchangeUint8:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(b, Register32);\n      break;\n    case kAtomicExchangeInt16:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(h, Register32);\n      __ Sxth(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicExchangeUint16:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(h, Register32);\n      break;\n    case kAtomicExchangeWord32:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(, Register32);\n      break;\n    case kArm64Word64AtomicExchangeUint64:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(, Register);\n      break;\n    case kAtomicCompareExchangeInt8:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(b, UXTB, Register32);\n      __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicCompareExchangeUint8:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(b, UXTB, Register32);\n      break;\n    case kAtomicCompareExchangeInt16:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(h, UXTH, Register32);\n      __ Sxth(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicCompareExchangeUint16:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(h, UXTH, Register32);\n      break;\n    case kAtomicCompareExchangeWord32:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(, UXTW, Register32);\n      break;\n    case kArm64Word64AtomicCompareExchangeUint64:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(, UXTX, Register);\n      break;\n    case kAtomicSubInt8:\n      ASSEMBLE_ATOMIC_SUB(b, Register32);\n      __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicSubUint8:\n      ASSEMBLE_ATOMIC_SUB(b, Register32);\n      break;\n    case kAtomicSubInt16:\n      ASSEMBLE_ATOMIC_SUB(h, Register32);\n      __ Sxth(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicSubUint16:\n      ASSEMBLE_ATOMIC_SUB(h, Register32);\n      break;\n    case kAtomicSubWord32:\n      ASSEMBLE_ATOMIC_SUB(, Register32);\n      break;\n    case kArm64Word64AtomicSubUint64:\n      ASSEMBLE_ATOMIC_SUB(, Register);\n      break;\n    case kAtomicAndInt8:\n      ASSEMBLE_ATOMIC_AND(b, Register32);\n      __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicAndUint8:\n      ASSEMBLE_ATOMIC_AND(b, Register32);\n      break;\n    case kAtomicAndInt16:\n      ASSEMBLE_ATOMIC_AND(h, Register32);\n      __ Sxth(i.OutputRegister(0), i.OutputRegister(0));\n      break;\n    case kAtomicAndUint16:\n      ASSEMBLE_ATOMIC_AND(h, Register32);\n      break;\n    case kAtomicAndWord32:\n      ASSEMBLE_ATOMIC_AND(, Register32);\n      break;\n    case kArm64Word64AtomicAndUint64:\n      ASSEMBLE_ATOMIC_AND(, Register);\n      break;\n#define ATOMIC_BINOP_CASE(op, inst, lse_instr)             \\\n  case kAtomic##op##Int8:                                  \\\n    ASSEMBLE_ATOMIC_BINOP(b, inst, lse_instr, Register32); \\\n    __ Sxtb(i.OutputRegister(0), i.OutputRegister(0));     \\\n    break;                                                 \\\n  case kAtomic##op##Uint8:                                 \\\n    ASSEMBLE_ATOMIC_BINOP(b, inst, lse_instr, Register32); \\\n    break;                                                 \\\n  case kAtomic##op##Int16:                                 \\\n    ASSEMBLE_ATOMIC_BINOP(h, inst, lse_instr, Register32); \\\n    __ Sxth(i.OutputRegister(0), i.OutputRegister(0));     \\\n    break;                                                 \\\n  case kAtomic##op##Uint16:                                \\\n    ASSEMBLE_ATOMIC_BINOP(h, inst, lse_instr, Register32); \\\n    break;                                                 \\\n  case kAtomic##op##Word32:                                \\\n    ASSEMBLE_ATOMIC_BINOP(, inst, lse_instr, Register32);  \\\n    break;                                                 \\\n  case kArm64Word64Atomic##op##Uint64:                     \\\n    ASSEMBLE_ATOMIC_BINOP(, inst, lse_instr, Register);    \\\n    break;\n      ATOMIC_BINOP_CASE(Add, Add, Ldaddal)\n      ATOMIC_BINOP_CASE(Or, Orr, Ldsetal)\n      ATOMIC_BINOP_CASE(Xor, Eor, Ldeoral)\n#undef ATOMIC_BINOP_CASE\n#undef ASSEMBLE_SHIFT\n#undef ASSEMBLE_ATOMIC_LOAD_INTEGER\n#undef ASSEMBLE_ATOMIC_STORE_INTEGER\n#undef ASSEMBLE_ATOMIC_EXCHANGE_INTEGER\n#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER\n#undef ASSEMBLE_ATOMIC_BINOP\n#undef ASSEMBLE_IEEE754_BINOP\n#undef ASSEMBLE_IEEE754_UNOP\n\n#if V8_ENABLE_WEBASSEMBLY\n#define SIMD_UNOP_CASE(Op, Instr, FORMAT)            \\\n  case Op:                                           \\\n    __ Instr(i.OutputSimd128Register().V##FORMAT(),  \\\n             i.InputSimd128Register(0).V##FORMAT()); \\\n    break;\n#define SIMD_UNOP_LANE_SIZE_CASE(Op, Instr)                            \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    __ Instr(i.OutputSimd128Register().Format(f),                      \\\n             i.InputSimd128Register(0).Format(f));                     \\\n    break;                                                             \\\n  }\n#define SIMD_BINOP_CASE(Op, Instr, FORMAT)           \\\n  case Op:                                           \\\n    __ Instr(i.OutputSimd128Register().V##FORMAT(),  \\\n             i.InputSimd128Register(0).V##FORMAT(),  \\\n             i.InputSimd128Register(1).V##FORMAT()); \\\n    break;\n#define SIMD_BINOP_LANE_SIZE_CASE(Op, Instr)                           \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    __ Instr(i.OutputSimd128Register().Format(f),                      \\\n             i.InputSimd128Register(0).Format(f),                      \\\n             i.InputSimd128Register(1).Format(f));                     \\\n    break;                                                             \\\n  }\n#define SIMD_FCM_L_CASE(Op, ImmOp, RegOp)                              \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    if (instr->InputCount() == 1) {                                    \\\n      __ Fcm##ImmOp(i.OutputSimd128Register().Format(f),               \\\n                    i.InputSimd128Register(0).Format(f), +0.0);        \\\n    } else {                                                           \\\n      __ Fcm##RegOp(i.OutputSimd128Register().Format(f),               \\\n                    i.InputSimd128Register(1).Format(f),               \\\n                    i.InputSimd128Register(0).Format(f));              \\\n    }                                                                  \\\n    break;                                                             \\\n  }\n#define SIMD_FCM_G_CASE(Op, ImmOp)                                     \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    /* Currently Gt/Ge instructions are only used with zero */         \\\n    DCHECK_EQ(instr->InputCount(), 1);                                 \\\n    __ Fcm##ImmOp(i.OutputSimd128Register().Format(f),                 \\\n                  i.InputSimd128Register(0).Format(f), +0.0);          \\\n    break;                                                             \\\n  }\n#define SIMD_CM_L_CASE(Op, ImmOp)                                      \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    DCHECK_EQ(instr->InputCount(), 1);                                 \\\n    __ Cm##ImmOp(i.OutputSimd128Register().Format(f),                  \\\n                 i.InputSimd128Register(0).Format(f), 0);              \\\n    break;                                                             \\\n  }\n#define SIMD_CM_G_CASE(Op, CmOp)                                       \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    if (instr->InputCount() == 1) {                                    \\\n      __ Cm##CmOp(i.OutputSimd128Register().Format(f),                 \\\n                  i.InputSimd128Register(0).Format(f), 0);             \\\n    } else {                                                           \\\n      __ Cm##CmOp(i.OutputSimd128Register().Format(f),                 \\\n                  i.InputSimd128Register(0).Format(f),                 \\\n                  i.InputSimd128Register(1).Format(f));                \\\n    }                                                                  \\\n    break;                                                             \\\n  }\n#define SIMD_DESTRUCTIVE_BINOP_CASE(Op, Instr, FORMAT)     \\\n  case Op: {                                               \\\n    VRegister dst = i.OutputSimd128Register().V##FORMAT(); \\\n    DCHECK_EQ(dst, i.InputSimd128Register(0).V##FORMAT()); \\\n    __ Instr(dst, i.InputSimd128Register(1).V##FORMAT(),   \\\n             i.InputSimd128Register(2).V##FORMAT());       \\\n    break;                                                 \\\n  }\n#define SIMD_DESTRUCTIVE_BINOP_LANE_SIZE_CASE(Op, Instr)               \\\n  case Op: {                                                           \\\n    VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode)); \\\n    VRegister dst = i.OutputSimd128Register().Format(f);               \\\n    DCHECK_EQ(dst, i.InputSimd128Register(0).Format(f));               \\\n    __ Instr(dst, i.InputSimd128Register(1).Format(f),                 \\\n             i.InputSimd128Register(2).Format(f));                     \\\n    break;                                                             \\\n  }\n#define SIMD_DESTRUCTIVE_RELAXED_FUSED_CASE(Op, Instr, FORMAT) \\\n  case Op: {                                                   \\\n    VRegister dst = i.OutputSimd128Register().V##FORMAT();     \\\n    DCHECK_EQ(dst, i.InputSimd128Register(2).V##FORMAT());     \\\n    __ Instr(dst, i.InputSimd128Register(0).V##FORMAT(),       \\\n             i.InputSimd128Register(1).V##FORMAT());           \\\n    break;                                                     \\\n  }\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FMin, Fmin);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FMax, Fmax);\n      SIMD_UNOP_LANE_SIZE_CASE(kArm64FAbs, Fabs);\n      SIMD_UNOP_LANE_SIZE_CASE(kArm64FSqrt, Fsqrt);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FAdd, Fadd);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FSub, Fsub);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FMul, Fmul);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64FDiv, Fdiv);\n      SIMD_UNOP_LANE_SIZE_CASE(kArm64FNeg, Fneg);\n      SIMD_UNOP_LANE_SIZE_CASE(kArm64IAbs, Abs);\n      SIMD_UNOP_LANE_SIZE_CASE(kArm64INeg, Neg);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64RoundingAverageU, Urhadd);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IMinS, Smin);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IMaxS, Smax);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IMinU, Umin);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IMaxU, Umax);\n      SIMD_DESTRUCTIVE_BINOP_LANE_SIZE_CASE(kArm64Mla, Mla);\n      SIMD_DESTRUCTIVE_BINOP_LANE_SIZE_CASE(kArm64Mls, Mls);\n    case kArm64Sxtl: {\n      VectorFormat wide = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat narrow = VectorFormatHalfWidth(wide);\n      __ Sxtl(i.OutputSimd128Register().Format(wide),\n              i.InputSimd128Register(0).Format(narrow));\n      break;\n    }\n    case kArm64Sxtl2: {\n      VectorFormat wide = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat narrow = VectorFormatHalfWidthDoubleLanes(wide);\n      __ Sxtl2(i.OutputSimd128Register().Format(wide),\n               i.InputSimd128Register(0).Format(narrow));\n      break;\n    }\n    case kArm64Uxtl: {\n      VectorFormat wide = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat narrow = VectorFormatHalfWidth(wide);\n      __ Uxtl(i.OutputSimd128Register().Format(wide),\n              i.InputSimd128Register(0).Format(narrow));\n      break;\n    }\n    case kArm64Uxtl2: {\n      VectorFormat wide = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VectorFormat narrow = VectorFormatHalfWidthDoubleLanes(wide);\n      __ Uxtl2(i.OutputSimd128Register().Format(wide),\n               i.InputSimd128Register(0).Format(narrow));\n      break;\n    }\n    case kArm64F64x2ConvertLowI32x4S: {\n      VRegister dst = i.OutputSimd128Register().V2D();\n      __ Sxtl(dst, i.InputSimd128Register(0).V2S());\n      __ Scvtf(dst, dst);\n      break;\n    }\n    case kArm64F64x2ConvertLowI32x4U: {\n      VRegister dst = i.OutputSimd128Register().V2D();\n      __ Uxtl(dst, i.InputSimd128Register(0).V2S());\n      __ Ucvtf(dst, dst);\n      break;\n    }\n    case kArm64I32x4TruncSatF64x2SZero: {\n      VRegister dst = i.OutputSimd128Register();\n      __ Fcvtzs(dst.V2D(), i.InputSimd128Register(0).V2D());\n      __ Sqxtn(dst.V2S(), dst.V2D());\n      break;\n    }\n    case kArm64I32x4TruncSatF64x2UZero: {\n      VRegister dst = i.OutputSimd128Register();\n      __ Fcvtzu(dst.V2D(), i.InputSimd128Register(0).V2D());\n      __ Uqxtn(dst.V2S(), dst.V2D());\n      break;\n    }\n    case kArm64F32x4DemoteF64x2Zero: {\n      __ Fcvtn(i.OutputSimd128Register().V2S(),\n               i.InputSimd128Register(0).V2D());\n      break;\n    }\n    case kArm64F64x2PromoteLowF32x4: {\n      __ Fcvtl(i.OutputSimd128Register().V2D(),\n               i.InputSimd128Register(0).V2S());\n      break;\n    }\n    case kArm64FExtractLane: {\n      VectorFormat dst_f =\n          ScalarFormatFromLaneSize(LaneSizeField::decode(opcode));\n      VectorFormat src_f = VectorFormatFillQ(dst_f);\n      __ Mov(i.OutputSimd128Register().Format(dst_f),\n             i.InputSimd128Register(0).Format(src_f), i.InputInt8(1));\n      break;\n    }\n    case kArm64FReplaceLane: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VRegister dst = i.OutputSimd128Register().Format(f),\n                src1 = i.InputSimd128Register(0).Format(f);\n      if (dst != src1) {\n        __ Mov(dst, src1);\n      }\n      __ Mov(dst, i.InputInt8(1), i.InputSimd128Register(2).Format(f), 0);\n      break;\n    }\n      SIMD_FCM_L_CASE(kArm64FEq, eq, eq);\n    case kArm64FNe: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VRegister dst = i.OutputSimd128Register().Format(f);\n      if (instr->InputCount() == 1) {\n        __ Fcmeq(dst, i.InputSimd128Register(0).Format(f), +0.0);\n      } else {\n        __ Fcmeq(dst, i.InputSimd128Register(0).Format(f),\n                 i.InputSimd128Register(1).Format(f));\n      }\n      __ Mvn(dst, dst);\n      break;\n    }\n      SIMD_FCM_L_CASE(kArm64FLt, lt, gt);\n      SIMD_FCM_L_CASE(kArm64FLe, le, ge);\n      SIMD_FCM_G_CASE(kArm64FGt, gt);\n      SIMD_FCM_G_CASE(kArm64FGe, ge);\n      SIMD_DESTRUCTIVE_RELAXED_FUSED_CASE(kArm64F64x2Qfma, Fmla, 2D);\n      SIMD_DESTRUCTIVE_RELAXED_FUSED_CASE(kArm64F64x2Qfms, Fmls, 2D);\n    case kArm64F64x2Pmin: {\n      VRegister dst = i.OutputSimd128Register().V2D();\n      VRegister lhs = i.InputSimd128Register(0).V2D();\n      VRegister rhs = i.InputSimd128Register(1).V2D();\n      // f64x2.pmin(lhs, rhs)\n      // = v128.bitselect(rhs, lhs, f64x2.lt(rhs,lhs))\n      // = v128.bitselect(rhs, lhs, f64x2.gt(lhs,rhs))\n      __ Fcmgt(dst, lhs, rhs);\n      __ Bsl(dst.V16B(), rhs.V16B(), lhs.V16B());\n      break;\n    }\n    case kArm64F64x2Pmax: {\n      VRegister dst = i.OutputSimd128Register().V2D();\n      VRegister lhs = i.InputSimd128Register(0).V2D();\n      VRegister rhs = i.InputSimd128Register(1).V2D();\n      // f64x2.pmax(lhs, rhs)\n      // = v128.bitselect(rhs, lhs, f64x2.gt(rhs, lhs))\n      __ Fcmgt(dst, rhs, lhs);\n      __ Bsl(dst.V16B(), rhs.V16B(), lhs.V16B());\n      break;\n    }\n      SIMD_UNOP_CASE(kArm64F32x4SConvertI32x4, Scvtf, 4S);\n      SIMD_UNOP_CASE(kArm64F32x4UConvertI32x4, Ucvtf, 4S);\n    case kArm64FMulElement: {\n      VectorFormat s_f =\n          ScalarFormatFromLaneSize(LaneSizeField::decode(opcode));\n      VectorFormat v_f = VectorFormatFillQ(s_f);\n      __ Fmul(i.OutputSimd128Register().Format(v_f),\n              i.InputSimd128Register(0).Format(v_f),\n              i.InputSimd128Register(1).Format(s_f), i.InputInt8(2));\n      break;\n    }\n      SIMD_DESTRUCTIVE_RELAXED_FUSED_CASE(kArm64F32x4Qfma, Fmla, 4S);\n      SIMD_DESTRUCTIVE_RELAXED_FUSED_CASE(kArm64F32x4Qfms, Fmls, 4S);\n    case kArm64F32x4Pmin: {\n      VRegister dst = i.OutputSimd128Register().V4S();\n      VRegister lhs = i.InputSimd128Register(0).V4S();\n      VRegister rhs = i.InputSimd128Register(1).V4S();\n      // f32x4.pmin(lhs, rhs)\n      // = v128.bitselect(rhs, lhs, f32x4.lt(rhs, lhs))\n      // = v128.bitselect(rhs, lhs, f32x4.gt(lhs, rhs))\n      __ Fcmgt(dst, lhs, rhs);\n      __ Bsl(dst.V16B(), rhs.V16B(), lhs.V16B());\n      break;\n    }\n    case kArm64F32x4Pmax: {\n      VRegister dst = i.OutputSimd128Register().V4S();\n      VRegister lhs = i.InputSimd128Register(0).V4S();\n      VRegister rhs = i.InputSimd128Register(1).V4S();\n      // f32x4.pmax(lhs, rhs)\n      // = v128.bitselect(rhs, lhs, f32x4.gt(rhs, lhs))\n      __ Fcmgt(dst, rhs, lhs);\n      __ Bsl(dst.V16B(), rhs.V16B(), lhs.V16B());\n      break;\n    }\n    case kArm64IExtractLane: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      Register dst =\n          f == kFormat2D ? i.OutputRegister64() : i.OutputRegister32();\n      __ Mov(dst, i.InputSimd128Register(0).Format(f), i.InputInt8(1));\n      break;\n    }\n    case kArm64IReplaceLane: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VRegister dst = i.OutputSimd128Register().Format(f),\n                src1 = i.InputSimd128Register(0).Format(f);\n      Register src2 =\n          f == kFormat2D ? i.InputRegister64(2) : i.InputRegister32(2);\n      if (dst != src1) {\n        __ Mov(dst, src1);\n      }\n      __ Mov(dst, i.InputInt8(1), src2);\n      break;\n    }\n    case kArm64I64x2Shl: {\n      ASSEMBLE_SIMD_SHIFT_LEFT(Shl, 6, V2D, Sshl, X);\n      break;\n    }\n    case kArm64I64x2ShrS: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Sshr, 6, V2D, Sshl, X);\n      break;\n    }\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IAdd, Add);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64ISub, Sub);\n    case kArm64I64x2Mul: {\n      UseScratchRegisterScope scope(masm());\n      VRegister dst = i.OutputSimd128Register();\n      VRegister src1 = i.InputSimd128Register(0);\n      VRegister src2 = i.InputSimd128Register(1);\n      VRegister tmp1 = scope.AcquireSameSizeAs(dst);\n      VRegister tmp2 = scope.AcquireSameSizeAs(dst);\n      VRegister tmp3 = i.ToSimd128Register(instr->TempAt(0));\n\n      // This 2x64-bit multiplication is performed with several 32-bit\n      // multiplications.\n\n      // 64-bit numbers x and y, can be represented as:\n      //   x = a + 2^32(b)\n      //   y = c + 2^32(d)\n\n      // A 64-bit multiplication is:\n      //   x * y = ac + 2^32(ad + bc) + 2^64(bd)\n      // note: `2^64(bd)` can be ignored, the value is too large to fit in\n      // 64-bits.\n\n      // This sequence implements a 2x64bit multiply, where the registers\n      // `src1` and `src2` are split up into 32-bit components:\n      //   src1 = |d|c|b|a|\n      //   src2 = |h|g|f|e|\n      //\n      //   src1 * src2 = |cg + 2^32(ch + dg)|ae + 2^32(af + be)|\n\n      // Reverse the 32-bit elements in the 64-bit words.\n      //   tmp2 = |g|h|e|f|\n      __ Rev64(tmp2.V4S(), src2.V4S());\n\n      // Calculate the high half components.\n      //   tmp2 = |dg|ch|be|af|\n      __ Mul(tmp2.V4S(), tmp2.V4S(), src1.V4S());\n\n      // Extract the low half components of src1.\n      //   tmp1 = |c|a|\n      __ Xtn(tmp1.V2S(), src1.V2D());\n\n      // Sum the respective high half components.\n      //   tmp2 = |dg+ch|be+af||dg+ch|be+af|\n      __ Addp(tmp2.V4S(), tmp2.V4S(), tmp2.V4S());\n\n      // Extract the low half components of src2.\n      //   tmp3 = |g|e|\n      __ Xtn(tmp3.V2S(), src2.V2D());\n\n      // Shift the high half components, into the high half.\n      //   dst = |dg+ch << 32|be+af << 32|\n      __ Shll(dst.V2D(), tmp2.V2S(), 32);\n\n      // Multiply the low components together, and accumulate with the high\n      // half.\n      //   dst = |dst[1] + cg|dst[0] + ae|\n      __ Umlal(dst.V2D(), tmp3.V2S(), tmp1.V2S());\n\n      break;\n    }\n      SIMD_CM_G_CASE(kArm64IEq, eq);\n    case kArm64INe: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      VRegister dst = i.OutputSimd128Register().Format(f);\n      if (instr->InputCount() == 1) {\n        __ Cmeq(dst, i.InputSimd128Register(0).Format(f), 0);\n      } else {\n        __ Cmeq(dst, i.InputSimd128Register(0).Format(f),\n                i.InputSimd128Register(1).Format(f));\n      }\n      __ Mvn(dst, dst);\n      break;\n    }\n      SIMD_CM_L_CASE(kArm64ILtS, lt);\n      SIMD_CM_L_CASE(kArm64ILeS, le);\n      SIMD_CM_G_CASE(kArm64IGtS, gt);\n      SIMD_CM_G_CASE(kArm64IGeS, ge);\n    case kArm64I64x2ShrU: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Ushr, 6, V2D, Ushl, X);\n      break;\n    }\n    case kArm64I64x2BitMask: {\n      __ I64x2BitMask(i.OutputRegister32(), i.InputSimd128Register(0));\n      break;\n    }\n      SIMD_UNOP_CASE(kArm64I32x4SConvertF32x4, Fcvtzs, 4S);\n    case kArm64I32x4Shl: {\n      ASSEMBLE_SIMD_SHIFT_LEFT(Shl, 5, V4S, Sshl, W);\n      break;\n    }\n    case kArm64I32x4ShrS: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Sshr, 5, V4S, Sshl, W);\n      break;\n    }\n      SIMD_BINOP_CASE(kArm64I32x4Mul, Mul, 4S);\n      SIMD_UNOP_CASE(kArm64I32x4UConvertF32x4, Fcvtzu, 4S);\n    case kArm64I32x4ShrU: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Ushr, 5, V4S, Ushl, W);\n      break;\n    }\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IGtU, Cmhi);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IGeU, Cmhs);\n    case kArm64I32x4BitMask: {\n      __ I32x4BitMask(i.OutputRegister32(), i.InputSimd128Register(0));\n      break;\n    }\n    case kArm64I32x4DotI16x8S: {\n      UseScratchRegisterScope scope(masm());\n      VRegister lhs = i.InputSimd128Register(0);\n      VRegister rhs = i.InputSimd128Register(1);\n      VRegister tmp1 = scope.AcquireV(kFormat4S);\n      VRegister tmp2 = scope.AcquireV(kFormat4S);\n      __ Smull(tmp1, lhs.V4H(), rhs.V4H());\n      __ Smull2(tmp2, lhs.V8H(), rhs.V8H());\n      __ Addp(i.OutputSimd128Register().V4S(), tmp1, tmp2);\n      break;\n    }\n    case kArm64I16x8DotI8x16S: {\n      UseScratchRegisterScope scope(masm());\n      VRegister lhs = i.InputSimd128Register(0);\n      VRegister rhs = i.InputSimd128Register(1);\n      VRegister tmp1 = scope.AcquireV(kFormat8H);\n      VRegister tmp2 = scope.AcquireV(kFormat8H);\n      __ Smull(tmp1, lhs.V8B(), rhs.V8B());\n      __ Smull2(tmp2, lhs.V16B(), rhs.V16B());\n      __ Addp(i.OutputSimd128Register().V8H(), tmp1, tmp2);\n      break;\n    }\n    case kArm64I32x4DotI8x16AddS: {\n      if (CpuFeatures::IsSupported(DOTPROD)) {\n        CpuFeatureScope scope(masm(), DOTPROD);\n\n        DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(2));\n        __ Sdot(i.InputSimd128Register(2).V4S(),\n                i.InputSimd128Register(0).V16B(),\n                i.InputSimd128Register(1).V16B());\n\n      } else {\n        UseScratchRegisterScope scope(masm());\n        VRegister lhs = i.InputSimd128Register(0);\n        VRegister rhs = i.InputSimd128Register(1);\n        VRegister tmp1 = scope.AcquireV(kFormat8H);\n        VRegister tmp2 = scope.AcquireV(kFormat8H);\n        __ Smull(tmp1, lhs.V8B(), rhs.V8B());\n        __ Smull2(tmp2, lhs.V16B(), rhs.V16B());\n        __ Addp(tmp1, tmp1, tmp2);\n        __ Saddlp(tmp1.V4S(), tmp1);\n        __ Add(i.OutputSimd128Register().V4S(), tmp1.V4S(),\n               i.InputSimd128Register(2).V4S());\n      }\n      break;\n    }\n    case kArm64IExtractLaneU: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      __ Umov(i.OutputRegister32(), i.InputSimd128Register(0).Format(f),\n              i.InputInt8(1));\n      break;\n    }\n    case kArm64IExtractLaneS: {\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      __ Smov(i.OutputRegister32(), i.InputSimd128Register(0).Format(f),\n              i.InputInt8(1));\n      break;\n    }\n    case kArm64I16x8Shl: {\n      ASSEMBLE_SIMD_SHIFT_LEFT(Shl, 4, V8H, Sshl, W);\n      break;\n    }\n    case kArm64I16x8ShrS: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Sshr, 4, V8H, Sshl, W);\n      break;\n    }\n    case kArm64I16x8SConvertI32x4: {\n      VRegister dst = i.OutputSimd128Register(),\n                src0 = i.InputSimd128Register(0),\n                src1 = i.InputSimd128Register(1);\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat4S);\n      if (dst == src1) {\n        __ Mov(temp, src1.V4S());\n        src1 = temp;\n      }\n      __ Sqxtn(dst.V4H(), src0.V4S());\n      __ Sqxtn2(dst.V8H(), src1.V4S());\n      break;\n    }\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IAddSatS, Sqadd);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64ISubSatS, Sqsub);\n      SIMD_BINOP_CASE(kArm64I16x8Mul, Mul, 8H);\n    case kArm64I16x8ShrU: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Ushr, 4, V8H, Ushl, W);\n      break;\n    }\n    case kArm64I16x8UConvertI32x4: {\n      VRegister dst = i.OutputSimd128Register(),\n                src0 = i.InputSimd128Register(0),\n                src1 = i.InputSimd128Register(1);\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat4S);\n      if (dst == src1) {\n        __ Mov(temp, src1.V4S());\n        src1 = temp;\n      }\n      __ Sqxtun(dst.V4H(), src0.V4S());\n      __ Sqxtun2(dst.V8H(), src1.V4S());\n      break;\n    }\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64IAddSatU, Uqadd);\n      SIMD_BINOP_LANE_SIZE_CASE(kArm64ISubSatU, Uqsub);\n      SIMD_BINOP_CASE(kArm64I16x8Q15MulRSatS, Sqrdmulh, 8H);\n    case kArm64I16x8BitMask: {\n      __ I16x8BitMask(i.OutputRegister32(), i.InputSimd128Register(0));\n      break;\n    }\n    case kArm64I8x16Shl: {\n      ASSEMBLE_SIMD_SHIFT_LEFT(Shl, 3, V16B, Sshl, W);\n      break;\n    }\n    case kArm64I8x16ShrS: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Sshr, 3, V16B, Sshl, W);\n      break;\n    }\n    case kArm64I8x16SConvertI16x8: {\n      VRegister dst = i.OutputSimd128Register(),\n                src0 = i.InputSimd128Register(0),\n                src1 = i.InputSimd128Register(1);\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat8H);\n      if (dst == src1) {\n        __ Mov(temp, src1.V8H());\n        src1 = temp;\n      }\n      __ Sqxtn(dst.V8B(), src0.V8H());\n      __ Sqxtn2(dst.V16B(), src1.V8H());\n      break;\n    }\n    case kArm64I8x16ShrU: {\n      ASSEMBLE_SIMD_SHIFT_RIGHT(Ushr, 3, V16B, Ushl, W);\n      break;\n    }\n    case kArm64I8x16UConvertI16x8: {\n      VRegister dst = i.OutputSimd128Register(),\n                src0 = i.InputSimd128Register(0),\n                src1 = i.InputSimd128Register(1);\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat8H);\n      if (dst == src1) {\n        __ Mov(temp, src1.V8H());\n        src1 = temp;\n      }\n      __ Sqxtun(dst.V8B(), src0.V8H());\n      __ Sqxtun2(dst.V16B(), src1.V8H());\n      break;\n    }\n    case kArm64I8x16BitMask: {\n      VRegister temp = NoVReg;\n\n      if (CpuFeatures::IsSupported(PMULL1Q)) {\n        temp = i.TempSimd128Register(0);\n      }\n\n      __ I8x16BitMask(i.OutputRegister32(), i.InputSimd128Register(0), temp);\n      break;\n    }\n    case kArm64S128Const: {\n      uint64_t imm1 = make_uint64(i.InputUint32(1), i.InputUint32(0));\n      uint64_t imm2 = make_uint64(i.InputUint32(3), i.InputUint32(2));\n      __ Movi(i.OutputSimd128Register().V16B(), imm2, imm1);\n      break;\n    }\n      SIMD_BINOP_CASE(kArm64S128And, And, 16B);\n      SIMD_BINOP_CASE(kArm64S128Or, Orr, 16B);\n      SIMD_BINOP_CASE(kArm64S128Xor, Eor, 16B);\n      SIMD_UNOP_CASE(kArm64S128Not, Mvn, 16B);\n    case kArm64S128Dup: {\n      VRegister dst = i.OutputSimd128Register(),\n                src = i.InputSimd128Register(0);\n      int lanes = i.InputInt32(1);\n      int index = i.InputInt32(2);\n      switch (lanes) {\n        case 4:\n          __ Dup(dst.V4S(), src.V4S(), index);\n          break;\n        case 8:\n          __ Dup(dst.V8H(), src.V8H(), index);\n          break;\n        case 16:\n          __ Dup(dst.V16B(), src.V16B(), index);\n          break;\n        default:\n          UNREACHABLE();\n      }\n      break;\n    }\n      SIMD_DESTRUCTIVE_BINOP_CASE(kArm64S128Select, Bsl, 16B);\n    case kArm64S128AndNot:\n      if (instr->InputAt(1)->IsImmediate()) {\n        VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n        VRegister dst = i.OutputSimd128Register().Format(f);\n        DCHECK_EQ(dst, i.InputSimd128Register(0).Format(f));\n        __ Bic(dst, i.InputInt32(1), i.InputInt8(2));\n      } else {\n        __ Bic(i.OutputSimd128Register().V16B(),\n               i.InputSimd128Register(0).V16B(),\n               i.InputSimd128Register(1).V16B());\n      }\n      break;\n    case kArm64Ssra: {\n      int8_t laneSize = LaneSizeField::decode(opcode);\n      VectorFormat f = VectorFormatFillQ(laneSize);\n      int8_t mask = laneSize - 1;\n      VRegister dst = i.OutputSimd128Register().Format(f);\n      DCHECK_EQ(dst, i.InputSimd128Register(0).Format(f));\n      __ Ssra(dst, i.InputSimd128Register(1).Format(f), i.InputInt8(2) & mask);\n      break;\n    }\n    case kArm64Usra: {\n      int8_t laneSize = LaneSizeField::decode(opcode);\n      VectorFormat f = VectorFormatFillQ(laneSize);\n      int8_t mask = laneSize - 1;\n      VRegister dst = i.OutputSimd128Register().Format(f);\n      DCHECK_EQ(dst, i.InputSimd128Register(0).Format(f));\n      __ Usra(dst, i.InputSimd128Register(1).Format(f), i.InputUint8(2) & mask);\n      break;\n    }\n    case kArm64S32x4Shuffle: {\n      Simd128Register dst = i.OutputSimd128Register().V4S(),\n                      src0 = i.InputSimd128Register(0).V4S(),\n                      src1 = i.InputSimd128Register(1).V4S();\n      // Check for in-place shuffles.\n      // If dst == src0 == src1, then the shuffle is unary and we only use src0.\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat4S);\n      if (dst == src0) {\n        __ Mov(temp, src0);\n        src0 = temp;\n      } else if (dst == src1) {\n        __ Mov(temp, src1);\n        src1 = temp;\n      }\n      int32_t shuffle = i.InputInt32(2);\n\n      // Check whether we can reduce the number of vmovs by performing a dup\n      // first.\n      if (src0 == src1) {\n        const std::array<int, 4> lanes{shuffle & 0x3, shuffle >> 8 & 0x3,\n                                       shuffle >> 16 & 0x3,\n                                       shuffle >> 24 & 0x3};\n        std::array<int, 4> lane_counts{};\n        for (int lane : lanes) {\n          ++lane_counts[lane];\n        }\n\n        int duplicate_lane = -1;\n        for (int lane = 0; lane < 4; ++lane) {\n          if (lane_counts[lane] > 1) {\n            duplicate_lane = lane;\n            break;\n          }\n        }\n\n        if (duplicate_lane != -1) {\n          __ Dup(dst, src0, duplicate_lane);\n          for (int i = 0; i < 4; ++i) {\n            int lane = lanes[i];\n            if (lane == duplicate_lane) continue;\n            __ Mov(dst, i, src0, lane);\n          }\n          break;\n        }\n      }\n\n      // Perform shuffle as a vmov per lane.\n      for (int i = 0; i < 4; i++) {\n        VRegister src = src0;\n        int lane = shuffle & 0x7;\n        if (lane >= 4) {\n          src = src1;\n          lane &= 0x3;\n        }\n        __ Mov(dst, i, src, lane);\n        shuffle >>= 8;\n      }\n      break;\n    }\n      SIMD_BINOP_CASE(kArm64S32x4ZipLeft, Zip1, 4S);\n      SIMD_BINOP_CASE(kArm64S32x4ZipRight, Zip2, 4S);\n      SIMD_BINOP_CASE(kArm64S32x4UnzipLeft, Uzp1, 4S);\n      SIMD_BINOP_CASE(kArm64S32x4UnzipRight, Uzp2, 4S);\n      SIMD_BINOP_CASE(kArm64S32x4TransposeLeft, Trn1, 4S);\n      SIMD_BINOP_CASE(kArm64S32x4TransposeRight, Trn2, 4S);\n      SIMD_BINOP_CASE(kArm64S16x8ZipLeft, Zip1, 8H);\n      SIMD_BINOP_CASE(kArm64S16x8ZipRight, Zip2, 8H);\n      SIMD_BINOP_CASE(kArm64S16x8UnzipLeft, Uzp1, 8H);\n      SIMD_BINOP_CASE(kArm64S16x8UnzipRight, Uzp2, 8H);\n      SIMD_BINOP_CASE(kArm64S16x8TransposeLeft, Trn1, 8H);\n      SIMD_BINOP_CASE(kArm64S16x8TransposeRight, Trn2, 8H);\n      SIMD_BINOP_CASE(kArm64S8x16ZipLeft, Zip1, 16B);\n      SIMD_BINOP_CASE(kArm64S8x16ZipRight, Zip2, 16B);\n      SIMD_BINOP_CASE(kArm64S8x16UnzipLeft, Uzp1, 16B);\n      SIMD_BINOP_CASE(kArm64S8x16UnzipRight, Uzp2, 16B);\n      SIMD_BINOP_CASE(kArm64S8x16TransposeLeft, Trn1, 16B);\n      SIMD_BINOP_CASE(kArm64S8x16TransposeRight, Trn2, 16B);\n    case kArm64S8x16Concat: {\n      __ Ext(i.OutputSimd128Register().V16B(), i.InputSimd128Register(0).V16B(),\n             i.InputSimd128Register(1).V16B(), i.InputInt4(2));\n      break;\n    }\n    case kArm64I8x16Swizzle: {\n      __ Tbl(i.OutputSimd128Register().V16B(), i.InputSimd128Register(0).V16B(),\n             i.InputSimd128Register(1).V16B());\n      break;\n    }\n    case kArm64I8x16Shuffle: {\n      Simd128Register dst = i.OutputSimd128Register().V16B(),\n                      src0 = i.InputSimd128Register(0).V16B(),\n                      src1 = i.InputSimd128Register(1).V16B();\n      // Unary shuffle table is in src0, binary shuffle table is in src0, src1,\n      // which must be consecutive.\n      if (src0 != src1) {\n        DCHECK(AreConsecutive(src0, src1));\n      }\n\n      int64_t imm1 = make_uint64(i.InputInt32(3), i.InputInt32(2));\n      int64_t imm2 = make_uint64(i.InputInt32(5), i.InputInt32(4));\n      DCHECK_EQ(0, (imm1 | imm2) & (src0 == src1 ? 0xF0F0F0F0F0F0F0F0\n                                                 : 0xE0E0E0E0E0E0E0E0));\n\n      UseScratchRegisterScope scope(masm());\n      VRegister temp = scope.AcquireV(kFormat16B);\n      __ Movi(temp, imm2, imm1);\n\n      if (src0 == src1) {\n        __ Tbl(dst, src0, temp.V16B());\n      } else {\n        __ Tbl(dst, src0, src1, temp.V16B());\n      }\n      break;\n    }\n    case kArm64S32x4Reverse: {\n      Simd128Register dst = i.OutputSimd128Register().V16B(),\n                      src = i.InputSimd128Register(0).V16B();\n      __ Rev64(dst.V4S(), src.V4S());\n      __ Ext(dst.V16B(), dst.V16B(), dst.V16B(), 8);\n      break;\n    }\n      SIMD_UNOP_CASE(kArm64S32x2Reverse, Rev64, 4S);\n      SIMD_UNOP_CASE(kArm64S16x4Reverse, Rev64, 8H);\n      SIMD_UNOP_CASE(kArm64S16x2Reverse, Rev32, 8H);\n      SIMD_UNOP_CASE(kArm64S8x8Reverse, Rev64, 16B);\n      SIMD_UNOP_CASE(kArm64S8x4Reverse, Rev32, 16B);\n      SIMD_UNOP_CASE(kArm64S8x2Reverse, Rev16, 16B);\n    case kArm64LoadSplat: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      __ ld1r(i.OutputSimd128Register().Format(f), i.MemoryOperand(0));\n      break;\n    }\n    case kArm64LoadLane: {\n      DCHECK_EQ(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      int laneidx = i.InputInt8(1);\n      __ ld1(i.OutputSimd128Register().Format(f), laneidx, i.MemoryOperand(2));\n      break;\n    }\n    case kArm64StoreLane: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      VectorFormat f = VectorFormatFillQ(LaneSizeField::decode(opcode));\n      int laneidx = i.InputInt8(1);\n      __ st1(i.InputSimd128Register(0).Format(f), laneidx, i.MemoryOperand(2));\n      break;\n    }\n    case kArm64S128Load8x8S: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V8B(), i.MemoryOperand(0));\n      __ Sxtl(i.OutputSimd128Register().V8H(), i.OutputSimd128Register().V8B());\n      break;\n    }\n    case kArm64S128Load8x8U: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V8B(), i.MemoryOperand(0));\n      __ Uxtl(i.OutputSimd128Register().V8H(), i.OutputSimd128Register().V8B());\n      break;\n    }\n    case kArm64S128Load16x4S: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V4H(), i.MemoryOperand(0));\n      __ Sxtl(i.OutputSimd128Register().V4S(), i.OutputSimd128Register().V4H());\n      break;\n    }\n    case kArm64S128Load16x4U: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V4H(), i.MemoryOperand(0));\n      __ Uxtl(i.OutputSimd128Register().V4S(), i.OutputSimd128Register().V4H());\n      break;\n    }\n    case kArm64S128Load32x2S: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V2S(), i.MemoryOperand(0));\n      __ Sxtl(i.OutputSimd128Register().V2D(), i.OutputSimd128Register().V2S());\n      break;\n    }\n    case kArm64S128Load32x2U: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Ldr(i.OutputSimd128Register().V2S(), i.MemoryOperand(0));\n      __ Uxtl(i.OutputSimd128Register().V2D(), i.OutputSimd128Register().V2S());\n      break;\n    }\n    case kArm64I64x2AllTrue: {\n      __ I64x2AllTrue(i.OutputRegister32(), i.InputSimd128Register(0));\n      break;\n    }\n    case kArm64V128AnyTrue: {\n      UseScratchRegisterScope scope(masm());\n      // For AnyTrue, the format does not matter; also, we would like to avoid\n      // an expensive horizontal reduction.\n      VRegister temp = scope.AcquireV(kFormat4S);\n      __ Umaxp(temp, i.InputSimd128Register(0).V4S(),\n               i.InputSimd128Register(0).V4S());\n      __ Fmov(i.OutputRegister64(), temp.D());\n      __ Cmp(i.OutputRegister64(), 0);\n      __ Cset(i.OutputRegister32(), ne);\n      break;\n    }\n    case kArm64S32x4OneLaneSwizzle: {\n      Simd128Register dst = i.OutputSimd128Register().V4S(),\n                      src = i.InputSimd128Register(0).V4S();\n      int from = i.InputInt32(1);\n      int to = i.InputInt32(2);\n      if (dst != src) {\n        __ Mov(dst, src);\n      }\n      __ Mov(dst, to, src, from);\n      break;\n    }\n#define SIMD_REDUCE_OP_CASE(Op, Instr, format, FORMAT)     \\\n  case Op: {                                               \\\n    UseScratchRegisterScope scope(masm());                 \\\n    VRegister temp = scope.AcquireV(format);               \\\n    __ Instr(temp, i.InputSimd128Register(0).V##FORMAT()); \\\n    __ Umov(i.OutputRegister32(), temp, 0);                \\\n    __ Cmp(i.OutputRegister32(), 0);                       \\\n    __ Cset(i.OutputRegister32(), ne);                     \\\n    break;                                                 \\\n  }\n      SIMD_REDUCE_OP_CASE(kArm64I32x4AllTrue, Uminv, kFormatS, 4S);\n      SIMD_REDUCE_OP_CASE(kArm64I16x8AllTrue, Uminv, kFormatH, 8H);\n      SIMD_REDUCE_OP_CASE(kArm64I8x16AllTrue, Uminv, kFormatB, 16B);\n#endif  // V8_ENABLE_WEBASSEMBLY\n  }\n  return kSuccess;\n}", "name_and_para": "CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(\n    Instruction* instr) "}, {"name": "CodeGenerator::CodeGenResult", "content": "CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(\n    Instruction* instr) {\n  RiscvOperandConverter i(this, instr);\n  InstructionCode opcode = instr->opcode();\n  ArchOpcode arch_opcode = ArchOpcodeField::decode(opcode);\n  switch (arch_opcode) {\n    case kArchCallCodeObject: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        __ Call(i.InputCode(0), RelocInfo::CODE_TARGET);\n      } else {\n        Register reg = i.InputRegister(0);\n        CodeEntrypointTag tag =\n            i.InputCodeEntrypointTag(instr->CodeEnrypointTagInputIndex());\n        DCHECK_IMPLIES(\n            instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n            reg == kJavaScriptCallCodeStartRegister);\n        __ CallCodeObject(reg, tag);\n      }\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchCallBuiltinPointer: {\n      DCHECK(!instr->InputAt(0)->IsImmediate());\n      Register builtin_index = i.InputRegister(0);\n      Register target =\n          instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister)\n              ? kJavaScriptCallCodeStartRegister\n              : builtin_index;\n      __ CallBuiltinByIndex(builtin_index, target);\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchCallWasmFunction: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        Constant constant = i.ToConstant(instr->InputAt(0));\n        Address wasm_code = static_cast<Address>(constant.ToInt64());\n        __ Call(wasm_code, constant.rmode());\n      } else {\n        __ AddWord(t6, i.InputOrZeroRegister(0), 0);\n        __ Call(t6);\n      }\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchTailCallCodeObject: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        __ Jump(i.InputCode(0), RelocInfo::CODE_TARGET);\n      } else {\n        Register reg = i.InputOrZeroRegister(0);\n        CodeEntrypointTag tag =\n            i.InputCodeEntrypointTag(instr->CodeEnrypointTagInputIndex());\n        DCHECK_IMPLIES(\n            instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n            reg == kJavaScriptCallCodeStartRegister);\n        __ JumpCodeObject(reg, tag);\n      }\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n    case kArchTailCallWasm: {\n      if (instr->InputAt(0)->IsImmediate()) {\n        Constant constant = i.ToConstant(instr->InputAt(0));\n        Address wasm_code = static_cast<Address>(constant.ToInt64());\n        __ Jump(wasm_code, constant.rmode());\n      } else {\n        __ AddWord(kScratchReg, i.InputOrZeroRegister(0), 0);\n        __ Jump(kScratchReg);\n      }\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n    case kArchTailCallAddress: {\n      CHECK(!instr->InputAt(0)->IsImmediate());\n      Register reg = i.InputOrZeroRegister(0);\n      DCHECK_IMPLIES(\n          instr->HasCallDescriptorFlag(CallDescriptor::kFixedTargetRegister),\n          reg == kJavaScriptCallCodeStartRegister);\n      __ Jump(reg);\n      frame_access_state()->ClearSPDelta();\n      frame_access_state()->SetFrameAccessToDefault();\n      break;\n    }\n    case kArchCallJSFunction: {\n      Register func = i.InputOrZeroRegister(0);\n      if (v8_flags.debug_code) {\n        // Check the function's context matches the context argument.\n        __ LoadTaggedField(kScratchReg,\n                           FieldMemOperand(func, JSFunction::kContextOffset));\n        __ Assert(eq, AbortReason::kWrongFunctionContext, cp,\n                  Operand(kScratchReg));\n      }\n      __ CallJSFunction(func);\n      RecordCallPosition(instr);\n      frame_access_state()->ClearSPDelta();\n      break;\n    }\n    case kArchPrepareCallCFunction: {\n#ifdef V8_TARGET_ARCH_RISCV64\n      int const num_gp_parameters = ParamField::decode(instr->opcode());\n      int const num_fp_parameters = FPParamField::decode(instr->opcode());\n      __ PrepareCallCFunction(num_gp_parameters, num_fp_parameters,\n                              kScratchReg);\n#else\n      int const num_parameters = MiscField::decode(instr->opcode());\n      __ PrepareCallCFunction(num_parameters, kScratchReg);\n#endif\n      // Frame alignment requires using FP-relative frame addressing.\n      frame_access_state()->SetFrameAccessToFP();\n      break;\n    }\n    case kArchSaveCallerRegisters: {\n      fp_mode_ =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      DCHECK(fp_mode_ == SaveFPRegsMode::kIgnore ||\n             fp_mode_ == SaveFPRegsMode::kSave);\n      // kReturnRegister0 should have been saved before entering the stub.\n      int bytes = __ PushCallerSaved(fp_mode_, kReturnRegister0);\n      DCHECK(IsAligned(bytes, kSystemPointerSize));\n      DCHECK_EQ(0, frame_access_state()->sp_delta());\n      frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);\n      DCHECK(!caller_registers_saved_);\n      caller_registers_saved_ = true;\n      break;\n    }\n    case kArchRestoreCallerRegisters: {\n      DCHECK(fp_mode_ ==\n             static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode())));\n      DCHECK(fp_mode_ == SaveFPRegsMode::kIgnore ||\n             fp_mode_ == SaveFPRegsMode::kSave);\n      // Don't overwrite the returned value.\n      int bytes = __ PopCallerSaved(fp_mode_, kReturnRegister0);\n      frame_access_state()->IncreaseSPDelta(-(bytes / kSystemPointerSize));\n      DCHECK_EQ(0, frame_access_state()->sp_delta());\n      DCHECK(caller_registers_saved_);\n      caller_registers_saved_ = false;\n      break;\n    }\n    case kArchPrepareTailCall:\n      AssemblePrepareTailCall();\n      break;\n    case kArchCallCFunctionWithFrameState:\n    case kArchCallCFunction: {\n      int const num_gp_parameters = ParamField::decode(instr->opcode());\n      int const num_fp_parameters = FPParamField::decode(instr->opcode());\n      Label return_location;\n      SetIsolateDataSlots set_isolate_data_slots = SetIsolateDataSlots::kYes;\n#if V8_ENABLE_WEBASSEMBLY\n      bool isWasmCapiFunction =\n          linkage()->GetIncomingDescriptor()->IsWasmCapiFunction();\n      if (isWasmCapiFunction) {\n        // Put the return address in a stack slot.\n        __ LoadAddress(kScratchReg, &return_location,\n                       RelocInfo::EXTERNAL_REFERENCE);\n        __ StoreWord(kScratchReg,\n                     MemOperand(fp, WasmExitFrameConstants::kCallingPCOffset));\n        set_isolate_data_slots = SetIsolateDataSlots::kNo;\n      }\n#endif  // V8_ENABLE_WEBASSEMBLY\n      int pc_offset;\n      if (instr->InputAt(0)->IsImmediate()) {\n        ExternalReference ref = i.InputExternalReference(0);\n        pc_offset = __ CallCFunction(ref, num_gp_parameters, num_fp_parameters,\n                                     set_isolate_data_slots, &return_location);\n      } else {\n        Register func = i.InputRegister(0);\n        pc_offset = __ CallCFunction(func, num_gp_parameters, num_fp_parameters,\n                                     set_isolate_data_slots, &return_location);\n      }\n      RecordSafepoint(instr->reference_map(), pc_offset);\n\n      bool const needs_frame_state =\n          (arch_opcode == kArchCallCFunctionWithFrameState);\n      if (needs_frame_state) {\n        RecordDeoptInfo(instr, pc_offset);\n      }\n\n      frame_access_state()->SetFrameAccessToDefault();\n      // Ideally, we should decrement SP delta to match the change of stack\n      // pointer in CallCFunction. However, for certain architectures (e.g.\n      // ARM), there may be more strict alignment requirement, causing old SP\n      // to be saved on the stack. In those cases, we can not calculate the SP\n      // delta statically.\n      frame_access_state()->ClearSPDelta();\n      if (caller_registers_saved_) {\n        // Need to re-sync SP delta introduced in kArchSaveCallerRegisters.\n        // Here, we assume the sequence to be:\n        //   kArchSaveCallerRegisters;\n        //   kArchCallCFunction;\n        //   kArchRestoreCallerRegisters;\n        int bytes =\n            __ RequiredStackSizeForCallerSaved(fp_mode_, kReturnRegister0);\n        frame_access_state()->IncreaseSPDelta(bytes / kSystemPointerSize);\n      }\n      break;\n    }\n    case kArchJmp:\n      AssembleArchJump(i.InputRpo(0));\n      break;\n    case kArchBinarySearchSwitch:\n      AssembleArchBinarySearchSwitch(instr);\n      break;\n    case kArchTableSwitch:\n      AssembleArchTableSwitch(instr);\n      break;\n    case kArchAbortCSADcheck:\n      DCHECK(i.InputRegister(0) == a0);\n      {\n        // We don't actually want to generate a pile of code for this, so just\n        // claim there is a stack frame, without generating one.\n        FrameScope scope(masm(), StackFrame::NO_FRAME_TYPE);\n        __ CallBuiltin(Builtin::kAbortCSADcheck);\n      }\n      __ stop();\n      break;\n    case kArchDebugBreak:\n      __ DebugBreak();\n      break;\n    case kArchComment:\n      __ RecordComment(reinterpret_cast<const char*>(i.InputInt64(0)),\n                       SourceLocation());\n      break;\n    case kArchNop:\n    case kArchThrowTerminator:\n      // don't emit code for nops.\n      break;\n    case kArchDeoptimize: {\n      DeoptimizationExit* exit =\n          BuildTranslation(instr, -1, 0, 0, OutputFrameStateCombine::Ignore());\n      __ Branch(exit->label());\n      break;\n    }\n    case kArchRet:\n      AssembleReturn(instr->InputAt(0));\n      break;\n#if V8_ENABLE_WEBASSEMBLY\n    case kArchStackPointer:\n      // The register allocator expects an allocatable register for the output,\n      // we cannot use sp directly.\n      __ Move(i.OutputRegister(), sp);\n      break;\n    case kArchSetStackPointer: {\n      DCHECK(instr->InputAt(0)->IsRegister());\n      __ Move(sp, i.InputRegister(0));\n      break;\n    }\n#endif  // V8_ENABLE_WEBASSEMBLY\n    case kArchStackPointerGreaterThan:\n      // Pseudo-instruction used for cmp/branch. No opcode emitted here.\n      break;\n    case kArchStackCheckOffset:\n      __ Move(i.OutputRegister(), Smi::FromInt(GetStackCheckOffset()));\n      break;\n    case kArchFramePointer:\n      __ Move(i.OutputRegister(), fp);\n      break;\n    case kArchParentFramePointer:\n      if (frame_access_state()->has_frame()) {\n        __ LoadWord(i.OutputRegister(), MemOperand(fp, 0));\n      } else {\n        __ Move(i.OutputRegister(), fp);\n      }\n      break;\n    case kArchTruncateDoubleToI:\n      __ TruncateDoubleToI(isolate(), zone(), i.OutputRegister(),\n                           i.InputDoubleRegister(0), DetermineStubCallMode());\n      break;\n    case kArchStoreWithWriteBarrier: {\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      // Indirect pointer writes must use a different opcode.\n      DCHECK_NE(mode, RecordWriteMode::kValueIsIndirectPointer);\n      Register object = i.InputRegister(0);\n      Register value = i.InputRegister(2);\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, Operand(i.InputRegister(1)), value, mode,\n          DetermineStubCallMode());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ AddWord(kScratchReg, object, i.InputRegister(1));\n      __ StoreTaggedField(value, MemOperand(kScratchReg, 0));\n      if (mode > RecordWriteMode::kValueIsIndirectPointer) {\n        __ JumpIfSmi(value, ool->exit());\n      }\n      __ CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                       ne, ool->entry());\n      __ bind(ool->exit());\n      break;\n    }\n    case kArchAtomicStoreWithWriteBarrier: {\n#ifdef V8_TARGET_ARCH_RISCV64\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      // Indirect pointer writes must use a different opcode.\n      DCHECK_NE(mode, RecordWriteMode::kValueIsIndirectPointer);\n      Register object = i.InputRegister(0);\n      Register offset = i.InputRegister(1);\n      Register value = i.InputRegister(2);\n\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, Operand(offset), value, mode, DetermineStubCallMode());\n      __ AddWord(kScratchReg, object, offset);\n      __ AtomicStoreTaggedField(value, MemOperand(kScratchReg, 0));\n      // Skip the write barrier if the value is a Smi. However, this is only\n      // valid if the value isn't an indirect pointer. Otherwise the value will\n      // be a pointer table index, which will always look like a Smi (but\n      // actually reference a pointer in the pointer table).\n      if (mode > RecordWriteMode::kValueIsIndirectPointer) {\n        __ JumpIfSmi(value, ool->exit());\n      }\n      __ CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                       ne, ool->entry());\n      __ bind(ool->exit());\n      break;\n#else\n      UNREACHABLE();\n#endif\n    }\n    case kArchStoreIndirectWithWriteBarrier: {\n#ifdef V8_TARGET_ARCH_RISCV64\n      RecordWriteMode mode = RecordWriteModeField::decode(instr->opcode());\n      DCHECK_EQ(mode, RecordWriteMode::kValueIsIndirectPointer);\n      IndirectPointerTag tag = static_cast<IndirectPointerTag>(i.InputInt64(3));\n      DCHECK(IsValidIndirectPointerTag(tag));\n      Register object = i.InputRegister(0);\n      Register value = i.InputRegister(2);\n      auto ool = zone()->New<OutOfLineRecordWrite>(\n          this, object, Operand(i.InputRegister(1)), value, mode,\n          DetermineStubCallMode(), tag);\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ AddWord(kScratchReg, object, i.InputRegister(1));\n      __ StoreIndirectPointerField(value, MemOperand(kScratchReg, 0));\n      __ CheckPageFlag(object, MemoryChunk::kPointersFromHereAreInterestingMask,\n                       ne, ool->entry());\n      __ bind(ool->exit());\n      break;\n#else\n      UNREACHABLE();\n#endif\n    }\n    case kArchStackSlot: {\n      FrameOffset offset =\n          frame_access_state()->GetFrameOffset(i.InputInt32(0));\n      Register base_reg = offset.from_stack_pointer() ? sp : fp;\n      __ AddWord(i.OutputRegister(), base_reg, Operand(offset.offset()));\n      break;\n    }\n    case kIeee754Float64Acos:\n      ASSEMBLE_IEEE754_UNOP(acos);\n      break;\n    case kIeee754Float64Acosh:\n      ASSEMBLE_IEEE754_UNOP(acosh);\n      break;\n    case kIeee754Float64Asin:\n      ASSEMBLE_IEEE754_UNOP(asin);\n      break;\n    case kIeee754Float64Asinh:\n      ASSEMBLE_IEEE754_UNOP(asinh);\n      break;\n    case kIeee754Float64Atan:\n      ASSEMBLE_IEEE754_UNOP(atan);\n      break;\n    case kIeee754Float64Atanh:\n      ASSEMBLE_IEEE754_UNOP(atanh);\n      break;\n    case kIeee754Float64Atan2:\n      ASSEMBLE_IEEE754_BINOP(atan2);\n      break;\n    case kIeee754Float64Cos:\n      ASSEMBLE_IEEE754_UNOP(cos);\n      break;\n    case kIeee754Float64Cosh:\n      ASSEMBLE_IEEE754_UNOP(cosh);\n      break;\n    case kIeee754Float64Cbrt:\n      ASSEMBLE_IEEE754_UNOP(cbrt);\n      break;\n    case kIeee754Float64Exp:\n      ASSEMBLE_IEEE754_UNOP(exp);\n      break;\n    case kIeee754Float64Expm1:\n      ASSEMBLE_IEEE754_UNOP(expm1);\n      break;\n    case kIeee754Float64Log:\n      ASSEMBLE_IEEE754_UNOP(log);\n      break;\n    case kIeee754Float64Log1p:\n      ASSEMBLE_IEEE754_UNOP(log1p);\n      break;\n    case kIeee754Float64Log2:\n      ASSEMBLE_IEEE754_UNOP(log2);\n      break;\n    case kIeee754Float64Log10:\n      ASSEMBLE_IEEE754_UNOP(log10);\n      break;\n    case kIeee754Float64Pow:\n      ASSEMBLE_IEEE754_BINOP(pow);\n      break;\n    case kIeee754Float64Sin:\n      ASSEMBLE_IEEE754_UNOP(sin);\n      break;\n    case kIeee754Float64Sinh:\n      ASSEMBLE_IEEE754_UNOP(sinh);\n      break;\n    case kIeee754Float64Tan:\n      ASSEMBLE_IEEE754_UNOP(tan);\n      break;\n    case kIeee754Float64Tanh:\n      ASSEMBLE_IEEE754_UNOP(tanh);\n      break;\n    case kRiscvAdd32:\n      __ Add32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvSub32:\n      __ Sub32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvMul32:\n      __ Mul32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvMulOvf32:\n      __ MulOverflow32(i.OutputRegister(), i.InputOrZeroRegister(0),\n                       i.InputOperand(1), kScratchReg);\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdd64:\n      __ AddWord(i.OutputRegister(), i.InputOrZeroRegister(0),\n                 i.InputOperand(1));\n      break;\n    case kRiscvAddOvf64:\n      __ AddOverflow64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                       i.InputOperand(1), kScratchReg);\n      break;\n    case kRiscvSub64:\n      __ Sub64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvSubOvf64:\n      __ SubOverflow64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                       i.InputOperand(1), kScratchReg);\n      break;\n    case kRiscvMulHigh32:\n      __ Mulh32(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      break;\n    case kRiscvMulHighU32:\n      __ Mulhu32(i.OutputRegister(), i.InputOrZeroRegister(0),\n                 i.InputOperand(1), kScratchReg, kScratchReg2);\n      break;\n    case kRiscvMulHigh64:\n      __ Mulh64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      break;\n    case kRiscvMulHighU64:\n      __ Mulhu64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                 i.InputOperand(1));\n      break;\n    case kRiscvMulOvf64:\n      __ MulOverflow64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                       i.InputOperand(1), kScratchReg);\n      break;\n    case kRiscvDiv32: {\n      __ Div32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvDivU32: {\n      __ Divu32(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvMod32:\n      __ Mod32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvModU32:\n      __ Modu32(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      break;\n    case kRiscvMul64:\n      __ Mul64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvDiv64: {\n      __ Div64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvDivU64: {\n      __ Divu64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvMod64:\n      __ Mod64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvModU64:\n      __ Modu64(i.OutputRegister(), i.InputOrZeroRegister(0),\n                i.InputOperand(1));\n      break;\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvAddOvf:\n      __ AddOverflow(i.OutputRegister(), i.InputOrZeroRegister(0),\n                     i.InputOperand(1), kScratchReg);\n      break;\n    case kRiscvSubOvf:\n      __ SubOverflow(i.OutputRegister(), i.InputOrZeroRegister(0),\n                     i.InputOperand(1), kScratchReg);\n      break;\n    case kRiscvMulHigh32:\n      __ Mulh(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvMulHighU32:\n      __ Mulhu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1),\n               kScratchReg, kScratchReg2);\n      break;\n    case kRiscvDiv32: {\n      __ Div(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvDivU32: {\n      __ Divu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      // Set ouput to zero if divisor == 0\n      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));\n      break;\n    }\n    case kRiscvMod32:\n      __ Mod(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvModU32:\n      __ Modu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n#endif\n    case kRiscvAnd:\n      __ And(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvAnd32:\n      __ And(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      __ Sll32(i.OutputRegister(), i.OutputRegister(), 0x0);\n      break;\n    case kRiscvOr:\n      __ Or(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvOr32:\n      __ Or(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      __ Sll32(i.OutputRegister(), i.OutputRegister(), 0x0);\n      break;\n    case kRiscvNor:\n      if (instr->InputAt(1)->IsRegister()) {\n        __ Nor(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      } else {\n        DCHECK_EQ(0, i.InputOperand(1).immediate());\n        __ Nor(i.OutputRegister(), i.InputOrZeroRegister(0), zero_reg);\n      }\n      break;\n    case kRiscvNor32:\n      if (instr->InputAt(1)->IsRegister()) {\n        __ Nor(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n        __ Sll32(i.OutputRegister(), i.OutputRegister(), 0x0);\n      } else {\n        DCHECK_EQ(0, i.InputOperand(1).immediate());\n        __ Nor(i.OutputRegister(), i.InputOrZeroRegister(0), zero_reg);\n        __ Sll32(i.OutputRegister(), i.OutputRegister(), 0x0);\n      }\n      break;\n    case kRiscvXor:\n      __ Xor(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvXor32:\n      __ Xor(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));\n      __ Sll32(i.OutputRegister(), i.OutputRegister(), 0x0);\n      break;\n    case kRiscvClz32:\n      __ Clz32(i.OutputRegister(), i.InputOrZeroRegister(0));\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClz64:\n      __ Clz64(i.OutputRegister(), i.InputOrZeroRegister(0));\n      break;\n    case kRiscvCtz64: {\n      Register src = i.InputRegister(0);\n      Register dst = i.OutputRegister();\n      __ Ctz64(dst, src);\n    } break;\n    case kRiscvPopcnt64: {\n      Register src = i.InputRegister(0);\n      Register dst = i.OutputRegister();\n      __ Popcnt64(dst, src, kScratchReg);\n    } break;\n#endif\n    case kRiscvCtz32: {\n      Register src = i.InputRegister(0);\n      Register dst = i.OutputRegister();\n      __ Ctz32(dst, src);\n    } break;\n    case kRiscvPopcnt32: {\n      Register src = i.InputRegister(0);\n      Register dst = i.OutputRegister();\n      __ Popcnt32(dst, src, kScratchReg);\n    } break;\n    case kRiscvShl32:\n      if (instr->InputAt(1)->IsRegister()) {\n        __ Sll32(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      } else {\n        int64_t imm = i.InputOperand(1).immediate();\n        __ Sll32(i.OutputRegister(), i.InputRegister(0),\n                 static_cast<uint16_t>(imm));\n      }\n      break;\n    case kRiscvShr32:\n      if (instr->InputAt(1)->IsRegister()) {\n        __ Srl32(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      } else {\n        int64_t imm = i.InputOperand(1).immediate();\n        __ Srl32(i.OutputRegister(), i.InputRegister(0),\n                 static_cast<uint16_t>(imm));\n      }\n      break;\n    case kRiscvSar32:\n      if (instr->InputAt(1)->IsRegister()) {\n        __ Sra32(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      } else {\n        int64_t imm = i.InputOperand(1).immediate();\n        __ Sra32(i.OutputRegister(), i.InputRegister(0),\n                 static_cast<uint16_t>(imm));\n      }\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvZeroExtendWord: {\n      __ ZeroExtendWord(i.OutputRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvSignExtendWord: {\n      __ SignExtendWord(i.OutputRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvShl64:\n      __ Sll64(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvShr64:\n      __ Srl64(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvSar64:\n      __ Sra64(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvRor64:\n      __ Dror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvTst64:\n      __ And(kScratchReg, i.InputRegister(0), i.InputOperand(1));\n      // Pseudo-instruction used for cmp/branch. No opcode emitted here.\n      break;\n#endif\n    case kRiscvRev8:\n      __ rev8(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvAndn:\n      __ andn(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvOrn:\n      __ orn(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvXnor:\n      __ xnor(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvClz:\n      __ clz(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvCtz:\n      __ ctz(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvCpop:\n      __ cpop(i.OutputRegister(), i.InputRegister(0));\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClzw:\n      __ clzw(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvCtzw:\n      __ ctzw(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvCpopw:\n      __ cpopw(i.OutputRegister(), i.InputRegister(0));\n      break;\n#endif\n    case kRiscvMax:\n      __ max(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvMaxu:\n      __ maxu(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvMin:\n      __ min(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvMinu:\n      __ minu(i.OutputRegister(), i.InputRegister(0), i.InputRegister(1));\n      break;\n    case kRiscvSextb:\n      __ sextb(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvSexth:\n      __ sexth(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvZexth:\n      __ zexth(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvTst32:\n      __ And(kScratchReg, i.InputRegister(0), i.InputOperand(1));\n      __ Sll32(kScratchReg, kScratchReg, 0x0);\n      // Pseudo-instruction used for cmp/branch. No opcode emitted here.\n      break;\n    case kRiscvRor32:\n      __ Ror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));\n      break;\n    case kRiscvCmp:\n#ifdef V8_TARGET_ARCH_RISCV64\n    case kRiscvCmp32:\n    case kRiscvCmpZero32:\n#endif\n      // Pseudo-instruction used for cmp/branch. No opcode emitted here.\n      break;\n    case kRiscvCmpZero:\n      // Pseudo-instruction used for cmpzero/branch. No opcode emitted here.\n      break;\n    case kRiscvMov:\n      // TODO(plind): Should we combine mov/li like this, or use separate instr?\n      //    - Also see x64 ASSEMBLE_BINOP & RegisterOrOperandType\n      if (HasRegisterInput(instr, 0)) {\n        __ Move(i.OutputRegister(), i.InputRegister(0));\n      } else {\n        __ li(i.OutputRegister(), i.InputOperand(0));\n      }\n      break;\n\n    case kRiscvCmpS: {\n      FPURegister left = i.InputOrZeroSingleRegister(0);\n      FPURegister right = i.InputOrZeroSingleRegister(1);\n      bool predicate;\n      FPUCondition cc =\n          FlagsConditionToConditionCmpFPU(&predicate, instr->flags_condition());\n\n      if ((left == kSingleRegZero || right == kSingleRegZero) &&\n          !__ IsSingleZeroRegSet()) {\n        __ LoadFPRImmediate(kSingleRegZero, 0.0f);\n      }\n      // compare result set to kScratchReg\n      __ CompareF32(kScratchReg, cc, left, right);\n    } break;\n    case kRiscvAddS:\n      // TODO(plind): add special case: combine mult & add.\n      __ fadd_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvSubS:\n      __ fsub_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvMulS:\n      // TODO(plind): add special case: right op is -1.0, see arm port.\n      __ fmul_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvDivS:\n      __ fdiv_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvModS: {\n      // TODO(bmeurer): We should really get rid of this special instruction,\n      // and generate a CallAddress instruction instead.\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ PrepareCallCFunction(0, 2, kScratchReg);\n      __ MovToFloatParameters(i.InputDoubleRegister(0),\n                              i.InputDoubleRegister(1));\n      // TODO(balazs.kilvady): implement mod_two_floats_operation(isolate())\n      __ CallCFunction(ExternalReference::mod_two_doubles_operation(), 0, 2);\n      // Move the result in the double result register.\n      __ MovFromFloatResult(i.OutputSingleRegister());\n      break;\n    }\n    case kRiscvAbsS:\n      __ fabs_s(i.OutputSingleRegister(), i.InputSingleRegister(0));\n      break;\n    case kRiscvNegS:\n      __ Neg_s(i.OutputSingleRegister(), i.InputSingleRegister(0));\n      break;\n    case kRiscvSqrtS: {\n      __ fsqrt_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    }\n    case kRiscvMaxS:\n      __ fmax_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvMinS:\n      __ fmin_s(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvCmpD: {\n      FPURegister left = i.InputOrZeroDoubleRegister(0);\n      FPURegister right = i.InputOrZeroDoubleRegister(1);\n      bool predicate;\n      FPUCondition cc =\n          FlagsConditionToConditionCmpFPU(&predicate, instr->flags_condition());\n      if ((left == kDoubleRegZero || right == kDoubleRegZero) &&\n          !__ IsDoubleZeroRegSet()) {\n        __ LoadFPRImmediate(kDoubleRegZero, 0.0);\n      }\n      // compare result set to kScratchReg\n      __ CompareF64(kScratchReg, cc, left, right);\n    } break;\n#if V8_TARGET_ARCH_RISCV32\n    case kRiscvAddPair:\n      __ AddPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),\n                 kScratchReg, kScratchReg2);\n      break;\n    case kRiscvSubPair:\n      __ SubPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),\n                 kScratchReg, kScratchReg2);\n      break;\n    case kRiscvAndPair:\n      __ AndPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));\n      break;\n    case kRiscvOrPair:\n      __ OrPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));\n      break;\n    case kRiscvXorPair:\n      __ XorPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));\n      break;\n    case kRiscvMulPair:\n      __ MulPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),\n                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),\n                 kScratchReg, kScratchReg2);\n      break;\n    case kRiscvShlPair: {\n      Register second_output =\n          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);\n      if (instr->InputAt(2)->IsRegister()) {\n        __ ShlPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), i.InputRegister(2), kScratchReg,\n                   kScratchReg2);\n      } else {\n        uint32_t imm = i.InputOperand(2).immediate();\n        __ ShlPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);\n      }\n    } break;\n    case kRiscvShrPair: {\n      Register second_output =\n          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);\n      if (instr->InputAt(2)->IsRegister()) {\n        __ ShrPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), i.InputRegister(2), kScratchReg,\n                   kScratchReg2);\n      } else {\n        uint32_t imm = i.InputOperand(2).immediate();\n        __ ShrPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);\n      }\n    } break;\n    case kRiscvSarPair: {\n      Register second_output =\n          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);\n      if (instr->InputAt(2)->IsRegister()) {\n        __ SarPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), i.InputRegister(2), kScratchReg,\n                   kScratchReg2);\n      } else {\n        uint32_t imm = i.InputOperand(2).immediate();\n        __ SarPair(i.OutputRegister(0), second_output, i.InputRegister(0),\n                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);\n      }\n    } break;\n#endif\n    case kRiscvAddD:\n      // TODO(plind): add special case: combine mult & add.\n      __ fadd_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvSubD:\n      __ fsub_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvMulD:\n      // TODO(plind): add special case: right op is -1.0, see arm port.\n      __ fmul_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvDivD:\n      __ fdiv_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvModD: {\n      // TODO(bmeurer): We should really get rid of this special instruction,\n      // and generate a CallAddress instruction instead.\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ PrepareCallCFunction(0, 2, kScratchReg);\n      __ MovToFloatParameters(i.InputDoubleRegister(0),\n                              i.InputDoubleRegister(1));\n      __ CallCFunction(ExternalReference::mod_two_doubles_operation(), 0, 2);\n      // Move the result in the double result register.\n      __ MovFromFloatResult(i.OutputDoubleRegister());\n      break;\n    }\n    case kRiscvAbsD:\n      __ fabs_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvNegD:\n      __ Neg_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvSqrtD: {\n      __ fsqrt_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    }\n    case kRiscvMaxD:\n      __ fmax_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n    case kRiscvMinD:\n      __ fmin_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                i.InputDoubleRegister(1));\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvFloat64RoundDown: {\n      __ Floor_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat64RoundTruncate: {\n      __ Trunc_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat64RoundUp: {\n      __ Ceil_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                  kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat64RoundTiesEven: {\n      __ Round_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n#endif\n    case kRiscvFloat32RoundDown: {\n      __ Floor_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat32RoundTruncate: {\n      __ Trunc_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat32RoundUp: {\n      __ Ceil_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                  kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat32RoundTiesEven: {\n      __ Round_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                   kScratchDoubleReg);\n      break;\n    }\n    case kRiscvFloat32Max: {\n      __ Float32Max(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                    i.InputSingleRegister(1));\n      break;\n    }\n    case kRiscvFloat64Max: {\n      __ Float64Max(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                    i.InputSingleRegister(1));\n      break;\n    }\n    case kRiscvFloat32Min: {\n      __ Float32Min(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                    i.InputSingleRegister(1));\n      break;\n    }\n    case kRiscvFloat64Min: {\n      __ Float64Min(i.OutputSingleRegister(), i.InputSingleRegister(0),\n                    i.InputSingleRegister(1));\n      break;\n    }\n    case kRiscvFloat64SilenceNaN:\n      __ FPUCanonicalizeNaN(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvCvtSD: {\n      Label done;\n      __ feq_d(kScratchReg, i.InputDoubleRegister(0), i.InputDoubleRegister(0));\n#if V8_TARGET_ARCH_RISCV64\n      __ fmv_x_d(kScratchReg2, i.InputDoubleRegister(0));\n#elif V8_TARGET_ARCH_RISCV32\n      __ StoreDouble(i.InputDoubleRegister(0),\n                     MemOperand(sp, -kDoubleSize));  // store whole 64 bit\n#endif\n      __ fcvt_s_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0));\n      __ Branch(&done, ne, kScratchReg, Operand(zero_reg));\n#if V8_TARGET_ARCH_RISCV64\n      __ And(kScratchReg2, kScratchReg2, Operand(0x8000000000000000));\n      __ srai(kScratchReg2, kScratchReg2, 32);\n      __ fmv_d_x(kScratchDoubleReg, kScratchReg2);\n#elif V8_TARGET_ARCH_RISCV32\n      __ Lw(kScratchReg2,\n            MemOperand(sp,\n                       -kDoubleSize /\n                           2));  // only load the high half to get the sign bit\n      __ fmv_w_x(kScratchDoubleReg, kScratchReg2);\n#endif\n      __ fsgnj_s(i.OutputDoubleRegister(), i.OutputDoubleRegister(),\n                 kScratchDoubleReg);\n      __ bind(&done);\n      break;\n    }\n    case kRiscvCvtDS: {\n      Label done;\n      __ feq_s(kScratchReg, i.InputDoubleRegister(0), i.InputDoubleRegister(0));\n#if V8_TARGET_ARCH_RISCV64\n      __ fmv_x_d(kScratchReg2, i.InputDoubleRegister(0));\n#elif V8_TARGET_ARCH_RISCV32\n      __ StoreFloat(i.InputDoubleRegister(0), MemOperand(sp, -kFloatSize));\n#endif\n      __ fcvt_d_s(i.OutputDoubleRegister(), i.InputSingleRegister(0));\n      __ Branch(&done, ne, kScratchReg, Operand(zero_reg));\n#if V8_TARGET_ARCH_RISCV64\n      __ And(kScratchReg2, kScratchReg2, Operand(0x80000000));\n      __ slli(kScratchReg2, kScratchReg2, 32);\n      __ fmv_d_x(kScratchDoubleReg, kScratchReg2);\n#elif V8_TARGET_ARCH_RISCV32\n      __ Lw(kScratchReg2, MemOperand(sp, -kFloatSize));\n      __ fcvt_d_w(kScratchDoubleReg, kScratchReg2);\n#endif\n      __ fsgnj_d(i.OutputDoubleRegister(), i.OutputDoubleRegister(),\n                 kScratchDoubleReg);\n      __ bind(&done);\n      break;\n    }\n    case kRiscvCvtDW: {\n      __ fcvt_d_w(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvCvtSW: {\n      __ fcvt_s_w(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvCvtSUw: {\n      __ Cvt_s_uw(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvCvtSL: {\n      __ fcvt_s_l(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvCvtDL: {\n      __ fcvt_d_l(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvCvtDUl: {\n      __ Cvt_d_ul(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvCvtSUl: {\n      __ Cvt_s_ul(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n#endif\n    case kRiscvCvtDUw: {\n      __ Cvt_d_uw(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    }\n    case kRiscvFloorWD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Floor_w_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvCeilWD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Ceil_w_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvRoundWD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Round_w_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvTruncWD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Trunc_w_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvFloorWS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Floor_w_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvCeilWS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Ceil_w_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvRoundWS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Round_w_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvTruncWS: {\n      Label done;\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      bool set_overflow_to_min_i32 = MiscField::decode(instr->opcode());\n      __ Trunc_w_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n\n      // On RISCV, if the input value exceeds INT32_MAX, the result of fcvt\n      // is INT32_MAX. Note that, since INT32_MAX means the lower 31-bits are\n      // all 1s, INT32_MAX cannot be represented precisely as a float, so an\n      // fcvt result of INT32_MAX always indicate overflow.\n      //\n      // In wasm_compiler, to detect overflow in converting a FP value, fval, to\n      // integer, V8 checks whether I2F(F2I(fval)) equals fval. However, if fval\n      // == INT32_MAX+1, the value of I2F(F2I(fval)) happens to be fval. So,\n      // INT32_MAX is not a good value to indicate overflow. Instead, we will\n      // use INT32_MIN as the converted result of an out-of-range FP value,\n      // exploiting the fact that INT32_MAX+1 is INT32_MIN.\n      //\n      // If the result of conversion overflow, the result will be set to\n      // INT32_MIN. Here we detect overflow by testing whether output + 1 <\n      // output (i.e., kScratchReg  < output)\n      if (set_overflow_to_min_i32) {\n        __ Add32(kScratchReg, i.OutputRegister(), 1);\n        __ BranchShort(&done, lt, i.OutputRegister(), Operand(kScratchReg));\n        __ Move(i.OutputRegister(), kScratchReg);\n        __ bind(&done);\n      }\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvTruncLS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Trunc_l_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvTruncLD: {\n      Label done;\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      bool set_overflow_to_min_i64 = MiscField::decode(instr->opcode());\n      __ Trunc_l_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      if (set_overflow_to_min_i64) {\n        __ AddWord(kScratchReg, i.OutputRegister(), 1);\n        __ BranchShort(&done, lt, i.OutputRegister(), Operand(kScratchReg));\n        __ Move(i.OutputRegister(), kScratchReg);\n        __ bind(&done);\n      }\n      break;\n    }\n#endif\n    case kRiscvTruncUwD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Trunc_uw_d(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvTruncUwS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      bool set_overflow_to_min_u32 = MiscField::decode(instr->opcode());\n      __ Trunc_uw_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n\n      // On RISCV, if the input value exceeds UINT32_MAX, the result of fcvt\n      // is UINT32_MAX. Note that, since UINT32_MAX means all 32-bits are 1s,\n      // UINT32_MAX cannot be represented precisely as float, so an fcvt result\n      // of UINT32_MAX always indicates overflow.\n      //\n      // In wasm_compiler.cc, to detect overflow in converting a FP value, fval,\n      // to integer, V8 checks whether I2F(F2I(fval)) equals fval. However, if\n      // fval == UINT32_MAX+1, the value of I2F(F2I(fval)) happens to be fval.\n      // So, UINT32_MAX is not a good value to indicate overflow. Instead, we\n      // will use 0 as the converted result of an out-of-range FP value,\n      // exploiting the fact that UINT32_MAX+1 is 0.\n      if (set_overflow_to_min_u32) {\n        __ Add32(kScratchReg, i.OutputRegister(), 1);\n        // Set ouput to zero if result overflows (i.e., UINT32_MAX)\n        __ LoadZeroIfConditionZero(i.OutputRegister(), kScratchReg);\n      }\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvTruncUlS: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Trunc_ul_s(i.OutputRegister(), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvTruncUlD: {\n      Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;\n      __ Trunc_ul_d(i.OutputRegister(0), i.InputDoubleRegister(0), result);\n      break;\n    }\n    case kRiscvBitcastDL:\n      __ fmv_x_d(i.OutputRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvBitcastLD:\n      __ fmv_d_x(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n#endif\n    case kRiscvBitcastInt32ToFloat32:\n      __ fmv_w_x(i.OutputDoubleRegister(), i.InputRegister(0));\n      break;\n    case kRiscvBitcastFloat32ToInt32:\n      __ fmv_x_w(i.OutputRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvFloat64ExtractLowWord32:\n      __ ExtractLowWordFromF64(i.OutputRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvFloat64ExtractHighWord32:\n      __ ExtractHighWordFromF64(i.OutputRegister(), i.InputDoubleRegister(0));\n      break;\n    case kRiscvFloat64InsertLowWord32:\n      __ InsertLowWordF64(i.OutputDoubleRegister(), i.InputRegister(1));\n      break;\n    case kRiscvFloat64InsertHighWord32:\n      __ InsertHighWordF64(i.OutputDoubleRegister(), i.InputRegister(1));\n      break;\n      // ... more basic instructions ...\n\n    case kRiscvSignExtendByte:\n      __ SignExtendByte(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvSignExtendShort:\n      __ SignExtendShort(i.OutputRegister(), i.InputRegister(0));\n      break;\n    case kRiscvLbu:\n      __ Lbu(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvLb:\n      __ Lb(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvSb:\n      __ Sb(i.InputOrZeroRegister(0), i.MemoryOperand(1));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvLhu:\n      __ Lhu(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUlhu:\n      __ Ulhu(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvLh:\n      __ Lh(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUlh:\n      __ Ulh(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvSh:\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ Sh(i.InputOrZeroRegister(0), i.MemoryOperand(1));\n      break;\n    case kRiscvUsh:\n      __ Ush(i.InputOrZeroRegister(2), i.MemoryOperand());\n      break;\n    case kRiscvLw:\n      __ Lw(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUlw:\n      __ Ulw(i.OutputRegister(), i.MemoryOperand());\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvLwu:\n      __ Lwu(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUlwu:\n      __ Ulwu(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvLd:\n      __ Ld(i.OutputRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUld:\n      __ Uld(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvSd:\n      __ Sd(i.InputOrZeroRegister(0), i.MemoryOperand(1));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUsd:\n      __ Usd(i.InputOrZeroRegister(2), i.MemoryOperand());\n      break;\n#endif\n    case kRiscvSw:\n      __ Sw(i.InputOrZeroRegister(0), i.MemoryOperand(1));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvUsw:\n      __ Usw(i.InputOrZeroRegister(2), i.MemoryOperand());\n      break;\n    case kRiscvLoadFloat: {\n      __ LoadFloat(i.OutputSingleRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    }\n    case kRiscvULoadFloat: {\n      __ ULoadFloat(i.OutputSingleRegister(), i.MemoryOperand(), kScratchReg);\n      break;\n    }\n    case kRiscvStoreFloat: {\n      MemOperand operand = i.MemoryOperand(1);\n      FPURegister ft = i.InputOrZeroSingleRegister(0);\n      if (ft == kSingleRegZero && !__ IsSingleZeroRegSet()) {\n        __ LoadFPRImmediate(kSingleRegZero, 0.0f);\n      }\n      __ StoreFloat(ft, operand);\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    }\n    case kRiscvUStoreFloat: {\n      size_t index = 0;\n      MemOperand operand = i.MemoryOperand(&index);\n      FPURegister ft = i.InputOrZeroSingleRegister(index);\n      if (ft == kSingleRegZero && !__ IsSingleZeroRegSet()) {\n        __ LoadFPRImmediate(kSingleRegZero, 0.0f);\n      }\n      __ UStoreFloat(ft, operand, kScratchReg);\n      break;\n    }\n    case kRiscvLoadDouble:\n      __ LoadDouble(i.OutputDoubleRegister(), i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    case kRiscvULoadDouble:\n      __ ULoadDouble(i.OutputDoubleRegister(), i.MemoryOperand(), kScratchReg);\n      break;\n    case kRiscvStoreDouble: {\n      FPURegister ft = i.InputOrZeroDoubleRegister(0);\n      if (ft == kDoubleRegZero && !__ IsDoubleZeroRegSet()) {\n        __ LoadFPRImmediate(kDoubleRegZero, 0.0);\n      }\n      __ StoreDouble(ft, i.MemoryOperand(1));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    }\n    case kRiscvUStoreDouble: {\n      FPURegister ft = i.InputOrZeroDoubleRegister(2);\n      if (ft == kDoubleRegZero && !__ IsDoubleZeroRegSet()) {\n        __ LoadFPRImmediate(kDoubleRegZero, 0.0);\n      }\n      __ UStoreDouble(ft, i.MemoryOperand(), kScratchReg);\n      break;\n    }\n    case kRiscvSync: {\n      __ sync();\n      break;\n    }\n    case kRiscvPush:\n      if (instr->InputAt(0)->IsFPRegister()) {\n        __ StoreDouble(i.InputDoubleRegister(0), MemOperand(sp, -kDoubleSize));\n        __ Sub32(sp, sp, Operand(kDoubleSize));\n        frame_access_state()->IncreaseSPDelta(kDoubleSize / kSystemPointerSize);\n      } else {\n        __ Push(i.InputOrZeroRegister(0));\n        frame_access_state()->IncreaseSPDelta(1);\n      }\n      break;\n    case kRiscvPeek: {\n      int reverse_slot = i.InputInt32(0);\n      int offset =\n          FrameSlotToFPOffset(frame()->GetTotalFrameSlotCount() - reverse_slot);\n      if (instr->OutputAt(0)->IsFPRegister()) {\n        LocationOperand* op = LocationOperand::cast(instr->OutputAt(0));\n        if (op->representation() == MachineRepresentation::kFloat64) {\n          __ LoadDouble(i.OutputDoubleRegister(), MemOperand(fp, offset));\n        } else {\n          DCHECK_EQ(op->representation(), MachineRepresentation::kFloat32);\n          __ LoadFloat(\n              i.OutputSingleRegister(0),\n              MemOperand(fp, offset + kLessSignificantWordInDoublewordOffset));\n        }\n      } else {\n        __ LoadWord(i.OutputRegister(0), MemOperand(fp, offset));\n      }\n      break;\n    }\n    case kRiscvStackClaim: {\n      __ SubWord(sp, sp, Operand(i.InputInt32(0)));\n      frame_access_state()->IncreaseSPDelta(i.InputInt32(0) /\n                                            kSystemPointerSize);\n      break;\n    }\n    case kRiscvStoreToStackSlot: {\n      if (instr->InputAt(0)->IsFPRegister()) {\n        if (instr->InputAt(0)->IsSimd128Register()) {\n          Register dst = sp;\n          if (i.InputInt32(1) != 0) {\n            dst = kScratchReg2;\n            __ AddWord(kScratchReg2, sp, Operand(i.InputInt32(1)));\n          }\n          __ VU.set(kScratchReg, E8, m1);\n          __ vs(i.InputSimd128Register(0), dst, 0, E8);\n        } else {\n#if V8_TARGET_ARCH_RISCV64\n          __ StoreDouble(i.InputDoubleRegister(0),\n                         MemOperand(sp, i.InputInt32(1)));\n#elif V8_TARGET_ARCH_RISCV32\n          if (instr->InputAt(0)->IsDoubleRegister()) {\n            __ StoreDouble(i.InputDoubleRegister(0),\n                           MemOperand(sp, i.InputInt32(1)));\n          } else if (instr->InputAt(0)->IsFloatRegister()) {\n            __ StoreFloat(i.InputSingleRegister(0),\n                          MemOperand(sp, i.InputInt32(1)));\n          }\n#endif\n        }\n      } else {\n        __ StoreWord(i.InputOrZeroRegister(0), MemOperand(sp, i.InputInt32(1)));\n      }\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvByteSwap64: {\n      __ ByteSwap(i.OutputRegister(0), i.InputRegister(0), 8, kScratchReg);\n      break;\n    }\n#endif\n    case kRiscvByteSwap32: {\n      __ ByteSwap(i.OutputRegister(0), i.InputRegister(0), 4, kScratchReg);\n      break;\n    }\n    case kAtomicLoadInt8:\n#if V8_TARGET_ARCH_RISCV64\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n#endif\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Lb);\n      break;\n    case kAtomicLoadUint8:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Lbu);\n      break;\n    case kAtomicLoadInt16:\n#if V8_TARGET_ARCH_RISCV64\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n#endif\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Lh);\n      break;\n    case kAtomicLoadUint16:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Lhu);\n      break;\n    case kAtomicLoadWord32:\n#if V8_TARGET_ARCH_RISCV64\n      if (AtomicWidthField::decode(opcode) == AtomicWidth::kWord64) {\n        ASSEMBLE_ATOMIC_LOAD_INTEGER(Lwu);\n        break;\n      }\n#endif  // V8_TARGET_ARCH_RISCV64\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Lw);\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvWord64AtomicLoadUint64:\n      ASSEMBLE_ATOMIC_LOAD_INTEGER(Ld);\n      break;\n    case kRiscvWord64AtomicStoreWord64:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Sd);\n      break;\n#endif\n    case kAtomicStoreWord8:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Sb);\n      break;\n    case kAtomicStoreWord16:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Sh);\n      break;\n    case kAtomicStoreWord32:\n      ASSEMBLE_ATOMIC_STORE_INTEGER(Sw);\n      break;\n#if V8_TARGET_ARCH_RISCV32\n    case kRiscvWord32AtomicPairLoad: {\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));\n      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      __ PrepareCallCFunction(1, 0, kScratchReg);\n      __ CallCFunction(ExternalReference::atomic_pair_load_function(), 1, 0);\n      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      break;\n    }\n    case kRiscvWord32AtomicPairStore: {\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));\n      __ PushCallerSaved(SaveFPRegsMode::kIgnore);\n      __ PrepareCallCFunction(3, 0, kScratchReg);\n      __ CallCFunction(ExternalReference::atomic_pair_store_function(), 3, 0);\n      __ PopCallerSaved(SaveFPRegsMode::kIgnore);\n      break;\n    }\n#define ATOMIC64_BINOP_ARITH_CASE(op, instr, external) \\\n  case kRiscvWord32AtomicPair##op:                     \\\n    ASSEMBLE_ATOMIC64_ARITH_BINOP(instr, external);    \\\n    break;\n      ATOMIC64_BINOP_ARITH_CASE(Add, AddPair, atomic_pair_add_function)\n      ATOMIC64_BINOP_ARITH_CASE(Sub, SubPair, atomic_pair_sub_function)\n#undef ATOMIC64_BINOP_ARITH_CASE\n#define ATOMIC64_BINOP_LOGIC_CASE(op, instr, external) \\\n  case kRiscvWord32AtomicPair##op:                     \\\n    ASSEMBLE_ATOMIC64_LOGIC_BINOP(instr, external);    \\\n    break;\n      ATOMIC64_BINOP_LOGIC_CASE(And, AndPair, atomic_pair_and_function)\n      ATOMIC64_BINOP_LOGIC_CASE(Or, OrPair, atomic_pair_or_function)\n      ATOMIC64_BINOP_LOGIC_CASE(Xor, XorPair, atomic_pair_xor_function)\n    case kRiscvWord32AtomicPairExchange: {\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      __ PrepareCallCFunction(3, 0, kScratchReg);\n      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));\n      __ CallCFunction(ExternalReference::atomic_pair_exchange_function(), 3,\n                       0);\n      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      break;\n    }\n    case kRiscvWord32AtomicPairCompareExchange: {\n      FrameScope scope(masm(), StackFrame::MANUAL);\n      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      __ PrepareCallCFunction(5, 0, kScratchReg);\n      __ add(a0, i.InputRegister(0), i.InputRegister(1));\n      __ CallCFunction(\n          ExternalReference::atomic_pair_compare_exchange_function(), 5, 0);\n      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);\n      break;\n    }\n#endif\n    case kAtomicExchangeInt8:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 8, 32);\n      break;\n    case kAtomicExchangeUint8:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 8, 32);\n          break;\n        case AtomicWidth::kWord64:\n#if V8_TARGET_ARCH_RISCV64\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 8, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n    case kAtomicExchangeInt16:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 16, 32);\n      break;\n    case kAtomicExchangeUint16:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 16, 32);\n          break;\n#if V8_TARGET_ARCH_RISCV64\n        case AtomicWidth::kWord64:\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 16, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n    case kAtomicExchangeWord32:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(Ll, Sc);\n          break;\n#if V8_TARGET_ARCH_RISCV64\n        case AtomicWidth::kWord64:\n          ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 32, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvWord64AtomicExchangeUint64:\n      ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(Lld, Scd);\n      break;\n#endif\n    case kAtomicCompareExchangeInt8:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 8, 32);\n      break;\n    case kAtomicCompareExchangeUint8:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 8, 32);\n          break;\n#if V8_TARGET_ARCH_RISCV64\n        case AtomicWidth::kWord64:\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 8, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n    case kAtomicCompareExchangeInt16:\n      DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 16, 32);\n      break;\n    case kAtomicCompareExchangeUint16:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 16, 32);\n          break;\n#if V8_TARGET_ARCH_RISCV64\n        case AtomicWidth::kWord64:\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 16, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n    case kAtomicCompareExchangeWord32:\n      switch (AtomicWidthField::decode(opcode)) {\n        case AtomicWidth::kWord32:\n          __ Sll32(i.InputRegister(2), i.InputRegister(2), 0);\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Ll, Sc);\n          break;\n#if V8_TARGET_ARCH_RISCV64\n        case AtomicWidth::kWord64:\n          ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 32, 64);\n          break;\n#endif\n        default:\n          UNREACHABLE();\n      }\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvWord64AtomicCompareExchangeUint64:\n      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Lld, Scd);\n      break;\n#define ATOMIC_BINOP_CASE(op, inst32, inst64)                          \\\n  case kAtomic##op##Int8:                                              \\\n    DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32); \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 8, inst32, 32);            \\\n    break;                                                             \\\n  case kAtomic##op##Uint8:                                             \\\n    switch (AtomicWidthField::decode(opcode)) {                        \\\n      case AtomicWidth::kWord32:                                       \\\n        ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 8, inst32, 32);       \\\n        break;                                                         \\\n      case AtomicWidth::kWord64:                                       \\\n        ASSEMBLE_ATOMIC_BINOP_EXT(Lld, Scd, false, 8, inst64, 64);     \\\n        break;                                                         \\\n    }                                                                  \\\n    break;                                                             \\\n  case kAtomic##op##Int16:                                             \\\n    DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32); \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 16, inst32, 32);           \\\n    break;                                                             \\\n  case kAtomic##op##Uint16:                                            \\\n    switch (AtomicWidthField::decode(opcode)) {                        \\\n      case AtomicWidth::kWord32:                                       \\\n        ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 16, inst32, 32);      \\\n        break;                                                         \\\n      case AtomicWidth::kWord64:                                       \\\n        ASSEMBLE_ATOMIC_BINOP_EXT(Lld, Scd, false, 16, inst64, 64);    \\\n        break;                                                         \\\n    }                                                                  \\\n    break;                                                             \\\n  case kAtomic##op##Word32:                                            \\\n    switch (AtomicWidthField::decode(opcode)) {                        \\\n      case AtomicWidth::kWord32:                                       \\\n        ASSEMBLE_ATOMIC_BINOP(Ll, Sc, inst32);                         \\\n        break;                                                         \\\n      case AtomicWidth::kWord64:                                       \\\n        ASSEMBLE_ATOMIC_BINOP_EXT(Lld, Scd, false, 32, inst64, 64);    \\\n        break;                                                         \\\n    }                                                                  \\\n    break;                                                             \\\n  case kRiscvWord64Atomic##op##Uint64:                                 \\\n    ASSEMBLE_ATOMIC_BINOP(Lld, Scd, inst64);                           \\\n    break;\n      ATOMIC_BINOP_CASE(Add, Add32, AddWord)\n      ATOMIC_BINOP_CASE(Sub, Sub32, Sub64)\n      ATOMIC_BINOP_CASE(And, And, And)\n      ATOMIC_BINOP_CASE(Or, Or, Or)\n      ATOMIC_BINOP_CASE(Xor, Xor, Xor)\n#undef ATOMIC_BINOP_CASE\n#elif V8_TARGET_ARCH_RISCV32\n#define ATOMIC_BINOP_CASE(op, inst32, inst64, amoinst32)                   \\\n  case kAtomic##op##Int8:                                                  \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 8, inst32, 32);                \\\n    break;                                                                 \\\n  case kAtomic##op##Uint8:                                                 \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 8, inst32, 32);               \\\n    break;                                                                 \\\n  case kAtomic##op##Int16:                                                 \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 16, inst32, 32);               \\\n    break;                                                                 \\\n  case kAtomic##op##Uint16:                                                \\\n    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 16, inst32, 32);              \\\n    break;                                                                 \\\n  case kAtomic##op##Word32:                                                \\\n    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1)); \\\n    __ amoinst32(true, true, i.OutputRegister(0), i.TempRegister(0),       \\\n                 i.InputRegister(2));                                      \\\n    break;\n      ATOMIC_BINOP_CASE(Add, Add32, Add64, amoadd_w)  // todo: delete 64\n      ATOMIC_BINOP_CASE(Sub, Sub32, Sub64, Amosub_w)  // todo: delete 64\n      ATOMIC_BINOP_CASE(And, And, And, amoand_w)\n      ATOMIC_BINOP_CASE(Or, Or, Or, amoor_w)\n      ATOMIC_BINOP_CASE(Xor, Xor, Xor, amoxor_w)\n#undef ATOMIC_BINOP_CASE\n#endif\n    case kRiscvAssertEqual:\n      __ Assert(eq, static_cast<AbortReason>(i.InputOperand(2).immediate()),\n                i.InputRegister(0), Operand(i.InputRegister(1)));\n      break;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvStoreCompressTagged: {\n      MemOperand mem = i.MemoryOperand(1);\n      __ StoreTaggedField(i.InputOrZeroRegister(0), mem);\n      break;\n    }\n    case kRiscvLoadDecompressTaggedSigned: {\n      CHECK(instr->HasOutput());\n      Register result = i.OutputRegister();\n      MemOperand operand = i.MemoryOperand();\n      __ DecompressTaggedSigned(result, operand);\n      break;\n    }\n    case kRiscvLoadDecompressTagged: {\n      CHECK(instr->HasOutput());\n      Register result = i.OutputRegister();\n      MemOperand operand = i.MemoryOperand();\n      __ DecompressTagged(result, operand);\n      break;\n    }\n    case kRiscvLoadDecodeSandboxedPointer:\n      __ LoadSandboxedPointerField(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvStoreEncodeSandboxedPointer: {\n      MemOperand mem = i.MemoryOperand(1);\n      __ StoreSandboxedPointerField(i.InputOrZeroRegister(0), mem);\n      break;\n    }\n    case kRiscvStoreIndirectPointer: {\n      MemOperand mem = i.MemoryOperand(1);\n      __ StoreIndirectPointerField(i.InputOrZeroRegister(0), mem);\n      break;\n    }\n    case kRiscvAtomicLoadDecompressTaggedSigned:\n      __ AtomicDecompressTaggedSigned(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvAtomicLoadDecompressTagged:\n      __ AtomicDecompressTagged(i.OutputRegister(), i.MemoryOperand());\n      break;\n    case kRiscvAtomicStoreCompressTagged: {\n      size_t index = 0;\n      MemOperand mem = i.MemoryOperand(&index);\n      __ AtomicStoreTaggedField(i.InputOrZeroRegister(index), mem);\n      break;\n    }\n    case kRiscvLoadDecompressProtected: {\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ DecompressProtected(i.OutputRegister(), i.MemoryOperand());\n      break;\n    }\n#endif\n    case kRiscvRvvSt: {\n      (__ VU).set(kScratchReg, VSew::E8, Vlmul::m1);\n      auto memOperand = i.MemoryOperand(1);\n      Register dst = memOperand.offset() == 0 ? memOperand.rm() : kScratchReg;\n      if (memOperand.offset() != 0) {\n        __ AddWord(dst, memOperand.rm(), memOperand.offset());\n      }\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ vs(i.InputSimd128Register(0), dst, 0, VSew::E8);\n      break;\n    }\n    case kRiscvRvvLd: {\n      (__ VU).set(kScratchReg, VSew::E8, Vlmul::m1);\n      Register src = i.MemoryOperand().offset() == 0 ? i.MemoryOperand().rm()\n                                                     : kScratchReg;\n      if (i.MemoryOperand().offset() != 0) {\n        __ AddWord(src, i.MemoryOperand().rm(), i.MemoryOperand().offset());\n      }\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ vl(i.OutputSimd128Register(), src, 0, VSew::E8);\n      break;\n    }\n    case kRiscvS128Zero: {\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E8, m1);\n      __ vmv_vx(dst, zero_reg);\n      break;\n    }\n    case kRiscvS128Load32Zero: {\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E32, m1);\n      __ Load32U(kScratchReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vmv_sx(dst, kScratchReg);\n      break;\n    }\n    case kRiscvS128Load64Zero: {\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E64, m1);\n#if V8_TARGET_ARCH_RISCV64\n      __ LoadWord(kScratchReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vmv_sx(dst, kScratchReg);\n#elif V8_TARGET_ARCH_RISCV32\n      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vfmv_sf(dst, kScratchDoubleReg);\n#endif\n      break;\n    }\n    case kRiscvS128LoadLane: {\n      Simd128Register dst = i.OutputSimd128Register();\n      DCHECK_EQ(dst, i.InputSimd128Register(0));\n      auto sz = LaneSizeField::decode(opcode);\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr, __ pc_offset());\n      __ LoadLane(sz, dst, i.InputUint8(1), i.MemoryOperand(2));\n      break;\n    }\n    case kRiscvS128StoreLane: {\n      Simd128Register src = i.InputSimd128Register(0);\n      DCHECK_EQ(src, i.InputSimd128Register(0));\n      auto sz = LaneSizeField::decode(opcode);\n      __ StoreLane(sz, src, i.InputUint8(1), i.MemoryOperand(2));\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      break;\n    }\n    case kRiscvS128Load64ExtendS: {\n      __ VU.set(kScratchReg, E64, m1);\n#if V8_TARGET_ARCH_RISCV64\n      __ LoadWord(kScratchReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vmv_vx(kSimd128ScratchReg, kScratchReg);\n#elif V8_TARGET_ARCH_RISCV32\n      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);\n#endif\n      __ VU.set(kScratchReg, i.InputInt8(2), m1);\n      __ vsext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvS128Load64ExtendU: {\n      __ VU.set(kScratchReg, E64, m1);\n#if V8_TARGET_ARCH_RISCV64\n      __ LoadWord(kScratchReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vmv_vx(kSimd128ScratchReg, kScratchReg);\n#elif V8_TARGET_ARCH_RISCV32\n      __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());\n      RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                             (__ pc_offset() - kInstrSize));\n      __ vfmv_vf(kSimd128ScratchReg, kScratchDoubleReg);\n#endif\n      __ VU.set(kScratchReg, i.InputInt8(2), m1);\n      __ vzext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvS128LoadSplat: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      switch (i.InputInt8(2)) {\n        case E8:\n          __ Lb(kScratchReg, i.MemoryOperand());\n          RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                                 (__ pc_offset() - kInstrSize));\n          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);\n          break;\n        case E16:\n          __ Lh(kScratchReg, i.MemoryOperand());\n          RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                                 (__ pc_offset() - kInstrSize));\n          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);\n          break;\n        case E32:\n          __ Lw(kScratchReg, i.MemoryOperand());\n          RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                                 (__ pc_offset() - kInstrSize));\n          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);\n          break;\n        case E64:\n#if V8_TARGET_ARCH_RISCV64\n          __ LoadWord(kScratchReg, i.MemoryOperand());\n          RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                                 (__ pc_offset() - kInstrSize));\n          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);\n#elif V8_TARGET_ARCH_RISCV32\n          __ LoadDouble(kScratchDoubleReg, i.MemoryOperand());\n          RecordTrapInfoIfNeeded(zone(), this, opcode, instr,\n                                 (__ pc_offset() - kInstrSize));\n          __ vfmv_vf(i.OutputSimd128Register(), kScratchDoubleReg);\n#endif\n          break;\n        default:\n          UNREACHABLE();\n      }\n      break;\n    }\n    case kRiscvS128AllOnes: {\n      __ VU.set(kScratchReg, E8, m1);\n      __ vmv_vx(i.OutputSimd128Register(), zero_reg);\n      __ vnot_vv(i.OutputSimd128Register(), i.OutputSimd128Register());\n      break;\n    }\n    case kRiscvS128Select: {\n      __ VU.set(kScratchReg, E8, m1);\n      __ vand_vv(kSimd128ScratchReg, i.InputSimd128Register(1),\n                 i.InputSimd128Register(0));\n      __ vnot_vv(kSimd128ScratchReg2, i.InputSimd128Register(0));\n      __ vand_vv(kSimd128ScratchReg2, i.InputSimd128Register(2),\n                 kSimd128ScratchReg2);\n      __ vor_vv(i.OutputSimd128Register(), kSimd128ScratchReg,\n                kSimd128ScratchReg2);\n      break;\n    }\n    case kRiscvVnot: {\n      (__ VU).set(kScratchReg, VSew::E8, Vlmul::m1);\n      __ vnot_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvS128Const: {\n      Simd128Register dst = i.OutputSimd128Register();\n      uint8_t imm[16];\n      *reinterpret_cast<uint64_t*>(imm) =\n          make_uint64(i.InputUint32(1), i.InputUint32(0));\n      *(reinterpret_cast<uint64_t*>(imm) + 1) =\n          make_uint64(i.InputUint32(3), i.InputUint32(2));\n      __ WasmRvvS128const(dst, imm);\n      break;\n    }\n    case kRiscvVrgather: {\n      Simd128Register index = i.InputSimd128Register(0);\n      if (!(instr->InputAt(1)->IsImmediate())) {\n        index = i.InputSimd128Register(1);\n      } else {\n#if V8_TARGET_ARCH_RISCV64\n        __ VU.set(kScratchReg, E64, m1);\n        __ li(kScratchReg, i.InputInt64(1));\n        __ vmv_vi(kSimd128ScratchReg3, -1);\n        __ vmv_sx(kSimd128ScratchReg3, kScratchReg);\n        index = kSimd128ScratchReg3;\n#elif V8_TARGET_ARCH_RISCV32\n        int64_t intput_int64 = i.InputInt64(1);\n        int32_t input_int32[2];\n        memcpy(input_int32, &intput_int64, sizeof(intput_int64));\n        __ VU.set(kScratchReg, E32, m1);\n        __ li(kScratchReg, input_int32[1]);\n        __ vmv_vx(kSimd128ScratchReg3, kScratchReg);\n        __ li(kScratchReg, input_int32[0]);\n        __ vmv_sx(kSimd128ScratchReg3, kScratchReg);\n        index = kSimd128ScratchReg3;\n#endif\n      }\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (i.OutputSimd128Register() == i.InputSimd128Register(0)) {\n        __ vrgather_vv(kSimd128ScratchReg, i.InputSimd128Register(0), index);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);\n      } else {\n        __ vrgather_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                       index);\n      }\n      break;\n    }\n    case kRiscvVslidedown: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsImmediate()) {\n        DCHECK(is_uint5(i.InputInt32(1)));\n        __ vslidedown_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                         i.InputInt5(1));\n      } else {\n        __ vslidedown_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                         i.InputRegister(1));\n      }\n      break;\n    }\n    case kRiscvI8x16ExtractLaneU: {\n      __ VU.set(kScratchReg, E8, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       i.InputInt8(1));\n      __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);\n      __ slli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 8);\n      __ srli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 8);\n      break;\n    }\n    case kRiscvI8x16ExtractLaneS: {\n      __ VU.set(kScratchReg, E8, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       i.InputInt8(1));\n      __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI16x8ExtractLaneU: {\n      __ VU.set(kScratchReg, E16, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       i.InputInt8(1));\n      __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);\n      __ slli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 16);\n      __ srli(i.OutputRegister(), i.OutputRegister(), sizeof(void*) * 8 - 16);\n      break;\n    }\n    case kRiscvI16x8ExtractLaneS: {\n      __ VU.set(kScratchReg, E16, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       i.InputInt8(1));\n      __ vmv_xs(i.OutputRegister(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI8x16ShrU: {\n      __ VU.set(kScratchReg, E8, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ andi(i.InputRegister(1), i.InputRegister(1), 8 - 1);\n        __ vsrl_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsrl_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 8);\n      }\n      break;\n    }\n    case kRiscvI16x8ShrU: {\n      __ VU.set(kScratchReg, E16, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ andi(i.InputRegister(1), i.InputRegister(1), 16 - 1);\n        __ vsrl_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsrl_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 16);\n      }\n      break;\n    }\n    case kRiscvI32x4TruncSatF64x2SZero: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmv_vx(kSimd128ScratchReg, zero_reg);\n      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));\n      __ vmv_vv(kSimd128ScratchReg3, i.InputSimd128Register(0));\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vfncvt_x_f_w(kSimd128ScratchReg, kSimd128ScratchReg3, MaskType::Mask);\n      __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI32x4TruncSatF64x2UZero: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmv_vx(kSimd128ScratchReg, zero_reg);\n      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));\n      __ vmv_vv(kSimd128ScratchReg3, i.InputSimd128Register(0));\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vfncvt_xu_f_w(kSimd128ScratchReg, kSimd128ScratchReg3, MaskType::Mask);\n      __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI32x4ShrU: {\n      __ VU.set(kScratchReg, E32, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsrl_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsrl_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 32);\n      }\n      break;\n    }\n    case kRiscvI64x2ShrU: {\n      __ VU.set(kScratchReg, E64, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsrl_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        if (is_uint5(i.InputInt6(1) % 64)) {\n          __ vsrl_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputInt6(1) % 64);\n        } else {\n          __ li(kScratchReg, i.InputInt6(1) % 64);\n          __ vsrl_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvI8x16ShrS: {\n      __ VU.set(kScratchReg, E8, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsra_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsra_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 8);\n      }\n      break;\n    }\n    case kRiscvI16x8ShrS: {\n      __ VU.set(kScratchReg, E16, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsra_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsra_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 16);\n      }\n      break;\n    }\n    case kRiscvI32x4ShrS: {\n      __ VU.set(kScratchReg, E32, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsra_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsra_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 32);\n      }\n      break;\n    }\n    case kRiscvI64x2ShrS: {\n      __ VU.set(kScratchReg, E64, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsra_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        if (is_uint5(i.InputInt6(1) % 64)) {\n          __ vsra_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputInt6(1) % 64);\n        } else {\n          __ li(kScratchReg, i.InputInt6(1) % 64);\n          __ vsra_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvI32x4ExtractLane: {\n      __ WasmRvvExtractLane(i.OutputRegister(), i.InputSimd128Register(0),\n                            i.InputInt8(1), E32, m1);\n      break;\n    }\n    case kRiscvVAbs: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ vmslt_vx(v0, i.InputSimd128Register(0), zero_reg);\n      __ vneg_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 MaskType::Mask);\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvI64x2ExtractLane: {\n      __ WasmRvvExtractLane(i.OutputRegister(), i.InputSimd128Register(0),\n                            i.InputInt8(1), E64, m1);\n      break;\n    }\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvI64x2ExtractLane: {\n      uint8_t imm_lane_idx = i.InputInt8(1);\n      __ VU.set(kScratchReg, E32, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       (imm_lane_idx << 0x1) + 1);\n      __ vmv_xs(i.OutputRegister(1), kSimd128ScratchReg);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       (imm_lane_idx << 0x1));\n      __ vmv_xs(i.OutputRegister(0), kSimd128ScratchReg);\n      break;\n    }\n#endif\n    case kRiscvI8x16Shl: {\n      __ VU.set(kScratchReg, E8, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsll_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 8);\n      }\n      break;\n    }\n    case kRiscvI16x8Shl: {\n      __ VU.set(kScratchReg, E16, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsll_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 16);\n      }\n      break;\n    }\n    case kRiscvI32x4Shl: {\n      __ VU.set(kScratchReg, E32, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        __ vsll_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputInt5(1) % 32);\n      }\n      break;\n    }\n    case kRiscvI64x2Shl: {\n      __ VU.set(kScratchReg, E64, m1);\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else {\n        if (is_int5(i.InputInt6(1) % 64)) {\n          __ vsll_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputInt6(1) % 64);\n        } else {\n          __ li(kScratchReg, i.InputInt6(1) % 64);\n          __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvI8x16ReplaceLane: {\n      Simd128Register src = i.InputSimd128Register(0);\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E64, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ VU.set(kScratchReg, E8, m1);\n      __ vmerge_vx(dst, i.InputRegister(2), src);\n      break;\n    }\n    case kRiscvI16x8ReplaceLane: {\n      Simd128Register src = i.InputSimd128Register(0);\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E16, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ vmerge_vx(dst, i.InputRegister(2), src);\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvI64x2ReplaceLane: {\n      Simd128Register src = i.InputSimd128Register(0);\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E64, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ vmerge_vx(dst, i.InputRegister(2), src);\n      break;\n    }\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvI64x2ReplaceLaneI32Pair: {\n      Simd128Register src = i.InputSimd128Register(0);\n      Simd128Register dst = i.OutputSimd128Register();\n      Register int64_low = i.InputRegister(2);\n      Register int64_high = i.InputRegister(3);\n      __ VU.set(kScratchReg, E32, m1);\n      __ vmv_vx(kSimd128ScratchReg, int64_high);\n      __ vmv_sx(kSimd128ScratchReg, int64_low);\n      __ VU.set(kScratchReg, E64, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ vfmv_fs(kScratchDoubleReg, kSimd128ScratchReg);\n      __ vfmerge_vf(dst, kScratchDoubleReg, src);\n      break;\n    }\n#endif\n    case kRiscvI32x4ReplaceLane: {\n      Simd128Register src = i.InputSimd128Register(0);\n      Simd128Register dst = i.OutputSimd128Register();\n      __ VU.set(kScratchReg, E32, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ vmerge_vx(dst, i.InputRegister(2), src);\n      break;\n    }\n    case kRiscvV128AnyTrue: {\n      __ VU.set(kScratchReg, E8, m1);\n      Register dst = i.OutputRegister();\n      Label t;\n      __ vmv_sx(kSimd128ScratchReg, zero_reg);\n      __ vredmaxu_vs(kSimd128ScratchReg, i.InputSimd128Register(0),\n                     kSimd128ScratchReg);\n      __ vmv_xs(dst, kSimd128ScratchReg);\n      __ beq(dst, zero_reg, &t);\n      __ li(dst, 1);\n      __ bind(&t);\n      break;\n    }\n    case kRiscvVAllTrue: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      Register dst = i.OutputRegister();\n      Label notalltrue;\n      __ vmv_vi(kSimd128ScratchReg, -1);\n      __ vredminu_vs(kSimd128ScratchReg, i.InputSimd128Register(0),\n                     kSimd128ScratchReg);\n      __ vmv_xs(dst, kSimd128ScratchReg);\n      __ beqz(dst, &notalltrue);\n      __ li(dst, 1);\n      __ bind(&notalltrue);\n      break;\n    }\n    case kRiscvI8x16Shuffle: {\n      VRegister dst = i.OutputSimd128Register(),\n                src0 = i.InputSimd128Register(0),\n                src1 = i.InputSimd128Register(1);\n\n#if V8_TARGET_ARCH_RISCV64\n      int64_t imm1 = make_uint64(i.InputInt32(3), i.InputInt32(2));\n      int64_t imm2 = make_uint64(i.InputInt32(5), i.InputInt32(4));\n      __ VU.set(kScratchReg, VSew::E64, Vlmul::m1);\n      __ li(kScratchReg, imm2);\n      __ vmv_sx(kSimd128ScratchReg2, kScratchReg);\n      __ vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);\n      __ li(kScratchReg, imm1);\n      __ vmv_sx(kSimd128ScratchReg, kScratchReg);\n#elif V8_TARGET_ARCH_RISCV32\n      __ VU.set(kScratchReg, VSew::E32, Vlmul::m1);\n      __ li(kScratchReg, i.InputInt32(5));\n      __ vmv_vx(kSimd128ScratchReg2, kScratchReg);\n      __ li(kScratchReg, i.InputInt32(4));\n      __ vmv_sx(kSimd128ScratchReg2, kScratchReg);\n      __ li(kScratchReg, i.InputInt32(3));\n      __ vmv_vx(kSimd128ScratchReg, kScratchReg);\n      __ li(kScratchReg, i.InputInt32(2));\n      __ vmv_sx(kSimd128ScratchReg, kScratchReg);\n      __ vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 2);\n#endif\n\n      __ VU.set(kScratchReg, E8, m1);\n      if (dst == src0) {\n        __ vmv_vv(kSimd128ScratchReg2, src0);\n        src0 = kSimd128ScratchReg2;\n      } else if (dst == src1) {\n        __ vmv_vv(kSimd128ScratchReg2, src1);\n        src1 = kSimd128ScratchReg2;\n      }\n      __ vrgather_vv(dst, src0, kSimd128ScratchReg);\n      __ vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg, -16);\n      __ vrgather_vv(kSimd128ScratchReg3, src1, kSimd128ScratchReg);\n      __ vor_vv(dst, dst, kSimd128ScratchReg3);\n      break;\n    }\n    case kRiscvI8x16Popcnt: {\n      VRegister dst = i.OutputSimd128Register(),\n                src = i.InputSimd128Register(0);\n      Label t;\n\n      __ VU.set(kScratchReg, E8, m1);\n      __ vmv_vv(kSimd128ScratchReg, src);\n      __ vmv_vv(dst, kSimd128RegZero);\n\n      __ bind(&t);\n      __ vmsne_vv(v0, kSimd128ScratchReg, kSimd128RegZero);\n      __ vadd_vi(dst, dst, 1, Mask);\n      __ vadd_vi(kSimd128ScratchReg2, kSimd128ScratchReg, -1, Mask);\n      __ vand_vv(kSimd128ScratchReg, kSimd128ScratchReg, kSimd128ScratchReg2);\n      // kScratchReg = -1 if kSimd128ScratchReg == 0 i.e. no active element\n      __ vfirst_m(kScratchReg, kSimd128ScratchReg);\n      __ bgez(kScratchReg, &t);\n      break;\n    }\n    case kRiscvF64x2NearestInt: {\n      __ Round_d(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF64x2Trunc: {\n      __ Trunc_d(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF64x2Sqrt: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vfsqrt_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2Abs: {\n      __ VU.set(kScratchReg, VSew::E64, Vlmul::m1);\n      __ vfabs_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2Ceil: {\n      __ Ceil_d(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF64x2Floor: {\n      __ Floor_d(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF64x2ReplaceLane: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ vfmerge_vf(i.OutputSimd128Register(), i.InputSingleRegister(2),\n                    i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2Pmax: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmflt_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(1));\n      __ vmerge_vv(i.OutputSimd128Register(), i.InputSimd128Register(1),\n                   i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2Pmin: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmflt_vv(v0, i.InputSimd128Register(1), i.InputSimd128Register(0));\n      __ vmerge_vv(i.OutputSimd128Register(), i.InputSimd128Register(1),\n                   i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2ExtractLane: {\n      __ VU.set(kScratchReg, E64, m1);\n      if (is_uint5(i.InputInt8(1))) {\n        __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                         i.InputInt8(1));\n      } else {\n        __ li(kScratchReg, i.InputInt8(1));\n        __ vslidedown_vx(kSimd128ScratchReg, i.InputSimd128Register(0),\n                         kScratchReg);\n      }\n      __ vfmv_fs(i.OutputDoubleRegister(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF64x2PromoteLowF32x4: {\n      __ VU.set(kScratchReg, E32, mf2);\n      if (i.OutputSimd128Register() != i.InputSimd128Register(0)) {\n        __ vfwcvt_f_f_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      } else {\n        __ vfwcvt_f_f_v(kSimd128ScratchReg3, i.InputSimd128Register(0));\n        __ VU.set(kScratchReg, E64, m1);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg3);\n      }\n      break;\n    }\n    case kRiscvF64x2ConvertLowI32x4S: {\n      __ VU.set(kScratchReg, E32, mf2);\n      if (i.OutputSimd128Register() != i.InputSimd128Register(0)) {\n        __ vfwcvt_f_x_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      } else {\n        __ vfwcvt_f_x_v(kSimd128ScratchReg3, i.InputSimd128Register(0));\n        __ VU.set(kScratchReg, E64, m1);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg3);\n      }\n      break;\n    }\n    case kRiscvF64x2ConvertLowI32x4U: {\n      __ VU.set(kScratchReg, E32, mf2);\n      if (i.OutputSimd128Register() != i.InputSimd128Register(0)) {\n        __ vfwcvt_f_xu_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      } else {\n        __ vfwcvt_f_xu_v(kSimd128ScratchReg3, i.InputSimd128Register(0));\n        __ VU.set(kScratchReg, E64, m1);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg3);\n      }\n      break;\n    }\n    case kRiscvF64x2Qfma: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vfmadd_vv(i.InputSimd128Register(0), i.InputSimd128Register(1),\n                   i.InputSimd128Register(2));\n      __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF64x2Qfms: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vfnmsub_vv(i.InputSimd128Register(0), i.InputSimd128Register(1),\n                    i.InputSimd128Register(2));\n      __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4ExtractLane: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0),\n                       i.InputInt8(1));\n      __ vfmv_fs(i.OutputDoubleRegister(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF32x4Trunc: {\n      __ Trunc_f(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF32x4NearestInt: {\n      __ Round_f(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF32x4DemoteF64x2Zero: {\n      __ VU.set(kScratchReg, E32, mf2);\n      __ vfncvt_f_f_w(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      __ VU.set(kScratchReg, E32, m1);\n      __ vmv_vi(v0, 12);\n      __ vmerge_vx(i.OutputSimd128Register(), zero_reg,\n                   i.OutputSimd128Register());\n      break;\n    }\n    case kRiscvF32x4Abs: {\n      __ VU.set(kScratchReg, VSew::E32, Vlmul::m1);\n      __ vfabs_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Ceil: {\n      __ Ceil_f(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF32x4Floor: {\n      __ Floor_f(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 kScratchReg, kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvF32x4UConvertI32x4: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vfcvt_f_xu_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4SConvertI32x4: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vfcvt_f_x_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4ReplaceLane: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ li(kScratchReg, 0x1 << i.InputInt8(1));\n      __ vmv_sx(v0, kScratchReg);\n      __ fmv_x_w(kScratchReg, i.InputSingleRegister(2));\n      __ vmerge_vx(i.OutputSimd128Register(), kScratchReg,\n                   i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Pmax: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vmflt_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(1));\n      __ vmerge_vv(i.OutputSimd128Register(), i.InputSimd128Register(1),\n                   i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Pmin: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vmflt_vv(v0, i.InputSimd128Register(1), i.InputSimd128Register(0));\n      __ vmerge_vv(i.OutputSimd128Register(), i.InputSimd128Register(1),\n                   i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Sqrt: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vfsqrt_v(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Qfma: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vfmadd_vv(i.InputSimd128Register(0), i.InputSimd128Register(1),\n                   i.InputSimd128Register(2));\n      __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvF32x4Qfms: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vfnmsub_vv(i.InputSimd128Register(0), i.InputSimd128Register(1),\n                    i.InputSimd128Register(2));\n      __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvI64x2SConvertI32x4Low: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmv_vv(kSimd128ScratchReg, i.InputSimd128Register(0));\n      __ vsext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n\n      break;\n    }\n    case kRiscvI64x2SConvertI32x4High: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0), 2);\n      __ VU.set(kScratchReg, E64, m1);\n      __ vsext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI64x2UConvertI32x4Low: {\n      __ VU.set(kScratchReg, E64, m1);\n      __ vmv_vv(kSimd128ScratchReg, i.InputSimd128Register(0));\n      __ vzext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI64x2UConvertI32x4High: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vslidedown_vi(kSimd128ScratchReg, i.InputSimd128Register(0), 2);\n      __ VU.set(kScratchReg, E64, m1);\n      __ vzext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);\n      break;\n    }\n    case kRiscvI32x4SConvertF32x4: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));\n      if (i.OutputSimd128Register() != i.InputSimd128Register(0)) {\n        __ vmv_vx(i.OutputSimd128Register(), zero_reg);\n        __ vfcvt_x_f_v(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                       Mask);\n      } else {\n        __ vmv_vx(kSimd128ScratchReg, zero_reg);\n        __ vfcvt_x_f_v(kSimd128ScratchReg, i.InputSimd128Register(0), Mask);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);\n      }\n      break;\n    }\n    case kRiscvI32x4UConvertF32x4: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ VU.set(FPURoundingMode::RTZ);\n      __ vmfeq_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(0));\n      if (i.OutputSimd128Register() != i.InputSimd128Register(0)) {\n        __ vmv_vx(i.OutputSimd128Register(), zero_reg);\n        __ vfcvt_xu_f_v(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                        Mask);\n      } else {\n        __ vmv_vx(kSimd128ScratchReg, zero_reg);\n        __ vfcvt_xu_f_v(kSimd128ScratchReg, i.InputSimd128Register(0), Mask);\n        __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);\n      }\n      break;\n    }\n#if V8_TARGET_ARCH_RISCV32\n    case kRiscvI64x2SplatI32Pair: {\n      __ VU.set(kScratchReg, E32, m1);\n      __ vmv_vi(v0, 0b0101);\n      __ vmv_vx(kSimd128ScratchReg, i.InputRegister(1));\n      __ vmerge_vx(i.OutputSimd128Register(), i.InputRegister(0),\n                   kSimd128ScratchReg);\n      break;\n    }\n#endif\n    case kRiscvVwaddVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vwadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVwadduVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vwaddu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVwadduWx: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vwaddu_wx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputRegister(1));\n      } else {\n        __ li(kScratchReg, i.InputInt64(1));\n        __ vwaddu_wx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     kScratchReg);\n      }\n      break;\n    }\n    case kRiscvVdivu: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vdivu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1));\n      } else if ((instr->InputAt(1)->IsRegister())) {\n        __ vdivu_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputRegister(1));\n      } else {\n        __ li(kScratchReg, i.InputInt64(1));\n        __ vdivu_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    kScratchReg);\n      }\n      break;\n    }\n    case kRiscvVnclipu: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ VU.set(FPURoundingMode(i.InputInt8(4)));\n      if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vnclipu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                      i.InputSimd128Register(1));\n      } else if (instr->InputAt(1)->IsRegister()) {\n        __ vnclipu_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                      i.InputRegister(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        __ vnclipu_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                      i.InputInt8(1));\n      }\n      break;\n    }\n    case kRiscvVnclip: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ VU.set(FPURoundingMode(i.InputInt8(4)));\n      if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vnclip_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputSimd128Register(1));\n      } else if (instr->InputAt(1)->IsRegister()) {\n        __ vnclip_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputRegister(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        __ vnclip_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputInt8(1));\n      }\n      break;\n    }\n    case kRiscvVwmul: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vwmul_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVwmulu: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vwmulu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmvSx: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      if (instr->InputAt(0)->IsRegister()) {\n        __ vmv_sx(i.OutputSimd128Register(), i.InputRegister(0));\n      } else {\n        DCHECK(instr->InputAt(0)->IsImmediate());\n        __ li(kScratchReg, i.InputInt64(0));\n        __ vmv_sx(i.OutputSimd128Register(), kScratchReg);\n      }\n      break;\n    }\n    case kRiscvVmvXs: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vmv_xs(i.OutputRegister(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvVcompress: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vcompress_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                        i.InputSimd128Register(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        __ li(kScratchReg, i.InputInt64(1));\n        __ vmv_sx(v0, kScratchReg);\n        __ vcompress_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                        v0);\n      }\n      break;\n    }\n    case kRiscvVsll: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n      } else if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vsll_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        if (is_int5(i.InputInt64(1))) {\n          __ vsll_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputInt8(1));\n        } else {\n          __ li(kScratchReg, i.InputInt64(1));\n          __ vsll_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvVmslt: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (i.InputInt8(4)) {\n        DCHECK(i.OutputSimd128Register() != i.InputSimd128Register(0));\n        __ vmv_vx(i.OutputSimd128Register(), zero_reg);\n      }\n      if (instr->InputAt(1)->IsRegister()) {\n        __ vmslt_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputRegister(1));\n      } else if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vmslt_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1));\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        if (is_int5(i.InputInt64(1))) {\n          __ vmslt_vi(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                      i.InputInt8(1));\n        } else {\n          __ li(kScratchReg, i.InputInt64(1));\n          __ vmslt_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                      kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvVaddVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVsubVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vsub_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmv: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      if (instr->InputAt(0)->IsSimd128Register()) {\n        __ vmv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      } else if (instr->InputAt(0)->IsRegister()) {\n        __ vmv_vx(i.OutputSimd128Register(), i.InputRegister(0));\n      } else {\n        if (i.ToConstant(instr->InputAt(0)).FitsInInt32() &&\n            is_int8(i.InputInt32(0))) {\n          __ vmv_vi(i.OutputSimd128Register(), i.InputInt8(0));\n        } else {\n          __ li(kScratchReg, i.InputInt64(0));\n          __ vmv_vx(i.OutputSimd128Register(), kScratchReg);\n        }\n      }\n      break;\n    }\n    case kRiscvVfmvVf: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vfmv_vf(i.OutputSimd128Register(), i.InputDoubleRegister(0));\n      break;\n    }\n    case kRiscvVnegVv: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vneg_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvVfnegVv: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vfneg_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvVmaxuVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmaxu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmax: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(1)->IsSimd128Register()) {\n        __ vmax_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      } else if (instr->InputAt(1)->IsRegister()) {\n        __ vmax_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputRegister(1));\n\n      } else {\n        DCHECK(instr->InputAt(1)->IsImmediate());\n        __ li(kScratchReg, i.InputInt64(1));\n        __ vmax_vx(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   kScratchReg);\n      }\n      break;\n    }\n    case kRiscvVminuVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vminu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVminsVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmin_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmulVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmul_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVgtsVv: {\n      __ WasmRvvGtS(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                    Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVgesVv: {\n      __ WasmRvvGeS(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                    Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVgeuVv: {\n      __ WasmRvvGeU(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                    Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVgtuVv: {\n      __ WasmRvvGtU(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                    i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                    Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVeqVv: {\n      __ WasmRvvEq(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                   Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVneVv: {\n      __ WasmRvvNe(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1), VSew(i.InputInt8(2)),\n                   Vlmul(i.InputInt8(3)));\n      break;\n    }\n    case kRiscvVaddSatSVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vsadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVaddSatUVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vsaddu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVsubSatSVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vssub_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVsubSatUVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vssubu_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                   i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVfaddVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfadd_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVfsubVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfsub_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVfmulVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfmul_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVfdivVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfdiv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmfeqVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmfeq_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmfneVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmfne_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmfltVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmflt_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVmfleVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vmfle_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVfminVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfmin_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1), MaskType(i.InputInt8(4)));\n      break;\n    }\n    case kRiscvVfmaxVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vfmax_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1), MaskType(i.InputInt8(4)));\n      break;\n    }\n    case kRiscvVandVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vand_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVorVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vor_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVxorVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vxor_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                 i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVnotVv: {\n      (__ VU).set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vnot_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvVmergeVx: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      if (instr->InputAt(0)->IsRegister()) {\n        __ vmerge_vx(i.OutputSimd128Register(), i.InputRegister(0),\n                     i.InputSimd128Register(1));\n      } else {\n        DCHECK(is_int5(i.InputInt32(0)));\n        __ vmerge_vi(i.OutputSimd128Register(), i.InputInt8(0),\n                     i.InputSimd128Register(1));\n      }\n      break;\n    }\n    case kRiscvVsmulVv: {\n      __ VU.set(kScratchReg, i.InputInt8(2), i.InputInt8(3));\n      __ vsmul_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                  i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVredminuVs: {\n      __ vredminu_vs(i.OutputSimd128Register(), i.InputSimd128Register(0),\n                     i.InputSimd128Register(1));\n      break;\n    }\n    case kRiscvVzextVf2: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vzext_vf2(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    case kRiscvVsextVf2: {\n      __ VU.set(kScratchReg, i.InputInt8(1), i.InputInt8(2));\n      __ vsext_vf2(i.OutputSimd128Register(), i.InputSimd128Register(0));\n      break;\n    }\n    default:\n#ifdef DEBUG\n      switch (arch_opcode) {\n#define Print(name)       \\\n  case k##name:           \\\n    printf(\"k%s\", #name); \\\n    break;\n        ARCH_OPCODE_LIST(Print);\n#undef Print\n        default:\n          break;\n      }\n#endif\n      UNIMPLEMENTED();\n  }\n  return kSuccess;\n}", "name_and_para": "CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(\n    Instruction* instr) "}], [{"name": "CodeGenerator::BailoutIfDeoptimized", "content": "void CodeGenerator::BailoutIfDeoptimized() { __ BailoutIfDeoptimized(); }", "name_and_para": "void CodeGenerator::BailoutIfDeoptimized() "}, {"name": "CodeGenerator::BailoutIfDeoptimized", "content": "void CodeGenerator::BailoutIfDeoptimized() {\n  int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;\n  __ LoadProtectedPointerField(\n      kScratchReg, MemOperand(kJavaScriptCallCodeStartRegister, offset));\n  __ Lw(kScratchReg, FieldMemOperand(kScratchReg, Code::kFlagsOffset));\n  __ And(kScratchReg, kScratchReg,\n         Operand(1 << Code::kMarkedForDeoptimizationBit));\n  __ TailCallBuiltin(Builtin::kCompileLazyDeoptimizedCode, ne, kScratchReg,\n                     Operand(zero_reg));\n}", "name_and_para": "void CodeGenerator::BailoutIfDeoptimized() "}], [{"name": "CodeGenerator::AssembleCodeStartRegisterCheck", "content": "void CodeGenerator::AssembleCodeStartRegisterCheck() {\n  UseScratchRegisterScope temps(masm());\n  Register scratch = temps.AcquireX();\n  __ ComputeCodeStartAddress(scratch);\n  __ cmp(scratch, kJavaScriptCallCodeStartRegister);\n  __ Assert(eq, AbortReason::kWrongFunctionCodeStart);\n}", "name_and_para": "void CodeGenerator::AssembleCodeStartRegisterCheck() "}, {"name": "CodeGenerator::AssembleCodeStartRegisterCheck", "content": "void CodeGenerator::AssembleCodeStartRegisterCheck() {\n  __ ComputeCodeStartAddress(kScratchReg);\n  __ Assert(eq, AbortReason::kWrongFunctionCodeStart,\n            kJavaScriptCallCodeStartRegister, Operand(kScratchReg));\n}", "name_and_para": "void CodeGenerator::AssembleCodeStartRegisterCheck() "}], [{"name": "CodeGenerator::AssembleTailCallAfterGap", "content": "void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,\n                                             int first_unused_slot_offset) {\n  DCHECK_EQ(first_unused_slot_offset % 2, 0);\n  AdjustStackPointerForTailCall(masm(), frame_access_state(),\n                                first_unused_slot_offset);\n  DCHECK(instr->IsTailCall());\n  InstructionOperandConverter g(this, instr);\n  int optional_padding_offset = g.InputInt32(instr->InputCount() - 2);\n  if (optional_padding_offset % 2) {\n    __ Poke(padreg, optional_padding_offset * kSystemPointerSize);\n  }\n}", "name_and_para": "void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,\n                                             int first_unused_slot_offset) "}, {"name": "CodeGenerator::AssembleTailCallAfterGap", "content": "void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,\n                                             int first_unused_slot_offset) {\n  AdjustStackPointerForTailCall(masm(), frame_access_state(),\n                                first_unused_slot_offset);\n}", "name_and_para": "void CodeGenerator::AssembleTailCallAfterGap(Instruction* instr,\n                                             int first_unused_slot_offset) "}], [{"name": "CodeGenerator::AssembleTailCallBeforeGap", "content": "void CodeGenerator::AssembleTailCallBeforeGap(Instruction* instr,\n                                              int first_unused_slot_offset) {\n  AdjustStackPointerForTailCall(masm(), frame_access_state(),\n                                first_unused_slot_offset, false);\n}", "name_and_para": "void CodeGenerator::AssembleTailCallBeforeGap(Instruction* instr,\n                                              int first_unused_slot_offset) "}, {"name": "CodeGenerator::AssembleTailCallBeforeGap", "content": "void CodeGenerator::AssembleTailCallBeforeGap(Instruction* instr,\n                                              int first_unused_slot_offset) {\n  AdjustStackPointerForTailCall(masm(), frame_access_state(),\n                                first_unused_slot_offset, false);\n}", "name_and_para": "void CodeGenerator::AssembleTailCallBeforeGap(Instruction* instr,\n                                              int first_unused_slot_offset) "}], [{"name": "AdjustStackPointerForTailCall", "content": "void AdjustStackPointerForTailCall(MacroAssembler* masm,\n                                   FrameAccessState* state,\n                                   int new_slot_above_sp,\n                                   bool allow_shrinkage = true) {\n  int current_sp_offset = state->GetSPToFPSlotCount() +\n                          StandardFrameConstants::kFixedSlotCountAboveFp;\n  int stack_slot_delta = new_slot_above_sp - current_sp_offset;\n  DCHECK_EQ(stack_slot_delta % 2, 0);\n  if (stack_slot_delta > 0) {\n    masm->Claim(stack_slot_delta);\n    state->IncreaseSPDelta(stack_slot_delta);\n  } else if (allow_shrinkage && stack_slot_delta < 0) {\n    masm->Drop(-stack_slot_delta);\n    state->IncreaseSPDelta(stack_slot_delta);\n  }\n}", "name_and_para": "void AdjustStackPointerForTailCall(MacroAssembler* masm,\n                                   FrameAccessState* state,\n                                   int new_slot_above_sp,\n                                   bool allow_shrinkage = true) "}, {"name": "AdjustStackPointerForTailCall", "content": "void AdjustStackPointerForTailCall(MacroAssembler* masm,\n                                   FrameAccessState* state,\n                                   int new_slot_above_sp,\n                                   bool allow_shrinkage = true) {\n  int current_sp_offset = state->GetSPToFPSlotCount() +\n                          StandardFrameConstants::kFixedSlotCountAboveFp;\n  int stack_slot_delta = new_slot_above_sp - current_sp_offset;\n  if (stack_slot_delta > 0) {\n    masm->SubWord(sp, sp, stack_slot_delta * kSystemPointerSize);\n    state->IncreaseSPDelta(stack_slot_delta);\n  } else if (allow_shrinkage && stack_slot_delta < 0) {\n    masm->AddWord(sp, sp, -stack_slot_delta * kSystemPointerSize);\n    state->IncreaseSPDelta(stack_slot_delta);\n  }\n}", "name_and_para": "void AdjustStackPointerForTailCall(MacroAssembler* masm,\n                                   FrameAccessState* state,\n                                   int new_slot_above_sp,\n                                   bool allow_shrinkage = true) "}], [{"name": "CodeGenerator::AssemblePrepareTailCall", "content": "void CodeGenerator::AssemblePrepareTailCall() {\n  if (frame_access_state()->has_frame()) {\n    __ RestoreFPAndLR();\n  }\n  frame_access_state()->SetFrameAccessToSP();\n}", "name_and_para": "void CodeGenerator::AssemblePrepareTailCall() "}, {"name": "CodeGenerator::AssemblePrepareTailCall", "content": "void CodeGenerator::AssemblePrepareTailCall() {\n  if (frame_access_state()->has_frame()) {\n    __ LoadWord(ra, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));\n    __ LoadWord(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));\n  }\n  frame_access_state()->SetFrameAccessToSP();\n}", "name_and_para": "void CodeGenerator::AssemblePrepareTailCall() "}], [{"name": "CodeGenerator::AssembleDeconstructFrame", "content": "void CodeGenerator::AssembleDeconstructFrame() {\n  __ Mov(sp, fp);\n  __ Pop<MacroAssembler::kAuthLR>(fp, lr);\n\n  unwinding_info_writer_.MarkFrameDeconstructed(__ pc_offset());\n}", "name_and_para": "void CodeGenerator::AssembleDeconstructFrame() "}, {"name": "CodeGenerator::AssembleDeconstructFrame", "content": "void CodeGenerator::AssembleDeconstructFrame() {\n  __ Move(sp, fp);\n  __ Pop(ra, fp);\n}", "name_and_para": "void CodeGenerator::AssembleDeconstructFrame() "}], [{"name": "RecordTrapInfoIfNeeded", "content": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) {\n  DCHECK_EQ(kMemoryAccessDirect, AccessModeField::decode(opcode));\n}", "name_and_para": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) "}, {"name": "RecordTrapInfoIfNeeded", "content": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) {\n  DCHECK_EQ(kMemoryAccessDirect, AccessModeField::decode(opcode));\n}", "name_and_para": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) "}], [{"name": "RecordTrapInfoIfNeeded", "content": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) {\n  const MemoryAccessMode access_mode = AccessModeField::decode(opcode);\n  if (access_mode == kMemoryAccessProtectedMemOutOfBounds ||\n      access_mode == kMemoryAccessProtectedNullDereference) {\n    codegen->RecordProtectedInstruction(pc);\n  }\n}", "name_and_para": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) "}, {"name": "RecordTrapInfoIfNeeded", "content": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) {\n  DCHECK_EQ(kMemoryAccessDirect, AccessModeField::decode(opcode));\n}", "name_and_para": "void RecordTrapInfoIfNeeded(Zone* zone, CodeGenerator* codegen,\n                            InstructionCode opcode, Instruction* instr,\n                            int pc) "}], [{"name": "WasmOutOfLineTrap", "content": "class WasmOutOfLineTrap : public OutOfLineCode {\n public:\n  WasmOutOfLineTrap(CodeGenerator* gen, Instruction* instr)\n      : OutOfLineCode(gen), gen_(gen), instr_(instr) {}\n  void Generate() override {\n    Arm64OperandConverter i(gen_, instr_);\n    TrapId trap_id =\n        static_cast<TrapId>(i.InputInt32(instr_->InputCount() - 1));\n    GenerateCallToTrap(trap_id);\n  }\n\n protected:\n  CodeGenerator* gen_;\n\n  void GenerateWithTrapId(TrapId trap_id) { GenerateCallToTrap(trap_id); }\n\n private:\n  void GenerateCallToTrap(TrapId trap_id) {\n    gen_->AssembleSourcePosition(instr_);\n    __ Call(static_cast<Address>(trap_id), RelocInfo::WASM_STUB_CALL);\n    ReferenceMap* reference_map = gen_->zone()->New<ReferenceMap>(gen_->zone());\n    gen_->RecordSafepoint(reference_map);\n    __ AssertUnreachable(AbortReason::kUnexpectedReturnFromWasmTrap);\n  }\n\n  Instruction* instr_;\n}", "name_and_para": ""}, {"name": "WasmOutOfLineTrap", "content": "class WasmOutOfLineTrap : public OutOfLineCode {\n public:\n  WasmOutOfLineTrap(CodeGenerator* gen, Instruction* instr)\n      : OutOfLineCode(gen), gen_(gen), instr_(instr) {}\n  void Generate() override {\n    RiscvOperandConverter i(gen_, instr_);\n    TrapId trap_id =\n        static_cast<TrapId>(i.InputInt32(instr_->InputCount() - 1));\n    GenerateCallToTrap(trap_id);\n  }\n\n protected:\n  CodeGenerator* gen_;\n\n  void GenerateWithTrapId(TrapId trap_id) { GenerateCallToTrap(trap_id); }\n\n private:\n  void GenerateCallToTrap(TrapId trap_id) {\n    gen_->AssembleSourcePosition(instr_);\n    // A direct call to a wasm runtime stub defined in this module.\n    // Just encode the stub index. This will be patched when the code\n    // is added to the native module and copied into wasm code space.\n    __ Call(static_cast<Address>(trap_id), RelocInfo::WASM_STUB_CALL);\n    ReferenceMap* reference_map = gen_->zone()->New<ReferenceMap>(gen_->zone());\n    gen_->RecordSafepoint(reference_map);\n    __ AssertUnreachable(AbortReason::kUnexpectedReturnFromWasmTrap);\n  }\n\n  Instruction* instr_;\n}", "name_and_para": ""}], [{"name": "OutOfLineRecordWrite", "content": "class OutOfLineRecordWrite final : public OutOfLineCode {\n public:\n  OutOfLineRecordWrite(\n      CodeGenerator* gen, Register object, Operand offset, Register value,\n      RecordWriteMode mode, StubCallMode stub_mode,\n      UnwindingInfoWriter* unwinding_info_writer,\n      IndirectPointerTag indirect_pointer_tag = kIndirectPointerNullTag)\n      : OutOfLineCode(gen),\n        object_(object),\n        offset_(offset),\n        value_(value),\n        mode_(mode),\n#if V8_ENABLE_WEBASSEMBLY\n        stub_mode_(stub_mode),\n#endif  // V8_ENABLE_WEBASSEMBLY\n        must_save_lr_(!gen->frame_access_state()->has_frame()),\n        unwinding_info_writer_(unwinding_info_writer),\n        zone_(gen->zone()),\n        indirect_pointer_tag_(indirect_pointer_tag) {\n  }\n\n  void Generate() final {\n    // When storing an indirect pointer, the value will always be a\n    // full/decompressed pointer.\n    if (COMPRESS_POINTERS_BOOL &&\n        mode_ != RecordWriteMode::kValueIsIndirectPointer) {\n      __ DecompressTagged(value_, value_);\n    }\n\n    // No need to check value page flags with the indirect pointer write barrier\n    // because the value is always an ExposedTrustedObject.\n    if (mode_ != RecordWriteMode::kValueIsIndirectPointer) {\n      __ CheckPageFlag(value_, MemoryChunk::kPointersToHereAreInterestingMask,\n                       eq, exit());\n    }\n\n    SaveFPRegsMode const save_fp_mode = frame()->DidAllocateDoubleRegisters()\n                                            ? SaveFPRegsMode::kSave\n                                            : SaveFPRegsMode::kIgnore;\n    if (must_save_lr_) {\n      // We need to save and restore lr if the frame was elided.\n      __ Push<MacroAssembler::kSignLR>(lr, padreg);\n      unwinding_info_writer_->MarkLinkRegisterOnTopOfStack(__ pc_offset(), sp);\n    }\n    if (mode_ == RecordWriteMode::kValueIsEphemeronKey) {\n      __ CallEphemeronKeyBarrier(object_, offset_, save_fp_mode);\n    } else if (mode_ == RecordWriteMode::kValueIsIndirectPointer) {\n      // We must have a valid indirect pointer tag here. Otherwise, we risk not\n      // invoking the correct write barrier, which may lead to subtle issues.\n      CHECK(IsValidIndirectPointerTag(indirect_pointer_tag_));\n      __ CallIndirectPointerBarrier(object_, offset_, save_fp_mode,\n                                    indirect_pointer_tag_);\n#if V8_ENABLE_WEBASSEMBLY\n    } else if (stub_mode_ == StubCallMode::kCallWasmRuntimeStub) {\n      // A direct call to a wasm runtime stub defined in this module.\n      // Just encode the stub index. This will be patched when the code\n      // is added to the native module and copied into wasm code space.\n      __ CallRecordWriteStubSaveRegisters(object_, offset_, save_fp_mode,\n                                          StubCallMode::kCallWasmRuntimeStub);\n#endif  // V8_ENABLE_WEBASSEMBLY\n    } else {\n      __ CallRecordWriteStubSaveRegisters(object_, offset_, save_fp_mode);\n    }\n    if (must_save_lr_) {\n      __ Pop<MacroAssembler::kAuthLR>(padreg, lr);\n      unwinding_info_writer_->MarkPopLinkRegisterFromTopOfStack(__ pc_offset());\n    }\n  }\n\n private:\n  Register const object_;\n  Operand const offset_;\n  Register const value_;\n  RecordWriteMode const mode_;\n#if V8_ENABLE_WEBASSEMBLY\n  StubCallMode const stub_mode_;\n#endif  // V8_ENABLE_WEBASSEMBLY\n  bool must_save_lr_;\n  UnwindingInfoWriter* const unwinding_info_writer_;\n  Zone* zone_;\n  IndirectPointerTag indirect_pointer_tag_;\n}", "name_and_para": ""}, {"name": "OutOfLineRecordWrite", "content": "class OutOfLineRecordWrite final : public OutOfLineCode {\n public:\n  OutOfLineRecordWrite(\n      CodeGenerator* gen, Register object, Operand offset, Register value,\n      RecordWriteMode mode, StubCallMode stub_mode,\n      IndirectPointerTag indirect_pointer_tag = kIndirectPointerNullTag)\n      : OutOfLineCode(gen),\n        object_(object),\n        offset_(offset),\n        value_(value),\n        mode_(mode),\n#if V8_ENABLE_WEBASSEMBLY\n        stub_mode_(stub_mode),\n#endif  // V8_ENABLE_WEBASSEMBLY\n        must_save_lr_(!gen->frame_access_state()->has_frame()),\n        zone_(gen->zone()),\n        indirect_pointer_tag_(indirect_pointer_tag) {\n  }\n\n  void Generate() final {\n#ifdef V8_TARGET_ARCH_RISCV64\n    // When storing an indirect pointer, the value will always be a\n    // full/decompressed pointer.\n    if (COMPRESS_POINTERS_BOOL &&\n        mode_ != RecordWriteMode::kValueIsIndirectPointer) {\n      __ DecompressTagged(value_, value_);\n    }\n#endif\n    __ CheckPageFlag(value_, MemoryChunk::kPointersToHereAreInterestingMask, eq,\n                     exit());\n\n    SaveFPRegsMode const save_fp_mode = frame()->DidAllocateDoubleRegisters()\n                                            ? SaveFPRegsMode::kSave\n                                            : SaveFPRegsMode::kIgnore;\n    if (must_save_lr_) {\n      // We need to save and restore ra if the frame was elided.\n      __ Push(ra);\n    }\n    if (mode_ == RecordWriteMode::kValueIsEphemeronKey) {\n      __ CallEphemeronKeyBarrier(object_, offset_, save_fp_mode);\n    } else if (mode_ == RecordWriteMode::kValueIsIndirectPointer) {\n      DCHECK(IsValidIndirectPointerTag(indirect_pointer_tag_));\n      __ CallIndirectPointerBarrier(object_, offset_, save_fp_mode,\n                                    indirect_pointer_tag_);\n#if V8_ENABLE_WEBASSEMBLY\n    } else if (stub_mode_ == StubCallMode::kCallWasmRuntimeStub) {\n      // A direct call to a wasm runtime stub defined in this module.\n      // Just encode the stub index. This will be patched when the code\n      // is added to the native module and copied into wasm code space.\n      __ CallRecordWriteStubSaveRegisters(object_, offset_, save_fp_mode,\n                                          StubCallMode::kCallWasmRuntimeStub);\n#endif  // V8_ENABLE_WEBASSEMBLY\n    } else {\n      __ CallRecordWriteStubSaveRegisters(object_, offset_, save_fp_mode);\n    }\n    if (must_save_lr_) {\n      __ Pop(ra);\n    }\n  }\n\n private:\n  Register const object_;\n  Operand const offset_;\n  Register const value_;\n  RecordWriteMode const mode_;\n#if V8_ENABLE_WEBASSEMBLY\n  StubCallMode const stub_mode_;\n#endif  // V8_ENABLE_WEBASSEMBLY\n  bool must_save_lr_;\n  Zone* zone_;\n  IndirectPointerTag indirect_pointer_tag_;\n}", "name_and_para": ""}], [{"name": "Arm64OperandConverter", "content": "class Arm64OperandConverter final : public InstructionOperandConverter {\n public:\n  Arm64OperandConverter(CodeGenerator* gen, Instruction* instr)\n      : InstructionOperandConverter(gen, instr) {}\n\n  DoubleRegister InputFloat32Register(size_t index) {\n    return InputDoubleRegister(index).S();\n  }\n\n  DoubleRegister InputFloat64Register(size_t index) {\n    return InputDoubleRegister(index);\n  }\n\n  DoubleRegister InputSimd128Register(size_t index) {\n    return InputDoubleRegister(index).Q();\n  }\n\n  CPURegister InputFloat32OrZeroRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) {\n      DCHECK_EQ(0, base::bit_cast<int32_t>(InputFloat32(index)));\n      return wzr;\n    }\n    DCHECK(instr_->InputAt(index)->IsFPRegister());\n    return InputDoubleRegister(index).S();\n  }\n\n  DoubleRegister InputFloat32OrFPZeroRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) {\n      DCHECK_EQ(0, base::bit_cast<int32_t>(InputFloat32(index)));\n      return fp_zero.S();\n    }\n    DCHECK(instr_->InputAt(index)->IsFPRegister());\n    return InputDoubleRegister(index).S();\n  }\n\n  CPURegister InputFloat64OrZeroRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) {\n      DCHECK_EQ(0, base::bit_cast<int64_t>(InputDouble(index)));\n      return xzr;\n    }\n    DCHECK(instr_->InputAt(index)->IsDoubleRegister());\n    return InputDoubleRegister(index);\n  }\n\n  DoubleRegister InputFloat64OrFPZeroRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) {\n      DCHECK_EQ(0, base::bit_cast<int64_t>(InputDouble(index)));\n      return fp_zero;\n    }\n    DCHECK(instr_->InputAt(index)->IsDoubleRegister());\n    return InputDoubleRegister(index);\n  }\n\n  size_t OutputCount() { return instr_->OutputCount(); }\n\n  DoubleRegister OutputFloat32Register(size_t index = 0) {\n    return OutputDoubleRegister(index).S();\n  }\n\n  DoubleRegister OutputFloat64Register(size_t index = 0) {\n    return OutputDoubleRegister(index);\n  }\n\n  DoubleRegister OutputSimd128Register() { return OutputDoubleRegister().Q(); }\n\n  Register InputRegister32(size_t index) {\n    return ToRegister(instr_->InputAt(index)).W();\n  }\n\n  Register InputOrZeroRegister32(size_t index) {\n    DCHECK(instr_->InputAt(index)->IsRegister() ||\n           (instr_->InputAt(index)->IsImmediate() && (InputInt32(index) == 0)));\n    if (instr_->InputAt(index)->IsImmediate()) {\n      return wzr;\n    }\n    return InputRegister32(index);\n  }\n\n  Register InputRegister64(size_t index) { return InputRegister(index); }\n\n  Register InputOrZeroRegister64(size_t index) {\n    DCHECK(instr_->InputAt(index)->IsRegister() ||\n           (instr_->InputAt(index)->IsImmediate() && (InputInt64(index) == 0)));\n    if (instr_->InputAt(index)->IsImmediate()) {\n      return xzr;\n    }\n    return InputRegister64(index);\n  }\n\n  Operand InputOperand(size_t index) {\n    return ToOperand(instr_->InputAt(index));\n  }\n\n  Operand InputOperand64(size_t index) { return InputOperand(index); }\n\n  Operand InputOperand32(size_t index) {\n    return ToOperand32(instr_->InputAt(index));\n  }\n\n  Register OutputRegister64(size_t index = 0) { return OutputRegister(index); }\n\n  Register OutputRegister32(size_t index = 0) {\n    return OutputRegister(index).W();\n  }\n\n  Register TempRegister32(size_t index) {\n    return ToRegister(instr_->TempAt(index)).W();\n  }\n\n  Operand InputOperand2_32(size_t index) {\n    switch (AddressingModeField::decode(instr_->opcode())) {\n      case kMode_None:\n        return InputOperand32(index);\n      case kMode_Operand2_R_LSL_I:\n        return Operand(InputRegister32(index), LSL, InputInt5(index + 1));\n      case kMode_Operand2_R_LSR_I:\n        return Operand(InputRegister32(index), LSR, InputInt5(index + 1));\n      case kMode_Operand2_R_ASR_I:\n        return Operand(InputRegister32(index), ASR, InputInt5(index + 1));\n      case kMode_Operand2_R_ROR_I:\n        return Operand(InputRegister32(index), ROR, InputInt5(index + 1));\n      case kMode_Operand2_R_UXTB:\n        return Operand(InputRegister32(index), UXTB);\n      case kMode_Operand2_R_UXTH:\n        return Operand(InputRegister32(index), UXTH);\n      case kMode_Operand2_R_SXTB:\n        return Operand(InputRegister32(index), SXTB);\n      case kMode_Operand2_R_SXTH:\n        return Operand(InputRegister32(index), SXTH);\n      case kMode_Operand2_R_SXTW:\n        return Operand(InputRegister32(index), SXTW);\n      case kMode_MRI:\n      case kMode_MRR:\n      case kMode_Root:\n        break;\n    }\n    UNREACHABLE();\n  }\n\n  Operand InputOperand2_64(size_t index) {\n    switch (AddressingModeField::decode(instr_->opcode())) {\n      case kMode_None:\n        return InputOperand64(index);\n      case kMode_Operand2_R_LSL_I:\n        return Operand(InputRegister64(index), LSL, InputInt6(index + 1));\n      case kMode_Operand2_R_LSR_I:\n        return Operand(InputRegister64(index), LSR, InputInt6(index + 1));\n      case kMode_Operand2_R_ASR_I:\n        return Operand(InputRegister64(index), ASR, InputInt6(index + 1));\n      case kMode_Operand2_R_ROR_I:\n        return Operand(InputRegister64(index), ROR, InputInt6(index + 1));\n      case kMode_Operand2_R_UXTB:\n        return Operand(InputRegister64(index), UXTB);\n      case kMode_Operand2_R_UXTH:\n        return Operand(InputRegister64(index), UXTH);\n      case kMode_Operand2_R_SXTB:\n        return Operand(InputRegister64(index), SXTB);\n      case kMode_Operand2_R_SXTH:\n        return Operand(InputRegister64(index), SXTH);\n      case kMode_Operand2_R_SXTW:\n        return Operand(InputRegister64(index), SXTW);\n      case kMode_MRI:\n      case kMode_MRR:\n      case kMode_Root:\n        break;\n    }\n    UNREACHABLE();\n  }\n\n  MemOperand MemoryOperand(size_t index = 0) {\n    switch (AddressingModeField::decode(instr_->opcode())) {\n      case kMode_None:\n      case kMode_Operand2_R_LSR_I:\n      case kMode_Operand2_R_ASR_I:\n      case kMode_Operand2_R_ROR_I:\n      case kMode_Operand2_R_UXTB:\n      case kMode_Operand2_R_UXTH:\n      case kMode_Operand2_R_SXTB:\n      case kMode_Operand2_R_SXTH:\n      case kMode_Operand2_R_SXTW:\n        break;\n      case kMode_Root:\n        return MemOperand(kRootRegister, InputInt64(index));\n      case kMode_Operand2_R_LSL_I:\n        return MemOperand(InputRegister(index + 0), InputRegister(index + 1),\n                          LSL, InputInt32(index + 2));\n      case kMode_MRI:\n        return MemOperand(InputRegister(index + 0), InputInt32(index + 1));\n      case kMode_MRR:\n        return MemOperand(InputRegister(index + 0), InputRegister(index + 1));\n    }\n    UNREACHABLE();\n  }\n\n  Operand ToOperand(InstructionOperand* op) {\n    if (op->IsRegister()) {\n      return Operand(ToRegister(op));\n    }\n    return ToImmediate(op);\n  }\n\n  Operand ToOperand32(InstructionOperand* op) {\n    if (op->IsRegister()) {\n      return Operand(ToRegister(op).W());\n    }\n    return ToImmediate(op);\n  }\n\n  Operand ToImmediate(InstructionOperand* operand) {\n    Constant constant = ToConstant(operand);\n    switch (constant.type()) {\n      case Constant::kInt32:\n        return Operand(constant.ToInt32());\n      case Constant::kInt64:\n#if V8_ENABLE_WEBASSEMBLY\n        if (RelocInfo::IsWasmReference(constant.rmode())) {\n          return Operand(constant.ToInt64(), constant.rmode());\n        }\n#endif  // V8_ENABLE_WEBASSEMBLY\n        return Operand(constant.ToInt64());\n      case Constant::kFloat32:\n        return Operand::EmbeddedNumber(constant.ToFloat32());\n      case Constant::kFloat64:\n        return Operand::EmbeddedNumber(constant.ToFloat64().value());\n      case Constant::kExternalReference:\n        return Operand(constant.ToExternalReference());\n      case Constant::kCompressedHeapObject: {\n        RootIndex root_index;\n        if (gen_->isolate()->roots_table().IsRootHandle(constant.ToHeapObject(),\n                                                        &root_index)) {\n          CHECK(COMPRESS_POINTERS_BOOL);\n          CHECK(V8_STATIC_ROOTS_BOOL || !gen_->isolate()->bootstrapper());\n          Tagged_t ptr =\n              MacroAssemblerBase::ReadOnlyRootPtr(root_index, gen_->isolate());\n          CHECK(Assembler::IsImmAddSub(ptr));\n          return Immediate(ptr);\n        }\n\n        return Operand(constant.ToHeapObject());\n      }\n      case Constant::kHeapObject:\n        return Operand(constant.ToHeapObject());\n      case Constant::kRpoNumber:\n        UNREACHABLE();  // TODO(dcarney): RPO immediates on arm64.\n    }\n    UNREACHABLE();\n  }\n\n  MemOperand ToMemOperand(InstructionOperand* op, MacroAssembler* masm) const {\n    DCHECK_NOT_NULL(op);\n    DCHECK(op->IsStackSlot() || op->IsFPStackSlot());\n    return SlotToMemOperand(AllocatedOperand::cast(op)->index(), masm);\n  }\n\n  MemOperand SlotToMemOperand(int slot, MacroAssembler* masm) const {\n    FrameOffset offset = frame_access_state()->GetFrameOffset(slot);\n    if (offset.from_frame_pointer()) {\n      int from_sp = offset.offset() + frame_access_state()->GetSPToFPOffset();\n      // Convert FP-offsets to SP-offsets if it results in better code.\n      if (!frame_access_state()->FPRelativeOnly() &&\n          (Assembler::IsImmLSUnscaled(from_sp) ||\n           Assembler::IsImmLSScaled(from_sp, 3))) {\n        offset = FrameOffset::FromStackPointer(from_sp);\n      }\n    }\n    // Access below the stack pointer is not expected in arm64 and is actively\n    // prevented at run time in the simulator.\n    DCHECK_IMPLIES(offset.from_stack_pointer(), offset.offset() >= 0);\n    return MemOperand(offset.from_stack_pointer() ? sp : fp, offset.offset());\n  }\n}", "name_and_para": ""}, {"name": "RiscvOperandConverter", "content": "class RiscvOperandConverter final : public InstructionOperandConverter {\n public:\n  RiscvOperandConverter(CodeGenerator* gen, Instruction* instr)\n      : InstructionOperandConverter(gen, instr) {}\n\n  FloatRegister OutputSingleRegister(size_t index = 0) {\n    return ToSingleRegister(instr_->OutputAt(index));\n  }\n\n  FloatRegister InputSingleRegister(size_t index) {\n    return ToSingleRegister(instr_->InputAt(index));\n  }\n\n  FloatRegister ToSingleRegister(InstructionOperand* op) {\n    // Single (Float) and Double register namespace is same on RISC-V,\n    // both are typedefs of FPURegister.\n    return ToDoubleRegister(op);\n  }\n\n  Register InputOrZeroRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) {\n      Constant constant = ToConstant(instr_->InputAt(index));\n      switch (constant.type()) {\n        case Constant::kInt32:\n        case Constant::kInt64:\n          DCHECK_EQ(0, InputInt32(index));\n          break;\n        case Constant::kFloat32:\n          DCHECK_EQ(0, base::bit_cast<int32_t>(InputFloat32(index)));\n          break;\n        case Constant::kFloat64:\n          DCHECK_EQ(0, base::bit_cast<int64_t>(InputDouble(index)));\n          break;\n        default:\n          UNREACHABLE();\n      }\n      return zero_reg;\n    }\n    return InputRegister(index);\n  }\n\n  DoubleRegister InputOrZeroDoubleRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) return kDoubleRegZero;\n\n    return InputDoubleRegister(index);\n  }\n\n  DoubleRegister InputOrZeroSingleRegister(size_t index) {\n    if (instr_->InputAt(index)->IsImmediate()) return kSingleRegZero;\n\n    return InputSingleRegister(index);\n  }\n\n  Operand InputImmediate(size_t index) {\n    Constant constant = ToConstant(instr_->InputAt(index));\n    switch (constant.type()) {\n      case Constant::kInt32:\n        return Operand(constant.ToInt32());\n      case Constant::kInt64:\n        return Operand(constant.ToInt64());\n      case Constant::kFloat32:\n        return Operand::EmbeddedNumber(constant.ToFloat32());\n      case Constant::kFloat64:\n        return Operand::EmbeddedNumber(constant.ToFloat64().value());\n      case Constant::kCompressedHeapObject: {\n        RootIndex root_index;\n        if (gen_->isolate()->roots_table().IsRootHandle(constant.ToHeapObject(),\n                                                        &root_index)) {\n          CHECK(COMPRESS_POINTERS_BOOL);\n          CHECK(V8_STATIC_ROOTS_BOOL || !gen_->isolate()->bootstrapper());\n          Tagged_t ptr =\n              MacroAssemblerBase::ReadOnlyRootPtr(root_index, gen_->isolate());\n          return Operand(ptr);\n        }\n        return Operand(constant.ToHeapObject());\n      }\n      case Constant::kExternalReference:\n      case Constant::kHeapObject:\n        // TODO(plind): Maybe we should handle ExtRef & HeapObj here?\n        //    maybe not done on arm due to const pool ??\n        break;\n      case Constant::kRpoNumber:\n        UNREACHABLE();  // TODO(titzer): RPO immediates\n    }\n    UNREACHABLE();\n  }\n\n  Operand InputOperand(size_t index) {\n    InstructionOperand* op = instr_->InputAt(index);\n    if (op->IsRegister()) {\n      return Operand(ToRegister(op));\n    }\n    return InputImmediate(index);\n  }\n\n  MemOperand MemoryOperand(size_t* first_index) {\n    const size_t index = *first_index;\n    switch (AddressingModeField::decode(instr_->opcode())) {\n      case kMode_None:\n        break;\n      case kMode_MRI:\n        *first_index += 2;\n        return MemOperand(InputRegister(index + 0), InputInt32(index + 1));\n      case kMode_Root:\n        return MemOperand(kRootRegister, InputInt32(index));\n      case kMode_MRR:\n        // TODO(plind): r6 address mode, to be implemented ...\n        UNREACHABLE();\n    }\n    UNREACHABLE();\n  }\n\n  MemOperand MemoryOperand(size_t index = 0) { return MemoryOperand(&index); }\n\n  MemOperand ToMemOperand(InstructionOperand* op) const {\n    DCHECK_NOT_NULL(op);\n    DCHECK(op->IsStackSlot() || op->IsFPStackSlot());\n    return SlotToMemOperand(AllocatedOperand::cast(op)->index());\n  }\n\n  MemOperand SlotToMemOperand(int slot) const {\n    FrameOffset offset = frame_access_state()->GetFrameOffset(slot);\n    return MemOperand(offset.from_stack_pointer() ? sp : fp, offset.offset());\n  }\n}", "name_and_para": ""}]]], [["./v8/src/compiler/backend/riscv/instruction-scheduler-riscv.cc", "./v8/src/compiler/backend/arm64/instruction-scheduler-arm64.cc"], 1.0, 0.028037383177570093, [[{"name": "InstructionScheduler::GetInstructionLatency", "content": "int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {\n  // Basic latency modeling for arm64 instructions. They have been determined\n  // in an empirical way.\n  switch (instr->arch_opcode()) {\n    case kArm64Add:\n    case kArm64Add32:\n    case kArm64And:\n    case kArm64And32:\n    case kArm64Bic:\n    case kArm64Bic32:\n    case kArm64Cmn:\n    case kArm64Cmn32:\n    case kArm64Cmp:\n    case kArm64Cmp32:\n    case kArm64Eon:\n    case kArm64Eon32:\n    case kArm64Eor:\n    case kArm64Eor32:\n    case kArm64Not:\n    case kArm64Not32:\n    case kArm64Or:\n    case kArm64Or32:\n    case kArm64Orn:\n    case kArm64Orn32:\n    case kArm64Sub:\n    case kArm64Sub32:\n    case kArm64Tst:\n    case kArm64Tst32:\n      if (instr->addressing_mode() != kMode_None) {\n        return 3;\n      } else {\n        return 1;\n      }\n\n    case kArm64Clz:\n    case kArm64Clz32:\n    case kArm64Sbfx:\n    case kArm64Sbfx32:\n    case kArm64Sxtb32:\n    case kArm64Sxth32:\n    case kArm64Sxtw:\n    case kArm64Ubfiz32:\n    case kArm64Sbfiz:\n    case kArm64Ubfx:\n    case kArm64Ubfx32:\n      return 1;\n\n    case kArm64Lsl:\n    case kArm64Lsl32:\n    case kArm64Lsr:\n    case kArm64Lsr32:\n    case kArm64Asr:\n    case kArm64Asr32:\n    case kArm64Ror:\n    case kArm64Ror32:\n      return 1;\n\n    case kArm64LdrDecompressTaggedSigned:\n    case kArm64LdrDecompressTagged:\n    case kArm64LdrDecompressProtected:\n    case kArm64Ldr:\n    case kArm64LdrD:\n    case kArm64LdrS:\n    case kArm64LdrW:\n    case kArm64Ldrb:\n    case kArm64Ldrh:\n    case kArm64Ldrsb:\n    case kArm64Ldrsh:\n    case kArm64Ldrsw:\n      return 11;\n\n    case kArm64Str:\n    case kArm64StrD:\n    case kArm64StrS:\n    case kArm64StrW:\n    case kArm64Strb:\n    case kArm64Strh:\n      return 1;\n\n    case kArm64Madd32:\n    case kArm64Mneg32:\n    case kArm64Msub32:\n    case kArm64Mul32:\n      return 3;\n\n    case kArm64Madd:\n    case kArm64Mneg:\n    case kArm64Msub:\n    case kArm64Mul:\n      return 5;\n\n    case kArm64Idiv32:\n    case kArm64Udiv32:\n      return 12;\n\n    case kArm64Idiv:\n    case kArm64Udiv:\n      return 20;\n\n    case kArm64Float32Add:\n    case kArm64Float32Sub:\n    case kArm64Float64Add:\n    case kArm64Float64Sub:\n      return 5;\n\n    case kArm64Float32Abs:\n    case kArm64Float32Cmp:\n    case kArm64Float32Neg:\n    case kArm64Float64Abs:\n    case kArm64Float64Cmp:\n    case kArm64Float64Neg:\n      return 3;\n\n    case kArm64Float32Div:\n    case kArm64Float32Sqrt:\n      return 12;\n\n    case kArm64Float64Div:\n    case kArm64Float64Sqrt:\n      return 19;\n\n    case kArm64Float32RoundDown:\n    case kArm64Float32RoundTiesEven:\n    case kArm64Float32RoundTruncate:\n    case kArm64Float32RoundUp:\n    case kArm64Float64RoundDown:\n    case kArm64Float64RoundTiesAway:\n    case kArm64Float64RoundTiesEven:\n    case kArm64Float64RoundTruncate:\n    case kArm64Float64RoundUp:\n      return 5;\n\n    case kArm64Float32ToFloat64:\n    case kArm64Float64ToFloat32:\n    case kArm64Float64ToInt32:\n    case kArm64Float64ToUint32:\n    case kArm64Float32ToInt64:\n    case kArm64Float64ToInt64:\n    case kArm64Float32ToUint64:\n    case kArm64Float64ToUint64:\n    case kArm64Int32ToFloat64:\n    case kArm64Int64ToFloat32:\n    case kArm64Int64ToFloat64:\n    case kArm64Uint32ToFloat64:\n    case kArm64Uint64ToFloat32:\n    case kArm64Uint64ToFloat64:\n      return 5;\n\n    default:\n      return 2;\n  }\n}", "name_and_para": "int InstructionScheduler::GetInstructionLatency(const Instruction* instr) "}, {"name": "InstructionScheduler::GetInstructionLatency", "content": "int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {\n  // TODO(RISCV): Verify these latencies for RISC-V (currently using MIPS\n  // numbers).\n  switch (instr->arch_opcode()) {\n    case kArchCallCodeObject:\n    case kArchCallWasmFunction:\n      return CallLatency();\n    case kArchTailCallCodeObject:\n    case kArchTailCallWasm:\n    case kArchTailCallAddress:\n      return JumpLatency();\n    case kArchCallJSFunction: {\n      int latency = 0;\n      if (v8_flags.debug_code) {\n        latency = 1 + AssertLatency();\n      }\n      return latency + 1 + Add64Latency(false) + CallLatency();\n    }\n    case kArchPrepareCallCFunction:\n      return PrepareCallCFunctionLatency();\n    case kArchSaveCallerRegisters: {\n      auto fp_mode =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      return PushCallerSavedLatency(fp_mode);\n    }\n    case kArchRestoreCallerRegisters: {\n      auto fp_mode =\n          static_cast<SaveFPRegsMode>(MiscField::decode(instr->opcode()));\n      return PopCallerSavedLatency(fp_mode);\n    }\n    case kArchPrepareTailCall:\n      return 2;\n    case kArchCallCFunction:\n      return CallCFunctionLatency();\n    case kArchJmp:\n      return AssembleArchJumpLatency();\n    case kArchTableSwitch:\n      return AssembleArchTableSwitchLatency();\n    case kArchAbortCSADcheck:\n      return CallLatency() + 1;\n    case kArchDebugBreak:\n      return 1;\n    case kArchComment:\n    case kArchNop:\n    case kArchThrowTerminator:\n    case kArchDeoptimize:\n      return 0;\n    case kArchRet:\n      return AssemblerReturnLatency();\n    case kArchFramePointer:\n      return 1;\n    case kArchParentFramePointer:\n      // Estimated max.\n      return AlignedMemoryLatency();\n    case kArchTruncateDoubleToI:\n      return TruncateDoubleToIDelayedLatency();\n    case kArchStoreWithWriteBarrier:\n      return Add64Latency() + 1 + CheckPageFlagLatency();\n    case kArchStackSlot:\n      // Estimated max.\n      return Add64Latency(false) + AndLatency(false) + AssertLatency() +\n             Add64Latency(false) + AndLatency(false) + BranchShortLatency() +\n             1 + Sub64Latency() + Add64Latency();\n    case kIeee754Float64Acos:\n    case kIeee754Float64Acosh:\n    case kIeee754Float64Asin:\n    case kIeee754Float64Asinh:\n    case kIeee754Float64Atan:\n    case kIeee754Float64Atanh:\n    case kIeee754Float64Atan2:\n    case kIeee754Float64Cos:\n    case kIeee754Float64Cosh:\n    case kIeee754Float64Cbrt:\n    case kIeee754Float64Exp:\n    case kIeee754Float64Expm1:\n    case kIeee754Float64Log:\n    case kIeee754Float64Log1p:\n    case kIeee754Float64Log10:\n    case kIeee754Float64Log2:\n    case kIeee754Float64Pow:\n    case kIeee754Float64Sin:\n    case kIeee754Float64Sinh:\n    case kIeee754Float64Tan:\n    case kIeee754Float64Tanh:\n      return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +\n             CallCFunctionLatency() + MovFromFloatResultLatency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdd32:\n    case kRiscvAdd64:\n      return Add64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvAddOvf64:\n      return AddOverflow64Latency();\n    case kRiscvSub32:\n    case kRiscvSub64:\n      return Sub64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvSubOvf64:\n      return SubOverflow64Latency();\n    case kRiscvMulHigh64:\n      return Mulh64Latency();\n    case kRiscvMul64:\n      return Mul64Latency();\n    case kRiscvMulOvf64:\n      return MulOverflow64Latency();\n    case kRiscvDiv64: {\n      int latency = Div64Latency();\n      return latency + MovzLatency();\n    }\n    case kRiscvDivU64: {\n      int latency = Divu64Latency();\n      return latency + MovzLatency();\n    }\n    case kRiscvMod64:\n      return Mod64Latency();\n    case kRiscvModU64:\n      return Modu64Latency();\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvAdd32:\n      return Add64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvAddOvf:\n      return AddOverflow64Latency();\n    case kRiscvSub32:\n      return Sub64Latency(instr->InputAt(1)->IsRegister());\n    case kRiscvSubOvf:\n      return SubOverflow64Latency();\n#endif\n    case kRiscvMul32:\n      return Mul32Latency();\n    case kRiscvMulOvf32:\n      return MulOverflow32Latency();\n    case kRiscvMulHigh32:\n      return Mulh32Latency();\n    case kRiscvMulHighU32:\n      return Mulhu32Latency();\n    case kRiscvDiv32: {\n      int latency = Div32Latency(instr->InputAt(1)->IsRegister());\n      return latency + MovzLatency();\n    }\n    case kRiscvDivU32: {\n      int latency = Divu32Latency(instr->InputAt(1)->IsRegister());\n      return latency + MovzLatency();\n    }\n    case kRiscvMod32:\n      return Mod32Latency();\n    case kRiscvModU32:\n      return Modu32Latency();\n    case kRiscvAnd:\n      return AndLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvAnd32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = AndLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvOr:\n      return OrLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvOr32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = OrLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvNor:\n      return NorLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvNor32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = NorLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvXor:\n      return XorLatency(instr->InputAt(1)->IsRegister());\n    case kRiscvXor32: {\n      bool is_operand_register = instr->InputAt(1)->IsRegister();\n      int latency = XorLatency(is_operand_register);\n      if (is_operand_register) {\n        return latency + 2;\n      } else {\n        return latency + 1;\n      }\n    }\n    case kRiscvClz32:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClz64:\n#endif\n      return Clz64Latency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvCtz64:\n      return Ctz64Latency();\n    case kRiscvPopcnt64:\n      return Popcnt64Latency();\n#endif\n    case kRiscvCtz32:\n      return Ctz32Latency();\n    case kRiscvPopcnt32:\n      return Popcnt32Latency();\n    case kRiscvShl32:\n      return 1;\n    case kRiscvShr32:\n    case kRiscvSar32:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvZeroExtendWord:\n#endif\n      return 2;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvSignExtendWord:\n    case kRiscvShl64:\n    case kRiscvShr64:\n    case kRiscvSar64:\n    case kRiscvRor64:\n    case kRiscvTst64:\n#endif\n    case kRiscvTst32:\n      return AndLatency(instr->InputAt(0)->IsRegister());\n    case kRiscvRor32:\n      return 1;\n    case kRiscvMov:\n      return 1;\n    case kRiscvCmpS:\n      return MoveLatency() + CompareF32Latency();\n    case kRiscvAddS:\n      return Latency::ADD_S;\n    case kRiscvSubS:\n      return Latency::SUB_S;\n    case kRiscvMulS:\n      return Latency::MUL_S;\n    case kRiscvDivS:\n      return Latency::DIV_S;\n    case kRiscvModS:\n      return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +\n             CallCFunctionLatency() + MovFromFloatResultLatency();\n    case kRiscvAbsS:\n      return Latency::ABS_S;\n    case kRiscvNegS:\n      return NegdLatency();\n    case kRiscvSqrtS:\n      return Latency::SQRT_S;\n    case kRiscvMaxS:\n      return Latency::MAX_S;\n    case kRiscvMinS:\n      return Latency::MIN_S;\n    case kRiscvCmpD:\n      return MoveLatency() + CompareF64Latency();\n    case kRiscvAddD:\n      return Latency::ADD_D;\n    case kRiscvSubD:\n      return Latency::SUB_D;\n    case kRiscvMulD:\n      return Latency::MUL_D;\n    case kRiscvDivD:\n      return Latency::DIV_D;\n    case kRiscvModD:\n      return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +\n             CallCFunctionLatency() + MovFromFloatResultLatency();\n    case kRiscvAbsD:\n      return Latency::ABS_D;\n    case kRiscvNegD:\n      return NegdLatency();\n    case kRiscvSqrtD:\n      return Latency::SQRT_D;\n    case kRiscvMaxD:\n      return Latency::MAX_D;\n    case kRiscvMinD:\n      return Latency::MIN_D;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvFloat64RoundDown:\n    case kRiscvFloat64RoundTruncate:\n    case kRiscvFloat64RoundUp:\n    case kRiscvFloat64RoundTiesEven:\n      return Float64RoundLatency();\n#endif\n    case kRiscvFloat32RoundDown:\n    case kRiscvFloat32RoundTruncate:\n    case kRiscvFloat32RoundUp:\n    case kRiscvFloat32RoundTiesEven:\n      return Float32RoundLatency();\n    case kRiscvFloat32Max:\n      return Float32MaxLatency();\n    case kRiscvFloat64Max:\n      return Float64MaxLatency();\n    case kRiscvFloat32Min:\n      return Float32MinLatency();\n    case kRiscvFloat64Min:\n      return Float64MinLatency();\n    case kRiscvFloat64SilenceNaN:\n      return Latency::SUB_D;\n    case kRiscvCvtSD:\n      return Latency::CVT_S_D;\n    case kRiscvCvtDS:\n      return Latency::CVT_D_S;\n    case kRiscvCvtDW:\n      return Latency::MOVT_FREG + Latency::CVT_D_W;\n    case kRiscvCvtSW:\n      return Latency::MOVT_FREG + Latency::CVT_S_W;\n    case kRiscvCvtSUw:\n      return 1 + Latency::MOVT_DREG + Latency::CVT_S_L;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvCvtSL:\n      return Latency::MOVT_DREG + Latency::CVT_S_L;\n    case kRiscvCvtDL:\n      return Latency::MOVT_DREG + Latency::CVT_D_L;\n    case kRiscvCvtDUl:\n      return 2 * Latency::BRANCH + 3 + 2 * Latency::MOVT_DREG +\n             2 * Latency::CVT_D_L + Latency::ADD_D;\n    case kRiscvCvtSUl:\n      return 2 * Latency::BRANCH + 3 + 2 * Latency::MOVT_DREG +\n             2 * Latency::CVT_S_L + Latency::ADD_S;\n#endif\n    case kRiscvCvtDUw:\n      return 1 + Latency::MOVT_DREG + Latency::CVT_D_L;\n    case kRiscvFloorWD:\n      return Latency::FLOOR_W_D + Latency::MOVF_FREG;\n    case kRiscvCeilWD:\n      return Latency::CEIL_W_D + Latency::MOVF_FREG;\n    case kRiscvRoundWD:\n      return Latency::ROUND_W_D + Latency::MOVF_FREG;\n    case kRiscvTruncWD:\n      return Latency::TRUNC_W_D + Latency::MOVF_FREG;\n    case kRiscvFloorWS:\n      return Latency::FLOOR_W_S + Latency::MOVF_FREG;\n    case kRiscvCeilWS:\n      return Latency::CEIL_W_S + Latency::MOVF_FREG;\n    case kRiscvRoundWS:\n      return Latency::ROUND_W_S + Latency::MOVF_FREG;\n    case kRiscvTruncWS:\n      return Latency::TRUNC_W_S + Latency::MOVF_FREG + 2 + MovnLatency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvTruncLS:\n      return TruncLSLatency(instr->OutputCount() > 1);\n    case kRiscvTruncLD:\n      return TruncLDLatency(instr->OutputCount() > 1);\n    case kRiscvTruncUlS:\n      return TruncUlSLatency();\n    case kRiscvTruncUlD:\n      return TruncUlDLatency();\n    case kRiscvBitcastDL:\n      return Latency::MOVF_HIGH_DREG;\n    case kRiscvBitcastLD:\n      return Latency::MOVT_DREG;\n#endif\n    case kRiscvTruncUwD:\n      // Estimated max.\n      return CompareF64Latency() + 2 * Latency::BRANCH +\n             2 * Latency::TRUNC_W_D + Latency::SUB_D + OrLatency() +\n             Latency::MOVT_FREG + Latency::MOVF_FREG + Latency::MOVT_HIGH_FREG +\n             1;\n    case kRiscvTruncUwS:\n      // Estimated max.\n      return CompareF32Latency() + 2 * Latency::BRANCH +\n             2 * Latency::TRUNC_W_S + Latency::SUB_S + OrLatency() +\n             Latency::MOVT_FREG + 2 * Latency::MOVF_FREG + 2 + MovzLatency();\n    case kRiscvFloat64ExtractLowWord32:\n      return Latency::MOVF_FREG;\n    case kRiscvFloat64InsertLowWord32:\n      return Latency::MOVF_HIGH_FREG + Latency::MOVT_FREG +\n             Latency::MOVT_HIGH_FREG;\n    case kRiscvFloat64ExtractHighWord32:\n      return Latency::MOVF_HIGH_FREG;\n    case kRiscvFloat64InsertHighWord32:\n      return Latency::MOVT_HIGH_FREG;\n    case kRiscvSignExtendByte:\n    case kRiscvSignExtendShort:\n      return 1;\n    case kRiscvLbu:\n    case kRiscvLb:\n    case kRiscvLhu:\n    case kRiscvLh:\n    case kRiscvLw:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvLd:\n    case kRiscvSd:\n    case kRiscvLwu:\n#endif\n    case kRiscvSb:\n    case kRiscvSh:\n    case kRiscvSw:\n      return AlignedMemoryLatency();\n    case kRiscvLoadFloat:\n      return ULoadFloatLatency();\n    case kRiscvLoadDouble:\n      return LoadDoubleLatency();\n    case kRiscvStoreFloat:\n      return StoreFloatLatency();\n    case kRiscvStoreDouble:\n      return StoreDoubleLatency();\n    case kRiscvUlhu:\n    case kRiscvUlh:\n      return UlhuLatency();\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvUlwu:\n      return UlwuLatency();\n    case kRiscvUld:\n      return UldLatency();\n    case kRiscvUsd:\n      return UsdLatency();\n    case kRiscvByteSwap64:\n      return ByteSwapSignedLatency();\n#endif\n    case kRiscvUlw:\n      return UlwLatency();\n    case kRiscvULoadFloat:\n      return ULoadFloatLatency();\n    case kRiscvULoadDouble:\n      return ULoadDoubleLatency();\n    case kRiscvUsh:\n      return UshLatency();\n    case kRiscvUsw:\n      return UswLatency();\n    case kRiscvUStoreFloat:\n      return UStoreFloatLatency();\n    case kRiscvUStoreDouble:\n      return UStoreDoubleLatency();\n    case kRiscvPush: {\n      int latency = 0;\n      if (instr->InputAt(0)->IsFPRegister()) {\n        latency = StoreDoubleLatency() + Sub64Latency(false);\n      } else {\n        latency = PushLatency();\n      }\n      return latency;\n    }\n    case kRiscvPeek: {\n      int latency = 0;\n      if (instr->OutputAt(0)->IsFPRegister()) {\n        auto op = LocationOperand::cast(instr->OutputAt(0));\n        switch (op->representation()) {\n          case MachineRepresentation::kFloat64:\n            latency = LoadDoubleLatency();\n            break;\n          case MachineRepresentation::kFloat32:\n            latency = Latency::LOAD_FLOAT;\n            break;\n          default:\n            UNREACHABLE();\n        }\n      } else {\n        latency = AlignedMemoryLatency();\n      }\n      return latency;\n    }\n    case kRiscvStackClaim:\n      return Sub64Latency(false);\n    case kRiscvStoreToStackSlot: {\n      int latency = 0;\n      if (instr->InputAt(0)->IsFPRegister()) {\n        if (instr->InputAt(0)->IsSimd128Register()) {\n          latency = 1;  // Estimated value.\n        } else {\n          latency = StoreDoubleLatency();\n        }\n      } else {\n        latency = AlignedMemoryLatency();\n      }\n      return latency;\n    }\n    case kRiscvByteSwap32:\n      return ByteSwapSignedLatency();\n    case kAtomicLoadInt8:\n    case kAtomicLoadUint8:\n    case kAtomicLoadInt16:\n    case kAtomicLoadUint16:\n    case kAtomicLoadWord32:\n      return 2;\n    case kAtomicStoreWord8:\n    case kAtomicStoreWord16:\n    case kAtomicStoreWord32:\n      return 3;\n    case kAtomicExchangeInt8:\n      return Word32AtomicExchangeLatency(true, 8);\n    case kAtomicExchangeUint8:\n      return Word32AtomicExchangeLatency(false, 8);\n    case kAtomicExchangeInt16:\n      return Word32AtomicExchangeLatency(true, 16);\n    case kAtomicExchangeUint16:\n      return Word32AtomicExchangeLatency(false, 16);\n    case kAtomicExchangeWord32:\n      return 2 + LlLatency(0) + 1 + ScLatency(0) + BranchShortLatency() + 1;\n    case kAtomicCompareExchangeInt8:\n      return Word32AtomicCompareExchangeLatency(true, 8);\n    case kAtomicCompareExchangeUint8:\n      return Word32AtomicCompareExchangeLatency(false, 8);\n    case kAtomicCompareExchangeInt16:\n      return Word32AtomicCompareExchangeLatency(true, 16);\n    case kAtomicCompareExchangeUint16:\n      return Word32AtomicCompareExchangeLatency(false, 16);\n    case kAtomicCompareExchangeWord32:\n      return 3 + LlLatency(0) + BranchShortLatency() + 1 + ScLatency(0) +\n             BranchShortLatency() + 1;\n    case kRiscvAssertEqual:\n      return AssertLatency();\n#ifdef V8_TARGET_ARCH_RISCV64\n    case kRiscvLoadDecompressProtected:\n      return 11;\n#endif\n    default:\n      return 1;\n  }\n}", "name_and_para": "int InstructionScheduler::GetInstructionLatency(const Instruction* instr) "}], [{"name": "InstructionScheduler::GetTargetInstructionFlags", "content": "int InstructionScheduler::GetTargetInstructionFlags(\n    const Instruction* instr) const {\n  switch (instr->arch_opcode()) {\n    case kArm64Add:\n    case kArm64Add32:\n    case kArm64And:\n    case kArm64And32:\n    case kArm64Bic:\n    case kArm64Bic32:\n    case kArm64Clz:\n    case kArm64Clz32:\n    case kArm64Cmp:\n    case kArm64Cmp32:\n    case kArm64Cmn:\n    case kArm64Cmn32:\n    case kArm64Cnt:\n    case kArm64Cnt32:\n    case kArm64Cnt64:\n    case kArm64Tst:\n    case kArm64Tst32:\n    case kArm64Or:\n    case kArm64Or32:\n    case kArm64Orn:\n    case kArm64Orn32:\n    case kArm64Eor:\n    case kArm64Eor32:\n    case kArm64Eon:\n    case kArm64Eon32:\n    case kArm64Sub:\n    case kArm64Sub32:\n    case kArm64Mul:\n    case kArm64Mul32:\n    case kArm64Smulh:\n    case kArm64Smull:\n    case kArm64Smull2:\n    case kArm64Umulh:\n    case kArm64Umull:\n    case kArm64Umull2:\n    case kArm64Madd:\n    case kArm64Madd32:\n    case kArm64Msub:\n    case kArm64Msub32:\n    case kArm64Mneg:\n    case kArm64Mneg32:\n    case kArm64Idiv:\n    case kArm64Idiv32:\n    case kArm64Udiv:\n    case kArm64Udiv32:\n    case kArm64Imod:\n    case kArm64Imod32:\n    case kArm64Umod:\n    case kArm64Umod32:\n    case kArm64Not:\n    case kArm64Not32:\n    case kArm64Lsl:\n    case kArm64Lsl32:\n    case kArm64Lsr:\n    case kArm64Lsr32:\n    case kArm64Asr:\n    case kArm64Asr32:\n    case kArm64Ror:\n    case kArm64Ror32:\n    case kArm64Mov32:\n    case kArm64Sxtb:\n    case kArm64Sxtb32:\n    case kArm64Sxth:\n    case kArm64Sxth32:\n    case kArm64Sxtw:\n    case kArm64Sbfx:\n    case kArm64Sbfx32:\n    case kArm64Ubfx:\n    case kArm64Ubfx32:\n    case kArm64Ubfiz32:\n    case kArm64Sbfiz:\n    case kArm64Bfi:\n    case kArm64Rbit:\n    case kArm64Rbit32:\n    case kArm64Rev:\n    case kArm64Rev32:\n    case kArm64Float32Cmp:\n    case kArm64Float32Add:\n    case kArm64Float32Sub:\n    case kArm64Float32Mul:\n    case kArm64Float32Div:\n    case kArm64Float32Abs:\n    case kArm64Float32Abd:\n    case kArm64Float32Neg:\n    case kArm64Float32Sqrt:\n    case kArm64Float32Fnmul:\n    case kArm64Float32RoundDown:\n    case kArm64Float32Max:\n    case kArm64Float32Min:\n    case kArm64Float64Cmp:\n    case kArm64Float64Add:\n    case kArm64Float64Sub:\n    case kArm64Float64Mul:\n    case kArm64Float64Div:\n    case kArm64Float64Max:\n    case kArm64Float64Min:\n    case kArm64Float64Abs:\n    case kArm64Float64Abd:\n    case kArm64Float64Neg:\n    case kArm64Float64Sqrt:\n    case kArm64Float64Fnmul:\n    case kArm64Float64RoundDown:\n    case kArm64Float64RoundTiesAway:\n    case kArm64Float64RoundTruncate:\n    case kArm64Float64RoundTiesEven:\n    case kArm64Float64RoundUp:\n    case kArm64Float32RoundTiesEven:\n    case kArm64Float32RoundTruncate:\n    case kArm64Float32RoundUp:\n    case kArm64Float32ToFloat64:\n    case kArm64Float64ToFloat32:\n    case kArm64Float32ToInt32:\n    case kArm64Float64ToInt32:\n    case kArm64Float32ToUint32:\n    case kArm64Float64ToUint32:\n    case kArm64Float32ToInt64:\n    case kArm64Float64ToInt64:\n    case kArm64Float32ToUint64:\n    case kArm64Float64ToUint64:\n    case kArm64Int32ToFloat32:\n    case kArm64Int32ToFloat64:\n    case kArm64Int64ToFloat32:\n    case kArm64Int64ToFloat64:\n    case kArm64Uint32ToFloat32:\n    case kArm64Uint32ToFloat64:\n    case kArm64Uint64ToFloat32:\n    case kArm64Uint64ToFloat64:\n    case kArm64Float64ExtractLowWord32:\n    case kArm64Float64ExtractHighWord32:\n    case kArm64Float64InsertLowWord32:\n    case kArm64Float64InsertHighWord32:\n    case kArm64Float64Mod:\n    case kArm64Float64MoveU64:\n    case kArm64U64MoveFloat64:\n    case kArm64Float64SilenceNaN:\n#if V8_ENABLE_WEBASSEMBLY\n    case kArm64Sadalp:\n    case kArm64Saddlp:\n    case kArm64Uadalp:\n    case kArm64Uaddlp:\n    case kArm64Smlal:\n    case kArm64Smlal2:\n    case kArm64Umlal:\n    case kArm64Umlal2:\n    case kArm64FAdd:\n    case kArm64FSub:\n    case kArm64FMul:\n    case kArm64FMulElement:\n    case kArm64FDiv:\n    case kArm64FMin:\n    case kArm64FMax:\n    case kArm64FEq:\n    case kArm64FNe:\n    case kArm64FLt:\n    case kArm64FLe:\n    case kArm64FGt:\n    case kArm64FGe:\n    case kArm64FExtractLane:\n    case kArm64FReplaceLane:\n    case kArm64FSplat:\n    case kArm64FAbs:\n    case kArm64FNeg:\n    case kArm64FSqrt:\n    case kArm64F64x2Qfma:\n    case kArm64F64x2Qfms:\n    case kArm64F64x2Pmin:\n    case kArm64F64x2Pmax:\n    case kArm64F64x2ConvertLowI32x4S:\n    case kArm64F64x2ConvertLowI32x4U:\n    case kArm64F64x2PromoteLowF32x4:\n    case kArm64F32x4SConvertI32x4:\n    case kArm64F32x4UConvertI32x4:\n    case kArm64F32x4Qfma:\n    case kArm64F32x4Qfms:\n    case kArm64F32x4Pmin:\n    case kArm64F32x4Pmax:\n    case kArm64F32x4DemoteF64x2Zero:\n    case kArm64IExtractLane:\n    case kArm64IReplaceLane:\n    case kArm64ISplat:\n    case kArm64IAbs:\n    case kArm64INeg:\n    case kArm64Mla:\n    case kArm64Mls:\n    case kArm64I64x2Shl:\n    case kArm64I64x2ShrS:\n    case kArm64I64x2Mul:\n    case kArm64I64x2ShrU:\n    case kArm64I64x2BitMask:\n    case kArm64I32x4SConvertF32x4:\n    case kArm64Sxtl:\n    case kArm64Sxtl2:\n    case kArm64Uxtl:\n    case kArm64Uxtl2:\n    case kArm64I32x4Shl:\n    case kArm64I32x4ShrS:\n    case kArm64I32x4Mul:\n    case kArm64I32x4UConvertF32x4:\n    case kArm64I32x4ShrU:\n    case kArm64I32x4BitMask:\n    case kArm64I32x4DotI16x8S:\n    case kArm64I16x8DotI8x16S:\n    case kArm64I32x4DotI8x16AddS:\n    case kArm64I32x4TruncSatF64x2SZero:\n    case kArm64I32x4TruncSatF64x2UZero:\n    case kArm64IExtractLaneU:\n    case kArm64IExtractLaneS:\n    case kArm64I16x8Shl:\n    case kArm64I16x8ShrS:\n    case kArm64I16x8SConvertI32x4:\n    case kArm64I16x8Mul:\n    case kArm64I16x8ShrU:\n    case kArm64I16x8UConvertI32x4:\n    case kArm64I16x8Q15MulRSatS:\n    case kArm64I16x8BitMask:\n    case kArm64I8x16Shl:\n    case kArm64I8x16ShrS:\n    case kArm64I8x16SConvertI16x8:\n    case kArm64I8x16UConvertI16x8:\n    case kArm64I8x16ShrU:\n    case kArm64I8x16BitMask:\n    case kArm64S128Const:\n    case kArm64S128Dup:\n    case kArm64S128And:\n    case kArm64S128Or:\n    case kArm64S128Xor:\n    case kArm64S128Not:\n    case kArm64S128Select:\n    case kArm64S128AndNot:\n    case kArm64Ssra:\n    case kArm64Usra:\n    case kArm64S32x4ZipLeft:\n    case kArm64S32x4ZipRight:\n    case kArm64S32x4UnzipLeft:\n    case kArm64S32x4UnzipRight:\n    case kArm64S32x4TransposeLeft:\n    case kArm64S32x4TransposeRight:\n    case kArm64S32x4OneLaneSwizzle:\n    case kArm64S32x4Shuffle:\n    case kArm64S16x8ZipLeft:\n    case kArm64S16x8ZipRight:\n    case kArm64S16x8UnzipLeft:\n    case kArm64S16x8UnzipRight:\n    case kArm64S16x8TransposeLeft:\n    case kArm64S16x8TransposeRight:\n    case kArm64S8x16ZipLeft:\n    case kArm64S8x16ZipRight:\n    case kArm64S8x16UnzipLeft:\n    case kArm64S8x16UnzipRight:\n    case kArm64S8x16TransposeLeft:\n    case kArm64S8x16TransposeRight:\n    case kArm64S8x16Concat:\n    case kArm64I8x16Swizzle:\n    case kArm64I8x16Shuffle:\n    case kArm64S32x4Reverse:\n    case kArm64S32x2Reverse:\n    case kArm64S16x4Reverse:\n    case kArm64S16x2Reverse:\n    case kArm64S8x8Reverse:\n    case kArm64S8x4Reverse:\n    case kArm64S8x2Reverse:\n    case kArm64V128AnyTrue:\n    case kArm64I64x2AllTrue:\n    case kArm64I32x4AllTrue:\n    case kArm64I16x8AllTrue:\n    case kArm64I8x16AllTrue:\n    case kArm64RoundingAverageU:\n    case kArm64IAdd:\n    case kArm64ISub:\n    case kArm64IEq:\n    case kArm64INe:\n    case kArm64IGtS:\n    case kArm64IGeS:\n    case kArm64ILtS:\n    case kArm64ILeS:\n    case kArm64IMinS:\n    case kArm64IMaxS:\n    case kArm64IMinU:\n    case kArm64IMaxU:\n    case kArm64IGtU:\n    case kArm64IGeU:\n    case kArm64IAddSatS:\n    case kArm64ISubSatS:\n    case kArm64IAddSatU:\n    case kArm64ISubSatU:\n#endif  // V8_ENABLE_WEBASSEMBLY\n    case kArm64TestAndBranch32:\n    case kArm64TestAndBranch:\n    case kArm64CompareAndBranch32:\n    case kArm64CompareAndBranch:\n      return kNoOpcodeFlags;\n\n    case kArm64LdrS:\n    case kArm64LdrD:\n    case kArm64LdrQ:\n    case kArm64Ldrb:\n    case kArm64Ldrsb:\n    case kArm64LdrsbW:\n    case kArm64Ldrh:\n    case kArm64Ldrsh:\n    case kArm64LdrshW:\n    case kArm64Ldrsw:\n    case kArm64LdrW:\n    case kArm64Ldr:\n    case kArm64LdrDecompressTaggedSigned:\n    case kArm64LdrDecompressTagged:\n    case kArm64LdrDecompressProtected:\n    case kArm64LdarDecompressTaggedSigned:\n    case kArm64LdarDecompressTagged:\n    case kArm64LdrDecodeSandboxedPointer:\n    case kArm64Peek:\n#if V8_ENABLE_WEBASSEMBLY\n    case kArm64LoadSplat:\n    case kArm64LoadLane:\n    case kArm64S128Load8x8S:\n    case kArm64S128Load8x8U:\n    case kArm64S128Load16x4S:\n    case kArm64S128Load16x4U:\n    case kArm64S128Load32x2S:\n    case kArm64S128Load32x2U:\n#endif  // V8_ENABLE_WEBASSEMBLY\n      return kIsLoadOperation;\n\n    case kArm64Claim:\n    case kArm64Poke:\n    case kArm64PokePair:\n    case kArm64StrS:\n    case kArm64StrD:\n    case kArm64StrQ:\n    case kArm64Strb:\n    case kArm64Strh:\n    case kArm64StrW:\n    case kArm64StrWPair:\n    case kArm64Str:\n    case kArm64StrPair:\n    case kArm64StrCompressTagged:\n    case kArm64StlrCompressTagged:\n    case kArm64StrIndirectPointer:\n    case kArm64StrEncodeSandboxedPointer:\n    case kArm64DmbIsh:\n    case kArm64DsbIsb:\n#if V8_ENABLE_WEBASSEMBLY\n    case kArm64StoreLane:\n#endif  // V8_ENABLE_WEBASSEMBLY\n      return kHasSideEffect;\n\n    case kArm64Word64AtomicLoadUint64:\n      return kIsLoadOperation;\n\n    case kArm64Word64AtomicStoreWord64:\n    case kArm64Word64AtomicAddUint64:\n    case kArm64Word64AtomicSubUint64:\n    case kArm64Word64AtomicAndUint64:\n    case kArm64Word64AtomicOrUint64:\n    case kArm64Word64AtomicXorUint64:\n    case kArm64Word64AtomicExchangeUint64:\n    case kArm64Word64AtomicCompareExchangeUint64:\n      return kHasSideEffect;\n\n#define CASE(Name) case k##Name:\n      COMMON_ARCH_OPCODE_LIST(CASE)\n#undef CASE\n      // Already covered in architecture independent code.\n      UNREACHABLE();\n  }\n\n  UNREACHABLE();\n}", "name_and_para": "int InstructionScheduler::GetTargetInstructionFlags(\n    const Instruction* instr) const "}, {"name": "InstructionScheduler::GetTargetInstructionFlags", "content": "int InstructionScheduler::GetTargetInstructionFlags(\n    const Instruction* instr) const {\n  switch (instr->arch_opcode()) {\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdd32:\n    case kRiscvBitcastDL:\n    case kRiscvBitcastLD:\n    case kRiscvByteSwap64:\n    case kRiscvCvtDL:\n    case kRiscvCvtDUl:\n    case kRiscvCvtSL:\n    case kRiscvCvtSUl:\n    case kRiscvMulHigh64:\n    case kRiscvMulHighU64:\n    case kRiscvAdd64:\n    case kRiscvAddOvf64:\n    case kRiscvClz64:\n    case kRiscvCtz64:\n    case kRiscvDiv64:\n    case kRiscvDivU64:\n    case kRiscvZeroExtendWord:\n    case kRiscvSignExtendWord:\n    case kRiscvMod64:\n    case kRiscvModU64:\n    case kRiscvMul64:\n    case kRiscvMulOvf64:\n    case kRiscvPopcnt64:\n    case kRiscvRor64:\n    case kRiscvSar64:\n    case kRiscvShl64:\n    case kRiscvShr64:\n    case kRiscvSub64:\n    case kRiscvSubOvf64:\n    case kRiscvFloat64RoundDown:\n    case kRiscvFloat64RoundTiesEven:\n    case kRiscvFloat64RoundTruncate:\n    case kRiscvFloat64RoundUp:\n    case kRiscvSub32:\n    case kRiscvTruncLD:\n    case kRiscvTruncLS:\n    case kRiscvTruncUlD:\n    case kRiscvTruncUlS:\n    case kRiscvCmp32:\n    case kRiscvCmpZero32:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvAdd32:\n    case kRiscvAddPair:\n    case kRiscvSubPair:\n    case kRiscvMulPair:\n    case kRiscvAndPair:\n    case kRiscvOrPair:\n    case kRiscvXorPair:\n    case kRiscvShlPair:\n    case kRiscvShrPair:\n    case kRiscvSarPair:\n    case kRiscvAddOvf:\n    case kRiscvSubOvf:\n    case kRiscvSub32:\n#endif\n    case kRiscvSh1add:\n    case kRiscvSh2add:\n    case kRiscvSh3add:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvAdduw:\n    case kRiscvSh1adduw:\n    case kRiscvSh2adduw:\n    case kRiscvSh3adduw:\n    case kRiscvSlliuw:\n#endif\n    case kRiscvAndn:\n    case kRiscvOrn:\n    case kRiscvXnor:\n    case kRiscvClz:\n    case kRiscvCtz:\n    case kRiscvCpop:\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvClzw:\n    case kRiscvCtzw:\n    case kRiscvCpopw:\n#endif\n    case kRiscvMax:\n    case kRiscvMaxu:\n    case kRiscvMin:\n    case kRiscvMinu:\n    case kRiscvSextb:\n    case kRiscvSexth:\n    case kRiscvZexth:\n    case kRiscvRev8:\n    case kRiscvBclr:\n    case kRiscvBclri:\n    case kRiscvBext:\n    case kRiscvBexti:\n    case kRiscvBinv:\n    case kRiscvBinvi:\n    case kRiscvBset:\n    case kRiscvBseti:\n    case kRiscvAbsD:\n    case kRiscvAbsS:\n    case kRiscvAddD:\n    case kRiscvAddS:\n    case kRiscvAnd:\n    case kRiscvAnd32:\n    case kRiscvAssertEqual:\n    case kRiscvBitcastInt32ToFloat32:\n    case kRiscvBitcastFloat32ToInt32:\n    case kRiscvByteSwap32:\n    case kRiscvCeilWD:\n    case kRiscvCeilWS:\n    case kRiscvClz32:\n    case kRiscvCmp:\n    case kRiscvCmpZero:\n    case kRiscvCmpD:\n    case kRiscvCmpS:\n    case kRiscvCtz32:\n    case kRiscvCvtDS:\n    case kRiscvCvtDUw:\n    case kRiscvCvtDW:\n    case kRiscvCvtSD:\n    case kRiscvCvtSUw:\n    case kRiscvCvtSW:\n    case kRiscvMulHighU32:\n    case kRiscvDiv32:\n    case kRiscvDivD:\n    case kRiscvDivS:\n    case kRiscvDivU32:\n    case kRiscvF64x2Abs:\n    case kRiscvF64x2Sqrt:\n    case kRiscvF64x2Pmin:\n    case kRiscvF64x2Pmax:\n    case kRiscvF64x2ConvertLowI32x4S:\n    case kRiscvF64x2ConvertLowI32x4U:\n    case kRiscvF64x2PromoteLowF32x4:\n    case kRiscvF64x2Ceil:\n    case kRiscvF64x2Floor:\n    case kRiscvF64x2Trunc:\n    case kRiscvF64x2NearestInt:\n    case kRiscvI64x2SplatI32Pair:\n    case kRiscvI64x2ExtractLane:\n    case kRiscvI64x2ReplaceLane:\n    case kRiscvI64x2ReplaceLaneI32Pair:\n    case kRiscvI64x2Shl:\n    case kRiscvI64x2ShrS:\n    case kRiscvI64x2ShrU:\n    case kRiscvF32x4Abs:\n    case kRiscvF32x4ExtractLane:\n    case kRiscvF32x4Sqrt:\n    case kRiscvF64x2Qfma:\n    case kRiscvF64x2Qfms:\n    case kRiscvF32x4Qfma:\n    case kRiscvF32x4Qfms:\n    case kRiscvF32x4ReplaceLane:\n    case kRiscvF32x4SConvertI32x4:\n    case kRiscvF32x4UConvertI32x4:\n    case kRiscvF32x4Pmin:\n    case kRiscvF32x4Pmax:\n    case kRiscvF32x4DemoteF64x2Zero:\n    case kRiscvF32x4Ceil:\n    case kRiscvF32x4Floor:\n    case kRiscvF32x4Trunc:\n    case kRiscvF32x4NearestInt:\n    case kRiscvF64x2ExtractLane:\n    case kRiscvF64x2ReplaceLane:\n    case kRiscvFloat32Max:\n    case kRiscvFloat32Min:\n    case kRiscvFloat32RoundDown:\n    case kRiscvFloat32RoundTiesEven:\n    case kRiscvFloat32RoundTruncate:\n    case kRiscvFloat32RoundUp:\n    case kRiscvFloat64ExtractLowWord32:\n    case kRiscvFloat64ExtractHighWord32:\n    case kRiscvFloat64InsertLowWord32:\n    case kRiscvFloat64InsertHighWord32:\n    case kRiscvFloat64Max:\n    case kRiscvFloat64Min:\n    case kRiscvFloat64SilenceNaN:\n    case kRiscvFloorWD:\n    case kRiscvFloorWS:\n    case kRiscvI64x2SConvertI32x4Low:\n    case kRiscvI64x2SConvertI32x4High:\n    case kRiscvI64x2UConvertI32x4Low:\n    case kRiscvI64x2UConvertI32x4High:\n    case kRiscvI16x8ExtractLaneU:\n    case kRiscvI16x8ExtractLaneS:\n    case kRiscvI16x8ReplaceLane:\n    case kRiscvI16x8Shl:\n    case kRiscvI16x8ShrS:\n    case kRiscvI16x8ShrU:\n    case kRiscvI32x4TruncSatF64x2SZero:\n    case kRiscvI32x4TruncSatF64x2UZero:\n    case kRiscvI32x4ExtractLane:\n    case kRiscvI32x4ReplaceLane:\n    case kRiscvI32x4SConvertF32x4:\n    case kRiscvI32x4Shl:\n    case kRiscvI32x4ShrS:\n    case kRiscvI32x4ShrU:\n    case kRiscvI32x4UConvertF32x4:\n    case kRiscvI8x16ExtractLaneU:\n    case kRiscvI8x16ExtractLaneS:\n    case kRiscvI8x16ReplaceLane:\n    case kRiscvI8x16Shl:\n    case kRiscvI8x16ShrS:\n    case kRiscvI8x16ShrU:\n    case kRiscvI8x16RoundingAverageU:\n    case kRiscvI8x16Popcnt:\n    case kRiscvMaxD:\n    case kRiscvMaxS:\n    case kRiscvMinD:\n    case kRiscvMinS:\n    case kRiscvMod32:\n    case kRiscvModU32:\n    case kRiscvMov:\n    case kRiscvMul32:\n    case kRiscvMulD:\n    case kRiscvMulHigh32:\n    case kRiscvMulOvf32:\n    case kRiscvMulS:\n    case kRiscvNegD:\n    case kRiscvNegS:\n    case kRiscvNor:\n    case kRiscvNor32:\n    case kRiscvOr:\n    case kRiscvOr32:\n    case kRiscvPopcnt32:\n    case kRiscvRor32:\n    case kRiscvRoundWD:\n    case kRiscvRoundWS:\n    case kRiscvVnot:\n    case kRiscvS128Select:\n    case kRiscvS128Const:\n    case kRiscvS128Zero:\n    case kRiscvS128Load32Zero:\n    case kRiscvS128Load64Zero:\n    case kRiscvS128AllOnes:\n    case kRiscvV128AnyTrue:\n    case kRiscvI8x16Shuffle:\n    case kRiscvVwmul:\n    case kRiscvVwmulu:\n    case kRiscvVmv:\n    case kRiscvVandVv:\n    case kRiscvVorVv:\n    case kRiscvVnotVv:\n    case kRiscvVxorVv:\n    case kRiscvVmvSx:\n    case kRiscvVmvXs:\n    case kRiscvVfmvVf:\n    case kRiscvVcompress:\n    case kRiscvVaddVv:\n    case kRiscvVwaddVv:\n    case kRiscvVwadduVv:\n    case kRiscvVwadduWx:\n    case kRiscvVsubVv:\n    case kRiscvVnegVv:\n    case kRiscvVfnegVv:\n    case kRiscvVmaxuVv:\n    case kRiscvVmax:\n    case kRiscvVminsVv:\n    case kRiscvVminuVv:\n    case kRiscvVmulVv:\n    case kRiscvVdivu:\n    case kRiscvVsmulVv:\n    case kRiscvVmslt:\n    case kRiscvVgtsVv:\n    case kRiscvVgesVv:\n    case kRiscvVgeuVv:\n    case kRiscvVgtuVv:\n    case kRiscvVeqVv:\n    case kRiscvVneVv:\n    case kRiscvVAbs:\n    case kRiscvVaddSatUVv:\n    case kRiscvVaddSatSVv:\n    case kRiscvVsubSatUVv:\n    case kRiscvVsubSatSVv:\n    case kRiscvVrgather:\n    case kRiscvVslidedown:\n    case kRiscvVredminuVs:\n    case kRiscvVAllTrue:\n    case kRiscvVnclipu:\n    case kRiscvVnclip:\n    case kRiscvVsll:\n    case kRiscvVfaddVv:\n    case kRiscvVfsubVv:\n    case kRiscvVfmulVv:\n    case kRiscvVfdivVv:\n    case kRiscvVfminVv:\n    case kRiscvVfmaxVv:\n    case kRiscvVmfeqVv:\n    case kRiscvVmfneVv:\n    case kRiscvVmfltVv:\n    case kRiscvVmfleVv:\n    case kRiscvVmergeVx:\n    case kRiscvVzextVf2:\n    case kRiscvVsextVf2:\n    case kRiscvSar32:\n    case kRiscvSignExtendByte:\n    case kRiscvSignExtendShort:\n    case kRiscvShl32:\n    case kRiscvShr32:\n    case kRiscvSqrtD:\n    case kRiscvSqrtS:\n    case kRiscvSubD:\n    case kRiscvSubS:\n    case kRiscvTruncUwD:\n    case kRiscvTruncUwS:\n    case kRiscvTruncWD:\n    case kRiscvTruncWS:\n    case kRiscvTst32:\n    case kRiscvXor:\n    case kRiscvXor32:\n      return kNoOpcodeFlags;\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvTst64:\n    case kRiscvLd:\n    case kRiscvLwu:\n    case kRiscvUlwu:\n    case kRiscvWord64AtomicLoadUint64:\n    case kRiscvLoadDecompressTaggedSigned:\n    case kRiscvLoadDecompressTagged:\n    case kRiscvLoadDecodeSandboxedPointer:\n    case kRiscvAtomicLoadDecompressTaggedSigned:\n    case kRiscvAtomicLoadDecompressTagged:\n    case kRiscvAtomicStoreCompressTagged:\n    case kRiscvLoadDecompressProtected:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvWord32AtomicPairLoad:\n#endif\n    case kRiscvLb:\n    case kRiscvLbu:\n    case kRiscvLoadDouble:\n    case kRiscvLh:\n    case kRiscvLhu:\n    case kRiscvLw:\n    case kRiscvLoadFloat:\n    case kRiscvRvvLd:\n    case kRiscvPeek:\n    case kRiscvUld:\n    case kRiscvULoadDouble:\n    case kRiscvUlh:\n    case kRiscvUlhu:\n    case kRiscvUlw:\n    case kRiscvULoadFloat:\n    case kRiscvS128LoadSplat:\n    case kRiscvS128Load64ExtendU:\n    case kRiscvS128Load64ExtendS:\n    case kRiscvS128LoadLane:\n      return kIsLoadOperation;\n\n#if V8_TARGET_ARCH_RISCV64\n    case kRiscvSd:\n    case kRiscvUsd:\n    case kRiscvWord64AtomicStoreWord64:\n    case kRiscvWord64AtomicAddUint64:\n    case kRiscvWord64AtomicSubUint64:\n    case kRiscvWord64AtomicAndUint64:\n    case kRiscvWord64AtomicOrUint64:\n    case kRiscvWord64AtomicXorUint64:\n    case kRiscvWord64AtomicExchangeUint64:\n    case kRiscvWord64AtomicCompareExchangeUint64:\n    case kRiscvStoreCompressTagged:\n    case kRiscvStoreEncodeSandboxedPointer:\n    case kRiscvStoreIndirectPointer:\n#elif V8_TARGET_ARCH_RISCV32\n    case kRiscvWord32AtomicPairStore:\n    case kRiscvWord32AtomicPairAdd:\n    case kRiscvWord32AtomicPairSub:\n    case kRiscvWord32AtomicPairAnd:\n    case kRiscvWord32AtomicPairOr:\n    case kRiscvWord32AtomicPairXor:\n    case kRiscvWord32AtomicPairExchange:\n    case kRiscvWord32AtomicPairCompareExchange:\n#endif\n    case kRiscvModD:\n    case kRiscvModS:\n    case kRiscvRvvSt:\n    case kRiscvPush:\n    case kRiscvSb:\n    case kRiscvStoreDouble:\n    case kRiscvSh:\n    case kRiscvStackClaim:\n    case kRiscvStoreToStackSlot:\n    case kRiscvSw:\n    case kRiscvStoreFloat:\n    case kRiscvUStoreDouble:\n    case kRiscvUsh:\n    case kRiscvUsw:\n    case kRiscvUStoreFloat:\n    case kRiscvSync:\n    case kRiscvS128StoreLane:\n      return kHasSideEffect;\n\n#define CASE(Name) case k##Name:\n      COMMON_ARCH_OPCODE_LIST(CASE)\n#undef CASE\n      // Already covered in architecture independent code.\n      UNREACHABLE();\n  }\n\n  UNREACHABLE();\n}", "name_and_para": "int InstructionScheduler::GetTargetInstructionFlags(\n    const Instruction* instr) const "}], [{"name": "InstructionScheduler::SchedulerSupported", "content": "bool InstructionScheduler::SchedulerSupported() { return true; }", "name_and_para": "bool InstructionScheduler::SchedulerSupported() "}, {"name": "InstructionScheduler::SchedulerSupported", "content": "bool InstructionScheduler::SchedulerSupported() { return true; }", "name_and_para": "bool InstructionScheduler::SchedulerSupported() "}]]]]