diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index 193c5a03525..5fda7e8b565 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -31,10 +31,13 @@ enum ImmediateMode {
 };
 
 // Adds Arm64-specific methods for generating operands.
-class Arm64OperandGenerator final : public OperandGenerator {
+template <typename Adapter>
+class Arm64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
  public:
-  explicit Arm64OperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
+  OPERAND_GENERATOR_T_BOILERPLATE(Adapter)
+
+  explicit Arm64OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
+      : super(selector) {}
 
   InstructionOperand UseOperand(Node* node, ImmediateMode mode) {
     if (CanBeImmediate(node, mode)) {
@@ -162,37 +165,44 @@ class Arm64OperandGenerator final : public OperandGenerator {
 
 namespace {
 
-void VisitRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+             Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
              Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)));
 }
 
-void VisitRRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRR(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+              Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 
-void VisitRRR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
 }
 
-void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
-                       Node* node, int width) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitSimdShiftRRR(InstructionSelectorT<Adapter>* selector,
+                       ArchOpcode opcode, Node* node, int width) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   if (g.IsIntegerConstant(node->InputAt(1))) {
     if (g.GetIntegerConstantValue(node->InputAt(1)) % width == 0) {
       selector->EmitIdentity(node);
@@ -208,33 +218,37 @@ void VisitSimdShiftRRR(InstructionSelector* selector, ArchOpcode opcode,
   }
 }
 
-void VisitRRI(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRI(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
               Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
 }
 
-void VisitRRO(InstructionSelector* selector, ArchOpcode opcode, Node* node,
-              ImmediateMode operand_mode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void VisitRRO(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+              Node* node, ImmediateMode operand_mode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)),
                  g.UseOperand(node->InputAt(1), operand_mode));
 }
 
-void VisitRRIR(InstructionSelector* selector, InstructionCode opcode,
+template <typename Adapter>
+void VisitRRIR(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
                Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   int32_t imm = OpParameter<int32_t>(node->op());
   selector->Emit(opcode, g.DefineAsRegister(node),
                  g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
                  g.UseUniqueRegister(node->InputAt(1)));
 }
 
+template <typename Adapter>
 struct ExtendingLoadMatcher {
-  ExtendingLoadMatcher(Node* node, InstructionSelector* selector)
+  ExtendingLoadMatcher(Node* node, InstructionSelectorT<Adapter>* selector)
       : matches_(false), selector_(selector), base_(nullptr), immediate_(0) {
     Initialize(node);
   }
@@ -256,7 +270,7 @@ struct ExtendingLoadMatcher {
 
  private:
   bool matches_;
-  InstructionSelector* selector_;
+  InstructionSelectorT<Adapter>* selector_;
   Node* base_;
   int64_t immediate_;
   ArchOpcode opcode_;
@@ -269,7 +283,7 @@ struct ExtendingLoadMatcher {
     DCHECK(m.IsWord64Sar());
     if (m.left().IsLoad() && m.right().Is(32) &&
         selector_->CanCover(m.node(), m.left().node())) {
-      Arm64OperandGenerator g(selector_);
+      Arm64OperandGeneratorT<Adapter> g(selector_);
       Node* load = m.left().node();
       Node* offset = load->InputAt(1);
       base_ = load->InputAt(0);
@@ -282,14 +296,17 @@ struct ExtendingLoadMatcher {
   }
 };
 
-bool TryMatchExtendingLoad(InstructionSelector* selector, Node* node) {
-  ExtendingLoadMatcher m(node, selector);
+template <typename Adapter>
+bool TryMatchExtendingLoad(InstructionSelectorT<Adapter>* selector,
+                           Node* node) {
+  ExtendingLoadMatcher<Adapter> m(node, selector);
   return m.Matches();
 }
 
-bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node) {
-  ExtendingLoadMatcher m(node, selector);
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool TryEmitExtendingLoad(InstructionSelectorT<Adapter>* selector, Node* node) {
+  ExtendingLoadMatcher<Adapter> m(node, selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   if (m.Matches()) {
     InstructionOperand inputs[2];
     inputs[0] = g.UseRegister(m.base());
@@ -305,10 +322,11 @@ bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node) {
   return false;
 }
 
-bool TryMatchAnyShift(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool TryMatchAnyShift(InstructionSelectorT<Adapter>* selector, Node* node,
                       Node* input_node, InstructionCode* opcode, bool try_ror,
                       MachineRepresentation rep) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   if (!selector->CanCover(node, input_node)) return false;
   if (input_node->InputCount() != 2) return false;
@@ -359,8 +377,10 @@ bool TryMatchAnyShift(InstructionSelector* selector, Node* node,
   }
 }
 
-bool TryMatchAnyExtend(Arm64OperandGenerator* g, InstructionSelector* selector,
-                       Node* node, Node* left_node, Node* right_node,
+template <typename Adapter>
+bool TryMatchAnyExtend(Arm64OperandGeneratorT<Adapter>* g,
+                       InstructionSelectorT<Adapter>* selector, Node* node,
+                       Node* left_node, Node* right_node,
                        InstructionOperand* left_op,
                        InstructionOperand* right_op, InstructionCode* opcode) {
   if (!selector->CanCover(node, right_node)) return false;
@@ -402,8 +422,9 @@ bool TryMatchAnyExtend(Arm64OperandGenerator* g, InstructionSelector* selector,
   return false;
 }
 
-bool TryMatchLoadStoreShift(Arm64OperandGenerator* g,
-                            InstructionSelector* selector,
+template <typename Adapter>
+bool TryMatchLoadStoreShift(Arm64OperandGeneratorT<Adapter>* g,
+                            InstructionSelectorT<Adapter>* selector,
                             MachineRepresentation rep, Node* node, Node* index,
                             InstructionOperand* index_op,
                             InstructionOperand* shift_immediate_op) {
@@ -491,11 +512,11 @@ uint8_t GetBinopProperties(InstructionCode opcode) {
 }
 
 // Shared routine for multiple binary operations.
-template <typename Matcher>
-void VisitBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter, typename Matcher>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, ImmediateMode operand_mode,
                 FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand inputs[5];
   size_t input_count = 0;
   InstructionOperand outputs[1];
@@ -567,17 +588,17 @@ void VisitBinop(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple binary operations.
-template <typename Matcher>
-void VisitBinop(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                ImmediateMode operand_mode) {
+template <typename Adapter, typename Matcher>
+void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
+                ArchOpcode opcode, ImmediateMode operand_mode) {
   FlagsContinuation cont;
-  VisitBinop<Matcher>(selector, node, opcode, operand_mode, &cont);
+  VisitBinop<Adapter, Matcher>(selector, node, opcode, operand_mode, &cont);
 }
 
-template <typename Matcher>
-void VisitAddSub(InstructionSelector* selector, Node* node, ArchOpcode opcode,
-                 ArchOpcode negate_opcode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter, typename Matcher>
+void VisitAddSub(InstructionSelectorT<Adapter>* selector, Node* node,
+                 ArchOpcode opcode, ArchOpcode negate_opcode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Matcher m(node);
   if (m.right().HasResolvedValue() && (m.right().ResolvedValue() < 0) &&
       (m.right().ResolvedValue() > std::numeric_limits<int>::min()) &&
@@ -586,7 +607,7 @@ void VisitAddSub(InstructionSelector* selector, Node* node, ArchOpcode opcode,
         negate_opcode, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
         g.TempImmediate(static_cast<int32_t>(-m.right().ResolvedValue())));
   } else {
-    VisitBinop<Matcher>(selector, node, opcode, kArithmeticImm);
+    VisitBinop<Adapter, Matcher>(selector, node, opcode, kArithmeticImm);
   }
 }
 
@@ -607,9 +628,11 @@ int32_t LeftShiftForReducedMultiply(Matcher* m) {
 
 }  // namespace
 
-void InstructionSelector::VisitTraceInstruction(Node* node) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTraceInstruction(Node* node) {}
 
-void InstructionSelector::VisitStackSlot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackSlot(Node* node) {
   StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
   int slot = frame_->AllocateSpillSlot(rep.size(), rep.alignment());
   OperandGenerator g(this);
@@ -618,15 +641,17 @@ void InstructionSelector::VisitStackSlot(Node* node) {
        sequence()->AddImmediate(Constant(slot)), 0, nullptr);
 }
 
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), x1));
 }
 
-void EmitLoad(InstructionSelector* selector, Node* node, InstructionCode opcode,
-              ImmediateMode immediate_mode, MachineRepresentation rep,
-              Node* output = nullptr) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitLoad(InstructionSelectorT<Adapter>* selector, Node* node,
+              InstructionCode opcode, ImmediateMode immediate_mode,
+              MachineRepresentation rep, Node* output = nullptr) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[3];
@@ -686,10 +711,11 @@ namespace {
 // Manually add base and index into a register to get the actual address.
 // This should be used prior to instructions that only support
 // immediate/post-index addressing, like ld1 and st1.
-InstructionOperand EmitAddBeforeLoadOrStore(InstructionSelector* selector,
-                                            Node* node,
-                                            InstructionCode* opcode) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+InstructionOperand EmitAddBeforeLoadOrStore(
+    InstructionSelectorT<Adapter>* selector, Node* node,
+    InstructionCode* opcode) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand addr = g.TempRegister();
   selector->Emit(kArm64Add, addr, g.UseRegister(node->InputAt(0)),
                  g.UseRegister(node->InputAt(1)));
@@ -698,7 +724,8 @@ InstructionOperand EmitAddBeforeLoadOrStore(InstructionSelector* selector,
 }
 }  // namespace
 
-void InstructionSelector::VisitLoadLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadLane(Node* node) {
   LoadLaneParameters params = LoadLaneParametersOf(node->op());
   DCHECK(
       params.rep == MachineType::Int8() || params.rep == MachineType::Int16() ||
@@ -710,13 +737,14 @@ void InstructionSelector::VisitLoadLane(Node* node) {
     opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
   }
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand addr = EmitAddBeforeLoadOrStore(this, node, &opcode);
   Emit(opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(2)),
        g.UseImmediate(params.laneidx), addr, g.TempImmediate(0));
 }
 
-void InstructionSelector::VisitStoreLane(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
   StoreLaneParameters params = StoreLaneParametersOf(node->op());
   DCHECK_LE(MachineRepresentation::kWord8, params.rep);
   DCHECK_GE(MachineRepresentation::kWord64, params.rep);
@@ -728,7 +756,7 @@ void InstructionSelector::VisitStoreLane(Node* node) {
     opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
   }
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand addr = EmitAddBeforeLoadOrStore(this, node, &opcode);
   InstructionOperand inputs[4] = {
       g.UseRegister(node->InputAt(2)),
@@ -740,7 +768,8 @@ void InstructionSelector::VisitStoreLane(Node* node) {
   Emit(opcode, 0, nullptr, 4, inputs);
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
   LoadTransformParameters params = LoadTransformParametersOf(node->op());
   InstructionCode opcode = kArchNop;
   bool require_add = false;
@@ -795,7 +824,7 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   // ARM64 supports unaligned loads
   DCHECK_NE(params.kind, MemoryAccessKind::kUnaligned);
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[2];
@@ -820,7 +849,8 @@ void InstructionSelector::VisitLoadTransform(Node* node) {
   Emit(opcode, 1, outputs, 2, inputs);
 }
 
-void InstructionSelector::VisitLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   InstructionCode opcode = kArchNop;
   ImmediateMode immediate_mode = kNoImmediate;
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
@@ -905,14 +935,21 @@ void InstructionSelector::VisitLoad(Node* node) {
   EmitLoad(this, node, opcode, immediate_mode, rep);
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) { VisitLoad(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+  VisitLoad(node);
+}
 
-void InstructionSelector::VisitStorePair(Node* node) { VisitStore(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+  VisitStore(node);
+}
 
-void InstructionSelector::VisitStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStore(Node* node) {
   const bool kStorePair = node->opcode() == IrOpcode::kStorePair;
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -1143,23 +1180,33 @@ void InstructionSelector::VisitStore(Node* node) {
   Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) { VisitStore(node); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
+  VisitStore(node);
+}
 
-void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
   UNREACHABLE();
 }
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
-void InstructionSelector::VisitUnalignedLoad(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+  UNREACHABLE();
+}
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
-void InstructionSelector::VisitUnalignedStore(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+  UNREACHABLE();
+}
 
-template <typename Matcher>
-static void VisitLogical(InstructionSelector* selector, Node* node, Matcher* m,
-                         ArchOpcode opcode, bool left_can_cover,
+template <typename Adapter, typename Matcher>
+static void VisitLogical(InstructionSelectorT<Adapter>* selector, Node* node,
+                         Matcher* m, ArchOpcode opcode, bool left_can_cover,
                          bool right_can_cover, ImmediateMode imm_mode) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   // Map instruction to equivalent operation with inverted right input.
   ArchOpcode inv_opcode = opcode;
@@ -1218,12 +1265,13 @@ static void VisitLogical(InstructionSelector* selector, Node* node, Matcher* m,
     selector->Emit(kArm64Not, g.DefineAsRegister(node),
                    g.UseRegister(m->left().node()));
   } else {
-    VisitBinop<Matcher>(selector, node, opcode, imm_mode);
+    VisitBinop<Adapter, Matcher>(selector, node, opcode, imm_mode);
   }
 }
 
-void InstructionSelector::VisitWord32And(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32And(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   if (m.left().IsWord32Shr() && CanCover(node, m.left().node()) &&
       m.right().HasResolvedValue()) {
@@ -1257,13 +1305,14 @@ void InstructionSelector::VisitWord32And(Node* node) {
       // Other cases fall through to the normal And operation.
     }
   }
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64And32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64And(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64And(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   if (m.left().IsWord64Shr() && CanCover(node, m.left().node()) &&
       m.right().HasResolvedValue()) {
@@ -1298,44 +1347,49 @@ void InstructionSelector::VisitWord64And(Node* node) {
       // Other cases fall through to the normal And operation.
     }
   }
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64And, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Or(Node* node) {
   Int32BinopMatcher m(node);
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64Or32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64Or(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Or(Node* node) {
   Int64BinopMatcher m(node);
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64Or, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Xor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
   Int32BinopMatcher m(node);
-  VisitLogical<Int32BinopMatcher>(
+  VisitLogical<Adapter, Int32BinopMatcher>(
       this, node, &m, kArm64Eor32, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical32Imm);
 }
 
-void InstructionSelector::VisitWord64Xor(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
   Int64BinopMatcher m(node);
-  VisitLogical<Int64BinopMatcher>(
+  VisitLogical<Adapter, Int64BinopMatcher>(
       this, node, &m, kArm64Eor, CanCover(node, m.left().node()),
       CanCover(node, m.right().node()), kLogical64Imm);
 }
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shl(Node* node) {
   Int32BinopMatcher m(node);
   if (m.left().IsWord32And() && CanCover(node, m.left().node()) &&
       m.right().IsInRange(1, 31)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Int32BinopMatcher mleft(m.left().node());
     if (mleft.right().HasResolvedValue()) {
       uint32_t mask = mleft.right().ResolvedValue();
@@ -1367,8 +1421,9 @@ void InstructionSelector::VisitWord32Shl(Node* node) {
   VisitRRO(this, kArm64Lsl32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Shl(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shl(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   if ((m.left().IsChangeInt32ToInt64() || m.left().IsChangeUint32ToUint64()) &&
       m.right().IsInRange(32, 63) && CanCover(node, m.left().node())) {
@@ -1382,13 +1437,14 @@ void InstructionSelector::VisitWord64Shl(Node* node) {
   VisitRRO(this, kArm64Lsl, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitStackPointerGreaterThan(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
     Node* node, FlagsContinuation* cont) {
   StackCheckKind kind = StackCheckKindOf(node->op());
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   // No outputs.
   InstructionOperand* const outputs = nullptr;
@@ -1413,8 +1469,10 @@ void InstructionSelector::VisitStackPointerGreaterThan(
 
 namespace {
 
-bool TryEmitBitfieldExtract32(InstructionSelector* selector, Node* node) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool TryEmitBitfieldExtract32(InstructionSelectorT<Adapter>* selector,
+                              Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   if (selector->CanCover(node, m.left().node()) && m.left().IsWord32Shl()) {
     // Select Ubfx or Sbfx for (x << (K & 0x1F)) OP (K & 0x1F), where
@@ -1441,7 +1499,8 @@ bool TryEmitBitfieldExtract32(InstructionSelector* selector, Node* node) {
 
 }  // namespace
 
-void InstructionSelector::VisitWord32Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Shr(Node* node) {
   Int32BinopMatcher m(node);
   if (m.left().IsWord32And() && m.right().HasResolvedValue()) {
     uint32_t lsb = m.right().ResolvedValue() & 0x1F;
@@ -1455,7 +1514,7 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
       unsigned mask_width = base::bits::CountPopulation(mask);
       unsigned mask_msb = base::bits::CountLeadingZeros32(mask);
       if ((mask_msb + mask_width + lsb) == 32) {
-        Arm64OperandGenerator g(this);
+        Arm64OperandGeneratorT<Adapter> g(this);
         DCHECK_EQ(lsb, base::bits::CountTrailingZeros32(mask));
         Emit(kArm64Ubfx32, g.DefineAsRegister(node),
              g.UseRegister(mleft.left().node()),
@@ -1472,7 +1531,7 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
       CanCover(node, node->InputAt(0))) {
     // Combine this shift with the multiply and shift that would be generated
     // by Uint32MulHigh.
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Node* left = m.left().node();
     int shift = m.right().ResolvedValue() & 0x1F;
     InstructionOperand const smull_operand = g.TempRegister();
@@ -1486,7 +1545,8 @@ void InstructionSelector::VisitWord32Shr(Node* node) {
   VisitRRO(this, kArm64Lsr32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Shr(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Shr(Node* node) {
   Int64BinopMatcher m(node);
   if (m.left().IsWord64And() && m.right().HasResolvedValue()) {
     uint32_t lsb = m.right().ResolvedValue() & 0x3F;
@@ -1500,7 +1560,7 @@ void InstructionSelector::VisitWord64Shr(Node* node) {
       unsigned mask_width = base::bits::CountPopulation(mask);
       unsigned mask_msb = base::bits::CountLeadingZeros64(mask);
       if ((mask_msb + mask_width + lsb) == 64) {
-        Arm64OperandGenerator g(this);
+        Arm64OperandGeneratorT<Adapter> g(this);
         DCHECK_EQ(lsb, base::bits::CountTrailingZeros64(mask));
         Emit(kArm64Ubfx, g.DefineAsRegister(node),
              g.UseRegister(mleft.left().node()),
@@ -1513,7 +1573,8 @@ void InstructionSelector::VisitWord64Shr(Node* node) {
   VisitRRO(this, kArm64Lsr, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitWord32Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Sar(Node* node) {
   if (TryEmitBitfieldExtract32(this, node)) {
     return;
   }
@@ -1523,7 +1584,7 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
       CanCover(node, node->InputAt(0))) {
     // Combine this shift with the multiply and shift that would be generated
     // by Int32MulHigh.
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Node* left = m.left().node();
     int shift = m.right().ResolvedValue() & 0x1F;
     InstructionOperand const smull_operand = g.TempRegister();
@@ -1544,7 +1605,7 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
       // on the left of this Sar operation. We do it here, as the result of the
       // add potentially has 33 bits, so we have to ensure the result is
       // truncated by being the input to this 32-bit Sar operation.
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       Node* mul_node = madd_node.left().node();
 
       InstructionOperand const smull_operand = g.TempRegister();
@@ -1565,7 +1626,8 @@ void InstructionSelector::VisitWord32Sar(Node* node) {
   VisitRRO(this, kArm64Asr32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Sar(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Sar(Node* node) {
   if (TryEmitExtendingLoad(this, node)) return;
 
   // Select Sbfx(x, imm, 32-imm) for Word64Sar(ChangeInt32ToInt64(x), imm)
@@ -1578,7 +1640,7 @@ void InstructionSelector::VisitWord64Sar(Node* node) {
     if ((m.left().InputAt(0)->opcode() != IrOpcode::kLoad &&
          m.left().InputAt(0)->opcode() != IrOpcode::kLoadImmutable) ||
         !CanCover(m.left().node(), m.left().InputAt(0))) {
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       int right = static_cast<int>(m.right().ResolvedValue());
       Emit(kArm64Sbfx, g.DefineAsRegister(node),
            g.UseRegister(m.left().node()->InputAt(0)),
@@ -1590,15 +1652,23 @@ void InstructionSelector::VisitWord64Sar(Node* node) {
   VisitRRO(this, kArm64Asr, node, kShift64Imm);
 }
 
-void InstructionSelector::VisitWord32Rol(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Rol(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64Rol(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Rol(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord32Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ror(Node* node) {
   VisitRRO(this, kArm64Ror32, node, kShift32Imm);
 }
 
-void InstructionSelector::VisitWord64Ror(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ror(Node* node) {
   VisitRRO(this, kArm64Ror, node, kShift64Imm);
 }
 
@@ -1677,28 +1747,37 @@ void InstructionSelector::VisitWord64Ror(Node* node) {
   V(Float64Min, kArm64Float64Min) \
   V(I8x16Swizzle, kArm64I8x16Swizzle)
 
-#define RR_VISITOR(Name, opcode)                      \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, opcode, node);                      \
+#define RR_VISITOR(Name, opcode)                                \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, opcode, node);                                \
   }
 RR_OP_LIST(RR_VISITOR)
 #undef RR_VISITOR
 #undef RR_OP_LIST
 
-#define RRR_VISITOR(Name, opcode)                     \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, opcode, node);                     \
+#define RRR_VISITOR(Name, opcode)                               \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, opcode, node);                               \
   }
 RRR_OP_LIST(RRR_VISITOR)
 #undef RRR_VISITOR
 #undef RRR_OP_LIST
 
-void InstructionSelector::VisitWord32Ctz(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Ctz(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitWord64Ctz(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Ctz(Node* node) {
+  UNREACHABLE();
+}
 
-void InstructionSelector::VisitInt32Add(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Add(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
   // Select Madd(x, y, z) for Add(Mul(x, y), z).
   if (m.left().IsInt32Mul() && CanCover(node, m.left().node())) {
@@ -1724,11 +1803,12 @@ void InstructionSelector::VisitInt32Add(Node* node) {
       return;
     }
   }
-  VisitAddSub<Int32BinopMatcher>(this, node, kArm64Add32, kArm64Sub32);
+  VisitAddSub<Adapter, Int32BinopMatcher>(this, node, kArm64Add32, kArm64Sub32);
 }
 
-void InstructionSelector::VisitInt64Add(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
   // Select Madd(x, y, z) for Add(Mul(x, y), z).
   if (m.left().IsInt64Mul() && CanCover(node, m.left().node())) {
@@ -1754,11 +1834,12 @@ void InstructionSelector::VisitInt64Add(Node* node) {
       return;
     }
   }
-  VisitAddSub<Int64BinopMatcher>(this, node, kArm64Add, kArm64Sub);
+  VisitAddSub<Adapter, Int64BinopMatcher>(this, node, kArm64Add, kArm64Sub);
 }
 
-void InstructionSelector::VisitInt32Sub(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Sub(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
 
   // Select Msub(x, y, a) for Sub(a, Mul(x, y)).
@@ -1774,11 +1855,12 @@ void InstructionSelector::VisitInt32Sub(Node* node) {
     }
   }
 
-  VisitAddSub<Int32BinopMatcher>(this, node, kArm64Sub32, kArm64Add32);
+  VisitAddSub<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32, kArm64Add32);
 }
 
-void InstructionSelector::VisitInt64Sub(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
 
   // Select Msub(x, y, a) for Sub(a, Mul(x, y)).
@@ -1794,14 +1876,15 @@ void InstructionSelector::VisitInt64Sub(Node* node) {
     }
   }
 
-  VisitAddSub<Int64BinopMatcher>(this, node, kArm64Sub, kArm64Add);
+  VisitAddSub<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArm64Add);
 }
 
 namespace {
 
-void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
-                              FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
+                              Node* node, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
   InstructionOperand left = g.UseRegister(m.left().node());
@@ -1822,9 +1905,10 @@ void EmitInt32MulWithOverflow(InstructionSelector* selector, Node* node,
   selector->EmitWithContinuation(opcode, result, result, cont);
 }
 
-void EmitInt64MulWithOverflow(InstructionSelector* selector, Node* node,
-                              FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+void EmitInt64MulWithOverflow(InstructionSelectorT<Adapter>* selector,
+                              Node* node, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Int64BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
   InstructionOperand left = g.UseRegister(m.left().node());
@@ -1843,8 +1927,9 @@ void EmitInt64MulWithOverflow(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitInt32Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int32BinopMatcher m(node);
 
   // First, try to reduce the multiplication to addition with left shift.
@@ -1884,8 +1969,9 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   VisitRRR(this, kArm64Mul32, node);
 }
 
-void InstructionSelector::VisitInt64Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Int64BinopMatcher m(node);
 
   // First, try to reduce the multiplication to addition with left shift.
@@ -1925,113 +2011,136 @@ void InstructionSelector::VisitInt64Mul(Node* node) {
 }
 
 namespace {
-void VisitExtMul(InstructionSelector* selector, ArchOpcode opcode, Node* node,
-                 int dst_lane_size) {
+template <typename Adapter>
+void VisitExtMul(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
+                 Node* node, int dst_lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(dst_lane_size);
   VisitRRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI16x8ExtMulLowI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulLowI8x16S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulHighI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulHighI8x16S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulLowI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulLowI8x16U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtMulHighI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtMulHighI8x16U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 16);
 }
 
-void InstructionSelector::VisitI32x4ExtMulLowI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulLowI16x8S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulHighI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulHighI16x8S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulLowI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulLowI16x8U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtMulHighI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtMulHighI16x8U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 32);
 }
 
-void InstructionSelector::VisitI64x2ExtMulLowI32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulLowI32x4S(Node* node) {
   VisitExtMul(this, kArm64Smull, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulHighI32x4S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulHighI32x4S(Node* node) {
   VisitExtMul(this, kArm64Smull2, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulLowI32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulLowI32x4U(Node* node) {
   VisitExtMul(this, kArm64Umull, node, 64);
 }
 
-void InstructionSelector::VisitI64x2ExtMulHighI32x4U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2ExtMulHighI32x4U(Node* node) {
   VisitExtMul(this, kArm64Umull2, node, 64);
 }
 
 namespace {
-void VisitExtAddPairwise(InstructionSelector* selector, ArchOpcode opcode,
-                         Node* node, int dst_lane_size) {
+template <typename Adapter>
+void VisitExtAddPairwise(InstructionSelectorT<Adapter>* selector,
+                         ArchOpcode opcode, Node* node, int dst_lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(dst_lane_size);
   VisitRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
   VisitExtAddPairwise(this, kArm64Saddlp, node, 32);
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
   VisitExtAddPairwise(this, kArm64Uaddlp, node, 32);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
   VisitExtAddPairwise(this, kArm64Saddlp, node, 16);
 }
 
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
   VisitExtAddPairwise(this, kArm64Uaddlp, node, 16);
 }
 
-void InstructionSelector::VisitInt32MulHigh(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulHigh(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand const smull_operand = g.TempRegister();
   Emit(kArm64Smull, smull_operand, g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)));
   Emit(kArm64Asr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));
 }
 
-void InstructionSelector::VisitInt64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulHigh(Node* node) {
   return VisitRRR(this, kArm64Smulh, node);
 }
 
-void InstructionSelector::VisitUint32MulHigh(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32MulHigh(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand const smull_operand = g.TempRegister();
   Emit(kArm64Umull, smull_operand, g.UseRegister(node->InputAt(0)),
        g.UseRegister(node->InputAt(1)));
   Emit(kArm64Lsr, g.DefineAsRegister(node), smull_operand, g.TempImmediate(32));
 }
 
-void InstructionSelector::VisitUint64MulHigh(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64MulHigh(Node* node) {
   return VisitRRR(this, kArm64Umulh, node);
 }
 
-void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float32ToInt32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2040,8 +2149,9 @@ void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat32ToUint32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float32ToUint32;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2052,8 +2162,9 @@ void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2068,8 +2179,9 @@ void InstructionSelector::VisitTryTruncateFloat32ToInt64(Node* node) {
   Emit(kArm64Float32ToInt64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateFloat64ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionCode opcode = kArm64Float64ToInt64;
   TruncateKind kind = OpParameter<TruncateKind>(node->op());
@@ -2080,8 +2192,9 @@ void InstructionSelector::VisitTruncateFloat64ToInt64(Node* node) {
   Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2096,8 +2209,10 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt64(Node* node) {
   Emit(kArm64Float64ToInt64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat32ToUint64(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2112,8 +2227,10 @@ void InstructionSelector::VisitTryTruncateFloat32ToUint64(Node* node) {
   Emit(kArm64Float32ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint64(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
@@ -2128,8 +2245,9 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint64(Node* node) {
   Emit(kArm64Float64ToUint64, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -2143,8 +2261,10 @@ void InstructionSelector::VisitTryTruncateFloat64ToInt32(Node* node) {
   Emit(kArm64Float64ToInt32, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTryTruncateFloat64ToUint32(
+    Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand inputs[] = {g.UseRegister(node->InputAt(0))};
   InstructionOperand outputs[2];
   size_t output_count = 0;
@@ -2158,13 +2278,15 @@ void InstructionSelector::VisitTryTruncateFloat64ToUint32(Node* node) {
   Emit(kArm64Float64ToUint32, output_count, outputs, 1, inputs);
 }
 
-void InstructionSelector::VisitBitcastWord32ToWord64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWord32ToWord64(Node* node) {
   DCHECK(SmiValuesAre31Bits());
   DCHECK(COMPRESS_POINTERS_BOOL);
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeInt32ToInt64(Node* node) {
   Node* value = node->InputAt(0);
   if ((value->opcode() == IrOpcode::kLoad ||
        value->opcode() == IrOpcode::kLoadImmutable) &&
@@ -2204,7 +2326,7 @@ void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
   if (value->opcode() == IrOpcode::kWord32Sar && CanCover(node, value)) {
     Int32BinopMatcher m(value);
     if (m.right().HasResolvedValue()) {
-      Arm64OperandGenerator g(this);
+      Arm64OperandGeneratorT<Adapter> g(this);
       // Mask the shift amount, to keep the same semantics as Word32Sar.
       int right = m.right().ResolvedValue() & 0x1F;
       Emit(kArm64Sbfx, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
@@ -2216,7 +2338,9 @@ void InstructionSelector::VisitChangeInt32ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtw, node);
 }
 
-bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::ZeroExtendsWord32ToWord64NoPhis(
+    Node* node) {
   DCHECK_NE(node->opcode(), IrOpcode::kPhi);
   switch (node->opcode()) {
     case IrOpcode::kWord32And:
@@ -2266,8 +2390,9 @@ bool InstructionSelector::ZeroExtendsWord32ToWord64NoPhis(Node* node) {
   }
 }
 
-void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* value = node->InputAt(0);
   if (ZeroExtendsWord32ToWord64(value)) {
     return EmitIdentity(node);
@@ -2275,44 +2400,51 @@ void InstructionSelector::VisitChangeUint32ToUint64(Node* node) {
   Emit(kArm64Mov32, g.DefineAsRegister(node), g.UseRegister(value));
 }
 
-void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTruncateInt64ToInt32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   // The top 32 bits in the 64-bit register will be undefined, and
   // must not be used by a dependent node.
   EmitIdentity(node);
 }
 
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mod(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64Float64Mod, g.DefineAsFixed(node, d0),
        g.UseFixed(node->InputAt(0), d0), g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
-                                                   InstructionCode opcode) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Binop(
+    Node* node, InstructionCode opcode) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0),
        g.UseFixed(node->InputAt(1), d1))
       ->MarkAsCall();
 }
 
-void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
-                                                  InstructionCode opcode) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
+    Node* node, InstructionCode opcode) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(opcode, g.DefineAsFixed(node, d0), g.UseFixed(node->InputAt(0), d0))
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
 
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
+    InstructionOperand* op, LinkageLocation location) {}
 
-void InstructionSelector::EmitPrepareArguments(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   // `arguments` includes alignment "holes". This means that slots bigger than
   // kSystemPointerSize, e.g. Simd128, will span across multiple arguments.
@@ -2358,10 +2490,11 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
     Node* node) {
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
 
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
@@ -2385,16 +2518,20 @@ void InstructionSelector::EmitPrepareResults(
   }
 }
 
-bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
+template <typename Adapter>
+bool InstructionSelectorT<Adapter>::IsTailCallAddressImmediate() {
+  return false;
+}
 
 namespace {
 
 // Shared routine for multiple compare operations.
-void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                  InstructionOperand left, InstructionOperand right,
-                  FlagsContinuation* cont) {
+template <typename Adapter>
+void VisitCompare(InstructionSelectorT<Adapter>* selector,
+                  InstructionCode opcode, InstructionOperand left,
+                  InstructionOperand right, FlagsContinuation* cont) {
   if (cont->IsSelect()) {
-    Arm64OperandGenerator g(selector);
+    Arm64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[] = {left, right,
                                    g.UseRegister(cont->true_value()),
                                    g.UseRegister(cont->false_value())};
@@ -2456,12 +2593,11 @@ FlagsCondition MapForFlagSettingBinop(FlagsCondition cond) {
 // (a <ops> b), b.<cond'>
 // where <ops> is the flag setting version of <op>, and if so,
 // updates {node}, {opcode} and {cont} accordingly.
-void MaybeReplaceCmpZeroWithFlagSettingBinop(InstructionSelector* selector,
-                                             Node** node, Node* binop,
-                                             ArchOpcode* opcode,
-                                             FlagsCondition cond,
-                                             FlagsContinuation* cont,
-                                             ImmediateMode* immediate_mode) {
+template <typename Adapter>
+void MaybeReplaceCmpZeroWithFlagSettingBinop(
+    InstructionSelectorT<Adapter>* selector, Node** node, Node* binop,
+    ArchOpcode* opcode, FlagsCondition cond, FlagsContinuation* cont,
+    ImmediateMode* immediate_mode) {
   ArchOpcode binop_opcode;
   ArchOpcode no_output_opcode;
   ImmediateMode binop_immediate_mode;
@@ -2528,7 +2664,8 @@ FlagsCondition MapForCbz(FlagsCondition cond) {
   }
 }
 
-void EmitBranchOrDeoptimize(InstructionSelector* selector,
+template <typename Adapter>
+void EmitBranchOrDeoptimize(InstructionSelectorT<Adapter>* selector,
                             InstructionCode opcode, InstructionOperand value,
                             FlagsContinuation* cont) {
   DCHECK(cont->IsBranch() || cont->IsDeoptimize());
@@ -2561,8 +2698,8 @@ struct CbzOrTbzMatchTrait<64> {
 
 // Try to emit TBZ, TBNZ, CBZ or CBNZ for certain comparisons of {node}
 // against {value}, depending on the condition.
-template <int N>
-bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
+template <typename Adapter, int N>
+bool TryEmitCbzOrTbz(InstructionSelectorT<Adapter>* selector, Node* node,
                      typename CbzOrTbzMatchTrait<N>::IntegralType value,
                      Node* user, FlagsCondition cond, FlagsContinuation* cont) {
   // Only handle branches and deoptimisations.
@@ -2577,7 +2714,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
       // shorter range than conditional branches and generating them for
       // deoptimisations results in more veneers.
       if (cont->IsDeoptimize()) return false;
-      Arm64OperandGenerator g(selector);
+      Arm64OperandGeneratorT<Adapter> g(selector);
       cont->Overwrite(MapForTbz(cond));
 
       if (N == 32) {
@@ -2608,7 +2745,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
         typename CbzOrTbzMatchTrait<N>::BinopMatcher m_and(node);
         if (cont->IsBranch() && base::bits::IsPowerOfTwo(value) &&
             m_and.right().Is(value) && selector->CanCover(user, node)) {
-          Arm64OperandGenerator g(selector);
+          Arm64OperandGeneratorT<Adapter> g(selector);
           // In the code generator, Equal refers to a bit being cleared. We want
           // the opposite here so negate the condition.
           cont->Negate();
@@ -2624,7 +2761,7 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
     case kUnsignedLessThanOrEqual:
     case kUnsignedGreaterThan: {
       if (value != 0) return false;
-      Arm64OperandGenerator g(selector);
+      Arm64OperandGeneratorT<Adapter> g(selector);
       cont->Overwrite(MapForCbz(cond));
       EmitBranchOrDeoptimize(selector,
                              CbzOrTbzMatchTrait<N>::kCompareAndBranchOpcode,
@@ -2637,10 +2774,11 @@ bool TryEmitCbzOrTbz(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
                       InstructionCode opcode, FlagsContinuation* cont,
                       ImmediateMode immediate_mode) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
 
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
@@ -2655,8 +2793,8 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
   if (opcode == kArm64Cmp) {
     Int64Matcher m(right);
     if (m.HasResolvedValue()) {
-      if (TryEmitCbzOrTbz<64>(selector, left, m.ResolvedValue(), node,
-                              cont->condition(), cont)) {
+      if (TryEmitCbzOrTbz<Adapter, 64>(selector, left, m.ResolvedValue(), node,
+                                       cont->condition(), cont)) {
         return;
       }
     }
@@ -2666,20 +2804,22 @@ void VisitWordCompare(InstructionSelector* selector, Node* node,
                g.UseOperand(right, immediate_mode), cont);
 }
 
-void VisitWord32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                         FlagsContinuation* cont) {
   Int32BinopMatcher m(node);
   FlagsCondition cond = cont->condition();
   if (m.right().HasResolvedValue()) {
-    if (TryEmitCbzOrTbz<32>(selector, m.left().node(),
-                            m.right().ResolvedValue(), node, cond, cont)) {
+    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.left().node(),
+                                     m.right().ResolvedValue(), node, cond,
+                                     cont)) {
       return;
     }
   } else if (m.left().HasResolvedValue()) {
     FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
-    if (TryEmitCbzOrTbz<32>(selector, m.right().node(),
-                            m.left().ResolvedValue(), node, commuted_cond,
-                            cont)) {
+    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.right().node(),
+                                     m.left().ResolvedValue(), node,
+                                     commuted_cond, cont)) {
       return;
     }
   }
@@ -2725,22 +2865,26 @@ void VisitWord32Compare(InstructionSelector* selector, Node* node,
       opcode = kArm64Cmn32;
     }
   }
-  VisitBinop<Int32BinopMatcher>(selector, node, opcode, immediate_mode, cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(selector, node, opcode, immediate_mode,
+                                         cont);
 }
 
-void VisitWordTest(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWordTest(InstructionSelectorT<Adapter>* selector, Node* node,
                    InstructionCode opcode, FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   VisitCompare(selector, opcode, g.UseRegister(node), g.UseRegister(node),
                cont);
 }
 
-void VisitWord32Test(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord32Test(InstructionSelectorT<Adapter>* selector, Node* node,
                      FlagsContinuation* cont) {
   VisitWordTest(selector, node, kArm64Tst32, cont);
 }
 
-void VisitWord64Test(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitWord64Test(InstructionSelectorT<Adapter>* selector, Node* node,
                      FlagsContinuation* cont) {
   VisitWordTest(selector, node, kArm64Tst, cont);
 }
@@ -2782,9 +2926,10 @@ struct TestAndBranchMatcher {
 };
 
 // Shared routine for multiple float32 compare operations.
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Float32BinopMatcher m(node);
   if (m.right().Is(0.0f)) {
     VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
@@ -2800,9 +2945,10 @@ void VisitFloat32Compare(InstructionSelector* selector, Node* node,
 }
 
 // Shared routine for multiple float64 compare operations.
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
                          FlagsContinuation* cont) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Float64BinopMatcher m(node);
   if (m.right().Is(0.0)) {
     VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
@@ -2817,10 +2963,11 @@ void VisitFloat64Compare(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicExchange(InstructionSelectorT<Adapter>* selector, Node* node,
                          ArchOpcode opcode, AtomicWidth width,
                          MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -2843,10 +2990,12 @@ void VisitAtomicExchange(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
-                                ArchOpcode opcode, AtomicWidth width,
+template <typename Adapter>
+void VisitAtomicCompareExchange(InstructionSelectorT<Adapter>* selector,
+                                Node* node, ArchOpcode opcode,
+                                AtomicWidth width,
                                 MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* old_value = node->InputAt(2);
@@ -2873,9 +3022,10 @@ void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicLoad(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicLoad(InstructionSelectorT<Adapter>* selector, Node* node,
                      AtomicWidth width) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index)};
@@ -2943,9 +3093,10 @@ void VisitAtomicLoad(InstructionSelector* selector, Node* node,
                  arraysize(temps), temps);
 }
 
-void VisitAtomicStore(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicStore(InstructionSelectorT<Adapter>* selector, Node* node,
                       AtomicWidth width) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -3018,10 +3169,11 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
                  temps);
 }
 
-void VisitAtomicBinop(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+void VisitAtomicBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                       ArchOpcode opcode, AtomicWidth width,
                       MemoryAccessKind access_kind) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -3049,9 +3201,10 @@ void VisitAtomicBinop(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
-                                               FlagsContinuation* cont) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+    Node* user, Node* value, FlagsContinuation* cont) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
     Int32BinopMatcher m(value);
@@ -3077,7 +3230,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
           // into a tbz/tbnz instruction.
           TestAndBranchMatcher<Uint64BinopMatcher> tbm(left, cont);
           if (tbm.Matches()) {
-            Arm64OperandGenerator gen(this);
+            Arm64OperandGeneratorT<Adapter> gen(this);
             cont->OverwriteAndNegateIfEqual(kEqual);
             this->EmitWithContinuation(kArm64TestAndBranch,
                                        gen.UseRegister(tbm.input()),
@@ -3091,7 +3244,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
     case IrOpcode::kWord32And: {
       TestAndBranchMatcher<Uint32BinopMatcher> tbm(value, cont);
       if (tbm.Matches()) {
-        Arm64OperandGenerator gen(this);
+        Arm64OperandGeneratorT<Adapter> gen(this);
         this->EmitWithContinuation(kArm64TestAndBranch32,
                                    gen.UseRegister(tbm.input()),
                                    gen.TempImmediate(tbm.bit()), cont);
@@ -3102,7 +3255,7 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
     case IrOpcode::kWord64And: {
       TestAndBranchMatcher<Uint64BinopMatcher> tbm(value, cont);
       if (tbm.Matches()) {
-        Arm64OperandGenerator gen(this);
+        Arm64OperandGeneratorT<Adapter> gen(this);
         this->EmitWithContinuation(kArm64TestAndBranch,
                                    gen.UseRegister(tbm.input()),
                                    gen.TempImmediate(tbm.bit()), cont);
@@ -3187,12 +3340,12 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
             switch (node->opcode()) {
               case IrOpcode::kInt32AddWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int32BinopMatcher>(
+                    this, node, kArm64Add32, kArithmeticImm, cont);
               case IrOpcode::kInt32SubWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int32BinopMatcher>(
+                    this, node, kArm64Sub32, kArithmeticImm, cont);
               case IrOpcode::kInt32MulWithOverflow:
                 // ARM64 doesn't set the overflow flag for multiplication, so we
                 // need to test on kNotEqual. Here is the code sequence used:
@@ -3202,12 +3355,12 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
                 return EmitInt32MulWithOverflow(this, node, cont);
               case IrOpcode::kInt64AddWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int64BinopMatcher>(this, node, kArm64Add,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int64BinopMatcher>(
+                    this, node, kArm64Add, kArithmeticImm, cont);
               case IrOpcode::kInt64SubWithOverflow:
                 cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub,
-                                                     kArithmeticImm, cont);
+                return VisitBinop<Adapter, Int64BinopMatcher>(
+                    this, node, kArm64Sub, kArithmeticImm, cont);
               case IrOpcode::kInt64MulWithOverflow:
                 // ARM64 doesn't set the overflow flag for multiplication, so we
                 // need to test on kNotEqual. Here is the code sequence used:
@@ -3249,8 +3402,10 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   }
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
+                                                const SwitchInfo& sw) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
@@ -3280,7 +3435,8 @@ void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
   return EmitBinarySearchSwitch(sw, value_operand);
 }
 
-void InstructionSelector::VisitWord32Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
   Node* const user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(user);
@@ -3318,7 +3474,7 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
 
   if (isolate() && (V8_STATIC_ROOTS_BOOL ||
                     (COMPRESS_POINTERS_BOOL && !isolate()->bootstrapper()))) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     const RootsTable& roots_table = isolate()->roots_table();
     RootIndex root_index;
     Node* left = nullptr;
@@ -3355,29 +3511,34 @@ void InstructionSelector::VisitWord32Equal(Node* const node) {
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitWord64Equal(Node* const node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* const node) {
   Node* const user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int64BinopMatcher m(user);
@@ -3396,27 +3557,32 @@ void InstructionSelector::VisitWord64Equal(Node* const node) {
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32,
-                                         kArithmeticImm, &cont);
+    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int32BinopMatcher>(this, node, kArm64Add32, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                         kArithmeticImm, &cont);
 }
 
-void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32,
-                                         kArithmeticImm, &cont);
+    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int32BinopMatcher>(this, node, kArm64Sub32, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                         kArithmeticImm, &cont);
 }
 
-void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     // ARM64 doesn't set the overflow flag for multiplication, so we need to
     // test on kNotEqual. Here is the code sequence used:
@@ -3429,27 +3595,32 @@ void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
   EmitInt32MulWithOverflow(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt64AddWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm,
-                                         &cont);
+    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm,
+                                         &cont);
 }
 
-void InstructionSelector::VisitInt64SubWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm,
-                                         &cont);
+    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,
+                                                  kArithmeticImm, &cont);
   }
   FlagsContinuation cont;
-  VisitBinop<Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm, &cont);
+  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm,
+                                         &cont);
 }
 
-void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
   if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
     // ARM64 doesn't set the overflow flag for multiplication, so we need to
     // test on kNotEqual. Here is the code sequence used:
@@ -3463,30 +3634,35 @@ void InstructionSelector::VisitInt64MulWithOverflow(Node* node) {
   EmitInt64MulWithOverflow(this, node, &cont);
 }
 
-void InstructionSelector::VisitInt64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitInt64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitUint64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
-void InstructionSelector::VisitFloat32Neg(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Neg(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat32Mul && CanCover(node, in)) {
     Float32BinopMatcher m(in);
@@ -3497,8 +3673,9 @@ void InstructionSelector::VisitFloat32Neg(Node* node) {
   VisitRR(this, kArm64Float32Neg, node);
 }
 
-void InstructionSelector::VisitFloat32Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Float32BinopMatcher m(node);
 
   if (m.left().IsFloat32Neg() && CanCover(node, m.left().node())) {
@@ -3517,8 +3694,9 @@ void InstructionSelector::VisitFloat32Mul(Node* node) {
   return VisitRRR(this, kArm64Float32Mul, node);
 }
 
-void InstructionSelector::VisitFloat32Abs(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Abs(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat32Sub && CanCover(node, in)) {
     Emit(kArm64Float32Abd, g.DefineAsRegister(node),
@@ -3529,8 +3707,9 @@ void InstructionSelector::VisitFloat32Abs(Node* node) {
   return VisitRR(this, kArm64Float32Abs, node);
 }
 
-void InstructionSelector::VisitFloat64Abs(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Abs(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat64Sub && CanCover(node, in)) {
     Emit(kArm64Float64Abd, g.DefineAsRegister(node),
@@ -3541,40 +3720,47 @@ void InstructionSelector::VisitFloat64Abs(Node* node) {
   return VisitRR(this, kArm64Float64Abs, node);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64Equal(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
@@ -3590,8 +3776,9 @@ void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
        g.UseRegister(left), g.UseRegister(right));
 }
 
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* left = node->InputAt(0);
   Node* right = node->InputAt(1);
   if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
@@ -3606,8 +3793,9 @@ void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
        g.UseRegister(left), g.UseRegister(right));
 }
 
-void InstructionSelector::VisitFloat64Neg(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Neg(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Node* in = node->InputAt(0);
   if (in->opcode() == IrOpcode::kFloat64Mul && CanCover(node, in)) {
     Float64BinopMatcher m(in);
@@ -3618,8 +3806,9 @@ void InstructionSelector::VisitFloat64Neg(Node* node) {
   VisitRR(this, kArm64Float64Neg, node);
 }
 
-void InstructionSelector::VisitFloat64Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitFloat64Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Float64BinopMatcher m(node);
 
   if (m.left().IsFloat64Neg() && CanCover(node, m.left().node())) {
@@ -3638,29 +3827,35 @@ void InstructionSelector::VisitFloat64Mul(Node* node) {
   return VisitRRR(this, kArm64Float64Mul, node);
 }
 
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
   // Use DMB ISH for both acquire-release and sequentially consistent barriers.
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64DmbIsh, g.NoOutput());
 }
 
-void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicLoad(Node* node) {
   VisitAtomicLoad(this, node, AtomicWidth::kWord32);
 }
 
-void InstructionSelector::VisitWord64AtomicLoad(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicLoad(Node* node) {
   VisitAtomicLoad(this, node, AtomicWidth::kWord64);
 }
 
-void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicStore(Node* node) {
   VisitAtomicStore(this, node, AtomicWidth::kWord32);
 }
 
-void InstructionSelector::VisitWord64AtomicStore(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicStore(Node* node) {
   VisitAtomicStore(this, node, AtomicWidth::kWord64);
 }
 
-void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicExchange(Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Int8()) {
@@ -3680,7 +3875,8 @@ void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicExchange(Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Uint8()) {
@@ -3697,7 +3893,9 @@ void InstructionSelector::VisitWord64AtomicExchange(Node* node) {
   VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord64, params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicCompareExchange(
+    Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Int8()) {
@@ -3718,7 +3916,9 @@ void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicCompareExchange(
+    Node* node) {
   ArchOpcode opcode;
   AtomicOpParameters params = AtomicOpParametersOf(node->op());
   if (params.type() == MachineType::Uint8()) {
@@ -3736,7 +3936,8 @@ void InstructionSelector::VisitWord64AtomicCompareExchange(Node* node) {
                              params.kind());
 }
 
-void InstructionSelector::VisitWord32AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord32AtomicBinaryOperation(
     Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
     ArchOpcode uint16_op, ArchOpcode word32_op) {
   ArchOpcode opcode;
@@ -3758,11 +3959,12 @@ void InstructionSelector::VisitWord32AtomicBinaryOperation(
   VisitAtomicBinop(this, node, opcode, AtomicWidth::kWord32, params.kind());
 }
 
-#define VISIT_ATOMIC_BINOP(op)                                           \
-  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
-    VisitWord32AtomicBinaryOperation(                                    \
-        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
-        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+#define VISIT_ATOMIC_BINOP(op)                                            \
+  template <typename Adapter>                                             \
+  void InstructionSelectorT<Adapter>::VisitWord32Atomic##op(Node* node) { \
+    VisitWord32AtomicBinaryOperation(                                     \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16,  \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                        \
   }
 VISIT_ATOMIC_BINOP(Add)
 VISIT_ATOMIC_BINOP(Sub)
@@ -3771,7 +3973,8 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitWord64AtomicBinaryOperation(
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitWord64AtomicBinaryOperation(
     Node* node, ArchOpcode uint8_op, ArchOpcode uint16_op, ArchOpcode uint32_op,
     ArchOpcode uint64_op) {
   ArchOpcode opcode;
@@ -3791,7 +3994,8 @@ void InstructionSelector::VisitWord64AtomicBinaryOperation(
 }
 
 #define VISIT_ATOMIC_BINOP(op)                                                 \
-  void InstructionSelector::VisitWord64Atomic##op(Node* node) {                \
+  template <typename Adapter>                                                  \
+  void InstructionSelectorT<Adapter>::VisitWord64Atomic##op(Node* node) {      \
     VisitWord64AtomicBinaryOperation(node, kAtomic##op##Uint8,                 \
                                      kAtomic##op##Uint16, kAtomic##op##Word32, \
                                      kArm64Word64Atomic##op##Uint64);          \
@@ -3803,11 +4007,13 @@ VISIT_ATOMIC_BINOP(Or)
 VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
-void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
@@ -3933,8 +4139,9 @@ void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
   V(I8x16MinU, kArm64IMinU, 8)                         \
   V(I8x16MaxU, kArm64IMaxU, 8)
 
-void InstructionSelector::VisitS128Const(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Const(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   static const int kUint32Immediates = 4;
   uint32_t val[kUint32Immediates];
   static_assert(sizeof(val) == kSimd128Size);
@@ -4017,9 +4224,10 @@ base::Optional<BicImmResult> BicImmHelper(Node* and_node, bool not_imm) {
   return base::nullopt;
 }
 
-bool TryEmitS128AndNotImm(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool TryEmitS128AndNotImm(InstructionSelectorT<Adapter>* selector, Node* node,
                           bool not_imm) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   base::Optional<BicImmResult> result = BicImmHelper(node, not_imm);
   if (!result.has_value()) return false;
   base::Optional<BicImmParam> param = result->param;
@@ -4037,27 +4245,31 @@ bool TryEmitS128AndNotImm(InstructionSelector* selector, Node* node,
 
 }  // namespace
 
-void InstructionSelector::VisitS128AndNot(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128AndNot(Node* node) {
   if (!TryEmitS128AndNotImm(this, node, false)) {
     VisitRRR(this, kArm64S128AndNot, node);
   }
 }
 
-void InstructionSelector::VisitS128And(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128And(Node* node) {
   // AndNot can be used if we negate the immediate input of And.
   if (!TryEmitS128AndNotImm(this, node, true)) {
     VisitRRR(this, kArm64S128And, node);
   }
 }
 
-void InstructionSelector::VisitS128Zero(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Zero(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64S128Const, g.DefineAsRegister(node), g.UseImmediate(0),
        g.UseImmediate(0), g.UseImmediate(0), g.UseImmediate(0));
 }
 
-void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand output = CpuFeatures::IsSupported(DOTPROD)
                                   ? g.DefineSameAsInput(node, 2)
                                   : g.DefineAsRegister(node);
@@ -4066,7 +4278,9 @@ void InstructionSelector::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 }
 
 #define SIMD_VISIT_EXTRACT_LANE(Type, T, Sign, LaneSize)                     \
-  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) {     \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##ExtractLane##Sign(        \
+      Node* node) {                                                          \
     VisitRRI(this,                                                           \
              kArm64##T##ExtractLane##Sign | LaneSizeField::encode(LaneSize), \
              node);                                                          \
@@ -4082,7 +4296,8 @@ SIMD_VISIT_EXTRACT_LANE(I8x16, I, S, 8)
 #undef SIMD_VISIT_EXTRACT_LANE
 
 #define SIMD_VISIT_REPLACE_LANE(Type, T, LaneSize)                            \
-  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) {            \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Type##ReplaceLane(Node* node) {  \
     VisitRRIR(this, kArm64##T##ReplaceLane | LaneSizeField::encode(LaneSize), \
               node);                                                          \
   }
@@ -4094,32 +4309,36 @@ SIMD_VISIT_REPLACE_LANE(I16x8, I, 16)
 SIMD_VISIT_REPLACE_LANE(I8x16, I, 8)
 #undef SIMD_VISIT_REPLACE_LANE
 
-#define SIMD_VISIT_UNOP(Name, instruction)            \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, instruction, node);                 \
+#define SIMD_VISIT_UNOP(Name, instruction)                      \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRR(this, instruction, node);                           \
   }
 SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
 #undef SIMD_VISIT_UNOP
 #undef SIMD_UNOP_LIST
 
-#define SIMD_VISIT_SHIFT_OP(Name, width)                \
-  void InstructionSelector::Visit##Name(Node* node) {   \
-    VisitSimdShiftRRR(this, kArm64##Name, node, width); \
+#define SIMD_VISIT_SHIFT_OP(Name, width)                        \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitSimdShiftRRR(this, kArm64##Name, node, width);         \
   }
 SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
 #undef SIMD_VISIT_SHIFT_OP
 #undef SIMD_SHIFT_OP_LIST
 
-#define SIMD_VISIT_BINOP(Name, instruction)           \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, instruction, node);                \
+#define SIMD_VISIT_BINOP(Name, instruction)                     \
+  template <typename Adapter>                                   \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
+    VisitRRR(this, instruction, node);                          \
   }
 SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
 #undef SIMD_VISIT_BINOP
 #undef SIMD_BINOP_LIST
 
 #define SIMD_VISIT_BINOP_LANE_SIZE(Name, instruction, LaneSize)          \
-  void InstructionSelector::Visit##Name(Node* node) {                    \
+  template <typename Adapter>                                            \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {          \
     VisitRRR(this, instruction | LaneSizeField::encode(LaneSize), node); \
   }
 SIMD_BINOP_LANE_SIZE_LIST(SIMD_VISIT_BINOP_LANE_SIZE)
@@ -4127,7 +4346,8 @@ SIMD_BINOP_LANE_SIZE_LIST(SIMD_VISIT_BINOP_LANE_SIZE)
 #undef SIMD_BINOP_LANE_SIZE_LIST
 
 #define SIMD_VISIT_UNOP_LANE_SIZE(Name, instruction, LaneSize)          \
-  void InstructionSelector::Visit##Name(Node* node) {                   \
+  template <typename Adapter>                                           \
+  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) {         \
     VisitRR(this, instruction | LaneSizeField::encode(LaneSize), node); \
   }
 SIMD_UNOP_LANE_SIZE_LIST(SIMD_VISIT_UNOP_LANE_SIZE)
@@ -4192,9 +4412,10 @@ MulWithDupResult TryMatchMulWithDup(Node* node) {
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Mul(Node* node) {
   if (MulWithDupResult result = TryMatchMulWithDup<4>(node)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Emit(kArm64FMulElement | LaneSizeField::encode(32),
          g.DefineAsRegister(node), g.UseRegister(result.input),
          g.UseRegister(result.dup_node), g.UseImmediate(result.index));
@@ -4203,9 +4424,10 @@ void InstructionSelector::VisitF32x4Mul(Node* node) {
   }
 }
 
-void InstructionSelector::VisitF64x2Mul(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Mul(Node* node) {
   if (MulWithDupResult result = TryMatchMulWithDup<2>(node)) {
-    Arm64OperandGenerator g(this);
+    Arm64OperandGeneratorT<Adapter> g(this);
     Emit(kArm64FMulElement | LaneSizeField::encode(64),
          g.DefineAsRegister(node), g.UseRegister(result.input),
          g.UseRegister(result.dup_node), g.UseImmediate(result.index));
@@ -4214,8 +4436,9 @@ void InstructionSelector::VisitF64x2Mul(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI64x2Mul(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Mul(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   InstructionOperand temps[] = {g.TempSimd128Register()};
   Emit(kArm64I64x2Mul, g.DefineAsRegister(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
@@ -4253,10 +4476,11 @@ struct SimdAddOpMatcher : public NodeMatcher {
   Node* right_;
 };
 
-bool ShraHelper(InstructionSelector* selector, Node* node, int lane_size,
-                InstructionCode shra_code, InstructionCode add_code,
-                IrOpcode::Value shift_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool ShraHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                int lane_size, InstructionCode shra_code,
+                InstructionCode add_code, IrOpcode::Value shift_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, shift_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   if (!g.IsIntegerConstant(m.left()->InputAt(1))) return false;
@@ -4275,9 +4499,11 @@ bool ShraHelper(InstructionSelector* selector, Node* node, int lane_size,
   return true;
 }
 
-bool AdalpHelper(InstructionSelector* selector, Node* node, int lane_size,
-                 InstructionCode adalp_code, IrOpcode::Value ext_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool AdalpHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                 int lane_size, InstructionCode adalp_code,
+                 IrOpcode::Value ext_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, ext_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   selector->Emit(adalp_code | LaneSizeField::encode(lane_size),
@@ -4286,9 +4512,10 @@ bool AdalpHelper(InstructionSelector* selector, Node* node, int lane_size,
   return true;
 }
 
-bool MlaHelper(InstructionSelector* selector, Node* node,
+template <typename Adapter>
+bool MlaHelper(InstructionSelectorT<Adapter>* selector, Node* node,
                InstructionCode mla_code, IrOpcode::Value mul_op) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, mul_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
   selector->Emit(mla_code, g.DefineSameAsFirst(node), g.UseRegister(m.right()),
@@ -4297,9 +4524,11 @@ bool MlaHelper(InstructionSelector* selector, Node* node,
   return true;
 }
 
-bool SmlalHelper(InstructionSelector* selector, Node* node, int lane_size,
-                 InstructionCode smlal_code, IrOpcode::Value ext_mul_op) {
-  Arm64OperandGenerator g(selector);
+template <typename Adapter>
+bool SmlalHelper(InstructionSelectorT<Adapter>* selector, Node* node,
+                 int lane_size, InstructionCode smlal_code,
+                 IrOpcode::Value ext_mul_op) {
+  Arm64OperandGeneratorT<Adapter> g(selector);
   SimdAddOpMatcher m(node, ext_mul_op);
   if (!m.Matches() || !selector->CanCover(node, m.left())) return false;
 
@@ -4312,7 +4541,8 @@ bool SmlalHelper(InstructionSelector* selector, Node* node, int lane_size,
 
 }  // namespace
 
-void InstructionSelector::VisitI64x2Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2Add(Node* node) {
   if (!ShraHelper(this, node, 64, kArm64Ssra,
                   kArm64IAdd | LaneSizeField::encode(64),
                   IrOpcode::kI64x2ShrS) &&
@@ -4323,7 +4553,8 @@ void InstructionSelector::VisitI64x2Add(Node* node) {
   }
 }
 
-void InstructionSelector::VisitI8x16Add(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Add(Node* node) {
   if (!ShraHelper(this, node, 8, kArm64Ssra,
                   kArm64IAdd | LaneSizeField::encode(8),
                   IrOpcode::kI8x16ShrS) &&
@@ -4335,7 +4566,8 @@ void InstructionSelector::VisitI8x16Add(Node* node) {
 }
 
 #define VISIT_SIMD_ADD(Type, PairwiseType, LaneSize)                       \
-  void InstructionSelector::Visit##Type##Add(Node* node) {                 \
+  template <typename Adapter>                                              \
+  void InstructionSelectorT<Adapter>::Visit##Type##Add(Node* node) {       \
     /* Select Mla(z, x, y) for Add(x, Mul(y, z)). */                       \
     if (MlaHelper(this, node, kArm64Mla | LaneSizeField::encode(LaneSize), \
                   IrOpcode::k##Type##Mul)) {                               \
@@ -4377,8 +4609,9 @@ VISIT_SIMD_ADD(I16x8, I8x16, 16)
 #undef VISIT_SIMD_ADD
 
 #define VISIT_SIMD_SUB(Type, LaneSize)                                        \
-  void InstructionSelector::Visit##Type##Sub(Node* node) {                    \
-    Arm64OperandGenerator g(this);                                            \
+  template <typename Adapter>                                                 \
+  void InstructionSelectorT<Adapter>::Visit##Type##Sub(Node* node) {          \
+    Arm64OperandGeneratorT<Adapter> g(this);                                  \
     Node* left = node->InputAt(0);                                            \
     Node* right = node->InputAt(1);                                           \
     /* Select Mls(z, x, y) for Sub(z, Mul(x, y)). */                          \
@@ -4397,7 +4630,8 @@ VISIT_SIMD_SUB(I16x8, 16)
 #undef VISIT_SIMD_SUB
 
 namespace {
-bool isSimdZero(Arm64OperandGenerator& g, Node* node) {
+template <typename Adapter>
+bool isSimdZero(Arm64OperandGeneratorT<Adapter>& g, Node* node) {
   auto m = V128ConstMatcher(node);
   if (m.HasResolvedValue()) {
     auto imms = m.ResolvedValue().immediate();
@@ -4408,8 +4642,9 @@ bool isSimdZero(Arm64OperandGenerator& g, Node* node) {
 }  // namespace
 
 #define VISIT_SIMD_CM(Type, T, CmOp, CmOpposite, LaneSize)                   \
-  void InstructionSelector::Visit##Type##CmOp(Node* node) {                  \
-    Arm64OperandGenerator g(this);                                           \
+  template <typename Adapter>                                                \
+  void InstructionSelectorT<Adapter>::Visit##Type##CmOp(Node* node) {        \
+    Arm64OperandGeneratorT<Adapter> g(this);                                 \
     Node* left = node->InputAt(0);                                           \
     Node* right = node->InputAt(1);                                          \
     if (isSimdZero(g, left)) {                                               \
@@ -4451,32 +4686,38 @@ VISIT_SIMD_CM(I8x16, I, GtS, LtS, 8)
 VISIT_SIMD_CM(I8x16, I, GeS, LeS, 8)
 #undef VISIT_SIMD_CM
 
-void InstructionSelector::VisitS128Select(Node* node) {
-  Arm64OperandGenerator g(this);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitS128Select(Node* node) {
+  Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64S128Select, g.DefineSameAsFirst(node),
        g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)),
        g.UseRegister(node->InputAt(2)));
 }
 
-void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI16x8RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI32x4RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
-void InstructionSelector::VisitI64x2RelaxedLaneSelect(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2RelaxedLaneSelect(Node* node) {
   VisitS128Select(node);
 }
 
 #define VISIT_SIMD_QFMOP(op)                                               \
-  void InstructionSelector::Visit##op(Node* node) {                        \
-    Arm64OperandGenerator g(this);                                         \
+  template <typename Adapter>                                              \
+  void InstructionSelectorT<Adapter>::Visit##op(Node* node) {              \
+    Arm64OperandGeneratorT<Adapter> g(this);                               \
     Emit(kArm64##op, g.DefineSameAsInput(node, 2),                         \
          g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)), \
          g.UseRegister(node->InputAt(2)));                                 \
@@ -4565,8 +4806,10 @@ bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
   return false;
 }
 
-void ArrangeShuffleTable(Arm64OperandGenerator* g, Node* input0, Node* input1,
-                         InstructionOperand* src0, InstructionOperand* src1) {
+template <typename Adapter>
+void ArrangeShuffleTable(Arm64OperandGeneratorT<Adapter>* g, Node* input0,
+                         Node* input1, InstructionOperand* src0,
+                         InstructionOperand* src1) {
   if (input0 == input1) {
     // Unary, any q-register can be the table.
     *src0 = *src1 = g->UseRegister(input0);
@@ -4579,12 +4822,13 @@ void ArrangeShuffleTable(Arm64OperandGenerator* g, Node* input0, Node* input1,
 
 }  // namespace
 
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
   uint8_t shuffle[kSimd128Size];
   bool is_swizzle;
   CanonicalizeShuffle(node, shuffle, &is_swizzle);
   uint8_t shuffle32x4[4];
-  Arm64OperandGenerator g(this);
+  Arm64OperandGeneratorT<Adapter> g(this);
   ArchOpcode opcode;
   if (TryMatchArchShuffle(shuffle, arch_shuffles, arraysize(arch_shuffles),
                           is_swizzle, &opcode)) {
@@ -4636,33 +4880,42 @@ void InstructionSelector::VisitI8x16Shuffle(Node* node) {
        g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
 }
 #else
-void InstructionSelector::VisitI8x16Shuffle(Node* node) { UNREACHABLE(); }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Shuffle(Node* node) {
+  UNREACHABLE();
+}
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt32(Node* node) {
   VisitRR(this, kArm64Sxtb32, node);
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt32(Node* node) {
   VisitRR(this, kArm64Sxth32, node);
 }
 
-void InstructionSelector::VisitSignExtendWord8ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord8ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtb, node);
 }
 
-void InstructionSelector::VisitSignExtendWord16ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(Node* node) {
   VisitRR(this, kArm64Sxth, node);
 }
 
-void InstructionSelector::VisitSignExtendWord32ToInt64(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
   VisitRR(this, kArm64Sxtw, node);
 }
 
 namespace {
-void VisitPminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
+template <typename Adapter>
+void VisitPminOrPmax(InstructionSelectorT<Adapter>* selector, ArchOpcode opcode,
                      Node* node) {
-  Arm64OperandGenerator g(selector);
+  Arm64OperandGeneratorT<Adapter> g(selector);
   // Need all unique registers because we first compare the two inputs, then we
   // need the inputs to remain unchanged for the bitselect later.
   selector->Emit(opcode, g.DefineAsRegister(node),
@@ -4671,94 +4924,113 @@ void VisitPminOrPmax(InstructionSelector* selector, ArchOpcode opcode,
 }
 }  // namespace
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmin(Node* node) {
   VisitPminOrPmax(this, kArm64F32x4Pmin, node);
 }
 
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF32x4Pmax(Node* node) {
   VisitPminOrPmax(this, kArm64F32x4Pmax, node);
 }
 
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmin(Node* node) {
   VisitPminOrPmax(this, kArm64F64x2Pmin, node);
 }
 
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
   VisitPminOrPmax(this, kArm64F64x2Pmax, node);
 }
 
 namespace {
-void VisitSignExtendLong(InstructionSelector* selector, ArchOpcode opcode,
-                         Node* node, int lane_size) {
+template <typename Adapter>
+void VisitSignExtendLong(InstructionSelectorT<Adapter>* selector,
+                         ArchOpcode opcode, Node* node, int lane_size) {
   InstructionCode code = opcode;
   code |= LaneSizeField::encode(lane_size);
   VisitRR(selector, code, node);
 }
 }  // namespace
 
-void InstructionSelector::VisitI64x2SConvertI32x4Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SConvertI32x4Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 64);
 }
 
-void InstructionSelector::VisitI64x2SConvertI32x4High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2SConvertI32x4High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 64);
 }
 
-void InstructionSelector::VisitI64x2UConvertI32x4Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2UConvertI32x4Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 64);
 }
 
-void InstructionSelector::VisitI64x2UConvertI32x4High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI64x2UConvertI32x4High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 64);
 }
 
-void InstructionSelector::VisitI32x4SConvertI16x8Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertI16x8Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 32);
 }
 
-void InstructionSelector::VisitI32x4SConvertI16x8High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4SConvertI16x8High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 32);
 }
 
-void InstructionSelector::VisitI32x4UConvertI16x8Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertI16x8Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 32);
 }
 
-void InstructionSelector::VisitI32x4UConvertI16x8High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI32x4UConvertI16x8High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 32);
 }
 
-void InstructionSelector::VisitI16x8SConvertI8x16Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8SConvertI8x16Low(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl, node, 16);
 }
 
-void InstructionSelector::VisitI16x8SConvertI8x16High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8SConvertI8x16High(Node* node) {
   VisitSignExtendLong(this, kArm64Sxtl2, node, 16);
 }
 
-void InstructionSelector::VisitI16x8UConvertI8x16Low(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8UConvertI8x16Low(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl, node, 16);
 }
 
-void InstructionSelector::VisitI16x8UConvertI8x16High(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI16x8UConvertI8x16High(Node* node) {
   VisitSignExtendLong(this, kArm64Uxtl2, node, 16);
 }
 
-void InstructionSelector::VisitI8x16Popcnt(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitI8x16Popcnt(Node* node) {
   InstructionCode code = kArm64Cnt;
   code |= LaneSizeField::encode(8);
   VisitRR(this, code, node);
 }
 
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
+    OperandGenerator* g, int first_input_index, Node* node) {
   continuation_outputs_.push_back(g->DefineAsRegister(node));
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelector::SupportedMachineOperatorFlags() {
+InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   return MachineOperatorBuilder::kFloat32RoundDown |
          MachineOperatorBuilder::kFloat64RoundDown |
          MachineOperatorBuilder::kFloat32RoundUp |
@@ -4782,12 +5054,18 @@ InstructionSelector::SupportedMachineOperatorFlags() {
 }
 
 // static
+template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
+InstructionSelectorT<Adapter>::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
 
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurbofanAdapter>;
+template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
+    InstructionSelectorT<TurboshaftAdapter>;
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
