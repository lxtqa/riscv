diff --git a/src/baseline/arm64/baseline-assembler-arm64-inl.h b/src/baseline/arm64/baseline-assembler-arm64-inl.h
index 63e90df4d62..7310e33f65e 100644
--- a/src/baseline/arm64/baseline-assembler-arm64-inl.h
+++ b/src/baseline/arm64/baseline-assembler-arm64-inl.h
@@ -115,19 +115,19 @@ void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,
   __ JumpIfNotSmi(value, target);
 }
 
-void BaselineAssembler::CallBuiltin(Builtins::Name builtin) {
+void BaselineAssembler::CallBuiltin(Builtin builtin) {
   if (masm()->options().short_builtin_calls) {
     // Generate pc-relative call.
     __ CallBuiltin(builtin);
   } else {
     ScratchRegisterScope temps(this);
     Register temp = temps.AcquireScratch();
-    __ LoadEntryFromBuiltinIndex(builtin, temp);
+    __ LoadEntryFromBuiltin(builtin, temp);
     __ Call(temp);
   }
 }
 
-void BaselineAssembler::TailCallBuiltin(Builtins::Name builtin) {
+void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
   if (masm()->options().short_builtin_calls) {
     // Generate pc-relative call.
     __ TailCallBuiltin(builtin);
@@ -147,7 +147,7 @@ void BaselineAssembler::TailCallBuiltin(Builtins::Name builtin) {
     UseScratchRegisterScope temps(masm());
     temps.Exclude(temp);
 
-    __ LoadEntryFromBuiltinIndex(builtin, temp);
+    __ LoadEntryFromBuiltin(builtin, temp);
     __ Jump(temp);
   }
 }
