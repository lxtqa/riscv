diff --git a/src/maglev/arm64/maglev-ir-arm64.cc b/src/maglev/arm64/maglev-ir-arm64.cc
index 9c8ea0eeec9..60ff0dc41aa 100644
--- a/src/maglev/arm64/maglev-ir-arm64.cc
+++ b/src/maglev/arm64/maglev-ir-arm64.cc
@@ -656,7 +656,7 @@ enum class ReduceInterruptBudgetType { kLoop, kReturn };
 
 void HandleInterruptsAndTiering(MaglevAssembler* masm, ZoneLabelRef done,
                                 Node* node, ReduceInterruptBudgetType type,
-                                Input function) {
+                                Register scratch0) {
   // For loops, first check for interrupts. Don't do this for returns, as we
   // can't lazy deopt to the end of a return.
   if (type == ReduceInterruptBudgetType::kLoop) {
@@ -664,8 +664,7 @@ void HandleInterruptsAndTiering(MaglevAssembler* masm, ZoneLabelRef done,
     // Here, we only care about interrupts since we've already guarded against
     // real stack overflows on function entry.
     {
-      MaglevAssembler::ScratchRegisterScope temps(masm);
-      Register stack_limit = temps.Acquire();
+      Register stack_limit = scratch0;
       __ LoadStackLimit(stack_limit, StackLimitKind::kInterruptStackLimit);
       __ Cmp(sp, stack_limit);
       __ B(&next, hi);
@@ -677,9 +676,11 @@ void HandleInterruptsAndTiering(MaglevAssembler* masm, ZoneLabelRef done,
     {
       SaveRegisterStateForCall save_register_state(masm,
                                                    node->register_snapshot());
+      Register function = scratch0;
+      __ Ldr(function, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
       __ Push(function);
-      // Move into kContextRegister after the push, in case that one clobbers
-      // kContextRegister.
+      // Move into kContextRegister after the load into scratch0, just in case
+      // scratch0 happens to be kContextRegister.
       __ Move(kContextRegister, masm->native_context().object());
       __ CallRuntime(Runtime::kBytecodeBudgetInterruptWithStackCheck_Maglev, 1);
       save_register_state.DefineSafepointWithLazyDeopt(node->lazy_deopt_info());
@@ -692,9 +693,11 @@ void HandleInterruptsAndTiering(MaglevAssembler* masm, ZoneLabelRef done,
   {
     SaveRegisterStateForCall save_register_state(masm,
                                                  node->register_snapshot());
+    Register function = scratch0;
+    __ Ldr(function, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
     __ Push(function);
-    // Move into kContextRegister after the push, in case that one clobbers
-    // kContextRegister.
+    // Move into kContextRegister after the load into scratch0, just in case
+    // scratch0 happens to be kContextRegister.
     __ Move(kContextRegister, masm->native_context().object());
     // Note: must not cause a lazy deopt!
     __ CallRuntime(Runtime::kBytecodeBudgetInterrupt_Maglev, 1);
@@ -704,27 +707,24 @@ void HandleInterruptsAndTiering(MaglevAssembler* masm, ZoneLabelRef done,
 }
 
 void GenerateReduceInterruptBudget(MaglevAssembler* masm, Node* node,
-                                   ReduceInterruptBudgetType type, int amount,
-                                   Input& function) {
-  {
-    MaglevAssembler::ScratchRegisterScope temps(masm);
-    Register feedback_cell = temps.Acquire();
-    Register budget = temps.Acquire().W();
-    __ Ldr(feedback_cell,
-           MemOperand(fp, StandardFrameConstants::kFunctionOffset));
-    __ LoadTaggedField(
-        feedback_cell,
-        FieldMemOperand(feedback_cell, JSFunction::kFeedbackCellOffset));
-    __ Ldr(budget, FieldMemOperand(feedback_cell,
-                                   FeedbackCell::kInterruptBudgetOffset));
-    __ Subs(budget, budget, Immediate(amount));
-    __ Str(budget, FieldMemOperand(feedback_cell,
-                                   FeedbackCell::kInterruptBudgetOffset));
-  }
-
+                                   ReduceInterruptBudgetType type, int amount) {
+  MaglevAssembler::ScratchRegisterScope temps(masm);
+  Register scratch = temps.Acquire();
+  Register feedback_cell = scratch;
+  Register budget = temps.Acquire().W();
+  __ Ldr(feedback_cell,
+         MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ LoadTaggedField(
+      feedback_cell,
+      FieldMemOperand(feedback_cell, JSFunction::kFeedbackCellOffset));
+  __ Ldr(budget,
+         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));
+  __ Subs(budget, budget, Immediate(amount));
+  __ Str(budget,
+         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));
   ZoneLabelRef done(masm);
   __ JumpToDeferredIf(lt, HandleInterruptsAndTiering, done, node, type,
-                      function);
+                      scratch);
   __ Bind(*done);
 }
 
@@ -732,24 +732,22 @@ void GenerateReduceInterruptBudget(MaglevAssembler* masm, Node* node,
 
 int ReduceInterruptBudgetForLoop::MaxCallStackArgs() const { return 1; }
 void ReduceInterruptBudgetForLoop::SetValueLocationConstraints() {
-  UseAny(function());
   set_temporaries_needed(2);
 }
 void ReduceInterruptBudgetForLoop::GenerateCode(MaglevAssembler* masm,
                                                 const ProcessingState& state) {
   GenerateReduceInterruptBudget(masm, this, ReduceInterruptBudgetType::kLoop,
-                                amount(), function());
+                                amount());
 }
 
 int ReduceInterruptBudgetForReturn::MaxCallStackArgs() const { return 1; }
 void ReduceInterruptBudgetForReturn::SetValueLocationConstraints() {
-  UseAny(function());
   set_temporaries_needed(2);
 }
 void ReduceInterruptBudgetForReturn::GenerateCode(
     MaglevAssembler* masm, const ProcessingState& state) {
   GenerateReduceInterruptBudget(masm, this, ReduceInterruptBudgetType::kReturn,
-                                amount(), function());
+                                amount());
 }
 
 int FunctionEntryStackCheck::MaxCallStackArgs() const { return 1; }
