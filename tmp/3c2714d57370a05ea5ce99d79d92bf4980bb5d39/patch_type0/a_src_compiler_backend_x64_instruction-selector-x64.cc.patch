diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index f501996cf7d..07aaae3c3d1 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -11,11 +11,14 @@
 #include "src/codegen/machine-type.h"
 #include "src/compiler/backend/instruction-codes.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/instruction-selector.h"
 #include "src/compiler/backend/instruction.h"
 #include "src/compiler/machine-operator.h"
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
 #include "src/compiler/opcodes.h"
+#include "src/compiler/turboshaft/operations.h"
+#include "src/compiler/turboshaft/representations.h"
 #include "src/roots/roots-inl.h"
 
 #if V8_ENABLE_WEBASSEMBLY
@@ -56,41 +59,51 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
   explicit X64OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
       : super(selector) {}
 
-  bool CanBeImmediate(Node* node) {
-    switch (node->opcode()) {
-      case IrOpcode::kCompressedHeapConstant: {
-        if (!COMPRESS_POINTERS_BOOL) return false;
-        // For builtin code we need static roots
-        if (selector()->isolate()->bootstrapper() && !V8_STATIC_ROOTS_BOOL) {
+  template <typename T>
+  bool CanBeImmediate(T*) {
+    UNREACHABLE(/*REMOVE*/);
+  }
+
+  bool CanBeImmediate(node_t node) {
+    if constexpr (Adapter::IsTurboshaft) {
+      // TODO(nicohartmann@): Implement this for Turobshaft.
+      return false;
+    } else {
+      switch (node->opcode()) {
+        case IrOpcode::kCompressedHeapConstant: {
+          if (!COMPRESS_POINTERS_BOOL) return false;
+          // For builtin code we need static roots
+          if (selector()->isolate()->bootstrapper() && !V8_STATIC_ROOTS_BOOL) {
+            return false;
+          }
+          const RootsTable& roots_table = selector()->isolate()->roots_table();
+          RootIndex root_index;
+          CompressedHeapObjectMatcher m(node);
+          if (m.HasResolvedValue() &&
+              roots_table.IsRootHandle(m.ResolvedValue(), &root_index)) {
+            return RootsTable::IsReadOnly(root_index);
+          }
           return false;
         }
-        const RootsTable& roots_table = selector()->isolate()->roots_table();
-        RootIndex root_index;
-        CompressedHeapObjectMatcher m(node);
-        if (m.HasResolvedValue() &&
-            roots_table.IsRootHandle(m.ResolvedValue(), &root_index)) {
-          return RootsTable::IsReadOnly(root_index);
+        case IrOpcode::kInt32Constant:
+        case IrOpcode::kRelocatableInt32Constant: {
+          const int32_t value = OpParameter<int32_t>(node->op());
+          // int32_t min will overflow if displacement mode is
+          // kNegativeDisplacement.
+          return value != std::numeric_limits<int32_t>::min();
         }
-        return false;
-      }
-      case IrOpcode::kInt32Constant:
-      case IrOpcode::kRelocatableInt32Constant: {
-        const int32_t value = OpParameter<int32_t>(node->op());
-        // int32_t min will overflow if displacement mode is
-        // kNegativeDisplacement.
-        return value != std::numeric_limits<int32_t>::min();
-      }
-      case IrOpcode::kInt64Constant: {
-        const int64_t value = OpParameter<int64_t>(node->op());
-        return std::numeric_limits<int32_t>::min() < value &&
-               value <= std::numeric_limits<int32_t>::max();
-      }
-      case IrOpcode::kNumberConstant: {
-        const double value = OpParameter<double>(node->op());
-        return base::bit_cast<int64_t>(value) == 0;
+        case IrOpcode::kInt64Constant: {
+          const int64_t value = OpParameter<int64_t>(node->op());
+          return std::numeric_limits<int32_t>::min() < value &&
+                 value <= std::numeric_limits<int32_t>::max();
+        }
+        case IrOpcode::kNumberConstant: {
+          const double value = OpParameter<double>(node->op());
+          return base::bit_cast<int64_t>(value) == 0;
+        }
+        default:
+          return false;
       }
-      default:
-        return false;
     }
   }
 
@@ -103,63 +116,69 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     return static_cast<int32_t>(OpParameter<int64_t>(node->op()));
   }
 
-  bool CanBeMemoryOperand(InstructionCode opcode, Node* node, Node* input,
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanBeMemoryOperand)
+  bool CanBeMemoryOperand(InstructionCode opcode, node_t node, node_t input,
                           int effect_level) {
-    if ((input->opcode() != IrOpcode::kLoad &&
-         input->opcode() != IrOpcode::kLoadImmutable) ||
-        !selector()->CanCover(node, input)) {
+    if constexpr (Adapter::IsTurboshaft) {
+      // TODO(nicohartmann@): Implement this for Turboshaft.
       return false;
-    }
-    if (effect_level != selector()->GetEffectLevel(input)) {
+    } else {
+      if ((input->opcode() != IrOpcode::kLoad &&
+           input->opcode() != IrOpcode::kLoadImmutable) ||
+          !selector()->CanCover(node, input)) {
+        return false;
+      }
+      if (effect_level != selector()->GetEffectLevel(input)) {
+        return false;
+      }
+      MachineRepresentation rep =
+          LoadRepresentationOf(input->op()).representation();
+      switch (opcode) {
+        case kX64And:
+        case kX64Or:
+        case kX64Xor:
+        case kX64Add:
+        case kX64Sub:
+        case kX64Push:
+        case kX64Cmp:
+        case kX64Test:
+          // When pointer compression is enabled 64-bit memory operands can't be
+          // used for tagged values.
+          return rep == MachineRepresentation::kWord64 ||
+                 (!COMPRESS_POINTERS_BOOL && IsAnyTagged(rep));
+        case kX64And32:
+        case kX64Or32:
+        case kX64Xor32:
+        case kX64Add32:
+        case kX64Sub32:
+        case kX64Cmp32:
+        case kX64Test32:
+          // When pointer compression is enabled 32-bit memory operands can be
+          // used for tagged values.
+          return rep == MachineRepresentation::kWord32 ||
+                 (COMPRESS_POINTERS_BOOL &&
+                  (IsAnyTagged(rep) || IsAnyCompressed(rep)));
+        case kAVXFloat64Add:
+        case kAVXFloat64Sub:
+        case kAVXFloat64Mul:
+          DCHECK_EQ(MachineRepresentation::kFloat64, rep);
+          return true;
+        case kAVXFloat32Add:
+        case kAVXFloat32Sub:
+        case kAVXFloat32Mul:
+          DCHECK_EQ(MachineRepresentation::kFloat32, rep);
+          return true;
+        case kX64Cmp16:
+        case kX64Test16:
+          return rep == MachineRepresentation::kWord16;
+        case kX64Cmp8:
+        case kX64Test8:
+          return rep == MachineRepresentation::kWord8;
+        default:
+          break;
+      }
       return false;
     }
-    MachineRepresentation rep =
-        LoadRepresentationOf(input->op()).representation();
-    switch (opcode) {
-      case kX64And:
-      case kX64Or:
-      case kX64Xor:
-      case kX64Add:
-      case kX64Sub:
-      case kX64Push:
-      case kX64Cmp:
-      case kX64Test:
-        // When pointer compression is enabled 64-bit memory operands can't be
-        // used for tagged values.
-        return rep == MachineRepresentation::kWord64 ||
-               (!COMPRESS_POINTERS_BOOL && IsAnyTagged(rep));
-      case kX64And32:
-      case kX64Or32:
-      case kX64Xor32:
-      case kX64Add32:
-      case kX64Sub32:
-      case kX64Cmp32:
-      case kX64Test32:
-        // When pointer compression is enabled 32-bit memory operands can be
-        // used for tagged values.
-        return rep == MachineRepresentation::kWord32 ||
-               (COMPRESS_POINTERS_BOOL &&
-                (IsAnyTagged(rep) || IsAnyCompressed(rep)));
-      case kAVXFloat64Add:
-      case kAVXFloat64Sub:
-      case kAVXFloat64Mul:
-        DCHECK_EQ(MachineRepresentation::kFloat64, rep);
-        return true;
-      case kAVXFloat32Add:
-      case kAVXFloat32Sub:
-      case kAVXFloat32Mul:
-        DCHECK_EQ(MachineRepresentation::kFloat32, rep);
-        return true;
-      case kX64Cmp16:
-      case kX64Test16:
-        return rep == MachineRepresentation::kWord16;
-      case kX64Cmp8:
-      case kX64Test8:
-        return rep == MachineRepresentation::kWord8;
-      default:
-        break;
-    }
-    return false;
   }
 
   AddressingMode GenerateMemoryOperandInputs(
@@ -266,66 +285,12 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     return mode;
   }
 
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(AddressingMode,
+                                          GetEffectiveAddressMemoryOperand)
+
   AddressingMode GetEffectiveAddressMemoryOperand(
-      Node* operand, InstructionOperand inputs[], size_t* input_count,
-      RegisterUseKind reg_kind = RegisterUseKind::kUseRegister) {
-    {
-      LoadMatcher<ExternalReferenceMatcher> m(operand);
-      if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
-          selector()->CanAddressRelativeToRootsRegister(
-              m.object().ResolvedValue())) {
-        ptrdiff_t const delta =
-            m.index().ResolvedValue() +
-            MacroAssemblerBase::RootRegisterOffsetForExternalReference(
-                selector()->isolate(), m.object().ResolvedValue());
-        if (is_int32(delta)) {
-          inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
-          return kMode_Root;
-        }
-      }
-    }
-    BaseWithIndexAndDisplacement64Matcher m(operand, AddressOption::kAllowAll);
-    DCHECK(m.matches());
-    // Decompress pointer by complex addressing mode.
-    if (IsCompressed(m.base())) {
-      DCHECK(m.index() == nullptr);
-      DCHECK(m.displacement() == nullptr || CanBeImmediate(m.displacement()));
-      AddressingMode mode = kMode_MCR;
-      inputs[(*input_count)++] = UseRegister(m.base(), reg_kind);
-      if (m.displacement() != nullptr) {
-        inputs[(*input_count)++] =
-            m.displacement_mode() == kNegativeDisplacement
-                ? UseNegatedImmediate(m.displacement())
-                : UseImmediate(m.displacement());
-        mode = kMode_MCRI;
-      }
-      return mode;
-    }
-    if (m.base() != nullptr &&
-        m.base()->opcode() == IrOpcode::kLoadRootRegister) {
-      DCHECK_EQ(m.index(), nullptr);
-      DCHECK_EQ(m.scale(), 0);
-      inputs[(*input_count)++] = UseImmediate(m.displacement());
-      return kMode_Root;
-    } else if (m.displacement() == nullptr ||
-               CanBeImmediate(m.displacement())) {
-      return GenerateMemoryOperandInputs(
-          m.index(), m.scale(), m.base(), m.displacement(),
-          m.displacement_mode(), inputs, input_count, reg_kind);
-    } else if (m.base() == nullptr &&
-               m.displacement_mode() == kPositiveDisplacement) {
-      // The displacement cannot be an immediate, but we can use the
-      // displacement as base instead and still benefit from addressing
-      // modes for the scale.
-      return GenerateMemoryOperandInputs(m.index(), m.scale(), m.displacement(),
-                                         nullptr, m.displacement_mode(), inputs,
-                                         input_count, reg_kind);
-    } else {
-      inputs[(*input_count)++] = UseRegister(operand->InputAt(0), reg_kind);
-      inputs[(*input_count)++] = UseRegister(operand->InputAt(1), reg_kind);
-      return kMode_MR1;
-    }
-  }
+      node_t operand, InstructionOperand inputs[], size_t* input_count,
+      RegisterUseKind reg_kind = RegisterUseKind::kUseRegister);
 
   InstructionOperand GetEffectiveIndexOperand(Node* index,
                                               AddressingMode* mode) {
@@ -338,11 +303,107 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     }
   }
 
-  bool CanBeBetterLeftOperand(Node* node) const {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanBeBetterLeftOperand)
+  bool CanBeBetterLeftOperand(node_t node) const {
     return !selector()->IsLive(node);
   }
 };
 
+template <>
+AddressingMode
+X64OperandGeneratorT<TurboshaftAdapter>::GetEffectiveAddressMemoryOperand(
+    node_t operand, InstructionOperand inputs[], size_t* input_count,
+    RegisterUseKind reg_kind) {
+  const turboshaft::Operation& op = this->turboshaft_graph()->Get(operand);
+  const turboshaft::LoadOp& load = op.Cast<turboshaft::LoadOp>();
+  const turboshaft::Operation& base =
+      this->turboshaft_graph()->Get(load.base());
+
+  if (const auto constant = base.TryCast<turboshaft::ConstantOp>();
+      constant != nullptr &&
+      constant->kind == turboshaft::ConstantOp::Kind::kExternal) {
+    ExternalReference reference = constant->external_reference();
+    if (selector()->CanAddressRelativeToRootsRegister(reference)) {
+      const ptrdiff_t delta =
+          load.offset +
+          MacroAssemblerBase::RootRegisterOffsetForExternalReference(
+              selector()->isolate(), reference);
+      if (is_int32(delta)) {
+        inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
+        return kMode_Root;
+      }
+    }
+  }
+
+  DCHECK_GE(op.input_count, 2);
+
+  // TODO(nicohartmann@): Implement this properly for Turboshaft.
+  inputs[(*input_count)++] = UseRegister(op.input(0), reg_kind);
+  inputs[(*input_count)++] = UseRegister(op.input(1), reg_kind);
+  return kMode_MR1;
+}
+
+template <>
+AddressingMode
+X64OperandGeneratorT<TurbofanAdapter>::GetEffectiveAddressMemoryOperand(
+    node_t operand, InstructionOperand inputs[], size_t* input_count,
+    RegisterUseKind reg_kind) {
+  {
+    LoadMatcher<ExternalReferenceMatcher> m(operand);
+    if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
+        selector()->CanAddressRelativeToRootsRegister(
+            m.object().ResolvedValue())) {
+      ptrdiff_t const delta =
+          m.index().ResolvedValue() +
+          MacroAssemblerBase::RootRegisterOffsetForExternalReference(
+              selector()->isolate(), m.object().ResolvedValue());
+      if (is_int32(delta)) {
+        inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
+        return kMode_Root;
+      }
+    }
+  }
+  BaseWithIndexAndDisplacement64Matcher m(operand, AddressOption::kAllowAll);
+  DCHECK(m.matches());
+  // Decompress pointer by complex addressing mode.
+  if (IsCompressed(m.base())) {
+    DCHECK(m.index() == nullptr);
+    DCHECK(m.displacement() == nullptr || CanBeImmediate(m.displacement()));
+    AddressingMode mode = kMode_MCR;
+    inputs[(*input_count)++] = UseRegister(m.base(), reg_kind);
+    if (m.displacement() != nullptr) {
+      inputs[(*input_count)++] = m.displacement_mode() == kNegativeDisplacement
+                                     ? UseNegatedImmediate(m.displacement())
+                                     : UseImmediate(m.displacement());
+      mode = kMode_MCRI;
+    }
+    return mode;
+  }
+  if (m.base() != nullptr &&
+      m.base()->opcode() == IrOpcode::kLoadRootRegister) {
+    DCHECK_EQ(m.index(), nullptr);
+    DCHECK_EQ(m.scale(), 0);
+    inputs[(*input_count)++] = UseImmediate(m.displacement());
+    return kMode_Root;
+  } else if (m.displacement() == nullptr || CanBeImmediate(m.displacement())) {
+    return GenerateMemoryOperandInputs(m.index(), m.scale(), m.base(),
+                                       m.displacement(), m.displacement_mode(),
+                                       inputs, input_count, reg_kind);
+  } else if (m.base() == nullptr &&
+             m.displacement_mode() == kPositiveDisplacement) {
+    // The displacement cannot be an immediate, but we can use the
+    // displacement as base instead and still benefit from addressing
+    // modes for the scale.
+    return GenerateMemoryOperandInputs(m.index(), m.scale(), m.displacement(),
+                                       nullptr, m.displacement_mode(), inputs,
+                                       input_count, reg_kind);
+  } else {
+    inputs[(*input_count)++] = UseRegister(operand->InputAt(0), reg_kind);
+    inputs[(*input_count)++] = UseRegister(operand->InputAt(1), reg_kind);
+    return kMode_MR1;
+  }
+}
+
 namespace {
 
 ArchOpcode GetLoadOpcode(LoadRepresentation load_rep) {
@@ -594,7 +655,7 @@ void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node, node_t value,
                                               InstructionCode opcode) {
   X64OperandGeneratorT<Adapter> g(this);
 #ifdef V8_IS_TSAN
@@ -615,21 +676,25 @@ void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
   AddressingMode mode =
       g.GetEffectiveAddressMemoryOperand(value, inputs, &input_count, reg_kind);
   InstructionCode code = opcode | AddressingModeField::encode(mode);
-  if (node->opcode() == IrOpcode::kProtectedLoad ||
-      ((node->opcode() == IrOpcode::kWord32AtomicLoad ||
-        node->opcode() == IrOpcode::kWord64AtomicLoad) &&
-       (AtomicLoadParametersOf(node->op()).kind() ==
-        MemoryAccessKind::kProtected))) {
-    code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
-  } else if (node->opcode() == IrOpcode::kLoadTrapOnNull) {
-    code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+  } else {
+    if (node->opcode() == IrOpcode::kProtectedLoad ||
+        ((node->opcode() == IrOpcode::kWord32AtomicLoad ||
+          node->opcode() == IrOpcode::kWord64AtomicLoad) &&
+         (AtomicLoadParametersOf(node->op()).kind() ==
+          MemoryAccessKind::kProtected))) {
+      code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
+    } else if (node->opcode() == IrOpcode::kLoadTrapOnNull) {
+      code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);
+    }
   }
   Emit(code, 1, outputs, input_count, inputs, temp_count, temps);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
-  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
+  LoadRepresentation load_rep = this->load_view(node).loaded_rep();
   DCHECK(!load_rep.IsMapWord());
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
@@ -831,14 +896,22 @@ void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
   Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
+static void VisitBinop(InstructionSelectorT<TurboshaftAdapter>*, Node* node,
+                       InstructionCode opcode,
+                       FlagsContinuationT<TurboshaftAdapter>* cont) {
+  UNIMPLEMENTED();
+}
+
 // Shared routine for multiple binary operations.
 template <typename Adapter>
-static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
-                       InstructionCode opcode, FlagsContinuation* cont) {
+static void VisitBinop(InstructionSelectorT<Adapter>* selector,
+                       typename Adapter::node_t node, InstructionCode opcode,
+                       FlagsContinuationT<Adapter>* cont) {
   X64OperandGeneratorT<Adapter> g(selector);
-  Int32BinopMatcher m(node);
-  Node* left = m.left().node();
-  Node* right = m.right().node();
+  auto binop = selector->word_binop_view(node);
+  binop.EnsureConstantIsRightIfCommutative();
+  typename Adapter::node_t left = binop.left();
+  typename Adapter::node_t right = binop.right();
   InstructionOperand inputs[8];
   size_t input_count = 0;
   InstructionOperand outputs[1];
@@ -861,8 +934,7 @@ static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
     inputs[input_count++] = g.UseImmediate(right);
   } else {
     int effect_level = selector->GetEffectLevel(node, cont);
-    if (node->op()->HasProperty(Operator::kCommutative) &&
-        g.CanBeBetterLeftOperand(right) &&
+    if (selector->IsCommutative(node) && g.CanBeBetterLeftOperand(right) &&
         (!g.CanBeBetterLeftOperand(left) ||
          !g.CanBeMemoryOperand(opcode, node, right, effect_level))) {
       std::swap(left, right);
@@ -898,7 +970,7 @@ static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter>
 static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                        InstructionCode opcode) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitBinop(selector, node, opcode, &cont);
 }
 
@@ -968,18 +1040,25 @@ void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
-    Node* node, FlagsContinuation* cont) {
-  StackCheckKind kind = StackCheckKindOf(node->op());
+    node_t node, FlagsContinuation* cont) {
+  StackCheckKind kind;
+  if constexpr (Adapter::IsTurboshaft) {
+    kind = this->turboshaft_graph()
+               ->Get(node)
+               .template Cast<turboshaft::StackPointerGreaterThanOp>()
+               .kind;
+  } else {
+    kind = StackCheckKindOf(node->op());
+  }
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
   int effect_level = GetEffectLevel(node, cont);
 
   X64OperandGeneratorT<Adapter> g(this);
-  Node* const value = node->InputAt(0);
+  node_t value = this->input_at(node, 0);
   if (g.CanBeMemoryOperand(kX64Cmp, node, value, effect_level)) {
-    DCHECK(IrOpcode::kLoad == value->opcode() ||
-           IrOpcode::kLoadImmutable == value->opcode());
+    DCHECK(this->IsLoadOrLoadImmutable(value));
 
     // GetEffectiveAddressMemoryOperand can create at most 3 inputs.
     static constexpr int kMaxInputCount = 3;
@@ -1104,7 +1183,7 @@ void VisitWord64Shift(InstructionSelectorT<Adapter>* selector, Node* node,
 // Shared routine for multiple shift operations with continuation.
 template <typename Adapter, typename BinopMatcher, int Bits>
 bool TryVisitWordShift(InstructionSelectorT<Adapter>* selector, Node* node,
-                       ArchOpcode opcode, FlagsContinuation* cont) {
+                       ArchOpcode opcode, FlagsContinuationT<Adapter>* cont) {
   X64OperandGeneratorT<Adapter> g(selector);
   BinopMatcher m(node);
   Node* left = m.left().node();
@@ -1416,12 +1495,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Add, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Add, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Add, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Add, &cont);
 }
 
 template <typename Adapter>
@@ -1488,12 +1572,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Sub, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Sub, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Sub, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Sub, &cont);
 }
 
 namespace {
@@ -1570,13 +1659,18 @@ void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
-  // TODO(mvstanton): Use Int32ScaleMatcher somehow.
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Imul32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    // TODO(mvstanton): Use Int32ScaleMatcher somehow.
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Imul32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Imul32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Imul32, &cont);
 }
 
 template <typename Adapter>
@@ -1594,12 +1688,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Imul, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Imul, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Imul, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Imul, &cont);
 }
 
 template <typename Adapter>
@@ -2262,7 +2361,8 @@ void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(node_t node, int index) {
+}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
@@ -2271,7 +2371,7 @@ void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
-    Node* node) {
+    node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
 
   // Prepare for C function call.
@@ -2283,7 +2383,7 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     // Poke any stack arguments.
     for (size_t n = 0; n < arguments->size(); ++n) {
       PushParameter input = (*arguments)[n];
-      if (input.node) {
+      if (this->valid(input.node)) {
         int slot = static_cast<int>(n);
         InstructionOperand value = g.CanBeImmediate(input.node)
                                        ? g.UseImmediate(input.node)
@@ -2299,7 +2399,7 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
       stack_decrement += kSystemPointerSize;
       // Skip holes in the param array. These represent both extra slots for
       // multi-slot values and padding slots for alignment.
-      if (input.node == nullptr) continue;
+      if (!this->valid(input.node)) continue;
       InstructionOperand decrement = g.UseImmediate(stack_decrement);
       stack_decrement = 0;
       if (g.CanBeImmediate(input.node)) {
@@ -2329,12 +2429,12 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
+    node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
     // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
+    if (this->valid(output.node)) {
       DCHECK(!call_descriptor->IsCFunctionCall());
       if (output.location.GetType() == MachineType::Float32()) {
         MarkAsFloat32(output.node);
@@ -2361,11 +2461,11 @@ namespace {
 
 template <typename Adapter>
 void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
-                                   InstructionCode opcode, Node* left,
+                                   InstructionCode opcode,
+                                   typename Adapter::node_t left,
                                    InstructionOperand right,
-                                   FlagsContinuation* cont) {
-  DCHECK(IrOpcode::kLoad == left->opcode() ||
-         IrOpcode::kLoadImmutable == left->opcode());
+                                   FlagsContinuationT<Adapter>* cont) {
+  DCHECK(selector->IsLoadOrLoadImmutable(left));
   X64OperandGeneratorT<Adapter> g(selector);
   size_t input_count = 0;
   InstructionOperand inputs[6];
@@ -2391,7 +2491,7 @@ void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
                   InstructionCode opcode, InstructionOperand left,
-                  InstructionOperand right, FlagsContinuation* cont) {
+                  InstructionOperand right, FlagsContinuationT<Adapter>* cont) {
   if (cont->IsSelect()) {
     X64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[4] = {left, right};
@@ -2412,8 +2512,9 @@ void VisitCompare(InstructionSelectorT<Adapter>* selector,
 // Shared routine for multiple compare operations.
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
-                  InstructionCode opcode, Node* left, Node* right,
-                  FlagsContinuation* cont, bool commutative) {
+                  InstructionCode opcode, typename Adapter::node_t left,
+                  typename Adapter::node_t right,
+                  FlagsContinuationT<Adapter>* cont, bool commutative) {
   X64OperandGeneratorT<Adapter> g(selector);
   if (commutative && g.CanBeBetterLeftOperand(right)) {
     std::swap(left, right);
@@ -2520,8 +2621,10 @@ MachineType MachineTypeForNarrowWordAnd(Node* and_node, Node* constant_node) {
 
 // Tries to match the size of the given opcode to that of the operands, if
 // possible.
+template <typename Adapter>
 InstructionCode TryNarrowOpcodeSize(InstructionCode opcode, Node* left,
-                                    Node* right, FlagsContinuation* cont) {
+                                    Node* right,
+                                    FlagsContinuationT<Adapter>* cont) {
   MachineType left_type = MachineType::None();
   MachineType right_type = MachineType::None();
   if (IsWordAnd(left) && IsIntConstant(right)) {
@@ -2641,64 +2744,69 @@ void RemoveUnnecessaryWordAnd(InstructionCode opcode, Node** and_node) {
 
 // Shared routine for multiple word compare operations.
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont) {
-  X64OperandGeneratorT<Adapter> g(selector);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node, InstructionCode opcode,
+                      FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    X64OperandGeneratorT<Adapter> g(selector);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+
+    // The 32-bit comparisons automatically truncate Word64
+    // values to Word32 range, no need to do that explicitly.
+    if (opcode == kX64Cmp32 || opcode == kX64Test32) {
+      if (left->opcode() == IrOpcode::kTruncateInt64ToInt32) {
+        left = left->InputAt(0);
+      }
 
-  // The 32-bit comparisons automatically truncate Word64
-  // values to Word32 range, no need to do that explicitly.
-  if (opcode == kX64Cmp32 || opcode == kX64Test32) {
-    if (left->opcode() == IrOpcode::kTruncateInt64ToInt32) {
-      left = left->InputAt(0);
+      if (right->opcode() == IrOpcode::kTruncateInt64ToInt32) {
+        right = right->InputAt(0);
+      }
     }
 
-    if (right->opcode() == IrOpcode::kTruncateInt64ToInt32) {
-      right = right->InputAt(0);
-    }
-  }
+    opcode = TryNarrowOpcodeSize(opcode, left, right, cont);
 
-  opcode = TryNarrowOpcodeSize(opcode, left, right, cont);
+    // If one of the two inputs is an immediate, make sure it's on the right, or
+    // if one of the two inputs is a memory operand, make sure it's on the left.
+    int effect_level = selector->GetEffectLevel(node, cont);
 
-  // If one of the two inputs is an immediate, make sure it's on the right, or
-  // if one of the two inputs is a memory operand, make sure it's on the left.
-  int effect_level = selector->GetEffectLevel(node, cont);
+    if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
+        (g.CanBeMemoryOperand(opcode, node, right, effect_level) &&
+         !g.CanBeMemoryOperand(opcode, node, left, effect_level))) {
+      if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
+      std::swap(left, right);
+    }
 
-  if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
-      (g.CanBeMemoryOperand(opcode, node, right, effect_level) &&
-       !g.CanBeMemoryOperand(opcode, node, left, effect_level))) {
-    if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
-    std::swap(left, right);
-  }
+    if (IsWordAnd(left)) {
+      RemoveUnnecessaryWordAnd(opcode, &left);
+    }
 
-  if (IsWordAnd(left)) {
-    RemoveUnnecessaryWordAnd(opcode, &left);
-  }
+    // Match immediates on right side of comparison.
+    if (g.CanBeImmediate(right)) {
+      if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
+        return VisitCompareWithMemoryOperand(selector, opcode, left,
+                                             g.UseImmediate(right), cont);
+      }
+      return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
+                          cont);
+    }
 
-  // Match immediates on right side of comparison.
-  if (g.CanBeImmediate(right)) {
+    // Match memory operands on left side of comparison.
     if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
       return VisitCompareWithMemoryOperand(selector, opcode, left,
-                                           g.UseImmediate(right), cont);
+                                           g.UseRegister(right), cont);
     }
-    return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
-                        cont);
-  }
 
-  // Match memory operands on left side of comparison.
-  if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
-    return VisitCompareWithMemoryOperand(selector, opcode, left,
-                                         g.UseRegister(right), cont);
+    return VisitCompare(selector, opcode, left, right, cont,
+                        node->op()->HasProperty(Operator::kCommutative));
   }
-
-  return VisitCompare(selector, opcode, left, right, cont,
-                      node->op()->HasProperty(Operator::kCommutative));
 }
 
 template <typename Adapter>
 void VisitWord64EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
-                          FlagsContinuation* cont) {
+                          FlagsContinuationT<Adapter>* cont) {
   if (selector->CanUseRootsRegister()) {
     X64OperandGeneratorT<Adapter> g(selector);
     const RootsTable& roots_table = selector->isolate()->roots_table();
@@ -2719,59 +2827,129 @@ void VisitWord64EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
 }
 
 template <typename Adapter>
-void VisitWord32EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
-                          FlagsContinuation* cont) {
-  if (COMPRESS_POINTERS_BOOL && selector->isolate()) {
-    X64OperandGeneratorT<Adapter> g(selector);
-    const RootsTable& roots_table = selector->isolate()->roots_table();
-    RootIndex root_index;
-    Node* left = nullptr;
-    Handle<HeapObject> right;
-    // HeapConstants and CompressedHeapConstants can be treated the same when
-    // using them as an input to a 32-bit comparison. Check whether either is
-    // present.
-    {
-      CompressedHeapObjectBinopMatcher m(node);
-      if (m.right().HasResolvedValue()) {
-        left = m.left().node();
-        right = m.right().ResolvedValue();
-      } else {
-        HeapObjectBinopMatcher m2(node);
-        if (m2.right().HasResolvedValue()) {
-          left = m2.left().node();
-          right = m2.right().ResolvedValue();
+void VisitWord32EqualImpl(InstructionSelectorT<Adapter>* selector,
+                          typename Adapter::node_t node,
+                          FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (COMPRESS_POINTERS_BOOL && selector->isolate()) {
+      X64OperandGeneratorT<Adapter> g(selector);
+      const RootsTable& roots_table = selector->isolate()->roots_table();
+      RootIndex root_index;
+      Node* left = nullptr;
+      Handle<HeapObject> right;
+      // HeapConstants and CompressedHeapConstants can be treated the same when
+      // using them as an input to a 32-bit comparison. Check whether either is
+      // present.
+      {
+        CompressedHeapObjectBinopMatcher m(node);
+        if (m.right().HasResolvedValue()) {
+          left = m.left().node();
+          right = m.right().ResolvedValue();
+        } else {
+          HeapObjectBinopMatcher m2(node);
+          if (m2.right().HasResolvedValue()) {
+            left = m2.left().node();
+            right = m2.right().ResolvedValue();
+          }
+        }
+      }
+      if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
+        DCHECK_NE(left, nullptr);
+        if (RootsTable::IsReadOnly(root_index) &&
+            (V8_STATIC_ROOTS_BOOL || !selector->isolate()->bootstrapper())) {
+          return VisitCompare(
+              selector, kX64Cmp32, g.UseRegister(left),
+              g.TempImmediate(MacroAssemblerBase::ReadOnlyRootPtr(
+                  root_index, selector->isolate())),
+              cont);
+        }
+        if (selector->CanUseRootsRegister()) {
+          InstructionCode opcode =
+              kX64Cmp32 | AddressingModeField::encode(kMode_Root);
+          return VisitCompare(
+              selector, opcode,
+              g.TempImmediate(
+                  MacroAssemblerBase::RootRegisterOffsetForRootIndex(
+                      root_index)),
+              g.UseRegister(left), cont);
         }
       }
     }
-    if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
-      DCHECK_NE(left, nullptr);
-      if (RootsTable::IsReadOnly(root_index) &&
-          (V8_STATIC_ROOTS_BOOL || !selector->isolate()->bootstrapper())) {
-        return VisitCompare(selector, kX64Cmp32, g.UseRegister(left),
-                            g.TempImmediate(MacroAssemblerBase::ReadOnlyRootPtr(
-                                root_index, selector->isolate())),
-                            cont);
+    VisitWordCompare(selector, node, kX64Cmp32, cont);
+  }
+}
+
+void VisitCompareZero(InstructionSelectorT<TurboshaftAdapter>* selector,
+                      turboshaft::OpIndex user, turboshaft::OpIndex node,
+                      InstructionCode opcode,
+                      FlagsContinuationT<TurboshaftAdapter>* cont) {
+  X64OperandGeneratorT<TurboshaftAdapter> g(selector);
+  const turboshaft::Operation& op = selector->turboshaft_graph()->Get(node);
+  if (cont->IsBranch() &&
+      (cont->condition() == kNotEqual || cont->condition() == kEqual)) {
+    if (const auto binop = op.TryCast<turboshaft::WordBinopOp>()) {
+      using Kind = turboshaft::WordBinopOp::Kind;
+      if (selector->IsOnlyUserOfNodeInSameBlock(user, node)) {
+        const bool is64 =
+            binop->rep == turboshaft::WordRepresentation::Word64();
+        switch (binop->kind) {
+          case Kind::kAdd:
+            return VisitBinop(selector, node, is64 ? kX64Add : kX64Add32, cont);
+          case Kind::kSub:
+            return VisitBinop(selector, node, is64 ? kX64Sub : kX64Sub32, cont);
+          case Kind::kBitwiseAnd:
+            return VisitBinop(selector, node, is64 ? kX64And : kX64And32, cont);
+          case Kind::kBitwiseOr:
+            return VisitBinop(selector, node, is64 ? kX64Or : kX64Or32, cont);
+          default:
+            break;
+        }
       }
-      if (selector->CanUseRootsRegister()) {
-        InstructionCode opcode =
-            kX64Cmp32 | AddressingModeField::encode(kMode_Root);
-        return VisitCompare(
-            selector, opcode,
-            g.TempImmediate(
-                MacroAssemblerBase::RootRegisterOffsetForRootIndex(root_index)),
-            g.UseRegister(left), cont);
+    } else if (const auto shift = op.TryCast<turboshaft::ShiftOp>()) {
+      using Kind = turboshaft::ShiftOp::Kind;
+      if (selector->IsOnlyUserOfNodeInSameBlock(user, node)) {
+        switch (shift->kind) {
+          case Kind::kShiftLeft:
+          case Kind::kShiftRightLogical:
+            UNIMPLEMENTED();
+          default:
+            break;
+        }
       }
     }
   }
-  VisitWordCompare(selector, node, kX64Cmp32, cont);
+
+  int effect_level = selector->GetEffectLevel(node, cont);
+  if (const auto load = op.TryCast<turboshaft::LoadOp>()) {
+    if (load->loaded_rep == turboshaft::MemoryRepresentation::Int8()) {
+      if (opcode == kX64Cmp32) {
+        opcode = kX64Cmp8;
+      } else if (opcode == kX64Test32) {
+        opcode = kX64Test8;
+      }
+    } else if (load->loaded_rep == turboshaft::MemoryRepresentation::Int16()) {
+      if (opcode == kX64Cmp32) {
+        opcode = kX64Cmp16;
+      } else if (opcode == kX64Test32) {
+        opcode = kX64Test16;
+      }
+    }
+  }
+  if (g.CanBeMemoryOperand(opcode, user, node, effect_level)) {
+    VisitCompareWithMemoryOperand(selector, opcode, node, g.TempImmediate(0),
+                                  cont);
+  } else {
+    VisitCompare(selector, opcode, g.Use(node), g.TempImmediate(0), cont);
+  }
 }
 
 // Shared routine for comparison with zero.
-template <typename Adapter>
-void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
-                      Node* node, InstructionCode opcode,
-                      FlagsContinuation* cont) {
-  X64OperandGeneratorT<Adapter> g(selector);
+void VisitCompareZero(InstructionSelectorT<TurbofanAdapter>* selector,
+                      Node* user, Node* node, InstructionCode opcode,
+                      FlagsContinuationT<TurbofanAdapter>* cont) {
+  X64OperandGeneratorT<TurbofanAdapter> g(selector);
   if (cont->IsBranch() &&
       (cont->condition() == kNotEqual || cont->condition() == kEqual)) {
     switch (node->opcode()) {
@@ -2794,8 +2972,10 @@ void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
 #undef FLAGS_SET_BINOP_LIST
 #undef FLAGS_SET_BINOP
 
-#define TRY_VISIT_WORD32_SHIFT TryVisitWordShift<Adapter, Int32BinopMatcher, 32>
-#define TRY_VISIT_WORD64_SHIFT TryVisitWordShift<Adapter, Int64BinopMatcher, 64>
+#define TRY_VISIT_WORD32_SHIFT \
+  TryVisitWordShift<TurbofanAdapter, Int32BinopMatcher, 32>
+#define TRY_VISIT_WORD64_SHIFT \
+  TryVisitWordShift<TurbofanAdapter, Int64BinopMatcher, 64>
 // Skip Word64Sar/Word32Sar since no instruction reduction in most cases.
 #define FLAGS_SET_SHIFT_LIST(V)                    \
   V(kWord32Shl, TRY_VISIT_WORD32_SHIFT, kX64Shl32) \
@@ -2849,10 +3029,11 @@ void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
 
 // Shared routine for multiple float32 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   InstructionCode const opcode =
       selector->IsSupported(AVX) ? kAVXFloat32Cmp : kSSEFloat32Cmp;
   VisitCompare(selector, opcode, right, left, cont, false);
@@ -2860,10 +3041,11 @@ void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
 
 // Shared routine for multiple float64 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   InstructionCode const opcode =
       selector->IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
   VisitCompare(selector, opcode, right, left, cont, false);
@@ -2921,8 +3103,35 @@ void VisitAtomicCompareExchange(InstructionSelectorT<Adapter>* selector,
 }  // namespace
 
 // Shared routine for word comparison against zero.
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(
+    node_t user, node_t value, FlagsContinuation* cont) {
+  for (const turboshaft::EqualOp* equal =
+           this->turboshaft_graph()->Get(value).TryCast<turboshaft::EqualOp>();
+       equal != nullptr && CanCover(user, value);) {
+    const turboshaft::ConstantOp* right_constant =
+        this->turboshaft_graph()
+            ->Get(equal->right())
+            .TryCast<turboshaft::ConstantOp>();
+    if (right_constant == nullptr) break;
+    if (right_constant->kind != turboshaft::ConstantOp::Kind::kWord32) break;
+    if (right_constant->word32() != 0) break;
+
+    user = value;
+    value = equal->left();
+    cont->Negate();
+  }
+
+  if (CanCover(user, value)) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+  }
+
+  // Branch could not be combined with a compare, emit compare against 0.
+  VisitCompareZero(this, user, value, kX64Cmp32, cont);
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(
     Node* user, Node* value, FlagsContinuation* cont) {
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
@@ -3079,7 +3288,8 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+  if (enable_switch_jump_table_ ==
+      InstructionSelector::kEnableSwitchJumpTable) {
     static const size_t kMaxTableSwitchValueRange = 2 << 16;
     size_t table_space_cost = 4 + sw.value_range();
     size_t table_time_cost = 3;
@@ -3115,37 +3325,43 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
-  Node* user = node;
+void InstructionSelectorT<Adapter>::VisitWord32Equal(node_t node) {
+  node_t user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int32BinopMatcher m(user);
-  if (m.right().Is(0)) {
-    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    USE(user);
+    UNIMPLEMENTED();
+  } else {
+    Int32BinopMatcher m(user);
+    if (m.right().Is(0)) {
+      return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+    }
   }
   VisitWord32EqualImpl(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
@@ -3153,115 +3369,155 @@ void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int64BinopMatcher m(node);
-  if (m.right().Is(0)) {
-    // Try to combine the equality check with a comparison.
-    Node* const user = m.node();
-    Node* const value = m.left().node();
-    if (CanCover(user, value)) {
-      switch (value->opcode()) {
-        case IrOpcode::kInt64Sub:
-          return VisitWordCompare(this, value, kX64Cmp, &cont);
-        case IrOpcode::kWord64And:
-          return VisitWordCompare(this, value, kX64Test, &cont);
-        default:
-          break;
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+    Int64BinopMatcher m(node);
+    if (m.right().Is(0)) {
+      // Try to combine the equality check with a comparison.
+      Node* const user = m.node();
+      Node* const value = m.left().node();
+      if (CanCover(user, value)) {
+        switch (value->opcode()) {
+          case IrOpcode::kInt64Sub:
+            return VisitWordCompare(this, value, kX64Cmp, &cont);
+          case IrOpcode::kWord64And:
+            return VisitWordCompare(this, value, kX64Test, &cont);
+          default:
+            break;
+        }
       }
     }
+    VisitWord64EqualImpl(this, node, &cont);
   }
-  VisitWord64EqualImpl(this, node, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Add32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Add32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Add32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Add32, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Sub32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Sub32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Sub32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Sub32, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
-  Float64BinopMatcher m(node);
-  if (m.left().Is(0.0) && m.right().IsFloat64Abs()) {
-    // This matches the pattern
-    //
-    //   Float64LessThan(#0.0, Float64Abs(x))
-    //
-    // which TurboFan generates for NumberToBoolean in the general case,
-    // and which evaluates to false if x is 0, -0 or NaN. We can compile
-    // this to a simple (v)ucomisd using not_equal flags condition, which
-    // avoids the costly Float64Abs.
-    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, node);
-    InstructionCode const opcode =
-        IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
-    return VisitCompare(this, opcode, m.left().node(), m.right().InputAt(0),
-                        &cont, false);
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    Float64BinopMatcher m(node);
+    if (m.left().Is(0.0) && m.right().IsFloat64Abs()) {
+      // This matches the pattern
+      //
+      //   Float64LessThan(#0.0, Float64Abs(x))
+      //
+      // which TurboFan generates for NumberToBoolean in the general case,
+      // and which evaluates to false if x is 0, -0 or NaN. We can compile
+      // this to a simple (v)ucomisd using not_equal flags condition, which
+      // avoids the costly Float64Abs.
+      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, node);
+      InstructionCode const opcode =
+          IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
+      return VisitCompare(this, opcode, m.left().node(), m.right().InputAt(0),
+                          &cont, false);
+    }
   }
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
@@ -3269,7 +3525,7 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
@@ -4968,15 +5224,14 @@ void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
-    OperandGenerator* g, int first_input_index, Node* node) {
+    OperandGenerator* g, int first_input_index, node_t node) {
   continuation_outputs_.push_back(
       g->DefineSameAsInput(node, first_input_index));
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags =
       MachineOperatorBuilder::kWord32ShiftIsSafe |
       MachineOperatorBuilder::kWord32Ctz | MachineOperatorBuilder::kWord64Ctz |
@@ -5001,9 +5256,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
