diff --git a/src/maglev/arm64/maglev-assembler-arm64.cc b/src/maglev/arm64/maglev-assembler-arm64.cc
index 6ca8a0b0830..27aa499a53a 100644
--- a/src/maglev/arm64/maglev-assembler-arm64.cc
+++ b/src/maglev/arm64/maglev-assembler-arm64.cc
@@ -4,7 +4,7 @@
 
 #include "src/codegen/interface-descriptors-inl.h"
 #include "src/deoptimizer/deoptimizer.h"
-#include "src/maglev/arm64/maglev-assembler-arm64-inl.h"
+#include "src/maglev/maglev-assembler-inl.h"
 #include "src/maglev/maglev-graph.h"
 
 namespace v8 {
@@ -35,8 +35,8 @@ void MaglevAssembler::Allocate(RegisterSnapshot& register_snapshot,
           : ExternalReference::old_space_allocation_limit_address(isolate_);
 
   ZoneLabelRef done(this);
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.AcquireX();
+  ScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
   // We are a bit short on registers, so we use the same register for {object}
   // and {new_top}. Once we have defined {new_top}, we don't use {object} until
   // {new_top} is used for the last time. And there (at the end of this
@@ -86,10 +86,10 @@ void MaglevAssembler::AllocateHeapNumber(RegisterSnapshot register_snapshot,
   // allocation call might trash it.
   register_snapshot.live_double_registers.set(value);
   Allocate(register_snapshot, result, HeapNumber::kSize);
-  // `Allocate` needs 2 scratch registers, so it's important to `AcquireX` after
+  // `Allocate` needs 2 scratch registers, so it's important to `Acquire` after
   // `Allocate` is done and not before.
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.AcquireX();
+  ScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
   LoadRoot(scratch, RootIndex::kHeapNumberMap);
   StoreTaggedField(scratch, FieldMemOperand(result, HeapObject::kMapOffset));
   Str(value, FieldMemOperand(result, HeapNumber::kValueOffset));
@@ -98,8 +98,8 @@ void MaglevAssembler::AllocateHeapNumber(RegisterSnapshot register_snapshot,
 void MaglevAssembler::ToBoolean(Register value, ZoneLabelRef is_true,
                                 ZoneLabelRef is_false,
                                 bool fallthrough_when_true) {
-  UseScratchRegisterScope temps(this);
-  Register map = temps.AcquireX();
+  ScratchRegisterScope temps(this);
+  Register map = temps.Acquire();
 
   // Check if {{value}} is Smi.
   Condition is_smi = CheckSmi(value);
@@ -125,8 +125,8 @@ void MaglevAssembler::ToBoolean(Register value, ZoneLabelRef is_true,
   // Check if {{value}} is undetectable.
   LoadMap(map, value);
   {
-    UseScratchRegisterScope scope(this);
-    Register tmp = scope.AcquireW();
+    ScratchRegisterScope scope(this);
+    Register tmp = scope.Acquire().W();
     Move(tmp, FieldMemOperand(map, Map::kBitFieldOffset));
     Tst(tmp, Immediate(Map::Bits1::IsUndetectableBit::kMask));
     JumpIf(ne, *is_false);
@@ -138,8 +138,8 @@ void MaglevAssembler::ToBoolean(Register value, ZoneLabelRef is_true,
       eq,
       [](MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
          ZoneLabelRef is_false) {
-        UseScratchRegisterScope scope(masm);
-        DoubleRegister value_double = scope.AcquireD();
+        ScratchRegisterScope scope(masm);
+        DoubleRegister value_double = scope.AcquireDouble();
         __ Ldr(value_double, FieldMemOperand(value, HeapNumber::kValueOffset));
         __ Fcmp(value_double, 0.0);
         __ JumpIf(eq, *is_false);
@@ -154,8 +154,8 @@ void MaglevAssembler::ToBoolean(Register value, ZoneLabelRef is_true,
       eq,
       [](MaglevAssembler* masm, Register value, ZoneLabelRef is_true,
          ZoneLabelRef is_false) {
-        UseScratchRegisterScope scope(masm);
-        Register tmp = scope.AcquireW();
+        ScratchRegisterScope scope(masm);
+        Register tmp = scope.Acquire().W();
         __ Ldr(tmp, FieldMemOperand(value, BigInt::kBitfieldOffset));
         __ Tst(tmp, Immediate(BigInt::LengthBits::kMask));
         __ JumpIf(eq, *is_false);
@@ -175,6 +175,16 @@ void MaglevAssembler::Prologue(Graph* graph) {
     UNREACHABLE();
   }
 
+  ScratchRegisterScope temps(this);
+  //  We add two extra registers to the scope. Ideally we could add all the
+  //  allocatable general registers, except Context, JSFunction, NewTarget and
+  //  ArgCount. Unfortunately, OptimizeCodeOrTailCallOptimizedCodeSlot and
+  //  LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing pick random registers and
+  //  we could alias those.
+  // TODO(victorgomes): Fix these builtins to either use the scope or pass the
+  // used registers manually.
+  temps.Include({x14, x15});
+
   CallTarget();
 
   BailoutIfDeoptimized();
@@ -182,25 +192,19 @@ void MaglevAssembler::Prologue(Graph* graph) {
   // Tiering support.
   // TODO(jgruber): Extract to a builtin.
   {
-    UseScratchRegisterScope temps(this);
-    Register flags = temps.AcquireX();
-    // TODO(v8:7700): There are only 2 available scratch registers, we use x9,
-    // which is a local caller saved register instead here, since
-    // LoadFeedbackVectorFlagsAndJumpIfNeedsProcessing requests a scratch
-    // register as well.
-    Register feedback_vector = x9;
+    ScratchRegisterScope temps(this);
+    Register flags = temps.Acquire();
+    Register feedback_vector = temps.Acquire();
 
     DeferredCodeInfo* deferred_flags_need_processing = PushDeferredCode(
-        [](MaglevAssembler* masm, Register feedback_vector) {
+        [](MaglevAssembler* masm, Register flags, Register feedback_vector) {
           ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
           // TODO(leszeks): This could definitely be a builtin that we
           // tail-call.
-          UseScratchRegisterScope temps(masm);
-          Register flags = temps.AcquireX();
           __ OptimizeCodeOrTailCallOptimizedCodeSlot(flags, feedback_vector);
           __ Trap();
         },
-        feedback_vector);
+        flags, feedback_vector);
 
     Move(feedback_vector,
          compilation_info()->toplevel_compilation_unit()->feedback().object());
@@ -227,8 +231,8 @@ void MaglevAssembler::Prologue(Graph* graph) {
     // interrupt limit. The interrupt limit is either equal to the real
     // stack limit or tighter. By ensuring we have space until that limit
     // after building the frame we can quickly precheck both at once.
-    UseScratchRegisterScope temps(this);
-    Register stack_slots_size = temps.AcquireX();
+    ScratchRegisterScope temps(this);
+    Register stack_slots_size = temps.Acquire();
     Mov(stack_slots_size, fp);
     // Round up the stack slots and max call args separately, since both will be
     // padded by their respective uses.
@@ -238,7 +242,7 @@ void MaglevAssembler::Prologue(Graph* graph) {
         std::max(static_cast<int>(graph->max_deopted_stack_size()),
                  max_stack_slots_used * kSystemPointerSize);
     Sub(stack_slots_size, stack_slots_size, Immediate(max_stack_size));
-    Register interrupt_stack_limit = temps.AcquireX();
+    Register interrupt_stack_limit = temps.Acquire();
     LoadStackLimit(interrupt_stack_limit, StackLimitKind::kInterruptStackLimit);
     Cmp(stack_slots_size, interrupt_stack_limit);
 
@@ -249,8 +253,8 @@ void MaglevAssembler::Prologue(Graph* graph) {
            int max_stack_size) {
           ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
           __ PushAll(register_inputs);
-          UseScratchRegisterScope temps(masm);
-          Register scratch = temps.AcquireX();
+          ScratchRegisterScope temps(masm);
+          Register scratch = temps.Acquire();
           __ Mov(scratch, Smi::FromInt(max_stack_size * kSystemPointerSize));
           __ PushArgument(scratch);
           __ CallRuntime(Runtime::kStackGuardWithGap, 1);
@@ -284,8 +288,8 @@ void MaglevAssembler::Prologue(Graph* graph) {
         Push(xzr, xzr);
       }
     } else {
-      UseScratchRegisterScope temps(this);
-      Register count = temps.AcquireX();
+      ScratchRegisterScope temps(this);
+      Register count = temps.Acquire();
       // Extract the first few slots to round to the unroll size.
       int first_slots = tagged_two_slots_count % kLoopUnrollSize;
       for (int i = 0; i < first_slots; ++i) {
@@ -325,8 +329,8 @@ void MaglevAssembler::MaybeEmitDeoptBuiltinsCall(size_t eager_deopt_count,
       false, false,
       static_cast<int>(deopt_count) * Deoptimizer::kLazyDeoptExitSize);
 
-  UseScratchRegisterScope scope(this);
-  Register scratch = scope.AcquireX();
+  ScratchRegisterScope scope(this);
+  Register scratch = scope.Acquire();
   if (eager_deopt_count > 0) {
     Bind(eager_deopt_entry);
     LoadEntryFromBuiltin(Builtin::kDeoptimizationEntry_Eager, scratch);
@@ -343,8 +347,8 @@ void MaglevAssembler::AllocateTwoByteString(RegisterSnapshot register_snapshot,
                                             Register result, int length) {
   int size = SeqTwoByteString::SizeFor(length);
   Allocate(register_snapshot, result, size);
-  UseScratchRegisterScope scope(this);
-  Register scratch = scope.AcquireX();
+  ScratchRegisterScope scope(this);
+  Register scratch = scope.Acquire();
   Move(scratch, 0);
   StoreTaggedField(scratch, FieldMemOperand(result, size - kObjectAlignment));
   LoadRoot(scratch, RootIndex::kStringMap);
@@ -463,8 +467,8 @@ void MaglevAssembler::StringCharCodeAt(RegisterSnapshot& register_snapshot,
       FieldMemOperand(instance_type, Map::kInstanceTypeOffset));
 
   {
-    UseScratchRegisterScope temps(this);
-    Register representation = temps.AcquireW();
+    ScratchRegisterScope temps(this);
+    Register representation = temps.Acquire().W();
 
     // TODO(victorgomes): Add fast path for external strings.
     And(representation, instance_type.W(),
@@ -489,8 +493,8 @@ void MaglevAssembler::StringCharCodeAt(RegisterSnapshot& register_snapshot,
 
   bind(&sliced_string);
   {
-    UseScratchRegisterScope temps(this);
-    Register offset = temps.AcquireX();
+    ScratchRegisterScope temps(this);
+    Register offset = temps.Acquire();
 
     Ldr(offset.W(), FieldMemOperand(string, SlicedString::kOffsetOffset));
     SmiUntag(offset);
