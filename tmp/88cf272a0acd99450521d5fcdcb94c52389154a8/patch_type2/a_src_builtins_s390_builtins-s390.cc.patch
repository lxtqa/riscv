diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index 562f0ccb073..4e0ee24e79f 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -722,9 +722,8 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   {
     __ LoadTaggedField(
         r2, FieldMemOperand(r6, JSFunction::kSharedFunctionInfoOffset));
-    __ LoadS16(
-        r2,
-        FieldMemOperand(r2, SharedFunctionInfo::kFormalParameterCountOffset));
+    __ LoadS16(r2, FieldMemOperand(
+                       r2, SharedFunctionInfo::kFormalParameterCountOffset));
     // We abuse new.target both to indicate that this is a resume call and to
     // pass in the generator object.  In ordinary calls, new.target is always
     // undefined because generator functions are non-constructable.
@@ -888,9 +887,8 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // If this is the outermost JS call, set js_entry_sp value.
   Label non_outermost_js;
-  ExternalReference js_entry_sp =
-      ExternalReference::Create(IsolateAddressId::kJSEntrySPAddress,
-                                masm->isolate());
+  ExternalReference js_entry_sp = ExternalReference::Create(
+      IsolateAddressId::kJSEntrySPAddress, masm->isolate());
   __ Move(r7, js_entry_sp);
   __ LoadAndTestP(scrach, MemOperand(r7));
   __ bne(&non_outermost_js, Label::kNear);
@@ -1456,7 +1454,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(
 
   // Increment invocation count for the function.
   __ LoadS32(r1, FieldMemOperand(feedback_vector,
-                               FeedbackVector::kInvocationCountOffset));
+                                 FeedbackVector::kInvocationCountOffset));
   __ AddS64(r1, r1, Operand(1));
   __ StoreU32(r1, FieldMemOperand(feedback_vector,
                                   FeedbackVector::kInvocationCountOffset));
@@ -1489,7 +1487,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(
   {
     // Load frame size (word) from the BytecodeArray object.
     __ LoadU32(r4, FieldMemOperand(kInterpreterBytecodeArrayRegister,
-                                  BytecodeArray::kFrameSizeOffset));
+                                   BytecodeArray::kFrameSizeOffset));
 
     // Do a stack check to ensure we don't go over the limit.
     __ SubS64(r8, sp, r4);
@@ -1514,9 +1512,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(
   // If the bytecode array has a valid incoming new target or generator object
   // register, initialize it with incoming value which was passed in r5.
   Label no_incoming_new_target_or_generator_register;
-  __ LoadS32(r8, FieldMemOperand(
-                   kInterpreterBytecodeArrayRegister,
-                   BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));
+  __ LoadS32(r8,
+             FieldMemOperand(
+                 kInterpreterBytecodeArrayRegister,
+                 BytecodeArray::kIncomingNewTargetOrGeneratorRegisterOffset));
   __ CmpS64(r8, Operand::Zero());
   __ beq(&no_incoming_new_target_or_generator_register);
   __ ShiftLeftU64(r8, r8, Operand(kSystemPointerSizeLog2));
@@ -2249,8 +2248,7 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     Label ok, fail;
     __ AssertNotSmi(r4);
     __ LoadTaggedField(scratch, FieldMemOperand(r4, HeapObject::kMapOffset));
-    __ LoadS16(scratch,
-                     FieldMemOperand(scratch, Map::kInstanceTypeOffset));
+    __ LoadS16(scratch, FieldMemOperand(scratch, Map::kInstanceTypeOffset));
     __ CmpS64(scratch, Operand(FIXED_ARRAY_TYPE));
     __ beq(&ok);
     __ CmpS64(scratch, Operand(FIXED_DOUBLE_ARRAY_TYPE));
@@ -3153,9 +3151,9 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   argument_offset += 2 * kSystemPointerSize;
 
   __ LoadU32(scratch_high,
-            MemOperand(sp, argument_offset + Register::kExponentOffset));
+             MemOperand(sp, argument_offset + Register::kExponentOffset));
   __ LoadU32(scratch_low,
-            MemOperand(sp, argument_offset + Register::kMantissaOffset));
+             MemOperand(sp, argument_offset + Register::kMantissaOffset));
 
   __ ExtractBitMask(scratch, scratch_high, HeapNumber::kExponentMask);
   // Load scratch with exponent - 1. This is faster than loading
@@ -3267,52 +3265,62 @@ static void CallApiFunctionAndReturn(MacroAssembler* masm,
   // r6 - next_address->kNextOffset
   // r7 - next_address->kLimitOffset
   // r8 - next_address->kLevelOffset
-  __ Move(r9, next_address);
-  __ LoadU64(r6, MemOperand(r9, kNextOffset));
-  __ LoadU64(r7, MemOperand(r9, kLimitOffset));
-  __ LoadU32(r8, MemOperand(r9, kLevelOffset));
-  __ AddS64(r8, Operand(1));
-  __ StoreU32(r8, MemOperand(r9, kLevelOffset));
+  {
+    ASM_CODE_COMMENT_STRING(masm,
+                            "Allocate HandleScope in callee-save registers.");
+    __ Move(r9, next_address);
+    __ LoadU64(r6, MemOperand(r9, kNextOffset));
+    __ LoadU64(r7, MemOperand(r9, kLimitOffset));
+    __ LoadU32(r8, MemOperand(r9, kLevelOffset));
+    __ AddS64(r8, Operand(1));
+    __ StoreU32(r8, MemOperand(r9, kLevelOffset));
+  }
 
   Label profiler_enabled, done_api_call;
+  __ RecordComment("Check if profiler is enabled");
   __ LoadU8(scratch,
             __ ExternalReferenceAsOperand(
                 ExternalReference::is_profiling_address(isolate), scratch));
   __ CmpS64(scratch, Operand::Zero());
   __ bne(&profiler_enabled, Label::kNear);
 #ifdef V8_RUNTIME_CALL_STATS
+  __ RecordComment("Check if RCS is enabled");
   __ Move(scratch, ExternalReference::address_of_runtime_stats_flag());
   __ LoadU32(scratch, MemOperand(scratch, 0));
   __ CmpS64(scratch, Operand::Zero());
   __ bne(&profiler_enabled, Label::kNear);
 #endif  // V8_RUNTIME_CALL_STATS
 
-  // Call the api function directly.
+  __ RecordComment("Call the api function directly.");
   __ StoreReturnAddressAndCall(function_address);
   __ bind(&done_api_call);
 
   Label promote_scheduled_exception;
   Label delete_allocated_handles;
   Label leave_exit_frame;
-  Label return_value_loaded;
 
-  // load value from ReturnValue
+  __ RecordComment("Load the value from ReturnValue");
+  Register return_value = r2;
   __ LoadU64(r2, return_value_operand);
-  __ bind(&return_value_loaded);
-  // No more valid handles (the result handle was the last one). Restore
-  // previous handle scope.
-  __ StoreU64(r6, MemOperand(r9, kNextOffset));
-  if (v8_flags.debug_code) {
-    __ LoadU32(r3, MemOperand(r9, kLevelOffset));
-    __ CmpS64(r3, r8);
-    __ Check(eq, AbortReason::kUnexpectedLevelAfterReturnFromApiCall);
+
+  {
+    ASM_CODE_COMMENT_STRING(
+        masm,
+        "No more valid handles (the result handle was the last one)."
+        "Restore previous handle scope.");
+    __ StoreU64(r6, MemOperand(r9, kNextOffset));
+    if (v8_flags.debug_code) {
+      __ LoadU32(r3, MemOperand(r9, kLevelOffset));
+      __ CmpS64(r3, r8);
+      __ Check(eq, AbortReason::kUnexpectedLevelAfterReturnFromApiCall);
+    }
+    __ SubS64(r8, Operand(1));
+    __ StoreU32(r8, MemOperand(r9, kLevelOffset));
+    __ CmpS64(r7, MemOperand(r9, kLimitOffset));
+    __ bne(&delete_allocated_handles, Label::kNear);
   }
-  __ SubS64(r8, Operand(1));
-  __ StoreU32(r8, MemOperand(r9, kLevelOffset));
-  __ CmpS64(r7, MemOperand(r9, kLimitOffset));
-  __ bne(&delete_allocated_handles, Label::kNear);
 
-  // Leave the API exit frame.
+  __ RecordComment("Leave the API exit frame.");
   __ bind(&leave_exit_frame);
   // LeaveExitFrame expects unwind space to be in a register.
   if (stack_space_operand == nullptr) {
@@ -3325,40 +3333,66 @@ static void CallApiFunctionAndReturn(MacroAssembler* masm,
   __ LeaveExitFrame(r6, stack_space_operand != nullptr);
 
   // Check if the function scheduled an exception.
-  __ Move(r7, ExternalReference::scheduled_exception_address(isolate));
-  __ LoadU64(r7, MemOperand(r7));
-  __ CompareRoot(r7, RootIndex::kTheHoleValue);
-  __ bne(&promote_scheduled_exception, Label::kNear);
+  {
+    ASM_CODE_COMMENT_STRING(masm,
+                            "Check if the function scheduled an exception.");
+    __ Move(r7, ExternalReference::scheduled_exception_address(isolate));
+    __ LoadU64(r7, MemOperand(r7));
+    __ CompareRoot(r7, RootIndex::kTheHoleValue);
+    __ bne(&promote_scheduled_exception, Label::kNear);
+  }
+
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Convert return value");
+    Label finish_return;
+    __ CompareRoot(return_value, RootIndex::kTheHoleValue);
+    __ bne(&finish_return);
+    __ LoadRoot(return_value, RootIndex::kUndefinedValue);
+    __ bind(&finish_return);
+  }
+
+  {
+    Register map = r6;
+    Register tmp = r7;
+    __ AssertJSAny(return_value, map, tmp,
+                   AbortReason::kAPICallReturnedInvalidObject);
+  }
 
   __ b(r14);
 
-  // Call the api function via thunk wrapper.
-  __ bind(&profiler_enabled);
-  // Additional parameter is the address of the actual callback.
-  __ Move(r5, function_address);
-  __ Move(ip, thunk_ref);
-  __ StoreReturnAddressAndCall(ip);
-  __ b(&done_api_call);
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Call the api function via thunk wrapper.");
+    __ bind(&profiler_enabled);
+    // Additional parameter is the address of the actual callback.
+    __ Move(r5, function_address);
+    __ Move(ip, thunk_ref);
+    __ StoreReturnAddressAndCall(ip);
+    __ b(&done_api_call);
+  }
 
-  // Re-throw by promoting a scheduled exception.
+  __ RecordComment("Re-throw by promoting a scheduled exception.");
   __ bind(&promote_scheduled_exception);
   __ TailCallRuntime(Runtime::kPromoteScheduledException);
 
   // HandleScope limit has changed. Delete allocated extensions.
-  __ bind(&delete_allocated_handles);
-  __ StoreU64(r7, MemOperand(r9, kLimitOffset));
-  __ mov(r6, r2);
-  __ PrepareCallCFunction(1, r7);
-  __ Move(r2, ExternalReference::isolate_address(isolate));
-  __ CallCFunction(ExternalReference::delete_handle_scope_extensions(), 1);
-  __ mov(r2, r6);
-  __ b(&leave_exit_frame, Label::kNear);
+  {
+    ASM_CODE_COMMENT_STRING(
+        masm, "HandleScope limit has changed. Delete allocated extensions.");
+    __ bind(&delete_allocated_handles);
+    __ StoreU64(r7, MemOperand(r9, kLimitOffset));
+    __ mov(r6, r2);
+    __ PrepareCallCFunction(1, r7);
+    __ Move(r2, ExternalReference::isolate_address(isolate));
+    __ CallCFunction(ExternalReference::delete_handle_scope_extensions(), 1);
+    __ mov(r2, r6);
+    __ b(&leave_exit_frame, Label::kNear);
+  }
 }
 
 MemOperand ExitFrameStackSlotOperand(int offset) {
-  static constexpr int kFrameOffset = 1 * kSystemPointerSize;
+  static constexpr int kSPOffset = 1 * kSystemPointerSize;
   return MemOperand(sp, (kStackFrameExtraParamSlot * kSystemPointerSize) +
-                            offset + kFrameOffset);
+                            offset + kSPOffset);
 }
 
 MemOperand ExitFrameCallerStackSlotOperand(int index) {
@@ -3396,42 +3430,47 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   static_assert(FCA::kNewTargetIndex == 5);
   static_assert(FCA::kDataIndex == 4);
   static_assert(FCA::kReturnValueIndex == 3);
-  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kUnusedIndex == 2);
   static_assert(FCA::kIsolateIndex == 1);
   static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
   // Target state:
-  //   sp[0 * kSystemPointerSize]: kHolder   <= FCI::implicit_args_
-  //   sp[1 * kSystemPointerSize]: kIsolate
-  //   sp[2 * kSystemPointerSize]: undefined (kReturnValueDefaultValue)
-  //   sp[3 * kSystemPointerSize]: undefined (kReturnValue)
-  //   sp[4 * kSystemPointerSize]: kData
-  //   sp[5 * kSystemPointerSize]: undefined (kNewTarget)
+  //   sp[1 * kSystemPointerSize]: kHolder   <= FCA::implicit_args_
+  //   sp[2 * kSystemPointerSize]: kIsolate
+  //   sp[3 * kSystemPointerSize]: Smi::zero(padding, unused)
+  //   sp[4 * kSystemPointerSize]: undefined (kReturnValue)
+  //   sp[5 * kSystemPointerSize]: kData
+  //   sp[6 * kSystemPointerSize]: undefined (kNewTarget)
   // Existing state:
-  //   sp[6 * kSystemPointerSize]:            <= FCI:::values_
+  //   sp[7 * kSystemPointerSize]:            <= FCA:::values_
 
   // Reserve space on the stack.
   __ lay(sp, MemOperand(sp, -(FCA::kArgsLength * kSystemPointerSize)));
 
   // kHolder.
-  __ StoreU64(holder, MemOperand(sp, 0 * kSystemPointerSize));
+  __ StoreU64(holder, MemOperand(sp, FCA::kHolderIndex * kSystemPointerSize));
 
   // kIsolate.
   __ Move(scratch, ExternalReference::isolate_address(masm->isolate()));
-  __ StoreU64(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+  __ StoreU64(scratch, MemOperand(sp, FCA::kIsolateIndex * kSystemPointerSize));
+
+  // kUnused
+  __ Move(scratch, Smi::zero());
+  __ StoreU64(scratch, MemOperand(sp, FCA::kUnusedIndex * kSystemPointerSize));
 
-  // kReturnValueDefaultValue and kReturnValue.
+  // kReturnValue.
   __ LoadRoot(scratch, RootIndex::kUndefinedValue);
-  __ StoreU64(scratch, MemOperand(sp, 2 * kSystemPointerSize));
-  __ StoreU64(scratch, MemOperand(sp, 3 * kSystemPointerSize));
+  __ StoreU64(scratch,
+              MemOperand(sp, FCA::kReturnValueIndex * kSystemPointerSize));
 
   // kData.
-  __ StoreU64(call_data, MemOperand(sp, 4 * kSystemPointerSize));
+  __ StoreU64(call_data, MemOperand(sp, FCA::kDataIndex * kSystemPointerSize));
 
   // kNewTarget.
-  __ StoreU64(scratch, MemOperand(sp, 5 * kSystemPointerSize));
+  __ StoreU64(scratch,
+              MemOperand(sp, FCA::kNewTargetIndex * kSystemPointerSize));
 
   // Keep a pointer to kHolder (= implicit_args) in a scratch register.
   // We use it below to set up the FunctionCallbackInfo object.
@@ -3445,8 +3484,9 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   //    [0] space for DirectCEntryStub's LR save
   //    [1-3] FunctionCallbackInfo
   //    [4] number of bytes to drop from the stack after returning
-  static constexpr int kSlotsToDropSize = 1 * kSystemPointerSize;
+  static constexpr int kSlotsToDropOnStackSize = 1 * kSystemPointerSize;
   static constexpr int kApiStackSpace = 5;
+  // (FCI::kSize + kSlotsToDropOnStackSize) / kSystemPointerSize;
   static_assert(FCI::kImplicitArgsOffset == 0);
   static_assert(FCI::kValuesOffset == 1 * kSystemPointerSize);
   static_assert(FCI::kLengthOffset == 2 * kSystemPointerSize);
@@ -3454,30 +3494,33 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   FrameScope frame_scope(masm, StackFrame::MANUAL);
   __ EnterExitFrame(kApiStackSpace, StackFrame::EXIT);
 
-  // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).
-  // Arguments are after the return address (pushed by EnterExitFrame()).
-  __ StoreU64(scratch, ExitFrameStackSlotOperand(FCI::kImplicitArgsOffset));
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Initialize FunctionCallbackInfo");
+    // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).
+    // Arguments are after the return address (pushed by EnterExitFrame()).
+    __ StoreU64(scratch, ExitFrameStackSlotOperand(FCI::kImplicitArgsOffset));
 
-  // FunctionCallbackInfo::values_ (points at the first varargs argument passed
-  // on the stack).
-  __ AddS64(scratch, scratch,
-            Operand(FCA::kArgsLengthWithReceiver * kSystemPointerSize));
-  __ StoreU64(scratch, ExitFrameStackSlotOperand(FCI::kValuesOffset));
+    // FunctionCallbackInfo::values_ (points at the first varargs argument
+    // passed on the stack).
+    __ AddS64(scratch, scratch,
+              Operand(FCA::kArgsLengthWithReceiver * kSystemPointerSize));
+    __ StoreU64(scratch, ExitFrameStackSlotOperand(FCI::kValuesOffset));
 
-  // FunctionCallbackInfo::length_.
-  __ StoreU32(argc, ExitFrameStackSlotOperand(FCI::kLengthOffset));
+    // FunctionCallbackInfo::length_.
+    __ StoreU32(argc, ExitFrameStackSlotOperand(FCI::kLengthOffset));
+  }
 
   // We also store the number of bytes to drop from the stack after returning
   // from the API function here.
   MemOperand stack_space_operand =
-      ExitFrameStackSlotOperand(FCI::kLengthOffset + kSlotsToDropSize);
+      ExitFrameStackSlotOperand(FCI::kLengthOffset + kSlotsToDropOnStackSize);
   __ mov(scratch,
          Operand((FCA::kArgsLength + 1 /* receiver */) * kSystemPointerSize));
   __ ShiftLeftU64(r1, argc, Operand(kSystemPointerSizeLog2));
   __ AddS64(scratch, r1);
   __ StoreU64(scratch, stack_space_operand);
 
-  // v8::InvocationCallback's argument.
+  __ RecordComment("v8::InvocationCallback's argument.");
   __ lay(r2,
          MemOperand(sp, (kStackFrameExtraParamSlot + 1) * kSystemPointerSize));
 
@@ -3502,42 +3545,52 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   static_assert(PCA::kShouldThrowOnErrorIndex == 0);
   static_assert(PCA::kHolderIndex == 1);
   static_assert(PCA::kIsolateIndex == 2);
-  static_assert(PCA::kReturnValueDefaultValueIndex == 3);
+  static_assert(PCA::kUnusedIndex == 3);
   static_assert(PCA::kReturnValueIndex == 4);
   static_assert(PCA::kDataIndex == 5);
   static_assert(PCA::kThisIndex == 6);
   static_assert(PCA::kArgsLength == 7);
 
+  // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
+  // Target state:
+  //   sp[1 * kSystemPointerSize]: name
+  //   sp[2 * kSystemPointerSize]: kShouldThrowOnErrorIndex   <= PCI:args_
+  //   sp[3 * kSystemPointerSize]: kHolderIndex
+  //   sp[4 * kSystemPointerSize]: kIsolateIndex
+  //   sp[5 * kSystemPointerSize]: kUnusedIndex
+  //   sp[6 * kSystemPointerSize]: kReturnValueIndex
+  //   sp[7 * kSystemPointerSize]: kDataIndex
+  //   sp[8 * kSystemPointerSize]: kThisIndex / receiver
+
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
   Register callback = ApiGetterDescriptor::CallbackRegister();
   Register scratch = r6;
-  DCHECK(!AreAliased(receiver, holder, callback, scratch));
+  Register smi_zero = r7;
+  DCHECK(!AreAliased(receiver, holder, callback, scratch, smi_zero));
 
   Register api_function_address = r4;
 
-  __ push(receiver);
-  // Push data from AccessorInfo.
   __ LoadTaggedField(scratch,
                      FieldMemOperand(callback, AccessorInfo::kDataOffset), r1);
-  __ push(scratch);
+  __ Push(receiver, scratch);
   __ LoadRoot(scratch, RootIndex::kUndefinedValue);
-  __ Push(scratch, scratch);
+  __ Move(smi_zero, Smi::zero());
+  __ Push(scratch, smi_zero);  // kReturnValueIndex, kUnusedIndex
   __ Move(scratch, ExternalReference::isolate_address(masm->isolate()));
   __ Push(scratch, holder);
-  __ Push(Smi::zero());  // should_throw_on_error -> false
   __ LoadTaggedField(scratch,
                      FieldMemOperand(callback, AccessorInfo::kNameOffset), r1);
-  __ push(scratch);
-
-  // v8::PropertyCallbackInfo::args_ array and name handle.
-  constexpr int kNameHandleStackSize = 1;
-  constexpr int kStackUnwindSpace = PCA::kArgsLength + kNameHandleStackSize;
+  __ Push(smi_zero, scratch);
+  __ RecordComment(
+      "Load address of v8::PropertyAccessorInfo::args_ array and name handle.");
 
-  // Load address of v8::PropertyAccessorInfo::args_ array and name handle.
-  __ mov(r2, sp);                                    // r2 = Handle<Name>
+  __ mov(r2, sp);                                      // r2 = Handle<Name>
   __ AddS64(r3, r2, Operand(1 * kSystemPointerSize));  // r3 = v8::PCI::args_
 
+  constexpr int kNameOnStackSize = 1;
+  constexpr int kStackUnwindSpace = PCA::kArgsLength + kNameOnStackSize;
+
   // If ABI passes Handles (pointer-sized struct) in a register:
   //
   // Create 2 extra slots on stack:
@@ -3568,20 +3621,23 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
     __ AddS64(r2, sp, Operand(arg0Slot * kSystemPointerSize));
   }
 
+  __ RecordComment("Create v8::PropertyCallbackInfo object on the stack.");
   // Create v8::PropertyCallbackInfo object on the stack and initialize
   // it's args_ field.
   __ StoreU64(r3, MemOperand(sp, accessorInfoSlot * kSystemPointerSize));
   __ AddS64(r3, sp, Operand(accessorInfoSlot * kSystemPointerSize));
   // r3 = v8::PropertyCallbackInfo&
 
+  __ RecordComment("Load api_function_address");
   ExternalReference thunk_ref =
       ExternalReference::invoke_accessor_getter_callback();
   __ LoadU64(
       api_function_address,
       FieldMemOperand(callback, AccessorInfo::kMaybeRedirectedGetterOffset));
   MemOperand return_value_operand = ExitFrameCallerStackSlotOperand(
-      PCA::kReturnValueIndex + kNameHandleStackSize);
+      PCA::kReturnValueIndex + kNameOnStackSize);
   MemOperand* const kUseStackSpaceConstant = nullptr;
+
   CallApiFunctionAndReturn(masm, api_function_address, thunk_ref,
                            kStackUnwindSpace, kUseStackSpaceConstant,
                            return_value_operand);
@@ -3834,7 +3890,7 @@ void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   static_assert(D::kParameterCount == 1);
 
   __ LoadU64(kContextRegister,
-         MemOperand(fp, BaselineFrameConstants::kContextOffset));
+             MemOperand(fp, BaselineFrameConstants::kContextOffset));
   OnStackReplacement(masm, OsrSourceTier::kBaseline,
                      D::MaybeTargetCodeRegister());
 }
