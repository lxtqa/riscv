diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index 935897e0fb..db146edf96 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -182,7 +182,7 @@ class V8_EXPORT_PRIVATE Operand {
     // introduces additional padding between them and the union, increasing the
     // size unnecessarily.
     bool is_label_operand = true;
-    byte rex = 0;  // REX prefix, always zero for label operands.
+    uint8_t rex = 0;  // REX prefix, always zero for label operands.
 
     int8_t addend;  // Used for rip + offset + addend operands.
     Label* label;
@@ -190,10 +190,10 @@ class V8_EXPORT_PRIVATE Operand {
 
   struct MemoryOperand {
     bool is_label_operand = false;
-    byte rex = 0;  // REX prefix.
+    uint8_t rex = 0;  // REX prefix.
 
     // Register (1 byte) + SIB (0 or 1 byte) + displacement (0, 1, or 4 byte).
-    byte buf[6] = {0};
+    uint8_t buf[6] = {0};
     // Number of bytes of buf in use.
     // We must keep {len} and {buf} together for the compiler to elide the
     // stack canary protection code.
@@ -283,7 +283,7 @@ class V8_EXPORT_PRIVATE Operand {
     return memory_.is_label_operand;
   }
 
-  V8_INLINE constexpr byte rex() const {
+  V8_INLINE constexpr uint8_t rex() const {
     // Since both fields are in the common initial sequence of {label_} and
     // {memory_}, the access is valid regardless of the active union member.
     // Label operands always have a REX prefix of zero.
@@ -548,18 +548,18 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   static constexpr int kSpecialTargetSize = 4;  // 32-bit displacement.
 
   // One byte opcode for test eax,0xXXXXXXXX.
-  static constexpr byte kTestEaxByte = 0xA9;
+  static constexpr uint8_t kTestEaxByte = 0xA9;
   // One byte opcode for test al, 0xXX.
-  static constexpr byte kTestAlByte = 0xA8;
+  static constexpr uint8_t kTestAlByte = 0xA8;
   // One byte opcode for nop.
-  static constexpr byte kNopByte = 0x90;
+  static constexpr uint8_t kNopByte = 0x90;
 
   // One byte prefix for a short conditional jump.
-  static constexpr byte kJccShortPrefix = 0x70;
-  static constexpr byte kJncShortOpcode = kJccShortPrefix | not_carry;
-  static constexpr byte kJcShortOpcode = kJccShortPrefix | carry;
-  static constexpr byte kJnzShortOpcode = kJccShortPrefix | not_zero;
-  static constexpr byte kJzShortOpcode = kJccShortPrefix | zero;
+  static constexpr uint8_t kJccShortPrefix = 0x70;
+  static constexpr uint8_t kJncShortOpcode = kJccShortPrefix | not_carry;
+  static constexpr uint8_t kJcShortOpcode = kJccShortPrefix | carry;
+  static constexpr uint8_t kJnzShortOpcode = kJccShortPrefix | not_zero;
+  static constexpr uint8_t kJzShortOpcode = kJccShortPrefix | zero;
 
   // VEX prefix encodings.
   enum SIMDPrefix { kNoPrefix = 0x0, k66 = 0x1, kF3 = 0x2, kF2 = 0x3 };
@@ -1007,7 +1007,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void movhps(XMMRegister dst, Operand src);
   void movhps(Operand dst, XMMRegister src);
 
-  void shufps(XMMRegister dst, XMMRegister src, byte imm8);
+  void shufps(XMMRegister dst, XMMRegister src, uint8_t imm8);
 
   void cvttss2si(Register dst, Operand src);
   void cvttss2si(Register dst, XMMRegister src);
@@ -1016,18 +1016,19 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 
   void movmskps(Register dst, XMMRegister src);
 
-  void vinstr(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2,
+  void vinstr(uint8_t op, XMMRegister dst, XMMRegister src1, XMMRegister src2,
               SIMDPrefix pp, LeadingOpcode m, VexW w, CpuFeature feature = AVX);
-  void vinstr(byte op, XMMRegister dst, XMMRegister src1, Operand src2,
+  void vinstr(uint8_t op, XMMRegister dst, XMMRegister src1, Operand src2,
               SIMDPrefix pp, LeadingOpcode m, VexW w, CpuFeature feature = AVX);
 
   template <typename Reg1, typename Reg2, typename Op>
-  void vinstr(byte op, Reg1 dst, Reg2 src1, Op src2, SIMDPrefix pp,
+  void vinstr(uint8_t op, Reg1 dst, Reg2 src1, Op src2, SIMDPrefix pp,
               LeadingOpcode m, VexW w, CpuFeature feature = AVX2);
 
   // SSE instructions
-  void sse_instr(XMMRegister dst, XMMRegister src, byte escape, byte opcode);
-  void sse_instr(XMMRegister dst, Operand src, byte escape, byte opcode);
+  void sse_instr(XMMRegister dst, XMMRegister src, uint8_t escape,
+                 uint8_t opcode);
+  void sse_instr(XMMRegister dst, Operand src, uint8_t escape, uint8_t opcode);
 #define DECLARE_SSE_INSTRUCTION(instruction, escape, opcode) \
   void instruction(XMMRegister dst, XMMRegister src) {       \
     sse_instr(dst, src, 0x##escape, 0x##opcode);             \
@@ -1041,10 +1042,10 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 #undef DECLARE_SSE_INSTRUCTION
 
   // SSE instructions with prefix and SSE2 instructions
-  void sse2_instr(XMMRegister dst, XMMRegister src, byte prefix, byte escape,
-                  byte opcode);
-  void sse2_instr(XMMRegister dst, Operand src, byte prefix, byte escape,
-                  byte opcode);
+  void sse2_instr(XMMRegister dst, XMMRegister src, uint8_t prefix,
+                  uint8_t escape, uint8_t opcode);
+  void sse2_instr(XMMRegister dst, Operand src, uint8_t prefix, uint8_t escape,
+                  uint8_t opcode);
 #define DECLARE_SSE2_INSTRUCTION(instruction, prefix, escape, opcode) \
   void instruction(XMMRegister dst, XMMRegister src) {                \
     sse2_instr(dst, src, 0x##prefix, 0x##escape, 0x##opcode);         \
@@ -1060,15 +1061,15 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   SSE2_UNOP_INSTRUCTION_LIST(DECLARE_SSE2_INSTRUCTION)
 #undef DECLARE_SSE2_INSTRUCTION
 
-  void sse2_instr(XMMRegister reg, byte imm8, byte prefix, byte escape,
-                  byte opcode, int extension) {
+  void sse2_instr(XMMRegister reg, uint8_t imm8, uint8_t prefix, uint8_t escape,
+                  uint8_t opcode, int extension) {
     XMMRegister ext_reg = XMMRegister::from_code(extension);
     sse2_instr(ext_reg, reg, prefix, escape, opcode);
     emit(imm8);
   }
 
 #define DECLARE_SSE2_SHIFT_IMM(instruction, prefix, escape, opcode, extension) \
-  void instruction(XMMRegister reg, byte imm8) {                               \
+  void instruction(XMMRegister reg, uint8_t imm8) {                            \
     sse2_instr(reg, imm8, 0x##prefix, 0x##escape, 0x##opcode, 0x##extension);  \
   }
   SSE2_INSTRUCTION_LIST_SHIFT_IMM(DECLARE_SSE2_SHIFT_IMM)
@@ -1156,10 +1157,10 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void movshdup(XMMRegister dst, XMMRegister src);
 
   // SSSE3
-  void ssse3_instr(XMMRegister dst, XMMRegister src, byte prefix, byte escape1,
-                   byte escape2, byte opcode);
-  void ssse3_instr(XMMRegister dst, Operand src, byte prefix, byte escape1,
-                   byte escape2, byte opcode);
+  void ssse3_instr(XMMRegister dst, XMMRegister src, uint8_t prefix,
+                   uint8_t escape1, uint8_t escape2, uint8_t opcode);
+  void ssse3_instr(XMMRegister dst, Operand src, uint8_t prefix,
+                   uint8_t escape1, uint8_t escape2, uint8_t opcode);
 
 #define DECLARE_SSSE3_INSTRUCTION(instruction, prefix, escape1, escape2,     \
                                   opcode)                                    \
@@ -1175,16 +1176,18 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 #undef DECLARE_SSSE3_INSTRUCTION
 
   // SSE4
-  void sse4_instr(Register dst, XMMRegister src, byte prefix, byte escape1,
-                  byte escape2, byte opcode, int8_t imm8);
-  void sse4_instr(Operand dst, XMMRegister src, byte prefix, byte escape1,
-                  byte escape2, byte opcode, int8_t imm8);
-  void sse4_instr(XMMRegister dst, Register src, byte prefix, byte escape1,
-                  byte escape2, byte opcode, int8_t imm8);
-  void sse4_instr(XMMRegister dst, XMMRegister src, byte prefix, byte escape1,
-                  byte escape2, byte opcode);
-  void sse4_instr(XMMRegister dst, Operand src, byte prefix, byte escape1,
-                  byte escape2, byte opcode);
+  void sse4_instr(Register dst, XMMRegister src, uint8_t prefix,
+                  uint8_t escape1, uint8_t escape2, uint8_t opcode,
+                  int8_t imm8);
+  void sse4_instr(Operand dst, XMMRegister src, uint8_t prefix, uint8_t escape1,
+                  uint8_t escape2, uint8_t opcode, int8_t imm8);
+  void sse4_instr(XMMRegister dst, Register src, uint8_t prefix,
+                  uint8_t escape1, uint8_t escape2, uint8_t opcode,
+                  int8_t imm8);
+  void sse4_instr(XMMRegister dst, XMMRegister src, uint8_t prefix,
+                  uint8_t escape1, uint8_t escape2, uint8_t opcode);
+  void sse4_instr(XMMRegister dst, Operand src, uint8_t prefix, uint8_t escape1,
+                  uint8_t escape2, uint8_t opcode);
 #define DECLARE_SSE4_INSTRUCTION(instruction, prefix, escape1, escape2,     \
                                  opcode)                                    \
   void instruction(XMMRegister dst, XMMRegister src) {                      \
@@ -1216,10 +1219,10 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 #undef DECLARE_SSE4_EXTRACT_INSTRUCTION
 
   // SSE4.2
-  void sse4_2_instr(XMMRegister dst, XMMRegister src, byte prefix, byte escape1,
-                    byte escape2, byte opcode);
-  void sse4_2_instr(XMMRegister dst, Operand src, byte prefix, byte escape1,
-                    byte escape2, byte opcode);
+  void sse4_2_instr(XMMRegister dst, XMMRegister src, uint8_t prefix,
+                    uint8_t escape1, uint8_t escape2, uint8_t opcode);
+  void sse4_2_instr(XMMRegister dst, Operand src, uint8_t prefix,
+                    uint8_t escape1, uint8_t escape2, uint8_t opcode);
 #define DECLARE_SSE4_2_INSTRUCTION(instruction, prefix, escape1, escape2,     \
                                    opcode)                                    \
   void instruction(XMMRegister dst, XMMRegister src) {                        \
@@ -1403,8 +1406,8 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void pinsrw(XMMRegister dst, Operand src, uint8_t imm8);
 
   // SSE 4.1 instruction
-  void insertps(XMMRegister dst, XMMRegister src, byte imm8);
-  void insertps(XMMRegister dst, Operand src, byte imm8);
+  void insertps(XMMRegister dst, XMMRegister src, uint8_t imm8);
+  void insertps(XMMRegister dst, Operand src, uint8_t imm8);
   void pextrq(Register dst, XMMRegister src, int8_t imm8);
   void pinsrb(XMMRegister dst, Register src, uint8_t imm8);
   void pinsrb(XMMRegister dst, Operand src, uint8_t imm8);
@@ -1471,9 +1474,10 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void vbroadcastss(YMMRegister dst, Operand src);
   void vbroadcastss(YMMRegister dst, XMMRegister src);
 
-  void fma_instr(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2,
-                 VectorLength l, SIMDPrefix pp, LeadingOpcode m, VexW w);
-  void fma_instr(byte op, XMMRegister dst, XMMRegister src1, Operand src2,
+  void fma_instr(uint8_t op, XMMRegister dst, XMMRegister src1,
+                 XMMRegister src2, VectorLength l, SIMDPrefix pp,
+                 LeadingOpcode m, VexW w);
+  void fma_instr(uint8_t op, XMMRegister dst, XMMRegister src1, Operand src2,
                  VectorLength l, SIMDPrefix pp, LeadingOpcode m, VexW w);
 
 #define FMA(instr, length, prefix, escape1, escape2, extension, opcode) \
@@ -1574,13 +1578,13 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 #undef AVX_3
 
 #define AVX_SSE2_SHIFT_IMM(instr, prefix, escape, opcode, extension)   \
-  void v##instr(XMMRegister dst, XMMRegister src, byte imm8) {         \
+  void v##instr(XMMRegister dst, XMMRegister src, uint8_t imm8) {      \
     XMMRegister ext_reg = XMMRegister::from_code(extension);           \
     vinstr(0x##opcode, ext_reg, dst, src, k##prefix, k##escape, kWIG); \
     emit(imm8);                                                        \
   }                                                                    \
                                                                        \
-  void v##instr(YMMRegister dst, YMMRegister src, byte imm8) {         \
+  void v##instr(YMMRegister dst, YMMRegister src, uint8_t imm8) {      \
     YMMRegister ext_reg = YMMRegister::from_code(extension);           \
     vinstr(0x##opcode, ext_reg, dst, src, k##prefix, k##escape, kWIG); \
     emit(imm8);                                                        \
@@ -1679,42 +1683,42 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void vroundss(XMMRegister dst, XMMRegister src1, XMMRegister src2,
                 RoundingMode mode) {
     vinstr(0x0a, dst, src1, src2, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundss(XMMRegister dst, XMMRegister src1, Operand src2,
                 RoundingMode mode) {
     vinstr(0x0a, dst, src1, src2, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundsd(XMMRegister dst, XMMRegister src1, XMMRegister src2,
                 RoundingMode mode) {
     vinstr(0x0b, dst, src1, src2, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundsd(XMMRegister dst, XMMRegister src1, Operand src2,
                 RoundingMode mode) {
     vinstr(0x0b, dst, src1, src2, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundps(XMMRegister dst, XMMRegister src, RoundingMode mode) {
     vinstr(0x08, dst, xmm0, src, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundps(YMMRegister dst, YMMRegister src, RoundingMode mode) {
     vinstr(0x08, dst, ymm0, src, k66, k0F3A, kWIG, AVX);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundpd(XMMRegister dst, XMMRegister src, RoundingMode mode) {
     vinstr(0x09, dst, xmm0, src, k66, k0F3A, kWIG);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
   void vroundpd(YMMRegister dst, YMMRegister src, RoundingMode mode) {
     vinstr(0x09, dst, ymm0, src, k66, k0F3A, kWIG, AVX);
-    emit(static_cast<byte>(mode) | 0x8);  // Mask precision exception.
+    emit(static_cast<uint8_t>(mode) | 0x8);  // Mask precision exception.
   }
 
   template <typename Reg, typename Op>
-  void vsd(byte op, Reg dst, Reg src1, Op src2) {
+  void vsd(uint8_t op, Reg dst, Reg src1, Op src2) {
     vinstr(op, dst, src1, src2, kF2, k0F, kWIG, AVX);
   }
 
@@ -1725,13 +1729,15 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void vmovss(Operand dst, XMMRegister src) { vss(0x11, src, xmm0, dst); }
   void vucomiss(XMMRegister dst, XMMRegister src);
   void vucomiss(XMMRegister dst, Operand src);
-  void vss(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
-  void vss(byte op, XMMRegister dst, XMMRegister src1, Operand src2);
+  void vss(uint8_t op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
+  void vss(uint8_t op, XMMRegister dst, XMMRegister src1, Operand src2);
 
-  void vshufps(XMMRegister dst, XMMRegister src1, XMMRegister src2, byte imm8) {
+  void vshufps(XMMRegister dst, XMMRegister src1, XMMRegister src2,
+               uint8_t imm8) {
     vps(0xC6, dst, src1, src2, imm8);
   }
-  void vshufps(YMMRegister dst, YMMRegister src1, YMMRegister src2, byte imm8) {
+  void vshufps(YMMRegister dst, YMMRegister src1, YMMRegister src2,
+               uint8_t imm8) {
     vps(0xC6, dst, src1, src2, imm8);
   }
 
@@ -1837,11 +1843,12 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
     vinstr(0xF0, dst, xmm0, src, kF2, k0F, kWIG);
   }
   void vinsertps(XMMRegister dst, XMMRegister src1, XMMRegister src2,
-                 byte imm8) {
+                 uint8_t imm8) {
     vinstr(0x21, dst, src1, src2, k66, k0F3A, kWIG);
     emit(imm8);
   }
-  void vinsertps(XMMRegister dst, XMMRegister src1, Operand src2, byte imm8) {
+  void vinsertps(XMMRegister dst, XMMRegister src1, Operand src2,
+                 uint8_t imm8) {
     vinstr(0x21, dst, src1, src2, k66, k0F3A, kWIG);
     emit(imm8);
   }
@@ -1974,20 +1981,20 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
     emit(imm8);
   }
 
-  void vps(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
-  void vps(byte op, YMMRegister dst, YMMRegister src1, YMMRegister src2);
-  void vps(byte op, XMMRegister dst, XMMRegister src1, Operand src2);
-  void vps(byte op, YMMRegister dst, YMMRegister src1, Operand src2);
-  void vps(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2,
-           byte imm8);
-  void vps(byte op, YMMRegister dst, YMMRegister src1, YMMRegister src2,
-           byte imm8);
-  void vpd(byte op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
-  void vpd(byte op, YMMRegister dst, YMMRegister src1, YMMRegister src2);
-  void vpd(byte op, XMMRegister dst, YMMRegister src1, YMMRegister src2);
-  void vpd(byte op, XMMRegister dst, XMMRegister src1, Operand src2);
-  void vpd(byte op, YMMRegister dst, YMMRegister src1, Operand src2);
-  void vpd(byte op, XMMRegister dst, YMMRegister src1, Operand src2);
+  void vps(uint8_t op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
+  void vps(uint8_t op, YMMRegister dst, YMMRegister src1, YMMRegister src2);
+  void vps(uint8_t op, XMMRegister dst, XMMRegister src1, Operand src2);
+  void vps(uint8_t op, YMMRegister dst, YMMRegister src1, Operand src2);
+  void vps(uint8_t op, XMMRegister dst, XMMRegister src1, XMMRegister src2,
+           uint8_t imm8);
+  void vps(uint8_t op, YMMRegister dst, YMMRegister src1, YMMRegister src2,
+           uint8_t imm8);
+  void vpd(uint8_t op, XMMRegister dst, XMMRegister src1, XMMRegister src2);
+  void vpd(uint8_t op, YMMRegister dst, YMMRegister src1, YMMRegister src2);
+  void vpd(uint8_t op, XMMRegister dst, YMMRegister src1, YMMRegister src2);
+  void vpd(uint8_t op, XMMRegister dst, XMMRegister src1, Operand src2);
+  void vpd(uint8_t op, YMMRegister dst, YMMRegister src1, Operand src2);
+  void vpd(uint8_t op, XMMRegister dst, YMMRegister src1, Operand src2);
 
   // AVX2 instructions
 #define AVX2_INSTRUCTION(instr, prefix, escape1, escape2, opcode)           \
@@ -2135,10 +2142,10 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void shrxl(Register dst, Operand src1, Register src2) {
     bmi2l(kF2, 0xf7, dst, src2, src1);
   }
-  void rorxq(Register dst, Register src, byte imm8);
-  void rorxq(Register dst, Operand src, byte imm8);
-  void rorxl(Register dst, Register src, byte imm8);
-  void rorxl(Register dst, Operand src, byte imm8);
+  void rorxq(Register dst, Register src, uint8_t imm8);
+  void rorxq(Register dst, Operand src, uint8_t imm8);
+  void rorxl(Register dst, Register src, uint8_t imm8);
+  void rorxl(Register dst, Operand src, uint8_t imm8);
 
   void mfence();
   void lfence();
@@ -2185,8 +2192,8 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // Avoid overflows for displacements etc.
   static constexpr int kMaximalBufferSize = 512 * MB;
 
-  byte byte_at(int pos) { return buffer_start_[pos]; }
-  void set_byte_at(int pos, byte value) { buffer_start_[pos] = value; }
+  uint8_t byte_at(int pos) { return buffer_start_[pos]; }
+  void set_byte_at(int pos, uint8_t value) { buffer_start_[pos] = value; }
 
 #if defined(V8_OS_WIN_X64)
   win64_unwindinfo::BuiltinUnwindInfo GetUnwindInfo() const;
@@ -2419,23 +2426,23 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // AND, OR, XOR, or CMP.  The encodings of these operations are all
   // similar, differing just in the opcode or in the reg field of the
   // ModR/M byte.
-  void arithmetic_op_8(byte opcode, Register reg, Register rm_reg);
-  void arithmetic_op_8(byte opcode, Register reg, Operand rm_reg);
-  void arithmetic_op_16(byte opcode, Register reg, Register rm_reg);
-  void arithmetic_op_16(byte opcode, Register reg, Operand rm_reg);
+  void arithmetic_op_8(uint8_t opcode, Register reg, Register rm_reg);
+  void arithmetic_op_8(uint8_t opcode, Register reg, Operand rm_reg);
+  void arithmetic_op_16(uint8_t opcode, Register reg, Register rm_reg);
+  void arithmetic_op_16(uint8_t opcode, Register reg, Operand rm_reg);
   // Operate on operands/registers with pointer size, 32-bit or 64-bit size.
-  void arithmetic_op(byte opcode, Register reg, Register rm_reg, int size);
-  void arithmetic_op(byte opcode, Register reg, Operand rm_reg, int size);
+  void arithmetic_op(uint8_t opcode, Register reg, Register rm_reg, int size);
+  void arithmetic_op(uint8_t opcode, Register reg, Operand rm_reg, int size);
   // Operate on a byte in memory or register.
-  void immediate_arithmetic_op_8(byte subcode, Register dst, Immediate src);
-  void immediate_arithmetic_op_8(byte subcode, Operand dst, Immediate src);
+  void immediate_arithmetic_op_8(uint8_t subcode, Register dst, Immediate src);
+  void immediate_arithmetic_op_8(uint8_t subcode, Operand dst, Immediate src);
   // Operate on a word in memory or register.
-  void immediate_arithmetic_op_16(byte subcode, Register dst, Immediate src);
-  void immediate_arithmetic_op_16(byte subcode, Operand dst, Immediate src);
+  void immediate_arithmetic_op_16(uint8_t subcode, Register dst, Immediate src);
+  void immediate_arithmetic_op_16(uint8_t subcode, Operand dst, Immediate src);
   // Operate on operands/registers with pointer size, 32-bit or 64-bit size.
-  void immediate_arithmetic_op(byte subcode, Register dst, Immediate src,
+  void immediate_arithmetic_op(uint8_t subcode, Register dst, Immediate src,
                                int size);
-  void immediate_arithmetic_op(byte subcode, Operand dst, Immediate src,
+  void immediate_arithmetic_op(uint8_t subcode, Operand dst, Immediate src,
                                int size);
 
   // Emit machine code for a shift operation.
@@ -2645,14 +2652,18 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   }
 
   // Most BMI instructions are similar.
-  void bmi1q(byte op, Register reg, Register vreg, Register rm);
-  void bmi1q(byte op, Register reg, Register vreg, Operand rm);
-  void bmi1l(byte op, Register reg, Register vreg, Register rm);
-  void bmi1l(byte op, Register reg, Register vreg, Operand rm);
-  void bmi2q(SIMDPrefix pp, byte op, Register reg, Register vreg, Register rm);
-  void bmi2q(SIMDPrefix pp, byte op, Register reg, Register vreg, Operand rm);
-  void bmi2l(SIMDPrefix pp, byte op, Register reg, Register vreg, Register rm);
-  void bmi2l(SIMDPrefix pp, byte op, Register reg, Register vreg, Operand rm);
+  void bmi1q(uint8_t op, Register reg, Register vreg, Register rm);
+  void bmi1q(uint8_t op, Register reg, Register vreg, Operand rm);
+  void bmi1l(uint8_t op, Register reg, Register vreg, Register rm);
+  void bmi1l(uint8_t op, Register reg, Register vreg, Operand rm);
+  void bmi2q(SIMDPrefix pp, uint8_t op, Register reg, Register vreg,
+             Register rm);
+  void bmi2q(SIMDPrefix pp, uint8_t op, Register reg, Register vreg,
+             Operand rm);
+  void bmi2l(SIMDPrefix pp, uint8_t op, Register reg, Register vreg,
+             Register rm);
+  void bmi2l(SIMDPrefix pp, uint8_t op, Register reg, Register vreg,
+             Operand rm);
 
   // record the position of jmp/jcc instruction
   void record_farjmp_position(Label* L, int pos);
@@ -2686,26 +2697,34 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 #endif
 };
 
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void Assembler::vinstr(byte op, YMMRegister dst, YMMRegister src1,
-                       YMMRegister src2, SIMDPrefix pp,
-                       LeadingOpcode m, VexW w, CpuFeature feature);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void Assembler::vinstr(byte op, YMMRegister dst, XMMRegister src1,
-                       XMMRegister src2, SIMDPrefix pp,
-                       LeadingOpcode m, VexW w, CpuFeature feature);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void Assembler::vinstr(byte op, YMMRegister dst, YMMRegister src1,
-                       Operand src2, SIMDPrefix pp, LeadingOpcode m,
-                       VexW w, CpuFeature feature);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void Assembler::vinstr(byte op, YMMRegister dst, YMMRegister src1,
-                       XMMRegister src2, SIMDPrefix pp,
-                       LeadingOpcode m, VexW w, CpuFeature feature);
-extern template EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-void Assembler::vinstr(byte op, YMMRegister dst, XMMRegister src1,
-                       Operand src2, SIMDPrefix pp, LeadingOpcode m,
-                       VexW w, CpuFeature feature);
+extern template EXPORT_TEMPLATE_DECLARE(
+    V8_EXPORT_PRIVATE) void Assembler::vinstr(uint8_t op, YMMRegister dst,
+                                              YMMRegister src1,
+                                              YMMRegister src2, SIMDPrefix pp,
+                                              LeadingOpcode m, VexW w,
+                                              CpuFeature feature);
+extern template EXPORT_TEMPLATE_DECLARE(
+    V8_EXPORT_PRIVATE) void Assembler::vinstr(uint8_t op, YMMRegister dst,
+                                              XMMRegister src1,
+                                              XMMRegister src2, SIMDPrefix pp,
+                                              LeadingOpcode m, VexW w,
+                                              CpuFeature feature);
+extern template EXPORT_TEMPLATE_DECLARE(
+    V8_EXPORT_PRIVATE) void Assembler::vinstr(uint8_t op, YMMRegister dst,
+                                              YMMRegister src1, Operand src2,
+                                              SIMDPrefix pp, LeadingOpcode m,
+                                              VexW w, CpuFeature feature);
+extern template EXPORT_TEMPLATE_DECLARE(
+    V8_EXPORT_PRIVATE) void Assembler::vinstr(uint8_t op, YMMRegister dst,
+                                              YMMRegister src1,
+                                              XMMRegister src2, SIMDPrefix pp,
+                                              LeadingOpcode m, VexW w,
+                                              CpuFeature feature);
+extern template EXPORT_TEMPLATE_DECLARE(
+    V8_EXPORT_PRIVATE) void Assembler::vinstr(uint8_t op, YMMRegister dst,
+                                              XMMRegister src1, Operand src2,
+                                              SIMDPrefix pp, LeadingOpcode m,
+                                              VexW w, CpuFeature feature);
 
 // Helper class that ensures that there is enough space for generating
 // instructions and relocation information.  The constructor makes
