From 942a67ca0180ddf163aaed3cf7ca44cb70c5de8a Mon Sep 17 00:00:00 2001
From: Lu Yahan <yahan@iscas.ac.cn>
Date: Thu, 4 Aug 2022 09:56:44 +0800
Subject: [PATCH] Reland "[riscv32] Add RISCV32 backend"

This is a reland of commit 491de34bcc84e33468c7b299027b1f6b59ec2d15

co-authors: Ji Qiu <qiuji@iscas.ac.cn>
            Alvise De Faveri Tron <elvisilde@gmail.com>
            Usman Zain <uszain@gmail.com>
            Zheng Quan <vitalyankh@gmail.com>

Original change's description:
> [riscv32] Add RISCV32 backend
>
> This very large changeset adds support for RISCV32.
>
> Bug: v8:13025
> Change-Id: Ieacc857131e6620f0fcfd7daa88a0f8d77056aa9
> Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3736732
> Reviewed-by: Michael Achenbach <machenbach@chromium.org>
> Commit-Queue: Yahan Lu <yahan@iscas.ac.cn>
> Reviewed-by: ji qiu <qiuji@iscas.ac.cn>
> Reviewed-by: Andreas Haas <ahaas@chromium.org>
> Reviewed-by: Hannes Payer <hpayer@chromium.org>
> Reviewed-by: Nico Hartmann <nicohartmann@chromium.org>
> Cr-Commit-Position: refs/heads/main@{#82053}

Bug: v8:13025
Change-Id: I220fae4b8e2679bdc111724e08817b079b373bd5
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3807124
Commit-Queue: Yahan Lu <yahan@iscas.ac.cn>
Reviewed-by: Michael Achenbach <machenbach@chromium.org>
Reviewed-by: ji qiu <qiuji@iscas.ac.cn>
Reviewed-by: Hannes Payer <hpayer@chromium.org>
Reviewed-by: Andreas Haas <ahaas@chromium.org>
Cr-Commit-Position: refs/heads/main@{#82198}
---
 BUILD.gn                                      |  182 +-
 OWNERS                                        |    2 +-
 gni/snapshot_toolchain.gni                    |    3 +-
 include/v8-unwinder-state.h                   |    2 +-
 include/v8config.h                            |   15 +-
 src/base/compiler-specific.h                  |    8 +-
 src/base/platform/platform-posix.cc           |    6 +
 src/baseline/baseline-assembler-inl.h         |    4 +-
 src/baseline/baseline-compiler.cc             |    4 +-
 .../baseline-assembler-riscv-inl.h}           |   30 +-
 .../baseline-compiler-riscv-inl.h}            |   30 +-
 .../builtins-sharedarraybuffer-gen.cc         |    8 +-
 .../builtins-riscv.cc}                        |  552 +--
 src/codegen/assembler-arch.h                  |    4 +-
 src/codegen/assembler-inl.h                   |    4 +-
 src/codegen/constant-pool.cc                  |    4 +-
 src/codegen/constant-pool.h                   |    6 +-
 src/codegen/constants-arch.h                  |    4 +-
 src/codegen/cpu-features.h                    |    4 +
 src/codegen/external-reference.cc             |    2 +-
 src/codegen/interface-descriptors-inl.h       |    8 +-
 src/codegen/macro-assembler.h                 |    6 +-
 src/codegen/register-arch.h                   |    4 +-
 src/codegen/register-configuration.cc         |   15 +-
 src/codegen/reglist.h                         |    4 +-
 src/codegen/reloc-info.cc                     |    9 +-
 src/codegen/reloc-info.h                      |    3 +-
 .../assembler-riscv-inl.h}                    |   69 +-
 src/codegen/riscv/assembler-riscv.cc          | 1927 ++++++++
 src/codegen/riscv/assembler-riscv.h           |  848 ++++
 src/codegen/riscv/base-assembler-riscv.cc     |  492 ++
 src/codegen/riscv/base-assembler-riscv.h      |  192 +
 .../base-constants-riscv.cc}                  |  109 +-
 .../base-constants-riscv.h}                   |  995 +---
 src/codegen/riscv/base-riscv-i.cc             |  330 ++
 src/codegen/riscv/base-riscv-i.h              |  212 +
 src/codegen/riscv/constant-riscv-a.h          |   43 +
 src/codegen/riscv/constant-riscv-c.h          |   62 +
 src/codegen/riscv/constant-riscv-d.h          |   55 +
 src/codegen/riscv/constant-riscv-f.h          |   51 +
 src/codegen/riscv/constant-riscv-i.h          |   73 +
 src/codegen/riscv/constant-riscv-m.h          |   34 +
 src/codegen/riscv/constant-riscv-v.h          |  493 ++
 src/codegen/riscv/constant-riscv-zicsr.h      |   30 +
 src/codegen/riscv/constant-riscv-zifencei.h   |   15 +
 src/codegen/riscv/constants-riscv.h           |   20 +
 .../cpu-riscv64.cc => riscv/cpu-riscv.cc}     |    4 -
 src/codegen/riscv/extension-riscv-a.cc        |  121 +
 src/codegen/riscv/extension-riscv-a.h         |   45 +
 src/codegen/riscv/extension-riscv-c.cc        |  276 ++
 src/codegen/riscv/extension-riscv-c.h         |   76 +
 src/codegen/riscv/extension-riscv-d.cc        |  165 +
 src/codegen/riscv/extension-riscv-d.h         |   67 +
 src/codegen/riscv/extension-riscv-f.cc        |  156 +
 src/codegen/riscv/extension-riscv-f.h         |   65 +
 src/codegen/riscv/extension-riscv-m.cc        |   66 +
 src/codegen/riscv/extension-riscv-m.h         |   36 +
 src/codegen/riscv/extension-riscv-v.cc        |  889 ++++
 src/codegen/riscv/extension-riscv-v.h         |  485 ++
 src/codegen/riscv/extension-riscv-zicsr.cc    |   44 +
 src/codegen/riscv/extension-riscv-zicsr.h     |   56 +
 src/codegen/riscv/extension-riscv-zifencei.cc |   16 +
 src/codegen/riscv/extension-riscv-zifencei.h  |   19 +
 .../interface-descriptors-riscv-inl.h}        |   10 +-
 .../macro-assembler-riscv.cc}                 | 1402 +++++-
 .../macro-assembler-riscv.h}                  |  370 +-
 .../register-riscv.h}                         |    9 +-
 .../reglist-riscv.h}                          |    7 +-
 src/codegen/riscv64/assembler-riscv64.cc      | 4096 -----------------
 src/codegen/riscv64/assembler-riscv64.h       | 1830 --------
 src/common/globals.h                          |    5 +-
 src/compiler/backend/instruction-codes.h      |    4 +-
 src/compiler/backend/instruction-selector.cc  |   19 +-
 .../code-generator-riscv.cc}                  |  592 ++-
 .../backend/riscv/instruction-codes-riscv.h   |  462 ++
 .../instruction-scheduler-riscv.cc}           |  253 +-
 .../riscv/instruction-selector-riscv.h        | 1239 +++++
 .../riscv/instruction-selector-riscv32.cc     | 1325 ++++++
 .../instruction-selector-riscv64.cc           | 1404 +-----
 .../riscv64/instruction-codes-riscv64.h       |  432 --
 src/compiler/c-linkage.cc                     |    2 +-
 .../deoptimizer-riscv.cc}                     |    0
 src/diagnostics/perf-jit.h                    |    2 +-
 .../disasm-riscv.cc}                          |   41 +-
 .../unwinder-riscv.cc}                        |    0
 src/execution/frame-constants.h               |    4 +-
 src/execution/frames.h                        |    2 +-
 .../frame-constants-riscv.cc}                 |    7 +-
 .../frame-constants-riscv.h}                  |    6 +-
 .../simulator-riscv.cc}                       |  833 ++--
 .../simulator-riscv.h}                        |  226 +-
 src/execution/simulator-base.h                |    3 +-
 src/execution/simulator.h                     |    4 +-
 src/flags/flag-definitions.h                  |    2 +-
 .../{riscv64 => riscv}/push_registers_asm.cc  |   42 +
 src/interpreter/interpreter-assembler.cc      |    3 +-
 src/libsampler/sampler.cc                     |    2 +-
 src/logging/log.cc                            |    2 +
 src/objects/code.cc                           |    9 +-
 src/objects/code.h                            |    2 +
 src/profiler/tick-sample.cc                   |    7 +
 src/regexp/regexp-macro-assembler-arch.h      |    4 +-
 src/regexp/regexp-macro-assembler.h           |    1 +
 src/regexp/regexp.cc                          |    3 +
 .../regexp-macro-assembler-riscv.cc}          |  349 +-
 .../regexp-macro-assembler-riscv.h}           |    8 +-
 src/runtime/runtime-atomics.cc                |    6 +-
 src/snapshot/deserializer.h                   |    4 +-
 src/snapshot/embedded/embedded-data.cc        |    9 +-
 src/wasm/baseline/liftoff-assembler-defs.h    |   12 +
 src/wasm/baseline/liftoff-assembler.h         |    4 +-
 .../liftoff-assembler-riscv.h}                | 1874 +-------
 .../riscv/liftoff-assembler-riscv32.h         | 2088 +++++++++
 .../riscv/liftoff-assembler-riscv64.h         | 1715 +++++++
 src/wasm/jump-table-assembler.cc              |   40 +
 src/wasm/jump-table-assembler.h               |    2 +-
 src/wasm/wasm-linkage.h                       |    2 +-
 src/wasm/wasm-serialization.cc                |    2 +-
 test/cctest/BUILD.gn                          |   10 +-
 test/cctest/cctest-utils.h                    |    2 +-
 test/cctest/cctest.status                     |   12 +-
 test/cctest/compiler/test-run-machops.cc      |    2 +-
 test/cctest/compiler/value-helper.h           |   22 +-
 test/cctest/test-assembler-riscv32.cc         | 2565 +++++++++++
 test/cctest/test-helper-riscv32.cc            |   49 +
 test/cctest/test-helper-riscv32.h             |  337 ++
 test/cctest/test-icache.cc                    |    4 +
 test/cctest/test-macro-assembler-riscv32.cc   | 1324 ++++++
 test/cctest/test-simple-riscv32.cc            |  229 +
 test/cctest/wasm/test-jump-table-assembler.cc |    2 +-
 test/inspector/inspector.status               |   13 +-
 test/message/message.status                   |    4 +
 test/mjsunit/mjsunit.status                   |   41 +-
 test/unittests/BUILD.gn                       |   12 +-
 ...4-unittest.cc => disasm-riscv-unittest.cc} |   25 +-
 ...t.cc => turbo-assembler-riscv-unittest.cc} |    2 +-
 .../instruction-selector-riscv32-unittest.cc  | 1159 +++++
 test/unittests/regexp/regexp-unittest.cc      |    2 +
 test/unittests/unittests.status               |    2 +-
 .../wasm/liftoff-register-unittests.cc        |    4 +-
 test/wasm-spec-tests/wasm-spec-tests.status   |   18 +-
 tools/dev/gm.py                               |    6 +-
 tools/generate-header-include-checks.py       |    2 +-
 tools/testrunner/build_config.py              |    5 +-
 tools/testrunner/local/statusfile.py          |    2 +-
 145 files changed, 24140 insertions(+), 12055 deletions(-)
 rename src/baseline/{riscv64/baseline-assembler-riscv64-inl.h => riscv/baseline-assembler-riscv-inl.h} (97%)
 rename src/baseline/{riscv64/baseline-compiler-riscv64-inl.h => riscv/baseline-compiler-riscv-inl.h} (69%)
 rename src/builtins/{riscv64/builtins-riscv64.cc => riscv/builtins-riscv.cc} (90%)
 rename src/codegen/{riscv64/assembler-riscv64-inl.h => riscv/assembler-riscv-inl.h} (89%)
 create mode 100644 src/codegen/riscv/assembler-riscv.cc
 create mode 100644 src/codegen/riscv/assembler-riscv.h
 create mode 100644 src/codegen/riscv/base-assembler-riscv.cc
 create mode 100644 src/codegen/riscv/base-assembler-riscv.h
 rename src/codegen/{riscv64/constants-riscv64.cc => riscv/base-constants-riscv.cc} (66%)
 rename src/codegen/{riscv64/constants-riscv64.h => riscv/base-constants-riscv.h} (50%)
 create mode 100644 src/codegen/riscv/base-riscv-i.cc
 create mode 100644 src/codegen/riscv/base-riscv-i.h
 create mode 100644 src/codegen/riscv/constant-riscv-a.h
 create mode 100644 src/codegen/riscv/constant-riscv-c.h
 create mode 100644 src/codegen/riscv/constant-riscv-d.h
 create mode 100644 src/codegen/riscv/constant-riscv-f.h
 create mode 100644 src/codegen/riscv/constant-riscv-i.h
 create mode 100644 src/codegen/riscv/constant-riscv-m.h
 create mode 100644 src/codegen/riscv/constant-riscv-v.h
 create mode 100644 src/codegen/riscv/constant-riscv-zicsr.h
 create mode 100644 src/codegen/riscv/constant-riscv-zifencei.h
 create mode 100644 src/codegen/riscv/constants-riscv.h
 rename src/codegen/{riscv64/cpu-riscv64.cc => riscv/cpu-riscv.cc} (93%)
 create mode 100644 src/codegen/riscv/extension-riscv-a.cc
 create mode 100644 src/codegen/riscv/extension-riscv-a.h
 create mode 100644 src/codegen/riscv/extension-riscv-c.cc
 create mode 100644 src/codegen/riscv/extension-riscv-c.h
 create mode 100644 src/codegen/riscv/extension-riscv-d.cc
 create mode 100644 src/codegen/riscv/extension-riscv-d.h
 create mode 100644 src/codegen/riscv/extension-riscv-f.cc
 create mode 100644 src/codegen/riscv/extension-riscv-f.h
 create mode 100644 src/codegen/riscv/extension-riscv-m.cc
 create mode 100644 src/codegen/riscv/extension-riscv-m.h
 create mode 100644 src/codegen/riscv/extension-riscv-v.cc
 create mode 100644 src/codegen/riscv/extension-riscv-v.h
 create mode 100644 src/codegen/riscv/extension-riscv-zicsr.cc
 create mode 100644 src/codegen/riscv/extension-riscv-zicsr.h
 create mode 100644 src/codegen/riscv/extension-riscv-zifencei.cc
 create mode 100644 src/codegen/riscv/extension-riscv-zifencei.h
 rename src/codegen/{riscv64/interface-descriptors-riscv64-inl.h => riscv/interface-descriptors-riscv-inl.h} (97%)
 rename src/codegen/{riscv64/macro-assembler-riscv64.cc => riscv/macro-assembler-riscv.cc} (81%)
 rename src/codegen/{riscv64/macro-assembler-riscv64.h => riscv/macro-assembler-riscv.h} (83%)
 rename src/codegen/{riscv64/register-riscv64.h => riscv/register-riscv.h} (98%)
 rename src/codegen/{riscv64/reglist-riscv64.h => riscv/reglist-riscv.h} (92%)
 delete mode 100644 src/codegen/riscv64/assembler-riscv64.cc
 delete mode 100644 src/codegen/riscv64/assembler-riscv64.h
 rename src/compiler/backend/{riscv64/code-generator-riscv64.cc => riscv/code-generator-riscv.cc} (90%)
 create mode 100644 src/compiler/backend/riscv/instruction-codes-riscv.h
 rename src/compiler/backend/{riscv64/instruction-scheduler-riscv64.cc => riscv/instruction-scheduler-riscv.cc} (96%)
 create mode 100644 src/compiler/backend/riscv/instruction-selector-riscv.h
 create mode 100644 src/compiler/backend/riscv/instruction-selector-riscv32.cc
 rename src/compiler/backend/{riscv64 => riscv}/instruction-selector-riscv64.cc (58%)
 delete mode 100644 src/compiler/backend/riscv64/instruction-codes-riscv64.h
 rename src/deoptimizer/{riscv64/deoptimizer-riscv64.cc => riscv/deoptimizer-riscv.cc} (100%)
 rename src/diagnostics/{riscv64/disasm-riscv64.cc => riscv/disasm-riscv.cc} (99%)
 rename src/diagnostics/{riscv64/unwinder-riscv64.cc => riscv/unwinder-riscv.cc} (100%)
 rename src/execution/{riscv64/frame-constants-riscv64.cc => riscv/frame-constants-riscv.cc} (81%)
 rename src/execution/{riscv64/frame-constants-riscv64.h => riscv/frame-constants-riscv.h} (94%)
 rename src/execution/{riscv64/simulator-riscv64.cc => riscv/simulator-riscv.cc} (91%)
 rename src/execution/{riscv64/simulator-riscv64.h => riscv/simulator-riscv.h} (82%)
 rename src/heap/base/asm/{riscv64 => riscv}/push_registers_asm.cc (54%)
 rename src/regexp/{riscv64/regexp-macro-assembler-riscv64.cc => riscv/regexp-macro-assembler-riscv.cc} (82%)
 rename src/regexp/{riscv64/regexp-macro-assembler-riscv64.h => riscv/regexp-macro-assembler-riscv.h} (97%)
 rename src/wasm/baseline/{riscv64/liftoff-assembler-riscv64.h => riscv/liftoff-assembler-riscv.h} (56%)
 create mode 100644 src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
 create mode 100644 src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
 create mode 100644 test/cctest/test-assembler-riscv32.cc
 create mode 100644 test/cctest/test-helper-riscv32.cc
 create mode 100644 test/cctest/test-helper-riscv32.h
 create mode 100644 test/cctest/test-macro-assembler-riscv32.cc
 create mode 100644 test/cctest/test-simple-riscv32.cc
 rename test/unittests/assembler/{disasm-riscv64-unittest.cc => disasm-riscv-unittest.cc} (99%)
 rename test/unittests/assembler/{turbo-assembler-riscv64-unittest.cc => turbo-assembler-riscv-unittest.cc} (97%)
 create mode 100644 test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc

diff --git a/BUILD.gn b/BUILD.gn
index b1d706a145b..0c1a72dfd2a 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -654,7 +654,7 @@ config("internal_config") {
     defines += [ "BUILDING_V8_SHARED" ]
   }
 
-  if (v8_current_cpu == "riscv64") {
+  if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs = [ "atomic" ]
   }
 }
@@ -726,7 +726,7 @@ config("external_config") {
     defines += [ "USING_V8_SHARED" ]
   }
 
-  if (current_cpu == "riscv64") {
+  if (current_cpu == "riscv64" || current_cpu == "riscv32") {
     libs = [ "atomic" ]
   }
 }
@@ -1243,14 +1243,18 @@ config("toolchain") {
   if (v8_current_cpu == "riscv64") {
     defines += [ "V8_TARGET_ARCH_RISCV64" ]
     defines += [ "__riscv_xlen=64" ]
-
-    #FIXME: Temporarily use MIPS macro for the building.
     defines += [ "CAN_USE_FPU_INSTRUCTIONS" ]
     if (target_is_simulator) {
       defines += [ "CAN_USE_RVV_INSTRUCTIONS" ]
     }
   }
 
+  if (v8_current_cpu == "riscv32") {
+    defines += [ "V8_TARGET_ARCH_RISCV32" ]
+    defines += [ "__riscv_xlen=32" ]
+    defines += [ "CAN_USE_FPU_INSTRUCTIONS" ]
+  }
+
   if (v8_current_cpu == "x86") {
     defines += [ "V8_TARGET_ARCH_IA32" ]
     if (is_win) {
@@ -2543,7 +2547,12 @@ v8_source_set("v8_initializers") {
   } else if (v8_current_cpu == "riscv64") {
     sources += [
       ### gcmole(arch:riscv64) ###
-      "src/builtins/riscv64/builtins-riscv64.cc",
+      "src/builtins/riscv/builtins-riscv.cc",
+    ]
+  } else if (v8_current_cpu == "riscv32") {
+    sources += [
+      ### gcmole(arch:riscv32) ###
+      "src/builtins/riscv/builtins-riscv.cc",
     ]
   }
 
@@ -3979,20 +3988,81 @@ v8_header_set("v8_internal_headers") {
     ]
   } else if (v8_current_cpu == "riscv64") {
     sources += [  ### gcmole(arch:riscv64) ###
-      "src/baseline/riscv64/baseline-assembler-riscv64-inl.h",
-      "src/baseline/riscv64/baseline-compiler-riscv64-inl.h",
-      "src/codegen/riscv64/assembler-riscv64-inl.h",
-      "src/codegen/riscv64/assembler-riscv64.h",
-      "src/codegen/riscv64/constants-riscv64.h",
-      "src/codegen/riscv64/macro-assembler-riscv64.h",
-      "src/codegen/riscv64/register-riscv64.h",
-      "src/codegen/riscv64/reglist-riscv64.h",
-      "src/compiler/backend/riscv64/instruction-codes-riscv64.h",
-      "src/execution/riscv64/frame-constants-riscv64.h",
-      "src/execution/riscv64/simulator-riscv64.h",
-      "src/regexp/riscv64/regexp-macro-assembler-riscv64.h",
+      "src/baseline/riscv/baseline-assembler-riscv-inl.h",
+      "src/baseline/riscv/baseline-compiler-riscv-inl.h",
+      "src/codegen/riscv/assembler-riscv-inl.h",
+      "src/codegen/riscv/assembler-riscv-inl.h",
+      "src/codegen/riscv/assembler-riscv.h",
+      "src/codegen/riscv/base-assembler-riscv.h",
+      "src/codegen/riscv/base-constants-riscv.h",
+      "src/codegen/riscv/base-riscv-i.h",
+      "src/codegen/riscv/base-riscv-i.h",
+      "src/codegen/riscv/constant-riscv-a.h",
+      "src/codegen/riscv/constant-riscv-c.h",
+      "src/codegen/riscv/constant-riscv-d.h",
+      "src/codegen/riscv/constant-riscv-f.h",
+      "src/codegen/riscv/constant-riscv-m.h",
+      "src/codegen/riscv/constant-riscv-v.h",
+      "src/codegen/riscv/constant-riscv-zicsr.h",
+      "src/codegen/riscv/constant-riscv-zifencei.h",
+      "src/codegen/riscv/constants-riscv.h",
+      "src/codegen/riscv/extension-riscv-a.h",
+      "src/codegen/riscv/extension-riscv-c.h",
+      "src/codegen/riscv/extension-riscv-d.h",
+      "src/codegen/riscv/extension-riscv-d.h",
+      "src/codegen/riscv/extension-riscv-inl.h",
+      "src/codegen/riscv/extension-riscv-m.h",
+      "src/codegen/riscv/extension-riscv-v.h",
+      "src/codegen/riscv/extension-riscv-zicsr.h",
+      "src/codegen/riscv/extension-riscv-zifencei.h",
+      "src/codegen/riscv/interface-descriptors-riscv-inl.h",
+      "src/codegen/riscv/macro-assembler-riscv.h",
+      "src/codegen/riscv/register-riscv.h",
+      "src/codegen/riscv/reglist-riscv.h",
+      "src/compiler/backend/riscv/instruction-codes-riscv.h",
+      "src/execution/riscv/frame-constants-riscv.h",
+      "src/execution/riscv/simulator-riscv.h",
+      "src/regexp/riscv/regexp-macro-assembler-riscv.h",
       "src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h",
     ]
+  } else if (v8_current_cpu == "riscv32") {
+    sources += [  ### gcmole(arch:riscv32) ###
+      "src/baseline/riscv/baseline-assembler-riscv-inl.h",
+      "src/baseline/riscv/baseline-compiler-riscv-inl.h",
+      "src/codegen/riscv/assembler-riscv.h",
+      "src/codegen/riscv/assembler-riscv32-inl.h",
+      "src/codegen/riscv/base-assembler-riscv.h",
+      "src/codegen/riscv/base-constants-riscv.h",
+      "src/codegen/riscv/base-riscv-i.h",
+      "src/codegen/riscv/constant-riscv-a.h",
+      "src/codegen/riscv/constant-riscv-c.h",
+      "src/codegen/riscv/constant-riscv-d.h",
+      "src/codegen/riscv/constant-riscv-f.h",
+      "src/codegen/riscv/constant-riscv-i.h",
+      "src/codegen/riscv/constant-riscv-m.h",
+      "src/codegen/riscv/constant-riscv-v.h",
+      "src/codegen/riscv/constant-riscv-zicsr.h",
+      "src/codegen/riscv/constant-riscv-zifencei.h",
+      "src/codegen/riscv/constants-riscv.h",
+      "src/codegen/riscv/extension-riscv-a.h",
+      "src/codegen/riscv/extension-riscv-c.h",
+      "src/codegen/riscv/extension-riscv-d.h",
+      "src/codegen/riscv/extension-riscv-f.h",
+      "src/codegen/riscv/extension-riscv-inl.h",
+      "src/codegen/riscv/extension-riscv-m.h",
+      "src/codegen/riscv/extension-riscv-v.h",
+      "src/codegen/riscv/extension-riscv-zicsr.h",
+      "src/codegen/riscv/extension-riscv-zifencei.h",
+      "src/codegen/riscv/interface-descriptors-riscv-inl.h",
+      "src/codegen/riscv/macro-assembler-riscv.h",
+      "src/codegen/riscv/register-riscv.h",
+      "src/codegen/riscv/reglist-riscv.h",
+      "src/compiler/backend/riscv/instruction-codes-riscv.h",
+      "src/execution/riscv/frame-constants-riscv.h",
+      "src/execution/riscv/simulator-riscv.h",
+      "src/regexp/riscv/regexp-macro-assembler-riscv.h",
+      "src/wasm/baseline/riscv32/liftoff-assembler-riscv32.h",
+    ]
   }
 
   public_deps = [
@@ -5022,23 +5092,55 @@ v8_source_set("v8_base_without_compiler") {
     ]
   } else if (v8_current_cpu == "riscv64") {
     sources += [  ### gcmole(arch:riscv64) ###
-      "src/baseline/riscv64/baseline-assembler-riscv64-inl.h",
-      "src/baseline/riscv64/baseline-compiler-riscv64-inl.h",
-      "src/codegen/riscv64/assembler-riscv64-inl.h",
-      "src/codegen/riscv64/assembler-riscv64.cc",
-      "src/codegen/riscv64/constants-riscv64.cc",
-      "src/codegen/riscv64/cpu-riscv64.cc",
-      "src/codegen/riscv64/interface-descriptors-riscv64-inl.h",
-      "src/codegen/riscv64/macro-assembler-riscv64.cc",
-      "src/compiler/backend/riscv64/code-generator-riscv64.cc",
-      "src/compiler/backend/riscv64/instruction-scheduler-riscv64.cc",
-      "src/compiler/backend/riscv64/instruction-selector-riscv64.cc",
-      "src/deoptimizer/riscv64/deoptimizer-riscv64.cc",
-      "src/diagnostics/riscv64/disasm-riscv64.cc",
-      "src/diagnostics/riscv64/unwinder-riscv64.cc",
-      "src/execution/riscv64/frame-constants-riscv64.cc",
-      "src/execution/riscv64/simulator-riscv64.cc",
-      "src/regexp/riscv64/regexp-macro-assembler-riscv64.cc",
+      "src/codegen/riscv/assembler-riscv.cc",
+      "src/codegen/riscv/base-assembler-riscv.cc",
+      "src/codegen/riscv/base-constants-riscv.cc",
+      "src/codegen/riscv/base-riscv-i.cc",
+      "src/codegen/riscv/cpu-riscv.cc",
+      "src/codegen/riscv/extension-riscv-a.cc",
+      "src/codegen/riscv/extension-riscv-c.cc",
+      "src/codegen/riscv/extension-riscv-d.cc",
+      "src/codegen/riscv/extension-riscv-f.cc",
+      "src/codegen/riscv/extension-riscv-m.cc",
+      "src/codegen/riscv/extension-riscv-v.cc",
+      "src/codegen/riscv/extension-riscv-zicsr.cc",
+      "src/codegen/riscv/extension-riscv-zifencei.cc",
+      "src/codegen/riscv/macro-assembler-riscv.cc",
+      "src/compiler/backend/riscv/code-generator-riscv.cc",
+      "src/compiler/backend/riscv/instruction-scheduler-riscv.cc",
+      "src/compiler/backend/riscv/instruction-selector-riscv64.cc",
+      "src/deoptimizer/riscv/deoptimizer-riscv.cc",
+      "src/diagnostics/riscv/disasm-riscv.cc",
+      "src/diagnostics/riscv/unwinder-riscv.cc",
+      "src/execution/riscv/frame-constants-riscv.cc",
+      "src/execution/riscv/simulator-riscv.cc",
+      "src/regexp/riscv/regexp-macro-assembler-riscv.cc",
+    ]
+  } else if (v8_current_cpu == "riscv32") {
+    sources += [  ### gcmole(arch:riscv32) ###
+      "src/codegen/riscv/assembler-riscv.cc",
+      "src/codegen/riscv/base-assembler-riscv.cc",
+      "src/codegen/riscv/base-constants-riscv.cc",
+      "src/codegen/riscv/base-riscv-i.cc",
+      "src/codegen/riscv/cpu-riscv.cc",
+      "src/codegen/riscv/extension-riscv-a.cc",
+      "src/codegen/riscv/extension-riscv-c.cc",
+      "src/codegen/riscv/extension-riscv-d.cc",
+      "src/codegen/riscv/extension-riscv-f.cc",
+      "src/codegen/riscv/extension-riscv-m.cc",
+      "src/codegen/riscv/extension-riscv-v.cc",
+      "src/codegen/riscv/extension-riscv-zicsr.cc",
+      "src/codegen/riscv/extension-riscv-zifencei.cc",
+      "src/codegen/riscv/macro-assembler-riscv.cc",
+      "src/compiler/backend/riscv/code-generator-riscv.cc",
+      "src/compiler/backend/riscv/instruction-scheduler-riscv.cc",
+      "src/compiler/backend/riscv/instruction-selector-riscv32.cc",
+      "src/deoptimizer/riscv/deoptimizer-riscv.cc",
+      "src/diagnostics/riscv/disasm-riscv.cc",
+      "src/diagnostics/riscv/unwinder-riscv.cc",
+      "src/execution/riscv/frame-constants-riscv.cc",
+      "src/execution/riscv/simulator-riscv.cc",
+      "src/regexp/riscv/regexp-macro-assembler-riscv.cc",
     ]
   }
 
@@ -5127,7 +5229,7 @@ v8_source_set("v8_base_without_compiler") {
       v8_current_cpu == "mips64" || v8_current_cpu == "mips64el" ||
       v8_current_cpu == "ppc" || v8_current_cpu == "ppc64" ||
       v8_current_cpu == "s390" || v8_current_cpu == "s390x" ||
-      v8_current_cpu == "riscv64") {
+      v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs += [ "atomic" ]
   }
 
@@ -5527,7 +5629,7 @@ v8_component("v8_libbase") {
     sources += [ "src/base/ubsan.cc" ]
   }
 
-  if (v8_current_cpu == "riscv64") {
+  if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs += [ "atomic" ]
   }
 
@@ -5616,7 +5718,7 @@ v8_component("v8_libplatform") {
     }
   }
 
-  if (v8_current_cpu == "riscv64") {
+  if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs = [ "atomic" ]
   }
 }
@@ -5708,8 +5810,8 @@ v8_source_set("v8_heap_base") {
       sources += [ "src/heap/base/asm/mips64/push_registers_asm.cc" ]
     } else if (current_cpu == "loong64") {
       sources += [ "src/heap/base/asm/loong64/push_registers_asm.cc" ]
-    } else if (current_cpu == "riscv64") {
-      sources += [ "src/heap/base/asm/riscv64/push_registers_asm.cc" ]
+    } else if (current_cpu == "riscv64" || current_cpu == "riscv32") {
+      sources += [ "src/heap/base/asm/riscv/push_registers_asm.cc" ]
     }
   } else if (is_win) {
     if (current_cpu == "x64") {
@@ -6532,7 +6634,7 @@ if (want_v8_shell) {
 v8_executable("cppgc_hello_world") {
   sources = [ "samples/cppgc/hello-world.cc" ]
 
-  if (v8_current_cpu == "riscv64") {
+  if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
     libs = [ "atomic" ]
   }
 
diff --git a/OWNERS b/OWNERS
index a5e1106d153..db579fb0e6a 100644
--- a/OWNERS
+++ b/OWNERS
@@ -27,5 +27,5 @@ per-file ...-loong64*=file:LOONG_OWNERS
 per-file ...-mips*=file:MIPS_OWNERS
 per-file ...-mips64*=file:MIPS_OWNERS
 per-file ...-ppc*=file:PPC_OWNERS
-per-file ...-riscv64*=file:RISCV_OWNERS
+per-file ...-riscv*=file:RISCV_OWNERS
 per-file ...-s390*=file:S390_OWNERS
diff --git a/gni/snapshot_toolchain.gni b/gni/snapshot_toolchain.gni
index 39b196521c3..637dcd39c45 100644
--- a/gni/snapshot_toolchain.gni
+++ b/gni/snapshot_toolchain.gni
@@ -96,7 +96,8 @@ if (v8_snapshot_toolchain == "") {
       } else {
         _cpus = "x64_v8_${v8_current_cpu}"
       }
-    } else if (v8_current_cpu == "arm" || v8_current_cpu == "mipsel") {
+    } else if (v8_current_cpu == "arm" || v8_current_cpu == "mipsel" ||
+               v8_current_cpu == "riscv32") {
       _cpus = "x86_v8_${v8_current_cpu}"
     } else {
       # This branch should not be reached; leave _cpus blank so the assert
diff --git a/include/v8-unwinder-state.h b/include/v8-unwinder-state.h
index a30f7325f48..4154905d135 100644
--- a/include/v8-unwinder-state.h
+++ b/include/v8-unwinder-state.h
@@ -20,7 +20,7 @@ struct CalleeSavedRegisters {
 #elif V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_ARM64 ||   \
     V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC ||    \
     V8_TARGET_ARCH_PPC64 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_S390 || \
-    V8_TARGET_ARCH_LOONG64
+    V8_TARGET_ARCH_LOONG64 || V8_TARGET_ARCH_RISCV32
 struct CalleeSavedRegisters {};
 #else
 #error Target architecture was not detected as supported by v8
diff --git a/include/v8config.h b/include/v8config.h
index bbd0540847c..85bbfce01df 100644
--- a/include/v8config.h
+++ b/include/v8config.h
@@ -674,6 +674,9 @@ V8 shared library set USING_V8_SHARED.
 #if __riscv_xlen == 64
 #define V8_HOST_ARCH_RISCV64 1
 #define V8_HOST_ARCH_64_BIT 1
+#elif __riscv_xlen == 32
+#define V8_HOST_ARCH_RISCV32 1
+#define V8_HOST_ARCH_32_BIT 1
 #else
 #error "Cannot detect Riscv's bitwidth"
 #endif
@@ -689,7 +692,8 @@ V8 shared library set USING_V8_SHARED.
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM &&      \
     !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_MIPS && !V8_TARGET_ARCH_MIPS64 && \
     !V8_TARGET_ARCH_PPC && !V8_TARGET_ARCH_PPC64 && !V8_TARGET_ARCH_S390 &&    \
-    !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_LOONG64
+    !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_LOONG64 &&                      \
+    !V8_TARGET_ARCH_RISCV32
 #if defined(_M_X64) || defined(__x86_64__)
 #define V8_TARGET_ARCH_X64 1
 #elif defined(_M_IX86) || defined(__i386__)
@@ -714,6 +718,8 @@ V8 shared library set USING_V8_SHARED.
 #elif defined(__riscv) || defined(__riscv__)
 #if __riscv_xlen == 64
 #define V8_TARGET_ARCH_RISCV64 1
+#elif __riscv_xlen == 32
+#define V8_TARGET_ARCH_RISCV32 1
 #endif
 #else
 #error Target architecture was not detected as supported by v8
@@ -753,6 +759,8 @@ V8 shared library set USING_V8_SHARED.
 #endif
 #elif V8_TARGET_ARCH_RISCV64
 #define V8_TARGET_ARCH_64_BIT 1
+#elif V8_TARGET_ARCH_RISCV32
+#define V8_TARGET_ARCH_32_BIT 1
 #else
 #error Unknown target architecture pointer size
 #endif
@@ -784,6 +792,9 @@ V8 shared library set USING_V8_SHARED.
 #if (V8_TARGET_ARCH_RISCV64 && !(V8_HOST_ARCH_X64 || V8_HOST_ARCH_RISCV64))
 #error Target architecture riscv64 is only supported on riscv64 and x64 host
 #endif
+#if (V8_TARGET_ARCH_RISCV32 && !(V8_HOST_ARCH_IA32 || V8_HOST_ARCH_RISCV32))
+#error Target architecture riscv32 is only supported on riscv32 and ia32 host
+#endif
 #if (V8_TARGET_ARCH_LOONG64 && !(V8_HOST_ARCH_X64 || V8_HOST_ARCH_LOONG64))
 #error Target architecture loong64 is only supported on loong64 and x64 host
 #endif
@@ -823,7 +834,7 @@ V8 shared library set USING_V8_SHARED.
 #else
 #define V8_TARGET_BIG_ENDIAN 1
 #endif
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 #define V8_TARGET_LITTLE_ENDIAN 1
 #elif defined(__BYTE_ORDER__)
 #if __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__
diff --git a/src/base/compiler-specific.h b/src/base/compiler-specific.h
index 0c37e56afae..c1dd63e86d4 100644
--- a/src/base/compiler-specific.h
+++ b/src/base/compiler-specific.h
@@ -98,10 +98,10 @@
 // do not support adding noexcept to default members.
 // Disabled on MSVC because constructors of standard containers are not noexcept
 // there.
-#if ((!defined(V8_CC_GNU) && !defined(V8_CC_MSVC) &&                      \
-      !defined(V8_TARGET_ARCH_MIPS) && !defined(V8_TARGET_ARCH_MIPS64) && \
-      !defined(V8_TARGET_ARCH_PPC) && !defined(V8_TARGET_ARCH_PPC64) &&   \
-      !defined(V8_TARGET_ARCH_RISCV64)) ||                                \
+#if ((!defined(V8_CC_GNU) && !defined(V8_CC_MSVC) &&                           \
+      !defined(V8_TARGET_ARCH_MIPS) && !defined(V8_TARGET_ARCH_MIPS64) &&      \
+      !defined(V8_TARGET_ARCH_PPC) && !defined(V8_TARGET_ARCH_PPC64) &&        \
+      !defined(V8_TARGET_ARCH_RISCV64) && !defined(V8_TARGET_ARCH_RISCV32)) || \
      (defined(__clang__) && __cplusplus > 201300L))
 #define V8_NOEXCEPT noexcept
 #else
diff --git a/src/base/platform/platform-posix.cc b/src/base/platform/platform-posix.cc
index 6455f8757df..90b83597a7b 100644
--- a/src/base/platform/platform-posix.cc
+++ b/src/base/platform/platform-posix.cc
@@ -354,6 +354,10 @@ void* OS::GetRandomMmapAddr() {
   // TODO(RISCV): We need more information from the kernel to correctly mask
   // this address for RISC-V. https://github.com/v8-riscv/v8/issues/375
   raw_addr &= uint64_t{0xFFFFFF0000};
+#elif V8_TARGET_ARCH_RISCV32
+  // TODO(RISCV): We need more information from the kernel to correctly mask
+  // this address for RISC-V. https://github.com/v8-riscv/v8/issues/375
+  raw_addr &= 0x3FFFF000;
 #elif V8_TARGET_ARCH_LOONG64
   // 42 bits of virtual addressing. Truncate to 40 bits to allow kernel chance
   // to fulfill request.
@@ -685,6 +689,8 @@ void OS::DebugBreak() {
   asm volatile(".word 0x0001");
 #elif V8_HOST_ARCH_RISCV64
   asm("ebreak");
+#elif V8_HOST_ARCH_RISCV32
+  asm("ebreak");
 #else
 #error Unsupported host architecture.
 #endif
diff --git a/src/baseline/baseline-assembler-inl.h b/src/baseline/baseline-assembler-inl.h
index fc1f544f68c..5dcce90f378 100644
--- a/src/baseline/baseline-assembler-inl.h
+++ b/src/baseline/baseline-assembler-inl.h
@@ -32,8 +32,8 @@
 #include "src/baseline/ppc/baseline-assembler-ppc-inl.h"
 #elif V8_TARGET_ARCH_S390X
 #include "src/baseline/s390/baseline-assembler-s390-inl.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/baseline/riscv64/baseline-assembler-riscv64-inl.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/baseline/riscv/baseline-assembler-riscv-inl.h"
 #elif V8_TARGET_ARCH_MIPS64
 #include "src/baseline/mips64/baseline-assembler-mips64-inl.h"
 #elif V8_TARGET_ARCH_MIPS
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index 244f808c4ac..81a98046075 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -48,7 +48,9 @@
 #elif V8_TARGET_ARCH_S390X
 #include "src/baseline/s390/baseline-compiler-s390-inl.h"
 #elif V8_TARGET_ARCH_RISCV64
-#include "src/baseline/riscv64/baseline-compiler-riscv64-inl.h"
+#include "src/baseline/riscv/baseline-compiler-riscv-inl.h"
+#elif V8_TARGET_ARCH_RISCV32
+#include "src/baseline/riscv/baseline-compiler-riscv-inl.h"
 #elif V8_TARGET_ARCH_MIPS64
 #include "src/baseline/mips64/baseline-compiler-mips64-inl.h"
 #elif V8_TARGET_ARCH_MIPS
diff --git a/src/baseline/riscv64/baseline-assembler-riscv64-inl.h b/src/baseline/riscv/baseline-assembler-riscv-inl.h
similarity index 97%
rename from src/baseline/riscv64/baseline-assembler-riscv64-inl.h
rename to src/baseline/riscv/baseline-assembler-riscv-inl.h
index 5fa4abd3114..1f7e12de161 100644
--- a/src/baseline/riscv64/baseline-assembler-riscv64-inl.h
+++ b/src/baseline/riscv/baseline-assembler-riscv-inl.h
@@ -2,8 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_BASELINE_RISCV64_BASELINE_ASSEMBLER_RISCV64_INL_H_
-#define V8_BASELINE_RISCV64_BASELINE_ASSEMBLER_RISCV64_INL_H_
+#ifndef V8_BASELINE_RISCV_BASELINE_ASSEMBLER_RISCV_INL_H_
+#define V8_BASELINE_RISCV_BASELINE_ASSEMBLER_RISCV_INL_H_
 
 #include "src/baseline/baseline-assembler.h"
 #include "src/codegen/assembler-inl.h"
@@ -79,8 +79,8 @@ MemOperand BaselineAssembler::RegisterFrameOperand(
 }
 void BaselineAssembler::RegisterFrameAddress(
     interpreter::Register interpreter_register, Register rscratch) {
-  return __ Add64(rscratch, fp,
-                  interpreter_register.ToOperand() * kSystemPointerSize);
+  return __ AddWord(rscratch, fp,
+                    interpreter_register.ToOperand() * kSystemPointerSize);
 }
 MemOperand BaselineAssembler::FeedbackVectorOperand() {
   return MemOperand(fp, BaselineFrameConstants::kFeedbackVectorFromFp);
@@ -163,7 +163,7 @@ void BaselineAssembler::JumpIfInstanceType(Condition cc, Register map,
     __ GetObjectType(map, type, type);
     __ Assert(eq, AbortReason::kUnexpectedValue, type, Operand(MAP_TYPE));
   }
-  __ Ld(type, FieldMemOperand(map, Map::kInstanceTypeOffset));
+  __ LoadWord(type, FieldMemOperand(map, Map::kInstanceTypeOffset));
   __ Branch(target, AsMasmCondition(cc), type, Operand(instance_type));
 }
 void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
@@ -171,7 +171,7 @@ void BaselineAssembler::JumpIfPointer(Condition cc, Register value,
                                       Label::Distance) {
   ScratchRegisterScope temps(this);
   Register temp = temps.AcquireScratch();
-  __ Ld(temp, operand);
+  __ LoadWord(temp, operand);
   __ Branch(target, AsMasmCondition(cc), value, Operand(temp));
 }
 void BaselineAssembler::JumpIfSmi(Condition cc, Register value, Smi smi,
@@ -195,7 +195,7 @@ void BaselineAssembler::JumpIfTagged(Condition cc, Register value,
   // todo: compress pointer
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
-  __ Ld(scratch, operand);
+  __ LoadWord(scratch, operand);
   __ Branch(target, AsMasmCondition(cc), value, Operand(scratch));
 }
 void BaselineAssembler::JumpIfTagged(Condition cc, MemOperand operand,
@@ -204,7 +204,7 @@ void BaselineAssembler::JumpIfTagged(Condition cc, MemOperand operand,
   // todo: compress pointer
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
-  __ Ld(scratch, operand);
+  __ LoadWord(scratch, operand);
   __ Branch(target, AsMasmCondition(cc), scratch, Operand(value));
 }
 void BaselineAssembler::JumpIfByte(Condition cc, Register value, int32_t byte,
@@ -219,7 +219,7 @@ void BaselineAssembler::Move(Register output, TaggedIndex value) {
   __ li(output, Operand(value.ptr()));
 }
 void BaselineAssembler::Move(MemOperand output, Register source) {
-  __ Sd(source, output);
+  __ StoreWord(source, output);
 }
 void BaselineAssembler::Move(Register output, ExternalReference reference) {
   __ li(output, Operand(reference));
@@ -446,8 +446,9 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
   __ Add32(interrupt_budget, interrupt_budget, weight);
   __ Sw(interrupt_budget,
         FieldMemOperand(feedback_cell, FeedbackCell::kInterruptBudgetOffset));
-  if (skip_interrupt_label)
+  if (skip_interrupt_label) {
     __ Branch(skip_interrupt_label, ge, interrupt_budget, Operand(zero_reg));
+  }
 }
 
 void BaselineAssembler::LdaContextSlot(Register context, uint32_t index,
@@ -510,27 +511,26 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
   if (SmiValuesAre31Bits()) {
     __ Add32(lhs, lhs, Operand(rhs));
   } else {
-    __ Add64(lhs, lhs, Operand(rhs));
+    __ AddWord(lhs, lhs, Operand(rhs));
   }
 }
 
 void BaselineAssembler::Word32And(Register output, Register lhs, int rhs) {
   __ And(output, lhs, Operand(rhs));
 }
-
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
   ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base != 0) {
-    __ Sub64(reg, reg, Operand(case_value_base));
+    __ SubWord(reg, reg, Operand(case_value_base));
   }
 
   // Mostly copied from code-generator-riscv64.cc
   ScratchRegisterScope scope(this);
   Label table;
   __ Branch(&fallthrough, AsMasmCondition(Condition::kUnsignedGreaterThanEqual),
-            reg, Operand(int64_t(num_labels)));
+            reg, Operand(num_labels));
   int64_t imm64;
   imm64 = __ branch_long_offset(&table);
   CHECK(is_int32(imm64 + 0x800));
@@ -619,4 +619,4 @@ inline void EnsureAccumulatorPreservedScope::AssertEqualToAccumulator(
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_BASELINE_RISCV64_BASELINE_ASSEMBLER_RISCV64_INL_H_
+#endif  // V8_BASELINE_RISCV_BASELINE_ASSEMBLER_RISCV_INL_H_
diff --git a/src/baseline/riscv64/baseline-compiler-riscv64-inl.h b/src/baseline/riscv/baseline-compiler-riscv-inl.h
similarity index 69%
rename from src/baseline/riscv64/baseline-compiler-riscv64-inl.h
rename to src/baseline/riscv/baseline-compiler-riscv-inl.h
index 16e178b1064..00d1d2f3e63 100644
--- a/src/baseline/riscv64/baseline-compiler-riscv64-inl.h
+++ b/src/baseline/riscv/baseline-compiler-riscv-inl.h
@@ -2,8 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_BASELINE_RISCV64_BASELINE_COMPILER_RISCV64_INL_H_
-#define V8_BASELINE_RISCV64_BASELINE_COMPILER_RISCV64_INL_H_
+#ifndef V8_BASELINE_RISCV_BASELINE_COMPILER_RISCV_INL_H_
+#define V8_BASELINE_RISCV_BASELINE_COMPILER_RISCV_INL_H_
 
 #include "src/baseline/baseline-compiler.h"
 
@@ -39,10 +39,10 @@ void BaselineCompiler::PrologueFillFrame() {
   const bool has_new_target = new_target_index != kMaxInt;
   if (has_new_target) {
     DCHECK_LE(new_target_index, register_count);
-    __ masm()->Add64(sp, sp, Operand(-(kPointerSize * new_target_index)));
+    __ masm()->AddWord(sp, sp, Operand(-(kPointerSize * new_target_index)));
     for (int i = 0; i < new_target_index; i++) {
-      __ masm()->Sd(kInterpreterAccumulatorRegister,
-                    MemOperand(sp, i * kSystemPointerSize));
+      __ masm()->StoreWord(kInterpreterAccumulatorRegister,
+                           MemOperand(sp, i * kSystemPointerSize));
     }
     // Push new_target_or_generator.
     __ Push(kJavaScriptCallNewTargetRegister);
@@ -50,25 +50,25 @@ void BaselineCompiler::PrologueFillFrame() {
   }
   if (register_count < 2 * kLoopUnrollSize) {
     // If the frame is small enough, just unroll the frame fill completely.
-    __ masm()->Add64(sp, sp, Operand(-(kPointerSize * register_count)));
+    __ masm()->AddWord(sp, sp, Operand(-(kPointerSize * register_count)));
     for (int i = 0; i < register_count; ++i) {
-      __ masm()->Sd(kInterpreterAccumulatorRegister,
-                    MemOperand(sp, i * kSystemPointerSize));
+      __ masm()->StoreWord(kInterpreterAccumulatorRegister,
+                           MemOperand(sp, i * kSystemPointerSize));
     }
   } else {
-    __ masm()->Add64(sp, sp, Operand(-(kPointerSize * register_count)));
+    __ masm()->AddWord(sp, sp, Operand(-(kPointerSize * register_count)));
     for (int i = 0; i < register_count; ++i) {
-      __ masm()->Sd(kInterpreterAccumulatorRegister,
-                    MemOperand(sp, i * kSystemPointerSize));
+      __ masm()->StoreWord(kInterpreterAccumulatorRegister,
+                           MemOperand(sp, i * kSystemPointerSize));
     }
   }
 }
 
 void BaselineCompiler::VerifyFrameSize() {
   ASM_CODE_COMMENT(&masm_);
-  __ masm()->Add64(kScratchReg, sp,
-                   Operand(InterpreterFrameConstants::kFixedFrameSizeFromFp +
-                           bytecode_->frame_size()));
+  __ masm()->AddWord(kScratchReg, sp,
+                     Operand(InterpreterFrameConstants::kFixedFrameSizeFromFp +
+                             bytecode_->frame_size()));
   __ masm()->Assert(eq, AbortReason::kUnexpectedStackPointer, kScratchReg,
                     Operand(fp));
 }
@@ -79,4 +79,4 @@ void BaselineCompiler::VerifyFrameSize() {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_BASELINE_RISCV64_BASELINE_COMPILER_RISCV64_INL_H_
+#endif  // V8_BASELINE_RISCV_BASELINE_COMPILER_RISCV_INL_H_
diff --git a/src/builtins/builtins-sharedarraybuffer-gen.cc b/src/builtins/builtins-sharedarraybuffer-gen.cc
index a3931e6cc90..ac4320fc5c5 100644
--- a/src/builtins/builtins-sharedarraybuffer-gen.cc
+++ b/src/builtins/builtins-sharedarraybuffer-gen.cc
@@ -251,7 +251,7 @@ TF_BUILTIN(AtomicsLoad, SharedArrayBufferBuiltinsAssembler) {
   BIND(&u32);
   Return(ChangeUint32ToTagged(AtomicLoad<Uint32T>(
       AtomicMemoryOrder::kSeqCst, backing_store, WordShl(index_word, 2))));
-#if V8_TARGET_ARCH_MIPS && !_MIPS_ARCH_MIPS32R6
+#if (V8_TARGET_ARCH_MIPS && !_MIPS_ARCH_MIPS32R6) || V8_TARGET_ARCH_RISCV32
   BIND(&i64);
   Goto(&u64);
 
@@ -268,7 +268,8 @@ TF_BUILTIN(AtomicsLoad, SharedArrayBufferBuiltinsAssembler) {
   BIND(&u64);
   Return(BigIntFromUnsigned64(AtomicLoad64<AtomicUint64>(
       AtomicMemoryOrder::kSeqCst, backing_store, WordShl(index_word, 3))));
-#endif
+#endif  //(V8_TARGET_ARCH_MIPS && !_MIPS_ARCH_MIPS32R6) ||
+        // V8_TARGET_ARCH_RISCV32
 
   // This shouldn't happen, we've already validated the type.
   BIND(&other);
@@ -523,8 +524,7 @@ TF_BUILTIN(AtomicsExchange, SharedArrayBufferBuiltinsAssembler) {
   // This shouldn't happen, we've already validated the type.
   BIND(&other);
   Unreachable();
-#endif  // V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 ||
-        // V8_TARGET_ARCH_RISCV64
+#endif  // V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64
 
   BIND(&detached_or_out_of_bounds);
   {
diff --git a/src/builtins/riscv64/builtins-riscv64.cc b/src/builtins/riscv/builtins-riscv.cc
similarity index 90%
rename from src/builtins/riscv64/builtins-riscv64.cc
rename to src/builtins/riscv/builtins-riscv.cc
index 3ad8bb0d57b..ff908cc307d 100644
--- a/src/builtins/riscv64/builtins-riscv64.cc
+++ b/src/builtins/riscv/builtins-riscv.cc
@@ -2,8 +2,6 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#if V8_TARGET_ARCH_RISCV64
-
 #include "src/api/api-arguments.h"
 #include "src/codegen/code-factory.h"
 #include "src/codegen/interface-descriptors-inl.h"
@@ -15,7 +13,6 @@
 // For interpreter_entry_return_pc_offset. TODO(jkummerow): Drop.
 #include "src/codegen/macro-assembler-inl.h"
 #include "src/codegen/register-configuration.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
 #include "src/heap/heap-inl.h"
 #include "src/objects/cell.h"
 #include "src/objects/foreign.h"
@@ -57,7 +54,7 @@ static void GenerateTailCallToReturnedCode(MacroAssembler* masm,
 
     __ CallRuntime(function_id, 1);
     // Use the return value before restoring a0
-    __ Add64(a2, a0, Operand(Code::kHeaderSize - kHeapObjectTag));
+    __ AddWord(a2, a0, Operand(Code::kHeaderSize - kHeapObjectTag));
     // Restore target function, new target and actual argument count.
     __ Pop(kJavaScriptCallTargetRegister, kJavaScriptCallNewTargetRegister,
            kJavaScriptCallArgCountRegister);
@@ -80,17 +77,17 @@ void Generate_PushArguments(MacroAssembler* masm, Register array, Register argc,
                             ArgumentsElementType element_type) {
   DCHECK(!AreAliased(array, argc, scratch));
   Label loop, entry;
-  __ Sub64(scratch, argc, Operand(kJSArgcReceiverSlots));
+  __ SubWord(scratch, argc, Operand(kJSArgcReceiverSlots));
   __ Branch(&entry);
   __ bind(&loop);
   __ CalcScaledAddress(scratch2, array, scratch, kSystemPointerSizeLog2);
-  __ Ld(scratch2, MemOperand(scratch2));
+  __ LoadWord(scratch2, MemOperand(scratch2));
   if (element_type == ArgumentsElementType::kHandle) {
-    __ Ld(scratch2, MemOperand(scratch2));
+    __ LoadWord(scratch2, MemOperand(scratch2));
   }
   __ push(scratch2);
   __ bind(&entry);
-  __ Add64(scratch, scratch, Operand(-1));
+  __ AddWord(scratch, scratch, Operand(-1));
   __ Branch(&loop, greater_equal, scratch, Operand(zero_reg));
 }
 
@@ -114,7 +111,7 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ SmiUntag(a0);
 
     // Set up pointer to first argument (skip receiver).
-    __ Add64(
+    __ AddWord(
         t2, fp,
         Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
     // t2: Pointer to start of arguments.
@@ -135,9 +132,10 @@ void Generate_JSBuiltinsConstructStubHelper(MacroAssembler* masm) {
     __ InvokeFunctionWithNewTarget(a1, a3, a0, InvokeType::kCall);
 
     // Restore context from the frame.
-    __ Ld(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+    __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
     // Restore smi-tagged arguments count from the frame.
-    __ Ld(kScratchReg, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+    __ LoadWord(kScratchReg,
+                MemOperand(fp, ConstructFrameConstants::kLengthOffset));
     // Leave construct frame.
   }
 
@@ -184,8 +182,8 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     Register func_info = temps.Acquire();
     __ LoadTaggedPointerField(
         func_info, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
-    __ Lwu(func_info,
-           FieldMemOperand(func_info, SharedFunctionInfo::kFlagsOffset));
+    __ Load32U(func_info,
+               FieldMemOperand(func_info, SharedFunctionInfo::kFlagsOffset));
     __ DecodeField<SharedFunctionInfo::FunctionKindBits>(func_info);
     __ JumpIfIsInRange(
         func_info,
@@ -227,7 +225,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   __ Move(a6, a0);
 
   // Set up pointer to first argument (skip receiver)..
-  __ Add64(
+  __ AddWord(
       t2, fp,
       Operand(StandardFrameConstants::kCallerSPOffset + kSystemPointerSize));
 
@@ -242,8 +240,8 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // -----------------------------------
 
   // Restore constructor function and argument count.
-  __ Ld(a1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));
-  __ Ld(a0, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+  __ LoadWord(a1, MemOperand(fp, ConstructFrameConstants::kConstructorOffset));
+  __ LoadWord(a0, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
   __ SmiUntag(a0);
 
   Label stack_overflow;
@@ -287,7 +285,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
       masm->pc_offset());
 
   // Restore the context from the frame.
-  __ Ld(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+  __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
 
   // If the result is an object (in the ECMA sense), we should get rid
   // of the receiver and use the result; see ECMA-262 section 13.2.2-7
@@ -303,12 +301,12 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // Throw away the result of the constructor invocation and use the
   // on-stack receiver as the result.
   __ bind(&use_receiver);
-  __ Ld(a0, MemOperand(sp, 0 * kSystemPointerSize));
+  __ LoadWord(a0, MemOperand(sp, 0 * kSystemPointerSize));
   __ JumpIfRoot(a0, RootIndex::kTheHoleValue, &do_throw);
 
   __ bind(&leave_and_return);
   // Restore smi-tagged arguments count from the frame.
-  __ Ld(a1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
+  __ LoadWord(a1, MemOperand(fp, ConstructFrameConstants::kLengthOffset));
   // Leave construct frame.
   __ LeaveFrame(StackFrame::CONSTRUCT);
 
@@ -334,13 +332,13 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   }
   __ bind(&do_throw);
   // Restore the context from the frame.
-  __ Ld(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+  __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
   __ CallRuntime(Runtime::kThrowConstructorReturnedNonObject);
   __ break_(0xCC);
 
   __ bind(&stack_overflow);
   // Restore the context from the frame.
-  __ Ld(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
+  __ LoadWord(cp, MemOperand(fp, ConstructFrameConstants::kContextOffset));
   __ CallRuntime(Runtime::kThrowStackOverflow);
   __ break_(0xCC);
 }
@@ -353,7 +351,7 @@ static void AssertCodeIsBaseline(MacroAssembler* masm, Register code,
                                  Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
-  __ Ld(scratch, FieldMemOperand(code, Code::kFlagsOffset));
+  __ LoadWord(scratch, FieldMemOperand(code, Code::kFlagsOffset));
   __ DecodeField<Code::KindField>(scratch);
   __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,
             Operand(static_cast<int64_t>(CodeKind::BASELINE)));
@@ -414,7 +412,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
   ExternalReference debug_suspended_generator =
       ExternalReference::debug_suspended_generator_address(masm->isolate());
   __ li(a5, debug_suspended_generator);
-  __ Ld(a5, MemOperand(a5));
+  __ LoadWord(a5, MemOperand(a5));
   __ Branch(&prepare_step_in_suspended_generator, eq, a1, Operand(a5));
   __ bind(&stepping_prepared);
 
@@ -440,14 +438,14 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
       a3, FieldMemOperand(a4, JSFunction::kSharedFunctionInfoOffset));
   __ Lhu(a3,
          FieldMemOperand(a3, SharedFunctionInfo::kFormalParameterCountOffset));
-  __ Sub64(a3, a3, Operand(kJSArgcReceiverSlots));
+  __ SubWord(a3, a3, Operand(kJSArgcReceiverSlots));
   __ LoadTaggedPointerField(
       t1,
       FieldMemOperand(a1, JSGeneratorObject::kParametersAndRegistersOffset));
   {
     Label done_loop, loop;
     __ bind(&loop);
-    __ Sub64(a3, a3, Operand(1));
+    __ SubWord(a3, a3, Operand(1));
     __ Branch(&done_loop, lt, a3, Operand(zero_reg), Label::Distance::kNear);
     __ CalcScaledAddress(kScratchReg, t1, a3, kTaggedSizeLog2);
     __ LoadAnyTaggedField(
@@ -539,9 +537,9 @@ static void Generate_CheckStackOverflow(MacroAssembler* masm, Register argc,
   __ LoadStackLimit(scratch1, MacroAssembler::StackLimitKind::kRealStackLimit);
   // Make a2 the space we have left. The stack might already be overflowed
   // here which will cause r2 to become negative.
-  __ Sub64(scratch1, sp, scratch1);
+  __ SubWord(scratch1, sp, scratch1);
   // Check if the arguments will overflow the stack.
-  __ Sll64(scratch2, argc, kSystemPointerSizeLog2);
+  __ SllWord(scratch2, argc, kSystemPointerSizeLog2);
   __ Branch(&okay, gt, scratch1, Operand(scratch2),
             Label::Distance::kNear);  // Signed comparison.
 
@@ -614,15 +612,15 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   ExternalReference c_entry_fp = ExternalReference::Create(
       IsolateAddressId::kCEntryFPAddress, masm->isolate());
   __ li(s5, c_entry_fp);
-  __ Ld(s4, MemOperand(s5));
+  __ LoadWord(s4, MemOperand(s5));
   __ Push(s1, s2, s3, s4);
   // Clear c_entry_fp, now we've pushed its previous value to the stack.
   // If the c_entry_fp is not already zero and we don't clear it, the
   // SafeStackFrameIterator will assume we are executing C++ and miss the JS
   // frames on top.
-  __ Sd(zero_reg, MemOperand(s5));
+  __ StoreWord(zero_reg, MemOperand(s5));
   // Set up frame pointer for the frame to be pushed.
-  __ Add64(fp, sp, -EntryFrameConstants::kCallerFPOffset);
+  __ AddWord(fp, sp, -EntryFrameConstants::kCallerFPOffset);
   // Registers:
   //  either
   //   a1: entry address
@@ -647,10 +645,10 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   ExternalReference js_entry_sp = ExternalReference::Create(
       IsolateAddressId::kJSEntrySPAddress, masm->isolate());
   __ li(s1, js_entry_sp);
-  __ Ld(s2, MemOperand(s1));
+  __ LoadWord(s2, MemOperand(s1));
   __ Branch(&non_outermost_js, ne, s2, Operand(zero_reg),
             Label::Distance::kNear);
-  __ Sd(fp, MemOperand(s1));
+  __ StoreWord(fp, MemOperand(s1));
   __ li(s3, Operand(StackFrame::OUTERMOST_JSENTRY_FRAME));
   Label cont;
   __ Branch(&cont);
@@ -674,7 +672,8 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   // signal the existence of the JSEntry frame.
   __ li(s1, ExternalReference::Create(
                 IsolateAddressId::kPendingExceptionAddress, masm->isolate()));
-  __ Sd(a0, MemOperand(s1));  // We come back from 'invoke'. result is in a0.
+  __ StoreWord(a0,
+               MemOperand(s1));  // We come back from 'invoke'. result is in a0.
   __ LoadRoot(a0, RootIndex::kException);
   __ BranchShort(&exit);
 
@@ -723,17 +722,17 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
             Operand(StackFrame::OUTERMOST_JSENTRY_FRAME),
             Label::Distance::kNear);
   __ li(a5, js_entry_sp);
-  __ Sd(zero_reg, MemOperand(a5));
+  __ StoreWord(zero_reg, MemOperand(a5));
   __ bind(&non_outermost_js_2);
 
   // Restore the top frame descriptors from the stack.
   __ pop(a5);
   __ li(a4, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,
                                       masm->isolate()));
-  __ Sd(a5, MemOperand(a4));
+  __ StoreWord(a5, MemOperand(a4));
 
   // Reset the stack to the callee saved registers.
-  __ Add64(sp, sp, -EntryFrameConstants::kCallerFPOffset);
+  __ AddWord(sp, sp, -EntryFrameConstants::kCallerFPOffset);
 
   // Restore callee-saved fpu registers.
   __ MultiPopFPU(kCalleeSavedFPU);
@@ -778,7 +777,7 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     ExternalReference context_address = ExternalReference::Create(
         IsolateAddressId::kContextAddress, masm->isolate());
     __ li(cp, context_address);
-    __ Ld(cp, MemOperand(cp));
+    __ LoadWord(cp, MemOperand(cp));
 
     // Push the function onto the stack.
     __ Push(a2);
@@ -867,17 +866,17 @@ static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
   Register params_size = scratch1;
 
   // Get the size of the formal parameters + receiver (in bytes).
-  __ Ld(params_size,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(params_size,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
   __ Lw(params_size,
         FieldMemOperand(params_size, BytecodeArray::kParameterSizeOffset));
 
   Register actual_params_size = scratch2;
   Label L1;
   // Compute the size of the actual parameters + receiver (in bytes).
-  __ Ld(actual_params_size,
-        MemOperand(fp, StandardFrameConstants::kArgCOffset));
-  __ Sll64(actual_params_size, actual_params_size, kSystemPointerSizeLog2);
+  __ LoadWord(actual_params_size,
+              MemOperand(fp, StandardFrameConstants::kArgCOffset));
+  __ SllWord(actual_params_size, actual_params_size, kSystemPointerSizeLog2);
   // If actual is bigger than formal, then we should use it to free up the stack
   // arguments.
   __ Branch(&L1, le, actual_params_size, Operand(params_size),
@@ -1012,20 +1011,20 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
             Label::Distance::kNear);
 
   // Load the next bytecode and update table to the wide scaled table.
-  __ Add64(bytecode_offset, bytecode_offset, Operand(1));
-  __ Add64(scratch2, bytecode_array, bytecode_offset);
+  __ AddWord(bytecode_offset, bytecode_offset, Operand(1));
+  __ AddWord(scratch2, bytecode_array, bytecode_offset);
   __ Lbu(bytecode, MemOperand(scratch2));
-  __ Add64(bytecode_size_table, bytecode_size_table,
-           Operand(kByteSize * interpreter::Bytecodes::kBytecodeCount));
+  __ AddWord(bytecode_size_table, bytecode_size_table,
+             Operand(kByteSize * interpreter::Bytecodes::kBytecodeCount));
   __ BranchShort(&process_bytecode);
 
   __ bind(&extra_wide);
   // Load the next bytecode and update table to the extra wide scaled table.
-  __ Add64(bytecode_offset, bytecode_offset, Operand(1));
-  __ Add64(scratch2, bytecode_array, bytecode_offset);
+  __ AddWord(bytecode_offset, bytecode_offset, Operand(1));
+  __ AddWord(scratch2, bytecode_array, bytecode_offset);
   __ Lbu(bytecode, MemOperand(scratch2));
-  __ Add64(bytecode_size_table, bytecode_size_table,
-           Operand(2 * kByteSize * interpreter::Bytecodes::kBytecodeCount));
+  __ AddWord(bytecode_size_table, bytecode_size_table,
+             Operand(2 * kByteSize * interpreter::Bytecodes::kBytecodeCount));
 
   __ bind(&process_bytecode);
 
@@ -1049,9 +1048,9 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   __ bind(&not_jump_loop);
   // Otherwise, load the size of the current bytecode and advance the offset.
-  __ Add64(scratch2, bytecode_size_table, bytecode);
+  __ AddWord(scratch2, bytecode_size_table, bytecode);
   __ Lb(scratch2, MemOperand(scratch2));
-  __ Add64(bytecode_offset, bytecode_offset, scratch2);
+  __ AddWord(bytecode_offset, bytecode_offset, scratch2);
 
   __ bind(&end);
 }
@@ -1133,9 +1132,10 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
       BaselineOutOfLinePrologueDescriptor::kClosure);
   // Load the feedback vector from the closure.
   Register feedback_vector = temps.Acquire();
-  __ Ld(feedback_vector,
-        FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));
-  __ Ld(feedback_vector, FieldMemOperand(feedback_vector, Cell::kValueOffset));
+  __ LoadWord(feedback_vector,
+              FieldMemOperand(closure, JSFunction::kFeedbackCellOffset));
+  __ LoadWord(feedback_vector,
+              FieldMemOperand(feedback_vector, Cell::kValueOffset));
   if (FLAG_debug_code) {
     UseScratchRegisterScope temps(masm);
     Register type = temps.Acquire();
@@ -1220,7 +1220,7 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // building the frame we can quickly precheck both at once.
     UseScratchRegisterScope temps(masm);
     Register sp_minus_frame_size = temps.Acquire();
-    __ Sub64(sp_minus_frame_size, sp, frame_size);
+    __ SubWord(sp_minus_frame_size, sp, frame_size);
     Register interrupt_limit = temps.Acquire();
     __ LoadStackLimit(interrupt_limit,
                       MacroAssembler::StackLimitKind::kInterruptStackLimit);
@@ -1322,7 +1322,6 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
     UseScratchRegisterScope temps(masm);
     ResetFeedbackVectorOsrUrgency(masm, feedback_vector, temps.Acquire());
   }
-
   Label not_optimized;
   __ bind(&not_optimized);
 
@@ -1358,7 +1357,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
                               BytecodeArray::kFrameSizeOffset));
 
     // Do a stack check to ensure we don't go over the limit.
-    __ Sub64(a5, sp, Operand(a4));
+    __ SubWord(a5, sp, Operand(a4));
     __ LoadStackLimit(a2, MacroAssembler::StackLimitKind::kRealStackLimit);
     __ Branch(&stack_overflow, Uless, a5, Operand(a2));
 
@@ -1372,7 +1371,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
     __ push(a5);
     // Continue loop if not done.
     __ bind(&loop_check);
-    __ Sub64(a4, a4, Operand(kSystemPointerSize));
+    __ SubWord(a4, a4, Operand(kSystemPointerSize));
     __ Branch(&loop_header, ge, a4, Operand(zero_reg));
   }
 
@@ -1385,7 +1384,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ Branch(&no_incoming_new_target_or_generator_register, eq, a5,
             Operand(zero_reg), Label::Distance::kNear);
   __ CalcScaledAddress(a5, fp, a5, kSystemPointerSizeLog2);
-  __ Sd(a3, MemOperand(a5));
+  __ StoreWord(a3, MemOperand(a5));
   __ bind(&no_incoming_new_target_or_generator_register);
 
   // Perform interrupt stack check.
@@ -1405,12 +1404,12 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ bind(&do_dispatch);
   __ li(kInterpreterDispatchTableRegister,
         ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
-  __ Add64(a1, kInterpreterBytecodeArrayRegister,
-           kInterpreterBytecodeOffsetRegister);
+  __ AddWord(a1, kInterpreterBytecodeArrayRegister,
+             kInterpreterBytecodeOffsetRegister);
   __ Lbu(a7, MemOperand(a1));
   __ CalcScaledAddress(kScratchReg, kInterpreterDispatchTableRegister, a7,
                        kSystemPointerSizeLog2);
-  __ Ld(kJavaScriptCallCodeStartRegister, MemOperand(kScratchReg));
+  __ LoadWord(kJavaScriptCallCodeStartRegister, MemOperand(kScratchReg));
   __ Call(kJavaScriptCallCodeStartRegister);
   masm->isolate()->heap()->SetInterpreterEntryReturnPCOffset(masm->pc_offset());
 
@@ -1418,16 +1417,16 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   // or the interpreter tail calling a builtin and then a dispatch.
 
   // Get bytecode array and bytecode offset from the stack frame.
-  __ Ld(kInterpreterBytecodeArrayRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
-  __ Ld(kInterpreterBytecodeOffsetRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ LoadWord(kInterpreterBytecodeArrayRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(kInterpreterBytecodeOffsetRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
   __ SmiUntag(kInterpreterBytecodeOffsetRegister);
 
   // Either return, or advance to the next bytecode and dispatch.
   Label do_return;
-  __ Add64(a1, kInterpreterBytecodeArrayRegister,
-           kInterpreterBytecodeOffsetRegister);
+  __ AddWord(a1, kInterpreterBytecodeArrayRegister,
+             kInterpreterBytecodeOffsetRegister);
   __ Lbu(a1, MemOperand(a1));
   AdvanceBytecodeOffsetOrReturn(masm, kInterpreterBytecodeArrayRegister,
                                 kInterpreterBytecodeOffsetRegister, a1, a2, a3,
@@ -1445,21 +1444,23 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ li(kInterpreterBytecodeOffsetRegister,
         Operand(Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +
                              kFunctionEntryBytecodeOffset)));
-  __ Sd(kInterpreterBytecodeOffsetRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ StoreWord(
+      kInterpreterBytecodeOffsetRegister,
+      MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
   __ CallRuntime(Runtime::kStackGuard);
 
   // After the call, restore the bytecode array, bytecode offset and accumulator
   // registers again. Also, restore the bytecode offset in the stack to its
   // previous value.
-  __ Ld(kInterpreterBytecodeArrayRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(kInterpreterBytecodeArrayRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
   __ li(kInterpreterBytecodeOffsetRegister,
         Operand(BytecodeArray::kHeaderSize - kHeapObjectTag));
   __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
 
   __ SmiTag(a5, kInterpreterBytecodeOffsetRegister);
-  __ Sd(a5, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ StoreWord(
+      a5, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
 
   __ Branch(&after_stack_check_interrupt);
 
@@ -1515,9 +1516,9 @@ static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
                                         Register scratch) {
   ASM_CODE_COMMENT(masm);
   // Find the address of the last argument.
-  __ Sub64(scratch, num_args, Operand(1));
-  __ Sll64(scratch, scratch, kSystemPointerSizeLog2);
-  __ Sub64(start_address, start_address, scratch);
+  __ SubWord(scratch, num_args, Operand(1));
+  __ SllWord(scratch, scratch, kSystemPointerSizeLog2);
+  __ SubWord(start_address, start_address, scratch);
 
   // Push the arguments.
   __ PushArray(start_address, num_args,
@@ -1539,11 +1540,11 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   Label stack_overflow;
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
-    __ Sub64(a0, a0, Operand(1));
+    __ SubWord(a0, a0, Operand(1));
   }
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
-    __ Sub64(a3, a0, Operand(kJSArgcReceiverSlots));
+    __ SubWord(a3, a0, Operand(kJSArgcReceiverSlots));
   } else {
     __ Move(a3, a0);
   }
@@ -1559,7 +1560,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
     // Pass the spread in the register a2.
     // a2 already points to the penultime argument, the spread
     // is below that.
-    __ Ld(a2, MemOperand(a2, -kSystemPointerSize));
+    __ LoadWord(a2, MemOperand(a2, -kSystemPointerSize));
   }
 
   // Call the target.
@@ -1594,10 +1595,10 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // The spread argument should not be pushed.
-    __ Sub64(a0, a0, Operand(1));
+    __ SubWord(a0, a0, Operand(1));
   }
   Register argc_without_receiver = a6;
-  __ Sub64(argc_without_receiver, a0, Operand(kJSArgcReceiverSlots));
+  __ SubWord(argc_without_receiver, a0, Operand(kJSArgcReceiverSlots));
   // Push the arguments, This function modifies a4 and a5.
   GenerateInterpreterPushArgs(masm, argc_without_receiver, a4, a5);
 
@@ -1608,7 +1609,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
     // Pass the spread in the register a2.
     // a4 already points to the penultimate argument, the spread
     // lies in the next interpreter register.
-    __ Ld(a2, MemOperand(a4, -kSystemPointerSize));
+    __ LoadWord(a2, MemOperand(a4, -kSystemPointerSize));
   } else {
     __ AssertUndefinedOrAllocationSite(a2, t0);
   }
@@ -1650,7 +1651,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // custom copy of the interpreter entry trampoline for profiling. If so,
   // get the custom trampoline, otherwise grab the entry address of the global
   // trampoline.
-  __ Ld(t0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ LoadWord(t0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
   __ LoadTaggedPointerField(
       t0, FieldMemOperand(t0, JSFunction::kSharedFunctionInfoOffset));
   __ LoadTaggedPointerField(
@@ -1662,25 +1663,25 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
 
   __ LoadTaggedPointerField(
       t0, FieldMemOperand(t0, InterpreterData::kInterpreterTrampolineOffset));
-  __ Add64(t0, t0, Operand(Code::kHeaderSize - kHeapObjectTag));
+  __ AddWord(t0, t0, Operand(Code::kHeaderSize - kHeapObjectTag));
   __ BranchShort(&trampoline_loaded);
 
   __ bind(&builtin_trampoline);
   __ li(t0, ExternalReference::
                 address_of_interpreter_entry_trampoline_instruction_start(
                     masm->isolate()));
-  __ Ld(t0, MemOperand(t0));
+  __ LoadWord(t0, MemOperand(t0));
 
   __ bind(&trampoline_loaded);
-  __ Add64(ra, t0, Operand(interpreter_entry_return_pc_offset.value()));
+  __ AddWord(ra, t0, Operand(interpreter_entry_return_pc_offset.value()));
 
   // Initialize the dispatch table register.
   __ li(kInterpreterDispatchTableRegister,
         ExternalReference::interpreter_dispatch_table_address(masm->isolate()));
 
   // Get the bytecode array pointer from the frame.
-  __ Ld(kInterpreterBytecodeArrayRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(kInterpreterBytecodeArrayRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
 
   if (FLAG_debug_code) {
     // Check function data field is actually a BytecodeArray object.
@@ -1709,12 +1710,12 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   }
 
   // Dispatch to the target bytecode.
-  __ Add64(a1, kInterpreterBytecodeArrayRegister,
-           kInterpreterBytecodeOffsetRegister);
+  __ AddWord(a1, kInterpreterBytecodeArrayRegister,
+             kInterpreterBytecodeOffsetRegister);
   __ Lbu(a7, MemOperand(a1));
   __ CalcScaledAddress(a1, kInterpreterDispatchTableRegister, a7,
                        kSystemPointerSizeLog2);
-  __ Ld(kJavaScriptCallCodeStartRegister, MemOperand(a1));
+  __ LoadWord(kJavaScriptCallCodeStartRegister, MemOperand(a1));
   __ Jump(kJavaScriptCallCodeStartRegister);
 }
 
@@ -1722,10 +1723,10 @@ void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) {
   // Advance the current bytecode offset stored within the given interpreter
   // stack frame. This simulates what all bytecode handlers do upon completion
   // of the underlying operation.
-  __ Ld(kInterpreterBytecodeArrayRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
-  __ Ld(kInterpreterBytecodeOffsetRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ LoadWord(kInterpreterBytecodeArrayRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(kInterpreterBytecodeOffsetRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
   __ SmiUntag(kInterpreterBytecodeOffsetRegister);
 
   Label enter_bytecode, function_entry_bytecode;
@@ -1734,8 +1735,8 @@ void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) {
                     kFunctionEntryBytecodeOffset));
 
   // Load the current bytecode.
-  __ Add64(a1, kInterpreterBytecodeArrayRegister,
-           kInterpreterBytecodeOffsetRegister);
+  __ AddWord(a1, kInterpreterBytecodeArrayRegister,
+             kInterpreterBytecodeOffsetRegister);
   __ Lbu(a1, MemOperand(a1));
 
   // Advance to the next bytecode.
@@ -1747,7 +1748,8 @@ void Builtins::Generate_InterpreterEnterAtNextBytecode(MacroAssembler* masm) {
   __ bind(&enter_bytecode);
   // Convert new bytecode offset to a Smi and save in the stackframe.
   __ SmiTag(a2, kInterpreterBytecodeOffsetRegister);
-  __ Sd(a2, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ StoreWord(
+      a2, MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
 
   Generate_InterpreterEnterBytecode(masm);
 
@@ -1783,11 +1785,11 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     } else {
       // Overwrite the hole inserted by the deoptimizer with the return value
       // from the LAZY deopt point.
-      __ Sd(a0,
-            MemOperand(sp,
-                       config->num_allocatable_general_registers() *
-                               kSystemPointerSize +
-                           BuiltinContinuationFrameConstants::kFixedFrameSize));
+      __ StoreWord(
+          a0, MemOperand(
+                  sp, config->num_allocatable_general_registers() *
+                              kSystemPointerSize +
+                          BuiltinContinuationFrameConstants::kFixedFrameSize));
     }
   }
   for (int i = allocatable_register_count - 1; i >= 0; --i) {
@@ -1805,20 +1807,21 @@ void Generate_ContinueToBuiltinHelper(MacroAssembler* masm,
     constexpr int return_value_offset =
         BuiltinContinuationFrameConstants::kFixedSlotCount -
         kJSArgcReceiverSlots;
-    __ Add64(a0, a0, Operand(return_value_offset));
+    __ AddWord(a0, a0, Operand(return_value_offset));
     __ CalcScaledAddress(t0, sp, a0, kSystemPointerSizeLog2);
-    __ Sd(scratch, MemOperand(t0));
+    __ StoreWord(scratch, MemOperand(t0));
     // Recover arguments count.
-    __ Sub64(a0, a0, Operand(return_value_offset));
+    __ SubWord(a0, a0, Operand(return_value_offset));
   }
 
-  __ Ld(fp, MemOperand(
-                sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
+  __ LoadWord(
+      fp,
+      MemOperand(sp, BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
   // Load builtin index (stored as a Smi) and use it to get the builtin start
   // address from the builtins table.
   __ Pop(t6);
-  __ Add64(sp, sp,
-           Operand(BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
+  __ AddWord(sp, sp,
+             Operand(BuiltinContinuationFrameConstants::kFixedFrameSizeFromFp));
   __ Pop(ra);
   __ LoadEntryFromBuiltinIndex(t6);
   __ Jump(t6);
@@ -1850,16 +1853,16 @@ void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {
   }
 
   DCHECK_EQ(kInterpreterAccumulatorRegister.code(), a0.code());
-  __ Ld(a0, MemOperand(sp, 0 * kSystemPointerSize));
-  __ Add64(sp, sp, Operand(1 * kSystemPointerSize));  // Remove state.
+  __ LoadWord(a0, MemOperand(sp, 0 * kSystemPointerSize));
+  __ AddWord(sp, sp, Operand(1 * kSystemPointerSize));  // Remove state.
   __ Ret();
 }
 
 namespace {
 
 void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
-                       Operand offset = Operand(int64_t(0))) {
-  __ Add64(ra, entry_address, offset);
+                       Operand offset = Operand(0)) {
+  __ AddWord(ra, entry_address, offset);
   // And "return" to the OSR entry point of the function.
   __ Ret();
 }
@@ -1887,7 +1890,6 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
   // If the code object is null, just return to the caller.
   __ Ret(eq, a0, Operand(Smi::zero()));
-
   __ bind(&jump_to_optimized_code);
   if (source == OsrSourceTier::kInterpreter) {
     // Drop the handler frame that is be sitting on top of the actual
@@ -1908,7 +1910,7 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
   // Compute the target address = code_obj + header_size + osr_offset
   // <entry_addr> = <code_obj> + #header_size + <osr_offset>
-  __ Add64(a0, a0, a1);
+  __ AddWord(a0, a0, a1);
   Generate_OSREntry(masm, a0, Operand(Code::kHeaderSize - kHeapObjectTag));
 }
 }  // namespace
@@ -1924,8 +1926,8 @@ void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
   static_assert(D::kParameterCount == 1);
 
-  __ Ld(kContextRegister,
-        MemOperand(fp, BaselineFrameConstants::kContextOffset));
+  __ LoadWord(kContextRegister,
+              MemOperand(fp, BaselineFrameConstants::kContextOffset));
   OnStackReplacement(masm, OsrSourceTier::kBaseline,
                      D::MaybeTargetCodeRegister());
 }
@@ -1954,13 +1956,13 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     // Claim (2 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
 
-    __ Ld(this_arg, MemOperand(sp, kSystemPointerSize));
-    __ Ld(arg_array, MemOperand(sp, 2 * kSystemPointerSize));
+    __ LoadWord(this_arg, MemOperand(sp, kSystemPointerSize));
+    __ LoadWord(arg_array, MemOperand(sp, 2 * kSystemPointerSize));
 
     Label done0, done1;
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
-    __ Sub64(scratch, argc, JSParameterCount(0));
+    __ SubWord(scratch, argc, JSParameterCount(0));
     __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);
     __ Move(arg_array, undefined_value);  // if argc == 0
     __ Move(this_arg, undefined_value);   // if argc == 0
@@ -1970,7 +1972,7 @@ void Builtins::Generate_FunctionPrototypeApply(MacroAssembler* masm) {
     __ Move(arg_array, undefined_value);  // if argc == 1
     __ bind(&done1);                      // else (i.e., argc > 1)
 
-    __ Ld(receiver, MemOperand(sp));
+    __ LoadWord(receiver, MemOperand(sp));
     __ DropArgumentsAndPushNewReceiver(argc, this_arg,
                                        MacroAssembler::kCountIsInteger,
                                        MacroAssembler::kCountIncludesReceiver);
@@ -2019,12 +2021,12 @@ void Builtins::Generate_FunctionPrototypeCall(MacroAssembler* masm) {
     __ Branch(&done, ne, a0, Operand(JSParameterCount(0)),
               Label::Distance::kNear);
     __ PushRoot(RootIndex::kUndefinedValue);
-    __ Add64(a0, a0, Operand(1));
+    __ AddWord(a0, a0, Operand(1));
     __ bind(&done);
   }
 
   // 3. Adjust the actual number of arguments.
-  __ Add64(a0, a0, -1);
+  __ AddWord(a0, a0, -1);
 
   // 4. Call the callable.
   __ Jump(masm->isolate()->builtins()->Call(), RelocInfo::CODE_TARGET);
@@ -2054,14 +2056,14 @@ void Builtins::Generate_ReflectApply(MacroAssembler* masm) {
     // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
 
-    __ Ld(target, MemOperand(sp, kSystemPointerSize));
-    __ Ld(this_argument, MemOperand(sp, 2 * kSystemPointerSize));
-    __ Ld(arguments_list, MemOperand(sp, 3 * kSystemPointerSize));
+    __ LoadWord(target, MemOperand(sp, kSystemPointerSize));
+    __ LoadWord(this_argument, MemOperand(sp, 2 * kSystemPointerSize));
+    __ LoadWord(arguments_list, MemOperand(sp, 3 * kSystemPointerSize));
 
     Label done0, done1, done2;
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
-    __ Sub64(scratch, argc, Operand(JSParameterCount(0)));
+    __ SubWord(scratch, argc, Operand(JSParameterCount(0)));
     __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);
     __ Move(arguments_list, undefined_value);  // if argc == 0
     __ Move(this_argument, undefined_value);   // if argc == 0
@@ -2121,14 +2123,14 @@ void Builtins::Generate_ReflectConstruct(MacroAssembler* masm) {
   {
     // Claim (3 - argc) dummy arguments form the stack, to put the stack in a
     // consistent state for a simple pop operation.
-    __ Ld(target, MemOperand(sp, kSystemPointerSize));
-    __ Ld(arguments_list, MemOperand(sp, 2 * kSystemPointerSize));
-    __ Ld(new_target, MemOperand(sp, 3 * kSystemPointerSize));
+    __ LoadWord(target, MemOperand(sp, kSystemPointerSize));
+    __ LoadWord(arguments_list, MemOperand(sp, 2 * kSystemPointerSize));
+    __ LoadWord(new_target, MemOperand(sp, 3 * kSystemPointerSize));
 
     Label done0, done1, done2;
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
-    __ Sub64(scratch, argc, Operand(JSParameterCount(0)));
+    __ SubWord(scratch, argc, Operand(JSParameterCount(0)));
     __ Branch(&done0, ne, scratch, Operand(zero_reg), Label::Distance::kNear);
     __ Move(arguments_list, undefined_value);  // if argc == 0
     __ Move(new_target, undefined_value);      // if argc == 0
@@ -2188,7 +2190,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
   Register new_space = scratch2;
   __ mv(old_sp, sp);
   __ slli(new_space, count, kPointerSizeLog2);
-  __ Sub64(sp, sp, Operand(new_space));
+  __ SubWord(sp, sp, Operand(new_space));
 
   Register end = scratch2;
   Register value = scratch3;
@@ -2198,15 +2200,15 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
   Label loop, done;
   __ Branch(&done, ge, old_sp, Operand(end));
   __ bind(&loop);
-  __ Ld(value, MemOperand(old_sp, 0));
-  __ Sd(value, MemOperand(dest, 0));
-  __ Add64(old_sp, old_sp, Operand(kSystemPointerSize));
-  __ Add64(dest, dest, Operand(kSystemPointerSize));
+  __ LoadWord(value, MemOperand(old_sp, 0));
+  __ StoreWord(value, MemOperand(dest, 0));
+  __ AddWord(old_sp, old_sp, Operand(kSystemPointerSize));
+  __ AddWord(dest, dest, Operand(kSystemPointerSize));
   __ Branch(&loop, lt, old_sp, Operand(end));
   __ bind(&done);
 
   // Update total number of arguments.
-  __ Add64(argc_in_out, argc_in_out, count);
+  __ AddWord(argc_in_out, argc_in_out, count);
 }
 
 }  // namespace
@@ -2261,20 +2263,20 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
     Register scratch = len;
     UseScratchRegisterScope temps(masm);
     Register hole_value = temps.Acquire();
-    __ Add64(src, args, FixedArray::kHeaderSize - kHeapObjectTag);
+    __ AddWord(src, args, FixedArray::kHeaderSize - kHeapObjectTag);
     __ Branch(&done, eq, len, Operand(zero_reg), Label::Distance::kNear);
-    __ Sll64(scratch, len, kTaggedSizeLog2);
-    __ Sub64(scratch, sp, Operand(scratch));
+    __ SllWord(scratch, len, kTaggedSizeLog2);
+    __ SubWord(scratch, sp, Operand(scratch));
     __ LoadRoot(hole_value, RootIndex::kTheHoleValue);
     __ bind(&loop);
     __ LoadTaggedPointerField(a5, MemOperand(src));
-    __ Add64(src, src, kTaggedSize);
+    __ AddWord(src, src, kTaggedSize);
     __ Branch(&push, ne, a5, Operand(hole_value), Label::Distance::kNear);
     __ LoadRoot(a5, RootIndex::kUndefinedValue);
     __ bind(&push);
-    __ Sd(a5, MemOperand(a7, 0));
-    __ Add64(a7, a7, Operand(kSystemPointerSize));
-    __ Add64(scratch, scratch, Operand(kTaggedSize));
+    __ StoreWord(a5, MemOperand(a7, 0));
+    __ AddWord(a7, a7, Operand(kSystemPointerSize));
+    __ AddWord(scratch, scratch, Operand(kTaggedSize));
     __ Branch(&loop, ne, scratch, Operand(sp));
     __ bind(&done);
   }
@@ -2324,11 +2326,11 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
   // TODO(victorgomes): Remove this copy when all the arguments adaptor frame
   // code is erased.
   __ Move(a6, fp);
-  __ Ld(a7, MemOperand(fp, StandardFrameConstants::kArgCOffset));
+  __ LoadWord(a7, MemOperand(fp, StandardFrameConstants::kArgCOffset));
 
   Label stack_done, stack_overflow;
-  __ Sub64(a7, a7, Operand(kJSArgcReceiverSlots));
-  __ Sub64(a7, a7, a2);
+  __ SubWord(a7, a7, Operand(kJSArgcReceiverSlots));
+  __ SubWord(a7, a7, a2);
   __ Branch(&stack_done, le, a7, Operand(zero_reg));
   {
     // Check for stack overflow.
@@ -2337,9 +2339,9 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
     // Forward the arguments from the caller frame.
 
     // Point to the first argument to copy (skipping the receiver).
-    __ Add64(a6, a6,
-             Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
-                     kSystemPointerSize));
+    __ AddWord(a6, a6,
+               Operand(CommonFrameConstants::kFixedFrameSizeAboveFp +
+                       kSystemPointerSize));
     __ CalcScaledAddress(a6, a6, a2, kSystemPointerSizeLog2);
 
     // Move the arguments already in the stack,
@@ -2360,9 +2362,9 @@ void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
         Register scratch = temps.Acquire(), addr = temps.Acquire();
         __ Sub32(a7, a7, Operand(1));
         __ CalcScaledAddress(addr, a6, a7, kSystemPointerSizeLog2);
-        __ Ld(scratch, MemOperand(addr));
+        __ LoadWord(scratch, MemOperand(addr));
         __ CalcScaledAddress(addr, a2, a7, kSystemPointerSizeLog2);
-        __ Sd(scratch, MemOperand(addr));
+        __ StoreWord(scratch, MemOperand(addr));
         __ Branch(&loop, ne, a7, Operand(zero_reg));
       }
     }
@@ -2388,7 +2390,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
   Label class_constructor;
   __ LoadTaggedPointerField(
       a2, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
-  __ Lwu(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
+  __ Load32U(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
   __ And(kScratchReg, a3,
          Operand(SharedFunctionInfo::IsClassConstructorBit::kMask));
   __ Branch(&class_constructor, ne, kScratchReg, Operand(zero_reg));
@@ -2400,7 +2402,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
                             FieldMemOperand(a1, JSFunction::kContextOffset));
   // We need to convert the receiver for non-native sloppy mode functions.
   Label done_convert;
-  __ Lwu(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
+  __ Load32U(a3, FieldMemOperand(a2, SharedFunctionInfo::kFlagsOffset));
   __ And(kScratchReg, a3,
          Operand(SharedFunctionInfo::IsNativeBit::kMask |
                  SharedFunctionInfo::IsStrictBit::kMask));
@@ -2532,10 +2534,10 @@ void Generate_PushBoundArguments(MacroAssembler* masm) {
     {
       Label loop, done_loop;
       __ SmiUntag(a4, FieldMemOperand(a2, FixedArray::kLengthOffset));
-      __ Add64(a0, a0, Operand(a4));
-      __ Add64(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
+      __ AddWord(a0, a0, Operand(a4));
+      __ AddWord(a2, a2, Operand(FixedArray::kHeaderSize - kHeapObjectTag));
       __ bind(&loop);
-      __ Sub64(a4, a4, Operand(1));
+      __ SubWord(a4, a4, Operand(1));
       __ Branch(&done_loop, lt, a4, Operand(zero_reg), Label::Distance::kNear);
       __ CalcScaledAddress(a5, a2, a4, kTaggedSizeLog2);
       __ LoadAnyTaggedField(kScratchReg, MemOperand(a5));
@@ -2665,7 +2667,7 @@ void Builtins::Generate_ConstructFunction(MacroAssembler* masm) {
   // Jump to JSBuiltinsConstructStub or JSConstructStubGeneric.
   __ LoadTaggedPointerField(
       a4, FieldMemOperand(a1, JSFunction::kSharedFunctionInfoOffset));
-  __ Lwu(a4, FieldMemOperand(a4, SharedFunctionInfo::kFlagsOffset));
+  __ Load32U(a4, FieldMemOperand(a4, SharedFunctionInfo::kFlagsOffset));
   __ And(a4, a4, Operand(SharedFunctionInfo::ConstructAsBuiltinBit::kMask));
   __ Branch(&call_generic_stub, eq, a4, Operand(zero_reg),
             Label::Distance::kNear);
@@ -2693,10 +2695,14 @@ void Builtins::Generate_ConstructBoundFunction(MacroAssembler* masm) {
   // Patch new.target to [[BoundTargetFunction]] if new.target equals target.
   Label skip;
   {
+#ifdef V8_COMPRESS_POINTERS
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
     __ CmpTagged(scratch, a1, a3);
     __ Branch(&skip, ne, scratch, Operand(zero_reg), Label::Distance::kNear);
+#else
+    __ Branch(&skip, ne, a1, Operand(a3), Label::Distance::kNear);
+#endif
   }
   __ LoadTaggedPointerField(
       a3, FieldMemOperand(a1, JSBoundFunction::kBoundTargetFunctionOffset));
@@ -2827,10 +2833,11 @@ void Builtins::Generate_WasmCompileLazy(MacroAssembler* masm) {
 
   // The runtime function returned the jump table slot offset as a Smi (now in
   // x17). Use that to compute the jump target.
-  __ Ld(kScratchReg,
-        MemOperand(kWasmInstanceRegister,
-                   WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
-  __ Add64(s1, s1, Operand(kScratchReg));
+  __ LoadWord(
+      kScratchReg,
+      MemOperand(kWasmInstanceRegister,
+                 WasmInstanceObject::kJumpTableStartOffset - kHeapObjectTag));
+  __ AddWord(s1, s1, Operand(kScratchReg));
   // Finally, jump to the entrypoint.
   __ Jump(s1);
 }
@@ -2877,7 +2884,7 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
   } else {
     // Compute the argv pointer in a callee-saved register.
     __ CalcScaledAddress(s1, sp, a0, kSystemPointerSizeLog2);
-    __ Sub64(s1, s1, kSystemPointerSize);
+    __ SubWord(s1, s1, kSystemPointerSize);
   }
 
   // Enter the exit frame that transitions from JavaScript to C++.
@@ -2920,7 +2927,7 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
     ExternalReference pending_exception_address = ExternalReference::Create(
         IsolateAddressId::kPendingExceptionAddress, masm->isolate());
     __ li(a2, pending_exception_address);
-    __ Ld(a2, MemOperand(a2));
+    __ LoadWord(a2, MemOperand(a2));
     __ LoadRoot(a4, RootIndex::kTheHoleValue);
     // Cannot use check here as it attempts to generate call into runtime.
     __ Branch(&okay, eq, a4, Operand(a2), Label::Distance::kNear);
@@ -2967,24 +2974,24 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
 
   // Retrieve the handler context, SP and FP.
   __ li(cp, pending_handler_context_address);
-  __ Ld(cp, MemOperand(cp));
+  __ LoadWord(cp, MemOperand(cp));
   __ li(sp, pending_handler_sp_address);
-  __ Ld(sp, MemOperand(sp));
+  __ LoadWord(sp, MemOperand(sp));
   __ li(fp, pending_handler_fp_address);
-  __ Ld(fp, MemOperand(fp));
+  __ LoadWord(fp, MemOperand(fp));
 
   // If the handler is a JS frame, restore the context to the frame. Note that
   // the context will be set to (cp == 0) for non-JS frames.
   Label zero;
   __ Branch(&zero, eq, cp, Operand(zero_reg), Label::Distance::kNear);
-  __ Sd(cp, MemOperand(fp, StandardFrameConstants::kContextOffset));
+  __ StoreWord(cp, MemOperand(fp, StandardFrameConstants::kContextOffset));
   __ bind(&zero);
 
   // Compute the handler entry address and jump to it.
   UseScratchRegisterScope temp(masm);
   Register scratch = temp.Acquire();
   __ li(scratch, pending_handler_entrypoint_address);
-  __ Ld(scratch, MemOperand(scratch));
+  __ LoadWord(scratch, MemOperand(scratch));
   __ Jump(scratch);
 }
 
@@ -3080,7 +3087,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   // Replace the shifted bits with bits from the lower mantissa word.
   Label pos_shift, shift_done, sign_negative;
   __ li(kScratchReg, 32);
-  __ subw(scratch, kScratchReg, scratch);
+  __ Sub32(scratch, kScratchReg, scratch);
   __ Branch(&pos_shift, ge, scratch, Operand(zero_reg), Label::Distance::kNear);
 
   // Negate scratch.
@@ -3089,7 +3096,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   __ BranchShort(&shift_done);
 
   __ bind(&pos_shift);
-  __ srlw(input_low, input_low, scratch);
+  __ Srl32(input_low, input_low, scratch);
 
   __ bind(&shift_done);
   __ Or(input_high, input_high, Operand(input_low));
@@ -3105,7 +3112,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
 
   __ bind(&done);
 
-  __ Sd(result_reg, MemOperand(sp, kArgumentOffset));
+  __ StoreWord(result_reg, MemOperand(sp, kArgumentOffset));
   __ Pop(scratch, scratch2, scratch3);
   __ Pop(result_reg);
   __ Ret();
@@ -3195,8 +3202,8 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
 
     // Allocate HandleScope in callee-save registers.
     __ li(s5, next_address);
-    __ Ld(s3, MemOperand(s5, kNextOffset));
-    __ Ld(s1, MemOperand(s5, kLimitOffset));
+    __ LoadWord(s3, MemOperand(s5, kNextOffset));
+    __ LoadWord(s1, MemOperand(s5, kLimitOffset));
     __ Lw(s2, MemOperand(s5, kLevelOffset));
     __ Add32(s2, s2, Operand(1));
     __ Sw(s2, MemOperand(s5, kLevelOffset));
@@ -3210,12 +3217,12 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
   Label return_value_loaded;
 
   // Load value from ReturnValue.
-  __ Ld(a0, return_value_operand);
+  __ LoadWord(a0, return_value_operand);
   __ bind(&return_value_loaded);
 
   // No more valid handles (the result handle was the last one). Restore
   // previous handle scope.
-  __ Sd(s3, MemOperand(s5, kNextOffset));
+  __ StoreWord(s3, MemOperand(s5, kNextOffset));
   if (FLAG_debug_code) {
     __ Lw(a1, MemOperand(s5, kLevelOffset));
     __ Check(eq, AbortReason::kUnexpectedLevelAfterReturnFromApiCall, a1,
@@ -3223,7 +3230,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
   }
   __ Sub32(s2, s2, Operand(1));
   __ Sw(s2, MemOperand(s5, kLevelOffset));
-  __ Ld(kScratchReg, MemOperand(s5, kLimitOffset));
+  __ LoadWord(kScratchReg, MemOperand(s5, kLimitOffset));
   __ Branch(&delete_allocated_handles, ne, s1, Operand(kScratchReg));
 
   // Leave the API exit frame.
@@ -3235,7 +3242,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
   } else {
     DCHECK_EQ(stack_space, 0);
     static_assert(kCArgSlotCount == 0);
-    __ Ld(s3, *stack_space_operand);
+    __ LoadWord(s3, *stack_space_operand);
   }
 
   static constexpr bool kDontSaveDoubles = false;
@@ -3246,7 +3253,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
   // Check if the function scheduled an exception.
   __ LoadRoot(a4, RootIndex::kTheHoleValue);
   __ li(kScratchReg, ExternalReference::scheduled_exception_address(isolate));
-  __ Ld(a5, MemOperand(kScratchReg));
+  __ LoadWord(a5, MemOperand(kScratchReg));
   __ Branch(&promote_scheduled_exception, ne, a4, Operand(a5),
             Label::Distance::kNear);
 
@@ -3258,7 +3265,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
 
   // HandleScope limit has changed. Delete allocated extensions.
   __ bind(&delete_allocated_handles);
-  __ Sd(s1, MemOperand(s5, kLimitOffset));
+  __ StoreWord(s1, MemOperand(s5, kLimitOffset));
   __ Move(s3, a0);
   __ PrepareCallCFunction(1, s1);
   __ li(a0, ExternalReference::isolate_address(isolate));
@@ -3319,25 +3326,25 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   __ CalcScaledAddress(base, sp, argc, kSystemPointerSizeLog2);
 
   // Reserve space on the stack.
-  __ Sub64(sp, sp, Operand(FCA::kArgsLength * kSystemPointerSize));
+  __ SubWord(sp, sp, Operand(FCA::kArgsLength * kSystemPointerSize));
 
   // kHolder.
-  __ Sd(holder, MemOperand(sp, 0 * kSystemPointerSize));
+  __ StoreWord(holder, MemOperand(sp, 0 * kSystemPointerSize));
 
   // kIsolate.
   __ li(scratch, ExternalReference::isolate_address(masm->isolate()));
-  __ Sd(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 1 * kSystemPointerSize));
 
   // kReturnValueDefaultValue and kReturnValue.
   __ LoadRoot(scratch, RootIndex::kUndefinedValue);
-  __ Sd(scratch, MemOperand(sp, 2 * kSystemPointerSize));
-  __ Sd(scratch, MemOperand(sp, 3 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 2 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 3 * kSystemPointerSize));
 
   // kData.
-  __ Sd(call_data, MemOperand(sp, 4 * kSystemPointerSize));
+  __ StoreWord(call_data, MemOperand(sp, 4 * kSystemPointerSize));
 
   // kNewTarget.
-  __ Sd(scratch, MemOperand(sp, 5 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 5 * kSystemPointerSize));
 
   // Keep a pointer to kHolder (= implicit_args) in a scratch register.
   // We use it below to set up the FunctionCallbackInfo object.
@@ -3354,13 +3361,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   // FunctionCallbackInfo::implicit_args_ (points at kHolder as set up above).
   // Arguments are after the return address (pushed by EnterExitFrame()).
-  __ Sd(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 1 * kSystemPointerSize));
 
   // FunctionCallbackInfo::values_ (points at the first varargs argument passed
   // on the stack).
-  __ Add64(scratch, scratch,
-           Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
-  __ Sd(scratch, MemOperand(sp, 2 * kSystemPointerSize));
+  __ AddWord(scratch, scratch,
+             Operand((FCA::kArgsLength + 1) * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 2 * kSystemPointerSize));
 
   // FunctionCallbackInfo::length_.
   // Stored as int field, 32-bit integers within struct on stack always left
@@ -3371,12 +3378,12 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
   // from the API function here.
   // Note: Unlike on other architectures, this stores the number of slots to
   // drop, not the number of bytes.
-  __ Add64(scratch, argc, Operand(FCA::kArgsLength + 1 /* receiver */));
-  __ Sd(scratch, MemOperand(sp, 4 * kSystemPointerSize));
+  __ AddWord(scratch, argc, Operand(FCA::kArgsLength + 1 /* receiver */));
+  __ StoreWord(scratch, MemOperand(sp, 4 * kSystemPointerSize));
 
   // v8::InvocationCallback's argument.
   DCHECK(!AreAliased(api_function_address, scratch, a0));
-  __ Add64(a0, sp, Operand(1 * kSystemPointerSize));
+  __ AddWord(a0, sp, Operand(1 * kSystemPointerSize));
 
   ExternalReference thunk_ref = ExternalReference::invoke_function_callback();
 
@@ -3417,33 +3424,38 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
 
   // Here and below +1 is for name() pushed after the args_ array.
   using PCA = PropertyCallbackArguments;
-  __ Sub64(sp, sp, (PCA::kArgsLength + 1) * kSystemPointerSize);
-  __ Sd(receiver, MemOperand(sp, (PCA::kThisIndex + 1) * kSystemPointerSize));
+  __ SubWord(sp, sp, (PCA::kArgsLength + 1) * kSystemPointerSize);
+  __ StoreWord(receiver,
+               MemOperand(sp, (PCA::kThisIndex + 1) * kSystemPointerSize));
   __ LoadAnyTaggedField(scratch,
                         FieldMemOperand(callback, AccessorInfo::kDataOffset));
-  __ Sd(scratch, MemOperand(sp, (PCA::kDataIndex + 1) * kSystemPointerSize));
+  __ StoreWord(scratch,
+               MemOperand(sp, (PCA::kDataIndex + 1) * kSystemPointerSize));
   __ LoadRoot(scratch, RootIndex::kUndefinedValue);
-  __ Sd(scratch,
-        MemOperand(sp, (PCA::kReturnValueOffset + 1) * kSystemPointerSize));
-  __ Sd(scratch, MemOperand(sp, (PCA::kReturnValueDefaultValueIndex + 1) *
-                                    kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, (PCA::kReturnValueOffset + 1) *
+                                           kSystemPointerSize));
+  __ StoreWord(scratch,
+               MemOperand(sp, (PCA::kReturnValueDefaultValueIndex + 1) *
+                                  kSystemPointerSize));
   __ li(scratch, ExternalReference::isolate_address(masm->isolate()));
-  __ Sd(scratch, MemOperand(sp, (PCA::kIsolateIndex + 1) * kSystemPointerSize));
-  __ Sd(holder, MemOperand(sp, (PCA::kHolderIndex + 1) * kSystemPointerSize));
+  __ StoreWord(scratch,
+               MemOperand(sp, (PCA::kIsolateIndex + 1) * kSystemPointerSize));
+  __ StoreWord(holder,
+               MemOperand(sp, (PCA::kHolderIndex + 1) * kSystemPointerSize));
   // should_throw_on_error -> false
   DCHECK_EQ(0, Smi::zero().ptr());
-  __ Sd(zero_reg, MemOperand(sp, (PCA::kShouldThrowOnErrorIndex + 1) *
-                                     kSystemPointerSize));
+  __ StoreWord(zero_reg, MemOperand(sp, (PCA::kShouldThrowOnErrorIndex + 1) *
+                                            kSystemPointerSize));
   __ LoadTaggedPointerField(
       scratch, FieldMemOperand(callback, AccessorInfo::kNameOffset));
-  __ Sd(scratch, MemOperand(sp, 0 * kSystemPointerSize));
+  __ StoreWord(scratch, MemOperand(sp, 0 * kSystemPointerSize));
 
   // v8::PropertyCallbackInfo::args_ array and name handle.
   const int kStackUnwindSpace = PropertyCallbackArguments::kArgsLength + 1;
 
   // Load address of v8::PropertyAccessorInfo::args_ array and name handle.
-  __ Move(a0, sp);                                    // a0 = Handle<Name>
-  __ Add64(a1, a0, Operand(1 * kSystemPointerSize));  // a1 = v8::PCI::args_
+  __ Move(a0, sp);                                      // a0 = Handle<Name>
+  __ AddWord(a1, a0, Operand(1 * kSystemPointerSize));  // a1 = v8::PCI::args_
 
   const int kApiStackSpace = 1;
   FrameScope frame_scope(masm, StackFrame::MANUAL);
@@ -3451,15 +3463,15 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
 
   // Create v8::PropertyCallbackInfo object on the stack and initialize
   // it's args_ field.
-  __ Sd(a1, MemOperand(sp, 1 * kSystemPointerSize));
-  __ Add64(a1, sp, Operand(1 * kSystemPointerSize));
+  __ StoreWord(a1, MemOperand(sp, 1 * kSystemPointerSize));
+  __ AddWord(a1, sp, Operand(1 * kSystemPointerSize));
   // a1 = v8::PropertyCallbackInfo&
 
   ExternalReference thunk_ref =
       ExternalReference::invoke_accessor_getter_callback();
 
-  __ Ld(api_function_address,
-        FieldMemOperand(callback, AccessorInfo::kJsGetterOffset));
+  __ LoadWord(api_function_address,
+              FieldMemOperand(callback, AccessorInfo::kJsGetterOffset));
 
   // +3 is to skip prolog, return address and name handle.
   MemOperand return_value_operand(
@@ -3483,11 +3495,12 @@ void Builtins::Generate_DirectCEntry(MacroAssembler* masm) {
   // EnterExitFrame/LeaveExitFrame so they handle stack restoring and we don't
   // have to do that here. Any caller must drop kCArgsSlotsSize stack space
   // after the call.
-  __ Add64(sp, sp, -kCArgsSlotsSize);
+  __ AddWord(sp, sp, -kCArgsSlotsSize);
 
-  __ Sd(ra, MemOperand(sp, kCArgsSlotsSize));  // Store the return address.
-  __ Call(t6);                                 // Call the C++ function.
-  __ Ld(t6, MemOperand(sp, kCArgsSlotsSize));  // Return to calling code.
+  __ StoreWord(ra,
+               MemOperand(sp, kCArgsSlotsSize));  // Store the return address.
+  __ Call(t6);                                    // Call the C++ function.
+  __ LoadWord(t6, MemOperand(sp, kCArgsSlotsSize));  // Return to calling code.
 
   if (FLAG_debug_code && FLAG_enable_slow_asserts) {
     // In case of an error the return address may point to a memory area
@@ -3495,7 +3508,7 @@ void Builtins::Generate_DirectCEntry(MacroAssembler* masm) {
     // this.
     __ Uld(a4, MemOperand(t6));
     __ Assert(ne, AbortReason::kReceivedInvalidReturnAddress, a4,
-              Operand(reinterpret_cast<uint64_t>(kZapValue)));
+              Operand(kZapValue));
   }
 
   __ Jump(t6);
@@ -3519,7 +3532,7 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   const int kDoubleRegsSize = kDoubleSize * DoubleRegister::kNumRegisters;
 
   // Save all double FPU registers before messing with them.
-  __ Sub64(sp, sp, Operand(kDoubleRegsSize));
+  __ SubWord(sp, sp, Operand(kDoubleRegsSize));
   const RegisterConfiguration* config = RegisterConfiguration::Default();
   for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
     int code = config->GetAllocatableDoubleCode(i);
@@ -3530,36 +3543,37 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
 
   // Push saved_regs (needed to populate FrameDescription::registers_).
   // Leave gaps for other registers.
-  __ Sub64(sp, sp, kNumberOfRegisters * kSystemPointerSize);
+  __ SubWord(sp, sp, kNumberOfRegisters * kSystemPointerSize);
   for (int16_t i = kNumberOfRegisters - 1; i >= 0; i--) {
     if ((saved_regs.bits() & (1 << i)) != 0) {
-      __ Sd(ToRegister(i), MemOperand(sp, kSystemPointerSize * i));
+      __ StoreWord(ToRegister(i), MemOperand(sp, kSystemPointerSize * i));
     }
   }
 
   __ li(a2,
         ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate));
-  __ Sd(fp, MemOperand(a2));
+  __ StoreWord(fp, MemOperand(a2));
 
   const int kSavedRegistersAreaSize =
       (kNumberOfRegisters * kSystemPointerSize) + kDoubleRegsSize;
 
-  // Get the address of the location in the code object (a3) (return
+  // Get the address of the location in the code object (a2) (return
   // address for lazy deoptimization) and compute the fp-to-sp delta in
   // register a4.
   __ Move(a2, ra);
-  __ Add64(a3, sp, Operand(kSavedRegistersAreaSize));
+  __ AddWord(a3, sp, Operand(kSavedRegistersAreaSize));
 
-  __ Sub64(a3, fp, a3);
+  __ SubWord(a3, fp, a3);
 
   // Allocate a new deoptimizer object.
   __ PrepareCallCFunction(5, a4);
   // Pass five arguments, according to n64 ABI.
   __ Move(a0, zero_reg);
   Label context_check;
-  __ Ld(a1, MemOperand(fp, CommonFrameConstants::kContextOrFrameTypeOffset));
+  __ LoadWord(a1,
+              MemOperand(fp, CommonFrameConstants::kContextOrFrameTypeOffset));
   __ JumpIfSmi(a1, &context_check);
-  __ Ld(a0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ LoadWord(a0, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
   __ bind(&context_check);
   __ li(a1, Operand(static_cast<int64_t>(deopt_kind)));
   // a2: code object address
@@ -3574,7 +3588,7 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
 
   // Preserve "deoptimizer" object in register a0 and get the input
   // frame descriptor pointer to a1 (deoptimizer->input_);
-  __ Ld(a1, MemOperand(a0, Deoptimizer::input_offset()));
+  __ LoadWord(a1, MemOperand(a0, Deoptimizer::input_offset()));
 
   // Copy core registers into FrameDescription::registers_[kNumRegisters].
   DCHECK_EQ(Register::kNumRegisters, kNumberOfRegisters);
@@ -3582,11 +3596,11 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
     int offset =
         (i * kSystemPointerSize) + FrameDescription::registers_offset();
     if ((saved_regs.bits() & (1 << i)) != 0) {
-      __ Ld(a2, MemOperand(sp, i * kSystemPointerSize));
-      __ Sd(a2, MemOperand(a1, offset));
+      __ LoadWord(a2, MemOperand(sp, i * kSystemPointerSize));
+      __ StoreWord(a2, MemOperand(a1, offset));
     } else if (FLAG_debug_code) {
       __ li(a2, kDebugZapValue);
-      __ Sd(a2, MemOperand(a1, offset));
+      __ StoreWord(a2, MemOperand(a1, offset));
     }
   }
 
@@ -3603,24 +3617,24 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   }
 
   // Remove the saved registers from the stack.
-  __ Add64(sp, sp, Operand(kSavedRegistersAreaSize));
+  __ AddWord(sp, sp, Operand(kSavedRegistersAreaSize));
 
   // Compute a pointer to the unwinding limit in register a2; that is
   // the first stack slot not part of the input frame.
-  __ Ld(a2, MemOperand(a1, FrameDescription::frame_size_offset()));
-  __ Add64(a2, a2, sp);
+  __ LoadWord(a2, MemOperand(a1, FrameDescription::frame_size_offset()));
+  __ AddWord(a2, a2, sp);
 
   // Unwind the stack down to - but not including - the unwinding
   // limit and copy the contents of the activation frame to the input
   // frame description.
-  __ Add64(a3, a1, Operand(FrameDescription::frame_content_offset()));
+  __ AddWord(a3, a1, Operand(FrameDescription::frame_content_offset()));
   Label pop_loop;
   Label pop_loop_header;
   __ BranchShort(&pop_loop_header);
   __ bind(&pop_loop);
   __ pop(a4);
-  __ Sd(a4, MemOperand(a3, 0));
-  __ Add64(a3, a3, sizeof(uint64_t));
+  __ StoreWord(a4, MemOperand(a3, 0));
+  __ AddWord(a3, a3, kSystemPointerSize);
   __ bind(&pop_loop_header);
   __ Branch(&pop_loop, ne, a2, Operand(sp), Label::Distance::kNear);
   // Compute the output frame in the deoptimizer.
@@ -3634,34 +3648,35 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   }
   __ pop(a0);  // Restore deoptimizer object (class Deoptimizer).
 
-  __ Ld(sp, MemOperand(a0, Deoptimizer::caller_frame_top_offset()));
+  __ LoadWord(sp, MemOperand(a0, Deoptimizer::caller_frame_top_offset()));
 
   // Replace the current (input) frame with the output frames.
   Label outer_push_loop, inner_push_loop, outer_loop_header, inner_loop_header;
   // Outer loop state: a4 = current "FrameDescription** output_",
   // a1 = one past the last FrameDescription**.
   __ Lw(a1, MemOperand(a0, Deoptimizer::output_count_offset()));
-  __ Ld(a4, MemOperand(a0, Deoptimizer::output_offset()));  // a4 is output_.
+  __ LoadWord(a4,
+              MemOperand(a0, Deoptimizer::output_offset()));  // a4 is output_.
   __ CalcScaledAddress(a1, a4, a1, kSystemPointerSizeLog2);
   __ BranchShort(&outer_loop_header);
   __ bind(&outer_push_loop);
   // Inner loop state: a2 = current FrameDescription*, a3 = loop index.
-  __ Ld(a2, MemOperand(a4, 0));  // output_[ix]
-  __ Ld(a3, MemOperand(a2, FrameDescription::frame_size_offset()));
+  __ LoadWord(a2, MemOperand(a4, 0));  // output_[ix]
+  __ LoadWord(a3, MemOperand(a2, FrameDescription::frame_size_offset()));
   __ BranchShort(&inner_loop_header);
   __ bind(&inner_push_loop);
-  __ Sub64(a3, a3, Operand(sizeof(uint64_t)));
-  __ Add64(a6, a2, Operand(a3));
-  __ Ld(a7, MemOperand(a6, FrameDescription::frame_content_offset()));
+  __ SubWord(a3, a3, Operand(kSystemPointerSize));
+  __ AddWord(a6, a2, Operand(a3));
+  __ LoadWord(a7, MemOperand(a6, FrameDescription::frame_content_offset()));
   __ push(a7);
   __ bind(&inner_loop_header);
   __ Branch(&inner_push_loop, ne, a3, Operand(zero_reg));
 
-  __ Add64(a4, a4, Operand(kSystemPointerSize));
+  __ AddWord(a4, a4, Operand(kSystemPointerSize));
   __ bind(&outer_loop_header);
   __ Branch(&outer_push_loop, lt, a4, Operand(a1));
 
-  __ Ld(a1, MemOperand(a0, Deoptimizer::input_offset()));
+  __ LoadWord(a1, MemOperand(a0, Deoptimizer::input_offset()));
   for (int i = 0; i < config->num_allocatable_double_registers(); ++i) {
     int code = config->GetAllocatableDoubleCode(i);
     const DoubleRegister fpu_reg = DoubleRegister::from_code(code);
@@ -3670,9 +3685,9 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
   }
 
   // Push pc and continuation from the last output frame.
-  __ Ld(a6, MemOperand(a2, FrameDescription::pc_offset()));
+  __ LoadWord(a6, MemOperand(a2, FrameDescription::pc_offset()));
   __ push(a6);
-  __ Ld(a6, MemOperand(a2, FrameDescription::continuation_offset()));
+  __ LoadWord(a6, MemOperand(a2, FrameDescription::continuation_offset()));
   __ push(a6);
 
   // Technically restoring 't3' should work unless zero_reg is also restored
@@ -3684,7 +3699,7 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
     int offset =
         (i * kSystemPointerSize) + FrameDescription::registers_offset();
     if ((restored_regs.bits() & (1 << i)) != 0) {
-      __ Ld(ToRegister(i), MemOperand(t3, offset));
+      __ LoadWord(ToRegister(i), MemOperand(t3, offset));
     }
   }
 
@@ -3718,7 +3733,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
 
   // Get function from the frame.
   Register closure = a1;
-  __ Ld(closure, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ LoadWord(closure, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
 
   // Get the Code object from the shared function info.
   Register code_obj = s1;
@@ -3777,8 +3792,9 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   __ SmiUntag(kInterpreterBytecodeOffsetRegister,
               MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
   // Replace BytecodeOffset with the feedback vector.
-  __ Sd(feedback_vector,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
+  __ StoreWord(
+      feedback_vector,
+      MemOperand(fp, InterpreterFrameConstants::kBytecodeOffsetFromFp));
   feedback_vector = no_reg;
 
   // Compute baseline pc for bytecode offset.
@@ -3806,14 +3822,14 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
                       kFunctionEntryBytecodeOffset));
   }
 
-  __ Sub64(kInterpreterBytecodeOffsetRegister,
-           kInterpreterBytecodeOffsetRegister,
-           (BytecodeArray::kHeaderSize - kHeapObjectTag));
+  __ SubWord(kInterpreterBytecodeOffsetRegister,
+             kInterpreterBytecodeOffsetRegister,
+             (BytecodeArray::kHeaderSize - kHeapObjectTag));
 
   __ bind(&valid_bytecode_offset);
   // Get bytecode array from the stack frame.
-  __ Ld(kInterpreterBytecodeArrayRegister,
-        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+  __ LoadWord(kInterpreterBytecodeArrayRegister,
+              MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
   __ Push(kInterpreterAccumulatorRegister);
   {
     Register arg_reg_1 = a0;
@@ -3825,17 +3841,21 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ Add64(code_obj, code_obj, kReturnRegister0);
+  __ AddWord(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
-    __ Ld(kInterpreterBytecodeArrayRegister,
-          MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
+    // Reset the OSR loop nesting depth to disarm back edges.
+    // TODO(pthier): Separate baseline Sparkplug from TF arming and don't disarm
+    // Sparkplug here.
+    __ LoadWord(
+        kInterpreterBytecodeArrayRegister,
+        MemOperand(fp, InterpreterFrameConstants::kBytecodeArrayFromFp));
     ResetBytecodeAge(masm, kInterpreterBytecodeArrayRegister);
     Generate_OSREntry(masm, code_obj,
                       Operand(Code::kHeaderSize - kHeapObjectTag));
   } else {
-    __ Add64(code_obj, code_obj, Code::kHeaderSize - kHeapObjectTag);
+    __ AddWord(code_obj, code_obj, Code::kHeaderSize - kHeapObjectTag);
     __ Jump(code_obj);
   }
   __ Trap();  // Unreachable.
@@ -3844,7 +3864,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ bind(&function_entry_bytecode);
     // If the bytecode offset is kFunctionEntryOffset, get the start address of
     // the first bytecode.
-    __ li(kInterpreterBytecodeOffsetRegister, Operand(int64_t(0)));
+    __ li(kInterpreterBytecodeOffsetRegister, Operand(0));
     if (next_bytecode) {
       __ li(get_baseline_pc,
             ExternalReference::baseline_pc_for_bytecode_offset());
@@ -3887,8 +3907,8 @@ void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) {
   // - Leave the frame.
   // - Restart the frame by calling the function.
 
-  __ Ld(a1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
-  __ Ld(a0, MemOperand(fp, StandardFrameConstants::kArgCOffset));
+  __ LoadWord(a1, MemOperand(fp, StandardFrameConstants::kFunctionOffset));
+  __ LoadWord(a0, MemOperand(fp, StandardFrameConstants::kArgCOffset));
 
   // Pop return address and frame.
   __ LeaveFrame(StackFrame::INTERPRETED);
@@ -3902,5 +3922,3 @@ void Builtins::Generate_RestartFrameTrampoline(MacroAssembler* masm) {
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/codegen/assembler-arch.h b/src/codegen/assembler-arch.h
index 2e1b56c467e..9655c000ff7 100644
--- a/src/codegen/assembler-arch.h
+++ b/src/codegen/assembler-arch.h
@@ -25,8 +25,8 @@
 #include "src/codegen/loong64/assembler-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/assembler-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/assembler-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/assembler-riscv.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/codegen/assembler-inl.h b/src/codegen/assembler-inl.h
index 084f12cc7ef..f76995b9e80 100644
--- a/src/codegen/assembler-inl.h
+++ b/src/codegen/assembler-inl.h
@@ -25,8 +25,8 @@
 #include "src/codegen/loong64/assembler-loong64-inl.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/assembler-s390-inl.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/assembler-riscv64-inl.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/assembler-riscv-inl.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/codegen/constant-pool.cc b/src/codegen/constant-pool.cc
index 4f8a6286a46..f73cf5cf7d3 100644
--- a/src/codegen/constant-pool.cc
+++ b/src/codegen/constant-pool.cc
@@ -459,7 +459,7 @@ void ConstantPool::MaybeCheck() {
 
 #endif  // defined(V8_TARGET_ARCH_ARM64)
 
-#if defined(V8_TARGET_ARCH_RISCV64)
+#if defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_RISCV32)
 
 // Constant Pool.
 
@@ -706,7 +706,7 @@ void ConstantPool::MaybeCheck() {
   }
 }
 
-#endif  // defined(V8_TARGET_ARCH_RISCV64)
+#endif  // defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_RISCV32)
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/codegen/constant-pool.h b/src/codegen/constant-pool.h
index 76956381a26..16ddc33c6fc 100644
--- a/src/codegen/constant-pool.h
+++ b/src/codegen/constant-pool.h
@@ -163,7 +163,8 @@ class ConstantPoolBuilder {
 
 #endif  // defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_PPC64)
 
-#if defined(V8_TARGET_ARCH_ARM64) || defined(V8_TARGET_ARCH_RISCV64)
+#if defined(V8_TARGET_ARCH_ARM64) || defined(V8_TARGET_ARCH_RISCV64) || \
+    defined(V8_TARGET_ARCH_RISCV32)
 
 class ConstantPoolKey {
  public:
@@ -345,7 +346,8 @@ class ConstantPool {
   int blocked_nesting_ = 0;
 };
 
-#endif  // defined(V8_TARGET_ARCH_ARM64)
+#endif  // defined(V8_TARGET_ARCH_ARM64) || defined(V8_TARGET_ARCH_RISCV64) ||
+        // defined(V8_TARGET_ARCH_RISCV32)
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/codegen/constants-arch.h b/src/codegen/constants-arch.h
index 7eb32bafde4..b5c65143311 100644
--- a/src/codegen/constants-arch.h
+++ b/src/codegen/constants-arch.h
@@ -23,8 +23,8 @@
 #include "src/codegen/s390/constants-s390.h"
 #elif V8_TARGET_ARCH_X64
 #include "src/codegen/x64/constants-x64.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/constants-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/constants-riscv.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/codegen/cpu-features.h b/src/codegen/cpu-features.h
index fc85f064fd6..8b177826b67 100644
--- a/src/codegen/cpu-features.h
+++ b/src/codegen/cpu-features.h
@@ -76,6 +76,10 @@ enum CpuFeature {
   FPU,
   FP64FPU,
   RISCV_SIMD,
+#elif V8_TARGET_ARCH_RISCV32
+  FPU,
+  FP64FPU,
+  RISCV_SIMD,
 #endif
 
   NUMBER_OF_CPU_FEATURES
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 5120dd6c056..9baa485add0 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -757,7 +757,7 @@ ExternalReference ExternalReference::invoke_accessor_getter_callback() {
 #define re_stack_check_func RegExpMacroAssemblerLOONG64::CheckStackGuardState
 #elif V8_TARGET_ARCH_S390
 #define re_stack_check_func RegExpMacroAssemblerS390::CheckStackGuardState
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 #define re_stack_check_func RegExpMacroAssemblerRISCV::CheckStackGuardState
 #else
 UNREACHABLE();
diff --git a/src/codegen/interface-descriptors-inl.h b/src/codegen/interface-descriptors-inl.h
index 5b8337b6cab..8c8f12f3a59 100644
--- a/src/codegen/interface-descriptors-inl.h
+++ b/src/codegen/interface-descriptors-inl.h
@@ -29,8 +29,8 @@
 #include "src/codegen/mips/interface-descriptors-mips-inl.h"
 #elif V8_TARGET_ARCH_LOONG64
 #include "src/codegen/loong64/interface-descriptors-loong64-inl.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/interface-descriptors-riscv64-inl.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/interface-descriptors-riscv-inl.h"
 #else
 #error Unsupported target architecture.
 #endif
@@ -336,7 +336,7 @@ constexpr auto BaselineOutOfLinePrologueDescriptor::registers() {
 #if V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_ARM ||       \
     V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64 || V8_TARGET_ARCH_S390 ||      \
     V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_MIPS || \
-    V8_TARGET_ARCH_LOONG64
+    V8_TARGET_ARCH_LOONG64 || V8_TARGET_ARCH_RISCV32
   return RegisterArray(
       kContextRegister, kJSFunctionRegister, kJavaScriptCallArgCountRegister,
       kJavaScriptCallExtraArg1Register, kJavaScriptCallNewTargetRegister,
@@ -357,7 +357,7 @@ constexpr auto BaselineLeaveFrameDescriptor::registers() {
 #if V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_ARM64 ||      \
     V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64 ||       \
     V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_MIPS64 || \
-    V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_LOONG64
+    V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_LOONG64 || V8_TARGET_ARCH_RISCV32
   return RegisterArray(ParamsSizeRegister(), WeightRegister());
 #else
   return DefaultRegisterArray();
diff --git a/src/codegen/macro-assembler.h b/src/codegen/macro-assembler.h
index 8f1668b0c2c..08f47638fd8 100644
--- a/src/codegen/macro-assembler.h
+++ b/src/codegen/macro-assembler.h
@@ -63,9 +63,9 @@ enum class SmiCheck { kOmit, kInline };
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/constants-s390.h"
 #include "src/codegen/s390/macro-assembler-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/constants-riscv64.h"
-#include "src/codegen/riscv64/macro-assembler-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/constants-riscv.h"
+#include "src/codegen/riscv/macro-assembler-riscv.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/codegen/register-arch.h b/src/codegen/register-arch.h
index c9c3a984071..8ec621ce177 100644
--- a/src/codegen/register-arch.h
+++ b/src/codegen/register-arch.h
@@ -25,8 +25,8 @@
 #include "src/codegen/loong64/register-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/register-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/register-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/register-riscv.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/codegen/register-configuration.cc b/src/codegen/register-configuration.cc
index 78a8ec41bf7..0c589fd157c 100644
--- a/src/codegen/register-configuration.cc
+++ b/src/codegen/register-configuration.cc
@@ -19,7 +19,7 @@ static const int kMaxAllocatableGeneralRegisterCount =
     ALLOCATABLE_GENERAL_REGISTERS(REGISTER_COUNT) 0;
 static const int kMaxAllocatableDoubleRegisterCount =
     ALLOCATABLE_DOUBLE_REGISTERS(REGISTER_COUNT) 0;
-#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
 static const int kMaxAllocatableSIMD128RegisterCount =
     ALLOCATABLE_SIMD128_REGISTERS(REGISTER_COUNT) 0;
 #endif
@@ -38,16 +38,17 @@ static const int kAllocatableNoVFP32DoubleCodes[] = {
 #endif  // V8_TARGET_ARCH_ARM
 #undef REGISTER_CODE
 
-#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
 static const int kAllocatableSIMD128Codes[] = {
-#if V8_TARGET_ARCH_RISCV64
+#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_RISCV32
 #define REGISTER_CODE(R) kVRCode_##R,
 #else
 #define REGISTER_CODE(R) kSimd128Code_##R,
 #endif
     ALLOCATABLE_SIMD128_REGISTERS(REGISTER_CODE)};
 #undef REGISTER_CODE
-#endif  // V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#endif  // V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 ||
+        // V8_TARGET_ARCH_PPC64
 
 static_assert(RegisterConfiguration::kMaxGeneralRegisters >=
               Register::kNumRegisters);
@@ -95,6 +96,8 @@ static int get_num_allocatable_double_registers() {
       kMaxAllocatableDoubleRegisterCount;
 #elif V8_TARGET_ARCH_RISCV64
       kMaxAllocatableDoubleRegisterCount;
+#elif V8_TARGET_ARCH_RISCV32
+      kMaxAllocatableDoubleRegisterCount;
 #else
 #error Unsupported target architecture.
 #endif
@@ -104,7 +107,7 @@ static int get_num_allocatable_double_registers() {
 
 static int get_num_allocatable_simd128_registers() {
   return
-#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
       kMaxAllocatableSIMD128RegisterCount;
 #else
       0;
@@ -125,7 +128,7 @@ static const int* get_allocatable_double_codes() {
 
 static const int* get_allocatable_simd128_codes() {
   return
-#if V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
+#if V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_PPC64
       kAllocatableSIMD128Codes;
 #else
       kAllocatableDoubleCodes;
diff --git a/src/codegen/reglist.h b/src/codegen/reglist.h
index e9fc5728a99..ea9358b0c46 100644
--- a/src/codegen/reglist.h
+++ b/src/codegen/reglist.h
@@ -23,8 +23,8 @@
 #include "src/codegen/loong64/reglist-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/codegen/s390/reglist-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/codegen/riscv64/reglist-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/codegen/riscv/reglist-riscv.h"
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index f13d21ab44f..2cf33573ce8 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -309,10 +309,11 @@ bool RelocInfo::OffHeapTargetIsCodedSpecially() {
 #if defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_ARM64) || \
     defined(V8_TARGET_ARCH_X64)
   return false;
-#elif defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_MIPS) || \
-    defined(V8_TARGET_ARCH_MIPS64) || defined(V8_TARGET_ARCH_PPC) ||  \
-    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_S390) ||  \
-    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64)
+#elif defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_MIPS) ||     \
+    defined(V8_TARGET_ARCH_MIPS64) || defined(V8_TARGET_ARCH_PPC) ||      \
+    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_S390) ||      \
+    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64) || \
+    defined(V8_TARGET_ARCH_RISCV32)
   return true;
 #endif
 }
diff --git a/src/codegen/reloc-info.h b/src/codegen/reloc-info.h
index 98adfb97c59..8f6d8090068 100644
--- a/src/codegen/reloc-info.h
+++ b/src/codegen/reloc-info.h
@@ -72,7 +72,8 @@ class RelocInfo {
     EXTERNAL_REFERENCE,  // The address of an external C++ function.
     INTERNAL_REFERENCE,  // An address inside the same function.
 
-    // Encoded internal reference, used only on RISCV64, MIPS, MIPS64 and PPC.
+    // Encoded internal reference, used only on RISCV64, RISCV32, MIPS, MIPS64
+    // and PPC.
     INTERNAL_REFERENCE_ENCODED,
 
     // An off-heap instruction stream target. See http://goo.gl/Z2HUiM.
diff --git a/src/codegen/riscv64/assembler-riscv64-inl.h b/src/codegen/riscv/assembler-riscv-inl.h
similarity index 89%
rename from src/codegen/riscv64/assembler-riscv64-inl.h
rename to src/codegen/riscv/assembler-riscv-inl.h
index f463c9a9873..325a52b78db 100644
--- a/src/codegen/riscv64/assembler-riscv64-inl.h
+++ b/src/codegen/riscv/assembler-riscv-inl.h
@@ -32,11 +32,11 @@
 // modified significantly by Google Inc.
 // Copyright 2021 the V8 project authors. All rights reserved.
 
-#ifndef V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_INL_H_
-#define V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_INL_H_
+#ifndef V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_
+#define V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_
 
+#include "src/codegen/assembler-arch.h"
 #include "src/codegen/assembler.h"
-#include "src/codegen/riscv64/assembler-riscv64.h"
 #include "src/debug/debug.h"
 #include "src/objects/objects-inl.h"
 
@@ -45,15 +45,10 @@ namespace internal {
 
 bool CpuFeatures::SupportsOptimizer() { return IsSupported(FPU); }
 
-// -----------------------------------------------------------------------------
-// Operand and MemOperand.
-
-bool Operand::is_reg() const { return rm_.is_valid(); }
-
-int64_t Operand::immediate() const {
-  DCHECK(!is_reg());
-  DCHECK(!IsHeapObjectRequest());
-  return value_.immediate;
+void Assembler::CheckBuffer() {
+  if (buffer_space() <= kGap) {
+    GrowBuffer();
+  }
 }
 
 // -----------------------------------------------------------------------------
@@ -91,7 +86,11 @@ Address RelocInfo::target_address_address() {
   // place, ready to be patched with the target. After jump optimization,
   // that is the address of the instruction that follows J/JAL/JR/JALR
   // instruction.
+#ifdef V8_TARGET_ARCH_RISCV64
   return pc_ + Assembler::kInstructionsFor64BitConstant * kInstrSize;
+#elif defined(V8_TARGET_ARCH_RISCV32)
+  return pc_ + Assembler::kInstructionsFor32BitConstant * kInstrSize;
+#endif
 }
 
 Address RelocInfo::constant_pool_entry_address() { UNREACHABLE(); }
@@ -142,7 +141,11 @@ int Assembler::deserialization_special_target_size(
 
 void Assembler::set_target_internal_reference_encoded_at(Address pc,
                                                          Address target) {
+#ifdef V8_TARGET_ARCH_RISCV64
   set_target_value_at(pc, static_cast<uint64_t>(target));
+#elif defined(V8_TARGET_ARCH_RISCV32)
+  set_target_value_at(pc, static_cast<uint32_t>(target));
+#endif
 }
 
 void Assembler::deserialization_set_target_internal_reference_at(
@@ -279,49 +282,9 @@ void RelocInfo::WipeOut() {
   }
 }
 
-// -----------------------------------------------------------------------------
-// Assembler.
-
-void Assembler::CheckBuffer() {
-  if (buffer_space() <= kGap) {
-    GrowBuffer();
-  }
-}
-
-template <typename T>
-void Assembler::EmitHelper(T x) {
-  *reinterpret_cast<T*>(pc_) = x;
-  pc_ += sizeof(x);
-}
-
-void Assembler::emit(Instr x) {
-  if (!is_buffer_growth_blocked()) {
-    CheckBuffer();
-  }
-  DEBUG_PRINTF("%p: ", pc_);
-  disassembleInstr(x);
-  EmitHelper(x);
-  CheckTrampolinePoolQuick();
-}
-
-void Assembler::emit(ShortInstr x) {
-  if (!is_buffer_growth_blocked()) {
-    CheckBuffer();
-  }
-  DEBUG_PRINTF("%p: ", pc_);
-  disassembleInstr(x);
-  EmitHelper(x);
-  CheckTrampolinePoolQuick();
-}
-
-void Assembler::emit(uint64_t data) {
-  if (!is_buffer_growth_blocked()) CheckBuffer();
-  EmitHelper(data);
-}
-
 EnsureSpace::EnsureSpace(Assembler* assembler) { assembler->CheckBuffer(); }
 
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_INL_H_
+#endif  // V8_CODEGEN_RISCV_ASSEMBLER_RISCV_INL_H_
diff --git a/src/codegen/riscv/assembler-riscv.cc b/src/codegen/riscv/assembler-riscv.cc
new file mode 100644
index 00000000000..f8ac0dba8c3
--- /dev/null
+++ b/src/codegen/riscv/assembler-riscv.cc
@@ -0,0 +1,1927 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2021 the V8 project authors. All rights reserved.
+
+#include "src/codegen/riscv/assembler-riscv.h"
+
+#include "src/base/cpu.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/safepoint-table.h"
+#include "src/codegen/string-constants.h"
+#include "src/deoptimizer/deoptimizer.h"
+#include "src/diagnostics/disasm.h"
+#include "src/diagnostics/disassembler.h"
+#include "src/objects/heap-number-inl.h"
+
+namespace v8 {
+namespace internal {
+// Get the CPU features enabled by the build. For cross compilation the
+// preprocessor symbols CAN_USE_FPU_INSTRUCTIONS
+// can be defined to enable FPU instructions when building the
+// snapshot.
+static unsigned CpuFeaturesImpliedByCompiler() {
+  unsigned answer = 0;
+#ifdef CAN_USE_FPU_INSTRUCTIONS
+  answer |= 1u << FPU;
+#endif  // def CAN_USE_FPU_INSTRUCTIONS
+
+#if (defined CAN_USE_RVV_INSTRUCTIONS)
+  answer |= 1u << RISCV_SIMD;
+#endif  // def CAN_USE_RVV_INSTRUCTIONS
+  return answer;
+}
+
+bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(RISCV_SIMD); }
+
+void CpuFeatures::ProbeImpl(bool cross_compile) {
+  supported_ |= CpuFeaturesImpliedByCompiler();
+  // Only use statically determined features for cross compile (snapshot).
+  if (cross_compile) return;
+  // Probe for additional features at runtime.
+  base::CPU cpu;
+  if (cpu.has_fpu()) supported_ |= 1u << FPU;
+  if (cpu.has_rvv()) supported_ |= 1u << RISCV_SIMD;
+  // Set a static value on whether SIMD is supported.
+  // This variable is only used for certain archs to query SupportWasmSimd128()
+  // at runtime in builtins using an extern ref. Other callers should use
+  // CpuFeatures::SupportWasmSimd128().
+  CpuFeatures::supports_wasm_simd_128_ = CpuFeatures::SupportsWasmSimd128();
+}
+
+void CpuFeatures::PrintTarget() {}
+void CpuFeatures::PrintFeatures() {}
+int ToNumber(Register reg) {
+  DCHECK(reg.is_valid());
+  const int kNumbers[] = {
+      0,   // zero_reg
+      1,   // ra
+      2,   // sp
+      3,   // gp
+      4,   // tp
+      5,   // t0
+      6,   // t1
+      7,   // t2
+      8,   // s0/fp
+      9,   // s1
+      10,  // a0
+      11,  // a1
+      12,  // a2
+      13,  // a3
+      14,  // a4
+      15,  // a5
+      16,  // a6
+      17,  // a7
+      18,  // s2
+      19,  // s3
+      20,  // s4
+      21,  // s5
+      22,  // s6
+      23,  // s7
+      24,  // s8
+      25,  // s9
+      26,  // s10
+      27,  // s11
+      28,  // t3
+      29,  // t4
+      30,  // t5
+      31,  // t6
+  };
+  return kNumbers[reg.code()];
+}
+
+Register ToRegister(int num) {
+  DCHECK(num >= 0 && num < kNumRegisters);
+  const Register kRegisters[] = {
+      zero_reg, ra, sp, gp, tp, t0, t1, t2, fp, s1, a0,  a1,  a2, a3, a4, a5,
+      a6,       a7, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, t3, t4, t5, t6};
+  return kRegisters[num];
+}
+
+// -----------------------------------------------------------------------------
+// Implementation of RelocInfo.
+
+const int RelocInfo::kApplyMask =
+    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE) |
+    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE_ENCODED) |
+    RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
+
+bool RelocInfo::IsCodedSpecially() {
+  // The deserializer needs to know whether a pointer is specially coded.  Being
+  // specially coded on RISC-V means that it is a lui/addi instruction, and that
+  // is always the case inside code objects.
+  return true;
+}
+
+bool RelocInfo::IsInConstantPool() { return false; }
+
+uint32_t RelocInfo::wasm_call_tag() const {
+  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);
+  return static_cast<uint32_t>(
+      Assembler::target_address_at(pc_, constant_pool_));
+}
+
+// -----------------------------------------------------------------------------
+// Implementation of Operand and MemOperand.
+// See assembler-riscv-inl.h for inlined constructors.
+
+Operand::Operand(Handle<HeapObject> handle)
+    : rm_(no_reg), rmode_(RelocInfo::FULL_EMBEDDED_OBJECT) {
+  value_.immediate = static_cast<intptr_t>(handle.address());
+}
+
+Operand Operand::EmbeddedNumber(double value) {
+  int32_t smi;
+  if (DoubleToSmiInteger(value, &smi)) return Operand(Smi::FromInt(smi));
+  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
+  result.is_heap_object_request_ = true;
+  result.value_.heap_object_request = HeapObjectRequest(value);
+  return result;
+}
+
+Operand Operand::EmbeddedStringConstant(const StringConstantBase* str) {
+  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
+  result.is_heap_object_request_ = true;
+  result.value_.heap_object_request = HeapObjectRequest(str);
+  return result;
+}
+
+MemOperand::MemOperand(Register rm, int32_t offset) : Operand(rm) {
+  offset_ = offset;
+}
+
+MemOperand::MemOperand(Register rm, int32_t unit, int32_t multiplier,
+                       OffsetAddend offset_addend)
+    : Operand(rm) {
+  offset_ = unit * multiplier + offset_addend;
+}
+
+void Assembler::AllocateAndInstallRequestedHeapObjects(Isolate* isolate) {
+  DCHECK_IMPLIES(isolate == nullptr, heap_object_requests_.empty());
+  for (auto& request : heap_object_requests_) {
+    Handle<HeapObject> object;
+    switch (request.kind()) {
+      case HeapObjectRequest::kHeapNumber:
+        object = isolate->factory()->NewHeapNumber<AllocationType::kOld>(
+            request.heap_number());
+        break;
+      case HeapObjectRequest::kStringConstant:
+        const StringConstantBase* str = request.string();
+        CHECK_NOT_NULL(str);
+        object = str->AllocateStringConstant(isolate);
+        break;
+    }
+    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();
+    set_target_value_at(pc, reinterpret_cast<uintptr_t>(object.location()));
+  }
+}
+
+// -----------------------------------------------------------------------------
+// Specific instructions, constants, and masks.
+
+Assembler::Assembler(const AssemblerOptions& options,
+                     std::unique_ptr<AssemblerBuffer> buffer)
+    : AssemblerBase(options, std::move(buffer)),
+      VU(this),
+      scratch_register_list_({t3, t5}),
+      constpool_(this) {
+  reloc_info_writer.Reposition(buffer_start_ + buffer_->size(), pc_);
+
+  last_trampoline_pool_end_ = 0;
+  no_trampoline_pool_before_ = 0;
+  trampoline_pool_blocked_nesting_ = 0;
+  // We leave space (16 * kTrampolineSlotsSize)
+  // for BlockTrampolinePoolScope buffer.
+  next_buffer_check_ = FLAG_force_long_branches
+                           ? kMaxInt
+                           : kMaxBranchOffset - kTrampolineSlotsSize * 16;
+  internal_trampoline_exception_ = false;
+  last_bound_pos_ = 0;
+
+  trampoline_emitted_ = FLAG_force_long_branches;
+  unbound_labels_count_ = 0;
+  block_buffer_growth_ = false;
+}
+
+void Assembler::AbortedCodeGeneration() { constpool_.Clear(); }
+Assembler::~Assembler() { CHECK(constpool_.IsEmpty()); }
+
+void Assembler::GetCode(Isolate* isolate, CodeDesc* desc,
+                        SafepointTableBuilder* safepoint_table_builder,
+                        int handler_table_offset) {
+  // As a crutch to avoid having to add manual Align calls wherever we use a
+  // raw workflow to create Code objects (mostly in tests), add another Align
+  // call here. It does no harm - the end of the Code object is aligned to the
+  // (larger) kCodeAlignment anyways.
+  // TODO(jgruber): Consider moving responsibility for proper alignment to
+  // metadata table builders (safepoint, handler, constant pool, code
+  // comments).
+  DataAlign(Code::kMetadataAlignment);
+
+  ForceConstantPoolEmissionWithoutJump();
+
+  int code_comments_size = WriteCodeComments();
+
+  DCHECK(pc_ <= reloc_info_writer.pos());  // No overlap.
+
+  AllocateAndInstallRequestedHeapObjects(isolate);
+
+  // Set up code descriptor.
+  // TODO(jgruber): Reconsider how these offsets and sizes are maintained up to
+  // this point to make CodeDesc initialization less fiddly.
+
+  static constexpr int kConstantPoolSize = 0;
+  const int instruction_size = pc_offset();
+  const int code_comments_offset = instruction_size - code_comments_size;
+  const int constant_pool_offset = code_comments_offset - kConstantPoolSize;
+  const int handler_table_offset2 = (handler_table_offset == kNoHandlerTable)
+                                        ? constant_pool_offset
+                                        : handler_table_offset;
+  const int safepoint_table_offset =
+      (safepoint_table_builder == kNoSafepointTable)
+          ? handler_table_offset2
+          : safepoint_table_builder->safepoint_table_offset();
+  const int reloc_info_offset =
+      static_cast<int>(reloc_info_writer.pos() - buffer_->start());
+  CodeDesc::Initialize(desc, this, safepoint_table_offset,
+                       handler_table_offset2, constant_pool_offset,
+                       code_comments_offset, reloc_info_offset);
+}
+
+void Assembler::Align(int m) {
+  DCHECK(m >= 4 && base::bits::IsPowerOfTwo(m));
+  while ((pc_offset() & (m - 1)) != 0) {
+    NOP();
+  }
+}
+
+void Assembler::CodeTargetAlign() {
+  // No advantage to aligning branch/call targets to more than
+  // single instruction, that I am aware of.
+  Align(4);
+}
+
+// Labels refer to positions in the (to be) generated code.
+// There are bound, linked, and unused labels.
+//
+// Bound labels refer to known positions in the already
+// generated code. pos() is the position the label refers to.
+//
+// Linked labels refer to unknown positions in the code
+// to be generated; pos() is the position of the last
+// instruction using the label.
+
+// The link chain is terminated by a value in the instruction of 0,
+// which is an otherwise illegal value (branch 0 is inf loop). When this case
+// is detected, return an position of -1, an otherwise illegal position.
+const int kEndOfChain = -1;
+const int kEndOfJumpChain = 0;
+
+int Assembler::target_at(int pos, bool is_internal) {
+  if (is_internal) {
+    uintptr_t* p = reinterpret_cast<uintptr_t*>(buffer_start_ + pos);
+    uintptr_t address = *p;
+    if (address == kEndOfJumpChain) {
+      return kEndOfChain;
+    } else {
+      uintptr_t instr_address = reinterpret_cast<uintptr_t>(p);
+      DCHECK(instr_address - address < INT_MAX);
+      int delta = static_cast<int>(instr_address - address);
+      DCHECK(pos > delta);
+      return pos - delta;
+    }
+  }
+  Instruction* instruction = Instruction::At(buffer_start_ + pos);
+  DEBUG_PRINTF("target_at: %p (%d)\n\t",
+               reinterpret_cast<Instr*>(buffer_start_ + pos), pos);
+  Instr instr = instruction->InstructionBits();
+  disassembleInstr(instruction->InstructionBits());
+
+  switch (instruction->InstructionOpcodeType()) {
+    case BRANCH: {
+      int32_t imm13 = BranchOffset(instr);
+      if (imm13 == kEndOfJumpChain) {
+        // EndOfChain sentinel is returned directly, not relative to pc or pos.
+        return kEndOfChain;
+      } else {
+        return pos + imm13;
+      }
+    }
+    case JAL: {
+      int32_t imm21 = JumpOffset(instr);
+      if (imm21 == kEndOfJumpChain) {
+        // EndOfChain sentinel is returned directly, not relative to pc or pos.
+        return kEndOfChain;
+      } else {
+        return pos + imm21;
+      }
+    }
+    case JALR: {
+      int32_t imm12 = instr >> 20;
+      if (imm12 == kEndOfJumpChain) {
+        // EndOfChain sentinel is returned directly, not relative to pc or pos.
+        return kEndOfChain;
+      } else {
+        return pos + imm12;
+      }
+    }
+    case LUI: {
+      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
+      pc = target_address_at(pc);
+      uintptr_t instr_address =
+          reinterpret_cast<uintptr_t>(buffer_start_ + pos);
+      uintptr_t imm = reinterpret_cast<uintptr_t>(pc);
+      if (imm == kEndOfJumpChain) {
+        return kEndOfChain;
+      } else {
+        DCHECK(instr_address - imm < INT_MAX);
+        int32_t delta = static_cast<int32_t>(instr_address - imm);
+        DCHECK(pos > delta);
+        return pos - delta;
+      }
+    }
+    case AUIPC: {
+      Instr instr_auipc = instr;
+      Instr instr_I = instr_at(pos + 4);
+      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));
+      int32_t offset = BrachlongOffset(instr_auipc, instr_I);
+      if (offset == kEndOfJumpChain) return kEndOfChain;
+      return offset + pos;
+    }
+    case RO_C_J: {
+      int32_t offset = instruction->RvcImm11CJValue();
+      if (offset == kEndOfJumpChain) return kEndOfChain;
+      return offset + pos;
+    }
+    case RO_C_BNEZ:
+    case RO_C_BEQZ: {
+      int32_t offset = instruction->RvcImm8BValue();
+      if (offset == kEndOfJumpChain) return kEndOfChain;
+      return pos + offset;
+    }
+    default: {
+      if (instr == kEndOfJumpChain) {
+        return kEndOfChain;
+      } else {
+        int32_t imm18 =
+            ((instr & static_cast<int32_t>(kImm16Mask)) << 16) >> 14;
+        return (imm18 + pos);
+      }
+    }
+  }
+}
+
+static inline Instr SetBranchOffset(int32_t pos, int32_t target_pos,
+                                    Instr instr) {
+  int32_t imm = target_pos - pos;
+  DCHECK_EQ(imm & 1, 0);
+  DCHECK(is_intn(imm, Assembler::kBranchOffsetBits));
+
+  instr &= ~kBImm12Mask;
+  int32_t imm12 = ((imm & 0x800) >> 4) |   // bit  11
+                  ((imm & 0x1e) << 7) |    // bits 4-1
+                  ((imm & 0x7e0) << 20) |  // bits 10-5
+                  ((imm & 0x1000) << 19);  // bit 12
+
+  return instr | (imm12 & kBImm12Mask);
+}
+
+static inline Instr SetLoadOffset(int32_t offset, Instr instr) {
+#if V8_TARGET_ARCH_RISCV64
+  DCHECK(Assembler::IsLd(instr));
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK(Assembler::IsLw(instr));
+#endif
+  DCHECK(is_int12(offset));
+  instr &= ~kImm12Mask;
+  int32_t imm12 = offset << kImm12Shift;
+  return instr | (imm12 & kImm12Mask);
+}
+
+static inline Instr SetAuipcOffset(int32_t offset, Instr instr) {
+  DCHECK(Assembler::IsAuipc(instr));
+  DCHECK(is_int20(offset));
+  instr = (instr & ~kImm31_12Mask) | ((offset & kImm19_0Mask) << 12);
+  return instr;
+}
+
+static inline Instr SetJalrOffset(int32_t offset, Instr instr) {
+  DCHECK(Assembler::IsJalr(instr));
+  DCHECK(is_int12(offset));
+  instr &= ~kImm12Mask;
+  int32_t imm12 = offset << kImm12Shift;
+  DCHECK(Assembler::IsJalr(instr | (imm12 & kImm12Mask)));
+  DCHECK_EQ(Assembler::JalrOffset(instr | (imm12 & kImm12Mask)), offset);
+  return instr | (imm12 & kImm12Mask);
+}
+
+static inline Instr SetJalOffset(int32_t pos, int32_t target_pos, Instr instr) {
+  DCHECK(Assembler::IsJal(instr));
+  int32_t imm = target_pos - pos;
+  DCHECK_EQ(imm & 1, 0);
+  DCHECK(is_intn(imm, Assembler::kJumpOffsetBits));
+
+  instr &= ~kImm20Mask;
+  int32_t imm20 = (imm & 0xff000) |          // bits 19-12
+                  ((imm & 0x800) << 9) |     // bit  11
+                  ((imm & 0x7fe) << 20) |    // bits 10-1
+                  ((imm & 0x100000) << 11);  // bit  20
+
+  return instr | (imm20 & kImm20Mask);
+}
+
+static inline ShortInstr SetCJalOffset(int32_t pos, int32_t target_pos,
+                                       Instr instr) {
+  DCHECK(Assembler::IsCJal(instr));
+  int32_t imm = target_pos - pos;
+  DCHECK_EQ(imm & 1, 0);
+  DCHECK(is_intn(imm, Assembler::kCJalOffsetBits));
+  instr &= ~kImm11Mask;
+  int16_t imm11 = ((imm & 0x800) >> 1) | ((imm & 0x400) >> 4) |
+                  ((imm & 0x300) >> 1) | ((imm & 0x80) >> 3) |
+                  ((imm & 0x40) >> 1) | ((imm & 0x20) >> 5) |
+                  ((imm & 0x10) << 5) | (imm & 0xe);
+  imm11 = imm11 << kImm11Shift;
+  DCHECK(Assembler::IsCJal(instr | (imm11 & kImm11Mask)));
+  return instr | (imm11 & kImm11Mask);
+}
+static inline Instr SetCBranchOffset(int32_t pos, int32_t target_pos,
+                                     Instr instr) {
+  DCHECK(Assembler::IsCBranch(instr));
+  int32_t imm = target_pos - pos;
+  DCHECK_EQ(imm & 1, 0);
+  DCHECK(is_intn(imm, Assembler::kCBranchOffsetBits));
+
+  instr &= ~kRvcBImm8Mask;
+  int32_t imm8 = ((imm & 0x20) >> 5) | ((imm & 0x6)) | ((imm & 0xc0) >> 3) |
+                 ((imm & 0x18) << 2) | ((imm & 0x100) >> 1);
+  imm8 = ((imm8 & 0x1f) << 2) | ((imm8 & 0xe0) << 5);
+  DCHECK(Assembler::IsCBranch(instr | imm8 & kRvcBImm8Mask));
+
+  return instr | (imm8 & kRvcBImm8Mask);
+}
+
+// We have to use a temporary register for things that can be relocated even
+// if they can be encoded in RISC-V's 12 bits of immediate-offset instruction
+// space.  There is no guarantee that the relocated location can be similarly
+// encoded.
+bool Assembler::MustUseReg(RelocInfo::Mode rmode) {
+  return !RelocInfo::IsNoInfo(rmode);
+}
+
+void Assembler::disassembleInstr(Instr instr) {
+  if (!FLAG_riscv_debug) return;
+  disasm::NameConverter converter;
+  disasm::Disassembler disasm(converter);
+  base::EmbeddedVector<char, 128> disasm_buffer;
+
+  disasm.InstructionDecode(disasm_buffer, reinterpret_cast<byte*>(&instr));
+  DEBUG_PRINTF("%s\n", disasm_buffer.begin());
+}
+
+void Assembler::target_at_put(int pos, int target_pos, bool is_internal,
+                              bool trampoline) {
+  if (is_internal) {
+    uintptr_t imm = reinterpret_cast<uintptr_t>(buffer_start_) + target_pos;
+    *reinterpret_cast<uintptr_t*>(buffer_start_ + pos) = imm;
+    return;
+  }
+  DEBUG_PRINTF("target_at_put: %p (%d) to %p (%d)\n",
+               reinterpret_cast<Instr*>(buffer_start_ + pos), pos,
+               reinterpret_cast<Instr*>(buffer_start_ + target_pos),
+               target_pos);
+  Instruction* instruction = Instruction::At(buffer_start_ + pos);
+  Instr instr = instruction->InstructionBits();
+
+  switch (instruction->InstructionOpcodeType()) {
+    case BRANCH: {
+      instr = SetBranchOffset(pos, target_pos, instr);
+      instr_at_put(pos, instr);
+    } break;
+    case JAL: {
+      DCHECK(IsJal(instr));
+      instr = SetJalOffset(pos, target_pos, instr);
+      instr_at_put(pos, instr);
+    } break;
+    case LUI: {
+      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
+      set_target_value_at(
+          pc, reinterpret_cast<uintptr_t>(buffer_start_ + target_pos));
+    } break;
+    case AUIPC: {
+      Instr instr_auipc = instr;
+      Instr instr_I = instr_at(pos + 4);
+      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));
+
+      intptr_t offset = target_pos - pos;
+      if (is_int21(offset) && IsJalr(instr_I) && trampoline) {
+        DCHECK(is_int21(offset) && ((offset & 1) == 0));
+        Instr instr = JAL;
+        instr = SetJalOffset(pos, target_pos, instr);
+        DCHECK(IsJal(instr));
+        DCHECK(JumpOffset(instr) == offset);
+        instr_at_put(pos, instr);
+        instr_at_put(pos + 4, kNopByte);
+      } else {
+        CHECK(is_int32(offset + 0x800));
+
+        int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
+        int32_t Lo12 = (int32_t)offset << 20 >> 20;
+
+        instr_auipc =
+            (instr_auipc & ~kImm31_12Mask) | ((Hi20 & kImm19_0Mask) << 12);
+        instr_at_put(pos, instr_auipc);
+
+        const int kImm31_20Mask = ((1 << 12) - 1) << 20;
+        const int kImm11_0Mask = ((1 << 12) - 1);
+        instr_I = (instr_I & ~kImm31_20Mask) | ((Lo12 & kImm11_0Mask) << 20);
+        instr_at_put(pos + 4, instr_I);
+      }
+    } break;
+    case RO_C_J: {
+      ShortInstr short_instr = SetCJalOffset(pos, target_pos, instr);
+      instr_at_put(pos, short_instr);
+    } break;
+    case RO_C_BNEZ:
+    case RO_C_BEQZ: {
+      instr = SetCBranchOffset(pos, target_pos, instr);
+      instr_at_put(pos, instr);
+    } break;
+    default: {
+      // Emitted label constant, not part of a branch.
+      // Make label relative to Code pointer of generated Code object.
+      instr_at_put(pos, target_pos + (Code::kHeaderSize - kHeapObjectTag));
+    } break;
+  }
+  disassembleInstr(instr);
+}
+
+void Assembler::print(const Label* L) {
+  if (L->is_unused()) {
+    PrintF("unused label\n");
+  } else if (L->is_bound()) {
+    PrintF("bound label to %d\n", L->pos());
+  } else if (L->is_linked()) {
+    Label l;
+    l.link_to(L->pos());
+    PrintF("unbound label");
+    while (l.is_linked()) {
+      PrintF("@ %d ", l.pos());
+      Instr instr = instr_at(l.pos());
+      if ((instr & ~kImm16Mask) == 0) {
+        PrintF("value\n");
+      } else {
+        PrintF("%d\n", instr);
+      }
+      next(&l, is_internal_reference(&l));
+    }
+  } else {
+    PrintF("label in inconsistent state (pos = %d)\n", L->pos_);
+  }
+}
+
+void Assembler::bind_to(Label* L, int pos) {
+  DCHECK(0 <= pos && pos <= pc_offset());  // Must have valid binding position.
+  DEBUG_PRINTF("binding %d to label %p\n", pos, L);
+  int trampoline_pos = kInvalidSlotPos;
+  bool is_internal = false;
+  if (L->is_linked() && !trampoline_emitted_) {
+    unbound_labels_count_--;
+    if (!is_internal_reference(L)) {
+      next_buffer_check_ += kTrampolineSlotsSize;
+    }
+  }
+
+  while (L->is_linked()) {
+    int fixup_pos = L->pos();
+    int dist = pos - fixup_pos;
+    is_internal = is_internal_reference(L);
+    next(L, is_internal);  // Call next before overwriting link with target
+                           // at fixup_pos.
+    Instr instr = instr_at(fixup_pos);
+    DEBUG_PRINTF("\tfixup: %d to %d\n", fixup_pos, dist);
+    if (is_internal) {
+      target_at_put(fixup_pos, pos, is_internal);
+    } else {
+      if (IsBranch(instr)) {
+        if (dist > kMaxBranchOffset) {
+          if (trampoline_pos == kInvalidSlotPos) {
+            trampoline_pos = get_trampoline_entry(fixup_pos);
+            CHECK_NE(trampoline_pos, kInvalidSlotPos);
+          }
+          CHECK((trampoline_pos - fixup_pos) <= kMaxBranchOffset);
+          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
+          target_at_put(fixup_pos, trampoline_pos, false, true);
+          fixup_pos = trampoline_pos;
+        }
+        target_at_put(fixup_pos, pos, false);
+      } else if (IsJal(instr)) {
+        if (dist > kMaxJumpOffset) {
+          if (trampoline_pos == kInvalidSlotPos) {
+            trampoline_pos = get_trampoline_entry(fixup_pos);
+            CHECK_NE(trampoline_pos, kInvalidSlotPos);
+          }
+          CHECK((trampoline_pos - fixup_pos) <= kMaxJumpOffset);
+          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
+          target_at_put(fixup_pos, trampoline_pos, false, true);
+          fixup_pos = trampoline_pos;
+        }
+        target_at_put(fixup_pos, pos, false);
+      } else {
+        target_at_put(fixup_pos, pos, false);
+      }
+    }
+  }
+  L->bind_to(pos);
+
+  // Keep track of the last bound label so we don't eliminate any instructions
+  // before a bound label.
+  if (pos > last_bound_pos_) last_bound_pos_ = pos;
+}
+
+void Assembler::bind(Label* L) {
+  DCHECK(!L->is_bound());  // Label can only be bound once.
+  bind_to(L, pc_offset());
+}
+
+void Assembler::next(Label* L, bool is_internal) {
+  DCHECK(L->is_linked());
+  int link = target_at(L->pos(), is_internal);
+  if (link == kEndOfChain) {
+    L->Unuse();
+  } else {
+    DCHECK_GE(link, 0);
+    DEBUG_PRINTF("next: %p to %p (%d)\n", L,
+                 reinterpret_cast<Instr*>(buffer_start_ + link), link);
+    L->link_to(link);
+  }
+}
+
+bool Assembler::is_near(Label* L) {
+  DCHECK(L->is_bound());
+  return is_intn((pc_offset() - L->pos()), kJumpOffsetBits);
+}
+
+bool Assembler::is_near(Label* L, OffsetSize bits) {
+  if (L == nullptr || !L->is_bound()) return true;
+  return is_intn((pc_offset() - L->pos()), bits);
+}
+
+bool Assembler::is_near_branch(Label* L) {
+  DCHECK(L->is_bound());
+  return is_intn((pc_offset() - L->pos()), kBranchOffsetBits);
+}
+
+int Assembler::BranchOffset(Instr instr) {
+  // | imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1|11] | opcode |
+  //  31          25                      11          7
+  int32_t imm13 = ((instr & 0xf00) >> 7) | ((instr & 0x7e000000) >> 20) |
+                  ((instr & 0x80) << 4) | ((instr & 0x80000000) >> 19);
+  imm13 = imm13 << 19 >> 19;
+  return imm13;
+}
+
+int Assembler::BrachlongOffset(Instr auipc, Instr instr_I) {
+  DCHECK(reinterpret_cast<Instruction*>(&instr_I)->InstructionType() ==
+         InstructionBase::kIType);
+  DCHECK(IsAuipc(auipc));
+  DCHECK_EQ((auipc & kRdFieldMask) >> kRdShift,
+            (instr_I & kRs1FieldMask) >> kRs1Shift);
+  int32_t imm_auipc = AuipcOffset(auipc);
+  int32_t imm12 = static_cast<int32_t>(instr_I & kImm12Mask) >> 20;
+  int32_t offset = imm12 + imm_auipc;
+  return offset;
+}
+
+int Assembler::PatchBranchlongOffset(Address pc, Instr instr_auipc,
+                                     Instr instr_jalr, int32_t offset) {
+  DCHECK(IsAuipc(instr_auipc));
+  DCHECK(IsJalr(instr_jalr));
+  CHECK(is_int32(offset + 0x800));
+  int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
+  int32_t Lo12 = (int32_t)offset << 20 >> 20;
+  instr_at_put(pc, SetAuipcOffset(Hi20, instr_auipc));
+  instr_at_put(pc + 4, SetJalrOffset(Lo12, instr_jalr));
+  DCHECK(offset ==
+         BrachlongOffset(Assembler::instr_at(pc), Assembler::instr_at(pc + 4)));
+  return 2;
+}
+
+// Returns the next free trampoline entry.
+int32_t Assembler::get_trampoline_entry(int32_t pos) {
+  int32_t trampoline_entry = kInvalidSlotPos;
+  if (!internal_trampoline_exception_) {
+    DEBUG_PRINTF("\tstart: %d,pos: %d\n", trampoline_.start(), pos);
+    if (trampoline_.start() > pos) {
+      trampoline_entry = trampoline_.take_slot();
+    }
+
+    if (kInvalidSlotPos == trampoline_entry) {
+      internal_trampoline_exception_ = true;
+    }
+  }
+  return trampoline_entry;
+}
+
+uintptr_t Assembler::jump_address(Label* L) {
+  intptr_t target_pos;
+  DEBUG_PRINTF("jump_address: %p to %p (%d)\n", L,
+               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
+               pc_offset());
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      L->link_to(pc_offset());
+    } else {
+      L->link_to(pc_offset());
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+      DEBUG_PRINTF("\tstarted link\n");
+      return kEndOfJumpChain;
+    }
+  }
+  uintptr_t imm = reinterpret_cast<uintptr_t>(buffer_start_) + target_pos;
+  if (FLAG_riscv_c_extension)
+    DCHECK_EQ(imm & 1, 0);
+  else
+    DCHECK_EQ(imm & 3, 0);
+
+  return imm;
+}
+
+int32_t Assembler::branch_long_offset(Label* L) {
+  intptr_t target_pos;
+
+  DEBUG_PRINTF("branch_long_offset: %p to %p (%d)\n", L,
+               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
+               pc_offset());
+  if (L->is_bound()) {
+    target_pos = L->pos();
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      L->link_to(pc_offset());
+    } else {
+      L->link_to(pc_offset());
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+      DEBUG_PRINTF("\tstarted link\n");
+      return kEndOfJumpChain;
+    }
+  }
+  intptr_t offset = target_pos - pc_offset();
+  if (FLAG_riscv_c_extension)
+    DCHECK_EQ(offset & 1, 0);
+  else
+    DCHECK_EQ(offset & 3, 0);
+  DCHECK(is_int32(offset));
+  return static_cast<int32_t>(offset);
+}
+
+int32_t Assembler::branch_offset_helper(Label* L, OffsetSize bits) {
+  int32_t target_pos;
+
+  DEBUG_PRINTF("branch_offset_helper: %p to %p (%d)\n", L,
+               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
+               pc_offset());
+  if (L->is_bound()) {
+    target_pos = L->pos();
+    DEBUG_PRINTF("\tbound: %d", target_pos);
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();
+      L->link_to(pc_offset());
+      DEBUG_PRINTF("\tadded to link: %d\n", target_pos);
+    } else {
+      L->link_to(pc_offset());
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+      DEBUG_PRINTF("\tstarted link\n");
+      return kEndOfJumpChain;
+    }
+  }
+
+  int32_t offset = target_pos - pc_offset();
+  DCHECK(is_intn(offset, bits));
+  DCHECK_EQ(offset & 1, 0);
+  DEBUG_PRINTF("\toffset = %d\n", offset);
+  return offset;
+}
+
+void Assembler::label_at_put(Label* L, int at_offset) {
+  int target_pos;
+  DEBUG_PRINTF("label_at_put: %p @ %p (%d)\n", L,
+               reinterpret_cast<Instr*>(buffer_start_ + at_offset), at_offset);
+  if (L->is_bound()) {
+    target_pos = L->pos();
+    instr_at_put(at_offset, target_pos + (Code::kHeaderSize - kHeapObjectTag));
+  } else {
+    if (L->is_linked()) {
+      target_pos = L->pos();  // L's link.
+      int32_t imm18 = target_pos - at_offset;
+      DCHECK_EQ(imm18 & 3, 0);
+      int32_t imm16 = imm18 >> 2;
+      DCHECK(is_int16(imm16));
+      instr_at_put(at_offset, (int32_t)(imm16 & kImm16Mask));
+    } else {
+      target_pos = kEndOfJumpChain;
+      instr_at_put(at_offset, target_pos);
+      if (!trampoline_emitted_) {
+        unbound_labels_count_++;
+        next_buffer_check_ -= kTrampolineSlotsSize;
+      }
+    }
+    L->link_to(at_offset);
+  }
+}
+
+//===----------------------------------------------------------------------===//
+// Instructions
+//===----------------------------------------------------------------------===//
+
+// Definitions for using compressed vs non compressed
+
+void Assembler::NOP() {
+  if (FLAG_riscv_c_extension)
+    c_nop();
+  else
+    nop();
+}
+
+void Assembler::EBREAK() {
+  if (FLAG_riscv_c_extension)
+    c_ebreak();
+  else
+    ebreak();
+}
+
+// Assembler Pseudo Instructions (Tables 25.2 and 25.3, RISC-V Unprivileged ISA)
+
+void Assembler::nop() { addi(ToRegister(0), ToRegister(0), 0); }
+
+#if V8_TARGET_ARCH_RISCV64
+void Assembler::RV_li(Register rd, int64_t imm) {
+  // 64-bit imm is put in the register rd.
+  // In most cases the imm is 32 bit and 2 instructions are generated. If a
+  // temporary register is available, in the worst case, 6 instructions are
+  // generated for a full 64-bit immediate. If temporay register is not
+  // available the maximum will be 8 instructions. If imm is more than 32 bits
+  // and a temp register is available, imm is divided into two 32-bit parts,
+  // low_32 and up_32. Each part is built in a separate register. low_32 is
+  // built before up_32. If low_32 is negative (upper 32 bits are 1), 0xffffffff
+  // is subtracted from up_32 before up_32 is built. This compensates for 32
+  // bits of 1's in the lower when the two registers are added. If no temp is
+  // available, the upper 32 bit is built in rd, and the lower 32 bits are
+  // devided to 3 parts (11, 11, and 10 bits). The parts are shifted and added
+  // to the upper part built in rd.
+  if (is_int32(imm + 0x800)) {
+    // 32-bit case. Maximum of 2 instructions generated
+    int64_t high_20 = ((imm + 0x800) >> 12);
+    int64_t low_12 = imm << 52 >> 52;
+    if (high_20) {
+      lui(rd, (int32_t)high_20);
+      if (low_12) {
+        addi(rd, rd, low_12);
+      }
+    } else {
+      addi(rd, zero_reg, low_12);
+    }
+    return;
+  } else {
+    // 64-bit case: divide imm into two 32-bit parts, upper and lower
+    int64_t up_32 = imm >> 32;
+    int64_t low_32 = imm & 0xffffffffull;
+    Register temp_reg = rd;
+    // Check if a temporary register is available
+    if (up_32 == 0 || low_32 == 0) {
+      // No temp register is needed
+    } else {
+      UseScratchRegisterScope temps(this);
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      temp_reg = temps.hasAvailable() ? temps.Acquire() : no_reg;
+    }
+    if (temp_reg != no_reg) {
+      // keep track of hardware behavior for lower part in sim_low
+      int64_t sim_low = 0;
+      // Build lower part
+      if (low_32 != 0) {
+        int64_t high_20 = ((low_32 + 0x800) >> 12);
+        int64_t low_12 = low_32 & 0xfff;
+        if (high_20) {
+          // Adjust to 20 bits for the case of overflow
+          high_20 &= 0xfffff;
+          sim_low = ((high_20 << 12) << 32) >> 32;
+          lui(rd, (int32_t)high_20);
+          if (low_12) {
+            sim_low += (low_12 << 52 >> 52) | low_12;
+            addi(rd, rd, low_12);
+          }
+        } else {
+          sim_low = low_12;
+          ori(rd, zero_reg, low_12);
+        }
+      }
+      if (sim_low & 0x100000000) {
+        // Bit 31 is 1. Either an overflow or a negative 64 bit
+        if (up_32 == 0) {
+          // Positive number, but overflow because of the add 0x800
+          slli(rd, rd, 32);
+          srli(rd, rd, 32);
+          return;
+        }
+        // low_32 is a negative 64 bit after the build
+        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
+      }
+      if (up_32 == 0) {
+        return;
+      }
+      // Build upper part in a temporary register
+      if (low_32 == 0) {
+        // Build upper part in rd
+        temp_reg = rd;
+      }
+      int64_t high_20 = (up_32 + 0x800) >> 12;
+      int64_t low_12 = up_32 & 0xfff;
+      if (high_20) {
+        // Adjust to 20 bits for the case of overflow
+        high_20 &= 0xfffff;
+        lui(temp_reg, (int32_t)high_20);
+        if (low_12) {
+          addi(temp_reg, temp_reg, low_12);
+        }
+      } else {
+        ori(temp_reg, zero_reg, low_12);
+      }
+      // Put it at the bgining of register
+      slli(temp_reg, temp_reg, 32);
+      if (low_32 != 0) {
+        add(rd, rd, temp_reg);
+      }
+      return;
+    }
+    // No temp register. Build imm in rd.
+    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
+    // parts to the upper part by doing shift and add.
+    // First build upper part in rd.
+    int64_t high_20 = (up_32 + 0x800) >> 12;
+    int64_t low_12 = up_32 & 0xfff;
+    if (high_20) {
+      // Adjust to 20 bits for the case of overflow
+      high_20 &= 0xfffff;
+      lui(rd, (int32_t)high_20);
+      if (low_12) {
+        addi(rd, rd, low_12);
+      }
+    } else {
+      ori(rd, zero_reg, low_12);
+    }
+    // upper part already in rd. Each part to be added to rd, has maximum of 11
+    // bits, and always starts with a 1. rd is shifted by the size of the part
+    // plus the number of zeros between the parts. Each part is added after the
+    // left shift.
+    uint32_t mask = 0x80000000;
+    int32_t shift_val = 0;
+    int32_t i;
+    for (i = 0; i < 32; i++) {
+      if ((low_32 & mask) == 0) {
+        mask >>= 1;
+        shift_val++;
+        if (i == 31) {
+          // rest is zero
+          slli(rd, rd, shift_val);
+        }
+        continue;
+      }
+      // The first 1 seen
+      int32_t part;
+      if ((i + 11) < 32) {
+        // Pick 11 bits
+        part = ((uint32_t)(low_32 << i) >> i) >> (32 - (i + 11));
+        slli(rd, rd, shift_val + 11);
+        ori(rd, rd, part);
+        i += 10;
+        mask >>= 11;
+      } else {
+        part = (uint32_t)(low_32 << i) >> i;
+        slli(rd, rd, shift_val + (32 - i));
+        ori(rd, rd, part);
+        break;
+      }
+      shift_val = 0;
+    }
+  }
+}
+
+int Assembler::li_estimate(int64_t imm, bool is_get_temp_reg) {
+  int count = 0;
+  // imitate Assembler::RV_li
+  if (is_int32(imm + 0x800)) {
+    // 32-bit case. Maximum of 2 instructions generated
+    int64_t high_20 = ((imm + 0x800) >> 12);
+    int64_t low_12 = imm << 52 >> 52;
+    if (high_20) {
+      count++;
+      if (low_12) {
+        count++;
+      }
+    } else {
+      count++;
+    }
+    return count;
+  } else {
+    // 64-bit case: divide imm into two 32-bit parts, upper and lower
+    int64_t up_32 = imm >> 32;
+    int64_t low_32 = imm & 0xffffffffull;
+    // Check if a temporary register is available
+    if (is_get_temp_reg) {
+      // keep track of hardware behavior for lower part in sim_low
+      int64_t sim_low = 0;
+      // Build lower part
+      if (low_32 != 0) {
+        int64_t high_20 = ((low_32 + 0x800) >> 12);
+        int64_t low_12 = low_32 & 0xfff;
+        if (high_20) {
+          // Adjust to 20 bits for the case of overflow
+          high_20 &= 0xfffff;
+          sim_low = ((high_20 << 12) << 32) >> 32;
+          count++;
+          if (low_12) {
+            sim_low += (low_12 << 52 >> 52) | low_12;
+            count++;
+          }
+        } else {
+          sim_low = low_12;
+          count++;
+        }
+      }
+      if (sim_low & 0x100000000) {
+        // Bit 31 is 1. Either an overflow or a negative 64 bit
+        if (up_32 == 0) {
+          // Positive number, but overflow because of the add 0x800
+          count++;
+          count++;
+          return count;
+        }
+        // low_32 is a negative 64 bit after the build
+        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
+      }
+      if (up_32 == 0) {
+        return count;
+      }
+      int64_t high_20 = (up_32 + 0x800) >> 12;
+      int64_t low_12 = up_32 & 0xfff;
+      if (high_20) {
+        // Adjust to 20 bits for the case of overflow
+        high_20 &= 0xfffff;
+        count++;
+        if (low_12) {
+          count++;
+        }
+      } else {
+        count++;
+      }
+      // Put it at the bgining of register
+      count++;
+      if (low_32 != 0) {
+        count++;
+      }
+      return count;
+    }
+    // No temp register. Build imm in rd.
+    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
+    // parts to the upper part by doing shift and add.
+    // First build upper part in rd.
+    int64_t high_20 = (up_32 + 0x800) >> 12;
+    int64_t low_12 = up_32 & 0xfff;
+    if (high_20) {
+      // Adjust to 20 bits for the case of overflow
+      high_20 &= 0xfffff;
+      count++;
+      if (low_12) {
+        count++;
+      }
+    } else {
+      count++;
+    }
+    // upper part already in rd. Each part to be added to rd, has maximum of 11
+    // bits, and always starts with a 1. rd is shifted by the size of the part
+    // plus the number of zeros between the parts. Each part is added after the
+    // left shift.
+    uint32_t mask = 0x80000000;
+    int32_t i;
+    for (i = 0; i < 32; i++) {
+      if ((low_32 & mask) == 0) {
+        mask >>= 1;
+        if (i == 31) {
+          // rest is zero
+          count++;
+        }
+        continue;
+      }
+      // The first 1 seen
+      if ((i + 11) < 32) {
+        // Pick 11 bits
+        count++;
+        count++;
+        i += 10;
+        mask >>= 11;
+      } else {
+        count++;
+        count++;
+        break;
+      }
+    }
+  }
+  return count;
+}
+
+void Assembler::li_ptr(Register rd, int64_t imm) {
+  // Initialize rd with an address
+  // Pointers are 48 bits
+  // 6 fixed instructions are generated
+  DCHECK_EQ((imm & 0xfff0000000000000ll), 0);
+  int64_t a6 = imm & 0x3f;                      // bits 0:5. 6 bits
+  int64_t b11 = (imm >> 6) & 0x7ff;             // bits 6:11. 11 bits
+  int64_t high_31 = (imm >> 17) & 0x7fffffff;   // 31 bits
+  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
+  int64_t low_12 = high_31 & 0xfff;             // 12 bits
+  lui(rd, (int32_t)high_20);
+  addi(rd, rd, low_12);  // 31 bits in rd.
+  slli(rd, rd, 11);      // Space for next 11 bis
+  ori(rd, rd, b11);      // 11 bits are put in. 42 bit in rd
+  slli(rd, rd, 6);       // Space for next 6 bits
+  ori(rd, rd, a6);       // 6 bits are put in. 48 bis in rd
+}
+
+void Assembler::li_constant(Register rd, int64_t imm) {
+  DEBUG_PRINTF("li_constant(%d, %lx <%ld>)\n", ToNumber(rd), imm, imm);
+  lui(rd, (imm + (1LL << 47) + (1LL << 35) + (1LL << 23) + (1LL << 11)) >>
+              48);  // Bits 63:48
+  addiw(rd, rd,
+        (imm + (1LL << 35) + (1LL << 23) + (1LL << 11)) << 16 >>
+            52);  // Bits 47:36
+  slli(rd, rd, 12);
+  addi(rd, rd, (imm + (1LL << 23) + (1LL << 11)) << 28 >> 52);  // Bits 35:24
+  slli(rd, rd, 12);
+  addi(rd, rd, (imm + (1LL << 11)) << 40 >> 52);  // Bits 23:12
+  slli(rd, rd, 12);
+  addi(rd, rd, imm << 52 >> 52);  // Bits 11:0
+}
+
+#elif V8_TARGET_ARCH_RISCV32
+void Assembler::RV_li(Register rd, int32_t imm) {
+  int32_t high_20 = ((imm + 0x800) >> 12);
+  int32_t low_12 = imm & 0xfff;
+  if (high_20) {
+    lui(rd, high_20);
+    if (low_12) {
+      addi(rd, rd, low_12);
+    }
+  } else {
+    addi(rd, zero_reg, low_12);
+  }
+}
+
+int Assembler::li_estimate(int32_t imm, bool is_get_temp_reg) {
+  int count = 0;
+  // imitate Assembler::RV_li
+  int32_t high_20 = ((imm + 0x800) >> 12);
+  int32_t low_12 = imm & 0xfff;
+  if (high_20) {
+    count++;
+    if (low_12) {
+      count++;
+    }
+  } else {
+    // if high_20 is 0, always need one instruction to load the low_12 bit
+    count++;
+  }
+
+  return count;
+}
+
+void Assembler::li_ptr(Register rd, int32_t imm) {
+  // Initialize rd with an address
+  // Pointers are 32 bits
+  // 2 fixed instructions are generated
+  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
+  int32_t low_12 = imm & 0xfff;             // bits11:0
+  lui(rd, high_20);
+  addi(rd, rd, low_12);
+}
+
+void Assembler::li_constant(Register rd, int32_t imm) {
+  DEBUG_PRINTF("li_constant(%d, %x <%d>)\n", ToNumber(rd), imm, imm);
+  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
+  int32_t low_12 = imm & 0xfff;             // bits11:0
+  lui(rd, high_20);
+  addi(rd, rd, low_12);
+}
+#endif
+
+// Break / Trap instructions.
+void Assembler::break_(uint32_t code, bool break_as_stop) {
+  // We need to invalidate breaks that could be stops as well because the
+  // simulator expects a char pointer after the stop instruction.
+  // See constants-mips.h for explanation.
+  DCHECK(
+      (break_as_stop && code <= kMaxStopCode && code > kMaxWatchpointCode) ||
+      (!break_as_stop && (code > kMaxStopCode || code <= kMaxWatchpointCode)));
+
+  // since ebreak does not allow additional immediate field, we use the
+  // immediate field of lui instruction immediately following the ebreak to
+  // encode the "code" info
+  ebreak();
+  DCHECK(is_uint20(code));
+  lui(zero_reg, code);
+}
+
+void Assembler::stop(uint32_t code) {
+  DCHECK_GT(code, kMaxWatchpointCode);
+  DCHECK_LE(code, kMaxStopCode);
+#if defined(V8_HOST_ARCH_RISCV64) || defined(V8_HOST_ARCH_RISCV32)
+  break_(0x54321);
+#else  // V8_HOST_ARCH_RISCV64 || V8_HOST_ARCH_RISCV32
+  break_(code, true);
+#endif
+}
+
+// Original MIPS Instructions
+
+// ------------Memory-instructions-------------
+
+bool Assembler::NeedAdjustBaseAndOffset(const MemOperand& src,
+                                        OffsetAccessType access_type,
+                                        int second_access_add_to_offset) {
+  bool two_accesses = static_cast<bool>(access_type);
+  DCHECK_LE(second_access_add_to_offset, 7);  // Must be <= 7.
+
+  // is_int12 must be passed a signed value, hence the static cast below.
+  if (is_int12(src.offset()) &&
+      (!two_accesses || is_int12(static_cast<int32_t>(
+                            src.offset() + second_access_add_to_offset)))) {
+    // Nothing to do: 'offset' (and, if needed, 'offset + 4', or other specified
+    // value) fits into int12.
+    return false;
+  }
+  return true;
+}
+
+void Assembler::AdjustBaseAndOffset(MemOperand* src, Register scratch,
+                                    OffsetAccessType access_type,
+                                    int second_Access_add_to_offset) {
+  // This method is used to adjust the base register and offset pair
+  // for a load/store when the offset doesn't fit into int12.
+
+  // Must not overwrite the register 'base' while loading 'offset'.
+  constexpr int32_t kMinOffsetForSimpleAdjustment = 0x7F8;
+  constexpr int32_t kMaxOffsetForSimpleAdjustment =
+      2 * kMinOffsetForSimpleAdjustment;
+  if (0 <= src->offset() && src->offset() <= kMaxOffsetForSimpleAdjustment) {
+    addi(scratch, src->rm(), kMinOffsetForSimpleAdjustment);
+    src->offset_ -= kMinOffsetForSimpleAdjustment;
+  } else if (-kMaxOffsetForSimpleAdjustment <= src->offset() &&
+             src->offset() < 0) {
+    addi(scratch, src->rm(), -kMinOffsetForSimpleAdjustment);
+    src->offset_ += kMinOffsetForSimpleAdjustment;
+  } else if (access_type == OffsetAccessType::SINGLE_ACCESS) {
+    RV_li(scratch, (static_cast<intptr_t>(src->offset()) + 0x800) >> 12 << 12);
+    add(scratch, scratch, src->rm());
+    src->offset_ = src->offset() << 20 >> 20;
+  } else {
+    RV_li(scratch, src->offset());
+    add(scratch, scratch, src->rm());
+    src->offset_ = 0;
+  }
+  src->rm_ = scratch;
+}
+
+int Assembler::RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
+                                         intptr_t pc_delta) {
+  if (RelocInfo::IsInternalReference(rmode)) {
+    intptr_t* p = reinterpret_cast<intptr_t*>(pc);
+    if (*p == kEndOfJumpChain) {
+      return 0;  // Number of instructions patched.
+    }
+    *p += pc_delta;
+    return 2;  // Number of instructions patched.
+  }
+  Instr instr = instr_at(pc);
+  DCHECK(RelocInfo::IsInternalReferenceEncoded(rmode));
+  if (IsLui(instr)) {
+    uintptr_t target_address = target_address_at(pc) + pc_delta;
+    DEBUG_PRINTF("target_address 0x%" PRIxPTR "\n", target_address);
+    set_target_value_at(pc, target_address);
+#if V8_TARGET_ARCH_RISCV64
+    return 8;  // Number of instructions patched.
+#elif V8_TARGET_ARCH_RISCV32
+    return 2;  // Number of instructions patched.
+#endif
+  } else {
+    UNIMPLEMENTED();
+  }
+}
+
+void Assembler::RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
+                                          intptr_t pc_delta) {
+  Instr instr = instr_at(pc);
+  Instr instr1 = instr_at(pc + 1 * kInstrSize);
+  DCHECK(RelocInfo::IsRelativeCodeTarget(rmode));
+  if (IsAuipc(instr) && IsJalr(instr1)) {
+    int32_t imm;
+    imm = BrachlongOffset(instr, instr1);
+    imm -= pc_delta;
+    PatchBranchlongOffset(pc, instr, instr1, imm);
+    return;
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void Assembler::GrowBuffer() {
+  DEBUG_PRINTF("GrowBuffer: %p -> ", buffer_start_);
+  // Compute new buffer size.
+  int old_size = buffer_->size();
+  int new_size = std::min(2 * old_size, old_size + 1 * MB);
+
+  // Some internal data structures overflow for very large buffers,
+  // they must ensure that kMaximalBufferSize is not too large.
+  if (new_size > kMaximalBufferSize) {
+    V8::FatalProcessOutOfMemory(nullptr, "Assembler::GrowBuffer");
+  }
+
+  // Set up new buffer.
+  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);
+  DCHECK_EQ(new_size, new_buffer->size());
+  byte* new_start = new_buffer->start();
+
+  // Copy the data.
+  intptr_t pc_delta = new_start - buffer_start_;
+  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);
+  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();
+  MemMove(new_start, buffer_start_, pc_offset());
+  MemMove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),
+          reloc_size);
+
+  // Switch buffers.
+  buffer_ = std::move(new_buffer);
+  buffer_start_ = new_start;
+  DEBUG_PRINTF("%p\n", buffer_start_);
+  pc_ += pc_delta;
+  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,
+                               reloc_info_writer.last_pc() + pc_delta);
+
+  // Relocate runtime entries.
+  base::Vector<byte> instructions{buffer_start_,
+                                  static_cast<size_t>(pc_offset())};
+  base::Vector<const byte> reloc_info{reloc_info_writer.pos(), reloc_size};
+  for (RelocIterator it(instructions, reloc_info, 0); !it.done(); it.next()) {
+    RelocInfo::Mode rmode = it.rinfo()->rmode();
+    if (rmode == RelocInfo::INTERNAL_REFERENCE) {
+      RelocateInternalReference(rmode, it.rinfo()->pc(), pc_delta);
+    }
+  }
+
+  DCHECK(!overflow());
+}
+
+void Assembler::db(uint8_t data) {
+  if (!is_buffer_growth_blocked()) CheckBuffer();
+  DEBUG_PRINTF("%p(%x): constant 0x%x\n", pc_, pc_offset(), data);
+  EmitHelper(data);
+}
+
+void Assembler::dd(uint32_t data, RelocInfo::Mode rmode) {
+  if (!RelocInfo::IsNoInfo(rmode)) {
+    DCHECK(RelocInfo::IsDataEmbeddedObject(rmode) ||
+           RelocInfo::IsLiteralConstant(rmode));
+    RecordRelocInfo(rmode);
+  }
+  if (!is_buffer_growth_blocked()) CheckBuffer();
+  DEBUG_PRINTF("%p(%x): constant 0x%x\n", pc_, pc_offset(), data);
+  EmitHelper(data);
+}
+
+void Assembler::dq(uint64_t data, RelocInfo::Mode rmode) {
+  if (!RelocInfo::IsNoInfo(rmode)) {
+    DCHECK(RelocInfo::IsDataEmbeddedObject(rmode) ||
+           RelocInfo::IsLiteralConstant(rmode));
+    RecordRelocInfo(rmode);
+  }
+  if (!is_buffer_growth_blocked()) CheckBuffer();
+  EmitHelper(data);
+}
+
+void Assembler::dd(Label* label) {
+  uintptr_t data;
+  if (!is_buffer_growth_blocked()) CheckBuffer();
+  if (label->is_bound()) {
+    data = reinterpret_cast<uintptr_t>(buffer_start_ + label->pos());
+  } else {
+    data = jump_address(label);
+    internal_reference_positions_.insert(label->pos());
+  }
+  RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE);
+  EmitHelper(data);
+}
+
+void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
+  if (!ShouldRecordRelocInfo(rmode)) return;
+  // We do not try to reuse pool constants.
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code());
+  DCHECK_GE(buffer_space(), kMaxRelocSize);  // Too late to grow buffer here.
+  reloc_info_writer.Write(&rinfo);
+}
+
+void Assembler::BlockTrampolinePoolFor(int instructions) {
+  DEBUG_PRINTF("\tBlockTrampolinePoolFor %d", instructions);
+  CheckTrampolinePoolQuick(instructions);
+  DEBUG_PRINTF("\tpc_offset %d,BlockTrampolinePoolBefore %d\n", pc_offset(),
+               pc_offset() + instructions * kInstrSize);
+  BlockTrampolinePoolBefore(pc_offset() + instructions * kInstrSize);
+}
+
+void Assembler::CheckTrampolinePool() {
+  // Some small sequences of instructions must not be broken up by the
+  // insertion of a trampoline pool; such sequences are protected by setting
+  // either trampoline_pool_blocked_nesting_ or no_trampoline_pool_before_,
+  // which are both checked here. Also, recursive calls to CheckTrampolinePool
+  // are blocked by trampoline_pool_blocked_nesting_.
+  DEBUG_PRINTF("\tpc_offset %d no_trampoline_pool_before:%d\n", pc_offset(),
+               no_trampoline_pool_before_);
+  DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
+               trampoline_pool_blocked_nesting_);
+  if ((trampoline_pool_blocked_nesting_ > 0) ||
+      (pc_offset() < no_trampoline_pool_before_)) {
+    // Emission is currently blocked; make sure we try again as soon as
+    // possible.
+    if (trampoline_pool_blocked_nesting_ > 0) {
+      next_buffer_check_ = pc_offset() + kInstrSize;
+    } else {
+      next_buffer_check_ = no_trampoline_pool_before_;
+    }
+    return;
+  }
+
+  DCHECK(!trampoline_emitted_);
+  DCHECK_GE(unbound_labels_count_, 0);
+  if (unbound_labels_count_ > 0) {
+    // First we emit jump, then we emit trampoline pool.
+    {
+      DEBUG_PRINTF("inserting trampoline pool at %p (%d)\n",
+                   reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
+                   pc_offset());
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      Label after_pool;
+      j(&after_pool);
+
+      int pool_start = pc_offset();
+      for (int i = 0; i < unbound_labels_count_; i++) {
+        int32_t imm;
+        imm = branch_long_offset(&after_pool);
+        CHECK(is_int32(imm + 0x800));
+        int32_t Hi20 = (((int32_t)imm + 0x800) >> 12);
+        int32_t Lo12 = (int32_t)imm << 20 >> 20;
+        auipc(t6, Hi20);  // Read PC + Hi20 into t6
+        jr(t6, Lo12);     // jump PC + Hi20 + Lo12
+      }
+      // If unbound_labels_count_ is big enough, label after_pool will
+      // need a trampoline too, so we must create the trampoline before
+      // the bind operation to make sure function 'bind' can get this
+      // information.
+      trampoline_ = Trampoline(pool_start, unbound_labels_count_);
+      bind(&after_pool);
+
+      trampoline_emitted_ = true;
+      // As we are only going to emit trampoline once, we need to prevent any
+      // further emission.
+      next_buffer_check_ = kMaxInt;
+    }
+  } else {
+    // Number of branches to unbound label at this point is zero, so we can
+    // move next buffer check to maximum.
+    next_buffer_check_ =
+        pc_offset() + kMaxBranchOffset - kTrampolineSlotsSize * 16;
+  }
+  return;
+}
+
+void Assembler::set_target_address_at(Address pc, Address constant_pool,
+                                      Address target,
+                                      ICacheFlushMode icache_flush_mode) {
+  Instr* instr = reinterpret_cast<Instr*>(pc);
+  if (IsAuipc(*instr)) {
+#if V8_TARGET_ARCH_RISCV64
+    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
+#elif V8_TARGET_ARCH_RISCV32
+    if (IsLw(*reinterpret_cast<Instr*>(pc + 4))) {
+#endif
+      int32_t Hi20 = AuipcOffset(*instr);
+      int32_t Lo12 = LoadOffset(*reinterpret_cast<Instr*>(pc + 4));
+      Memory<Address>(pc + Hi20 + Lo12) = target;
+      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
+        FlushInstructionCache(pc + Hi20 + Lo12, 2 * kInstrSize);
+      }
+    } else {
+      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
+      intptr_t imm = (intptr_t)target - (intptr_t)pc;
+      Instr instr = instr_at(pc);
+      Instr instr1 = instr_at(pc + 1 * kInstrSize);
+      DCHECK(is_int32(imm + 0x800));
+      int num = PatchBranchlongOffset(pc, instr, instr1, (int32_t)imm);
+      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
+        FlushInstructionCache(pc, num * kInstrSize);
+      }
+    }
+  } else {
+    set_target_address_at(pc, target, icache_flush_mode);
+  }
+}
+
+Address Assembler::target_address_at(Address pc, Address constant_pool) {
+  Instr* instr = reinterpret_cast<Instr*>(pc);
+  if (IsAuipc(*instr)) {
+#if V8_TARGET_ARCH_RISCV64
+    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
+#elif V8_TARGET_ARCH_RISCV32
+    if (IsLw(*reinterpret_cast<Instr*>(pc + 4))) {
+#endif
+      int32_t Hi20 = AuipcOffset(*instr);
+      int32_t Lo12 = LoadOffset(*reinterpret_cast<Instr*>(pc + 4));
+      return Memory<Address>(pc + Hi20 + Lo12);
+    } else {
+      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
+      int32_t Hi20 = AuipcOffset(*instr);
+      int32_t Lo12 = JalrOffset(*reinterpret_cast<Instr*>(pc + 4));
+      return pc + Hi20 + Lo12;
+    }
+
+  } else {
+    return target_address_at(pc);
+  }
+}
+
+#if V8_TARGET_ARCH_RISCV64
+Address Assembler::target_address_at(Address pc) {
+  DEBUG_PRINTF("target_address_at: pc: %lx\t", pc);
+  Instruction* instr0 = Instruction::At((unsigned char*)pc);
+  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
+  Instruction* instr2 = Instruction::At((unsigned char*)(pc + 2 * kInstrSize));
+  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
+  Instruction* instr4 = Instruction::At((unsigned char*)(pc + 4 * kInstrSize));
+  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));
+
+  // Interpret instructions for address generated by li: See listing in
+  // Assembler::set_target_address_at() just below.
+  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
+      IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
+      IsSlli(*reinterpret_cast<Instr*>(instr2)) &&
+      IsOri(*reinterpret_cast<Instr*>(instr3)) &&
+      IsSlli(*reinterpret_cast<Instr*>(instr4)) &&
+      IsOri(*reinterpret_cast<Instr*>(instr5))) {
+    // Assemble the 64 bit value.
+    int64_t addr = (int64_t)(instr0->Imm20UValue() << kImm20Shift) +
+                   (int64_t)instr1->Imm12Value();
+    addr <<= 11;
+    addr |= (int64_t)instr3->Imm12Value();
+    addr <<= 6;
+    addr |= (int64_t)instr5->Imm12Value();
+
+    DEBUG_PRINTF("addr: %lx\n", addr);
+    return static_cast<Address>(addr);
+  }
+  // We should never get here, force a bad address if we do.
+  UNREACHABLE();
+}
+// On RISC-V, a 48-bit target address is stored in an 6-instruction sequence:
+//  lui(reg, (int32_t)high_20); // 19 high bits
+//  addi(reg, reg, low_12); // 12 following bits. total is 31 high bits in reg.
+//  slli(reg, reg, 11); // Space for next 11 bits
+//  ori(reg, reg, b11); // 11 bits are put in. 42 bit in reg
+//  slli(reg, reg, 6); // Space for next 6 bits
+//  ori(reg, reg, a6); // 6 bits are put in. all 48 bis in reg
+//
+// Patching the address must replace all instructions, and flush the i-cache.
+// Note that this assumes the use of SV48, the 48-bit virtual memory system.
+void Assembler::set_target_value_at(Address pc, uint64_t target,
+                                    ICacheFlushMode icache_flush_mode) {
+  DEBUG_PRINTF("set_target_value_at: pc: %lx\ttarget: %lx\n", pc, target);
+  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
+  DCHECK_EQ((target & 0xffff000000000000ll), 0);
+#ifdef DEBUG
+  // Check we have the result from a li macro-instruction.
+  Instruction* instr0 = Instruction::At((unsigned char*)pc);
+  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
+  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
+  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));
+  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
+         IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
+         IsOri(*reinterpret_cast<Instr*>(instr3)) &&
+         IsOri(*reinterpret_cast<Instr*>(instr5)));
+#endif
+  int64_t a6 = target & 0x3f;                     // bits 0:6. 6 bits
+  int64_t b11 = (target >> 6) & 0x7ff;            // bits 6:11. 11 bits
+  int64_t high_31 = (target >> 17) & 0x7fffffff;  // 31 bits
+  int64_t high_20 = ((high_31 + 0x800) >> 12);    // 19 bits
+  int64_t low_12 = high_31 & 0xfff;               // 12 bits
+  *p = *p & 0xfff;
+  *p = *p | ((int32_t)high_20 << 12);
+  *(p + 1) = *(p + 1) & 0xfffff;
+  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
+  *(p + 2) = *(p + 2) & 0xfffff;
+  *(p + 2) = *(p + 2) | (11 << 20);
+  *(p + 3) = *(p + 3) & 0xfffff;
+  *(p + 3) = *(p + 3) | ((int32_t)b11 << 20);
+  *(p + 4) = *(p + 4) & 0xfffff;
+  *(p + 4) = *(p + 4) | (6 << 20);
+  *(p + 5) = *(p + 5) & 0xfffff;
+  *(p + 5) = *(p + 5) | ((int32_t)a6 << 20);
+  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
+    FlushInstructionCache(pc, 8 * kInstrSize);
+  }
+  DCHECK_EQ(target_address_at(pc), target);
+}
+#elif V8_TARGET_ARCH_RISCV32
+Address Assembler::target_address_at(Address pc) {
+  DEBUG_PRINTF("target_address_at: pc: %x\t", pc);
+  Instruction* instr0 = Instruction::At((unsigned char*)pc);
+  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
+
+  // Interpret instructions for address generated by li: See listing in
+  // Assembler::set_target_address_at() just below.
+  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
+      IsAddi(*reinterpret_cast<Instr*>(instr1))) {
+    // Assemble the 32bit value.
+    int32_t addr = (int32_t)(instr0->Imm20UValue() << kImm20Shift) +
+                   (int32_t)instr1->Imm12Value();
+    DEBUG_PRINTF("addr: %x\n", addr);
+    return static_cast<Address>(addr);
+  }
+  // We should never get here, force a bad address if we do.
+  UNREACHABLE();
+}
+// On RISC-V, a 32-bit target address is stored in an 2-instruction sequence:
+//  lui(reg, high_20); // 20 high bits
+//  addi(reg, reg, low_12); // 12 following bits. total is 31 high bits in reg.
+//
+// Patching the address must replace all instructions, and flush the i-cache.
+void Assembler::set_target_value_at(Address pc, uint32_t target,
+                                    ICacheFlushMode icache_flush_mode) {
+  DEBUG_PRINTF("set_target_value_at: pc: %x\ttarget: %x\n", pc, target);
+  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
+#ifdef DEBUG
+  // Check we have the result from a li macro-instruction.
+  Instruction* instr0 = Instruction::At((unsigned char*)pc);
+  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
+  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
+         IsAddi(*reinterpret_cast<Instr*>(instr1)));
+#endif
+  int32_t high_20 = ((target + 0x800) >> 12);  // 20 bits
+  int32_t low_12 = target & 0xfff;             // 12 bits
+  *p = *p & 0xfff;
+  *p = *p | ((int32_t)high_20 << 12);
+  *(p + 1) = *(p + 1) & 0xfffff;
+  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
+  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
+    FlushInstructionCache(pc, 2 * kInstrSize);
+  }
+  DCHECK_EQ(target_address_at(pc), target);
+}
+#endif
+
+UseScratchRegisterScope::UseScratchRegisterScope(Assembler* assembler)
+    : available_(assembler->GetScratchRegisterList()),
+      old_available_(*available_) {}
+
+UseScratchRegisterScope::~UseScratchRegisterScope() {
+  *available_ = old_available_;
+}
+
+Register UseScratchRegisterScope::Acquire() {
+  DCHECK_NOT_NULL(available_);
+  DCHECK(!available_->is_empty());
+  int index =
+      static_cast<int>(base::bits::CountTrailingZeros32(available_->bits()));
+  *available_ &= RegList::FromBits(~(1U << index));
+
+  return Register::from_code(index);
+}
+
+bool UseScratchRegisterScope::hasAvailable() const {
+  return !available_->is_empty();
+}
+
+bool Assembler::IsConstantPoolAt(Instruction* instr) {
+  // The constant pool marker is made of two instructions. These instructions
+  // will never be emitted by the JIT, so checking for the first one is enough:
+  // 0: ld x0, x0, #offset
+  Instr instr_value = *reinterpret_cast<Instr*>(instr);
+#if V8_TARGET_ARCH_RISCV64
+  bool result = IsLd(instr_value) && (instr->Rs1Value() == kRegCode_zero_reg) &&
+                (instr->RdValue() == kRegCode_zero_reg);
+#elif V8_TARGET_ARCH_RISCV32
+  bool result = IsLw(instr_value) && (instr->Rs1Value() == kRegCode_zero_reg) &&
+                (instr->RdValue() == kRegCode_zero_reg);
+#endif
+#ifdef DEBUG
+  // It is still worth asserting the marker is complete.
+  // 1: j 0x0
+  Instruction* instr_following = instr + kInstrSize;
+  DCHECK(!result || (IsJal(*reinterpret_cast<Instr*>(instr_following)) &&
+                     instr_following->Imm20JValue() == 0 &&
+                     instr_following->RdValue() == kRegCode_zero_reg));
+#endif
+  return result;
+}
+
+int Assembler::ConstantPoolSizeAt(Instruction* instr) {
+  if (IsConstantPoolAt(instr)) {
+    return instr->Imm12Value();
+  } else {
+    return -1;
+  }
+}
+
+void Assembler::RecordConstPool(int size) {
+  // We only need this for debugger support, to correctly compute offsets in the
+  // code.
+  Assembler::BlockPoolsScope block_pools(this);
+  RecordRelocInfo(RelocInfo::CONST_POOL, static_cast<intptr_t>(size));
+}
+
+void Assembler::EmitPoolGuard() {
+  // We must generate only one instruction as this is used in scopes that
+  // control the size of the code generated.
+  j(0);
+}
+
+// -----------------------------------------------------------------------------
+// Assembler.
+template <typename T>
+void Assembler::EmitHelper(T x) {
+  *reinterpret_cast<T*>(pc_) = x;
+  pc_ += sizeof(x);
+}
+
+void Assembler::emit(Instr x) {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  DEBUG_PRINTF("%p(%x): ", pc_, pc_offset());
+  disassembleInstr(x);
+  EmitHelper(x);
+  CheckTrampolinePoolQuick();
+}
+
+void Assembler::emit(ShortInstr x) {
+  if (!is_buffer_growth_blocked()) {
+    CheckBuffer();
+  }
+  DEBUG_PRINTF("%p(%x): ", pc_, pc_offset());
+  disassembleInstr(x);
+  EmitHelper(x);
+  CheckTrampolinePoolQuick();
+}
+
+void Assembler::emit(uint64_t data) {
+  if (!is_buffer_growth_blocked()) CheckBuffer();
+  EmitHelper(data);
+}
+
+// Constant Pool
+
+void ConstantPool::EmitPrologue(Alignment require_alignment) {
+  // Recorded constant pool size is expressed in number of 32-bits words,
+  // and includes prologue and alignment, but not the jump around the pool
+  // and the size of the marker itself.
+  const int marker_size = 1;
+  int word_count =
+      ComputeSize(Jump::kOmitted, require_alignment) / kInt32Size - marker_size;
+#if V8_TARGET_ARCH_RISCV64
+  assm_->ld(zero_reg, zero_reg, word_count);
+#elif V8_TARGET_ARCH_RISCV32
+  assm_->lw(zero_reg, zero_reg, word_count);
+#endif
+  assm_->EmitPoolGuard();
+}
+
+int ConstantPool::PrologueSize(Jump require_jump) const {
+  // Prologue is:
+  //   j over  ;; if require_jump
+  //   ld x0, x0, #pool_size
+  //   j 0x0
+  int prologue_size = require_jump == Jump::kRequired ? kInstrSize : 0;
+  prologue_size += 2 * kInstrSize;
+  return prologue_size;
+}
+
+void ConstantPool::SetLoadOffsetToConstPoolEntry(int load_offset,
+                                                 Instruction* entry_offset,
+                                                 const ConstantPoolKey& key) {
+  Instr instr_auipc = assm_->instr_at(load_offset);
+  Instr instr_load = assm_->instr_at(load_offset + 4);
+  // Instruction to patch must be 'ld/lw rd, offset(rd)' with 'offset == 0'.
+  DCHECK(assm_->IsAuipc(instr_auipc));
+#if V8_TARGET_ARCH_RISCV64
+  DCHECK(assm_->IsLd(instr_load));
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK(assm_->IsLw(instr_load));
+#endif
+  DCHECK_EQ(assm_->LoadOffset(instr_load), 0);
+  DCHECK_EQ(assm_->AuipcOffset(instr_auipc), 0);
+  int32_t distance = static_cast<int32_t>(
+      reinterpret_cast<Address>(entry_offset) -
+      reinterpret_cast<Address>(assm_->toAddress(load_offset)));
+  CHECK(is_int32(distance + 0x800));
+  int32_t Hi20 = (((int32_t)distance + 0x800) >> 12);
+  int32_t Lo12 = (int32_t)distance << 20 >> 20;
+  assm_->instr_at_put(load_offset, SetAuipcOffset(Hi20, instr_auipc));
+  assm_->instr_at_put(load_offset + 4, SetLoadOffset(Lo12, instr_load));
+}
+
+void ConstantPool::Check(Emission force_emit, Jump require_jump,
+                         size_t margin) {
+  // Some short sequence of instruction must not be broken up by constant pool
+  // emission, such sequences are protected by a ConstPool::BlockScope.
+  if (IsBlocked()) {
+    // Something is wrong if emission is forced and blocked at the same time.
+    DCHECK_EQ(force_emit, Emission::kIfNeeded);
+    return;
+  }
+
+  // We emit a constant pool only if :
+  //  * it is not empty
+  //  * emission is forced by parameter force_emit (e.g. at function end).
+  //  * emission is mandatory or opportune according to {ShouldEmitNow}.
+  if (!IsEmpty() && (force_emit == Emission::kForced ||
+                     ShouldEmitNow(require_jump, margin))) {
+    // Emit veneers for branches that would go out of range during emission of
+    // the constant pool.
+    int worst_case_size = ComputeSize(Jump::kRequired, Alignment::kRequired);
+
+    // Check that the code buffer is large enough before emitting the constant
+    // pool (this includes the gap to the relocation information).
+    int needed_space = worst_case_size + assm_->kGap;
+    while (assm_->buffer_space() <= needed_space) {
+      assm_->GrowBuffer();
+    }
+
+    EmitAndClear(require_jump);
+  }
+  // Since a constant pool is (now) empty, move the check offset forward by
+  // the standard interval.
+  SetNextCheckIn(ConstantPool::kCheckInterval);
+}
+
+// Pool entries are accessed with pc relative load therefore this cannot be more
+// than 1 * MB. Since constant pool emission checks are interval based, and we
+// want to keep entries close to the code, we try to emit every 64KB.
+const size_t ConstantPool::kMaxDistToPool32 = 1 * MB;
+const size_t ConstantPool::kMaxDistToPool64 = 1 * MB;
+const size_t ConstantPool::kCheckInterval = 128 * kInstrSize;
+const size_t ConstantPool::kApproxDistToPool32 = 64 * KB;
+const size_t ConstantPool::kApproxDistToPool64 = kApproxDistToPool32;
+
+const size_t ConstantPool::kOpportunityDistToPool32 = 64 * KB;
+const size_t ConstantPool::kOpportunityDistToPool64 = 64 * KB;
+const size_t ConstantPool::kApproxMaxEntryCount = 512;
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/assembler-riscv.h b/src/codegen/riscv/assembler-riscv.h
new file mode 100644
index 00000000000..5a5df7dfae5
--- /dev/null
+++ b/src/codegen/riscv/assembler-riscv.h
@@ -0,0 +1,848 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2021 the V8 project authors. All rights reserved.
+
+#ifndef V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_
+#define V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_
+
+#include <stdio.h>
+
+#include <memory>
+#include <set>
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/constant-pool.h"
+#include "src/codegen/constants-arch.h"
+#include "src/codegen/external-reference.h"
+#include "src/codegen/flush-instruction-cache.h"
+#include "src/codegen/label.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/base-riscv-i.h"
+#include "src/codegen/riscv/extension-riscv-a.h"
+#include "src/codegen/riscv/extension-riscv-c.h"
+#include "src/codegen/riscv/extension-riscv-d.h"
+#include "src/codegen/riscv/extension-riscv-f.h"
+#include "src/codegen/riscv/extension-riscv-m.h"
+#include "src/codegen/riscv/extension-riscv-v.h"
+#include "src/codegen/riscv/extension-riscv-zicsr.h"
+#include "src/codegen/riscv/extension-riscv-zifencei.h"
+#include "src/codegen/riscv/register-riscv.h"
+#include "src/objects/contexts.h"
+#include "src/objects/smi.h"
+
+namespace v8 {
+namespace internal {
+
+#define DEBUG_PRINTF(...) \
+  if (FLAG_riscv_debug) { \
+    printf(__VA_ARGS__);  \
+  }
+
+class SafepointTableBuilder;
+
+// -----------------------------------------------------------------------------
+// Machine instruction Operands.
+constexpr int kSmiShift = kSmiTagSize + kSmiShiftSize;
+constexpr uintptr_t kSmiShiftMask = (1UL << kSmiShift) - 1;
+// Class Operand represents a shifter operand in data processing instructions.
+class Operand {
+ public:
+  // Immediate.
+  V8_INLINE explicit Operand(intptr_t immediate,
+                             RelocInfo::Mode rmode = RelocInfo::NO_INFO)
+      : rm_(no_reg), rmode_(rmode) {
+    value_.immediate = immediate;
+  }
+  V8_INLINE explicit Operand(const ExternalReference& f)
+      : rm_(no_reg), rmode_(RelocInfo::EXTERNAL_REFERENCE) {
+    value_.immediate = static_cast<intptr_t>(f.address());
+  }
+
+  explicit Operand(Handle<HeapObject> handle);
+  V8_INLINE explicit Operand(Smi value)
+      : rm_(no_reg), rmode_(RelocInfo::NO_INFO) {
+    value_.immediate = static_cast<intptr_t>(value.ptr());
+  }
+
+  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.
+  static Operand EmbeddedStringConstant(const StringConstantBase* str);
+
+  // Register.
+  V8_INLINE explicit Operand(Register rm) : rm_(rm) {}
+
+  // Return true if this is a register operand.
+  V8_INLINE bool is_reg() const { return rm_.is_valid(); }
+  inline intptr_t immediate() const {
+    DCHECK(!is_reg());
+    DCHECK(!IsHeapObjectRequest());
+    return value_.immediate;
+  }
+
+  bool IsImmediate() const { return !rm_.is_valid(); }
+
+  HeapObjectRequest heap_object_request() const {
+    DCHECK(IsHeapObjectRequest());
+    return value_.heap_object_request;
+  }
+
+  bool IsHeapObjectRequest() const {
+    DCHECK_IMPLIES(is_heap_object_request_, IsImmediate());
+    DCHECK_IMPLIES(is_heap_object_request_,
+                   rmode_ == RelocInfo::FULL_EMBEDDED_OBJECT ||
+                       rmode_ == RelocInfo::CODE_TARGET);
+    return is_heap_object_request_;
+  }
+
+  Register rm() const { return rm_; }
+
+  RelocInfo::Mode rmode() const { return rmode_; }
+
+ private:
+  Register rm_;
+  union Value {
+    Value() {}
+    HeapObjectRequest heap_object_request;  // if is_heap_object_request_
+    intptr_t immediate;                     // otherwise
+  } value_;                                 // valid if rm_ == no_reg
+  bool is_heap_object_request_ = false;
+  RelocInfo::Mode rmode_;
+
+  friend class Assembler;
+  friend class MacroAssembler;
+};
+
+// On RISC-V we have only one addressing mode with base_reg + offset.
+// Class MemOperand represents a memory operand in load and store instructions.
+class V8_EXPORT_PRIVATE MemOperand : public Operand {
+ public:
+  // Immediate value attached to offset.
+  enum OffsetAddend { offset_minus_one = -1, offset_zero = 0 };
+
+  explicit MemOperand(Register rn, int32_t offset = 0);
+  explicit MemOperand(Register rn, int32_t unit, int32_t multiplier,
+                      OffsetAddend offset_addend = offset_zero);
+  int32_t offset() const { return offset_; }
+
+  void set_offset(int32_t offset) { offset_ = offset; }
+
+  bool OffsetIsInt12Encodable() const { return is_int12(offset_); }
+
+ private:
+  int32_t offset_;
+
+  friend class Assembler;
+};
+
+class V8_EXPORT_PRIVATE Assembler : public AssemblerBase,
+                                    public AssemblerRISCVI,
+                                    public AssemblerRISCVA,
+                                    public AssemblerRISCVF,
+                                    public AssemblerRISCVD,
+                                    public AssemblerRISCVM,
+                                    public AssemblerRISCVC,
+                                    public AssemblerRISCVZifencei,
+                                    public AssemblerRISCVZicsr,
+                                    public AssemblerRISCVV {
+ public:
+  // Create an assembler. Instructions and relocation information are emitted
+  // into a buffer, with the instructions starting from the beginning and the
+  // relocation information starting from the end of the buffer. See CodeDesc
+  // for a detailed comment on the layout (globals.h).
+  //
+  // If the provided buffer is nullptr, the assembler allocates and grows its
+  // own buffer. Otherwise it takes ownership of the provided buffer.
+  explicit Assembler(const AssemblerOptions&,
+                     std::unique_ptr<AssemblerBuffer> = {});
+
+  virtual ~Assembler();
+  void AbortedCodeGeneration();
+  // GetCode emits any pending (non-emitted) code and fills the descriptor desc.
+  static constexpr int kNoHandlerTable = 0;
+  static constexpr SafepointTableBuilder* kNoSafepointTable = nullptr;
+  void GetCode(Isolate* isolate, CodeDesc* desc,
+               SafepointTableBuilder* safepoint_table_builder,
+               int handler_table_offset);
+
+  // Convenience wrapper for code without safepoint or handler tables.
+  void GetCode(Isolate* isolate, CodeDesc* desc) {
+    GetCode(isolate, desc, kNoSafepointTable, kNoHandlerTable);
+  }
+
+  // Unused on this architecture.
+  void MaybeEmitOutOfLineConstantPool() {}
+
+  // Label operations & relative jumps (PPUM Appendix D).
+  //
+  // Takes a branch opcode (cc) and a label (L) and generates
+  // either a backward branch or a forward branch and links it
+  // to the label fixup chain. Usage:
+  //
+  // Label L;    // unbound label
+  // j(cc, &L);  // forward branch to unbound label
+  // bind(&L);   // bind label to the current pc
+  // j(cc, &L);  // backward branch to bound label
+  // bind(&L);   // illegal: a label may be bound only once
+  //
+  // Note: The same Label can be used for forward and backward branches
+  // but it may be bound only once.
+  void bind(Label* L);  // Binds an unbound label L to current code position.
+
+  // Determines if Label is bound and near enough so that branch instruction
+  // can be used to reach it, instead of jump instruction.
+  bool is_near(Label* L);
+  bool is_near(Label* L, OffsetSize bits);
+  bool is_near_branch(Label* L);
+
+  // Get offset from instr.
+  int BranchOffset(Instr instr);
+  static int BrachlongOffset(Instr auipc, Instr jalr);
+  static int PatchBranchlongOffset(Address pc, Instr auipc, Instr instr_I,
+                                   int32_t offset);
+
+  // Returns the branch offset to the given label from the current code
+  // position. Links the label to the current position if it is still unbound.
+  // Manages the jump elimination optimization if the second parameter is true.
+  virtual int32_t branch_offset_helper(Label* L, OffsetSize bits);
+  uintptr_t jump_address(Label* L);
+  int32_t branch_long_offset(Label* L);
+
+  // Puts a labels target address at the given position.
+  // The high 8 bits are set to zero.
+  void label_at_put(Label* L, int at_offset);
+
+  // Read/Modify the code target address in the branch/call instruction at pc.
+  // The isolate argument is unused (and may be nullptr) when skipping flushing.
+  static Address target_address_at(Address pc);
+  V8_INLINE static void set_target_address_at(
+      Address pc, Address target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED) {
+    set_target_value_at(pc, target, icache_flush_mode);
+  }
+
+  static Address target_address_at(Address pc, Address constant_pool);
+
+  static void set_target_address_at(
+      Address pc, Address constant_pool, Address target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
+
+  // Read/Modify the code target address in the branch/call instruction at pc.
+  inline static Tagged_t target_compressed_address_at(Address pc,
+                                                      Address constant_pool);
+  inline static void set_target_compressed_address_at(
+      Address pc, Address constant_pool, Tagged_t target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
+
+  inline Handle<Object> code_target_object_handle_at(Address pc,
+                                                     Address constant_pool);
+  inline Handle<HeapObject> compressed_embedded_object_handle_at(
+      Address pc, Address constant_pool);
+
+  static bool IsConstantPoolAt(Instruction* instr);
+  static int ConstantPoolSizeAt(Instruction* instr);
+  // See Assembler::CheckConstPool for more info.
+  void EmitPoolGuard();
+
+  static void set_target_value_at(
+      Address pc, uintptr_t target,
+      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
+
+  static void JumpLabelToJumpRegister(Address pc);
+
+  // This sets the branch destination (which gets loaded at the call address).
+  // This is for calls and branches within generated code.  The serializer
+  // has already deserialized the lui/ori instructions etc.
+  inline static void deserialization_set_special_target_at(
+      Address instruction_payload, Code code, Address target);
+
+  // Get the size of the special target encoded at 'instruction_payload'.
+  inline static int deserialization_special_target_size(
+      Address instruction_payload);
+
+  // This sets the internal reference at the pc.
+  inline static void deserialization_set_target_internal_reference_at(
+      Address pc, Address target,
+      RelocInfo::Mode mode = RelocInfo::INTERNAL_REFERENCE);
+
+  // Here we are patching the address in the LUI/ADDI instruction pair.
+  // These values are used in the serialization process and must be zero for
+  // RISC-V platform, as Code, Embedded Object or External-reference pointers
+  // are split across two consecutive instructions and don't exist separately
+  // in the code, so the serializer should not step forwards in memory after
+  // a target is resolved and written.
+  static constexpr int kSpecialTargetSize = 0;
+
+  // Number of consecutive instructions used to store 32bit/64bit constant.
+  // This constant was used in RelocInfo::target_address_address() function
+  // to tell serializer address of the instruction that follows
+  // LUI/ADDI instruction pair.
+  static constexpr int kInstructionsFor32BitConstant = 2;
+  static constexpr int kInstructionsFor64BitConstant = 8;
+
+  // Difference between address of current opcode and value read from pc
+  // register.
+  static constexpr int kPcLoadDelta = 4;
+
+  // Bits available for offset field in branches
+  static constexpr int kBranchOffsetBits = 13;
+
+  // Bits available for offset field in jump
+  static constexpr int kJumpOffsetBits = 21;
+
+  // Bits available for offset field in compresed jump
+  static constexpr int kCJalOffsetBits = 12;
+
+  // Bits available for offset field in compressed branch
+  static constexpr int kCBranchOffsetBits = 9;
+
+  // Max offset for b instructions with 12-bit offset field (multiple of 2)
+  static constexpr int kMaxBranchOffset = (1 << (13 - 1)) - 1;
+
+  // Max offset for jal instruction with 20-bit offset field (multiple of 2)
+  static constexpr int kMaxJumpOffset = (1 << (21 - 1)) - 1;
+
+  static constexpr int kTrampolineSlotsSize = 2 * kInstrSize;
+
+  RegList* GetScratchRegisterList() { return &scratch_register_list_; }
+
+  // ---------------------------------------------------------------------------
+  // Code generation.
+
+  // Insert the smallest number of nop instructions
+  // possible to align the pc offset to a multiple
+  // of m. m must be a power of 2 (>= 4).
+  void Align(int m);
+  // Insert the smallest number of zero bytes possible to align the pc offset
+  // to a mulitple of m. m must be a power of 2 (>= 2).
+  void DataAlign(int m);
+  // Aligns code to something that's optimal for a jump target for the platform.
+  void CodeTargetAlign();
+  void LoopHeaderAlign() { CodeTargetAlign(); }
+
+  // Different nop operations are used by the code generator to detect certain
+  // states of the generated code.
+  enum NopMarkerTypes {
+    NON_MARKING_NOP = 0,
+    DEBUG_BREAK_NOP,
+    // IC markers.
+    PROPERTY_ACCESS_INLINED,
+    PROPERTY_ACCESS_INLINED_CONTEXT,
+    PROPERTY_ACCESS_INLINED_CONTEXT_DONT_DELETE,
+    // Helper values.
+    LAST_CODE_MARKER,
+    FIRST_IC_MARKER = PROPERTY_ACCESS_INLINED,
+  };
+
+  void NOP();
+  void EBREAK();
+
+  // Assembler Pseudo Instructions (Tables 25.2, 25.3, RISC-V Unprivileged ISA)
+  void nop();
+  void RV_li(Register rd, intptr_t imm);
+  // Returns the number of instructions required to load the immediate
+  static int li_estimate(intptr_t imm, bool is_get_temp_reg = false);
+  // Loads an immediate, always using 8 instructions, regardless of the value,
+  // so that it can be modified later.
+  void li_constant(Register rd, intptr_t imm);
+  void li_ptr(Register rd, intptr_t imm);
+
+  void break_(uint32_t code, bool break_as_stop = false);
+  void stop(uint32_t code = kMaxStopCode);
+
+  // Check the code size generated from label to here.
+  int SizeOfCodeGeneratedSince(Label* label) {
+    return pc_offset() - label->pos();
+  }
+
+  // Check the number of instructions generated from label to here.
+  int InstructionsGeneratedSince(Label* label) {
+    return SizeOfCodeGeneratedSince(label) / kInstrSize;
+  }
+
+  using BlockConstPoolScope = ConstantPool::BlockScope;
+  // Class for scoping postponing the trampoline pool generation.
+  class BlockTrampolinePoolScope {
+   public:
+    explicit BlockTrampolinePoolScope(Assembler* assem, int margin = 0)
+        : assem_(assem) {
+      assem_->StartBlockTrampolinePool();
+    }
+
+    explicit BlockTrampolinePoolScope(Assembler* assem, PoolEmissionCheck check)
+        : assem_(assem) {
+      assem_->StartBlockTrampolinePool();
+    }
+    ~BlockTrampolinePoolScope() { assem_->EndBlockTrampolinePool(); }
+
+   private:
+    Assembler* assem_;
+    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockTrampolinePoolScope);
+  };
+
+  // Class for postponing the assembly buffer growth. Typically used for
+  // sequences of instructions that must be emitted as a unit, before
+  // buffer growth (and relocation) can occur.
+  // This blocking scope is not nestable.
+  class BlockGrowBufferScope {
+   public:
+    explicit BlockGrowBufferScope(Assembler* assem) : assem_(assem) {
+      assem_->StartBlockGrowBuffer();
+    }
+    ~BlockGrowBufferScope() { assem_->EndBlockGrowBuffer(); }
+
+   private:
+    Assembler* assem_;
+
+    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockGrowBufferScope);
+  };
+
+  // Record a deoptimization reason that can be used by a log or cpu profiler.
+  // Use --trace-deopt to enable.
+  void RecordDeoptReason(DeoptimizeReason reason, uint32_t node_id,
+                         SourcePosition position, int id);
+
+  static int RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
+                                       intptr_t pc_delta);
+  static void RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
+                                        intptr_t pc_delta);
+
+  // Writes a single byte or word of data in the code stream.  Used for
+  // inline tables, e.g., jump-tables.
+  void db(uint8_t data);
+  void dd(uint32_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO);
+  void dq(uint64_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO);
+  void dp(uintptr_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO) {
+    dq(data, rmode);
+  }
+  void dd(Label* label);
+
+  Instruction* pc() const { return reinterpret_cast<Instruction*>(pc_); }
+
+  // Postpone the generation of the trampoline pool for the specified number of
+  // instructions.
+  void BlockTrampolinePoolFor(int instructions);
+
+  // Check if there is less than kGap bytes available in the buffer.
+  // If this is the case, we need to grow the buffer before emitting
+  // an instruction or relocation information.
+  inline bool overflow() const { return pc_ >= reloc_info_writer.pos() - kGap; }
+
+  // Get the number of bytes available in the buffer.
+  inline intptr_t available_space() const {
+    return reloc_info_writer.pos() - pc_;
+  }
+
+  // Read/patch instructions.
+  static Instr instr_at(Address pc) { return *reinterpret_cast<Instr*>(pc); }
+  static void instr_at_put(Address pc, Instr instr) {
+    *reinterpret_cast<Instr*>(pc) = instr;
+  }
+  Instr instr_at(int pos) {
+    return *reinterpret_cast<Instr*>(buffer_start_ + pos);
+  }
+  void instr_at_put(int pos, Instr instr) {
+    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
+  }
+
+  void instr_at_put(int pos, ShortInstr instr) {
+    *reinterpret_cast<ShortInstr*>(buffer_start_ + pos) = instr;
+  }
+
+  Address toAddress(int pos) {
+    return reinterpret_cast<Address>(buffer_start_ + pos);
+  }
+
+  void CheckTrampolinePool();
+
+  // Get the code target object for a pc-relative call or jump.
+  V8_INLINE Handle<Code> relative_code_target_object_handle_at(
+      Address pc_) const;
+
+  inline int UnboundLabelsCount() { return unbound_labels_count_; }
+
+  using BlockPoolsScope = BlockTrampolinePoolScope;
+
+  void RecordConstPool(int size);
+
+  void ForceConstantPoolEmissionWithoutJump() {
+    constpool_.Check(Emission::kForced, Jump::kOmitted);
+  }
+  void ForceConstantPoolEmissionWithJump() {
+    constpool_.Check(Emission::kForced, Jump::kRequired);
+  }
+  // Check if the const pool needs to be emitted while pretending that {margin}
+  // more bytes of instructions have already been emitted.
+  void EmitConstPoolWithJumpIfNeeded(size_t margin = 0) {
+    constpool_.Check(Emission::kIfNeeded, Jump::kRequired, margin);
+  }
+
+  void EmitConstPoolWithoutJumpIfNeeded(size_t margin = 0) {
+    constpool_.Check(Emission::kIfNeeded, Jump::kOmitted, margin);
+  }
+
+  void RecordEntry(uint32_t data, RelocInfo::Mode rmode) {
+    constpool_.RecordEntry(data, rmode);
+  }
+
+  void RecordEntry(uint64_t data, RelocInfo::Mode rmode) {
+    constpool_.RecordEntry(data, rmode);
+  }
+
+  void CheckTrampolinePoolQuick(int extra_instructions = 0) {
+    DEBUG_PRINTF("\tpc_offset:%d %d\n", pc_offset(),
+                 next_buffer_check_ - extra_instructions * kInstrSize);
+    if (pc_offset() >= next_buffer_check_ - extra_instructions * kInstrSize) {
+      CheckTrampolinePool();
+    }
+  }
+
+  friend class VectorUnit;
+  class VectorUnit {
+   public:
+    inline int32_t sew() const { return 2 ^ (sew_ + 3); }
+
+    inline int32_t vlmax() const {
+      if ((lmul_ & 0b100) != 0) {
+        return (kRvvVLEN / sew()) >> (lmul_ & 0b11);
+      } else {
+        return ((kRvvVLEN << lmul_) / sew());
+      }
+    }
+
+    explicit VectorUnit(Assembler* assm) : assm_(assm) {}
+
+    void set(Register rd, VSew sew, Vlmul lmul) {
+      if (sew != sew_ || lmul != lmul_ || vl != vlmax()) {
+        sew_ = sew;
+        lmul_ = lmul;
+        vl = vlmax();
+        assm_->vsetvlmax(rd, sew_, lmul_);
+      }
+    }
+
+    void set(Register rd, int8_t sew, int8_t lmul) {
+      DCHECK_GE(sew, E8);
+      DCHECK_LE(sew, E64);
+      DCHECK_GE(lmul, m1);
+      DCHECK_LE(lmul, mf2);
+      set(rd, VSew(sew), Vlmul(lmul));
+    }
+
+    void set(FPURoundingMode mode) {
+      if (mode_ != mode) {
+        assm_->addi(kScratchReg, zero_reg, mode << kFcsrFrmShift);
+        assm_->fscsr(kScratchReg);
+        mode_ = mode;
+      }
+    }
+    void set(Register rd, Register rs1, VSew sew, Vlmul lmul) {
+      if (sew != sew_ || lmul != lmul_) {
+        sew_ = sew;
+        lmul_ = lmul;
+        vl = 0;
+        assm_->vsetvli(rd, rs1, sew_, lmul_);
+      }
+    }
+
+    void set(VSew sew, Vlmul lmul) {
+      if (sew != sew_ || lmul != lmul_) {
+        sew_ = sew;
+        lmul_ = lmul;
+        assm_->vsetvl(sew_, lmul_);
+      }
+    }
+
+   private:
+    VSew sew_ = E8;
+    Vlmul lmul_ = m1;
+    int32_t vl = 0;
+    Assembler* assm_;
+    FPURoundingMode mode_ = RNE;
+  };
+
+  VectorUnit VU;
+
+ protected:
+  // Readable constants for base and offset adjustment helper, these indicate if
+  // aside from offset, another value like offset + 4 should fit into int16.
+  enum class OffsetAccessType : bool {
+    SINGLE_ACCESS = false,
+    TWO_ACCESSES = true
+  };
+
+  // Determine whether need to adjust base and offset of memroy load/store
+  bool NeedAdjustBaseAndOffset(
+      const MemOperand& src, OffsetAccessType = OffsetAccessType::SINGLE_ACCESS,
+      int second_Access_add_to_offset = 4);
+
+  // Helper function for memory load/store using base register and offset.
+  void AdjustBaseAndOffset(
+      MemOperand* src, Register scratch,
+      OffsetAccessType access_type = OffsetAccessType::SINGLE_ACCESS,
+      int second_access_add_to_offset = 4);
+
+  inline static void set_target_internal_reference_encoded_at(Address pc,
+                                                              Address target);
+
+  intptr_t buffer_space() const { return reloc_info_writer.pos() - pc_; }
+
+  // Decode branch instruction at pos and return branch target pos.
+  int target_at(int pos, bool is_internal);
+
+  // Patch branch instruction at pos to branch to given branch target pos.
+  void target_at_put(int pos, int target_pos, bool is_internal,
+                     bool trampoline = false);
+
+  // Say if we need to relocate with this mode.
+  bool MustUseReg(RelocInfo::Mode rmode);
+
+  // Record reloc info for current pc_.
+  void RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data = 0);
+
+  // Block the emission of the trampoline pool before pc_offset.
+  void BlockTrampolinePoolBefore(int pc_offset) {
+    if (no_trampoline_pool_before_ < pc_offset)
+      no_trampoline_pool_before_ = pc_offset;
+  }
+
+  void StartBlockTrampolinePool() {
+    DEBUG_PRINTF("\tStartBlockTrampolinePool\n");
+    trampoline_pool_blocked_nesting_++;
+  }
+
+  void EndBlockTrampolinePool() {
+    trampoline_pool_blocked_nesting_--;
+    DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
+                 trampoline_pool_blocked_nesting_);
+    if (trampoline_pool_blocked_nesting_ == 0) {
+      CheckTrampolinePoolQuick(1);
+    }
+  }
+
+  bool is_trampoline_pool_blocked() const {
+    return trampoline_pool_blocked_nesting_ > 0;
+  }
+
+  bool has_exception() const { return internal_trampoline_exception_; }
+
+  bool is_trampoline_emitted() const { return trampoline_emitted_; }
+
+  // Temporarily block automatic assembly buffer growth.
+  void StartBlockGrowBuffer() {
+    DCHECK(!block_buffer_growth_);
+    block_buffer_growth_ = true;
+  }
+
+  void EndBlockGrowBuffer() {
+    DCHECK(block_buffer_growth_);
+    block_buffer_growth_ = false;
+  }
+
+  bool is_buffer_growth_blocked() const { return block_buffer_growth_; }
+
+ private:
+  // Avoid overflows for displacements etc.
+  static const int kMaximalBufferSize = 512 * MB;
+
+  // Buffer size and constant pool distance are checked together at regular
+  // intervals of kBufferCheckInterval emitted bytes.
+  static constexpr int kBufferCheckInterval = 1 * KB / 2;
+
+  // Code generation.
+  // The relocation writer's position is at least kGap bytes below the end of
+  // the generated instructions. This is so that multi-instruction sequences do
+  // not have to check for overflow. The same is true for writes of large
+  // relocation info entries.
+  static constexpr int kGap = 64;
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+
+  // Repeated checking whether the trampoline pool should be emitted is rather
+  // expensive. By default we only check again once a number of instructions
+  // has been generated.
+  static constexpr int kCheckConstIntervalInst = 32;
+  static constexpr int kCheckConstInterval =
+      kCheckConstIntervalInst * kInstrSize;
+
+  int next_buffer_check_;  // pc offset of next buffer check.
+
+  // Emission of the trampoline pool may be blocked in some code sequences.
+  int trampoline_pool_blocked_nesting_;  // Block emission if this is not zero.
+  int no_trampoline_pool_before_;  // Block emission before this pc offset.
+
+  // Keep track of the last emitted pool to guarantee a maximal distance.
+  int last_trampoline_pool_end_;  // pc offset of the end of the last pool.
+
+  // Automatic growth of the assembly buffer may be blocked for some sequences.
+  bool block_buffer_growth_;  // Block growth when true.
+
+  // Relocation information generation.
+  // Each relocation is encoded as a variable size value.
+  static constexpr int kMaxRelocSize = RelocInfoWriter::kMaxSize;
+  RelocInfoWriter reloc_info_writer;
+
+  // The bound position, before this we cannot do instruction elimination.
+  int last_bound_pos_;
+
+  // Code emission.
+  inline void CheckBuffer();
+  void GrowBuffer();
+  void emit(Instr x);
+  void emit(ShortInstr x);
+  void emit(uint64_t x);
+  template <typename T>
+  inline void EmitHelper(T x);
+
+  static void disassembleInstr(Instr instr);
+
+  // Labels.
+  void print(const Label* L);
+  void bind_to(Label* L, int pos);
+  void next(Label* L, bool is_internal);
+
+  // One trampoline consists of:
+  // - space for trampoline slots,
+  // - space for labels.
+  //
+  // Space for trampoline slots is equal to slot_count * 2 * kInstrSize.
+  // Space for trampoline slots precedes space for labels. Each label is of one
+  // instruction size, so total amount for labels is equal to
+  // label_count *  kInstrSize.
+  class Trampoline {
+   public:
+    Trampoline() {
+      start_ = 0;
+      next_slot_ = 0;
+      free_slot_count_ = 0;
+      end_ = 0;
+    }
+    Trampoline(int start, int slot_count) {
+      start_ = start;
+      next_slot_ = start;
+      free_slot_count_ = slot_count;
+      end_ = start + slot_count * kTrampolineSlotsSize;
+    }
+    int start() { return start_; }
+    int end() { return end_; }
+    int take_slot() {
+      int trampoline_slot = kInvalidSlotPos;
+      if (free_slot_count_ <= 0) {
+        // We have run out of space on trampolines.
+        // Make sure we fail in debug mode, so we become aware of each case
+        // when this happens.
+        DCHECK(0);
+        // Internal exception will be caught.
+      } else {
+        trampoline_slot = next_slot_;
+        free_slot_count_--;
+        next_slot_ += kTrampolineSlotsSize;
+      }
+      return trampoline_slot;
+    }
+
+   private:
+    int start_;
+    int end_;
+    int next_slot_;
+    int free_slot_count_;
+  };
+
+  int32_t get_trampoline_entry(int32_t pos);
+  int unbound_labels_count_;
+  // After trampoline is emitted, long branches are used in generated code for
+  // the forward branches whose target offsets could be beyond reach of branch
+  // instruction. We use this information to trigger different mode of
+  // branch instruction generation, where we use jump instructions rather
+  // than regular branch instructions.
+  bool trampoline_emitted_ = false;
+  static constexpr int kInvalidSlotPos = -1;
+
+  // Internal reference positions, required for unbounded internal reference
+  // labels.
+  std::set<intptr_t> internal_reference_positions_;
+  bool is_internal_reference(Label* L) {
+    return internal_reference_positions_.find(L->pos()) !=
+           internal_reference_positions_.end();
+  }
+
+  Trampoline trampoline_;
+  bool internal_trampoline_exception_;
+
+  RegList scratch_register_list_;
+
+ private:
+  ConstantPool constpool_;
+
+  void AllocateAndInstallRequestedHeapObjects(Isolate* isolate);
+
+  int WriteCodeComments();
+
+  friend class RegExpMacroAssemblerRISCV;
+  friend class RelocInfo;
+  friend class BlockTrampolinePoolScope;
+  friend class EnsureSpace;
+  friend class ConstantPool;
+};
+
+class EnsureSpace {
+ public:
+  explicit inline EnsureSpace(Assembler* assembler);
+};
+
+class V8_EXPORT_PRIVATE UseScratchRegisterScope {
+ public:
+  explicit UseScratchRegisterScope(Assembler* assembler);
+  ~UseScratchRegisterScope();
+
+  Register Acquire();
+  bool hasAvailable() const;
+  void Include(const RegList& list) { *available_ |= list; }
+  void Exclude(const RegList& list) {
+    *available_ &= RegList::FromBits(~list.bits());
+  }
+  void Include(const Register& reg1, const Register& reg2 = no_reg) {
+    RegList list({reg1, reg2});
+    Include(list);
+  }
+  void Exclude(const Register& reg1, const Register& reg2 = no_reg) {
+    RegList list({reg1, reg2});
+    Exclude(list);
+  }
+
+ private:
+  RegList* available_;
+  RegList old_available_;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_ASSEMBLER_RISCV_H_
diff --git a/src/codegen/riscv/base-assembler-riscv.cc b/src/codegen/riscv/base-assembler-riscv.cc
new file mode 100644
index 00000000000..e6d31b4ffb8
--- /dev/null
+++ b/src/codegen/riscv/base-assembler-riscv.cc
@@ -0,0 +1,492 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2021 the V8 project authors. All rights reserved.
+
+#include "src/codegen/riscv/base-assembler-riscv.h"
+
+#include "src/base/cpu.h"
+
+namespace v8 {
+namespace internal {
+
+// ----- Top-level instruction formats match those in the ISA manual
+// (R, I, S, B, U, J). These match the formats defined in the compiler
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, Register rd, Register rs1,
+                                   Register rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, FPURegister rd,
+                                   FPURegister rs1, FPURegister rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, Register rd,
+                                   FPURegister rs1, Register rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, FPURegister rd,
+                                   Register rs1, Register rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, FPURegister rd,
+                                   FPURegister rs1, Register rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR(uint8_t funct7, uint8_t funct3,
+                                   BaseOpcode opcode, Register rd,
+                                   FPURegister rs1, FPURegister rs2) {
+  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR4(uint8_t funct2, BaseOpcode opcode,
+                                    Register rd, Register rs1, Register rs2,
+                                    Register rs3, FPURoundingMode frm) {
+  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
+         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
+  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrR4(uint8_t funct2, BaseOpcode opcode,
+                                    FPURegister rd, FPURegister rs1,
+                                    FPURegister rs2, FPURegister rs3,
+                                    FPURoundingMode frm) {
+  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
+         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
+  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrRAtomic(uint8_t funct5, bool aq, bool rl,
+                                         uint8_t funct3, Register rd,
+                                         Register rs1, Register rs2) {
+  DCHECK(is_uint5(funct5) && is_uint3(funct3) && rd.is_valid() &&
+         rs1.is_valid() && rs2.is_valid());
+  Instr instr = AMO | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (rl << kRlShift) | (aq << kAqShift) | (funct5 << kFunct5Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrRFrm(uint8_t funct7, BaseOpcode opcode,
+                                      Register rd, Register rs1, Register rs2,
+                                      FPURoundingMode frm) {
+  DCHECK(rd.is_valid() && rs1.is_valid() && rs2.is_valid() && is_uint3(frm));
+  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
+                (funct7 << kFunct7Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrI(uint8_t funct3, BaseOpcode opcode,
+                                   Register rd, Register rs1, int16_t imm12) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         (is_uint12(imm12) || is_int12(imm12)));
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrI(uint8_t funct3, BaseOpcode opcode,
+                                   FPURegister rd, Register rs1,
+                                   int16_t imm12) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         (is_uint12(imm12) || is_int12(imm12)));
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrIShift(bool arithshift, uint8_t funct3,
+                                        BaseOpcode opcode, Register rd,
+                                        Register rs1, uint8_t shamt) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         is_uint6(shamt));
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (shamt << kShamtShift) |
+                (arithshift << kArithShiftShift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrIShiftW(bool arithshift, uint8_t funct3,
+                                         BaseOpcode opcode, Register rd,
+                                         Register rs1, uint8_t shamt) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         is_uint5(shamt));
+  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
+                (rs1.code() << kRs1Shift) | (shamt << kShamtWShift) |
+                (arithshift << kArithShiftShift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrS(uint8_t funct3, BaseOpcode opcode,
+                                   Register rs1, Register rs2, int16_t imm12) {
+  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
+         is_int12(imm12));
+  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
+                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
+                (rs2.code() << kRs2Shift) |
+                ((imm12 & 0xfe0) << 20);  // bits 11-5
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrS(uint8_t funct3, BaseOpcode opcode,
+                                   Register rs1, FPURegister rs2,
+                                   int16_t imm12) {
+  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
+         is_int12(imm12));
+  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
+                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
+                (rs2.code() << kRs2Shift) |
+                ((imm12 & 0xfe0) << 20);  // bits 11-5
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrB(uint8_t funct3, BaseOpcode opcode,
+                                   Register rs1, Register rs2, int16_t imm13) {
+  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
+         is_int13(imm13) && ((imm13 & 1) == 0));
+  Instr instr = opcode | ((imm13 & 0x800) >> 4) |  // bit  11
+                ((imm13 & 0x1e) << 7) |            // bits 4-1
+                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
+                (rs2.code() << kRs2Shift) |
+                ((imm13 & 0x7e0) << 20) |  // bits 10-5
+                ((imm13 & 0x1000) << 19);  // bit 12
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrU(BaseOpcode opcode, Register rd,
+                                   int32_t imm20) {
+  DCHECK(rd.is_valid() && (is_int20(imm20) || is_uint20(imm20)));
+  Instr instr = opcode | (rd.code() << kRdShift) | (imm20 << kImm20Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrJ(BaseOpcode opcode, Register rd,
+                                   int32_t imm21) {
+  DCHECK(rd.is_valid() && is_int21(imm21) && ((imm21 & 1) == 0));
+  Instr instr = opcode | (rd.code() << kRdShift) |
+                (imm21 & 0xff000) |          // bits 19-12
+                ((imm21 & 0x800) << 9) |     // bit  11
+                ((imm21 & 0x7fe) << 20) |    // bits 10-1
+                ((imm21 & 0x100000) << 11);  // bit  20
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCR(uint8_t funct4, BaseOpcode opcode,
+                                    Register rd, Register rs2) {
+  DCHECK(is_uint4(funct4) && rd.is_valid() && rs2.is_valid());
+  ShortInstr instr = opcode | (rs2.code() << kRvcRs2Shift) |
+                     (rd.code() << kRvcRdShift) | (funct4 << kRvcFunct4Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCA(uint8_t funct6, BaseOpcode opcode,
+                                    Register rd, uint8_t funct, Register rs2) {
+  DCHECK(is_uint6(funct6) && rd.is_valid() && rs2.is_valid() &&
+         is_uint2(funct));
+  ShortInstr instr = opcode | ((rs2.code() & 0x7) << kRvcRs2sShift) |
+                     ((rd.code() & 0x7) << kRvcRs1sShift) |
+                     (funct6 << kRvcFunct6Shift) | (funct << kRvcFunct2Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCI(uint8_t funct3, BaseOpcode opcode,
+                                    Register rd, int8_t imm6) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && is_int6(imm6));
+  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) |
+                     (rd.code() << kRvcRdShift) | ((imm6 & 0x20) << 7) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCIU(uint8_t funct3, BaseOpcode opcode,
+                                     Register rd, uint8_t uimm6) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
+  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
+                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCIU(uint8_t funct3, BaseOpcode opcode,
+                                     FPURegister rd, uint8_t uimm6) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
+  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
+                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCIW(uint8_t funct3, BaseOpcode opcode,
+                                     Register rd, uint8_t uimm8) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint8(uimm8));
+  ShortInstr instr = opcode | ((uimm8) << 5) |
+                     ((rd.code() & 0x7) << kRvcRs2sShift) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCSS(uint8_t funct3, BaseOpcode opcode,
+                                     Register rs2, uint8_t uimm6) {
+  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
+  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCSS(uint8_t funct3, BaseOpcode opcode,
+                                     FPURegister rs2, uint8_t uimm6) {
+  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
+  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCL(uint8_t funct3, BaseOpcode opcode,
+                                    Register rd, Register rs1, uint8_t uimm5) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         is_uint5(uimm5));
+  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
+                     ((rd.code() & 0x7) << kRvcRs2sShift) |
+                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCL(uint8_t funct3, BaseOpcode opcode,
+                                    FPURegister rd, Register rs1,
+                                    uint8_t uimm5) {
+  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
+         is_uint5(uimm5));
+  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
+                     ((rd.code() & 0x7) << kRvcRs2sShift) |
+                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift);
+  emit(instr);
+}
+void AssemblerRiscvBase::GenInstrCJ(uint8_t funct3, BaseOpcode opcode,
+                                    uint16_t uint11) {
+  DCHECK(is_uint11(uint11));
+  ShortInstr instr = opcode | (funct3 << kRvcFunct3Shift) | (uint11 << 2);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCS(uint8_t funct3, BaseOpcode opcode,
+                                    Register rs2, Register rs1, uint8_t uimm5) {
+  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
+         is_uint5(uimm5));
+  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
+                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
+                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCS(uint8_t funct3, BaseOpcode opcode,
+                                    FPURegister rs2, Register rs1,
+                                    uint8_t uimm5) {
+  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
+         is_uint5(uimm5));
+  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
+                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
+                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCB(uint8_t funct3, BaseOpcode opcode,
+                                    Register rs1, uint8_t uimm8) {
+  DCHECK(is_uint3(funct3) && is_uint8(uimm8));
+  ShortInstr instr = opcode | ((uimm8 & 0x1f) << 2) | ((uimm8 & 0xe0) << 5) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
+                     (funct3 << kRvcFunct3Shift);
+  emit(instr);
+}
+
+void AssemblerRiscvBase::GenInstrCBA(uint8_t funct3, uint8_t funct2,
+                                     BaseOpcode opcode, Register rs1,
+                                     int8_t imm6) {
+  DCHECK(is_uint3(funct3) && is_uint2(funct2) && is_int6(imm6));
+  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) | ((imm6 & 0x20) << 7) |
+                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
+                     (funct3 << kRvcFunct3Shift) | (funct2 << 10);
+  emit(instr);
+}
+// ----- Instruction class templates match those in the compiler
+
+void AssemblerRiscvBase::GenInstrBranchCC_rri(uint8_t funct3, Register rs1,
+                                              Register rs2, int16_t imm13) {
+  GenInstrB(funct3, BRANCH, rs1, rs2, imm13);
+}
+
+void AssemblerRiscvBase::GenInstrLoad_ri(uint8_t funct3, Register rd,
+                                         Register rs1, int16_t imm12) {
+  GenInstrI(funct3, LOAD, rd, rs1, imm12);
+}
+
+void AssemblerRiscvBase::GenInstrStore_rri(uint8_t funct3, Register rs1,
+                                           Register rs2, int16_t imm12) {
+  GenInstrS(funct3, STORE, rs1, rs2, imm12);
+}
+
+void AssemblerRiscvBase::GenInstrALU_ri(uint8_t funct3, Register rd,
+                                        Register rs1, int16_t imm12) {
+  GenInstrI(funct3, OP_IMM, rd, rs1, imm12);
+}
+
+void AssemblerRiscvBase::GenInstrShift_ri(bool arithshift, uint8_t funct3,
+                                          Register rd, Register rs1,
+                                          uint8_t shamt) {
+  DCHECK(is_uint6(shamt));
+  GenInstrI(funct3, OP_IMM, rd, rs1, (arithshift << 10) | shamt);
+}
+
+void AssemblerRiscvBase::GenInstrALU_rr(uint8_t funct7, uint8_t funct3,
+                                        Register rd, Register rs1,
+                                        Register rs2) {
+  GenInstrR(funct7, funct3, OP, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrCSR_ir(uint8_t funct3, Register rd,
+                                        ControlStatusReg csr, Register rs1) {
+  GenInstrI(funct3, SYSTEM, rd, rs1, csr);
+}
+
+void AssemblerRiscvBase::GenInstrCSR_ii(uint8_t funct3, Register rd,
+                                        ControlStatusReg csr, uint8_t imm5) {
+  GenInstrI(funct3, SYSTEM, rd, ToRegister(imm5), csr);
+}
+
+void AssemblerRiscvBase::GenInstrShiftW_ri(bool arithshift, uint8_t funct3,
+                                           Register rd, Register rs1,
+                                           uint8_t shamt) {
+  GenInstrIShiftW(arithshift, funct3, OP_IMM_32, rd, rs1, shamt);
+}
+
+void AssemblerRiscvBase::GenInstrALUW_rr(uint8_t funct7, uint8_t funct3,
+                                         Register rd, Register rs1,
+                                         Register rs2) {
+  GenInstrR(funct7, funct3, OP_32, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrPriv(uint8_t funct7, Register rs1,
+                                      Register rs2) {
+  GenInstrR(funct7, 0b000, SYSTEM, ToRegister(0), rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd,
+                                           Register rs1, int16_t imm12) {
+  GenInstrI(funct3, LOAD_FP, rd, rs1, imm12);
+}
+
+void AssemblerRiscvBase::GenInstrStoreFP_rri(uint8_t funct3, Register rs1,
+                                             FPURegister rs2, int16_t imm12) {
+  GenInstrS(funct3, STORE_FP, rs1, rs2, imm12);
+}
+
+void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
+                                          FPURegister rd, FPURegister rs1,
+                                          FPURegister rs2) {
+  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
+                                          FPURegister rd, Register rs1,
+                                          Register rs2) {
+  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
+                                          FPURegister rd, FPURegister rs1,
+                                          Register rs2) {
+  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
+                                          Register rd, FPURegister rs1,
+                                          Register rs2) {
+  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
+}
+
+void AssemblerRiscvBase::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3,
+                                          Register rd, FPURegister rs1,
+                                          FPURegister rs2) {
+  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/base-assembler-riscv.h b/src/codegen/riscv/base-assembler-riscv.h
new file mode 100644
index 00000000000..460a86c0afb
--- /dev/null
+++ b/src/codegen/riscv/base-assembler-riscv.h
@@ -0,0 +1,192 @@
+// Copyright (c) 1994-2006 Sun Microsystems Inc.
+// All Rights Reserved.
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+// - Redistributions of source code must retain the above copyright notice,
+// this list of conditions and the following disclaimer.
+//
+// - Redistribution in binary form must reproduce the above copyright
+// notice, this list of conditions and the following disclaimer in the
+// documentation and/or other materials provided with the distribution.
+//
+// - Neither the name of Sun Microsystems or the names of contributors may
+// be used to endorse or promote products derived from this software without
+// specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
+// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
+// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
+// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
+// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// The original source code covered by the above license above has been
+// modified significantly by Google Inc.
+// Copyright 2021 the V8 project authors. All rights reserved.
+
+#ifndef V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_
+#define V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_
+
+#include <stdio.h>
+
+#include <memory>
+#include <set>
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/constant-pool.h"
+#include "src/codegen/external-reference.h"
+#include "src/codegen/label.h"
+#include "src/codegen/riscv/constants-riscv.h"
+#include "src/codegen/riscv/register-riscv.h"
+#include "src/objects/contexts.h"
+#include "src/objects/smi.h"
+
+namespace v8 {
+namespace internal {
+
+#define DEBUG_PRINTF(...) \
+  if (FLAG_riscv_debug) { \
+    printf(__VA_ARGS__);  \
+  }
+
+class SafepointTableBuilder;
+
+class AssemblerRiscvBase {
+ protected:
+  // Returns the branch offset to the given label from the current code
+  // position. Links the label to the current position if it is still unbound.
+  // Manages the jump elimination optimization if the second parameter is true.
+  enum OffsetSize : int {
+    kOffset21 = 21,  // RISCV jal
+    kOffset12 = 12,  // RISCV imm12
+    kOffset20 = 20,  // RISCV imm20
+    kOffset13 = 13,  // RISCV branch
+    kOffset32 = 32,  // RISCV auipc + instr_I
+    kOffset11 = 11,  // RISCV C_J
+    kOffset9 = 9     // RISCV compressed branch
+  };
+  virtual int32_t branch_offset_helper(Label* L, OffsetSize bits) = 0;
+
+  virtual void emit(Instr x) = 0;
+  virtual void emit(ShortInstr x) = 0;
+  virtual void emit(uint64_t x) = 0;
+  // Instruction generation.
+
+  // ----- Top-level instruction formats match those in the ISA manual
+  // (R, I, S, B, U, J). These match the formats defined in LLVM's
+  // RISCVInstrFormats.td.
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
+                 Register rs1, Register rs2);
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
+                 FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
+                 FPURegister rs1, Register rs2);
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
+                 FPURegister rd, Register rs1, Register rs2);
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode,
+                 FPURegister rd, FPURegister rs1, Register rs2);
+  void GenInstrR(uint8_t funct7, uint8_t funct3, BaseOpcode opcode, Register rd,
+                 FPURegister rs1, FPURegister rs2);
+  void GenInstrR4(uint8_t funct2, BaseOpcode opcode, Register rd, Register rs1,
+                  Register rs2, Register rs3, FPURoundingMode frm);
+  void GenInstrR4(uint8_t funct2, BaseOpcode opcode, FPURegister rd,
+                  FPURegister rs1, FPURegister rs2, FPURegister rs3,
+                  FPURoundingMode frm);
+  void GenInstrRAtomic(uint8_t funct5, bool aq, bool rl, uint8_t funct3,
+                       Register rd, Register rs1, Register rs2);
+  void GenInstrRFrm(uint8_t funct7, BaseOpcode opcode, Register rd,
+                    Register rs1, Register rs2, FPURoundingMode frm);
+  void GenInstrI(uint8_t funct3, BaseOpcode opcode, Register rd, Register rs1,
+                 int16_t imm12);
+  void GenInstrI(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
+                 Register rs1, int16_t imm12);
+  void GenInstrIShift(bool arithshift, uint8_t funct3, BaseOpcode opcode,
+                      Register rd, Register rs1, uint8_t shamt);
+  void GenInstrIShiftW(bool arithshift, uint8_t funct3, BaseOpcode opcode,
+                       Register rd, Register rs1, uint8_t shamt);
+  void GenInstrS(uint8_t funct3, BaseOpcode opcode, Register rs1, Register rs2,
+                 int16_t imm12);
+  void GenInstrS(uint8_t funct3, BaseOpcode opcode, Register rs1,
+                 FPURegister rs2, int16_t imm12);
+  void GenInstrB(uint8_t funct3, BaseOpcode opcode, Register rs1, Register rs2,
+                 int16_t imm12);
+  void GenInstrU(BaseOpcode opcode, Register rd, int32_t imm20);
+  void GenInstrJ(BaseOpcode opcode, Register rd, int32_t imm20);
+  void GenInstrCR(uint8_t funct4, BaseOpcode opcode, Register rd, Register rs2);
+  void GenInstrCA(uint8_t funct6, BaseOpcode opcode, Register rd, uint8_t funct,
+                  Register rs2);
+  void GenInstrCI(uint8_t funct3, BaseOpcode opcode, Register rd, int8_t imm6);
+  void GenInstrCIU(uint8_t funct3, BaseOpcode opcode, Register rd,
+                   uint8_t uimm6);
+  void GenInstrCIU(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
+                   uint8_t uimm6);
+  void GenInstrCIW(uint8_t funct3, BaseOpcode opcode, Register rd,
+                   uint8_t uimm8);
+  void GenInstrCSS(uint8_t funct3, BaseOpcode opcode, FPURegister rs2,
+                   uint8_t uimm6);
+  void GenInstrCSS(uint8_t funct3, BaseOpcode opcode, Register rs2,
+                   uint8_t uimm6);
+  void GenInstrCL(uint8_t funct3, BaseOpcode opcode, Register rd, Register rs1,
+                  uint8_t uimm5);
+  void GenInstrCL(uint8_t funct3, BaseOpcode opcode, FPURegister rd,
+                  Register rs1, uint8_t uimm5);
+  void GenInstrCS(uint8_t funct3, BaseOpcode opcode, Register rs2, Register rs1,
+                  uint8_t uimm5);
+  void GenInstrCS(uint8_t funct3, BaseOpcode opcode, FPURegister rs2,
+                  Register rs1, uint8_t uimm5);
+  void GenInstrCJ(uint8_t funct3, BaseOpcode opcode, uint16_t uint11);
+  void GenInstrCB(uint8_t funct3, BaseOpcode opcode, Register rs1,
+                  uint8_t uimm8);
+  void GenInstrCBA(uint8_t funct3, uint8_t funct2, BaseOpcode opcode,
+                   Register rs1, int8_t imm6);
+
+  // ----- Instruction class templates match those in LLVM's RISCVInstrInfo.td
+  void GenInstrBranchCC_rri(uint8_t funct3, Register rs1, Register rs2,
+                            int16_t imm12);
+  void GenInstrLoad_ri(uint8_t funct3, Register rd, Register rs1,
+                       int16_t imm12);
+  void GenInstrStore_rri(uint8_t funct3, Register rs1, Register rs2,
+                         int16_t imm12);
+  void GenInstrALU_ri(uint8_t funct3, Register rd, Register rs1, int16_t imm12);
+  void GenInstrShift_ri(bool arithshift, uint8_t funct3, Register rd,
+                        Register rs1, uint8_t shamt);
+  void GenInstrALU_rr(uint8_t funct7, uint8_t funct3, Register rd, Register rs1,
+                      Register rs2);
+  void GenInstrCSR_ir(uint8_t funct3, Register rd, ControlStatusReg csr,
+                      Register rs1);
+  void GenInstrCSR_ii(uint8_t funct3, Register rd, ControlStatusReg csr,
+                      uint8_t rs1);
+  void GenInstrShiftW_ri(bool arithshift, uint8_t funct3, Register rd,
+                         Register rs1, uint8_t shamt);
+  void GenInstrALUW_rr(uint8_t funct7, uint8_t funct3, Register rd,
+                       Register rs1, Register rs2);
+  void GenInstrPriv(uint8_t funct7, Register rs1, Register rs2);
+  void GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd, Register rs1,
+                         int16_t imm12);
+  void GenInstrStoreFP_rri(uint8_t funct3, Register rs1, FPURegister rs2,
+                           int16_t imm12);
+  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
+                        FPURegister rs1, FPURegister rs2);
+  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
+                        Register rs1, Register rs2);
+  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
+                        FPURegister rs1, Register rs2);
+  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
+                        FPURegister rs1, Register rs2);
+  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
+                        FPURegister rs1, FPURegister rs2);
+  virtual void BlockTrampolinePoolFor(int instructions) = 0;
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_BASE_ASSEMBLER_RISCV_H_
diff --git a/src/codegen/riscv64/constants-riscv64.cc b/src/codegen/riscv/base-constants-riscv.cc
similarity index 66%
rename from src/codegen/riscv64/constants-riscv64.cc
rename to src/codegen/riscv/base-constants-riscv.cc
index 655a97c12f5..4e2b78d9f54 100644
--- a/src/codegen/riscv64/constants-riscv64.cc
+++ b/src/codegen/riscv/base-constants-riscv.cc
@@ -1,10 +1,8 @@
 // Copyright 2021 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
-
-#if V8_TARGET_ARCH_RISCV64
-
-#include "src/codegen/riscv64/constants-riscv64.h"
+#include "src/codegen/riscv/constants-riscv.h"
+#include "src/execution/simulator.h"
 
 namespace v8 {
 namespace internal {
@@ -144,6 +142,95 @@ int VRegisters::Number(const char* name) {
   return kInvalidVRegister;
 }
 
+bool InstructionBase::IsShortInstruction() const {
+  uint8_t FirstByte = *reinterpret_cast<const uint8_t*>(this);
+  return (FirstByte & 0x03) <= C2;
+}
+
+template <class T>
+int InstructionGetters<T>::RvcRdValue() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcRdShift + kRvcRdBits - 1, kRvcRdShift);
+}
+
+template <class T>
+int InstructionGetters<T>::RvcRs2Value() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcRs2Shift + kRvcRs2Bits - 1, kRvcRs2Shift);
+}
+
+template <class T>
+int InstructionGetters<T>::RvcRs1sValue() const {
+  DCHECK(this->IsShortInstruction());
+  return 0b1000 + this->Bits(kRvcRs1sShift + kRvcRs1sBits - 1, kRvcRs1sShift);
+}
+
+template <class T>
+int InstructionGetters<T>::RvcRs2sValue() const {
+  DCHECK(this->IsShortInstruction());
+  return 0b1000 + this->Bits(kRvcRs2sShift + kRvcRs2sBits - 1, kRvcRs2sShift);
+}
+
+template <class T>
+inline int InstructionGetters<T>::RvcFunct6Value() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcFunct6Shift + kRvcFunct6Bits - 1, kRvcFunct6Shift);
+}
+
+template <class T>
+inline int InstructionGetters<T>::RvcFunct4Value() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcFunct4Shift + kRvcFunct4Bits - 1, kRvcFunct4Shift);
+}
+
+template <class T>
+inline int InstructionGetters<T>::RvcFunct3Value() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcFunct3Shift + kRvcFunct3Bits - 1, kRvcFunct3Shift);
+}
+
+template <class T>
+inline int InstructionGetters<T>::RvcFunct2Value() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcFunct2Shift + kRvcFunct2Bits - 1, kRvcFunct2Shift);
+}
+
+template <class T>
+inline int InstructionGetters<T>::RvcFunct2BValue() const {
+  DCHECK(this->IsShortInstruction());
+  return this->Bits(kRvcFunct2BShift + kRvcFunct2Bits - 1, kRvcFunct2BShift);
+}
+
+template <class T>
+uint32_t InstructionGetters<T>::Rvvzimm() const {
+  if ((this->InstructionBits() &
+       (kBaseOpcodeMask | kFunct3Mask | 0x80000000)) == RO_V_VSETVLI) {
+    uint32_t Bits = this->InstructionBits();
+    uint32_t zimm = Bits & kRvvZimmMask;
+    return zimm >> kRvvZimmShift;
+  } else {
+    DCHECK_EQ(
+        this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
+        RO_V_VSETIVLI);
+    uint32_t Bits = this->InstructionBits();
+    uint32_t zimm = Bits & kRvvZimmMask;
+    return (zimm >> kRvvZimmShift) & 0x3FF;
+  }
+}
+
+template <class T>
+uint32_t InstructionGetters<T>::Rvvuimm() const {
+  DCHECK_EQ(
+      this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
+      RO_V_VSETIVLI);
+  uint32_t Bits = this->InstructionBits();
+  uint32_t uimm = Bits & kRvvUimmMask;
+  return uimm >> kRvvUimmShift;
+}
+
+template class InstructionGetters<InstructionBase>;
+template class InstructionGetters<SimInstructionBase>;
+
 InstructionBase::Type InstructionBase::InstructionType() const {
   if (IsIllegalInstruction()) {
     return kUnsupported;
@@ -155,15 +242,21 @@ InstructionBase::Type InstructionBase::InstructionType() const {
         return kCIWType;
       case RO_C_FLD:
       case RO_C_LW:
+#ifdef V8_TARGET_ARCH_RISCV64
       case RO_C_LD:
+#endif
         return kCLType;
       case RO_C_FSD:
       case RO_C_SW:
+#ifdef V8_TARGET_ARCH_RISCV64
       case RO_C_SD:
+#endif
         return kCSType;
       case RO_C_NOP_ADDI:
-      case RO_C_ADDIW:
       case RO_C_LI:
+#ifdef V8_TARGET_ARCH_RISCV64
+      case RO_C_ADDIW:
+#endif
       case RO_C_LUI_ADD:
         return kCIType;
       case RO_C_MISC_ALU:
@@ -179,13 +272,17 @@ InstructionBase::Type InstructionBase::InstructionType() const {
       case RO_C_SLLI:
       case RO_C_FLDSP:
       case RO_C_LWSP:
+#ifdef V8_TARGET_ARCH_RISCV64
       case RO_C_LDSP:
+#endif
         return kCIType;
       case RO_C_JR_MV_ADD:
         return kCRType;
       case RO_C_FSDSP:
       case RO_C_SWSP:
+#ifdef V8_TARGET_ARCH_RISCV64
       case RO_C_SDSP:
+#endif
         return kCSSType;
       default:
         break;
@@ -241,5 +338,3 @@ InstructionBase::Type InstructionBase::InstructionType() const {
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/codegen/riscv64/constants-riscv64.h b/src/codegen/riscv/base-constants-riscv.h
similarity index 50%
rename from src/codegen/riscv64/constants-riscv64.h
rename to src/codegen/riscv/base-constants-riscv.h
index 0929916c819..e64f36416e9 100644
--- a/src/codegen/riscv64/constants-riscv64.h
+++ b/src/codegen/riscv/base-constants-riscv.h
@@ -1,16 +1,14 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
+// Copyright 2022 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
-
-#ifndef V8_CODEGEN_RISCV64_CONSTANTS_RISCV64_H_
-#define V8_CODEGEN_RISCV64_CONSTANTS_RISCV64_H_
+#ifndef V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_
+#define V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_
 
 #include "src/base/logging.h"
 #include "src/base/macros.h"
 #include "src/common/globals.h"
 #include "src/flags/flags.h"
 
-// UNIMPLEMENTED_ macro for RISCV.
 #ifdef DEBUG
 #define UNIMPLEMENTED_RISCV()                                               \
   v8::internal::PrintF("%s, \tline %d: \tfunction %s  not implemented. \n", \
@@ -53,10 +51,42 @@ const uint32_t kLessSignificantWordInDoublewordOffset = 4;
 // See: The RISC-V Instruction Set Manual
 //      Volume I: User-Level ISA
 // Try https://content.riscv.org/wp-content/uploads/2017/05/riscv-spec-v2.2.pdf.
-
 namespace v8 {
 namespace internal {
 
+// Actual value of root register is offset from the root array's start
+// to take advantage of negative displacement values.
+// TODO(sigurds): Choose best value.
+constexpr int kRootRegisterBias = 256;
+
+#define RVV_LMUL(V) \
+  V(m1)             \
+  V(m2)             \
+  V(m4)             \
+  V(m8)             \
+  V(RESERVERD)      \
+  V(mf8)            \
+  V(mf4)            \
+  V(mf2)
+
+enum Vlmul {
+#define DEFINE_FLAG(name) name,
+  RVV_LMUL(DEFINE_FLAG)
+#undef DEFINE_FLAG
+};
+
+#define RVV_SEW(V) \
+  V(E8)            \
+  V(E16)           \
+  V(E32)           \
+  V(E64)
+
+#define DEFINE_FLAG(name) name,
+enum VSew {
+  RVV_SEW(DEFINE_FLAG)
+#undef DEFINE_FLAG
+};
+
 constexpr size_t kMaxPCRelativeCodeRangeInMB = 4094;
 
 // -----------------------------------------------------------------------------
@@ -89,11 +119,6 @@ const int32_t kPrefHintStoreRetained = 7;
 const int32_t kPrefHintWritebackInvalidate = 25;
 const int32_t kPrefHintPrepareForStore = 30;
 
-// Actual value of root register is offset from the root array's start
-// to take advantage of negative displacement values.
-// TODO(sigurds): Choose best value.
-constexpr int kRootRegisterBias = 256;
-
 // Helper functions for converting between register numbers and names.
 class Registers {
  public:
@@ -108,9 +133,6 @@ class Registers {
     const char* name;
   };
 
-  static const int64_t kMaxValue = 0x7fffffffffffffffl;
-  static const int64_t kMinValue = 0x8000000000000000l;
-
  private:
   static const char* names_[kNumSimuRegisters];
   static const RegisterAlias aliases_[];
@@ -213,6 +235,7 @@ const int kImm11Bits = 11;
 const int kShamtShift = 20;
 const int kShamtBits = 5;
 const int kShamtWShift = 20;
+// FIXME: remove this once we have a proper way to handle the wide shift amount
 const int kShamtWBits = 6;
 const int kArithShiftShift = 30;
 const int kImm20Shift = 12;
@@ -222,6 +245,7 @@ const int kCsrBits = 12;
 const int kMemOrderBits = 4;
 const int kPredOrderShift = 24;
 const int kSuccOrderShift = 20;
+
 // for C extension
 const int kRvcFunct4Shift = 12;
 const int kRvcFunct4Bits = 4;
@@ -243,6 +267,23 @@ const int kRvcFunct2Bits = 2;
 const int kRvcFunct6Shift = 10;
 const int kRvcFunct6Bits = 6;
 
+const uint32_t kRvcOpcodeMask =
+    0b11 | (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
+const uint32_t kRvcFunct3Mask =
+    (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
+const uint32_t kRvcFunct4Mask =
+    (((1 << kRvcFunct4Bits) - 1) << kRvcFunct4Shift);
+const uint32_t kRvcFunct6Mask =
+    (((1 << kRvcFunct6Bits) - 1) << kRvcFunct6Shift);
+const uint32_t kRvcFunct2Mask =
+    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2Shift);
+const uint32_t kRvcFunct2BMask =
+    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2BShift);
+const uint32_t kCRTypeMask = kRvcOpcodeMask | kRvcFunct4Mask;
+const uint32_t kCSTypeMask = kRvcOpcodeMask | kRvcFunct6Mask;
+const uint32_t kCATypeMask = kRvcOpcodeMask | kRvcFunct6Mask | kRvcFunct2Mask;
+const uint32_t kRvcBImm8Mask = (((1 << 5) - 1) << 2) | (((1 << 3) - 1) << 10);
+
 // for RVV extension
 constexpr int kRvvELEN = 64;
 constexpr int kRvvVLEN = 128;
@@ -335,787 +376,19 @@ const uint32_t kImm12Mask = ((1 << kImm12Bits) - 1) << kImm12Shift;
 const uint32_t kImm11Mask = ((1 << kImm11Bits) - 1) << kImm11Shift;
 const uint32_t kImm31_12Mask = ((1 << 20) - 1) << 12;
 const uint32_t kImm19_0Mask = ((1 << 20) - 1);
-const uint32_t kRvcOpcodeMask =
-    0b11 | (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
-const uint32_t kRvcFunct3Mask =
-    (((1 << kRvcFunct3Bits) - 1) << kRvcFunct3Shift);
-const uint32_t kRvcFunct4Mask =
-    (((1 << kRvcFunct4Bits) - 1) << kRvcFunct4Shift);
-const uint32_t kRvcFunct6Mask =
-    (((1 << kRvcFunct6Bits) - 1) << kRvcFunct6Shift);
-const uint32_t kRvcFunct2Mask =
-    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2Shift);
-const uint32_t kRvcFunct2BMask =
-    (((1 << kRvcFunct2Bits) - 1) << kRvcFunct2BShift);
-const uint32_t kCRTypeMask = kRvcOpcodeMask | kRvcFunct4Mask;
-const uint32_t kCSTypeMask = kRvcOpcodeMask | kRvcFunct6Mask;
-const uint32_t kCATypeMask = kRvcOpcodeMask | kRvcFunct6Mask | kRvcFunct2Mask;
-const uint32_t kRvcBImm8Mask = (((1 << 5) - 1) << 2) | (((1 << 3) - 1) << 10);
-
-// RISCV CSR related bit mask and shift
-const int kFcsrFlagsBits = 5;
-const uint32_t kFcsrFlagsMask = (1 << kFcsrFlagsBits) - 1;
-const int kFcsrFrmBits = 3;
-const int kFcsrFrmShift = kFcsrFlagsBits;
-const uint32_t kFcsrFrmMask = ((1 << kFcsrFrmBits) - 1) << kFcsrFrmShift;
-const int kFcsrBits = kFcsrFlagsBits + kFcsrFrmBits;
-const uint32_t kFcsrMask = kFcsrFlagsMask | kFcsrFrmMask;
 
 const int kNopByte = 0x00000013;
 // Original MIPS constants
-// TODO(RISCV): to be cleaned up
 const int kImm16Shift = 0;
 const int kImm16Bits = 16;
 const uint32_t kImm16Mask = ((1 << kImm16Bits) - 1) << kImm16Shift;
-// end of TODO(RISCV): to be cleaned up
-
-// ----- RISCV Base Opcodes
-
-enum BaseOpcode : uint32_t {};
-
-// ----- RISC-V Opcodes and Function Fields.
-enum Opcode : uint32_t {
-  LOAD = 0b0000011,      // I form: LB LH LW LBU LHU
-  LOAD_FP = 0b0000111,   // I form: FLW FLD FLQ
-  MISC_MEM = 0b0001111,  // I special form: FENCE FENCE.I
-  OP_IMM = 0b0010011,    // I form: ADDI SLTI SLTIU XORI ORI ANDI SLLI SRLI SARI
-  // Note: SLLI/SRLI/SRAI I form first, then func3 001/101 => R type
-  AUIPC = 0b0010111,      // U form: AUIPC
-  OP_IMM_32 = 0b0011011,  // I form: ADDIW SLLIW SRLIW SRAIW
-  // Note:  SRLIW SRAIW I form first, then func3 101 special shift encoding
-  STORE = 0b0100011,     // S form: SB SH SW SD
-  STORE_FP = 0b0100111,  // S form: FSW FSD FSQ
-  AMO = 0b0101111,       // R form: All A instructions
-  OP = 0b0110011,      // R: ADD SUB SLL SLT SLTU XOR SRL SRA OR AND and 32M set
-  LUI = 0b0110111,     // U form: LUI
-  OP_32 = 0b0111011,   // R: ADDW SUBW SLLW SRLW SRAW MULW DIVW DIVUW REMW REMUW
-  MADD = 0b1000011,    // R4 type: FMADD.S FMADD.D FMADD.Q
-  MSUB = 0b1000111,    // R4 type: FMSUB.S FMSUB.D FMSUB.Q
-  NMSUB = 0b1001011,   // R4 type: FNMSUB.S FNMSUB.D FNMSUB.Q
-  NMADD = 0b1001111,   // R4 type: FNMADD.S FNMADD.D FNMADD.Q
-  OP_FP = 0b1010011,   // R type: Q ext
-  BRANCH = 0b1100011,  // B form: BEQ BNE, BLT, BGE, BLTU BGEU
-  JALR = 0b1100111,    // I form: JALR
-  JAL = 0b1101111,     // J form: JAL
-  SYSTEM = 0b1110011,  // I form: ECALL EBREAK Zicsr ext
-  // C extension
-  C0 = 0b00,
-  C1 = 0b01,
-  C2 = 0b10,
-  FUNCT2_0 = 0b00,
-  FUNCT2_1 = 0b01,
-  FUNCT2_2 = 0b10,
-  FUNCT2_3 = 0b11,
-
-  // Note use RO (RiscV Opcode) prefix
-  // RV32I Base Instruction Set
-  RO_LUI = LUI,
-  RO_AUIPC = AUIPC,
-  RO_JAL = JAL,
-  RO_JALR = JALR | (0b000 << kFunct3Shift),
-  RO_BEQ = BRANCH | (0b000 << kFunct3Shift),
-  RO_BNE = BRANCH | (0b001 << kFunct3Shift),
-  RO_BLT = BRANCH | (0b100 << kFunct3Shift),
-  RO_BGE = BRANCH | (0b101 << kFunct3Shift),
-  RO_BLTU = BRANCH | (0b110 << kFunct3Shift),
-  RO_BGEU = BRANCH | (0b111 << kFunct3Shift),
-  RO_LB = LOAD | (0b000 << kFunct3Shift),
-  RO_LH = LOAD | (0b001 << kFunct3Shift),
-  RO_LW = LOAD | (0b010 << kFunct3Shift),
-  RO_LBU = LOAD | (0b100 << kFunct3Shift),
-  RO_LHU = LOAD | (0b101 << kFunct3Shift),
-  RO_SB = STORE | (0b000 << kFunct3Shift),
-  RO_SH = STORE | (0b001 << kFunct3Shift),
-  RO_SW = STORE | (0b010 << kFunct3Shift),
-  RO_ADDI = OP_IMM | (0b000 << kFunct3Shift),
-  RO_SLTI = OP_IMM | (0b010 << kFunct3Shift),
-  RO_SLTIU = OP_IMM | (0b011 << kFunct3Shift),
-  RO_XORI = OP_IMM | (0b100 << kFunct3Shift),
-  RO_ORI = OP_IMM | (0b110 << kFunct3Shift),
-  RO_ANDI = OP_IMM | (0b111 << kFunct3Shift),
-  RO_SLLI = OP_IMM | (0b001 << kFunct3Shift),
-  RO_SRLI = OP_IMM | (0b101 << kFunct3Shift),
-  // RO_SRAI = OP_IMM | (0b101 << kFunct3Shift), // Same as SRLI, use func7
-  RO_ADD = OP | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SUB = OP | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
-  RO_SLL = OP | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SLT = OP | (0b010 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SLTU = OP | (0b011 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_XOR = OP | (0b100 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SRL = OP | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SRA = OP | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
-  RO_OR = OP | (0b110 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_AND = OP | (0b111 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_FENCE = MISC_MEM | (0b000 << kFunct3Shift),
-  RO_ECALL = SYSTEM | (0b000 << kFunct3Shift),
-  // RO_EBREAK = SYSTEM | (0b000 << kFunct3Shift), // Same as ECALL, use imm12
-
-  // RV64I Base Instruction Set (in addition to RV32I)
-  RO_LWU = LOAD | (0b110 << kFunct3Shift),
-  RO_LD = LOAD | (0b011 << kFunct3Shift),
-  RO_SD = STORE | (0b011 << kFunct3Shift),
-  RO_ADDIW = OP_IMM_32 | (0b000 << kFunct3Shift),
-  RO_SLLIW = OP_IMM_32 | (0b001 << kFunct3Shift),
-  RO_SRLIW = OP_IMM_32 | (0b101 << kFunct3Shift),
-  // RO_SRAIW = OP_IMM_32 | (0b101 << kFunct3Shift), // Same as SRLIW, use func7
-  RO_ADDW = OP_32 | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SUBW = OP_32 | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
-  RO_SLLW = OP_32 | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SRLW = OP_32 | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
-  RO_SRAW = OP_32 | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
-
-  // RV32/RV64 Zifencei Standard Extension
-  RO_FENCE_I = MISC_MEM | (0b001 << kFunct3Shift),
-
-  // RV32/RV64 Zicsr Standard Extension
-  RO_CSRRW = SYSTEM | (0b001 << kFunct3Shift),
-  RO_CSRRS = SYSTEM | (0b010 << kFunct3Shift),
-  RO_CSRRC = SYSTEM | (0b011 << kFunct3Shift),
-  RO_CSRRWI = SYSTEM | (0b101 << kFunct3Shift),
-  RO_CSRRSI = SYSTEM | (0b110 << kFunct3Shift),
-  RO_CSRRCI = SYSTEM | (0b111 << kFunct3Shift),
-
-  // RV32M Standard Extension
-  RO_MUL = OP | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_MULH = OP | (0b001 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_MULHSU = OP | (0b010 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_MULHU = OP | (0b011 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_DIV = OP | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_DIVU = OP | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_REM = OP | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_REMU = OP | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-
-  // RV64M Standard Extension (in addition to RV32M)
-  RO_MULW = OP_32 | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_DIVW = OP_32 | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_DIVUW = OP_32 | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_REMW = OP_32 | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-  RO_REMUW = OP_32 | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
-
-  // RV32A Standard Extension
-  RO_LR_W = AMO | (0b010 << kFunct3Shift) | (0b00010 << kFunct5Shift),
-  RO_SC_W = AMO | (0b010 << kFunct3Shift) | (0b00011 << kFunct5Shift),
-  RO_AMOSWAP_W = AMO | (0b010 << kFunct3Shift) | (0b00001 << kFunct5Shift),
-  RO_AMOADD_W = AMO | (0b010 << kFunct3Shift) | (0b00000 << kFunct5Shift),
-  RO_AMOXOR_W = AMO | (0b010 << kFunct3Shift) | (0b00100 << kFunct5Shift),
-  RO_AMOAND_W = AMO | (0b010 << kFunct3Shift) | (0b01100 << kFunct5Shift),
-  RO_AMOOR_W = AMO | (0b010 << kFunct3Shift) | (0b01000 << kFunct5Shift),
-  RO_AMOMIN_W = AMO | (0b010 << kFunct3Shift) | (0b10000 << kFunct5Shift),
-  RO_AMOMAX_W = AMO | (0b010 << kFunct3Shift) | (0b10100 << kFunct5Shift),
-  RO_AMOMINU_W = AMO | (0b010 << kFunct3Shift) | (0b11000 << kFunct5Shift),
-  RO_AMOMAXU_W = AMO | (0b010 << kFunct3Shift) | (0b11100 << kFunct5Shift),
-
-  // RV64A Standard Extension (in addition to RV32A)
-  RO_LR_D = AMO | (0b011 << kFunct3Shift) | (0b00010 << kFunct5Shift),
-  RO_SC_D = AMO | (0b011 << kFunct3Shift) | (0b00011 << kFunct5Shift),
-  RO_AMOSWAP_D = AMO | (0b011 << kFunct3Shift) | (0b00001 << kFunct5Shift),
-  RO_AMOADD_D = AMO | (0b011 << kFunct3Shift) | (0b00000 << kFunct5Shift),
-  RO_AMOXOR_D = AMO | (0b011 << kFunct3Shift) | (0b00100 << kFunct5Shift),
-  RO_AMOAND_D = AMO | (0b011 << kFunct3Shift) | (0b01100 << kFunct5Shift),
-  RO_AMOOR_D = AMO | (0b011 << kFunct3Shift) | (0b01000 << kFunct5Shift),
-  RO_AMOMIN_D = AMO | (0b011 << kFunct3Shift) | (0b10000 << kFunct5Shift),
-  RO_AMOMAX_D = AMO | (0b011 << kFunct3Shift) | (0b10100 << kFunct5Shift),
-  RO_AMOMINU_D = AMO | (0b011 << kFunct3Shift) | (0b11000 << kFunct5Shift),
-  RO_AMOMAXU_D = AMO | (0b011 << kFunct3Shift) | (0b11100 << kFunct5Shift),
-
-  // RV32F Standard Extension
-  RO_FLW = LOAD_FP | (0b010 << kFunct3Shift),
-  RO_FSW = STORE_FP | (0b010 << kFunct3Shift),
-  RO_FMADD_S = MADD | (0b00 << kFunct2Shift),
-  RO_FMSUB_S = MSUB | (0b00 << kFunct2Shift),
-  RO_FNMSUB_S = NMSUB | (0b00 << kFunct2Shift),
-  RO_FNMADD_S = NMADD | (0b00 << kFunct2Shift),
-  RO_FADD_S = OP_FP | (0b0000000 << kFunct7Shift),
-  RO_FSUB_S = OP_FP | (0b0000100 << kFunct7Shift),
-  RO_FMUL_S = OP_FP | (0b0001000 << kFunct7Shift),
-  RO_FDIV_S = OP_FP | (0b0001100 << kFunct7Shift),
-  RO_FSQRT_S = OP_FP | (0b0101100 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FSGNJ_S = OP_FP | (0b000 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
-  RO_FSGNJN_S = OP_FP | (0b001 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
-  RO_FSQNJX_S = OP_FP | (0b010 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
-  RO_FMIN_S = OP_FP | (0b000 << kFunct3Shift) | (0b0010100 << kFunct7Shift),
-  RO_FMAX_S = OP_FP | (0b001 << kFunct3Shift) | (0b0010100 << kFunct7Shift),
-  RO_FCVT_W_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FCVT_WU_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00001 << kRs2Shift),
-  RO_FMV = OP_FP | (0b1110000 << kFunct7Shift) | (0b000 << kFunct3Shift) |
-           (0b00000 << kRs2Shift),
-  RO_FEQ_S = OP_FP | (0b010 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
-  RO_FLT_S = OP_FP | (0b001 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
-  RO_FLE_S = OP_FP | (0b000 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
-  RO_FCLASS_S = OP_FP | (0b001 << kFunct3Shift) | (0b1110000 << kFunct7Shift),
-  RO_FCVT_S_W = OP_FP | (0b1101000 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FCVT_S_WU = OP_FP | (0b1101000 << kFunct7Shift) | (0b00001 << kRs2Shift),
-  RO_FMV_W_X = OP_FP | (0b000 << kFunct3Shift) | (0b1111000 << kFunct7Shift),
-
-  // RV64F Standard Extension (in addition to RV32F)
-  RO_FCVT_L_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00010 << kRs2Shift),
-  RO_FCVT_LU_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00011 << kRs2Shift),
-  RO_FCVT_S_L = OP_FP | (0b1101000 << kFunct7Shift) | (0b00010 << kRs2Shift),
-  RO_FCVT_S_LU = OP_FP | (0b1101000 << kFunct7Shift) | (0b00011 << kRs2Shift),
-
-  // RV32D Standard Extension
-  RO_FLD = LOAD_FP | (0b011 << kFunct3Shift),
-  RO_FSD = STORE_FP | (0b011 << kFunct3Shift),
-  RO_FMADD_D = MADD | (0b01 << kFunct2Shift),
-  RO_FMSUB_D = MSUB | (0b01 << kFunct2Shift),
-  RO_FNMSUB_D = NMSUB | (0b01 << kFunct2Shift),
-  RO_FNMADD_D = NMADD | (0b01 << kFunct2Shift),
-  RO_FADD_D = OP_FP | (0b0000001 << kFunct7Shift),
-  RO_FSUB_D = OP_FP | (0b0000101 << kFunct7Shift),
-  RO_FMUL_D = OP_FP | (0b0001001 << kFunct7Shift),
-  RO_FDIV_D = OP_FP | (0b0001101 << kFunct7Shift),
-  RO_FSQRT_D = OP_FP | (0b0101101 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FSGNJ_D = OP_FP | (0b000 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
-  RO_FSGNJN_D = OP_FP | (0b001 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
-  RO_FSQNJX_D = OP_FP | (0b010 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
-  RO_FMIN_D = OP_FP | (0b000 << kFunct3Shift) | (0b0010101 << kFunct7Shift),
-  RO_FMAX_D = OP_FP | (0b001 << kFunct3Shift) | (0b0010101 << kFunct7Shift),
-  RO_FCVT_S_D = OP_FP | (0b0100000 << kFunct7Shift) | (0b00001 << kRs2Shift),
-  RO_FCVT_D_S = OP_FP | (0b0100001 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FEQ_D = OP_FP | (0b010 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
-  RO_FLT_D = OP_FP | (0b001 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
-  RO_FLE_D = OP_FP | (0b000 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
-  RO_FCLASS_D = OP_FP | (0b001 << kFunct3Shift) | (0b1110001 << kFunct7Shift) |
-                (0b00000 << kRs2Shift),
-  RO_FCVT_W_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FCVT_WU_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00001 << kRs2Shift),
-  RO_FCVT_D_W = OP_FP | (0b1101001 << kFunct7Shift) | (0b00000 << kRs2Shift),
-  RO_FCVT_D_WU = OP_FP | (0b1101001 << kFunct7Shift) | (0b00001 << kRs2Shift),
-
-  // RV64D Standard Extension (in addition to RV32D)
-  RO_FCVT_L_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00010 << kRs2Shift),
-  RO_FCVT_LU_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00011 << kRs2Shift),
-  RO_FMV_X_D = OP_FP | (0b000 << kFunct3Shift) | (0b1110001 << kFunct7Shift) |
-               (0b00000 << kRs2Shift),
-  RO_FCVT_D_L = OP_FP | (0b1101001 << kFunct7Shift) | (0b00010 << kRs2Shift),
-  RO_FCVT_D_LU = OP_FP | (0b1101001 << kFunct7Shift) | (0b00011 << kRs2Shift),
-  RO_FMV_D_X = OP_FP | (0b000 << kFunct3Shift) | (0b1111001 << kFunct7Shift) |
-               (0b00000 << kRs2Shift),
-
-  // RV64C Standard Extension
-  RO_C_ADDI4SPN = C0 | (0b000 << kRvcFunct3Shift),
-  RO_C_FLD = C0 | (0b001 << kRvcFunct3Shift),
-  RO_C_LW = C0 | (0b010 << kRvcFunct3Shift),
-  RO_C_LD = C0 | (0b011 << kRvcFunct3Shift),
-  RO_C_FSD = C0 | (0b101 << kRvcFunct3Shift),
-  RO_C_SW = C0 | (0b110 << kRvcFunct3Shift),
-  RO_C_SD = C0 | (0b111 << kRvcFunct3Shift),
-  RO_C_NOP_ADDI = C1 | (0b000 << kRvcFunct3Shift),
-  RO_C_ADDIW = C1 | (0b001 << kRvcFunct3Shift),
-  RO_C_LI = C1 | (0b010 << kRvcFunct3Shift),
-  RO_C_SUB = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift),
-  RO_C_XOR = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift),
-  RO_C_OR = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_2 << kRvcFunct2Shift),
-  RO_C_AND = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_3 << kRvcFunct2Shift),
-  RO_C_SUBW =
-      C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift),
-  RO_C_ADDW =
-      C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift),
-  RO_C_LUI_ADD = C1 | (0b011 << kRvcFunct3Shift),
-  RO_C_MISC_ALU = C1 | (0b100 << kRvcFunct3Shift),
-  RO_C_J = C1 | (0b101 << kRvcFunct3Shift),
-  RO_C_BEQZ = C1 | (0b110 << kRvcFunct3Shift),
-  RO_C_BNEZ = C1 | (0b111 << kRvcFunct3Shift),
-  RO_C_SLLI = C2 | (0b000 << kRvcFunct3Shift),
-  RO_C_FLDSP = C2 | (0b001 << kRvcFunct3Shift),
-  RO_C_LWSP = C2 | (0b010 << kRvcFunct3Shift),
-  RO_C_LDSP = C2 | (0b011 << kRvcFunct3Shift),
-  RO_C_JR_MV_ADD = C2 | (0b100 << kRvcFunct3Shift),
-  RO_C_JR = C2 | (0b1000 << kRvcFunct4Shift),
-  RO_C_MV = C2 | (0b1000 << kRvcFunct4Shift),
-  RO_C_EBREAK = C2 | (0b1001 << kRvcFunct4Shift),
-  RO_C_JALR = C2 | (0b1001 << kRvcFunct4Shift),
-  RO_C_ADD = C2 | (0b1001 << kRvcFunct4Shift),
-  RO_C_FSDSP = C2 | (0b101 << kRvcFunct3Shift),
-  RO_C_SWSP = C2 | (0b110 << kRvcFunct3Shift),
-  RO_C_SDSP = C2 | (0b111 << kRvcFunct3Shift),
-
-  // RVV Extension
-  OP_V = 0b1010111,
-  OP_IVV = OP_V | (0b000 << kFunct3Shift),
-  OP_FVV = OP_V | (0b001 << kFunct3Shift),
-  OP_MVV = OP_V | (0b010 << kFunct3Shift),
-  OP_IVI = OP_V | (0b011 << kFunct3Shift),
-  OP_IVX = OP_V | (0b100 << kFunct3Shift),
-  OP_FVF = OP_V | (0b101 << kFunct3Shift),
-  OP_MVX = OP_V | (0b110 << kFunct3Shift),
-
-  RO_V_VSETVLI = OP_V | (0b111 << kFunct3Shift) | 0b0 << 31,
-  RO_V_VSETIVLI = OP_V | (0b111 << kFunct3Shift) | 0b11 << 30,
-  RO_V_VSETVL = OP_V | (0b111 << kFunct3Shift) | 0b1 << 31,
-
-  // RVV LOAD/STORE
-  RO_V_VL = LOAD_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  RO_V_VLS = LOAD_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  RO_V_VLX = LOAD_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift),
-
-  RO_V_VS = STORE_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  RO_V_VSS = STORE_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  RO_V_VSX = STORE_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  RO_V_VSU = STORE_FP | (0b01 << kRvvMopShift) | (0b000 << kRvvNfShift),
-  // THE kFunct6Shift is mop
-  RO_V_VLSEG2 = LOAD_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VLSEG3 = LOAD_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VLSEG4 = LOAD_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VLSEG5 = LOAD_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VLSEG6 = LOAD_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VLSEG7 = LOAD_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VLSEG8 = LOAD_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  RO_V_VSSEG2 = STORE_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VSSEG3 = STORE_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VSSEG4 = STORE_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VSSEG5 = STORE_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VSSEG6 = STORE_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VSSEG7 = STORE_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VSSEG8 = STORE_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  RO_V_VLSSEG2 = LOAD_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VLSSEG3 = LOAD_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VLSSEG4 = LOAD_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VLSSEG5 = LOAD_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VLSSEG6 = LOAD_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VLSSEG7 = LOAD_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VLSSEG8 = LOAD_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  RO_V_VSSSEG2 = STORE_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VSSSEG3 = STORE_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VSSSEG4 = STORE_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VSSSEG5 = STORE_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VSSSEG6 = STORE_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VSSSEG7 = STORE_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VSSSEG8 = STORE_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  RO_V_VLXSEG2 = LOAD_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VLXSEG3 = LOAD_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VLXSEG4 = LOAD_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VLXSEG5 = LOAD_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VLXSEG6 = LOAD_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VLXSEG7 = LOAD_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VLXSEG8 = LOAD_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  RO_V_VSXSEG2 = STORE_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift),
-  RO_V_VSXSEG3 = STORE_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift),
-  RO_V_VSXSEG4 = STORE_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift),
-  RO_V_VSXSEG5 = STORE_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift),
-  RO_V_VSXSEG6 = STORE_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift),
-  RO_V_VSXSEG7 = STORE_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift),
-  RO_V_VSXSEG8 = STORE_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift),
-
-  // RVV Vector Arithmetic Instruction
-  VADD_FUNCT6 = 0b000000,
-  RO_V_VADD_VI = OP_IVI | (VADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VADD_VV = OP_IVV | (VADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VADD_VX = OP_IVX | (VADD_FUNCT6 << kRvvFunct6Shift),
-
-  VSUB_FUNCT6 = 0b000010,
-  RO_V_VSUB_VX = OP_IVX | (VSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSUB_VV = OP_IVV | (VSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VDIVU_FUNCT6 = 0b100000,
-  RO_V_VDIVU_VX = OP_MVX | (VDIVU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VDIVU_VV = OP_MVV | (VDIVU_FUNCT6 << kRvvFunct6Shift),
-
-  VDIV_FUNCT6 = 0b100001,
-  RO_V_VDIV_VX = OP_MVX | (VDIV_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VDIV_VV = OP_MVV | (VDIV_FUNCT6 << kRvvFunct6Shift),
-
-  VREMU_FUNCT6 = 0b100010,
-  RO_V_VREMU_VX = OP_MVX | (VREMU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VREMU_VV = OP_MVV | (VREMU_FUNCT6 << kRvvFunct6Shift),
-
-  VREM_FUNCT6 = 0b100011,
-  RO_V_VREM_VX = OP_MVX | (VREM_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VREM_VV = OP_MVV | (VREM_FUNCT6 << kRvvFunct6Shift),
-
-  VMULHU_FUNCT6 = 0b100100,
-  RO_V_VMULHU_VX = OP_MVX | (VMULHU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMULHU_VV = OP_MVV | (VMULHU_FUNCT6 << kRvvFunct6Shift),
-
-  VMUL_FUNCT6 = 0b100101,
-  RO_V_VMUL_VX = OP_MVX | (VMUL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMUL_VV = OP_MVV | (VMUL_FUNCT6 << kRvvFunct6Shift),
-
-  VWMUL_FUNCT6 = 0b111011,
-  RO_V_VWMUL_VX = OP_MVX | (VWMUL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VWMUL_VV = OP_MVV | (VWMUL_FUNCT6 << kRvvFunct6Shift),
-
-  VWMULU_FUNCT6 = 0b111000,
-  RO_V_VWMULU_VX = OP_MVX | (VWMULU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VWMULU_VV = OP_MVV | (VWMULU_FUNCT6 << kRvvFunct6Shift),
-
-  VMULHSU_FUNCT6 = 0b100110,
-  RO_V_VMULHSU_VX = OP_MVX | (VMULHSU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMULHSU_VV = OP_MVV | (VMULHSU_FUNCT6 << kRvvFunct6Shift),
-
-  VMULH_FUNCT6 = 0b100111,
-  RO_V_VMULH_VX = OP_MVX | (VMULH_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMULH_VV = OP_MVV | (VMULH_FUNCT6 << kRvvFunct6Shift),
-
-  VWADD_FUNCT6 = 0b110001,
-  RO_V_VWADD_VV = OP_MVV | (VWADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VWADD_VX = OP_MVX | (VWADD_FUNCT6 << kRvvFunct6Shift),
-
-  VWADDU_FUNCT6 = 0b110000,
-  RO_V_VWADDU_VV = OP_MVV | (VWADDU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VWADDU_VX = OP_MVX | (VWADDU_FUNCT6 << kRvvFunct6Shift),
-
-  VWADDUW_FUNCT6 = 0b110101,
-  RO_V_VWADDUW_VX = OP_MVX | (VWADDUW_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VWADDUW_VV = OP_MVV | (VWADDUW_FUNCT6 << kRvvFunct6Shift),
-
-  VCOMPRESS_FUNCT6 = 0b010111,
-  RO_V_VCOMPRESS_VV = OP_MVV | (VCOMPRESS_FUNCT6 << kRvvFunct6Shift),
-
-  VSADDU_FUNCT6 = 0b100000,
-  RO_V_VSADDU_VI = OP_IVI | (VSADDU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSADDU_VV = OP_IVV | (VSADDU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSADDU_VX = OP_IVX | (VSADDU_FUNCT6 << kRvvFunct6Shift),
-
-  VSADD_FUNCT6 = 0b100001,
-  RO_V_VSADD_VI = OP_IVI | (VSADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSADD_VV = OP_IVV | (VSADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSADD_VX = OP_IVX | (VSADD_FUNCT6 << kRvvFunct6Shift),
-
-  VSSUB_FUNCT6 = 0b100011,
-  RO_V_VSSUB_VV = OP_IVV | (VSSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSSUB_VX = OP_IVX | (VSSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VSSUBU_FUNCT6 = 0b100010,
-  RO_V_VSSUBU_VV = OP_IVV | (VSSUBU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSSUBU_VX = OP_IVX | (VSSUBU_FUNCT6 << kRvvFunct6Shift),
-
-  VRSUB_FUNCT6 = 0b000011,
-  RO_V_VRSUB_VX = OP_IVX | (VRSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VRSUB_VI = OP_IVI | (VRSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VMINU_FUNCT6 = 0b000100,
-  RO_V_VMINU_VX = OP_IVX | (VMINU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMINU_VV = OP_IVV | (VMINU_FUNCT6 << kRvvFunct6Shift),
-
-  VMIN_FUNCT6 = 0b000101,
-  RO_V_VMIN_VX = OP_IVX | (VMIN_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMIN_VV = OP_IVV | (VMIN_FUNCT6 << kRvvFunct6Shift),
-
-  VMAXU_FUNCT6 = 0b000110,
-  RO_V_VMAXU_VX = OP_IVX | (VMAXU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMAXU_VV = OP_IVV | (VMAXU_FUNCT6 << kRvvFunct6Shift),
-
-  VMAX_FUNCT6 = 0b000111,
-  RO_V_VMAX_VX = OP_IVX | (VMAX_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMAX_VV = OP_IVV | (VMAX_FUNCT6 << kRvvFunct6Shift),
-
-  VAND_FUNCT6 = 0b001001,
-  RO_V_VAND_VI = OP_IVI | (VAND_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VAND_VV = OP_IVV | (VAND_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VAND_VX = OP_IVX | (VAND_FUNCT6 << kRvvFunct6Shift),
-
-  VOR_FUNCT6 = 0b001010,
-  RO_V_VOR_VI = OP_IVI | (VOR_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VOR_VV = OP_IVV | (VOR_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VOR_VX = OP_IVX | (VOR_FUNCT6 << kRvvFunct6Shift),
-
-  VXOR_FUNCT6 = 0b001011,
-  RO_V_VXOR_VI = OP_IVI | (VXOR_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VXOR_VV = OP_IVV | (VXOR_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VXOR_VX = OP_IVX | (VXOR_FUNCT6 << kRvvFunct6Shift),
-
-  VRGATHER_FUNCT6 = 0b001100,
-  RO_V_VRGATHER_VI = OP_IVI | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VRGATHER_VV = OP_IVV | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VRGATHER_VX = OP_IVX | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
-
-  VMV_FUNCT6 = 0b010111,
-  RO_V_VMV_VI = OP_IVI | (VMV_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMV_VV = OP_IVV | (VMV_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMV_VX = OP_IVX | (VMV_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMV_VF = OP_FVF | (VMV_FUNCT6 << kRvvFunct6Shift),
-
-  RO_V_VMERGE_VI = RO_V_VMV_VI,
-  RO_V_VMERGE_VV = RO_V_VMV_VV,
-  RO_V_VMERGE_VX = RO_V_VMV_VX,
-
-  VMSEQ_FUNCT6 = 0b011000,
-  RO_V_VMSEQ_VI = OP_IVI | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSEQ_VV = OP_IVV | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSEQ_VX = OP_IVX | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
-
-  VMSNE_FUNCT6 = 0b011001,
-  RO_V_VMSNE_VI = OP_IVI | (VMSNE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSNE_VV = OP_IVV | (VMSNE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSNE_VX = OP_IVX | (VMSNE_FUNCT6 << kRvvFunct6Shift),
-
-  VMSLTU_FUNCT6 = 0b011010,
-  RO_V_VMSLTU_VV = OP_IVV | (VMSLTU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLTU_VX = OP_IVX | (VMSLTU_FUNCT6 << kRvvFunct6Shift),
-
-  VMSLT_FUNCT6 = 0b011011,
-  RO_V_VMSLT_VV = OP_IVV | (VMSLT_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLT_VX = OP_IVX | (VMSLT_FUNCT6 << kRvvFunct6Shift),
-
-  VMSLE_FUNCT6 = 0b011101,
-  RO_V_VMSLE_VI = OP_IVI | (VMSLE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLE_VV = OP_IVV | (VMSLE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLE_VX = OP_IVX | (VMSLE_FUNCT6 << kRvvFunct6Shift),
-
-  VMSLEU_FUNCT6 = 0b011100,
-  RO_V_VMSLEU_VI = OP_IVI | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLEU_VV = OP_IVV | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSLEU_VX = OP_IVX | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
-
-  VMSGTU_FUNCT6 = 0b011110,
-  RO_V_VMSGTU_VI = OP_IVI | (VMSGTU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSGTU_VX = OP_IVX | (VMSGTU_FUNCT6 << kRvvFunct6Shift),
-
-  VMSGT_FUNCT6 = 0b011111,
-  RO_V_VMSGT_VI = OP_IVI | (VMSGT_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMSGT_VX = OP_IVX | (VMSGT_FUNCT6 << kRvvFunct6Shift),
-
-  VSLIDEUP_FUNCT6 = 0b001110,
-  RO_V_VSLIDEUP_VI = OP_IVI | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSLIDEUP_VX = OP_IVX | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift),
-
-  VSLIDEDOWN_FUNCT6 = 0b001111,
-  RO_V_VSLIDEDOWN_VI = OP_IVI | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSLIDEDOWN_VX = OP_IVX | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift),
-
-  VSRL_FUNCT6 = 0b101000,
-  RO_V_VSRL_VI = OP_IVI | (VSRL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSRL_VV = OP_IVV | (VSRL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSRL_VX = OP_IVX | (VSRL_FUNCT6 << kRvvFunct6Shift),
-
-  VSRA_FUNCT6 = 0b101001,
-  RO_V_VSRA_VI = OP_IVI | (VSRA_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSRA_VV = OP_IVV | (VSRA_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSRA_VX = OP_IVX | (VSRA_FUNCT6 << kRvvFunct6Shift),
-
-  VSLL_FUNCT6 = 0b100101,
-  RO_V_VSLL_VI = OP_IVI | (VSLL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSLL_VV = OP_IVV | (VSLL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSLL_VX = OP_IVX | (VSLL_FUNCT6 << kRvvFunct6Shift),
-
-  VSMUL_FUNCT6 = 0b100111,
-  RO_V_VSMUL_VV = OP_IVV | (VSMUL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VSMUL_VX = OP_IVX | (VSMUL_FUNCT6 << kRvvFunct6Shift),
-
-  VADC_FUNCT6 = 0b010000,
-  RO_V_VADC_VI = OP_IVI | (VADC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VADC_VV = OP_IVV | (VADC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VADC_VX = OP_IVX | (VADC_FUNCT6 << kRvvFunct6Shift),
-
-  VMADC_FUNCT6 = 0b010001,
-  RO_V_VMADC_VI = OP_IVI | (VMADC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMADC_VV = OP_IVV | (VMADC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMADC_VX = OP_IVX | (VMADC_FUNCT6 << kRvvFunct6Shift),
-
-  VWXUNARY0_FUNCT6 = 0b010000,
-  VRXUNARY0_FUNCT6 = 0b010000,
-  VMUNARY0_FUNCT6 = 0b010100,
-
-  RO_V_VWXUNARY0 = OP_MVV | (VWXUNARY0_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VRXUNARY0 = OP_MVX | (VRXUNARY0_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMUNARY0 = OP_MVV | (VMUNARY0_FUNCT6 << kRvvFunct6Shift),
-
-  VID_V = 0b10001,
-
-  VXUNARY0_FUNCT6 = 0b010010,
-  RO_V_VXUNARY0 = OP_MVV | (VXUNARY0_FUNCT6 << kRvvFunct6Shift),
-
-  VWFUNARY0_FUNCT6 = 0b010000,
-  RO_V_VFMV_FS = OP_FVV | (VWFUNARY0_FUNCT6 << kRvvFunct6Shift),
-
-  VRFUNARY0_FUNCT6 = 0b010000,
-  RO_V_VFMV_SF = OP_FVF | (VRFUNARY0_FUNCT6 << kRvvFunct6Shift),
-
-  VREDMAXU_FUNCT6 = 0b000110,
-  RO_V_VREDMAXU = OP_MVV | (VREDMAXU_FUNCT6 << kRvvFunct6Shift),
-  VREDMAX_FUNCT6 = 0b000111,
-  RO_V_VREDMAX = OP_MVV | (VREDMAX_FUNCT6 << kRvvFunct6Shift),
-
-  VREDMINU_FUNCT6 = 0b000100,
-  RO_V_VREDMINU = OP_MVV | (VREDMINU_FUNCT6 << kRvvFunct6Shift),
-  VREDMIN_FUNCT6 = 0b000101,
-  RO_V_VREDMIN = OP_MVV | (VREDMIN_FUNCT6 << kRvvFunct6Shift),
-
-  VFUNARY0_FUNCT6 = 0b010010,
-  RO_V_VFUNARY0 = OP_FVV | (VFUNARY0_FUNCT6 << kRvvFunct6Shift),
-  VFUNARY1_FUNCT6 = 0b010011,
-  RO_V_VFUNARY1 = OP_FVV | (VFUNARY1_FUNCT6 << kRvvFunct6Shift),
-
-  VFCVT_XU_F_V = 0b00000,
-  VFCVT_X_F_V = 0b00001,
-  VFCVT_F_XU_V = 0b00010,
-  VFCVT_F_X_V = 0b00011,
-  VFWCVT_XU_F_V = 0b01000,
-  VFWCVT_X_F_V = 0b01001,
-  VFWCVT_F_XU_V = 0b01010,
-  VFWCVT_F_X_V = 0b01011,
-  VFWCVT_F_F_V = 0b01100,
-  VFNCVT_F_F_W = 0b10100,
-  VFNCVT_X_F_W = 0b10001,
-  VFNCVT_XU_F_W = 0b10000,
-
-  VFCLASS_V = 0b10000,
-  VFSQRT_V = 0b00000,
-  VFRSQRT7_V = 0b00100,
-  VFREC7_V = 0b00101,
-
-  VFADD_FUNCT6 = 0b000000,
-  RO_V_VFADD_VV = OP_FVV | (VFADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFADD_VF = OP_FVF | (VFADD_FUNCT6 << kRvvFunct6Shift),
-
-  VFSUB_FUNCT6 = 0b000010,
-  RO_V_VFSUB_VV = OP_FVV | (VFSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFSUB_VF = OP_FVF | (VFSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VFDIV_FUNCT6 = 0b100000,
-  RO_V_VFDIV_VV = OP_FVV | (VFDIV_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFDIV_VF = OP_FVF | (VFDIV_FUNCT6 << kRvvFunct6Shift),
-
-  VFMUL_FUNCT6 = 0b100100,
-  RO_V_VFMUL_VV = OP_FVV | (VFMUL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMUL_VF = OP_FVF | (VFMUL_FUNCT6 << kRvvFunct6Shift),
-
-  // Vector Widening Floating-Point Add/Subtract Instructions
-  VFWADD_FUNCT6 = 0b110000,
-  RO_V_VFWADD_VV = OP_FVV | (VFWADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWADD_VF = OP_FVF | (VFWADD_FUNCT6 << kRvvFunct6Shift),
-
-  VFWSUB_FUNCT6 = 0b110010,
-  RO_V_VFWSUB_VV = OP_FVV | (VFWSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWSUB_VF = OP_FVF | (VFWSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VFWADD_W_FUNCT6 = 0b110100,
-  RO_V_VFWADD_W_VV = OP_FVV | (VFWADD_W_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWADD_W_VF = OP_FVF | (VFWADD_W_FUNCT6 << kRvvFunct6Shift),
-
-  VFWSUB_W_FUNCT6 = 0b110110,
-  RO_V_VFWSUB_W_VV = OP_FVV | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWSUB_W_VF = OP_FVF | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift),
-
-  // Vector Widening Floating-Point Reduction Instructions
-  VFWREDUSUM_FUNCT6 = 0b110001,
-  RO_V_VFWREDUSUM_VV = OP_FVV | (VFWREDUSUM_FUNCT6 << kRvvFunct6Shift),
-
-  VFWREDOSUM_FUNCT6 = 0b110011,
-  RO_V_VFWREDOSUM_VV = OP_FVV | (VFWREDOSUM_FUNCT6 << kRvvFunct6Shift),
-
-  // Vector Widening Floating-Point Multiply
-  VFWMUL_FUNCT6 = 0b111000,
-  RO_V_VFWMUL_VV = OP_FVV | (VFWMUL_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWMUL_VF = OP_FVF | (VFWMUL_FUNCT6 << kRvvFunct6Shift),
-
-  VMFEQ_FUNCT6 = 0b011000,
-  RO_V_VMFEQ_VV = OP_FVV | (VMFEQ_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMFEQ_VF = OP_FVF | (VMFEQ_FUNCT6 << kRvvFunct6Shift),
-
-  VMFNE_FUNCT6 = 0b011100,
-  RO_V_VMFNE_VV = OP_FVV | (VMFNE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMFNE_VF = OP_FVF | (VMFNE_FUNCT6 << kRvvFunct6Shift),
-
-  VMFLT_FUNCT6 = 0b011011,
-  RO_V_VMFLT_VV = OP_FVV | (VMFLT_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMFLT_VF = OP_FVF | (VMFLT_FUNCT6 << kRvvFunct6Shift),
-
-  VMFLE_FUNCT6 = 0b011001,
-  RO_V_VMFLE_VV = OP_FVV | (VMFLE_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VMFLE_VF = OP_FVF | (VMFLE_FUNCT6 << kRvvFunct6Shift),
-
-  VMFGE_FUNCT6 = 0b011111,
-  RO_V_VMFGE_VF = OP_FVF | (VMFGE_FUNCT6 << kRvvFunct6Shift),
-
-  VMFGT_FUNCT6 = 0b011101,
-  RO_V_VMFGT_VF = OP_FVF | (VMFGT_FUNCT6 << kRvvFunct6Shift),
-
-  VFMAX_FUNCT6 = 0b000110,
-  RO_V_VFMAX_VV = OP_FVV | (VFMAX_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMAX_VF = OP_FVF | (VFMAX_FUNCT6 << kRvvFunct6Shift),
-
-  VFREDMAX_FUNCT6 = 0b0001111,
-  RO_V_VFREDMAX_VV = OP_FVV | (VFREDMAX_FUNCT6 << kRvvFunct6Shift),
-
-  VFMIN_FUNCT6 = 0b000100,
-  RO_V_VFMIN_VV = OP_FVV | (VFMIN_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMIN_VF = OP_FVF | (VFMIN_FUNCT6 << kRvvFunct6Shift),
-
-  VFSGNJ_FUNCT6 = 0b001000,
-  RO_V_VFSGNJ_VV = OP_FVV | (VFSGNJ_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFSGNJ_VF = OP_FVF | (VFSGNJ_FUNCT6 << kRvvFunct6Shift),
-
-  VFSGNJN_FUNCT6 = 0b001001,
-  RO_V_VFSGNJN_VV = OP_FVV | (VFSGNJN_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFSGNJN_VF = OP_FVF | (VFSGNJN_FUNCT6 << kRvvFunct6Shift),
-
-  VFSGNJX_FUNCT6 = 0b001010,
-  RO_V_VFSGNJX_VV = OP_FVV | (VFSGNJX_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFSGNJX_VF = OP_FVF | (VFSGNJX_FUNCT6 << kRvvFunct6Shift),
-
-  VFMADD_FUNCT6 = 0b101000,
-  RO_V_VFMADD_VV = OP_FVV | (VFMADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMADD_VF = OP_FVF | (VFMADD_FUNCT6 << kRvvFunct6Shift),
-
-  VFNMADD_FUNCT6 = 0b101001,
-  RO_V_VFNMADD_VV = OP_FVV | (VFNMADD_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFNMADD_VF = OP_FVF | (VFNMADD_FUNCT6 << kRvvFunct6Shift),
-
-  VFMSUB_FUNCT6 = 0b101010,
-  RO_V_VFMSUB_VV = OP_FVV | (VFMSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMSUB_VF = OP_FVF | (VFMSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VFNMSUB_FUNCT6 = 0b101011,
-  RO_V_VFNMSUB_VV = OP_FVV | (VFNMSUB_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFNMSUB_VF = OP_FVF | (VFNMSUB_FUNCT6 << kRvvFunct6Shift),
-
-  VFMACC_FUNCT6 = 0b101100,
-  RO_V_VFMACC_VV = OP_FVV | (VFMACC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMACC_VF = OP_FVF | (VFMACC_FUNCT6 << kRvvFunct6Shift),
-
-  VFNMACC_FUNCT6 = 0b101101,
-  RO_V_VFNMACC_VV = OP_FVV | (VFNMACC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFNMACC_VF = OP_FVF | (VFNMACC_FUNCT6 << kRvvFunct6Shift),
-
-  VFMSAC_FUNCT6 = 0b101110,
-  RO_V_VFMSAC_VV = OP_FVV | (VFMSAC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFMSAC_VF = OP_FVF | (VFMSAC_FUNCT6 << kRvvFunct6Shift),
-
-  VFNMSAC_FUNCT6 = 0b101111,
-  RO_V_VFNMSAC_VV = OP_FVV | (VFNMSAC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFNMSAC_VF = OP_FVF | (VFNMSAC_FUNCT6 << kRvvFunct6Shift),
-
-  // Vector Widening Floating-Point Fused Multiply-Add Instructions
-  VFWMACC_FUNCT6 = 0b111100,
-  RO_V_VFWMACC_VV = OP_FVV | (VFWMACC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWMACC_VF = OP_FVF | (VFWMACC_FUNCT6 << kRvvFunct6Shift),
-
-  VFWNMACC_FUNCT6 = 0b111101,
-  RO_V_VFWNMACC_VV = OP_FVV | (VFWNMACC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWNMACC_VF = OP_FVF | (VFWNMACC_FUNCT6 << kRvvFunct6Shift),
-
-  VFWMSAC_FUNCT6 = 0b111110,
-  RO_V_VFWMSAC_VV = OP_FVV | (VFWMSAC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWMSAC_VF = OP_FVF | (VFWMSAC_FUNCT6 << kRvvFunct6Shift),
-
-  VFWNMSAC_FUNCT6 = 0b111111,
-  RO_V_VFWNMSAC_VV = OP_FVV | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VFWNMSAC_VF = OP_FVF | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift),
-
-  VNCLIP_FUNCT6 = 0b101111,
-  RO_V_VNCLIP_WV = OP_IVV | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VNCLIP_WX = OP_IVX | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VNCLIP_WI = OP_IVI | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
-
-  VNCLIPU_FUNCT6 = 0b101110,
-  RO_V_VNCLIPU_WV = OP_IVV | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VNCLIPU_WX = OP_IVX | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
-  RO_V_VNCLIPU_WI = OP_IVI | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
-};
 
 // ----- Emulated conditions.
 // On RISC-V we use this enum to abstract from conditional branch instructions.
 // The 'U' prefix is used to specify unsigned comparisons.
 // Opposite conditions must be paired as odd/even numbers
 // because 'NegateCondition' function flips LSB to negate condition.
-enum Condition {
+enum Condition {  // Any value < 0 is considered no_condition.
   overflow = 0,
   no_overflow = 1,
   Uless = 2,
@@ -1257,34 +530,6 @@ enum FClassFlag {
   kQuietNaN = 1 << 9
 };
 
-#define RVV_SEW(V) \
-  V(E8)            \
-  V(E16)           \
-  V(E32)           \
-  V(E64)
-
-#define DEFINE_FLAG(name) name,
-enum VSew {
-  RVV_SEW(DEFINE_FLAG)
-#undef DEFINE_FLAG
-};
-
-#define RVV_LMUL(V) \
-  V(m1)             \
-  V(m2)             \
-  V(m4)             \
-  V(m8)             \
-  V(RESERVERD)      \
-  V(mf8)            \
-  V(mf4)            \
-  V(mf2)
-
-enum Vlmul {
-#define DEFINE_FLAG(name) name,
-  RVV_LMUL(DEFINE_FLAG)
-#undef DEFINE_FLAG
-};
-
 enum TailAgnosticType {
   ta = 0x1,  // Tail agnostic
   tu = 0x0,  // Tail undisturbed
@@ -1309,6 +554,42 @@ enum Hint { no_hint = 0 };
 
 inline Hint NegateHint(Hint hint) { return no_hint; }
 
+enum BaseOpcode : uint32_t {
+  LOAD = 0b0000011,      // I form: LB LH LW LBU LHU
+  LOAD_FP = 0b0000111,   // I form: FLW FLD FLQ
+  MISC_MEM = 0b0001111,  // I special form: FENCE FENCE.I
+  OP_IMM = 0b0010011,    // I form: ADDI SLTI SLTIU XORI ORI ANDI SLLI SRLI SRAI
+  // Note: SLLI/SRLI/SRAI I form first, then func3 001/101 => R type
+  AUIPC = 0b0010111,      // U form: AUIPC
+  OP_IMM_32 = 0b0011011,  // I form: ADDIW SLLIW SRLIW SRAIW
+  // Note:  SRLIW SRAIW I form first, then func3 101 special shift encoding
+  STORE = 0b0100011,     // S form: SB SH SW SD
+  STORE_FP = 0b0100111,  // S form: FSW FSD FSQ
+  AMO = 0b0101111,       // R form: All A instructions
+  OP = 0b0110011,      // R: ADD SUB SLL SLT SLTU XOR SRL SRA OR AND and 32M set
+  LUI = 0b0110111,     // U form: LUI
+  OP_32 = 0b0111011,   // R: ADDW SUBW SLLW SRLW SRAW MULW DIVW DIVUW REMW REMUW
+  MADD = 0b1000011,    // R4 type: FMADD.S FMADD.D FMADD.Q
+  MSUB = 0b1000111,    // R4 type: FMSUB.S FMSUB.D FMSUB.Q
+  NMSUB = 0b1001011,   // R4 type: FNMSUB.S FNMSUB.D FNMSUB.Q
+  NMADD = 0b1001111,   // R4 type: FNMADD.S FNMADD.D FNMADD.Q
+  OP_FP = 0b1010011,   // R type: Q ext
+  BRANCH = 0b1100011,  // B form: BEQ BNE, BLT, BGE, BLTU BGEU
+  JALR = 0b1100111,    // I form: JALR
+  JAL = 0b1101111,     // J form: JAL
+  SYSTEM = 0b1110011,  // I form: ECALL EBREAK Zicsr ext
+  OP_V = 0b1010111,    // V form: RVV
+
+  // C extension
+  C0 = 0b00,
+  C1 = 0b01,
+  C2 = 0b10,
+  FUNCT2_0 = 0b00,
+  FUNCT2_1 = 0b01,
+  FUNCT2_2 = 0b10,
+  FUNCT2_3 = 0b11,
+};
+
 // -----------------------------------------------------------------------------
 // Specific instructions, constants, and masks.
 // These constants are declared in assembler-riscv64.cc, as they use named
@@ -1373,10 +654,7 @@ class InstructionBase {
     return FirstHalfWord == 0;
   }
 
-  inline bool IsShortInstruction() const {
-    uint8_t FirstByte = *reinterpret_cast<const uint8_t*>(this);
-    return (FirstByte & 0x03) <= C2;
-  }
+  bool IsShortInstruction() const;
 
   inline uint8_t InstructionSize() const {
     return (FLAG_riscv_c_extension && this->IsShortInstruction())
@@ -1406,14 +684,14 @@ class InstructionBase {
   }
 
   // Accessors for the different named fields used in the RISC-V encoding.
-  inline Opcode BaseOpcodeValue() const {
-    return static_cast<Opcode>(
+  inline BaseOpcode BaseOpcodeValue() const {
+    return static_cast<BaseOpcode>(
         Bits(kBaseOpcodeShift + kBaseOpcodeBits - 1, kBaseOpcodeShift));
   }
 
   // Return the fields at their original place in the instruction encoding.
-  inline Opcode BaseOpcodeFieldRaw() const {
-    return static_cast<Opcode>(InstructionBits() & kBaseOpcodeMask);
+  inline BaseOpcode BaseOpcodeFieldRaw() const {
+    return static_cast<BaseOpcode>(InstructionBits() & kBaseOpcodeMask);
   }
 
   // Safe to call within R-type instructions
@@ -1524,32 +802,17 @@ class InstructionGetters : public T {
     return this->Bits(kRdShift + kRdBits - 1, kRdShift);
   }
 
-  inline int RvcRdValue() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcRdShift + kRvcRdBits - 1, kRvcRdShift);
-  }
-
   inline int RvcRs1Value() const { return this->RvcRdValue(); }
 
-  inline int RvcRs2Value() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcRs2Shift + kRvcRs2Bits - 1, kRvcRs2Shift);
-  }
+  int RvcRdValue() const;
 
-  inline int RvcRs1sValue() const {
-    DCHECK(this->IsShortInstruction());
-    return 0b1000 + this->Bits(kRvcRs1sShift + kRvcRs1sBits - 1, kRvcRs1sShift);
-  }
+  int RvcRs2Value() const;
 
-  inline int RvcRs2sValue() const {
-    DCHECK(this->IsShortInstruction());
-    return 0b1000 + this->Bits(kRvcRs2sShift + kRvcRs2sBits - 1, kRvcRs2sShift);
-  }
+  int RvcRs1sValue() const;
 
-  inline int Funct7Value() const {
-    DCHECK(this->InstructionType() == InstructionBase::kRType);
-    return this->Bits(kFunct7Shift + kFunct7Bits - 1, kFunct7Shift);
-  }
+  int RvcRs2sValue() const;
+
+  int Funct7Value() const;
 
   inline int Funct3Value() const {
     DCHECK(this->InstructionType() == InstructionBase::kRType ||
@@ -1565,30 +828,15 @@ class InstructionGetters : public T {
     return this->Bits(kFunct5Shift + kFunct5Bits - 1, kFunct5Shift);
   }
 
-  inline int RvcFunct6Value() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcFunct6Shift + kRvcFunct6Bits - 1, kRvcFunct6Shift);
-  }
+  int RvcFunct6Value() const;
 
-  inline int RvcFunct4Value() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcFunct4Shift + kRvcFunct4Bits - 1, kRvcFunct4Shift);
-  }
+  int RvcFunct4Value() const;
 
-  inline int RvcFunct3Value() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcFunct3Shift + kRvcFunct3Bits - 1, kRvcFunct3Shift);
-  }
+  int RvcFunct3Value() const;
 
-  inline int RvcFunct2Value() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcFunct2Shift + kRvcFunct2Bits - 1, kRvcFunct2Shift);
-  }
+  int RvcFunct2Value() const;
 
-  inline int RvcFunct2BValue() const {
-    DCHECK(this->IsShortInstruction());
-    return this->Bits(kRvcFunct2BShift + kRvcFunct2Bits - 1, kRvcFunct2BShift);
-  }
+  int RvcFunct2BValue() const;
 
   inline int CsrValue() const {
     DCHECK(this->InstructionType() == InstructionBase::kIType &&
@@ -1844,30 +1092,9 @@ class InstructionGetters : public T {
     return width;
   }
 
-  inline uint32_t Rvvzimm() const {
-    if ((this->InstructionBits() &
-         (kBaseOpcodeMask | kFunct3Mask | 0x80000000)) == RO_V_VSETVLI) {
-      uint32_t Bits = this->InstructionBits();
-      uint32_t zimm = Bits & kRvvZimmMask;
-      return zimm >> kRvvZimmShift;
-    } else {
-      DCHECK_EQ(this->InstructionBits() &
-                    (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
-                RO_V_VSETIVLI);
-      uint32_t Bits = this->InstructionBits();
-      uint32_t zimm = Bits & kRvvZimmMask;
-      return (zimm >> kRvvZimmShift) & 0x3FF;
-    }
-  }
+  uint32_t Rvvzimm() const;
 
-  inline uint32_t Rvvuimm() const {
-    DCHECK_EQ(
-        this->InstructionBits() & (kBaseOpcodeMask | kFunct3Mask | 0xC0000000),
-        RO_V_VSETIVLI);
-    uint32_t Bits = this->InstructionBits();
-    uint32_t uimm = Bits & kRvvUimmMask;
-    return uimm >> kRvvUimmShift;
-  }
+  uint32_t Rvvuimm() const;
 
   inline uint32_t RvvVsew() const {
     uint32_t zimm = this->Rvvzimm();
@@ -1979,4 +1206,4 @@ bool InstructionGetters<P>::IsTrap() const {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_CODEGEN_RISCV64_CONSTANTS_RISCV64_H_
+#endif  // V8_CODEGEN_RISCV_BASE_CONSTANTS_RISCV_H_
diff --git a/src/codegen/riscv/base-riscv-i.cc b/src/codegen/riscv/base-riscv-i.cc
new file mode 100644
index 00000000000..19687c9370b
--- /dev/null
+++ b/src/codegen/riscv/base-riscv-i.cc
@@ -0,0 +1,330 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/base-riscv-i.h"
+
+namespace v8 {
+namespace internal {
+
+void AssemblerRISCVI::lui(Register rd, int32_t imm20) {
+  GenInstrU(LUI, rd, imm20);
+}
+
+void AssemblerRISCVI::auipc(Register rd, int32_t imm20) {
+  GenInstrU(AUIPC, rd, imm20);
+}
+
+// Jumps
+
+void AssemblerRISCVI::jal(Register rd, int32_t imm21) {
+  GenInstrJ(JAL, rd, imm21);
+  BlockTrampolinePoolFor(1);
+}
+
+void AssemblerRISCVI::jalr(Register rd, Register rs1, int16_t imm12) {
+  GenInstrI(0b000, JALR, rd, rs1, imm12);
+  BlockTrampolinePoolFor(1);
+}
+
+// Branches
+
+void AssemblerRISCVI::beq(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b000, rs1, rs2, imm13);
+}
+
+void AssemblerRISCVI::bne(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b001, rs1, rs2, imm13);
+}
+
+void AssemblerRISCVI::blt(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b100, rs1, rs2, imm13);
+}
+
+void AssemblerRISCVI::bge(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b101, rs1, rs2, imm13);
+}
+
+void AssemblerRISCVI::bltu(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b110, rs1, rs2, imm13);
+}
+
+void AssemblerRISCVI::bgeu(Register rs1, Register rs2, int16_t imm13) {
+  GenInstrBranchCC_rri(0b111, rs1, rs2, imm13);
+}
+
+// Loads
+
+void AssemblerRISCVI::lb(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b000, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::lh(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b001, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::lw(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b010, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::lbu(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b100, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::lhu(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b101, rd, rs1, imm12);
+}
+
+// Stores
+
+void AssemblerRISCVI::sb(Register source, Register base, int16_t imm12) {
+  GenInstrStore_rri(0b000, base, source, imm12);
+}
+
+void AssemblerRISCVI::sh(Register source, Register base, int16_t imm12) {
+  GenInstrStore_rri(0b001, base, source, imm12);
+}
+
+void AssemblerRISCVI::sw(Register source, Register base, int16_t imm12) {
+  GenInstrStore_rri(0b010, base, source, imm12);
+}
+
+// Arithmetic with immediate
+
+void AssemblerRISCVI::addi(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b000, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::slti(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b010, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::sltiu(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b011, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::xori(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b100, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::ori(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b110, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::andi(Register rd, Register rs1, int16_t imm12) {
+  GenInstrALU_ri(0b111, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::slli(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShift_ri(0, 0b001, rd, rs1, shamt & 0x3f);
+}
+
+void AssemblerRISCVI::srli(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShift_ri(0, 0b101, rd, rs1, shamt & 0x3f);
+}
+
+void AssemblerRISCVI::srai(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShift_ri(1, 0b101, rd, rs1, shamt & 0x3f);
+}
+
+// Arithmetic
+
+void AssemblerRISCVI::add(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sub(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0100000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sll(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::slt(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sltu(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::xor_(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b100, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::srl(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b101, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sra(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0100000, 0b101, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::or_(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b110, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::and_(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000000, 0b111, rd, rs1, rs2);
+}
+
+// Memory fences
+
+void AssemblerRISCVI::fence(uint8_t pred, uint8_t succ) {
+  DCHECK(is_uint4(pred) && is_uint4(succ));
+  uint16_t imm12 = succ | (pred << 4) | (0b0000 << 8);
+  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
+}
+
+void AssemblerRISCVI::fence_tso() {
+  uint16_t imm12 = (0b0011) | (0b0011 << 4) | (0b1000 << 8);
+  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
+}
+
+// Environment call / break
+
+void AssemblerRISCVI::ecall() {
+  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 0);
+}
+
+void AssemblerRISCVI::ebreak() {
+  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 1);
+}
+
+// This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
+// instruction (i.e., it should always trap, if your implementation has invalid
+// instruction traps).
+void AssemblerRISCVI::unimp() {
+  GenInstrI(0b001, SYSTEM, ToRegister(0), ToRegister(0), 0b110000000000);
+}
+
+bool AssemblerRISCVI::IsBranch(Instr instr) {
+  return (instr & kBaseOpcodeMask) == BRANCH;
+}
+
+bool AssemblerRISCVI::IsJump(Instr instr) {
+  int Op = instr & kBaseOpcodeMask;
+  return Op == JAL || Op == JALR;
+}
+
+bool AssemblerRISCVI::IsNop(Instr instr) { return instr == kNopByte; }
+
+bool AssemblerRISCVI::IsJal(Instr instr) {
+  return (instr & kBaseOpcodeMask) == JAL;
+}
+
+bool AssemblerRISCVI::IsJalr(Instr instr) {
+  return (instr & kBaseOpcodeMask) == JALR;
+}
+
+bool AssemblerRISCVI::IsLui(Instr instr) {
+  return (instr & kBaseOpcodeMask) == LUI;
+}
+bool AssemblerRISCVI::IsAuipc(Instr instr) {
+  return (instr & kBaseOpcodeMask) == AUIPC;
+}
+bool AssemblerRISCVI::IsAddi(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDI;
+}
+bool AssemblerRISCVI::IsOri(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ORI;
+}
+bool AssemblerRISCVI::IsSlli(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_SLLI;
+}
+
+int AssemblerRISCVI::JumpOffset(Instr instr) {
+  int32_t imm21 = ((instr & 0x7fe00000) >> 20) | ((instr & 0x100000) >> 9) |
+                  (instr & 0xff000) | ((instr & 0x80000000) >> 11);
+  imm21 = imm21 << 11 >> 11;
+  return imm21;
+}
+
+int AssemblerRISCVI::JalrOffset(Instr instr) {
+  DCHECK(IsJalr(instr));
+  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
+  return imm12;
+}
+
+int AssemblerRISCVI::AuipcOffset(Instr instr) {
+  DCHECK(IsAuipc(instr));
+  int32_t imm20 = static_cast<int32_t>(instr & kImm20Mask);
+  return imm20;
+}
+
+bool AssemblerRISCVI::IsLw(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_LW;
+}
+
+int AssemblerRISCVI::LoadOffset(Instr instr) {
+#if V8_TARGET_ARCH_RISCV64
+  DCHECK(IsLd(instr));
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK(IsLw(instr));
+#endif
+  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
+  return imm12;
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+
+bool AssemblerRISCVI::IsAddiw(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDIW;
+}
+
+bool AssemblerRISCVI::IsLd(Instr instr) {
+  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_LD;
+}
+
+void AssemblerRISCVI::lwu(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b110, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::ld(Register rd, Register rs1, int16_t imm12) {
+  GenInstrLoad_ri(0b011, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::sd(Register source, Register base, int16_t imm12) {
+  GenInstrStore_rri(0b011, base, source, imm12);
+}
+
+void AssemblerRISCVI::addiw(Register rd, Register rs1, int16_t imm12) {
+  GenInstrI(0b000, OP_IMM_32, rd, rs1, imm12);
+}
+
+void AssemblerRISCVI::slliw(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShiftW_ri(0, 0b001, rd, rs1, shamt & 0x1f);
+}
+
+void AssemblerRISCVI::srliw(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShiftW_ri(0, 0b101, rd, rs1, shamt & 0x1f);
+}
+
+void AssemblerRISCVI::sraiw(Register rd, Register rs1, uint8_t shamt) {
+  GenInstrShiftW_ri(1, 0b101, rd, rs1, shamt & 0x1f);
+}
+
+void AssemblerRISCVI::addw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::subw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0100000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sllw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000000, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::srlw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000000, 0b101, rd, rs1, rs2);
+}
+
+void AssemblerRISCVI::sraw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0100000, 0b101, rd, rs1, rs2);
+}
+
+#endif
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/base-riscv-i.h b/src/codegen/riscv/base-riscv-i.h
new file mode 100644
index 00000000000..5b3a2070bdd
--- /dev/null
+++ b/src/codegen/riscv/base-riscv-i.h
@@ -0,0 +1,212 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-i.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+#ifndef V8_CODEGEN_RISCV_BASE_RISCV_I_H_
+#define V8_CODEGEN_RISCV_BASE_RISCV_I_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVI : public AssemblerRiscvBase {
+ public:
+  void lui(Register rd, int32_t imm20);
+  void auipc(Register rd, int32_t imm20);
+
+  // Jumps
+  void jal(Register rd, int32_t imm20);
+  void jalr(Register rd, Register rs1, int16_t imm12);
+
+  // Branches
+  void beq(Register rs1, Register rs2, int16_t imm12);
+  void bne(Register rs1, Register rs2, int16_t imm12);
+  void blt(Register rs1, Register rs2, int16_t imm12);
+  void bge(Register rs1, Register rs2, int16_t imm12);
+  void bltu(Register rs1, Register rs2, int16_t imm12);
+  void bgeu(Register rs1, Register rs2, int16_t imm12);
+  // Loads
+  void lb(Register rd, Register rs1, int16_t imm12);
+  void lh(Register rd, Register rs1, int16_t imm12);
+  void lw(Register rd, Register rs1, int16_t imm12);
+  void lbu(Register rd, Register rs1, int16_t imm12);
+  void lhu(Register rd, Register rs1, int16_t imm12);
+
+  // Stores
+  void sb(Register source, Register base, int16_t imm12);
+  void sh(Register source, Register base, int16_t imm12);
+  void sw(Register source, Register base, int16_t imm12);
+
+  // Arithmetic with immediate
+  void addi(Register rd, Register rs1, int16_t imm12);
+  void slti(Register rd, Register rs1, int16_t imm12);
+  void sltiu(Register rd, Register rs1, int16_t imm12);
+  void xori(Register rd, Register rs1, int16_t imm12);
+  void ori(Register rd, Register rs1, int16_t imm12);
+  void andi(Register rd, Register rs1, int16_t imm12);
+  void slli(Register rd, Register rs1, uint8_t shamt);
+  void srli(Register rd, Register rs1, uint8_t shamt);
+  void srai(Register rd, Register rs1, uint8_t shamt);
+
+  // Arithmetic
+  void add(Register rd, Register rs1, Register rs2);
+  void sub(Register rd, Register rs1, Register rs2);
+  void sll(Register rd, Register rs1, Register rs2);
+  void slt(Register rd, Register rs1, Register rs2);
+  void sltu(Register rd, Register rs1, Register rs2);
+  void xor_(Register rd, Register rs1, Register rs2);
+  void srl(Register rd, Register rs1, Register rs2);
+  void sra(Register rd, Register rs1, Register rs2);
+  void or_(Register rd, Register rs1, Register rs2);
+  void and_(Register rd, Register rs1, Register rs2);
+
+  // Other pseudo instructions that are not part of RISCV pseudo assemly
+  void nor(Register rd, Register rs, Register rt) {
+    or_(rd, rs, rt);
+    not_(rd, rd);
+  }
+
+  // Memory fences
+  void fence(uint8_t pred, uint8_t succ);
+  void fence_tso();
+
+  // Environment call / break
+  void ecall();
+  void ebreak();
+
+  void sync() { fence(0b1111, 0b1111); }
+
+  // This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
+  // instruction (i.e., it should always trap, if your implementation has
+  // invalid instruction traps).
+  void unimp();
+
+  static int JumpOffset(Instr instr);
+  static int AuipcOffset(Instr instr);
+  static int JalrOffset(Instr instr);
+  static int LoadOffset(Instr instr);
+
+  // Check if an instruction is a branch of some kind.
+  static bool IsBranch(Instr instr);
+  static bool IsNop(Instr instr);
+  static bool IsJump(Instr instr);
+  static bool IsJal(Instr instr);
+  static bool IsJalr(Instr instr);
+  static bool IsLui(Instr instr);
+  static bool IsAuipc(Instr instr);
+  static bool IsAddi(Instr instr);
+  static bool IsOri(Instr instr);
+  static bool IsSlli(Instr instr);
+  static bool IsLw(Instr instr);
+
+  inline int32_t branch_offset(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset13);
+  }
+  inline int32_t jump_offset(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset21);
+  }
+
+  // Branches
+  void beq(Register rs1, Register rs2, Label* L) {
+    beq(rs1, rs2, branch_offset(L));
+  }
+  void bne(Register rs1, Register rs2, Label* L) {
+    bne(rs1, rs2, branch_offset(L));
+  }
+  void blt(Register rs1, Register rs2, Label* L) {
+    blt(rs1, rs2, branch_offset(L));
+  }
+  void bge(Register rs1, Register rs2, Label* L) {
+    bge(rs1, rs2, branch_offset(L));
+  }
+  void bltu(Register rs1, Register rs2, Label* L) {
+    bltu(rs1, rs2, branch_offset(L));
+  }
+  void bgeu(Register rs1, Register rs2, Label* L) {
+    bgeu(rs1, rs2, branch_offset(L));
+  }
+
+  void beqz(Register rs, int16_t imm13) { beq(rs, zero_reg, imm13); }
+  void beqz(Register rs1, Label* L) { beqz(rs1, branch_offset(L)); }
+  void bnez(Register rs, int16_t imm13) { bne(rs, zero_reg, imm13); }
+  void bnez(Register rs1, Label* L) { bnez(rs1, branch_offset(L)); }
+  void blez(Register rs, int16_t imm13) { bge(zero_reg, rs, imm13); }
+  void blez(Register rs1, Label* L) { blez(rs1, branch_offset(L)); }
+  void bgez(Register rs, int16_t imm13) { bge(rs, zero_reg, imm13); }
+  void bgez(Register rs1, Label* L) { bgez(rs1, branch_offset(L)); }
+  void bltz(Register rs, int16_t imm13) { blt(rs, zero_reg, imm13); }
+  void bltz(Register rs1, Label* L) { bltz(rs1, branch_offset(L)); }
+  void bgtz(Register rs, int16_t imm13) { blt(zero_reg, rs, imm13); }
+
+  void bgtz(Register rs1, Label* L) { bgtz(rs1, branch_offset(L)); }
+  void bgt(Register rs1, Register rs2, int16_t imm13) { blt(rs2, rs1, imm13); }
+  void bgt(Register rs1, Register rs2, Label* L) {
+    bgt(rs1, rs2, branch_offset(L));
+  }
+  void ble(Register rs1, Register rs2, int16_t imm13) { bge(rs2, rs1, imm13); }
+  void ble(Register rs1, Register rs2, Label* L) {
+    ble(rs1, rs2, branch_offset(L));
+  }
+  void bgtu(Register rs1, Register rs2, int16_t imm13) {
+    bltu(rs2, rs1, imm13);
+  }
+  void bgtu(Register rs1, Register rs2, Label* L) {
+    bgtu(rs1, rs2, branch_offset(L));
+  }
+  void bleu(Register rs1, Register rs2, int16_t imm13) {
+    bgeu(rs2, rs1, imm13);
+  }
+  void bleu(Register rs1, Register rs2, Label* L) {
+    bleu(rs1, rs2, branch_offset(L));
+  }
+
+  void j(int32_t imm21) { jal(zero_reg, imm21); }
+  void j(Label* L) { j(jump_offset(L)); }
+  void b(Label* L) { j(L); }
+  void jal(int32_t imm21) { jal(ra, imm21); }
+  void jal(Label* L) { jal(jump_offset(L)); }
+  void jr(Register rs) { jalr(zero_reg, rs, 0); }
+  void jr(Register rs, int32_t imm12) { jalr(zero_reg, rs, imm12); }
+  void jalr(Register rs, int32_t imm12) { jalr(ra, rs, imm12); }
+  void jalr(Register rs) { jalr(ra, rs, 0); }
+  void ret() { jalr(zero_reg, ra, 0); }
+  void call(int32_t offset) {
+    auipc(ra, (offset >> 12) + ((offset & 0x800) >> 11));
+    jalr(ra, ra, offset << 20 >> 20);
+  }
+
+  void mv(Register rd, Register rs) { addi(rd, rs, 0); }
+  void not_(Register rd, Register rs) { xori(rd, rs, -1); }
+  void neg(Register rd, Register rs) { sub(rd, zero_reg, rs); }
+  void seqz(Register rd, Register rs) { sltiu(rd, rs, 1); }
+  void snez(Register rd, Register rs) { sltu(rd, zero_reg, rs); }
+  void sltz(Register rd, Register rs) { slt(rd, rs, zero_reg); }
+  void sgtz(Register rd, Register rs) { slt(rd, zero_reg, rs); }
+
+#if V8_TARGET_ARCH_RISCV64
+  void lwu(Register rd, Register rs1, int16_t imm12);
+  void ld(Register rd, Register rs1, int16_t imm12);
+  void sd(Register source, Register base, int16_t imm12);
+  void addiw(Register rd, Register rs1, int16_t imm12);
+  void slliw(Register rd, Register rs1, uint8_t shamt);
+  void srliw(Register rd, Register rs1, uint8_t shamt);
+  void sraiw(Register rd, Register rs1, uint8_t shamt);
+  void addw(Register rd, Register rs1, Register rs2);
+  void subw(Register rd, Register rs1, Register rs2);
+  void sllw(Register rd, Register rs1, Register rs2);
+  void srlw(Register rd, Register rs1, Register rs2);
+  void sraw(Register rd, Register rs1, Register rs2);
+  void negw(Register rd, Register rs) { subw(rd, zero_reg, rs); }
+  void sext_w(Register rd, Register rs) { addiw(rd, rs, 0); }
+
+  static bool IsAddiw(Instr instr);
+  static bool IsLd(Instr instr);
+#endif
+};
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_BASE_RISCV_I_H_
diff --git a/src/codegen/riscv/constant-riscv-a.h b/src/codegen/riscv/constant-riscv-a.h
new file mode 100644
index 00000000000..afd335ce592
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-a.h
@@ -0,0 +1,43 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVA : uint32_t {
+  // RV32A Standard Extension
+  RO_LR_W = AMO | (0b010 << kFunct3Shift) | (0b00010 << kFunct5Shift),
+  RO_SC_W = AMO | (0b010 << kFunct3Shift) | (0b00011 << kFunct5Shift),
+  RO_AMOSWAP_W = AMO | (0b010 << kFunct3Shift) | (0b00001 << kFunct5Shift),
+  RO_AMOADD_W = AMO | (0b010 << kFunct3Shift) | (0b00000 << kFunct5Shift),
+  RO_AMOXOR_W = AMO | (0b010 << kFunct3Shift) | (0b00100 << kFunct5Shift),
+  RO_AMOAND_W = AMO | (0b010 << kFunct3Shift) | (0b01100 << kFunct5Shift),
+  RO_AMOOR_W = AMO | (0b010 << kFunct3Shift) | (0b01000 << kFunct5Shift),
+  RO_AMOMIN_W = AMO | (0b010 << kFunct3Shift) | (0b10000 << kFunct5Shift),
+  RO_AMOMAX_W = AMO | (0b010 << kFunct3Shift) | (0b10100 << kFunct5Shift),
+  RO_AMOMINU_W = AMO | (0b010 << kFunct3Shift) | (0b11000 << kFunct5Shift),
+  RO_AMOMAXU_W = AMO | (0b010 << kFunct3Shift) | (0b11100 << kFunct5Shift),
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64A Standard Extension (in addition to RV32A)
+  RO_LR_D = AMO | (0b011 << kFunct3Shift) | (0b00010 << kFunct5Shift),
+  RO_SC_D = AMO | (0b011 << kFunct3Shift) | (0b00011 << kFunct5Shift),
+  RO_AMOSWAP_D = AMO | (0b011 << kFunct3Shift) | (0b00001 << kFunct5Shift),
+  RO_AMOADD_D = AMO | (0b011 << kFunct3Shift) | (0b00000 << kFunct5Shift),
+  RO_AMOXOR_D = AMO | (0b011 << kFunct3Shift) | (0b00100 << kFunct5Shift),
+  RO_AMOAND_D = AMO | (0b011 << kFunct3Shift) | (0b01100 << kFunct5Shift),
+  RO_AMOOR_D = AMO | (0b011 << kFunct3Shift) | (0b01000 << kFunct5Shift),
+  RO_AMOMIN_D = AMO | (0b011 << kFunct3Shift) | (0b10000 << kFunct5Shift),
+  RO_AMOMAX_D = AMO | (0b011 << kFunct3Shift) | (0b10100 << kFunct5Shift),
+  RO_AMOMINU_D = AMO | (0b011 << kFunct3Shift) | (0b11000 << kFunct5Shift),
+  RO_AMOMAXU_D = AMO | (0b011 << kFunct3Shift) | (0b11100 << kFunct5Shift),
+#endif  // V8_TARGET_ARCH_RISCV64
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_A_H_
diff --git a/src/codegen/riscv/constant-riscv-c.h b/src/codegen/riscv/constant-riscv-c.h
new file mode 100644
index 00000000000..2f8a5047808
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-c.h
@@ -0,0 +1,62 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVC : uint32_t {
+
+  RO_C_ADDI4SPN = C0 | (0b000 << kRvcFunct3Shift),
+  RO_C_ADDI16SP = C1 | (0b011 << kRvcFunct3Shift),
+  RO_C_LW = C0 | (0b010 << kRvcFunct3Shift),
+  RO_C_SW = C0 | (0b110 << kRvcFunct3Shift),
+  RO_C_NOP_ADDI = C1 | (0b000 << kRvcFunct3Shift),
+  RO_C_LI = C1 | (0b010 << kRvcFunct3Shift),
+  RO_C_SUB = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift),
+  RO_C_XOR = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift),
+  RO_C_OR = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_2 << kRvcFunct2Shift),
+  RO_C_AND = C1 | (0b100011 << kRvcFunct6Shift) | (FUNCT2_3 << kRvcFunct2Shift),
+  RO_C_LUI_ADD = C1 | (0b011 << kRvcFunct3Shift),
+  RO_C_MISC_ALU = C1 | (0b100 << kRvcFunct3Shift),
+  RO_C_J = C1 | (0b101 << kRvcFunct3Shift),
+  RO_C_BEQZ = C1 | (0b110 << kRvcFunct3Shift),
+  RO_C_BNEZ = C1 | (0b111 << kRvcFunct3Shift),
+  RO_C_SLLI = C2 | (0b000 << kRvcFunct3Shift),
+  RO_C_LWSP = C2 | (0b010 << kRvcFunct3Shift),
+  RO_C_JR_MV_ADD = C2 | (0b100 << kRvcFunct3Shift),
+  RO_C_JR = C2 | (0b1000 << kRvcFunct4Shift),
+  RO_C_MV = C2 | (0b1000 << kRvcFunct4Shift),
+  RO_C_EBREAK = C2 | (0b1001 << kRvcFunct4Shift),
+  RO_C_JALR = C2 | (0b1001 << kRvcFunct4Shift),
+  RO_C_ADD = C2 | (0b1001 << kRvcFunct4Shift),
+  RO_C_SWSP = C2 | (0b110 << kRvcFunct3Shift),
+
+  RO_C_FSD = C0 | (0b101 << kRvcFunct3Shift),
+  RO_C_FLD = C0 | (0b001 << kRvcFunct3Shift),
+  RO_C_FLDSP = C2 | (0b001 << kRvcFunct3Shift),
+  RO_C_FSDSP = C2 | (0b101 << kRvcFunct3Shift),
+#ifdef V8_TARGET_ARCH_RISCV64
+  RO_C_LD = C0 | (0b011 << kRvcFunct3Shift),
+  RO_C_SD = C0 | (0b111 << kRvcFunct3Shift),
+  RO_C_LDSP = C2 | (0b011 << kRvcFunct3Shift),
+  RO_C_SDSP = C2 | (0b111 << kRvcFunct3Shift),
+  RO_C_ADDIW = C1 | (0b001 << kRvcFunct3Shift),
+  RO_C_SUBW =
+      C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_0 << kRvcFunct2Shift),
+  RO_C_ADDW =
+      C1 | (0b100111 << kRvcFunct6Shift) | (FUNCT2_1 << kRvcFunct2Shift),
+#endif
+#ifdef V8_TARGET_ARCH_RISCV32
+  RO_C_FLWSP = C2 | (0b011 << kRvcFunct3Shift),
+  RO_C_FSWSP = C2 | (0b111 << kRvcFunct3Shift),
+  RO_C_FLW = C0 | (0b011 << kRvcFunct3Shift),
+  RO_C_FSW = C0 | (0b111 << kRvcFunct3Shift),
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_C_H_
diff --git a/src/codegen/riscv/constant-riscv-d.h b/src/codegen/riscv/constant-riscv-d.h
new file mode 100644
index 00000000000..3fd0b251bdb
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-d.h
@@ -0,0 +1,55 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVD : uint32_t {
+  // RV32D Standard Extension
+  RO_FLD = LOAD_FP | (0b011 << kFunct3Shift),
+  RO_FSD = STORE_FP | (0b011 << kFunct3Shift),
+  RO_FMADD_D = MADD | (0b01 << kFunct2Shift),
+  RO_FMSUB_D = MSUB | (0b01 << kFunct2Shift),
+  RO_FNMSUB_D = NMSUB | (0b01 << kFunct2Shift),
+  RO_FNMADD_D = NMADD | (0b01 << kFunct2Shift),
+  RO_FADD_D = OP_FP | (0b0000001 << kFunct7Shift),
+  RO_FSUB_D = OP_FP | (0b0000101 << kFunct7Shift),
+  RO_FMUL_D = OP_FP | (0b0001001 << kFunct7Shift),
+  RO_FDIV_D = OP_FP | (0b0001101 << kFunct7Shift),
+  RO_FSQRT_D = OP_FP | (0b0101101 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FSGNJ_D = OP_FP | (0b000 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
+  RO_FSGNJN_D = OP_FP | (0b001 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
+  RO_FSQNJX_D = OP_FP | (0b010 << kFunct3Shift) | (0b0010001 << kFunct7Shift),
+  RO_FMIN_D = OP_FP | (0b000 << kFunct3Shift) | (0b0010101 << kFunct7Shift),
+  RO_FMAX_D = OP_FP | (0b001 << kFunct3Shift) | (0b0010101 << kFunct7Shift),
+  RO_FCVT_S_D = OP_FP | (0b0100000 << kFunct7Shift) | (0b00001 << kRs2Shift),
+  RO_FCVT_D_S = OP_FP | (0b0100001 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FEQ_D = OP_FP | (0b010 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
+  RO_FLT_D = OP_FP | (0b001 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
+  RO_FLE_D = OP_FP | (0b000 << kFunct3Shift) | (0b1010001 << kFunct7Shift),
+  RO_FCLASS_D = OP_FP | (0b001 << kFunct3Shift) | (0b1110001 << kFunct7Shift) |
+                (0b00000 << kRs2Shift),
+  RO_FCVT_W_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FCVT_WU_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00001 << kRs2Shift),
+  RO_FCVT_D_W = OP_FP | (0b1101001 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FCVT_D_WU = OP_FP | (0b1101001 << kFunct7Shift) | (0b00001 << kRs2Shift),
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64D Standard Extension (in addition to RV32D)
+  RO_FCVT_L_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00010 << kRs2Shift),
+  RO_FCVT_LU_D = OP_FP | (0b1100001 << kFunct7Shift) | (0b00011 << kRs2Shift),
+  RO_FMV_X_D = OP_FP | (0b000 << kFunct3Shift) | (0b1110001 << kFunct7Shift) |
+               (0b00000 << kRs2Shift),
+  RO_FCVT_D_L = OP_FP | (0b1101001 << kFunct7Shift) | (0b00010 << kRs2Shift),
+  RO_FCVT_D_LU = OP_FP | (0b1101001 << kFunct7Shift) | (0b00011 << kRs2Shift),
+  RO_FMV_D_X = OP_FP | (0b000 << kFunct3Shift) | (0b1111001 << kFunct7Shift) |
+               (0b00000 << kRs2Shift),
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_D_H_
diff --git a/src/codegen/riscv/constant-riscv-f.h b/src/codegen/riscv/constant-riscv-f.h
new file mode 100644
index 00000000000..fc742e7d574
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-f.h
@@ -0,0 +1,51 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVF : uint32_t {
+  // RV32F Standard Extension
+  RO_FLW = LOAD_FP | (0b010 << kFunct3Shift),
+  RO_FSW = STORE_FP | (0b010 << kFunct3Shift),
+  RO_FMADD_S = MADD | (0b00 << kFunct2Shift),
+  RO_FMSUB_S = MSUB | (0b00 << kFunct2Shift),
+  RO_FNMSUB_S = NMSUB | (0b00 << kFunct2Shift),
+  RO_FNMADD_S = NMADD | (0b00 << kFunct2Shift),
+  RO_FADD_S = OP_FP | (0b0000000 << kFunct7Shift),
+  RO_FSUB_S = OP_FP | (0b0000100 << kFunct7Shift),
+  RO_FMUL_S = OP_FP | (0b0001000 << kFunct7Shift),
+  RO_FDIV_S = OP_FP | (0b0001100 << kFunct7Shift),
+  RO_FSQRT_S = OP_FP | (0b0101100 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FSGNJ_S = OP_FP | (0b000 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
+  RO_FSGNJN_S = OP_FP | (0b001 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
+  RO_FSQNJX_S = OP_FP | (0b010 << kFunct3Shift) | (0b0010000 << kFunct7Shift),
+  RO_FMIN_S = OP_FP | (0b000 << kFunct3Shift) | (0b0010100 << kFunct7Shift),
+  RO_FMAX_S = OP_FP | (0b001 << kFunct3Shift) | (0b0010100 << kFunct7Shift),
+  RO_FCVT_W_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FCVT_WU_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00001 << kRs2Shift),
+  RO_FMV = OP_FP | (0b1110000 << kFunct7Shift) | (0b000 << kFunct3Shift) |
+           (0b00000 << kRs2Shift),
+  RO_FEQ_S = OP_FP | (0b010 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
+  RO_FLT_S = OP_FP | (0b001 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
+  RO_FLE_S = OP_FP | (0b000 << kFunct3Shift) | (0b1010000 << kFunct7Shift),
+  RO_FCLASS_S = OP_FP | (0b001 << kFunct3Shift) | (0b1110000 << kFunct7Shift),
+  RO_FCVT_S_W = OP_FP | (0b1101000 << kFunct7Shift) | (0b00000 << kRs2Shift),
+  RO_FCVT_S_WU = OP_FP | (0b1101000 << kFunct7Shift) | (0b00001 << kRs2Shift),
+  RO_FMV_W_X = OP_FP | (0b000 << kFunct3Shift) | (0b1111000 << kFunct7Shift),
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64F Standard Extension (in addition to RV32F)
+  RO_FCVT_L_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00010 << kRs2Shift),
+  RO_FCVT_LU_S = OP_FP | (0b1100000 << kFunct7Shift) | (0b00011 << kRs2Shift),
+  RO_FCVT_S_L = OP_FP | (0b1101000 << kFunct7Shift) | (0b00010 << kRs2Shift),
+  RO_FCVT_S_LU = OP_FP | (0b1101000 << kFunct7Shift) | (0b00011 << kRs2Shift),
+#endif  // V8_TARGET_ARCH_RISCV64
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_F_H_
diff --git a/src/codegen/riscv/constant-riscv-i.h b/src/codegen/riscv/constant-riscv-i.h
new file mode 100644
index 00000000000..75c6c44565b
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-i.h
@@ -0,0 +1,73 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCV32I : uint32_t {
+  // Note use RO (RiscV Opcode) prefix
+  // RV32I Base Instruction Set
+  RO_LUI = LUI,
+  RO_AUIPC = AUIPC,
+  RO_JAL = JAL,
+  RO_JALR = JALR | (0b000 << kFunct3Shift),
+  RO_BEQ = BRANCH | (0b000 << kFunct3Shift),
+  RO_BNE = BRANCH | (0b001 << kFunct3Shift),
+  RO_BLT = BRANCH | (0b100 << kFunct3Shift),
+  RO_BGE = BRANCH | (0b101 << kFunct3Shift),
+  RO_BLTU = BRANCH | (0b110 << kFunct3Shift),
+  RO_BGEU = BRANCH | (0b111 << kFunct3Shift),
+  RO_LB = LOAD | (0b000 << kFunct3Shift),
+  RO_LH = LOAD | (0b001 << kFunct3Shift),
+  RO_LW = LOAD | (0b010 << kFunct3Shift),
+  RO_LBU = LOAD | (0b100 << kFunct3Shift),
+  RO_LHU = LOAD | (0b101 << kFunct3Shift),
+  RO_SB = STORE | (0b000 << kFunct3Shift),
+  RO_SH = STORE | (0b001 << kFunct3Shift),
+  RO_SW = STORE | (0b010 << kFunct3Shift),
+  RO_ADDI = OP_IMM | (0b000 << kFunct3Shift),
+  RO_SLTI = OP_IMM | (0b010 << kFunct3Shift),
+  RO_SLTIU = OP_IMM | (0b011 << kFunct3Shift),
+  RO_XORI = OP_IMM | (0b100 << kFunct3Shift),
+  RO_ORI = OP_IMM | (0b110 << kFunct3Shift),
+  RO_ANDI = OP_IMM | (0b111 << kFunct3Shift),
+  RO_SLLI = OP_IMM | (0b001 << kFunct3Shift),
+  RO_SRLI = OP_IMM | (0b101 << kFunct3Shift),
+  // RO_SRAI = OP_IMM | (0b101 << kFunct3Shift), // Same as SRLI, use func7
+  RO_ADD = OP | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SUB = OP | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
+  RO_SLL = OP | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SLT = OP | (0b010 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SLTU = OP | (0b011 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_XOR = OP | (0b100 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SRL = OP | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SRA = OP | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
+  RO_OR = OP | (0b110 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_AND = OP | (0b111 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_FENCE = MISC_MEM | (0b000 << kFunct3Shift),
+  RO_ECALL = SYSTEM | (0b000 << kFunct3Shift),
+// RO_EBREAK = SYSTEM | (0b000 << kFunct3Shift), // Same as ECALL, use imm12
+
+#if V8_TARGET_ARCH_RISCV64
+  // RV64I Base Instruction Set (in addition to RV32I)
+  RO_LWU = LOAD | (0b110 << kFunct3Shift),
+  RO_LD = LOAD | (0b011 << kFunct3Shift),
+  RO_SD = STORE | (0b011 << kFunct3Shift),
+  RO_ADDIW = OP_IMM_32 | (0b000 << kFunct3Shift),
+  RO_SLLIW = OP_IMM_32 | (0b001 << kFunct3Shift),
+  RO_SRLIW = OP_IMM_32 | (0b101 << kFunct3Shift),
+  // RO_SRAIW = OP_IMM_32 | (0b101 << kFunct3Shift), // Same as SRLIW, use func7
+  RO_ADDW = OP_32 | (0b000 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SUBW = OP_32 | (0b000 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
+  RO_SLLW = OP_32 | (0b001 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SRLW = OP_32 | (0b101 << kFunct3Shift) | (0b0000000 << kFunct7Shift),
+  RO_SRAW = OP_32 | (0b101 << kFunct3Shift) | (0b0100000 << kFunct7Shift),
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_I_H_
diff --git a/src/codegen/riscv/constant-riscv-m.h b/src/codegen/riscv/constant-riscv-m.h
new file mode 100644
index 00000000000..2ad1ffd1b57
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-m.h
@@ -0,0 +1,34 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVM : uint32_t {
+  // RV32M Standard Extension
+  RO_MUL = OP | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_MULH = OP | (0b001 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_MULHSU = OP | (0b010 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_MULHU = OP | (0b011 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_DIV = OP | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_DIVU = OP | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_REM = OP | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_REMU = OP | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64M Standard Extension (in addition to RV32M)
+  RO_MULW = OP_32 | (0b000 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_DIVW = OP_32 | (0b100 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_DIVUW = OP_32 | (0b101 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_REMW = OP_32 | (0b110 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+  RO_REMUW = OP_32 | (0b111 << kFunct3Shift) | (0b0000001 << kFunct7Shift),
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_M_H_
diff --git a/src/codegen/riscv/constant-riscv-v.h b/src/codegen/riscv/constant-riscv-v.h
new file mode 100644
index 00000000000..30ff0c1a244
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-v.h
@@ -0,0 +1,493 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+
+enum OpcodeRISCVV : uint32_t {
+  // RVV Extension
+  OP_IVV = OP_V | (0b000 << kFunct3Shift),
+  OP_FVV = OP_V | (0b001 << kFunct3Shift),
+  OP_MVV = OP_V | (0b010 << kFunct3Shift),
+  OP_IVI = OP_V | (0b011 << kFunct3Shift),
+  OP_IVX = OP_V | (0b100 << kFunct3Shift),
+  OP_FVF = OP_V | (0b101 << kFunct3Shift),
+  OP_MVX = OP_V | (0b110 << kFunct3Shift),
+
+  RO_V_VSETVLI = OP_V | (0b111 << kFunct3Shift) | 0b0 << 31,
+  RO_V_VSETIVLI = OP_V | (0b111 << kFunct3Shift) | 0b11 << 30,
+  RO_V_VSETVL = OP_V | (0b111 << kFunct3Shift) | 0b1 << 31,
+
+  // RVV LOAD/STORE
+  RO_V_VL = LOAD_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  RO_V_VLS = LOAD_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  RO_V_VLX = LOAD_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift),
+
+  RO_V_VS = STORE_FP | (0b00 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  RO_V_VSS = STORE_FP | (0b10 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  RO_V_VSX = STORE_FP | (0b11 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  RO_V_VSU = STORE_FP | (0b01 << kRvvMopShift) | (0b000 << kRvvNfShift),
+  // THE kFunct6Shift is mop
+  RO_V_VLSEG2 = LOAD_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VLSEG3 = LOAD_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VLSEG4 = LOAD_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VLSEG5 = LOAD_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VLSEG6 = LOAD_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VLSEG7 = LOAD_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VLSEG8 = LOAD_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  RO_V_VSSEG2 = STORE_FP | (0b00 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VSSEG3 = STORE_FP | (0b00 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VSSEG4 = STORE_FP | (0b00 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VSSEG5 = STORE_FP | (0b00 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VSSEG6 = STORE_FP | (0b00 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VSSEG7 = STORE_FP | (0b00 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VSSEG8 = STORE_FP | (0b00 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  RO_V_VLSSEG2 = LOAD_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VLSSEG3 = LOAD_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VLSSEG4 = LOAD_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VLSSEG5 = LOAD_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VLSSEG6 = LOAD_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VLSSEG7 = LOAD_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VLSSEG8 = LOAD_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  RO_V_VSSSEG2 = STORE_FP | (0b10 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VSSSEG3 = STORE_FP | (0b10 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VSSSEG4 = STORE_FP | (0b10 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VSSSEG5 = STORE_FP | (0b10 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VSSSEG6 = STORE_FP | (0b10 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VSSSEG7 = STORE_FP | (0b10 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VSSSEG8 = STORE_FP | (0b10 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  RO_V_VLXSEG2 = LOAD_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VLXSEG3 = LOAD_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VLXSEG4 = LOAD_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VLXSEG5 = LOAD_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VLXSEG6 = LOAD_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VLXSEG7 = LOAD_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VLXSEG8 = LOAD_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  RO_V_VSXSEG2 = STORE_FP | (0b11 << kRvvMopShift) | (0b001 << kRvvNfShift),
+  RO_V_VSXSEG3 = STORE_FP | (0b11 << kRvvMopShift) | (0b010 << kRvvNfShift),
+  RO_V_VSXSEG4 = STORE_FP | (0b11 << kRvvMopShift) | (0b011 << kRvvNfShift),
+  RO_V_VSXSEG5 = STORE_FP | (0b11 << kRvvMopShift) | (0b100 << kRvvNfShift),
+  RO_V_VSXSEG6 = STORE_FP | (0b11 << kRvvMopShift) | (0b101 << kRvvNfShift),
+  RO_V_VSXSEG7 = STORE_FP | (0b11 << kRvvMopShift) | (0b110 << kRvvNfShift),
+  RO_V_VSXSEG8 = STORE_FP | (0b11 << kRvvMopShift) | (0b111 << kRvvNfShift),
+
+  // RVV Vector Arithmetic Instruction
+  VADD_FUNCT6 = 0b000000,
+  RO_V_VADD_VI = OP_IVI | (VADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VADD_VV = OP_IVV | (VADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VADD_VX = OP_IVX | (VADD_FUNCT6 << kRvvFunct6Shift),
+
+  VSUB_FUNCT6 = 0b000010,
+  RO_V_VSUB_VX = OP_IVX | (VSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSUB_VV = OP_IVV | (VSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VDIVU_FUNCT6 = 0b100000,
+  RO_V_VDIVU_VX = OP_MVX | (VDIVU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VDIVU_VV = OP_MVV | (VDIVU_FUNCT6 << kRvvFunct6Shift),
+
+  VDIV_FUNCT6 = 0b100001,
+  RO_V_VDIV_VX = OP_MVX | (VDIV_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VDIV_VV = OP_MVV | (VDIV_FUNCT6 << kRvvFunct6Shift),
+
+  VREMU_FUNCT6 = 0b100010,
+  RO_V_VREMU_VX = OP_MVX | (VREMU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VREMU_VV = OP_MVV | (VREMU_FUNCT6 << kRvvFunct6Shift),
+
+  VREM_FUNCT6 = 0b100011,
+  RO_V_VREM_VX = OP_MVX | (VREM_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VREM_VV = OP_MVV | (VREM_FUNCT6 << kRvvFunct6Shift),
+
+  VMULHU_FUNCT6 = 0b100100,
+  RO_V_VMULHU_VX = OP_MVX | (VMULHU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMULHU_VV = OP_MVV | (VMULHU_FUNCT6 << kRvvFunct6Shift),
+
+  VMUL_FUNCT6 = 0b100101,
+  RO_V_VMUL_VX = OP_MVX | (VMUL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMUL_VV = OP_MVV | (VMUL_FUNCT6 << kRvvFunct6Shift),
+
+  VWMUL_FUNCT6 = 0b111011,
+  RO_V_VWMUL_VX = OP_MVX | (VWMUL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VWMUL_VV = OP_MVV | (VWMUL_FUNCT6 << kRvvFunct6Shift),
+
+  VWMULU_FUNCT6 = 0b111000,
+  RO_V_VWMULU_VX = OP_MVX | (VWMULU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VWMULU_VV = OP_MVV | (VWMULU_FUNCT6 << kRvvFunct6Shift),
+
+  VMULHSU_FUNCT6 = 0b100110,
+  RO_V_VMULHSU_VX = OP_MVX | (VMULHSU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMULHSU_VV = OP_MVV | (VMULHSU_FUNCT6 << kRvvFunct6Shift),
+
+  VMULH_FUNCT6 = 0b100111,
+  RO_V_VMULH_VX = OP_MVX | (VMULH_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMULH_VV = OP_MVV | (VMULH_FUNCT6 << kRvvFunct6Shift),
+
+  VWADD_FUNCT6 = 0b110001,
+  RO_V_VWADD_VV = OP_MVV | (VWADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VWADD_VX = OP_MVX | (VWADD_FUNCT6 << kRvvFunct6Shift),
+
+  VWADDU_FUNCT6 = 0b110000,
+  RO_V_VWADDU_VV = OP_MVV | (VWADDU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VWADDU_VX = OP_MVX | (VWADDU_FUNCT6 << kRvvFunct6Shift),
+
+  VWADDUW_FUNCT6 = 0b110101,
+  RO_V_VWADDUW_VX = OP_MVX | (VWADDUW_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VWADDUW_VV = OP_MVV | (VWADDUW_FUNCT6 << kRvvFunct6Shift),
+
+  VCOMPRESS_FUNCT6 = 0b010111,
+  RO_V_VCOMPRESS_VV = OP_MVV | (VCOMPRESS_FUNCT6 << kRvvFunct6Shift),
+
+  VSADDU_FUNCT6 = 0b100000,
+  RO_V_VSADDU_VI = OP_IVI | (VSADDU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSADDU_VV = OP_IVV | (VSADDU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSADDU_VX = OP_IVX | (VSADDU_FUNCT6 << kRvvFunct6Shift),
+
+  VSADD_FUNCT6 = 0b100001,
+  RO_V_VSADD_VI = OP_IVI | (VSADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSADD_VV = OP_IVV | (VSADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSADD_VX = OP_IVX | (VSADD_FUNCT6 << kRvvFunct6Shift),
+
+  VSSUB_FUNCT6 = 0b100011,
+  RO_V_VSSUB_VV = OP_IVV | (VSSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSSUB_VX = OP_IVX | (VSSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VSSUBU_FUNCT6 = 0b100010,
+  RO_V_VSSUBU_VV = OP_IVV | (VSSUBU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSSUBU_VX = OP_IVX | (VSSUBU_FUNCT6 << kRvvFunct6Shift),
+
+  VRSUB_FUNCT6 = 0b000011,
+  RO_V_VRSUB_VX = OP_IVX | (VRSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VRSUB_VI = OP_IVI | (VRSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VMINU_FUNCT6 = 0b000100,
+  RO_V_VMINU_VX = OP_IVX | (VMINU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMINU_VV = OP_IVV | (VMINU_FUNCT6 << kRvvFunct6Shift),
+
+  VMIN_FUNCT6 = 0b000101,
+  RO_V_VMIN_VX = OP_IVX | (VMIN_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMIN_VV = OP_IVV | (VMIN_FUNCT6 << kRvvFunct6Shift),
+
+  VMAXU_FUNCT6 = 0b000110,
+  RO_V_VMAXU_VX = OP_IVX | (VMAXU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMAXU_VV = OP_IVV | (VMAXU_FUNCT6 << kRvvFunct6Shift),
+
+  VMAX_FUNCT6 = 0b000111,
+  RO_V_VMAX_VX = OP_IVX | (VMAX_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMAX_VV = OP_IVV | (VMAX_FUNCT6 << kRvvFunct6Shift),
+
+  VAND_FUNCT6 = 0b001001,
+  RO_V_VAND_VI = OP_IVI | (VAND_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VAND_VV = OP_IVV | (VAND_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VAND_VX = OP_IVX | (VAND_FUNCT6 << kRvvFunct6Shift),
+
+  VOR_FUNCT6 = 0b001010,
+  RO_V_VOR_VI = OP_IVI | (VOR_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VOR_VV = OP_IVV | (VOR_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VOR_VX = OP_IVX | (VOR_FUNCT6 << kRvvFunct6Shift),
+
+  VXOR_FUNCT6 = 0b001011,
+  RO_V_VXOR_VI = OP_IVI | (VXOR_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VXOR_VV = OP_IVV | (VXOR_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VXOR_VX = OP_IVX | (VXOR_FUNCT6 << kRvvFunct6Shift),
+
+  VRGATHER_FUNCT6 = 0b001100,
+  RO_V_VRGATHER_VI = OP_IVI | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VRGATHER_VV = OP_IVV | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VRGATHER_VX = OP_IVX | (VRGATHER_FUNCT6 << kRvvFunct6Shift),
+
+  VMV_FUNCT6 = 0b010111,
+  RO_V_VMV_VI = OP_IVI | (VMV_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMV_VV = OP_IVV | (VMV_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMV_VX = OP_IVX | (VMV_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMV_VF = OP_FVF | (VMV_FUNCT6 << kRvvFunct6Shift),
+
+  RO_V_VMERGE_VI = RO_V_VMV_VI,
+  RO_V_VMERGE_VV = RO_V_VMV_VV,
+  RO_V_VMERGE_VX = RO_V_VMV_VX,
+
+  VMSEQ_FUNCT6 = 0b011000,
+  RO_V_VMSEQ_VI = OP_IVI | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSEQ_VV = OP_IVV | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSEQ_VX = OP_IVX | (VMSEQ_FUNCT6 << kRvvFunct6Shift),
+
+  VMSNE_FUNCT6 = 0b011001,
+  RO_V_VMSNE_VI = OP_IVI | (VMSNE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSNE_VV = OP_IVV | (VMSNE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSNE_VX = OP_IVX | (VMSNE_FUNCT6 << kRvvFunct6Shift),
+
+  VMSLTU_FUNCT6 = 0b011010,
+  RO_V_VMSLTU_VV = OP_IVV | (VMSLTU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLTU_VX = OP_IVX | (VMSLTU_FUNCT6 << kRvvFunct6Shift),
+
+  VMSLT_FUNCT6 = 0b011011,
+  RO_V_VMSLT_VV = OP_IVV | (VMSLT_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLT_VX = OP_IVX | (VMSLT_FUNCT6 << kRvvFunct6Shift),
+
+  VMSLE_FUNCT6 = 0b011101,
+  RO_V_VMSLE_VI = OP_IVI | (VMSLE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLE_VV = OP_IVV | (VMSLE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLE_VX = OP_IVX | (VMSLE_FUNCT6 << kRvvFunct6Shift),
+
+  VMSLEU_FUNCT6 = 0b011100,
+  RO_V_VMSLEU_VI = OP_IVI | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLEU_VV = OP_IVV | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSLEU_VX = OP_IVX | (VMSLEU_FUNCT6 << kRvvFunct6Shift),
+
+  VMSGTU_FUNCT6 = 0b011110,
+  RO_V_VMSGTU_VI = OP_IVI | (VMSGTU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSGTU_VX = OP_IVX | (VMSGTU_FUNCT6 << kRvvFunct6Shift),
+
+  VMSGT_FUNCT6 = 0b011111,
+  RO_V_VMSGT_VI = OP_IVI | (VMSGT_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMSGT_VX = OP_IVX | (VMSGT_FUNCT6 << kRvvFunct6Shift),
+
+  VSLIDEUP_FUNCT6 = 0b001110,
+  RO_V_VSLIDEUP_VI = OP_IVI | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSLIDEUP_VX = OP_IVX | (VSLIDEUP_FUNCT6 << kRvvFunct6Shift),
+
+  VSLIDEDOWN_FUNCT6 = 0b001111,
+  RO_V_VSLIDEDOWN_VI = OP_IVI | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSLIDEDOWN_VX = OP_IVX | (VSLIDEDOWN_FUNCT6 << kRvvFunct6Shift),
+
+  VSRL_FUNCT6 = 0b101000,
+  RO_V_VSRL_VI = OP_IVI | (VSRL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSRL_VV = OP_IVV | (VSRL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSRL_VX = OP_IVX | (VSRL_FUNCT6 << kRvvFunct6Shift),
+
+  VSRA_FUNCT6 = 0b101001,
+  RO_V_VSRA_VI = OP_IVI | (VSRA_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSRA_VV = OP_IVV | (VSRA_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSRA_VX = OP_IVX | (VSRA_FUNCT6 << kRvvFunct6Shift),
+
+  VSLL_FUNCT6 = 0b100101,
+  RO_V_VSLL_VI = OP_IVI | (VSLL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSLL_VV = OP_IVV | (VSLL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSLL_VX = OP_IVX | (VSLL_FUNCT6 << kRvvFunct6Shift),
+
+  VSMUL_FUNCT6 = 0b100111,
+  RO_V_VSMUL_VV = OP_IVV | (VSMUL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VSMUL_VX = OP_IVX | (VSMUL_FUNCT6 << kRvvFunct6Shift),
+
+  VADC_FUNCT6 = 0b010000,
+  RO_V_VADC_VI = OP_IVI | (VADC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VADC_VV = OP_IVV | (VADC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VADC_VX = OP_IVX | (VADC_FUNCT6 << kRvvFunct6Shift),
+
+  VMADC_FUNCT6 = 0b010001,
+  RO_V_VMADC_VI = OP_IVI | (VMADC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMADC_VV = OP_IVV | (VMADC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMADC_VX = OP_IVX | (VMADC_FUNCT6 << kRvvFunct6Shift),
+
+  VWXUNARY0_FUNCT6 = 0b010000,
+  VRXUNARY0_FUNCT6 = 0b010000,
+  VMUNARY0_FUNCT6 = 0b010100,
+
+  RO_V_VWXUNARY0 = OP_MVV | (VWXUNARY0_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VRXUNARY0 = OP_MVX | (VRXUNARY0_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMUNARY0 = OP_MVV | (VMUNARY0_FUNCT6 << kRvvFunct6Shift),
+
+  VID_V = 0b10001,
+
+  VXUNARY0_FUNCT6 = 0b010010,
+  RO_V_VXUNARY0 = OP_MVV | (VXUNARY0_FUNCT6 << kRvvFunct6Shift),
+
+  VWFUNARY0_FUNCT6 = 0b010000,
+  RO_V_VFMV_FS = OP_FVV | (VWFUNARY0_FUNCT6 << kRvvFunct6Shift),
+
+  VRFUNARY0_FUNCT6 = 0b010000,
+  RO_V_VFMV_SF = OP_FVF | (VRFUNARY0_FUNCT6 << kRvvFunct6Shift),
+
+  VREDMAXU_FUNCT6 = 0b000110,
+  RO_V_VREDMAXU = OP_MVV | (VREDMAXU_FUNCT6 << kRvvFunct6Shift),
+  VREDMAX_FUNCT6 = 0b000111,
+  RO_V_VREDMAX = OP_MVV | (VREDMAX_FUNCT6 << kRvvFunct6Shift),
+
+  VREDMINU_FUNCT6 = 0b000100,
+  RO_V_VREDMINU = OP_MVV | (VREDMINU_FUNCT6 << kRvvFunct6Shift),
+  VREDMIN_FUNCT6 = 0b000101,
+  RO_V_VREDMIN = OP_MVV | (VREDMIN_FUNCT6 << kRvvFunct6Shift),
+
+  VFUNARY0_FUNCT6 = 0b010010,
+  RO_V_VFUNARY0 = OP_FVV | (VFUNARY0_FUNCT6 << kRvvFunct6Shift),
+  VFUNARY1_FUNCT6 = 0b010011,
+  RO_V_VFUNARY1 = OP_FVV | (VFUNARY1_FUNCT6 << kRvvFunct6Shift),
+
+  VFCVT_XU_F_V = 0b00000,
+  VFCVT_X_F_V = 0b00001,
+  VFCVT_F_XU_V = 0b00010,
+  VFCVT_F_X_V = 0b00011,
+  VFWCVT_XU_F_V = 0b01000,
+  VFWCVT_X_F_V = 0b01001,
+  VFWCVT_F_XU_V = 0b01010,
+  VFWCVT_F_X_V = 0b01011,
+  VFWCVT_F_F_V = 0b01100,
+  VFNCVT_F_F_W = 0b10100,
+  VFNCVT_X_F_W = 0b10001,
+  VFNCVT_XU_F_W = 0b10000,
+
+  VFCLASS_V = 0b10000,
+  VFSQRT_V = 0b00000,
+  VFRSQRT7_V = 0b00100,
+  VFREC7_V = 0b00101,
+
+  VFADD_FUNCT6 = 0b000000,
+  RO_V_VFADD_VV = OP_FVV | (VFADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFADD_VF = OP_FVF | (VFADD_FUNCT6 << kRvvFunct6Shift),
+
+  VFSUB_FUNCT6 = 0b000010,
+  RO_V_VFSUB_VV = OP_FVV | (VFSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFSUB_VF = OP_FVF | (VFSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VFDIV_FUNCT6 = 0b100000,
+  RO_V_VFDIV_VV = OP_FVV | (VFDIV_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFDIV_VF = OP_FVF | (VFDIV_FUNCT6 << kRvvFunct6Shift),
+
+  VFMUL_FUNCT6 = 0b100100,
+  RO_V_VFMUL_VV = OP_FVV | (VFMUL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMUL_VF = OP_FVF | (VFMUL_FUNCT6 << kRvvFunct6Shift),
+
+  // Vector Widening Floating-Point Add/Subtract Instructions
+  VFWADD_FUNCT6 = 0b110000,
+  RO_V_VFWADD_VV = OP_FVV | (VFWADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWADD_VF = OP_FVF | (VFWADD_FUNCT6 << kRvvFunct6Shift),
+
+  VFWSUB_FUNCT6 = 0b110010,
+  RO_V_VFWSUB_VV = OP_FVV | (VFWSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWSUB_VF = OP_FVF | (VFWSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VFWADD_W_FUNCT6 = 0b110100,
+  RO_V_VFWADD_W_VV = OP_FVV | (VFWADD_W_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWADD_W_VF = OP_FVF | (VFWADD_W_FUNCT6 << kRvvFunct6Shift),
+
+  VFWSUB_W_FUNCT6 = 0b110110,
+  RO_V_VFWSUB_W_VV = OP_FVV | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWSUB_W_VF = OP_FVF | (VFWSUB_W_FUNCT6 << kRvvFunct6Shift),
+
+  // Vector Widening Floating-Point Reduction Instructions
+  VFWREDUSUM_FUNCT6 = 0b110001,
+  RO_V_VFWREDUSUM_VV = OP_FVV | (VFWREDUSUM_FUNCT6 << kRvvFunct6Shift),
+
+  VFWREDOSUM_FUNCT6 = 0b110011,
+  RO_V_VFWREDOSUM_VV = OP_FVV | (VFWREDOSUM_FUNCT6 << kRvvFunct6Shift),
+
+  // Vector Widening Floating-Point Multiply
+  VFWMUL_FUNCT6 = 0b111000,
+  RO_V_VFWMUL_VV = OP_FVV | (VFWMUL_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWMUL_VF = OP_FVF | (VFWMUL_FUNCT6 << kRvvFunct6Shift),
+
+  VMFEQ_FUNCT6 = 0b011000,
+  RO_V_VMFEQ_VV = OP_FVV | (VMFEQ_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMFEQ_VF = OP_FVF | (VMFEQ_FUNCT6 << kRvvFunct6Shift),
+
+  VMFNE_FUNCT6 = 0b011100,
+  RO_V_VMFNE_VV = OP_FVV | (VMFNE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMFNE_VF = OP_FVF | (VMFNE_FUNCT6 << kRvvFunct6Shift),
+
+  VMFLT_FUNCT6 = 0b011011,
+  RO_V_VMFLT_VV = OP_FVV | (VMFLT_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMFLT_VF = OP_FVF | (VMFLT_FUNCT6 << kRvvFunct6Shift),
+
+  VMFLE_FUNCT6 = 0b011001,
+  RO_V_VMFLE_VV = OP_FVV | (VMFLE_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VMFLE_VF = OP_FVF | (VMFLE_FUNCT6 << kRvvFunct6Shift),
+
+  VMFGE_FUNCT6 = 0b011111,
+  RO_V_VMFGE_VF = OP_FVF | (VMFGE_FUNCT6 << kRvvFunct6Shift),
+
+  VMFGT_FUNCT6 = 0b011101,
+  RO_V_VMFGT_VF = OP_FVF | (VMFGT_FUNCT6 << kRvvFunct6Shift),
+
+  VFMAX_FUNCT6 = 0b000110,
+  RO_V_VFMAX_VV = OP_FVV | (VFMAX_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMAX_VF = OP_FVF | (VFMAX_FUNCT6 << kRvvFunct6Shift),
+
+  VFREDMAX_FUNCT6 = 0b0001111,
+  RO_V_VFREDMAX_VV = OP_FVV | (VFREDMAX_FUNCT6 << kRvvFunct6Shift),
+
+  VFMIN_FUNCT6 = 0b000100,
+  RO_V_VFMIN_VV = OP_FVV | (VFMIN_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMIN_VF = OP_FVF | (VFMIN_FUNCT6 << kRvvFunct6Shift),
+
+  VFSGNJ_FUNCT6 = 0b001000,
+  RO_V_VFSGNJ_VV = OP_FVV | (VFSGNJ_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFSGNJ_VF = OP_FVF | (VFSGNJ_FUNCT6 << kRvvFunct6Shift),
+
+  VFSGNJN_FUNCT6 = 0b001001,
+  RO_V_VFSGNJN_VV = OP_FVV | (VFSGNJN_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFSGNJN_VF = OP_FVF | (VFSGNJN_FUNCT6 << kRvvFunct6Shift),
+
+  VFSGNJX_FUNCT6 = 0b001010,
+  RO_V_VFSGNJX_VV = OP_FVV | (VFSGNJX_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFSGNJX_VF = OP_FVF | (VFSGNJX_FUNCT6 << kRvvFunct6Shift),
+
+  VFMADD_FUNCT6 = 0b101000,
+  RO_V_VFMADD_VV = OP_FVV | (VFMADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMADD_VF = OP_FVF | (VFMADD_FUNCT6 << kRvvFunct6Shift),
+
+  VFNMADD_FUNCT6 = 0b101001,
+  RO_V_VFNMADD_VV = OP_FVV | (VFNMADD_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFNMADD_VF = OP_FVF | (VFNMADD_FUNCT6 << kRvvFunct6Shift),
+
+  VFMSUB_FUNCT6 = 0b101010,
+  RO_V_VFMSUB_VV = OP_FVV | (VFMSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMSUB_VF = OP_FVF | (VFMSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VFNMSUB_FUNCT6 = 0b101011,
+  RO_V_VFNMSUB_VV = OP_FVV | (VFNMSUB_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFNMSUB_VF = OP_FVF | (VFNMSUB_FUNCT6 << kRvvFunct6Shift),
+
+  VFMACC_FUNCT6 = 0b101100,
+  RO_V_VFMACC_VV = OP_FVV | (VFMACC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMACC_VF = OP_FVF | (VFMACC_FUNCT6 << kRvvFunct6Shift),
+
+  VFNMACC_FUNCT6 = 0b101101,
+  RO_V_VFNMACC_VV = OP_FVV | (VFNMACC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFNMACC_VF = OP_FVF | (VFNMACC_FUNCT6 << kRvvFunct6Shift),
+
+  VFMSAC_FUNCT6 = 0b101110,
+  RO_V_VFMSAC_VV = OP_FVV | (VFMSAC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFMSAC_VF = OP_FVF | (VFMSAC_FUNCT6 << kRvvFunct6Shift),
+
+  VFNMSAC_FUNCT6 = 0b101111,
+  RO_V_VFNMSAC_VV = OP_FVV | (VFNMSAC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFNMSAC_VF = OP_FVF | (VFNMSAC_FUNCT6 << kRvvFunct6Shift),
+
+  // Vector Widening Floating-Point Fused Multiply-Add Instructions
+  VFWMACC_FUNCT6 = 0b111100,
+  RO_V_VFWMACC_VV = OP_FVV | (VFWMACC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWMACC_VF = OP_FVF | (VFWMACC_FUNCT6 << kRvvFunct6Shift),
+
+  VFWNMACC_FUNCT6 = 0b111101,
+  RO_V_VFWNMACC_VV = OP_FVV | (VFWNMACC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWNMACC_VF = OP_FVF | (VFWNMACC_FUNCT6 << kRvvFunct6Shift),
+
+  VFWMSAC_FUNCT6 = 0b111110,
+  RO_V_VFWMSAC_VV = OP_FVV | (VFWMSAC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWMSAC_VF = OP_FVF | (VFWMSAC_FUNCT6 << kRvvFunct6Shift),
+
+  VFWNMSAC_FUNCT6 = 0b111111,
+  RO_V_VFWNMSAC_VV = OP_FVV | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VFWNMSAC_VF = OP_FVF | (VFWNMSAC_FUNCT6 << kRvvFunct6Shift),
+
+  VNCLIP_FUNCT6 = 0b101111,
+  RO_V_VNCLIP_WV = OP_IVV | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VNCLIP_WX = OP_IVX | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VNCLIP_WI = OP_IVI | (VNCLIP_FUNCT6 << kRvvFunct6Shift),
+
+  VNCLIPU_FUNCT6 = 0b101110,
+  RO_V_VNCLIPU_WV = OP_IVV | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VNCLIPU_WX = OP_IVX | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
+  RO_V_VNCLIPU_WI = OP_IVI | (VNCLIPU_FUNCT6 << kRvvFunct6Shift),
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_V_H_
diff --git a/src/codegen/riscv/constant-riscv-zicsr.h b/src/codegen/riscv/constant-riscv-zicsr.h
new file mode 100644
index 00000000000..d6171859eeb
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-zicsr.h
@@ -0,0 +1,30 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+// RISCV CSR related bit mask and shift
+const int kFcsrFlagsBits = 5;
+const uint32_t kFcsrFlagsMask = (1 << kFcsrFlagsBits) - 1;
+const int kFcsrFrmBits = 3;
+const int kFcsrFrmShift = kFcsrFlagsBits;
+const uint32_t kFcsrFrmMask = ((1 << kFcsrFrmBits) - 1) << kFcsrFrmShift;
+const int kFcsrBits = kFcsrFlagsBits + kFcsrFrmBits;
+const uint32_t kFcsrMask = kFcsrFlagsMask | kFcsrFrmMask;
+
+enum OpcodeRISCVZICSR : uint32_t {
+  // RV32/RV64 Zicsr Standard Extension
+  RO_CSRRW = SYSTEM | (0b001 << kFunct3Shift),
+  RO_CSRRS = SYSTEM | (0b010 << kFunct3Shift),
+  RO_CSRRC = SYSTEM | (0b011 << kFunct3Shift),
+  RO_CSRRWI = SYSTEM | (0b101 << kFunct3Shift),
+  RO_CSRRSI = SYSTEM | (0b110 << kFunct3Shift),
+  RO_CSRRCI = SYSTEM | (0b111 << kFunct3Shift),
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_ZICSR_H_
diff --git a/src/codegen/riscv/constant-riscv-zifencei.h b/src/codegen/riscv/constant-riscv-zifencei.h
new file mode 100644
index 00000000000..49105017cbe
--- /dev/null
+++ b/src/codegen/riscv/constant-riscv-zifencei.h
@@ -0,0 +1,15 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_
+#define V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_
+
+#include "src/codegen/riscv/base-constants-riscv.h"
+namespace v8 {
+namespace internal {
+enum OpcodeRISCVIFENCEI : uint32_t {
+  RO_FENCE_I = MISC_MEM | (0b001 << kFunct3Shift),
+};
+}
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_CONSTANT_RISCV_ZIFENCEI_H_
diff --git a/src/codegen/riscv/constants-riscv.h b/src/codegen/riscv/constants-riscv.h
new file mode 100644
index 00000000000..7d4beebb9ee
--- /dev/null
+++ b/src/codegen/riscv/constants-riscv.h
@@ -0,0 +1,20 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
+#define V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
+#include "src/codegen/riscv/base-constants-riscv.h"
+#include "src/codegen/riscv/constant-riscv-a.h"
+#include "src/codegen/riscv/constant-riscv-c.h"
+#include "src/codegen/riscv/constant-riscv-d.h"
+#include "src/codegen/riscv/constant-riscv-f.h"
+#include "src/codegen/riscv/constant-riscv-i.h"
+#include "src/codegen/riscv/constant-riscv-m.h"
+#include "src/codegen/riscv/constant-riscv-v.h"
+#include "src/codegen/riscv/constant-riscv-zicsr.h"
+#include "src/codegen/riscv/constant-riscv-zifencei.h"
+namespace v8 {
+namespace internal {}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_CONSTANTS_RISCV_H_
diff --git a/src/codegen/riscv64/cpu-riscv64.cc b/src/codegen/riscv/cpu-riscv.cc
similarity index 93%
rename from src/codegen/riscv64/cpu-riscv64.cc
rename to src/codegen/riscv/cpu-riscv.cc
index aad09378f99..205e13fa988 100644
--- a/src/codegen/riscv64/cpu-riscv64.cc
+++ b/src/codegen/riscv/cpu-riscv.cc
@@ -7,8 +7,6 @@
 #include <sys/syscall.h>
 #include <unistd.h>
 
-#if V8_TARGET_ARCH_RISCV64
-
 #include "src/codegen/cpu-features.h"
 
 namespace v8 {
@@ -28,5 +26,3 @@ void CpuFeatures::FlushICache(void* start, size_t size) {
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/codegen/riscv/extension-riscv-a.cc b/src/codegen/riscv/extension-riscv-a.cc
new file mode 100644
index 00000000000..51f12fd2c55
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-a.cc
@@ -0,0 +1,121 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-a.h"
+
+namespace v8 {
+namespace internal {
+
+// RV32A Standard Extension
+void AssemblerRISCVA::lr_w(bool aq, bool rl, Register rd, Register rs1) {
+  GenInstrRAtomic(0b00010, aq, rl, 0b010, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVA::sc_w(bool aq, bool rl, Register rd, Register rs1,
+                           Register rs2) {
+  GenInstrRAtomic(0b00011, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoswap_w(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b00001, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoadd_w(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b00000, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoxor_w(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b00100, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoand_w(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b01100, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoor_w(bool aq, bool rl, Register rd, Register rs1,
+                              Register rs2) {
+  GenInstrRAtomic(0b01000, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomin_w(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b10000, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomax_w(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b10100, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amominu_w(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b11000, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomaxu_w(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b11100, aq, rl, 0b010, rd, rs1, rs2);
+}
+
+// RV64A Standard Extension (in addition to RV32A)
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVA::lr_d(bool aq, bool rl, Register rd, Register rs1) {
+  GenInstrRAtomic(0b00010, aq, rl, 0b011, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVA::sc_d(bool aq, bool rl, Register rd, Register rs1,
+                           Register rs2) {
+  GenInstrRAtomic(0b00011, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoswap_d(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b00001, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoadd_d(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b00000, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoxor_d(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b00100, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoand_d(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b01100, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amoor_d(bool aq, bool rl, Register rd, Register rs1,
+                              Register rs2) {
+  GenInstrRAtomic(0b01000, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomin_d(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b10000, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomax_d(bool aq, bool rl, Register rd, Register rs1,
+                               Register rs2) {
+  GenInstrRAtomic(0b10100, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amominu_d(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b11000, aq, rl, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVA::amomaxu_d(bool aq, bool rl, Register rd, Register rs1,
+                                Register rs2) {
+  GenInstrRAtomic(0b11100, aq, rl, 0b011, rd, rs1, rs2);
+}
+#endif
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-a.h b/src/codegen/riscv/extension-riscv-a.h
new file mode 100644
index 00000000000..69213c7ee15
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-a.h
@@ -0,0 +1,45 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-a.h"
+#include "src/codegen/riscv/register-riscv.h"
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVA : public AssemblerRiscvBase {
+  // RV32A Standard Extension
+ public:
+  void lr_w(bool aq, bool rl, Register rd, Register rs1);
+  void sc_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoswap_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoadd_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoxor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoand_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomin_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomax_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amominu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomaxu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64A Standard Extension (in addition to RV32A)
+  void lr_d(bool aq, bool rl, Register rd, Register rs1);
+  void sc_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoswap_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoadd_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoxor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoand_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amoor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomin_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomax_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amominu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+  void amomaxu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_A_H_
diff --git a/src/codegen/riscv/extension-riscv-c.cc b/src/codegen/riscv/extension-riscv-c.cc
new file mode 100644
index 00000000000..2721d3a8480
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-c.cc
@@ -0,0 +1,276 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-c.h"
+
+namespace v8 {
+namespace internal {
+// RV64C Standard Extension
+void AssemblerRISCVC::c_nop() { GenInstrCI(0b000, C1, zero_reg, 0); }
+
+void AssemblerRISCVC::c_addi(Register rd, int8_t imm6) {
+  DCHECK(rd != zero_reg && imm6 != 0);
+  GenInstrCI(0b000, C1, rd, imm6);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_addiw(Register rd, int8_t imm6) {
+  DCHECK(rd != zero_reg);
+  GenInstrCI(0b001, C1, rd, imm6);
+}
+#endif
+
+void AssemblerRISCVC::c_addi16sp(int16_t imm10) {
+  DCHECK(is_int10(imm10) && (imm10 & 0xf) == 0);
+  uint8_t uimm6 = ((imm10 & 0x200) >> 4) | (imm10 & 0x10) |
+                  ((imm10 & 0x40) >> 3) | ((imm10 & 0x180) >> 6) |
+                  ((imm10 & 0x20) >> 5);
+  GenInstrCIU(0b011, C1, sp, uimm6);
+}
+
+void AssemblerRISCVC::c_addi4spn(Register rd, int16_t uimm10) {
+  DCHECK(is_uint10(uimm10) && (uimm10 != 0));
+  uint8_t uimm8 = ((uimm10 & 0x4) >> 1) | ((uimm10 & 0x8) >> 3) |
+                  ((uimm10 & 0x30) << 2) | ((uimm10 & 0x3c0) >> 4);
+  GenInstrCIW(0b000, C0, rd, uimm8);
+}
+
+void AssemblerRISCVC::c_li(Register rd, int8_t imm6) {
+  DCHECK(rd != zero_reg);
+  GenInstrCI(0b010, C1, rd, imm6);
+}
+
+void AssemblerRISCVC::c_lui(Register rd, int8_t imm6) {
+  DCHECK(rd != zero_reg && rd != sp && imm6 != 0);
+  GenInstrCI(0b011, C1, rd, imm6);
+}
+
+void AssemblerRISCVC::c_slli(Register rd, uint8_t shamt6) {
+  DCHECK(rd != zero_reg && shamt6 != 0);
+  GenInstrCIU(0b000, C2, rd, shamt6);
+}
+
+void AssemblerRISCVC::c_fldsp(FPURegister rd, uint16_t uimm9) {
+  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
+  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
+  GenInstrCIU(0b001, C2, rd, uimm6);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_ldsp(Register rd, uint16_t uimm9) {
+  DCHECK(rd != zero_reg && is_uint9(uimm9) && (uimm9 & 0x7) == 0);
+  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
+  GenInstrCIU(0b011, C2, rd, uimm6);
+}
+#endif
+
+void AssemblerRISCVC::c_lwsp(Register rd, uint16_t uimm8) {
+  DCHECK(rd != zero_reg && is_uint8(uimm8) && (uimm8 & 0x3) == 0);
+  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCIU(0b010, C2, rd, uimm6);
+}
+
+void AssemblerRISCVC::c_jr(Register rs1) {
+  DCHECK(rs1 != zero_reg);
+  GenInstrCR(0b1000, C2, rs1, zero_reg);
+  BlockTrampolinePoolFor(1);
+}
+
+void AssemblerRISCVC::c_mv(Register rd, Register rs2) {
+  DCHECK(rd != zero_reg && rs2 != zero_reg);
+  GenInstrCR(0b1000, C2, rd, rs2);
+}
+
+void AssemblerRISCVC::c_ebreak() { GenInstrCR(0b1001, C2, zero_reg, zero_reg); }
+
+void AssemblerRISCVC::c_jalr(Register rs1) {
+  DCHECK(rs1 != zero_reg);
+  GenInstrCR(0b1001, C2, rs1, zero_reg);
+  BlockTrampolinePoolFor(1);
+}
+
+void AssemblerRISCVC::c_add(Register rd, Register rs2) {
+  DCHECK(rd != zero_reg && rs2 != zero_reg);
+  GenInstrCR(0b1001, C2, rd, rs2);
+}
+
+// CA Instructions
+void AssemblerRISCVC::c_sub(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100011, C1, rd, 0b00, rs2);
+}
+
+void AssemblerRISCVC::c_xor(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100011, C1, rd, 0b01, rs2);
+}
+
+void AssemblerRISCVC::c_or(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100011, C1, rd, 0b10, rs2);
+}
+
+void AssemblerRISCVC::c_and(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100011, C1, rd, 0b11, rs2);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_subw(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100111, C1, rd, 0b00, rs2);
+}
+
+void AssemblerRISCVC::c_addw(Register rd, Register rs2) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs2.code() & 0b11000) == 0b01000));
+  GenInstrCA(0b100111, C1, rd, 0b01, rs2);
+}
+#endif
+
+void AssemblerRISCVC::c_swsp(Register rs2, uint16_t uimm8) {
+  DCHECK(is_uint8(uimm8) && (uimm8 & 0x3) == 0);
+  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCSS(0b110, C2, rs2, uimm6);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_sdsp(Register rs2, uint16_t uimm9) {
+  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
+  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
+  GenInstrCSS(0b111, C2, rs2, uimm6);
+}
+#endif
+
+void AssemblerRISCVC::c_fsdsp(FPURegister rs2, uint16_t uimm9) {
+  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
+  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
+  GenInstrCSS(0b101, C2, rs2, uimm6);
+}
+
+// CL Instructions
+
+void AssemblerRISCVC::c_lw(Register rd, Register rs1, uint16_t uimm7) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
+         ((uimm7 & 0x3) == 0));
+  uint8_t uimm5 =
+      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
+  GenInstrCL(0b010, C0, rd, rs1, uimm5);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_ld(Register rd, Register rs1, uint16_t uimm8) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
+         ((uimm8 & 0x7) == 0));
+  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCL(0b011, C0, rd, rs1, uimm5);
+}
+#endif
+
+void AssemblerRISCVC::c_fld(FPURegister rd, Register rs1, uint16_t uimm8) {
+  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
+         ((uimm8 & 0x7) == 0));
+  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCL(0b001, C0, rd, rs1, uimm5);
+}
+
+// CS Instructions
+
+void AssemblerRISCVC::c_sw(Register rs2, Register rs1, uint16_t uimm7) {
+  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
+         ((uimm7 & 0x3) == 0));
+  uint8_t uimm5 =
+      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
+  GenInstrCS(0b110, C0, rs2, rs1, uimm5);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+void AssemblerRISCVC::c_sd(Register rs2, Register rs1, uint16_t uimm8) {
+  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
+         ((uimm8 & 0x7) == 0));
+  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCS(0b111, C0, rs2, rs1, uimm5);
+}
+#endif
+
+void AssemblerRISCVC::c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8) {
+  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
+         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
+         ((uimm8 & 0x7) == 0));
+  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
+  GenInstrCS(0b101, C0, rs2, rs1, uimm5);
+}
+
+// CJ Instructions
+
+void AssemblerRISCVC::c_j(int16_t imm12) {
+  DCHECK(is_int12(imm12));
+  int16_t uimm11 = ((imm12 & 0x800) >> 1) | ((imm12 & 0x400) >> 4) |
+                   ((imm12 & 0x300) >> 1) | ((imm12 & 0x80) >> 3) |
+                   ((imm12 & 0x40) >> 1) | ((imm12 & 0x20) >> 5) |
+                   ((imm12 & 0x10) << 5) | (imm12 & 0xe);
+  GenInstrCJ(0b101, C1, uimm11);
+  BlockTrampolinePoolFor(1);
+}
+
+// CB Instructions
+
+void AssemblerRISCVC::c_bnez(Register rs1, int16_t imm9) {
+  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
+  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
+                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
+  GenInstrCB(0b111, C1, rs1, uimm8);
+}
+
+void AssemblerRISCVC::c_beqz(Register rs1, int16_t imm9) {
+  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
+  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
+                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
+  GenInstrCB(0b110, C1, rs1, uimm8);
+}
+
+void AssemblerRISCVC::c_srli(Register rs1, int8_t shamt6) {
+  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
+  GenInstrCBA(0b100, 0b00, C1, rs1, shamt6);
+}
+
+void AssemblerRISCVC::c_srai(Register rs1, int8_t shamt6) {
+  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
+  GenInstrCBA(0b100, 0b01, C1, rs1, shamt6);
+}
+
+void AssemblerRISCVC::c_andi(Register rs1, int8_t imm6) {
+  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(imm6));
+  GenInstrCBA(0b100, 0b10, C1, rs1, imm6);
+}
+
+bool AssemblerRISCVC::IsCJal(Instr instr) {
+  return (instr & kRvcOpcodeMask) == RO_C_J;
+}
+
+bool AssemblerRISCVC::IsCBranch(Instr instr) {
+  int Op = instr & kRvcOpcodeMask;
+  return Op == RO_C_BNEZ || Op == RO_C_BEQZ;
+}
+
+int AssemblerRISCVC::CJumpOffset(Instr instr) {
+  int32_t imm12 = ((instr & 0x4) << 3) | ((instr & 0x38) >> 2) |
+                  ((instr & 0x40) << 1) | ((instr & 0x80) >> 1) |
+                  ((instr & 0x100) << 2) | ((instr & 0x600) >> 1) |
+                  ((instr & 0x800) >> 7) | ((instr & 0x1000) >> 1);
+  imm12 = imm12 << 20 >> 20;
+  return imm12;
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-c.h b/src/codegen/riscv/extension-riscv-c.h
new file mode 100644
index 00000000000..5fe0b206985
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-c.h
@@ -0,0 +1,76 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-c.h"
+#include "src/codegen/riscv/register-riscv.h"
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVC : public AssemblerRiscvBase {
+  // RV64C Standard Extension
+ public:
+  void c_nop();
+  void c_addi(Register rd, int8_t imm6);
+
+  void c_addi16sp(int16_t imm10);
+  void c_addi4spn(Register rd, int16_t uimm10);
+  void c_li(Register rd, int8_t imm6);
+  void c_lui(Register rd, int8_t imm6);
+  void c_slli(Register rd, uint8_t shamt6);
+  void c_lwsp(Register rd, uint16_t uimm8);
+  void c_jr(Register rs1);
+  void c_mv(Register rd, Register rs2);
+  void c_ebreak();
+  void c_jalr(Register rs1);
+  void c_j(int16_t imm12);
+  void c_add(Register rd, Register rs2);
+  void c_sub(Register rd, Register rs2);
+  void c_and(Register rd, Register rs2);
+  void c_xor(Register rd, Register rs2);
+  void c_or(Register rd, Register rs2);
+  void c_swsp(Register rs2, uint16_t uimm8);
+  void c_lw(Register rd, Register rs1, uint16_t uimm7);
+  void c_sw(Register rs2, Register rs1, uint16_t uimm7);
+  void c_bnez(Register rs1, int16_t imm9);
+  void c_beqz(Register rs1, int16_t imm9);
+  void c_srli(Register rs1, int8_t shamt6);
+  void c_srai(Register rs1, int8_t shamt6);
+  void c_andi(Register rs1, int8_t imm6);
+
+  void c_fld(FPURegister rd, Register rs1, uint16_t uimm8);
+  void c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8);
+  void c_fldsp(FPURegister rd, uint16_t uimm9);
+  void c_fsdsp(FPURegister rs2, uint16_t uimm9);
+#ifdef V8_TARGET_ARCH_RISCV64
+  void c_ld(Register rd, Register rs1, uint16_t uimm8);
+  void c_sd(Register rs2, Register rs1, uint16_t uimm8);
+  void c_subw(Register rd, Register rs2);
+  void c_addw(Register rd, Register rs2);
+  void c_addiw(Register rd, int8_t imm6);
+  void c_ldsp(Register rd, uint16_t uimm9);
+  void c_sdsp(Register rs2, uint16_t uimm9);
+#endif
+
+  int CJumpOffset(Instr instr);
+
+  static bool IsCBranch(Instr instr);
+  static bool IsCJal(Instr instr);
+
+  inline int16_t cjump_offset(Label* L) {
+    return (int16_t)branch_offset_helper(L, OffsetSize::kOffset11);
+  }
+  inline int32_t cbranch_offset(Label* L) {
+    return branch_offset_helper(L, OffsetSize::kOffset9);
+  }
+
+  void c_j(Label* L) { c_j(cjump_offset(L)); }
+  void c_bnez(Register rs1, Label* L) { c_bnez(rs1, cbranch_offset(L)); }
+  void c_beqz(Register rs1, Label* L) { c_beqz(rs1, cbranch_offset(L)); }
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_C_H_
diff --git a/src/codegen/riscv/extension-riscv-d.cc b/src/codegen/riscv/extension-riscv-d.cc
new file mode 100644
index 00000000000..ab2cd12d958
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-d.cc
@@ -0,0 +1,165 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-d.h"
+
+namespace v8 {
+namespace internal {
+// RV32D Standard Extension
+
+void AssemblerRISCVD::fld(FPURegister rd, Register rs1, int16_t imm12) {
+  GenInstrLoadFP_ri(0b011, rd, rs1, imm12);
+}
+
+void AssemblerRISCVD::fsd(FPURegister source, Register base, int16_t imm12) {
+  GenInstrStoreFP_rri(0b011, base, source, imm12);
+}
+
+void AssemblerRISCVD::fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                              FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b01, MADD, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVD::fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                              FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b01, MSUB, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVD::fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                               FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b01, NMSUB, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVD::fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                               FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b01, NMADD, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVD::fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0000001, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0000101, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0001001, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0001101, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fsqrt_d(FPURegister rd, FPURegister rs1,
+                              FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0101101, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::fsgnj_d(FPURegister rd, FPURegister rs1,
+                              FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010001, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fsgnjn_d(FPURegister rd, FPURegister rs1,
+                               FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010001, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fsgnjx_d(FPURegister rd, FPURegister rs1,
+                               FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010001, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010101, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010101, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fcvt_s_d(FPURegister rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0100000, frm, rd, rs1, ToRegister(1));
+}
+
+void AssemblerRISCVD::fcvt_d_s(FPURegister rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0100001, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::feq_d(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010001, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::flt_d(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010001, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fle_d(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010001, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVD::fclass_d(Register rd, FPURegister rs1) {
+  GenInstrALUFP_rr(0b1110001, 0b001, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::fcvt_w_d(Register rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::fcvt_wu_d(Register rd, FPURegister rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(1));
+}
+
+void AssemblerRISCVD::fcvt_d_w(FPURegister rd, Register rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::fcvt_d_wu(FPURegister rd, Register rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(1));
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+// RV64D Standard Extension (in addition to RV32D)
+
+void AssemblerRISCVD::fcvt_l_d(Register rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(2));
+}
+
+void AssemblerRISCVD::fcvt_lu_d(Register rd, FPURegister rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(3));
+}
+
+void AssemblerRISCVD::fmv_x_d(Register rd, FPURegister rs1) {
+  GenInstrALUFP_rr(0b1110001, 0b000, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVD::fcvt_d_l(FPURegister rd, Register rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(2));
+}
+
+void AssemblerRISCVD::fcvt_d_lu(FPURegister rd, Register rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(3));
+}
+
+void AssemblerRISCVD::fmv_d_x(FPURegister rd, Register rs1) {
+  GenInstrALUFP_rr(0b1111001, 0b000, rd, rs1, zero_reg);
+}
+#endif
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-d.h b/src/codegen/riscv/extension-riscv-d.h
new file mode 100644
index 00000000000..e7a707a793c
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-d.h
@@ -0,0 +1,67 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-d.h"
+#include "src/codegen/riscv/register-riscv.h"
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVD : public AssemblerRiscvBase {
+  // RV32D Standard Extension
+ public:
+  void fld(FPURegister rd, Register rs1, int16_t imm12);
+  void fsd(FPURegister source, Register base, int16_t imm12);
+  void fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+               FPURegister rs3, FPURoundingMode frm = RNE);
+  void fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+               FPURegister rs3, FPURoundingMode frm = RNE);
+  void fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                FPURegister rs3, FPURoundingMode frm = RNE);
+  void fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                FPURegister rs3, FPURoundingMode frm = RNE);
+  void fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fsqrt_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fsgnj_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fsgnjn_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fsgnjx_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fcvt_s_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_d_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void feq_d(Register rd, FPURegister rs1, FPURegister rs2);
+  void flt_d(Register rd, FPURegister rs1, FPURegister rs2);
+  void fle_d(Register rd, FPURegister rs1, FPURegister rs2);
+  void fclass_d(Register rd, FPURegister rs1);
+  void fcvt_w_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_wu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_d_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fcvt_d_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64D Standard Extension (in addition to RV32D)
+  void fcvt_l_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_lu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fmv_x_d(Register rd, FPURegister rs1);
+  void fcvt_d_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fcvt_d_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fmv_d_x(FPURegister rd, Register rs1);
+#endif
+
+  void fmv_d(FPURegister rd, FPURegister rs) { fsgnj_d(rd, rs, rs); }
+  void fabs_d(FPURegister rd, FPURegister rs) { fsgnjx_d(rd, rs, rs); }
+  void fneg_d(FPURegister rd, FPURegister rs) { fsgnjn_d(rd, rs, rs); }
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_D_H_
diff --git a/src/codegen/riscv/extension-riscv-f.cc b/src/codegen/riscv/extension-riscv-f.cc
new file mode 100644
index 00000000000..12e6268aec0
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-f.cc
@@ -0,0 +1,156 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-f.h"
+
+namespace v8 {
+namespace internal {
+
+// RV32F Standard Extension
+
+void AssemblerRISCVF::flw(FPURegister rd, Register rs1, int16_t imm12) {
+  GenInstrLoadFP_ri(0b010, rd, rs1, imm12);
+}
+
+void AssemblerRISCVF::fsw(FPURegister source, Register base, int16_t imm12) {
+  GenInstrStoreFP_rri(0b010, base, source, imm12);
+}
+
+void AssemblerRISCVF::fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                              FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b00, MADD, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVF::fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                              FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b00, MSUB, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVF::fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                               FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b00, NMSUB, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVF::fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                               FPURegister rs3, FPURoundingMode frm) {
+  GenInstrR4(0b00, NMADD, rd, rs1, rs2, rs3, frm);
+}
+
+void AssemblerRISCVF::fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0000000, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0000100, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0001000, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                             FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0001100, frm, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fsqrt_s(FPURegister rd, FPURegister rs1,
+                              FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b0101100, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVF::fsgnj_s(FPURegister rd, FPURegister rs1,
+                              FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fsgnjn_s(FPURegister rd, FPURegister rs1,
+                               FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010000, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fsgnjx_s(FPURegister rd, FPURegister rs1,
+                               FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010000, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010100, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b0010100, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fcvt_w_s(Register rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVF::fcvt_wu_s(Register rd, FPURegister rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(1));
+}
+
+void AssemblerRISCVF::fmv_x_w(Register rd, FPURegister rs1) {
+  GenInstrALUFP_rr(0b1110000, 0b000, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVF::feq_s(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010000, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::flt_s(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010000, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fle_s(Register rd, FPURegister rs1, FPURegister rs2) {
+  GenInstrALUFP_rr(0b1010000, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVF::fclass_s(Register rd, FPURegister rs1) {
+  GenInstrALUFP_rr(0b1110000, 0b001, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVF::fcvt_s_w(FPURegister rd, Register rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, zero_reg);
+}
+
+void AssemblerRISCVF::fcvt_s_wu(FPURegister rd, Register rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(1));
+}
+
+void AssemblerRISCVF::fmv_w_x(FPURegister rd, Register rs1) {
+  GenInstrALUFP_rr(0b1111000, 0b000, rd, rs1, zero_reg);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+// RV64F Standard Extension (in addition to RV32F)
+
+void AssemblerRISCVF::fcvt_l_s(Register rd, FPURegister rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(2));
+}
+
+void AssemblerRISCVF::fcvt_lu_s(Register rd, FPURegister rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(3));
+}
+
+void AssemblerRISCVF::fcvt_s_l(FPURegister rd, Register rs1,
+                               FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(2));
+}
+
+void AssemblerRISCVF::fcvt_s_lu(FPURegister rd, Register rs1,
+                                FPURoundingMode frm) {
+  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(3));
+}
+#endif
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-f.h b/src/codegen/riscv/extension-riscv-f.h
new file mode 100644
index 00000000000..83c51719e91
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-f.h
@@ -0,0 +1,65 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-f.h"
+#include "src/codegen/riscv/register-riscv.h"
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVF : public AssemblerRiscvBase {
+  // RV32F Standard Extension
+ public:
+  void flw(FPURegister rd, Register rs1, int16_t imm12);
+  void fsw(FPURegister source, Register base, int16_t imm12);
+  void fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+               FPURegister rs3, FPURoundingMode frm = RNE);
+  void fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+               FPURegister rs3, FPURoundingMode frm = RNE);
+  void fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                FPURegister rs3, FPURoundingMode frm = RNE);
+  void fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+                FPURegister rs3, FPURoundingMode frm = RNE);
+  void fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
+              FPURoundingMode frm = RNE);
+  void fsqrt_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fsgnj_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fsgnjn_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fsgnjx_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
+  void fcvt_w_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_wu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fmv_x_w(Register rd, FPURegister rs1);
+  void feq_s(Register rd, FPURegister rs1, FPURegister rs2);
+  void flt_s(Register rd, FPURegister rs1, FPURegister rs2);
+  void fle_s(Register rd, FPURegister rs1, FPURegister rs2);
+  void fclass_s(Register rd, FPURegister rs1);
+  void fcvt_s_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fcvt_s_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fmv_w_x(FPURegister rd, Register rs1);
+
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64F Standard Extension (in addition to RV32F)
+  void fcvt_l_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_lu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
+  void fcvt_s_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+  void fcvt_s_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
+#endif
+
+  void fmv_s(FPURegister rd, FPURegister rs) { fsgnj_s(rd, rs, rs); }
+  void fabs_s(FPURegister rd, FPURegister rs) { fsgnjx_s(rd, rs, rs); }
+  void fneg_s(FPURegister rd, FPURegister rs) { fsgnjn_s(rd, rs, rs); }
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_F_H_
diff --git a/src/codegen/riscv/extension-riscv-m.cc b/src/codegen/riscv/extension-riscv-m.cc
new file mode 100644
index 00000000000..35a8bf13ed5
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-m.cc
@@ -0,0 +1,66 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-m.h"
+
+namespace v8 {
+namespace internal {
+// RV32M Standard Extension
+
+void AssemblerRISCVM::mul(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::mulh(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b001, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::mulhsu(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b010, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::mulhu(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b011, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::div(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b100, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::divu(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b101, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::rem(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b110, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::remu(Register rd, Register rs1, Register rs2) {
+  GenInstrALU_rr(0b0000001, 0b111, rd, rs1, rs2);
+}
+
+#ifdef V8_TARGET_ARCH_RISCV64
+// RV64M Standard Extension (in addition to RV32M)
+
+void AssemblerRISCVM::mulw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000001, 0b000, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::divw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000001, 0b100, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::divuw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000001, 0b101, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::remw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000001, 0b110, rd, rs1, rs2);
+}
+
+void AssemblerRISCVM::remuw(Register rd, Register rs1, Register rs2) {
+  GenInstrALUW_rr(0b0000001, 0b111, rd, rs1, rs2);
+}
+#endif
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-m.h b/src/codegen/riscv/extension-riscv-m.h
new file mode 100644
index 00000000000..1b700a4adb7
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-m.h
@@ -0,0 +1,36 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-m.h"
+#include "src/codegen/riscv/register-riscv.h"
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVM : public AssemblerRiscvBase {
+  // RV32M Standard Extension
+ public:
+  void mul(Register rd, Register rs1, Register rs2);
+  void mulh(Register rd, Register rs1, Register rs2);
+  void mulhsu(Register rd, Register rs1, Register rs2);
+  void mulhu(Register rd, Register rs1, Register rs2);
+  void div(Register rd, Register rs1, Register rs2);
+  void divu(Register rd, Register rs1, Register rs2);
+  void rem(Register rd, Register rs1, Register rs2);
+  void remu(Register rd, Register rs1, Register rs2);
+#ifdef V8_TARGET_ARCH_RISCV64
+  // RV64M Standard Extension (in addition to RV32M)
+  void mulw(Register rd, Register rs1, Register rs2);
+  void divw(Register rd, Register rs1, Register rs2);
+  void divuw(Register rd, Register rs1, Register rs2);
+  void remw(Register rd, Register rs1, Register rs2);
+  void remuw(Register rd, Register rs1, Register rs2);
+#endif
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_M_H_
diff --git a/src/codegen/riscv/extension-riscv-v.cc b/src/codegen/riscv/extension-riscv-v.cc
new file mode 100644
index 00000000000..c5be03a1819
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-v.cc
@@ -0,0 +1,889 @@
+
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/codegen/riscv/extension-riscv-v.h"
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/constant-riscv-v.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+namespace v8 {
+namespace internal {
+
+// RVV
+
+void AssemblerRISCVV::vredmaxu_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                                  MaskType mask) {
+  GenInstrV(VREDMAXU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vredmax_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                                 MaskType mask) {
+  GenInstrV(VREDMAX_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vredmin_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                                 MaskType mask) {
+  GenInstrV(VREDMIN_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vredminu_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                                  MaskType mask) {
+  GenInstrV(VREDMINU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vmv_vv(VRegister vd, VRegister vs1) {
+  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, v0, NoMask);
+}
+
+void AssemblerRISCVV::vmv_vx(VRegister vd, Register rs1) {
+  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, v0, NoMask);
+}
+
+void AssemblerRISCVV::vmv_vi(VRegister vd, uint8_t simm5) {
+  GenInstrV(VMV_FUNCT6, vd, simm5, v0, NoMask);
+}
+
+void AssemblerRISCVV::vmv_xs(Register rd, VRegister vs2) {
+  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b00000, vs2, NoMask);
+}
+
+void AssemblerRISCVV::vmv_sx(VRegister vd, Register rs1) {
+  GenInstrV(VRXUNARY0_FUNCT6, OP_MVX, vd, rs1, v0, NoMask);
+}
+
+void AssemblerRISCVV::vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2) {
+  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vmerge_vx(VRegister vd, Register rs1, VRegister vs2) {
+  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vmerge_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
+  GenInstrV(VMV_FUNCT6, vd, imm5, vs2, Mask);
+}
+
+void AssemblerRISCVV::vadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
+  GenInstrV(VADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vadc_vx(VRegister vd, Register rs1, VRegister vs2) {
+  GenInstrV(VADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
+  GenInstrV(VADC_FUNCT6, vd, imm5, vs2, Mask);
+}
+
+void AssemblerRISCVV::vmadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
+  GenInstrV(VMADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vmadc_vx(VRegister vd, Register rs1, VRegister vs2) {
+  GenInstrV(VMADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
+}
+
+void AssemblerRISCVV::vmadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
+  GenInstrV(VMADC_FUNCT6, vd, imm5, vs2, Mask);
+}
+
+void AssemblerRISCVV::vrgather_vv(VRegister vd, VRegister vs2, VRegister vs1,
+                                  MaskType mask) {
+  DCHECK_NE(vd, vs1);
+  DCHECK_NE(vd, vs2);
+  GenInstrV(VRGATHER_FUNCT6, OP_IVV, vd, vs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vrgather_vi(VRegister vd, VRegister vs2, int8_t imm5,
+                                  MaskType mask) {
+  DCHECK_NE(vd, vs2);
+  GenInstrV(VRGATHER_FUNCT6, vd, imm5, vs2, mask);
+}
+
+void AssemblerRISCVV::vrgather_vx(VRegister vd, VRegister vs2, Register rs1,
+                                  MaskType mask) {
+  DCHECK_NE(vd, vs2);
+  GenInstrV(VRGATHER_FUNCT6, OP_IVX, vd, rs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vwaddu_wx(VRegister vd, VRegister vs2, Register rs1,
+                                MaskType mask) {
+  GenInstrV(VWADDUW_FUNCT6, OP_MVX, vd, rs1, vs2, mask);
+}
+
+void AssemblerRISCVV::vid_v(VRegister vd, MaskType mask) {
+  GenInstrV(VMUNARY0_FUNCT6, OP_MVV, vd, VID_V, v0, mask);
+}
+
+#define DEFINE_OPIVV(name, funct6)                                            \
+  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_IVV, vd, vs1, vs2, mask);                            \
+  }
+
+#define DEFINE_OPFVV(name, funct6)                                            \
+  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
+  }
+
+#define DEFINE_OPFWV(name, funct6)                                            \
+  void AssemblerRISCVV::name##_wv(VRegister vd, VRegister vs2, VRegister vs1, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
+  }
+
+#define DEFINE_OPFRED(name, funct6)                                           \
+  void AssemblerRISCVV::name##_vs(VRegister vd, VRegister vs2, VRegister vs1, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
+  }
+
+#define DEFINE_OPIVX(name, funct6)                                           \
+  void AssemblerRISCVV::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
+                                  MaskType mask) {                           \
+    GenInstrV(funct6, OP_IVX, vd, rs1, vs2, mask);                           \
+  }
+
+#define DEFINE_OPIVI(name, funct6)                                          \
+  void AssemblerRISCVV::name##_vi(VRegister vd, VRegister vs2, int8_t imm5, \
+                                  MaskType mask) {                          \
+    GenInstrV(funct6, vd, imm5, vs2, mask);                                 \
+  }
+
+#define DEFINE_OPMVV(name, funct6)                                            \
+  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_MVV, vd, vs1, vs2, mask);                            \
+  }
+
+// void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd, Register
+// rs1,
+//                  VRegister vs2, MaskType mask = NoMask);
+#define DEFINE_OPMVX(name, funct6)                                           \
+  void AssemblerRISCVV::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
+                                  MaskType mask) {                           \
+    GenInstrV(funct6, OP_MVX, vd, rs1, vs2, mask);                           \
+  }
+
+#define DEFINE_OPFVF(name, funct6)                                  \
+  void AssemblerRISCVV::name##_vf(VRegister vd, VRegister vs2,      \
+                                  FPURegister fs1, MaskType mask) { \
+    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                  \
+  }
+
+#define DEFINE_OPFWF(name, funct6)                                  \
+  void AssemblerRISCVV::name##_wf(VRegister vd, VRegister vs2,      \
+                                  FPURegister fs1, MaskType mask) { \
+    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                  \
+  }
+
+#define DEFINE_OPFVV_FMA(name, funct6)                                        \
+  void AssemblerRISCVV::name##_vv(VRegister vd, VRegister vs1, VRegister vs2, \
+                                  MaskType mask) {                            \
+    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                            \
+  }
+
+#define DEFINE_OPFVF_FMA(name, funct6)                            \
+  void AssemblerRISCVV::name##_vf(VRegister vd, FPURegister fs1,  \
+                                  VRegister vs2, MaskType mask) { \
+    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                \
+  }
+
+// vector integer extension
+#define DEFINE_OPMVV_VIE(name, vs1)                                        \
+  void AssemblerRISCVV::name(VRegister vd, VRegister vs2, MaskType mask) { \
+    GenInstrV(VXUNARY0_FUNCT6, OP_MVV, vd, vs1, vs2, mask);                \
+  }
+
+void AssemblerRISCVV::vfmv_vf(VRegister vd, FPURegister fs1, MaskType mask) {
+  GenInstrV(VMV_FUNCT6, OP_FVF, vd, fs1, v0, mask);
+}
+
+void AssemblerRISCVV::vfmv_fs(FPURegister fd, VRegister vs2) {
+  GenInstrV(VWFUNARY0_FUNCT6, OP_FVV, fd, v0, vs2, NoMask);
+}
+
+void AssemblerRISCVV::vfmv_sf(VRegister vd, FPURegister fs) {
+  GenInstrV(VRFUNARY0_FUNCT6, OP_FVF, vd, fs, v0, NoMask);
+}
+
+DEFINE_OPIVV(vadd, VADD_FUNCT6)
+DEFINE_OPIVX(vadd, VADD_FUNCT6)
+DEFINE_OPIVI(vadd, VADD_FUNCT6)
+DEFINE_OPIVV(vsub, VSUB_FUNCT6)
+DEFINE_OPIVX(vsub, VSUB_FUNCT6)
+DEFINE_OPMVX(vdiv, VDIV_FUNCT6)
+DEFINE_OPMVX(vdivu, VDIVU_FUNCT6)
+DEFINE_OPMVX(vmul, VMUL_FUNCT6)
+DEFINE_OPMVX(vmulhu, VMULHU_FUNCT6)
+DEFINE_OPMVX(vmulhsu, VMULHSU_FUNCT6)
+DEFINE_OPMVX(vmulh, VMULH_FUNCT6)
+DEFINE_OPMVV(vdiv, VDIV_FUNCT6)
+DEFINE_OPMVV(vdivu, VDIVU_FUNCT6)
+DEFINE_OPMVV(vmul, VMUL_FUNCT6)
+DEFINE_OPMVV(vmulhu, VMULHU_FUNCT6)
+DEFINE_OPMVV(vmulhsu, VMULHSU_FUNCT6)
+DEFINE_OPMVV(vwmul, VWMUL_FUNCT6)
+DEFINE_OPMVV(vwmulu, VWMULU_FUNCT6)
+DEFINE_OPMVV(vmulh, VMULH_FUNCT6)
+DEFINE_OPMVV(vwadd, VWADD_FUNCT6)
+DEFINE_OPMVV(vwaddu, VWADDU_FUNCT6)
+DEFINE_OPMVV(vcompress, VCOMPRESS_FUNCT6)
+DEFINE_OPIVX(vsadd, VSADD_FUNCT6)
+DEFINE_OPIVV(vsadd, VSADD_FUNCT6)
+DEFINE_OPIVI(vsadd, VSADD_FUNCT6)
+DEFINE_OPIVX(vsaddu, VSADDU_FUNCT6)
+DEFINE_OPIVV(vsaddu, VSADDU_FUNCT6)
+DEFINE_OPIVI(vsaddu, VSADDU_FUNCT6)
+DEFINE_OPIVX(vssub, VSSUB_FUNCT6)
+DEFINE_OPIVV(vssub, VSSUB_FUNCT6)
+DEFINE_OPIVX(vssubu, VSSUBU_FUNCT6)
+DEFINE_OPIVV(vssubu, VSSUBU_FUNCT6)
+DEFINE_OPIVX(vrsub, VRSUB_FUNCT6)
+DEFINE_OPIVI(vrsub, VRSUB_FUNCT6)
+DEFINE_OPIVV(vminu, VMINU_FUNCT6)
+DEFINE_OPIVX(vminu, VMINU_FUNCT6)
+DEFINE_OPIVV(vmin, VMIN_FUNCT6)
+DEFINE_OPIVX(vmin, VMIN_FUNCT6)
+DEFINE_OPIVV(vmaxu, VMAXU_FUNCT6)
+DEFINE_OPIVX(vmaxu, VMAXU_FUNCT6)
+DEFINE_OPIVV(vmax, VMAX_FUNCT6)
+DEFINE_OPIVX(vmax, VMAX_FUNCT6)
+DEFINE_OPIVV(vand, VAND_FUNCT6)
+DEFINE_OPIVX(vand, VAND_FUNCT6)
+DEFINE_OPIVI(vand, VAND_FUNCT6)
+DEFINE_OPIVV(vor, VOR_FUNCT6)
+DEFINE_OPIVX(vor, VOR_FUNCT6)
+DEFINE_OPIVI(vor, VOR_FUNCT6)
+DEFINE_OPIVV(vxor, VXOR_FUNCT6)
+DEFINE_OPIVX(vxor, VXOR_FUNCT6)
+DEFINE_OPIVI(vxor, VXOR_FUNCT6)
+
+DEFINE_OPIVX(vslidedown, VSLIDEDOWN_FUNCT6)
+DEFINE_OPIVI(vslidedown, VSLIDEDOWN_FUNCT6)
+DEFINE_OPIVX(vslideup, VSLIDEUP_FUNCT6)
+DEFINE_OPIVI(vslideup, VSLIDEUP_FUNCT6)
+
+DEFINE_OPIVV(vmseq, VMSEQ_FUNCT6)
+DEFINE_OPIVX(vmseq, VMSEQ_FUNCT6)
+DEFINE_OPIVI(vmseq, VMSEQ_FUNCT6)
+
+DEFINE_OPIVV(vmsne, VMSNE_FUNCT6)
+DEFINE_OPIVX(vmsne, VMSNE_FUNCT6)
+DEFINE_OPIVI(vmsne, VMSNE_FUNCT6)
+
+DEFINE_OPIVV(vmsltu, VMSLTU_FUNCT6)
+DEFINE_OPIVX(vmsltu, VMSLTU_FUNCT6)
+
+DEFINE_OPIVV(vmslt, VMSLT_FUNCT6)
+DEFINE_OPIVX(vmslt, VMSLT_FUNCT6)
+
+DEFINE_OPIVV(vmsle, VMSLE_FUNCT6)
+DEFINE_OPIVX(vmsle, VMSLE_FUNCT6)
+DEFINE_OPIVI(vmsle, VMSLE_FUNCT6)
+
+DEFINE_OPIVV(vmsleu, VMSLEU_FUNCT6)
+DEFINE_OPIVX(vmsleu, VMSLEU_FUNCT6)
+DEFINE_OPIVI(vmsleu, VMSLEU_FUNCT6)
+
+DEFINE_OPIVI(vmsgt, VMSGT_FUNCT6)
+DEFINE_OPIVX(vmsgt, VMSGT_FUNCT6)
+
+DEFINE_OPIVI(vmsgtu, VMSGTU_FUNCT6)
+DEFINE_OPIVX(vmsgtu, VMSGTU_FUNCT6)
+
+DEFINE_OPIVV(vsrl, VSRL_FUNCT6)
+DEFINE_OPIVX(vsrl, VSRL_FUNCT6)
+DEFINE_OPIVI(vsrl, VSRL_FUNCT6)
+
+DEFINE_OPIVV(vsra, VSRA_FUNCT6)
+DEFINE_OPIVX(vsra, VSRA_FUNCT6)
+DEFINE_OPIVI(vsra, VSRA_FUNCT6)
+
+DEFINE_OPIVV(vsll, VSLL_FUNCT6)
+DEFINE_OPIVX(vsll, VSLL_FUNCT6)
+DEFINE_OPIVI(vsll, VSLL_FUNCT6)
+
+DEFINE_OPIVV(vsmul, VSMUL_FUNCT6)
+DEFINE_OPIVX(vsmul, VSMUL_FUNCT6)
+
+DEFINE_OPFVV(vfadd, VFADD_FUNCT6)
+DEFINE_OPFVF(vfadd, VFADD_FUNCT6)
+DEFINE_OPFVV(vfsub, VFSUB_FUNCT6)
+DEFINE_OPFVF(vfsub, VFSUB_FUNCT6)
+DEFINE_OPFVV(vfdiv, VFDIV_FUNCT6)
+DEFINE_OPFVF(vfdiv, VFDIV_FUNCT6)
+DEFINE_OPFVV(vfmul, VFMUL_FUNCT6)
+DEFINE_OPFVF(vfmul, VFMUL_FUNCT6)
+DEFINE_OPFVV(vmfeq, VMFEQ_FUNCT6)
+DEFINE_OPFVV(vmfne, VMFNE_FUNCT6)
+DEFINE_OPFVV(vmflt, VMFLT_FUNCT6)
+DEFINE_OPFVV(vmfle, VMFLE_FUNCT6)
+DEFINE_OPFVV(vfmax, VFMAX_FUNCT6)
+DEFINE_OPFVV(vfmin, VFMIN_FUNCT6)
+
+// Vector Widening Floating-Point Add/Subtract Instructions
+DEFINE_OPFVV(vfwadd, VFWADD_FUNCT6)
+DEFINE_OPFVF(vfwadd, VFWADD_FUNCT6)
+DEFINE_OPFVV(vfwsub, VFWSUB_FUNCT6)
+DEFINE_OPFVF(vfwsub, VFWSUB_FUNCT6)
+DEFINE_OPFWV(vfwadd, VFWADD_W_FUNCT6)
+DEFINE_OPFWF(vfwadd, VFWADD_W_FUNCT6)
+DEFINE_OPFWV(vfwsub, VFWSUB_W_FUNCT6)
+DEFINE_OPFWF(vfwsub, VFWSUB_W_FUNCT6)
+
+// Vector Widening Floating-Point Reduction Instructions
+DEFINE_OPFVV(vfwredusum, VFWREDUSUM_FUNCT6)
+DEFINE_OPFVV(vfwredosum, VFWREDOSUM_FUNCT6)
+
+// Vector Widening Floating-Point Multiply
+DEFINE_OPFVV(vfwmul, VFWMUL_FUNCT6)
+DEFINE_OPFVF(vfwmul, VFWMUL_FUNCT6)
+
+DEFINE_OPFRED(vfredmax, VFREDMAX_FUNCT6)
+
+DEFINE_OPFVV(vfsngj, VFSGNJ_FUNCT6)
+DEFINE_OPFVF(vfsngj, VFSGNJ_FUNCT6)
+DEFINE_OPFVV(vfsngjn, VFSGNJN_FUNCT6)
+DEFINE_OPFVF(vfsngjn, VFSGNJN_FUNCT6)
+DEFINE_OPFVV(vfsngjx, VFSGNJX_FUNCT6)
+DEFINE_OPFVF(vfsngjx, VFSGNJX_FUNCT6)
+
+// Vector Single-Width Floating-Point Fused Multiply-Add Instructions
+DEFINE_OPFVV_FMA(vfmadd, VFMADD_FUNCT6)
+DEFINE_OPFVF_FMA(vfmadd, VFMADD_FUNCT6)
+DEFINE_OPFVV_FMA(vfmsub, VFMSUB_FUNCT6)
+DEFINE_OPFVF_FMA(vfmsub, VFMSUB_FUNCT6)
+DEFINE_OPFVV_FMA(vfmacc, VFMACC_FUNCT6)
+DEFINE_OPFVF_FMA(vfmacc, VFMACC_FUNCT6)
+DEFINE_OPFVV_FMA(vfmsac, VFMSAC_FUNCT6)
+DEFINE_OPFVF_FMA(vfmsac, VFMSAC_FUNCT6)
+DEFINE_OPFVV_FMA(vfnmadd, VFNMADD_FUNCT6)
+DEFINE_OPFVF_FMA(vfnmadd, VFNMADD_FUNCT6)
+DEFINE_OPFVV_FMA(vfnmsub, VFNMSUB_FUNCT6)
+DEFINE_OPFVF_FMA(vfnmsub, VFNMSUB_FUNCT6)
+DEFINE_OPFVV_FMA(vfnmacc, VFNMACC_FUNCT6)
+DEFINE_OPFVF_FMA(vfnmacc, VFNMACC_FUNCT6)
+DEFINE_OPFVV_FMA(vfnmsac, VFNMSAC_FUNCT6)
+DEFINE_OPFVF_FMA(vfnmsac, VFNMSAC_FUNCT6)
+
+// Vector Widening Floating-Point Fused Multiply-Add Instructions
+DEFINE_OPFVV_FMA(vfwmacc, VFWMACC_FUNCT6)
+DEFINE_OPFVF_FMA(vfwmacc, VFWMACC_FUNCT6)
+DEFINE_OPFVV_FMA(vfwnmacc, VFWNMACC_FUNCT6)
+DEFINE_OPFVF_FMA(vfwnmacc, VFWNMACC_FUNCT6)
+DEFINE_OPFVV_FMA(vfwmsac, VFWMSAC_FUNCT6)
+DEFINE_OPFVF_FMA(vfwmsac, VFWMSAC_FUNCT6)
+DEFINE_OPFVV_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
+DEFINE_OPFVF_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
+
+// Vector Narrowing Fixed-Point Clip Instructions
+DEFINE_OPIVV(vnclip, VNCLIP_FUNCT6)
+DEFINE_OPIVX(vnclip, VNCLIP_FUNCT6)
+DEFINE_OPIVI(vnclip, VNCLIP_FUNCT6)
+DEFINE_OPIVV(vnclipu, VNCLIPU_FUNCT6)
+DEFINE_OPIVX(vnclipu, VNCLIPU_FUNCT6)
+DEFINE_OPIVI(vnclipu, VNCLIPU_FUNCT6)
+
+// Vector Integer Extension
+DEFINE_OPMVV_VIE(vzext_vf8, 0b00010)
+DEFINE_OPMVV_VIE(vsext_vf8, 0b00011)
+DEFINE_OPMVV_VIE(vzext_vf4, 0b00100)
+DEFINE_OPMVV_VIE(vsext_vf4, 0b00101)
+DEFINE_OPMVV_VIE(vzext_vf2, 0b00110)
+DEFINE_OPMVV_VIE(vsext_vf2, 0b00111)
+
+#undef DEFINE_OPIVI
+#undef DEFINE_OPIVV
+#undef DEFINE_OPIVX
+#undef DEFINE_OPFVV
+#undef DEFINE_OPFWV
+#undef DEFINE_OPFVF
+#undef DEFINE_OPFWF
+#undef DEFINE_OPFVV_FMA
+#undef DEFINE_OPFVF_FMA
+#undef DEFINE_OPMVV_VIE
+
+void AssemblerRISCVV::vsetvli(Register rd, Register rs1, VSew vsew, Vlmul vlmul,
+                              TailAgnosticType tail, MaskAgnosticType mask) {
+  int32_t zimm = GenZimm(vsew, vlmul, tail, mask);
+  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
+                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
+                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x0 << 31;
+  emit(instr);
+}
+
+void AssemblerRISCVV::vsetivli(Register rd, uint8_t uimm, VSew vsew,
+                               Vlmul vlmul, TailAgnosticType tail,
+                               MaskAgnosticType mask) {
+  DCHECK(is_uint5(uimm));
+  int32_t zimm = GenZimm(vsew, vlmul, tail, mask) & 0x3FF;
+  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
+                ((uimm & 0x1F) << kRvvUimmShift) |
+                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x3 << 30;
+  emit(instr);
+}
+
+void AssemblerRISCVV::vsetvl(Register rd, Register rs1, Register rs2) {
+  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
+                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
+                ((rs2.code() & 0x1F) << kRvvRs2Shift) | 0x40 << 25;
+  emit(instr);
+}
+
+uint8_t vsew_switch(VSew vsew) {
+  uint8_t width;
+  switch (vsew) {
+    case E8:
+      width = 0b000;
+      break;
+    case E16:
+      width = 0b101;
+      break;
+    case E32:
+      width = 0b110;
+      break;
+    default:
+      width = 0b111;
+      break;
+  }
+  return width;
+}
+
+// OPIVV OPFVV OPMVV
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                VRegister vd, VRegister vs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((vd.code() & 0x1F) << kRvvVdShift) |
+                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                VRegister vd, int8_t vs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((vd.code() & 0x1F) << kRvvVdShift) |
+                ((vs1 & 0x1F) << kRvvVs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+// OPMVV OPFVV
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                Register rd, VRegister vs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_MVV || opcode == OP_FVV);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((rd.code() & 0x1F) << kRvvVdShift) |
+                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+// OPFVV
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                FPURegister fd, VRegister vs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_FVV);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((fd.code() & 0x1F) << kRvvVdShift) |
+                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+// OPIVX OPMVX
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                VRegister vd, Register rs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_IVX || opcode == OP_MVX);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((vd.code() & 0x1F) << kRvvVdShift) |
+                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+// OPFVF
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                VRegister vd, FPURegister fs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_FVF);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((vd.code() & 0x1F) << kRvvVdShift) |
+                ((fs1.code() & 0x1F) << kRvvRs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+// OPMVX
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, Register rd, Register rs1,
+                                VRegister vs2, MaskType mask) {
+  Instr instr = (funct6 << kRvvFunct6Shift) | OP_MVX | (mask << kRvvVmShift) |
+                ((rd.code() & 0x1F) << kRvvVdShift) |
+                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+// OPIVI
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, VRegister vd, int8_t imm5,
+                                VRegister vs2, MaskType mask) {
+  DCHECK(is_uint5(imm5) || is_int5(imm5));
+  Instr instr = (funct6 << kRvvFunct6Shift) | OP_IVI | (mask << kRvvVmShift) |
+                ((vd.code() & 0x1F) << kRvvVdShift) |
+                (((uint32_t)imm5 << kRvvImm5Shift) & kRvvImm5Mask) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+// VL VS
+void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
+                                Register rs1, uint8_t umop, MaskType mask,
+                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
+  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
+  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
+                ((width << kRvvWidthShift) & kRvvWidthMask) |
+                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
+                ((umop << kRvvRs2Shift) & kRvvRs2Mask) |
+                ((mask << kRvvVmShift) & kRvvVmMask) |
+                ((IsMop << kRvvMopShift) & kRvvMopMask) |
+                ((IsMew << kRvvMewShift) & kRvvMewMask) |
+                ((Nf << kRvvNfShift) & kRvvNfMask);
+  emit(instr);
+}
+void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
+                                Register rs1, Register rs2, MaskType mask,
+                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
+  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
+  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
+                ((width << kRvvWidthShift) & kRvvWidthMask) |
+                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
+                ((rs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
+                ((mask << kRvvVmShift) & kRvvVmMask) |
+                ((IsMop << kRvvMopShift) & kRvvMopMask) |
+                ((IsMew << kRvvMewShift) & kRvvMewMask) |
+                ((Nf << kRvvNfShift) & kRvvNfMask);
+  emit(instr);
+}
+// VL VS AMO
+void AssemblerRISCVV::GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd,
+                                Register rs1, VRegister vs2, MaskType mask,
+                                uint8_t IsMop, bool IsMew, uint8_t Nf) {
+  DCHECK(opcode == LOAD_FP || opcode == STORE_FP || opcode == AMO);
+  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
+                ((width << kRvvWidthShift) & kRvvWidthMask) |
+                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
+                ((vs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
+                ((mask << kRvvVmShift) & kRvvVmMask) |
+                ((IsMop << kRvvMopShift) & kRvvMopMask) |
+                ((IsMew << kRvvMewShift) & kRvvMewMask) |
+                ((Nf << kRvvNfShift) & kRvvNfMask);
+  emit(instr);
+}
+// vmv_xs vcpop_m vfirst_m
+void AssemblerRISCVV::GenInstrV(uint8_t funct6, OpcodeRISCVV opcode,
+                                Register rd, uint8_t vs1, VRegister vs2,
+                                MaskType mask) {
+  DCHECK(opcode == OP_MVV);
+  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
+                ((rd.code() & 0x1F) << kRvvVdShift) |
+                ((vs1 & 0x1F) << kRvvVs1Shift) |
+                ((vs2.code() & 0x1F) << kRvvVs2Shift);
+  emit(instr);
+}
+
+void AssemblerRISCVV::vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
+                         MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b000);
+}
+void AssemblerRISCVV::vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
+                          MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b000);
+}
+void AssemblerRISCVV::vlx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
+                          MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0);
+}
+
+void AssemblerRISCVV::vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
+                         MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b000);
+}
+void AssemblerRISCVV::vss(VRegister vs3, Register rs1, Register rs2, VSew vsew,
+                          MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vs3, rs1, rs2, mask, 0b10, 0, 0b000);
+}
+
+void AssemblerRISCVV::vsx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
+                          MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0b000);
+}
+void AssemblerRISCVV::vsu(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
+                          MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b01, 0, 0b000);
+}
+
+void AssemblerRISCVV::vlseg2(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b001);
+}
+
+void AssemblerRISCVV::vlseg3(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b010);
+}
+
+void AssemblerRISCVV::vlseg4(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b011);
+}
+
+void AssemblerRISCVV::vlseg5(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b100);
+}
+
+void AssemblerRISCVV::vlseg6(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b101);
+}
+
+void AssemblerRISCVV::vlseg7(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b110);
+}
+
+void AssemblerRISCVV::vlseg8(VRegister vd, Register rs1, uint8_t lumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b111);
+}
+void AssemblerRISCVV::vsseg2(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b001);
+}
+void AssemblerRISCVV::vsseg3(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b010);
+}
+void AssemblerRISCVV::vsseg4(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b011);
+}
+void AssemblerRISCVV::vsseg5(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b100);
+}
+void AssemblerRISCVV::vsseg6(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b101);
+}
+void AssemblerRISCVV::vsseg7(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b110);
+}
+void AssemblerRISCVV::vsseg8(VRegister vd, Register rs1, uint8_t sumop,
+                             VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b111);
+}
+
+void AssemblerRISCVV::vlsseg2(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
+}
+void AssemblerRISCVV::vlsseg3(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
+}
+void AssemblerRISCVV::vlsseg4(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
+}
+void AssemblerRISCVV::vlsseg5(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
+}
+void AssemblerRISCVV::vlsseg6(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
+}
+void AssemblerRISCVV::vlsseg7(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
+}
+void AssemblerRISCVV::vlsseg8(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
+}
+void AssemblerRISCVV::vssseg2(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
+}
+void AssemblerRISCVV::vssseg3(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
+}
+void AssemblerRISCVV::vssseg4(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
+}
+void AssemblerRISCVV::vssseg5(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
+}
+void AssemblerRISCVV::vssseg6(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
+}
+void AssemblerRISCVV::vssseg7(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
+}
+void AssemblerRISCVV::vssseg8(VRegister vd, Register rs1, Register rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
+}
+
+void AssemblerRISCVV::vlxseg2(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
+}
+void AssemblerRISCVV::vlxseg3(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
+}
+void AssemblerRISCVV::vlxseg4(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
+}
+void AssemblerRISCVV::vlxseg5(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
+}
+void AssemblerRISCVV::vlxseg6(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
+}
+void AssemblerRISCVV::vlxseg7(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
+}
+void AssemblerRISCVV::vlxseg8(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
+}
+void AssemblerRISCVV::vsxseg2(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
+}
+void AssemblerRISCVV::vsxseg3(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
+}
+void AssemblerRISCVV::vsxseg4(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
+}
+void AssemblerRISCVV::vsxseg5(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
+}
+void AssemblerRISCVV::vsxseg6(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
+}
+void AssemblerRISCVV::vsxseg7(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
+}
+void AssemblerRISCVV::vsxseg8(VRegister vd, Register rs1, VRegister rs2,
+                              VSew vsew, MaskType mask) {
+  uint8_t width = vsew_switch(vsew);
+  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
+}
+
+void AssemblerRISCVV::vfirst_m(Register rd, VRegister vs2, MaskType mask) {
+  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10001, vs2, mask);
+}
+
+void AssemblerRISCVV::vcpop_m(Register rd, VRegister vs2, MaskType mask) {
+  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10000, vs2, mask);
+}
+
+LoadStoreLaneParams::LoadStoreLaneParams(MachineRepresentation rep,
+                                         uint8_t laneidx) {
+  switch (rep) {
+    case MachineRepresentation::kWord8:
+      *this = LoadStoreLaneParams(laneidx, 8, kRvvVLEN / 16);
+      break;
+    case MachineRepresentation::kWord16:
+      *this = LoadStoreLaneParams(laneidx, 16, kRvvVLEN / 8);
+      break;
+    case MachineRepresentation::kWord32:
+      *this = LoadStoreLaneParams(laneidx, 32, kRvvVLEN / 4);
+      break;
+    case MachineRepresentation::kWord64:
+      *this = LoadStoreLaneParams(laneidx, 64, kRvvVLEN / 2);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-v.h b/src/codegen/riscv/extension-riscv-v.h
new file mode 100644
index 00000000000..2682f6c0452
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-v.h
@@ -0,0 +1,485 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_V_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_V_H_
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-v.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+namespace v8 {
+namespace internal {
+
+class AssemblerRISCVV : public AssemblerRiscvBase {
+ public:
+  // RVV
+  static int32_t GenZimm(VSew vsew, Vlmul vlmul, TailAgnosticType tail = tu,
+                         MaskAgnosticType mask = mu) {
+    return (mask << 7) | (tail << 6) | ((vsew & 0x7) << 3) | (vlmul & 0x7);
+  }
+
+  void vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
+          MaskType mask = NoMask);
+  void vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
+           MaskType mask = NoMask);
+  void vlx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
+           MaskType mask = NoMask);
+
+  void vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
+          MaskType mask = NoMask);
+  void vss(VRegister vd, Register rs1, Register rs2, VSew vsew,
+           MaskType mask = NoMask);
+  void vsx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
+           MaskType mask = NoMask);
+
+  void vsu(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
+           MaskType mask = NoMask);
+
+#define SegInstr(OP)  \
+  void OP##seg2(ARG); \
+  void OP##seg3(ARG); \
+  void OP##seg4(ARG); \
+  void OP##seg5(ARG); \
+  void OP##seg6(ARG); \
+  void OP##seg7(ARG); \
+  void OP##seg8(ARG);
+
+#define ARG \
+  VRegister vd, Register rs1, uint8_t lumop, VSew vsew, MaskType mask = NoMask
+
+  SegInstr(vl) SegInstr(vs)
+#undef ARG
+
+#define ARG \
+  VRegister vd, Register rs1, Register rs2, VSew vsew, MaskType mask = NoMask
+
+      SegInstr(vls) SegInstr(vss)
+#undef ARG
+
+#define ARG \
+  VRegister vd, Register rs1, VRegister rs2, VSew vsew, MaskType mask = NoMask
+
+          SegInstr(vsx) SegInstr(vlx)
+#undef ARG
+#undef SegInstr
+
+      // RVV Vector Arithmetic Instruction
+
+      void vmv_vv(VRegister vd, VRegister vs1);
+  void vmv_vx(VRegister vd, Register rs1);
+  void vmv_vi(VRegister vd, uint8_t simm5);
+  void vmv_xs(Register rd, VRegister vs2);
+  void vmv_sx(VRegister vd, Register rs1);
+  void vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2);
+  void vmerge_vx(VRegister vd, Register rs1, VRegister vs2);
+  void vmerge_vi(VRegister vd, uint8_t imm5, VRegister vs2);
+
+  void vredmaxu_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                   MaskType mask = NoMask);
+  void vredmax_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                  MaskType mask = NoMask);
+  void vredmin_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                  MaskType mask = NoMask);
+  void vredminu_vs(VRegister vd, VRegister vs2, VRegister vs1,
+                   MaskType mask = NoMask);
+
+  void vadc_vv(VRegister vd, VRegister vs1, VRegister vs2);
+  void vadc_vx(VRegister vd, Register rs1, VRegister vs2);
+  void vadc_vi(VRegister vd, uint8_t imm5, VRegister vs2);
+
+  void vmadc_vv(VRegister vd, VRegister vs1, VRegister vs2);
+  void vmadc_vx(VRegister vd, Register rs1, VRegister vs2);
+  void vmadc_vi(VRegister vd, uint8_t imm5, VRegister vs2);
+
+  void vfmv_vf(VRegister vd, FPURegister fs1, MaskType mask = NoMask);
+  void vfmv_fs(FPURegister fd, VRegister vs2);
+  void vfmv_sf(VRegister vd, FPURegister fs);
+
+  void vwaddu_wx(VRegister vd, VRegister vs2, Register rs1,
+                 MaskType mask = NoMask);
+  void vid_v(VRegister vd, MaskType mask = Mask);
+
+#define DEFINE_OPIVV(name, funct6)                           \
+  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPIVX(name, funct6)                          \
+  void name##_vx(VRegister vd, VRegister vs2, Register rs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPIVI(name, funct6)                         \
+  void name##_vi(VRegister vd, VRegister vs2, int8_t imm5, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPMVV(name, funct6)                           \
+  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPMVX(name, funct6)                          \
+  void name##_vx(VRegister vd, VRegister vs2, Register rs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFVV(name, funct6)                           \
+  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFWV(name, funct6)                           \
+  void name##_wv(VRegister vd, VRegister vs2, VRegister vs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFRED(name, funct6)                          \
+  void name##_vs(VRegister vd, VRegister vs2, VRegister vs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFVF(name, funct6)                             \
+  void name##_vf(VRegister vd, VRegister vs2, FPURegister fs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFWF(name, funct6)                             \
+  void name##_wf(VRegister vd, VRegister vs2, FPURegister fs1, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFVV_FMA(name, funct6)                       \
+  void name##_vv(VRegister vd, VRegister vs1, VRegister vs2, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPFVF_FMA(name, funct6)                         \
+  void name##_vf(VRegister vd, FPURegister fs1, VRegister vs2, \
+                 MaskType mask = NoMask);
+
+#define DEFINE_OPMVV_VIE(name) \
+  void name(VRegister vd, VRegister vs2, MaskType mask = NoMask);
+
+  DEFINE_OPIVV(vadd, VADD_FUNCT6)
+  DEFINE_OPIVX(vadd, VADD_FUNCT6)
+  DEFINE_OPIVI(vadd, VADD_FUNCT6)
+  DEFINE_OPIVV(vsub, VSUB_FUNCT6)
+  DEFINE_OPIVX(vsub, VSUB_FUNCT6)
+  DEFINE_OPMVX(vdiv, VDIV_FUNCT6)
+  DEFINE_OPMVX(vdivu, VDIVU_FUNCT6)
+  DEFINE_OPMVX(vmul, VMUL_FUNCT6)
+  DEFINE_OPMVX(vmulhu, VMULHU_FUNCT6)
+  DEFINE_OPMVX(vmulhsu, VMULHSU_FUNCT6)
+  DEFINE_OPMVX(vmulh, VMULH_FUNCT6)
+  DEFINE_OPMVV(vdiv, VDIV_FUNCT6)
+  DEFINE_OPMVV(vdivu, VDIVU_FUNCT6)
+  DEFINE_OPMVV(vmul, VMUL_FUNCT6)
+  DEFINE_OPMVV(vmulhu, VMULHU_FUNCT6)
+  DEFINE_OPMVV(vmulhsu, VMULHSU_FUNCT6)
+  DEFINE_OPMVV(vmulh, VMULH_FUNCT6)
+  DEFINE_OPMVV(vwmul, VWMUL_FUNCT6)
+  DEFINE_OPMVV(vwmulu, VWMULU_FUNCT6)
+  DEFINE_OPMVV(vwaddu, VWADDU_FUNCT6)
+  DEFINE_OPMVV(vwadd, VWADD_FUNCT6)
+  DEFINE_OPMVV(vcompress, VCOMPRESS_FUNCT6)
+  DEFINE_OPIVX(vsadd, VSADD_FUNCT6)
+  DEFINE_OPIVV(vsadd, VSADD_FUNCT6)
+  DEFINE_OPIVI(vsadd, VSADD_FUNCT6)
+  DEFINE_OPIVX(vsaddu, VSADD_FUNCT6)
+  DEFINE_OPIVV(vsaddu, VSADDU_FUNCT6)
+  DEFINE_OPIVI(vsaddu, VSADDU_FUNCT6)
+  DEFINE_OPIVX(vssub, VSSUB_FUNCT6)
+  DEFINE_OPIVV(vssub, VSSUB_FUNCT6)
+  DEFINE_OPIVX(vssubu, VSSUBU_FUNCT6)
+  DEFINE_OPIVV(vssubu, VSSUBU_FUNCT6)
+  DEFINE_OPIVX(vrsub, VRSUB_FUNCT6)
+  DEFINE_OPIVI(vrsub, VRSUB_FUNCT6)
+  DEFINE_OPIVV(vminu, VMINU_FUNCT6)
+  DEFINE_OPIVX(vminu, VMINU_FUNCT6)
+  DEFINE_OPIVV(vmin, VMIN_FUNCT6)
+  DEFINE_OPIVX(vmin, VMIN_FUNCT6)
+  DEFINE_OPIVV(vmaxu, VMAXU_FUNCT6)
+  DEFINE_OPIVX(vmaxu, VMAXU_FUNCT6)
+  DEFINE_OPIVV(vmax, VMAX_FUNCT6)
+  DEFINE_OPIVX(vmax, VMAX_FUNCT6)
+  DEFINE_OPIVV(vand, VAND_FUNCT6)
+  DEFINE_OPIVX(vand, VAND_FUNCT6)
+  DEFINE_OPIVI(vand, VAND_FUNCT6)
+  DEFINE_OPIVV(vor, VOR_FUNCT6)
+  DEFINE_OPIVX(vor, VOR_FUNCT6)
+  DEFINE_OPIVI(vor, VOR_FUNCT6)
+  DEFINE_OPIVV(vxor, VXOR_FUNCT6)
+  DEFINE_OPIVX(vxor, VXOR_FUNCT6)
+  DEFINE_OPIVI(vxor, VXOR_FUNCT6)
+  DEFINE_OPIVV(vrgather, VRGATHER_FUNCT6)
+  DEFINE_OPIVX(vrgather, VRGATHER_FUNCT6)
+  DEFINE_OPIVI(vrgather, VRGATHER_FUNCT6)
+
+  DEFINE_OPIVX(vslidedown, VSLIDEDOWN_FUNCT6)
+  DEFINE_OPIVI(vslidedown, VSLIDEDOWN_FUNCT6)
+  DEFINE_OPIVX(vslideup, VSLIDEUP_FUNCT6)
+  DEFINE_OPIVI(vslideup, VSLIDEUP_FUNCT6)
+
+  DEFINE_OPIVV(vmseq, VMSEQ_FUNCT6)
+  DEFINE_OPIVX(vmseq, VMSEQ_FUNCT6)
+  DEFINE_OPIVI(vmseq, VMSEQ_FUNCT6)
+
+  DEFINE_OPIVV(vmsne, VMSNE_FUNCT6)
+  DEFINE_OPIVX(vmsne, VMSNE_FUNCT6)
+  DEFINE_OPIVI(vmsne, VMSNE_FUNCT6)
+
+  DEFINE_OPIVV(vmsltu, VMSLTU_FUNCT6)
+  DEFINE_OPIVX(vmsltu, VMSLTU_FUNCT6)
+
+  DEFINE_OPIVV(vmslt, VMSLT_FUNCT6)
+  DEFINE_OPIVX(vmslt, VMSLT_FUNCT6)
+
+  DEFINE_OPIVV(vmsle, VMSLE_FUNCT6)
+  DEFINE_OPIVX(vmsle, VMSLE_FUNCT6)
+  DEFINE_OPIVI(vmsle, VMSLE_FUNCT6)
+
+  DEFINE_OPIVV(vmsleu, VMSLEU_FUNCT6)
+  DEFINE_OPIVX(vmsleu, VMSLEU_FUNCT6)
+  DEFINE_OPIVI(vmsleu, VMSLEU_FUNCT6)
+
+  DEFINE_OPIVI(vmsgt, VMSGT_FUNCT6)
+  DEFINE_OPIVX(vmsgt, VMSGT_FUNCT6)
+
+  DEFINE_OPIVI(vmsgtu, VMSGTU_FUNCT6)
+  DEFINE_OPIVX(vmsgtu, VMSGTU_FUNCT6)
+
+  DEFINE_OPIVV(vsrl, VSRL_FUNCT6)
+  DEFINE_OPIVX(vsrl, VSRL_FUNCT6)
+  DEFINE_OPIVI(vsrl, VSRL_FUNCT6)
+
+  DEFINE_OPIVV(vsra, VSRA_FUNCT6)
+  DEFINE_OPIVX(vsra, VSRA_FUNCT6)
+  DEFINE_OPIVI(vsra, VSRA_FUNCT6)
+
+  DEFINE_OPIVV(vsll, VSLL_FUNCT6)
+  DEFINE_OPIVX(vsll, VSLL_FUNCT6)
+  DEFINE_OPIVI(vsll, VSLL_FUNCT6)
+
+  DEFINE_OPIVV(vsmul, VSMUL_FUNCT6)
+  DEFINE_OPIVX(vsmul, VSMUL_FUNCT6)
+
+  DEFINE_OPFVV(vfadd, VFADD_FUNCT6)
+  DEFINE_OPFVF(vfadd, VFADD_FUNCT6)
+  DEFINE_OPFVV(vfsub, VFSUB_FUNCT6)
+  DEFINE_OPFVF(vfsub, VFSUB_FUNCT6)
+  DEFINE_OPFVV(vfdiv, VFDIV_FUNCT6)
+  DEFINE_OPFVF(vfdiv, VFDIV_FUNCT6)
+  DEFINE_OPFVV(vfmul, VFMUL_FUNCT6)
+  DEFINE_OPFVF(vfmul, VFMUL_FUNCT6)
+
+  // Vector Widening Floating-Point Add/Subtract Instructions
+  DEFINE_OPFVV(vfwadd, VFWADD_FUNCT6)
+  DEFINE_OPFVF(vfwadd, VFWADD_FUNCT6)
+  DEFINE_OPFVV(vfwsub, VFWSUB_FUNCT6)
+  DEFINE_OPFVF(vfwsub, VFWSUB_FUNCT6)
+  DEFINE_OPFWV(vfwadd, VFWADD_W_FUNCT6)
+  DEFINE_OPFWF(vfwadd, VFWADD_W_FUNCT6)
+  DEFINE_OPFWV(vfwsub, VFWSUB_W_FUNCT6)
+  DEFINE_OPFWF(vfwsub, VFWSUB_W_FUNCT6)
+
+  // Vector Widening Floating-Point Reduction Instructions
+  DEFINE_OPFVV(vfwredusum, VFWREDUSUM_FUNCT6)
+  DEFINE_OPFVV(vfwredosum, VFWREDOSUM_FUNCT6)
+
+  // Vector Widening Floating-Point Multiply
+  DEFINE_OPFVV(vfwmul, VFWMUL_FUNCT6)
+  DEFINE_OPFVF(vfwmul, VFWMUL_FUNCT6)
+
+  DEFINE_OPFVV(vmfeq, VMFEQ_FUNCT6)
+  DEFINE_OPFVV(vmfne, VMFNE_FUNCT6)
+  DEFINE_OPFVV(vmflt, VMFLT_FUNCT6)
+  DEFINE_OPFVV(vmfle, VMFLE_FUNCT6)
+  DEFINE_OPFVV(vfmax, VMFMAX_FUNCT6)
+  DEFINE_OPFVV(vfmin, VMFMIN_FUNCT6)
+  DEFINE_OPFRED(vfredmax, VFREDMAX_FUNCT6)
+
+  DEFINE_OPFVV(vfsngj, VFSGNJ_FUNCT6)
+  DEFINE_OPFVF(vfsngj, VFSGNJ_FUNCT6)
+  DEFINE_OPFVV(vfsngjn, VFSGNJN_FUNCT6)
+  DEFINE_OPFVF(vfsngjn, VFSGNJN_FUNCT6)
+  DEFINE_OPFVV(vfsngjx, VFSGNJX_FUNCT6)
+  DEFINE_OPFVF(vfsngjx, VFSGNJX_FUNCT6)
+
+  // Vector Single-Width Floating-Point Fused Multiply-Add Instructions
+  DEFINE_OPFVV_FMA(vfmadd, VFMADD_FUNCT6)
+  DEFINE_OPFVF_FMA(vfmadd, VFMADD_FUNCT6)
+  DEFINE_OPFVV_FMA(vfmsub, VFMSUB_FUNCT6)
+  DEFINE_OPFVF_FMA(vfmsub, VFMSUB_FUNCT6)
+  DEFINE_OPFVV_FMA(vfmacc, VFMACC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfmacc, VFMACC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfmsac, VFMSAC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfmsac, VFMSAC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfnmadd, VFNMADD_FUNCT6)
+  DEFINE_OPFVF_FMA(vfnmadd, VFNMADD_FUNCT6)
+  DEFINE_OPFVV_FMA(vfnmsub, VFNMSUB_FUNCT6)
+  DEFINE_OPFVF_FMA(vfnmsub, VFNMSUB_FUNCT6)
+  DEFINE_OPFVV_FMA(vfnmacc, VFNMACC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfnmacc, VFNMACC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfnmsac, VFNMSAC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfnmsac, VFNMSAC_FUNCT6)
+
+  // Vector Widening Floating-Point Fused Multiply-Add Instructions
+  DEFINE_OPFVV_FMA(vfwmacc, VFWMACC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfwmacc, VFWMACC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfwnmacc, VFWNMACC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfwnmacc, VFWNMACC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfwmsac, VFWMSAC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfwmsac, VFWMSAC_FUNCT6)
+  DEFINE_OPFVV_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
+  DEFINE_OPFVF_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
+
+  // Vector Narrowing Fixed-Point Clip Instructions
+  DEFINE_OPIVV(vnclip, VNCLIP_FUNCT6)
+  DEFINE_OPIVX(vnclip, VNCLIP_FUNCT6)
+  DEFINE_OPIVI(vnclip, VNCLIP_FUNCT6)
+  DEFINE_OPIVV(vnclipu, VNCLIPU_FUNCT6)
+  DEFINE_OPIVX(vnclipu, VNCLIPU_FUNCT6)
+  DEFINE_OPIVI(vnclipu, VNCLIPU_FUNCT6)
+
+  // Vector Integer Extension
+  DEFINE_OPMVV_VIE(vzext_vf8)
+  DEFINE_OPMVV_VIE(vsext_vf8)
+  DEFINE_OPMVV_VIE(vzext_vf4)
+  DEFINE_OPMVV_VIE(vsext_vf4)
+  DEFINE_OPMVV_VIE(vzext_vf2)
+  DEFINE_OPMVV_VIE(vsext_vf2)
+
+#undef DEFINE_OPIVI
+#undef DEFINE_OPIVV
+#undef DEFINE_OPIVX
+#undef DEFINE_OPMVV
+#undef DEFINE_OPMVX
+#undef DEFINE_OPFVV
+#undef DEFINE_OPFWV
+#undef DEFINE_OPFVF
+#undef DEFINE_OPFWF
+#undef DEFINE_OPFVV_FMA
+#undef DEFINE_OPFVF_FMA
+#undef DEFINE_OPMVV_VIE
+#undef DEFINE_OPFRED
+
+#define DEFINE_VFUNARY(name, funct6, vs1)                          \
+  void name(VRegister vd, VRegister vs2, MaskType mask = NoMask) { \
+    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                 \
+  }
+
+  DEFINE_VFUNARY(vfcvt_xu_f_v, VFUNARY0_FUNCT6, VFCVT_XU_F_V)
+  DEFINE_VFUNARY(vfcvt_x_f_v, VFUNARY0_FUNCT6, VFCVT_X_F_V)
+  DEFINE_VFUNARY(vfcvt_f_x_v, VFUNARY0_FUNCT6, VFCVT_F_X_V)
+  DEFINE_VFUNARY(vfcvt_f_xu_v, VFUNARY0_FUNCT6, VFCVT_F_XU_V)
+  DEFINE_VFUNARY(vfwcvt_xu_f_v, VFUNARY0_FUNCT6, VFWCVT_XU_F_V)
+  DEFINE_VFUNARY(vfwcvt_x_f_v, VFUNARY0_FUNCT6, VFWCVT_X_F_V)
+  DEFINE_VFUNARY(vfwcvt_f_x_v, VFUNARY0_FUNCT6, VFWCVT_F_X_V)
+  DEFINE_VFUNARY(vfwcvt_f_xu_v, VFUNARY0_FUNCT6, VFWCVT_F_XU_V)
+  DEFINE_VFUNARY(vfwcvt_f_f_v, VFUNARY0_FUNCT6, VFWCVT_F_F_V)
+
+  DEFINE_VFUNARY(vfncvt_f_f_w, VFUNARY0_FUNCT6, VFNCVT_F_F_W)
+  DEFINE_VFUNARY(vfncvt_x_f_w, VFUNARY0_FUNCT6, VFNCVT_X_F_W)
+  DEFINE_VFUNARY(vfncvt_xu_f_w, VFUNARY0_FUNCT6, VFNCVT_XU_F_W)
+
+  DEFINE_VFUNARY(vfclass_v, VFUNARY1_FUNCT6, VFCLASS_V)
+  DEFINE_VFUNARY(vfsqrt_v, VFUNARY1_FUNCT6, VFSQRT_V)
+  DEFINE_VFUNARY(vfrsqrt7_v, VFUNARY1_FUNCT6, VFRSQRT7_V)
+  DEFINE_VFUNARY(vfrec7_v, VFUNARY1_FUNCT6, VFREC7_V)
+#undef DEFINE_VFUNARY
+
+  void vnot_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
+    vxor_vi(dst, src, -1, mask);
+  }
+
+  void vneg_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
+    vrsub_vx(dst, src, zero_reg, mask);
+  }
+
+  void vfneg_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
+    vfsngjn_vv(dst, src, src, mask);
+  }
+  void vfabs_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
+    vfsngjx_vv(dst, src, src, mask);
+  }
+  void vfirst_m(Register rd, VRegister vs2, MaskType mask = NoMask);
+
+  void vcpop_m(Register rd, VRegister vs2, MaskType mask = NoMask);
+
+ protected:
+  void vsetvli(Register rd, Register rs1, VSew vsew, Vlmul vlmul,
+               TailAgnosticType tail = tu, MaskAgnosticType mask = mu);
+
+  void vsetivli(Register rd, uint8_t uimm, VSew vsew, Vlmul vlmul,
+                TailAgnosticType tail = tu, MaskAgnosticType mask = mu);
+
+  inline void vsetvlmax(Register rd, VSew vsew, Vlmul vlmul,
+                        TailAgnosticType tail = tu,
+                        MaskAgnosticType mask = mu) {
+    vsetvli(rd, zero_reg, vsew, vlmul, tu, mu);
+  }
+
+  inline void vsetvl(VSew vsew, Vlmul vlmul, TailAgnosticType tail = tu,
+                     MaskAgnosticType mask = mu) {
+    vsetvli(zero_reg, zero_reg, vsew, vlmul, tu, mu);
+  }
+
+  void vsetvl(Register rd, Register rs1, Register rs2);
+
+  // ----------------------------RVV------------------------------------------
+  // vsetvl
+  void GenInstrV(Register rd, Register rs1, Register rs2);
+  // vsetvli
+  void GenInstrV(Register rd, Register rs1, uint32_t zimm);
+  // OPIVV OPFVV OPMVV
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd,
+                 VRegister vs1, VRegister vs2, MaskType mask = NoMask);
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd, int8_t vs1,
+                 VRegister vs2, MaskType mask = NoMask);
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd,
+                 VRegister vs2, MaskType mask = NoMask);
+  // OPMVV OPFVV
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, Register rd,
+                 VRegister vs1, VRegister vs2, MaskType mask = NoMask);
+  // OPFVV
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, FPURegister fd,
+                 VRegister vs1, VRegister vs2, MaskType mask = NoMask);
+
+  // OPIVX OPMVX
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd,
+                 Register rs1, VRegister vs2, MaskType mask = NoMask);
+  // OPFVF
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, VRegister vd,
+                 FPURegister fs1, VRegister vs2, MaskType mask = NoMask);
+  // OPMVX
+  void GenInstrV(uint8_t funct6, Register rd, Register rs1, VRegister vs2,
+                 MaskType mask = NoMask);
+  // OPIVI
+  void GenInstrV(uint8_t funct6, VRegister vd, int8_t simm5, VRegister vs2,
+                 MaskType mask = NoMask);
+
+  // VL VS
+  void GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd, Register rs1,
+                 uint8_t umop, MaskType mask, uint8_t IsMop, bool IsMew,
+                 uint8_t Nf);
+
+  void GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd, Register rs1,
+                 Register rs2, MaskType mask, uint8_t IsMop, bool IsMew,
+                 uint8_t Nf);
+  // VL VS AMO
+  void GenInstrV(BaseOpcode opcode, uint8_t width, VRegister vd, Register rs1,
+                 VRegister vs2, MaskType mask, uint8_t IsMop, bool IsMew,
+                 uint8_t Nf);
+  // vmv_xs vcpop_m vfirst_m
+  void GenInstrV(uint8_t funct6, OpcodeRISCVV opcode, Register rd, uint8_t vs1,
+                 VRegister vs2, MaskType mask);
+};
+
+class LoadStoreLaneParams {
+ public:
+  int sz;
+  uint8_t laneidx;
+
+  LoadStoreLaneParams(MachineRepresentation rep, uint8_t laneidx);
+
+ private:
+  LoadStoreLaneParams(uint8_t laneidx, int sz, int lanes)
+      : sz(sz), laneidx(laneidx % lanes) {}
+};
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_V_H_
diff --git a/src/codegen/riscv/extension-riscv-zicsr.cc b/src/codegen/riscv/extension-riscv-zicsr.cc
new file mode 100644
index 00000000000..9ec2474e888
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-zicsr.cc
@@ -0,0 +1,44 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-zicsr.h"
+
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/constant-riscv-zicsr.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+namespace v8 {
+namespace internal {
+
+void AssemblerRISCVZicsr::csrrw(Register rd, ControlStatusReg csr,
+                                Register rs1) {
+  GenInstrCSR_ir(0b001, rd, csr, rs1);
+}
+
+void AssemblerRISCVZicsr::csrrs(Register rd, ControlStatusReg csr,
+                                Register rs1) {
+  GenInstrCSR_ir(0b010, rd, csr, rs1);
+}
+
+void AssemblerRISCVZicsr::csrrc(Register rd, ControlStatusReg csr,
+                                Register rs1) {
+  GenInstrCSR_ir(0b011, rd, csr, rs1);
+}
+
+void AssemblerRISCVZicsr::csrrwi(Register rd, ControlStatusReg csr,
+                                 uint8_t imm5) {
+  GenInstrCSR_ii(0b101, rd, csr, imm5);
+}
+
+void AssemblerRISCVZicsr::csrrsi(Register rd, ControlStatusReg csr,
+                                 uint8_t imm5) {
+  GenInstrCSR_ii(0b110, rd, csr, imm5);
+}
+
+void AssemblerRISCVZicsr::csrrci(Register rd, ControlStatusReg csr,
+                                 uint8_t imm5) {
+  GenInstrCSR_ii(0b111, rd, csr, imm5);
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-zicsr.h b/src/codegen/riscv/extension-riscv-zicsr.h
new file mode 100644
index 00000000000..629fcbfffd0
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-zicsr.h
@@ -0,0 +1,56 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_ZICSR_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_ZICSR_H_
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-zicsr.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+namespace v8 {
+namespace internal {
+
+class AssemblerRISCVZicsr : public AssemblerRiscvBase {
+ public:
+  // CSR
+  void csrrw(Register rd, ControlStatusReg csr, Register rs1);
+  void csrrs(Register rd, ControlStatusReg csr, Register rs1);
+  void csrrc(Register rd, ControlStatusReg csr, Register rs1);
+  void csrrwi(Register rd, ControlStatusReg csr, uint8_t imm5);
+  void csrrsi(Register rd, ControlStatusReg csr, uint8_t imm5);
+  void csrrci(Register rd, ControlStatusReg csr, uint8_t imm5);
+
+  // Read instructions-retired counter
+  void rdinstret(Register rd) { csrrs(rd, csr_instret, zero_reg); }
+  void rdinstreth(Register rd) { csrrs(rd, csr_instreth, zero_reg); }
+  void rdcycle(Register rd) { csrrs(rd, csr_cycle, zero_reg); }
+  void rdcycleh(Register rd) { csrrs(rd, csr_cycleh, zero_reg); }
+  void rdtime(Register rd) { csrrs(rd, csr_time, zero_reg); }
+  void rdtimeh(Register rd) { csrrs(rd, csr_timeh, zero_reg); }
+
+  void csrr(Register rd, ControlStatusReg csr) { csrrs(rd, csr, zero_reg); }
+  void csrw(ControlStatusReg csr, Register rs) { csrrw(zero_reg, csr, rs); }
+  void csrs(ControlStatusReg csr, Register rs) { csrrs(zero_reg, csr, rs); }
+  void csrc(ControlStatusReg csr, Register rs) { csrrc(zero_reg, csr, rs); }
+
+  void csrwi(ControlStatusReg csr, uint8_t imm) { csrrwi(zero_reg, csr, imm); }
+  void csrsi(ControlStatusReg csr, uint8_t imm) { csrrsi(zero_reg, csr, imm); }
+  void csrci(ControlStatusReg csr, uint8_t imm) { csrrci(zero_reg, csr, imm); }
+
+  void frcsr(Register rd) { csrrs(rd, csr_fcsr, zero_reg); }
+  void fscsr(Register rd, Register rs) { csrrw(rd, csr_fcsr, rs); }
+  void fscsr(Register rs) { csrrw(zero_reg, csr_fcsr, rs); }
+
+  void frrm(Register rd) { csrrs(rd, csr_frm, zero_reg); }
+  void fsrm(Register rd, Register rs) { csrrw(rd, csr_frm, rs); }
+  void fsrm(Register rs) { csrrw(zero_reg, csr_frm, rs); }
+
+  void frflags(Register rd) { csrrs(rd, csr_fflags, zero_reg); }
+  void fsflags(Register rd, Register rs) { csrrw(rd, csr_fflags, rs); }
+  void fsflags(Register rs) { csrrw(zero_reg, csr_fflags, rs); }
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_ZICSR_H_
diff --git a/src/codegen/riscv/extension-riscv-zifencei.cc b/src/codegen/riscv/extension-riscv-zifencei.cc
new file mode 100644
index 00000000000..d934bedde81
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-zifencei.cc
@@ -0,0 +1,16 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#include "src/codegen/riscv/extension-riscv-zifencei.h"
+
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/constant-riscv-zifencei.h"
+
+namespace v8 {
+namespace internal {
+
+void AssemblerRISCVZifencei::fence_i() {
+  GenInstrI(0b001, MISC_MEM, ToRegister(0), ToRegister(0), 0);
+}
+}  // namespace internal
+}  // namespace v8
diff --git a/src/codegen/riscv/extension-riscv-zifencei.h b/src/codegen/riscv/extension-riscv-zifencei.h
new file mode 100644
index 00000000000..62da76326ab
--- /dev/null
+++ b/src/codegen/riscv/extension-riscv-zifencei.h
@@ -0,0 +1,19 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CODEGEN_RISCV_EXTENSION_RISCV_ZIFENCEI_H_
+#define V8_CODEGEN_RISCV_EXTENSION_RISCV_ZIFENCEI_H_
+#include "src/codegen/assembler.h"
+#include "src/codegen/riscv/base-assembler-riscv.h"
+#include "src/codegen/riscv/register-riscv.h"
+
+namespace v8 {
+namespace internal {
+class AssemblerRISCVZifencei : public AssemblerRiscvBase {
+ public:
+  void fence_i();
+};
+}  // namespace internal
+}  // namespace v8
+#endif  // V8_CODEGEN_RISCV_EXTENSION_RISCV_ZIFENCEI_H_
diff --git a/src/codegen/riscv64/interface-descriptors-riscv64-inl.h b/src/codegen/riscv/interface-descriptors-riscv-inl.h
similarity index 97%
rename from src/codegen/riscv64/interface-descriptors-riscv64-inl.h
rename to src/codegen/riscv/interface-descriptors-riscv-inl.h
index 0559c46a600..f379d829147 100644
--- a/src/codegen/riscv64/interface-descriptors-riscv64-inl.h
+++ b/src/codegen/riscv/interface-descriptors-riscv-inl.h
@@ -2,10 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_CODEGEN_RISCV64_INTERFACE_DESCRIPTORS_RISCV64_INL_H_
-#define V8_CODEGEN_RISCV64_INTERFACE_DESCRIPTORS_RISCV64_INL_H_
-
-#if V8_TARGET_ARCH_RISCV64
+#ifndef V8_CODEGEN_RISCV_INTERFACE_DESCRIPTORS_RISCV_INL_H_
+#define V8_CODEGEN_RISCV_INTERFACE_DESCRIPTORS_RISCV_INL_H_
 
 #include "src/base/template-utils.h"
 #include "src/codegen/interface-descriptors.h"
@@ -321,6 +319,4 @@ constexpr auto RunMicrotasksEntryDescriptor::registers() {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_TARGET_ARCH_RISCV64
-
-#endif  // V8_CODEGEN_RISCV64_INTERFACE_DESCRIPTORS_RISCV64_INL_H_
+#endif  // V8_CODEGEN_RISCV_INTERFACE_DESCRIPTORS_RISCV_INL_H_
diff --git a/src/codegen/riscv64/macro-assembler-riscv64.cc b/src/codegen/riscv/macro-assembler-riscv.cc
similarity index 81%
rename from src/codegen/riscv64/macro-assembler-riscv64.cc
rename to src/codegen/riscv/macro-assembler-riscv.cc
index d824b5f3f2a..9fd9c7c4c4f 100644
--- a/src/codegen/riscv64/macro-assembler-riscv64.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -4,8 +4,6 @@
 
 #include <limits.h>  // For LONG_MIN, LONG_MAX.
 
-#if V8_TARGET_ARCH_RISCV64
-
 #include "src/base/bits.h"
 #include "src/base/division-by-constant.h"
 #include "src/codegen/assembler-inl.h"
@@ -29,7 +27,7 @@
 // Satisfy cpplint check, but don't include platform-specific header. It is
 // included recursively via macro-assembler.h.
 #if 0
-#include "src/codegen/riscv64/macro-assembler-riscv64.h"
+#include "src/codegen/riscv/macro-assembler-riscv.h"
 #endif
 
 namespace v8 {
@@ -94,8 +92,8 @@ int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
 }
 
 void TurboAssembler::LoadRoot(Register destination, RootIndex index) {
-  Ld(destination,
-     MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
+  LoadWord(destination,
+           MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
 }
 
 void TurboAssembler::LoadRoot(Register destination, RootIndex index,
@@ -103,15 +101,15 @@ void TurboAssembler::LoadRoot(Register destination, RootIndex index,
                               const Operand& src2) {
   Label skip;
   BranchShort(&skip, NegateCondition(cond), src1, src2);
-  Ld(destination,
-     MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
+  LoadWord(destination,
+           MemOperand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
   bind(&skip);
 }
 
 void TurboAssembler::PushCommonFrame(Register marker_reg) {
   if (marker_reg.is_valid()) {
     Push(ra, fp, marker_reg);
-    Add64(fp, sp, Operand(kSystemPointerSize));
+    AddWord(fp, sp, Operand(kSystemPointerSize));
   } else {
     Push(ra, fp);
     Mv(fp, sp);
@@ -127,7 +125,7 @@ void TurboAssembler::PushStandardFrame(Register function_reg) {
     Push(ra, fp, cp, kJavaScriptCallArgCountRegister);
     offset += kSystemPointerSize;
   }
-  Add64(fp, sp, Operand(offset));
+  AddWord(fp, sp, Operand(offset));
 }
 
 int MacroAssembler::SafepointRegisterStackIndex(int reg_code) {
@@ -162,7 +160,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     DCHECK(!AreAliased(object, value, scratch));
-    Add64(scratch, object, offset - kHeapObjectTag);
+    AddWord(scratch, object, offset - kHeapObjectTag);
     And(scratch, scratch, Operand(kTaggedSize - 1));
     BranchShort(&ok, eq, scratch, Operand(zero_reg));
     Abort(AbortReason::kUnalignedCellInWriteBarrier);
@@ -272,7 +270,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
     UseScratchRegisterScope temps(this);
     Register temp = temps.Acquire();
     DCHECK(!AreAliased(object, value, temp));
-    Add64(temp, object, offset);
+    AddWord(temp, object, offset);
     LoadTaggedPointerField(temp, MemOperand(temp));
     Assert(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite, temp,
            Operand(value));
@@ -316,7 +314,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
   DCHECK(!AreAliased(object, slot_address, value));
   // TODO(cbruni): Turn offset into int.
   DCHECK(offset.IsImmediate());
-  Add64(slot_address, object, offset);
+  AddWord(slot_address, object, offset);
   CallRecordWriteStub(object, slot_address, fp_mode);
   if (ra_status == kRAHasNotBeenSaved) {
     pop(ra);
@@ -328,7 +326,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
 
 // ---------------------------------------------------------------------------
 // Instruction macros.
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
@@ -359,45 +357,6 @@ void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
   }
 }
 
-void TurboAssembler::Add64(Register rd, Register rs, const Operand& rt) {
-  if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
-        (rt.rm() != zero_reg) && (rs != zero_reg)) {
-      c_add(rd, rt.rm());
-    } else {
-      add(rd, rs, rt.rm());
-    }
-  } else {
-    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
-        (rd.code() == rs.code()) && (rd != zero_reg) && (rt.immediate() != 0) &&
-        !MustUseReg(rt.rmode())) {
-      c_addi(rd, static_cast<int8_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && is_int10(rt.immediate()) &&
-               (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
-               (rd.code() == rs.code()) && (rd == sp) &&
-               !MustUseReg(rt.rmode())) {
-      c_addi16sp(static_cast<int16_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && ((rd.code() & 0b11000) == 0b01000) &&
-               (rs == sp) && is_uint10(rt.immediate()) &&
-               (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
-      c_addi4spn(rd, static_cast<uint16_t>(rt.immediate()));
-    } else if (is_int12(rt.immediate()) && !MustUseReg(rt.rmode())) {
-      addi(rd, rs, static_cast<int32_t>(rt.immediate()));
-    } else if ((-4096 <= rt.immediate() && rt.immediate() <= -2049) ||
-               (2048 <= rt.immediate() && rt.immediate() <= 4094)) {
-      addi(rd, rs, rt.immediate() / 2);
-      addi(rd, rd, rt.immediate() - (rt.immediate() / 2));
-    } else {
-      // li handles the relocation.
-      UseScratchRegisterScope temps(this);
-      Register scratch = temps.Acquire();
-      BlockTrampolinePoolScope block_trampoline_pool(this);
-      li(scratch, rt);
-      add(rd, rs, scratch);
-    }
-  }
-}
-
 void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
@@ -440,6 +399,14 @@ void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+void TurboAssembler::AddWord(Register rd, Register rs, const Operand& rt) {
+  Add64(rd, rs, rt);
+}
+
+void TurboAssembler::SubWord(Register rd, Register rs, const Operand& rt) {
+  Sub64(rd, rs, rt);
+}
+
 void TurboAssembler::Sub64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
@@ -489,6 +456,45 @@ void TurboAssembler::Sub64(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+void TurboAssembler::Add64(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+        (rt.rm() != zero_reg) && (rs != zero_reg)) {
+      c_add(rd, rt.rm());
+    } else {
+      add(rd, rs, rt.rm());
+    }
+  } else {
+    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+        (rd.code() == rs.code()) && (rd != zero_reg) && (rt.immediate() != 0) &&
+        !MustUseReg(rt.rmode())) {
+      c_addi(rd, static_cast<int8_t>(rt.immediate()));
+    } else if (FLAG_riscv_c_extension && is_int10(rt.immediate()) &&
+               (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
+               (rd.code() == rs.code()) && (rd == sp) &&
+               !MustUseReg(rt.rmode())) {
+      c_addi16sp(static_cast<int16_t>(rt.immediate()));
+    } else if (FLAG_riscv_c_extension && ((rd.code() & 0b11000) == 0b01000) &&
+               (rs == sp) && is_uint10(rt.immediate()) &&
+               (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
+      c_addi4spn(rd, static_cast<uint16_t>(rt.immediate()));
+    } else if (is_int12(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      addi(rd, rs, static_cast<int32_t>(rt.immediate()));
+    } else if ((-4096 <= rt.immediate() && rt.immediate() <= -2049) ||
+               (2048 <= rt.immediate() && rt.immediate() <= 4094)) {
+      addi(rd, rs, rt.immediate() / 2);
+      addi(rd, rd, rt.immediate() - (rt.immediate() / 2));
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      li(scratch, rt);
+      add(rd, rs, scratch);
+    }
+  }
+}
+
 void TurboAssembler::Mul32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     mulw(rd, rs, rt.rm());
@@ -645,6 +651,194 @@ void TurboAssembler::Modu64(Register rd, Register rs, const Operand& rt) {
     remu(rd, rs, scratch);
   }
 }
+#elif V8_TARGET_ARCH_RISCV32
+void TurboAssembler::AddWord(Register rd, Register rs, const Operand& rt) {
+  Add32(rd, rs, rt);
+}
+
+void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+        (rt.rm() != zero_reg) && (rs != zero_reg)) {
+      c_add(rd, rt.rm());
+    } else {
+      add(rd, rs, rt.rm());
+    }
+  } else {
+    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+        (rd.code() == rs.code()) && (rd != zero_reg) && (rt.immediate() != 0) &&
+        !MustUseReg(rt.rmode())) {
+      c_addi(rd, static_cast<int8_t>(rt.immediate()));
+    } else if (FLAG_riscv_c_extension && is_int10(rt.immediate()) &&
+               (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
+               (rd.code() == rs.code()) && (rd == sp) &&
+               !MustUseReg(rt.rmode())) {
+      c_addi16sp(static_cast<int16_t>(rt.immediate()));
+    } else if (FLAG_riscv_c_extension && ((rd.code() & 0b11000) == 0b01000) &&
+               (rs == sp) && is_uint10(rt.immediate()) &&
+               (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
+      c_addi4spn(rd, static_cast<uint16_t>(rt.immediate()));
+    } else if (is_int12(rt.immediate()) && !MustUseReg(rt.rmode())) {
+      addi(rd, rs, static_cast<int32_t>(rt.immediate()));
+    } else if ((-4096 <= rt.immediate() && rt.immediate() <= -2049) ||
+               (2048 <= rt.immediate() && rt.immediate() <= 4094)) {
+      addi(rd, rs, rt.immediate() / 2);
+      addi(rd, rd, rt.immediate() - (rt.immediate() / 2));
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      BlockTrampolinePoolScope block_trampoline_pool(this);
+      li(scratch, rt);
+      add(rd, rs, scratch);
+    }
+  }
+}
+
+void TurboAssembler::SubWord(Register rd, Register rs, const Operand& rt) {
+  Sub32(rd, rs, rt);
+}
+
+void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+        ((rd.code() & 0b11000) == 0b01000) &&
+        ((rt.rm().code() & 0b11000) == 0b01000)) {
+      c_sub(rd, rt.rm());
+    } else {
+      sub(rd, rs, rt.rm());
+    }
+  } else if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+             (rd != zero_reg) && is_int6(-rt.immediate()) &&
+             (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
+    c_addi(rd,
+           static_cast<int8_t>(
+               -rt.immediate()));  // No c_subi instr, use c_addi(x, y, -imm).
+
+  } else if (FLAG_riscv_c_extension && is_int10(-rt.immediate()) &&
+             (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
+             (rd.code() == rs.code()) && (rd == sp) &&
+             !MustUseReg(rt.rmode())) {
+    c_addi16sp(static_cast<int16_t>(-rt.immediate()));
+  } else if (is_int12(-rt.immediate()) && !MustUseReg(rt.rmode())) {
+    addi(rd, rs,
+         static_cast<int32_t>(
+             -rt.immediate()));  // No subi instr, use addi(x, y, -imm).
+  } else if ((-4096 <= -rt.immediate() && -rt.immediate() <= -2049) ||
+             (2048 <= -rt.immediate() && -rt.immediate() <= 4094)) {
+    addi(rd, rs, -rt.immediate() / 2);
+    addi(rd, rd, -rt.immediate() - (-rt.immediate() / 2));
+  } else {
+    // RV32G todo: imm64 or imm32 here
+    int li_count = InstrCountForLi64Bit(rt.immediate());
+    int li_neg_count = InstrCountForLi64Bit(-rt.immediate());
+    if (li_neg_count < li_count && !MustUseReg(rt.rmode())) {
+      // Use load -imm and add when loading -imm generates one instruction.
+      DCHECK(rt.immediate() != std::numeric_limits<int32_t>::min());
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      li(scratch, Operand(-rt.immediate()));
+      add(rd, rs, scratch);
+    } else {
+      // li handles the relocation.
+      UseScratchRegisterScope temps(this);
+      Register scratch = temps.Acquire();
+      li(scratch, rt);
+      sub(rd, rs, scratch);
+    }
+  }
+}
+
+void TurboAssembler::Mul32(Register rd, Register rs, const Operand& rt) {
+  Mul(rd, rs, rt);
+}
+
+void TurboAssembler::Mul(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    mul(rd, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    mul(rd, rs, scratch);
+  }
+}
+
+void TurboAssembler::Mulh(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    mulh(rd, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    mulh(rd, rs, scratch);
+  }
+}
+
+void TurboAssembler::Mulhu(Register rd, Register rs, const Operand& rt,
+                           Register rsz, Register rtz) {
+  if (rt.is_reg()) {
+    mulhu(rd, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    mulhu(rd, rs, scratch);
+  }
+}
+
+void TurboAssembler::Div(Register res, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    div(res, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    div(res, rs, scratch);
+  }
+}
+
+void TurboAssembler::Mod(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    rem(rd, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    rem(rd, rs, scratch);
+  }
+}
+
+void TurboAssembler::Modu(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    remu(rd, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    remu(rd, rs, scratch);
+  }
+}
+
+void TurboAssembler::Divu(Register res, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    divu(res, rs, rt.rm());
+  } else {
+    // li handles the relocation.
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    Li(scratch, rt.immediate());
+    divu(res, rs, scratch);
+  }
+}
+
+#endif
 
 void TurboAssembler::And(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
@@ -753,7 +947,7 @@ void TurboAssembler::Seq(Register rd, Register rs, const Operand& rt) {
   } else if (IsZero(rt)) {
     seqz(rd, rs);
   } else {
-    Sub64(rd, rs, rt);
+    SubWord(rd, rs, rt);
     seqz(rd, rd);
   }
 }
@@ -764,7 +958,7 @@ void TurboAssembler::Sne(Register rd, Register rs, const Operand& rt) {
   } else if (IsZero(rt)) {
     snez(rd, rs);
   } else {
-    Sub64(rd, rs, rt);
+    SubWord(rd, rs, rt);
     snez(rd, rd);
   }
 }
@@ -867,6 +1061,7 @@ void TurboAssembler::Sgtu(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Sll32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     sllw(rd, rs, rt.rm());
@@ -894,6 +1089,10 @@ void TurboAssembler::Srl32(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+void TurboAssembler::SraWord(Register rd, Register rs, const Operand& rt) {
+  Sra64(rd, rs, rt);
+}
+
 void TurboAssembler::Sra64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     sra(rd, rs, rt.rm());
@@ -907,6 +1106,10 @@ void TurboAssembler::Sra64(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+void TurboAssembler::SrlWord(Register rd, Register rs, const Operand& rt) {
+  Srl64(rd, rs, rt);
+}
+
 void TurboAssembler::Srl64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     srl(rd, rs, rt.rm());
@@ -920,6 +1123,10 @@ void TurboAssembler::Srl64(Register rd, Register rs, const Operand& rt) {
   }
 }
 
+void TurboAssembler::SllWord(Register rd, Register rs, const Operand& rt) {
+  Sll64(rd, rs, rt);
+}
+
 void TurboAssembler::Sll64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     sll(rd, rs, rt.rm());
@@ -934,22 +1141,6 @@ void TurboAssembler::Sll64(Register rd, Register rs, const Operand& rt) {
   }
 }
 
-void TurboAssembler::Li(Register rd, int64_t imm) {
-  if (FLAG_riscv_c_extension && (rd != zero_reg) && is_int6(imm)) {
-    c_li(rd, imm);
-  } else {
-    RV_li(rd, imm);
-  }
-}
-
-void TurboAssembler::Mv(Register rd, const Operand& rt) {
-  if (FLAG_riscv_c_extension && (rd != zero_reg) && (rt.rm() != zero_reg)) {
-    c_mv(rd, rt.rm());
-  } else {
-    mv(rd, rt.rm());
-  }
-}
-
 void TurboAssembler::Ror(Register rd, Register rs, const Operand& rt) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
@@ -997,6 +1188,85 @@ void TurboAssembler::Dror(Register rd, Register rs, const Operand& rt) {
     or_(rd, scratch, rd);
   }
 }
+#elif V8_TARGET_ARCH_RISCV32
+void TurboAssembler::SllWord(Register rd, Register rs, const Operand& rt) {
+  Sll32(rd, rs, rt);
+}
+
+void TurboAssembler::Sll32(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    sll(rd, rs, rt.rm());
+  } else {
+    uint8_t shamt = static_cast<uint8_t>(rt.immediate());
+    slli(rd, rs, shamt);
+  }
+}
+
+void TurboAssembler::SraWord(Register rd, Register rs, const Operand& rt) {
+  Sra32(rd, rs, rt);
+}
+
+void TurboAssembler::Sra32(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    sra(rd, rs, rt.rm());
+  } else {
+    uint8_t shamt = static_cast<uint8_t>(rt.immediate());
+    srai(rd, rs, shamt);
+  }
+}
+
+void TurboAssembler::SrlWord(Register rd, Register rs, const Operand& rt) {
+  Srl32(rd, rs, rt);
+}
+
+void TurboAssembler::Srl32(Register rd, Register rs, const Operand& rt) {
+  if (rt.is_reg()) {
+    srl(rd, rs, rt.rm());
+  } else {
+    uint8_t shamt = static_cast<uint8_t>(rt.immediate());
+    srli(rd, rs, shamt);
+  }
+}
+
+void TurboAssembler::Ror(Register rd, Register rs, const Operand& rt) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  if (rt.is_reg()) {
+    neg(scratch, rt.rm());
+    sll(scratch, rs, scratch);
+    srl(rd, rs, rt.rm());
+    or_(rd, scratch, rd);
+  } else {
+    int32_t ror_value = rt.immediate() % 32;
+    if (ror_value == 0) {
+      Mv(rd, rs);
+      return;
+    } else if (ror_value < 0) {
+      ror_value += 32;
+    }
+    srli(scratch, rs, ror_value);
+    slli(rd, rs, 32 - ror_value);
+    or_(rd, scratch, rd);
+  }
+}
+#endif
+
+void TurboAssembler::Li(Register rd, intptr_t imm) {
+  if (FLAG_riscv_c_extension && (rd != zero_reg) && is_int6(imm)) {
+    c_li(rd, imm);
+  } else {
+    RV_li(rd, imm);
+  }
+}
+
+void TurboAssembler::Mv(Register rd, const Operand& rt) {
+  if (FLAG_riscv_c_extension && (rd != zero_reg) && (rt.rm() != zero_reg)) {
+    c_mv(rd, rt.rm());
+  } else {
+    mv(rd, rt.rm());
+  }
+}
 
 void TurboAssembler::CalcScaledAddress(Register rd, Register rt, Register rs,
                                        uint8_t sa) {
@@ -1005,11 +1275,12 @@ void TurboAssembler::CalcScaledAddress(Register rd, Register rt, Register rs,
   Register tmp = rd == rt ? temps.Acquire() : rd;
   DCHECK(tmp != rt);
   slli(tmp, rs, sa);
-  Add64(rd, rt, tmp);
+  AddWord(rd, rt, tmp);
 }
 
 // ------------Pseudo-instructions-------------
 // Change endianness
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::ByteSwap(Register rd, Register rs, int operand_size,
                               Register scratch) {
   DCHECK_NE(scratch, rs);
@@ -1067,6 +1338,33 @@ void TurboAssembler::ByteSwap(Register rd, Register rs, int operand_size,
   }
 }
 
+#elif V8_TARGET_ARCH_RISCV32
+void TurboAssembler::ByteSwap(Register rd, Register rs, int operand_size,
+                              Register scratch) {
+  DCHECK_NE(scratch, rs);
+  DCHECK_NE(scratch, rd);
+  // Uint32_t x1 = 0x00FF00FF;
+  // x0 = (x0 << 16 | x0 >> 16);
+  // x0 = (((x0 & x1) << 8)  | ((x0 & (x1 << 8)) >> 8));
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  DCHECK((rd != t6) && (rs != t6));
+  Register x0 = temps.Acquire();
+  Register x1 = temps.Acquire();
+  Register x2 = scratch;
+  li(x1, 0x00FF00FF);
+  slli(x0, rs, 16);
+  srli(rd, rs, 16);
+  or_(x0, rd, x0);   // x0 <- x0 << 16 | x0 >> 16
+  and_(x2, x0, x1);  // x2 <- x0 & 0x00FF00FF
+  slli(x2, x2, 8);   // x2 <- (x0 & x1) << 8
+  slli(x1, x1, 8);   // x1 <- 0xFF00FF00
+  and_(rd, x0, x1);  // x0 & 0xFF00FF00
+  srli(rd, rd, 8);
+  or_(rd, rd, x2);  // (((x0 & x1) << 8)  | ((x0 & (x1 << 8)) >> 8))
+}
+#endif
+
 template <int NBYTES, bool LOAD_SIGNED>
 void TurboAssembler::LoadNBytes(Register rd, const MemOperand& rs,
                                 Register scratch) {
@@ -1148,6 +1446,7 @@ void TurboAssembler::UnalignedLoadHelper(Register rd, const MemOperand& rs) {
   }
 }
 
+#if V8_TARGET_ARCH_RISCV64
 template <int NBYTES>
 void TurboAssembler::UnalignedFLoadHelper(FPURegister frd, const MemOperand& rs,
                                           Register scratch_base) {
@@ -1172,6 +1471,56 @@ void TurboAssembler::UnalignedFLoadHelper(FPURegister frd, const MemOperand& rs,
   else
     fmv_d_x(frd, scratch);
 }
+#elif V8_TARGET_ARCH_RISCV32
+template <int NBYTES>
+void TurboAssembler::UnalignedFLoadHelper(FPURegister frd, const MemOperand& rs,
+                                          Register scratch_base) {
+  DCHECK_EQ(NBYTES, 4);
+  DCHECK_NE(scratch_base, rs.rm());
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  MemOperand source = rs;
+  if (NeedAdjustBaseAndOffset(rs, OffsetAccessType::TWO_ACCESSES, NBYTES - 1)) {
+    // Adjust offset for two accesses and check if offset + 3 fits into int12.
+    DCHECK(scratch_base != rs.rm());
+    AdjustBaseAndOffset(&source, scratch_base, OffsetAccessType::TWO_ACCESSES,
+                        NBYTES - 1);
+  }
+  UseScratchRegisterScope temps(this);
+  Register scratch_other = temps.Acquire();
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs.rm() && scratch_other != scratch &&
+         scratch_other != rs.rm());
+  LoadNBytes<NBYTES, true>(scratch, source, scratch_other);
+  fmv_w_x(frd, scratch);
+}
+
+void TurboAssembler::UnalignedDoubleHelper(FPURegister frd,
+                                           const MemOperand& rs,
+                                           Register scratch_base) {
+  DCHECK_NE(scratch_base, rs.rm());
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  MemOperand source = rs;
+  if (NeedAdjustBaseAndOffset(rs, OffsetAccessType::TWO_ACCESSES, 8 - 1)) {
+    // Adjust offset for two accesses and check if offset + 3 fits into int12.
+    DCHECK(scratch_base != rs.rm());
+    AdjustBaseAndOffset(&source, scratch_base, OffsetAccessType::TWO_ACCESSES,
+                        8 - 1);
+  }
+  UseScratchRegisterScope temps(this);
+  Register scratch_other = temps.Acquire();
+  Register scratch = temps.Acquire();
+  DCHECK(scratch != rs.rm() && scratch_other != scratch &&
+         scratch_other != rs.rm());
+  LoadNBytes<4, true>(scratch, source, scratch_other);
+  SubWord(sp, sp, 8);
+  Sw(scratch, MemOperand(sp, 0));
+  source.set_offset(source.offset() + 4);
+  LoadNBytes<4, true>(scratch, source, scratch_other);
+  Sw(scratch, MemOperand(sp, 4));
+  LoadDouble(frd, MemOperand(sp, 0));
+  AddWord(sp, sp, 8);
+}
+#endif
 
 template <int NBYTES>
 void TurboAssembler::UnalignedStoreHelper(Register rd, const MemOperand& rs,
@@ -1211,6 +1560,7 @@ void TurboAssembler::UnalignedStoreHelper(Register rd, const MemOperand& rs,
   }
 }
 
+#if V8_TARGET_ARCH_RISCV64
 template <int NBYTES>
 void TurboAssembler::UnalignedFStoreHelper(FPURegister frd,
                                            const MemOperand& rs,
@@ -1224,6 +1574,31 @@ void TurboAssembler::UnalignedFStoreHelper(FPURegister frd,
   }
   UnalignedStoreHelper<NBYTES>(scratch, rs);
 }
+#elif V8_TARGET_ARCH_RISCV32
+template <int NBYTES>
+void TurboAssembler::UnalignedFStoreHelper(FPURegister frd,
+                                           const MemOperand& rs,
+                                           Register scratch) {
+  DCHECK_EQ(NBYTES, 4);
+  DCHECK_NE(scratch, rs.rm());
+  fmv_x_w(scratch, frd);
+  UnalignedStoreHelper<NBYTES>(scratch, rs);
+}
+void TurboAssembler::UnalignedDStoreHelper(FPURegister frd,
+                                           const MemOperand& rs,
+                                           Register scratch) {
+  DCHECK_NE(scratch, rs.rm());
+  Sub32(sp, sp, 8);
+  StoreDouble(frd, MemOperand(sp, 0));
+  Lw(scratch, MemOperand(sp, 0));
+  UnalignedStoreHelper<4>(scratch, rs);
+  Lw(scratch, MemOperand(sp, 4));
+  MemOperand source = rs;
+  source.set_offset(source.offset() + 4);
+  UnalignedStoreHelper<4>(scratch, source);
+  Add32(sp, sp, 8);
+}
+#endif
 
 template <typename Reg_T, typename Func>
 void TurboAssembler::AlignedLoadHelper(Reg_T target, const MemOperand& rs,
@@ -1260,10 +1635,11 @@ void TurboAssembler::Ulw(Register rd, const MemOperand& rs) {
   UnalignedLoadHelper<4, true>(rd, rs);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Ulwu(Register rd, const MemOperand& rs) {
   UnalignedLoadHelper<4, false>(rd, rs);
 }
-
+#endif
 void TurboAssembler::Usw(Register rd, const MemOperand& rs) {
   UnalignedStoreHelper<4>(rd, rs);
 }
@@ -1283,7 +1659,7 @@ void TurboAssembler::Ush(Register rd, const MemOperand& rs) {
 void TurboAssembler::Uld(Register rd, const MemOperand& rs) {
   UnalignedLoadHelper<8, true>(rd, rs);
 }
-
+#if V8_TARGET_ARCH_RISCV64
 // Load consequent 32-bit word pair in 64-bit reg. and put first word in low
 // bits,
 // second word in high bits.
@@ -1293,11 +1669,7 @@ void MacroAssembler::LoadWordPair(Register rd, const MemOperand& rs) {
   Lwu(rd, rs);
   Lw(scratch, MemOperand(rs.rm(), rs.offset() + kSystemPointerSize / 2));
   slli(scratch, scratch, 32);
-  Add64(rd, rd, scratch);
-}
-
-void TurboAssembler::Usd(Register rd, const MemOperand& rs) {
-  UnalignedStoreHelper<8>(rd, rs);
+  AddWord(rd, rd, scratch);
 }
 
 // Do 64-bit store as two consequent 32-bit stores to unaligned address.
@@ -1308,6 +1680,11 @@ void MacroAssembler::StoreWordPair(Register rd, const MemOperand& rs) {
   srai(scratch, rd, 32);
   Sw(scratch, MemOperand(rs.rm(), rs.offset() + kSystemPointerSize / 2));
 }
+#endif
+
+void TurboAssembler::Usd(Register rd, const MemOperand& rs) {
+  UnalignedStoreHelper<8>(rd, rs);
+}
 
 void TurboAssembler::ULoadFloat(FPURegister fd, const MemOperand& rs,
                                 Register scratch) {
@@ -1324,13 +1701,21 @@ void TurboAssembler::UStoreFloat(FPURegister fd, const MemOperand& rs,
 void TurboAssembler::ULoadDouble(FPURegister fd, const MemOperand& rs,
                                  Register scratch) {
   DCHECK_NE(scratch, rs.rm());
+#if V8_TARGET_ARCH_RISCV64
   UnalignedFLoadHelper<8>(fd, rs, scratch);
+#elif V8_TARGET_ARCH_RISCV32
+  UnalignedDoubleHelper(fd, rs, scratch);
+#endif
 }
 
 void TurboAssembler::UStoreDouble(FPURegister fd, const MemOperand& rs,
                                   Register scratch) {
   DCHECK_NE(scratch, rs.rm());
+#if V8_TARGET_ARCH_RISCV64
   UnalignedFStoreHelper<8>(fd, rs, scratch);
+#elif V8_TARGET_ARCH_RISCV32
+  UnalignedDStoreHelper(fd, rs, scratch);
+#endif
 }
 
 void TurboAssembler::Lb(Register rd, const MemOperand& rs) {
@@ -1392,13 +1777,14 @@ void TurboAssembler::Lw(Register rd, const MemOperand& rs) {
   AlignedLoadHelper(rd, rs, fn);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Lwu(Register rd, const MemOperand& rs) {
   auto fn = [this](Register target, const MemOperand& source) {
     this->lwu(target, source.rm(), source.offset());
   };
   AlignedLoadHelper(rd, rs, fn);
 }
-
+#endif
 void TurboAssembler::Sw(Register rd, const MemOperand& rs) {
   auto fn = [this](Register value, const MemOperand& source) {
     if (FLAG_riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
@@ -1415,6 +1801,7 @@ void TurboAssembler::Sw(Register rd, const MemOperand& rs) {
   AlignedStoreHelper(rd, rs, fn);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Ld(Register rd, const MemOperand& rs) {
   auto fn = [this](Register target, const MemOperand& source) {
     if (FLAG_riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
@@ -1447,7 +1834,7 @@ void TurboAssembler::Sd(Register rd, const MemOperand& rs) {
   };
   AlignedStoreHelper(rd, rs, fn);
 }
-
+#endif
 void TurboAssembler::LoadFloat(FPURegister fd, const MemOperand& src) {
   auto fn = [this](FPURegister target, const MemOperand& source) {
     this->flw(target, source.rm(), source.offset());
@@ -1501,11 +1888,12 @@ void TurboAssembler::Ll(Register rd, const MemOperand& rs) {
   } else {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs.rm(), rs.offset());
+    AddWord(scratch, rs.rm(), rs.offset());
     lr_w(false, false, rd, scratch);
   }
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Lld(Register rd, const MemOperand& rs) {
   bool is_one_instruction = rs.offset() == 0;
   if (is_one_instruction) {
@@ -1513,11 +1901,11 @@ void TurboAssembler::Lld(Register rd, const MemOperand& rs) {
   } else {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs.rm(), rs.offset());
+    AddWord(scratch, rs.rm(), rs.offset());
     lr_d(false, false, rd, scratch);
   }
 }
-
+#endif
 void TurboAssembler::Sc(Register rd, const MemOperand& rs) {
   bool is_one_instruction = rs.offset() == 0;
   if (is_one_instruction) {
@@ -1525,11 +1913,11 @@ void TurboAssembler::Sc(Register rd, const MemOperand& rs) {
   } else {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs.rm(), rs.offset());
+    AddWord(scratch, rs.rm(), rs.offset());
     sc_w(false, false, rd, scratch, rd);
   }
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Scd(Register rd, const MemOperand& rs) {
   bool is_one_instruction = rs.offset() == 0;
   if (is_one_instruction) {
@@ -1537,11 +1925,11 @@ void TurboAssembler::Scd(Register rd, const MemOperand& rs) {
   } else {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs.rm(), rs.offset());
+    AddWord(scratch, rs.rm(), rs.offset());
     sc_d(false, false, rd, scratch, rd);
   }
 }
-
+#endif
 void TurboAssembler::li(Register dst, Handle<HeapObject> value,
                         RelocInfo::Mode rmode) {
   // TODO(jgruber,v8:8887): Also consider a root-relative load when generating
@@ -1610,11 +1998,11 @@ void TurboAssembler::li(Register rd, Operand j, LiFlags mode) {
     int count = li_estimate(j.immediate(), temps.hasAvailable());
     int reverse_count = li_estimate(~j.immediate(), temps.hasAvailable());
     if (FLAG_riscv_constant_pool && count >= 4 && reverse_count >= 4) {
-      // Ld a Address from a constant pool.
-      RecordEntry((uint64_t)j.immediate(), j.rmode());
+      // Ld/Lw a Address from a constant pool.
+      RecordEntry((uintptr_t)j.immediate(), j.rmode());
       auipc(rd, 0);
       // Record a value into constant pool.
-      ld(rd, rd, 0);
+      LoadWord(rd, MemOperand(rd, 0));
     } else {
       if ((count - reverse_count) > 1) {
         Li(rd, ~j.immediate());
@@ -1653,11 +2041,11 @@ void TurboAssembler::MultiPush(RegList regs) {
   int16_t num_to_push = regs.Count();
   int16_t stack_offset = num_to_push * kSystemPointerSize;
 
-#define TEST_AND_PUSH_REG(reg)             \
-  if (regs.has(reg)) {                     \
-    stack_offset -= kSystemPointerSize;    \
-    Sd(reg, MemOperand(sp, stack_offset)); \
-    regs.clear(reg);                       \
+#define TEST_AND_PUSH_REG(reg)                    \
+  if (regs.has(reg)) {                            \
+    stack_offset -= kSystemPointerSize;           \
+    StoreWord(reg, MemOperand(sp, stack_offset)); \
+    regs.clear(reg);                              \
   }
 
 #define T_REGS(V) V(t6) V(t5) V(t4) V(t3) V(t2) V(t1) V(t0)
@@ -1665,7 +2053,7 @@ void TurboAssembler::MultiPush(RegList regs) {
 #define S_REGS(V) \
   V(s11) V(s10) V(s9) V(s8) V(s7) V(s6) V(s5) V(s4) V(s3) V(s2) V(s1)
 
-  Sub64(sp, sp, Operand(stack_offset));
+  SubWord(sp, sp, Operand(stack_offset));
 
   // Certain usage of MultiPush requires that registers are pushed onto the
   // stack in a particular: ra, fp, sp, gp, .... (basically in the decreasing
@@ -1696,11 +2084,11 @@ void TurboAssembler::MultiPush(RegList regs) {
 void TurboAssembler::MultiPop(RegList regs) {
   int16_t stack_offset = 0;
 
-#define TEST_AND_POP_REG(reg)              \
-  if (regs.has(reg)) {                     \
-    Ld(reg, MemOperand(sp, stack_offset)); \
-    stack_offset += kSystemPointerSize;    \
-    regs.clear(reg);                       \
+#define TEST_AND_POP_REG(reg)                    \
+  if (regs.has(reg)) {                           \
+    LoadWord(reg, MemOperand(sp, stack_offset)); \
+    stack_offset += kSystemPointerSize;          \
+    regs.clear(reg);                             \
   }
 
 #define T_REGS(V) V(t0) V(t1) V(t2) V(t3) V(t4) V(t5) V(t6)
@@ -1738,7 +2126,7 @@ void TurboAssembler::MultiPushFPU(DoubleRegList regs) {
   int16_t num_to_push = regs.Count();
   int16_t stack_offset = num_to_push * kDoubleSize;
 
-  Sub64(sp, sp, Operand(stack_offset));
+  SubWord(sp, sp, Operand(stack_offset));
   for (int16_t i = kNumRegisters - 1; i >= 0; i--) {
     if ((regs.bits() & (1 << i)) != 0) {
       stack_offset -= kDoubleSize;
@@ -1759,8 +2147,253 @@ void TurboAssembler::MultiPopFPU(DoubleRegList regs) {
   addi(sp, sp, stack_offset);
 }
 
+#if V8_TARGET_ARCH_RISCV32
+void TurboAssembler::AddPair(Register dst_low, Register dst_high,
+                             Register left_low, Register left_high,
+                             Register right_low, Register right_high,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+
+  Add32(scratch1, left_low, right_low);
+  // Save the carry
+  Sltu(scratch3, scratch1, left_low);
+  Add32(scratch2, left_high, right_high);
+
+  // Output higher 32 bits + carry
+  Add32(dst_high, scratch2, scratch3);
+  Move(dst_low, scratch1);
+}
+
+void TurboAssembler::SubPair(Register dst_low, Register dst_high,
+                             Register left_low, Register left_high,
+                             Register right_low, Register right_high,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+
+  // Check if we need a borrow
+  Sltu(scratch3, left_low, right_low);
+  Sub32(scratch1, left_low, right_low);
+  Sub32(scratch2, left_high, right_high);
+
+  // Output higher 32 bits - borrow
+  Sub32(dst_high, scratch2, scratch3);
+  Move(dst_low, scratch1);
+}
+
+void TurboAssembler::AndPair(Register dst_low, Register dst_high,
+                             Register left_low, Register left_high,
+                             Register right_low, Register right_high) {
+  And(dst_low, left_low, right_low);
+  And(dst_high, left_high, right_high);
+}
+
+void TurboAssembler::OrPair(Register dst_low, Register dst_high,
+                            Register left_low, Register left_high,
+                            Register right_low, Register right_high) {
+  Or(dst_low, left_low, right_low);
+  Or(dst_high, left_high, right_high);
+}
+void TurboAssembler::XorPair(Register dst_low, Register dst_high,
+                             Register left_low, Register left_high,
+                             Register right_low, Register right_high) {
+  Xor(dst_low, left_low, right_low);
+  Xor(dst_high, left_high, right_high);
+}
+
+void TurboAssembler::MulPair(Register dst_low, Register dst_high,
+                             Register left_low, Register left_high,
+                             Register right_low, Register right_high,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  if (dst_low == right_low) {
+    mv(scratch1, right_low);
+  }
+  Mul(scratch3, left_low, right_high);
+  // NOTE: do not move these around, recommended sequence is MULH-MUL
+  // LL * RL : higher 32 bits
+  mulhu(scratch2, left_low, right_low);
+  // LL * RL : lower 32 bits
+  Mul(dst_low, left_low, right_low);
+  // (LL * RH) + (LL * RL : higher 32 bits)
+  Add32(scratch2, scratch2, scratch3);
+  if (dst_low != right_low) {
+    Mul(scratch3, left_high, right_low);
+  } else {
+    Mul(scratch3, left_high, scratch1);
+  }
+  Add32(dst_high, scratch2, scratch3);
+}
+
+void TurboAssembler::ShlPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high,
+                             Register shift, Register scratch1,
+                             Register scratch2) {
+  ASM_CODE_COMMENT(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Label done;
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = no_reg;
+  if (dst_low == src_low) {
+    scratch3 = temps.Acquire();
+    mv(scratch3, src_low);
+  }
+  And(scratch1, shift, 0x1F);
+  // LOW32 << shamt
+  sll(dst_low, src_low, scratch1);
+  // HIGH32 << shamt
+  sll(dst_high, src_high, scratch1);
+
+  // If the shift amount is 0, we're done
+  Branch(&done, eq, shift, Operand(zero_reg));
+
+  // LOW32 >> (32 - shamt)
+  li(scratch2, 32);
+  Sub32(scratch2, scratch2, scratch1);
+  if (dst_low == src_low) {
+    srl(scratch1, scratch3, scratch2);
+  } else {
+    srl(scratch1, src_low, scratch2);
+  }
+
+  // (HIGH32 << shamt) | (LOW32 >> (32 - shamt))
+  Or(dst_high, dst_high, scratch1);
+
+  // If the shift amount is < 32, we're done
+  // Note: the shift amount is always < 64, so we can just test if the 6th bit
+  // is set
+  And(scratch1, shift, 32);
+  Branch(&done, eq, scratch1, Operand(zero_reg));
+  Move(dst_high, dst_low);
+  Move(dst_low, zero_reg);
+
+  bind(&done);
+}
+
+void TurboAssembler::ShlPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high, int32_t shift,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  li(scratch3, shift);
+  ShlPair(dst_low, dst_high, src_low, src_high, scratch3, scratch1, scratch2);
+}
+
+void TurboAssembler::ShrPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high,
+                             Register shift, Register scratch1,
+                             Register scratch2) {
+  ASM_CODE_COMMENT(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Label done;
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = no_reg;
+  if (dst_high == src_high) {
+    scratch3 = temps.Acquire();
+    mv(scratch3, src_high);
+  }
+  And(scratch1, shift, 0x1F);
+  // HIGH32 >> shamt
+  srl(dst_high, src_high, scratch1);
+  // LOW32 >> shamt
+  srl(dst_low, src_low, scratch1);
+
+  // If the shift amount is 0, we're done
+  Branch(&done, eq, shift, Operand(zero_reg));
+
+  // HIGH32 << (32 - shamt)
+  li(scratch2, 32);
+  Sub32(scratch2, scratch2, scratch1);
+  if (dst_high == src_high) {
+    sll(scratch1, scratch3, scratch2);
+  } else {
+    sll(scratch1, src_high, scratch2);
+  }
+
+  // (HIGH32 << (32 - shamt)) | (LOW32 >> shamt)
+  Or(dst_low, dst_low, scratch1);
+
+  // If the shift amount is < 32, we're done
+  // Note: the shift amount is always < 64, so we can just test if the 6th bit
+  // is set
+  And(scratch1, shift, 32);
+  Branch(&done, eq, scratch1, Operand(zero_reg));
+  Move(dst_low, dst_high);
+  Move(dst_high, zero_reg);
+
+  bind(&done);
+}
+
+void TurboAssembler::ShrPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high, int32_t shift,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  li(scratch3, shift);
+  ShrPair(dst_low, dst_high, src_low, src_high, scratch3, scratch1, scratch2);
+}
+
+void TurboAssembler::SarPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high,
+                             Register shift, Register scratch1,
+                             Register scratch2) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Label done;
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = no_reg;
+  if (dst_high == src_high) {
+    scratch3 = temps.Acquire();
+    mv(scratch3, src_high);
+  }
+  And(scratch1, shift, 0x1F);
+  // HIGH32 >> shamt (arithmetic)
+  sra(dst_high, src_high, scratch1);
+  // LOW32 >> shamt (logical)
+  srl(dst_low, src_low, scratch1);
+
+  // If the shift amount is 0, we're done
+  Branch(&done, eq, shift, Operand(zero_reg));
+
+  // HIGH32 << (32 - shamt)
+  li(scratch2, 32);
+  Sub32(scratch2, scratch2, scratch1);
+  if (dst_high == src_high) {
+    sll(scratch1, scratch3, scratch2);
+  } else {
+    sll(scratch1, src_high, scratch2);
+  }
+  // (HIGH32 << (32 - shamt)) | (LOW32 >> shamt)
+  Or(dst_low, dst_low, scratch1);
+
+  // If the shift amount is < 32, we're done
+  // Note: the shift amount is always < 64, so we can just test if the 6th bit
+  // is set
+  And(scratch1, shift, 32);
+  Branch(&done, eq, scratch1, Operand(zero_reg));
+  Move(dst_low, dst_high);
+  Sra32(dst_high, dst_high, 31);
+
+  bind(&done);
+}
+
+void TurboAssembler::SarPair(Register dst_low, Register dst_high,
+                             Register src_low, Register src_high, int32_t shift,
+                             Register scratch1, Register scratch2) {
+  UseScratchRegisterScope temps(this);
+  Register scratch3 = temps.Acquire();
+  li(scratch3, shift);
+  SarPair(dst_low, dst_high, src_low, src_high, scratch3, scratch1, scratch2);
+}
+#endif
+
 void TurboAssembler::ExtractBits(Register rt, Register rs, uint16_t pos,
                                  uint16_t size, bool sign_extend) {
+#if V8_TARGET_ARCH_RISCV64
   DCHECK(pos < 64 && 0 < size && size <= 64 && 0 < pos + size &&
          pos + size <= 64);
   slli(rt, rs, 64 - (pos + size));
@@ -1769,11 +2402,28 @@ void TurboAssembler::ExtractBits(Register rt, Register rs, uint16_t pos,
   } else {
     srli(rt, rt, 64 - size);
   }
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK_LT(pos, 32);
+  DCHECK_GT(size, 0);
+  DCHECK_LE(size, 32);
+  DCHECK_GT(pos + size, 0);
+  DCHECK_LE(pos + size, 32);
+  slli(rt, rs, 32 - (pos + size));
+  if (sign_extend) {
+    srai(rt, rt, 32 - size);
+  } else {
+    srli(rt, rt, 32 - size);
+  }
+#endif
 }
 
 void TurboAssembler::InsertBits(Register dest, Register source, Register pos,
                                 int size) {
+#if V8_TARGET_ARCH_RISCV64
   DCHECK_LT(size, 64);
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK_LT(size, 32);
+#endif
   UseScratchRegisterScope temps(this);
   Register mask = temps.Acquire();
   BlockTrampolinePoolScope block_trampoline_pool(this);
@@ -1806,12 +2456,12 @@ void TurboAssembler::Cvt_d_w(FPURegister fd, Register rs) {
   // Convert rs to a FP value in fd.
   fcvt_d_w(fd, rs);
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Cvt_d_ul(FPURegister fd, Register rs) {
   // Convert rs to a FP value in fd.
   fcvt_d_lu(fd, rs);
 }
-
+#endif
 void TurboAssembler::Cvt_s_uw(FPURegister fd, Register rs) {
   // Convert rs to a FP value in fd.
   fcvt_s_wu(fd, rs);
@@ -1821,12 +2471,12 @@ void TurboAssembler::Cvt_s_w(FPURegister fd, Register rs) {
   // Convert rs to a FP value in fd.
   fcvt_s_w(fd, rs);
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Cvt_s_ul(FPURegister fd, Register rs) {
   // Convert rs to a FP value in fd.
   fcvt_s_lu(fd, rs);
 }
-
+#endif
 template <typename CvtFunc>
 void TurboAssembler::RoundFloatingPointToInteger(Register rd, FPURegister fs,
                                                  Register result,
@@ -1900,7 +2550,7 @@ void TurboAssembler::Trunc_w_s(Register rd, FPURegister fs, Register result) {
         tasm->fcvt_w_s(dst, src, RTZ);
       });
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Trunc_ul_d(Register rd, FPURegister fs, Register result) {
   RoundFloatingPointToInteger(
       rd, fs, result, [](TurboAssembler* tasm, Register dst, FPURegister src) {
@@ -1928,7 +2578,7 @@ void TurboAssembler::Trunc_l_s(Register rd, FPURegister fs, Register result) {
         tasm->fcvt_l_s(dst, src, RTZ);
       });
 }
-
+#endif
 void TurboAssembler::Round_w_s(Register rd, FPURegister fs, Register result) {
   RoundFloatingPointToInteger(
       rd, fs, result, [](TurboAssembler* tasm, Register dst, FPURegister src) {
@@ -1976,6 +2626,7 @@ void TurboAssembler::Floor_w_d(Register rd, FPURegister fs, Register result) {
 // rounded result; this differs from behavior of RISCV fcvt instructions (which
 // round out-of-range values to the nearest max or min value), therefore special
 // handling is needed by NaN, +/-Infinity, +/-0
+#if V8_TARGET_ARCH_RISCV64
 template <typename F>
 void TurboAssembler::RoundHelper(FPURegister dst, FPURegister src,
                                  FPURegister fpu_scratch, FPURoundingMode frm) {
@@ -2090,7 +2741,99 @@ void TurboAssembler::RoundHelper(FPURegister dst, FPURegister src,
 
   bind(&done);
 }
+#elif V8_TARGET_ARCH_RISCV32
+// According to JS ECMA specification, for floating-point round operations, if
+// the input is NaN, +/-infinity, or +/-0, the same input is returned as the
+// rounded result; this differs from behavior of RISCV fcvt instructions (which
+// round out-of-range values to the nearest max or min value), therefore special
+// handling is needed by NaN, +/-Infinity, +/-0
+void TurboAssembler::RoundFloat(FPURegister dst, FPURegister src,
+                                FPURegister fpu_scratch, FPURoundingMode frm) {
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  UseScratchRegisterScope temps(this);
+  Register scratch2 = temps.Acquire();
+
+  // Need at least two FPRs, so check against dst == src == fpu_scratch
+  DCHECK(!(dst == src && dst == fpu_scratch));
+
+  const int kFloatMantissaBits = kFloat32MantissaBits;
+  const int kFloatExponentBits = kFloat32ExponentBits;
+  const int kFloatExponentBias = kFloat32ExponentBias;
+  Label done;
+
+  {
+    UseScratchRegisterScope temps2(this);
+    Register scratch = temps2.Acquire();
+    // extract exponent value of the source floating-point to scratch
+    fmv_x_w(scratch, src);
+    ExtractBits(scratch2, scratch, kFloatMantissaBits, kFloatExponentBits);
+  }
+
+  // if src is NaN/+-Infinity/+-Zero or if the exponent is larger than # of bits
+  // in mantissa, the result is the same as src, so move src to dest  (to avoid
+  // generating another branch)
+  if (dst != src) {
+    fmv_s(dst, src);
+  }
+  {
+    Label not_NaN;
+    UseScratchRegisterScope temps2(this);
+    Register scratch = temps2.Acquire();
+    // According to the wasm spec
+    // (https://webassembly.github.io/spec/core/exec/numerics.html#aux-nans)
+    // if input is canonical NaN, then output is canonical NaN, and if input is
+    // any other NaN, then output is any NaN with most significant bit of
+    // payload is 1. In RISC-V, feq_d will set scratch to 0 if src is a NaN. If
+    // src is not a NaN, branch to the label and do nothing, but if it is,
+    // fmin_d will set dst to the canonical NaN.
+    feq_s(scratch, src, src);
+    bnez(scratch, &not_NaN);
+    fmin_s(dst, src, src);
+    bind(&not_NaN);
+  }
+
+  // If real exponent (i.e., scratch2 - kFloatExponentBias) is greater than
+  // kFloat32MantissaBits, it means the floating-point value has no fractional
+  // part, thus the input is already rounded, jump to done. Note that, NaN and
+  // Infinity in floating-point representation sets maximal exponent value, so
+  // they also satisfy (scratch2 - kFloatExponentBias >= kFloatMantissaBits),
+  // and JS round semantics specify that rounding of NaN (Infinity) returns NaN
+  // (Infinity), so NaN and Infinity are considered rounded value too.
+  Branch(&done, greater_equal, scratch2,
+         Operand(kFloatExponentBias + kFloatMantissaBits));
+
+  // Actual rounding is needed along this path
+
+  // old_src holds the original input, needed for the case of src == dst
+  FPURegister old_src = src;
+  if (src == dst) {
+    DCHECK(fpu_scratch != dst);
+    Move(fpu_scratch, src);
+    old_src = fpu_scratch;
+  }
+
+  // Since only input whose real exponent value is less than kMantissaBits
+  // (i.e., 23 or 52-bits) falls into this path, the value range of the input
+  // falls into that of 23- or 53-bit integers. So we round the input to integer
+  // values, then convert them back to floating-point.
+  {
+    UseScratchRegisterScope temps(this);
+    Register scratch = temps.Acquire();
+    fcvt_w_s(scratch, src, frm);
+    fcvt_s_w(dst, scratch, frm);
+  }
+  // A special handling is needed if the input is a very small positive/negative
+  // number that rounds to zero. JS semantics requires that the rounded result
+  // retains the sign of the input, so a very small positive (negative)
+  // floating-point number should be rounded to positive (negative) 0.
+  // Therefore, we use sign-bit injection to produce +/-0 correctly. Instead of
+  // testing for zero w/ a branch, we just insert sign-bit for everyone on this
+  // path (this is where old_src is needed)
+  fsgnj_s(dst, dst, old_src);
 
+  bind(&done);
+}
+#endif  // V8_TARGET_ARCH_RISCV32
 // According to JS ECMA specification, for floating-point round operations, if
 // the input is NaN, +/-infinity, or +/-0, the same input is returned as the
 // rounded result; this differs from behavior of RISCV fcvt instructions (which
@@ -2191,6 +2934,7 @@ void TurboAssembler::Round_d(VRegister vdst, VRegister vsrc, Register scratch,
   RoundHelper<double>(vdst, vsrc, scratch, v_scratch, RNE);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Floor_d_d(FPURegister dst, FPURegister src,
                                FPURegister fpu_scratch) {
   RoundHelper<double>(dst, src, fpu_scratch, RDN);
@@ -2210,25 +2954,42 @@ void TurboAssembler::Round_d_d(FPURegister dst, FPURegister src,
                                FPURegister fpu_scratch) {
   RoundHelper<double>(dst, src, fpu_scratch, RNE);
 }
+#endif
 
 void TurboAssembler::Floor_s_s(FPURegister dst, FPURegister src,
                                FPURegister fpu_scratch) {
+#if V8_TARGET_ARCH_RISCV64
   RoundHelper<float>(dst, src, fpu_scratch, RDN);
+#elif V8_TARGET_ARCH_RISCV32
+  RoundFloat(dst, src, fpu_scratch, RDN);
+#endif
 }
 
 void TurboAssembler::Ceil_s_s(FPURegister dst, FPURegister src,
                               FPURegister fpu_scratch) {
+#if V8_TARGET_ARCH_RISCV64
   RoundHelper<float>(dst, src, fpu_scratch, RUP);
+#elif V8_TARGET_ARCH_RISCV32
+  RoundFloat(dst, src, fpu_scratch, RUP);
+#endif
 }
 
 void TurboAssembler::Trunc_s_s(FPURegister dst, FPURegister src,
                                FPURegister fpu_scratch) {
+#if V8_TARGET_ARCH_RISCV64
   RoundHelper<float>(dst, src, fpu_scratch, RTZ);
+#elif V8_TARGET_ARCH_RISCV32
+  RoundFloat(dst, src, fpu_scratch, RTZ);
+#endif
 }
 
 void TurboAssembler::Round_s_s(FPURegister dst, FPURegister src,
                                FPURegister fpu_scratch) {
+#if V8_TARGET_ARCH_RISCV64
   RoundHelper<float>(dst, src, fpu_scratch, RNE);
+#elif V8_TARGET_ARCH_RISCV32
+  RoundFloat(dst, src, fpu_scratch, RNE);
+#endif
 }
 
 void MacroAssembler::Madd_s(FPURegister fd, FPURegister fr, FPURegister fs,
@@ -2374,6 +3135,7 @@ void TurboAssembler::BranchFalseF(Register rs, Label* target) {
 }
 
 void TurboAssembler::InsertHighWordF64(FPURegister dst, Register src_high) {
+#if V8_TARGET_ARCH_RISCV64
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   Register scratch2 = temps.Acquire();
@@ -2387,9 +3149,18 @@ void TurboAssembler::InsertHighWordF64(FPURegister dst, Register src_high) {
   srli(scratch, scratch, 32);
   or_(scratch, scratch, scratch2);
   fmv_d_x(dst, scratch);
+#elif V8_TARGET_ARCH_RISCV32
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Add32(sp, sp, Operand(-8));
+  StoreDouble(dst, MemOperand(sp, 0));
+  Sw(src_high, MemOperand(sp, 4));
+  LoadDouble(dst, MemOperand(sp, 0));
+  Add32(sp, sp, Operand(8));
+#endif
 }
 
 void TurboAssembler::InsertLowWordF64(FPURegister dst, Register src_low) {
+#if V8_TARGET_ARCH_RISCV64
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   Register scratch2 = temps.Acquire();
@@ -2403,6 +3174,14 @@ void TurboAssembler::InsertLowWordF64(FPURegister dst, Register src_low) {
   slli(scratch, scratch, 32);
   or_(scratch, scratch, scratch2);
   fmv_d_x(dst, scratch);
+#elif V8_TARGET_ARCH_RISCV32
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  AddWord(sp, sp, Operand(-8));
+  StoreDouble(dst, MemOperand(sp, 0));
+  Sw(src_low, MemOperand(sp, 0));
+  LoadDouble(dst, MemOperand(sp, 0));
+  AddWord(sp, sp, Operand(8));
+#endif
 }
 
 void TurboAssembler::LoadFPRImmediate(FPURegister dst, uint32_t src) {
@@ -2420,11 +3199,11 @@ void TurboAssembler::LoadFPRImmediate(FPURegister dst, uint32_t src) {
       has_single_zero_reg_set_ = true;
       has_double_zero_reg_set_ = false;
     } else {
-      UseScratchRegisterScope temps(this);
-      Register scratch = temps.Acquire();
       if (src == base::bit_cast<uint32_t>(0.0f)) {
         fcvt_s_w(dst, zero_reg);
       } else {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
         li(scratch, Operand(static_cast<int32_t>(src)));
         fmv_w_x(dst, scratch);
       }
@@ -2441,6 +3220,7 @@ void TurboAssembler::LoadFPRImmediate(FPURegister dst, uint64_t src) {
              has_double_zero_reg_set_) {
     Neg_d(dst, kDoubleRegZero);
   } else {
+#if V8_TARGET_ARCH_RISCV64
     if (dst == kDoubleRegZero) {
       DCHECK(src == base::bit_cast<uint64_t>(0.0));
       fcvt_d_l(dst, zero_reg);
@@ -2456,6 +3236,31 @@ void TurboAssembler::LoadFPRImmediate(FPURegister dst, uint64_t src) {
         fmv_d_x(dst, scratch);
       }
     }
+#elif V8_TARGET_ARCH_RISCV32
+    if (dst == kDoubleRegZero) {
+      DCHECK(src == base::bit_cast<uint64_t>(0.0));
+      fcvt_d_w(dst, zero_reg);
+      has_double_zero_reg_set_ = true;
+      has_single_zero_reg_set_ = false;
+    } else {
+      // Todo: need to clear the stack content?
+      if (src == base::bit_cast<uint64_t>(0.0)) {
+        fcvt_d_w(dst, zero_reg);
+      } else {
+        UseScratchRegisterScope temps(this);
+        Register scratch = temps.Acquire();
+        uint32_t low_32 = src & 0xffffffffull;
+        uint32_t up_32 = src >> 32;
+        AddWord(sp, sp, Operand(-8));
+        li(scratch, Operand(static_cast<int32_t>(low_32)));
+        Sw(scratch, MemOperand(sp, 0));
+        li(scratch, Operand(static_cast<int32_t>(up_32)));
+        Sw(scratch, MemOperand(sp, 4));
+        LoadDouble(dst, MemOperand(sp, 0));
+        AddWord(sp, sp, Operand(8));
+      }
+    }
+#endif
   }
 }
 
@@ -2546,6 +3351,7 @@ void TurboAssembler::Clz32(Register rd, Register xx) {
   DCHECK(xx != y && xx != n);
   Move(x, xx);
   li(n, Operand(32));
+#if V8_TARGET_ARCH_RISCV64
   srliw(y, x, 16);
   BranchShort(&L0, eq, y, Operand(zero_reg));
   Move(x, y);
@@ -2571,8 +3377,36 @@ void TurboAssembler::Clz32(Register rd, Register xx) {
   BranchShort(&L4, eq, y, Operand(zero_reg));
   addiw(rd, n, -2);
   bind(&L4);
+#elif V8_TARGET_ARCH_RISCV32
+  srli(y, x, 16);
+  BranchShort(&L0, eq, y, Operand(zero_reg));
+  Move(x, y);
+  addi(n, n, -16);
+  bind(&L0);
+  srli(y, x, 8);
+  BranchShort(&L1, eq, y, Operand(zero_reg));
+  addi(n, n, -8);
+  Move(x, y);
+  bind(&L1);
+  srli(y, x, 4);
+  BranchShort(&L2, eq, y, Operand(zero_reg));
+  addi(n, n, -4);
+  Move(x, y);
+  bind(&L2);
+  srli(y, x, 2);
+  BranchShort(&L3, eq, y, Operand(zero_reg));
+  addi(n, n, -2);
+  Move(x, y);
+  bind(&L3);
+  srli(y, x, 1);
+  sub(rd, n, x);
+  BranchShort(&L4, eq, y, Operand(zero_reg));
+  addi(rd, n, -2);
+  bind(&L4);
+#endif
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Clz64(Register rd, Register xx) {
   // 64 bit: count number of leading zeros.
   //  int n = 64;
@@ -2626,7 +3460,7 @@ void TurboAssembler::Clz64(Register rd, Register xx) {
   addiw(rd, n, -2);
   bind(&L5);
 }
-
+#endif
 void TurboAssembler::Ctz32(Register rd, Register rs) {
   // Convert trailing zeroes to trailing ones, and bits to their left
   // to zeroes.
@@ -2635,7 +3469,7 @@ void TurboAssembler::Ctz32(Register rd, Register rs) {
   {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs, -1);
+    AddWord(scratch, rs, -1);
     Xor(rd, scratch, rs);
     And(rd, rd, scratch);
     // Count number of leading zeroes.
@@ -2650,16 +3484,15 @@ void TurboAssembler::Ctz32(Register rd, Register rs) {
     Sub32(rd, scratch, rd);
   }
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Ctz64(Register rd, Register rs) {
   // Convert trailing zeroes to trailing ones, and bits to their left
   // to zeroes.
-
   BlockTrampolinePoolScope block_trampoline_pool(this);
   {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Add64(scratch, rs, -1);
+    AddWord(scratch, rs, -1);
     Xor(rd, scratch, rs);
     And(rd, rd, scratch);
     // Count number of leading zeroes.
@@ -2671,10 +3504,10 @@ void TurboAssembler::Ctz64(Register rd, Register rs) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     li(scratch, 64);
-    Sub64(rd, scratch, rd);
+    SubWord(rd, scratch, rd);
   }
 }
-
+#endif
 void TurboAssembler::Popcnt32(Register rd, Register rs, Register scratch) {
   DCHECK_NE(scratch, rs);
   DCHECK_NE(scratch, rd);
@@ -2716,7 +3549,7 @@ void TurboAssembler::Popcnt32(Register rd, Register rs, Register scratch) {
   Srl32(scratch, scratch, 2);
   And(scratch, scratch, scratch2);
   Add32(scratch, rd, scratch);
-  srliw(rd, scratch, 4);
+  Srl32(rd, scratch, 4);
   Add32(rd, rd, scratch);
   li(scratch2, 0xF);
   Mul32(scratch2, value, scratch2);  // B2 = 0x0F0F0F0F;
@@ -2725,6 +3558,7 @@ void TurboAssembler::Popcnt32(Register rd, Register rs, Register scratch) {
   Srl32(rd, rd, shift);
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Popcnt64(Register rd, Register rs, Register scratch) {
   DCHECK_NE(scratch, rs);
   DCHECK_NE(scratch, rd);
@@ -2733,7 +3567,6 @@ void TurboAssembler::Popcnt64(Register rd, Register rs, Register scratch) {
   // uint64_t B2 = 0x0F0F0F0F0F0F0F0Fl;     // (T)~(T)0/255*15
   // uint64_t value = 0x0101010101010101l;  // (T)~(T)0/255
   // uint64_t shift = 24;                   // (sizeof(T) - 1) * BITS_PER_BYTE
-
   uint64_t shift = 24;
   UseScratchRegisterScope temps(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
@@ -2745,15 +3578,15 @@ void TurboAssembler::Popcnt64(Register rd, Register rs, Register scratch) {
   Mul64(scratch2, value, scratch2);  // B0 = 0x5555555555555555l;
   Srl64(scratch, rs, 1);
   And(scratch, scratch, scratch2);
-  Sub64(scratch, rs, scratch);
+  SubWord(scratch, rs, scratch);
   li(scratch2, 3);
   Mul64(scratch2, value, scratch2);  // B1 = 0x3333333333333333l;
   And(rd, scratch, scratch2);
   Srl64(scratch, scratch, 2);
   And(scratch, scratch, scratch2);
-  Add64(scratch, rd, scratch);
+  AddWord(scratch, rd, scratch);
   Srl64(rd, scratch, 4);
-  Add64(rd, rd, scratch);
+  AddWord(rd, rd, scratch);
   li(scratch2, 0xF);
   li(value, 0x0101010101010101l);    // value = 0x0101010101010101l;
   Mul64(scratch2, value, scratch2);  // B2 = 0x0F0F0F0F0F0F0F0Fl;
@@ -2761,7 +3594,7 @@ void TurboAssembler::Popcnt64(Register rd, Register rs, Register scratch) {
   Mul64(rd, rd, value);
   srli(rd, rd, 32 + shift);
 }
-
+#endif
 void TurboAssembler::TryInlineTruncateDoubleToI(Register result,
                                                 DoubleRegister double_input,
                                                 Label* done) {
@@ -2784,7 +3617,7 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
   // If we fell through then inline version didn't succeed - call stub
   // instead.
   push(ra);
-  Sub64(sp, sp, Operand(kDoubleSize));  // Put input on stack.
+  SubWord(sp, sp, Operand(kDoubleSize));  // Put input on stack.
   fsd(double_input, sp, 0);
 
   if (stub_mode == StubCallMode::kCallWasmRuntimeStub) {
@@ -2792,9 +3625,9 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
   } else {
     Call(BUILTIN_CODE(isolate, DoubleToI), RelocInfo::CODE_TARGET);
   }
-  ld(result, sp, 0);
+  LoadWord(result, MemOperand(sp, 0));
 
-  Add64(sp, sp, Operand(kDoubleSize));
+  AddWord(sp, sp, Operand(kDoubleSize));
   pop(ra);
 
   bind(&done);
@@ -3190,7 +4023,7 @@ void TurboAssembler::LoadFromConstantsTable(Register destination,
 }
 
 void TurboAssembler::LoadRootRelative(Register destination, int32_t offset) {
-  Ld(destination, MemOperand(kRootRegister, offset));
+  LoadWord(destination, MemOperand(kRootRegister, offset));
 }
 
 void TurboAssembler::LoadRootRegisterOffset(Register destination,
@@ -3198,7 +4031,7 @@ void TurboAssembler::LoadRootRegisterOffset(Register destination,
   if (offset == 0) {
     Move(destination, kRootRegister);
   } else {
-    Add64(destination, kRootRegister, Operand(offset));
+    AddWord(destination, kRootRegister, Operand(offset));
   }
 }
 
@@ -3261,7 +4094,7 @@ void TurboAssembler::Jump(Handle<Code> code, RelocInfo::Mode rmode,
              target_is_isolate_independent_builtin) {
     int offset = static_cast<int>(code->builtin_id()) * kSystemPointerSize +
                  IsolateData::builtin_entry_table_offset();
-    Ld(t6, MemOperand(kRootRegister, offset));
+    LoadWord(t6, MemOperand(kRootRegister, offset));
     Jump(t6, cond, rs, rt);
     return;
   } else if (options().inline_offheap_trampolines &&
@@ -3301,7 +4134,7 @@ void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
   if (lower_limit != 0) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
-    Sub64(scratch, value, Operand(lower_limit));
+    SubWord(scratch, value, Operand(lower_limit));
     Branch(on_in_range, Uless_equal, scratch,
            Operand(higher_limit - lower_limit));
   } else {
@@ -3312,7 +4145,7 @@ void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
 
 void TurboAssembler::Call(Address target, RelocInfo::Mode rmode, Condition cond,
                           Register rs, const Operand& rt) {
-  li(t6, Operand(static_cast<int64_t>(target), rmode), ADDRESS_LOAD);
+  li(t6, Operand(static_cast<intptr_t>(target), rmode), ADDRESS_LOAD);
   Call(t6, cond, rs, rt);
 }
 
@@ -3359,14 +4192,19 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin) {
+#if V8_TARGET_ARCH_RISCV64
   static_assert(kSystemPointerSize == 8);
+#elif V8_TARGET_ARCH_RISCV32
+  static_assert(kSystemPointerSize == 4);
+#endif
   static_assert(kSmiTagSize == 1);
   static_assert(kSmiTag == 0);
 
   // The builtin register contains the builtin index as a Smi.
   SmiUntag(builtin, builtin);
   CalcScaledAddress(builtin, kRootRegister, builtin, kSystemPointerSizeLog2);
-  Ld(builtin, MemOperand(builtin, IsolateData::builtin_entry_table_offset()));
+  LoadWord(builtin,
+           MemOperand(builtin, IsolateData::builtin_entry_table_offset()));
 }
 
 void TurboAssembler::CallBuiltinByIndex(Register builtin) {
@@ -3396,7 +4234,7 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
 
 void TurboAssembler::LoadEntryFromBuiltin(Builtin builtin,
                                           Register destination) {
-  Ld(destination, EntryFromBuiltinAsOperand(builtin));
+  LoadWord(destination, EntryFromBuiltinAsOperand(builtin));
 }
 
 MemOperand TurboAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
@@ -3409,12 +4247,16 @@ void TurboAssembler::PatchAndJump(Address target) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   auipc(scratch, 0);  // Load PC into scratch
-  Ld(t6, MemOperand(scratch, kInstrSize * 4));
+  LoadWord(t6, MemOperand(scratch, kInstrSize * 4));
   jr(t6);
   nop();  // For alignment
+#if V8_TARGET_ARCH_RISCV64
   DCHECK_EQ(reinterpret_cast<uint64_t>(pc_) % 8, 0);
-  *reinterpret_cast<uint64_t*>(pc_) = target;  // pc_ should be align.
-  pc_ += sizeof(uint64_t);
+#elif V8_TARGET_ARCH_RISCV32
+  DCHECK_EQ(reinterpret_cast<uint32_t>(pc_) % 4, 0);
+#endif
+  *reinterpret_cast<uintptr_t*>(pc_) = target;  // pc_ should be align.
+  pc_ += sizeof(uintptr_t);
 }
 
 void TurboAssembler::StoreReturnAddressAndCall(Register target) {
@@ -3443,7 +4285,7 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
            kInstrSize);  // Set ra to insn after the call
 
   // This spot was reserved in EnterExitFrame.
-  Sd(ra, MemOperand(sp));
+  StoreWord(ra, MemOperand(sp));
   addi(sp, sp, -kCArgsSlotsSize);
   // Stack is still aligned.
 
@@ -3465,22 +4307,22 @@ void TurboAssembler::Ret(Condition cond, Register rs, const Operand& rt) {
 void TurboAssembler::BranchLong(Label* L) {
   // Generate position independent long branch.
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  int64_t imm64;
-  imm64 = branch_long_offset(L);
-  GenPCRelativeJump(t6, imm64);
+  int32_t imm;
+  imm = branch_long_offset(L);
+  GenPCRelativeJump(t6, imm);
   EmitConstPoolWithJumpIfNeeded();
 }
 
 void TurboAssembler::BranchAndLinkLong(Label* L) {
   // Generate position independent long branch and link.
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  int64_t imm64;
-  imm64 = branch_long_offset(L);
-  GenPCRelativeJumpAndLink(t6, imm64);
+  int32_t imm;
+  imm = branch_long_offset(L);
+  GenPCRelativeJumpAndLink(t6, imm);
 }
 
 void TurboAssembler::DropAndRet(int drop) {
-  Add64(sp, sp, drop * kSystemPointerSize);
+  AddWord(sp, sp, drop * kSystemPointerSize);
   Ret();
 }
 
@@ -3512,7 +4354,7 @@ void TurboAssembler::Drop(int count, Condition cond, Register reg,
     Branch(&skip, NegateCondition(cond), reg, op);
   }
 
-  Add64(sp, sp, Operand(count * kSystemPointerSize));
+  AddWord(sp, sp, Operand(count * kSystemPointerSize));
 
   if (cond != al) {
     bind(&skip);
@@ -3544,7 +4386,7 @@ void TurboAssembler::LoadAddress(Register dst, Label* target,
     auipc(dst, Hi20);
     addi(dst, dst, Lo12);
   } else {
-    uint64_t address = jump_address(target);
+    uintptr_t address = jump_address(target);
     li(dst, Operand(address, rmode), ADDRESS_LOAD);
   }
 }
@@ -3567,9 +4409,9 @@ void TurboAssembler::PushArray(Register array, Register size,
     jmp(&entry);
     bind(&loop);
     CalcScaledAddress(scratch2, array, scratch, kSystemPointerSizeLog2);
-    Ld(scratch2, MemOperand(scratch2));
+    LoadWord(scratch2, MemOperand(scratch2));
     push(scratch2);
-    Add64(scratch, scratch, Operand(1));
+    AddWord(scratch, scratch, Operand(1));
     bind(&entry);
     Branch(&loop, less, scratch, Operand(size));
   } else {
@@ -3577,10 +4419,10 @@ void TurboAssembler::PushArray(Register array, Register size,
     jmp(&entry);
     bind(&loop);
     CalcScaledAddress(scratch2, array, scratch, kSystemPointerSizeLog2);
-    Ld(scratch2, MemOperand(scratch2));
+    LoadWord(scratch2, MemOperand(scratch2));
     push(scratch2);
     bind(&entry);
-    Add64(scratch, scratch, Operand(-1));
+    AddWord(scratch, scratch, Operand(-1));
     Branch(&loop, greater_equal, scratch, Operand(zero_reg));
   }
 }
@@ -3608,24 +4450,24 @@ void MacroAssembler::PushStackHandler() {
   li(handler_address,
      ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
   Register handler = temps.Acquire();
-  Ld(handler, MemOperand(handler_address));
+  LoadWord(handler, MemOperand(handler_address));
   push(handler);
 
   // Set this new handler as the current one.
-  Sd(sp, MemOperand(handler_address));
+  StoreWord(sp, MemOperand(handler_address));
 }
 
 void MacroAssembler::PopStackHandler() {
   static_assert(StackHandlerConstants::kNextOffset == 0);
   pop(a1);
-  Add64(sp, sp,
-        Operand(static_cast<int64_t>(StackHandlerConstants::kSize -
-                                     kSystemPointerSize)));
+  AddWord(sp, sp,
+          Operand(static_cast<intptr_t>(StackHandlerConstants::kSize -
+                                        kSystemPointerSize)));
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   li(scratch,
      ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
-  Sd(a1, MemOperand(scratch));
+  StoreWord(a1, MemOperand(scratch));
 }
 
 void TurboAssembler::FPUCanonicalizeNaN(const DoubleRegister dst,
@@ -3681,7 +4523,8 @@ void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind) {
   intptr_t offset =
       TurboAssembler::RootRegisterOffsetForExternalReference(isolate, limit);
   CHECK(is_int32(offset));
-  Ld(destination, MemOperand(kRootRegister, static_cast<int32_t>(offset)));
+  LoadWord(destination,
+           MemOperand(kRootRegister, static_cast<int32_t>(offset)));
 }
 
 void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch1,
@@ -3694,9 +4537,9 @@ void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch1,
   LoadStackLimit(scratch1, StackLimitKind::kRealStackLimit);
   // Make scratch1 the space we have left. The stack might already be overflowed
   // here which will cause scratch1 to become negative.
-  Sub64(scratch1, sp, scratch1);
+  SubWord(scratch1, sp, scratch1);
   // Check if the arguments will overflow the stack.
-  Sll64(scratch2, num_args, kSystemPointerSizeLog2);
+  SllWord(scratch2, num_args, kSystemPointerSizeLog2);
   // Signed comparison.
   if (stack_overflow != nullptr) {
     Branch(stack_overflow, le, scratch1, Operand(scratch2));
@@ -3727,8 +4570,8 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
   }
   // If overapplication or if the actual argument count is equal to the
   // formal parameter count, no need to push extra undefined values.
-  Sub64(expected_parameter_count, expected_parameter_count,
-        actual_parameter_count);
+  SubWord(expected_parameter_count, expected_parameter_count,
+          actual_parameter_count);
   Branch(&regular_invoke, le, expected_parameter_count, Operand(zero_reg));
 
   Label stack_overflow;
@@ -3743,17 +4586,17 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
     Label copy;
     Register src = a6, dest = a7;
     Move(src, sp);
-    Sll64(t0, expected_parameter_count, kSystemPointerSizeLog2);
-    Sub64(sp, sp, Operand(t0));
+    SllWord(t0, expected_parameter_count, kSystemPointerSizeLog2);
+    SubWord(sp, sp, Operand(t0));
     // Update stack pointer.
     Move(dest, sp);
     Move(t0, actual_parameter_count);
     bind(&copy);
-    Ld(t1, MemOperand(src, 0));
-    Sd(t1, MemOperand(dest, 0));
-    Sub64(t0, t0, Operand(1));
-    Add64(src, src, Operand(kSystemPointerSize));
-    Add64(dest, dest, Operand(kSystemPointerSize));
+    LoadWord(t1, MemOperand(src, 0));
+    StoreWord(t1, MemOperand(dest, 0));
+    SubWord(t0, t0, Operand(1));
+    AddWord(src, src, Operand(kSystemPointerSize));
+    AddWord(dest, dest, Operand(kSystemPointerSize));
     Branch(&copy, gt, t0, Operand(zero_reg));
   }
 
@@ -3762,9 +4605,9 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
   {
     Label loop;
     bind(&loop);
-    Sd(t0, MemOperand(a7, 0));
-    Sub64(expected_parameter_count, expected_parameter_count, Operand(1));
-    Add64(a7, a7, Operand(kSystemPointerSize));
+    StoreWord(t0, MemOperand(a7, 0));
+    SubWord(expected_parameter_count, expected_parameter_count, Operand(1));
+    AddWord(a7, a7, Operand(kSystemPointerSize));
     Branch(&loop, gt, expected_parameter_count, Operand(zero_reg));
   }
   Branch(&regular_invoke);
@@ -3922,7 +4765,7 @@ void MacroAssembler::GetInstanceTypeRange(Register map, Register type_reg,
                                           InstanceType lower_limit,
                                           Register range) {
   Lhu(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
-  Sub64(range, type_reg, Operand(lower_limit));
+  SubWord(range, type_reg, Operand(lower_limit));
 }
 //------------------------------------------------------------------------------
 // Wasm
@@ -4009,13 +4852,13 @@ void TurboAssembler::LoadLane(int ts, VRegister dst, uint8_t laneidx,
     vmv_sx(v0, kScratchReg);
     vmerge_vx(dst, kScratchReg2, dst);
   } else if (ts == 32) {
-    Lwu(kScratchReg2, src);
+    Load32U(kScratchReg2, src);
     VU.set(kScratchReg, E32, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
     vmerge_vx(dst, kScratchReg2, dst);
   } else if (ts == 64) {
-    Ld(kScratchReg2, src);
+    LoadWord(kScratchReg2, src);
     VU.set(kScratchReg, E64, m1);
     li(kScratchReg, 0x1 << laneidx);
     vmv_sx(v0, kScratchReg);
@@ -4047,12 +4890,12 @@ void TurboAssembler::StoreLane(int sz, VRegister src, uint8_t laneidx,
     VU.set(kScratchReg, E64, m1);
     vslidedown_vi(kSimd128ScratchReg, src, laneidx);
     vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sd(kScratchReg, dst);
+    StoreWord(kScratchReg, dst);
   }
 }
 // -----------------------------------------------------------------------------
 // Runtime calls.
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::AddOverflow64(Register dst, Register left,
                                    const Operand& right, Register overflow) {
   UseScratchRegisterScope temps(this);
@@ -4140,6 +4983,93 @@ void TurboAssembler::MulOverflow32(Register dst, Register left,
   sext_w(dst, overflow);
   xor_(overflow, overflow, dst);
 }
+#elif V8_TARGET_ARCH_RISCV32
+void TurboAssembler::AddOverflow(Register dst, Register left,
+                                 const Operand& right, Register overflow) {
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register right_reg = no_reg;
+  Register scratch = temps.Acquire();
+  Register scratch2 = temps.Acquire();
+  if (!right.is_reg()) {
+    li(scratch, Operand(right));
+    right_reg = scratch;
+  } else {
+    right_reg = right.rm();
+  }
+  DCHECK(left != scratch2 && right_reg != scratch2 && dst != scratch2 &&
+         overflow != scratch2);
+  DCHECK(overflow != left && overflow != right_reg);
+  if (dst == left || dst == right_reg) {
+    add(scratch2, left, right_reg);
+    xor_(overflow, scratch2, left);
+    xor_(scratch, scratch2, right_reg);
+    and_(overflow, overflow, scratch);
+    Mv(dst, scratch2);
+  } else {
+    add(dst, left, right_reg);
+    xor_(overflow, dst, left);
+    xor_(scratch, dst, right_reg);
+    and_(overflow, overflow, scratch);
+  }
+}
+
+void TurboAssembler::SubOverflow(Register dst, Register left,
+                                 const Operand& right, Register overflow) {
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register right_reg = no_reg;
+  Register scratch = temps.Acquire();
+  Register scratch2 = temps.Acquire();
+  if (!right.is_reg()) {
+    li(scratch, Operand(right));
+    right_reg = scratch;
+  } else {
+    right_reg = right.rm();
+  }
+
+  DCHECK(left != scratch2 && right_reg != scratch2 && dst != scratch2 &&
+         overflow != scratch2);
+  DCHECK(overflow != left && overflow != right_reg);
+
+  if (dst == left || dst == right_reg) {
+    sub(scratch2, left, right_reg);
+    xor_(overflow, left, scratch2);
+    xor_(scratch, left, right_reg);
+    and_(overflow, overflow, scratch);
+    Mv(dst, scratch2);
+  } else {
+    sub(dst, left, right_reg);
+    xor_(overflow, left, dst);
+    xor_(scratch, left, right_reg);
+    and_(overflow, overflow, scratch);
+  }
+}
+
+void TurboAssembler::MulOverflow32(Register dst, Register left,
+                                   const Operand& right, Register overflow) {
+  ASM_CODE_COMMENT(this);
+  UseScratchRegisterScope temps(this);
+  BlockTrampolinePoolScope block_trampoline_pool(this);
+  Register right_reg = no_reg;
+  Register scratch = temps.Acquire();
+  Register scratch2 = temps.Acquire();
+  if (!right.is_reg()) {
+    li(scratch, Operand(right));
+    right_reg = scratch;
+  } else {
+    right_reg = right.rm();
+  }
+
+  DCHECK(left != scratch2 && right_reg != scratch2 && dst != scratch2 &&
+         overflow != scratch2);
+  DCHECK(overflow != left && overflow != right_reg);
+  mulh(overflow, left, right_reg);
+  mul(dst, left, right_reg);
+  srai(scratch2, dst, 31);
+  xor_(overflow, overflow, scratch2);
+}
+#endif
 
 void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
                                  SaveFPRegsMode save_doubles) {
@@ -4191,7 +5121,8 @@ void MacroAssembler::JumpToOffHeapInstructionStream(Address entry) {
     RecordEntry(entry, RelocInfo::OFF_HEAP_TARGET);
     RecordRelocInfo(RelocInfo::OFF_HEAP_TARGET, entry);
     auipc(kOffHeapTrampolineRegister, 0);
-    ld(kOffHeapTrampolineRegister, kOffHeapTrampolineRegister, 0);
+    LoadWord(kOffHeapTrampolineRegister,
+             MemOperand(kOffHeapTrampolineRegister, 0));
   }
   Jump(kOffHeapTrampolineRegister);
 }
@@ -4274,7 +5205,7 @@ void TurboAssembler::Abort(AbortReason reason) {
     // We don't care if we constructed a frame. Just pretend we did.
     FrameScope assume_frame(this, StackFrame::NO_FRAME_TYPE);
     PrepareCallCFunction(0, a0);
-    li(a0, Operand(static_cast<int64_t>(reason)));
+    li(a0, Operand(static_cast<intptr_t>(reason)));
     CallCFunction(ExternalReference::abort_with_reason(), 1);
     return;
   }
@@ -4350,8 +5281,8 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
 void TurboAssembler::LeaveFrame(StackFrame::Type type) {
   ASM_CODE_COMMENT(this);
   addi(sp, fp, 2 * kSystemPointerSize);
-  Ld(ra, MemOperand(fp, 1 * kSystemPointerSize));
-  Ld(fp, MemOperand(fp, 0 * kSystemPointerSize));
+  LoadWord(ra, MemOperand(fp, 1 * kSystemPointerSize));
+  LoadWord(fp, MemOperand(fp, 0 * kSystemPointerSize));
 }
 
 void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
@@ -4378,19 +5309,19 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Save registers and reserve room for saved entry sp.
   addi(sp, sp,
        -2 * kSystemPointerSize - ExitFrameConstants::kFixedFrameSizeFromFp);
-  Sd(ra, MemOperand(sp, 3 * kSystemPointerSize));
-  Sd(fp, MemOperand(sp, 2 * kSystemPointerSize));
+  StoreWord(ra, MemOperand(sp, 3 * kSystemPointerSize));
+  StoreWord(fp, MemOperand(sp, 2 * kSystemPointerSize));
   {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     li(scratch, Operand(StackFrame::TypeToMarker(frame_type)));
-    Sd(scratch, MemOperand(sp, 1 * kSystemPointerSize));
+    StoreWord(scratch, MemOperand(sp, 1 * kSystemPointerSize));
   }
   // Set up new frame pointer.
   addi(fp, sp, ExitFrameConstants::kFixedFrameSizeFromFp);
 
   if (FLAG_debug_code) {
-    Sd(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
+    StoreWord(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
 
   {
@@ -4400,17 +5331,17 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
     // Save the frame pointer and the context in top.
     li(scratch, ExternalReference::Create(IsolateAddressId::kCEntryFPAddress,
                                           isolate()));
-    Sd(fp, MemOperand(scratch));
+    StoreWord(fp, MemOperand(scratch));
     li(scratch,
        ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
-    Sd(cp, MemOperand(scratch));
+    StoreWord(cp, MemOperand(scratch));
   }
 
   const int frame_alignment = MacroAssembler::ActivationFrameAlignment();
   if (save_doubles) {
     // The stack is already aligned to 0 modulo 8 for stores with sdc1.
     int space = kNumCallerSavedFPU * kDoubleSize;
-    Sub64(sp, sp, Operand(space));
+    SubWord(sp, sp, Operand(space));
     int count = 0;
     for (int i = 0; i < kNumFPURegisters; i++) {
       if (kCallerSavedFPU.bits() & (1 << i)) {
@@ -4425,7 +5356,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // (used by DirectCEntry to hold the return value if a struct is
   // returned) and align the frame preparing for calling the runtime function.
   DCHECK_GE(stack_space, 0);
-  Sub64(sp, sp, Operand((stack_space + 2) * kSystemPointerSize));
+  SubWord(sp, sp, Operand((stack_space + 2) * kSystemPointerSize));
   if (frame_alignment > 0) {
     DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
     And(sp, sp, Operand(-frame_alignment));  // Align stack.
@@ -4436,7 +5367,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   addi(scratch, sp, kSystemPointerSize);
-  Sd(scratch, MemOperand(fp, ExitFrameConstants::kSPOffset));
+  StoreWord(scratch, MemOperand(fp, ExitFrameConstants::kSPOffset));
 }
 
 void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
@@ -4449,9 +5380,9 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
   // Optionally restore all double registers.
   if (save_doubles) {
     // Remember: we only need to restore kCallerSavedFPU.
-    Sub64(scratch, fp,
-          Operand(ExitFrameConstants::kFixedFrameSizeFromFp +
-                  kNumCallerSavedFPU * kDoubleSize));
+    SubWord(scratch, fp,
+            Operand(ExitFrameConstants::kFixedFrameSizeFromFp +
+                    kNumCallerSavedFPU * kDoubleSize));
     int cout = 0;
     for (int i = 0; i < kNumFPURegisters; i++) {
       if (kCalleeSavedFPU.bits() & (1 << i)) {
@@ -4465,24 +5396,24 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
   // Clear top frame.
   li(scratch,
      ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate()));
-  Sd(zero_reg, MemOperand(scratch));
+  StoreWord(zero_reg, MemOperand(scratch));
 
   // Restore current context from top and clear it in debug mode.
   li(scratch,
      ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
-  Ld(cp, MemOperand(scratch));
+  LoadWord(cp, MemOperand(scratch));
 
   if (FLAG_debug_code) {
     UseScratchRegisterScope temp(this);
     Register scratch2 = temp.Acquire();
     li(scratch2, Operand(Context::kInvalidContext));
-    Sd(scratch2, MemOperand(scratch));
+    StoreWord(scratch2, MemOperand(scratch));
   }
 
   // Pop the arguments, restore registers, and return.
   Mv(sp, fp);  // Respect ABI stack constraint.
-  Ld(fp, MemOperand(sp, ExitFrameConstants::kCallerFPOffset));
-  Ld(ra, MemOperand(sp, ExitFrameConstants::kCallerPCOffset));
+  LoadWord(fp, MemOperand(sp, ExitFrameConstants::kCallerFPOffset));
+  LoadWord(ra, MemOperand(sp, ExitFrameConstants::kCallerPCOffset));
 
   if (argument_count.is_valid()) {
     if (argument_count_is_length) {
@@ -4500,7 +5431,7 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
 }
 
 int TurboAssembler::ActivationFrameAlignment() {
-#if V8_HOST_ARCH_RISCV64
+#if V8_HOST_ARCH_RISCV32 || V8_HOST_ARCH_RISCV64
   // Running on the real platform. Use the alignment as mandated by the local
   // environment.
   // Note: This will break if we ever start generating snapshots on one RISC-V
@@ -4546,7 +5477,7 @@ void TurboAssembler::SmiUntag(Register dst, const MemOperand& src) {
     if (COMPRESS_POINTERS_BOOL) {
       Lw(dst, src);
     } else {
-      Ld(dst, src);
+      LoadWord(dst, src);
     }
     SmiUntag(dst);
   }
@@ -4829,12 +5760,13 @@ void TurboAssembler::PrepareCallCFunction(int num_reg_arguments,
     // Make stack end at alignment and make room for stack arguments and the
     // original value of sp.
     Mv(scratch, sp);
-    Sub64(sp, sp, Operand((stack_passed_arguments + 1) * kSystemPointerSize));
+    SubWord(sp, sp, Operand((stack_passed_arguments + 1) * kSystemPointerSize));
     DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
     And(sp, sp, Operand(-frame_alignment));
-    Sd(scratch, MemOperand(sp, stack_passed_arguments * kSystemPointerSize));
+    StoreWord(scratch,
+              MemOperand(sp, stack_passed_arguments * kSystemPointerSize));
   } else {
-    Sub64(sp, sp, Operand(stack_passed_arguments * kSystemPointerSize));
+    SubWord(sp, sp, Operand(stack_passed_arguments * kSystemPointerSize));
   }
 }
 
@@ -4877,7 +5809,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
   // The argument stots are presumed to have been set up by
   // PrepareCallCFunction.
 
-#if V8_HOST_ARCH_RISCV64
+#if V8_HOST_ARCH_RISCV32 || V8_HOST_ARCH_RISCV64
   if (FLAG_debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
@@ -4896,7 +5828,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
       bind(&alignment_as_expected);
     }
   }
-#endif  // V8_HOST_ARCH_RISCV64
+#endif  // V8_HOST_ARCH_RISCV32 || V8_HOST_ARCH_RISCV64
 
   // Just call directly. The function called cannot cause a GC, or
   // allow preemption, so the return address in the link register
@@ -4917,16 +5849,17 @@ void TurboAssembler::CallCFunctionHelper(Register function,
     auipc(pc_scratch, 0);
     // See x64 code for reasoning about how to address the isolate data fields.
     if (root_array_available()) {
-      Sd(pc_scratch, MemOperand(kRootRegister,
-                                IsolateData::fast_c_call_caller_pc_offset()));
-      Sd(fp, MemOperand(kRootRegister,
-                        IsolateData::fast_c_call_caller_fp_offset()));
+      StoreWord(pc_scratch,
+                MemOperand(kRootRegister,
+                           IsolateData::fast_c_call_caller_pc_offset()));
+      StoreWord(fp, MemOperand(kRootRegister,
+                               IsolateData::fast_c_call_caller_fp_offset()));
     } else {
       DCHECK_NOT_NULL(isolate());
       li(scratch, ExternalReference::fast_c_call_caller_pc_address(isolate()));
-      Sd(pc_scratch, MemOperand(scratch));
+      StoreWord(pc_scratch, MemOperand(scratch));
       li(scratch, ExternalReference::fast_c_call_caller_fp_address(isolate()));
-      Sd(fp, MemOperand(scratch));
+      StoreWord(fp, MemOperand(scratch));
     }
 
     Call(function);
@@ -4936,7 +5869,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
       UseScratchRegisterScope temps(this);
       Register scratch = temps.Acquire();
       li(scratch, ExternalReference::fast_c_call_caller_fp_address(isolate()));
-      Sd(zero_reg, MemOperand(scratch));
+      StoreWord(zero_reg, MemOperand(scratch));
     }
   }
 
@@ -4944,9 +5877,9 @@ void TurboAssembler::CallCFunctionHelper(Register function,
       CalculateStackPassedDWords(num_reg_arguments, num_double_arguments);
 
   if (base::OS::ActivationFrameAlignment() > kSystemPointerSize) {
-    Ld(sp, MemOperand(sp, stack_passed_arguments * kSystemPointerSize));
+    LoadWord(sp, MemOperand(sp, stack_passed_arguments * kSystemPointerSize));
   } else {
-    Add64(sp, sp, Operand(stack_passed_arguments * kSystemPointerSize));
+    AddWord(sp, sp, Operand(stack_passed_arguments * kSystemPointerSize));
   }
 }
 
@@ -4955,7 +5888,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
 void TurboAssembler::CheckPageFlag(Register object, Register scratch, int mask,
                                    Condition cc, Label* condition_met) {
   And(scratch, object, Operand(~kPageAlignmentMask));
-  Ld(scratch, MemOperand(scratch, BasicMemoryChunk::kFlagsOffset));
+  LoadWord(scratch, MemOperand(scratch, BasicMemoryChunk::kFlagsOffset));
   And(scratch, scratch, Operand(mask));
   Branch(condition_met, cc, scratch, Operand(zero_reg));
 }
@@ -4984,7 +5917,7 @@ void TurboAssembler::ComputeCodeStartAddress(Register dst) {
   addi(ra, ra, kInstrSize * 2);  // ra = address of li
   int pc = pc_offset();
   li(dst, Operand(pc));
-  Sub64(dst, ra, dst);
+  SubWord(dst, ra, dst);
 
   pop(ra);  // Restore ra
 }
@@ -4994,8 +5927,8 @@ void TurboAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                            Label*) {
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  Ld(t6,
-     MemOperand(kRootRegister, IsolateData::BuiltinEntrySlotOffset(target)));
+  LoadWord(t6, MemOperand(kRootRegister,
+                          IsolateData::BuiltinEntrySlotOffset(target)));
   Call(t6);
   DCHECK_EQ(SizeOfCodeGeneratedSince(exit),
             (kind == DeoptimizeKind::kLazy) ? Deoptimizer::kLazyDeoptExitSize
@@ -5032,7 +5965,7 @@ void TurboAssembler::LoadCodeObjectEntry(Register destination,
     Branch(&if_code_is_off_heap, ne, scratch, Operand(zero_reg));
     // Not an off-heap trampoline object, the entry point is at
     // Code::raw_instruction_start().
-    Add64(destination, code_object, Code::kHeaderSize - kHeapObjectTag);
+    AddWord(destination, code_object, Code::kHeaderSize - kHeapObjectTag);
     Branch(&out);
 
     // An off-heap trampoline, the entry point is loaded from the builtin entry
@@ -5040,13 +5973,14 @@ void TurboAssembler::LoadCodeObjectEntry(Register destination,
     bind(&if_code_is_off_heap);
     Lw(scratch, FieldMemOperand(code_object, Code::kBuiltinIndexOffset));
     slli(destination, scratch, kSystemPointerSizeLog2);
-    Add64(destination, destination, kRootRegister);
-    Ld(destination,
-       MemOperand(destination, IsolateData::builtin_entry_table_offset()));
+    AddWord(destination, destination, kRootRegister);
+    LoadWord(
+        destination,
+        MemOperand(destination, IsolateData::builtin_entry_table_offset()));
 
     bind(&out);
   } else {
-    Add64(destination, code_object, Code::kHeaderSize - kHeapObjectTag);
+    AddWord(destination, code_object, Code::kHeaderSize - kHeapObjectTag);
   }
 }
 
@@ -5062,7 +5996,7 @@ void TurboAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
   LoadCodeObjectEntry(code_object, code_object);
   Jump(code_object);
 }
-
+#if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::LoadTaggedPointerField(const Register& destination,
                                             const MemOperand& field_operand) {
   if (COMPRESS_POINTERS_BOOL) {
@@ -5109,8 +6043,8 @@ void TurboAssembler::DecompressTaggedSigned(const Register& destination,
   Lwu(destination, field_operand);
   if (FLAG_debug_code) {
     // Corrupt the top 32 bits. Made up of 16 fixed bits and 16 pc offset bits.
-    Add64(destination, destination,
-          Operand(((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32));
+    AddWord(destination, destination,
+            Operand(((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32));
   }
 }
 
@@ -5118,23 +6052,23 @@ void TurboAssembler::DecompressTaggedPointer(const Register& destination,
                                              const MemOperand& field_operand) {
   ASM_CODE_COMMENT(this);
   Lwu(destination, field_operand);
-  Add64(destination, kPtrComprCageBaseRegister, destination);
+  AddWord(destination, kPtrComprCageBaseRegister, destination);
 }
 
 void TurboAssembler::DecompressTaggedPointer(const Register& destination,
                                              const Register& source) {
   ASM_CODE_COMMENT(this);
   And(destination, source, Operand(0xFFFFFFFF));
-  Add64(destination, kPtrComprCageBaseRegister, Operand(destination));
+  AddWord(destination, kPtrComprCageBaseRegister, Operand(destination));
 }
 
 void TurboAssembler::DecompressAnyTagged(const Register& destination,
                                          const MemOperand& field_operand) {
   ASM_CODE_COMMENT(this);
   Lwu(destination, field_operand);
-  Add64(destination, kPtrComprCageBaseRegister, destination);
+  AddWord(destination, kPtrComprCageBaseRegister, destination);
 }
-
+#endif
 void MacroAssembler::DropArguments(Register count, ArgumentsCountType type,
                                    ArgumentsCountMode mode, Register scratch) {
   switch (type) {
@@ -5146,16 +6080,16 @@ void MacroAssembler::DropArguments(Register count, ArgumentsCountType type,
       static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       DCHECK_NE(scratch, no_reg);
       SmiScale(scratch, count, kPointerSizeLog2);
-      Add64(sp, sp, scratch);
+      AddWord(sp, sp, scratch);
       break;
     }
     case kCountIsBytes: {
-      Add64(sp, sp, count);
+      AddWord(sp, sp, count);
       break;
     }
   }
   if (mode == kCountExcludesReceiver) {
-    Add64(sp, sp, kSystemPointerSize);
+    AddWord(sp, sp, kSystemPointerSize);
   }
 }
 
@@ -5168,7 +6102,7 @@ void MacroAssembler::DropArgumentsAndPushNewReceiver(Register argc,
   if (mode == kCountExcludesReceiver) {
     // Drop arguments without receiver and override old receiver.
     DropArguments(argc, type, kCountIncludesReceiver, scratch);
-    Sd(receiver, MemOperand(sp));
+    StoreWord(receiver, MemOperand(sp));
   } else {
     DropArguments(argc, type, mode, scratch);
     push(receiver);
@@ -5177,5 +6111,3 @@ void MacroAssembler::DropArgumentsAndPushNewReceiver(Register argc,
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/codegen/riscv64/macro-assembler-riscv64.h b/src/codegen/riscv/macro-assembler-riscv.h
similarity index 83%
rename from src/codegen/riscv64/macro-assembler-riscv64.h
rename to src/codegen/riscv/macro-assembler-riscv.h
index 51a63c7e205..31bb2f404c5 100644
--- a/src/codegen/riscv64/macro-assembler-riscv64.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -6,11 +6,11 @@
 #error This header must be included via macro-assembler.h
 #endif
 
-#ifndef V8_CODEGEN_RISCV64_MACRO_ASSEMBLER_RISCV64_H_
-#define V8_CODEGEN_RISCV64_MACRO_ASSEMBLER_RISCV64_H_
+#ifndef V8_CODEGEN_RISCV_MACRO_ASSEMBLER_RISCV_H_
+#define V8_CODEGEN_RISCV_MACRO_ASSEMBLER_RISCV_H_
 
+#include "src/codegen/assembler-arch.h"
 #include "src/codegen/assembler.h"
-#include "src/codegen/riscv64/assembler-riscv64.h"
 #include "src/common/globals.h"
 #include "src/execution/isolate-data.h"
 #include "src/objects/tagged-index.h"
@@ -18,6 +18,7 @@
 namespace v8 {
 namespace internal {
 
+#define xlen (uint8_t(sizeof(void*) * 8))
 // Forward declarations.
 enum class AbortReason : uint8_t;
 
@@ -160,12 +161,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 #undef COND_TYPED_ARGS
 #undef COND_ARGS
 
-  void AllocateStackSpace(Register bytes) { Sub64(sp, sp, bytes); }
+  void AllocateStackSpace(Register bytes) { SubWord(sp, sp, bytes); }
 
   void AllocateStackSpace(int bytes) {
     DCHECK_GE(bytes, 0);
     if (bytes == 0) return;
-    Sub64(sp, sp, Operand(bytes));
+    SubWord(sp, sp, Operand(bytes));
   }
 
   inline void NegateBool(Register rd, Register rs) { Xor(rd, rs, 1); }
@@ -195,11 +196,13 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void li_optimized(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);
   // Load int32 in the rd register.
   void li(Register rd, Operand j, LiFlags mode = OPTIMIZE_SIZE);
-  inline void li(Register rd, int64_t j, LiFlags mode = OPTIMIZE_SIZE) {
+  inline void li(Register rd, intptr_t j, LiFlags mode = OPTIMIZE_SIZE) {
     li(rd, Operand(j), mode);
   }
 
-  inline void Move(Register output, MemOperand operand) { Ld(output, operand); }
+  inline void Move(Register output, MemOperand operand) {
+    LoadWord(output, operand);
+  }
   void li(Register dst, Handle<HeapObject> value,
           RelocInfo::Mode rmode = RelocInfo::FULL_EMBEDDED_OBJECT);
   void li(Register dst, ExternalReference value, LiFlags mode = OPTIMIZE_SIZE);
@@ -210,18 +213,18 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void LoadRootRegisterOffset(Register destination, intptr_t offset) final;
   void LoadRootRelative(Register destination, int32_t offset) final;
 
-  inline void GenPCRelativeJump(Register rd, int64_t imm32) {
+  inline void GenPCRelativeJump(Register rd, int32_t imm32) {
     DCHECK(is_int32(imm32 + 0x800));
-    int32_t Hi20 = (((int32_t)imm32 + 0x800) >> 12);
-    int32_t Lo12 = (int32_t)imm32 << 20 >> 20;
+    int32_t Hi20 = ((imm32 + 0x800) >> 12);
+    int32_t Lo12 = imm32 << 20 >> 20;
     auipc(rd, Hi20);  // Read PC + Hi20 into scratch.
     jr(rd, Lo12);     // jump PC + Hi20 + Lo12
   }
 
-  inline void GenPCRelativeJumpAndLink(Register rd, int64_t imm32) {
+  inline void GenPCRelativeJumpAndLink(Register rd, int32_t imm32) {
     DCHECK(is_int32(imm32 + 0x800));
-    int32_t Hi20 = (((int32_t)imm32 + 0x800) >> 12);
-    int32_t Lo12 = (int32_t)imm32 << 20 >> 20;
+    int32_t Hi20 = ((imm32 + 0x800) >> 12);
+    int32_t Lo12 = imm32 << 20 >> 20;
     auipc(rd, Hi20);  // Read PC + Hi20 into scratch.
     jalr(rd, Lo12);   // jump PC + Hi20 + Lo12
   }
@@ -283,12 +286,9 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   void DropAndRet(int drop, Condition cond, Register reg, const Operand& op);
 
-  void Ld(Register rd, const MemOperand& rs);
-  void Sd(Register rd, const MemOperand& rs);
-
   void push(Register src) {
-    Add64(sp, sp, Operand(-kSystemPointerSize));
-    Sd(src, MemOperand(sp, 0));
+    AddWord(sp, sp, Operand(-kSystemPointerSize));
+    StoreWord(src, MemOperand(sp, 0));
   }
   void Push(Register src) { push(src); }
   void Push(Handle<HeapObject> handle);
@@ -296,44 +296,44 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   // Push two registers. Pushes leftmost register first (to highest address).
   void Push(Register src1, Register src2) {
-    Sub64(sp, sp, Operand(2 * kSystemPointerSize));
-    Sd(src1, MemOperand(sp, 1 * kSystemPointerSize));
-    Sd(src2, MemOperand(sp, 0 * kSystemPointerSize));
+    SubWord(sp, sp, Operand(2 * kSystemPointerSize));
+    StoreWord(src1, MemOperand(sp, 1 * kSystemPointerSize));
+    StoreWord(src2, MemOperand(sp, 0 * kSystemPointerSize));
   }
 
   // Push three registers. Pushes leftmost register first (to highest address).
   void Push(Register src1, Register src2, Register src3) {
-    Sub64(sp, sp, Operand(3 * kSystemPointerSize));
-    Sd(src1, MemOperand(sp, 2 * kSystemPointerSize));
-    Sd(src2, MemOperand(sp, 1 * kSystemPointerSize));
-    Sd(src3, MemOperand(sp, 0 * kSystemPointerSize));
+    SubWord(sp, sp, Operand(3 * kSystemPointerSize));
+    StoreWord(src1, MemOperand(sp, 2 * kSystemPointerSize));
+    StoreWord(src2, MemOperand(sp, 1 * kSystemPointerSize));
+    StoreWord(src3, MemOperand(sp, 0 * kSystemPointerSize));
   }
 
   // Push four registers. Pushes leftmost register first (to highest address).
   void Push(Register src1, Register src2, Register src3, Register src4) {
-    Sub64(sp, sp, Operand(4 * kSystemPointerSize));
-    Sd(src1, MemOperand(sp, 3 * kSystemPointerSize));
-    Sd(src2, MemOperand(sp, 2 * kSystemPointerSize));
-    Sd(src3, MemOperand(sp, 1 * kSystemPointerSize));
-    Sd(src4, MemOperand(sp, 0 * kSystemPointerSize));
+    SubWord(sp, sp, Operand(4 * kSystemPointerSize));
+    StoreWord(src1, MemOperand(sp, 3 * kSystemPointerSize));
+    StoreWord(src2, MemOperand(sp, 2 * kSystemPointerSize));
+    StoreWord(src3, MemOperand(sp, 1 * kSystemPointerSize));
+    StoreWord(src4, MemOperand(sp, 0 * kSystemPointerSize));
   }
 
   // Push five registers. Pushes leftmost register first (to highest address).
   void Push(Register src1, Register src2, Register src3, Register src4,
             Register src5) {
-    Sub64(sp, sp, Operand(5 * kSystemPointerSize));
-    Sd(src1, MemOperand(sp, 4 * kSystemPointerSize));
-    Sd(src2, MemOperand(sp, 3 * kSystemPointerSize));
-    Sd(src3, MemOperand(sp, 2 * kSystemPointerSize));
-    Sd(src4, MemOperand(sp, 1 * kSystemPointerSize));
-    Sd(src5, MemOperand(sp, 0 * kSystemPointerSize));
+    SubWord(sp, sp, Operand(5 * kSystemPointerSize));
+    StoreWord(src1, MemOperand(sp, 4 * kSystemPointerSize));
+    StoreWord(src2, MemOperand(sp, 3 * kSystemPointerSize));
+    StoreWord(src3, MemOperand(sp, 2 * kSystemPointerSize));
+    StoreWord(src4, MemOperand(sp, 1 * kSystemPointerSize));
+    StoreWord(src5, MemOperand(sp, 0 * kSystemPointerSize));
   }
 
   void Push(Register src, Condition cond, Register tst1, Register tst2) {
     // Since we don't have conditional execution we use a Branch.
     Branch(3, cond, tst1, Operand(tst2));
-    Sub64(sp, sp, Operand(kSystemPointerSize));
-    Sd(src, MemOperand(sp, 0));
+    SubWord(sp, sp, Operand(kSystemPointerSize));
+    StoreWord(src, MemOperand(sp, 0));
   }
 
   enum PushArrayOrder { kNormal, kReverse };
@@ -377,29 +377,29 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
                      Register exclusion3 = no_reg);
 
   void pop(Register dst) {
-    Ld(dst, MemOperand(sp, 0));
-    Add64(sp, sp, Operand(kSystemPointerSize));
+    LoadWord(dst, MemOperand(sp, 0));
+    AddWord(sp, sp, Operand(kSystemPointerSize));
   }
   void Pop(Register dst) { pop(dst); }
 
   // Pop two registers. Pops rightmost register first (from lower address).
   void Pop(Register src1, Register src2) {
     DCHECK(src1 != src2);
-    Ld(src2, MemOperand(sp, 0 * kSystemPointerSize));
-    Ld(src1, MemOperand(sp, 1 * kSystemPointerSize));
-    Add64(sp, sp, 2 * kSystemPointerSize);
+    LoadWord(src2, MemOperand(sp, 0 * kSystemPointerSize));
+    LoadWord(src1, MemOperand(sp, 1 * kSystemPointerSize));
+    AddWord(sp, sp, 2 * kSystemPointerSize);
   }
 
   // Pop three registers. Pops rightmost register first (from lower address).
   void Pop(Register src1, Register src2, Register src3) {
-    Ld(src3, MemOperand(sp, 0 * kSystemPointerSize));
-    Ld(src2, MemOperand(sp, 1 * kSystemPointerSize));
-    Ld(src1, MemOperand(sp, 2 * kSystemPointerSize));
-    Add64(sp, sp, 3 * kSystemPointerSize);
+    LoadWord(src3, MemOperand(sp, 0 * kSystemPointerSize));
+    LoadWord(src2, MemOperand(sp, 1 * kSystemPointerSize));
+    LoadWord(src1, MemOperand(sp, 2 * kSystemPointerSize));
+    AddWord(sp, sp, 3 * kSystemPointerSize);
   }
 
   void Pop(uint32_t count = 1) {
-    Add64(sp, sp, Operand(count * kSystemPointerSize));
+    AddWord(sp, sp, Operand(count * kSystemPointerSize));
   }
 
   // Pops multiple values from the stack and load them in the
@@ -419,8 +419,14 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void instr(Register rs, Register rt) { instr(rs, Operand(rt)); } \
   void instr(Register rs, int32_t j) { instr(rs, Operand(j)); }
 
-#define DEFINE_INSTRUCTION3(instr) void instr(Register rd, int64_t imm);
+#define DEFINE_INSTRUCTION3(instr) void instr(Register rd, intptr_t imm);
 
+  DEFINE_INSTRUCTION(AddWord)
+  DEFINE_INSTRUCTION(SubWord)
+  DEFINE_INSTRUCTION(SllWord)
+  DEFINE_INSTRUCTION(SrlWord)
+  DEFINE_INSTRUCTION(SraWord)
+#if V8_TARGET_ARCH_RISCV64
   DEFINE_INSTRUCTION(Add32)
   DEFINE_INSTRUCTION(Add64)
   DEFINE_INSTRUCTION(Div32)
@@ -441,7 +447,23 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   DEFINE_INSTRUCTION2(Div64)
   DEFINE_INSTRUCTION2(Divu32)
   DEFINE_INSTRUCTION2(Divu64)
-
+  DEFINE_INSTRUCTION(Sll64)
+  DEFINE_INSTRUCTION(Sra64)
+  DEFINE_INSTRUCTION(Srl64)
+  DEFINE_INSTRUCTION(Dror)
+#elif V8_TARGET_ARCH_RISCV32
+  DEFINE_INSTRUCTION(Add32)
+  DEFINE_INSTRUCTION(Div)
+  DEFINE_INSTRUCTION(Divu)
+  DEFINE_INSTRUCTION(Mod)
+  DEFINE_INSTRUCTION(Modu)
+  DEFINE_INSTRUCTION(Sub32)
+  DEFINE_INSTRUCTION(Mul)
+  DEFINE_INSTRUCTION(Mul32)
+  DEFINE_INSTRUCTION(Mulh)
+  DEFINE_INSTRUCTION2(Div)
+  DEFINE_INSTRUCTION2(Divu)
+#endif
   DEFINE_INSTRUCTION(And)
   DEFINE_INSTRUCTION(Or)
   DEFINE_INSTRUCTION(Xor)
@@ -458,10 +480,6 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   DEFINE_INSTRUCTION(Sgeu)
   DEFINE_INSTRUCTION(Seq)
   DEFINE_INSTRUCTION(Sne)
-
-  DEFINE_INSTRUCTION(Sll64)
-  DEFINE_INSTRUCTION(Sra64)
-  DEFINE_INSTRUCTION(Srl64)
   DEFINE_INSTRUCTION(Sll32)
   DEFINE_INSTRUCTION(Sra32)
   DEFINE_INSTRUCTION(Srl32)
@@ -470,7 +488,6 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   DEFINE_INSTRUCTION2(Snez)
 
   DEFINE_INSTRUCTION(Ror)
-  DEFINE_INSTRUCTION(Dror)
 
   DEFINE_INSTRUCTION3(Li)
   DEFINE_INSTRUCTION2(Mv)
@@ -479,14 +496,26 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 #undef DEFINE_INSTRUCTION2
 #undef DEFINE_INSTRUCTION3
 
+  void Amosub_w(bool aq, bool rl, Register rd, Register rs1, Register rs2) {
+    UseScratchRegisterScope temps(this);
+    Register temp = temps.Acquire();
+    sub(temp, zero_reg, rs2);
+    amoadd_w(aq, rl, rd, rs1, temp);
+  }
+
   void SmiUntag(Register dst, const MemOperand& src);
   void SmiUntag(Register dst, Register src) {
+#if V8_TARGET_ARCH_RISCV64
     DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
     if (COMPRESS_POINTERS_BOOL) {
       sraiw(dst, src, kSmiShift);
     } else {
       srai(dst, src, kSmiShift);
     }
+#elif V8_TARGET_ARCH_RISCV32
+    DCHECK(SmiValuesAre31Bits());
+    srai(dst, src, kSmiShift);
+#endif
   }
 
   void SmiUntag(Register reg) { SmiUntag(reg, reg); }
@@ -557,27 +586,72 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void LoadZeroIfConditionZero(Register dest, Register condition);
 
   void SignExtendByte(Register rd, Register rs) {
-    slli(rd, rs, 64 - 8);
-    srai(rd, rd, 64 - 8);
+    slli(rd, rs, xlen - 8);
+    srai(rd, rd, xlen - 8);
   }
 
   void SignExtendShort(Register rd, Register rs) {
-    slli(rd, rs, 64 - 16);
-    srai(rd, rd, 64 - 16);
+    slli(rd, rs, xlen - 16);
+    srai(rd, rd, xlen - 16);
   }
 
+  void Clz32(Register rd, Register rs);
+  void Ctz32(Register rd, Register rs);
+  void Popcnt32(Register rd, Register rs, Register scratch);
+
+#if V8_TARGET_ARCH_RISCV64
   void SignExtendWord(Register rd, Register rs) { sext_w(rd, rs); }
   void ZeroExtendWord(Register rd, Register rs) {
     slli(rd, rs, 32);
     srli(rd, rd, 32);
   }
-
-  void Clz32(Register rd, Register rs);
-  void Clz64(Register rd, Register rs);
-  void Ctz32(Register rd, Register rs);
-  void Ctz64(Register rd, Register rs);
-  void Popcnt32(Register rd, Register rs, Register scratch);
   void Popcnt64(Register rd, Register rs, Register scratch);
+  void Ctz64(Register rd, Register rs);
+  void Clz64(Register rd, Register rs);
+#elif V8_TARGET_ARCH_RISCV32
+  void AddPair(Register dst_low, Register dst_high, Register left_low,
+               Register left_high, Register right_low, Register right_high,
+               Register scratch1, Register scratch2);
+
+  void SubPair(Register dst_low, Register dst_high, Register left_low,
+               Register left_high, Register right_low, Register right_high,
+               Register scratch1, Register scratch2);
+
+  void AndPair(Register dst_low, Register dst_high, Register left_low,
+               Register left_high, Register right_low, Register right_high);
+
+  void OrPair(Register dst_low, Register dst_high, Register left_low,
+              Register left_high, Register right_low, Register right_high);
+
+  void XorPair(Register dst_low, Register dst_high, Register left_low,
+               Register left_high, Register right_low, Register right_high);
+
+  void MulPair(Register dst_low, Register dst_high, Register left_low,
+               Register left_high, Register right_low, Register right_high,
+               Register scratch1, Register scratch2);
+
+  void ShlPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, Register shift, Register scratch1,
+               Register scratch2);
+  void ShlPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, int32_t shift, Register scratch1,
+               Register scratch2);
+
+  void ShrPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, Register shift, Register scratch1,
+               Register scratch2);
+
+  void ShrPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, int32_t shift, Register scratch1,
+               Register scratch2);
+
+  void SarPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, Register shift, Register scratch1,
+               Register scratch2);
+  void SarPair(Register dst_low, Register dst_high, Register src_low,
+               Register src_high, int32_t shift, Register scratch1,
+               Register scratch2);
+#endif
 
   // Bit field starts at bit pos and extending for size bits is extracted from
   // rs and stored zero/sign-extended and right-justified in rt
@@ -617,6 +691,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   template <int NBYTES>
   void UnalignedFStoreHelper(FPURegister frd, const MemOperand& rs,
                              Register scratch);
+#if V8_TARGET_ARCH_RISCV32
+  void UnalignedDoubleHelper(FPURegister frd, const MemOperand& rs,
+                             Register scratch_base);
+  void UnalignedDStoreHelper(FPURegister frd, const MemOperand& rs,
+                             Register scratch);
+#endif
 
   template <typename Reg_T, typename Func>
   void AlignedLoadHelper(Reg_T target, const MemOperand& rs, Func generator);
@@ -634,7 +714,6 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void Ush(Register rd, const MemOperand& rs);
 
   void Ulw(Register rd, const MemOperand& rs);
-  void Ulwu(Register rd, const MemOperand& rs);
   void Usw(Register rd, const MemOperand& rs);
 
   void Uld(Register rd, const MemOperand& rs);
@@ -655,9 +734,24 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void Sh(Register rd, const MemOperand& rs);
 
   void Lw(Register rd, const MemOperand& rs);
-  void Lwu(Register rd, const MemOperand& rs);
   void Sw(Register rd, const MemOperand& rs);
 
+#if V8_TARGET_ARCH_RISCV64
+  void Ulwu(Register rd, const MemOperand& rs);
+  void Lwu(Register rd, const MemOperand& rs);
+  void Ld(Register rd, const MemOperand& rs);
+  void Sd(Register rd, const MemOperand& rs);
+  void Lld(Register rd, const MemOperand& rs);
+  void Scd(Register rd, const MemOperand& rs);
+
+  inline void Load32U(Register rd, const MemOperand& rs) { Lwu(rd, rs); }
+  inline void LoadWord(Register rd, const MemOperand& rs) { Ld(rd, rs); }
+  inline void StoreWord(Register rd, const MemOperand& rs) { Sd(rd, rs); }
+#elif V8_TARGET_ARCH_RISCV32
+  inline void Load32U(Register rd, const MemOperand& rs) { Lw(rd, rs); }
+  inline void LoadWord(Register rd, const MemOperand& rs) { Lw(rd, rs); }
+  inline void StoreWord(Register rd, const MemOperand& rs) { Sw(rd, rs); }
+#endif
   void LoadFloat(FPURegister fd, const MemOperand& src);
   void StoreFloat(FPURegister fs, const MemOperand& dst);
 
@@ -667,9 +761,6 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void Ll(Register rd, const MemOperand& rs);
   void Sc(Register rd, const MemOperand& rs);
 
-  void Lld(Register rd, const MemOperand& rs);
-  void Scd(Register rd, const MemOperand& rs);
-
   void Float32Max(FPURegister dst, FPURegister src1, FPURegister src2);
   void Float32Min(FPURegister dst, FPURegister src1, FPURegister src2);
   void Float64Max(FPURegister dst, FPURegister src1, FPURegister src2);
@@ -699,6 +790,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   inline void Move(FPURegister dst, FPURegister src) { MoveDouble(dst, src); }
 
+#if V8_TARGET_ARCH_RISCV64
   inline void Move(Register dst_low, Register dst_high, FPURegister src) {
     fmv_x_d(dst_high, src);
     fmv_x_w(dst_low, src);
@@ -708,11 +800,24 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   inline void Move(Register dst, FPURegister src) { fmv_x_d(dst, src); }
 
   inline void Move(FPURegister dst, Register src) { fmv_d_x(dst, src); }
+#elif V8_TARGET_ARCH_RISCV32
+  inline void Move(Register dst, FPURegister src) { fmv_x_w(dst, src); }
+
+  inline void Move(FPURegister dst, Register src) { fmv_w_x(dst, src); }
+#endif
 
   // Extract sign-extended word from high-half of FPR to GPR
   inline void ExtractHighWordFromF64(Register dst_high, FPURegister src) {
+#if V8_TARGET_ARCH_RISCV64
     fmv_x_d(dst_high, src);
     srai(dst_high, dst_high, 32);
+#elif V8_TARGET_ARCH_RISCV32
+    // todo(riscv32): delete storedouble
+    AddWord(sp, sp, Operand(-8));
+    StoreDouble(src, MemOperand(sp, 0));
+    Lw(dst_high, MemOperand(sp, 4));
+    AddWord(sp, sp, Operand(8));
+#endif
   }
 
   // Insert low-word from GPR (src_high) to the high-half of FPR (dst)
@@ -734,7 +839,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   }
   void LoadFPRImmediate(FPURegister dst, uint32_t src);
   void LoadFPRImmediate(FPURegister dst, uint64_t src);
-
+#if V8_TARGET_ARCH_RISCV64
   // AddOverflow64 sets overflow register to a negative value if
   // overflow occured, otherwise it is zero or positive
   void AddOverflow64(Register dst, Register left, const Operand& right,
@@ -743,13 +848,25 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // overflow occured, otherwise it is zero or positive
   void SubOverflow64(Register dst, Register left, const Operand& right,
                      Register overflow);
-  // MulOverflow32 sets overflow register to zero if no overflow occured
-  void MulOverflow32(Register dst, Register left, const Operand& right,
-                     Register overflow);
-
   // MIPS-style 32-bit unsigned mulh
   void Mulhu32(Register dst, Register left, const Operand& right,
                Register left_zero, Register right_zero);
+#elif V8_TARGET_ARCH_RISCV32
+  // AddOverflow sets overflow register to a negative value if
+  // overflow occured, otherwise it is zero or positive
+  void AddOverflow(Register dst, Register left, const Operand& right,
+                   Register overflow);
+  // SubOverflow sets overflow register to a negative value if
+  // overflow occured, otherwise it is zero or positive
+  void SubOverflow(Register dst, Register left, const Operand& right,
+                   Register overflow);
+  // MIPS-style 32-bit unsigned mulh
+  void Mulhu(Register dst, Register left, const Operand& right,
+             Register left_zero, Register right_zero);
+#endif
+  // MulOverflow32 sets overflow register to zero if no overflow occured
+  void MulOverflow32(Register dst, Register left, const Operand& right,
+                     Register overflow);
 
   // Number of instructions needed for calculation of switch table entry address
   static const int kSwitchTablePrologueSize = 6;
@@ -799,7 +916,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   // Convert single to signed word.
   void Trunc_w_s(Register rd, FPURegister fs, Register result = no_reg);
-
+#if V8_TARGET_ARCH_RISCV64
   // Convert double to unsigned long.
   void Trunc_ul_d(Register rd, FPURegister fs, Register result = no_reg);
 
@@ -812,6 +929,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // Convert singled to signed long.
   void Trunc_l_s(Register rd, FPURegister fs, Register result = no_reg);
 
+  // Round double functions
+  void Trunc_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
+  void Round_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
+  void Floor_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
+  void Ceil_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
+#endif
   // Round single to signed word.
   void Round_w_s(Register rd, FPURegister fs, Register result = no_reg);
 
@@ -830,12 +953,6 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // Floor double to signed word.
   void Floor_w_d(Register rd, FPURegister fs, Register result = no_reg);
 
-  // Round double functions
-  void Trunc_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
-  void Round_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
-  void Floor_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
-  void Ceil_d_d(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
-
   // Round float functions
   void Trunc_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
   void Round_s_s(FPURegister fd, FPURegister fs, FPURegister fpu_scratch);
@@ -860,12 +977,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
                VRegister v_scratch);
   void Round_d(VRegister dst, VRegister src, Register scratch,
                VRegister v_scratch);
-
   // -------------------------------------------------------------------------
   // Smi utilities.
 
   void SmiTag(Register dst, Register src) {
     static_assert(kSmiTag == 0);
+#if V8_TARGET_ARCH_RISCV64
     if (SmiValuesAre32Bits()) {
       // Smi goes to upper 32
       slli(dst, src, 32);
@@ -874,6 +991,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
       // Smi is shifted left by 1
       Add32(dst, src, src);
     }
+#elif V8_TARGET_ARCH_RISCV32
+
+    DCHECK(SmiValuesAre31Bits());
+    // Smi is shifted left by 1
+    slli(dst, src, kSmiShift);
+#endif
   }
 
   void SmiTag(Register reg) { SmiTag(reg, reg); }
@@ -902,16 +1025,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // This is an alternative to embedding the {CodeObject} handle as a reference.
   void ComputeCodeStartAddress(Register dst);
 
-  // Control-flow integrity:
-
-  // Define a function entrypoint. This doesn't emit any code for this
-  // architecture, as control-flow integrity is not supported for it.
-  void CodeEntry() {}
-  // Define an exception handler.
-  void ExceptionHandler() {}
-  // Define an exception handler and bind a label.
-  void BindExceptionHandler(Label* label) { bind(label); }
-
+#if V8_TARGET_ARCH_RISCV64
   // ---------------------------------------------------------------------------
   // Pointer compression Support
 
@@ -948,9 +1062,46 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
     if (COMPRESS_POINTERS_BOOL) {
       Sub32(rd, rs1, rs2);
     } else {
-      Sub64(rd, rs1, rs2);
+      SubWord(rd, rs1, rs2);
     }
   }
+#elif V8_TARGET_ARCH_RISCV32
+  // ---------------------------------------------------------------------------
+  // Pointer compression Support
+  // rv32 don't support Pointer compression. Defines these functions for
+  // simplify builtins.
+  inline void LoadTaggedPointerField(const Register& destination,
+                                     const MemOperand& field_operand) {
+    Lw(destination, field_operand);
+  }
+  inline void LoadAnyTaggedField(const Register& destination,
+                                 const MemOperand& field_operand) {
+    Lw(destination, field_operand);
+  }
+  inline void LoadTaggedSignedField(const Register& destination,
+                                    const MemOperand& field_operand) {
+    Lw(destination, field_operand);
+  }
+
+  inline void SmiUntagField(Register dst, const MemOperand& src) {
+    SmiUntag(dst, src);
+  }
+
+  // Compresses and stores tagged value to given on-heap location.
+  void StoreTaggedField(const Register& value,
+                        const MemOperand& dst_field_operand) {
+    Sw(value, dst_field_operand);
+  }
+#endif
+  // Control-flow integrity:
+
+  // Define a function entrypoint. This doesn't emit any code for this
+  // architecture, as control-flow integrity is not supported for it.
+  void CodeEntry() {}
+  // Define an exception handler.
+  void ExceptionHandler() {}
+  // Define an exception handler and bind a label.
+  void BindExceptionHandler(Label* label) { bind(label); }
   // Wasm into RVV
   void WasmRvvExtractLane(Register dst, VRegister src, int8_t idx, VSew sew,
                           Vlmul lmul) {
@@ -1017,11 +1168,17 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   bool BranchAndLinkShortCheck(int32_t offset, Label* L, Condition cond,
                                Register rs, const Operand& rt);
   void BranchAndLinkLong(Label* L);
-
+#if V8_TARGET_ARCH_RISCV64
   template <typename F_TYPE>
   void RoundHelper(FPURegister dst, FPURegister src, FPURegister fpu_scratch,
                    FPURoundingMode mode);
+#elif V8_TARGET_ARCH_RISCV32
+  void RoundDouble(FPURegister dst, FPURegister src, FPURegister fpu_scratch,
+                   FPURoundingMode mode);
 
+  void RoundFloat(FPURegister dst, FPURegister src, FPURegister fpu_scratch,
+                  FPURoundingMode mode);
+#endif
   template <typename F>
   void RoundHelper(VRegister dst, VRegister src, Register scratch,
                    VRegister v_scratch, FPURoundingMode frm);
@@ -1044,11 +1201,11 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // TODO(victorgomes): Remove this function once we stick with the reversed
   // arguments order.
   void LoadReceiver(Register dest, Register argc) {
-    Ld(dest, MemOperand(sp, 0));
+    LoadWord(dest, MemOperand(sp, 0));
   }
 
   void StoreReceiver(Register rec, Register argc, Register scratch) {
-    Sd(rec, MemOperand(sp, 0));
+    StoreWord(rec, MemOperand(sp, 0));
   }
 
   bool IsNear(Label* L, Condition cond, int rs_reg);
@@ -1245,6 +1402,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   // Left-shifted from int32 equivalent of Smi.
   void SmiScale(Register dst, Register src, int scale) {
+#if V8_TARGET_ARCH_RISCV64
     if (SmiValuesAre32Bits()) {
       // The int portion is upper 32-bits of 64-bit word.
       srai(dst, src, (kSmiShift - scale) & 0x3F);
@@ -1253,6 +1411,11 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
       DCHECK_GE(scale, kSmiTagSize);
       slliw(dst, src, scale - kSmiTagSize);
     }
+#elif V8_TARGET_ARCH_RISCV32
+    DCHECK(SmiValuesAre31Bits());
+    DCHECK_GE(scale, kSmiTagSize);
+    slli(dst, src, scale - kSmiTagSize);
+#endif
   }
 
   // Test if the register contains a smi.
@@ -1341,11 +1504,12 @@ void TurboAssembler::GenerateSwitchTable(Register index, size_t case_count,
        kSystemPointerSizeLog2);  // scratch2 = offset of indexth entry
   add(scratch2, scratch2,
       scratch);  // scratch2 = (saved PC) + (offset of indexth entry)
-  ld(scratch2, scratch2,
-     6 * kInstrSize);  // Add the size of these 6 instructions to the
-                       // offset, then load
-  jr(scratch2);        // Jump to the address loaded from the table
-  nop();               // For 16-byte alignment
+  LoadWord(scratch2,
+           MemOperand(scratch2,
+                      6 * kInstrSize));  // Add the size of these 6 instructions
+                                         // to the offset, then load
+  jr(scratch2);  // Jump to the address loaded from the table
+  nop();         // For 16-byte alignment
   for (size_t index = 0; index < case_count; ++index) {
     dd(GetLabelFunction(index));
   }
@@ -1362,4 +1526,4 @@ struct MoveCycleState {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_CODEGEN_RISCV64_MACRO_ASSEMBLER_RISCV64_H_
+#endif  // V8_CODEGEN_RISCV_MACRO_ASSEMBLER_RISCV_H_
diff --git a/src/codegen/riscv64/register-riscv64.h b/src/codegen/riscv/register-riscv.h
similarity index 98%
rename from src/codegen/riscv64/register-riscv64.h
rename to src/codegen/riscv/register-riscv.h
index c6a596a8904..a1a81db5ee8 100644
--- a/src/codegen/riscv64/register-riscv64.h
+++ b/src/codegen/riscv/register-riscv.h
@@ -2,11 +2,11 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_CODEGEN_RISCV64_REGISTER_RISCV64_H_
-#define V8_CODEGEN_RISCV64_REGISTER_RISCV64_H_
+#ifndef V8_CODEGEN_RISCV_REGISTER_RISCV_H_
+#define V8_CODEGEN_RISCV_REGISTER_RISCV_H_
 
 #include "src/codegen/register-base.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
+#include "src/codegen/riscv/constants-riscv.h"
 
 namespace v8 {
 namespace internal {
@@ -76,7 +76,6 @@ constexpr int ArgumentPaddingSlots(int argument_count) {
 // Note that the bit values must match those used in actual instruction
 // encoding.
 const int kNumRegs = 32;
-
 const int kUndefIndex = -1;
 // Map with indexes on stack that corresponds to codes of saved registers.
 const int kSafepointRegisterStackIndexMap[kNumRegs] = {kUndefIndex,  // zero_reg
@@ -310,4 +309,4 @@ constexpr Register kPtrComprCageBaseRegister = kRootRegister;
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_CODEGEN_RISCV64_REGISTER_RISCV64_H_
+#endif  // V8_CODEGEN_RISCV_REGISTER_RISCV_H_
diff --git a/src/codegen/riscv64/reglist-riscv64.h b/src/codegen/riscv/reglist-riscv.h
similarity index 92%
rename from src/codegen/riscv64/reglist-riscv64.h
rename to src/codegen/riscv/reglist-riscv.h
index 363dd46181f..ec4a6030d76 100644
--- a/src/codegen/riscv64/reglist-riscv64.h
+++ b/src/codegen/riscv/reglist-riscv.h
@@ -2,12 +2,11 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_CODEGEN_RISCV64_REGLIST_RISCV64_H_
-#define V8_CODEGEN_RISCV64_REGLIST_RISCV64_H_
+#ifndef V8_CODEGEN_RISCV_REGLIST_RISCV_H_
+#define V8_CODEGEN_RISCV_REGLIST_RISCV_H_
 
 #include "src/codegen/register-arch.h"
 #include "src/codegen/reglist-base.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
 
 namespace v8 {
 namespace internal {
@@ -61,4 +60,4 @@ const int kNumSafepointSavedRegisters = kNumJSCallerSaved + kNumCalleeSaved;
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_CODEGEN_RISCV64_REGLIST_RISCV64_H_
+#endif  // V8_CODEGEN_RISCV_REGLIST_RISCV_H_
diff --git a/src/codegen/riscv64/assembler-riscv64.cc b/src/codegen/riscv64/assembler-riscv64.cc
deleted file mode 100644
index a661be0ba0e..00000000000
--- a/src/codegen/riscv64/assembler-riscv64.cc
+++ /dev/null
@@ -1,4096 +0,0 @@
-// Copyright (c) 1994-2006 Sun Microsystems Inc.
-// All Rights Reserved.
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are
-// met:
-//
-// - Redistributions of source code must retain the above copyright notice,
-// this list of conditions and the following disclaimer.
-//
-// - Redistribution in binary form must reproduce the above copyright
-// notice, this list of conditions and the following disclaimer in the
-// documentation and/or other materials provided with the distribution.
-//
-// - Neither the name of Sun Microsystems or the names of contributors may
-// be used to endorse or promote products derived from this software without
-// specific prior written permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
-// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
-// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
-// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
-// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
-// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-// The original source code covered by the above license above has been
-// modified significantly by Google Inc.
-// Copyright 2021 the V8 project authors. All rights reserved.
-
-#if V8_TARGET_ARCH_RISCV64
-
-#include "src/codegen/riscv64/assembler-riscv64.h"
-
-#include "src/base/cpu.h"
-#include "src/codegen/flush-instruction-cache.h"
-#include "src/codegen/riscv64/assembler-riscv64-inl.h"
-#include "src/codegen/safepoint-table.h"
-#include "src/codegen/string-constants.h"
-#include "src/deoptimizer/deoptimizer.h"
-#include "src/diagnostics/disasm.h"
-#include "src/diagnostics/disassembler.h"
-#include "src/objects/heap-number-inl.h"
-
-namespace v8 {
-namespace internal {
-// Get the CPU features enabled by the build. For cross compilation the
-// preprocessor symbols CAN_USE_FPU_INSTRUCTIONS
-// can be defined to enable FPU instructions when building the
-// snapshot.
-static unsigned CpuFeaturesImpliedByCompiler() {
-  unsigned answer = 0;
-#ifdef CAN_USE_FPU_INSTRUCTIONS
-  answer |= 1u << FPU;
-#endif  // def CAN_USE_FPU_INSTRUCTIONS
-
-#if (defined CAN_USE_RVV_INSTRUCTIONS)
-  answer |= 1u << RISCV_SIMD;
-#endif  // def CAN_USE_RVV_INSTRUCTIONS
-  return answer;
-}
-
-bool CpuFeatures::SupportsWasmSimd128() { return IsSupported(RISCV_SIMD); }
-
-void CpuFeatures::ProbeImpl(bool cross_compile) {
-  supported_ |= CpuFeaturesImpliedByCompiler();
-  // Only use statically determined features for cross compile (snapshot).
-  if (cross_compile) return;
-  // Probe for additional features at runtime.
-  base::CPU cpu;
-  if (cpu.has_fpu()) supported_ |= 1u << FPU;
-  if (cpu.has_rvv()) supported_ |= 1u << RISCV_SIMD;
-  // Set a static value on whether SIMD is supported.
-  // This variable is only used for certain archs to query SupportWasmSimd128()
-  // at runtime in builtins using an extern ref. Other callers should use
-  // CpuFeatures::SupportWasmSimd128().
-  CpuFeatures::supports_wasm_simd_128_ = CpuFeatures::SupportsWasmSimd128();
-}
-
-void CpuFeatures::PrintTarget() {}
-void CpuFeatures::PrintFeatures() {}
-int ToNumber(Register reg) {
-  DCHECK(reg.is_valid());
-  const int kNumbers[] = {
-      0,   // zero_reg
-      1,   // ra
-      2,   // sp
-      3,   // gp
-      4,   // tp
-      5,   // t0
-      6,   // t1
-      7,   // t2
-      8,   // s0/fp
-      9,   // s1
-      10,  // a0
-      11,  // a1
-      12,  // a2
-      13,  // a3
-      14,  // a4
-      15,  // a5
-      16,  // a6
-      17,  // a7
-      18,  // s2
-      19,  // s3
-      20,  // s4
-      21,  // s5
-      22,  // s6
-      23,  // s7
-      24,  // s8
-      25,  // s9
-      26,  // s10
-      27,  // s11
-      28,  // t3
-      29,  // t4
-      30,  // t5
-      31,  // t6
-  };
-  return kNumbers[reg.code()];
-}
-
-Register ToRegister(int num) {
-  DCHECK(num >= 0 && num < kNumRegisters);
-  const Register kRegisters[] = {
-      zero_reg, ra, sp, gp, tp, t0, t1, t2, fp, s1, a0,  a1,  a2, a3, a4, a5,
-      a6,       a7, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, t3, t4, t5, t6};
-  return kRegisters[num];
-}
-
-// -----------------------------------------------------------------------------
-// Implementation of RelocInfo.
-
-const int RelocInfo::kApplyMask =
-    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE) |
-    RelocInfo::ModeMask(RelocInfo::INTERNAL_REFERENCE_ENCODED) |
-    RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
-
-bool RelocInfo::IsCodedSpecially() {
-  // The deserializer needs to know whether a pointer is specially coded.  Being
-  // specially coded on RISC-V means that it is a lui/addi instruction, and that
-  // is always the case inside code objects.
-  return true;
-}
-
-bool RelocInfo::IsInConstantPool() { return false; }
-
-uint32_t RelocInfo::wasm_call_tag() const {
-  DCHECK(rmode_ == WASM_CALL || rmode_ == WASM_STUB_CALL);
-  return static_cast<uint32_t>(
-      Assembler::target_address_at(pc_, constant_pool_));
-}
-
-// -----------------------------------------------------------------------------
-// Implementation of Operand and MemOperand.
-// See assembler-riscv64-inl.h for inlined constructors.
-
-Operand::Operand(Handle<HeapObject> handle)
-    : rm_(no_reg), rmode_(RelocInfo::FULL_EMBEDDED_OBJECT) {
-  value_.immediate = static_cast<intptr_t>(handle.address());
-}
-
-Operand Operand::EmbeddedNumber(double value) {
-  int32_t smi;
-  if (DoubleToSmiInteger(value, &smi)) return Operand(Smi::FromInt(smi));
-  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
-  result.is_heap_object_request_ = true;
-  result.value_.heap_object_request = HeapObjectRequest(value);
-  return result;
-}
-
-Operand Operand::EmbeddedStringConstant(const StringConstantBase* str) {
-  Operand result(0, RelocInfo::FULL_EMBEDDED_OBJECT);
-  result.is_heap_object_request_ = true;
-  result.value_.heap_object_request = HeapObjectRequest(str);
-  return result;
-}
-
-MemOperand::MemOperand(Register rm, int32_t offset) : Operand(rm) {
-  offset_ = offset;
-}
-
-MemOperand::MemOperand(Register rm, int32_t unit, int32_t multiplier,
-                       OffsetAddend offset_addend)
-    : Operand(rm) {
-  offset_ = unit * multiplier + offset_addend;
-}
-
-void Assembler::AllocateAndInstallRequestedHeapObjects(Isolate* isolate) {
-  DCHECK_IMPLIES(isolate == nullptr, heap_object_requests_.empty());
-  for (auto& request : heap_object_requests_) {
-    Handle<HeapObject> object;
-    switch (request.kind()) {
-      case HeapObjectRequest::kHeapNumber:
-        object = isolate->factory()->NewHeapNumber<AllocationType::kOld>(
-            request.heap_number());
-        break;
-      case HeapObjectRequest::kStringConstant:
-        const StringConstantBase* str = request.string();
-        CHECK_NOT_NULL(str);
-        object = str->AllocateStringConstant(isolate);
-        break;
-    }
-    Address pc = reinterpret_cast<Address>(buffer_start_) + request.offset();
-    set_target_value_at(pc, reinterpret_cast<uint64_t>(object.location()));
-  }
-}
-
-// -----------------------------------------------------------------------------
-// Specific instructions, constants, and masks.
-
-Assembler::Assembler(const AssemblerOptions& options,
-                     std::unique_ptr<AssemblerBuffer> buffer)
-    : AssemblerBase(options, std::move(buffer)),
-      VU(this),
-      scratch_register_list_({t3, t5}),
-      constpool_(this) {
-  reloc_info_writer.Reposition(buffer_start_ + buffer_->size(), pc_);
-
-  last_trampoline_pool_end_ = 0;
-  no_trampoline_pool_before_ = 0;
-  trampoline_pool_blocked_nesting_ = 0;
-  // We leave space (16 * kTrampolineSlotsSize)
-  // for BlockTrampolinePoolScope buffer.
-  next_buffer_check_ = FLAG_force_long_branches
-                           ? kMaxInt
-                           : kMaxBranchOffset - kTrampolineSlotsSize * 16;
-  internal_trampoline_exception_ = false;
-  last_bound_pos_ = 0;
-
-  trampoline_emitted_ = FLAG_force_long_branches;
-  unbound_labels_count_ = 0;
-  block_buffer_growth_ = false;
-}
-
-void Assembler::AbortedCodeGeneration() { constpool_.Clear(); }
-Assembler::~Assembler() { CHECK(constpool_.IsEmpty()); }
-
-void Assembler::GetCode(Isolate* isolate, CodeDesc* desc,
-                        SafepointTableBuilder* safepoint_table_builder,
-                        int handler_table_offset) {
-  // As a crutch to avoid having to add manual Align calls wherever we use a
-  // raw workflow to create Code objects (mostly in tests), add another Align
-  // call here. It does no harm - the end of the Code object is aligned to the
-  // (larger) kCodeAlignment anyways.
-  // TODO(jgruber): Consider moving responsibility for proper alignment to
-  // metadata table builders (safepoint, handler, constant pool, code
-  // comments).
-  DataAlign(Code::kMetadataAlignment);
-
-  ForceConstantPoolEmissionWithoutJump();
-
-  int code_comments_size = WriteCodeComments();
-
-  DCHECK(pc_ <= reloc_info_writer.pos());  // No overlap.
-
-  AllocateAndInstallRequestedHeapObjects(isolate);
-
-  // Set up code descriptor.
-  // TODO(jgruber): Reconsider how these offsets and sizes are maintained up to
-  // this point to make CodeDesc initialization less fiddly.
-
-  static constexpr int kConstantPoolSize = 0;
-  const int instruction_size = pc_offset();
-  const int code_comments_offset = instruction_size - code_comments_size;
-  const int constant_pool_offset = code_comments_offset - kConstantPoolSize;
-  const int handler_table_offset2 = (handler_table_offset == kNoHandlerTable)
-                                        ? constant_pool_offset
-                                        : handler_table_offset;
-  const int safepoint_table_offset =
-      (safepoint_table_builder == kNoSafepointTable)
-          ? handler_table_offset2
-          : safepoint_table_builder->safepoint_table_offset();
-  const int reloc_info_offset =
-      static_cast<int>(reloc_info_writer.pos() - buffer_->start());
-  CodeDesc::Initialize(desc, this, safepoint_table_offset,
-                       handler_table_offset2, constant_pool_offset,
-                       code_comments_offset, reloc_info_offset);
-}
-
-void Assembler::Align(int m) {
-  DCHECK(m >= 4 && base::bits::IsPowerOfTwo(m));
-  while ((pc_offset() & (m - 1)) != 0) {
-    NOP();
-  }
-}
-
-void Assembler::CodeTargetAlign() {
-  // No advantage to aligning branch/call targets to more than
-  // single instruction, that I am aware of.
-  Align(4);
-}
-
-// Labels refer to positions in the (to be) generated code.
-// There are bound, linked, and unused labels.
-//
-// Bound labels refer to known positions in the already
-// generated code. pos() is the position the label refers to.
-//
-// Linked labels refer to unknown positions in the code
-// to be generated; pos() is the position of the last
-// instruction using the label.
-
-// The link chain is terminated by a value in the instruction of 0,
-// which is an otherwise illegal value (branch 0 is inf loop). When this case
-// is detected, return an position of -1, an otherwise illegal position.
-const int kEndOfChain = -1;
-const int kEndOfJumpChain = 0;
-
-bool Assembler::IsBranch(Instr instr) {
-  return (instr & kBaseOpcodeMask) == BRANCH;
-}
-
-bool Assembler::IsCBranch(Instr instr) {
-  int Op = instr & kRvcOpcodeMask;
-  return Op == RO_C_BNEZ || Op == RO_C_BEQZ;
-}
-bool Assembler::IsJump(Instr instr) {
-  int Op = instr & kBaseOpcodeMask;
-  return Op == JAL || Op == JALR;
-}
-
-bool Assembler::IsNop(Instr instr) { return instr == kNopByte; }
-
-bool Assembler::IsJal(Instr instr) { return (instr & kBaseOpcodeMask) == JAL; }
-
-bool Assembler::IsJalr(Instr instr) {
-  return (instr & kBaseOpcodeMask) == JALR;
-}
-
-bool Assembler::IsCJal(Instr instr) {
-  return (instr & kRvcOpcodeMask) == RO_C_J;
-}
-
-bool Assembler::IsLui(Instr instr) { return (instr & kBaseOpcodeMask) == LUI; }
-bool Assembler::IsAuipc(Instr instr) {
-  return (instr & kBaseOpcodeMask) == AUIPC;
-}
-bool Assembler::IsAddiw(Instr instr) {
-  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDIW;
-}
-bool Assembler::IsAddi(Instr instr) {
-  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ADDI;
-}
-bool Assembler::IsOri(Instr instr) {
-  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_ORI;
-}
-bool Assembler::IsSlli(Instr instr) {
-  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_SLLI;
-}
-
-bool Assembler::IsLd(Instr instr) {
-  return (instr & (kBaseOpcodeMask | kFunct3Mask)) == RO_LD;
-}
-
-int Assembler::target_at(int pos, bool is_internal) {
-  if (is_internal) {
-    int64_t* p = reinterpret_cast<int64_t*>(buffer_start_ + pos);
-    int64_t address = *p;
-    if (address == kEndOfJumpChain) {
-      return kEndOfChain;
-    } else {
-      int64_t instr_address = reinterpret_cast<int64_t>(p);
-      DCHECK(instr_address - address < INT_MAX);
-      int delta = static_cast<int>(instr_address - address);
-      DCHECK(pos > delta);
-      return pos - delta;
-    }
-  }
-  Instruction* instruction = Instruction::At(buffer_start_ + pos);
-  DEBUG_PRINTF("target_at: %p (%d)\n\t",
-               reinterpret_cast<Instr*>(buffer_start_ + pos), pos);
-  Instr instr = instruction->InstructionBits();
-  disassembleInstr(instruction->InstructionBits());
-
-  switch (instruction->InstructionOpcodeType()) {
-    case BRANCH: {
-      int32_t imm13 = BranchOffset(instr);
-      if (imm13 == kEndOfJumpChain) {
-        // EndOfChain sentinel is returned directly, not relative to pc or pos.
-        return kEndOfChain;
-      } else {
-        return pos + imm13;
-      }
-    }
-    case JAL: {
-      int32_t imm21 = JumpOffset(instr);
-      if (imm21 == kEndOfJumpChain) {
-        // EndOfChain sentinel is returned directly, not relative to pc or pos.
-        return kEndOfChain;
-      } else {
-        return pos + imm21;
-      }
-    }
-    case JALR: {
-      int32_t imm12 = instr >> 20;
-      if (imm12 == kEndOfJumpChain) {
-        // EndOfChain sentinel is returned directly, not relative to pc or pos.
-        return kEndOfChain;
-      } else {
-        return pos + imm12;
-      }
-    }
-    case LUI: {
-      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
-      pc = target_address_at(pc);
-      uint64_t instr_address = reinterpret_cast<uint64_t>(buffer_start_ + pos);
-      uint64_t imm = reinterpret_cast<uint64_t>(pc);
-      if (imm == kEndOfJumpChain) {
-        return kEndOfChain;
-      } else {
-        DCHECK(instr_address - imm < INT_MAX);
-        int32_t delta = static_cast<int32_t>(instr_address - imm);
-        DCHECK(pos > delta);
-        return pos - delta;
-      }
-    }
-    case AUIPC: {
-      Instr instr_auipc = instr;
-      Instr instr_I = instr_at(pos + 4);
-      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));
-      int32_t offset = BrachlongOffset(instr_auipc, instr_I);
-      if (offset == kEndOfJumpChain) return kEndOfChain;
-      return offset + pos;
-    }
-    case RO_C_J: {
-      int32_t offset = instruction->RvcImm11CJValue();
-      if (offset == kEndOfJumpChain) return kEndOfChain;
-      return offset + pos;
-    }
-    case RO_C_BNEZ:
-    case RO_C_BEQZ: {
-      int32_t offset = instruction->RvcImm8BValue();
-      if (offset == kEndOfJumpChain) return kEndOfChain;
-      return pos + offset;
-    }
-    default: {
-      if (instr == kEndOfJumpChain) {
-        return kEndOfChain;
-      } else {
-        int32_t imm18 =
-            ((instr & static_cast<int32_t>(kImm16Mask)) << 16) >> 14;
-        return (imm18 + pos);
-      }
-    }
-  }
-}
-
-static inline Instr SetBranchOffset(int32_t pos, int32_t target_pos,
-                                    Instr instr) {
-  int32_t imm = target_pos - pos;
-  DCHECK_EQ(imm & 1, 0);
-  DCHECK(is_intn(imm, Assembler::kBranchOffsetBits));
-
-  instr &= ~kBImm12Mask;
-  int32_t imm12 = ((imm & 0x800) >> 4) |   // bit  11
-                  ((imm & 0x1e) << 7) |    // bits 4-1
-                  ((imm & 0x7e0) << 20) |  // bits 10-5
-                  ((imm & 0x1000) << 19);  // bit 12
-
-  return instr | (imm12 & kBImm12Mask);
-}
-
-static inline Instr SetLdOffset(int32_t offset, Instr instr) {
-  DCHECK(Assembler::IsLd(instr));
-  DCHECK(is_int12(offset));
-  instr &= ~kImm12Mask;
-  int32_t imm12 = offset << kImm12Shift;
-  return instr | (imm12 & kImm12Mask);
-}
-
-static inline Instr SetAuipcOffset(int32_t offset, Instr instr) {
-  DCHECK(Assembler::IsAuipc(instr));
-  DCHECK(is_int20(offset));
-  instr = (instr & ~kImm31_12Mask) | ((offset & kImm19_0Mask) << 12);
-  return instr;
-}
-
-static inline Instr SetJalrOffset(int32_t offset, Instr instr) {
-  DCHECK(Assembler::IsJalr(instr));
-  DCHECK(is_int12(offset));
-  instr &= ~kImm12Mask;
-  int32_t imm12 = offset << kImm12Shift;
-  DCHECK(Assembler::IsJalr(instr | (imm12 & kImm12Mask)));
-  DCHECK_EQ(Assembler::JalrOffset(instr | (imm12 & kImm12Mask)), offset);
-  return instr | (imm12 & kImm12Mask);
-}
-
-static inline Instr SetJalOffset(int32_t pos, int32_t target_pos, Instr instr) {
-  DCHECK(Assembler::IsJal(instr));
-  int32_t imm = target_pos - pos;
-  DCHECK_EQ(imm & 1, 0);
-  DCHECK(is_intn(imm, Assembler::kJumpOffsetBits));
-
-  instr &= ~kImm20Mask;
-  int32_t imm20 = (imm & 0xff000) |          // bits 19-12
-                  ((imm & 0x800) << 9) |     // bit  11
-                  ((imm & 0x7fe) << 20) |    // bits 10-1
-                  ((imm & 0x100000) << 11);  // bit  20
-
-  return instr | (imm20 & kImm20Mask);
-}
-
-static inline ShortInstr SetCJalOffset(int32_t pos, int32_t target_pos,
-                                       Instr instr) {
-  DCHECK(Assembler::IsCJal(instr));
-  int32_t imm = target_pos - pos;
-  DCHECK_EQ(imm & 1, 0);
-  DCHECK(is_intn(imm, Assembler::kCJalOffsetBits));
-  instr &= ~kImm11Mask;
-  int16_t imm11 = ((imm & 0x800) >> 1) | ((imm & 0x400) >> 4) |
-                  ((imm & 0x300) >> 1) | ((imm & 0x80) >> 3) |
-                  ((imm & 0x40) >> 1) | ((imm & 0x20) >> 5) |
-                  ((imm & 0x10) << 5) | (imm & 0xe);
-  imm11 = imm11 << kImm11Shift;
-  DCHECK(Assembler::IsCJal(instr | (imm11 & kImm11Mask)));
-  return instr | (imm11 & kImm11Mask);
-}
-static inline Instr SetCBranchOffset(int32_t pos, int32_t target_pos,
-                                     Instr instr) {
-  DCHECK(Assembler::IsCBranch(instr));
-  int32_t imm = target_pos - pos;
-  DCHECK_EQ(imm & 1, 0);
-  DCHECK(is_intn(imm, Assembler::kCBranchOffsetBits));
-
-  instr &= ~kRvcBImm8Mask;
-  int32_t imm8 = ((imm & 0x20) >> 5) | ((imm & 0x6)) | ((imm & 0xc0) >> 3) |
-                 ((imm & 0x18) << 2) | ((imm & 0x100) >> 1);
-  imm8 = ((imm8 & 0x1f) << 2) | ((imm8 & 0xe0) << 5);
-  DCHECK(Assembler::IsCBranch(instr | imm8 & kRvcBImm8Mask));
-
-  return instr | (imm8 & kRvcBImm8Mask);
-}
-
-void Assembler::target_at_put(int pos, int target_pos, bool is_internal,
-                              bool trampoline) {
-  if (is_internal) {
-    uint64_t imm = reinterpret_cast<uint64_t>(buffer_start_) + target_pos;
-    *reinterpret_cast<uint64_t*>(buffer_start_ + pos) = imm;
-    return;
-  }
-  DEBUG_PRINTF("target_at_put: %p (%d) to %p (%d)\n",
-               reinterpret_cast<Instr*>(buffer_start_ + pos), pos,
-               reinterpret_cast<Instr*>(buffer_start_ + target_pos),
-               target_pos);
-  Instruction* instruction = Instruction::At(buffer_start_ + pos);
-  Instr instr = instruction->InstructionBits();
-
-  switch (instruction->InstructionOpcodeType()) {
-    case BRANCH: {
-      instr = SetBranchOffset(pos, target_pos, instr);
-      instr_at_put(pos, instr);
-    } break;
-    case JAL: {
-      DCHECK(IsJal(instr));
-      instr = SetJalOffset(pos, target_pos, instr);
-      instr_at_put(pos, instr);
-    } break;
-    case LUI: {
-      Address pc = reinterpret_cast<Address>(buffer_start_ + pos);
-      set_target_value_at(
-          pc, reinterpret_cast<uint64_t>(buffer_start_ + target_pos));
-    } break;
-    case AUIPC: {
-      Instr instr_auipc = instr;
-      Instr instr_I = instr_at(pos + 4);
-      DCHECK(IsJalr(instr_I) || IsAddi(instr_I));
-
-      int64_t offset = target_pos - pos;
-      if (is_int21(offset) && IsJalr(instr_I) && trampoline) {
-        DCHECK(is_int21(offset) && ((offset & 1) == 0));
-        Instr instr = JAL;
-        instr = SetJalOffset(pos, target_pos, instr);
-        DCHECK(IsJal(instr));
-        DCHECK(JumpOffset(instr) == offset);
-        instr_at_put(pos, instr);
-        instr_at_put(pos + 4, kNopByte);
-      } else {
-        CHECK(is_int32(offset + 0x800));
-
-        int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
-        int32_t Lo12 = (int32_t)offset << 20 >> 20;
-
-        instr_auipc =
-            (instr_auipc & ~kImm31_12Mask) | ((Hi20 & kImm19_0Mask) << 12);
-        instr_at_put(pos, instr_auipc);
-
-        const int kImm31_20Mask = ((1 << 12) - 1) << 20;
-        const int kImm11_0Mask = ((1 << 12) - 1);
-        instr_I = (instr_I & ~kImm31_20Mask) | ((Lo12 & kImm11_0Mask) << 20);
-        instr_at_put(pos + 4, instr_I);
-      }
-    } break;
-    case RO_C_J: {
-      ShortInstr short_instr = SetCJalOffset(pos, target_pos, instr);
-      instr_at_put(pos, short_instr);
-    } break;
-    case RO_C_BNEZ:
-    case RO_C_BEQZ: {
-      instr = SetCBranchOffset(pos, target_pos, instr);
-      instr_at_put(pos, instr);
-    } break;
-    default: {
-      // Emitted label constant, not part of a branch.
-      // Make label relative to Code pointer of generated Code object.
-      instr_at_put(pos, target_pos + (Code::kHeaderSize - kHeapObjectTag));
-    } break;
-  }
-  disassembleInstr(instr);
-}
-
-void Assembler::print(const Label* L) {
-  if (L->is_unused()) {
-    PrintF("unused label\n");
-  } else if (L->is_bound()) {
-    PrintF("bound label to %d\n", L->pos());
-  } else if (L->is_linked()) {
-    Label l;
-    l.link_to(L->pos());
-    PrintF("unbound label");
-    while (l.is_linked()) {
-      PrintF("@ %d ", l.pos());
-      Instr instr = instr_at(l.pos());
-      if ((instr & ~kImm16Mask) == 0) {
-        PrintF("value\n");
-      } else {
-        PrintF("%d\n", instr);
-      }
-      next(&l, is_internal_reference(&l));
-    }
-  } else {
-    PrintF("label in inconsistent state (pos = %d)\n", L->pos_);
-  }
-}
-
-void Assembler::bind_to(Label* L, int pos) {
-  DCHECK(0 <= pos && pos <= pc_offset());  // Must have valid binding position.
-  DEBUG_PRINTF("binding %d to label %p\n", pos, L);
-  int trampoline_pos = kInvalidSlotPos;
-  bool is_internal = false;
-  if (L->is_linked() && !trampoline_emitted_) {
-    unbound_labels_count_--;
-    if (!is_internal_reference(L)) {
-      next_buffer_check_ += kTrampolineSlotsSize;
-    }
-  }
-
-  while (L->is_linked()) {
-    int fixup_pos = L->pos();
-    int dist = pos - fixup_pos;
-    is_internal = is_internal_reference(L);
-    next(L, is_internal);  // Call next before overwriting link with target
-                           // at fixup_pos.
-    Instr instr = instr_at(fixup_pos);
-    DEBUG_PRINTF("\tfixup: %d to %d\n", fixup_pos, dist);
-    if (is_internal) {
-      target_at_put(fixup_pos, pos, is_internal);
-    } else {
-      if (IsBranch(instr)) {
-        if (dist > kMaxBranchOffset) {
-          if (trampoline_pos == kInvalidSlotPos) {
-            trampoline_pos = get_trampoline_entry(fixup_pos);
-            CHECK_NE(trampoline_pos, kInvalidSlotPos);
-          }
-          CHECK((trampoline_pos - fixup_pos) <= kMaxBranchOffset);
-          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
-          target_at_put(fixup_pos, trampoline_pos, false, true);
-          fixup_pos = trampoline_pos;
-        }
-        target_at_put(fixup_pos, pos, false);
-      } else if (IsJal(instr)) {
-        if (dist > kMaxJumpOffset) {
-          if (trampoline_pos == kInvalidSlotPos) {
-            trampoline_pos = get_trampoline_entry(fixup_pos);
-            CHECK_NE(trampoline_pos, kInvalidSlotPos);
-          }
-          CHECK((trampoline_pos - fixup_pos) <= kMaxJumpOffset);
-          DEBUG_PRINTF("\t\ttrampolining: %d\n", trampoline_pos);
-          target_at_put(fixup_pos, trampoline_pos, false, true);
-          fixup_pos = trampoline_pos;
-        }
-        target_at_put(fixup_pos, pos, false);
-      } else {
-        target_at_put(fixup_pos, pos, false);
-      }
-    }
-  }
-  L->bind_to(pos);
-
-  // Keep track of the last bound label so we don't eliminate any instructions
-  // before a bound label.
-  if (pos > last_bound_pos_) last_bound_pos_ = pos;
-}
-
-void Assembler::bind(Label* L) {
-  DCHECK(!L->is_bound());  // Label can only be bound once.
-  bind_to(L, pc_offset());
-}
-
-void Assembler::next(Label* L, bool is_internal) {
-  DCHECK(L->is_linked());
-  int link = target_at(L->pos(), is_internal);
-  if (link == kEndOfChain) {
-    L->Unuse();
-  } else {
-    DCHECK_GE(link, 0);
-    DEBUG_PRINTF("next: %p to %p (%d)\n", L,
-                 reinterpret_cast<Instr*>(buffer_start_ + link), link);
-    L->link_to(link);
-  }
-}
-
-bool Assembler::is_near(Label* L) {
-  DCHECK(L->is_bound());
-  return is_intn((pc_offset() - L->pos()), kJumpOffsetBits);
-}
-
-bool Assembler::is_near(Label* L, OffsetSize bits) {
-  if (L == nullptr || !L->is_bound()) return true;
-  return is_intn((pc_offset() - L->pos()), bits);
-}
-
-bool Assembler::is_near_branch(Label* L) {
-  DCHECK(L->is_bound());
-  return is_intn((pc_offset() - L->pos()), kBranchOffsetBits);
-}
-
-int Assembler::BranchOffset(Instr instr) {
-  // | imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1|11] | opcode |
-  //  31          25                      11          7
-  int32_t imm13 = ((instr & 0xf00) >> 7) | ((instr & 0x7e000000) >> 20) |
-                  ((instr & 0x80) << 4) | ((instr & 0x80000000) >> 19);
-  imm13 = imm13 << 19 >> 19;
-  return imm13;
-}
-
-int Assembler::JumpOffset(Instr instr) {
-  int32_t imm21 = ((instr & 0x7fe00000) >> 20) | ((instr & 0x100000) >> 9) |
-                  (instr & 0xff000) | ((instr & 0x80000000) >> 11);
-  imm21 = imm21 << 11 >> 11;
-  return imm21;
-}
-
-int Assembler::CJumpOffset(Instr instr) {
-  int32_t imm12 = ((instr & 0x4) << 3) | ((instr & 0x38) >> 2) |
-                  ((instr & 0x40) << 1) | ((instr & 0x80) >> 1) |
-                  ((instr & 0x100) << 2) | ((instr & 0x600) >> 1) |
-                  ((instr & 0x800) >> 7) | ((instr & 0x1000) >> 1);
-  imm12 = imm12 << 20 >> 20;
-  return imm12;
-}
-
-int Assembler::BrachlongOffset(Instr auipc, Instr instr_I) {
-  DCHECK(reinterpret_cast<Instruction*>(&instr_I)->InstructionType() ==
-         InstructionBase::kIType);
-  DCHECK(IsAuipc(auipc));
-  DCHECK_EQ((auipc & kRdFieldMask) >> kRdShift,
-            (instr_I & kRs1FieldMask) >> kRs1Shift);
-  int32_t imm_auipc = AuipcOffset(auipc);
-  int32_t imm12 = static_cast<int32_t>(instr_I & kImm12Mask) >> 20;
-  int32_t offset = imm12 + imm_auipc;
-  return offset;
-}
-
-int Assembler::PatchBranchlongOffset(Address pc, Instr instr_auipc,
-                                     Instr instr_jalr, int32_t offset) {
-  DCHECK(IsAuipc(instr_auipc));
-  DCHECK(IsJalr(instr_jalr));
-  CHECK(is_int32(offset + 0x800));
-  int32_t Hi20 = (((int32_t)offset + 0x800) >> 12);
-  int32_t Lo12 = (int32_t)offset << 20 >> 20;
-  instr_at_put(pc, SetAuipcOffset(Hi20, instr_auipc));
-  instr_at_put(pc + 4, SetJalrOffset(Lo12, instr_jalr));
-  DCHECK(offset ==
-         BrachlongOffset(Assembler::instr_at(pc), Assembler::instr_at(pc + 4)));
-  return 2;
-}
-
-int Assembler::LdOffset(Instr instr) {
-  DCHECK(IsLd(instr));
-  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
-  return imm12;
-}
-
-int Assembler::JalrOffset(Instr instr) {
-  DCHECK(IsJalr(instr));
-  int32_t imm12 = static_cast<int32_t>(instr & kImm12Mask) >> 20;
-  return imm12;
-}
-
-int Assembler::AuipcOffset(Instr instr) {
-  DCHECK(IsAuipc(instr));
-  int32_t imm20 = static_cast<int32_t>(instr & kImm20Mask);
-  return imm20;
-}
-// We have to use a temporary register for things that can be relocated even
-// if they can be encoded in RISC-V's 12 bits of immediate-offset instruction
-// space.  There is no guarantee that the relocated location can be similarly
-// encoded.
-bool Assembler::MustUseReg(RelocInfo::Mode rmode) {
-  return !RelocInfo::IsNoInfo(rmode);
-}
-
-void Assembler::disassembleInstr(Instr instr) {
-  if (!FLAG_riscv_debug) return;
-  disasm::NameConverter converter;
-  disasm::Disassembler disasm(converter);
-  base::EmbeddedVector<char, 128> disasm_buffer;
-
-  disasm.InstructionDecode(disasm_buffer, reinterpret_cast<byte*>(&instr));
-  DEBUG_PRINTF("%s\n", disasm_buffer.begin());
-}
-
-// ----- Top-level instruction formats match those in the ISA manual
-// (R, I, S, B, U, J). These match the formats defined in the compiler
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          Register rd, Register rs1, Register rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          Register rd, FPURegister rs1, Register rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          FPURegister rd, Register rs1, Register rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          FPURegister rd, FPURegister rs1, Register rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode,
-                          Register rd, FPURegister rs1, FPURegister rs2) {
-  DCHECK(is_uint7(funct7) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR4(uint8_t funct2, Opcode opcode, Register rd,
-                           Register rs1, Register rs2, Register rs3,
-                           FPURoundingMode frm) {
-  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
-         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
-  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrR4(uint8_t funct2, Opcode opcode, FPURegister rd,
-                           FPURegister rs1, FPURegister rs2, FPURegister rs3,
-                           FPURoundingMode frm) {
-  DCHECK(is_uint2(funct2) && rd.is_valid() && rs1.is_valid() &&
-         rs2.is_valid() && rs3.is_valid() && is_uint3(frm));
-  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct2 << kFunct2Shift) | (rs3.code() << kRs3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrRAtomic(uint8_t funct5, bool aq, bool rl,
-                                uint8_t funct3, Register rd, Register rs1,
-                                Register rs2) {
-  DCHECK(is_uint5(funct5) && is_uint3(funct3) && rd.is_valid() &&
-         rs1.is_valid() && rs2.is_valid());
-  Instr instr = AMO | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (rl << kRlShift) | (aq << kAqShift) | (funct5 << kFunct5Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrRFrm(uint8_t funct7, Opcode opcode, Register rd,
-                             Register rs1, Register rs2, FPURoundingMode frm) {
-  DCHECK(rd.is_valid() && rs1.is_valid() && rs2.is_valid() && is_uint3(frm));
-  Instr instr = opcode | (rd.code() << kRdShift) | (frm << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (rs2.code() << kRs2Shift) |
-                (funct7 << kFunct7Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrI(uint8_t funct3, Opcode opcode, Register rd,
-                          Register rs1, int16_t imm12) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         (is_uint12(imm12) || is_int12(imm12)));
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrI(uint8_t funct3, Opcode opcode, FPURegister rd,
-                          Register rs1, int16_t imm12) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         (is_uint12(imm12) || is_int12(imm12)));
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (imm12 << kImm12Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrIShift(bool arithshift, uint8_t funct3, Opcode opcode,
-                               Register rd, Register rs1, uint8_t shamt) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         is_uint6(shamt));
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (shamt << kShamtShift) |
-                (arithshift << kArithShiftShift);
-  emit(instr);
-}
-
-void Assembler::GenInstrIShiftW(bool arithshift, uint8_t funct3, Opcode opcode,
-                                Register rd, Register rs1, uint8_t shamt) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         is_uint5(shamt));
-  Instr instr = opcode | (rd.code() << kRdShift) | (funct3 << kFunct3Shift) |
-                (rs1.code() << kRs1Shift) | (shamt << kShamtWShift) |
-                (arithshift << kArithShiftShift);
-  emit(instr);
-}
-
-void Assembler::GenInstrS(uint8_t funct3, Opcode opcode, Register rs1,
-                          Register rs2, int16_t imm12) {
-  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
-         is_int12(imm12));
-  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
-                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
-                (rs2.code() << kRs2Shift) |
-                ((imm12 & 0xfe0) << 20);  // bits 11-5
-  emit(instr);
-}
-
-void Assembler::GenInstrS(uint8_t funct3, Opcode opcode, Register rs1,
-                          FPURegister rs2, int16_t imm12) {
-  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
-         is_int12(imm12));
-  Instr instr = opcode | ((imm12 & 0x1f) << 7) |  // bits  4-0
-                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
-                (rs2.code() << kRs2Shift) |
-                ((imm12 & 0xfe0) << 20);  // bits 11-5
-  emit(instr);
-}
-
-void Assembler::GenInstrB(uint8_t funct3, Opcode opcode, Register rs1,
-                          Register rs2, int16_t imm13) {
-  DCHECK(is_uint3(funct3) && rs1.is_valid() && rs2.is_valid() &&
-         is_int13(imm13) && ((imm13 & 1) == 0));
-  Instr instr = opcode | ((imm13 & 0x800) >> 4) |  // bit  11
-                ((imm13 & 0x1e) << 7) |            // bits 4-1
-                (funct3 << kFunct3Shift) | (rs1.code() << kRs1Shift) |
-                (rs2.code() << kRs2Shift) |
-                ((imm13 & 0x7e0) << 20) |  // bits 10-5
-                ((imm13 & 0x1000) << 19);  // bit 12
-  emit(instr);
-}
-
-void Assembler::GenInstrU(Opcode opcode, Register rd, int32_t imm20) {
-  DCHECK(rd.is_valid() && (is_int20(imm20) || is_uint20(imm20)));
-  Instr instr = opcode | (rd.code() << kRdShift) | (imm20 << kImm20Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrJ(Opcode opcode, Register rd, int32_t imm21) {
-  DCHECK(rd.is_valid() && is_int21(imm21) && ((imm21 & 1) == 0));
-  Instr instr = opcode | (rd.code() << kRdShift) |
-                (imm21 & 0xff000) |          // bits 19-12
-                ((imm21 & 0x800) << 9) |     // bit  11
-                ((imm21 & 0x7fe) << 20) |    // bits 10-1
-                ((imm21 & 0x100000) << 11);  // bit  20
-  emit(instr);
-}
-
-void Assembler::GenInstrCR(uint8_t funct4, Opcode opcode, Register rd,
-                           Register rs2) {
-  DCHECK(is_uint4(funct4) && rd.is_valid() && rs2.is_valid());
-  ShortInstr instr = opcode | (rs2.code() << kRvcRs2Shift) |
-                     (rd.code() << kRvcRdShift) | (funct4 << kRvcFunct4Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCA(uint8_t funct6, Opcode opcode, Register rd,
-                           uint8_t funct, Register rs2) {
-  DCHECK(is_uint6(funct6) && rd.is_valid() && rs2.is_valid() &&
-         is_uint2(funct));
-  ShortInstr instr = opcode | ((rs2.code() & 0x7) << kRvcRs2sShift) |
-                     ((rd.code() & 0x7) << kRvcRs1sShift) |
-                     (funct6 << kRvcFunct6Shift) | (funct << kRvcFunct2Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCI(uint8_t funct3, Opcode opcode, Register rd,
-                           int8_t imm6) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && is_int6(imm6));
-  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) |
-                     (rd.code() << kRvcRdShift) | ((imm6 & 0x20) << 7) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCIU(uint8_t funct3, Opcode opcode, Register rd,
-                            uint8_t uimm6) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
-  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
-                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCIU(uint8_t funct3, Opcode opcode, FPURegister rd,
-                            uint8_t uimm6) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint6(uimm6));
-  ShortInstr instr = opcode | ((uimm6 & 0x1f) << 2) |
-                     (rd.code() << kRvcRdShift) | ((uimm6 & 0x20) << 7) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCIW(uint8_t funct3, Opcode opcode, Register rd,
-                            uint8_t uimm8) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && is_uint8(uimm8));
-  ShortInstr instr = opcode | ((uimm8) << 5) |
-                     ((rd.code() & 0x7) << kRvcRs2sShift) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCSS(uint8_t funct3, Opcode opcode, Register rs2,
-                            uint8_t uimm6) {
-  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
-  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCSS(uint8_t funct3, Opcode opcode, FPURegister rs2,
-                            uint8_t uimm6) {
-  DCHECK(is_uint3(funct3) && rs2.is_valid() && is_uint6(uimm6));
-  ShortInstr instr = opcode | (uimm6 << 7) | (rs2.code() << kRvcRs2Shift) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCL(uint8_t funct3, Opcode opcode, Register rd,
-                           Register rs1, uint8_t uimm5) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         is_uint5(uimm5));
-  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
-                     ((rd.code() & 0x7) << kRvcRs2sShift) |
-                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCL(uint8_t funct3, Opcode opcode, FPURegister rd,
-                           Register rs1, uint8_t uimm5) {
-  DCHECK(is_uint3(funct3) && rd.is_valid() && rs1.is_valid() &&
-         is_uint5(uimm5));
-  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
-                     ((rd.code() & 0x7) << kRvcRs2sShift) |
-                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift);
-  emit(instr);
-}
-void Assembler::GenInstrCJ(uint8_t funct3, Opcode opcode, uint16_t uint11) {
-  DCHECK(is_uint11(uint11));
-  ShortInstr instr = opcode | (funct3 << kRvcFunct3Shift) | (uint11 << 2);
-  emit(instr);
-}
-
-void Assembler::GenInstrCS(uint8_t funct3, Opcode opcode, Register rs2,
-                           Register rs1, uint8_t uimm5) {
-  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
-         is_uint5(uimm5));
-  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
-                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
-                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCS(uint8_t funct3, Opcode opcode, FPURegister rs2,
-                           Register rs1, uint8_t uimm5) {
-  DCHECK(is_uint3(funct3) && rs2.is_valid() && rs1.is_valid() &&
-         is_uint5(uimm5));
-  ShortInstr instr = opcode | ((uimm5 & 0x3) << 5) |
-                     ((rs2.code() & 0x7) << kRvcRs2sShift) |
-                     ((uimm5 & 0x1c) << 8) | (funct3 << kRvcFunct3Shift) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCB(uint8_t funct3, Opcode opcode, Register rs1,
-                           uint8_t uimm8) {
-  DCHECK(is_uint3(funct3) && is_uint8(uimm8));
-  ShortInstr instr = opcode | ((uimm8 & 0x1f) << 2) | ((uimm8 & 0xe0) << 5) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
-                     (funct3 << kRvcFunct3Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrCBA(uint8_t funct3, uint8_t funct2, Opcode opcode,
-                            Register rs1, int8_t imm6) {
-  DCHECK(is_uint3(funct3) && is_uint2(funct2) && is_int6(imm6));
-  ShortInstr instr = opcode | ((imm6 & 0x1f) << 2) | ((imm6 & 0x20) << 7) |
-                     ((rs1.code() & 0x7) << kRvcRs1sShift) |
-                     (funct3 << kRvcFunct3Shift) | (funct2 << 10);
-  emit(instr);
-}
-
-// OPIVV OPFVV OPMVV
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
-                          VRegister vs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((vd.code() & 0x1F) << kRvvVdShift) |
-                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
-                          int8_t vs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_MVV || opcode == OP_FVV || opcode == OP_IVV);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((vd.code() & 0x1F) << kRvvVdShift) |
-                ((vs1 & 0x1F) << kRvvVs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-// OPMVV OPFVV
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, Register rd,
-                          VRegister vs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_MVV || opcode == OP_FVV);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((rd.code() & 0x1F) << kRvvVdShift) |
-                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-// OPFVV
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, FPURegister fd,
-                          VRegister vs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_FVV);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((fd.code() & 0x1F) << kRvvVdShift) |
-                ((vs1.code() & 0x1F) << kRvvVs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-// OPIVX OPMVX
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
-                          Register rs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_IVX || opcode == OP_MVX);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((vd.code() & 0x1F) << kRvvVdShift) |
-                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-// OPFVF
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd,
-                          FPURegister fs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_FVF);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((vd.code() & 0x1F) << kRvvVdShift) |
-                ((fs1.code() & 0x1F) << kRvvRs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-// OPMVX
-void Assembler::GenInstrV(uint8_t funct6, Register rd, Register rs1,
-                          VRegister vs2, MaskType mask) {
-  Instr instr = (funct6 << kRvvFunct6Shift) | OP_MVX | (mask << kRvvVmShift) |
-                ((rd.code() & 0x1F) << kRvvVdShift) |
-                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-// OPIVI
-void Assembler::GenInstrV(uint8_t funct6, VRegister vd, int8_t imm5,
-                          VRegister vs2, MaskType mask) {
-  DCHECK(is_uint5(imm5) || is_int5(imm5));
-  Instr instr = (funct6 << kRvvFunct6Shift) | OP_IVI | (mask << kRvvVmShift) |
-                ((vd.code() & 0x1F) << kRvvVdShift) |
-                (((uint32_t)imm5 << kRvvImm5Shift) & kRvvImm5Mask) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-
-// VL VS
-void Assembler::GenInstrV(Opcode opcode, uint8_t width, VRegister vd,
-                          Register rs1, uint8_t umop, MaskType mask,
-                          uint8_t IsMop, bool IsMew, uint8_t Nf) {
-  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
-  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
-                ((width << kRvvWidthShift) & kRvvWidthMask) |
-                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
-                ((umop << kRvvRs2Shift) & kRvvRs2Mask) |
-                ((mask << kRvvVmShift) & kRvvVmMask) |
-                ((IsMop << kRvvMopShift) & kRvvMopMask) |
-                ((IsMew << kRvvMewShift) & kRvvMewMask) |
-                ((Nf << kRvvNfShift) & kRvvNfMask);
-  emit(instr);
-}
-void Assembler::GenInstrV(Opcode opcode, uint8_t width, VRegister vd,
-                          Register rs1, Register rs2, MaskType mask,
-                          uint8_t IsMop, bool IsMew, uint8_t Nf) {
-  DCHECK(opcode == LOAD_FP || opcode == STORE_FP);
-  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
-                ((width << kRvvWidthShift) & kRvvWidthMask) |
-                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
-                ((rs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
-                ((mask << kRvvVmShift) & kRvvVmMask) |
-                ((IsMop << kRvvMopShift) & kRvvMopMask) |
-                ((IsMew << kRvvMewShift) & kRvvMewMask) |
-                ((Nf << kRvvNfShift) & kRvvNfMask);
-  emit(instr);
-}
-// VL VS AMO
-void Assembler::GenInstrV(Opcode opcode, uint8_t width, VRegister vd,
-                          Register rs1, VRegister vs2, MaskType mask,
-                          uint8_t IsMop, bool IsMew, uint8_t Nf) {
-  DCHECK(opcode == LOAD_FP || opcode == STORE_FP || opcode == AMO);
-  Instr instr = opcode | ((vd.code() << kRvvVdShift) & kRvvVdMask) |
-                ((width << kRvvWidthShift) & kRvvWidthMask) |
-                ((rs1.code() << kRvvRs1Shift) & kRvvRs1Mask) |
-                ((vs2.code() << kRvvRs2Shift) & kRvvRs2Mask) |
-                ((mask << kRvvVmShift) & kRvvVmMask) |
-                ((IsMop << kRvvMopShift) & kRvvMopMask) |
-                ((IsMew << kRvvMewShift) & kRvvMewMask) |
-                ((Nf << kRvvNfShift) & kRvvNfMask);
-  emit(instr);
-}
-// vmv_xs vcpop_m vfirst_m
-void Assembler::GenInstrV(uint8_t funct6, Opcode opcode, Register rd,
-                          uint8_t vs1, VRegister vs2, MaskType mask) {
-  DCHECK(opcode == OP_MVV);
-  Instr instr = (funct6 << kRvvFunct6Shift) | opcode | (mask << kRvvVmShift) |
-                ((rd.code() & 0x1F) << kRvvVdShift) |
-                ((vs1 & 0x1F) << kRvvVs1Shift) |
-                ((vs2.code() & 0x1F) << kRvvVs2Shift);
-  emit(instr);
-}
-// ----- Instruction class templates match those in the compiler
-
-void Assembler::GenInstrBranchCC_rri(uint8_t funct3, Register rs1, Register rs2,
-                                     int16_t imm13) {
-  GenInstrB(funct3, BRANCH, rs1, rs2, imm13);
-}
-
-void Assembler::GenInstrLoad_ri(uint8_t funct3, Register rd, Register rs1,
-                                int16_t imm12) {
-  GenInstrI(funct3, LOAD, rd, rs1, imm12);
-}
-
-void Assembler::GenInstrStore_rri(uint8_t funct3, Register rs1, Register rs2,
-                                  int16_t imm12) {
-  GenInstrS(funct3, STORE, rs1, rs2, imm12);
-}
-
-void Assembler::GenInstrALU_ri(uint8_t funct3, Register rd, Register rs1,
-                               int16_t imm12) {
-  GenInstrI(funct3, OP_IMM, rd, rs1, imm12);
-}
-
-void Assembler::GenInstrShift_ri(bool arithshift, uint8_t funct3, Register rd,
-                                 Register rs1, uint8_t shamt) {
-  DCHECK(is_uint6(shamt));
-  GenInstrI(funct3, OP_IMM, rd, rs1, (arithshift << 10) | shamt);
-}
-
-void Assembler::GenInstrALU_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                               Register rs1, Register rs2) {
-  GenInstrR(funct7, funct3, OP, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrCSR_ir(uint8_t funct3, Register rd,
-                               ControlStatusReg csr, Register rs1) {
-  GenInstrI(funct3, SYSTEM, rd, rs1, csr);
-}
-
-void Assembler::GenInstrCSR_ii(uint8_t funct3, Register rd,
-                               ControlStatusReg csr, uint8_t imm5) {
-  GenInstrI(funct3, SYSTEM, rd, ToRegister(imm5), csr);
-}
-
-void Assembler::GenInstrShiftW_ri(bool arithshift, uint8_t funct3, Register rd,
-                                  Register rs1, uint8_t shamt) {
-  GenInstrIShiftW(arithshift, funct3, OP_IMM_32, rd, rs1, shamt);
-}
-
-void Assembler::GenInstrALUW_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                                Register rs1, Register rs2) {
-  GenInstrR(funct7, funct3, OP_32, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrPriv(uint8_t funct7, Register rs1, Register rs2) {
-  GenInstrR(funct7, 0b000, SYSTEM, ToRegister(0), rs1, rs2);
-}
-
-void Assembler::GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd, Register rs1,
-                                  int16_t imm12) {
-  GenInstrI(funct3, LOAD_FP, rd, rs1, imm12);
-}
-
-void Assembler::GenInstrStoreFP_rri(uint8_t funct3, Register rs1,
-                                    FPURegister rs2, int16_t imm12) {
-  GenInstrS(funct3, STORE_FP, rs1, rs2, imm12);
-}
-
-void Assembler::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                                 FPURegister rs1, FPURegister rs2) {
-  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                                 Register rs1, Register rs2) {
-  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                                 FPURegister rs1, Register rs2) {
-  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                                 FPURegister rs1, Register rs2) {
-  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
-}
-
-void Assembler::GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                                 FPURegister rs1, FPURegister rs2) {
-  GenInstrR(funct7, funct3, OP_FP, rd, rs1, rs2);
-}
-
-// Returns the next free trampoline entry.
-int32_t Assembler::get_trampoline_entry(int32_t pos) {
-  int32_t trampoline_entry = kInvalidSlotPos;
-  if (!internal_trampoline_exception_) {
-    DEBUG_PRINTF("\tstart: %d,pos: %d\n", trampoline_.start(), pos);
-    if (trampoline_.start() > pos) {
-      trampoline_entry = trampoline_.take_slot();
-    }
-
-    if (kInvalidSlotPos == trampoline_entry) {
-      internal_trampoline_exception_ = true;
-    }
-  }
-  return trampoline_entry;
-}
-
-uint64_t Assembler::jump_address(Label* L) {
-  int64_t target_pos;
-  DEBUG_PRINTF("jump_address: %p to %p (%d)\n", L,
-               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
-               pc_offset());
-  if (L->is_bound()) {
-    target_pos = L->pos();
-  } else {
-    if (L->is_linked()) {
-      target_pos = L->pos();  // L's link.
-      L->link_to(pc_offset());
-    } else {
-      L->link_to(pc_offset());
-      if (!trampoline_emitted_) {
-        unbound_labels_count_++;
-        next_buffer_check_ -= kTrampolineSlotsSize;
-      }
-      DEBUG_PRINTF("\tstarted link\n");
-      return kEndOfJumpChain;
-    }
-  }
-  uint64_t imm = reinterpret_cast<uint64_t>(buffer_start_) + target_pos;
-  if (FLAG_riscv_c_extension)
-    DCHECK_EQ(imm & 1, 0);
-  else
-    DCHECK_EQ(imm & 3, 0);
-
-  return imm;
-}
-
-uint64_t Assembler::branch_long_offset(Label* L) {
-  int64_t target_pos;
-
-  DEBUG_PRINTF("branch_long_offset: %p to %p (%d)\n", L,
-               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
-               pc_offset());
-  if (L->is_bound()) {
-    target_pos = L->pos();
-  } else {
-    if (L->is_linked()) {
-      target_pos = L->pos();  // L's link.
-      L->link_to(pc_offset());
-    } else {
-      L->link_to(pc_offset());
-      if (!trampoline_emitted_) {
-        unbound_labels_count_++;
-        next_buffer_check_ -= kTrampolineSlotsSize;
-      }
-      DEBUG_PRINTF("\tstarted link\n");
-      return kEndOfJumpChain;
-    }
-  }
-  int64_t offset = target_pos - pc_offset();
-  if (FLAG_riscv_c_extension)
-    DCHECK_EQ(offset & 1, 0);
-  else
-    DCHECK_EQ(offset & 3, 0);
-
-  return static_cast<uint64_t>(offset);
-}
-
-int32_t Assembler::branch_offset_helper(Label* L, OffsetSize bits) {
-  int32_t target_pos;
-
-  DEBUG_PRINTF("branch_offset_helper: %p to %p (%d)\n", L,
-               reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
-               pc_offset());
-  if (L->is_bound()) {
-    target_pos = L->pos();
-    DEBUG_PRINTF("\tbound: %d", target_pos);
-  } else {
-    if (L->is_linked()) {
-      target_pos = L->pos();
-      L->link_to(pc_offset());
-      DEBUG_PRINTF("\tadded to link: %d\n", target_pos);
-    } else {
-      L->link_to(pc_offset());
-      if (!trampoline_emitted_) {
-        unbound_labels_count_++;
-        next_buffer_check_ -= kTrampolineSlotsSize;
-      }
-      DEBUG_PRINTF("\tstarted link\n");
-      return kEndOfJumpChain;
-    }
-  }
-
-  int32_t offset = target_pos - pc_offset();
-  DCHECK(is_intn(offset, bits));
-  DCHECK_EQ(offset & 1, 0);
-  DEBUG_PRINTF("\toffset = %d\n", offset);
-  return offset;
-}
-
-void Assembler::label_at_put(Label* L, int at_offset) {
-  int target_pos;
-  DEBUG_PRINTF("label_at_put: %p @ %p (%d)\n", L,
-               reinterpret_cast<Instr*>(buffer_start_ + at_offset), at_offset);
-  if (L->is_bound()) {
-    target_pos = L->pos();
-    instr_at_put(at_offset, target_pos + (Code::kHeaderSize - kHeapObjectTag));
-  } else {
-    if (L->is_linked()) {
-      target_pos = L->pos();  // L's link.
-      int32_t imm18 = target_pos - at_offset;
-      DCHECK_EQ(imm18 & 3, 0);
-      int32_t imm16 = imm18 >> 2;
-      DCHECK(is_int16(imm16));
-      instr_at_put(at_offset, (int32_t)(imm16 & kImm16Mask));
-    } else {
-      target_pos = kEndOfJumpChain;
-      instr_at_put(at_offset, target_pos);
-      if (!trampoline_emitted_) {
-        unbound_labels_count_++;
-        next_buffer_check_ -= kTrampolineSlotsSize;
-      }
-    }
-    L->link_to(at_offset);
-  }
-}
-
-//===----------------------------------------------------------------------===//
-// Instructions
-//===----------------------------------------------------------------------===//
-
-void Assembler::lui(Register rd, int32_t imm20) { GenInstrU(LUI, rd, imm20); }
-
-void Assembler::auipc(Register rd, int32_t imm20) {
-  GenInstrU(AUIPC, rd, imm20);
-}
-
-// Jumps
-
-void Assembler::jal(Register rd, int32_t imm21) {
-  GenInstrJ(JAL, rd, imm21);
-  BlockTrampolinePoolFor(1);
-}
-
-void Assembler::jalr(Register rd, Register rs1, int16_t imm12) {
-  GenInstrI(0b000, JALR, rd, rs1, imm12);
-  BlockTrampolinePoolFor(1);
-}
-
-// Branches
-
-void Assembler::beq(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b000, rs1, rs2, imm13);
-}
-
-void Assembler::bne(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b001, rs1, rs2, imm13);
-}
-
-void Assembler::blt(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b100, rs1, rs2, imm13);
-}
-
-void Assembler::bge(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b101, rs1, rs2, imm13);
-}
-
-void Assembler::bltu(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b110, rs1, rs2, imm13);
-}
-
-void Assembler::bgeu(Register rs1, Register rs2, int16_t imm13) {
-  GenInstrBranchCC_rri(0b111, rs1, rs2, imm13);
-}
-
-// Loads
-
-void Assembler::lb(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b000, rd, rs1, imm12);
-}
-
-void Assembler::lh(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b001, rd, rs1, imm12);
-}
-
-void Assembler::lw(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b010, rd, rs1, imm12);
-}
-
-void Assembler::lbu(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b100, rd, rs1, imm12);
-}
-
-void Assembler::lhu(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b101, rd, rs1, imm12);
-}
-
-// Stores
-
-void Assembler::sb(Register source, Register base, int16_t imm12) {
-  GenInstrStore_rri(0b000, base, source, imm12);
-}
-
-void Assembler::sh(Register source, Register base, int16_t imm12) {
-  GenInstrStore_rri(0b001, base, source, imm12);
-}
-
-void Assembler::sw(Register source, Register base, int16_t imm12) {
-  GenInstrStore_rri(0b010, base, source, imm12);
-}
-
-// Arithmetic with immediate
-
-void Assembler::addi(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b000, rd, rs1, imm12);
-}
-
-void Assembler::slti(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b010, rd, rs1, imm12);
-}
-
-void Assembler::sltiu(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b011, rd, rs1, imm12);
-}
-
-void Assembler::xori(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b100, rd, rs1, imm12);
-}
-
-void Assembler::ori(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b110, rd, rs1, imm12);
-}
-
-void Assembler::andi(Register rd, Register rs1, int16_t imm12) {
-  GenInstrALU_ri(0b111, rd, rs1, imm12);
-}
-
-void Assembler::slli(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShift_ri(0, 0b001, rd, rs1, shamt & 0x3f);
-}
-
-void Assembler::srli(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShift_ri(0, 0b101, rd, rs1, shamt & 0x3f);
-}
-
-void Assembler::srai(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShift_ri(1, 0b101, rd, rs1, shamt & 0x3f);
-}
-
-// Arithmetic
-
-void Assembler::add(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::sub(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0100000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::sll(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::slt(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::sltu(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::xor_(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b100, rd, rs1, rs2);
-}
-
-void Assembler::srl(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b101, rd, rs1, rs2);
-}
-
-void Assembler::sra(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0100000, 0b101, rd, rs1, rs2);
-}
-
-void Assembler::or_(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b110, rd, rs1, rs2);
-}
-
-void Assembler::and_(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000000, 0b111, rd, rs1, rs2);
-}
-
-// Memory fences
-
-void Assembler::fence(uint8_t pred, uint8_t succ) {
-  DCHECK(is_uint4(pred) && is_uint4(succ));
-  uint16_t imm12 = succ | (pred << 4) | (0b0000 << 8);
-  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
-}
-
-void Assembler::fence_tso() {
-  uint16_t imm12 = (0b0011) | (0b0011 << 4) | (0b1000 << 8);
-  GenInstrI(0b000, MISC_MEM, ToRegister(0), ToRegister(0), imm12);
-}
-
-// Environment call / break
-
-void Assembler::ecall() {
-  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 0);
-}
-
-void Assembler::ebreak() {
-  GenInstrI(0b000, SYSTEM, ToRegister(0), ToRegister(0), 1);
-}
-
-// This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
-// instruction (i.e., it should always trap, if your implementation has invalid
-// instruction traps).
-void Assembler::unimp() {
-  GenInstrI(0b001, SYSTEM, ToRegister(0), ToRegister(0), 0b110000000000);
-}
-
-// CSR
-
-void Assembler::csrrw(Register rd, ControlStatusReg csr, Register rs1) {
-  GenInstrCSR_ir(0b001, rd, csr, rs1);
-}
-
-void Assembler::csrrs(Register rd, ControlStatusReg csr, Register rs1) {
-  GenInstrCSR_ir(0b010, rd, csr, rs1);
-}
-
-void Assembler::csrrc(Register rd, ControlStatusReg csr, Register rs1) {
-  GenInstrCSR_ir(0b011, rd, csr, rs1);
-}
-
-void Assembler::csrrwi(Register rd, ControlStatusReg csr, uint8_t imm5) {
-  GenInstrCSR_ii(0b101, rd, csr, imm5);
-}
-
-void Assembler::csrrsi(Register rd, ControlStatusReg csr, uint8_t imm5) {
-  GenInstrCSR_ii(0b110, rd, csr, imm5);
-}
-
-void Assembler::csrrci(Register rd, ControlStatusReg csr, uint8_t imm5) {
-  GenInstrCSR_ii(0b111, rd, csr, imm5);
-}
-
-// RV64I
-
-void Assembler::lwu(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b110, rd, rs1, imm12);
-}
-
-void Assembler::ld(Register rd, Register rs1, int16_t imm12) {
-  GenInstrLoad_ri(0b011, rd, rs1, imm12);
-}
-
-void Assembler::sd(Register source, Register base, int16_t imm12) {
-  GenInstrStore_rri(0b011, base, source, imm12);
-}
-
-void Assembler::addiw(Register rd, Register rs1, int16_t imm12) {
-  GenInstrI(0b000, OP_IMM_32, rd, rs1, imm12);
-}
-
-void Assembler::slliw(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShiftW_ri(0, 0b001, rd, rs1, shamt & 0x1f);
-}
-
-void Assembler::srliw(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShiftW_ri(0, 0b101, rd, rs1, shamt & 0x1f);
-}
-
-void Assembler::sraiw(Register rd, Register rs1, uint8_t shamt) {
-  GenInstrShiftW_ri(1, 0b101, rd, rs1, shamt & 0x1f);
-}
-
-void Assembler::addw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::subw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0100000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::sllw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000000, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::srlw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000000, 0b101, rd, rs1, rs2);
-}
-
-void Assembler::sraw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0100000, 0b101, rd, rs1, rs2);
-}
-
-// RV32M Standard Extension
-
-void Assembler::mul(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::mulh(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::mulhsu(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::mulhu(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::div(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b100, rd, rs1, rs2);
-}
-
-void Assembler::divu(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b101, rd, rs1, rs2);
-}
-
-void Assembler::rem(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b110, rd, rs1, rs2);
-}
-
-void Assembler::remu(Register rd, Register rs1, Register rs2) {
-  GenInstrALU_rr(0b0000001, 0b111, rd, rs1, rs2);
-}
-
-// RV64M Standard Extension (in addition to RV32M)
-
-void Assembler::mulw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000001, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::divw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000001, 0b100, rd, rs1, rs2);
-}
-
-void Assembler::divuw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000001, 0b101, rd, rs1, rs2);
-}
-
-void Assembler::remw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000001, 0b110, rd, rs1, rs2);
-}
-
-void Assembler::remuw(Register rd, Register rs1, Register rs2) {
-  GenInstrALUW_rr(0b0000001, 0b111, rd, rs1, rs2);
-}
-
-// RV32A Standard Extension
-
-void Assembler::lr_w(bool aq, bool rl, Register rd, Register rs1) {
-  GenInstrRAtomic(0b00010, aq, rl, 0b010, rd, rs1, zero_reg);
-}
-
-void Assembler::sc_w(bool aq, bool rl, Register rd, Register rs1,
-                     Register rs2) {
-  GenInstrRAtomic(0b00011, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amoswap_w(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b00001, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amoadd_w(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b00000, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amoxor_w(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b00100, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amoand_w(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b01100, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amoor_w(bool aq, bool rl, Register rd, Register rs1,
-                        Register rs2) {
-  GenInstrRAtomic(0b01000, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amomin_w(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b10000, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amomax_w(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b10100, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amominu_w(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b11000, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::amomaxu_w(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b11100, aq, rl, 0b010, rd, rs1, rs2);
-}
-
-// RV64A Standard Extension (in addition to RV32A)
-
-void Assembler::lr_d(bool aq, bool rl, Register rd, Register rs1) {
-  GenInstrRAtomic(0b00010, aq, rl, 0b011, rd, rs1, zero_reg);
-}
-
-void Assembler::sc_d(bool aq, bool rl, Register rd, Register rs1,
-                     Register rs2) {
-  GenInstrRAtomic(0b00011, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amoswap_d(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b00001, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amoadd_d(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b00000, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amoxor_d(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b00100, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amoand_d(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b01100, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amoor_d(bool aq, bool rl, Register rd, Register rs1,
-                        Register rs2) {
-  GenInstrRAtomic(0b01000, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amomin_d(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b10000, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amomax_d(bool aq, bool rl, Register rd, Register rs1,
-                         Register rs2) {
-  GenInstrRAtomic(0b10100, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amominu_d(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b11000, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-void Assembler::amomaxu_d(bool aq, bool rl, Register rd, Register rs1,
-                          Register rs2) {
-  GenInstrRAtomic(0b11100, aq, rl, 0b011, rd, rs1, rs2);
-}
-
-// RV32F Standard Extension
-
-void Assembler::flw(FPURegister rd, Register rs1, int16_t imm12) {
-  GenInstrLoadFP_ri(0b010, rd, rs1, imm12);
-}
-
-void Assembler::fsw(FPURegister source, Register base, int16_t imm12) {
-  GenInstrStoreFP_rri(0b010, base, source, imm12);
-}
-
-void Assembler::fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                        FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b00, MADD, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                        FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b00, MSUB, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                         FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b00, NMSUB, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                         FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b00, NMADD, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0000000, frm, rd, rs1, rs2);
-}
-
-void Assembler::fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0000100, frm, rd, rs1, rs2);
-}
-
-void Assembler::fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0001000, frm, rd, rs1, rs2);
-}
-
-void Assembler::fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0001100, frm, rd, rs1, rs2);
-}
-
-void Assembler::fsqrt_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0101100, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fsgnj_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fsgnjn_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010000, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fsgnjx_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010000, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010100, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010100, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fcvt_w_s(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_wu_s(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(1));
-}
-
-void Assembler::fmv_x_w(Register rd, FPURegister rs1) {
-  GenInstrALUFP_rr(0b1110000, 0b000, rd, rs1, zero_reg);
-}
-
-void Assembler::feq_s(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010000, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::flt_s(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010000, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fle_s(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010000, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fclass_s(Register rd, FPURegister rs1) {
-  GenInstrALUFP_rr(0b1110000, 0b001, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_s_w(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_s_wu(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(1));
-}
-
-void Assembler::fmv_w_x(FPURegister rd, Register rs1) {
-  GenInstrALUFP_rr(0b1111000, 0b000, rd, rs1, zero_reg);
-}
-
-// RV64F Standard Extension (in addition to RV32F)
-
-void Assembler::fcvt_l_s(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(2));
-}
-
-void Assembler::fcvt_lu_s(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100000, frm, rd, rs1, ToRegister(3));
-}
-
-void Assembler::fcvt_s_l(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(2));
-}
-
-void Assembler::fcvt_s_lu(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101000, frm, rd, rs1, ToRegister(3));
-}
-
-// RV32D Standard Extension
-
-void Assembler::fld(FPURegister rd, Register rs1, int16_t imm12) {
-  GenInstrLoadFP_ri(0b011, rd, rs1, imm12);
-}
-
-void Assembler::fsd(FPURegister source, Register base, int16_t imm12) {
-  GenInstrStoreFP_rri(0b011, base, source, imm12);
-}
-
-void Assembler::fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                        FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b01, MADD, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                        FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b01, MSUB, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                         FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b01, NMSUB, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                         FPURegister rs3, FPURoundingMode frm) {
-  GenInstrR4(0b01, NMADD, rd, rs1, rs2, rs3, frm);
-}
-
-void Assembler::fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0000001, frm, rd, rs1, rs2);
-}
-
-void Assembler::fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0000101, frm, rd, rs1, rs2);
-}
-
-void Assembler::fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0001001, frm, rd, rs1, rs2);
-}
-
-void Assembler::fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                       FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0001101, frm, rd, rs1, rs2);
-}
-
-void Assembler::fsqrt_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0101101, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fsgnj_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010001, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fsgnjn_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010001, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fsgnjx_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010001, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010101, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b0010101, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fcvt_s_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0100000, frm, rd, rs1, ToRegister(1));
-}
-
-void Assembler::fcvt_d_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b0100001, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::feq_d(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010001, 0b010, rd, rs1, rs2);
-}
-
-void Assembler::flt_d(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010001, 0b001, rd, rs1, rs2);
-}
-
-void Assembler::fle_d(Register rd, FPURegister rs1, FPURegister rs2) {
-  GenInstrALUFP_rr(0b1010001, 0b000, rd, rs1, rs2);
-}
-
-void Assembler::fclass_d(Register rd, FPURegister rs1) {
-  GenInstrALUFP_rr(0b1110001, 0b001, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_w_d(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_wu_d(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(1));
-}
-
-void Assembler::fcvt_d_w(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_d_wu(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(1));
-}
-
-// RV64D Standard Extension (in addition to RV32D)
-
-void Assembler::fcvt_l_d(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(2));
-}
-
-void Assembler::fcvt_lu_d(Register rd, FPURegister rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1100001, frm, rd, rs1, ToRegister(3));
-}
-
-void Assembler::fmv_x_d(Register rd, FPURegister rs1) {
-  GenInstrALUFP_rr(0b1110001, 0b000, rd, rs1, zero_reg);
-}
-
-void Assembler::fcvt_d_l(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(2));
-}
-
-void Assembler::fcvt_d_lu(FPURegister rd, Register rs1, FPURoundingMode frm) {
-  GenInstrALUFP_rr(0b1101001, frm, rd, rs1, ToRegister(3));
-}
-
-void Assembler::fmv_d_x(FPURegister rd, Register rs1) {
-  GenInstrALUFP_rr(0b1111001, 0b000, rd, rs1, zero_reg);
-}
-
-// RV64C Standard Extension
-void Assembler::c_nop() { GenInstrCI(0b000, C1, zero_reg, 0); }
-
-void Assembler::c_addi(Register rd, int8_t imm6) {
-  DCHECK(rd != zero_reg && imm6 != 0);
-  GenInstrCI(0b000, C1, rd, imm6);
-}
-
-void Assembler::c_addiw(Register rd, int8_t imm6) {
-  DCHECK(rd != zero_reg);
-  GenInstrCI(0b001, C1, rd, imm6);
-}
-
-void Assembler::c_addi16sp(int16_t imm10) {
-  DCHECK(is_int10(imm10) && (imm10 & 0xf) == 0);
-  uint8_t uimm6 = ((imm10 & 0x200) >> 4) | (imm10 & 0x10) |
-                  ((imm10 & 0x40) >> 3) | ((imm10 & 0x180) >> 6) |
-                  ((imm10 & 0x20) >> 5);
-  GenInstrCIU(0b011, C1, sp, uimm6);
-}
-
-void Assembler::c_addi4spn(Register rd, int16_t uimm10) {
-  DCHECK(is_uint10(uimm10) && (uimm10 != 0));
-  uint8_t uimm8 = ((uimm10 & 0x4) >> 1) | ((uimm10 & 0x8) >> 3) |
-                  ((uimm10 & 0x30) << 2) | ((uimm10 & 0x3c0) >> 4);
-  GenInstrCIW(0b000, C0, rd, uimm8);
-}
-
-void Assembler::c_li(Register rd, int8_t imm6) {
-  DCHECK(rd != zero_reg);
-  GenInstrCI(0b010, C1, rd, imm6);
-}
-
-void Assembler::c_lui(Register rd, int8_t imm6) {
-  DCHECK(rd != zero_reg && rd != sp && imm6 != 0);
-  GenInstrCI(0b011, C1, rd, imm6);
-}
-
-void Assembler::c_slli(Register rd, uint8_t shamt6) {
-  DCHECK(rd != zero_reg && shamt6 != 0);
-  GenInstrCIU(0b000, C2, rd, shamt6);
-}
-
-void Assembler::c_fldsp(FPURegister rd, uint16_t uimm9) {
-  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
-  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
-  GenInstrCIU(0b001, C2, rd, uimm6);
-}
-
-void Assembler::c_lwsp(Register rd, uint16_t uimm8) {
-  DCHECK(rd != zero_reg && is_uint8(uimm8) && (uimm8 & 0x3) == 0);
-  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCIU(0b010, C2, rd, uimm6);
-}
-
-void Assembler::c_ldsp(Register rd, uint16_t uimm9) {
-  DCHECK(rd != zero_reg && is_uint9(uimm9) && (uimm9 & 0x7) == 0);
-  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
-  GenInstrCIU(0b011, C2, rd, uimm6);
-}
-
-void Assembler::c_jr(Register rs1) {
-  DCHECK(rs1 != zero_reg);
-  GenInstrCR(0b1000, C2, rs1, zero_reg);
-  BlockTrampolinePoolFor(1);
-}
-
-void Assembler::c_mv(Register rd, Register rs2) {
-  DCHECK(rd != zero_reg && rs2 != zero_reg);
-  GenInstrCR(0b1000, C2, rd, rs2);
-}
-
-void Assembler::c_ebreak() { GenInstrCR(0b1001, C2, zero_reg, zero_reg); }
-
-void Assembler::c_jalr(Register rs1) {
-  DCHECK(rs1 != zero_reg);
-  GenInstrCR(0b1001, C2, rs1, zero_reg);
-  BlockTrampolinePoolFor(1);
-}
-
-void Assembler::c_add(Register rd, Register rs2) {
-  DCHECK(rd != zero_reg && rs2 != zero_reg);
-  GenInstrCR(0b1001, C2, rd, rs2);
-}
-
-// CA Instructions
-void Assembler::c_sub(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100011, C1, rd, 0b00, rs2);
-}
-
-void Assembler::c_xor(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100011, C1, rd, 0b01, rs2);
-}
-
-void Assembler::c_or(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100011, C1, rd, 0b10, rs2);
-}
-
-void Assembler::c_and(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100011, C1, rd, 0b11, rs2);
-}
-
-void Assembler::c_subw(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100111, C1, rd, 0b00, rs2);
-}
-
-void Assembler::c_addw(Register rd, Register rs2) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs2.code() & 0b11000) == 0b01000));
-  GenInstrCA(0b100111, C1, rd, 0b01, rs2);
-}
-
-void Assembler::c_swsp(Register rs2, uint16_t uimm8) {
-  DCHECK(is_uint8(uimm8) && (uimm8 & 0x3) == 0);
-  uint8_t uimm6 = (uimm8 & 0x3c) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCSS(0b110, C2, rs2, uimm6);
-}
-
-void Assembler::c_sdsp(Register rs2, uint16_t uimm9) {
-  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
-  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
-  GenInstrCSS(0b111, C2, rs2, uimm6);
-}
-
-void Assembler::c_fsdsp(FPURegister rs2, uint16_t uimm9) {
-  DCHECK(is_uint9(uimm9) && (uimm9 & 0x7) == 0);
-  uint8_t uimm6 = (uimm9 & 0x38) | ((uimm9 & 0x1c0) >> 6);
-  GenInstrCSS(0b101, C2, rs2, uimm6);
-}
-
-// CL Instructions
-
-void Assembler::c_lw(Register rd, Register rs1, uint16_t uimm7) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
-         ((uimm7 & 0x3) == 0));
-  uint8_t uimm5 =
-      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
-  GenInstrCL(0b010, C0, rd, rs1, uimm5);
-}
-
-void Assembler::c_ld(Register rd, Register rs1, uint16_t uimm8) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
-         ((uimm8 & 0x7) == 0));
-  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCL(0b011, C0, rd, rs1, uimm5);
-}
-
-void Assembler::c_fld(FPURegister rd, Register rs1, uint16_t uimm8) {
-  DCHECK(((rd.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
-         ((uimm8 & 0x7) == 0));
-  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCL(0b001, C0, rd, rs1, uimm5);
-}
-
-// CS Instructions
-
-void Assembler::c_sw(Register rs2, Register rs1, uint16_t uimm7) {
-  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint7(uimm7) &&
-         ((uimm7 & 0x3) == 0));
-  uint8_t uimm5 =
-      ((uimm7 & 0x4) >> 1) | ((uimm7 & 0x40) >> 6) | ((uimm7 & 0x38) >> 1);
-  GenInstrCS(0b110, C0, rs2, rs1, uimm5);
-}
-
-void Assembler::c_sd(Register rs2, Register rs1, uint16_t uimm8) {
-  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
-         ((uimm8 & 0x7) == 0));
-  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCS(0b111, C0, rs2, rs1, uimm5);
-}
-
-void Assembler::c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8) {
-  DCHECK(((rs2.code() & 0b11000) == 0b01000) &&
-         ((rs1.code() & 0b11000) == 0b01000) && is_uint8(uimm8) &&
-         ((uimm8 & 0x7) == 0));
-  uint8_t uimm5 = ((uimm8 & 0x38) >> 1) | ((uimm8 & 0xc0) >> 6);
-  GenInstrCS(0b101, C0, rs2, rs1, uimm5);
-}
-
-// CJ Instructions
-
-void Assembler::c_j(int16_t imm12) {
-  DCHECK(is_int12(imm12));
-  int16_t uimm11 = ((imm12 & 0x800) >> 1) | ((imm12 & 0x400) >> 4) |
-                   ((imm12 & 0x300) >> 1) | ((imm12 & 0x80) >> 3) |
-                   ((imm12 & 0x40) >> 1) | ((imm12 & 0x20) >> 5) |
-                   ((imm12 & 0x10) << 5) | (imm12 & 0xe);
-  GenInstrCJ(0b101, C1, uimm11);
-  BlockTrampolinePoolFor(1);
-}
-
-// CB Instructions
-
-void Assembler::c_bnez(Register rs1, int16_t imm9) {
-  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
-  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
-                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
-  GenInstrCB(0b111, C1, rs1, uimm8);
-}
-
-void Assembler::c_beqz(Register rs1, int16_t imm9) {
-  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int9(imm9));
-  uint8_t uimm8 = ((imm9 & 0x20) >> 5) | ((imm9 & 0x6)) | ((imm9 & 0xc0) >> 3) |
-                  ((imm9 & 0x18) << 2) | ((imm9 & 0x100) >> 1);
-  GenInstrCB(0b110, C1, rs1, uimm8);
-}
-
-void Assembler::c_srli(Register rs1, int8_t shamt6) {
-  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
-  GenInstrCBA(0b100, 0b00, C1, rs1, shamt6);
-}
-
-void Assembler::c_srai(Register rs1, int8_t shamt6) {
-  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(shamt6));
-  GenInstrCBA(0b100, 0b01, C1, rs1, shamt6);
-}
-
-void Assembler::c_andi(Register rs1, int8_t imm6) {
-  DCHECK(((rs1.code() & 0b11000) == 0b01000) && is_int6(imm6));
-  GenInstrCBA(0b100, 0b10, C1, rs1, imm6);
-}
-
-// Definitions for using compressed vs non compressed
-
-void Assembler::NOP() {
-  if (FLAG_riscv_c_extension)
-    c_nop();
-  else
-    nop();
-}
-
-void Assembler::EBREAK() {
-  if (FLAG_riscv_c_extension)
-    c_ebreak();
-  else
-    ebreak();
-}
-
-// RVV
-
-void Assembler::vredmaxu_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                            MaskType mask) {
-  GenInstrV(VREDMAXU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
-}
-
-void Assembler::vredmax_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                           MaskType mask) {
-  GenInstrV(VREDMAX_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
-}
-
-void Assembler::vredmin_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                           MaskType mask) {
-  GenInstrV(VREDMIN_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
-}
-
-void Assembler::vredminu_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                            MaskType mask) {
-  GenInstrV(VREDMINU_FUNCT6, OP_MVV, vd, vs1, vs2, mask);
-}
-
-void Assembler::vmv_vv(VRegister vd, VRegister vs1) {
-  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, v0, NoMask);
-}
-
-void Assembler::vmv_vx(VRegister vd, Register rs1) {
-  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, v0, NoMask);
-}
-
-void Assembler::vmv_vi(VRegister vd, uint8_t simm5) {
-  GenInstrV(VMV_FUNCT6, vd, simm5, v0, NoMask);
-}
-
-void Assembler::vmv_xs(Register rd, VRegister vs2) {
-  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b00000, vs2, NoMask);
-}
-
-void Assembler::vmv_sx(VRegister vd, Register rs1) {
-  GenInstrV(VRXUNARY0_FUNCT6, OP_MVX, vd, rs1, v0, NoMask);
-}
-
-void Assembler::vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2) {
-  GenInstrV(VMV_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
-}
-
-void Assembler::vmerge_vx(VRegister vd, Register rs1, VRegister vs2) {
-  GenInstrV(VMV_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
-}
-
-void Assembler::vmerge_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
-  GenInstrV(VMV_FUNCT6, vd, imm5, vs2, Mask);
-}
-
-void Assembler::vadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
-  GenInstrV(VADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
-}
-
-void Assembler::vadc_vx(VRegister vd, Register rs1, VRegister vs2) {
-  GenInstrV(VADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
-}
-
-void Assembler::vadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
-  GenInstrV(VADC_FUNCT6, vd, imm5, vs2, Mask);
-}
-
-void Assembler::vmadc_vv(VRegister vd, VRegister vs1, VRegister vs2) {
-  GenInstrV(VMADC_FUNCT6, OP_IVV, vd, vs1, vs2, Mask);
-}
-
-void Assembler::vmadc_vx(VRegister vd, Register rs1, VRegister vs2) {
-  GenInstrV(VMADC_FUNCT6, OP_IVX, vd, rs1, vs2, Mask);
-}
-
-void Assembler::vmadc_vi(VRegister vd, uint8_t imm5, VRegister vs2) {
-  GenInstrV(VMADC_FUNCT6, vd, imm5, vs2, Mask);
-}
-
-void Assembler::vrgather_vv(VRegister vd, VRegister vs2, VRegister vs1,
-                            MaskType mask) {
-  DCHECK_NE(vd, vs1);
-  DCHECK_NE(vd, vs2);
-  GenInstrV(VRGATHER_FUNCT6, OP_IVV, vd, vs1, vs2, mask);
-}
-
-void Assembler::vrgather_vi(VRegister vd, VRegister vs2, int8_t imm5,
-                            MaskType mask) {
-  DCHECK_NE(vd, vs2);
-  GenInstrV(VRGATHER_FUNCT6, vd, imm5, vs2, mask);
-}
-
-void Assembler::vrgather_vx(VRegister vd, VRegister vs2, Register rs1,
-                            MaskType mask) {
-  DCHECK_NE(vd, vs2);
-  GenInstrV(VRGATHER_FUNCT6, OP_IVX, vd, rs1, vs2, mask);
-}
-
-void Assembler::vwaddu_wx(VRegister vd, VRegister vs2, Register rs1,
-                          MaskType mask) {
-  GenInstrV(VWADDUW_FUNCT6, OP_MVX, vd, rs1, vs2, mask);
-}
-
-void Assembler::vid_v(VRegister vd, MaskType mask) {
-  GenInstrV(VMUNARY0_FUNCT6, OP_MVV, vd, VID_V, v0, mask);
-}
-
-#define DEFINE_OPIVV(name, funct6)                                      \
-  void Assembler::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_IVV, vd, vs1, vs2, mask);                      \
-  }
-
-#define DEFINE_OPFVV(name, funct6)                                      \
-  void Assembler::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                      \
-  }
-
-#define DEFINE_OPFWV(name, funct6)                                      \
-  void Assembler::name##_wv(VRegister vd, VRegister vs2, VRegister vs1, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                      \
-  }
-
-#define DEFINE_OPFRED(name, funct6)                                     \
-  void Assembler::name##_vs(VRegister vd, VRegister vs2, VRegister vs1, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                      \
-  }
-
-#define DEFINE_OPIVX(name, funct6)                                     \
-  void Assembler::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
-                            MaskType mask) {                           \
-    GenInstrV(funct6, OP_IVX, vd, rs1, vs2, mask);                     \
-  }
-
-#define DEFINE_OPIVI(name, funct6)                                    \
-  void Assembler::name##_vi(VRegister vd, VRegister vs2, int8_t imm5, \
-                            MaskType mask) {                          \
-    GenInstrV(funct6, vd, imm5, vs2, mask);                           \
-  }
-
-#define DEFINE_OPMVV(name, funct6)                                      \
-  void Assembler::name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_MVV, vd, vs1, vs2, mask);                      \
-  }
-
-// void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, Register rs1,
-//                  VRegister vs2, MaskType mask = NoMask);
-#define DEFINE_OPMVX(name, funct6)                                     \
-  void Assembler::name##_vx(VRegister vd, VRegister vs2, Register rs1, \
-                            MaskType mask) {                           \
-    GenInstrV(funct6, OP_MVX, vd, rs1, vs2, mask);                     \
-  }
-
-#define DEFINE_OPFVF(name, funct6)                                        \
-  void Assembler::name##_vf(VRegister vd, VRegister vs2, FPURegister fs1, \
-                            MaskType mask) {                              \
-    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                        \
-  }
-
-#define DEFINE_OPFWF(name, funct6)                                        \
-  void Assembler::name##_wf(VRegister vd, VRegister vs2, FPURegister fs1, \
-                            MaskType mask) {                              \
-    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                        \
-  }
-
-#define DEFINE_OPFVV_FMA(name, funct6)                                  \
-  void Assembler::name##_vv(VRegister vd, VRegister vs1, VRegister vs2, \
-                            MaskType mask) {                            \
-    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                      \
-  }
-
-#define DEFINE_OPFVF_FMA(name, funct6)                                    \
-  void Assembler::name##_vf(VRegister vd, FPURegister fs1, VRegister vs2, \
-                            MaskType mask) {                              \
-    GenInstrV(funct6, OP_FVF, vd, fs1, vs2, mask);                        \
-  }
-
-// vector integer extension
-#define DEFINE_OPMVV_VIE(name, vs1)                                  \
-  void Assembler::name(VRegister vd, VRegister vs2, MaskType mask) { \
-    GenInstrV(VXUNARY0_FUNCT6, OP_MVV, vd, vs1, vs2, mask);          \
-  }
-
-void Assembler::vfmv_vf(VRegister vd, FPURegister fs1, MaskType mask) {
-  GenInstrV(VMV_FUNCT6, OP_FVF, vd, fs1, v0, mask);
-}
-
-void Assembler::vfmv_fs(FPURegister fd, VRegister vs2) {
-  GenInstrV(VWFUNARY0_FUNCT6, OP_FVV, fd, v0, vs2, NoMask);
-}
-
-void Assembler::vfmv_sf(VRegister vd, FPURegister fs) {
-  GenInstrV(VRFUNARY0_FUNCT6, OP_FVF, vd, fs, v0, NoMask);
-}
-
-DEFINE_OPIVV(vadd, VADD_FUNCT6)
-DEFINE_OPIVX(vadd, VADD_FUNCT6)
-DEFINE_OPIVI(vadd, VADD_FUNCT6)
-DEFINE_OPIVV(vsub, VSUB_FUNCT6)
-DEFINE_OPIVX(vsub, VSUB_FUNCT6)
-DEFINE_OPMVX(vdiv, VDIV_FUNCT6)
-DEFINE_OPMVX(vdivu, VDIVU_FUNCT6)
-DEFINE_OPMVX(vmul, VMUL_FUNCT6)
-DEFINE_OPMVX(vmulhu, VMULHU_FUNCT6)
-DEFINE_OPMVX(vmulhsu, VMULHSU_FUNCT6)
-DEFINE_OPMVX(vmulh, VMULH_FUNCT6)
-DEFINE_OPMVV(vdiv, VDIV_FUNCT6)
-DEFINE_OPMVV(vdivu, VDIVU_FUNCT6)
-DEFINE_OPMVV(vmul, VMUL_FUNCT6)
-DEFINE_OPMVV(vmulhu, VMULHU_FUNCT6)
-DEFINE_OPMVV(vmulhsu, VMULHSU_FUNCT6)
-DEFINE_OPMVV(vwmul, VWMUL_FUNCT6)
-DEFINE_OPMVV(vwmulu, VWMULU_FUNCT6)
-DEFINE_OPMVV(vmulh, VMULH_FUNCT6)
-DEFINE_OPMVV(vwadd, VWADD_FUNCT6)
-DEFINE_OPMVV(vwaddu, VWADDU_FUNCT6)
-DEFINE_OPMVV(vcompress, VCOMPRESS_FUNCT6)
-DEFINE_OPIVX(vsadd, VSADD_FUNCT6)
-DEFINE_OPIVV(vsadd, VSADD_FUNCT6)
-DEFINE_OPIVI(vsadd, VSADD_FUNCT6)
-DEFINE_OPIVX(vsaddu, VSADDU_FUNCT6)
-DEFINE_OPIVV(vsaddu, VSADDU_FUNCT6)
-DEFINE_OPIVI(vsaddu, VSADDU_FUNCT6)
-DEFINE_OPIVX(vssub, VSSUB_FUNCT6)
-DEFINE_OPIVV(vssub, VSSUB_FUNCT6)
-DEFINE_OPIVX(vssubu, VSSUBU_FUNCT6)
-DEFINE_OPIVV(vssubu, VSSUBU_FUNCT6)
-DEFINE_OPIVX(vrsub, VRSUB_FUNCT6)
-DEFINE_OPIVI(vrsub, VRSUB_FUNCT6)
-DEFINE_OPIVV(vminu, VMINU_FUNCT6)
-DEFINE_OPIVX(vminu, VMINU_FUNCT6)
-DEFINE_OPIVV(vmin, VMIN_FUNCT6)
-DEFINE_OPIVX(vmin, VMIN_FUNCT6)
-DEFINE_OPIVV(vmaxu, VMAXU_FUNCT6)
-DEFINE_OPIVX(vmaxu, VMAXU_FUNCT6)
-DEFINE_OPIVV(vmax, VMAX_FUNCT6)
-DEFINE_OPIVX(vmax, VMAX_FUNCT6)
-DEFINE_OPIVV(vand, VAND_FUNCT6)
-DEFINE_OPIVX(vand, VAND_FUNCT6)
-DEFINE_OPIVI(vand, VAND_FUNCT6)
-DEFINE_OPIVV(vor, VOR_FUNCT6)
-DEFINE_OPIVX(vor, VOR_FUNCT6)
-DEFINE_OPIVI(vor, VOR_FUNCT6)
-DEFINE_OPIVV(vxor, VXOR_FUNCT6)
-DEFINE_OPIVX(vxor, VXOR_FUNCT6)
-DEFINE_OPIVI(vxor, VXOR_FUNCT6)
-
-DEFINE_OPIVX(vslidedown, VSLIDEDOWN_FUNCT6)
-DEFINE_OPIVI(vslidedown, VSLIDEDOWN_FUNCT6)
-DEFINE_OPIVX(vslideup, VSLIDEUP_FUNCT6)
-DEFINE_OPIVI(vslideup, VSLIDEUP_FUNCT6)
-
-DEFINE_OPIVV(vmseq, VMSEQ_FUNCT6)
-DEFINE_OPIVX(vmseq, VMSEQ_FUNCT6)
-DEFINE_OPIVI(vmseq, VMSEQ_FUNCT6)
-
-DEFINE_OPIVV(vmsne, VMSNE_FUNCT6)
-DEFINE_OPIVX(vmsne, VMSNE_FUNCT6)
-DEFINE_OPIVI(vmsne, VMSNE_FUNCT6)
-
-DEFINE_OPIVV(vmsltu, VMSLTU_FUNCT6)
-DEFINE_OPIVX(vmsltu, VMSLTU_FUNCT6)
-
-DEFINE_OPIVV(vmslt, VMSLT_FUNCT6)
-DEFINE_OPIVX(vmslt, VMSLT_FUNCT6)
-
-DEFINE_OPIVV(vmsle, VMSLE_FUNCT6)
-DEFINE_OPIVX(vmsle, VMSLE_FUNCT6)
-DEFINE_OPIVI(vmsle, VMSLE_FUNCT6)
-
-DEFINE_OPIVV(vmsleu, VMSLEU_FUNCT6)
-DEFINE_OPIVX(vmsleu, VMSLEU_FUNCT6)
-DEFINE_OPIVI(vmsleu, VMSLEU_FUNCT6)
-
-DEFINE_OPIVI(vmsgt, VMSGT_FUNCT6)
-DEFINE_OPIVX(vmsgt, VMSGT_FUNCT6)
-
-DEFINE_OPIVI(vmsgtu, VMSGTU_FUNCT6)
-DEFINE_OPIVX(vmsgtu, VMSGTU_FUNCT6)
-
-DEFINE_OPIVV(vsrl, VSRL_FUNCT6)
-DEFINE_OPIVX(vsrl, VSRL_FUNCT6)
-DEFINE_OPIVI(vsrl, VSRL_FUNCT6)
-
-DEFINE_OPIVV(vsra, VSRA_FUNCT6)
-DEFINE_OPIVX(vsra, VSRA_FUNCT6)
-DEFINE_OPIVI(vsra, VSRA_FUNCT6)
-
-DEFINE_OPIVV(vsll, VSLL_FUNCT6)
-DEFINE_OPIVX(vsll, VSLL_FUNCT6)
-DEFINE_OPIVI(vsll, VSLL_FUNCT6)
-
-DEFINE_OPIVV(vsmul, VSMUL_FUNCT6)
-DEFINE_OPIVX(vsmul, VSMUL_FUNCT6)
-
-DEFINE_OPFVV(vfadd, VFADD_FUNCT6)
-DEFINE_OPFVF(vfadd, VFADD_FUNCT6)
-DEFINE_OPFVV(vfsub, VFSUB_FUNCT6)
-DEFINE_OPFVF(vfsub, VFSUB_FUNCT6)
-DEFINE_OPFVV(vfdiv, VFDIV_FUNCT6)
-DEFINE_OPFVF(vfdiv, VFDIV_FUNCT6)
-DEFINE_OPFVV(vfmul, VFMUL_FUNCT6)
-DEFINE_OPFVF(vfmul, VFMUL_FUNCT6)
-DEFINE_OPFVV(vmfeq, VMFEQ_FUNCT6)
-DEFINE_OPFVV(vmfne, VMFNE_FUNCT6)
-DEFINE_OPFVV(vmflt, VMFLT_FUNCT6)
-DEFINE_OPFVV(vmfle, VMFLE_FUNCT6)
-DEFINE_OPFVV(vfmax, VFMAX_FUNCT6)
-DEFINE_OPFVV(vfmin, VFMIN_FUNCT6)
-
-// Vector Widening Floating-Point Add/Subtract Instructions
-DEFINE_OPFVV(vfwadd, VFWADD_FUNCT6)
-DEFINE_OPFVF(vfwadd, VFWADD_FUNCT6)
-DEFINE_OPFVV(vfwsub, VFWSUB_FUNCT6)
-DEFINE_OPFVF(vfwsub, VFWSUB_FUNCT6)
-DEFINE_OPFWV(vfwadd, VFWADD_W_FUNCT6)
-DEFINE_OPFWF(vfwadd, VFWADD_W_FUNCT6)
-DEFINE_OPFWV(vfwsub, VFWSUB_W_FUNCT6)
-DEFINE_OPFWF(vfwsub, VFWSUB_W_FUNCT6)
-
-// Vector Widening Floating-Point Reduction Instructions
-DEFINE_OPFVV(vfwredusum, VFWREDUSUM_FUNCT6)
-DEFINE_OPFVV(vfwredosum, VFWREDOSUM_FUNCT6)
-
-// Vector Widening Floating-Point Multiply
-DEFINE_OPFVV(vfwmul, VFWMUL_FUNCT6)
-DEFINE_OPFVF(vfwmul, VFWMUL_FUNCT6)
-
-DEFINE_OPFRED(vfredmax, VFREDMAX_FUNCT6)
-
-DEFINE_OPFVV(vfsngj, VFSGNJ_FUNCT6)
-DEFINE_OPFVF(vfsngj, VFSGNJ_FUNCT6)
-DEFINE_OPFVV(vfsngjn, VFSGNJN_FUNCT6)
-DEFINE_OPFVF(vfsngjn, VFSGNJN_FUNCT6)
-DEFINE_OPFVV(vfsngjx, VFSGNJX_FUNCT6)
-DEFINE_OPFVF(vfsngjx, VFSGNJX_FUNCT6)
-
-// Vector Single-Width Floating-Point Fused Multiply-Add Instructions
-DEFINE_OPFVV_FMA(vfmadd, VFMADD_FUNCT6)
-DEFINE_OPFVF_FMA(vfmadd, VFMADD_FUNCT6)
-DEFINE_OPFVV_FMA(vfmsub, VFMSUB_FUNCT6)
-DEFINE_OPFVF_FMA(vfmsub, VFMSUB_FUNCT6)
-DEFINE_OPFVV_FMA(vfmacc, VFMACC_FUNCT6)
-DEFINE_OPFVF_FMA(vfmacc, VFMACC_FUNCT6)
-DEFINE_OPFVV_FMA(vfmsac, VFMSAC_FUNCT6)
-DEFINE_OPFVF_FMA(vfmsac, VFMSAC_FUNCT6)
-DEFINE_OPFVV_FMA(vfnmadd, VFNMADD_FUNCT6)
-DEFINE_OPFVF_FMA(vfnmadd, VFNMADD_FUNCT6)
-DEFINE_OPFVV_FMA(vfnmsub, VFNMSUB_FUNCT6)
-DEFINE_OPFVF_FMA(vfnmsub, VFNMSUB_FUNCT6)
-DEFINE_OPFVV_FMA(vfnmacc, VFNMACC_FUNCT6)
-DEFINE_OPFVF_FMA(vfnmacc, VFNMACC_FUNCT6)
-DEFINE_OPFVV_FMA(vfnmsac, VFNMSAC_FUNCT6)
-DEFINE_OPFVF_FMA(vfnmsac, VFNMSAC_FUNCT6)
-
-// Vector Widening Floating-Point Fused Multiply-Add Instructions
-DEFINE_OPFVV_FMA(vfwmacc, VFWMACC_FUNCT6)
-DEFINE_OPFVF_FMA(vfwmacc, VFWMACC_FUNCT6)
-DEFINE_OPFVV_FMA(vfwnmacc, VFWNMACC_FUNCT6)
-DEFINE_OPFVF_FMA(vfwnmacc, VFWNMACC_FUNCT6)
-DEFINE_OPFVV_FMA(vfwmsac, VFWMSAC_FUNCT6)
-DEFINE_OPFVF_FMA(vfwmsac, VFWMSAC_FUNCT6)
-DEFINE_OPFVV_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
-DEFINE_OPFVF_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
-
-// Vector Narrowing Fixed-Point Clip Instructions
-DEFINE_OPIVV(vnclip, VNCLIP_FUNCT6)
-DEFINE_OPIVX(vnclip, VNCLIP_FUNCT6)
-DEFINE_OPIVI(vnclip, VNCLIP_FUNCT6)
-DEFINE_OPIVV(vnclipu, VNCLIPU_FUNCT6)
-DEFINE_OPIVX(vnclipu, VNCLIPU_FUNCT6)
-DEFINE_OPIVI(vnclipu, VNCLIPU_FUNCT6)
-
-// Vector Integer Extension
-DEFINE_OPMVV_VIE(vzext_vf8, 0b00010)
-DEFINE_OPMVV_VIE(vsext_vf8, 0b00011)
-DEFINE_OPMVV_VIE(vzext_vf4, 0b00100)
-DEFINE_OPMVV_VIE(vsext_vf4, 0b00101)
-DEFINE_OPMVV_VIE(vzext_vf2, 0b00110)
-DEFINE_OPMVV_VIE(vsext_vf2, 0b00111)
-
-#undef DEFINE_OPIVI
-#undef DEFINE_OPIVV
-#undef DEFINE_OPIVX
-#undef DEFINE_OPFVV
-#undef DEFINE_OPFWV
-#undef DEFINE_OPFVF
-#undef DEFINE_OPFWF
-#undef DEFINE_OPFVV_FMA
-#undef DEFINE_OPFVF_FMA
-#undef DEFINE_OPMVV_VIE
-
-void Assembler::vsetvli(Register rd, Register rs1, VSew vsew, Vlmul vlmul,
-                        TailAgnosticType tail, MaskAgnosticType mask) {
-  int32_t zimm = GenZimm(vsew, vlmul, tail, mask);
-  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
-                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
-                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x0 << 31;
-  emit(instr);
-}
-
-void Assembler::vsetivli(Register rd, uint8_t uimm, VSew vsew, Vlmul vlmul,
-                         TailAgnosticType tail, MaskAgnosticType mask) {
-  DCHECK(is_uint5(uimm));
-  int32_t zimm = GenZimm(vsew, vlmul, tail, mask) & 0x3FF;
-  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
-                ((uimm & 0x1F) << kRvvUimmShift) |
-                (((uint32_t)zimm << kRvvZimmShift) & kRvvZimmMask) | 0x3 << 30;
-  emit(instr);
-}
-
-void Assembler::vsetvl(Register rd, Register rs1, Register rs2) {
-  Instr instr = OP_V | ((rd.code() & 0x1F) << kRvvRdShift) | (0x7 << 12) |
-                ((rs1.code() & 0x1F) << kRvvRs1Shift) |
-                ((rs2.code() & 0x1F) << kRvvRs2Shift) | 0x40 << 25;
-  emit(instr);
-}
-
-uint8_t vsew_switch(VSew vsew) {
-  uint8_t width;
-  switch (vsew) {
-    case E8:
-      width = 0b000;
-      break;
-    case E16:
-      width = 0b101;
-      break;
-    case E32:
-      width = 0b110;
-      break;
-    default:
-      width = 0b111;
-      break;
-  }
-  return width;
-}
-
-void Assembler::vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                   MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b000);
-}
-void Assembler::vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                    MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b000);
-}
-void Assembler::vlx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
-                    MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0);
-}
-
-void Assembler::vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                   MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b000);
-}
-void Assembler::vss(VRegister vs3, Register rs1, Register rs2, VSew vsew,
-                    MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vs3, rs1, rs2, mask, 0b10, 0, 0b000);
-}
-
-void Assembler::vsx(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
-                    MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b11, 0, 0b000);
-}
-void Assembler::vsu(VRegister vd, Register rs1, VRegister vs2, VSew vsew,
-                    MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, vs2, mask, 0b01, 0, 0b000);
-}
-
-void Assembler::vlseg2(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b001);
-}
-
-void Assembler::vlseg3(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b010);
-}
-
-void Assembler::vlseg4(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b011);
-}
-
-void Assembler::vlseg5(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b100);
-}
-
-void Assembler::vlseg6(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b101);
-}
-
-void Assembler::vlseg7(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b110);
-}
-
-void Assembler::vlseg8(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, lumop, mask, 0b00, 0, 0b111);
-}
-void Assembler::vsseg2(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b001);
-}
-void Assembler::vsseg3(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b010);
-}
-void Assembler::vsseg4(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b011);
-}
-void Assembler::vsseg5(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b100);
-}
-void Assembler::vsseg6(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b101);
-}
-void Assembler::vsseg7(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b110);
-}
-void Assembler::vsseg8(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-                       MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, sumop, mask, 0b00, 0, 0b111);
-}
-
-void Assembler::vlsseg2(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
-}
-void Assembler::vlsseg3(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
-}
-void Assembler::vlsseg4(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
-}
-void Assembler::vlsseg5(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
-}
-void Assembler::vlsseg6(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
-}
-void Assembler::vlsseg7(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
-}
-void Assembler::vlsseg8(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
-}
-void Assembler::vssseg2(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b001);
-}
-void Assembler::vssseg3(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b010);
-}
-void Assembler::vssseg4(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b011);
-}
-void Assembler::vssseg5(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b100);
-}
-void Assembler::vssseg6(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b101);
-}
-void Assembler::vssseg7(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b110);
-}
-void Assembler::vssseg8(VRegister vd, Register rs1, Register rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b10, 0, 0b111);
-}
-
-void Assembler::vlxseg2(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
-}
-void Assembler::vlxseg3(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
-}
-void Assembler::vlxseg4(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
-}
-void Assembler::vlxseg5(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
-}
-void Assembler::vlxseg6(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
-}
-void Assembler::vlxseg7(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
-}
-void Assembler::vlxseg8(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(LOAD_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
-}
-void Assembler::vsxseg2(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b001);
-}
-void Assembler::vsxseg3(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b010);
-}
-void Assembler::vsxseg4(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b011);
-}
-void Assembler::vsxseg5(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b100);
-}
-void Assembler::vsxseg6(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b101);
-}
-void Assembler::vsxseg7(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b110);
-}
-void Assembler::vsxseg8(VRegister vd, Register rs1, VRegister rs2, VSew vsew,
-                        MaskType mask) {
-  uint8_t width = vsew_switch(vsew);
-  GenInstrV(STORE_FP, width, vd, rs1, rs2, mask, 0b11, 0, 0b111);
-}
-
-void Assembler::vfirst_m(Register rd, VRegister vs2, MaskType mask) {
-  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10001, vs2, mask);
-}
-
-void Assembler::vcpop_m(Register rd, VRegister vs2, MaskType mask) {
-  GenInstrV(VWXUNARY0_FUNCT6, OP_MVV, rd, 0b10000, vs2, mask);
-}
-
-// Privileged
-void Assembler::uret() {
-  GenInstrPriv(0b0000000, ToRegister(0), ToRegister(0b00010));
-}
-
-void Assembler::sret() {
-  GenInstrPriv(0b0001000, ToRegister(0), ToRegister(0b00010));
-}
-
-void Assembler::mret() {
-  GenInstrPriv(0b0011000, ToRegister(0), ToRegister(0b00010));
-}
-
-void Assembler::wfi() {
-  GenInstrPriv(0b0001000, ToRegister(0), ToRegister(0b00101));
-}
-
-void Assembler::sfence_vma(Register rs1, Register rs2) {
-  GenInstrR(0b0001001, 0b000, SYSTEM, ToRegister(0), rs1, rs2);
-}
-
-// Assembler Pseudo Instructions (Tables 25.2 and 25.3, RISC-V Unprivileged ISA)
-
-void Assembler::nop() { addi(ToRegister(0), ToRegister(0), 0); }
-
-void Assembler::RV_li(Register rd, int64_t imm) {
-  // 64-bit imm is put in the register rd.
-  // In most cases the imm is 32 bit and 2 instructions are generated. If a
-  // temporary register is available, in the worst case, 6 instructions are
-  // generated for a full 64-bit immediate. If temporay register is not
-  // available the maximum will be 8 instructions. If imm is more than 32 bits
-  // and a temp register is available, imm is divided into two 32-bit parts,
-  // low_32 and up_32. Each part is built in a separate register. low_32 is
-  // built before up_32. If low_32 is negative (upper 32 bits are 1), 0xffffffff
-  // is subtracted from up_32 before up_32 is built. This compensates for 32
-  // bits of 1's in the lower when the two registers are added. If no temp is
-  // available, the upper 32 bit is built in rd, and the lower 32 bits are
-  // devided to 3 parts (11, 11, and 10 bits). The parts are shifted and added
-  // to the upper part built in rd.
-  if (is_int32(imm + 0x800)) {
-    // 32-bit case. Maximum of 2 instructions generated
-    int64_t high_20 = ((imm + 0x800) >> 12);
-    int64_t low_12 = imm << 52 >> 52;
-    if (high_20) {
-      lui(rd, (int32_t)high_20);
-      if (low_12) {
-        addi(rd, rd, low_12);
-      }
-    } else {
-      addi(rd, zero_reg, low_12);
-    }
-    return;
-  } else {
-    // 64-bit case: divide imm into two 32-bit parts, upper and lower
-    int64_t up_32 = imm >> 32;
-    int64_t low_32 = imm & 0xffffffffull;
-    Register temp_reg = rd;
-    // Check if a temporary register is available
-    if (up_32 == 0 || low_32 == 0) {
-      // No temp register is needed
-    } else {
-      UseScratchRegisterScope temps(this);
-      BlockTrampolinePoolScope block_trampoline_pool(this);
-      temp_reg = temps.hasAvailable() ? temps.Acquire() : no_reg;
-    }
-    if (temp_reg != no_reg) {
-      // keep track of hardware behavior for lower part in sim_low
-      int64_t sim_low = 0;
-      // Build lower part
-      if (low_32 != 0) {
-        int64_t high_20 = ((low_32 + 0x800) >> 12);
-        int64_t low_12 = low_32 & 0xfff;
-        if (high_20) {
-          // Adjust to 20 bits for the case of overflow
-          high_20 &= 0xfffff;
-          sim_low = ((high_20 << 12) << 32) >> 32;
-          lui(rd, (int32_t)high_20);
-          if (low_12) {
-            sim_low += (low_12 << 52 >> 52) | low_12;
-            addi(rd, rd, low_12);
-          }
-        } else {
-          sim_low = low_12;
-          ori(rd, zero_reg, low_12);
-        }
-      }
-      if (sim_low & 0x100000000) {
-        // Bit 31 is 1. Either an overflow or a negative 64 bit
-        if (up_32 == 0) {
-          // Positive number, but overflow because of the add 0x800
-          slli(rd, rd, 32);
-          srli(rd, rd, 32);
-          return;
-        }
-        // low_32 is a negative 64 bit after the build
-        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
-      }
-      if (up_32 == 0) {
-        return;
-      }
-      // Build upper part in a temporary register
-      if (low_32 == 0) {
-        // Build upper part in rd
-        temp_reg = rd;
-      }
-      int64_t high_20 = (up_32 + 0x800) >> 12;
-      int64_t low_12 = up_32 & 0xfff;
-      if (high_20) {
-        // Adjust to 20 bits for the case of overflow
-        high_20 &= 0xfffff;
-        lui(temp_reg, (int32_t)high_20);
-        if (low_12) {
-          addi(temp_reg, temp_reg, low_12);
-        }
-      } else {
-        ori(temp_reg, zero_reg, low_12);
-      }
-      // Put it at the bgining of register
-      slli(temp_reg, temp_reg, 32);
-      if (low_32 != 0) {
-        add(rd, rd, temp_reg);
-      }
-      return;
-    }
-    // No temp register. Build imm in rd.
-    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
-    // parts to the upper part by doing shift and add.
-    // First build upper part in rd.
-    int64_t high_20 = (up_32 + 0x800) >> 12;
-    int64_t low_12 = up_32 & 0xfff;
-    if (high_20) {
-      // Adjust to 20 bits for the case of overflow
-      high_20 &= 0xfffff;
-      lui(rd, (int32_t)high_20);
-      if (low_12) {
-        addi(rd, rd, low_12);
-      }
-    } else {
-      ori(rd, zero_reg, low_12);
-    }
-    // upper part already in rd. Each part to be added to rd, has maximum of 11
-    // bits, and always starts with a 1. rd is shifted by the size of the part
-    // plus the number of zeros between the parts. Each part is added after the
-    // left shift.
-    uint32_t mask = 0x80000000;
-    int32_t shift_val = 0;
-    int32_t i;
-    for (i = 0; i < 32; i++) {
-      if ((low_32 & mask) == 0) {
-        mask >>= 1;
-        shift_val++;
-        if (i == 31) {
-          // rest is zero
-          slli(rd, rd, shift_val);
-        }
-        continue;
-      }
-      // The first 1 seen
-      int32_t part;
-      if ((i + 11) < 32) {
-        // Pick 11 bits
-        part = ((uint32_t)(low_32 << i) >> i) >> (32 - (i + 11));
-        slli(rd, rd, shift_val + 11);
-        ori(rd, rd, part);
-        i += 10;
-        mask >>= 11;
-      } else {
-        part = (uint32_t)(low_32 << i) >> i;
-        slli(rd, rd, shift_val + (32 - i));
-        ori(rd, rd, part);
-        break;
-      }
-      shift_val = 0;
-    }
-  }
-}
-
-int Assembler::li_estimate(int64_t imm, bool is_get_temp_reg) {
-  int count = 0;
-  // imitate Assembler::RV_li
-  if (is_int32(imm + 0x800)) {
-    // 32-bit case. Maximum of 2 instructions generated
-    int64_t high_20 = ((imm + 0x800) >> 12);
-    int64_t low_12 = imm << 52 >> 52;
-    if (high_20) {
-      count++;
-      if (low_12) {
-        count++;
-      }
-    } else {
-      count++;
-    }
-    return count;
-  } else {
-    // 64-bit case: divide imm into two 32-bit parts, upper and lower
-    int64_t up_32 = imm >> 32;
-    int64_t low_32 = imm & 0xffffffffull;
-    // Check if a temporary register is available
-    if (is_get_temp_reg) {
-      // keep track of hardware behavior for lower part in sim_low
-      int64_t sim_low = 0;
-      // Build lower part
-      if (low_32 != 0) {
-        int64_t high_20 = ((low_32 + 0x800) >> 12);
-        int64_t low_12 = low_32 & 0xfff;
-        if (high_20) {
-          // Adjust to 20 bits for the case of overflow
-          high_20 &= 0xfffff;
-          sim_low = ((high_20 << 12) << 32) >> 32;
-          count++;
-          if (low_12) {
-            sim_low += (low_12 << 52 >> 52) | low_12;
-            count++;
-          }
-        } else {
-          sim_low = low_12;
-          count++;
-        }
-      }
-      if (sim_low & 0x100000000) {
-        // Bit 31 is 1. Either an overflow or a negative 64 bit
-        if (up_32 == 0) {
-          // Positive number, but overflow because of the add 0x800
-          count++;
-          count++;
-          return count;
-        }
-        // low_32 is a negative 64 bit after the build
-        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
-      }
-      if (up_32 == 0) {
-        return count;
-      }
-      int64_t high_20 = (up_32 + 0x800) >> 12;
-      int64_t low_12 = up_32 & 0xfff;
-      if (high_20) {
-        // Adjust to 20 bits for the case of overflow
-        high_20 &= 0xfffff;
-        count++;
-        if (low_12) {
-          count++;
-        }
-      } else {
-        count++;
-      }
-      // Put it at the bgining of register
-      count++;
-      if (low_32 != 0) {
-        count++;
-      }
-      return count;
-    }
-    // No temp register. Build imm in rd.
-    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
-    // parts to the upper part by doing shift and add.
-    // First build upper part in rd.
-    int64_t high_20 = (up_32 + 0x800) >> 12;
-    int64_t low_12 = up_32 & 0xfff;
-    if (high_20) {
-      // Adjust to 20 bits for the case of overflow
-      high_20 &= 0xfffff;
-      count++;
-      if (low_12) {
-        count++;
-      }
-    } else {
-      count++;
-    }
-    // upper part already in rd. Each part to be added to rd, has maximum of 11
-    // bits, and always starts with a 1. rd is shifted by the size of the part
-    // plus the number of zeros between the parts. Each part is added after the
-    // left shift.
-    uint32_t mask = 0x80000000;
-    int32_t i;
-    for (i = 0; i < 32; i++) {
-      if ((low_32 & mask) == 0) {
-        mask >>= 1;
-        if (i == 31) {
-          // rest is zero
-          count++;
-        }
-        continue;
-      }
-      // The first 1 seen
-      if ((i + 11) < 32) {
-        // Pick 11 bits
-        count++;
-        count++;
-        i += 10;
-        mask >>= 11;
-      } else {
-        count++;
-        count++;
-        break;
-      }
-    }
-  }
-  return count;
-}
-
-void Assembler::li_ptr(Register rd, int64_t imm) {
-  // Initialize rd with an address
-  // Pointers are 48 bits
-  // 6 fixed instructions are generated
-  DCHECK_EQ((imm & 0xfff0000000000000ll), 0);
-  int64_t a6 = imm & 0x3f;                      // bits 0:5. 6 bits
-  int64_t b11 = (imm >> 6) & 0x7ff;             // bits 6:11. 11 bits
-  int64_t high_31 = (imm >> 17) & 0x7fffffff;   // 31 bits
-  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
-  int64_t low_12 = high_31 & 0xfff;             // 12 bits
-  lui(rd, (int32_t)high_20);
-  addi(rd, rd, low_12);  // 31 bits in rd.
-  slli(rd, rd, 11);      // Space for next 11 bis
-  ori(rd, rd, b11);      // 11 bits are put in. 42 bit in rd
-  slli(rd, rd, 6);       // Space for next 6 bits
-  ori(rd, rd, a6);       // 6 bits are put in. 48 bis in rd
-}
-
-void Assembler::li_constant(Register rd, int64_t imm) {
-  DEBUG_PRINTF("li_constant(%d, %lx <%ld>)\n", ToNumber(rd), imm, imm);
-  lui(rd, (imm + (1LL << 47) + (1LL << 35) + (1LL << 23) + (1LL << 11)) >>
-              48);  // Bits 63:48
-  addiw(rd, rd,
-        (imm + (1LL << 35) + (1LL << 23) + (1LL << 11)) << 16 >>
-            52);  // Bits 47:36
-  slli(rd, rd, 12);
-  addi(rd, rd, (imm + (1LL << 23) + (1LL << 11)) << 28 >> 52);  // Bits 35:24
-  slli(rd, rd, 12);
-  addi(rd, rd, (imm + (1LL << 11)) << 40 >> 52);  // Bits 23:12
-  slli(rd, rd, 12);
-  addi(rd, rd, imm << 52 >> 52);  // Bits 11:0
-}
-
-// Break / Trap instructions.
-void Assembler::break_(uint32_t code, bool break_as_stop) {
-  // We need to invalidate breaks that could be stops as well because the
-  // simulator expects a char pointer after the stop instruction.
-  // See constants-mips.h for explanation.
-  DCHECK(
-      (break_as_stop && code <= kMaxStopCode && code > kMaxWatchpointCode) ||
-      (!break_as_stop && (code > kMaxStopCode || code <= kMaxWatchpointCode)));
-
-  // since ebreak does not allow additional immediate field, we use the
-  // immediate field of lui instruction immediately following the ebreak to
-  // encode the "code" info
-  ebreak();
-  DCHECK(is_uint20(code));
-  lui(zero_reg, code);
-}
-
-void Assembler::stop(uint32_t code) {
-  DCHECK_GT(code, kMaxWatchpointCode);
-  DCHECK_LE(code, kMaxStopCode);
-#if defined(V8_HOST_ARCH_RISCV64)
-  break_(0x54321);
-#else  // V8_HOST_ARCH_RISCV64
-  break_(code, true);
-#endif
-}
-
-// Original MIPS Instructions
-
-// ------------Memory-instructions-------------
-
-bool Assembler::NeedAdjustBaseAndOffset(const MemOperand& src,
-                                        OffsetAccessType access_type,
-                                        int second_access_add_to_offset) {
-  bool two_accesses = static_cast<bool>(access_type);
-  DCHECK_LE(second_access_add_to_offset, 7);  // Must be <= 7.
-
-  // is_int12 must be passed a signed value, hence the static cast below.
-  if (is_int12(src.offset()) &&
-      (!two_accesses || is_int12(static_cast<int32_t>(
-                            src.offset() + second_access_add_to_offset)))) {
-    // Nothing to do: 'offset' (and, if needed, 'offset + 4', or other specified
-    // value) fits into int12.
-    return false;
-  }
-  return true;
-}
-
-void Assembler::AdjustBaseAndOffset(MemOperand* src, Register scratch,
-                                    OffsetAccessType access_type,
-                                    int second_Access_add_to_offset) {
-  // This method is used to adjust the base register and offset pair
-  // for a load/store when the offset doesn't fit into int12.
-
-  // Must not overwrite the register 'base' while loading 'offset'.
-  constexpr int32_t kMinOffsetForSimpleAdjustment = 0x7F8;
-  constexpr int32_t kMaxOffsetForSimpleAdjustment =
-      2 * kMinOffsetForSimpleAdjustment;
-  if (0 <= src->offset() && src->offset() <= kMaxOffsetForSimpleAdjustment) {
-    addi(scratch, src->rm(), kMinOffsetForSimpleAdjustment);
-    src->offset_ -= kMinOffsetForSimpleAdjustment;
-  } else if (-kMaxOffsetForSimpleAdjustment <= src->offset() &&
-             src->offset() < 0) {
-    addi(scratch, src->rm(), -kMinOffsetForSimpleAdjustment);
-    src->offset_ += kMinOffsetForSimpleAdjustment;
-  } else if (access_type == OffsetAccessType::SINGLE_ACCESS) {
-    RV_li(scratch, (static_cast<int64_t>(src->offset()) + 0x800) >> 12 << 12);
-    add(scratch, scratch, src->rm());
-    src->offset_ = src->offset() << 20 >> 20;
-  } else {
-    RV_li(scratch, src->offset());
-    add(scratch, scratch, src->rm());
-    src->offset_ = 0;
-  }
-  src->rm_ = scratch;
-}
-
-int Assembler::RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
-                                         intptr_t pc_delta) {
-  if (RelocInfo::IsInternalReference(rmode)) {
-    int64_t* p = reinterpret_cast<int64_t*>(pc);
-    if (*p == kEndOfJumpChain) {
-      return 0;  // Number of instructions patched.
-    }
-    *p += pc_delta;
-    return 2;  // Number of instructions patched.
-  }
-  Instr instr = instr_at(pc);
-  DCHECK(RelocInfo::IsInternalReferenceEncoded(rmode));
-  if (IsLui(instr)) {
-    uint64_t target_address = target_address_at(pc) + pc_delta;
-    DEBUG_PRINTF("target_address 0x%lx\n", target_address);
-    set_target_value_at(pc, target_address);
-    return 8;  // Number of instructions patched.
-  } else {
-    UNIMPLEMENTED();
-  }
-}
-
-void Assembler::RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
-                                          intptr_t pc_delta) {
-  Instr instr = instr_at(pc);
-  Instr instr1 = instr_at(pc + 1 * kInstrSize);
-  DCHECK(RelocInfo::IsRelativeCodeTarget(rmode));
-  if (IsAuipc(instr) && IsJalr(instr1)) {
-    int32_t imm;
-    imm = BrachlongOffset(instr, instr1);
-    imm -= pc_delta;
-    PatchBranchlongOffset(pc, instr, instr1, imm);
-    return;
-  } else {
-    UNREACHABLE();
-  }
-}
-
-void Assembler::GrowBuffer() {
-  DEBUG_PRINTF("GrowBuffer: %p -> ", buffer_start_);
-  // Compute new buffer size.
-  int old_size = buffer_->size();
-  int new_size = std::min(2 * old_size, old_size + 1 * MB);
-
-  // Some internal data structures overflow for very large buffers,
-  // they must ensure that kMaximalBufferSize is not too large.
-  if (new_size > kMaximalBufferSize) {
-    V8::FatalProcessOutOfMemory(nullptr, "Assembler::GrowBuffer");
-  }
-
-  // Set up new buffer.
-  std::unique_ptr<AssemblerBuffer> new_buffer = buffer_->Grow(new_size);
-  DCHECK_EQ(new_size, new_buffer->size());
-  byte* new_start = new_buffer->start();
-
-  // Copy the data.
-  intptr_t pc_delta = new_start - buffer_start_;
-  intptr_t rc_delta = (new_start + new_size) - (buffer_start_ + old_size);
-  size_t reloc_size = (buffer_start_ + old_size) - reloc_info_writer.pos();
-  MemMove(new_start, buffer_start_, pc_offset());
-  MemMove(reloc_info_writer.pos() + rc_delta, reloc_info_writer.pos(),
-          reloc_size);
-
-  // Switch buffers.
-  buffer_ = std::move(new_buffer);
-  buffer_start_ = new_start;
-  DEBUG_PRINTF("%p\n", buffer_start_);
-  pc_ += pc_delta;
-  reloc_info_writer.Reposition(reloc_info_writer.pos() + rc_delta,
-                               reloc_info_writer.last_pc() + pc_delta);
-
-  // Relocate runtime entries.
-  base::Vector<byte> instructions{buffer_start_,
-                                  static_cast<size_t>(pc_offset())};
-  base::Vector<const byte> reloc_info{reloc_info_writer.pos(), reloc_size};
-  for (RelocIterator it(instructions, reloc_info, 0); !it.done(); it.next()) {
-    RelocInfo::Mode rmode = it.rinfo()->rmode();
-    if (rmode == RelocInfo::INTERNAL_REFERENCE) {
-      RelocateInternalReference(rmode, it.rinfo()->pc(), pc_delta);
-    }
-  }
-
-  DCHECK(!overflow());
-}
-
-void Assembler::db(uint8_t data) {
-  if (!is_buffer_growth_blocked()) CheckBuffer();
-  DEBUG_PRINTF("%p: constant 0x%x\n", pc_, data);
-  EmitHelper(data);
-}
-
-void Assembler::dd(uint32_t data, RelocInfo::Mode rmode) {
-  if (!RelocInfo::IsNoInfo(rmode)) {
-    DCHECK(RelocInfo::IsDataEmbeddedObject(rmode) ||
-           RelocInfo::IsLiteralConstant(rmode));
-    RecordRelocInfo(rmode);
-  }
-  if (!is_buffer_growth_blocked()) CheckBuffer();
-  DEBUG_PRINTF("%p: constant 0x%x\n", pc_, data);
-  EmitHelper(data);
-}
-
-void Assembler::dq(uint64_t data, RelocInfo::Mode rmode) {
-  if (!RelocInfo::IsNoInfo(rmode)) {
-    DCHECK(RelocInfo::IsDataEmbeddedObject(rmode) ||
-           RelocInfo::IsLiteralConstant(rmode));
-    RecordRelocInfo(rmode);
-  }
-  if (!is_buffer_growth_blocked()) CheckBuffer();
-  DEBUG_PRINTF("%p: constant 0x%lx\n", pc_, data);
-  EmitHelper(data);
-}
-
-void Assembler::dd(Label* label) {
-  uint64_t data;
-  if (!is_buffer_growth_blocked()) CheckBuffer();
-  if (label->is_bound()) {
-    data = reinterpret_cast<uint64_t>(buffer_start_ + label->pos());
-  } else {
-    data = jump_address(label);
-    internal_reference_positions_.insert(label->pos());
-  }
-  RecordRelocInfo(RelocInfo::INTERNAL_REFERENCE);
-  EmitHelper(data);
-}
-
-void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
-  if (!ShouldRecordRelocInfo(rmode)) return;
-  // We do not try to reuse pool constants.
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code());
-  DCHECK_GE(buffer_space(), kMaxRelocSize);  // Too late to grow buffer here.
-  reloc_info_writer.Write(&rinfo);
-}
-
-void Assembler::BlockTrampolinePoolFor(int instructions) {
-  DEBUG_PRINTF("\tBlockTrampolinePoolFor %d", instructions);
-  CheckTrampolinePoolQuick(instructions);
-  DEBUG_PRINTF("\tpc_offset %d,BlockTrampolinePoolBefore %d\n", pc_offset(),
-               pc_offset() + instructions * kInstrSize);
-  BlockTrampolinePoolBefore(pc_offset() + instructions * kInstrSize);
-}
-
-void Assembler::CheckTrampolinePool() {
-  // Some small sequences of instructions must not be broken up by the
-  // insertion of a trampoline pool; such sequences are protected by setting
-  // either trampoline_pool_blocked_nesting_ or no_trampoline_pool_before_,
-  // which are both checked here. Also, recursive calls to CheckTrampolinePool
-  // are blocked by trampoline_pool_blocked_nesting_.
-  DEBUG_PRINTF("\tpc_offset %d no_trampoline_pool_before:%d\n", pc_offset(),
-               no_trampoline_pool_before_);
-  DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
-               trampoline_pool_blocked_nesting_);
-  if ((trampoline_pool_blocked_nesting_ > 0) ||
-      (pc_offset() < no_trampoline_pool_before_)) {
-    // Emission is currently blocked; make sure we try again as soon as
-    // possible.
-    if (trampoline_pool_blocked_nesting_ > 0) {
-      next_buffer_check_ = pc_offset() + kInstrSize;
-    } else {
-      next_buffer_check_ = no_trampoline_pool_before_;
-    }
-    return;
-  }
-
-  DCHECK(!trampoline_emitted_);
-  DCHECK_GE(unbound_labels_count_, 0);
-  if (unbound_labels_count_ > 0) {
-    // First we emit jump, then we emit trampoline pool.
-    {
-      DEBUG_PRINTF("inserting trampoline pool at %p (%d)\n",
-                   reinterpret_cast<Instr*>(buffer_start_ + pc_offset()),
-                   pc_offset());
-      BlockTrampolinePoolScope block_trampoline_pool(this);
-      Label after_pool;
-      j(&after_pool);
-
-      int pool_start = pc_offset();
-      for (int i = 0; i < unbound_labels_count_; i++) {
-        int64_t imm64;
-        imm64 = branch_long_offset(&after_pool);
-        CHECK(is_int32(imm64 + 0x800));
-        int32_t Hi20 = (((int32_t)imm64 + 0x800) >> 12);
-        int32_t Lo12 = (int32_t)imm64 << 20 >> 20;
-        auipc(t6, Hi20);  // Read PC + Hi20 into t6
-        jr(t6, Lo12);     // jump PC + Hi20 + Lo12
-      }
-      // If unbound_labels_count_ is big enough, label after_pool will
-      // need a trampoline too, so we must create the trampoline before
-      // the bind operation to make sure function 'bind' can get this
-      // information.
-      trampoline_ = Trampoline(pool_start, unbound_labels_count_);
-      bind(&after_pool);
-
-      trampoline_emitted_ = true;
-      // As we are only going to emit trampoline once, we need to prevent any
-      // further emission.
-      next_buffer_check_ = kMaxInt;
-    }
-  } else {
-    // Number of branches to unbound label at this point is zero, so we can
-    // move next buffer check to maximum.
-    next_buffer_check_ =
-        pc_offset() + kMaxBranchOffset - kTrampolineSlotsSize * 16;
-  }
-  return;
-}
-
-void Assembler::set_target_address_at(Address pc, Address constant_pool,
-                                      Address target,
-                                      ICacheFlushMode icache_flush_mode) {
-  Instr* instr = reinterpret_cast<Instr*>(pc);
-  if (IsAuipc(*instr)) {
-    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
-      int32_t Hi20 = AuipcOffset(*instr);
-      int32_t Lo12 = LdOffset(*reinterpret_cast<Instr*>(pc + 4));
-      Memory<Address>(pc + Hi20 + Lo12) = target;
-      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
-        FlushInstructionCache(pc + Hi20 + Lo12, 2 * kInstrSize);
-      }
-    } else {
-      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
-      int64_t imm = (int64_t)target - (int64_t)pc;
-      Instr instr = instr_at(pc);
-      Instr instr1 = instr_at(pc + 1 * kInstrSize);
-      DCHECK(is_int32(imm + 0x800));
-      int num = PatchBranchlongOffset(pc, instr, instr1, (int32_t)imm);
-      if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
-        FlushInstructionCache(pc, num * kInstrSize);
-      }
-    }
-  } else {
-    set_target_address_at(pc, target, icache_flush_mode);
-  }
-}
-
-Address Assembler::target_address_at(Address pc, Address constant_pool) {
-  Instr* instr = reinterpret_cast<Instr*>(pc);
-  if (IsAuipc(*instr)) {
-    if (IsLd(*reinterpret_cast<Instr*>(pc + 4))) {
-      int32_t Hi20 = AuipcOffset(*instr);
-      int32_t Lo12 = LdOffset(*reinterpret_cast<Instr*>(pc + 4));
-      return Memory<Address>(pc + Hi20 + Lo12);
-    } else {
-      DCHECK(IsJalr(*reinterpret_cast<Instr*>(pc + 4)));
-      int32_t Hi20 = AuipcOffset(*instr);
-      int32_t Lo12 = JalrOffset(*reinterpret_cast<Instr*>(pc + 4));
-      return pc + Hi20 + Lo12;
-    }
-
-  } else {
-    return target_address_at(pc);
-  }
-}
-Address Assembler::target_address_at(Address pc) {
-  DEBUG_PRINTF("target_address_at: pc: %lx\t", pc);
-  Instruction* instr0 = Instruction::At((unsigned char*)pc);
-  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
-  Instruction* instr2 = Instruction::At((unsigned char*)(pc + 2 * kInstrSize));
-  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
-  Instruction* instr4 = Instruction::At((unsigned char*)(pc + 4 * kInstrSize));
-  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));
-
-  // Interpret instructions for address generated by li: See listing in
-  // Assembler::set_target_address_at() just below.
-  if (IsLui(*reinterpret_cast<Instr*>(instr0)) &&
-      IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
-      IsSlli(*reinterpret_cast<Instr*>(instr2)) &&
-      IsOri(*reinterpret_cast<Instr*>(instr3)) &&
-      IsSlli(*reinterpret_cast<Instr*>(instr4)) &&
-      IsOri(*reinterpret_cast<Instr*>(instr5))) {
-    // Assemble the 64 bit value.
-    int64_t addr = (int64_t)(instr0->Imm20UValue() << kImm20Shift) +
-                   (int64_t)instr1->Imm12Value();
-    addr <<= 11;
-    addr |= (int64_t)instr3->Imm12Value();
-    addr <<= 6;
-    addr |= (int64_t)instr5->Imm12Value();
-
-    DEBUG_PRINTF("addr: %lx\n", addr);
-    return static_cast<Address>(addr);
-  }
-  // We should never get here, force a bad address if we do.
-  UNREACHABLE();
-}
-// On RISC-V, a 48-bit target address is stored in an 6-instruction sequence:
-//  lui(reg, (int32_t)high_20); // 19 high bits
-//  addi(reg, reg, low_12); // 12 following bits. total is 31 high bits in reg.
-//  slli(reg, reg, 11); // Space for next 11 bits
-//  ori(reg, reg, b11); // 11 bits are put in. 42 bit in reg
-//  slli(reg, reg, 6); // Space for next 6 bits
-//  ori(reg, reg, a6); // 6 bits are put in. all 48 bis in reg
-//
-// Patching the address must replace all instructions, and flush the i-cache.
-// Note that this assumes the use of SV48, the 48-bit virtual memory system.
-void Assembler::set_target_value_at(Address pc, uint64_t target,
-                                    ICacheFlushMode icache_flush_mode) {
-  DEBUG_PRINTF("set_target_value_at: pc: %lx\ttarget: %lx\n", pc, target);
-  uint32_t* p = reinterpret_cast<uint32_t*>(pc);
-  DCHECK_EQ((target & 0xffff000000000000ll), 0);
-#ifdef DEBUG
-  // Check we have the result from a li macro-instruction.
-  Instruction* instr0 = Instruction::At((unsigned char*)pc);
-  Instruction* instr1 = Instruction::At((unsigned char*)(pc + 1 * kInstrSize));
-  Instruction* instr3 = Instruction::At((unsigned char*)(pc + 3 * kInstrSize));
-  Instruction* instr5 = Instruction::At((unsigned char*)(pc + 5 * kInstrSize));
-  DCHECK(IsLui(*reinterpret_cast<Instr*>(instr0)) &&
-         IsAddi(*reinterpret_cast<Instr*>(instr1)) &&
-         IsOri(*reinterpret_cast<Instr*>(instr3)) &&
-         IsOri(*reinterpret_cast<Instr*>(instr5)));
-#endif
-  int64_t a6 = target & 0x3f;                     // bits 0:6. 6 bits
-  int64_t b11 = (target >> 6) & 0x7ff;            // bits 6:11. 11 bits
-  int64_t high_31 = (target >> 17) & 0x7fffffff;  // 31 bits
-  int64_t high_20 = ((high_31 + 0x800) >> 12);    // 19 bits
-  int64_t low_12 = high_31 & 0xfff;               // 12 bits
-  *p = *p & 0xfff;
-  *p = *p | ((int32_t)high_20 << 12);
-  *(p + 1) = *(p + 1) & 0xfffff;
-  *(p + 1) = *(p + 1) | ((int32_t)low_12 << 20);
-  *(p + 2) = *(p + 2) & 0xfffff;
-  *(p + 2) = *(p + 2) | (11 << 20);
-  *(p + 3) = *(p + 3) & 0xfffff;
-  *(p + 3) = *(p + 3) | ((int32_t)b11 << 20);
-  *(p + 4) = *(p + 4) & 0xfffff;
-  *(p + 4) = *(p + 4) | (6 << 20);
-  *(p + 5) = *(p + 5) & 0xfffff;
-  *(p + 5) = *(p + 5) | ((int32_t)a6 << 20);
-  if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
-    FlushInstructionCache(pc, 8 * kInstrSize);
-  }
-  DCHECK_EQ(target_address_at(pc), target);
-}
-UseScratchRegisterScope::UseScratchRegisterScope(Assembler* assembler)
-    : available_(assembler->GetScratchRegisterList()),
-      old_available_(*available_) {}
-
-UseScratchRegisterScope::~UseScratchRegisterScope() {
-  *available_ = old_available_;
-}
-
-Register UseScratchRegisterScope::Acquire() {
-  DCHECK_NOT_NULL(available_);
-  DCHECK(!available_->is_empty());
-  int index =
-      static_cast<int>(base::bits::CountTrailingZeros32(available_->bits()));
-  *available_ &= RegList::FromBits(~(1U << index));
-
-  return Register::from_code(index);
-}
-
-bool UseScratchRegisterScope::hasAvailable() const {
-  return !available_->is_empty();
-}
-
-bool Assembler::IsConstantPoolAt(Instruction* instr) {
-  // The constant pool marker is made of two instructions. These instructions
-  // will never be emitted by the JIT, so checking for the first one is enough:
-  // 0: ld x0, x0, #offset
-  Instr instr_value = *reinterpret_cast<Instr*>(instr);
-  bool result = IsLd(instr_value) && (instr->Rs1Value() == kRegCode_zero_reg) &&
-                (instr->RdValue() == kRegCode_zero_reg);
-#ifdef DEBUG
-  // It is still worth asserting the marker is complete.
-  // 1: j 0x0
-  Instruction* instr_following = instr + kInstrSize;
-  DCHECK(!result || (IsJal(*reinterpret_cast<Instr*>(instr_following)) &&
-                     instr_following->Imm20JValue() == 0 &&
-                     instr_following->RdValue() == kRegCode_zero_reg));
-#endif
-  return result;
-}
-
-int Assembler::ConstantPoolSizeAt(Instruction* instr) {
-  if (IsConstantPoolAt(instr)) {
-    return instr->Imm12Value();
-  } else {
-    return -1;
-  }
-}
-
-void Assembler::RecordConstPool(int size) {
-  // We only need this for debugger support, to correctly compute offsets in the
-  // code.
-  Assembler::BlockPoolsScope block_pools(this);
-  RecordRelocInfo(RelocInfo::CONST_POOL, static_cast<intptr_t>(size));
-}
-
-void Assembler::EmitPoolGuard() {
-  // We must generate only one instruction as this is used in scopes that
-  // control the size of the code generated.
-  j(0);
-}
-
-// Constant Pool
-
-void ConstantPool::EmitPrologue(Alignment require_alignment) {
-  // Recorded constant pool size is expressed in number of 32-bits words,
-  // and includes prologue and alignment, but not the jump around the pool
-  // and the size of the marker itself.
-  const int marker_size = 1;
-  int word_count =
-      ComputeSize(Jump::kOmitted, require_alignment) / kInt32Size - marker_size;
-  assm_->ld(zero_reg, zero_reg, word_count);
-  assm_->EmitPoolGuard();
-}
-
-int ConstantPool::PrologueSize(Jump require_jump) const {
-  // Prologue is:
-  //   j over  ;; if require_jump
-  //   ld x0, x0, #pool_size
-  //   j 0x0
-  int prologue_size = require_jump == Jump::kRequired ? kInstrSize : 0;
-  prologue_size += 2 * kInstrSize;
-  return prologue_size;
-}
-
-void ConstantPool::SetLoadOffsetToConstPoolEntry(int load_offset,
-                                                 Instruction* entry_offset,
-                                                 const ConstantPoolKey& key) {
-  Instr instr_auipc = assm_->instr_at(load_offset);
-  Instr instr_ld = assm_->instr_at(load_offset + 4);
-  // Instruction to patch must be 'ld rd, offset(rd)' with 'offset == 0'.
-  DCHECK(assm_->IsAuipc(instr_auipc));
-  DCHECK(assm_->IsLd(instr_ld));
-  DCHECK_EQ(assm_->LdOffset(instr_ld), 0);
-  DCHECK_EQ(assm_->AuipcOffset(instr_auipc), 0);
-  int32_t distance = static_cast<int32_t>(
-      reinterpret_cast<Address>(entry_offset) -
-      reinterpret_cast<Address>(assm_->toAddress(load_offset)));
-  CHECK(is_int32(distance + 0x800));
-  int32_t Hi20 = (((int32_t)distance + 0x800) >> 12);
-  int32_t Lo12 = (int32_t)distance << 20 >> 20;
-  assm_->instr_at_put(load_offset, SetAuipcOffset(Hi20, instr_auipc));
-  assm_->instr_at_put(load_offset + 4, SetLdOffset(Lo12, instr_ld));
-}
-
-void ConstantPool::Check(Emission force_emit, Jump require_jump,
-                         size_t margin) {
-  // Some short sequence of instruction must not be broken up by constant pool
-  // emission, such sequences are protected by a ConstPool::BlockScope.
-  if (IsBlocked()) {
-    // Something is wrong if emission is forced and blocked at the same time.
-    DCHECK_EQ(force_emit, Emission::kIfNeeded);
-    return;
-  }
-
-  // We emit a constant pool only if :
-  //  * it is not empty
-  //  * emission is forced by parameter force_emit (e.g. at function end).
-  //  * emission is mandatory or opportune according to {ShouldEmitNow}.
-  if (!IsEmpty() && (force_emit == Emission::kForced ||
-                     ShouldEmitNow(require_jump, margin))) {
-    // Emit veneers for branches that would go out of range during emission of
-    // the constant pool.
-    int worst_case_size = ComputeSize(Jump::kRequired, Alignment::kRequired);
-
-    // Check that the code buffer is large enough before emitting the constant
-    // pool (this includes the gap to the relocation information).
-    int needed_space = worst_case_size + assm_->kGap;
-    while (assm_->buffer_space() <= needed_space) {
-      assm_->GrowBuffer();
-    }
-
-    EmitAndClear(require_jump);
-  }
-  // Since a constant pool is (now) empty, move the check offset forward by
-  // the standard interval.
-  SetNextCheckIn(ConstantPool::kCheckInterval);
-}
-
-LoadStoreLaneParams::LoadStoreLaneParams(MachineRepresentation rep,
-                                         uint8_t laneidx) {
-  switch (rep) {
-    case MachineRepresentation::kWord8:
-      *this = LoadStoreLaneParams(laneidx, 8, kRvvVLEN / 16);
-      break;
-    case MachineRepresentation::kWord16:
-      *this = LoadStoreLaneParams(laneidx, 16, kRvvVLEN / 8);
-      break;
-    case MachineRepresentation::kWord32:
-      *this = LoadStoreLaneParams(laneidx, 32, kRvvVLEN / 4);
-      break;
-    case MachineRepresentation::kWord64:
-      *this = LoadStoreLaneParams(laneidx, 64, kRvvVLEN / 2);
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-
-// Pool entries are accessed with pc relative load therefore this cannot be more
-// than 1 * MB. Since constant pool emission checks are interval based, and we
-// want to keep entries close to the code, we try to emit every 64KB.
-const size_t ConstantPool::kMaxDistToPool32 = 1 * MB;
-const size_t ConstantPool::kMaxDistToPool64 = 1 * MB;
-const size_t ConstantPool::kCheckInterval = 128 * kInstrSize;
-const size_t ConstantPool::kApproxDistToPool32 = 64 * KB;
-const size_t ConstantPool::kApproxDistToPool64 = kApproxDistToPool32;
-
-const size_t ConstantPool::kOpportunityDistToPool32 = 64 * KB;
-const size_t ConstantPool::kOpportunityDistToPool64 = 64 * KB;
-const size_t ConstantPool::kApproxMaxEntryCount = 512;
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/codegen/riscv64/assembler-riscv64.h b/src/codegen/riscv64/assembler-riscv64.h
deleted file mode 100644
index ad456f545f2..00000000000
--- a/src/codegen/riscv64/assembler-riscv64.h
+++ /dev/null
@@ -1,1830 +0,0 @@
-// Copyright (c) 1994-2006 Sun Microsystems Inc.
-// All Rights Reserved.
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are
-// met:
-//
-// - Redistributions of source code must retain the above copyright notice,
-// this list of conditions and the following disclaimer.
-//
-// - Redistribution in binary form must reproduce the above copyright
-// notice, this list of conditions and the following disclaimer in the
-// documentation and/or other materials provided with the distribution.
-//
-// - Neither the name of Sun Microsystems or the names of contributors may
-// be used to endorse or promote products derived from this software without
-// specific prior written permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
-// IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
-// THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
-// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
-// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
-// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
-// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-
-// The original source code covered by the above license above has been
-// modified significantly by Google Inc.
-// Copyright 2021 the V8 project authors. All rights reserved.
-
-#ifndef V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_H_
-#define V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_H_
-
-#include <stdio.h>
-
-#include <memory>
-#include <set>
-
-#include "src/codegen/assembler.h"
-#include "src/codegen/constant-pool.h"
-#include "src/codegen/external-reference.h"
-#include "src/codegen/flush-instruction-cache.h"
-#include "src/codegen/label.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
-#include "src/codegen/riscv64/register-riscv64.h"
-#include "src/objects/contexts.h"
-#include "src/objects/smi.h"
-
-namespace v8 {
-namespace internal {
-
-#define DEBUG_PRINTF(...) \
-  if (FLAG_riscv_debug) { \
-    printf(__VA_ARGS__);  \
-  }
-
-class SafepointTableBuilder;
-
-// -----------------------------------------------------------------------------
-// Machine instruction Operands.
-constexpr int kSmiShift = kSmiTagSize + kSmiShiftSize;
-constexpr uint64_t kSmiShiftMask = (1UL << kSmiShift) - 1;
-// Class Operand represents a shifter operand in data processing instructions.
-class Operand {
- public:
-  // Immediate.
-  V8_INLINE explicit Operand(int64_t immediate,
-                             RelocInfo::Mode rmode = RelocInfo::NO_INFO)
-      : rm_(no_reg), rmode_(rmode) {
-    value_.immediate = immediate;
-  }
-  V8_INLINE explicit Operand(const ExternalReference& f)
-      : rm_(no_reg), rmode_(RelocInfo::EXTERNAL_REFERENCE) {
-    value_.immediate = static_cast<int64_t>(f.address());
-  }
-  V8_INLINE explicit Operand(const char* s);
-  explicit Operand(Handle<HeapObject> handle);
-  V8_INLINE explicit Operand(Smi value)
-      : rm_(no_reg), rmode_(RelocInfo::NO_INFO) {
-    value_.immediate = static_cast<intptr_t>(value.ptr());
-  }
-
-  static Operand EmbeddedNumber(double number);  // Smi or HeapNumber.
-  static Operand EmbeddedStringConstant(const StringConstantBase* str);
-
-  // Register.
-  V8_INLINE explicit Operand(Register rm) : rm_(rm) {}
-
-  // Return true if this is a register operand.
-  V8_INLINE bool is_reg() const;
-
-  inline int64_t immediate() const;
-
-  bool IsImmediate() const { return !rm_.is_valid(); }
-
-  HeapObjectRequest heap_object_request() const {
-    DCHECK(IsHeapObjectRequest());
-    return value_.heap_object_request;
-  }
-
-  bool IsHeapObjectRequest() const {
-    DCHECK_IMPLIES(is_heap_object_request_, IsImmediate());
-    DCHECK_IMPLIES(is_heap_object_request_,
-                   rmode_ == RelocInfo::FULL_EMBEDDED_OBJECT ||
-                       rmode_ == RelocInfo::CODE_TARGET);
-    return is_heap_object_request_;
-  }
-
-  Register rm() const { return rm_; }
-
-  RelocInfo::Mode rmode() const { return rmode_; }
-
- private:
-  Register rm_;
-  union Value {
-    Value() {}
-    HeapObjectRequest heap_object_request;  // if is_heap_object_request_
-    int64_t immediate;                      // otherwise
-  } value_;                                 // valid if rm_ == no_reg
-  bool is_heap_object_request_ = false;
-  RelocInfo::Mode rmode_;
-
-  friend class Assembler;
-  friend class MacroAssembler;
-};
-
-// On RISC-V we have only one addressing mode with base_reg + offset.
-// Class MemOperand represents a memory operand in load and store instructions.
-class V8_EXPORT_PRIVATE MemOperand : public Operand {
- public:
-  // Immediate value attached to offset.
-  enum OffsetAddend { offset_minus_one = -1, offset_zero = 0 };
-
-  explicit MemOperand(Register rn, int32_t offset = 0);
-  explicit MemOperand(Register rn, int32_t unit, int32_t multiplier,
-                      OffsetAddend offset_addend = offset_zero);
-  int32_t offset() const { return offset_; }
-
-  bool OffsetIsInt12Encodable() const { return is_int12(offset_); }
-
- private:
-  int32_t offset_;
-
-  friend class Assembler;
-};
-
-class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
- public:
-  // Create an assembler. Instructions and relocation information are emitted
-  // into a buffer, with the instructions starting from the beginning and the
-  // relocation information starting from the end of the buffer. See CodeDesc
-  // for a detailed comment on the layout (globals.h).
-  //
-  // If the provided buffer is nullptr, the assembler allocates and grows its
-  // own buffer. Otherwise it takes ownership of the provided buffer.
-  explicit Assembler(const AssemblerOptions&,
-                     std::unique_ptr<AssemblerBuffer> = {});
-
-  virtual ~Assembler();
-  void AbortedCodeGeneration();
-  // GetCode emits any pending (non-emitted) code and fills the descriptor desc.
-  static constexpr int kNoHandlerTable = 0;
-  static constexpr SafepointTableBuilder* kNoSafepointTable = nullptr;
-  void GetCode(Isolate* isolate, CodeDesc* desc,
-               SafepointTableBuilder* safepoint_table_builder,
-               int handler_table_offset);
-
-  // Convenience wrapper for code without safepoint or handler tables.
-  void GetCode(Isolate* isolate, CodeDesc* desc) {
-    GetCode(isolate, desc, kNoSafepointTable, kNoHandlerTable);
-  }
-
-  // Unused on this architecture.
-  void MaybeEmitOutOfLineConstantPool() {}
-
-  // Label operations & relative jumps (PPUM Appendix D).
-  //
-  // Takes a branch opcode (cc) and a label (L) and generates
-  // either a backward branch or a forward branch and links it
-  // to the label fixup chain. Usage:
-  //
-  // Label L;    // unbound label
-  // j(cc, &L);  // forward branch to unbound label
-  // bind(&L);   // bind label to the current pc
-  // j(cc, &L);  // backward branch to bound label
-  // bind(&L);   // illegal: a label may be bound only once
-  //
-  // Note: The same Label can be used for forward and backward branches
-  // but it may be bound only once.
-  void bind(Label* L);  // Binds an unbound label L to current code position.
-
-  enum OffsetSize : int {
-    kOffset21 = 21,  // RISCV jal
-    kOffset12 = 12,  // RISCV imm12
-    kOffset20 = 20,  // RISCV imm20
-    kOffset13 = 13,  // RISCV branch
-    kOffset32 = 32,  // RISCV auipc + instr_I
-    kOffset11 = 11,  // RISCV C_J
-    kOffset8 = 8     // RISCV compressed branch
-  };
-
-  // Determines if Label is bound and near enough so that branch instruction
-  // can be used to reach it, instead of jump instruction.
-  bool is_near(Label* L);
-  bool is_near(Label* L, OffsetSize bits);
-  bool is_near_branch(Label* L);
-
-  // Get offset from instr.
-  int BranchOffset(Instr instr);
-  static int BrachlongOffset(Instr auipc, Instr jalr);
-  static int PatchBranchlongOffset(Address pc, Instr auipc, Instr instr_I,
-                                   int32_t offset);
-  int JumpOffset(Instr instr);
-  int CJumpOffset(Instr instr);
-  int CBranchOffset(Instr instr);
-  static int LdOffset(Instr instr);
-  static int AuipcOffset(Instr instr);
-  static int JalrOffset(Instr instr);
-
-  // Returns the branch offset to the given label from the current code
-  // position. Links the label to the current position if it is still unbound.
-  // Manages the jump elimination optimization if the second parameter is true.
-  int32_t branch_offset_helper(Label* L, OffsetSize bits);
-  inline int32_t branch_offset(Label* L) {
-    return branch_offset_helper(L, OffsetSize::kOffset13);
-  }
-  inline int32_t jump_offset(Label* L) {
-    return branch_offset_helper(L, OffsetSize::kOffset21);
-  }
-  inline int16_t cjump_offset(Label* L) {
-    return (int16_t)branch_offset_helper(L, OffsetSize::kOffset11);
-  }
-  inline int32_t cbranch_offset(Label* L) {
-    return branch_offset_helper(L, OffsetSize::kOffset8);
-  }
-
-  uint64_t jump_address(Label* L);
-  uint64_t branch_long_offset(Label* L);
-
-  // Puts a labels target address at the given position.
-  // The high 8 bits are set to zero.
-  void label_at_put(Label* L, int at_offset);
-
-  // Read/Modify the code target address in the branch/call instruction at pc.
-  // The isolate argument is unused (and may be nullptr) when skipping flushing.
-  static Address target_address_at(Address pc);
-  V8_INLINE static void set_target_address_at(
-      Address pc, Address target,
-      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED) {
-    set_target_value_at(pc, target, icache_flush_mode);
-  }
-
-  static Address target_address_at(Address pc, Address constant_pool);
-
-  static void set_target_address_at(
-      Address pc, Address constant_pool, Address target,
-      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
-
-  // Read/Modify the code target address in the branch/call instruction at pc.
-  inline static Tagged_t target_compressed_address_at(Address pc,
-                                                      Address constant_pool);
-  inline static void set_target_compressed_address_at(
-      Address pc, Address constant_pool, Tagged_t target,
-      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
-
-  inline Handle<Object> code_target_object_handle_at(Address pc,
-                                                     Address constant_pool);
-  inline Handle<HeapObject> compressed_embedded_object_handle_at(
-      Address pc, Address constant_pool);
-
-  static bool IsConstantPoolAt(Instruction* instr);
-  static int ConstantPoolSizeAt(Instruction* instr);
-  // See Assembler::CheckConstPool for more info.
-  void EmitPoolGuard();
-
-  static void set_target_value_at(
-      Address pc, uint64_t target,
-      ICacheFlushMode icache_flush_mode = FLUSH_ICACHE_IF_NEEDED);
-
-  static void JumpLabelToJumpRegister(Address pc);
-
-  // This sets the branch destination (which gets loaded at the call address).
-  // This is for calls and branches within generated code.  The serializer
-  // has already deserialized the lui/ori instructions etc.
-  inline static void deserialization_set_special_target_at(
-      Address instruction_payload, Code code, Address target);
-
-  // Get the size of the special target encoded at 'instruction_payload'.
-  inline static int deserialization_special_target_size(
-      Address instruction_payload);
-
-  // This sets the internal reference at the pc.
-  inline static void deserialization_set_target_internal_reference_at(
-      Address pc, Address target,
-      RelocInfo::Mode mode = RelocInfo::INTERNAL_REFERENCE);
-
-  // Difference between address of current opcode and target address offset.
-  static constexpr int kBranchPCOffset = kInstrSize;
-
-  // Difference between address of current opcode and target address offset,
-  // when we are generatinga sequence of instructions for long relative PC
-  // branches
-  static constexpr int kLongBranchPCOffset = 3 * kInstrSize;
-
-  // Adjust ra register in branch delay slot of bal instruction so to skip
-  // instructions not needed after optimization of PIC in
-  // TurboAssembler::BranchAndLink method.
-
-  static constexpr int kOptimizedBranchAndLinkLongReturnOffset = 4 * kInstrSize;
-
-  // Here we are patching the address in the LUI/ADDI instruction pair.
-  // These values are used in the serialization process and must be zero for
-  // RISC-V platform, as Code, Embedded Object or External-reference pointers
-  // are split across two consecutive instructions and don't exist separately
-  // in the code, so the serializer should not step forwards in memory after
-  // a target is resolved and written.
-  static constexpr int kSpecialTargetSize = 0;
-
-  // Number of consecutive instructions used to store 32bit/64bit constant.
-  // This constant was used in RelocInfo::target_address_address() function
-  // to tell serializer address of the instruction that follows
-  // LUI/ADDI instruction pair.
-  static constexpr int kInstructionsFor32BitConstant = 2;
-  static constexpr int kInstructionsFor64BitConstant = 8;
-
-  // Difference between address of current opcode and value read from pc
-  // register.
-  static constexpr int kPcLoadDelta = 4;
-
-  // Bits available for offset field in branches
-  static constexpr int kBranchOffsetBits = 13;
-
-  // Bits available for offset field in jump
-  static constexpr int kJumpOffsetBits = 21;
-
-  // Bits available for offset field in compresed jump
-  static constexpr int kCJalOffsetBits = 12;
-
-  // Bits available for offset field in compressed branch
-  static constexpr int kCBranchOffsetBits = 9;
-
-  // Max offset for b instructions with 12-bit offset field (multiple of 2)
-  static constexpr int kMaxBranchOffset = (1 << (13 - 1)) - 1;
-
-  // Max offset for jal instruction with 20-bit offset field (multiple of 2)
-  static constexpr int kMaxJumpOffset = (1 << (21 - 1)) - 1;
-
-  static constexpr int kTrampolineSlotsSize = 2 * kInstrSize;
-
-  RegList* GetScratchRegisterList() { return &scratch_register_list_; }
-
-  // ---------------------------------------------------------------------------
-  // Code generation.
-
-  // Insert the smallest number of nop instructions
-  // possible to align the pc offset to a multiple
-  // of m. m must be a power of 2 (>= 4).
-  void Align(int m);
-  // Insert the smallest number of zero bytes possible to align the pc offset
-  // to a mulitple of m. m must be a power of 2 (>= 2).
-  void DataAlign(int m);
-  // Aligns code to something that's optimal for a jump target for the platform.
-  void CodeTargetAlign();
-  void LoopHeaderAlign() { CodeTargetAlign(); }
-
-  // Different nop operations are used by the code generator to detect certain
-  // states of the generated code.
-  enum NopMarkerTypes {
-    NON_MARKING_NOP = 0,
-    DEBUG_BREAK_NOP,
-    // IC markers.
-    PROPERTY_ACCESS_INLINED,
-    PROPERTY_ACCESS_INLINED_CONTEXT,
-    PROPERTY_ACCESS_INLINED_CONTEXT_DONT_DELETE,
-    // Helper values.
-    LAST_CODE_MARKER,
-    FIRST_IC_MARKER = PROPERTY_ACCESS_INLINED,
-  };
-
-  // RISC-V Instructions Emited to a buffer
-
-  void lui(Register rd, int32_t imm20);
-  void auipc(Register rd, int32_t imm20);
-
-  // Jumps
-  void jal(Register rd, int32_t imm20);
-  void jalr(Register rd, Register rs1, int16_t imm12);
-
-  // Branches
-  void beq(Register rs1, Register rs2, int16_t imm12);
-  inline void beq(Register rs1, Register rs2, Label* L) {
-    beq(rs1, rs2, branch_offset(L));
-  }
-  void bne(Register rs1, Register rs2, int16_t imm12);
-  inline void bne(Register rs1, Register rs2, Label* L) {
-    bne(rs1, rs2, branch_offset(L));
-  }
-  void blt(Register rs1, Register rs2, int16_t imm12);
-  inline void blt(Register rs1, Register rs2, Label* L) {
-    blt(rs1, rs2, branch_offset(L));
-  }
-  void bge(Register rs1, Register rs2, int16_t imm12);
-  inline void bge(Register rs1, Register rs2, Label* L) {
-    bge(rs1, rs2, branch_offset(L));
-  }
-  void bltu(Register rs1, Register rs2, int16_t imm12);
-  inline void bltu(Register rs1, Register rs2, Label* L) {
-    bltu(rs1, rs2, branch_offset(L));
-  }
-  void bgeu(Register rs1, Register rs2, int16_t imm12);
-  inline void bgeu(Register rs1, Register rs2, Label* L) {
-    bgeu(rs1, rs2, branch_offset(L));
-  }
-
-  // Loads
-  void lb(Register rd, Register rs1, int16_t imm12);
-  void lh(Register rd, Register rs1, int16_t imm12);
-  void lw(Register rd, Register rs1, int16_t imm12);
-  void lbu(Register rd, Register rs1, int16_t imm12);
-  void lhu(Register rd, Register rs1, int16_t imm12);
-
-  // Stores
-  void sb(Register source, Register base, int16_t imm12);
-  void sh(Register source, Register base, int16_t imm12);
-  void sw(Register source, Register base, int16_t imm12);
-
-  // Arithmetic with immediate
-  void addi(Register rd, Register rs1, int16_t imm12);
-  void slti(Register rd, Register rs1, int16_t imm12);
-  void sltiu(Register rd, Register rs1, int16_t imm12);
-  void xori(Register rd, Register rs1, int16_t imm12);
-  void ori(Register rd, Register rs1, int16_t imm12);
-  void andi(Register rd, Register rs1, int16_t imm12);
-  void slli(Register rd, Register rs1, uint8_t shamt);
-  void srli(Register rd, Register rs1, uint8_t shamt);
-  void srai(Register rd, Register rs1, uint8_t shamt);
-
-  // Arithmetic
-  void add(Register rd, Register rs1, Register rs2);
-  void sub(Register rd, Register rs1, Register rs2);
-  void sll(Register rd, Register rs1, Register rs2);
-  void slt(Register rd, Register rs1, Register rs2);
-  void sltu(Register rd, Register rs1, Register rs2);
-  void xor_(Register rd, Register rs1, Register rs2);
-  void srl(Register rd, Register rs1, Register rs2);
-  void sra(Register rd, Register rs1, Register rs2);
-  void or_(Register rd, Register rs1, Register rs2);
-  void and_(Register rd, Register rs1, Register rs2);
-
-  // Memory fences
-  void fence(uint8_t pred, uint8_t succ);
-  void fence_tso();
-
-  // Environment call / break
-  void ecall();
-  void ebreak();
-
-  // This is a de facto standard (as set by GNU binutils) 32-bit unimplemented
-  // instruction (i.e., it should always trap, if your implementation has
-  // invalid instruction traps).
-  void unimp();
-
-  // CSR
-  void csrrw(Register rd, ControlStatusReg csr, Register rs1);
-  void csrrs(Register rd, ControlStatusReg csr, Register rs1);
-  void csrrc(Register rd, ControlStatusReg csr, Register rs1);
-  void csrrwi(Register rd, ControlStatusReg csr, uint8_t imm5);
-  void csrrsi(Register rd, ControlStatusReg csr, uint8_t imm5);
-  void csrrci(Register rd, ControlStatusReg csr, uint8_t imm5);
-
-  // RV64I
-  void lwu(Register rd, Register rs1, int16_t imm12);
-  void ld(Register rd, Register rs1, int16_t imm12);
-  void sd(Register source, Register base, int16_t imm12);
-  void addiw(Register rd, Register rs1, int16_t imm12);
-  void slliw(Register rd, Register rs1, uint8_t shamt);
-  void srliw(Register rd, Register rs1, uint8_t shamt);
-  void sraiw(Register rd, Register rs1, uint8_t shamt);
-  void addw(Register rd, Register rs1, Register rs2);
-  void subw(Register rd, Register rs1, Register rs2);
-  void sllw(Register rd, Register rs1, Register rs2);
-  void srlw(Register rd, Register rs1, Register rs2);
-  void sraw(Register rd, Register rs1, Register rs2);
-
-  // RV32M Standard Extension
-  void mul(Register rd, Register rs1, Register rs2);
-  void mulh(Register rd, Register rs1, Register rs2);
-  void mulhsu(Register rd, Register rs1, Register rs2);
-  void mulhu(Register rd, Register rs1, Register rs2);
-  void div(Register rd, Register rs1, Register rs2);
-  void divu(Register rd, Register rs1, Register rs2);
-  void rem(Register rd, Register rs1, Register rs2);
-  void remu(Register rd, Register rs1, Register rs2);
-
-  // RV64M Standard Extension (in addition to RV32M)
-  void mulw(Register rd, Register rs1, Register rs2);
-  void divw(Register rd, Register rs1, Register rs2);
-  void divuw(Register rd, Register rs1, Register rs2);
-  void remw(Register rd, Register rs1, Register rs2);
-  void remuw(Register rd, Register rs1, Register rs2);
-
-  // RV32A Standard Extension
-  void lr_w(bool aq, bool rl, Register rd, Register rs1);
-  void sc_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoswap_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoadd_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoxor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoand_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoor_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomin_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomax_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amominu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomaxu_w(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-
-  // RV64A Standard Extension (in addition to RV32A)
-  void lr_d(bool aq, bool rl, Register rd, Register rs1);
-  void sc_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoswap_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoadd_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoxor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoand_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amoor_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomin_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomax_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amominu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-  void amomaxu_d(bool aq, bool rl, Register rd, Register rs1, Register rs2);
-
-  // RV32F Standard Extension
-  void flw(FPURegister rd, Register rs1, int16_t imm12);
-  void fsw(FPURegister source, Register base, int16_t imm12);
-  void fmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-               FPURegister rs3, FPURoundingMode frm = RNE);
-  void fmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-               FPURegister rs3, FPURoundingMode frm = RNE);
-  void fnmsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                FPURegister rs3, FPURoundingMode frm = RNE);
-  void fnmadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                FPURegister rs3, FPURoundingMode frm = RNE);
-  void fadd_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fsub_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fmul_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fdiv_s(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fsqrt_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fsgnj_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fsgnjn_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fsgnjx_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fmin_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fmax_s(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fcvt_w_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_wu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fmv_x_w(Register rd, FPURegister rs1);
-  void feq_s(Register rd, FPURegister rs1, FPURegister rs2);
-  void flt_s(Register rd, FPURegister rs1, FPURegister rs2);
-  void fle_s(Register rd, FPURegister rs1, FPURegister rs2);
-  void fclass_s(Register rd, FPURegister rs1);
-  void fcvt_s_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fcvt_s_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fmv_w_x(FPURegister rd, Register rs1);
-
-  // RV64F Standard Extension (in addition to RV32F)
-  void fcvt_l_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_lu_s(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_s_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fcvt_s_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-
-  // RV32D Standard Extension
-  void fld(FPURegister rd, Register rs1, int16_t imm12);
-  void fsd(FPURegister source, Register base, int16_t imm12);
-  void fmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-               FPURegister rs3, FPURoundingMode frm = RNE);
-  void fmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-               FPURegister rs3, FPURoundingMode frm = RNE);
-  void fnmsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                FPURegister rs3, FPURoundingMode frm = RNE);
-  void fnmadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-                FPURegister rs3, FPURoundingMode frm = RNE);
-  void fadd_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fsub_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fmul_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fdiv_d(FPURegister rd, FPURegister rs1, FPURegister rs2,
-              FPURoundingMode frm = RNE);
-  void fsqrt_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fsgnj_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fsgnjn_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fsgnjx_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fmin_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fmax_d(FPURegister rd, FPURegister rs1, FPURegister rs2);
-  void fcvt_s_d(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_d_s(FPURegister rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void feq_d(Register rd, FPURegister rs1, FPURegister rs2);
-  void flt_d(Register rd, FPURegister rs1, FPURegister rs2);
-  void fle_d(Register rd, FPURegister rs1, FPURegister rs2);
-  void fclass_d(Register rd, FPURegister rs1);
-  void fcvt_w_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_wu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_d_w(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fcvt_d_wu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-
-  // RV64D Standard Extension (in addition to RV32D)
-  void fcvt_l_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fcvt_lu_d(Register rd, FPURegister rs1, FPURoundingMode frm = RNE);
-  void fmv_x_d(Register rd, FPURegister rs1);
-  void fcvt_d_l(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fcvt_d_lu(FPURegister rd, Register rs1, FPURoundingMode frm = RNE);
-  void fmv_d_x(FPURegister rd, Register rs1);
-
-  // RV64C Standard Extension
-  void c_nop();
-  void c_addi(Register rd, int8_t imm6);
-  void c_addiw(Register rd, int8_t imm6);
-  void c_addi16sp(int16_t imm10);
-  void c_addi4spn(Register rd, int16_t uimm10);
-  void c_li(Register rd, int8_t imm6);
-  void c_lui(Register rd, int8_t imm6);
-  void c_slli(Register rd, uint8_t shamt6);
-  void c_fldsp(FPURegister rd, uint16_t uimm9);
-  void c_lwsp(Register rd, uint16_t uimm8);
-  void c_ldsp(Register rd, uint16_t uimm9);
-  void c_jr(Register rs1);
-  void c_mv(Register rd, Register rs2);
-  void c_ebreak();
-  void c_jalr(Register rs1);
-  void c_j(int16_t imm12);
-  inline void c_j(Label* L) { c_j(cjump_offset(L)); }
-  void c_add(Register rd, Register rs2);
-  void c_sub(Register rd, Register rs2);
-  void c_and(Register rd, Register rs2);
-  void c_xor(Register rd, Register rs2);
-  void c_or(Register rd, Register rs2);
-  void c_subw(Register rd, Register rs2);
-  void c_addw(Register rd, Register rs2);
-  void c_swsp(Register rs2, uint16_t uimm8);
-  void c_sdsp(Register rs2, uint16_t uimm9);
-  void c_fsdsp(FPURegister rs2, uint16_t uimm9);
-  void c_lw(Register rd, Register rs1, uint16_t uimm7);
-  void c_ld(Register rd, Register rs1, uint16_t uimm8);
-  void c_fld(FPURegister rd, Register rs1, uint16_t uimm8);
-  void c_sw(Register rs2, Register rs1, uint16_t uimm7);
-  void c_sd(Register rs2, Register rs1, uint16_t uimm8);
-  void c_fsd(FPURegister rs2, Register rs1, uint16_t uimm8);
-  void c_bnez(Register rs1, int16_t imm9);
-  inline void c_bnez(Register rs1, Label* L) { c_bnez(rs1, branch_offset(L)); }
-  void c_beqz(Register rs1, int16_t imm9);
-  inline void c_beqz(Register rs1, Label* L) { c_beqz(rs1, branch_offset(L)); }
-  void c_srli(Register rs1, int8_t shamt6);
-  void c_srai(Register rs1, int8_t shamt6);
-  void c_andi(Register rs1, int8_t imm6);
-  void NOP();
-  void EBREAK();
-
-  // RVV
-  static int32_t GenZimm(VSew vsew, Vlmul vlmul, TailAgnosticType tail = tu,
-                         MaskAgnosticType mask = mu) {
-    return (mask << 7) | (tail << 6) | ((vsew & 0x7) << 3) | (vlmul & 0x7);
-  }
-
-  void vl(VRegister vd, Register rs1, uint8_t lumop, VSew vsew,
-          MaskType mask = NoMask);
-  void vls(VRegister vd, Register rs1, Register rs2, VSew vsew,
-           MaskType mask = NoMask);
-  void vlx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
-           MaskType mask = NoMask);
-
-  void vs(VRegister vd, Register rs1, uint8_t sumop, VSew vsew,
-          MaskType mask = NoMask);
-  void vss(VRegister vd, Register rs1, Register rs2, VSew vsew,
-           MaskType mask = NoMask);
-  void vsx(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
-           MaskType mask = NoMask);
-
-  void vsu(VRegister vd, Register rs1, VRegister vs3, VSew vsew,
-           MaskType mask = NoMask);
-
-#define SegInstr(OP)  \
-  void OP##seg2(ARG); \
-  void OP##seg3(ARG); \
-  void OP##seg4(ARG); \
-  void OP##seg5(ARG); \
-  void OP##seg6(ARG); \
-  void OP##seg7(ARG); \
-  void OP##seg8(ARG);
-
-#define ARG \
-  VRegister vd, Register rs1, uint8_t lumop, VSew vsew, MaskType mask = NoMask
-
-  SegInstr(vl) SegInstr(vs)
-#undef ARG
-
-#define ARG \
-  VRegister vd, Register rs1, Register rs2, VSew vsew, MaskType mask = NoMask
-
-      SegInstr(vls) SegInstr(vss)
-#undef ARG
-
-#define ARG \
-  VRegister vd, Register rs1, VRegister rs2, VSew vsew, MaskType mask = NoMask
-
-          SegInstr(vsx) SegInstr(vlx)
-#undef ARG
-#undef SegInstr
-
-      // RVV Vector Arithmetic Instruction
-
-      void vmv_vv(VRegister vd, VRegister vs1);
-  void vmv_vx(VRegister vd, Register rs1);
-  void vmv_vi(VRegister vd, uint8_t simm5);
-  void vmv_xs(Register rd, VRegister vs2);
-  void vmv_sx(VRegister vd, Register rs1);
-  void vmerge_vv(VRegister vd, VRegister vs1, VRegister vs2);
-  void vmerge_vx(VRegister vd, Register rs1, VRegister vs2);
-  void vmerge_vi(VRegister vd, uint8_t imm5, VRegister vs2);
-
-  void vredmaxu_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                   MaskType mask = NoMask);
-  void vredmax_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                  MaskType mask = NoMask);
-  void vredmin_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                  MaskType mask = NoMask);
-  void vredminu_vs(VRegister vd, VRegister vs2, VRegister vs1,
-                   MaskType mask = NoMask);
-
-  void vadc_vv(VRegister vd, VRegister vs1, VRegister vs2);
-  void vadc_vx(VRegister vd, Register rs1, VRegister vs2);
-  void vadc_vi(VRegister vd, uint8_t imm5, VRegister vs2);
-
-  void vmadc_vv(VRegister vd, VRegister vs1, VRegister vs2);
-  void vmadc_vx(VRegister vd, Register rs1, VRegister vs2);
-  void vmadc_vi(VRegister vd, uint8_t imm5, VRegister vs2);
-
-  void vfmv_vf(VRegister vd, FPURegister fs1, MaskType mask = NoMask);
-  void vfmv_fs(FPURegister fd, VRegister vs2);
-  void vfmv_sf(VRegister vd, FPURegister fs);
-
-  void vwaddu_wx(VRegister vd, VRegister vs2, Register rs1,
-                 MaskType mask = NoMask);
-  void vid_v(VRegister vd, MaskType mask = Mask);
-
-#define DEFINE_OPIVV(name, funct6)                           \
-  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPIVX(name, funct6)                          \
-  void name##_vx(VRegister vd, VRegister vs2, Register rs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPIVI(name, funct6)                         \
-  void name##_vi(VRegister vd, VRegister vs2, int8_t imm5, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPMVV(name, funct6)                           \
-  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPMVX(name, funct6)                          \
-  void name##_vx(VRegister vd, VRegister vs2, Register rs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFVV(name, funct6)                           \
-  void name##_vv(VRegister vd, VRegister vs2, VRegister vs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFWV(name, funct6)                           \
-  void name##_wv(VRegister vd, VRegister vs2, VRegister vs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFRED(name, funct6)                          \
-  void name##_vs(VRegister vd, VRegister vs2, VRegister vs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFVF(name, funct6)                             \
-  void name##_vf(VRegister vd, VRegister vs2, FPURegister fs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFWF(name, funct6)                             \
-  void name##_wf(VRegister vd, VRegister vs2, FPURegister fs1, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFVV_FMA(name, funct6)                       \
-  void name##_vv(VRegister vd, VRegister vs1, VRegister vs2, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPFVF_FMA(name, funct6)                         \
-  void name##_vf(VRegister vd, FPURegister fs1, VRegister vs2, \
-                 MaskType mask = NoMask);
-
-#define DEFINE_OPMVV_VIE(name) \
-  void name(VRegister vd, VRegister vs2, MaskType mask = NoMask);
-
-  DEFINE_OPIVV(vadd, VADD_FUNCT6)
-  DEFINE_OPIVX(vadd, VADD_FUNCT6)
-  DEFINE_OPIVI(vadd, VADD_FUNCT6)
-  DEFINE_OPIVV(vsub, VSUB_FUNCT6)
-  DEFINE_OPIVX(vsub, VSUB_FUNCT6)
-  DEFINE_OPMVX(vdiv, VDIV_FUNCT6)
-  DEFINE_OPMVX(vdivu, VDIVU_FUNCT6)
-  DEFINE_OPMVX(vmul, VMUL_FUNCT6)
-  DEFINE_OPMVX(vmulhu, VMULHU_FUNCT6)
-  DEFINE_OPMVX(vmulhsu, VMULHSU_FUNCT6)
-  DEFINE_OPMVX(vmulh, VMULH_FUNCT6)
-  DEFINE_OPMVV(vdiv, VDIV_FUNCT6)
-  DEFINE_OPMVV(vdivu, VDIVU_FUNCT6)
-  DEFINE_OPMVV(vmul, VMUL_FUNCT6)
-  DEFINE_OPMVV(vmulhu, VMULHU_FUNCT6)
-  DEFINE_OPMVV(vmulhsu, VMULHSU_FUNCT6)
-  DEFINE_OPMVV(vmulh, VMULH_FUNCT6)
-  DEFINE_OPMVV(vwmul, VWMUL_FUNCT6)
-  DEFINE_OPMVV(vwmulu, VWMULU_FUNCT6)
-  DEFINE_OPMVV(vwaddu, VWADDU_FUNCT6)
-  DEFINE_OPMVV(vwadd, VWADD_FUNCT6)
-  DEFINE_OPMVV(vcompress, VCOMPRESS_FUNCT6)
-  DEFINE_OPIVX(vsadd, VSADD_FUNCT6)
-  DEFINE_OPIVV(vsadd, VSADD_FUNCT6)
-  DEFINE_OPIVI(vsadd, VSADD_FUNCT6)
-  DEFINE_OPIVX(vsaddu, VSADD_FUNCT6)
-  DEFINE_OPIVV(vsaddu, VSADDU_FUNCT6)
-  DEFINE_OPIVI(vsaddu, VSADDU_FUNCT6)
-  DEFINE_OPIVX(vssub, VSSUB_FUNCT6)
-  DEFINE_OPIVV(vssub, VSSUB_FUNCT6)
-  DEFINE_OPIVX(vssubu, VSSUBU_FUNCT6)
-  DEFINE_OPIVV(vssubu, VSSUBU_FUNCT6)
-  DEFINE_OPIVX(vrsub, VRSUB_FUNCT6)
-  DEFINE_OPIVI(vrsub, VRSUB_FUNCT6)
-  DEFINE_OPIVV(vminu, VMINU_FUNCT6)
-  DEFINE_OPIVX(vminu, VMINU_FUNCT6)
-  DEFINE_OPIVV(vmin, VMIN_FUNCT6)
-  DEFINE_OPIVX(vmin, VMIN_FUNCT6)
-  DEFINE_OPIVV(vmaxu, VMAXU_FUNCT6)
-  DEFINE_OPIVX(vmaxu, VMAXU_FUNCT6)
-  DEFINE_OPIVV(vmax, VMAX_FUNCT6)
-  DEFINE_OPIVX(vmax, VMAX_FUNCT6)
-  DEFINE_OPIVV(vand, VAND_FUNCT6)
-  DEFINE_OPIVX(vand, VAND_FUNCT6)
-  DEFINE_OPIVI(vand, VAND_FUNCT6)
-  DEFINE_OPIVV(vor, VOR_FUNCT6)
-  DEFINE_OPIVX(vor, VOR_FUNCT6)
-  DEFINE_OPIVI(vor, VOR_FUNCT6)
-  DEFINE_OPIVV(vxor, VXOR_FUNCT6)
-  DEFINE_OPIVX(vxor, VXOR_FUNCT6)
-  DEFINE_OPIVI(vxor, VXOR_FUNCT6)
-  DEFINE_OPIVV(vrgather, VRGATHER_FUNCT6)
-  DEFINE_OPIVX(vrgather, VRGATHER_FUNCT6)
-  DEFINE_OPIVI(vrgather, VRGATHER_FUNCT6)
-
-  DEFINE_OPIVX(vslidedown, VSLIDEDOWN_FUNCT6)
-  DEFINE_OPIVI(vslidedown, VSLIDEDOWN_FUNCT6)
-  DEFINE_OPIVX(vslideup, VSLIDEUP_FUNCT6)
-  DEFINE_OPIVI(vslideup, VSLIDEUP_FUNCT6)
-
-  DEFINE_OPIVV(vmseq, VMSEQ_FUNCT6)
-  DEFINE_OPIVX(vmseq, VMSEQ_FUNCT6)
-  DEFINE_OPIVI(vmseq, VMSEQ_FUNCT6)
-
-  DEFINE_OPIVV(vmsne, VMSNE_FUNCT6)
-  DEFINE_OPIVX(vmsne, VMSNE_FUNCT6)
-  DEFINE_OPIVI(vmsne, VMSNE_FUNCT6)
-
-  DEFINE_OPIVV(vmsltu, VMSLTU_FUNCT6)
-  DEFINE_OPIVX(vmsltu, VMSLTU_FUNCT6)
-
-  DEFINE_OPIVV(vmslt, VMSLT_FUNCT6)
-  DEFINE_OPIVX(vmslt, VMSLT_FUNCT6)
-
-  DEFINE_OPIVV(vmsle, VMSLE_FUNCT6)
-  DEFINE_OPIVX(vmsle, VMSLE_FUNCT6)
-  DEFINE_OPIVI(vmsle, VMSLE_FUNCT6)
-
-  DEFINE_OPIVV(vmsleu, VMSLEU_FUNCT6)
-  DEFINE_OPIVX(vmsleu, VMSLEU_FUNCT6)
-  DEFINE_OPIVI(vmsleu, VMSLEU_FUNCT6)
-
-  DEFINE_OPIVI(vmsgt, VMSGT_FUNCT6)
-  DEFINE_OPIVX(vmsgt, VMSGT_FUNCT6)
-
-  DEFINE_OPIVI(vmsgtu, VMSGTU_FUNCT6)
-  DEFINE_OPIVX(vmsgtu, VMSGTU_FUNCT6)
-
-  DEFINE_OPIVV(vsrl, VSRL_FUNCT6)
-  DEFINE_OPIVX(vsrl, VSRL_FUNCT6)
-  DEFINE_OPIVI(vsrl, VSRL_FUNCT6)
-
-  DEFINE_OPIVV(vsra, VSRA_FUNCT6)
-  DEFINE_OPIVX(vsra, VSRA_FUNCT6)
-  DEFINE_OPIVI(vsra, VSRA_FUNCT6)
-
-  DEFINE_OPIVV(vsll, VSLL_FUNCT6)
-  DEFINE_OPIVX(vsll, VSLL_FUNCT6)
-  DEFINE_OPIVI(vsll, VSLL_FUNCT6)
-
-  DEFINE_OPIVV(vsmul, VSMUL_FUNCT6)
-  DEFINE_OPIVX(vsmul, VSMUL_FUNCT6)
-
-  DEFINE_OPFVV(vfadd, VFADD_FUNCT6)
-  DEFINE_OPFVF(vfadd, VFADD_FUNCT6)
-  DEFINE_OPFVV(vfsub, VFSUB_FUNCT6)
-  DEFINE_OPFVF(vfsub, VFSUB_FUNCT6)
-  DEFINE_OPFVV(vfdiv, VFDIV_FUNCT6)
-  DEFINE_OPFVF(vfdiv, VFDIV_FUNCT6)
-  DEFINE_OPFVV(vfmul, VFMUL_FUNCT6)
-  DEFINE_OPFVF(vfmul, VFMUL_FUNCT6)
-
-  // Vector Widening Floating-Point Add/Subtract Instructions
-  DEFINE_OPFVV(vfwadd, VFWADD_FUNCT6)
-  DEFINE_OPFVF(vfwadd, VFWADD_FUNCT6)
-  DEFINE_OPFVV(vfwsub, VFWSUB_FUNCT6)
-  DEFINE_OPFVF(vfwsub, VFWSUB_FUNCT6)
-  DEFINE_OPFWV(vfwadd, VFWADD_W_FUNCT6)
-  DEFINE_OPFWF(vfwadd, VFWADD_W_FUNCT6)
-  DEFINE_OPFWV(vfwsub, VFWSUB_W_FUNCT6)
-  DEFINE_OPFWF(vfwsub, VFWSUB_W_FUNCT6)
-
-  // Vector Widening Floating-Point Reduction Instructions
-  DEFINE_OPFVV(vfwredusum, VFWREDUSUM_FUNCT6)
-  DEFINE_OPFVV(vfwredosum, VFWREDOSUM_FUNCT6)
-
-  // Vector Widening Floating-Point Multiply
-  DEFINE_OPFVV(vfwmul, VFWMUL_FUNCT6)
-  DEFINE_OPFVF(vfwmul, VFWMUL_FUNCT6)
-
-  DEFINE_OPFVV(vmfeq, VMFEQ_FUNCT6)
-  DEFINE_OPFVV(vmfne, VMFNE_FUNCT6)
-  DEFINE_OPFVV(vmflt, VMFLT_FUNCT6)
-  DEFINE_OPFVV(vmfle, VMFLE_FUNCT6)
-  DEFINE_OPFVV(vfmax, VMFMAX_FUNCT6)
-  DEFINE_OPFVV(vfmin, VMFMIN_FUNCT6)
-  DEFINE_OPFRED(vfredmax, VFREDMAX_FUNCT6)
-
-  DEFINE_OPFVV(vfsngj, VFSGNJ_FUNCT6)
-  DEFINE_OPFVF(vfsngj, VFSGNJ_FUNCT6)
-  DEFINE_OPFVV(vfsngjn, VFSGNJN_FUNCT6)
-  DEFINE_OPFVF(vfsngjn, VFSGNJN_FUNCT6)
-  DEFINE_OPFVV(vfsngjx, VFSGNJX_FUNCT6)
-  DEFINE_OPFVF(vfsngjx, VFSGNJX_FUNCT6)
-
-  // Vector Single-Width Floating-Point Fused Multiply-Add Instructions
-  DEFINE_OPFVV_FMA(vfmadd, VFMADD_FUNCT6)
-  DEFINE_OPFVF_FMA(vfmadd, VFMADD_FUNCT6)
-  DEFINE_OPFVV_FMA(vfmsub, VFMSUB_FUNCT6)
-  DEFINE_OPFVF_FMA(vfmsub, VFMSUB_FUNCT6)
-  DEFINE_OPFVV_FMA(vfmacc, VFMACC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfmacc, VFMACC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfmsac, VFMSAC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfmsac, VFMSAC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfnmadd, VFNMADD_FUNCT6)
-  DEFINE_OPFVF_FMA(vfnmadd, VFNMADD_FUNCT6)
-  DEFINE_OPFVV_FMA(vfnmsub, VFNMSUB_FUNCT6)
-  DEFINE_OPFVF_FMA(vfnmsub, VFNMSUB_FUNCT6)
-  DEFINE_OPFVV_FMA(vfnmacc, VFNMACC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfnmacc, VFNMACC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfnmsac, VFNMSAC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfnmsac, VFNMSAC_FUNCT6)
-
-  // Vector Widening Floating-Point Fused Multiply-Add Instructions
-  DEFINE_OPFVV_FMA(vfwmacc, VFWMACC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfwmacc, VFWMACC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfwnmacc, VFWNMACC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfwnmacc, VFWNMACC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfwmsac, VFWMSAC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfwmsac, VFWMSAC_FUNCT6)
-  DEFINE_OPFVV_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
-  DEFINE_OPFVF_FMA(vfwnmsac, VFWNMSAC_FUNCT6)
-
-  // Vector Narrowing Fixed-Point Clip Instructions
-  DEFINE_OPIVV(vnclip, VNCLIP_FUNCT6)
-  DEFINE_OPIVX(vnclip, VNCLIP_FUNCT6)
-  DEFINE_OPIVI(vnclip, VNCLIP_FUNCT6)
-  DEFINE_OPIVV(vnclipu, VNCLIPU_FUNCT6)
-  DEFINE_OPIVX(vnclipu, VNCLIPU_FUNCT6)
-  DEFINE_OPIVI(vnclipu, VNCLIPU_FUNCT6)
-
-  // Vector Integer Extension
-  DEFINE_OPMVV_VIE(vzext_vf8)
-  DEFINE_OPMVV_VIE(vsext_vf8)
-  DEFINE_OPMVV_VIE(vzext_vf4)
-  DEFINE_OPMVV_VIE(vsext_vf4)
-  DEFINE_OPMVV_VIE(vzext_vf2)
-  DEFINE_OPMVV_VIE(vsext_vf2)
-
-#undef DEFINE_OPIVI
-#undef DEFINE_OPIVV
-#undef DEFINE_OPIVX
-#undef DEFINE_OPMVV
-#undef DEFINE_OPMVX
-#undef DEFINE_OPFVV
-#undef DEFINE_OPFWV
-#undef DEFINE_OPFVF
-#undef DEFINE_OPFWF
-#undef DEFINE_OPFVV_FMA
-#undef DEFINE_OPFVF_FMA
-#undef DEFINE_OPMVV_VIE
-#undef DEFINE_OPFRED
-
-#define DEFINE_VFUNARY(name, funct6, vs1)                          \
-  void name(VRegister vd, VRegister vs2, MaskType mask = NoMask) { \
-    GenInstrV(funct6, OP_FVV, vd, vs1, vs2, mask);                 \
-  }
-
-  DEFINE_VFUNARY(vfcvt_xu_f_v, VFUNARY0_FUNCT6, VFCVT_XU_F_V)
-  DEFINE_VFUNARY(vfcvt_x_f_v, VFUNARY0_FUNCT6, VFCVT_X_F_V)
-  DEFINE_VFUNARY(vfcvt_f_x_v, VFUNARY0_FUNCT6, VFCVT_F_X_V)
-  DEFINE_VFUNARY(vfcvt_f_xu_v, VFUNARY0_FUNCT6, VFCVT_F_XU_V)
-  DEFINE_VFUNARY(vfwcvt_xu_f_v, VFUNARY0_FUNCT6, VFWCVT_XU_F_V)
-  DEFINE_VFUNARY(vfwcvt_x_f_v, VFUNARY0_FUNCT6, VFWCVT_X_F_V)
-  DEFINE_VFUNARY(vfwcvt_f_x_v, VFUNARY0_FUNCT6, VFWCVT_F_X_V)
-  DEFINE_VFUNARY(vfwcvt_f_xu_v, VFUNARY0_FUNCT6, VFWCVT_F_XU_V)
-  DEFINE_VFUNARY(vfwcvt_f_f_v, VFUNARY0_FUNCT6, VFWCVT_F_F_V)
-
-  DEFINE_VFUNARY(vfncvt_f_f_w, VFUNARY0_FUNCT6, VFNCVT_F_F_W)
-  DEFINE_VFUNARY(vfncvt_x_f_w, VFUNARY0_FUNCT6, VFNCVT_X_F_W)
-  DEFINE_VFUNARY(vfncvt_xu_f_w, VFUNARY0_FUNCT6, VFNCVT_XU_F_W)
-
-  DEFINE_VFUNARY(vfclass_v, VFUNARY1_FUNCT6, VFCLASS_V)
-  DEFINE_VFUNARY(vfsqrt_v, VFUNARY1_FUNCT6, VFSQRT_V)
-  DEFINE_VFUNARY(vfrsqrt7_v, VFUNARY1_FUNCT6, VFRSQRT7_V)
-  DEFINE_VFUNARY(vfrec7_v, VFUNARY1_FUNCT6, VFREC7_V)
-#undef DEFINE_VFUNARY
-
-  void vnot_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
-    vxor_vi(dst, src, -1, mask);
-  }
-
-  void vneg_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
-    vrsub_vx(dst, src, zero_reg, mask);
-  }
-
-  void vfneg_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
-    vfsngjn_vv(dst, src, src, mask);
-  }
-  void vfabs_vv(VRegister dst, VRegister src, MaskType mask = NoMask) {
-    vfsngjx_vv(dst, src, src, mask);
-  }
-  void vfirst_m(Register rd, VRegister vs2, MaskType mask = NoMask);
-
-  void vcpop_m(Register rd, VRegister vs2, MaskType mask = NoMask);
-
-  // Privileged
-  void uret();
-  void sret();
-  void mret();
-  void wfi();
-  void sfence_vma(Register rs1, Register rs2);
-
-  // Assembler Pseudo Instructions (Tables 25.2, 25.3, RISC-V Unprivileged ISA)
-  void nop();
-  void RV_li(Register rd, int64_t imm);
-  // Returns the number of instructions required to load the immediate
-  static int li_estimate(int64_t imm, bool is_get_temp_reg = false);
-  // Loads an immediate, always using 8 instructions, regardless of the value,
-  // so that it can be modified later.
-  void li_constant(Register rd, int64_t imm);
-  void li_ptr(Register rd, int64_t imm);
-
-  void mv(Register rd, Register rs) { addi(rd, rs, 0); }
-  void not_(Register rd, Register rs) { xori(rd, rs, -1); }
-  void neg(Register rd, Register rs) { sub(rd, zero_reg, rs); }
-  void negw(Register rd, Register rs) { subw(rd, zero_reg, rs); }
-  void sext_w(Register rd, Register rs) { addiw(rd, rs, 0); }
-  void seqz(Register rd, Register rs) { sltiu(rd, rs, 1); }
-  void snez(Register rd, Register rs) { sltu(rd, zero_reg, rs); }
-  void sltz(Register rd, Register rs) { slt(rd, rs, zero_reg); }
-  void sgtz(Register rd, Register rs) { slt(rd, zero_reg, rs); }
-
-  void fmv_s(FPURegister rd, FPURegister rs) { fsgnj_s(rd, rs, rs); }
-  void fabs_s(FPURegister rd, FPURegister rs) { fsgnjx_s(rd, rs, rs); }
-  void fneg_s(FPURegister rd, FPURegister rs) { fsgnjn_s(rd, rs, rs); }
-  void fmv_d(FPURegister rd, FPURegister rs) { fsgnj_d(rd, rs, rs); }
-  void fabs_d(FPURegister rd, FPURegister rs) { fsgnjx_d(rd, rs, rs); }
-  void fneg_d(FPURegister rd, FPURegister rs) { fsgnjn_d(rd, rs, rs); }
-
-  void beqz(Register rs, int16_t imm13) { beq(rs, zero_reg, imm13); }
-  inline void beqz(Register rs1, Label* L) { beqz(rs1, branch_offset(L)); }
-  void bnez(Register rs, int16_t imm13) { bne(rs, zero_reg, imm13); }
-  inline void bnez(Register rs1, Label* L) { bnez(rs1, branch_offset(L)); }
-  void blez(Register rs, int16_t imm13) { bge(zero_reg, rs, imm13); }
-  inline void blez(Register rs1, Label* L) { blez(rs1, branch_offset(L)); }
-  void bgez(Register rs, int16_t imm13) { bge(rs, zero_reg, imm13); }
-  inline void bgez(Register rs1, Label* L) { bgez(rs1, branch_offset(L)); }
-  void bltz(Register rs, int16_t imm13) { blt(rs, zero_reg, imm13); }
-  inline void bltz(Register rs1, Label* L) { bltz(rs1, branch_offset(L)); }
-  void bgtz(Register rs, int16_t imm13) { blt(zero_reg, rs, imm13); }
-
-  inline void bgtz(Register rs1, Label* L) { bgtz(rs1, branch_offset(L)); }
-  void bgt(Register rs1, Register rs2, int16_t imm13) { blt(rs2, rs1, imm13); }
-  inline void bgt(Register rs1, Register rs2, Label* L) {
-    bgt(rs1, rs2, branch_offset(L));
-  }
-  void ble(Register rs1, Register rs2, int16_t imm13) { bge(rs2, rs1, imm13); }
-  inline void ble(Register rs1, Register rs2, Label* L) {
-    ble(rs1, rs2, branch_offset(L));
-  }
-  void bgtu(Register rs1, Register rs2, int16_t imm13) {
-    bltu(rs2, rs1, imm13);
-  }
-  inline void bgtu(Register rs1, Register rs2, Label* L) {
-    bgtu(rs1, rs2, branch_offset(L));
-  }
-  void bleu(Register rs1, Register rs2, int16_t imm13) {
-    bgeu(rs2, rs1, imm13);
-  }
-  inline void bleu(Register rs1, Register rs2, Label* L) {
-    bleu(rs1, rs2, branch_offset(L));
-  }
-
-  void j(int32_t imm21) { jal(zero_reg, imm21); }
-  inline void j(Label* L) { j(jump_offset(L)); }
-  inline void b(Label* L) { j(L); }
-  void jal(int32_t imm21) { jal(ra, imm21); }
-  inline void jal(Label* L) { jal(jump_offset(L)); }
-  void jr(Register rs) { jalr(zero_reg, rs, 0); }
-  void jr(Register rs, int32_t imm12) { jalr(zero_reg, rs, imm12); }
-  void jalr(Register rs, int32_t imm12) { jalr(ra, rs, imm12); }
-  void jalr(Register rs) { jalr(ra, rs, 0); }
-  void ret() { jalr(zero_reg, ra, 0); }
-  void call(int32_t offset) {
-    auipc(ra, (offset >> 12) + ((offset & 0x800) >> 11));
-    jalr(ra, ra, offset << 20 >> 20);
-  }
-
-  // Read instructions-retired counter
-  void rdinstret(Register rd) { csrrs(rd, csr_instret, zero_reg); }
-  void rdinstreth(Register rd) { csrrs(rd, csr_instreth, zero_reg); }
-  void rdcycle(Register rd) { csrrs(rd, csr_cycle, zero_reg); }
-  void rdcycleh(Register rd) { csrrs(rd, csr_cycleh, zero_reg); }
-  void rdtime(Register rd) { csrrs(rd, csr_time, zero_reg); }
-  void rdtimeh(Register rd) { csrrs(rd, csr_timeh, zero_reg); }
-
-  void csrr(Register rd, ControlStatusReg csr) { csrrs(rd, csr, zero_reg); }
-  void csrw(ControlStatusReg csr, Register rs) { csrrw(zero_reg, csr, rs); }
-  void csrs(ControlStatusReg csr, Register rs) { csrrs(zero_reg, csr, rs); }
-  void csrc(ControlStatusReg csr, Register rs) { csrrc(zero_reg, csr, rs); }
-
-  void csrwi(ControlStatusReg csr, uint8_t imm) { csrrwi(zero_reg, csr, imm); }
-  void csrsi(ControlStatusReg csr, uint8_t imm) { csrrsi(zero_reg, csr, imm); }
-  void csrci(ControlStatusReg csr, uint8_t imm) { csrrci(zero_reg, csr, imm); }
-
-  void frcsr(Register rd) { csrrs(rd, csr_fcsr, zero_reg); }
-  void fscsr(Register rd, Register rs) { csrrw(rd, csr_fcsr, rs); }
-  void fscsr(Register rs) { csrrw(zero_reg, csr_fcsr, rs); }
-
-  void frrm(Register rd) { csrrs(rd, csr_frm, zero_reg); }
-  void fsrm(Register rd, Register rs) { csrrw(rd, csr_frm, rs); }
-  void fsrm(Register rs) { csrrw(zero_reg, csr_frm, rs); }
-
-  void frflags(Register rd) { csrrs(rd, csr_fflags, zero_reg); }
-  void fsflags(Register rd, Register rs) { csrrw(rd, csr_fflags, rs); }
-  void fsflags(Register rs) { csrrw(zero_reg, csr_fflags, rs); }
-
-  // Other pseudo instructions that are not part of RISCV pseudo assemly
-  void nor(Register rd, Register rs, Register rt) {
-    or_(rd, rs, rt);
-    not_(rd, rd);
-  }
-
-  void sync() { fence(0b1111, 0b1111); }
-  void break_(uint32_t code, bool break_as_stop = false);
-  void stop(uint32_t code = kMaxStopCode);
-
-  // Check the code size generated from label to here.
-  int SizeOfCodeGeneratedSince(Label* label) {
-    return pc_offset() - label->pos();
-  }
-
-  // Check the number of instructions generated from label to here.
-  int InstructionsGeneratedSince(Label* label) {
-    return SizeOfCodeGeneratedSince(label) / kInstrSize;
-  }
-
-  using BlockConstPoolScope = ConstantPool::BlockScope;
-  // Class for scoping postponing the trampoline pool generation.
-  class BlockTrampolinePoolScope {
-   public:
-    explicit BlockTrampolinePoolScope(Assembler* assem, int margin = 0)
-        : assem_(assem) {
-      assem_->StartBlockTrampolinePool();
-    }
-
-    explicit BlockTrampolinePoolScope(Assembler* assem, PoolEmissionCheck check)
-        : assem_(assem) {
-      assem_->StartBlockTrampolinePool();
-    }
-    ~BlockTrampolinePoolScope() { assem_->EndBlockTrampolinePool(); }
-
-   private:
-    Assembler* assem_;
-    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockTrampolinePoolScope);
-  };
-
-  // Class for postponing the assembly buffer growth. Typically used for
-  // sequences of instructions that must be emitted as a unit, before
-  // buffer growth (and relocation) can occur.
-  // This blocking scope is not nestable.
-  class BlockGrowBufferScope {
-   public:
-    explicit BlockGrowBufferScope(Assembler* assem) : assem_(assem) {
-      assem_->StartBlockGrowBuffer();
-    }
-    ~BlockGrowBufferScope() { assem_->EndBlockGrowBuffer(); }
-
-   private:
-    Assembler* assem_;
-
-    DISALLOW_IMPLICIT_CONSTRUCTORS(BlockGrowBufferScope);
-  };
-
-  // Record a deoptimization reason that can be used by a log or cpu profiler.
-  // Use --trace-deopt to enable.
-  void RecordDeoptReason(DeoptimizeReason reason, uint32_t node_id,
-                         SourcePosition position, int id);
-
-  static int RelocateInternalReference(RelocInfo::Mode rmode, Address pc,
-                                       intptr_t pc_delta);
-  static void RelocateRelativeReference(RelocInfo::Mode rmode, Address pc,
-                                        intptr_t pc_delta);
-
-  // Writes a single byte or word of data in the code stream.  Used for
-  // inline tables, e.g., jump-tables.
-  void db(uint8_t data);
-  void dd(uint32_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO);
-  void dq(uint64_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO);
-  void dp(uintptr_t data, RelocInfo::Mode rmode = RelocInfo::NO_INFO) {
-    dq(data, rmode);
-  }
-  void dd(Label* label);
-
-  Instruction* pc() const { return reinterpret_cast<Instruction*>(pc_); }
-
-  // Postpone the generation of the trampoline pool for the specified number of
-  // instructions.
-  void BlockTrampolinePoolFor(int instructions);
-
-  // Check if there is less than kGap bytes available in the buffer.
-  // If this is the case, we need to grow the buffer before emitting
-  // an instruction or relocation information.
-  inline bool overflow() const { return pc_ >= reloc_info_writer.pos() - kGap; }
-
-  // Get the number of bytes available in the buffer.
-  inline intptr_t available_space() const {
-    return reloc_info_writer.pos() - pc_;
-  }
-
-  // Read/patch instructions.
-  static Instr instr_at(Address pc) { return *reinterpret_cast<Instr*>(pc); }
-  static void instr_at_put(Address pc, Instr instr) {
-    *reinterpret_cast<Instr*>(pc) = instr;
-  }
-  Instr instr_at(int pos) {
-    return *reinterpret_cast<Instr*>(buffer_start_ + pos);
-  }
-  void instr_at_put(int pos, Instr instr) {
-    *reinterpret_cast<Instr*>(buffer_start_ + pos) = instr;
-  }
-
-  void instr_at_put(int pos, ShortInstr instr) {
-    *reinterpret_cast<ShortInstr*>(buffer_start_ + pos) = instr;
-  }
-
-  Address toAddress(int pos) {
-    return reinterpret_cast<Address>(buffer_start_ + pos);
-  }
-
-  // Check if an instruction is a branch of some kind.
-  static bool IsBranch(Instr instr);
-  static bool IsCBranch(Instr instr);
-  static bool IsNop(Instr instr);
-  static bool IsJump(Instr instr);
-  static bool IsJal(Instr instr);
-  static bool IsCJal(Instr instr);
-  static bool IsJalr(Instr instr);
-  static bool IsLui(Instr instr);
-  static bool IsAuipc(Instr instr);
-  static bool IsAddiw(Instr instr);
-  static bool IsAddi(Instr instr);
-  static bool IsOri(Instr instr);
-  static bool IsSlli(Instr instr);
-  static bool IsLd(Instr instr);
-  void CheckTrampolinePool();
-
-  // Get the code target object for a pc-relative call or jump.
-  V8_INLINE Handle<Code> relative_code_target_object_handle_at(
-      Address pc_) const;
-
-  inline int UnboundLabelsCount() { return unbound_labels_count_; }
-
-  using BlockPoolsScope = BlockTrampolinePoolScope;
-
-  void RecordConstPool(int size);
-
-  void ForceConstantPoolEmissionWithoutJump() {
-    constpool_.Check(Emission::kForced, Jump::kOmitted);
-  }
-  void ForceConstantPoolEmissionWithJump() {
-    constpool_.Check(Emission::kForced, Jump::kRequired);
-  }
-  // Check if the const pool needs to be emitted while pretending that {margin}
-  // more bytes of instructions have already been emitted.
-  void EmitConstPoolWithJumpIfNeeded(size_t margin = 0) {
-    constpool_.Check(Emission::kIfNeeded, Jump::kRequired, margin);
-  }
-
-  void EmitConstPoolWithoutJumpIfNeeded(size_t margin = 0) {
-    constpool_.Check(Emission::kIfNeeded, Jump::kOmitted, margin);
-  }
-
-  void RecordEntry(uint32_t data, RelocInfo::Mode rmode) {
-    constpool_.RecordEntry(data, rmode);
-  }
-
-  void RecordEntry(uint64_t data, RelocInfo::Mode rmode) {
-    constpool_.RecordEntry(data, rmode);
-  }
-
-  friend class VectorUnit;
-  class VectorUnit {
-   public:
-    inline int32_t sew() const { return 2 ^ (sew_ + 3); }
-
-    inline int32_t vlmax() const {
-      if ((lmul_ & 0b100) != 0) {
-        return (kRvvVLEN / sew()) >> (lmul_ & 0b11);
-      } else {
-        return ((kRvvVLEN << lmul_) / sew());
-      }
-    }
-
-    explicit VectorUnit(Assembler* assm) : assm_(assm) {}
-
-    void set(Register rd, VSew sew, Vlmul lmul) {
-      if (sew != sew_ || lmul != lmul_ || vl != vlmax()) {
-        sew_ = sew;
-        lmul_ = lmul;
-        vl = vlmax();
-        assm_->vsetvlmax(rd, sew_, lmul_);
-      }
-    }
-
-    void set(Register rd, int8_t sew, int8_t lmul) {
-      DCHECK_GE(sew, E8);
-      DCHECK_LE(sew, E64);
-      DCHECK_GE(lmul, m1);
-      DCHECK_LE(lmul, mf2);
-      set(rd, VSew(sew), Vlmul(lmul));
-    }
-
-    void set(FPURoundingMode mode) {
-      if (mode_ != mode) {
-        assm_->addi(kScratchReg, zero_reg, mode << kFcsrFrmShift);
-        assm_->fscsr(kScratchReg);
-        mode_ = mode;
-      }
-    }
-    void set(Register rd, Register rs1, VSew sew, Vlmul lmul) {
-      if (sew != sew_ || lmul != lmul_) {
-        sew_ = sew;
-        lmul_ = lmul;
-        vl = 0;
-        assm_->vsetvli(rd, rs1, sew_, lmul_);
-      }
-    }
-
-    void set(VSew sew, Vlmul lmul) {
-      if (sew != sew_ || lmul != lmul_) {
-        sew_ = sew;
-        lmul_ = lmul;
-        assm_->vsetvl(sew_, lmul_);
-      }
-    }
-
-   private:
-    VSew sew_ = E8;
-    Vlmul lmul_ = m1;
-    int32_t vl = 0;
-    Assembler* assm_;
-    FPURoundingMode mode_ = RNE;
-  };
-
-  VectorUnit VU;
-
-  void CheckTrampolinePoolQuick(int extra_instructions = 0) {
-    DEBUG_PRINTF("\tpc_offset:%d %d\n", pc_offset(),
-                 next_buffer_check_ - extra_instructions * kInstrSize);
-    if (pc_offset() >= next_buffer_check_ - extra_instructions * kInstrSize) {
-      CheckTrampolinePool();
-    }
-  }
-
- protected:
-  // Readable constants for base and offset adjustment helper, these indicate if
-  // aside from offset, another value like offset + 4 should fit into int16.
-  enum class OffsetAccessType : bool {
-    SINGLE_ACCESS = false,
-    TWO_ACCESSES = true
-  };
-
-  // Determine whether need to adjust base and offset of memroy load/store
-  bool NeedAdjustBaseAndOffset(
-      const MemOperand& src, OffsetAccessType = OffsetAccessType::SINGLE_ACCESS,
-      int second_Access_add_to_offset = 4);
-
-  // Helper function for memory load/store using base register and offset.
-  void AdjustBaseAndOffset(
-      MemOperand* src, Register scratch,
-      OffsetAccessType access_type = OffsetAccessType::SINGLE_ACCESS,
-      int second_access_add_to_offset = 4);
-
-  inline static void set_target_internal_reference_encoded_at(Address pc,
-                                                              Address target);
-
-  int64_t buffer_space() const { return reloc_info_writer.pos() - pc_; }
-
-  // Decode branch instruction at pos and return branch target pos.
-  int target_at(int pos, bool is_internal);
-
-  // Patch branch instruction at pos to branch to given branch target pos.
-  void target_at_put(int pos, int target_pos, bool is_internal,
-                     bool trampoline = false);
-
-  // Say if we need to relocate with this mode.
-  bool MustUseReg(RelocInfo::Mode rmode);
-
-  // Record reloc info for current pc_.
-  void RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data = 0);
-
-  // Block the emission of the trampoline pool before pc_offset.
-  void BlockTrampolinePoolBefore(int pc_offset) {
-    if (no_trampoline_pool_before_ < pc_offset)
-      no_trampoline_pool_before_ = pc_offset;
-  }
-
-  void StartBlockTrampolinePool() {
-    DEBUG_PRINTF("\tStartBlockTrampolinePool\n");
-    trampoline_pool_blocked_nesting_++;
-  }
-
-  void EndBlockTrampolinePool() {
-    trampoline_pool_blocked_nesting_--;
-    DEBUG_PRINTF("\ttrampoline_pool_blocked_nesting:%d\n",
-                 trampoline_pool_blocked_nesting_);
-    if (trampoline_pool_blocked_nesting_ == 0) {
-      CheckTrampolinePoolQuick(1);
-    }
-  }
-
-  bool is_trampoline_pool_blocked() const {
-    return trampoline_pool_blocked_nesting_ > 0;
-  }
-
-  bool has_exception() const { return internal_trampoline_exception_; }
-
-  bool is_trampoline_emitted() const { return trampoline_emitted_; }
-
-  // Temporarily block automatic assembly buffer growth.
-  void StartBlockGrowBuffer() {
-    DCHECK(!block_buffer_growth_);
-    block_buffer_growth_ = true;
-  }
-
-  void EndBlockGrowBuffer() {
-    DCHECK(block_buffer_growth_);
-    block_buffer_growth_ = false;
-  }
-
-  bool is_buffer_growth_blocked() const { return block_buffer_growth_; }
-
- private:
-  void vsetvli(Register rd, Register rs1, VSew vsew, Vlmul vlmul,
-               TailAgnosticType tail = tu, MaskAgnosticType mask = mu);
-
-  void vsetivli(Register rd, uint8_t uimm, VSew vsew, Vlmul vlmul,
-                TailAgnosticType tail = tu, MaskAgnosticType mask = mu);
-
-  inline void vsetvlmax(Register rd, VSew vsew, Vlmul vlmul,
-                        TailAgnosticType tail = tu,
-                        MaskAgnosticType mask = mu) {
-    vsetvli(rd, zero_reg, vsew, vlmul, tu, mu);
-  }
-
-  inline void vsetvl(VSew vsew, Vlmul vlmul, TailAgnosticType tail = tu,
-                     MaskAgnosticType mask = mu) {
-    vsetvli(zero_reg, zero_reg, vsew, vlmul, tu, mu);
-  }
-
-  void vsetvl(Register rd, Register rs1, Register rs2);
-
-  // Avoid overflows for displacements etc.
-  static const int kMaximalBufferSize = 512 * MB;
-
-  // Buffer size and constant pool distance are checked together at regular
-  // intervals of kBufferCheckInterval emitted bytes.
-  static constexpr int kBufferCheckInterval = 1 * KB / 2;
-
-  // Code generation.
-  // The relocation writer's position is at least kGap bytes below the end of
-  // the generated instructions. This is so that multi-instruction sequences do
-  // not have to check for overflow. The same is true for writes of large
-  // relocation info entries.
-  static constexpr int kGap = 64;
-  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
-
-  // Repeated checking whether the trampoline pool should be emitted is rather
-  // expensive. By default we only check again once a number of instructions
-  // has been generated.
-  static constexpr int kCheckConstIntervalInst = 32;
-  static constexpr int kCheckConstInterval =
-      kCheckConstIntervalInst * kInstrSize;
-
-  int next_buffer_check_;  // pc offset of next buffer check.
-
-  // Emission of the trampoline pool may be blocked in some code sequences.
-  int trampoline_pool_blocked_nesting_;  // Block emission if this is not zero.
-  int no_trampoline_pool_before_;  // Block emission before this pc offset.
-
-  // Keep track of the last emitted pool to guarantee a maximal distance.
-  int last_trampoline_pool_end_;  // pc offset of the end of the last pool.
-
-  // Automatic growth of the assembly buffer may be blocked for some sequences.
-  bool block_buffer_growth_;  // Block growth when true.
-
-  // Relocation information generation.
-  // Each relocation is encoded as a variable size value.
-  static constexpr int kMaxRelocSize = RelocInfoWriter::kMaxSize;
-  RelocInfoWriter reloc_info_writer;
-
-  // The bound position, before this we cannot do instruction elimination.
-  int last_bound_pos_;
-
-  // Code emission.
-  inline void CheckBuffer();
-  void GrowBuffer();
-  inline void emit(Instr x);
-  inline void emit(ShortInstr x);
-  inline void emit(uint64_t x);
-  template <typename T>
-  inline void EmitHelper(T x);
-
-  static void disassembleInstr(Instr instr);
-
-  // Instruction generation.
-
-  // ----- Top-level instruction formats match those in the ISA manual
-  // (R, I, S, B, U, J). These match the formats defined in LLVM's
-  // RISCVInstrFormats.td.
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, Register rd,
-                 Register rs1, Register rs2);
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, FPURegister rd,
-                 FPURegister rs1, FPURegister rs2);
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, Register rd,
-                 FPURegister rs1, Register rs2);
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, FPURegister rd,
-                 Register rs1, Register rs2);
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, FPURegister rd,
-                 FPURegister rs1, Register rs2);
-  void GenInstrR(uint8_t funct7, uint8_t funct3, Opcode opcode, Register rd,
-                 FPURegister rs1, FPURegister rs2);
-  void GenInstrR4(uint8_t funct2, Opcode opcode, Register rd, Register rs1,
-                  Register rs2, Register rs3, FPURoundingMode frm);
-  void GenInstrR4(uint8_t funct2, Opcode opcode, FPURegister rd,
-                  FPURegister rs1, FPURegister rs2, FPURegister rs3,
-                  FPURoundingMode frm);
-  void GenInstrRAtomic(uint8_t funct5, bool aq, bool rl, uint8_t funct3,
-                       Register rd, Register rs1, Register rs2);
-  void GenInstrRFrm(uint8_t funct7, Opcode opcode, Register rd, Register rs1,
-                    Register rs2, FPURoundingMode frm);
-  void GenInstrI(uint8_t funct3, Opcode opcode, Register rd, Register rs1,
-                 int16_t imm12);
-  void GenInstrI(uint8_t funct3, Opcode opcode, FPURegister rd, Register rs1,
-                 int16_t imm12);
-  void GenInstrIShift(bool arithshift, uint8_t funct3, Opcode opcode,
-                      Register rd, Register rs1, uint8_t shamt);
-  void GenInstrIShiftW(bool arithshift, uint8_t funct3, Opcode opcode,
-                       Register rd, Register rs1, uint8_t shamt);
-  void GenInstrS(uint8_t funct3, Opcode opcode, Register rs1, Register rs2,
-                 int16_t imm12);
-  void GenInstrS(uint8_t funct3, Opcode opcode, Register rs1, FPURegister rs2,
-                 int16_t imm12);
-  void GenInstrB(uint8_t funct3, Opcode opcode, Register rs1, Register rs2,
-                 int16_t imm12);
-  void GenInstrU(Opcode opcode, Register rd, int32_t imm20);
-  void GenInstrJ(Opcode opcode, Register rd, int32_t imm20);
-  void GenInstrCR(uint8_t funct4, Opcode opcode, Register rd, Register rs2);
-  void GenInstrCA(uint8_t funct6, Opcode opcode, Register rd, uint8_t funct,
-                  Register rs2);
-  void GenInstrCI(uint8_t funct3, Opcode opcode, Register rd, int8_t imm6);
-  void GenInstrCIU(uint8_t funct3, Opcode opcode, Register rd, uint8_t uimm6);
-  void GenInstrCIU(uint8_t funct3, Opcode opcode, FPURegister rd,
-                   uint8_t uimm6);
-  void GenInstrCIW(uint8_t funct3, Opcode opcode, Register rd, uint8_t uimm8);
-  void GenInstrCSS(uint8_t funct3, Opcode opcode, FPURegister rs2,
-                   uint8_t uimm6);
-  void GenInstrCSS(uint8_t funct3, Opcode opcode, Register rs2, uint8_t uimm6);
-  void GenInstrCL(uint8_t funct3, Opcode opcode, Register rd, Register rs1,
-                  uint8_t uimm5);
-  void GenInstrCL(uint8_t funct3, Opcode opcode, FPURegister rd, Register rs1,
-                  uint8_t uimm5);
-  void GenInstrCS(uint8_t funct3, Opcode opcode, Register rs2, Register rs1,
-                  uint8_t uimm5);
-  void GenInstrCS(uint8_t funct3, Opcode opcode, FPURegister rs2, Register rs1,
-                  uint8_t uimm5);
-  void GenInstrCJ(uint8_t funct3, Opcode opcode, uint16_t uint11);
-  void GenInstrCB(uint8_t funct3, Opcode opcode, Register rs1, uint8_t uimm8);
-  void GenInstrCBA(uint8_t funct3, uint8_t funct2, Opcode opcode, Register rs1,
-                   int8_t imm6);
-
-  // ----- Instruction class templates match those in LLVM's RISCVInstrInfo.td
-  void GenInstrBranchCC_rri(uint8_t funct3, Register rs1, Register rs2,
-                            int16_t imm12);
-  void GenInstrLoad_ri(uint8_t funct3, Register rd, Register rs1,
-                       int16_t imm12);
-  void GenInstrStore_rri(uint8_t funct3, Register rs1, Register rs2,
-                         int16_t imm12);
-  void GenInstrALU_ri(uint8_t funct3, Register rd, Register rs1, int16_t imm12);
-  void GenInstrShift_ri(bool arithshift, uint8_t funct3, Register rd,
-                        Register rs1, uint8_t shamt);
-  void GenInstrALU_rr(uint8_t funct7, uint8_t funct3, Register rd, Register rs1,
-                      Register rs2);
-  void GenInstrCSR_ir(uint8_t funct3, Register rd, ControlStatusReg csr,
-                      Register rs1);
-  void GenInstrCSR_ii(uint8_t funct3, Register rd, ControlStatusReg csr,
-                      uint8_t rs1);
-  void GenInstrShiftW_ri(bool arithshift, uint8_t funct3, Register rd,
-                         Register rs1, uint8_t shamt);
-  void GenInstrALUW_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                       Register rs1, Register rs2);
-  void GenInstrPriv(uint8_t funct7, Register rs1, Register rs2);
-  void GenInstrLoadFP_ri(uint8_t funct3, FPURegister rd, Register rs1,
-                         int16_t imm12);
-  void GenInstrStoreFP_rri(uint8_t funct3, Register rs1, FPURegister rs2,
-                           int16_t imm12);
-  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                        FPURegister rs1, FPURegister rs2);
-  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                        Register rs1, Register rs2);
-  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, FPURegister rd,
-                        FPURegister rs1, Register rs2);
-  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                        FPURegister rs1, Register rs2);
-  void GenInstrALUFP_rr(uint8_t funct7, uint8_t funct3, Register rd,
-                        FPURegister rs1, FPURegister rs2);
-
-  // ----------------------------RVV------------------------------------------
-  // vsetvl
-  void GenInstrV(Register rd, Register rs1, Register rs2);
-  // vsetvli
-  void GenInstrV(Register rd, Register rs1, uint32_t zimm);
-  // OPIVV OPFVV OPMVV
-  void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, VRegister vs1,
-                 VRegister vs2, MaskType mask = NoMask);
-  void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, int8_t vs1,
-                 VRegister vs2, MaskType mask = NoMask);
-  void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, VRegister vs2,
-                 MaskType mask = NoMask);
-  // OPMVV OPFVV
-  void GenInstrV(uint8_t funct6, Opcode opcode, Register rd, VRegister vs1,
-                 VRegister vs2, MaskType mask = NoMask);
-  // OPFVV
-  void GenInstrV(uint8_t funct6, Opcode opcode, FPURegister fd, VRegister vs1,
-                 VRegister vs2, MaskType mask = NoMask);
-
-  // OPIVX OPMVX
-  void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, Register rs1,
-                 VRegister vs2, MaskType mask = NoMask);
-  // OPFVF
-  void GenInstrV(uint8_t funct6, Opcode opcode, VRegister vd, FPURegister fs1,
-                 VRegister vs2, MaskType mask = NoMask);
-  // OPMVX
-  void GenInstrV(uint8_t funct6, Register rd, Register rs1, VRegister vs2,
-                 MaskType mask = NoMask);
-  // OPIVI
-  void GenInstrV(uint8_t funct6, VRegister vd, int8_t simm5, VRegister vs2,
-                 MaskType mask = NoMask);
-
-  // VL VS
-  void GenInstrV(Opcode opcode, uint8_t width, VRegister vd, Register rs1,
-                 uint8_t umop, MaskType mask, uint8_t IsMop, bool IsMew,
-                 uint8_t Nf);
-
-  void GenInstrV(Opcode opcode, uint8_t width, VRegister vd, Register rs1,
-                 Register rs2, MaskType mask, uint8_t IsMop, bool IsMew,
-                 uint8_t Nf);
-  // VL VS AMO
-  void GenInstrV(Opcode opcode, uint8_t width, VRegister vd, Register rs1,
-                 VRegister vs2, MaskType mask, uint8_t IsMop, bool IsMew,
-                 uint8_t Nf);
-  // vmv_xs vcpop_m vfirst_m
-  void GenInstrV(uint8_t funct6, Opcode opcode, Register rd, uint8_t vs1,
-                 VRegister vs2, MaskType mask);
-  // Labels.
-  void print(const Label* L);
-  void bind_to(Label* L, int pos);
-  void next(Label* L, bool is_internal);
-
-  // One trampoline consists of:
-  // - space for trampoline slots,
-  // - space for labels.
-  //
-  // Space for trampoline slots is equal to slot_count * 2 * kInstrSize.
-  // Space for trampoline slots precedes space for labels. Each label is of one
-  // instruction size, so total amount for labels is equal to
-  // label_count *  kInstrSize.
-  class Trampoline {
-   public:
-    Trampoline() {
-      start_ = 0;
-      next_slot_ = 0;
-      free_slot_count_ = 0;
-      end_ = 0;
-    }
-    Trampoline(int start, int slot_count) {
-      start_ = start;
-      next_slot_ = start;
-      free_slot_count_ = slot_count;
-      end_ = start + slot_count * kTrampolineSlotsSize;
-    }
-    int start() { return start_; }
-    int end() { return end_; }
-    int take_slot() {
-      int trampoline_slot = kInvalidSlotPos;
-      if (free_slot_count_ <= 0) {
-        // We have run out of space on trampolines.
-        // Make sure we fail in debug mode, so we become aware of each case
-        // when this happens.
-        DCHECK(0);
-        // Internal exception will be caught.
-      } else {
-        trampoline_slot = next_slot_;
-        free_slot_count_--;
-        next_slot_ += kTrampolineSlotsSize;
-      }
-      return trampoline_slot;
-    }
-
-   private:
-    int start_;
-    int end_;
-    int next_slot_;
-    int free_slot_count_;
-  };
-
-  int32_t get_trampoline_entry(int32_t pos);
-  int unbound_labels_count_;
-  // After trampoline is emitted, long branches are used in generated code for
-  // the forward branches whose target offsets could be beyond reach of branch
-  // instruction. We use this information to trigger different mode of
-  // branch instruction generation, where we use jump instructions rather
-  // than regular branch instructions.
-  bool trampoline_emitted_ = false;
-  static constexpr int kInvalidSlotPos = -1;
-
-  // Internal reference positions, required for unbounded internal reference
-  // labels.
-  std::set<int64_t> internal_reference_positions_;
-  bool is_internal_reference(Label* L) {
-    return internal_reference_positions_.find(L->pos()) !=
-           internal_reference_positions_.end();
-  }
-
-  Trampoline trampoline_;
-  bool internal_trampoline_exception_;
-
-  RegList scratch_register_list_;
-
- private:
-  ConstantPool constpool_;
-
-  void AllocateAndInstallRequestedHeapObjects(Isolate* isolate);
-
-  int WriteCodeComments();
-
-  friend class RegExpMacroAssemblerRISCV;
-  friend class RelocInfo;
-  friend class BlockTrampolinePoolScope;
-  friend class EnsureSpace;
-  friend class ConstantPool;
-};
-
-class EnsureSpace {
- public:
-  explicit inline EnsureSpace(Assembler* assembler);
-};
-
-class V8_EXPORT_PRIVATE UseScratchRegisterScope {
- public:
-  explicit UseScratchRegisterScope(Assembler* assembler);
-  ~UseScratchRegisterScope();
-
-  Register Acquire();
-  bool hasAvailable() const;
-  void Include(const RegList& list) { *available_ |= list; }
-  void Exclude(const RegList& list) {
-    *available_ &= RegList::FromBits(~list.bits());
-  }
-  void Include(const Register& reg1, const Register& reg2 = no_reg) {
-    RegList list({reg1, reg2});
-    Include(list);
-  }
-  void Exclude(const Register& reg1, const Register& reg2 = no_reg) {
-    RegList list({reg1, reg2});
-    Exclude(list);
-  }
-
- private:
-  RegList* available_;
-  RegList old_available_;
-};
-
-class LoadStoreLaneParams {
- public:
-  int sz;
-  uint8_t laneidx;
-
-  LoadStoreLaneParams(MachineRepresentation rep, uint8_t laneidx);
-
- private:
-  LoadStoreLaneParams(uint8_t laneidx, int sz, int lanes)
-      : sz(sz), laneidx(laneidx % lanes) {}
-};
-
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_CODEGEN_RISCV64_ASSEMBLER_RISCV64_H_
diff --git a/src/common/globals.h b/src/common/globals.h
index b17792f6e0b..d255e795393 100644
--- a/src/common/globals.h
+++ b/src/common/globals.h
@@ -58,6 +58,9 @@ namespace internal {
 #if (V8_TARGET_ARCH_RISCV64 && !V8_HOST_ARCH_RISCV64)
 #define USE_SIMULATOR 1
 #endif
+#if (V8_TARGET_ARCH_RISCV32 && !V8_HOST_ARCH_RISCV32)
+#define USE_SIMULATOR 1
+#endif
 #if (V8_TARGET_ARCH_LOONG64 && !V8_HOST_ARCH_LOONG64)
 #define USE_SIMULATOR 1
 #endif
@@ -335,7 +338,7 @@ constexpr bool kPlatformRequiresCodeRange = false;
 constexpr size_t kMaximalCodeRangeSize = 0 * MB;
 constexpr size_t kMinimumCodeRangeSize = 0 * MB;
 constexpr size_t kMinExpectedOSPageSize = 64 * KB;  // OS page on PPC Linux
-#elif V8_TARGET_ARCH_MIPS
+#elif V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_RISCV32
 constexpr bool kPlatformRequiresCodeRange = false;
 constexpr size_t kMaximalCodeRangeSize = 2048LL * MB;
 constexpr size_t kMinimumCodeRangeSize = 0 * MB;
diff --git a/src/compiler/backend/instruction-codes.h b/src/compiler/backend/instruction-codes.h
index b9d3821a2dd..2fe2cd1a74f 100644
--- a/src/compiler/backend/instruction-codes.h
+++ b/src/compiler/backend/instruction-codes.h
@@ -25,8 +25,8 @@
 #include "src/compiler/backend/ppc/instruction-codes-ppc.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/compiler/backend/s390/instruction-codes-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/compiler/backend/riscv64/instruction-codes-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/compiler/backend/riscv/instruction-codes-riscv.h"
 #else
 #define TARGET_ARCH_OPCODE_LIST(V)
 #define TARGET_ADDRESSING_MODE_LIST(V)
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index 91eaba15848..a47567179bd 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -2702,7 +2702,8 @@ void InstructionSelector::VisitWord32PairShr(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitWord32PairSar(Node* node) { UNIMPLEMENTED(); }
 #endif  // V8_TARGET_ARCH_64_BIT
 
-#if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_MIPS
+#if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_MIPS && \
+    !V8_TARGET_ARCH_RISCV32
 void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
   UNIMPLEMENTED();
 }
@@ -2739,6 +2740,7 @@ void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_MIPS
+        // && !V8_TARGET_ARCH_RISCV32
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_MIPS64 && \
     !V8_TARGET_ARCH_S390 && !V8_TARGET_ARCH_PPC64 &&                          \
@@ -2782,27 +2784,29 @@ void InstructionSelector::VisitI64x2ReplaceLaneI32Pair(Node* node) {
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
 #if !V8_TARGET_ARCH_ARM64
-#if !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_LOONG64 && !V8_TARGET_ARCH_RISCV64
+#if !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_LOONG64 && \
+    !V8_TARGET_ARCH_RISCV32 && !V8_TARGET_ARCH_RISCV64
 void InstructionSelector::VisitI64x2Splat(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitI64x2ExtractLane(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitI64x2ReplaceLane(Node* node) { UNIMPLEMENTED(); }
 #endif  // !V8_TARGET_ARCH_MIPS64 && !V8_TARGET_ARCH_LOONG64 &&
-        // !V8_TARGET_ARCH_RISCV64
+        // !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_RISCV32
 #endif  // !V8_TARGET_ARCH_ARM64
 #endif  // !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64 && \
-    !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_RISCV64
+    !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_IA32 &&                         \
+    !V8_TARGET_ARCH_RISCV32 && !V8_TARGET_ARCH_RISCV64
 void InstructionSelector::VisitF64x2Qfma(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitF64x2Qfms(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitF32x4Qfma(Node* node) { UNIMPLEMENTED(); }
 void InstructionSelector::VisitF32x4Qfms(Node* node) { UNIMPLEMENTED(); }
 #endif  // !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_S390X && !V8_TARGET_ARCH_PPC64
         // && !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_IA32 &&
-        // !V8_TARGET_ARCH_RISCV64
+        // !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_RISCV32
 
 #if !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM64 && \
-    !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_ARM
+    !V8_TARGET_ARCH_RISCV32 && !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARCH_ARM
 void InstructionSelector::VisitI8x16RelaxedLaneSelect(Node* node) {
   UNIMPLEMENTED();
 }
@@ -2832,7 +2836,8 @@ void InstructionSelector::VisitI32x4RelaxedTruncF32x4U(Node* node) {
   UNIMPLEMENTED();
 }
 #endif  // !V8_TARGET_ARCH_X64 && !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM64
-        // && !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARM
+        // && !V8_TARGET_ARCH_RISCV64 && !V8_TARGET_ARM &&
+        // !V8_TARGET_ARCH_RISCV32
 
 #if !V8_TARGET_ARCH_ARM64 && !V8_TARGET_ARCH_ARM
 void InstructionSelector::VisitI16x8RelaxedQ15MulRS(Node* node) {
diff --git a/src/compiler/backend/riscv64/code-generator-riscv64.cc b/src/compiler/backend/riscv/code-generator-riscv.cc
similarity index 90%
rename from src/compiler/backend/riscv64/code-generator-riscv64.cc
rename to src/compiler/backend/riscv/code-generator-riscv.cc
index 9003c1b9512..dfb57303e56 100644
--- a/src/compiler/backend/riscv64/code-generator-riscv64.cc
+++ b/src/compiler/backend/riscv/code-generator-riscv.cc
@@ -6,7 +6,7 @@
 #include "src/codegen/callable.h"
 #include "src/codegen/macro-assembler.h"
 #include "src/codegen/optimized-compilation-info.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
+#include "src/codegen/riscv/constants-riscv.h"
 #include "src/compiler/backend/code-generator-impl.h"
 #include "src/compiler/backend/code-generator.h"
 #include "src/compiler/backend/gap-resolver.h"
@@ -174,13 +174,15 @@ class OutOfLineRecordWrite final : public OutOfLineCode {
   }
 
   void Generate() final {
+#if V8_TARGET_ARCH_RISCV64
     if (COMPRESS_POINTERS_BOOL) {
       __ DecompressTaggedPointer(value_, value_);
     }
+#endif
     __ CheckPageFlag(value_, scratch0_,
                      MemoryChunk::kPointersToHereAreInterestingMask, eq,
                      exit());
-    __ Add64(scratch1_, object_, index_);
+    __ AddWord(scratch1_, object_, index_);
     SaveFPRegsMode const save_fp_mode = frame()->DidAllocateDoubleRegisters()
                                             ? SaveFPRegsMode::kSave
                                             : SaveFPRegsMode::kIgnore;
@@ -258,7 +260,7 @@ Condition FlagsConditionToConditionTst(FlagsCondition condition) {
   }
   UNREACHABLE();
 }
-
+#if V8_TARGET_ARCH_RISCV64
 Condition FlagsConditionToConditionOvf(FlagsCondition condition) {
   switch (condition) {
     case kOverflow:
@@ -270,7 +272,7 @@ Condition FlagsConditionToConditionOvf(FlagsCondition condition) {
   }
   UNREACHABLE();
 }
-
+#endif
 FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
                                              FlagsCondition condition) {
   switch (condition) {
@@ -321,7 +323,7 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
 #define ASSEMBLE_ATOMIC_BINOP(load_linked, store_conditional, bin_instr)       \
   do {                                                                         \
     Label binop;                                                               \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     __ sync();                                                                 \
     __ bind(&binop);                                                           \
     __ load_linked(i.OutputRegister(0), MemOperand(i.TempRegister(0), 0));     \
@@ -332,19 +334,39 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
     __ sync();                                                                 \
   } while (0)
 
+#define ASSEMBLE_ATOMIC64_LOGIC_BINOP(bin_instr, external)  \
+  do {                                                      \
+    FrameScope scope(tasm(), StackFrame::MANUAL);           \
+    __ AddWord(a0, i.InputRegister(0), i.InputRegister(1)); \
+    __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);    \
+    __ PrepareCallCFunction(3, 0, kScratchReg);             \
+    __ CallCFunction(ExternalReference::external(), 3, 0);  \
+    __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);     \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC64_ARITH_BINOP(bin_instr, external)  \
+  do {                                                      \
+    FrameScope scope(tasm(), StackFrame::MANUAL);           \
+    __ AddWord(a0, i.InputRegister(0), i.InputRegister(1)); \
+    __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);    \
+    __ PrepareCallCFunction(3, 0, kScratchReg);             \
+    __ CallCFunction(ExternalReference::external(), 3, 0);  \
+    __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);     \
+  } while (0)
+
 #define ASSEMBLE_ATOMIC_BINOP_EXT(load_linked, store_conditional, sign_extend, \
                                   size, bin_instr, representation)             \
   do {                                                                         \
     Label binop;                                                               \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     if (representation == 32) {                                                \
       __ And(i.TempRegister(3), i.TempRegister(0), 0x3);                       \
     } else {                                                                   \
       DCHECK_EQ(representation, 64);                                           \
       __ And(i.TempRegister(3), i.TempRegister(0), 0x7);                       \
     }                                                                          \
-    __ Sub64(i.TempRegister(0), i.TempRegister(0),                             \
-             Operand(i.TempRegister(3)));                                      \
+    __ SubWord(i.TempRegister(0), i.TempRegister(0),                           \
+               Operand(i.TempRegister(3)));                                    \
     __ Sll32(i.TempRegister(3), i.TempRegister(3), 3);                         \
     __ sync();                                                                 \
     __ bind(&binop);                                                           \
@@ -365,7 +387,7 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
     Label exchange;                                                            \
     __ sync();                                                                 \
     __ bind(&exchange);                                                        \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     __ load_linked(i.OutputRegister(0), MemOperand(i.TempRegister(0), 0));     \
     __ Move(i.TempRegister(1), i.InputRegister(2));                            \
     __ store_conditional(i.TempRegister(1), MemOperand(i.TempRegister(0), 0)); \
@@ -377,15 +399,15 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
     load_linked, store_conditional, sign_extend, size, representation)         \
   do {                                                                         \
     Label exchange;                                                            \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     if (representation == 32) {                                                \
       __ And(i.TempRegister(1), i.TempRegister(0), 0x3);                       \
     } else {                                                                   \
       DCHECK_EQ(representation, 64);                                           \
       __ And(i.TempRegister(1), i.TempRegister(0), 0x7);                       \
     }                                                                          \
-    __ Sub64(i.TempRegister(0), i.TempRegister(0),                             \
-             Operand(i.TempRegister(1)));                                      \
+    __ SubWord(i.TempRegister(0), i.TempRegister(0),                           \
+               Operand(i.TempRegister(1)));                                    \
     __ Sll32(i.TempRegister(1), i.TempRegister(1), 3);                         \
     __ sync();                                                                 \
     __ bind(&exchange);                                                        \
@@ -404,7 +426,7 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
   do {                                                                         \
     Label compareExchange;                                                     \
     Label exit;                                                                \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     __ sync();                                                                 \
     __ bind(&compareExchange);                                                 \
     __ load_linked(i.OutputRegister(0), MemOperand(i.TempRegister(0), 0));     \
@@ -423,15 +445,15 @@ FPUCondition FlagsConditionToConditionCmpFPU(bool* predicate,
   do {                                                                         \
     Label compareExchange;                                                     \
     Label exit;                                                                \
-    __ Add64(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));       \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1));     \
     if (representation == 32) {                                                \
       __ And(i.TempRegister(1), i.TempRegister(0), 0x3);                       \
     } else {                                                                   \
       DCHECK_EQ(representation, 64);                                           \
       __ And(i.TempRegister(1), i.TempRegister(0), 0x7);                       \
     }                                                                          \
-    __ Sub64(i.TempRegister(0), i.TempRegister(0),                             \
-             Operand(i.TempRegister(1)));                                      \
+    __ SubWord(i.TempRegister(0), i.TempRegister(0),                           \
+               Operand(i.TempRegister(1)));                                    \
     __ Sll32(i.TempRegister(1), i.TempRegister(1), 3);                         \
     __ sync();                                                                 \
     __ bind(&compareExchange);                                                 \
@@ -549,8 +571,8 @@ void CodeGenerator::AssembleDeconstructFrame() {
 
 void CodeGenerator::AssemblePrepareTailCall() {
   if (frame_access_state()->has_frame()) {
-    __ Ld(ra, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
-    __ Ld(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
+    __ LoadWord(ra, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
+    __ LoadWord(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
   }
   frame_access_state()->SetFrameAccessToSP();
 }
@@ -570,10 +592,10 @@ void AdjustStackPointerForTailCall(TurboAssembler* tasm,
                           StandardFrameConstants::kFixedSlotCountAboveFp;
   int stack_slot_delta = new_slot_above_sp - current_sp_offset;
   if (stack_slot_delta > 0) {
-    tasm->Sub64(sp, sp, stack_slot_delta * kSystemPointerSize);
+    tasm->SubWord(sp, sp, stack_slot_delta * kSystemPointerSize);
     state->IncreaseSPDelta(stack_slot_delta);
   } else if (allow_shrinkage && stack_slot_delta < 0) {
-    tasm->Add64(sp, sp, -stack_slot_delta * kSystemPointerSize);
+    tasm->AddWord(sp, sp, -stack_slot_delta * kSystemPointerSize);
     state->IncreaseSPDelta(stack_slot_delta);
   }
 }
@@ -654,7 +676,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         Address wasm_code = static_cast<Address>(constant.ToInt64());
         __ Call(wasm_code, constant.rmode());
       } else {
-        __ Add64(t6, i.InputOrZeroRegister(0), 0);
+        __ AddWord(t6, i.InputOrZeroRegister(0), 0);
         __ Call(t6);
       }
       RecordCallPosition(instr);
@@ -681,7 +703,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         Address wasm_code = static_cast<Address>(constant.ToInt64());
         __ Jump(wasm_code, constant.rmode());
       } else {
-        __ Add64(kScratchReg, i.InputOrZeroRegister(0), 0);
+        __ AddWord(kScratchReg, i.InputOrZeroRegister(0), 0);
         __ Jump(kScratchReg);
       }
       frame_access_state()->ClearSPDelta();
@@ -762,8 +784,8 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       if (isWasmCapiFunction) {
         // Put the return address in a stack slot.
         __ LoadAddress(kScratchReg, &after_call, RelocInfo::EXTERNAL_REFERENCE);
-        __ Sd(kScratchReg,
-              MemOperand(fp, WasmExitFrameConstants::kCallingPCOffset));
+        __ StoreWord(kScratchReg,
+                     MemOperand(fp, WasmExitFrameConstants::kCallingPCOffset));
       }
       if (instr->InputAt(0)->IsImmediate()) {
         ExternalReference ref = i.InputExternalReference(0);
@@ -846,7 +868,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       break;
     case kArchParentFramePointer:
       if (frame_access_state()->has_frame()) {
-        __ Ld(i.OutputRegister(), MemOperand(fp, 0));
+        __ LoadWord(i.OutputRegister(), MemOperand(fp, 0));
       } else {
         __ Move(i.OutputRegister(), fp);
       }
@@ -866,7 +888,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       auto ool = zone()->New<OutOfLineRecordWrite>(this, object, index, value,
                                                    scratch0, scratch1, mode,
                                                    DetermineStubCallMode());
-      __ Add64(kScratchReg, object, index);
+      __ AddWord(kScratchReg, object, index);
       __ StoreTaggedField(value, MemOperand(kScratchReg));
       if (mode > RecordWriteMode::kValueIsPointer) {
         __ JumpIfSmi(value, ool->exit());
@@ -881,7 +903,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       FrameOffset offset =
           frame_access_state()->GetFrameOffset(i.InputInt32(0));
       Register base_reg = offset.from_stack_pointer() ? sp : fp;
-      __ Add64(i.OutputRegister(), base_reg, Operand(offset.offset()));
+      __ AddWord(i.OutputRegister(), base_reg, Operand(offset.offset()));
       int alignment = i.InputInt32(1);
       DCHECK(alignment == 0 || alignment == 4 || alignment == 8 ||
              alignment == 16);
@@ -894,19 +916,19 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       if (alignment == 2 * kSystemPointerSize) {
         Label done;
-        __ Add64(kScratchReg, base_reg, Operand(offset.offset()));
+        __ AddWord(kScratchReg, base_reg, Operand(offset.offset()));
         __ And(kScratchReg, kScratchReg, Operand(alignment - 1));
         __ BranchShort(&done, eq, kScratchReg, Operand(zero_reg));
-        __ Add64(i.OutputRegister(), i.OutputRegister(), kSystemPointerSize);
+        __ AddWord(i.OutputRegister(), i.OutputRegister(), kSystemPointerSize);
         __ bind(&done);
       } else if (alignment > 2 * kSystemPointerSize) {
         Label done;
-        __ Add64(kScratchReg, base_reg, Operand(offset.offset()));
+        __ AddWord(kScratchReg, base_reg, Operand(offset.offset()));
         __ And(kScratchReg, kScratchReg, Operand(alignment - 1));
         __ BranchShort(&done, eq, kScratchReg, Operand(zero_reg));
         __ li(kScratchReg2, alignment);
-        __ Sub64(kScratchReg2, kScratchReg2, Operand(kScratchReg));
-        __ Add64(i.OutputRegister(), i.OutputRegister(), kScratchReg2);
+        __ SubWord(kScratchReg2, kScratchReg2, Operand(kScratchReg));
+        __ AddWord(i.OutputRegister(), i.OutputRegister(), kScratchReg2);
         __ bind(&done);
       }
 
@@ -978,16 +1000,25 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvAdd32:
       __ Add32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
       break;
+    case kRiscvSub32:
+      __ Sub32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      break;
+    case kRiscvMul32:
+      __ Mul32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      break;
+    case kRiscvMulOvf32:
+      __ MulOverflow32(i.OutputRegister(), i.InputOrZeroRegister(0),
+                       i.InputOperand(1), kScratchReg);
+      break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvAdd64:
-      __ Add64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      __ AddWord(i.OutputRegister(), i.InputOrZeroRegister(0),
+                 i.InputOperand(1));
       break;
     case kRiscvAddOvf64:
       __ AddOverflow64(i.OutputRegister(), i.InputOrZeroRegister(0),
                        i.InputOperand(1), kScratchReg);
       break;
-    case kRiscvSub32:
-      __ Sub32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
-      break;
     case kRiscvSub64:
       __ Sub64(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
       break;
@@ -995,13 +1026,6 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ SubOverflow64(i.OutputRegister(), i.InputOrZeroRegister(0),
                        i.InputOperand(1), kScratchReg);
       break;
-    case kRiscvMul32:
-      __ Mul32(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
-      break;
-    case kRiscvMulOvf32:
-      __ MulOverflow32(i.OutputRegister(), i.InputOrZeroRegister(0),
-                       i.InputOperand(1), kScratchReg);
-      break;
     case kRiscvMulHigh32:
       __ Mulh32(i.OutputRegister(), i.InputOrZeroRegister(0),
                 i.InputOperand(1));
@@ -1057,6 +1081,41 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ Modu64(i.OutputRegister(), i.InputOrZeroRegister(0),
                 i.InputOperand(1));
       break;
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvAddOvf:
+      __ AddOverflow(i.OutputRegister(), i.InputOrZeroRegister(0),
+                     i.InputOperand(1), kScratchReg);
+      break;
+    case kRiscvSubOvf:
+      __ SubOverflow(i.OutputRegister(), i.InputOrZeroRegister(0),
+                     i.InputOperand(1), kScratchReg);
+      break;
+    case kRiscvMulHigh32:
+      __ Mulh(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      break;
+    case kRiscvMulHighU32:
+      __ Mulhu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1),
+               kScratchReg, kScratchReg2);
+      break;
+    case kRiscvDiv32: {
+      __ Div(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      // Set ouput to zero if divisor == 0
+      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));
+      break;
+    }
+    case kRiscvDivU32: {
+      __ Divu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      // Set ouput to zero if divisor == 0
+      __ LoadZeroIfConditionZero(i.OutputRegister(), i.InputRegister(1));
+      break;
+    }
+    case kRiscvMod32:
+      __ Mod(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      break;
+    case kRiscvModU32:
+      __ Modu(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
+      break;
+#endif
     case kRiscvAnd:
       __ And(i.OutputRegister(), i.InputOrZeroRegister(0), i.InputOperand(1));
       break;
@@ -1099,28 +1158,30 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvClz32:
       __ Clz32(i.OutputRegister(), i.InputOrZeroRegister(0));
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvClz64:
       __ Clz64(i.OutputRegister(), i.InputOrZeroRegister(0));
       break;
-    case kRiscvCtz32: {
+    case kRiscvCtz64: {
       Register src = i.InputRegister(0);
       Register dst = i.OutputRegister();
-      __ Ctz32(dst, src);
+      __ Ctz64(dst, src);
     } break;
-    case kRiscvCtz64: {
+    case kRiscvPopcnt64: {
       Register src = i.InputRegister(0);
       Register dst = i.OutputRegister();
-      __ Ctz64(dst, src);
+      __ Popcnt64(dst, src, kScratchReg);
     } break;
-    case kRiscvPopcnt32: {
+#endif
+    case kRiscvCtz32: {
       Register src = i.InputRegister(0);
       Register dst = i.OutputRegister();
-      __ Popcnt32(dst, src, kScratchReg);
+      __ Ctz32(dst, src);
     } break;
-    case kRiscvPopcnt64: {
+    case kRiscvPopcnt32: {
       Register src = i.InputRegister(0);
       Register dst = i.OutputRegister();
-      __ Popcnt64(dst, src, kScratchReg);
+      __ Popcnt32(dst, src, kScratchReg);
     } break;
     case kRiscvShl32:
       if (instr->InputAt(1)->IsRegister()) {
@@ -1149,6 +1210,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                  static_cast<uint16_t>(imm));
       }
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvZeroExtendWord: {
       __ ZeroExtendWord(i.OutputRegister(), i.InputRegister(0));
       break;
@@ -1166,12 +1228,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvSar64:
       __ Sra64(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
       break;
-    case kRiscvRor32:
-      __ Ror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
-      break;
     case kRiscvRor64:
       __ Dror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
       break;
+#endif
+    case kRiscvRor32:
+      __ Ror(i.OutputRegister(), i.InputRegister(0), i.InputOperand(1));
+      break;
     case kRiscvTst:
       __ And(kScratchReg, i.InputRegister(0), i.InputOperand(1));
       // Pseudo-instruction used for cmp/branch. No opcode emitted here.
@@ -1268,6 +1331,74 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       // compare result set to kScratchReg
       __ CompareF64(kScratchReg, cc, left, right);
     } break;
+#if V8_TARGET_ARCH_RISCV32
+    case kRiscvAddPair:
+      __ AddPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),
+                 kScratchReg, kScratchReg2);
+      break;
+    case kRiscvSubPair:
+      __ SubPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),
+                 kScratchReg, kScratchReg2);
+      break;
+    case kRiscvAndPair:
+      __ AndPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));
+      break;
+    case kRiscvOrPair:
+      __ OrPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));
+      break;
+    case kRiscvXorPair:
+      __ XorPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3));
+      break;
+    case kRiscvMulPair:
+      __ MulPair(i.OutputRegister(0), i.OutputRegister(1), i.InputRegister(0),
+                 i.InputRegister(1), i.InputRegister(2), i.InputRegister(3),
+                 kScratchReg, kScratchReg2);
+      break;
+    case kRiscvShlPair: {
+      Register second_output =
+          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);
+      if (instr->InputAt(2)->IsRegister()) {
+        __ ShlPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), i.InputRegister(2), kScratchReg,
+                   kScratchReg2);
+      } else {
+        uint32_t imm = i.InputOperand(2).immediate();
+        __ ShlPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);
+      }
+    } break;
+    case kRiscvShrPair: {
+      Register second_output =
+          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);
+      if (instr->InputAt(2)->IsRegister()) {
+        __ ShrPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), i.InputRegister(2), kScratchReg,
+                   kScratchReg2);
+      } else {
+        uint32_t imm = i.InputOperand(2).immediate();
+        __ ShrPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);
+      }
+    } break;
+    case kRiscvSarPair: {
+      Register second_output =
+          instr->OutputCount() >= 2 ? i.OutputRegister(1) : i.TempRegister(0);
+      if (instr->InputAt(2)->IsRegister()) {
+        __ SarPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), i.InputRegister(2), kScratchReg,
+                   kScratchReg2);
+      } else {
+        uint32_t imm = i.InputOperand(2).immediate();
+        __ SarPair(i.OutputRegister(0), second_output, i.InputRegister(0),
+                   i.InputRegister(1), imm, kScratchReg, kScratchReg2);
+      }
+    } break;
+#endif
     case kRiscvAddD:
       // TODO(plind): add special case: combine mult & add.
       __ fadd_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
@@ -1316,29 +1447,36 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ fmin_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
                 i.InputDoubleRegister(1));
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvFloat64RoundDown: {
       __ Floor_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
                    kScratchDoubleReg);
       break;
     }
-    case kRiscvFloat32RoundDown: {
-      __ Floor_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),
+    case kRiscvFloat64RoundTruncate: {
+      __ Trunc_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
                    kScratchDoubleReg);
       break;
     }
-    case kRiscvFloat64RoundTruncate: {
-      __ Trunc_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
+    case kRiscvFloat64RoundUp: {
+      __ Ceil_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
+                  kScratchDoubleReg);
+      break;
+    }
+    case kRiscvFloat64RoundTiesEven: {
+      __ Round_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
                    kScratchDoubleReg);
       break;
     }
-    case kRiscvFloat32RoundTruncate: {
-      __ Trunc_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),
+#endif
+    case kRiscvFloat32RoundDown: {
+      __ Floor_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),
                    kScratchDoubleReg);
       break;
     }
-    case kRiscvFloat64RoundUp: {
-      __ Ceil_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
-                  kScratchDoubleReg);
+    case kRiscvFloat32RoundTruncate: {
+      __ Trunc_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),
+                   kScratchDoubleReg);
       break;
     }
     case kRiscvFloat32RoundUp: {
@@ -1346,11 +1484,6 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                   kScratchDoubleReg);
       break;
     }
-    case kRiscvFloat64RoundTiesEven: {
-      __ Round_d_d(i.OutputDoubleRegister(), i.InputDoubleRegister(0),
-                   kScratchDoubleReg);
-      break;
-    }
     case kRiscvFloat32RoundTiesEven: {
       __ Round_s_s(i.OutputSingleRegister(), i.InputSingleRegister(0),
                    kScratchDoubleReg);
@@ -1397,6 +1530,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ Cvt_s_uw(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvCvtSL: {
       __ fcvt_s_l(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
@@ -1405,10 +1539,6 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ fcvt_d_l(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
     }
-    case kRiscvCvtDUw: {
-      __ Cvt_d_uw(i.OutputDoubleRegister(), i.InputRegister(0));
-      break;
-    }
     case kRiscvCvtDUl: {
       __ Cvt_d_ul(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
@@ -1417,6 +1547,11 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ Cvt_s_ul(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
     }
+#endif
+    case kRiscvCvtDUw: {
+      __ Cvt_d_uw(i.OutputDoubleRegister(), i.InputRegister(0));
+      break;
+    }
     case kRiscvFloorWD: {
       Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
       __ Floor_w_d(i.OutputRegister(), i.InputDoubleRegister(0), result);
@@ -1481,6 +1616,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvTruncLS: {
       Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
       __ Trunc_l_s(i.OutputRegister(), i.InputDoubleRegister(0), result);
@@ -1492,13 +1628,14 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       bool set_overflow_to_min_i64 = MiscField::decode(instr->opcode());
       __ Trunc_l_d(i.OutputRegister(), i.InputDoubleRegister(0), result);
       if (set_overflow_to_min_i64) {
-        __ Add64(kScratchReg, i.OutputRegister(), 1);
+        __ AddWord(kScratchReg, i.OutputRegister(), 1);
         __ BranchShort(&done, lt, i.OutputRegister(), Operand(kScratchReg));
         __ Move(i.OutputRegister(), kScratchReg);
         __ bind(&done);
       }
       break;
     }
+#endif
     case kRiscvTruncUwD: {
       Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
       __ Trunc_uw_d(i.OutputRegister(), i.InputDoubleRegister(0), result);
@@ -1527,6 +1664,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       }
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvTruncUlS: {
       Register result = instr->OutputCount() > 1 ? i.OutputRegister(1) : no_reg;
       __ Trunc_ul_s(i.OutputRegister(), i.InputDoubleRegister(0), result);
@@ -1543,6 +1681,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvBitcastLD:
       __ fmv_d_x(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
+#endif
     case kRiscvBitcastInt32ToFloat32:
       __ fmv_w_x(i.OutputDoubleRegister(), i.InputRegister(0));
       break;
@@ -1602,6 +1741,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvUlw:
       __ Ulw(i.OutputRegister(), i.MemoryOperand());
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvLwu:
       __ Lwu(i.OutputRegister(), i.MemoryOperand());
       break;
@@ -1614,18 +1754,19 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvUld:
       __ Uld(i.OutputRegister(), i.MemoryOperand());
       break;
-    case kRiscvSw:
-      __ Sw(i.InputOrZeroRegister(2), i.MemoryOperand());
-      break;
-    case kRiscvUsw:
-      __ Usw(i.InputOrZeroRegister(2), i.MemoryOperand());
-      break;
     case kRiscvSd:
       __ Sd(i.InputOrZeroRegister(2), i.MemoryOperand());
       break;
     case kRiscvUsd:
       __ Usd(i.InputOrZeroRegister(2), i.MemoryOperand());
       break;
+#endif
+    case kRiscvSw:
+      __ Sw(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
+    case kRiscvUsw:
+      __ Usw(i.InputOrZeroRegister(2), i.MemoryOperand());
+      break;
     case kRiscvLoadFloat: {
       __ LoadFloat(i.OutputSingleRegister(), i.MemoryOperand());
       break;
@@ -1705,12 +1846,12 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
               MemOperand(fp, offset + kLessSignificantWordInDoublewordOffset));
         }
       } else {
-        __ Ld(i.OutputRegister(0), MemOperand(fp, offset));
+        __ LoadWord(i.OutputRegister(0), MemOperand(fp, offset));
       }
       break;
     }
     case kRiscvStackClaim: {
-      __ Sub64(sp, sp, Operand(i.InputInt32(0)));
+      __ SubWord(sp, sp, Operand(i.InputInt32(0)));
       frame_access_state()->IncreaseSPDelta(i.InputInt32(0) /
                                             kSystemPointerSize);
       break;
@@ -1721,36 +1862,52 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           Register dst = sp;
           if (i.InputInt32(1) != 0) {
             dst = kScratchReg2;
-            __ Add64(kScratchReg2, sp, Operand(i.InputInt32(1)));
+            __ AddWord(kScratchReg2, sp, Operand(i.InputInt32(1)));
           }
           __ VU.set(kScratchReg, E8, m1);
           __ vs(i.InputSimd128Register(0), dst, 0, E8);
         } else {
+#if V8_TARGET_ARCH_RISCV64
           __ StoreDouble(i.InputDoubleRegister(0),
                          MemOperand(sp, i.InputInt32(1)));
+#elif V8_TARGET_ARCH_RISCV32
+          if (instr->InputAt(0)->IsDoubleRegister()) {
+            __ StoreDouble(i.InputDoubleRegister(0),
+                           MemOperand(sp, i.InputInt32(1)));
+          } else if (instr->InputAt(0)->IsFloatRegister()) {
+            __ StoreFloat(i.InputSingleRegister(0),
+                          MemOperand(sp, i.InputInt32(1)));
+          }
+#endif
         }
       } else {
-        __ Sd(i.InputOrZeroRegister(0), MemOperand(sp, i.InputInt32(1)));
+        __ StoreWord(i.InputOrZeroRegister(0), MemOperand(sp, i.InputInt32(1)));
       }
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvByteSwap64: {
       __ ByteSwap(i.OutputRegister(0), i.InputRegister(0), 8, kScratchReg);
       break;
     }
+#endif
     case kRiscvByteSwap32: {
       __ ByteSwap(i.OutputRegister(0), i.InputRegister(0), 4, kScratchReg);
       break;
     }
     case kAtomicLoadInt8:
+#if V8_TARGET_ARCH_RISCV64
       DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);
+#endif
       ASSEMBLE_ATOMIC_LOAD_INTEGER(Lb);
       break;
     case kAtomicLoadUint8:
       ASSEMBLE_ATOMIC_LOAD_INTEGER(Lbu);
       break;
     case kAtomicLoadInt16:
+#if V8_TARGET_ARCH_RISCV64
       DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);
+#endif
       ASSEMBLE_ATOMIC_LOAD_INTEGER(Lh);
       break;
     case kAtomicLoadUint16:
@@ -1759,9 +1916,14 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kAtomicLoadWord32:
       ASSEMBLE_ATOMIC_LOAD_INTEGER(Lw);
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvWord64AtomicLoadUint64:
       ASSEMBLE_ATOMIC_LOAD_INTEGER(Ld);
       break;
+    case kRiscvWord64AtomicStoreWord64:
+      ASSEMBLE_ATOMIC_STORE_INTEGER(Sd);
+      break;
+#endif
     case kAtomicStoreWord8:
       ASSEMBLE_ATOMIC_STORE_INTEGER(Sb);
       break;
@@ -1771,9 +1933,61 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kAtomicStoreWord32:
       ASSEMBLE_ATOMIC_STORE_INTEGER(Sw);
       break;
-    case kRiscvWord64AtomicStoreWord64:
-      ASSEMBLE_ATOMIC_STORE_INTEGER(Sd);
+#if V8_TARGET_ARCH_RISCV32
+    case kRiscvWord32AtomicPairLoad: {
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));
+      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      __ PrepareCallCFunction(1, 0, kScratchReg);
+      __ CallCFunction(ExternalReference::atomic_pair_load_function(), 1, 0);
+      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      break;
+    }
+    case kRiscvWord32AtomicPairStore: {
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));
+      __ PushCallerSaved(SaveFPRegsMode::kIgnore);
+      __ PrepareCallCFunction(3, 0, kScratchReg);
+      __ CallCFunction(ExternalReference::atomic_pair_store_function(), 3, 0);
+      __ PopCallerSaved(SaveFPRegsMode::kIgnore);
       break;
+    }
+#define ATOMIC64_BINOP_ARITH_CASE(op, instr, external) \
+  case kRiscvWord32AtomicPair##op:                     \
+    ASSEMBLE_ATOMIC64_ARITH_BINOP(instr, external);    \
+    break;
+      ATOMIC64_BINOP_ARITH_CASE(Add, AddPair, atomic_pair_add_function)
+      ATOMIC64_BINOP_ARITH_CASE(Sub, SubPair, atomic_pair_sub_function)
+#undef ATOMIC64_BINOP_ARITH_CASE
+#define ATOMIC64_BINOP_LOGIC_CASE(op, instr, external) \
+  case kRiscvWord32AtomicPair##op:                     \
+    ASSEMBLE_ATOMIC64_LOGIC_BINOP(instr, external);    \
+    break;
+      ATOMIC64_BINOP_LOGIC_CASE(And, AndPair, atomic_pair_and_function)
+      ATOMIC64_BINOP_LOGIC_CASE(Or, OrPair, atomic_pair_or_function)
+      ATOMIC64_BINOP_LOGIC_CASE(Xor, XorPair, atomic_pair_xor_function)
+    case kRiscvWord32AtomicPairExchange: {
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      __ PrepareCallCFunction(3, 0, kScratchReg);
+      __ AddWord(a0, i.InputRegister(0), i.InputRegister(1));
+      __ CallCFunction(ExternalReference::atomic_pair_exchange_function(), 3,
+                       0);
+      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      break;
+    }
+    case kRiscvWord32AtomicPairCompareExchange: {
+      FrameScope scope(tasm(), StackFrame::MANUAL);
+      __ PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      __ PrepareCallCFunction(5, 0, kScratchReg);
+      __ add(a0, i.InputRegister(0), i.InputRegister(1));
+      __ Sw(i.InputRegister(5), MemOperand(sp, 16));
+      __ CallCFunction(
+          ExternalReference::atomic_pair_compare_exchange_function(), 5, 0);
+      __ PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+      break;
+    }
+#endif
     case kAtomicExchangeInt8:
       DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);
       ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 8, 32);
@@ -1784,8 +1998,12 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 8, 32);
           break;
         case AtomicWidth::kWord64:
+#if V8_TARGET_ARCH_RISCV64
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 8, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
     case kAtomicExchangeInt16:
@@ -1797,9 +2015,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         case AtomicWidth::kWord32:
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 16, 32);
           break;
+#if V8_TARGET_ARCH_RISCV64
         case AtomicWidth::kWord64:
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 16, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
     case kAtomicExchangeWord32:
@@ -1807,14 +2029,20 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         case AtomicWidth::kWord32:
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(Ll, Sc);
           break;
+#if V8_TARGET_ARCH_RISCV64
         case AtomicWidth::kWord64:
           ASSEMBLE_ATOMIC_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 32, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvWord64AtomicExchangeUint64:
       ASSEMBLE_ATOMIC_EXCHANGE_INTEGER(Lld, Scd);
       break;
+#endif
     case kAtomicCompareExchangeInt8:
       DCHECK_EQ(AtomicWidthField::decode(opcode), AtomicWidth::kWord32);
       ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, true, 8, 32);
@@ -1824,9 +2052,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         case AtomicWidth::kWord32:
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 8, 32);
           break;
+#if V8_TARGET_ARCH_RISCV64
         case AtomicWidth::kWord64:
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 8, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
     case kAtomicCompareExchangeInt16:
@@ -1838,9 +2070,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
         case AtomicWidth::kWord32:
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, false, 16, 32);
           break;
+#if V8_TARGET_ARCH_RISCV64
         case AtomicWidth::kWord64:
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 16, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
     case kAtomicCompareExchangeWord32:
@@ -1849,11 +2085,16 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           __ Sll32(i.InputRegister(2), i.InputRegister(2), 0);
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Ll, Sc);
           break;
+#if V8_TARGET_ARCH_RISCV64
         case AtomicWidth::kWord64:
           ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, false, 32, 64);
           break;
+#endif
+        default:
+          UNREACHABLE();
       }
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvWord64AtomicCompareExchangeUint64:
       ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Lld, Scd);
       break;
@@ -1899,16 +2140,43 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
   case kRiscvWord64Atomic##op##Uint64:                                 \
     ASSEMBLE_ATOMIC_BINOP(Lld, Scd, inst64);                           \
     break;
-      ATOMIC_BINOP_CASE(Add, Add32, Add64)
+      ATOMIC_BINOP_CASE(Add, Add32, AddWord)
       ATOMIC_BINOP_CASE(Sub, Sub32, Sub64)
       ATOMIC_BINOP_CASE(And, And, And)
       ATOMIC_BINOP_CASE(Or, Or, Or)
       ATOMIC_BINOP_CASE(Xor, Xor, Xor)
 #undef ATOMIC_BINOP_CASE
+#elif V8_TARGET_ARCH_RISCV32
+#define ATOMIC_BINOP_CASE(op, inst32, inst64, amoinst32)                   \
+  case kAtomic##op##Int8:                                                  \
+    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 8, inst32, 32);                \
+    break;                                                                 \
+  case kAtomic##op##Uint8:                                                 \
+    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 8, inst32, 32);               \
+    break;                                                                 \
+  case kAtomic##op##Int16:                                                 \
+    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, true, 16, inst32, 32);               \
+    break;                                                                 \
+  case kAtomic##op##Uint16:                                                \
+    ASSEMBLE_ATOMIC_BINOP_EXT(Ll, Sc, false, 16, inst32, 32);              \
+    break;                                                                 \
+  case kAtomic##op##Word32:                                                \
+    __ AddWord(i.TempRegister(0), i.InputRegister(0), i.InputRegister(1)); \
+    __ amoinst32(true, true, i.TempRegister(1), i.TempRegister(0),         \
+                 i.InputRegister(2));                                      \
+    break;
+      ATOMIC_BINOP_CASE(Add, Add32, Add64, amoadd_w)  // todo: delete 64
+      ATOMIC_BINOP_CASE(Sub, Sub32, Sub64, Amosub_w)  // todo: delete 64
+      ATOMIC_BINOP_CASE(And, And, And, amoand_w)
+      ATOMIC_BINOP_CASE(Or, Or, Or, amoor_w)
+      ATOMIC_BINOP_CASE(Xor, Xor, Xor, amoxor_w)
+#undef ATOMIC_BINOP_CASE
+#endif
     case kRiscvAssertEqual:
       __ Assert(eq, static_cast<AbortReason>(i.InputOperand(2).immediate()),
                 i.InputRegister(0), Operand(i.InputRegister(1)));
       break;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvStoreCompressTagged: {
       size_t index = 0;
       MemOperand operand = i.MemoryOperand(&index);
@@ -1936,12 +2204,13 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ DecompressAnyTagged(result, operand);
       break;
     }
+#endif
     case kRiscvRvvSt: {
       (__ VU).set(kScratchReg, VSew::E8, Vlmul::m1);
       Register dst = i.MemoryOperand().offset() == 0 ? i.MemoryOperand().rm()
                                                      : kScratchReg;
       if (i.MemoryOperand().offset() != 0) {
-        __ Add64(dst, i.MemoryOperand().rm(), i.MemoryOperand().offset());
+        __ AddWord(dst, i.MemoryOperand().rm(), i.MemoryOperand().offset());
       }
       __ vs(i.InputSimd128Register(2), dst, 0, VSew::E8);
       break;
@@ -1951,7 +2220,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       Register src = i.MemoryOperand().offset() == 0 ? i.MemoryOperand().rm()
                                                      : kScratchReg;
       if (i.MemoryOperand().offset() != 0) {
-        __ Add64(src, i.MemoryOperand().rm(), i.MemoryOperand().offset());
+        __ AddWord(src, i.MemoryOperand().rm(), i.MemoryOperand().offset());
       }
       __ vl(i.OutputSimd128Register(), src, 0, VSew::E8);
       break;
@@ -1965,14 +2234,14 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kRiscvS128Load32Zero: {
       Simd128Register dst = i.OutputSimd128Register();
       __ VU.set(kScratchReg, E32, m1);
-      __ Lwu(kScratchReg, i.MemoryOperand());
+      __ Load32U(kScratchReg, i.MemoryOperand());
       __ vmv_sx(dst, kScratchReg);
       break;
     }
     case kRiscvS128Load64Zero: {
       Simd128Register dst = i.OutputSimd128Register();
       __ VU.set(kScratchReg, E64, m1);
-      __ Ld(kScratchReg, i.MemoryOperand());
+      __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_sx(dst, kScratchReg);
       break;
     }
@@ -1992,7 +2261,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kRiscvS128Load64ExtendS: {
       __ VU.set(kScratchReg, E64, m1);
-      __ Ld(kScratchReg, i.MemoryOperand());
+      __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_vx(kSimd128ScratchReg, kScratchReg);
       __ VU.set(kScratchReg, i.InputInt8(2), m1);
       __ vsext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);
@@ -2000,7 +2269,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kRiscvS128Load64ExtendU: {
       __ VU.set(kScratchReg, E64, m1);
-      __ Ld(kScratchReg, i.MemoryOperand());
+      __ LoadWord(kScratchReg, i.MemoryOperand());
       __ vmv_vx(kSimd128ScratchReg, kScratchReg);
       __ VU.set(kScratchReg, i.InputInt8(2), m1);
       __ vzext_vf2(i.OutputSimd128Register(), kSimd128ScratchReg);
@@ -2019,7 +2288,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
           __ Lw(kScratchReg, i.MemoryOperand());
           break;
         case E64:
-          __ Ld(kScratchReg, i.MemoryOperand());
+          __ LoadWord(kScratchReg, i.MemoryOperand());
           break;
         default:
           UNREACHABLE();
@@ -2806,12 +3075,14 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vfsqrt_v(i.OutputSimd128Register(), i.InputSimd128Register(0));
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvF64x2Splat: {
       (__ VU).set(kScratchReg, E64, m1);
       __ fmv_x_d(kScratchReg, i.InputDoubleRegister(0));
       __ vmv_vx(i.OutputSimd128Register(), kScratchReg);
       break;
     }
+#endif
     case kRiscvF64x2Abs: {
       __ VU.set(kScratchReg, VSew::E64, Vlmul::m1);
       __ vfabs_vv(i.OutputSimd128Register(), i.InputSimd128Register(0));
@@ -2858,6 +3129,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vmerge_vi(i.OutputSimd128Register(), -1, i.OutputSimd128Register());
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvF64x2ReplaceLane: {
       __ VU.set(kScratchReg, E64, m1);
       __ li(kScratchReg, 0x1 << i.InputInt8(1));
@@ -2867,6 +3139,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                    i.InputSimd128Register(0));
       break;
     }
+#endif
     case kRiscvF64x2Lt: {
       __ VU.set(kScratchReg, E64, m1);
       __ vmflt_vv(v0, i.InputSimd128Register(0), i.InputSimd128Register(1));
@@ -2895,6 +3168,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
                    i.InputSimd128Register(0));
       break;
     }
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvF64x2Min: {
       __ VU.set(kScratchReg, E64, m1);
       const int64_t kNaN = 0x7ff8000000000000L;
@@ -2923,6 +3197,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
       __ vmv_vv(i.OutputSimd128Register(), kSimd128ScratchReg);
       break;
     }
+#endif
     case kRiscvF64x2Div: {
       __ VU.set(kScratchReg, E64, m1);
       __ vfdiv_vv(i.OutputSimd128Register(), i.InputSimd128Register(0),
@@ -3449,6 +3724,7 @@ void AssembleBranchToLabels(CodeGenerator* gen, TurboAssembler* tasm,
   if (instr->arch_opcode() == kRiscvTst) {
     Condition cc = FlagsConditionToConditionTst(condition);
     __ Branch(tlabel, cc, kScratchReg, Operand(zero_reg));
+#if V8_TARGET_ARCH_RISCV64
   } else if (instr->arch_opcode() == kRiscvAdd64 ||
              instr->arch_opcode() == kRiscvSub64) {
     Condition cc = FlagsConditionToConditionOvf(condition);
@@ -3457,6 +3733,10 @@ void AssembleBranchToLabels(CodeGenerator* gen, TurboAssembler* tasm,
     __ Branch(tlabel, cc, kScratchReg2, Operand(kScratchReg));
   } else if (instr->arch_opcode() == kRiscvAddOvf64 ||
              instr->arch_opcode() == kRiscvSubOvf64) {
+#elif V8_TARGET_ARCH_RISCV32
+  } else if (instr->arch_opcode() == kRiscvAddOvf ||
+             instr->arch_opcode() == kRiscvSubOvf) {
+#endif
     switch (condition) {
       // Overflow occurs if overflow register is negative
       case kOverflow:
@@ -3496,7 +3776,7 @@ void AssembleBranchToLabels(CodeGenerator* gen, TurboAssembler* tasm,
     uint32_t offset;
     if (gen->ShouldApplyOffsetToStackCheck(instr, &offset)) {
       lhs_register = i.TempRegister(0);
-      __ Sub64(lhs_register, sp, offset);
+      __ SubWord(lhs_register, sp, offset);
     }
     __ Branch(tlabel, cc, lhs_register, Operand(i.InputRegister(0)));
   } else if (instr->arch_opcode() == kRiscvCmpS ||
@@ -3612,6 +3892,7 @@ void CodeGenerator::AssembleArchBoolean(Instruction* instr,
       __ Sltu(result, zero_reg, kScratchReg);
     }
     return;
+#if V8_TARGET_ARCH_RISCV64
   } else if (instr->arch_opcode() == kRiscvAdd64 ||
              instr->arch_opcode() == kRiscvSub64) {
     Condition cc = FlagsConditionToConditionOvf(condition);
@@ -3624,6 +3905,10 @@ void CodeGenerator::AssembleArchBoolean(Instruction* instr,
     return;
   } else if (instr->arch_opcode() == kRiscvAddOvf64 ||
              instr->arch_opcode() == kRiscvSubOvf64) {
+#elif V8_TARGET_ARCH_RISCV32
+  } else if (instr->arch_opcode() == kRiscvAddOvf ||
+             instr->arch_opcode() == kRiscvSubOvf) {
+#endif
     // Overflow occurs if overflow register is negative
     __ Slt(result, kScratchReg, zero_reg);
   } else if (instr->arch_opcode() == kRiscvMulOvf32) {
@@ -3645,7 +3930,7 @@ void CodeGenerator::AssembleArchBoolean(Instruction* instr,
                 __ Sltu(result, zero_reg, left);
               }
             } else {
-              __ Add64(result, left, Operand(-right.immediate()));
+              __ AddWord(result, left, Operand(-right.immediate()));
               if (cc == eq) {
                 __ Sltu(result, result, 1);
               } else {
@@ -3771,7 +4056,7 @@ void CodeGenerator::AssembleArchBoolean(Instruction* instr,
     uint32_t offset;
     if (ShouldApplyOffsetToStackCheck(instr, &offset)) {
       lhs_register = i.TempRegister(0);
-      __ Sub64(lhs_register, sp, offset);
+      __ SubWord(lhs_register, sp, offset);
     }
     __ Sgtu(result, lhs_register, Operand(i.InputRegister(0)));
     return;
@@ -3855,7 +4140,7 @@ void CodeGenerator::AssembleConstructFrame() {
       if (info()->GetOutputStackFrameType() == StackFrame::C_WASM_ENTRY) {
         __ StubPrologue(StackFrame::C_WASM_ENTRY);
         // Reserve stack space for saving the c_entry_fp later.
-        __ Sub64(sp, sp, Operand(kSystemPointerSize));
+        __ SubWord(sp, sp, Operand(kSystemPointerSize));
       } else {
         __ Push(ra, fp);
         __ Move(fp, sp);
@@ -3871,7 +4156,7 @@ void CodeGenerator::AssembleConstructFrame() {
       }
       if (call_descriptor->IsWasmCapiFunction()) {
         // Reserve space for saving the PC later.
-        __ Sub64(sp, sp, Operand(kSystemPointerSize));
+        __ SubWord(sp, sp, Operand(kSystemPointerSize));
       }
     }
   }
@@ -3908,13 +4193,13 @@ void CodeGenerator::AssembleConstructFrame() {
       // exception unconditionally. Thereby we can avoid the integer overflow
       // check in the condition code.
       if ((required_slots * kSystemPointerSize) < (FLAG_stack_size * 1024)) {
-        __ Ld(
+        __ LoadWord(
             kScratchReg,
             FieldMemOperand(kWasmInstanceRegister,
                             WasmInstanceObject::kRealStackLimitAddressOffset));
-        __ Ld(kScratchReg, MemOperand(kScratchReg));
-        __ Add64(kScratchReg, kScratchReg,
-                 Operand(required_slots * kSystemPointerSize));
+        __ LoadWord(kScratchReg, MemOperand(kScratchReg));
+        __ AddWord(kScratchReg, kScratchReg,
+                   Operand(required_slots * kSystemPointerSize));
         __ BranchShort(&done, uge, sp, Operand(kScratchReg));
       }
 
@@ -3934,10 +4219,10 @@ void CodeGenerator::AssembleConstructFrame() {
 
   // Skip callee-saved and return slots, which are pushed below.
   required_slots -= saves.Count();
-  required_slots -= saves_fpu.Count();
+  required_slots -= saves_fpu.Count() * (kDoubleSize / kSystemPointerSize);
   required_slots -= returns;
   if (required_slots > 0) {
-    __ Sub64(sp, sp, Operand(required_slots * kSystemPointerSize));
+    __ SubWord(sp, sp, Operand(required_slots * kSystemPointerSize));
   }
 
   if (!saves_fpu.is_empty()) {
@@ -3953,7 +4238,7 @@ void CodeGenerator::AssembleConstructFrame() {
 
   if (returns != 0) {
     // Create space for returns.
-    __ Sub64(sp, sp, Operand(returns * kSystemPointerSize));
+    __ SubWord(sp, sp, Operand(returns * kSystemPointerSize));
   }
 }
 
@@ -3962,7 +4247,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
 
   const int returns = frame()->GetReturnSlotCount();
   if (returns != 0) {
-    __ Add64(sp, sp, Operand(returns * kSystemPointerSize));
+    __ AddWord(sp, sp, Operand(returns * kSystemPointerSize));
   }
 
   // Restore GP registers.
@@ -3990,7 +4275,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
     } else if (FLAG_debug_code) {
       __ Assert(eq, AbortReason::kUnexpectedAdditionalPopValue,
                 g.ToRegister(additional_pop_count),
-                Operand(static_cast<int64_t>(0)));
+                Operand(static_cast<intptr_t>(0)));
     }
   }
 
@@ -4018,7 +4303,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
     }
     if (drop_jsargs) {
       // Get the actual argument count
-      __ Ld(t0, MemOperand(fp, StandardFrameConstants::kArgCOffset));
+      __ LoadWord(t0, MemOperand(fp, StandardFrameConstants::kArgCOffset));
     }
     AssembleDeconstructFrame();
   }
@@ -4032,8 +4317,8 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
       __ Move(t0, kScratchReg);
       __ bind(&done);
     }
-    __ Sll64(t0, t0, kSystemPointerSizeLog2);
-    __ Add64(sp, sp, t0);
+    __ SllWord(t0, t0, kSystemPointerSizeLog2);
+    __ AddWord(sp, sp, t0);
   } else if (additional_pop_count->IsImmediate()) {
     // it should be a kInt32 or a kInt64
     DCHECK_LE(g.ToConstant(additional_pop_count).type(), Constant::kInt64);
@@ -4042,8 +4327,8 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   } else {
     Register pop_reg = g.ToRegister(additional_pop_count);
     __ Drop(parameter_slots);
-    __ Sll64(pop_reg, pop_reg, kSystemPointerSizeLog2);
-    __ Add64(sp, sp, pop_reg);
+    __ SllWord(pop_reg, pop_reg, kSystemPointerSizeLog2);
+    __ AddWord(sp, sp, pop_reg);
   }
   __ Ret();
 }
@@ -4081,9 +4366,13 @@ void CodeGenerator::MoveToTempLocation(InstructionOperand* source) {
     RiscvOperandConverter g(this, nullptr);
     if (source->IsRegister()) {
       __ Push(g.ToRegister(source));
+#if V8_TARGET_ARCH_RISCV64
     } else if (source->IsStackSlot() || source->IsFloatStackSlot() ||
                source->IsDoubleStackSlot()) {
-      __ Ld(kScratchReg, g.ToMemOperand(source));
+#elif V8_TARGET_ARCH_RISCV32
+    } else if (source->IsStackSlot() || source->IsFloatStackSlot()) {
+#endif
+      __ LoadWord(kScratchReg, g.ToMemOperand(source));
       __ Push(kScratchReg);
     } else {
       // Bump the stack pointer and assemble the move.
@@ -4091,7 +4380,7 @@ void CodeGenerator::MoveToTempLocation(InstructionOperand* source) {
           frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;
       int sp_delta = frame_access_state_->sp_delta();
       int temp_slot = last_frame_slot_id + sp_delta + new_slots;
-      __ Sub64(sp, sp, Operand(new_slots * kSystemPointerSize));
+      __ SubWord(sp, sp, Operand(new_slots * kSystemPointerSize));
       AllocatedOperand temp(LocationOperand::STACK_SLOT, rep, temp_slot);
       AssembleMove(source, &temp);
     }
@@ -4114,10 +4403,14 @@ void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,
     frame_access_state()->IncreaseSPDelta(-new_slots);
     if (dest->IsRegister()) {
       __ Pop(g.ToRegister(dest));
+#if V8_TARGET_ARCH_RISCV64
     } else if (dest->IsStackSlot() || dest->IsFloatStackSlot() ||
                dest->IsDoubleStackSlot()) {
+#elif V8_TARGET_ARCH_RISCV32
+    } else if (dest->IsStackSlot() || dest->IsFloatStackSlot()) {
+#endif
       __ Pop(kScratchReg);
-      __ Sd(kScratchReg, g.ToMemOperand(dest));
+      __ StoreWord(kScratchReg, g.ToMemOperand(dest));
     } else {
       int last_frame_slot_id =
           frame_access_state_->frame()->GetTotalFrameSlotCount() - 1;
@@ -4125,7 +4418,7 @@ void CodeGenerator::MoveTempLocationTo(InstructionOperand* dest,
       int temp_slot = last_frame_slot_id + sp_delta + new_slots;
       AllocatedOperand temp(LocationOperand::STACK_SLOT, rep, temp_slot);
       AssembleMove(&temp, dest);
-      __ Add64(sp, sp, Operand(new_slots * kSystemPointerSize));
+      __ AddWord(sp, sp, Operand(new_slots * kSystemPointerSize));
     }
   }
   move_cycle_ = MoveCycleState();
@@ -4162,17 +4455,17 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
     if (destination->IsRegister()) {
       __ Move(g.ToRegister(destination), src);
     } else {
-      __ Sd(src, g.ToMemOperand(destination));
+      __ StoreWord(src, g.ToMemOperand(destination));
     }
   } else if (source->IsStackSlot()) {
     DCHECK(destination->IsRegister() || destination->IsStackSlot());
     MemOperand src = g.ToMemOperand(source);
     if (destination->IsRegister()) {
-      __ Ld(g.ToRegister(destination), src);
+      __ LoadWord(g.ToRegister(destination), src);
     } else {
       Register temp = kScratchReg;
-      __ Ld(temp, src);
-      __ Sd(temp, g.ToMemOperand(destination));
+      __ LoadWord(temp, src);
+      __ StoreWord(temp, g.ToMemOperand(destination));
     }
   } else if (source->IsConstant()) {
     Constant src = g.ToConstant(source);
@@ -4181,7 +4474,9 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
           destination->IsRegister() ? g.ToRegister(destination) : kScratchReg;
       switch (src.type()) {
         case Constant::kInt32:
-          if (src.ToInt32() == 0 && destination->IsStackSlot()) {
+          if (RelocInfo::IsWasmReference(src.rmode())) {
+            __ li(dst, Operand(src.ToInt32(), src.rmode()));
+          } else if (src.ToInt32() == 0 && destination->IsStackSlot()) {
             dst = zero_reg;
           } else {
             __ li(dst, Operand(src.ToInt32()));
@@ -4233,7 +4528,8 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
         case Constant::kRpoNumber:
           UNREACHABLE();  // TODO(titzer): loading RPO numbers
       }
-      if (destination->IsStackSlot()) __ Sd(dst, g.ToMemOperand(destination));
+      if (destination->IsStackSlot())
+        __ StoreWord(dst, g.ToMemOperand(destination));
     } else if (src.type() == Constant::kFloat32) {
       if (destination->IsFPStackSlot()) {
         MemOperand dst = g.ToMemOperand(destination);
@@ -4273,7 +4569,7 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
         Register dst_r = dst.rm();
         if (dst.offset() != 0) {
           dst_r = kScratchReg;
-          __ Add64(dst_r, dst.rm(), dst.offset());
+          __ AddWord(dst_r, dst.rm(), dst.offset());
         }
         __ vs(src, dst_r, 0, E8);
       }
@@ -4301,7 +4597,7 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
       Register src_r = src.rm();
       if (src.offset() != 0) {
         src_r = kScratchReg;
-        __ Add64(src_r, src.rm(), src.offset());
+        __ AddWord(src_r, src.rm(), src.offset());
       }
       if (destination->IsSimd128Register()) {
         __ vl(g.ToSimd128Register(destination), src_r, 0, E8);
@@ -4312,7 +4608,7 @@ void CodeGenerator::AssembleMove(InstructionOperand* source,
         Register dst_r = dst.rm();
         if (dst.offset() != 0) {
           dst_r = kScratchReg2;
-          __ Add64(dst_r, dst.rm(), dst.offset());
+          __ AddWord(dst_r, dst.rm(), dst.offset());
         }
         __ vl(temp, src_r, 0, E8);
         __ vs(temp, dst_r, 0, E8);
@@ -4374,15 +4670,15 @@ void CodeGenerator::AssembleSwap(InstructionOperand* source,
           __ vmv_vv(dst, temp);
         }
       }
-      return;
+      break;
     case MoveType::kRegisterToStack: {
       MemOperand dst = g.ToMemOperand(destination);
       if (source->IsRegister()) {
         Register temp = kScratchReg;
         Register src = g.ToRegister(source);
         __ mv(temp, src);
-        __ Ld(src, dst);
-        __ Sd(temp, dst);
+        __ LoadWord(src, dst);
+        __ StoreWord(temp, dst);
       } else {
         MemOperand dst = g.ToMemOperand(destination);
         if (source->IsFloatRegister()) {
@@ -4406,14 +4702,13 @@ void CodeGenerator::AssembleSwap(InstructionOperand* source,
           Register dst_v = dst.rm();
           if (dst.offset() != 0) {
             dst_v = kScratchReg2;
-            __ Add64(dst_v, dst.rm(), Operand(dst.offset()));
+            __ AddWord(dst_v, dst.rm(), Operand(dst.offset()));
           }
           __ vl(src, dst_v, 0, E8);
           __ vs(temp, dst_v, 0, E8);
         }
       }
-      return;
-    }
+    } break;
     case MoveType::kStackToStack: {
       MemOperand src = g.ToMemOperand(source);
       MemOperand dst = g.ToMemOperand(destination);
@@ -4423,27 +4718,46 @@ void CodeGenerator::AssembleSwap(InstructionOperand* source,
         Register dst_v = dst.rm();
         if (src.offset() != 0) {
           src_v = kScratchReg;
-          __ Add64(src_v, src.rm(), Operand(src.offset()));
+          __ AddWord(src_v, src.rm(), Operand(src.offset()));
         }
         if (dst.offset() != 0) {
           dst_v = kScratchReg2;
-          __ Add64(dst_v, dst.rm(), Operand(dst.offset()));
+          __ AddWord(dst_v, dst.rm(), Operand(dst.offset()));
         }
         __ vl(kSimd128ScratchReg, src_v, 0, E8);
         __ vl(kSimd128ScratchReg2, dst_v, 0, E8);
         __ vs(kSimd128ScratchReg, dst_v, 0, E8);
         __ vs(kSimd128ScratchReg2, src_v, 0, E8);
       } else {
+#if V8_TARGET_ARCH_RISCV32
+        if (source->IsFPStackSlot()) {
+          DCHECK(destination->IsFPStackSlot());
+          MachineRepresentation rep =
+              LocationOperand::cast(source)->representation();
+          if (rep == MachineRepresentation::kFloat64) {
+            FPURegister temp_double = kScratchDoubleReg;
+            Register temp_word32 = kScratchReg;
+            MemOperand src_hi(src.rm(), src.offset() + kSystemPointerSize);
+            MemOperand dst_hi(dst.rm(), dst.offset() + kSystemPointerSize);
+            __ LoadDouble(temp_double, src);
+            __ Lw(temp_word32, dst);
+            __ Sw(temp_word32, src);
+            __ Lw(temp_word32, dst_hi);
+            __ Sw(temp_word32, src_hi);
+            __ StoreDouble(temp_double, dst);
+            break;
+          }
+        }
+#endif
         UseScratchRegisterScope scope(tasm());
         Register temp_0 = kScratchReg;
         Register temp_1 = kScratchReg2;
-        __ Ld(temp_0, src);
-        __ Ld(temp_1, dst);
-        __ Sd(temp_0, dst);
-        __ Sd(temp_1, src);
+        __ LoadWord(temp_0, src);
+        __ LoadWord(temp_1, dst);
+        __ StoreWord(temp_0, dst);
+        __ StoreWord(temp_1, src);
       }
-      return;
-    }
+    } break;
     default:
       UNREACHABLE();
   }
diff --git a/src/compiler/backend/riscv/instruction-codes-riscv.h b/src/compiler/backend/riscv/instruction-codes-riscv.h
new file mode 100644
index 00000000000..6e2e69d8ce4
--- /dev/null
+++ b/src/compiler/backend/riscv/instruction-codes-riscv.h
@@ -0,0 +1,462 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_COMPILER_BACKEND_RISCV_INSTRUCTION_CODES_RISCV_H_
+#define V8_COMPILER_BACKEND_RISCV_INSTRUCTION_CODES_RISCV_H_
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+// RISC-V-specific opcodes that specify which assembly sequence to emit.
+// Most opcodes specify a single instruction.
+#if V8_TARGET_ARCH_RISCV64
+#define TARGET_ARCH_OPCODE_LIST_SPECAIL(V) \
+  V(RiscvAdd64)                            \
+  V(RiscvAddOvf64)                         \
+  V(RiscvSub64)                            \
+  V(RiscvSubOvf64)                         \
+  V(RiscvMulHigh64)                        \
+  V(RiscvMul64)                            \
+  V(RiscvDiv64)                            \
+  V(RiscvDivU64)                           \
+  V(RiscvMod64)                            \
+  V(RiscvModU64)                           \
+  V(RiscvZeroExtendWord)                   \
+  V(RiscvSignExtendWord)                   \
+  V(RiscvClz64)                            \
+  V(RiscvCtz64)                            \
+  V(RiscvPopcnt64)                         \
+  V(RiscvShl64)                            \
+  V(RiscvShr64)                            \
+  V(RiscvSar64)                            \
+  V(RiscvRor64)                            \
+  V(RiscvFloat64RoundDown)                 \
+  V(RiscvFloat64RoundTruncate)             \
+  V(RiscvFloat64RoundUp)                   \
+  V(RiscvFloat64RoundTiesEven)             \
+  V(RiscvTruncLS)                          \
+  V(RiscvTruncLD)                          \
+  V(RiscvTruncUlS)                         \
+  V(RiscvTruncUlD)                         \
+  V(RiscvCvtSL)                            \
+  V(RiscvCvtSUl)                           \
+  V(RiscvCvtDL)                            \
+  V(RiscvCvtDUl)                           \
+  V(RiscvLd)                               \
+  V(RiscvSd)                               \
+  V(RiscvUsd)                              \
+  V(RiscvLwu)                              \
+  V(RiscvUlwu)                             \
+  V(RiscvBitcastDL)                        \
+  V(RiscvBitcastLD)                        \
+  V(RiscvByteSwap64)                       \
+  V(RiscvWord64AtomicLoadUint64)           \
+  V(RiscvWord64AtomicStoreWord64)          \
+  V(RiscvWord64AtomicAddUint64)            \
+  V(RiscvWord64AtomicSubUint64)            \
+  V(RiscvWord64AtomicAndUint64)            \
+  V(RiscvWord64AtomicOrUint64)             \
+  V(RiscvWord64AtomicXorUint64)            \
+  V(RiscvWord64AtomicExchangeUint64)       \
+  V(RiscvStoreCompressTagged)              \
+  V(RiscvLoadDecompressTaggedSigned)       \
+  V(RiscvLoadDecompressTaggedPointer)      \
+  V(RiscvLoadDecompressAnyTagged)          \
+  V(RiscvWord64AtomicCompareExchangeUint64)
+#elif V8_TARGET_ARCH_RISCV32
+#define TARGET_ARCH_OPCODE_LIST_SPECAIL(V) \
+  V(RiscvAddOvf)                           \
+  V(RiscvSubOvf)                           \
+  V(RiscvAddPair)                          \
+  V(RiscvSubPair)                          \
+  V(RiscvMulPair)                          \
+  V(RiscvAndPair)                          \
+  V(RiscvOrPair)                           \
+  V(RiscvXorPair)                          \
+  V(RiscvShlPair)                          \
+  V(RiscvShrPair)                          \
+  V(RiscvSarPair)                          \
+  V(RiscvWord32AtomicPairLoad)             \
+  V(RiscvWord32AtomicPairStore)            \
+  V(RiscvWord32AtomicPairAdd)              \
+  V(RiscvWord32AtomicPairSub)              \
+  V(RiscvWord32AtomicPairAnd)              \
+  V(RiscvWord32AtomicPairOr)               \
+  V(RiscvWord32AtomicPairXor)              \
+  V(RiscvWord32AtomicPairExchange)         \
+  V(RiscvWord32AtomicPairCompareExchange)
+#endif
+
+#define TARGET_ARCH_OPCODE_LIST_COMMON(V) \
+  V(RiscvAdd32)                           \
+  V(RiscvSub32)                           \
+  V(RiscvMul32)                           \
+  V(RiscvMulOvf32)                        \
+  V(RiscvMulHigh32)                       \
+  V(RiscvMulHighU32)                      \
+  V(RiscvDiv32)                           \
+  V(RiscvDivU32)                          \
+  V(RiscvMod32)                           \
+  V(RiscvModU32)                          \
+  V(RiscvAnd)                             \
+  V(RiscvAnd32)                           \
+  V(RiscvOr)                              \
+  V(RiscvOr32)                            \
+  V(RiscvNor)                             \
+  V(RiscvNor32)                           \
+  V(RiscvXor)                             \
+  V(RiscvXor32)                           \
+  V(RiscvClz32)                           \
+  V(RiscvShl32)                           \
+  V(RiscvShr32)                           \
+  V(RiscvSar32)                           \
+  V(RiscvCtz32)                           \
+  V(RiscvPopcnt32)                        \
+  V(RiscvRor32)                           \
+  V(RiscvMov)                             \
+  V(RiscvTst)                             \
+  V(RiscvCmp)                             \
+  V(RiscvCmpZero)                         \
+  V(RiscvCmpS)                            \
+  V(RiscvAddS)                            \
+  V(RiscvSubS)                            \
+  V(RiscvMulS)                            \
+  V(RiscvDivS)                            \
+  V(RiscvModS)                            \
+  V(RiscvAbsS)                            \
+  V(RiscvNegS)                            \
+  V(RiscvSqrtS)                           \
+  V(RiscvMaxS)                            \
+  V(RiscvMinS)                            \
+  V(RiscvCmpD)                            \
+  V(RiscvAddD)                            \
+  V(RiscvSubD)                            \
+  V(RiscvMulD)                            \
+  V(RiscvDivD)                            \
+  V(RiscvModD)                            \
+  V(RiscvAbsD)                            \
+  V(RiscvNegD)                            \
+  V(RiscvSqrtD)                           \
+  V(RiscvMaxD)                            \
+  V(RiscvMinD)                            \
+  V(RiscvFloat32RoundDown)                \
+  V(RiscvFloat32RoundTruncate)            \
+  V(RiscvFloat32RoundUp)                  \
+  V(RiscvFloat32RoundTiesEven)            \
+  V(RiscvCvtSD)                           \
+  V(RiscvCvtDS)                           \
+  V(RiscvTruncWD)                         \
+  V(RiscvRoundWD)                         \
+  V(RiscvFloorWD)                         \
+  V(RiscvCeilWD)                          \
+  V(RiscvTruncWS)                         \
+  V(RiscvRoundWS)                         \
+  V(RiscvFloorWS)                         \
+  V(RiscvCeilWS)                          \
+  V(RiscvTruncUwD)                        \
+  V(RiscvTruncUwS)                        \
+  V(RiscvCvtDW)                           \
+  V(RiscvCvtSW)                           \
+  V(RiscvCvtSUw)                          \
+  V(RiscvCvtDUw)                          \
+  V(RiscvLb)                              \
+  V(RiscvLbu)                             \
+  V(RiscvSb)                              \
+  V(RiscvLh)                              \
+  V(RiscvUlh)                             \
+  V(RiscvLhu)                             \
+  V(RiscvUlhu)                            \
+  V(RiscvSh)                              \
+  V(RiscvUsh)                             \
+  V(RiscvUld)                             \
+  V(RiscvLw)                              \
+  V(RiscvUlw)                             \
+  V(RiscvSw)                              \
+  V(RiscvUsw)                             \
+  V(RiscvLoadFloat)                       \
+  V(RiscvULoadFloat)                      \
+  V(RiscvStoreFloat)                      \
+  V(RiscvUStoreFloat)                     \
+  V(RiscvLoadDouble)                      \
+  V(RiscvULoadDouble)                     \
+  V(RiscvStoreDouble)                     \
+  V(RiscvUStoreDouble)                    \
+  V(RiscvBitcastInt32ToFloat32)           \
+  V(RiscvBitcastFloat32ToInt32)           \
+  V(RiscvFloat64ExtractLowWord32)         \
+  V(RiscvFloat64ExtractHighWord32)        \
+  V(RiscvFloat64InsertLowWord32)          \
+  V(RiscvFloat64InsertHighWord32)         \
+  V(RiscvFloat32Max)                      \
+  V(RiscvFloat64Max)                      \
+  V(RiscvFloat32Min)                      \
+  V(RiscvFloat64Min)                      \
+  V(RiscvFloat64SilenceNaN)               \
+  V(RiscvPush)                            \
+  V(RiscvPeek)                            \
+  V(RiscvByteSwap32)                      \
+  V(RiscvStoreToStackSlot)                \
+  V(RiscvStackClaim)                      \
+  V(RiscvSignExtendByte)                  \
+  V(RiscvSignExtendShort)                 \
+  V(RiscvSync)                            \
+  V(RiscvAssertEqual)                     \
+  V(RiscvS128Const)                       \
+  V(RiscvS128Zero)                        \
+  V(RiscvS128AllOnes)                     \
+  V(RiscvI32x4Splat)                      \
+  V(RiscvI32x4ExtractLane)                \
+  V(RiscvI32x4ReplaceLane)                \
+  V(RiscvI32x4Add)                        \
+  V(RiscvI32x4Sub)                        \
+  V(RiscvF64x2Abs)                        \
+  V(RiscvF64x2Neg)                        \
+  V(RiscvF32x4Splat)                      \
+  V(RiscvF32x4ExtractLane)                \
+  V(RiscvF32x4ReplaceLane)                \
+  V(RiscvF32x4SConvertI32x4)              \
+  V(RiscvF32x4UConvertI32x4)              \
+  V(RiscvI64x2SConvertI32x4Low)           \
+  V(RiscvI64x2SConvertI32x4High)          \
+  V(RiscvI64x2UConvertI32x4Low)           \
+  V(RiscvI64x2UConvertI32x4High)          \
+  V(RiscvI32x4Mul)                        \
+  V(RiscvI32x4MaxS)                       \
+  V(RiscvI32x4MinS)                       \
+  V(RiscvI32x4Eq)                         \
+  V(RiscvI32x4Ne)                         \
+  V(RiscvI32x4Shl)                        \
+  V(RiscvI32x4ShrS)                       \
+  V(RiscvI32x4ShrU)                       \
+  V(RiscvI32x4MaxU)                       \
+  V(RiscvI32x4MinU)                       \
+  V(RiscvI64x2GtS)                        \
+  V(RiscvI64x2GeS)                        \
+  V(RiscvI64x2Eq)                         \
+  V(RiscvI64x2Ne)                         \
+  V(RiscvF64x2Sqrt)                       \
+  V(RiscvF64x2Add)                        \
+  V(RiscvF64x2Sub)                        \
+  V(RiscvF64x2Mul)                        \
+  V(RiscvF64x2Div)                        \
+  V(RiscvF64x2Min)                        \
+  V(RiscvF64x2Max)                        \
+  V(RiscvF64x2ConvertLowI32x4S)           \
+  V(RiscvF64x2ConvertLowI32x4U)           \
+  V(RiscvF64x2PromoteLowF32x4)            \
+  V(RiscvF64x2Eq)                         \
+  V(RiscvF64x2Ne)                         \
+  V(RiscvF64x2Lt)                         \
+  V(RiscvF64x2Le)                         \
+  V(RiscvF64x2Splat)                      \
+  V(RiscvF64x2ExtractLane)                \
+  V(RiscvF64x2ReplaceLane)                \
+  V(RiscvF64x2Pmin)                       \
+  V(RiscvF64x2Pmax)                       \
+  V(RiscvF64x2Ceil)                       \
+  V(RiscvF64x2Floor)                      \
+  V(RiscvF64x2Trunc)                      \
+  V(RiscvF64x2NearestInt)                 \
+  V(RiscvI64x2Splat)                      \
+  V(RiscvI64x2ExtractLane)                \
+  V(RiscvI64x2ReplaceLane)                \
+  V(RiscvI64x2Add)                        \
+  V(RiscvI64x2Sub)                        \
+  V(RiscvI64x2Mul)                        \
+  V(RiscvI64x2Abs)                        \
+  V(RiscvI64x2Neg)                        \
+  V(RiscvI64x2Shl)                        \
+  V(RiscvI64x2ShrS)                       \
+  V(RiscvI64x2ShrU)                       \
+  V(RiscvI64x2BitMask)                    \
+  V(RiscvF32x4Abs)                        \
+  V(RiscvF32x4Neg)                        \
+  V(RiscvF32x4Sqrt)                       \
+  V(RiscvF32x4Qfma)                       \
+  V(RiscvF32x4Qfms)                       \
+  V(RiscvF64x2Qfma)                       \
+  V(RiscvF64x2Qfms)                       \
+  V(RiscvF32x4Add)                        \
+  V(RiscvF32x4Sub)                        \
+  V(RiscvF32x4Mul)                        \
+  V(RiscvF32x4Div)                        \
+  V(RiscvF32x4Max)                        \
+  V(RiscvF32x4Min)                        \
+  V(RiscvF32x4Eq)                         \
+  V(RiscvF32x4Ne)                         \
+  V(RiscvF32x4Lt)                         \
+  V(RiscvF32x4Le)                         \
+  V(RiscvF32x4Pmin)                       \
+  V(RiscvF32x4Pmax)                       \
+  V(RiscvF32x4DemoteF64x2Zero)            \
+  V(RiscvF32x4Ceil)                       \
+  V(RiscvF32x4Floor)                      \
+  V(RiscvF32x4Trunc)                      \
+  V(RiscvF32x4NearestInt)                 \
+  V(RiscvI32x4SConvertF32x4)              \
+  V(RiscvI32x4UConvertF32x4)              \
+  V(RiscvI32x4Neg)                        \
+  V(RiscvI32x4GtS)                        \
+  V(RiscvI32x4GeS)                        \
+  V(RiscvI32x4GtU)                        \
+  V(RiscvI32x4GeU)                        \
+  V(RiscvI32x4Abs)                        \
+  V(RiscvI32x4BitMask)                    \
+  V(RiscvI32x4TruncSatF64x2SZero)         \
+  V(RiscvI32x4TruncSatF64x2UZero)         \
+  V(RiscvI16x8Splat)                      \
+  V(RiscvI16x8ExtractLaneU)               \
+  V(RiscvI16x8ExtractLaneS)               \
+  V(RiscvI16x8ReplaceLane)                \
+  V(RiscvI16x8Neg)                        \
+  V(RiscvI16x8Shl)                        \
+  V(RiscvI16x8ShrS)                       \
+  V(RiscvI16x8ShrU)                       \
+  V(RiscvI16x8Add)                        \
+  V(RiscvI16x8AddSatS)                    \
+  V(RiscvI16x8Sub)                        \
+  V(RiscvI16x8SubSatS)                    \
+  V(RiscvI16x8Mul)                        \
+  V(RiscvI16x8MaxS)                       \
+  V(RiscvI16x8MinS)                       \
+  V(RiscvI16x8Eq)                         \
+  V(RiscvI16x8Ne)                         \
+  V(RiscvI16x8GtS)                        \
+  V(RiscvI16x8GeS)                        \
+  V(RiscvI16x8AddSatU)                    \
+  V(RiscvI16x8SubSatU)                    \
+  V(RiscvI16x8MaxU)                       \
+  V(RiscvI16x8MinU)                       \
+  V(RiscvI16x8GtU)                        \
+  V(RiscvI16x8GeU)                        \
+  V(RiscvI16x8RoundingAverageU)           \
+  V(RiscvI16x8Q15MulRSatS)                \
+  V(RiscvI16x8Abs)                        \
+  V(RiscvI16x8BitMask)                    \
+  V(RiscvI8x16Splat)                      \
+  V(RiscvI8x16ExtractLaneU)               \
+  V(RiscvI8x16ExtractLaneS)               \
+  V(RiscvI8x16ReplaceLane)                \
+  V(RiscvI8x16Neg)                        \
+  V(RiscvI8x16Shl)                        \
+  V(RiscvI8x16ShrS)                       \
+  V(RiscvI8x16Add)                        \
+  V(RiscvI8x16AddSatS)                    \
+  V(RiscvI8x16Sub)                        \
+  V(RiscvI8x16SubSatS)                    \
+  V(RiscvI8x16MaxS)                       \
+  V(RiscvI8x16MinS)                       \
+  V(RiscvI8x16Eq)                         \
+  V(RiscvI8x16Ne)                         \
+  V(RiscvI8x16GtS)                        \
+  V(RiscvI8x16GeS)                        \
+  V(RiscvI8x16ShrU)                       \
+  V(RiscvI8x16AddSatU)                    \
+  V(RiscvI8x16SubSatU)                    \
+  V(RiscvI8x16MaxU)                       \
+  V(RiscvI8x16MinU)                       \
+  V(RiscvI8x16GtU)                        \
+  V(RiscvI8x16GeU)                        \
+  V(RiscvI8x16RoundingAverageU)           \
+  V(RiscvI8x16Abs)                        \
+  V(RiscvI8x16BitMask)                    \
+  V(RiscvI8x16Popcnt)                     \
+  V(RiscvS128And)                         \
+  V(RiscvS128Or)                          \
+  V(RiscvS128Xor)                         \
+  V(RiscvS128Not)                         \
+  V(RiscvS128Select)                      \
+  V(RiscvS128AndNot)                      \
+  V(RiscvS128Load64Zero)                  \
+  V(RiscvS128Load32Zero)                  \
+  V(RiscvI32x4AllTrue)                    \
+  V(RiscvI16x8AllTrue)                    \
+  V(RiscvV128AnyTrue)                     \
+  V(RiscvI8x16AllTrue)                    \
+  V(RiscvI64x2AllTrue)                    \
+  V(RiscvS32x4InterleaveRight)            \
+  V(RiscvS32x4InterleaveLeft)             \
+  V(RiscvS32x4PackEven)                   \
+  V(RiscvS32x4PackOdd)                    \
+  V(RiscvS32x4InterleaveEven)             \
+  V(RiscvS32x4InterleaveOdd)              \
+  V(RiscvS32x4Shuffle)                    \
+  V(RiscvS16x8InterleaveRight)            \
+  V(RiscvS16x8InterleaveLeft)             \
+  V(RiscvS16x8PackEven)                   \
+  V(RiscvS16x8PackOdd)                    \
+  V(RiscvS16x8InterleaveEven)             \
+  V(RiscvS16x8InterleaveOdd)              \
+  V(RiscvS16x4Reverse)                    \
+  V(RiscvS16x2Reverse)                    \
+  V(RiscvS8x16InterleaveRight)            \
+  V(RiscvS8x16InterleaveLeft)             \
+  V(RiscvS8x16PackEven)                   \
+  V(RiscvS8x16PackOdd)                    \
+  V(RiscvS8x16InterleaveEven)             \
+  V(RiscvS8x16InterleaveOdd)              \
+  V(RiscvI8x16Shuffle)                    \
+  V(RiscvS8x16Concat)                     \
+  V(RiscvS8x8Reverse)                     \
+  V(RiscvS8x4Reverse)                     \
+  V(RiscvS8x2Reverse)                     \
+  V(RiscvS128LoadSplat)                   \
+  V(RiscvS128Load64ExtendS)               \
+  V(RiscvS128Load64ExtendU)               \
+  V(RiscvS128LoadLane)                    \
+  V(RiscvS128StoreLane)                   \
+  V(RiscvRvvLd)                           \
+  V(RiscvRvvSt)                           \
+  V(RiscvI32x4SConvertI16x8Low)           \
+  V(RiscvI32x4SConvertI16x8High)          \
+  V(RiscvI32x4UConvertI16x8Low)           \
+  V(RiscvI32x4UConvertI16x8High)          \
+  V(RiscvI16x8SConvertI8x16Low)           \
+  V(RiscvI16x8SConvertI8x16High)          \
+  V(RiscvI16x8SConvertI32x4)              \
+  V(RiscvI16x8UConvertI32x4)              \
+  V(RiscvI16x8UConvertI8x16Low)           \
+  V(RiscvI16x8UConvertI8x16High)          \
+  V(RiscvI8x16SConvertI16x8)              \
+  V(RiscvI8x16UConvertI16x8)              \
+  V(RiscvVwmul)                           \
+  V(RiscvVwmulu)                          \
+  V(RiscvVmvSx)                           \
+  V(RiscvVcompress)                       \
+  V(RiscvVaddVv)                          \
+  V(RiscvVwadd)                           \
+  V(RiscvVwaddu)                          \
+  V(RiscvVrgather)                        \
+  V(RiscvVslidedown)
+
+#define TARGET_ARCH_OPCODE_LIST(V)  \
+  TARGET_ARCH_OPCODE_LIST_COMMON(V) \
+  TARGET_ARCH_OPCODE_LIST_SPECAIL(V)
+
+// Addressing modes represent the "shape" of inputs to an instruction.
+// Many instructions support multiple addressing modes. Addressing modes
+// are encoded into the InstructionCode of the instruction and tell the
+// code generator after register allocation which assembler method to call.
+//
+// We use the following local notation for addressing modes:
+//
+// R = register
+// O = register or stack slot
+// D = double register
+// I = immediate (handle, external, int32)
+// MRI = [register + immediate]
+// MRR = [register + register]
+// Root = [kRootregister + immediate]
+// TODO(plind): Add the new r6 address modes.
+#define TARGET_ADDRESSING_MODE_LIST(V) \
+  V(MRI)  /* [%r0 + K] */              \
+  V(MRR)  /* [%r0 + %r1] */            \
+  V(Root) /* [root + k] */
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_COMPILER_BACKEND_RISCV_INSTRUCTION_CODES_RISCV_H_
diff --git a/src/compiler/backend/riscv64/instruction-scheduler-riscv64.cc b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
similarity index 96%
rename from src/compiler/backend/riscv64/instruction-scheduler-riscv64.cc
rename to src/compiler/backend/riscv/instruction-scheduler-riscv.cc
index a9e2acbffa0..879ac4393fd 100644
--- a/src/compiler/backend/riscv64/instruction-scheduler-riscv64.cc
+++ b/src/compiler/backend/riscv/instruction-scheduler-riscv.cc
@@ -14,20 +14,68 @@ bool InstructionScheduler::SchedulerSupported() { return true; }
 int InstructionScheduler::GetTargetInstructionFlags(
     const Instruction* instr) const {
   switch (instr->arch_opcode()) {
+#if V8_TARGET_ARCH_RISCV64
+    case kRiscvAdd32:
+    case kRiscvBitcastDL:
+    case kRiscvBitcastLD:
+    case kRiscvByteSwap64:
+    case kRiscvCvtDL:
+    case kRiscvCvtDUl:
+    case kRiscvCvtSL:
+    case kRiscvCvtSUl:
+    case kRiscvMulHigh64:
+    case kRiscvAdd64:
+    case kRiscvAddOvf64:
+    case kRiscvClz64:
+    case kRiscvCtz64:
+    case kRiscvDiv64:
+    case kRiscvDivU64:
+    case kRiscvZeroExtendWord:
+    case kRiscvSignExtendWord:
+    case kRiscvMod64:
+    case kRiscvModU64:
+    case kRiscvMul64:
+    case kRiscvPopcnt64:
+    case kRiscvRor64:
+    case kRiscvSar64:
+    case kRiscvShl64:
+    case kRiscvShr64:
+    case kRiscvSub64:
+    case kRiscvSubOvf64:
+    case kRiscvFloat64RoundDown:
+    case kRiscvFloat64RoundTiesEven:
+    case kRiscvFloat64RoundTruncate:
+    case kRiscvFloat64RoundUp:
+    case kRiscvSub32:
+    case kRiscvTruncLD:
+    case kRiscvTruncLS:
+    case kRiscvTruncUlD:
+    case kRiscvTruncUlS:
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvAdd32:
+    case kRiscvAddPair:
+    case kRiscvSubPair:
+    case kRiscvMulPair:
+    case kRiscvAndPair:
+    case kRiscvOrPair:
+    case kRiscvXorPair:
+    case kRiscvShlPair:
+    case kRiscvShrPair:
+    case kRiscvSarPair:
+    case kRiscvAddOvf:
+    case kRiscvSubOvf:
+    case kRiscvSub32:
+#endif
     case kRiscvAbsD:
     case kRiscvAbsS:
-    case kRiscvAdd32:
     case kRiscvAddD:
     case kRiscvAddS:
     case kRiscvAnd:
     case kRiscvAnd32:
     case kRiscvAssertEqual:
-    case kRiscvBitcastDL:
-    case kRiscvBitcastLD:
     case kRiscvBitcastInt32ToFloat32:
     case kRiscvBitcastFloat32ToInt32:
     case kRiscvByteSwap32:
-    case kRiscvByteSwap64:
     case kRiscvCeilWD:
     case kRiscvCeilWS:
     case kRiscvClz32:
@@ -36,40 +84,17 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvCmpD:
     case kRiscvCmpS:
     case kRiscvCtz32:
-    case kRiscvCvtDL:
     case kRiscvCvtDS:
-    case kRiscvCvtDUl:
     case kRiscvCvtDUw:
     case kRiscvCvtDW:
     case kRiscvCvtSD:
-    case kRiscvCvtSL:
-    case kRiscvCvtSUl:
     case kRiscvCvtSUw:
     case kRiscvCvtSW:
-    case kRiscvMulHigh64:
     case kRiscvMulHighU32:
-    case kRiscvAdd64:
-    case kRiscvAddOvf64:
-    case kRiscvClz64:
-    case kRiscvCtz64:
-    case kRiscvDiv64:
-    case kRiscvDivU64:
-    case kRiscvZeroExtendWord:
-    case kRiscvSignExtendWord:
     case kRiscvDiv32:
     case kRiscvDivD:
     case kRiscvDivS:
     case kRiscvDivU32:
-    case kRiscvMod64:
-    case kRiscvModU64:
-    case kRiscvMul64:
-    case kRiscvPopcnt64:
-    case kRiscvRor64:
-    case kRiscvSar64:
-    case kRiscvShl64:
-    case kRiscvShr64:
-    case kRiscvSub64:
-    case kRiscvSubOvf64:
     case kRiscvF64x2Abs:
     case kRiscvF64x2Neg:
     case kRiscvF64x2Sqrt:
@@ -152,10 +177,6 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvFloat64InsertHighWord32:
     case kRiscvFloat64Max:
     case kRiscvFloat64Min:
-    case kRiscvFloat64RoundDown:
-    case kRiscvFloat64RoundTiesEven:
-    case kRiscvFloat64RoundTruncate:
-    case kRiscvFloat64RoundUp:
     case kRiscvFloat64SilenceNaN:
     case kRiscvFloorWD:
     case kRiscvFloorWS:
@@ -338,13 +359,8 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvShr32:
     case kRiscvSqrtD:
     case kRiscvSqrtS:
-    case kRiscvSub32:
     case kRiscvSubD:
     case kRiscvSubS:
-    case kRiscvTruncLD:
-    case kRiscvTruncLS:
-    case kRiscvTruncUlD:
-    case kRiscvTruncUlS:
     case kRiscvTruncUwD:
     case kRiscvTruncUwS:
     case kRiscvTruncWD:
@@ -353,16 +369,24 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvXor:
     case kRiscvXor32:
       return kNoOpcodeFlags;
-
+#if V8_TARGET_ARCH_RISCV64
+    case kRiscvLd:
+    case kRiscvLwu:
+    case kRiscvUlwu:
+    case kRiscvWord64AtomicLoadUint64:
+    case kRiscvLoadDecompressTaggedSigned:
+    case kRiscvLoadDecompressTaggedPointer:
+    case kRiscvLoadDecompressAnyTagged:
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvWord32AtomicPairLoad:
+#endif
     case kRiscvLb:
     case kRiscvLbu:
-    case kRiscvLd:
     case kRiscvLoadDouble:
     case kRiscvLh:
     case kRiscvLhu:
     case kRiscvLw:
     case kRiscvLoadFloat:
-    case kRiscvLwu:
     case kRiscvRvvLd:
     case kRiscvPeek:
     case kRiscvUld:
@@ -370,45 +394,51 @@ int InstructionScheduler::GetTargetInstructionFlags(
     case kRiscvUlh:
     case kRiscvUlhu:
     case kRiscvUlw:
-    case kRiscvUlwu:
     case kRiscvULoadFloat:
     case kRiscvS128LoadSplat:
     case kRiscvS128Load64ExtendU:
     case kRiscvS128Load64ExtendS:
     case kRiscvS128LoadLane:
-    case kRiscvWord64AtomicLoadUint64:
-    case kRiscvLoadDecompressTaggedSigned:
-    case kRiscvLoadDecompressTaggedPointer:
-    case kRiscvLoadDecompressAnyTagged:
       return kIsLoadOperation;
 
+#if V8_TARGET_ARCH_RISCV64
+    case kRiscvSd:
+    case kRiscvUsd:
+    case kRiscvWord64AtomicStoreWord64:
+    case kRiscvWord64AtomicAddUint64:
+    case kRiscvWord64AtomicSubUint64:
+    case kRiscvWord64AtomicAndUint64:
+    case kRiscvWord64AtomicOrUint64:
+    case kRiscvWord64AtomicXorUint64:
+    case kRiscvWord64AtomicExchangeUint64:
+    case kRiscvWord64AtomicCompareExchangeUint64:
+    case kRiscvStoreCompressTagged:
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvWord32AtomicPairStore:
+    case kRiscvWord32AtomicPairAdd:
+    case kRiscvWord32AtomicPairSub:
+    case kRiscvWord32AtomicPairAnd:
+    case kRiscvWord32AtomicPairOr:
+    case kRiscvWord32AtomicPairXor:
+    case kRiscvWord32AtomicPairExchange:
+    case kRiscvWord32AtomicPairCompareExchange:
+#endif
     case kRiscvModD:
     case kRiscvModS:
     case kRiscvRvvSt:
     case kRiscvPush:
     case kRiscvSb:
-    case kRiscvSd:
     case kRiscvStoreDouble:
     case kRiscvSh:
     case kRiscvStackClaim:
     case kRiscvStoreToStackSlot:
     case kRiscvSw:
     case kRiscvStoreFloat:
-    case kRiscvUsd:
     case kRiscvUStoreDouble:
     case kRiscvUsh:
     case kRiscvUsw:
     case kRiscvUStoreFloat:
     case kRiscvSync:
-    case kRiscvWord64AtomicStoreWord64:
-    case kRiscvWord64AtomicAddUint64:
-    case kRiscvWord64AtomicSubUint64:
-    case kRiscvWord64AtomicAndUint64:
-    case kRiscvWord64AtomicOrUint64:
-    case kRiscvWord64AtomicXorUint64:
-    case kRiscvWord64AtomicExchangeUint64:
-    case kRiscvWord64AtomicCompareExchangeUint64:
-    case kRiscvStoreCompressTagged:
     case kRiscvS128StoreLane:
       return kHasSideEffect;
 
@@ -934,12 +964,13 @@ int Popcnt32Latency() {
          1 + Mul32Latency() + 1;
 }
 
+#if V8_TARGET_ARCH_RISCV64
 int Popcnt64Latency() {
   return 2 + AndLatency() + Sub64Latency() + 1 + AndLatency() + 1 +
          AndLatency() + Add64Latency() + 1 + Add64Latency() + 1 + AndLatency() +
          1 + Mul64Latency() + 1;
 }
-
+#endif
 int CompareFLatency() { return Latency::C_cond_S; }
 
 int CompareF32Latency() { return CompareFLatency(); }
@@ -964,13 +995,14 @@ int NegdLatency() {
          Latency::MOVF_HIGH_DREG + 1 + XorLatency() + Latency::MOVT_DREG;
 }
 
+#if V8_TARGET_ARCH_RISCV64
 int Float64RoundLatency() {
   // For ceil_l_d, floor_l_d, round_l_d, trunc_l_d latency is 4.
   return Latency::MOVF_HIGH_DREG + 1 + Latency::BRANCH + Latency::MOV_D + 4 +
          Latency::MOVF_HIGH_DREG + Latency::BRANCH + Latency::CVT_D_L + 2 +
          Latency::MOVT_HIGH_FREG;
 }
-
+#endif
 int Float32RoundLatency() {
   // For ceil_w_s, floor_w_s, round_w_s, trunc_w_s latency is 4.
   return Latency::MOVF_FREG + 1 + Latency::BRANCH + Latency::MOV_S + 4 +
@@ -1169,6 +1201,7 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
     case kIeee754Float64Tanh:
       return PrepareCallCFunctionLatency() + MovToFloatParametersLatency() +
              CallCFunctionLatency() + MovFromFloatResultLatency();
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvAdd32:
     case kRiscvAdd64:
       return Add64Latency(instr->InputAt(1)->IsRegister());
@@ -1179,6 +1212,32 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Sub64Latency(instr->InputAt(1)->IsRegister());
     case kRiscvSubOvf64:
       return SubOverflow64Latency();
+    case kRiscvMulHigh64:
+      return Mulh64Latency();
+    case kRiscvMul64:
+      return Mul64Latency();
+    case kRiscvDiv64: {
+      int latency = Div64Latency();
+      return latency + MovzLatency();
+    }
+    case kRiscvDivU64: {
+      int latency = Divu64Latency();
+      return latency + MovzLatency();
+    }
+    case kRiscvMod64:
+      return Mod64Latency();
+    case kRiscvModU64:
+      return Modu64Latency();
+#elif V8_TARGET_ARCH_RISCV32
+    case kRiscvAdd32:
+      return Add64Latency(instr->InputAt(1)->IsRegister());
+    case kRiscvAddOvf:
+      return AddOverflow64Latency();
+    case kRiscvSub32:
+      return Sub64Latency(instr->InputAt(1)->IsRegister());
+    case kRiscvSubOvf:
+      return SubOverflow64Latency();
+#endif
     case kRiscvMul32:
       return Mul32Latency();
     case kRiscvMulOvf32:
@@ -1187,8 +1246,6 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Mulh32Latency();
     case kRiscvMulHighU32:
       return Mulhu32Latency();
-    case kRiscvMulHigh64:
-      return Mulh64Latency();
     case kRiscvDiv32: {
       int latency = Div32Latency(instr->InputAt(1)->IsRegister());
       return latency + MovzLatency();
@@ -1201,20 +1258,6 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Mod32Latency();
     case kRiscvModU32:
       return Modu32Latency();
-    case kRiscvMul64:
-      return Mul64Latency();
-    case kRiscvDiv64: {
-      int latency = Div64Latency();
-      return latency + MovzLatency();
-    }
-    case kRiscvDivU64: {
-      int latency = Divu64Latency();
-      return latency + MovzLatency();
-    }
-    case kRiscvMod64:
-      return Mod64Latency();
-    case kRiscvModU64:
-      return Modu64Latency();
     case kRiscvAnd:
       return AndLatency(instr->InputAt(1)->IsRegister());
     case kRiscvAnd32: {
@@ -1260,28 +1303,36 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       }
     }
     case kRiscvClz32:
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvClz64:
+#endif
       return Clz64Latency();
-    case kRiscvCtz32:
-      return Ctz32Latency();
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvCtz64:
       return Ctz64Latency();
-    case kRiscvPopcnt32:
-      return Popcnt32Latency();
     case kRiscvPopcnt64:
       return Popcnt64Latency();
+#endif
+    case kRiscvCtz32:
+      return Ctz32Latency();
+    case kRiscvPopcnt32:
+      return Popcnt32Latency();
     case kRiscvShl32:
       return 1;
     case kRiscvShr32:
     case kRiscvSar32:
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvZeroExtendWord:
+#endif
       return 2;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvSignExtendWord:
     case kRiscvShl64:
     case kRiscvShr64:
     case kRiscvSar64:
-    case kRiscvRor32:
     case kRiscvRor64:
+#endif
+    case kRiscvRor32:
       return 1;
     case kRiscvTst:
       return AndLatency(instr->InputAt(1)->IsRegister());
@@ -1333,11 +1384,13 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Latency::MAX_D;
     case kRiscvMinD:
       return Latency::MIN_D;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvFloat64RoundDown:
     case kRiscvFloat64RoundTruncate:
     case kRiscvFloat64RoundUp:
     case kRiscvFloat64RoundTiesEven:
       return Float64RoundLatency();
+#endif
     case kRiscvFloat32RoundDown:
     case kRiscvFloat32RoundTruncate:
     case kRiscvFloat32RoundUp:
@@ -1363,18 +1416,20 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Latency::MOVT_FREG + Latency::CVT_S_W;
     case kRiscvCvtSUw:
       return 1 + Latency::MOVT_DREG + Latency::CVT_S_L;
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvCvtSL:
       return Latency::MOVT_DREG + Latency::CVT_S_L;
     case kRiscvCvtDL:
       return Latency::MOVT_DREG + Latency::CVT_D_L;
-    case kRiscvCvtDUw:
-      return 1 + Latency::MOVT_DREG + Latency::CVT_D_L;
     case kRiscvCvtDUl:
       return 2 * Latency::BRANCH + 3 + 2 * Latency::MOVT_DREG +
              2 * Latency::CVT_D_L + Latency::ADD_D;
     case kRiscvCvtSUl:
       return 2 * Latency::BRANCH + 3 + 2 * Latency::MOVT_DREG +
              2 * Latency::CVT_S_L + Latency::ADD_S;
+#endif
+    case kRiscvCvtDUw:
+      return 1 + Latency::MOVT_DREG + Latency::CVT_D_L;
     case kRiscvFloorWD:
       return Latency::FLOOR_W_D + Latency::MOVF_FREG;
     case kRiscvCeilWD:
@@ -1391,10 +1446,20 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return Latency::ROUND_W_S + Latency::MOVF_FREG;
     case kRiscvTruncWS:
       return Latency::TRUNC_W_S + Latency::MOVF_FREG + 2 + MovnLatency();
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvTruncLS:
       return TruncLSLatency(instr->OutputCount() > 1);
     case kRiscvTruncLD:
       return TruncLDLatency(instr->OutputCount() > 1);
+    case kRiscvTruncUlS:
+      return TruncUlSLatency();
+    case kRiscvTruncUlD:
+      return TruncUlDLatency();
+    case kRiscvBitcastDL:
+      return Latency::MOVF_HIGH_DREG;
+    case kRiscvBitcastLD:
+      return Latency::MOVT_DREG;
+#endif
     case kRiscvTruncUwD:
       // Estimated max.
       return CompareF64Latency() + 2 * Latency::BRANCH +
@@ -1406,14 +1471,6 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return CompareF32Latency() + 2 * Latency::BRANCH +
              2 * Latency::TRUNC_W_S + Latency::SUB_S + OrLatency() +
              Latency::MOVT_FREG + 2 * Latency::MOVF_FREG + 2 + MovzLatency();
-    case kRiscvTruncUlS:
-      return TruncUlSLatency();
-    case kRiscvTruncUlD:
-      return TruncUlDLatency();
-    case kRiscvBitcastDL:
-      return Latency::MOVF_HIGH_DREG;
-    case kRiscvBitcastLD:
-      return Latency::MOVT_DREG;
     case kRiscvFloat64ExtractLowWord32:
       return Latency::MOVF_FREG;
     case kRiscvFloat64InsertLowWord32:
@@ -1430,13 +1487,15 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
     case kRiscvLb:
     case kRiscvLhu:
     case kRiscvLh:
-    case kRiscvLwu:
     case kRiscvLw:
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvLd:
+    case kRiscvSd:
+    case kRiscvLwu:
+#endif
     case kRiscvSb:
     case kRiscvSh:
     case kRiscvSw:
-    case kRiscvSd:
       return AlignedMemoryLatency();
     case kRiscvLoadFloat:
       return ULoadFloatLatency();
@@ -1449,12 +1508,18 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
     case kRiscvUlhu:
     case kRiscvUlh:
       return UlhuLatency();
+#if V8_TARGET_ARCH_RISCV64
     case kRiscvUlwu:
       return UlwuLatency();
-    case kRiscvUlw:
-      return UlwLatency();
     case kRiscvUld:
       return UldLatency();
+    case kRiscvUsd:
+      return UsdLatency();
+    case kRiscvByteSwap64:
+      return ByteSwapSignedLatency();
+#endif
+    case kRiscvUlw:
+      return UlwLatency();
     case kRiscvULoadFloat:
       return ULoadFloatLatency();
     case kRiscvULoadDouble:
@@ -1463,8 +1528,6 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       return UshLatency();
     case kRiscvUsw:
       return UswLatency();
-    case kRiscvUsd:
-      return UsdLatency();
     case kRiscvUStoreFloat:
       return UStoreFloatLatency();
     case kRiscvUStoreDouble:
@@ -1512,8 +1575,6 @@ int InstructionScheduler::GetInstructionLatency(const Instruction* instr) {
       }
       return latency;
     }
-    case kRiscvByteSwap64:
-      return ByteSwapSignedLatency();
     case kRiscvByteSwap32:
       return ByteSwapSignedLatency();
     case kAtomicLoadInt8:
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv.h b/src/compiler/backend/riscv/instruction-selector-riscv.h
new file mode 100644
index 00000000000..1ac81a26995
--- /dev/null
+++ b/src/compiler/backend/riscv/instruction-selector-riscv.h
@@ -0,0 +1,1239 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+#ifndef V8_COMPILER_BACKEND_RISCV_INSTRUCTION_SELECTOR_RISCV_H_
+#define V8_COMPILER_BACKEND_RISCV_INSTRUCTION_SELECTOR_RISCV_H_
+
+#include "src/base/bits.h"
+#include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/node-matchers.h"
+#include "src/compiler/node-properties.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+#define TRACE_UNIMPL() \
+  PrintF("UNIMPLEMENTED instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
+
+#define TRACE() PrintF("instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
+
+// Adds RISC-V-specific methods for generating InstructionOperands.
+class RiscvOperandGenerator final : public OperandGenerator {
+ public:
+  explicit RiscvOperandGenerator(InstructionSelector* selector)
+      : OperandGenerator(selector) {}
+
+  InstructionOperand UseOperand(Node* node, InstructionCode opcode) {
+    if (CanBeImmediate(node, opcode)) {
+      return UseImmediate(node);
+    }
+    return UseRegister(node);
+  }
+
+  // Use the zero register if the node has the immediate value zero, otherwise
+  // assign a register.
+  InstructionOperand UseRegisterOrImmediateZero(Node* node) {
+    if ((IsIntegerConstant(node) && (GetIntegerConstantValue(node) == 0)) ||
+        (IsFloatConstant(node) &&
+         (base::bit_cast<int64_t>(GetFloatConstantValue(node)) == 0))) {
+      return UseImmediate(node);
+    }
+    return UseRegister(node);
+  }
+
+  bool IsIntegerConstant(Node* node);
+
+  int64_t GetIntegerConstantValue(Node* node);
+
+  bool IsFloatConstant(Node* node) {
+    return (node->opcode() == IrOpcode::kFloat32Constant) ||
+           (node->opcode() == IrOpcode::kFloat64Constant);
+  }
+
+  double GetFloatConstantValue(Node* node) {
+    if (node->opcode() == IrOpcode::kFloat32Constant) {
+      return OpParameter<float>(node->op());
+    }
+    DCHECK_EQ(IrOpcode::kFloat64Constant, node->opcode());
+    return OpParameter<double>(node->op());
+  }
+
+  bool CanBeImmediate(Node* node, InstructionCode mode) {
+    return IsIntegerConstant(node) &&
+           CanBeImmediate(GetIntegerConstantValue(node), mode);
+  }
+
+  bool CanBeImmediate(int64_t value, InstructionCode opcode);
+
+ private:
+  bool ImmediateFitsAddrMode1Instruction(int32_t imm) const {
+    TRACE_UNIMPL();
+    return false;
+  }
+};
+
+void InstructionSelector::VisitProtectedStore(Node* node) {
+  // TODO(eholk)
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitProtectedLoad(Node* node) {
+  // TODO(eholk)
+  UNIMPLEMENTED();
+}
+
+static void VisitRR(InstructionSelector* selector, ArchOpcode opcode,
+                    Node* node) {
+  RiscvOperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)));
+}
+
+static void VisitRRI(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  RiscvOperandGenerator g(selector);
+  int32_t imm = OpParameter<int32_t>(node->op());
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
+}
+
+static void VisitSimdShift(InstructionSelector* selector, ArchOpcode opcode,
+                           Node* node) {
+  RiscvOperandGenerator g(selector);
+  if (g.IsIntegerConstant(node->InputAt(1))) {
+    selector->Emit(opcode, g.DefineAsRegister(node),
+                   g.UseRegister(node->InputAt(0)),
+                   g.UseImmediate(node->InputAt(1)));
+  } else {
+    selector->Emit(opcode, g.DefineAsRegister(node),
+                   g.UseRegister(node->InputAt(0)),
+                   g.UseRegister(node->InputAt(1)));
+  }
+}
+
+static void VisitRRIR(InstructionSelector* selector, ArchOpcode opcode,
+                      Node* node) {
+  RiscvOperandGenerator g(selector);
+  int32_t imm = OpParameter<int32_t>(node->op());
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
+                 g.UseRegister(node->InputAt(1)));
+}
+
+static void VisitRRR(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  RiscvOperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)),
+                 g.UseRegister(node->InputAt(1)));
+}
+
+static void VisitUniqueRRR(InstructionSelector* selector, ArchOpcode opcode,
+                           Node* node) {
+  RiscvOperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseUniqueRegister(node->InputAt(0)),
+                 g.UseUniqueRegister(node->InputAt(1)));
+}
+
+void VisitRRRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
+  RiscvOperandGenerator g(selector);
+  selector->Emit(
+      opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)),
+      g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));
+}
+
+static void VisitRRO(InstructionSelector* selector, ArchOpcode opcode,
+                     Node* node) {
+  RiscvOperandGenerator g(selector);
+  selector->Emit(opcode, g.DefineAsRegister(node),
+                 g.UseRegister(node->InputAt(0)),
+                 g.UseOperand(node->InputAt(1), opcode));
+}
+
+bool TryMatchImmediate(InstructionSelector* selector,
+                       InstructionCode* opcode_return, Node* node,
+                       size_t* input_count_return, InstructionOperand* inputs) {
+  RiscvOperandGenerator g(selector);
+  if (g.CanBeImmediate(node, *opcode_return)) {
+    *opcode_return |= AddressingModeField::encode(kMode_MRI);
+    inputs[0] = g.UseImmediate(node);
+    *input_count_return = 1;
+    return true;
+  }
+  return false;
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, bool has_reverse_opcode,
+                       InstructionCode reverse_opcode,
+                       FlagsContinuation* cont) {
+  RiscvOperandGenerator g(selector);
+  Int32BinopMatcher m(node);
+  InstructionOperand inputs[2];
+  size_t input_count = 0;
+  InstructionOperand outputs[1];
+  size_t output_count = 0;
+
+  if (TryMatchImmediate(selector, &opcode, m.right().node(), &input_count,
+                        &inputs[1])) {
+    inputs[0] = g.UseRegisterOrImmediateZero(m.left().node());
+    input_count++;
+  } else if (has_reverse_opcode &&
+             TryMatchImmediate(selector, &reverse_opcode, m.left().node(),
+                               &input_count, &inputs[1])) {
+    inputs[0] = g.UseRegisterOrImmediateZero(m.right().node());
+    opcode = reverse_opcode;
+    input_count++;
+  } else {
+    inputs[input_count++] = g.UseRegister(m.left().node());
+    inputs[input_count++] = g.UseOperand(m.right().node(), opcode);
+  }
+
+  if (cont->IsDeoptimize()) {
+    // If we can deoptimize as a result of the binop, we need to make sure that
+    // the deopt inputs are not overwritten by the binop result. One way
+    // to achieve that is to declare the output register as same-as-first.
+    outputs[output_count++] = g.DefineSameAsFirst(node);
+  } else {
+    outputs[output_count++] = g.DefineAsRegister(node);
+  }
+
+  DCHECK_NE(0u, input_count);
+  DCHECK_EQ(1u, output_count);
+  DCHECK_GE(arraysize(inputs), input_count);
+  DCHECK_GE(arraysize(outputs), output_count);
+
+  selector->EmitWithContinuation(opcode, output_count, outputs, input_count,
+                                 inputs, cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, bool has_reverse_opcode,
+                       InstructionCode reverse_opcode) {
+  FlagsContinuation cont;
+  VisitBinop(selector, node, opcode, has_reverse_opcode, reverse_opcode, &cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode, FlagsContinuation* cont) {
+  VisitBinop(selector, node, opcode, false, kArchNop, cont);
+}
+
+static void VisitBinop(InstructionSelector* selector, Node* node,
+                       InstructionCode opcode) {
+  VisitBinop(selector, node, opcode, false, kArchNop);
+}
+
+void InstructionSelector::VisitStackSlot(Node* node) {
+  StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
+  int alignment = rep.alignment();
+  int slot = frame_->AllocateSpillSlot(rep.size(), alignment);
+  OperandGenerator g(this);
+
+  Emit(kArchStackSlot, g.DefineAsRegister(node),
+       sequence()->AddImmediate(Constant(slot)),
+       sequence()->AddImmediate(Constant(alignment)), 0, nullptr);
+}
+
+void InstructionSelector::VisitAbortCSADcheck(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), a0));
+}
+
+void EmitS128Load(InstructionSelector* selector, Node* node,
+                  InstructionCode opcode, VSew sew, Vlmul lmul);
+
+void InstructionSelector::VisitLoadTransform(Node* node) {
+  LoadTransformParameters params = LoadTransformParametersOf(node->op());
+
+  switch (params.transformation) {
+    case LoadTransformation::kS128Load8Splat:
+      EmitS128Load(this, node, kRiscvS128LoadSplat, E8, m1);
+      break;
+    case LoadTransformation::kS128Load16Splat:
+      EmitS128Load(this, node, kRiscvS128LoadSplat, E16, m1);
+      break;
+    case LoadTransformation::kS128Load32Splat:
+      EmitS128Load(this, node, kRiscvS128LoadSplat, E32, m1);
+      break;
+    case LoadTransformation::kS128Load64Splat:
+      EmitS128Load(this, node, kRiscvS128LoadSplat, E64, m1);
+      break;
+    case LoadTransformation::kS128Load8x8S:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E16, m1);
+      break;
+    case LoadTransformation::kS128Load8x8U:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E16, m1);
+      break;
+    case LoadTransformation::kS128Load16x4S:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E32, m1);
+      break;
+    case LoadTransformation::kS128Load16x4U:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E32, m1);
+      break;
+    case LoadTransformation::kS128Load32x2S:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E64, m1);
+      break;
+    case LoadTransformation::kS128Load32x2U:
+      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E64, m1);
+      break;
+    case LoadTransformation::kS128Load32Zero:
+      EmitS128Load(this, node, kRiscvS128Load32Zero, E32, m1);
+      break;
+    case LoadTransformation::kS128Load64Zero:
+      EmitS128Load(this, node, kRiscvS128Load64Zero, E64, m1);
+      break;
+    default:
+      UNIMPLEMENTED();
+  }
+}
+
+// Shared routine for multiple compare operations.
+static void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
+                         InstructionOperand left, InstructionOperand right,
+                         FlagsContinuation* cont) {
+  selector->EmitWithContinuation(opcode, left, right, cont);
+}
+
+// Shared routine for multiple compare operations.
+static void VisitWordCompareZero(InstructionSelector* selector,
+                                 InstructionOperand value,
+                                 FlagsContinuation* cont) {
+  selector->EmitWithContinuation(kRiscvCmpZero, value, cont);
+}
+
+// Shared routine for multiple float32 compare operations.
+void VisitFloat32Compare(InstructionSelector* selector, Node* node,
+                         FlagsContinuation* cont) {
+  RiscvOperandGenerator g(selector);
+  Float32BinopMatcher m(node);
+  InstructionOperand lhs, rhs;
+
+  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
+                          : g.UseRegister(m.left().node());
+  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
+                           : g.UseRegister(m.right().node());
+  VisitCompare(selector, kRiscvCmpS, lhs, rhs, cont);
+}
+
+// Shared routine for multiple float64 compare operations.
+void VisitFloat64Compare(InstructionSelector* selector, Node* node,
+                         FlagsContinuation* cont) {
+  RiscvOperandGenerator g(selector);
+  Float64BinopMatcher m(node);
+  InstructionOperand lhs, rhs;
+
+  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
+                          : g.UseRegister(m.left().node());
+  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
+                           : g.UseRegister(m.right().node());
+  VisitCompare(selector, kRiscvCmpD, lhs, rhs, cont);
+}
+
+// Shared routine for multiple word compare operations.
+void VisitWordCompare(InstructionSelector* selector, Node* node,
+                      InstructionCode opcode, FlagsContinuation* cont,
+                      bool commutative) {
+  RiscvOperandGenerator g(selector);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  // If one of the two inputs is an immediate, make sure it's on the right.
+  if (!g.CanBeImmediate(right, opcode) && g.CanBeImmediate(left, opcode)) {
+    cont->Commute();
+    std::swap(left, right);
+  }
+  // Match immediates on right side of comparison.
+  if (g.CanBeImmediate(right, opcode)) {
+    if (opcode == kRiscvTst) {
+      VisitCompare(selector, opcode, g.UseRegister(left), g.UseImmediate(right),
+                   cont);
+    } else {
+      switch (cont->condition()) {
+        case kEqual:
+        case kNotEqual:
+          if (cont->IsSet()) {
+            VisitCompare(selector, opcode, g.UseRegister(left),
+                         g.UseImmediate(right), cont);
+          } else {
+            Int32BinopMatcher m(node, true);
+            NumberBinopMatcher n(node, true);
+            if (m.right().Is(0) || n.right().IsZero()) {
+              VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
+                                   cont);
+            } else {
+              VisitCompare(selector, opcode, g.UseRegister(left),
+                           g.UseRegister(right), cont);
+            }
+          }
+          break;
+        case kSignedLessThan:
+        case kSignedGreaterThanOrEqual:
+        case kUnsignedLessThan:
+        case kUnsignedGreaterThanOrEqual: {
+          Int32BinopMatcher m(node, true);
+          if (m.right().Is(0)) {
+            VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
+                                 cont);
+          } else {
+            VisitCompare(selector, opcode, g.UseRegister(left),
+                         g.UseImmediate(right), cont);
+          }
+        } break;
+        default:
+          Int32BinopMatcher m(node, true);
+          if (m.right().Is(0)) {
+            VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
+                                 cont);
+          } else {
+            VisitCompare(selector, opcode, g.UseRegister(left),
+                         g.UseRegister(right), cont);
+          }
+      }
+    }
+  } else {
+    VisitCompare(selector, opcode, g.UseRegister(left), g.UseRegister(right),
+                 cont);
+  }
+}
+
+void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
+
+  // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
+  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+    static const size_t kMaxTableSwitchValueRange = 2 << 16;
+    size_t table_space_cost = 10 + 2 * sw.value_range();
+    size_t table_time_cost = 3;
+    size_t lookup_space_cost = 2 + 2 * sw.case_count();
+    size_t lookup_time_cost = sw.case_count();
+    if (sw.case_count() > 0 &&
+        table_space_cost + 3 * table_time_cost <=
+            lookup_space_cost + 3 * lookup_time_cost &&
+        sw.min_value() > std::numeric_limits<int32_t>::min() &&
+        sw.value_range() <= kMaxTableSwitchValueRange) {
+      InstructionOperand index_operand = value_operand;
+      if (sw.min_value()) {
+        index_operand = g.TempRegister();
+        Emit(kRiscvSub32, index_operand, value_operand,
+             g.TempImmediate(sw.min_value()));
+      }
+      // Generate a table lookup.
+      return EmitTableSwitch(sw, index_operand);
+    }
+  }
+
+  // Generate a tree of conditional jumps.
+  return EmitBinarySearchSwitch(sw, value_operand);
+}
+
+void EmitWordCompareZero(InstructionSelector* selector, Node* value,
+                         FlagsContinuation* cont) {
+  RiscvOperandGenerator g(selector);
+  selector->EmitWithContinuation(kRiscvCmpZero,
+                                 g.UseRegisterOrImmediateZero(value), cont);
+}
+
+void VisitAtomicExchange(InstructionSelector* selector, Node* node,
+                         ArchOpcode opcode, AtomicWidth width) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[3];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temp[3];
+  temp[0] = g.TempRegister();
+  temp[1] = g.TempRegister();
+  temp[2] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode) |
+                         AtomicWidthField::encode(width);
+  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
+}
+
+void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
+                                ArchOpcode opcode, AtomicWidth width) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* old_value = node->InputAt(2);
+  Node* new_value = node->InputAt(3);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[4];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(old_value);
+  inputs[input_count++] = g.UseUniqueRegister(new_value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temp[3];
+  temp[0] = g.TempRegister();
+  temp[1] = g.TempRegister();
+  temp[2] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode) |
+                         AtomicWidthField::encode(width);
+  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
+}
+
+void InstructionSelector::VisitFloat32Equal(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitFloat32Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat64Equal(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat64LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitFloat64Compare(this, node, &cont);
+}
+
+void InstructionSelector::VisitFloat64ExtractLowWord32(Node* node) {
+  VisitRR(this, kRiscvFloat64ExtractLowWord32, node);
+}
+
+void InstructionSelector::VisitFloat64ExtractHighWord32(Node* node) {
+  VisitRR(this, kRiscvFloat64ExtractHighWord32, node);
+}
+
+void InstructionSelector::VisitFloat64SilenceNaN(Node* node) {
+  VisitRR(this, kRiscvFloat64SilenceNaN, node);
+}
+
+void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  Emit(kRiscvFloat64InsertLowWord32, g.DefineSameAsFirst(node),
+       g.UseRegister(left), g.UseRegister(right));
+}
+
+void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* left = node->InputAt(0);
+  Node* right = node->InputAt(1);
+  Emit(kRiscvFloat64InsertHighWord32, g.DefineSameAsFirst(node),
+       g.UseRegister(left), g.UseRegister(right));
+}
+
+void InstructionSelector::VisitMemoryBarrier(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvSync, g.NoOutput());
+}
+
+bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
+
+void InstructionSelector::EmitPrepareResults(
+    ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
+    Node* node) {
+  RiscvOperandGenerator g(this);
+
+  int reverse_slot = 1;
+  for (PushParameter output : *results) {
+    if (!output.location.IsCallerFrameSlot()) continue;
+    // Skip any alignment holes in nodes.
+    if (output.node != nullptr) {
+      DCHECK(!call_descriptor->IsCFunctionCall());
+      if (output.location.GetType() == MachineType::Float32()) {
+        MarkAsFloat32(output.node);
+      } else if (output.location.GetType() == MachineType::Float64()) {
+        MarkAsFloat64(output.node);
+      }
+      Emit(kRiscvPeek, g.DefineAsRegister(output.node),
+           g.UseImmediate(reverse_slot));
+    }
+    reverse_slot += output.location.GetSizeInPointers();
+  }
+}
+
+void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
+
+void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
+                                             LinkageLocation location) {}
+
+void InstructionSelector::VisitFloat32Abs(Node* node) {
+  VisitRR(this, kRiscvAbsS, node);
+}
+
+void InstructionSelector::VisitFloat64Abs(Node* node) {
+  VisitRR(this, kRiscvAbsD, node);
+}
+
+void InstructionSelector::VisitFloat32Sqrt(Node* node) {
+  VisitRR(this, kRiscvSqrtS, node);
+}
+
+void InstructionSelector::VisitFloat64Sqrt(Node* node) {
+  VisitRR(this, kRiscvSqrtD, node);
+}
+
+void InstructionSelector::VisitFloat32RoundDown(Node* node) {
+  VisitRR(this, kRiscvFloat32RoundDown, node);
+}
+
+void InstructionSelector::VisitFloat32Add(Node* node) {
+  VisitRRR(this, kRiscvAddS, node);
+}
+
+void InstructionSelector::VisitFloat64Add(Node* node) {
+  VisitRRR(this, kRiscvAddD, node);
+}
+
+void InstructionSelector::VisitFloat32Sub(Node* node) {
+  VisitRRR(this, kRiscvSubS, node);
+}
+
+void InstructionSelector::VisitFloat64Sub(Node* node) {
+  VisitRRR(this, kRiscvSubD, node);
+}
+
+void InstructionSelector::VisitFloat32Mul(Node* node) {
+  VisitRRR(this, kRiscvMulS, node);
+}
+
+void InstructionSelector::VisitFloat64Mul(Node* node) {
+  VisitRRR(this, kRiscvMulD, node);
+}
+
+void InstructionSelector::VisitFloat32Div(Node* node) {
+  VisitRRR(this, kRiscvDivS, node);
+}
+
+void InstructionSelector::VisitFloat64Div(Node* node) {
+  VisitRRR(this, kRiscvDivD, node);
+}
+
+void InstructionSelector::VisitFloat64Mod(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvModD, g.DefineAsFixed(node, fa0),
+       g.UseFixed(node->InputAt(0), fa0), g.UseFixed(node->InputAt(1), fa1))
+      ->MarkAsCall();
+}
+
+void InstructionSelector::VisitFloat32Max(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvFloat32Max, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat64Max(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvFloat64Max, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat32Min(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvFloat32Min, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitFloat64Min(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvFloat64Min, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
+}
+
+void InstructionSelector::VisitTruncateFloat64ToWord32(Node* node) {
+  VisitRR(this, kArchTruncateDoubleToI, node);
+}
+
+void InstructionSelector::VisitRoundFloat64ToInt32(Node* node) {
+  VisitRR(this, kRiscvTruncWD, node);
+}
+
+void InstructionSelector::VisitTruncateFloat64ToFloat32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* value = node->InputAt(0);
+  // Match TruncateFloat64ToFloat32(ChangeInt32ToFloat64) to corresponding
+  // instruction.
+  if (CanCover(node, value) &&
+      value->opcode() == IrOpcode::kChangeInt32ToFloat64) {
+    Emit(kRiscvCvtSW, g.DefineAsRegister(node),
+         g.UseRegister(value->InputAt(0)));
+    return;
+  }
+  VisitRR(this, kRiscvCvtSD, node);
+}
+
+void InstructionSelector::VisitWord32Shl(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32And() && CanCover(node, m.left().node()) &&
+      m.right().IsInRange(1, 31)) {
+    RiscvOperandGenerator g(this);
+    Int32BinopMatcher mleft(m.left().node());
+    // Match Word32Shl(Word32And(x, mask), imm) to Shl where the mask is
+    // contiguous, and the shift immediate non-zero.
+    if (mleft.right().HasResolvedValue()) {
+      uint32_t mask = mleft.right().ResolvedValue();
+      uint32_t mask_width = base::bits::CountPopulation(mask);
+      uint32_t mask_msb = base::bits::CountLeadingZeros32(mask);
+      if ((mask_width != 0) && (mask_msb + mask_width == 32)) {
+        uint32_t shift = m.right().ResolvedValue();
+        DCHECK_EQ(0u, base::bits::CountTrailingZeros32(mask));
+        DCHECK_NE(0u, shift);
+        if ((shift + mask_width) >= 32) {
+          // If the mask is contiguous and reaches or extends beyond the top
+          // bit, only the shift is needed.
+          Emit(kRiscvShl32, g.DefineAsRegister(node),
+               g.UseRegister(mleft.left().node()),
+               g.UseImmediate(m.right().node()));
+          return;
+        }
+      }
+    }
+  }
+  VisitRRO(this, kRiscvShl32, node);
+}
+
+void InstructionSelector::VisitWord32Shr(Node* node) {
+  VisitRRO(this, kRiscvShr32, node);
+}
+
+void InstructionSelector::VisitWord32Sar(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32Shl() && CanCover(node, m.left().node())) {
+    Int32BinopMatcher mleft(m.left().node());
+    if (m.right().HasResolvedValue() && mleft.right().HasResolvedValue()) {
+      RiscvOperandGenerator g(this);
+      uint32_t sar = m.right().ResolvedValue();
+      uint32_t shl = mleft.right().ResolvedValue();
+      if ((sar == shl) && (sar == 16)) {
+        Emit(kRiscvSignExtendShort, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()));
+        return;
+      } else if ((sar == shl) && (sar == 24)) {
+        Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()));
+        return;
+      } else if ((sar == shl) && (sar == 32)) {
+        Emit(kRiscvShl32, g.DefineAsRegister(node),
+             g.UseRegister(mleft.left().node()), g.TempImmediate(0));
+        return;
+      }
+    }
+  }
+  VisitRRO(this, kRiscvSar32, node);
+}
+
+void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand src1 = g.TempSimd128Register();
+  InstructionOperand src2 = g.TempSimd128Register();
+  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
+  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0006000400020000),
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0007000500030001),
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVwadd, g.DefineAsRegister(node), src1, src2,
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(mf2)));
+}
+
+void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand src1 = g.TempSimd128Register();
+  InstructionOperand src2 = g.TempSimd128Register();
+  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
+  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0006000400020000),
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0007000500030001),
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVwaddu, g.DefineAsRegister(node), src1, src2,
+       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(mf2)));
+}
+
+void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand src1 = g.TempSimd128Register();
+  InstructionOperand src2 = g.TempSimd128Register();
+  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
+  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0E0C0A0806040200),
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0F0D0B0907050301),
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVwadd, g.DefineAsRegister(node), src1, src2,
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(mf2)));
+}
+
+void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand src1 = g.TempSimd128Register();
+  InstructionOperand src2 = g.TempSimd128Register();
+  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
+  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0E0C0A0806040200),
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0F0D0B0907050301),
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
+  Emit(kRiscvVwaddu, g.DefineAsRegister(node), src1, src2,
+       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(mf2)));
+}
+
+#define SIMD_TYPE_LIST(V) \
+  V(F32x4)                \
+  V(I64x2)                \
+  V(I32x4)                \
+  V(I16x8)                \
+  V(I8x16)
+
+#define SIMD_UNOP_LIST(V)                                       \
+  V(F64x2Abs, kRiscvF64x2Abs)                                   \
+  V(F64x2Neg, kRiscvF64x2Neg)                                   \
+  V(F64x2Sqrt, kRiscvF64x2Sqrt)                                 \
+  V(F64x2ConvertLowI32x4S, kRiscvF64x2ConvertLowI32x4S)         \
+  V(F64x2ConvertLowI32x4U, kRiscvF64x2ConvertLowI32x4U)         \
+  V(F64x2PromoteLowF32x4, kRiscvF64x2PromoteLowF32x4)           \
+  V(F64x2Ceil, kRiscvF64x2Ceil)                                 \
+  V(F64x2Floor, kRiscvF64x2Floor)                               \
+  V(F64x2Trunc, kRiscvF64x2Trunc)                               \
+  V(F64x2NearestInt, kRiscvF64x2NearestInt)                     \
+  V(I64x2Neg, kRiscvI64x2Neg)                                   \
+  V(I64x2Abs, kRiscvI64x2Abs)                                   \
+  V(I64x2BitMask, kRiscvI64x2BitMask)                           \
+  V(F32x4SConvertI32x4, kRiscvF32x4SConvertI32x4)               \
+  V(F32x4UConvertI32x4, kRiscvF32x4UConvertI32x4)               \
+  V(F32x4Abs, kRiscvF32x4Abs)                                   \
+  V(F32x4Neg, kRiscvF32x4Neg)                                   \
+  V(F32x4Sqrt, kRiscvF32x4Sqrt)                                 \
+  V(F32x4DemoteF64x2Zero, kRiscvF32x4DemoteF64x2Zero)           \
+  V(F32x4Ceil, kRiscvF32x4Ceil)                                 \
+  V(F32x4Floor, kRiscvF32x4Floor)                               \
+  V(F32x4Trunc, kRiscvF32x4Trunc)                               \
+  V(F32x4NearestInt, kRiscvF32x4NearestInt)                     \
+  V(I32x4RelaxedTruncF32x4S, kRiscvI32x4SConvertF32x4)          \
+  V(I32x4RelaxedTruncF32x4U, kRiscvI32x4UConvertF32x4)          \
+  V(I32x4RelaxedTruncF64x2SZero, kRiscvI32x4TruncSatF64x2SZero) \
+  V(I32x4RelaxedTruncF64x2UZero, kRiscvI32x4TruncSatF64x2UZero) \
+  V(I64x2SConvertI32x4Low, kRiscvI64x2SConvertI32x4Low)         \
+  V(I64x2SConvertI32x4High, kRiscvI64x2SConvertI32x4High)       \
+  V(I64x2UConvertI32x4Low, kRiscvI64x2UConvertI32x4Low)         \
+  V(I64x2UConvertI32x4High, kRiscvI64x2UConvertI32x4High)       \
+  V(I32x4SConvertF32x4, kRiscvI32x4SConvertF32x4)               \
+  V(I32x4UConvertF32x4, kRiscvI32x4UConvertF32x4)               \
+  V(I32x4Neg, kRiscvI32x4Neg)                                   \
+  V(I32x4SConvertI16x8Low, kRiscvI32x4SConvertI16x8Low)         \
+  V(I32x4SConvertI16x8High, kRiscvI32x4SConvertI16x8High)       \
+  V(I32x4UConvertI16x8Low, kRiscvI32x4UConvertI16x8Low)         \
+  V(I32x4UConvertI16x8High, kRiscvI32x4UConvertI16x8High)       \
+  V(I32x4Abs, kRiscvI32x4Abs)                                   \
+  V(I32x4BitMask, kRiscvI32x4BitMask)                           \
+  V(I32x4TruncSatF64x2SZero, kRiscvI32x4TruncSatF64x2SZero)     \
+  V(I32x4TruncSatF64x2UZero, kRiscvI32x4TruncSatF64x2UZero)     \
+  V(I16x8Neg, kRiscvI16x8Neg)                                   \
+  V(I16x8SConvertI8x16Low, kRiscvI16x8SConvertI8x16Low)         \
+  V(I16x8SConvertI8x16High, kRiscvI16x8SConvertI8x16High)       \
+  V(I16x8UConvertI8x16Low, kRiscvI16x8UConvertI8x16Low)         \
+  V(I16x8UConvertI8x16High, kRiscvI16x8UConvertI8x16High)       \
+  V(I16x8Abs, kRiscvI16x8Abs)                                   \
+  V(I16x8BitMask, kRiscvI16x8BitMask)                           \
+  V(I8x16Neg, kRiscvI8x16Neg)                                   \
+  V(I8x16Abs, kRiscvI8x16Abs)                                   \
+  V(I8x16BitMask, kRiscvI8x16BitMask)                           \
+  V(I8x16Popcnt, kRiscvI8x16Popcnt)                             \
+  V(S128Not, kRiscvS128Not)                                     \
+  V(V128AnyTrue, kRiscvV128AnyTrue)                             \
+  V(I32x4AllTrue, kRiscvI32x4AllTrue)                           \
+  V(I16x8AllTrue, kRiscvI16x8AllTrue)                           \
+  V(I8x16AllTrue, kRiscvI8x16AllTrue)                           \
+  V(I64x2AllTrue, kRiscvI64x2AllTrue)
+
+#define SIMD_SHIFT_OP_LIST(V) \
+  V(I64x2Shl)                 \
+  V(I64x2ShrS)                \
+  V(I64x2ShrU)                \
+  V(I32x4Shl)                 \
+  V(I32x4ShrS)                \
+  V(I32x4ShrU)                \
+  V(I16x8Shl)                 \
+  V(I16x8ShrS)                \
+  V(I16x8ShrU)                \
+  V(I8x16Shl)                 \
+  V(I8x16ShrS)                \
+  V(I8x16ShrU)
+
+#define SIMD_BINOP_LIST(V)                              \
+  V(F64x2Add, kRiscvF64x2Add)                           \
+  V(F64x2Sub, kRiscvF64x2Sub)                           \
+  V(F64x2Mul, kRiscvF64x2Mul)                           \
+  V(F64x2Div, kRiscvF64x2Div)                           \
+  V(F64x2Min, kRiscvF64x2Min)                           \
+  V(F64x2Max, kRiscvF64x2Max)                           \
+  V(F64x2Eq, kRiscvF64x2Eq)                             \
+  V(F64x2Ne, kRiscvF64x2Ne)                             \
+  V(F64x2Lt, kRiscvF64x2Lt)                             \
+  V(F64x2Le, kRiscvF64x2Le)                             \
+  V(I64x2Eq, kRiscvI64x2Eq)                             \
+  V(I64x2Ne, kRiscvI64x2Ne)                             \
+  V(I64x2GtS, kRiscvI64x2GtS)                           \
+  V(I64x2GeS, kRiscvI64x2GeS)                           \
+  V(I64x2Add, kRiscvI64x2Add)                           \
+  V(I64x2Sub, kRiscvI64x2Sub)                           \
+  V(I64x2Mul, kRiscvI64x2Mul)                           \
+  V(F32x4Add, kRiscvF32x4Add)                           \
+  V(F32x4Sub, kRiscvF32x4Sub)                           \
+  V(F32x4Mul, kRiscvF32x4Mul)                           \
+  V(F32x4Div, kRiscvF32x4Div)                           \
+  V(F32x4Max, kRiscvF32x4Max)                           \
+  V(F32x4Min, kRiscvF32x4Min)                           \
+  V(F32x4Eq, kRiscvF32x4Eq)                             \
+  V(F32x4Ne, kRiscvF32x4Ne)                             \
+  V(F32x4Lt, kRiscvF32x4Lt)                             \
+  V(F32x4Le, kRiscvF32x4Le)                             \
+  V(F32x4RelaxedMin, kRiscvF32x4Min)                    \
+  V(F32x4RelaxedMax, kRiscvF32x4Max)                    \
+  V(F64x2RelaxedMin, kRiscvF64x2Min)                    \
+  V(F64x2RelaxedMax, kRiscvF64x2Max)                    \
+  V(I32x4Add, kRiscvI32x4Add)                           \
+  V(I32x4Sub, kRiscvI32x4Sub)                           \
+  V(I32x4Mul, kRiscvI32x4Mul)                           \
+  V(I32x4MaxS, kRiscvI32x4MaxS)                         \
+  V(I32x4MinS, kRiscvI32x4MinS)                         \
+  V(I32x4MaxU, kRiscvI32x4MaxU)                         \
+  V(I32x4MinU, kRiscvI32x4MinU)                         \
+  V(I32x4Eq, kRiscvI32x4Eq)                             \
+  V(I32x4Ne, kRiscvI32x4Ne)                             \
+  V(I32x4GtS, kRiscvI32x4GtS)                           \
+  V(I32x4GeS, kRiscvI32x4GeS)                           \
+  V(I32x4GtU, kRiscvI32x4GtU)                           \
+  V(I32x4GeU, kRiscvI32x4GeU)                           \
+  V(I16x8Add, kRiscvI16x8Add)                           \
+  V(I16x8AddSatS, kRiscvI16x8AddSatS)                   \
+  V(I16x8AddSatU, kRiscvI16x8AddSatU)                   \
+  V(I16x8Sub, kRiscvI16x8Sub)                           \
+  V(I16x8SubSatS, kRiscvI16x8SubSatS)                   \
+  V(I16x8SubSatU, kRiscvI16x8SubSatU)                   \
+  V(I16x8Mul, kRiscvI16x8Mul)                           \
+  V(I16x8MaxS, kRiscvI16x8MaxS)                         \
+  V(I16x8MinS, kRiscvI16x8MinS)                         \
+  V(I16x8MaxU, kRiscvI16x8MaxU)                         \
+  V(I16x8MinU, kRiscvI16x8MinU)                         \
+  V(I16x8Eq, kRiscvI16x8Eq)                             \
+  V(I16x8Ne, kRiscvI16x8Ne)                             \
+  V(I16x8GtS, kRiscvI16x8GtS)                           \
+  V(I16x8GeS, kRiscvI16x8GeS)                           \
+  V(I16x8GtU, kRiscvI16x8GtU)                           \
+  V(I16x8GeU, kRiscvI16x8GeU)                           \
+  V(I16x8RoundingAverageU, kRiscvI16x8RoundingAverageU) \
+  V(I16x8Q15MulRSatS, kRiscvI16x8Q15MulRSatS)           \
+  V(I16x8SConvertI32x4, kRiscvI16x8SConvertI32x4)       \
+  V(I16x8UConvertI32x4, kRiscvI16x8UConvertI32x4)       \
+  V(I8x16Add, kRiscvI8x16Add)                           \
+  V(I8x16AddSatS, kRiscvI8x16AddSatS)                   \
+  V(I8x16AddSatU, kRiscvI8x16AddSatU)                   \
+  V(I8x16Sub, kRiscvI8x16Sub)                           \
+  V(I8x16SubSatS, kRiscvI8x16SubSatS)                   \
+  V(I8x16SubSatU, kRiscvI8x16SubSatU)                   \
+  V(I8x16MaxS, kRiscvI8x16MaxS)                         \
+  V(I8x16MinS, kRiscvI8x16MinS)                         \
+  V(I8x16MaxU, kRiscvI8x16MaxU)                         \
+  V(I8x16MinU, kRiscvI8x16MinU)                         \
+  V(I8x16Eq, kRiscvI8x16Eq)                             \
+  V(I8x16Ne, kRiscvI8x16Ne)                             \
+  V(I8x16GtS, kRiscvI8x16GtS)                           \
+  V(I8x16GeS, kRiscvI8x16GeS)                           \
+  V(I8x16GtU, kRiscvI8x16GtU)                           \
+  V(I8x16GeU, kRiscvI8x16GeU)                           \
+  V(I8x16RoundingAverageU, kRiscvI8x16RoundingAverageU) \
+  V(I8x16SConvertI16x8, kRiscvI8x16SConvertI16x8)       \
+  V(I8x16UConvertI16x8, kRiscvI8x16UConvertI16x8)       \
+  V(S128And, kRiscvS128And)                             \
+  V(S128Or, kRiscvS128Or)                               \
+  V(S128Xor, kRiscvS128Xor)                             \
+  V(S128AndNot, kRiscvS128AndNot)
+
+void InstructionSelector::VisitS128Const(Node* node) {
+  RiscvOperandGenerator g(this);
+  static const int kUint32Immediates = kSimd128Size / sizeof(uint32_t);
+  uint32_t val[kUint32Immediates];
+  memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
+  // If all bytes are zeros or ones, avoid emitting code for generic constants
+  bool all_zeros = !(val[0] || val[1] || val[2] || val[3]);
+  bool all_ones = val[0] == UINT32_MAX && val[1] == UINT32_MAX &&
+                  val[2] == UINT32_MAX && val[3] == UINT32_MAX;
+  InstructionOperand dst = g.DefineAsRegister(node);
+  if (all_zeros) {
+    Emit(kRiscvS128Zero, dst);
+  } else if (all_ones) {
+    Emit(kRiscvS128AllOnes, dst);
+  } else {
+    Emit(kRiscvS128Const, dst, g.UseImmediate(val[0]), g.UseImmediate(val[1]),
+         g.UseImmediate(val[2]), g.UseImmediate(val[3]));
+  }
+}
+
+void InstructionSelector::VisitS128Zero(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvS128Zero, g.DefineAsRegister(node));
+}
+
+#define SIMD_VISIT_SPLAT(Type)                               \
+  void InstructionSelector::Visit##Type##Splat(Node* node) { \
+    VisitRR(this, kRiscv##Type##Splat, node);                \
+  }
+SIMD_TYPE_LIST(SIMD_VISIT_SPLAT)
+SIMD_VISIT_SPLAT(F64x2)
+#undef SIMD_VISIT_SPLAT
+
+#define SIMD_VISIT_EXTRACT_LANE(Type, Sign)                              \
+  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) { \
+    VisitRRI(this, kRiscv##Type##ExtractLane##Sign, node);               \
+  }
+SIMD_VISIT_EXTRACT_LANE(F64x2, )
+SIMD_VISIT_EXTRACT_LANE(F32x4, )
+SIMD_VISIT_EXTRACT_LANE(I32x4, )
+SIMD_VISIT_EXTRACT_LANE(I64x2, )
+SIMD_VISIT_EXTRACT_LANE(I16x8, U)
+SIMD_VISIT_EXTRACT_LANE(I16x8, S)
+SIMD_VISIT_EXTRACT_LANE(I8x16, U)
+SIMD_VISIT_EXTRACT_LANE(I8x16, S)
+#undef SIMD_VISIT_EXTRACT_LANE
+
+#define SIMD_VISIT_REPLACE_LANE(Type)                              \
+  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) { \
+    VisitRRIR(this, kRiscv##Type##ReplaceLane, node);              \
+  }
+SIMD_TYPE_LIST(SIMD_VISIT_REPLACE_LANE)
+SIMD_VISIT_REPLACE_LANE(F64x2)
+#undef SIMD_VISIT_REPLACE_LANE
+
+#define SIMD_VISIT_UNOP(Name, instruction)            \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRR(this, instruction, node);                 \
+  }
+SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
+#undef SIMD_VISIT_UNOP
+
+#define SIMD_VISIT_SHIFT_OP(Name)                     \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitSimdShift(this, kRiscv##Name, node);         \
+  }
+SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
+#undef SIMD_VISIT_SHIFT_OP
+
+#define SIMD_VISIT_BINOP(Name, instruction)           \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRRR(this, instruction, node);                \
+  }
+SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
+#undef SIMD_VISIT_BINOP
+
+void InstructionSelector::VisitS128Select(Node* node) {
+  VisitRRRR(this, kRiscvS128Select, node);
+}
+
+#define SIMD_VISIT_SELECT_LANE(Name)                  \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRRRR(this, kRiscvS128Select, node);          \
+  }
+SIMD_VISIT_SELECT_LANE(I8x16RelaxedLaneSelect)
+SIMD_VISIT_SELECT_LANE(I16x8RelaxedLaneSelect)
+SIMD_VISIT_SELECT_LANE(I32x4RelaxedLaneSelect)
+SIMD_VISIT_SELECT_LANE(I64x2RelaxedLaneSelect)
+#undef SIMD_VISIT_SELECT_LANE
+
+#define VISIT_SIMD_QFMOP(Name, instruction)           \
+  void InstructionSelector::Visit##Name(Node* node) { \
+    VisitRRRR(this, instruction, node);               \
+  }
+VISIT_SIMD_QFMOP(F64x2Qfma, kRiscvF64x2Qfma)
+VISIT_SIMD_QFMOP(F64x2Qfms, kRiscvF64x2Qfms)
+VISIT_SIMD_QFMOP(F32x4Qfma, kRiscvF32x4Qfma)
+VISIT_SIMD_QFMOP(F32x4Qfms, kRiscvF32x4Qfms)
+#undef VISIT_SIMD_QFMOP
+
+void InstructionSelector::VisitI32x4DotI16x8S(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand temp = g.TempFpRegister(v16);
+  InstructionOperand temp1 = g.TempFpRegister(v14);
+  InstructionOperand temp2 = g.TempFpRegister(v30);
+  InstructionOperand dst = g.DefineAsRegister(node);
+  this->Emit(kRiscvVwmul, temp, g.UseRegister(node->InputAt(0)),
+             g.UseRegister(node->InputAt(1)), g.UseImmediate(E16),
+             g.UseImmediate(m1));
+  this->Emit(kRiscvVcompress, temp2, temp, g.UseImmediate(0b01010101),
+             g.UseImmediate(E32), g.UseImmediate(m2));
+  this->Emit(kRiscvVcompress, temp1, temp, g.UseImmediate(0b10101010),
+             g.UseImmediate(E32), g.UseImmediate(m2));
+  this->Emit(kRiscvVaddVv, dst, temp1, temp2, g.UseImmediate(E32),
+             g.UseImmediate(m1));
+}
+
+void InstructionSelector::VisitI8x16Shuffle(Node* node) {
+  uint8_t shuffle[kSimd128Size];
+  bool is_swizzle;
+  CanonicalizeShuffle(node, shuffle, &is_swizzle);
+  Node* input0 = node->InputAt(0);
+  Node* input1 = node->InputAt(1);
+  RiscvOperandGenerator g(this);
+  // uint8_t shuffle32x4[4];
+  // ArchOpcode opcode;
+  // if (TryMatchArchShuffle(shuffle, arch_shuffles, arraysize(arch_shuffles),
+  //                         is_swizzle, &opcode)) {
+  //   VisitRRR(this, opcode, node);
+  //   return;
+  // }
+  // uint8_t offset;
+  // if (wasm::SimdShuffle::TryMatchConcat(shuffle, &offset)) {
+  //   Emit(kRiscvS8x16Concat, g.DefineSameAsFirst(node), g.UseRegister(input1),
+  //        g.UseRegister(input0), g.UseImmediate(offset));
+  //   return;
+  // }
+  // if (wasm::SimdShuffle::TryMatch32x4Shuffle(shuffle, shuffle32x4)) {
+  //   Emit(kRiscvS32x4Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
+  //        g.UseRegister(input1),
+  //        g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle32x4)));
+  //   return;
+  // }
+  Emit(kRiscvI8x16Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
+       g.UseRegister(input1),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 4)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 8)),
+       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
+}
+
+void InstructionSelector::VisitI8x16Swizzle(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand temps[] = {g.TempSimd128Register()};
+  // We don't want input 0 or input 1 to be the same as output, since we will
+  // modify output before do the calculation.
+  Emit(kRiscvVrgather, g.DefineAsRegister(node),
+       g.UseUniqueRegister(node->InputAt(0)),
+       g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E8),
+       g.UseImmediate(m1), arraysize(temps), temps);
+}
+
+void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvSignExtendShort, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+#define VISIT_EXT_MUL(OPCODE1, OPCODE2, TYPE)                            \
+  void InstructionSelector::Visit##OPCODE1##ExtMulLow##OPCODE2##S(       \
+      Node* node) {                                                      \
+    RiscvOperandGenerator g(this);                                       \
+    Emit(kRiscvVwmul, g.DefineAsRegister(node),                          \
+         g.UseUniqueRegister(node->InputAt(0)),                          \
+         g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E##TYPE), \
+         g.UseImmediate(mf2));                                           \
+  }                                                                      \
+  void InstructionSelector::Visit##OPCODE1##ExtMulHigh##OPCODE2##S(      \
+      Node* node) {                                                      \
+    RiscvOperandGenerator g(this);                                       \
+    InstructionOperand t1 = g.TempFpRegister(v16);                       \
+    Emit(kRiscvVslidedown, t1, g.UseUniqueRegister(node->InputAt(0)),    \
+         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
+         g.UseImmediate(m1));                                            \
+    InstructionOperand t2 = g.TempFpRegister(v17);                       \
+    Emit(kRiscvVslidedown, t2, g.UseUniqueRegister(node->InputAt(1)),    \
+         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
+         g.UseImmediate(m1));                                            \
+    Emit(kRiscvVwmul, g.DefineAsRegister(node), t1, t2,                  \
+         g.UseImmediate(E##TYPE), g.UseImmediate(mf2));                  \
+  }                                                                      \
+  void InstructionSelector::Visit##OPCODE1##ExtMulLow##OPCODE2##U(       \
+      Node* node) {                                                      \
+    RiscvOperandGenerator g(this);                                       \
+    Emit(kRiscvVwmulu, g.DefineAsRegister(node),                         \
+         g.UseUniqueRegister(node->InputAt(0)),                          \
+         g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E##TYPE), \
+         g.UseImmediate(mf2));                                           \
+  }                                                                      \
+  void InstructionSelector::Visit##OPCODE1##ExtMulHigh##OPCODE2##U(      \
+      Node* node) {                                                      \
+    RiscvOperandGenerator g(this);                                       \
+    InstructionOperand t1 = g.TempFpRegister(v16);                       \
+    Emit(kRiscvVslidedown, t1, g.UseUniqueRegister(node->InputAt(0)),    \
+         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
+         g.UseImmediate(m1));                                            \
+    InstructionOperand t2 = g.TempFpRegister(v17);                       \
+    Emit(kRiscvVslidedown, t2, g.UseUniqueRegister(node->InputAt(1)),    \
+         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
+         g.UseImmediate(m1));                                            \
+    Emit(kRiscvVwmulu, g.DefineAsRegister(node), t1, t2,                 \
+         g.UseImmediate(E##TYPE), g.UseImmediate(mf2));                  \
+  }
+
+VISIT_EXT_MUL(I64x2, I32x4, 32)
+VISIT_EXT_MUL(I32x4, I16x8, 16)
+VISIT_EXT_MUL(I16x8, I8x16, 8)
+#undef VISIT_EXT_MUL
+
+void InstructionSelector::VisitF32x4Pmin(Node* node) {
+  VisitUniqueRRR(this, kRiscvF32x4Pmin, node);
+}
+
+void InstructionSelector::VisitF32x4Pmax(Node* node) {
+  VisitUniqueRRR(this, kRiscvF32x4Pmax, node);
+}
+
+void InstructionSelector::VisitF64x2Pmin(Node* node) {
+  VisitUniqueRRR(this, kRiscvF64x2Pmin, node);
+}
+
+void InstructionSelector::VisitF64x2Pmax(Node* node) {
+  VisitUniqueRRR(this, kRiscvF64x2Pmax, node);
+}
+
+// static
+MachineOperatorBuilder::AlignmentRequirements
+InstructionSelector::AlignmentRequirements() {
+#ifdef RISCV_HAS_NO_UNALIGNED
+  return MachineOperatorBuilder::AlignmentRequirements::
+      NoUnalignedAccessSupport();
+#else
+  return MachineOperatorBuilder::AlignmentRequirements::
+      FullUnalignedAccessSupport();
+#endif
+}
+
+void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
+                                                        int first_input_index,
+                                                        Node* node) {
+  UNREACHABLE();
+}
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
+
+#undef SIMD_BINOP_LIST
+#undef SIMD_SHIFT_OP_LIST
+#undef SIMD_UNOP_LIST
+#undef SIMD_TYPE_LIST
+#endif  // V8_COMPILER_BACKEND_RISCV_INSTRUCTION_SELECTOR_RISCV_H_
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv32.cc b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
new file mode 100644
index 00000000000..f2d2612415c
--- /dev/null
+++ b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
@@ -0,0 +1,1325 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/base/bits.h"
+#include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/riscv/instruction-selector-riscv.h"
+#include "src/compiler/node-matchers.h"
+#include "src/compiler/node-properties.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+bool RiscvOperandGenerator::IsIntegerConstant(Node* node) {
+  return (node->opcode() == IrOpcode::kInt32Constant);
+}
+
+int64_t RiscvOperandGenerator::GetIntegerConstantValue(Node* node) {
+  DCHECK_EQ(IrOpcode::kInt32Constant, node->opcode());
+  return OpParameter<int32_t>(node->op());
+}
+
+bool RiscvOperandGenerator::CanBeImmediate(int64_t value,
+                                           InstructionCode opcode) {
+  switch (ArchOpcodeField::decode(opcode)) {
+    case kRiscvShl32:
+    case kRiscvSar32:
+    case kRiscvShr32:
+      return is_uint5(value);
+    case kRiscvAdd32:
+    case kRiscvAnd32:
+    case kRiscvAnd:
+    case kRiscvOr32:
+    case kRiscvOr:
+    case kRiscvTst:
+    case kRiscvXor:
+      return is_int12(value);
+    case kRiscvLb:
+    case kRiscvLbu:
+    case kRiscvSb:
+    case kRiscvLh:
+    case kRiscvLhu:
+    case kRiscvSh:
+    case kRiscvLw:
+    case kRiscvSw:
+    case kRiscvLoadFloat:
+    case kRiscvStoreFloat:
+    case kRiscvLoadDouble:
+    case kRiscvStoreDouble:
+      return is_int32(value);
+    default:
+      return is_int12(value);
+  }
+}
+
+void EmitLoad(InstructionSelector* selector, Node* node, InstructionCode opcode,
+              Node* output = nullptr) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+
+  ExternalReferenceMatcher m(base);
+  if (m.HasResolvedValue() && g.IsIntegerConstant(index) &&
+      selector->CanAddressRelativeToRootsRegister(m.ResolvedValue())) {
+    ptrdiff_t const delta =
+        g.GetIntegerConstantValue(index) +
+        TurboAssemblerBase::RootRegisterOffsetForExternalReference(
+            selector->isolate(), m.ResolvedValue());
+    // Check that the delta is a 32-bit integer due to the limitations of
+    // immediate operands.
+    if (is_int32(delta)) {
+      opcode |= AddressingModeField::encode(kMode_Root);
+      selector->Emit(opcode,
+                     g.DefineAsRegister(output == nullptr ? node : output),
+                     g.UseImmediate(static_cast<int32_t>(delta)));
+      return;
+    }
+  }
+
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(output == nullptr ? node : output),
+                   g.UseRegister(base), g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(output == nullptr ? node : output),
+                   addr_reg, g.TempImmediate(0));
+  }
+}
+
+void EmitS128Load(InstructionSelector* selector, Node* node,
+                  InstructionCode opcode, VSew sew, Vlmul lmul) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(node), g.UseRegister(base),
+                   g.UseImmediate(index), g.UseImmediate(sew),
+                   g.UseImmediate(lmul));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI),
+                   g.DefineAsRegister(node), addr_reg, g.TempImmediate(0),
+                   g.UseImmediate(sew), g.UseImmediate(lmul));
+  }
+}
+
+void InstructionSelector::VisitStoreLane(Node* node) {
+  StoreLaneParameters params = StoreLaneParametersOf(node->op());
+  LoadStoreLaneParams f(params.rep, params.laneidx);
+  InstructionCode opcode = kRiscvS128StoreLane;
+  opcode |= MiscField::encode(f.sz);
+
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  InstructionOperand addr_reg = g.TempRegister();
+  Emit(kRiscvAdd32, addr_reg, g.UseRegister(base), g.UseRegister(index));
+  InstructionOperand inputs[4] = {
+      g.UseRegister(node->InputAt(2)),
+      g.UseImmediate(f.laneidx),
+      addr_reg,
+      g.TempImmediate(0),
+  };
+  opcode |= AddressingModeField::encode(kMode_MRI);
+  Emit(opcode, 0, nullptr, 4, inputs);
+}
+
+void InstructionSelector::VisitLoadLane(Node* node) {
+  LoadLaneParameters params = LoadLaneParametersOf(node->op());
+  LoadStoreLaneParams f(params.rep.representation(), params.laneidx);
+  InstructionCode opcode = kRiscvS128LoadLane;
+  opcode |= MiscField::encode(f.sz);
+
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  InstructionOperand addr_reg = g.TempRegister();
+  Emit(kRiscvAdd32, addr_reg, g.UseRegister(base), g.UseRegister(index));
+  opcode |= AddressingModeField::encode(kMode_MRI);
+  Emit(opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(2)),
+       g.UseImmediate(params.laneidx), addr_reg, g.TempImmediate(0));
+}
+
+void InstructionSelector::VisitLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+
+  InstructionCode opcode = kArchNop;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kFloat32:
+      opcode = kRiscvLoadFloat;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kRiscvLoadDouble;
+      break;
+    case MachineRepresentation::kBit:  // Fall through.
+    case MachineRepresentation::kWord8:
+      opcode = load_rep.IsUnsigned() ? kRiscvLbu : kRiscvLb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsUnsigned() ? kRiscvLhu : kRiscvLh;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:         // Fall through.
+    case MachineRepresentation::kWord32:
+      opcode = kRiscvLw;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kRiscvRvvLd;
+      break;
+    case MachineRepresentation::kCompressedPointer:
+    case MachineRepresentation::kCompressed:
+    case MachineRepresentation::kSandboxedPointer:
+    case MachineRepresentation::kMapWord:  // Fall through.
+    case MachineRepresentation::kWord64:
+    case MachineRepresentation::kNone:
+    case MachineRepresentation::kSimd256:  // Fall through.
+      UNREACHABLE();
+  }
+
+  EmitLoad(this, node, opcode);
+}
+
+void InstructionSelector::VisitStore(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  StoreRepresentation store_rep = StoreRepresentationOf(node->op());
+  WriteBarrierKind write_barrier_kind = store_rep.write_barrier_kind();
+  MachineRepresentation rep = store_rep.representation();
+
+  // TODO(riscv): I guess this could be done in a better way.
+  if (write_barrier_kind != kNoWriteBarrier &&
+      V8_LIKELY(!FLAG_disable_write_barriers)) {
+    DCHECK(CanBeTaggedPointer(rep));
+    InstructionOperand inputs[3];
+    size_t input_count = 0;
+    inputs[input_count++] = g.UseUniqueRegister(base);
+    inputs[input_count++] = g.UseUniqueRegister(index);
+    inputs[input_count++] = g.UseUniqueRegister(value);
+    RecordWriteMode record_write_mode =
+        WriteBarrierKindToRecordWriteMode(write_barrier_kind);
+    InstructionOperand temps[] = {g.TempRegister(), g.TempRegister()};
+    size_t const temp_count = arraysize(temps);
+    InstructionCode code = kArchStoreWithWriteBarrier;
+    code |= MiscField::encode(static_cast<int>(record_write_mode));
+    Emit(code, 0, nullptr, input_count, inputs, temp_count, temps);
+  } else {
+    ArchOpcode opcode;
+    switch (rep) {
+      case MachineRepresentation::kFloat32:
+        opcode = kRiscvStoreFloat;
+        break;
+      case MachineRepresentation::kFloat64:
+        opcode = kRiscvStoreDouble;
+        break;
+      case MachineRepresentation::kBit:  // Fall through.
+      case MachineRepresentation::kWord8:
+        opcode = kRiscvSb;
+        break;
+      case MachineRepresentation::kWord16:
+        opcode = kRiscvSh;
+        break;
+      case MachineRepresentation::kTaggedSigned:   // Fall through.
+      case MachineRepresentation::kTaggedPointer:  // Fall through.
+      case MachineRepresentation::kTagged:
+      case MachineRepresentation::kWord32:
+        opcode = kRiscvSw;
+        break;
+      case MachineRepresentation::kSimd128:
+        opcode = kRiscvRvvSt;
+        break;
+      case MachineRepresentation::kCompressedPointer:  // Fall through.
+      case MachineRepresentation::kCompressed:
+        UNREACHABLE();
+      case MachineRepresentation::kSandboxedPointer:
+      case MachineRepresentation::kMapWord:  // Fall through.
+      case MachineRepresentation::kNone:
+      case MachineRepresentation::kWord64:
+      case MachineRepresentation::kSimd256:  // Fall through.
+        UNREACHABLE();
+    }
+
+    if (g.CanBeImmediate(index, opcode)) {
+      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+           g.UseRegister(base), g.UseImmediate(index),
+           g.UseRegisterOrImmediateZero(value));
+    } else {
+      InstructionOperand addr_reg = g.TempRegister();
+      Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None), addr_reg,
+           g.UseRegister(index), g.UseRegister(base));
+      // Emit desired store opcode, using temp addr_reg.
+      Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+           addr_reg, g.TempImmediate(0), g.UseRegisterOrImmediateZero(value));
+    }
+  }
+}
+
+void InstructionSelector::VisitWord32And(Node* node) {
+  VisitBinop(this, node, kRiscvAnd, true, kRiscvAnd);
+}
+
+void InstructionSelector::VisitWord32Or(Node* node) {
+  VisitBinop(this, node, kRiscvOr, true, kRiscvOr);
+}
+
+void InstructionSelector::VisitWord32Xor(Node* node) {
+  Int32BinopMatcher m(node);
+  if (m.left().IsWord32Or() && CanCover(node, m.left().node()) &&
+      m.right().Is(-1)) {
+    Int32BinopMatcher mleft(m.left().node());
+    if (!mleft.right().HasResolvedValue()) {
+      RiscvOperandGenerator g(this);
+      Emit(kRiscvNor, g.DefineAsRegister(node),
+           g.UseRegister(mleft.left().node()),
+           g.UseRegister(mleft.right().node()));
+      return;
+    }
+  }
+  if (m.right().Is(-1)) {
+    // Use Nor for bit negation and eliminate constant loading for xori.
+    RiscvOperandGenerator g(this);
+    Emit(kRiscvNor, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+         g.TempImmediate(0));
+    return;
+  }
+  VisitBinop(this, node, kRiscvXor, true, kRiscvXor);
+}
+
+void InstructionSelector::VisitWord32Rol(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord32Ror(Node* node) {
+  VisitRRO(this, kRiscvRor32, node);
+}
+
+void InstructionSelector::VisitWord32Clz(Node* node) {
+  VisitRR(this, kRiscvClz32, node);
+}
+
+void InstructionSelector::VisitWord32ReverseBits(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord64ReverseBytes(Node* node) { UNREACHABLE(); }
+
+void InstructionSelector::VisitWord32ReverseBytes(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvByteSwap32, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitSimd128ReverseBytes(Node* node) {
+  UNREACHABLE();
+}
+
+void InstructionSelector::VisitWord32Ctz(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvCtz32, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitWord32Popcnt(Node* node) {
+  RiscvOperandGenerator g(this);
+  Emit(kRiscvPopcnt32, g.DefineAsRegister(node),
+       g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitInt32Add(Node* node) {
+  VisitBinop(this, node, kRiscvAdd32, true, kRiscvAdd32);
+}
+
+void InstructionSelector::VisitInt32Sub(Node* node) {
+  VisitBinop(this, node, kRiscvSub32);
+}
+
+void InstructionSelector::VisitInt32Mul(Node* node) {
+  RiscvOperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  if (m.right().HasResolvedValue() && m.right().ResolvedValue() > 0) {
+    uint32_t value = static_cast<uint32_t>(m.right().ResolvedValue());
+    if (base::bits::IsPowerOfTwo(value)) {
+      Emit(kRiscvShl32 | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value)));
+      return;
+    }
+    if (base::bits::IsPowerOfTwo(value + 1)) {
+      InstructionOperand temp = g.TempRegister();
+      Emit(kRiscvShl32 | AddressingModeField::encode(kMode_None), temp,
+           g.UseRegister(m.left().node()),
+           g.TempImmediate(base::bits::WhichPowerOfTwo(value + 1)));
+      Emit(kRiscvSub32 | AddressingModeField::encode(kMode_None),
+           g.DefineAsRegister(node), temp, g.UseRegister(m.left().node()));
+      return;
+    }
+  }
+
+  VisitRRR(this, kRiscvMul32, node);
+}
+
+void InstructionSelector::VisitInt32MulHigh(Node* node) {
+  VisitRRR(this, kRiscvMulHigh32, node);
+}
+
+void InstructionSelector::VisitUint32MulHigh(Node* node) {
+  VisitRRR(this, kRiscvMulHighU32, node);
+}
+
+void InstructionSelector::VisitInt32Div(Node* node) {
+  RiscvOperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kRiscvDiv32, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+void InstructionSelector::VisitUint32Div(Node* node) {
+  RiscvOperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kRiscvDivU32, g.DefineSameAsFirst(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+void InstructionSelector::VisitInt32Mod(Node* node) {
+  RiscvOperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kRiscvMod32, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+void InstructionSelector::VisitUint32Mod(Node* node) {
+  RiscvOperandGenerator g(this);
+  Int32BinopMatcher m(node);
+  Emit(kRiscvModU32, g.DefineAsRegister(node), g.UseRegister(m.left().node()),
+       g.UseRegister(m.right().node()));
+}
+
+void InstructionSelector::VisitChangeFloat32ToFloat64(Node* node) {
+  VisitRR(this, kRiscvCvtDS, node);
+}
+
+void InstructionSelector::VisitRoundInt32ToFloat32(Node* node) {
+  VisitRR(this, kRiscvCvtSW, node);
+}
+
+void InstructionSelector::VisitRoundUint32ToFloat32(Node* node) {
+  VisitRR(this, kRiscvCvtSUw, node);
+}
+
+void InstructionSelector::VisitChangeInt32ToFloat64(Node* node) {
+  VisitRR(this, kRiscvCvtDW, node);
+}
+
+void InstructionSelector::VisitChangeUint32ToFloat64(Node* node) {
+  VisitRR(this, kRiscvCvtDUw, node);
+}
+
+void InstructionSelector::VisitTruncateFloat32ToInt32(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionCode opcode = kRiscvTruncWS;
+  TruncateKind kind = OpParameter<TruncateKind>(node->op());
+  if (kind == TruncateKind::kSetOverflowToMin) {
+    opcode |= MiscField::encode(true);
+  }
+  Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitTruncateFloat32ToUint32(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionCode opcode = kRiscvTruncUwS;
+  TruncateKind kind = OpParameter<TruncateKind>(node->op());
+  if (kind == TruncateKind::kSetOverflowToMin) {
+    opcode |= MiscField::encode(true);
+  }
+  Emit(opcode, g.DefineAsRegister(node), g.UseRegister(node->InputAt(0)));
+}
+
+void InstructionSelector::VisitChangeFloat64ToInt32(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* value = node->InputAt(0);
+  if (CanCover(node, value)) {
+    if (value->opcode() == IrOpcode::kChangeFloat32ToFloat64) {
+      // Match float32 -> float64 -> int32 representation change path.
+      Emit(kRiscvTruncWS, g.DefineAsRegister(node),
+           g.UseRegister(value->InputAt(0)));
+      return;
+    }
+  }
+  VisitRR(this, kRiscvTruncWD, node);
+}
+
+void InstructionSelector::VisitChangeFloat64ToUint32(Node* node) {
+  VisitRR(this, kRiscvTruncUwD, node);
+}
+
+void InstructionSelector::VisitTruncateFloat64ToUint32(Node* node) {
+  VisitRR(this, kRiscvTruncUwD, node);
+}
+
+void InstructionSelector::VisitBitcastFloat32ToInt32(Node* node) {
+  VisitRR(this, kRiscvBitcastFloat32ToInt32, node);
+}
+
+void InstructionSelector::VisitBitcastInt32ToFloat32(Node* node) {
+  VisitRR(this, kRiscvBitcastInt32ToFloat32, node);
+}
+
+void InstructionSelector::VisitFloat64RoundDown(Node* node) { UNIMPLEMENTED(); }
+
+void InstructionSelector::VisitFloat32RoundUp(Node* node) {
+  VisitRR(this, kRiscvFloat32RoundUp, node);
+}
+
+void InstructionSelector::VisitFloat64RoundUp(Node* node) { UNIMPLEMENTED(); }
+
+void InstructionSelector::VisitFloat32RoundTruncate(Node* node) {
+  VisitRR(this, kRiscvFloat32RoundTruncate, node);
+}
+
+void InstructionSelector::VisitFloat64RoundTruncate(Node* node) {
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitFloat64RoundTiesAway(Node* node) {
+  UNREACHABLE();
+}
+
+void InstructionSelector::VisitFloat32RoundTiesEven(Node* node) {
+  VisitRR(this, kRiscvFloat32RoundTiesEven, node);
+}
+
+void InstructionSelector::VisitFloat64RoundTiesEven(Node* node) {
+  UNIMPLEMENTED();
+}
+
+void InstructionSelector::VisitFloat32Neg(Node* node) {
+  VisitRR(this, kRiscvNegS, node);
+}
+
+void InstructionSelector::VisitFloat64Neg(Node* node) {
+  VisitRR(this, kRiscvNegD, node);
+}
+
+void InstructionSelector::VisitFloat64Ieee754Binop(Node* node,
+                                                   InstructionCode opcode) {
+  RiscvOperandGenerator g(this);
+  Emit(opcode, g.DefineAsFixed(node, fa0), g.UseFixed(node->InputAt(0), fa0),
+       g.UseFixed(node->InputAt(1), fa1))
+      ->MarkAsCall();
+}
+
+void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
+                                                  InstructionCode opcode) {
+  RiscvOperandGenerator g(this);
+  Emit(opcode, g.DefineAsFixed(node, fa0), g.UseFixed(node->InputAt(0), fa1))
+      ->MarkAsCall();
+}
+
+void InstructionSelector::EmitPrepareArguments(
+    ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
+    Node* node) {
+  RiscvOperandGenerator g(this);
+
+  // Prepare for C function call.
+  if (call_descriptor->IsCFunctionCall()) {
+    Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
+                                         call_descriptor->ParameterCount())),
+         0, nullptr, 0, nullptr);
+
+    // Poke any stack arguments.
+    int slot = kCArgSlotCount;
+    for (PushParameter input : (*arguments)) {
+      Emit(kRiscvStoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),
+           g.TempImmediate(slot << kSystemPointerSizeLog2));
+      ++slot;
+    }
+  } else {
+    int push_count = static_cast<int>(call_descriptor->ParameterSlotCount());
+    if (push_count > 0) {
+      Emit(kRiscvStackClaim, g.NoOutput(),
+           g.TempImmediate(arguments->size() << kSystemPointerSizeLog2));
+    }
+    for (size_t n = 0; n < arguments->size(); ++n) {
+      PushParameter input = (*arguments)[n];
+      if (input.node) {
+        Emit(kRiscvStoreToStackSlot, g.NoOutput(), g.UseRegister(input.node),
+             g.TempImmediate(static_cast<int>(n << kSystemPointerSizeLog2)));
+      }
+    }
+  }
+}
+
+void InstructionSelector::VisitUnalignedLoad(Node* node) {
+  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+
+  ArchOpcode opcode;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kFloat32:
+      opcode = kRiscvULoadFloat;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kRiscvULoadDouble;
+      break;
+    case MachineRepresentation::kWord8:
+      opcode = load_rep.IsUnsigned() ? kRiscvLbu : kRiscvLb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsUnsigned() ? kRiscvUlhu : kRiscvUlh;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:         // Fall through.
+    case MachineRepresentation::kWord32:
+      opcode = kRiscvUlw;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kRiscvRvvLd;
+      break;
+    case MachineRepresentation::kSimd256:            // Fall through.
+    case MachineRepresentation::kBit:                // Fall through.
+    case MachineRepresentation::kCompressedPointer:  // Fall through.
+    case MachineRepresentation::kCompressed:         // Fall through.
+    case MachineRepresentation::kSandboxedPointer:   // Fall through.
+    case MachineRepresentation::kMapWord:            // Fall through.
+    case MachineRepresentation::kWord64:
+    case MachineRepresentation::kNone:
+      UNREACHABLE();
+  }
+
+  if (g.CanBeImmediate(index, opcode)) {
+    Emit(opcode | AddressingModeField::encode(kMode_MRI),
+         g.DefineAsRegister(node), g.UseRegister(base), g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None), addr_reg,
+         g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    Emit(opcode | AddressingModeField::encode(kMode_MRI),
+         g.DefineAsRegister(node), addr_reg, g.TempImmediate(0));
+  }
+}
+
+void InstructionSelector::VisitUnalignedStore(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  UnalignedStoreRepresentation rep = UnalignedStoreRepresentationOf(node->op());
+  ArchOpcode opcode;
+  switch (rep) {
+    case MachineRepresentation::kFloat32:
+      opcode = kRiscvUStoreFloat;
+      break;
+    case MachineRepresentation::kFloat64:
+      opcode = kRiscvUStoreDouble;
+      break;
+    case MachineRepresentation::kWord8:
+      opcode = kRiscvSb;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kRiscvUsh;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:         // Fall through.
+    case MachineRepresentation::kWord32:
+      opcode = kRiscvUsw;
+      break;
+    case MachineRepresentation::kSimd128:
+      opcode = kRiscvRvvSt;
+      break;
+    case MachineRepresentation::kSimd256:            // Fall through.
+    case MachineRepresentation::kBit:                // Fall through.
+    case MachineRepresentation::kCompressedPointer:  // Fall through.
+    case MachineRepresentation::kCompressed:         // Fall through.
+    case MachineRepresentation::kSandboxedPointer:
+    case MachineRepresentation::kMapWord:  // Fall through.
+    case MachineRepresentation::kNone:
+    case MachineRepresentation::kWord64:
+      UNREACHABLE();
+  }
+
+  if (g.CanBeImmediate(index, opcode)) {
+    Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+         g.UseRegister(base), g.UseImmediate(index),
+         g.UseRegisterOrImmediateZero(value));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None), addr_reg,
+         g.UseRegister(index), g.UseRegister(base));
+    // Emit desired store opcode, using temp addr_reg.
+    Emit(opcode | AddressingModeField::encode(kMode_MRI), g.NoOutput(),
+         addr_reg, g.TempImmediate(0), g.UseRegisterOrImmediateZero(value));
+  }
+}
+
+namespace {
+
+void VisitWordCompare(InstructionSelector* selector, Node* node,
+                      FlagsContinuation* cont) {
+  VisitWordCompare(selector, node, kRiscvCmp, cont, false);
+}
+
+void VisitAtomicLoad(InstructionSelector* selector, Node* node,
+                     ArchOpcode opcode, AtomicWidth width) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |
+                       AtomicWidthField::encode(width),
+                   g.DefineAsRegister(node), g.UseRegister(base),
+                   g.UseImmediate(index));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired load opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |
+                       AtomicWidthField::encode(width),
+                   g.DefineAsRegister(node), addr_reg, g.TempImmediate(0));
+  }
+}
+
+void VisitAtomicStore(InstructionSelector* selector, Node* node,
+                      ArchOpcode opcode, AtomicWidth width) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  if (g.CanBeImmediate(index, opcode)) {
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |
+                       AtomicWidthField::encode(width),
+                   g.NoOutput(), g.UseRegister(base), g.UseImmediate(index),
+                   g.UseRegisterOrImmediateZero(value));
+  } else {
+    InstructionOperand addr_reg = g.TempRegister();
+    selector->Emit(kRiscvAdd32 | AddressingModeField::encode(kMode_None),
+                   addr_reg, g.UseRegister(index), g.UseRegister(base));
+    // Emit desired store opcode, using temp addr_reg.
+    selector->Emit(opcode | AddressingModeField::encode(kMode_MRI) |
+                       AtomicWidthField::encode(width),
+                   g.NoOutput(), addr_reg, g.TempImmediate(0),
+                   g.UseRegisterOrImmediateZero(value));
+  }
+}
+
+void VisitAtomicBinop(InstructionSelector* selector, Node* node,
+                      ArchOpcode opcode) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionOperand inputs[3];
+  size_t input_count = 0;
+  inputs[input_count++] = g.UseUniqueRegister(base);
+  inputs[input_count++] = g.UseUniqueRegister(index);
+  inputs[input_count++] = g.UseUniqueRegister(value);
+  InstructionOperand outputs[1];
+  outputs[0] = g.UseUniqueRegister(node);
+  InstructionOperand temps[4];
+  temps[0] = g.TempRegister();
+  temps[1] = g.TempRegister();
+  temps[2] = g.TempRegister();
+  temps[3] = g.TempRegister();
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  selector->Emit(code, 1, outputs, input_count, inputs, 4, temps);
+}
+
+}  // namespace
+
+void InstructionSelector::VisitStackPointerGreaterThan(
+    Node* node, FlagsContinuation* cont) {
+  StackCheckKind kind = StackCheckKindOf(node->op());
+  InstructionCode opcode =
+      kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
+
+  RiscvOperandGenerator g(this);
+
+  // No outputs.
+  InstructionOperand* const outputs = nullptr;
+  const int output_count = 0;
+
+  // Applying an offset to this stack check requires a temp register. Offsets
+  // are only applied to the first stack check. If applying an offset, we must
+  // ensure the input and temp registers do not alias, thus kUniqueRegister.
+  InstructionOperand temps[] = {g.TempRegister()};
+  const int temp_count = (kind == StackCheckKind::kJSFunctionEntry ? 1 : 0);
+  const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)
+                                 ? OperandGenerator::kUniqueRegister
+                                 : OperandGenerator::kRegister;
+
+  Node* const value = node->InputAt(0);
+  InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};
+  static constexpr int input_count = arraysize(inputs);
+
+  EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
+                       temp_count, temps, cont);
+}
+
+// Shared routine for word comparisons against zero.
+void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
+                                               FlagsContinuation* cont) {
+  // Try to combine with comparisons against 0 by simply inverting the branch.
+  while (CanCover(user, value)) {
+    if (value->opcode() == IrOpcode::kWord32Equal) {
+      Int32BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+      user = value;
+      value = m.left().node();
+    } else if (value->opcode() == IrOpcode::kWord64Equal) {
+      Int64BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+      user = value;
+      value = m.left().node();
+    } else {
+      break;
+    }
+
+    cont->Negate();
+  }
+
+  if (CanCover(user, value)) {
+    switch (value->opcode()) {
+      case IrOpcode::kWord32Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitWordCompare(this, value, cont);
+      case IrOpcode::kInt32LessThan:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+        return VisitWordCompare(this, value, cont);
+      case IrOpcode::kInt32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+        return VisitWordCompare(this, value, cont);
+      case IrOpcode::kUint32LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitWordCompare(this, value, cont);
+      case IrOpcode::kUint32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitWordCompare(this, value, cont);
+      case IrOpcode::kFloat32Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat32LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat32LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitFloat32Compare(this, value, cont);
+      case IrOpcode::kFloat64Equal:
+        cont->OverwriteAndNegateIfEqual(kEqual);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kFloat64LessThan:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kFloat64LessThanOrEqual:
+        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+        return VisitFloat64Compare(this, value, cont);
+      case IrOpcode::kProjection:
+        // Check if this is the overflow output projection of an
+        // <Operation>WithOverflow node.
+        if (ProjectionIndexOf(value->op()) == 1u) {
+          // We cannot combine the <Operation>WithOverflow with this branch
+          // unless the 0th projection (the use of the actual value of the
+          // <Operation> is either nullptr, which means there's no use of the
+          // actual value, or was already defined, which means it is scheduled
+          // *AFTER* this branch).
+          Node* const node = value->InputAt(0);
+          Node* const result = NodeProperties::FindProjection(node, 0);
+          if (result == nullptr || IsDefined(result)) {
+            switch (node->opcode()) {
+              case IrOpcode::kInt32AddWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kRiscvAddOvf, cont);
+              case IrOpcode::kInt32SubWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kRiscvSubOvf, cont);
+              case IrOpcode::kInt32MulWithOverflow:
+                cont->OverwriteAndNegateIfEqual(kOverflow);
+                return VisitBinop(this, node, kRiscvMulOvf32, cont);
+              case IrOpcode::kInt64AddWithOverflow:
+              case IrOpcode::kInt64SubWithOverflow:
+                TRACE_UNIMPL();
+                break;
+              default:
+                break;
+            }
+          }
+        }
+        break;
+      case IrOpcode::kWord32And:
+        return VisitWordCompare(this, value, kRiscvTst, cont, true);
+      case IrOpcode::kStackPointerGreaterThan:
+        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
+        return VisitStackPointerGreaterThan(value, cont);
+      default:
+        break;
+    }
+  }
+
+  // Continuation could not be combined with a compare, emit compare against 0.
+  EmitWordCompareZero(this, value, cont);
+}
+
+void InstructionSelector::VisitWord32Equal(Node* const node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+  Int32BinopMatcher m(node);
+  if (m.right().Is(0)) {
+    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  }
+
+  VisitWordCompare(this, node, &cont);
+}
+
+void InstructionSelector::VisitInt32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+  VisitWordCompare(this, node, &cont);
+}
+
+void InstructionSelector::VisitInt32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+  VisitWordCompare(this, node, &cont);
+}
+
+void InstructionSelector::VisitUint32LessThan(Node* node) {
+  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+  VisitWordCompare(this, node, &cont);
+}
+
+void InstructionSelector::VisitUint32LessThanOrEqual(Node* node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+  VisitWordCompare(this, node, &cont);
+}
+
+void InstructionSelector::VisitInt32AddWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kRiscvAddOvf, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kRiscvAddOvf, &cont);
+}
+
+void InstructionSelector::VisitInt32SubWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kRiscvSubOvf, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kRiscvSubOvf, &cont);
+}
+
+void InstructionSelector::VisitInt32MulWithOverflow(Node* node) {
+  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+    return VisitBinop(this, node, kRiscvMulOvf32, &cont);
+  }
+  FlagsContinuation cont;
+  VisitBinop(this, node, kRiscvMulOvf32, &cont);
+}
+
+void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
+  AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
+  LoadRepresentation load_rep = atomic_load_params.representation();
+  ArchOpcode opcode;
+  switch (load_rep.representation()) {
+    case MachineRepresentation::kWord8:
+      opcode = load_rep.IsSigned() ? kAtomicLoadInt8 : kAtomicLoadUint8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = load_rep.IsSigned() ? kAtomicLoadInt16 : kAtomicLoadUint16;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:
+    case MachineRepresentation::kWord32:
+      opcode = kAtomicLoadWord32;
+      break;
+    default:
+      UNREACHABLE();
+  }
+  VisitAtomicLoad(this, node, opcode, AtomicWidth::kWord32);
+}
+
+void InstructionSelector::VisitWord32AtomicStore(Node* node) {
+  AtomicStoreParameters store_params = AtomicStoreParametersOf(node->op());
+  MachineRepresentation rep = store_params.representation();
+  ArchOpcode opcode;
+  switch (rep) {
+    case MachineRepresentation::kWord8:
+      opcode = kAtomicStoreWord8;
+      break;
+    case MachineRepresentation::kWord16:
+      opcode = kAtomicStoreWord16;
+      break;
+    case MachineRepresentation::kTaggedSigned:   // Fall through.
+    case MachineRepresentation::kTaggedPointer:  // Fall through.
+    case MachineRepresentation::kTagged:
+    case MachineRepresentation::kWord32:
+      opcode = kAtomicStoreWord32;
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  VisitAtomicStore(this, node, opcode, AtomicWidth::kWord32);
+}
+
+void InstructionSelector::VisitWord32AtomicExchange(Node* node) {
+  ArchOpcode opcode;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = kAtomicExchangeInt8;
+  } else if (type == MachineType::Uint8()) {
+    opcode = kAtomicExchangeUint8;
+  } else if (type == MachineType::Int16()) {
+    opcode = kAtomicExchangeInt16;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kAtomicExchangeUint16;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = kAtomicExchangeWord32;
+  } else {
+    UNREACHABLE();
+  }
+
+  VisitAtomicExchange(this, node, opcode, AtomicWidth::kWord32);
+}
+
+void InstructionSelector::VisitWord32AtomicCompareExchange(Node* node) {
+  ArchOpcode opcode;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = kAtomicCompareExchangeInt8;
+  } else if (type == MachineType::Uint8()) {
+    opcode = kAtomicCompareExchangeUint8;
+  } else if (type == MachineType::Int16()) {
+    opcode = kAtomicCompareExchangeInt16;
+  } else if (type == MachineType::Uint16()) {
+    opcode = kAtomicCompareExchangeUint16;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = kAtomicCompareExchangeWord32;
+  } else {
+    UNREACHABLE();
+  }
+
+  VisitAtomicCompareExchange(this, node, opcode, AtomicWidth::kWord32);
+}
+
+void InstructionSelector::VisitWord32AtomicBinaryOperation(
+    Node* node, ArchOpcode int8_op, ArchOpcode uint8_op, ArchOpcode int16_op,
+    ArchOpcode uint16_op, ArchOpcode word32_op) {
+  ArchOpcode opcode;
+  MachineType type = AtomicOpType(node->op());
+  if (type == MachineType::Int8()) {
+    opcode = int8_op;
+  } else if (type == MachineType::Uint8()) {
+    opcode = uint8_op;
+  } else if (type == MachineType::Int16()) {
+    opcode = int16_op;
+  } else if (type == MachineType::Uint16()) {
+    opcode = uint16_op;
+  } else if (type == MachineType::Int32() || type == MachineType::Uint32()) {
+    opcode = word32_op;
+  } else {
+    UNREACHABLE();
+  }
+
+  VisitAtomicBinop(this, node, opcode);
+}
+
+#define VISIT_ATOMIC_BINOP(op)                                           \
+  void InstructionSelector::VisitWord32Atomic##op(Node* node) {          \
+    VisitWord32AtomicBinaryOperation(                                    \
+        node, kAtomic##op##Int8, kAtomic##op##Uint8, kAtomic##op##Int16, \
+        kAtomic##op##Uint16, kAtomic##op##Word32);                       \
+  }
+VISIT_ATOMIC_BINOP(Add)
+VISIT_ATOMIC_BINOP(Sub)
+VISIT_ATOMIC_BINOP(And)
+VISIT_ATOMIC_BINOP(Or)
+VISIT_ATOMIC_BINOP(Xor)
+#undef VISIT_ATOMIC_BINOP
+
+void InstructionSelector::VisitInt32AbsWithOverflow(Node* node) {
+  UNREACHABLE();
+}
+
+void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
+  UNREACHABLE();
+}
+
+template <unsigned N>
+static void VisitInt32PairBinop(InstructionSelector* selector,
+                                InstructionCode pair_opcode,
+                                InstructionCode single_opcode, Node* node) {
+  static_assert(N == 3 || N == 4,
+                "Pair operations can only have 3 or 4 inputs");
+
+  RiscvOperandGenerator g(selector);
+
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+
+  if (projection1) {
+    InstructionOperand outputs[] = {
+        g.DefineAsRegister(node),
+        g.DefineAsRegister(NodeProperties::FindProjection(node, 1))};
+
+    if constexpr (N == 3) {
+      // We use UseUniqueRegister here to avoid register sharing with the output
+      // register.
+      InstructionOperand inputs[] = {g.UseUniqueRegister(node->InputAt(0)),
+                                     g.UseUniqueRegister(node->InputAt(1)),
+                                     g.UseUniqueRegister(node->InputAt(2))};
+
+      selector->Emit(pair_opcode, 2, outputs, N, inputs);
+
+    } else if constexpr (N == 4) {
+      // We use UseUniqueRegister here to avoid register sharing with the output
+      // register.
+      InstructionOperand inputs[] = {g.UseUniqueRegister(node->InputAt(0)),
+                                     g.UseUniqueRegister(node->InputAt(1)),
+                                     g.UseUniqueRegister(node->InputAt(2)),
+                                     g.UseUniqueRegister(node->InputAt(3))};
+
+      selector->Emit(pair_opcode, 2, outputs, N, inputs);
+    }
+
+  } else {
+    // The high word of the result is not used, so we emit the standard 32 bit
+    // instruction.
+    selector->Emit(single_opcode, g.DefineSameAsFirst(node),
+                   g.UseRegister(node->InputAt(0)),
+                   g.UseRegister(node->InputAt(2)));
+  }
+}
+
+void InstructionSelector::VisitInt32PairAdd(Node* node) {
+  VisitInt32PairBinop<4>(this, kRiscvAddPair, kRiscvAdd32, node);
+}
+
+void InstructionSelector::VisitInt32PairSub(Node* node) {
+  VisitInt32PairBinop<4>(this, kRiscvSubPair, kRiscvSub32, node);
+}
+
+void InstructionSelector::VisitInt32PairMul(Node* node) {
+  VisitInt32PairBinop<4>(this, kRiscvMulPair, kRiscvMul32, node);
+}
+
+// Shared routine for multiple shift operations.
+static void VisitWord32PairShift(InstructionSelector* selector,
+                                 InstructionCode opcode, Node* node) {
+  RiscvOperandGenerator g(selector);
+  Int32Matcher m(node->InputAt(2));
+  InstructionOperand shift_operand;
+  if (m.HasResolvedValue()) {
+    shift_operand = g.UseImmediate(m.node());
+  } else {
+    shift_operand = g.UseUniqueRegister(m.node());
+  }
+
+  // We use UseUniqueRegister here to avoid register sharing with the output
+  // register.
+  InstructionOperand inputs[] = {g.UseUniqueRegister(node->InputAt(0)),
+                                 g.UseUniqueRegister(node->InputAt(1)),
+                                 shift_operand};
+
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+
+  InstructionOperand outputs[2];
+  InstructionOperand temps[1];
+  int32_t output_count = 0;
+  int32_t temp_count = 0;
+
+  outputs[output_count++] = g.DefineAsRegister(node);
+  if (projection1) {
+    outputs[output_count++] = g.DefineAsRegister(projection1);
+  } else {
+    temps[temp_count++] = g.TempRegister();
+  }
+
+  selector->Emit(opcode, output_count, outputs, 3, inputs, temp_count, temps);
+}
+
+void InstructionSelector::VisitWord32PairShl(Node* node) {
+  VisitWord32PairShift(this, kRiscvShlPair, node);
+}
+
+void InstructionSelector::VisitWord32PairShr(Node* node) {
+  VisitWord32PairShift(this, kRiscvShrPair, node);
+}
+
+void InstructionSelector::VisitWord32PairSar(Node* node) {
+  VisitWord32PairShift(this, kRiscvSarPair, node);
+}
+
+void InstructionSelector::VisitWord32AtomicPairLoad(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  ArchOpcode opcode = kRiscvWord32AtomicPairLoad;
+  AddressingMode addressing_mode = kMode_MRI;
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index)};
+  InstructionOperand temps[3];
+  size_t temp_count = 0;
+  temps[temp_count++] = g.TempRegister(t0);
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+
+  Node* projection0 = NodeProperties::FindProjection(node, 0);
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+  if (projection0) {
+    outputs[output_count++] = g.DefineAsFixed(projection0, a0);
+  } else {
+    temps[temp_count++] = g.TempRegister(a0);
+  }
+  if (projection1) {
+    outputs[output_count++] = g.DefineAsFixed(projection1, a1);
+  } else {
+    temps[temp_count++] = g.TempRegister(a1);
+  }
+  Emit(code, output_count, outputs, arraysize(inputs), inputs, temp_count,
+       temps);
+}
+
+void InstructionSelector::VisitWord32AtomicPairStore(Node* node) {
+  RiscvOperandGenerator g(this);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value_low = node->InputAt(2);
+  Node* value_high = node->InputAt(3);
+
+  InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index),
+                                 g.UseFixed(value_low, a1),
+                                 g.UseFixed(value_high, a2)};
+  InstructionOperand temps[] = {g.TempRegister(a0), g.TempRegister(),
+                                g.TempRegister()};
+  Emit(kRiscvWord32AtomicPairStore | AddressingModeField::encode(kMode_MRI), 0,
+       nullptr, arraysize(inputs), inputs, arraysize(temps), temps);
+}
+
+static void VisitPairAtomicBinop(InstructionSelector* selector, Node* node,
+                                 ArchOpcode opcode) {
+  RiscvOperandGenerator g(selector);
+  Node* base = node->InputAt(0);
+  Node* index = node->InputAt(1);
+  Node* value = node->InputAt(2);
+  Node* value_high = node->InputAt(3);
+  AddressingMode addressing_mode = kMode_None;
+  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode);
+  InstructionOperand inputs[] = {g.UseRegister(base), g.UseRegister(index),
+                                 g.UseFixed(value, a1),
+                                 g.UseFixed(value_high, a2)};
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  InstructionOperand temps[3];
+  size_t temp_count = 0;
+  temps[temp_count++] = g.TempRegister(t0);
+
+  Node* projection0 = NodeProperties::FindProjection(node, 0);
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+  if (projection0) {
+    outputs[output_count++] = g.DefineAsFixed(projection0, a0);
+  } else {
+    temps[temp_count++] = g.TempRegister(a0);
+  }
+  if (projection1) {
+    outputs[output_count++] = g.DefineAsFixed(projection1, a1);
+  } else {
+    temps[temp_count++] = g.TempRegister(a1);
+  }
+  selector->Emit(code, output_count, outputs, arraysize(inputs), inputs,
+                 temp_count, temps);
+}
+
+void InstructionSelector::VisitWord32AtomicPairAdd(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairAdd);
+}
+
+void InstructionSelector::VisitWord32AtomicPairSub(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairSub);
+}
+
+void InstructionSelector::VisitWord32AtomicPairAnd(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairAnd);
+}
+
+void InstructionSelector::VisitWord32AtomicPairOr(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairOr);
+}
+
+void InstructionSelector::VisitWord32AtomicPairXor(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairXor);
+}
+
+void InstructionSelector::VisitWord32AtomicPairExchange(Node* node) {
+  VisitPairAtomicBinop(this, node, kRiscvWord32AtomicPairExchange);
+}
+void InstructionSelector::VisitWord32AtomicPairCompareExchange(Node* node) {
+  RiscvOperandGenerator g(this);
+  InstructionOperand inputs[] = {
+      g.UseRegister(node->InputAt(0)),  g.UseRegister(node->InputAt(1)),
+      g.UseFixed(node->InputAt(2), a1), g.UseFixed(node->InputAt(3), a2),
+      g.UseFixed(node->InputAt(4), a3), g.UseUniqueRegister(node->InputAt(5))};
+
+  InstructionCode code = kRiscvWord32AtomicPairCompareExchange |
+                         AddressingModeField::encode(kMode_MRI);
+  Node* projection0 = NodeProperties::FindProjection(node, 0);
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+  InstructionOperand outputs[2];
+  size_t output_count = 0;
+  InstructionOperand temps[3];
+  size_t temp_count = 0;
+  temps[temp_count++] = g.TempRegister(t0);
+  if (projection0) {
+    outputs[output_count++] = g.DefineAsFixed(projection0, a0);
+  } else {
+    temps[temp_count++] = g.TempRegister(a0);
+  }
+  if (projection1) {
+    outputs[output_count++] = g.DefineAsFixed(projection1, a1);
+  } else {
+    temps[temp_count++] = g.TempRegister(a1);
+  }
+  Emit(code, output_count, outputs, arraysize(inputs), inputs, temp_count,
+       temps);
+}
+// static
+MachineOperatorBuilder::Flags
+InstructionSelector::SupportedMachineOperatorFlags() {
+  MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
+  return flags | MachineOperatorBuilder::kWord32Ctz |
+         MachineOperatorBuilder::kWord32Ctz |
+         MachineOperatorBuilder::kWord32Popcnt |
+         MachineOperatorBuilder::kWord32ShiftIsSafe |
+         MachineOperatorBuilder::kInt32DivIsSafe |
+         MachineOperatorBuilder::kUint32DivIsSafe |
+         MachineOperatorBuilder::kFloat32RoundDown |
+         MachineOperatorBuilder::kFloat32RoundUp |
+         MachineOperatorBuilder::kFloat32RoundTruncate |
+         MachineOperatorBuilder::kFloat32RoundTiesEven;
+}
+#undef TRACE_UNIMPL
+#undef TRACE
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/src/compiler/backend/riscv64/instruction-selector-riscv64.cc b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
similarity index 58%
rename from src/compiler/backend/riscv64/instruction-selector-riscv64.cc
rename to src/compiler/backend/riscv/instruction-selector-riscv64.cc
index 0294f52c8c4..e1b6ff7eeef 100644
--- a/src/compiler/backend/riscv64/instruction-selector-riscv64.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
@@ -4,6 +4,7 @@
 
 #include "src/base/bits.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/riscv/instruction-selector-riscv.h"
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
 
@@ -11,189 +12,67 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
-#define TRACE_UNIMPL() \
-  PrintF("UNIMPLEMENTED instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
-
-#define TRACE() PrintF("instr_sel: %s at line %d\n", __FUNCTION__, __LINE__)
-
-// Adds RISC-V-specific methods for generating InstructionOperands.
-class RiscvOperandGenerator final : public OperandGenerator {
- public:
-  explicit RiscvOperandGenerator(InstructionSelector* selector)
-      : OperandGenerator(selector) {}
-
-  InstructionOperand UseOperand(Node* node, InstructionCode opcode) {
-    if (CanBeImmediate(node, opcode)) {
-      return UseImmediate(node);
-    }
-    return UseRegister(node);
-  }
-
-  // Use the zero register if the node has the immediate value zero, otherwise
-  // assign a register.
-  InstructionOperand UseRegisterOrImmediateZero(Node* node) {
-    if ((IsIntegerConstant(node) && (GetIntegerConstantValue(node) == 0)) ||
-        (IsFloatConstant(node) &&
-         (base::bit_cast<int64_t>(GetFloatConstantValue(node)) == 0))) {
-      return UseImmediate(node);
-    }
-    return UseRegister(node);
-  }
-
-  bool IsIntegerConstant(Node* node) {
-    if (node->opcode() == IrOpcode::kNumberConstant) {
-      const double value = OpParameter<double>(node->op());
-      return base::bit_cast<int64_t>(value) == 0;
-    }
-    return (node->opcode() == IrOpcode::kInt32Constant) ||
-           (node->opcode() == IrOpcode::kInt64Constant);
-  }
-
-  int64_t GetIntegerConstantValue(Node* node) {
-    if (node->opcode() == IrOpcode::kInt32Constant) {
-      return OpParameter<int32_t>(node->op());
-    } else if (node->opcode() == IrOpcode::kInt64Constant) {
-      return OpParameter<int64_t>(node->op());
-    }
-    DCHECK_EQ(node->opcode(), IrOpcode::kNumberConstant);
+bool RiscvOperandGenerator::IsIntegerConstant(Node* node) {
+  if (node->opcode() == IrOpcode::kNumberConstant) {
     const double value = OpParameter<double>(node->op());
-    DCHECK_EQ(base::bit_cast<int64_t>(value), 0);
-    return base::bit_cast<int64_t>(value);
-  }
-
-  bool IsFloatConstant(Node* node) {
-    return (node->opcode() == IrOpcode::kFloat32Constant) ||
-           (node->opcode() == IrOpcode::kFloat64Constant);
-  }
-
-  double GetFloatConstantValue(Node* node) {
-    if (node->opcode() == IrOpcode::kFloat32Constant) {
-      return OpParameter<float>(node->op());
-    }
-    DCHECK_EQ(IrOpcode::kFloat64Constant, node->opcode());
-    return OpParameter<double>(node->op());
-  }
-
-  bool CanBeImmediate(Node* node, InstructionCode mode) {
-    return IsIntegerConstant(node) &&
-           CanBeImmediate(GetIntegerConstantValue(node), mode);
-  }
-
-  bool CanBeImmediate(int64_t value, InstructionCode opcode) {
-    switch (ArchOpcodeField::decode(opcode)) {
-      case kRiscvShl32:
-      case kRiscvSar32:
-      case kRiscvShr32:
-        return is_uint5(value);
-      case kRiscvShl64:
-      case kRiscvSar64:
-      case kRiscvShr64:
-        return is_uint6(value);
-      case kRiscvAdd32:
-      case kRiscvAnd32:
-      case kRiscvAnd:
-      case kRiscvAdd64:
-      case kRiscvOr32:
-      case kRiscvOr:
-      case kRiscvTst:
-      case kRiscvXor:
-        return is_int12(value);
-      case kRiscvLb:
-      case kRiscvLbu:
-      case kRiscvSb:
-      case kRiscvLh:
-      case kRiscvLhu:
-      case kRiscvSh:
-      case kRiscvLw:
-      case kRiscvSw:
-      case kRiscvLd:
-      case kRiscvSd:
-      case kRiscvLoadFloat:
-      case kRiscvStoreFloat:
-      case kRiscvLoadDouble:
-      case kRiscvStoreDouble:
-        return is_int32(value);
-      default:
-        return is_int12(value);
-    }
-  }
-
- private:
-  bool ImmediateFitsAddrMode1Instruction(int32_t imm) const {
-    TRACE_UNIMPL();
-    return false;
-  }
-};
-
-static void VisitRR(InstructionSelector* selector, ArchOpcode opcode,
-                    Node* node) {
-  RiscvOperandGenerator g(selector);
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseRegister(node->InputAt(0)));
-}
-
-static void VisitRRI(InstructionSelector* selector, ArchOpcode opcode,
-                     Node* node) {
-  RiscvOperandGenerator g(selector);
-  int32_t imm = OpParameter<int32_t>(node->op());
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm));
-}
-
-static void VisitSimdShift(InstructionSelector* selector, ArchOpcode opcode,
-                           Node* node) {
-  RiscvOperandGenerator g(selector);
-  if (g.IsIntegerConstant(node->InputAt(1))) {
-    selector->Emit(opcode, g.DefineAsRegister(node),
-                   g.UseRegister(node->InputAt(0)),
-                   g.UseImmediate(node->InputAt(1)));
-  } else {
-    selector->Emit(opcode, g.DefineAsRegister(node),
-                   g.UseRegister(node->InputAt(0)),
-                   g.UseRegister(node->InputAt(1)));
+    return base::bit_cast<int64_t>(value) == 0;
+  }
+  return (node->opcode() == IrOpcode::kInt32Constant) ||
+         (node->opcode() == IrOpcode::kInt64Constant);
+}
+
+int64_t RiscvOperandGenerator::GetIntegerConstantValue(Node* node) {
+  if (node->opcode() == IrOpcode::kInt32Constant) {
+    return OpParameter<int32_t>(node->op());
+  } else if (node->opcode() == IrOpcode::kInt64Constant) {
+    return OpParameter<int64_t>(node->op());
+  }
+  DCHECK_EQ(node->opcode(), IrOpcode::kNumberConstant);
+  const double value = OpParameter<double>(node->op());
+  DCHECK_EQ(base::bit_cast<int64_t>(value), 0);
+  return base::bit_cast<int64_t>(value);
+}
+
+bool RiscvOperandGenerator::CanBeImmediate(int64_t value,
+                                           InstructionCode opcode) {
+  switch (ArchOpcodeField::decode(opcode)) {
+    case kRiscvShl32:
+    case kRiscvSar32:
+    case kRiscvShr32:
+      return is_uint5(value);
+    case kRiscvShl64:
+    case kRiscvSar64:
+    case kRiscvShr64:
+      return is_uint6(value);
+    case kRiscvAdd32:
+    case kRiscvAnd32:
+    case kRiscvAnd:
+    case kRiscvAdd64:
+    case kRiscvOr32:
+    case kRiscvOr:
+    case kRiscvTst:
+    case kRiscvXor:
+      return is_int12(value);
+    case kRiscvLb:
+    case kRiscvLbu:
+    case kRiscvSb:
+    case kRiscvLh:
+    case kRiscvLhu:
+    case kRiscvSh:
+    case kRiscvLw:
+    case kRiscvSw:
+    case kRiscvLd:
+    case kRiscvSd:
+    case kRiscvLoadFloat:
+    case kRiscvStoreFloat:
+    case kRiscvLoadDouble:
+    case kRiscvStoreDouble:
+      return is_int32(value);
+    default:
+      return is_int12(value);
   }
 }
 
-static void VisitRRIR(InstructionSelector* selector, ArchOpcode opcode,
-                      Node* node) {
-  RiscvOperandGenerator g(selector);
-  int32_t imm = OpParameter<int32_t>(node->op());
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseRegister(node->InputAt(0)), g.UseImmediate(imm),
-                 g.UseRegister(node->InputAt(1)));
-}
-
-static void VisitRRR(InstructionSelector* selector, ArchOpcode opcode,
-                     Node* node) {
-  RiscvOperandGenerator g(selector);
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseRegister(node->InputAt(0)),
-                 g.UseRegister(node->InputAt(1)));
-}
-
-static void VisitUniqueRRR(InstructionSelector* selector, ArchOpcode opcode,
-                           Node* node) {
-  RiscvOperandGenerator g(selector);
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseUniqueRegister(node->InputAt(0)),
-                 g.UseUniqueRegister(node->InputAt(1)));
-}
-
-void VisitRRRR(InstructionSelector* selector, ArchOpcode opcode, Node* node) {
-  RiscvOperandGenerator g(selector);
-  selector->Emit(
-      opcode, g.DefineSameAsFirst(node), g.UseRegister(node->InputAt(0)),
-      g.UseRegister(node->InputAt(1)), g.UseRegister(node->InputAt(2)));
-}
-
-static void VisitRRO(InstructionSelector* selector, ArchOpcode opcode,
-                     Node* node) {
-  RiscvOperandGenerator g(selector);
-  selector->Emit(opcode, g.DefineAsRegister(node),
-                 g.UseRegister(node->InputAt(0)),
-                 g.UseOperand(node->InputAt(1), opcode));
-}
-
 struct ExtendingLoadMatcher {
   ExtendingLoadMatcher(Node* node, InstructionSelector* selector)
       : matches_(false), selector_(selector), base_(nullptr), immediate_(0) {
@@ -278,96 +157,6 @@ bool TryEmitExtendingLoad(InstructionSelector* selector, Node* node,
   return false;
 }
 
-bool TryMatchImmediate(InstructionSelector* selector,
-                       InstructionCode* opcode_return, Node* node,
-                       size_t* input_count_return, InstructionOperand* inputs) {
-  RiscvOperandGenerator g(selector);
-  if (g.CanBeImmediate(node, *opcode_return)) {
-    *opcode_return |= AddressingModeField::encode(kMode_MRI);
-    inputs[0] = g.UseImmediate(node);
-    *input_count_return = 1;
-    return true;
-  }
-  return false;
-}
-
-static void VisitBinop(InstructionSelector* selector, Node* node,
-                       InstructionCode opcode, bool has_reverse_opcode,
-                       InstructionCode reverse_opcode,
-                       FlagsContinuation* cont) {
-  RiscvOperandGenerator g(selector);
-  Int32BinopMatcher m(node);
-  InstructionOperand inputs[2];
-  size_t input_count = 0;
-  InstructionOperand outputs[1];
-  size_t output_count = 0;
-
-  if (TryMatchImmediate(selector, &opcode, m.right().node(), &input_count,
-                        &inputs[1])) {
-    inputs[0] = g.UseRegisterOrImmediateZero(m.left().node());
-    input_count++;
-  } else if (has_reverse_opcode &&
-             TryMatchImmediate(selector, &reverse_opcode, m.left().node(),
-                               &input_count, &inputs[1])) {
-    inputs[0] = g.UseRegisterOrImmediateZero(m.right().node());
-    opcode = reverse_opcode;
-    input_count++;
-  } else {
-    inputs[input_count++] = g.UseRegister(m.left().node());
-    inputs[input_count++] = g.UseOperand(m.right().node(), opcode);
-  }
-
-  if (cont->IsDeoptimize()) {
-    // If we can deoptimize as a result of the binop, we need to make sure that
-    // the deopt inputs are not overwritten by the binop result. One way
-    // to achieve that is to declare the output register as same-as-first.
-    outputs[output_count++] = g.DefineSameAsFirst(node);
-  } else {
-    outputs[output_count++] = g.DefineAsRegister(node);
-  }
-
-  DCHECK_NE(0u, input_count);
-  DCHECK_EQ(1u, output_count);
-  DCHECK_GE(arraysize(inputs), input_count);
-  DCHECK_GE(arraysize(outputs), output_count);
-
-  selector->EmitWithContinuation(opcode, output_count, outputs, input_count,
-                                 inputs, cont);
-}
-
-static void VisitBinop(InstructionSelector* selector, Node* node,
-                       InstructionCode opcode, bool has_reverse_opcode,
-                       InstructionCode reverse_opcode) {
-  FlagsContinuation cont;
-  VisitBinop(selector, node, opcode, has_reverse_opcode, reverse_opcode, &cont);
-}
-
-static void VisitBinop(InstructionSelector* selector, Node* node,
-                       InstructionCode opcode, FlagsContinuation* cont) {
-  VisitBinop(selector, node, opcode, false, kArchNop, cont);
-}
-
-static void VisitBinop(InstructionSelector* selector, Node* node,
-                       InstructionCode opcode) {
-  VisitBinop(selector, node, opcode, false, kArchNop);
-}
-
-void InstructionSelector::VisitStackSlot(Node* node) {
-  StackSlotRepresentation rep = StackSlotRepresentationOf(node->op());
-  int alignment = rep.alignment();
-  int slot = frame_->AllocateSpillSlot(rep.size(), alignment);
-  OperandGenerator g(this);
-
-  Emit(kArchStackSlot, g.DefineAsRegister(node),
-       sequence()->AddImmediate(Constant(slot)),
-       sequence()->AddImmediate(Constant(alignment)), 0, nullptr);
-}
-
-void InstructionSelector::VisitAbortCSADcheck(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), a0));
-}
-
 void EmitLoad(InstructionSelector* selector, Node* node, InstructionCode opcode,
               Node* output = nullptr) {
   RiscvOperandGenerator g(selector);
@@ -465,51 +254,6 @@ void InstructionSelector::VisitLoadLane(Node* node) {
        g.UseImmediate(params.laneidx), addr_reg, g.TempImmediate(0));
 }
 
-void InstructionSelector::VisitLoadTransform(Node* node) {
-  LoadTransformParameters params = LoadTransformParametersOf(node->op());
-
-  switch (params.transformation) {
-    case LoadTransformation::kS128Load8Splat:
-      EmitS128Load(this, node, kRiscvS128LoadSplat, E8, m1);
-      break;
-    case LoadTransformation::kS128Load16Splat:
-      EmitS128Load(this, node, kRiscvS128LoadSplat, E16, m1);
-      break;
-    case LoadTransformation::kS128Load32Splat:
-      EmitS128Load(this, node, kRiscvS128LoadSplat, E32, m1);
-      break;
-    case LoadTransformation::kS128Load64Splat:
-      EmitS128Load(this, node, kRiscvS128LoadSplat, E64, m1);
-      break;
-    case LoadTransformation::kS128Load8x8S:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E16, m1);
-      break;
-    case LoadTransformation::kS128Load8x8U:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E16, m1);
-      break;
-    case LoadTransformation::kS128Load16x4S:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E32, m1);
-      break;
-    case LoadTransformation::kS128Load16x4U:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E32, m1);
-      break;
-    case LoadTransformation::kS128Load32x2S:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendS, E64, m1);
-      break;
-    case LoadTransformation::kS128Load32x2U:
-      EmitS128Load(this, node, kRiscvS128Load64ExtendU, E64, m1);
-      break;
-    case LoadTransformation::kS128Load32Zero:
-      EmitS128Load(this, node, kRiscvS128Load32Zero, E32, m1);
-      break;
-    case LoadTransformation::kS128Load64Zero:
-      EmitS128Load(this, node, kRiscvS128Load64Zero, E64, m1);
-      break;
-    default:
-      UNIMPLEMENTED();
-  }
-}
-
 void InstructionSelector::VisitLoad(Node* node) {
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
 
@@ -570,11 +314,6 @@ void InstructionSelector::VisitLoad(Node* node) {
   EmitLoad(this, node, opcode);
 }
 
-void InstructionSelector::VisitProtectedLoad(Node* node) {
-  // TODO(eholk)
-  UNIMPLEMENTED();
-}
-
 void InstructionSelector::VisitStore(Node* node) {
   RiscvOperandGenerator g(this);
   Node* base = node->InputAt(0);
@@ -663,11 +402,6 @@ void InstructionSelector::VisitStore(Node* node) {
   }
 }
 
-void InstructionSelector::VisitProtectedStore(Node* node) {
-  // TODO(eholk)
-  UNIMPLEMENTED();
-}
-
 void InstructionSelector::VisitWord32And(Node* node) {
   VisitBinop(this, node, kRiscvAnd32, true, kRiscvAnd32);
 }
@@ -763,66 +497,6 @@ void InstructionSelector::VisitWord64Xor(Node* node) {
   VisitBinop(this, node, kRiscvXor, true, kRiscvXor);
 }
 
-void InstructionSelector::VisitWord32Shl(Node* node) {
-  Int32BinopMatcher m(node);
-  if (m.left().IsWord32And() && CanCover(node, m.left().node()) &&
-      m.right().IsInRange(1, 31)) {
-    RiscvOperandGenerator g(this);
-    Int32BinopMatcher mleft(m.left().node());
-    // Match Word32Shl(Word32And(x, mask), imm) to Shl where the mask is
-    // contiguous, and the shift immediate non-zero.
-    if (mleft.right().HasResolvedValue()) {
-      uint32_t mask = mleft.right().ResolvedValue();
-      uint32_t mask_width = base::bits::CountPopulation(mask);
-      uint32_t mask_msb = base::bits::CountLeadingZeros32(mask);
-      if ((mask_width != 0) && (mask_msb + mask_width == 32)) {
-        uint32_t shift = m.right().ResolvedValue();
-        DCHECK_EQ(0u, base::bits::CountTrailingZeros32(mask));
-        DCHECK_NE(0u, shift);
-        if ((shift + mask_width) >= 32) {
-          // If the mask is contiguous and reaches or extends beyond the top
-          // bit, only the shift is needed.
-          Emit(kRiscvShl32, g.DefineAsRegister(node),
-               g.UseRegister(mleft.left().node()),
-               g.UseImmediate(m.right().node()));
-          return;
-        }
-      }
-    }
-  }
-  VisitRRO(this, kRiscvShl32, node);
-}
-
-void InstructionSelector::VisitWord32Shr(Node* node) {
-  VisitRRO(this, kRiscvShr32, node);
-}
-
-void InstructionSelector::VisitWord32Sar(Node* node) {
-  Int32BinopMatcher m(node);
-  if (m.left().IsWord32Shl() && CanCover(node, m.left().node())) {
-    Int32BinopMatcher mleft(m.left().node());
-    if (m.right().HasResolvedValue() && mleft.right().HasResolvedValue()) {
-      RiscvOperandGenerator g(this);
-      uint32_t sar = m.right().ResolvedValue();
-      uint32_t shl = mleft.right().ResolvedValue();
-      if ((sar == shl) && (sar == 16)) {
-        Emit(kRiscvSignExtendShort, g.DefineAsRegister(node),
-             g.UseRegister(mleft.left().node()));
-        return;
-      } else if ((sar == shl) && (sar == 24)) {
-        Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),
-             g.UseRegister(mleft.left().node()));
-        return;
-      } else if ((sar == shl) && (sar == 32)) {
-        Emit(kRiscvShl32, g.DefineAsRegister(node),
-             g.UseRegister(mleft.left().node()), g.TempImmediate(0));
-        return;
-      }
-    }
-  }
-  VisitRRO(this, kRiscvSar32, node);
-}
-
 void InstructionSelector::VisitWord64Shl(Node* node) {
   RiscvOperandGenerator g(this);
   Int64BinopMatcher m(node);
@@ -989,58 +663,6 @@ void InstructionSelector::VisitInt32Mul(Node* node) {
   VisitRRR(this, kRiscvMul32, node);
 }
 
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8S(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand src1 = g.TempSimd128Register();
-  InstructionOperand src2 = g.TempSimd128Register();
-  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
-  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0006000400020000),
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0007000500030001),
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVwadd, g.DefineAsRegister(node), src1, src2,
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(mf2)));
-}
-
-void InstructionSelector::VisitI32x4ExtAddPairwiseI16x8U(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand src1 = g.TempSimd128Register();
-  InstructionOperand src2 = g.TempSimd128Register();
-  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
-  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0006000400020000),
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0007000500030001),
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVwaddu, g.DefineAsRegister(node), src1, src2,
-       g.UseImmediate(int8_t(E16)), g.UseImmediate(int8_t(mf2)));
-}
-
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16S(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand src1 = g.TempSimd128Register();
-  InstructionOperand src2 = g.TempSimd128Register();
-  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
-  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0E0C0A0806040200),
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0F0D0B0907050301),
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVwadd, g.DefineAsRegister(node), src1, src2,
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(mf2)));
-}
-
-void InstructionSelector::VisitI16x8ExtAddPairwiseI8x16U(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand src1 = g.TempSimd128Register();
-  InstructionOperand src2 = g.TempSimd128Register();
-  InstructionOperand src = g.UseUniqueRegister(node->InputAt(0));
-  Emit(kRiscvVrgather, src1, src, g.UseImmediate64(0x0E0C0A0806040200),
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVrgather, src2, src, g.UseImmediate64(0x0F0D0B0907050301),
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(m1)));
-  Emit(kRiscvVwaddu, g.DefineAsRegister(node), src1, src2,
-       g.UseImmediate(int8_t(E8)), g.UseImmediate(int8_t(mf2)));
-}
-
 void InstructionSelector::VisitInt32MulHigh(Node* node) {
   VisitRRR(this, kRiscvMulHigh32, node);
 }
@@ -1538,28 +1160,6 @@ void InstructionSelector::VisitTruncateInt64ToInt32(Node* node) {
   EmitSignExtendWord(this, node);
 }
 
-void InstructionSelector::VisitTruncateFloat64ToFloat32(Node* node) {
-  RiscvOperandGenerator g(this);
-  Node* value = node->InputAt(0);
-  // Match TruncateFloat64ToFloat32(ChangeInt32ToFloat64) to corresponding
-  // instruction.
-  if (CanCover(node, value) &&
-      value->opcode() == IrOpcode::kChangeInt32ToFloat64) {
-    Emit(kRiscvCvtSW, g.DefineAsRegister(node),
-         g.UseRegister(value->InputAt(0)));
-    return;
-  }
-  VisitRR(this, kRiscvCvtSD, node);
-}
-
-void InstructionSelector::VisitTruncateFloat64ToWord32(Node* node) {
-  VisitRR(this, kArchTruncateDoubleToI, node);
-}
-
-void InstructionSelector::VisitRoundFloat64ToInt32(Node* node) {
-  VisitRR(this, kRiscvTruncWD, node);
-}
-
 void InstructionSelector::VisitRoundInt64ToFloat32(Node* node) {
   VisitRR(this, kRiscvCvtSL, node);
 }
@@ -1592,89 +1192,6 @@ void InstructionSelector::VisitBitcastInt64ToFloat64(Node* node) {
   VisitRR(this, kRiscvBitcastLD, node);
 }
 
-void InstructionSelector::VisitFloat32Add(Node* node) {
-  VisitRRR(this, kRiscvAddS, node);
-}
-
-void InstructionSelector::VisitFloat64Add(Node* node) {
-  VisitRRR(this, kRiscvAddD, node);
-}
-
-void InstructionSelector::VisitFloat32Sub(Node* node) {
-  VisitRRR(this, kRiscvSubS, node);
-}
-
-void InstructionSelector::VisitFloat64Sub(Node* node) {
-  VisitRRR(this, kRiscvSubD, node);
-}
-
-void InstructionSelector::VisitFloat32Mul(Node* node) {
-  VisitRRR(this, kRiscvMulS, node);
-}
-
-void InstructionSelector::VisitFloat64Mul(Node* node) {
-  VisitRRR(this, kRiscvMulD, node);
-}
-
-void InstructionSelector::VisitFloat32Div(Node* node) {
-  VisitRRR(this, kRiscvDivS, node);
-}
-
-void InstructionSelector::VisitFloat64Div(Node* node) {
-  VisitRRR(this, kRiscvDivD, node);
-}
-
-void InstructionSelector::VisitFloat64Mod(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvModD, g.DefineAsFixed(node, fa0),
-       g.UseFixed(node->InputAt(0), fa0), g.UseFixed(node->InputAt(1), fa1))
-      ->MarkAsCall();
-}
-
-void InstructionSelector::VisitFloat32Max(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvFloat32Max, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
-}
-
-void InstructionSelector::VisitFloat64Max(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvFloat64Max, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
-}
-
-void InstructionSelector::VisitFloat32Min(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvFloat32Min, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
-}
-
-void InstructionSelector::VisitFloat64Min(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvFloat64Min, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)), g.UseRegister(node->InputAt(1)));
-}
-
-void InstructionSelector::VisitFloat32Abs(Node* node) {
-  VisitRR(this, kRiscvAbsS, node);
-}
-
-void InstructionSelector::VisitFloat64Abs(Node* node) {
-  VisitRR(this, kRiscvAbsD, node);
-}
-
-void InstructionSelector::VisitFloat32Sqrt(Node* node) {
-  VisitRR(this, kRiscvSqrtS, node);
-}
-
-void InstructionSelector::VisitFloat64Sqrt(Node* node) {
-  VisitRR(this, kRiscvSqrtD, node);
-}
-
-void InstructionSelector::VisitFloat32RoundDown(Node* node) {
-  VisitRR(this, kRiscvFloat32RoundDown, node);
-}
-
 void InstructionSelector::VisitFloat64RoundDown(Node* node) {
   VisitRR(this, kRiscvFloat64RoundDown, node);
 }
@@ -1730,11 +1247,6 @@ void InstructionSelector::VisitFloat64Ieee754Unop(Node* node,
       ->MarkAsCall();
 }
 
-void InstructionSelector::EmitMoveParamToFPR(Node* node, int index) {}
-
-void InstructionSelector::EmitMoveFPRToParam(InstructionOperand* op,
-                                             LinkageLocation location) {}
-
 void InstructionSelector::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
     Node* node) {
@@ -1776,31 +1288,6 @@ void InstructionSelector::EmitPrepareArguments(
   }
 }
 
-void InstructionSelector::EmitPrepareResults(
-    ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
-  RiscvOperandGenerator g(this);
-
-  int reverse_slot = 1;
-  for (PushParameter output : *results) {
-    if (!output.location.IsCallerFrameSlot()) continue;
-    // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
-      DCHECK(!call_descriptor->IsCFunctionCall());
-      if (output.location.GetType() == MachineType::Float32()) {
-        MarkAsFloat32(output.node);
-      } else if (output.location.GetType() == MachineType::Float64()) {
-        MarkAsFloat64(output.node);
-      }
-      Emit(kRiscvPeek, g.DefineAsRegister(output.node),
-           g.UseImmediate(reverse_slot));
-    }
-    reverse_slot += output.location.GetSizeInPointers();
-  }
-}
-
-bool InstructionSelector::IsTailCallAddressImmediate() { return false; }
-
 void InstructionSelector::VisitUnalignedLoad(Node* node) {
   LoadRepresentation load_rep = LoadRepresentationOf(node->op());
   RiscvOperandGenerator g(this);
@@ -1915,113 +1402,6 @@ void InstructionSelector::VisitUnalignedStore(Node* node) {
 
 namespace {
 
-// Shared routine for multiple compare operations.
-static void VisitCompare(InstructionSelector* selector, InstructionCode opcode,
-                         InstructionOperand left, InstructionOperand right,
-                         FlagsContinuation* cont) {
-  selector->EmitWithContinuation(opcode, left, right, cont);
-}
-
-// Shared routine for multiple compare operations.
-static void VisitWordCompareZero(InstructionSelector* selector,
-                                 InstructionOperand value,
-                                 FlagsContinuation* cont) {
-  selector->EmitWithContinuation(kRiscvCmpZero, value, cont);
-}
-
-// Shared routine for multiple float32 compare operations.
-void VisitFloat32Compare(InstructionSelector* selector, Node* node,
-                         FlagsContinuation* cont) {
-  RiscvOperandGenerator g(selector);
-  Float32BinopMatcher m(node);
-  InstructionOperand lhs, rhs;
-
-  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
-                          : g.UseRegister(m.left().node());
-  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
-                           : g.UseRegister(m.right().node());
-  VisitCompare(selector, kRiscvCmpS, lhs, rhs, cont);
-}
-
-// Shared routine for multiple float64 compare operations.
-void VisitFloat64Compare(InstructionSelector* selector, Node* node,
-                         FlagsContinuation* cont) {
-  RiscvOperandGenerator g(selector);
-  Float64BinopMatcher m(node);
-  InstructionOperand lhs, rhs;
-
-  lhs = m.left().IsZero() ? g.UseImmediate(m.left().node())
-                          : g.UseRegister(m.left().node());
-  rhs = m.right().IsZero() ? g.UseImmediate(m.right().node())
-                           : g.UseRegister(m.right().node());
-  VisitCompare(selector, kRiscvCmpD, lhs, rhs, cont);
-}
-
-// Shared routine for multiple word compare operations.
-void VisitWordCompare(InstructionSelector* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont,
-                      bool commutative) {
-  RiscvOperandGenerator g(selector);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  // If one of the two inputs is an immediate, make sure it's on the right.
-  if (!g.CanBeImmediate(right, opcode) && g.CanBeImmediate(left, opcode)) {
-    cont->Commute();
-    std::swap(left, right);
-  }
-  // Match immediates on right side of comparison.
-  if (g.CanBeImmediate(right, opcode)) {
-    if (opcode == kRiscvTst) {
-      VisitCompare(selector, opcode, g.UseRegister(left), g.UseImmediate(right),
-                   cont);
-    } else {
-      switch (cont->condition()) {
-        case kEqual:
-        case kNotEqual:
-          if (cont->IsSet()) {
-            VisitCompare(selector, opcode, g.UseRegister(left),
-                         g.UseImmediate(right), cont);
-          } else {
-            Int32BinopMatcher m(node, true);
-            NumberBinopMatcher n(node, true);
-            if (m.right().Is(0) || n.right().IsZero()) {
-              VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
-                                   cont);
-            } else {
-              VisitCompare(selector, opcode, g.UseRegister(left),
-                           g.UseRegister(right), cont);
-            }
-          }
-          break;
-        case kSignedLessThan:
-        case kSignedGreaterThanOrEqual:
-        case kUnsignedLessThan:
-        case kUnsignedGreaterThanOrEqual: {
-          Int32BinopMatcher m(node, true);
-          if (m.right().Is(0)) {
-            VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
-                                 cont);
-          } else {
-            VisitCompare(selector, opcode, g.UseRegister(left),
-                         g.UseImmediate(right), cont);
-          }
-        } break;
-        default:
-          Int32BinopMatcher m(node, true);
-          if (m.right().Is(0)) {
-            VisitWordCompareZero(selector, g.UseRegisterOrImmediateZero(left),
-                                 cont);
-          } else {
-            VisitCompare(selector, opcode, g.UseRegister(left),
-                         g.UseRegister(right), cont);
-          }
-      }
-    }
-  } else {
-    VisitCompare(selector, opcode, g.UseRegister(left), g.UseRegister(right),
-                 cont);
-  }
-}
 #ifndef V8_COMPRESS_POINTERS
 bool IsNodeUnsigned(Node* n) {
   NodeMatcher m(n);
@@ -2129,13 +1509,6 @@ void VisitWord64Compare(InstructionSelector* selector, Node* node,
   VisitWordCompare(selector, node, kRiscvCmp, cont, false);
 }
 
-void EmitWordCompareZero(InstructionSelector* selector, Node* value,
-                         FlagsContinuation* cont) {
-  RiscvOperandGenerator g(selector);
-  selector->EmitWithContinuation(kRiscvCmpZero,
-                                 g.UseRegisterOrImmediateZero(value), cont);
-}
-
 void VisitAtomicLoad(InstructionSelector* selector, Node* node,
                      ArchOpcode opcode, AtomicWidth width) {
   RiscvOperandGenerator g(selector);
@@ -2181,56 +1554,6 @@ void VisitAtomicStore(InstructionSelector* selector, Node* node,
   }
 }
 
-void VisitAtomicExchange(InstructionSelector* selector, Node* node,
-                         ArchOpcode opcode, AtomicWidth width) {
-  RiscvOperandGenerator g(selector);
-  Node* base = node->InputAt(0);
-  Node* index = node->InputAt(1);
-  Node* value = node->InputAt(2);
-
-  AddressingMode addressing_mode = kMode_MRI;
-  InstructionOperand inputs[3];
-  size_t input_count = 0;
-  inputs[input_count++] = g.UseUniqueRegister(base);
-  inputs[input_count++] = g.UseUniqueRegister(index);
-  inputs[input_count++] = g.UseUniqueRegister(value);
-  InstructionOperand outputs[1];
-  outputs[0] = g.UseUniqueRegister(node);
-  InstructionOperand temp[3];
-  temp[0] = g.TempRegister();
-  temp[1] = g.TempRegister();
-  temp[2] = g.TempRegister();
-  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode) |
-                         AtomicWidthField::encode(width);
-  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
-}
-
-void VisitAtomicCompareExchange(InstructionSelector* selector, Node* node,
-                                ArchOpcode opcode, AtomicWidth width) {
-  RiscvOperandGenerator g(selector);
-  Node* base = node->InputAt(0);
-  Node* index = node->InputAt(1);
-  Node* old_value = node->InputAt(2);
-  Node* new_value = node->InputAt(3);
-
-  AddressingMode addressing_mode = kMode_MRI;
-  InstructionOperand inputs[4];
-  size_t input_count = 0;
-  inputs[input_count++] = g.UseUniqueRegister(base);
-  inputs[input_count++] = g.UseUniqueRegister(index);
-  inputs[input_count++] = g.UseUniqueRegister(old_value);
-  inputs[input_count++] = g.UseUniqueRegister(new_value);
-  InstructionOperand outputs[1];
-  outputs[0] = g.UseUniqueRegister(node);
-  InstructionOperand temp[3];
-  temp[0] = g.TempRegister();
-  temp[1] = g.TempRegister();
-  temp[2] = g.TempRegister();
-  InstructionCode code = opcode | AddressingModeField::encode(addressing_mode) |
-                         AtomicWidthField::encode(width);
-  selector->Emit(code, 1, outputs, input_count, inputs, 3, temp);
-}
-
 void VisitAtomicBinop(InstructionSelector* selector, Node* node,
                       ArchOpcode opcode, AtomicWidth width) {
   RiscvOperandGenerator g(selector);
@@ -2408,37 +1731,6 @@ void InstructionSelector::VisitWordCompareZero(Node* user, Node* value,
   EmitWordCompareZero(this, value, cont);
 }
 
-void InstructionSelector::VisitSwitch(Node* node, const SwitchInfo& sw) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
-
-  // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
-    static const size_t kMaxTableSwitchValueRange = 2 << 16;
-    size_t table_space_cost = 10 + 2 * sw.value_range();
-    size_t table_time_cost = 3;
-    size_t lookup_space_cost = 2 + 2 * sw.case_count();
-    size_t lookup_time_cost = sw.case_count();
-    if (sw.case_count() > 0 &&
-        table_space_cost + 3 * table_time_cost <=
-            lookup_space_cost + 3 * lookup_time_cost &&
-        sw.min_value() > std::numeric_limits<int32_t>::min() &&
-        sw.value_range() <= kMaxTableSwitchValueRange) {
-      InstructionOperand index_operand = value_operand;
-      if (sw.min_value()) {
-        index_operand = g.TempRegister();
-        Emit(kRiscvSub32, index_operand, value_operand,
-             g.TempImmediate(sw.min_value()));
-      }
-      // Generate a table lookup.
-      return EmitTableSwitch(sw, index_operand);
-    }
-  }
-
-  // Generate a tree of conditional jumps.
-  return EmitBinarySearchSwitch(sw, value_operand);
-}
-
 void InstructionSelector::VisitWord32Equal(Node* const node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   Int32BinopMatcher m(node);
@@ -2548,71 +1840,6 @@ void InstructionSelector::VisitUint64LessThanOrEqual(Node* node) {
   VisitWord64Compare(this, node, &cont);
 }
 
-void InstructionSelector::VisitFloat32Equal(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  VisitFloat32Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat32LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
-  VisitFloat32Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat32LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
-  VisitFloat32Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat64Equal(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  VisitFloat64Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
-  VisitFloat64Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
-  VisitFloat64Compare(this, node, &cont);
-}
-
-void InstructionSelector::VisitFloat64ExtractLowWord32(Node* node) {
-  VisitRR(this, kRiscvFloat64ExtractLowWord32, node);
-}
-
-void InstructionSelector::VisitFloat64ExtractHighWord32(Node* node) {
-  VisitRR(this, kRiscvFloat64ExtractHighWord32, node);
-}
-
-void InstructionSelector::VisitFloat64SilenceNaN(Node* node) {
-  VisitRR(this, kRiscvFloat64SilenceNaN, node);
-}
-
-void InstructionSelector::VisitFloat64InsertLowWord32(Node* node) {
-  RiscvOperandGenerator g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  Emit(kRiscvFloat64InsertLowWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.UseRegister(right));
-}
-
-void InstructionSelector::VisitFloat64InsertHighWord32(Node* node) {
-  RiscvOperandGenerator g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  Emit(kRiscvFloat64InsertHighWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.UseRegister(right));
-}
-
-void InstructionSelector::VisitMemoryBarrier(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvSync, g.NoOutput());
-}
-
 void InstructionSelector::VisitWord32AtomicLoad(Node* node) {
   AtomicLoadParameters atomic_load_params = AtomicLoadParametersOf(node->op());
   LoadRepresentation load_rep = atomic_load_params.representation();
@@ -2885,431 +2112,6 @@ void InstructionSelector::VisitInt64AbsWithOverflow(Node* node) {
   UNREACHABLE();
 }
 
-#define SIMD_TYPE_LIST(V) \
-  V(F32x4)                \
-  V(I64x2)                \
-  V(I32x4)                \
-  V(I16x8)                \
-  V(I8x16)
-
-#define SIMD_UNOP_LIST(V)                                       \
-  V(F64x2Abs, kRiscvF64x2Abs)                                   \
-  V(F64x2Neg, kRiscvF64x2Neg)                                   \
-  V(F64x2Sqrt, kRiscvF64x2Sqrt)                                 \
-  V(F64x2ConvertLowI32x4S, kRiscvF64x2ConvertLowI32x4S)         \
-  V(F64x2ConvertLowI32x4U, kRiscvF64x2ConvertLowI32x4U)         \
-  V(F64x2PromoteLowF32x4, kRiscvF64x2PromoteLowF32x4)           \
-  V(F64x2Ceil, kRiscvF64x2Ceil)                                 \
-  V(F64x2Floor, kRiscvF64x2Floor)                               \
-  V(F64x2Trunc, kRiscvF64x2Trunc)                               \
-  V(F64x2NearestInt, kRiscvF64x2NearestInt)                     \
-  V(I64x2Neg, kRiscvI64x2Neg)                                   \
-  V(I64x2Abs, kRiscvI64x2Abs)                                   \
-  V(I64x2BitMask, kRiscvI64x2BitMask)                           \
-  V(F32x4SConvertI32x4, kRiscvF32x4SConvertI32x4)               \
-  V(F32x4UConvertI32x4, kRiscvF32x4UConvertI32x4)               \
-  V(F32x4Abs, kRiscvF32x4Abs)                                   \
-  V(F32x4Neg, kRiscvF32x4Neg)                                   \
-  V(F32x4Sqrt, kRiscvF32x4Sqrt)                                 \
-  V(F32x4DemoteF64x2Zero, kRiscvF32x4DemoteF64x2Zero)           \
-  V(F32x4Ceil, kRiscvF32x4Ceil)                                 \
-  V(F32x4Floor, kRiscvF32x4Floor)                               \
-  V(F32x4Trunc, kRiscvF32x4Trunc)                               \
-  V(F32x4NearestInt, kRiscvF32x4NearestInt)                     \
-  V(I32x4RelaxedTruncF32x4S, kRiscvI32x4SConvertF32x4)          \
-  V(I32x4RelaxedTruncF32x4U, kRiscvI32x4UConvertF32x4)          \
-  V(I32x4RelaxedTruncF64x2SZero, kRiscvI32x4TruncSatF64x2SZero) \
-  V(I32x4RelaxedTruncF64x2UZero, kRiscvI32x4TruncSatF64x2UZero) \
-  V(I64x2SConvertI32x4Low, kRiscvI64x2SConvertI32x4Low)         \
-  V(I64x2SConvertI32x4High, kRiscvI64x2SConvertI32x4High)       \
-  V(I64x2UConvertI32x4Low, kRiscvI64x2UConvertI32x4Low)         \
-  V(I64x2UConvertI32x4High, kRiscvI64x2UConvertI32x4High)       \
-  V(I32x4SConvertF32x4, kRiscvI32x4SConvertF32x4)               \
-  V(I32x4UConvertF32x4, kRiscvI32x4UConvertF32x4)               \
-  V(I32x4Neg, kRiscvI32x4Neg)                                   \
-  V(I32x4SConvertI16x8Low, kRiscvI32x4SConvertI16x8Low)         \
-  V(I32x4SConvertI16x8High, kRiscvI32x4SConvertI16x8High)       \
-  V(I32x4UConvertI16x8Low, kRiscvI32x4UConvertI16x8Low)         \
-  V(I32x4UConvertI16x8High, kRiscvI32x4UConvertI16x8High)       \
-  V(I32x4Abs, kRiscvI32x4Abs)                                   \
-  V(I32x4BitMask, kRiscvI32x4BitMask)                           \
-  V(I32x4TruncSatF64x2SZero, kRiscvI32x4TruncSatF64x2SZero)     \
-  V(I32x4TruncSatF64x2UZero, kRiscvI32x4TruncSatF64x2UZero)     \
-  V(I16x8Neg, kRiscvI16x8Neg)                                   \
-  V(I16x8SConvertI8x16Low, kRiscvI16x8SConvertI8x16Low)         \
-  V(I16x8SConvertI8x16High, kRiscvI16x8SConvertI8x16High)       \
-  V(I16x8UConvertI8x16Low, kRiscvI16x8UConvertI8x16Low)         \
-  V(I16x8UConvertI8x16High, kRiscvI16x8UConvertI8x16High)       \
-  V(I16x8Abs, kRiscvI16x8Abs)                                   \
-  V(I16x8BitMask, kRiscvI16x8BitMask)                           \
-  V(I8x16Neg, kRiscvI8x16Neg)                                   \
-  V(I8x16Abs, kRiscvI8x16Abs)                                   \
-  V(I8x16BitMask, kRiscvI8x16BitMask)                           \
-  V(I8x16Popcnt, kRiscvI8x16Popcnt)                             \
-  V(S128Not, kRiscvS128Not)                                     \
-  V(V128AnyTrue, kRiscvV128AnyTrue)                             \
-  V(I32x4AllTrue, kRiscvI32x4AllTrue)                           \
-  V(I16x8AllTrue, kRiscvI16x8AllTrue)                           \
-  V(I8x16AllTrue, kRiscvI8x16AllTrue)                           \
-  V(I64x2AllTrue, kRiscvI64x2AllTrue)
-
-#define SIMD_SHIFT_OP_LIST(V) \
-  V(I64x2Shl)                 \
-  V(I64x2ShrS)                \
-  V(I64x2ShrU)                \
-  V(I32x4Shl)                 \
-  V(I32x4ShrS)                \
-  V(I32x4ShrU)                \
-  V(I16x8Shl)                 \
-  V(I16x8ShrS)                \
-  V(I16x8ShrU)                \
-  V(I8x16Shl)                 \
-  V(I8x16ShrS)                \
-  V(I8x16ShrU)
-
-#define SIMD_BINOP_LIST(V)                              \
-  V(F64x2Add, kRiscvF64x2Add)                           \
-  V(F64x2Sub, kRiscvF64x2Sub)                           \
-  V(F64x2Mul, kRiscvF64x2Mul)                           \
-  V(F64x2Div, kRiscvF64x2Div)                           \
-  V(F64x2Min, kRiscvF64x2Min)                           \
-  V(F64x2Max, kRiscvF64x2Max)                           \
-  V(F64x2Eq, kRiscvF64x2Eq)                             \
-  V(F64x2Ne, kRiscvF64x2Ne)                             \
-  V(F64x2Lt, kRiscvF64x2Lt)                             \
-  V(F64x2Le, kRiscvF64x2Le)                             \
-  V(I64x2Eq, kRiscvI64x2Eq)                             \
-  V(I64x2Ne, kRiscvI64x2Ne)                             \
-  V(I64x2GtS, kRiscvI64x2GtS)                           \
-  V(I64x2GeS, kRiscvI64x2GeS)                           \
-  V(I64x2Add, kRiscvI64x2Add)                           \
-  V(I64x2Sub, kRiscvI64x2Sub)                           \
-  V(I64x2Mul, kRiscvI64x2Mul)                           \
-  V(F32x4Add, kRiscvF32x4Add)                           \
-  V(F32x4Sub, kRiscvF32x4Sub)                           \
-  V(F32x4Mul, kRiscvF32x4Mul)                           \
-  V(F32x4Div, kRiscvF32x4Div)                           \
-  V(F32x4Max, kRiscvF32x4Max)                           \
-  V(F32x4Min, kRiscvF32x4Min)                           \
-  V(F32x4Eq, kRiscvF32x4Eq)                             \
-  V(F32x4Ne, kRiscvF32x4Ne)                             \
-  V(F32x4Lt, kRiscvF32x4Lt)                             \
-  V(F32x4Le, kRiscvF32x4Le)                             \
-  V(F32x4RelaxedMin, kRiscvF32x4Min)                    \
-  V(F32x4RelaxedMax, kRiscvF32x4Max)                    \
-  V(F64x2RelaxedMin, kRiscvF64x2Min)                    \
-  V(F64x2RelaxedMax, kRiscvF64x2Max)                    \
-  V(I32x4Add, kRiscvI32x4Add)                           \
-  V(I32x4Sub, kRiscvI32x4Sub)                           \
-  V(I32x4Mul, kRiscvI32x4Mul)                           \
-  V(I32x4MaxS, kRiscvI32x4MaxS)                         \
-  V(I32x4MinS, kRiscvI32x4MinS)                         \
-  V(I32x4MaxU, kRiscvI32x4MaxU)                         \
-  V(I32x4MinU, kRiscvI32x4MinU)                         \
-  V(I32x4Eq, kRiscvI32x4Eq)                             \
-  V(I32x4Ne, kRiscvI32x4Ne)                             \
-  V(I32x4GtS, kRiscvI32x4GtS)                           \
-  V(I32x4GeS, kRiscvI32x4GeS)                           \
-  V(I32x4GtU, kRiscvI32x4GtU)                           \
-  V(I32x4GeU, kRiscvI32x4GeU)                           \
-  V(I16x8Add, kRiscvI16x8Add)                           \
-  V(I16x8AddSatS, kRiscvI16x8AddSatS)                   \
-  V(I16x8AddSatU, kRiscvI16x8AddSatU)                   \
-  V(I16x8Sub, kRiscvI16x8Sub)                           \
-  V(I16x8SubSatS, kRiscvI16x8SubSatS)                   \
-  V(I16x8SubSatU, kRiscvI16x8SubSatU)                   \
-  V(I16x8Mul, kRiscvI16x8Mul)                           \
-  V(I16x8MaxS, kRiscvI16x8MaxS)                         \
-  V(I16x8MinS, kRiscvI16x8MinS)                         \
-  V(I16x8MaxU, kRiscvI16x8MaxU)                         \
-  V(I16x8MinU, kRiscvI16x8MinU)                         \
-  V(I16x8Eq, kRiscvI16x8Eq)                             \
-  V(I16x8Ne, kRiscvI16x8Ne)                             \
-  V(I16x8GtS, kRiscvI16x8GtS)                           \
-  V(I16x8GeS, kRiscvI16x8GeS)                           \
-  V(I16x8GtU, kRiscvI16x8GtU)                           \
-  V(I16x8GeU, kRiscvI16x8GeU)                           \
-  V(I16x8RoundingAverageU, kRiscvI16x8RoundingAverageU) \
-  V(I16x8Q15MulRSatS, kRiscvI16x8Q15MulRSatS)           \
-  V(I16x8SConvertI32x4, kRiscvI16x8SConvertI32x4)       \
-  V(I16x8UConvertI32x4, kRiscvI16x8UConvertI32x4)       \
-  V(I8x16Add, kRiscvI8x16Add)                           \
-  V(I8x16AddSatS, kRiscvI8x16AddSatS)                   \
-  V(I8x16AddSatU, kRiscvI8x16AddSatU)                   \
-  V(I8x16Sub, kRiscvI8x16Sub)                           \
-  V(I8x16SubSatS, kRiscvI8x16SubSatS)                   \
-  V(I8x16SubSatU, kRiscvI8x16SubSatU)                   \
-  V(I8x16MaxS, kRiscvI8x16MaxS)                         \
-  V(I8x16MinS, kRiscvI8x16MinS)                         \
-  V(I8x16MaxU, kRiscvI8x16MaxU)                         \
-  V(I8x16MinU, kRiscvI8x16MinU)                         \
-  V(I8x16Eq, kRiscvI8x16Eq)                             \
-  V(I8x16Ne, kRiscvI8x16Ne)                             \
-  V(I8x16GtS, kRiscvI8x16GtS)                           \
-  V(I8x16GeS, kRiscvI8x16GeS)                           \
-  V(I8x16GtU, kRiscvI8x16GtU)                           \
-  V(I8x16GeU, kRiscvI8x16GeU)                           \
-  V(I8x16RoundingAverageU, kRiscvI8x16RoundingAverageU) \
-  V(I8x16SConvertI16x8, kRiscvI8x16SConvertI16x8)       \
-  V(I8x16UConvertI16x8, kRiscvI8x16UConvertI16x8)       \
-  V(S128And, kRiscvS128And)                             \
-  V(S128Or, kRiscvS128Or)                               \
-  V(S128Xor, kRiscvS128Xor)                             \
-  V(S128AndNot, kRiscvS128AndNot)
-
-void InstructionSelector::VisitS128Const(Node* node) {
-  RiscvOperandGenerator g(this);
-  static const int kUint32Immediates = kSimd128Size / sizeof(uint32_t);
-  uint32_t val[kUint32Immediates];
-  memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
-  // If all bytes are zeros or ones, avoid emitting code for generic constants
-  bool all_zeros = !(val[0] || val[1] || val[2] || val[3]);
-  bool all_ones = val[0] == UINT32_MAX && val[1] == UINT32_MAX &&
-                  val[2] == UINT32_MAX && val[3] == UINT32_MAX;
-  InstructionOperand dst = g.DefineAsRegister(node);
-  if (all_zeros) {
-    Emit(kRiscvS128Zero, dst);
-  } else if (all_ones) {
-    Emit(kRiscvS128AllOnes, dst);
-  } else {
-    Emit(kRiscvS128Const, dst, g.UseImmediate(val[0]), g.UseImmediate(val[1]),
-         g.UseImmediate(val[2]), g.UseImmediate(val[3]));
-  }
-}
-
-void InstructionSelector::VisitS128Zero(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvS128Zero, g.DefineAsRegister(node));
-}
-
-#define SIMD_VISIT_SPLAT(Type)                               \
-  void InstructionSelector::Visit##Type##Splat(Node* node) { \
-    VisitRR(this, kRiscv##Type##Splat, node);                \
-  }
-SIMD_TYPE_LIST(SIMD_VISIT_SPLAT)
-SIMD_VISIT_SPLAT(F64x2)
-#undef SIMD_VISIT_SPLAT
-
-#define SIMD_VISIT_EXTRACT_LANE(Type, Sign)                              \
-  void InstructionSelector::Visit##Type##ExtractLane##Sign(Node* node) { \
-    VisitRRI(this, kRiscv##Type##ExtractLane##Sign, node);               \
-  }
-SIMD_VISIT_EXTRACT_LANE(F64x2, )
-SIMD_VISIT_EXTRACT_LANE(F32x4, )
-SIMD_VISIT_EXTRACT_LANE(I32x4, )
-SIMD_VISIT_EXTRACT_LANE(I64x2, )
-SIMD_VISIT_EXTRACT_LANE(I16x8, U)
-SIMD_VISIT_EXTRACT_LANE(I16x8, S)
-SIMD_VISIT_EXTRACT_LANE(I8x16, U)
-SIMD_VISIT_EXTRACT_LANE(I8x16, S)
-#undef SIMD_VISIT_EXTRACT_LANE
-
-#define SIMD_VISIT_REPLACE_LANE(Type)                              \
-  void InstructionSelector::Visit##Type##ReplaceLane(Node* node) { \
-    VisitRRIR(this, kRiscv##Type##ReplaceLane, node);              \
-  }
-SIMD_TYPE_LIST(SIMD_VISIT_REPLACE_LANE)
-SIMD_VISIT_REPLACE_LANE(F64x2)
-#undef SIMD_VISIT_REPLACE_LANE
-
-#define SIMD_VISIT_UNOP(Name, instruction)            \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRR(this, instruction, node);                 \
-  }
-SIMD_UNOP_LIST(SIMD_VISIT_UNOP)
-#undef SIMD_VISIT_UNOP
-
-#define SIMD_VISIT_SHIFT_OP(Name)                     \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitSimdShift(this, kRiscv##Name, node);         \
-  }
-SIMD_SHIFT_OP_LIST(SIMD_VISIT_SHIFT_OP)
-#undef SIMD_VISIT_SHIFT_OP
-
-#define SIMD_VISIT_BINOP(Name, instruction)           \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRR(this, instruction, node);                \
-  }
-SIMD_BINOP_LIST(SIMD_VISIT_BINOP)
-#undef SIMD_VISIT_BINOP
-
-void InstructionSelector::VisitS128Select(Node* node) {
-  VisitRRRR(this, kRiscvS128Select, node);
-}
-
-#define SIMD_VISIT_SELECT_LANE(Name)                  \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRRR(this, kRiscvS128Select, node);          \
-  }
-SIMD_VISIT_SELECT_LANE(I8x16RelaxedLaneSelect)
-SIMD_VISIT_SELECT_LANE(I16x8RelaxedLaneSelect)
-SIMD_VISIT_SELECT_LANE(I32x4RelaxedLaneSelect)
-SIMD_VISIT_SELECT_LANE(I64x2RelaxedLaneSelect)
-#undef SIMD_VISIT_SELECT_LANE
-
-#define VISIT_SIMD_QFMOP(Name, instruction)           \
-  void InstructionSelector::Visit##Name(Node* node) { \
-    VisitRRRR(this, instruction, node);               \
-  }
-VISIT_SIMD_QFMOP(F64x2Qfma, kRiscvF64x2Qfma)
-VISIT_SIMD_QFMOP(F64x2Qfms, kRiscvF64x2Qfms)
-VISIT_SIMD_QFMOP(F32x4Qfma, kRiscvF32x4Qfma)
-VISIT_SIMD_QFMOP(F32x4Qfms, kRiscvF32x4Qfms)
-#undef VISIT_SIMD_QFMOP
-
-void InstructionSelector::VisitI32x4DotI16x8S(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand temp = g.TempFpRegister(v16);
-  InstructionOperand temp1 = g.TempFpRegister(v14);
-  InstructionOperand temp2 = g.TempFpRegister(v30);
-  InstructionOperand dst = g.DefineAsRegister(node);
-  this->Emit(kRiscvVwmul, temp, g.UseRegister(node->InputAt(0)),
-             g.UseRegister(node->InputAt(1)), g.UseImmediate(E16),
-             g.UseImmediate(m1));
-  this->Emit(kRiscvVcompress, temp2, temp, g.UseImmediate(0b01010101),
-             g.UseImmediate(E32), g.UseImmediate(m2));
-  this->Emit(kRiscvVcompress, temp1, temp, g.UseImmediate(0b10101010),
-             g.UseImmediate(E32), g.UseImmediate(m2));
-  this->Emit(kRiscvVaddVv, dst, temp1, temp2, g.UseImmediate(E32),
-             g.UseImmediate(m1));
-}
-
-namespace {
-
-struct ShuffleEntry {
-  uint8_t shuffle[kSimd128Size];
-  ArchOpcode opcode;
-};
-
-// static const ShuffleEntry arch_shuffles[] = {
-//     {{0, 1, 2, 3, 16, 17, 18, 19, 4, 5, 6, 7, 20, 21, 22, 23},
-//      kRiscvS32x4InterleaveRight},
-//     {{8, 9, 10, 11, 24, 25, 26, 27, 12, 13, 14, 15, 28, 29, 30, 31},
-//      kRiscvS32x4InterleaveLeft},
-//     {{0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27},
-//      kRiscvS32x4PackEven},
-//     {{4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31},
-//      kRiscvS32x4PackOdd},
-//     {{0, 1, 2, 3, 16, 17, 18, 19, 8, 9, 10, 11, 24, 25, 26, 27},
-//      kRiscvS32x4InterleaveEven},
-//     {{4, 5, 6, 7, 20, 21, 22, 23, 12, 13, 14, 15, 28, 29, 30, 31},
-//      kRiscvS32x4InterleaveOdd},
-
-//     {{0, 1, 16, 17, 2, 3, 18, 19, 4, 5, 20, 21, 6, 7, 22, 23},
-//      kRiscvS16x8InterleaveRight},
-//     {{8, 9, 24, 25, 10, 11, 26, 27, 12, 13, 28, 29, 14, 15, 30, 31},
-//      kRiscvS16x8InterleaveLeft},
-//     {{0, 1, 4, 5, 8, 9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29},
-//      kRiscvS16x8PackEven},
-//     {{2, 3, 6, 7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31},
-//      kRiscvS16x8PackOdd},
-//     {{0, 1, 16, 17, 4, 5, 20, 21, 8, 9, 24, 25, 12, 13, 28, 29},
-//      kRiscvS16x8InterleaveEven},
-//     {{2, 3, 18, 19, 6, 7, 22, 23, 10, 11, 26, 27, 14, 15, 30, 31},
-//      kRiscvS16x8InterleaveOdd},
-//     {{6, 7, 4, 5, 2, 3, 0, 1, 14, 15, 12, 13, 10, 11, 8, 9},
-//      kRiscvS16x4Reverse},
-//     {{2, 3, 0, 1, 6, 7, 4, 5, 10, 11, 8, 9, 14, 15, 12, 13},
-//      kRiscvS16x2Reverse},
-
-//     {{0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23},
-//      kRiscvS8x16InterleaveRight},
-//     {{8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31},
-//      kRiscvS8x16InterleaveLeft},
-//     {{0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30},
-//      kRiscvS8x16PackEven},
-//     {{1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31},
-//      kRiscvS8x16PackOdd},
-//     {{0, 16, 2, 18, 4, 20, 6, 22, 8, 24, 10, 26, 12, 28, 14, 30},
-//      kRiscvS8x16InterleaveEven},
-//     {{1, 17, 3, 19, 5, 21, 7, 23, 9, 25, 11, 27, 13, 29, 15, 31},
-//      kRiscvS8x16InterleaveOdd},
-//     {{7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8},
-//     kRiscvS8x8Reverse},
-//     {{3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12},
-//     kRiscvS8x4Reverse},
-//     {{1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14},
-//      kRiscvS8x2Reverse}};
-
-// bool TryMatchArchShuffle(const uint8_t* shuffle, const ShuffleEntry* table,
-//                          size_t num_entries, bool is_swizzle,
-//                          ArchOpcode* opcode) {
-//   uint8_t mask = is_swizzle ? kSimd128Size - 1 : 2 * kSimd128Size - 1;
-//   for (size_t i = 0; i < num_entries; ++i) {
-//     const ShuffleEntry& entry = table[i];
-//     int j = 0;
-//     for (; j < kSimd128Size; ++j) {
-//       if ((entry.shuffle[j] & mask) != (shuffle[j] & mask)) {
-//         break;
-//       }
-//     }
-//     if (j == kSimd128Size) {
-//       *opcode = entry.opcode;
-//       return true;
-//     }
-//   }
-//   return false;
-// }
-
-}  // namespace
-
-void InstructionSelector::VisitI8x16Shuffle(Node* node) {
-  uint8_t shuffle[kSimd128Size];
-  bool is_swizzle;
-  CanonicalizeShuffle(node, shuffle, &is_swizzle);
-  Node* input0 = node->InputAt(0);
-  Node* input1 = node->InputAt(1);
-  RiscvOperandGenerator g(this);
-  // uint8_t shuffle32x4[4];
-  // ArchOpcode opcode;
-  // if (TryMatchArchShuffle(shuffle, arch_shuffles, arraysize(arch_shuffles),
-  //                         is_swizzle, &opcode)) {
-  //   VisitRRR(this, opcode, node);
-  //   return;
-  // }
-  // uint8_t offset;
-  // if (wasm::SimdShuffle::TryMatchConcat(shuffle, &offset)) {
-  //   Emit(kRiscvS8x16Concat, g.DefineSameAsFirst(node), g.UseRegister(input1),
-  //        g.UseRegister(input0), g.UseImmediate(offset));
-  //   return;
-  // }
-  // if (wasm::SimdShuffle::TryMatch32x4Shuffle(shuffle, shuffle32x4)) {
-  //   Emit(kRiscvS32x4Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
-  //        g.UseRegister(input1),
-  //        g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle32x4)));
-  //   return;
-  // }
-  Emit(kRiscvI8x16Shuffle, g.DefineAsRegister(node), g.UseRegister(input0),
-       g.UseRegister(input1),
-       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle)),
-       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 4)),
-       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 8)),
-       g.UseImmediate(wasm::SimdShuffle::Pack4Lanes(shuffle + 12)));
-}
-
-void InstructionSelector::VisitI8x16Swizzle(Node* node) {
-  RiscvOperandGenerator g(this);
-  InstructionOperand temps[] = {g.TempSimd128Register()};
-  // We don't want input 0 or input 1 to be the same as output, since we will
-  // modify output before do the calculation.
-  Emit(kRiscvVrgather, g.DefineAsRegister(node),
-       g.UseUniqueRegister(node->InputAt(0)),
-       g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E8),
-       g.UseImmediate(m1), arraysize(temps), temps);
-}
-
-void InstructionSelector::VisitSignExtendWord8ToInt32(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)));
-}
-
-void InstructionSelector::VisitSignExtendWord16ToInt32(Node* node) {
-  RiscvOperandGenerator g(this);
-  Emit(kRiscvSignExtendShort, g.DefineAsRegister(node),
-       g.UseRegister(node->InputAt(0)));
-}
-
 void InstructionSelector::VisitSignExtendWord8ToInt64(Node* node) {
   RiscvOperandGenerator g(this);
   Emit(kRiscvSignExtendByte, g.DefineAsRegister(node),
@@ -3326,79 +2128,6 @@ void InstructionSelector::VisitSignExtendWord32ToInt64(Node* node) {
   EmitSignExtendWord(this, node);
 }
 
-void InstructionSelector::VisitF32x4Pmin(Node* node) {
-  VisitUniqueRRR(this, kRiscvF32x4Pmin, node);
-}
-
-void InstructionSelector::VisitF32x4Pmax(Node* node) {
-  VisitUniqueRRR(this, kRiscvF32x4Pmax, node);
-}
-
-void InstructionSelector::VisitF64x2Pmin(Node* node) {
-  VisitUniqueRRR(this, kRiscvF64x2Pmin, node);
-}
-
-void InstructionSelector::VisitF64x2Pmax(Node* node) {
-  VisitUniqueRRR(this, kRiscvF64x2Pmax, node);
-}
-
-#define VISIT_EXT_MUL(OPCODE1, OPCODE2, TYPE)                            \
-  void InstructionSelector::Visit##OPCODE1##ExtMulLow##OPCODE2##S(       \
-      Node* node) {                                                      \
-    RiscvOperandGenerator g(this);                                       \
-    Emit(kRiscvVwmul, g.DefineAsRegister(node),                          \
-         g.UseUniqueRegister(node->InputAt(0)),                          \
-         g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E##TYPE), \
-         g.UseImmediate(mf2));                                           \
-  }                                                                      \
-  void InstructionSelector::Visit##OPCODE1##ExtMulHigh##OPCODE2##S(      \
-      Node* node) {                                                      \
-    RiscvOperandGenerator g(this);                                       \
-    InstructionOperand t1 = g.TempFpRegister(v16);                       \
-    Emit(kRiscvVslidedown, t1, g.UseUniqueRegister(node->InputAt(0)),    \
-         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
-         g.UseImmediate(m1));                                            \
-    InstructionOperand t2 = g.TempFpRegister(v17);                       \
-    Emit(kRiscvVslidedown, t2, g.UseUniqueRegister(node->InputAt(1)),    \
-         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
-         g.UseImmediate(m1));                                            \
-    Emit(kRiscvVwmul, g.DefineAsRegister(node), t1, t2,                  \
-         g.UseImmediate(E##TYPE), g.UseImmediate(mf2));                  \
-  }                                                                      \
-  void InstructionSelector::Visit##OPCODE1##ExtMulLow##OPCODE2##U(       \
-      Node* node) {                                                      \
-    RiscvOperandGenerator g(this);                                       \
-    Emit(kRiscvVwmulu, g.DefineAsRegister(node),                         \
-         g.UseUniqueRegister(node->InputAt(0)),                          \
-         g.UseUniqueRegister(node->InputAt(1)), g.UseImmediate(E##TYPE), \
-         g.UseImmediate(mf2));                                           \
-  }                                                                      \
-  void InstructionSelector::Visit##OPCODE1##ExtMulHigh##OPCODE2##U(      \
-      Node* node) {                                                      \
-    RiscvOperandGenerator g(this);                                       \
-    InstructionOperand t1 = g.TempFpRegister(v16);                       \
-    Emit(kRiscvVslidedown, t1, g.UseUniqueRegister(node->InputAt(0)),    \
-         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
-         g.UseImmediate(m1));                                            \
-    InstructionOperand t2 = g.TempFpRegister(v17);                       \
-    Emit(kRiscvVslidedown, t2, g.UseUniqueRegister(node->InputAt(1)),    \
-         g.UseImmediate(kRvvVLEN / TYPE / 2), g.UseImmediate(E##TYPE),   \
-         g.UseImmediate(m1));                                            \
-    Emit(kRiscvVwmulu, g.DefineAsRegister(node), t1, t2,                 \
-         g.UseImmediate(E##TYPE), g.UseImmediate(mf2));                  \
-  }
-
-VISIT_EXT_MUL(I64x2, I32x4, 32)
-VISIT_EXT_MUL(I32x4, I16x8, 16)
-VISIT_EXT_MUL(I16x8, I8x16, 8)
-#undef VISIT_EXT_MUL
-
-void InstructionSelector::AddOutputToSelectContinuation(OperandGenerator* g,
-                                                        int first_input_index,
-                                                        Node* node) {
-  UNREACHABLE();
-}
-
 // static
 MachineOperatorBuilder::Flags
 InstructionSelector::SupportedMachineOperatorFlags() {
@@ -3419,23 +2148,6 @@ InstructionSelector::SupportedMachineOperatorFlags() {
          MachineOperatorBuilder::kFloat64RoundTiesEven |
          MachineOperatorBuilder::kFloat32RoundTiesEven;
 }
-
-// static
-MachineOperatorBuilder::AlignmentRequirements
-InstructionSelector::AlignmentRequirements() {
-#ifdef RISCV_HAS_NO_UNALIGNED
-  return MachineOperatorBuilder::AlignmentRequirements::
-      NoUnalignedAccessSupport();
-#else
-  return MachineOperatorBuilder::AlignmentRequirements::
-      FullUnalignedAccessSupport();
-#endif
-}
-
-#undef SIMD_BINOP_LIST
-#undef SIMD_SHIFT_OP_LIST
-#undef SIMD_UNOP_LIST
-#undef SIMD_TYPE_LIST
 #undef TRACE_UNIMPL
 #undef TRACE
 
diff --git a/src/compiler/backend/riscv64/instruction-codes-riscv64.h b/src/compiler/backend/riscv64/instruction-codes-riscv64.h
deleted file mode 100644
index b7f9202c4ee..00000000000
--- a/src/compiler/backend/riscv64/instruction-codes-riscv64.h
+++ /dev/null
@@ -1,432 +0,0 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
-// Use of this source code is governed by a BSD-style license that can be
-// found in the LICENSE file.
-
-#ifndef V8_COMPILER_BACKEND_RISCV64_INSTRUCTION_CODES_RISCV64_H_
-#define V8_COMPILER_BACKEND_RISCV64_INSTRUCTION_CODES_RISCV64_H_
-
-namespace v8 {
-namespace internal {
-namespace compiler {
-
-// RISC-V-specific opcodes that specify which assembly sequence to emit.
-// Most opcodes specify a single instruction.
-#define TARGET_ARCH_OPCODE_LIST(V)          \
-  V(RiscvAdd32)                             \
-  V(RiscvAdd64)                             \
-  V(RiscvAddOvf64)                          \
-  V(RiscvSub32)                             \
-  V(RiscvSub64)                             \
-  V(RiscvSubOvf64)                          \
-  V(RiscvMul32)                             \
-  V(RiscvMulOvf32)                          \
-  V(RiscvMulHigh32)                         \
-  V(RiscvMulHigh64)                         \
-  V(RiscvMulHighU32)                        \
-  V(RiscvMul64)                             \
-  V(RiscvDiv32)                             \
-  V(RiscvDiv64)                             \
-  V(RiscvDivU32)                            \
-  V(RiscvDivU64)                            \
-  V(RiscvMod32)                             \
-  V(RiscvMod64)                             \
-  V(RiscvModU32)                            \
-  V(RiscvModU64)                            \
-  V(RiscvAnd)                               \
-  V(RiscvAnd32)                             \
-  V(RiscvOr)                                \
-  V(RiscvOr32)                              \
-  V(RiscvNor)                               \
-  V(RiscvNor32)                             \
-  V(RiscvXor)                               \
-  V(RiscvXor32)                             \
-  V(RiscvClz32)                             \
-  V(RiscvShl32)                             \
-  V(RiscvShr32)                             \
-  V(RiscvSar32)                             \
-  V(RiscvZeroExtendWord)                    \
-  V(RiscvSignExtendWord)                    \
-  V(RiscvClz64)                             \
-  V(RiscvCtz32)                             \
-  V(RiscvCtz64)                             \
-  V(RiscvPopcnt32)                          \
-  V(RiscvPopcnt64)                          \
-  V(RiscvShl64)                             \
-  V(RiscvShr64)                             \
-  V(RiscvSar64)                             \
-  V(RiscvRor32)                             \
-  V(RiscvRor64)                             \
-  V(RiscvMov)                               \
-  V(RiscvTst)                               \
-  V(RiscvCmp)                               \
-  V(RiscvCmpZero)                           \
-  V(RiscvCmpS)                              \
-  V(RiscvAddS)                              \
-  V(RiscvSubS)                              \
-  V(RiscvMulS)                              \
-  V(RiscvDivS)                              \
-  V(RiscvModS)                              \
-  V(RiscvAbsS)                              \
-  V(RiscvNegS)                              \
-  V(RiscvSqrtS)                             \
-  V(RiscvMaxS)                              \
-  V(RiscvMinS)                              \
-  V(RiscvCmpD)                              \
-  V(RiscvAddD)                              \
-  V(RiscvSubD)                              \
-  V(RiscvMulD)                              \
-  V(RiscvDivD)                              \
-  V(RiscvModD)                              \
-  V(RiscvAbsD)                              \
-  V(RiscvNegD)                              \
-  V(RiscvSqrtD)                             \
-  V(RiscvMaxD)                              \
-  V(RiscvMinD)                              \
-  V(RiscvFloat64RoundDown)                  \
-  V(RiscvFloat64RoundTruncate)              \
-  V(RiscvFloat64RoundUp)                    \
-  V(RiscvFloat64RoundTiesEven)              \
-  V(RiscvFloat32RoundDown)                  \
-  V(RiscvFloat32RoundTruncate)              \
-  V(RiscvFloat32RoundUp)                    \
-  V(RiscvFloat32RoundTiesEven)              \
-  V(RiscvCvtSD)                             \
-  V(RiscvCvtDS)                             \
-  V(RiscvTruncWD)                           \
-  V(RiscvRoundWD)                           \
-  V(RiscvFloorWD)                           \
-  V(RiscvCeilWD)                            \
-  V(RiscvTruncWS)                           \
-  V(RiscvRoundWS)                           \
-  V(RiscvFloorWS)                           \
-  V(RiscvCeilWS)                            \
-  V(RiscvTruncLS)                           \
-  V(RiscvTruncLD)                           \
-  V(RiscvTruncUwD)                          \
-  V(RiscvTruncUwS)                          \
-  V(RiscvTruncUlS)                          \
-  V(RiscvTruncUlD)                          \
-  V(RiscvCvtDW)                             \
-  V(RiscvCvtSL)                             \
-  V(RiscvCvtSW)                             \
-  V(RiscvCvtSUw)                            \
-  V(RiscvCvtSUl)                            \
-  V(RiscvCvtDL)                             \
-  V(RiscvCvtDUw)                            \
-  V(RiscvCvtDUl)                            \
-  V(RiscvLb)                                \
-  V(RiscvLbu)                               \
-  V(RiscvSb)                                \
-  V(RiscvLh)                                \
-  V(RiscvUlh)                               \
-  V(RiscvLhu)                               \
-  V(RiscvUlhu)                              \
-  V(RiscvSh)                                \
-  V(RiscvUsh)                               \
-  V(RiscvLd)                                \
-  V(RiscvUld)                               \
-  V(RiscvLw)                                \
-  V(RiscvUlw)                               \
-  V(RiscvLwu)                               \
-  V(RiscvUlwu)                              \
-  V(RiscvSw)                                \
-  V(RiscvUsw)                               \
-  V(RiscvSd)                                \
-  V(RiscvUsd)                               \
-  V(RiscvLoadFloat)                         \
-  V(RiscvULoadFloat)                        \
-  V(RiscvStoreFloat)                        \
-  V(RiscvUStoreFloat)                       \
-  V(RiscvLoadDouble)                        \
-  V(RiscvULoadDouble)                       \
-  V(RiscvStoreDouble)                       \
-  V(RiscvUStoreDouble)                      \
-  V(RiscvBitcastDL)                         \
-  V(RiscvBitcastLD)                         \
-  V(RiscvBitcastInt32ToFloat32)             \
-  V(RiscvBitcastFloat32ToInt32)             \
-  V(RiscvFloat64ExtractLowWord32)           \
-  V(RiscvFloat64ExtractHighWord32)          \
-  V(RiscvFloat64InsertLowWord32)            \
-  V(RiscvFloat64InsertHighWord32)           \
-  V(RiscvFloat32Max)                        \
-  V(RiscvFloat64Max)                        \
-  V(RiscvFloat32Min)                        \
-  V(RiscvFloat64Min)                        \
-  V(RiscvFloat64SilenceNaN)                 \
-  V(RiscvPush)                              \
-  V(RiscvPeek)                              \
-  V(RiscvByteSwap64)                        \
-  V(RiscvByteSwap32)                        \
-  V(RiscvStoreToStackSlot)                  \
-  V(RiscvStackClaim)                        \
-  V(RiscvSignExtendByte)                    \
-  V(RiscvSignExtendShort)                   \
-  V(RiscvSync)                              \
-  V(RiscvAssertEqual)                       \
-  V(RiscvS128Const)                         \
-  V(RiscvS128Zero)                          \
-  V(RiscvS128AllOnes)                       \
-  V(RiscvI32x4Splat)                        \
-  V(RiscvI32x4ExtractLane)                  \
-  V(RiscvI32x4ReplaceLane)                  \
-  V(RiscvI32x4Add)                          \
-  V(RiscvI32x4Sub)                          \
-  V(RiscvF64x2Abs)                          \
-  V(RiscvF64x2Neg)                          \
-  V(RiscvF32x4Splat)                        \
-  V(RiscvF32x4ExtractLane)                  \
-  V(RiscvF32x4ReplaceLane)                  \
-  V(RiscvF32x4SConvertI32x4)                \
-  V(RiscvF32x4UConvertI32x4)                \
-  V(RiscvI64x2SConvertI32x4Low)             \
-  V(RiscvI64x2SConvertI32x4High)            \
-  V(RiscvI64x2UConvertI32x4Low)             \
-  V(RiscvI64x2UConvertI32x4High)            \
-  V(RiscvI32x4Mul)                          \
-  V(RiscvI32x4MaxS)                         \
-  V(RiscvI32x4MinS)                         \
-  V(RiscvI32x4Eq)                           \
-  V(RiscvI32x4Ne)                           \
-  V(RiscvI32x4Shl)                          \
-  V(RiscvI32x4ShrS)                         \
-  V(RiscvI32x4ShrU)                         \
-  V(RiscvI32x4MaxU)                         \
-  V(RiscvI32x4MinU)                         \
-  V(RiscvI64x2GtS)                          \
-  V(RiscvI64x2GeS)                          \
-  V(RiscvI64x2Eq)                           \
-  V(RiscvI64x2Ne)                           \
-  V(RiscvF64x2Sqrt)                         \
-  V(RiscvF64x2Add)                          \
-  V(RiscvF64x2Sub)                          \
-  V(RiscvF64x2Mul)                          \
-  V(RiscvF64x2Div)                          \
-  V(RiscvF64x2Min)                          \
-  V(RiscvF64x2Max)                          \
-  V(RiscvF64x2ConvertLowI32x4S)             \
-  V(RiscvF64x2ConvertLowI32x4U)             \
-  V(RiscvF64x2PromoteLowF32x4)              \
-  V(RiscvF64x2Eq)                           \
-  V(RiscvF64x2Ne)                           \
-  V(RiscvF64x2Lt)                           \
-  V(RiscvF64x2Le)                           \
-  V(RiscvF64x2Splat)                        \
-  V(RiscvF64x2ExtractLane)                  \
-  V(RiscvF64x2ReplaceLane)                  \
-  V(RiscvF64x2Pmin)                         \
-  V(RiscvF64x2Pmax)                         \
-  V(RiscvF64x2Ceil)                         \
-  V(RiscvF64x2Floor)                        \
-  V(RiscvF64x2Trunc)                        \
-  V(RiscvF64x2NearestInt)                   \
-  V(RiscvI64x2Splat)                        \
-  V(RiscvI64x2ExtractLane)                  \
-  V(RiscvI64x2ReplaceLane)                  \
-  V(RiscvI64x2Add)                          \
-  V(RiscvI64x2Sub)                          \
-  V(RiscvI64x2Mul)                          \
-  V(RiscvI64x2Abs)                          \
-  V(RiscvI64x2Neg)                          \
-  V(RiscvI64x2Shl)                          \
-  V(RiscvI64x2ShrS)                         \
-  V(RiscvI64x2ShrU)                         \
-  V(RiscvI64x2BitMask)                      \
-  V(RiscvF32x4Abs)                          \
-  V(RiscvF32x4Neg)                          \
-  V(RiscvF32x4Sqrt)                         \
-  V(RiscvF32x4Qfma)                         \
-  V(RiscvF32x4Qfms)                         \
-  V(RiscvF64x2Qfma)                         \
-  V(RiscvF64x2Qfms)                         \
-  V(RiscvF32x4Add)                          \
-  V(RiscvF32x4Sub)                          \
-  V(RiscvF32x4Mul)                          \
-  V(RiscvF32x4Div)                          \
-  V(RiscvF32x4Max)                          \
-  V(RiscvF32x4Min)                          \
-  V(RiscvF32x4Eq)                           \
-  V(RiscvF32x4Ne)                           \
-  V(RiscvF32x4Lt)                           \
-  V(RiscvF32x4Le)                           \
-  V(RiscvF32x4Pmin)                         \
-  V(RiscvF32x4Pmax)                         \
-  V(RiscvF32x4DemoteF64x2Zero)              \
-  V(RiscvF32x4Ceil)                         \
-  V(RiscvF32x4Floor)                        \
-  V(RiscvF32x4Trunc)                        \
-  V(RiscvF32x4NearestInt)                   \
-  V(RiscvI32x4SConvertF32x4)                \
-  V(RiscvI32x4UConvertF32x4)                \
-  V(RiscvI32x4Neg)                          \
-  V(RiscvI32x4GtS)                          \
-  V(RiscvI32x4GeS)                          \
-  V(RiscvI32x4GtU)                          \
-  V(RiscvI32x4GeU)                          \
-  V(RiscvI32x4Abs)                          \
-  V(RiscvI32x4BitMask)                      \
-  V(RiscvI32x4TruncSatF64x2SZero)           \
-  V(RiscvI32x4TruncSatF64x2UZero)           \
-  V(RiscvI16x8Splat)                        \
-  V(RiscvI16x8ExtractLaneU)                 \
-  V(RiscvI16x8ExtractLaneS)                 \
-  V(RiscvI16x8ReplaceLane)                  \
-  V(RiscvI16x8Neg)                          \
-  V(RiscvI16x8Shl)                          \
-  V(RiscvI16x8ShrS)                         \
-  V(RiscvI16x8ShrU)                         \
-  V(RiscvI16x8Add)                          \
-  V(RiscvI16x8AddSatS)                      \
-  V(RiscvI16x8Sub)                          \
-  V(RiscvI16x8SubSatS)                      \
-  V(RiscvI16x8Mul)                          \
-  V(RiscvI16x8MaxS)                         \
-  V(RiscvI16x8MinS)                         \
-  V(RiscvI16x8Eq)                           \
-  V(RiscvI16x8Ne)                           \
-  V(RiscvI16x8GtS)                          \
-  V(RiscvI16x8GeS)                          \
-  V(RiscvI16x8AddSatU)                      \
-  V(RiscvI16x8SubSatU)                      \
-  V(RiscvI16x8MaxU)                         \
-  V(RiscvI16x8MinU)                         \
-  V(RiscvI16x8GtU)                          \
-  V(RiscvI16x8GeU)                          \
-  V(RiscvI16x8RoundingAverageU)             \
-  V(RiscvI16x8Q15MulRSatS)                  \
-  V(RiscvI16x8Abs)                          \
-  V(RiscvI16x8BitMask)                      \
-  V(RiscvI8x16Splat)                        \
-  V(RiscvI8x16ExtractLaneU)                 \
-  V(RiscvI8x16ExtractLaneS)                 \
-  V(RiscvI8x16ReplaceLane)                  \
-  V(RiscvI8x16Neg)                          \
-  V(RiscvI8x16Shl)                          \
-  V(RiscvI8x16ShrS)                         \
-  V(RiscvI8x16Add)                          \
-  V(RiscvI8x16AddSatS)                      \
-  V(RiscvI8x16Sub)                          \
-  V(RiscvI8x16SubSatS)                      \
-  V(RiscvI8x16MaxS)                         \
-  V(RiscvI8x16MinS)                         \
-  V(RiscvI8x16Eq)                           \
-  V(RiscvI8x16Ne)                           \
-  V(RiscvI8x16GtS)                          \
-  V(RiscvI8x16GeS)                          \
-  V(RiscvI8x16ShrU)                         \
-  V(RiscvI8x16AddSatU)                      \
-  V(RiscvI8x16SubSatU)                      \
-  V(RiscvI8x16MaxU)                         \
-  V(RiscvI8x16MinU)                         \
-  V(RiscvI8x16GtU)                          \
-  V(RiscvI8x16GeU)                          \
-  V(RiscvI8x16RoundingAverageU)             \
-  V(RiscvI8x16Abs)                          \
-  V(RiscvI8x16BitMask)                      \
-  V(RiscvI8x16Popcnt)                       \
-  V(RiscvS128And)                           \
-  V(RiscvS128Or)                            \
-  V(RiscvS128Xor)                           \
-  V(RiscvS128Not)                           \
-  V(RiscvS128Select)                        \
-  V(RiscvS128AndNot)                        \
-  V(RiscvS128Load64Zero)                    \
-  V(RiscvS128Load32Zero)                    \
-  V(RiscvI32x4AllTrue)                      \
-  V(RiscvI16x8AllTrue)                      \
-  V(RiscvV128AnyTrue)                       \
-  V(RiscvI8x16AllTrue)                      \
-  V(RiscvI64x2AllTrue)                      \
-  V(RiscvS32x4InterleaveRight)              \
-  V(RiscvS32x4InterleaveLeft)               \
-  V(RiscvS32x4PackEven)                     \
-  V(RiscvS32x4PackOdd)                      \
-  V(RiscvS32x4InterleaveEven)               \
-  V(RiscvS32x4InterleaveOdd)                \
-  V(RiscvS32x4Shuffle)                      \
-  V(RiscvS16x8InterleaveRight)              \
-  V(RiscvS16x8InterleaveLeft)               \
-  V(RiscvS16x8PackEven)                     \
-  V(RiscvS16x8PackOdd)                      \
-  V(RiscvS16x8InterleaveEven)               \
-  V(RiscvS16x8InterleaveOdd)                \
-  V(RiscvS16x4Reverse)                      \
-  V(RiscvS16x2Reverse)                      \
-  V(RiscvS8x16InterleaveRight)              \
-  V(RiscvS8x16InterleaveLeft)               \
-  V(RiscvS8x16PackEven)                     \
-  V(RiscvS8x16PackOdd)                      \
-  V(RiscvS8x16InterleaveEven)               \
-  V(RiscvS8x16InterleaveOdd)                \
-  V(RiscvI8x16Shuffle)                      \
-  V(RiscvS8x16Concat)                       \
-  V(RiscvS8x8Reverse)                       \
-  V(RiscvS8x4Reverse)                       \
-  V(RiscvS8x2Reverse)                       \
-  V(RiscvS128LoadSplat)                     \
-  V(RiscvS128Load64ExtendS)                 \
-  V(RiscvS128Load64ExtendU)                 \
-  V(RiscvS128LoadLane)                      \
-  V(RiscvS128StoreLane)                     \
-  V(RiscvRvvLd)                             \
-  V(RiscvRvvSt)                             \
-  V(RiscvI32x4SConvertI16x8Low)             \
-  V(RiscvI32x4SConvertI16x8High)            \
-  V(RiscvI32x4UConvertI16x8Low)             \
-  V(RiscvI32x4UConvertI16x8High)            \
-  V(RiscvI16x8SConvertI8x16Low)             \
-  V(RiscvI16x8SConvertI8x16High)            \
-  V(RiscvI16x8SConvertI32x4)                \
-  V(RiscvI16x8UConvertI32x4)                \
-  V(RiscvI16x8UConvertI8x16Low)             \
-  V(RiscvI16x8UConvertI8x16High)            \
-  V(RiscvI8x16SConvertI16x8)                \
-  V(RiscvI8x16UConvertI16x8)                \
-  V(RiscvVwmul)                             \
-  V(RiscvVwmulu)                            \
-  V(RiscvVmvSx)                             \
-  V(RiscvVcompress)                         \
-  V(RiscvVaddVv)                            \
-  V(RiscvVwadd)                             \
-  V(RiscvVwaddu)                            \
-  V(RiscvVrgather)                          \
-  V(RiscvVslidedown)                        \
-  V(RiscvWord64AtomicLoadUint64)            \
-  V(RiscvWord64AtomicStoreWord64)           \
-  V(RiscvWord64AtomicAddUint64)             \
-  V(RiscvWord64AtomicSubUint64)             \
-  V(RiscvWord64AtomicAndUint64)             \
-  V(RiscvWord64AtomicOrUint64)              \
-  V(RiscvWord64AtomicXorUint64)             \
-  V(RiscvWord64AtomicExchangeUint64)        \
-  V(RiscvWord64AtomicCompareExchangeUint64) \
-  V(RiscvStoreCompressTagged)               \
-  V(RiscvLoadDecompressTaggedSigned)        \
-  V(RiscvLoadDecompressTaggedPointer)       \
-  V(RiscvLoadDecompressAnyTagged)
-
-// Addressing modes represent the "shape" of inputs to an instruction.
-// Many instructions support multiple addressing modes. Addressing modes
-// are encoded into the InstructionCode of the instruction and tell the
-// code generator after register allocation which assembler method to call.
-//
-// We use the following local notation for addressing modes:
-//
-// R = register
-// O = register or stack slot
-// D = double register
-// I = immediate (handle, external, int32)
-// MRI = [register + immediate]
-// MRR = [register + register]
-// Root = [kRootregister + immediate]
-// TODO(plind): Add the new r6 address modes.
-#define TARGET_ADDRESSING_MODE_LIST(V) \
-  V(MRI)  /* [%r0 + K] */              \
-  V(MRR)  /* [%r0 + %r1] */            \
-  V(Root) /* [root + k] */
-
-}  // namespace compiler
-}  // namespace internal
-}  // namespace v8
-
-#endif  // V8_COMPILER_BACKEND_RISCV64_INSTRUCTION_CODES_RISCV64_H_
diff --git a/src/compiler/c-linkage.cc b/src/compiler/c-linkage.cc
index f51fd0adca9..9e34b26a68a 100644
--- a/src/compiler/c-linkage.cc
+++ b/src/compiler/c-linkage.cc
@@ -121,7 +121,7 @@ namespace {
 #define CALLEE_SAVE_REGISTERS r6, r7, r8, r9, r10, ip, r13
 #define CALLEE_SAVE_FP_REGISTERS d8, d9, d10, d11, d12, d13, d14, d15
 
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 // ===========================================================================
 // == riscv64 =================================================================
 // ===========================================================================
diff --git a/src/deoptimizer/riscv64/deoptimizer-riscv64.cc b/src/deoptimizer/riscv/deoptimizer-riscv.cc
similarity index 100%
rename from src/deoptimizer/riscv64/deoptimizer-riscv64.cc
rename to src/deoptimizer/riscv/deoptimizer-riscv.cc
diff --git a/src/diagnostics/perf-jit.h b/src/diagnostics/perf-jit.h
index 31a9287d39a..0ee2a53a22d 100644
--- a/src/diagnostics/perf-jit.h
+++ b/src/diagnostics/perf-jit.h
@@ -112,7 +112,7 @@ class LinuxPerfJitLogger : public CodeEventLogger {
     return kElfMachS390x;
 #elif V8_TARGET_ARCH_PPC64
     return kElfMachPPC64;
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
     return kElfMachRISCV;
 #else
     UNIMPLEMENTED();
diff --git a/src/diagnostics/riscv64/disasm-riscv64.cc b/src/diagnostics/riscv/disasm-riscv.cc
similarity index 99%
rename from src/diagnostics/riscv64/disasm-riscv64.cc
rename to src/diagnostics/riscv/disasm-riscv.cc
index 46d44433c53..1119a380143 100644
--- a/src/diagnostics/riscv64/disasm-riscv64.cc
+++ b/src/diagnostics/riscv/disasm-riscv.cc
@@ -27,13 +27,11 @@
 #include <stdio.h>
 #include <string.h>
 
-#if V8_TARGET_ARCH_RISCV64
-
 #include "src/base/platform/platform.h"
 #include "src/base/strings.h"
 #include "src/base/vector.h"
+#include "src/codegen/constants-arch.h"
 #include "src/codegen/macro-assembler.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
 #include "src/diagnostics/disasm.h"
 
 namespace v8 {
@@ -1708,10 +1706,10 @@ void Decoder::DecodeBType(Instruction* instr) {
 void Decoder::DecodeUType(Instruction* instr) {
   // U Type doesn't have additional mask
   switch (instr->BaseOpcodeFieldRaw()) {
-    case RO_LUI:
+    case LUI:
       Format(instr, "lui       'rd, 'imm20U");
       break;
-    case RO_AUIPC:
+    case AUIPC:
       Format(instr, "auipc     'rd, 'imm20U");
       break;
     default:
@@ -1722,7 +1720,7 @@ void Decoder::DecodeUType(Instruction* instr) {
 void Decoder::DecodeJType(Instruction* instr) {
   // J Type doesn't have additional mask
   switch (instr->BaseOpcodeValue()) {
-    case RO_JAL:
+    case JAL:
       if (instr->RdValue() == zero_reg.code())
         Format(instr, "j         'imm20J");
       else if (instr->RdValue() == ra.code())
@@ -1774,12 +1772,14 @@ void Decoder::DecodeCAType(Instruction* instr) {
     case RO_C_AND:
       Format(instr, "and       'Crs1s, 'Crs1s, 'Crs2s");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_SUBW:
       Format(instr, "subw       'Crs1s, 'Crs1s, 'Crs2s");
       break;
     case RO_C_ADDW:
       Format(instr, "addw       'Crs1s, 'Crs1s, 'Crs2s");
       break;
+#endif
     default:
       UNSUPPORTED_RISCV();
   }
@@ -1793,9 +1793,11 @@ void Decoder::DecodeCIType(Instruction* instr) {
       else
         Format(instr, "addi      'Crd, 'Crd, 'Cimm6");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_ADDIW:
       Format(instr, "addiw     'Crd, 'Crd, 'Cimm6");
       break;
+#endif
     case RO_C_LI:
       Format(instr, "li        'Crd, 'Cimm6");
       break;
@@ -1816,9 +1818,15 @@ void Decoder::DecodeCIType(Instruction* instr) {
     case RO_C_LWSP:
       Format(instr, "lw        'Crd, 'Cimm6Lwsp(sp)");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_LDSP:
       Format(instr, "ld        'Crd, 'Cimm6Ldsp(sp)");
       break;
+#elif defined(V8_TARGET_ARCH_32_BIT)
+    case RO_C_FLWSP:
+      Format(instr, "flw       'Cfd, 'Cimm6Ldsp(sp)");
+      break;
+#endif
     default:
       UNSUPPORTED_RISCV();
   }
@@ -1839,9 +1847,15 @@ void Decoder::DecodeCSSType(Instruction* instr) {
     case RO_C_SWSP:
       Format(instr, "sw        'Crs2, 'Cimm6Swsp(sp)");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_SDSP:
       Format(instr, "sd        'Crs2, 'Cimm6Sdsp(sp)");
       break;
+#elif defined(V8_TARGET_ARCH_32_BIT)
+    case RO_C_FSWSP:
+      Format(instr, "fsw       'Cfs2, 'Cimm6Sdsp(sp)");
+      break;
+#endif
     case RO_C_FSDSP:
       Format(instr, "fsd       'Cfs2, 'Cimm6Sdsp(sp)");
       break;
@@ -1858,9 +1872,16 @@ void Decoder::DecodeCLType(Instruction* instr) {
     case RO_C_LW:
       Format(instr, "lw       'Crs2s, 'Cimm5W('Crs1s)");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_LD:
       Format(instr, "ld       'Crs2s, 'Cimm5D('Crs1s)");
       break;
+#elif defined(V8_TARGET_ARCH_32_BIT)
+    case RO_C_FLW:
+      Format(instr, "fld       'Cfs2s, 'Cimm5D('Crs1s)");
+      break;
+#endif
+
     default:
       UNSUPPORTED_RISCV();
   }
@@ -1874,9 +1895,15 @@ void Decoder::DecodeCSType(Instruction* instr) {
     case RO_C_SW:
       Format(instr, "sw       'Crs2s, 'Cimm5W('Crs1s)");
       break;
+#ifdef V8_TARGET_ARCH_64_BIT
     case RO_C_SD:
       Format(instr, "sd       'Crs2s, 'Cimm5D('Crs1s)");
       break;
+#elif defined(V8_TARGET_ARCH_32_BIT)
+    case RO_C_FSW:
+      Format(instr, "fsw       'Cfs2s, 'Cimm5D('Crs1s)");
+      break;
+#endif
     default:
       UNSUPPORTED_RISCV();
   }
@@ -2954,5 +2981,3 @@ void Disassembler::Disassemble(FILE* f, byte* begin, byte* end,
 #undef STRING_STARTS_WITH
 
 }  // namespace disasm
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/diagnostics/riscv64/unwinder-riscv64.cc b/src/diagnostics/riscv/unwinder-riscv.cc
similarity index 100%
rename from src/diagnostics/riscv64/unwinder-riscv64.cc
rename to src/diagnostics/riscv/unwinder-riscv.cc
diff --git a/src/execution/frame-constants.h b/src/execution/frame-constants.h
index 968cfef60e0..1fcc9df8d22 100644
--- a/src/execution/frame-constants.h
+++ b/src/execution/frame-constants.h
@@ -424,8 +424,8 @@ inline static int FrameSlotToFPOffset(int slot) {
 #include "src/execution/loong64/frame-constants-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/execution/s390/frame-constants-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/execution/riscv64/frame-constants-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/execution/riscv/frame-constants-riscv.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 8719d3af673..e90454873b3 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -184,7 +184,7 @@ class StackFrame {
     // invalid frame markers.
 #if (defined(USE_SIMULATOR) &&                        \
      (V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_ARM)) || \
-    V8_TARGET_ARCH_RISCV64
+    (V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64)
     if (static_cast<uintptr_t>(type) >= Type::NUMBER_OF_TYPES) {
       // Appease UBSan.
       return Type::NUMBER_OF_TYPES;
diff --git a/src/execution/riscv64/frame-constants-riscv64.cc b/src/execution/riscv/frame-constants-riscv.cc
similarity index 81%
rename from src/execution/riscv64/frame-constants-riscv64.cc
rename to src/execution/riscv/frame-constants-riscv.cc
index 13e91639c98..833af91e7e6 100644
--- a/src/execution/riscv64/frame-constants-riscv64.cc
+++ b/src/execution/riscv/frame-constants-riscv.cc
@@ -2,11 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#if V8_TARGET_ARCH_RISCV64
+#include "src/execution/riscv/frame-constants-riscv.h"
 
-#include "src/execution/riscv64/frame-constants-riscv64.h"
-
-#include "src/codegen/riscv64/assembler-riscv64-inl.h"
 #include "src/execution/frame-constants.h"
 #include "src/execution/frames.h"
 
@@ -28,5 +25,3 @@ int BuiltinContinuationFrameConstants::PaddingSlotCount(int register_count) {
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/execution/riscv64/frame-constants-riscv64.h b/src/execution/riscv/frame-constants-riscv.h
similarity index 94%
rename from src/execution/riscv64/frame-constants-riscv64.h
rename to src/execution/riscv/frame-constants-riscv.h
index 6fa4d50a9d6..963efeec592 100644
--- a/src/execution/riscv64/frame-constants-riscv64.h
+++ b/src/execution/riscv/frame-constants-riscv.h
@@ -2,8 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_EXECUTION_RISCV64_FRAME_CONSTANTS_RISCV64_H_
-#define V8_EXECUTION_RISCV64_FRAME_CONSTANTS_RISCV64_H_
+#ifndef V8_EXECUTION_RISCV_FRAME_CONSTANTS_RISCV_H_
+#define V8_EXECUTION_RISCV_FRAME_CONSTANTS_RISCV_H_
 
 #include "src/base/bits.h"
 #include "src/base/macros.h"
@@ -82,4 +82,4 @@ class WasmDebugBreakFrameConstants : public TypedFrameConstants {
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_EXECUTION_RISCV64_FRAME_CONSTANTS_RISCV64_H_
+#endif  // V8_EXECUTION_RISCV_FRAME_CONSTANTS_RISCV_H_
diff --git a/src/execution/riscv64/simulator-riscv64.cc b/src/execution/riscv/simulator-riscv.cc
similarity index 91%
rename from src/execution/riscv64/simulator-riscv64.cc
rename to src/execution/riscv/simulator-riscv.cc
index 73204fee47b..1dcb1a8ddc9 100644
--- a/src/execution/riscv64/simulator-riscv64.cc
+++ b/src/execution/riscv/simulator-riscv.cc
@@ -40,7 +40,7 @@
 // The original source code covered by the above license above has been
 // modified significantly by the v8 project authors.
 
-#include "src/execution/riscv64/simulator-riscv64.h"
+#include "src/execution/riscv/simulator-riscv.h"
 
 // Only build the simulator if not compiling for real RISCV hardware.
 #if defined(USE_SIMULATOR)
@@ -54,14 +54,22 @@
 #include "src/base/overflowing-math.h"
 #include "src/base/vector.h"
 #include "src/codegen/assembler-inl.h"
+#include "src/codegen/constants-arch.h"
 #include "src/codegen/macro-assembler.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
 #include "src/diagnostics/disasm.h"
 #include "src/heap/combined-heap.h"
 #include "src/runtime/runtime-utils.h"
 #include "src/utils/ostreams.h"
 #include "src/utils/utils.h"
 
+#if V8_TARGET_ARCH_RISCV64
+#define REGIx_FORMAT PRIx64
+#define REGId_FORMAT PRId64
+#elif V8_TARGET_ARCH_RISCV32
+#define REGIx_FORMAT PRIx32
+#define REGId_FORMAT PRId32
+#endif
+
 // The following code about RVV was based from:
 //   https://github.com/riscv/riscv-isa-sim
 // Copyright (c) 2010-2017, The Regents of the University of California
@@ -1176,7 +1184,9 @@ struct type_sew_t<128> {
   set_rvv_vstart(0);                                                           \
   if (::v8::internal::FLAG_trace_sim) {                                        \
     __int128_t value = Vregister_[rvv_vd_reg()];                               \
-    SNPrintF(trace_buf_, "%016" PRIx64 "%016" PRIx64 " <-- 0x%016" PRIx64,     \
+    SNPrintF(trace_buf_,                                                       \
+             "%016" REGIx_FORMAT "%016" REGIx_FORMAT                           \
+             " <-- 0x%016" REGIx_FORMAT,                                       \
              *(reinterpret_cast<int64_t*>(&value) + 1),                        \
              *reinterpret_cast<int64_t*>(&value),                              \
              (uint64_t)(get_register(rs1_reg())));                             \
@@ -1200,7 +1210,9 @@ struct type_sew_t<128> {
   set_rvv_vstart(0);                                                           \
   if (::v8::internal::FLAG_trace_sim) {                                        \
     __int128_t value = Vregister_[rvv_vd_reg()];                               \
-    SNPrintF(trace_buf_, "%016" PRIx64 "%016" PRIx64 " --> 0x%016" PRIx64,     \
+    SNPrintF(trace_buf_,                                                       \
+             "%016" REGIx_FORMAT "%016" REGIx_FORMAT                           \
+             " --> 0x%016" REGIx_FORMAT,                                       \
              *(reinterpret_cast<int64_t*>(&value) + 1),                        \
              *reinterpret_cast<int64_t*>(&value),                              \
              (uint64_t)(get_register(rs1_reg())));                             \
@@ -1273,14 +1285,14 @@ static inline uint8_t get_round(int vxrm, uint64_t v, uint8_t shift) {
 
 template <typename Src, typename Dst>
 inline Dst signed_saturation(Src v, uint n) {
-  Dst smax = (Dst)(INT64_MAX >> (64 - n));
-  Dst smin = (Dst)(INT64_MIN >> (64 - n));
+  Dst smax = (Dst)(INTPTR_MAX >> (64 - n));
+  Dst smin = (Dst)(INTPTR_MIN >> (64 - n));
   return (v > smax) ? smax : ((v < smin) ? smin : (Dst)v);
 }
 
 template <typename Src, typename Dst>
 inline Dst unsigned_saturation(Src v, uint n) {
-  Dst umax = (Dst)(UINT64_MAX >> (64 - n));
+  Dst umax = (Dst)(UINTPTR_MAX >> (64 - n));
   return (v > umax) ? umax : ((v < 0) ? 0 : (Dst)v);
 }
 
@@ -1457,7 +1469,7 @@ static inline int32_t get_ebreak_code(Instruction* instr) {
   DCHECK(instr->InstructionBits() == kBreakInstr);
   byte* cur = reinterpret_cast<byte*>(instr);
   Instruction* next_instr = reinterpret_cast<Instruction*>(cur + kInstrSize);
-  if (next_instr->BaseOpcodeFieldRaw() == RO_LUI)
+  if (next_instr->BaseOpcodeFieldRaw() == LUI)
     return (next_instr->Imm20UValue());
   else
     return -1;
@@ -1486,22 +1498,27 @@ class RiscvDebugger {
  private:
   Simulator* sim_;
 
-  int64_t GetRegisterValue(int regnum);
+  sreg_t GetRegisterValue(int regnum);
   int64_t GetFPURegisterValue(int regnum);
   float GetFPURegisterValueFloat(int regnum);
   double GetFPURegisterValueDouble(int regnum);
 #ifdef CAN_USE_RVV_INSTRUCTIONS
   __int128_t GetVRegisterValue(int regnum);
 #endif
-  bool GetValue(const char* desc, int64_t* value);
+  bool GetValue(const char* desc, sreg_t* value);
 };
 
-#define UNSUPPORTED()                                                     \
-  printf("Sim: Unsupported instruction. Func:%s Line:%d\n", __FUNCTION__, \
-         __LINE__);                                                       \
+#define UNSUPPORTED()                                                  \
+  v8::base::EmbeddedVector<char, 256> buffer;                          \
+  disasm::NameConverter converter;                                     \
+  disasm::Disassembler dasm(converter);                                \
+  dasm.InstructionDecode(buffer, reinterpret_cast<byte*>(&instr_));    \
+  printf("Sim: Unsupported inst. Func:%s Line:%d PC:0x%" REGIx_FORMAT, \
+         __FUNCTION__, __LINE__, get_pc());                            \
+  PrintF(" %-44s\n", buffer.begin());                                  \
   base::OS::Abort();
 
-int64_t RiscvDebugger::GetRegisterValue(int regnum) {
+sreg_t RiscvDebugger::GetRegisterValue(int regnum) {
   if (regnum == kNumSimuRegisters) {
     return sim_->get_pc();
   } else {
@@ -1543,7 +1560,7 @@ __int128_t RiscvDebugger::GetVRegisterValue(int regnum) {
 }
 #endif
 
-bool RiscvDebugger::GetValue(const char* desc, int64_t* value) {
+bool RiscvDebugger::GetValue(const char* desc, sreg_t* value) {
   int regnum = Registers::Number(desc);
   int fpuregnum = FPURegisters::Number(desc);
 
@@ -1554,10 +1571,17 @@ bool RiscvDebugger::GetValue(const char* desc, int64_t* value) {
     *value = GetFPURegisterValue(fpuregnum);
     return true;
   } else if (strncmp(desc, "0x", 2) == 0) {
-    return SScanF(desc + 2, "%" SCNx64, reinterpret_cast<uint64_t*>(value)) ==
-           1;
+#if V8_TARGET_ARCH_RISCV64
+    return SScanF(desc + 2, "%" SCNx64, reinterpret_cast<reg_t*>(value)) == 1;
+#elif V8_TARGET_ARCH_RISCV32
+    return SScanF(desc + 2, "%" SCNx32, reinterpret_cast<reg_t*>(value)) == 1;
+#endif
   } else {
-    return SScanF(desc, "%" SCNu64, reinterpret_cast<uint64_t*>(value)) == 1;
+#if V8_TARGET_ARCH_RISCV64
+    return SScanF(desc, "%" SCNu64, reinterpret_cast<reg_t*>(value)) == 1;
+#elif V8_TARGET_ARCH_RISCV32
+    return SScanF(desc, "%" SCNu32, reinterpret_cast<reg_t*>(value)) == 1;
+#endif
   }
 }
 
@@ -1574,26 +1598,29 @@ void RiscvDebugger::PrintRegs(char name_prefix, int start_index,
   for (int i = 0; i < num_registers / 2; i++) {
     SNPrintF(name1, "%c%d", name_prefix, start_index + 2 * i);
     SNPrintF(name2, "%c%d", name_prefix, start_index + 2 * i + 1);
-    PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \t%3s: 0x%016" PRIx64
-           "  %14" PRId64 " \n",
+    PrintF("%3s: 0x%016" REGIx_FORMAT "  %14" REGId_FORMAT
+           " \t%3s: 0x%016" REGIx_FORMAT "  %14" REGId_FORMAT " \n",
            REG_INFO(name1.begin()), REG_INFO(name2.begin()));
   }
   if (num_registers % 2 == 1) {
     SNPrintF(name1, "%c%d", name_prefix, end_index);
-    PrintF("%3s: 0x%016" PRIx64 "  %14" PRId64 " \n", REG_INFO(name1.begin()));
+    PrintF("%3s: 0x%016" REGIx_FORMAT "  %14" REGId_FORMAT " \n",
+           REG_INFO(name1.begin()));
   }
 }
 
 void RiscvDebugger::PrintAllRegs() {
   PrintF("\n");
   // ra, sp, gp
-  PrintF("%3s: 0x%016" PRIx64 " %14" PRId64 "\t%3s: 0x%016" PRIx64 " %14" PRId64
-         "\t%3s: 0x%016" PRIx64 " %14" PRId64 "\n",
+  PrintF("%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT
+         "\t%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT
+         "\t%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT "\n",
          REG_INFO("ra"), REG_INFO("sp"), REG_INFO("gp"));
 
   // tp, fp, pc
-  PrintF("%3s: 0x%016" PRIx64 " %14" PRId64 "\t%3s: 0x%016" PRIx64 " %14" PRId64
-         "\t%3s: 0x%016" PRIx64 " %14" PRId64 "\n",
+  PrintF("%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT
+         "\t%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT
+         "\t%3s: 0x%016" REGIx_FORMAT " %14" REGId_FORMAT "\n",
          REG_INFO("tp"), REG_INFO("fp"), REG_INFO("pc"));
 
   // print register a0, .., a7
@@ -1652,7 +1679,7 @@ void RiscvDebugger::Debug() {
         PrintF("Call builtin:  %s\n", name);
       }
       dasm.InstructionDecode(buffer, reinterpret_cast<byte*>(sim_->get_pc()));
-      PrintF("  0x%016" PRIx64 "   %s\n", sim_->get_pc(), buffer.begin());
+      PrintF("  0x%016" REGIx_FORMAT "   %s\n", sim_->get_pc(), buffer.begin());
       last_pc = sim_->get_pc();
     }
     char* line = ReadLine("sim> ");
@@ -1678,6 +1705,7 @@ void RiscvDebugger::Debug() {
         Instruction* instr = reinterpret_cast<Instruction*>(sim_->get_pc());
         if (!(instr->IsTrap()) ||
             instr->InstructionBits() == rtCallRedirInstr) {
+          sim_->icount_++;
           sim_->InstructionDecode(
               reinterpret_cast<Instruction*>(sim_->get_pc()));
         } else {
@@ -1692,7 +1720,8 @@ void RiscvDebugger::Debug() {
         done = true;
       } else if ((strcmp(cmd, "p") == 0) || (strcmp(cmd, "print") == 0)) {
         if (argc == 2) {
-          int64_t value;
+          sreg_t value;
+          int64_t fvalue;
           double dvalue;
           if (strcmp(arg1, "all") == 0) {
             PrintAllRegs();
@@ -1706,17 +1735,17 @@ void RiscvDebugger::Debug() {
 #endif
             if (regnum != kInvalidRegister) {
               value = GetRegisterValue(regnum);
-              PrintF("%s: 0x%08" PRIx64 "  %" PRId64 "  \n", arg1, value,
-                     value);
+              PrintF("%s: 0x%08" REGIx_FORMAT "  %" REGId_FORMAT "  \n", arg1,
+                     value, value);
             } else if (fpuregnum != kInvalidFPURegister) {
-              value = GetFPURegisterValue(fpuregnum);
+              fvalue = GetFPURegisterValue(fpuregnum);
               dvalue = GetFPURegisterValueDouble(fpuregnum);
               PrintF("%3s: 0x%016" PRIx64 "  %16.4e\n",
-                     FPURegisters::Name(fpuregnum), value, dvalue);
+                     FPURegisters::Name(fpuregnum), fvalue, dvalue);
 #ifdef CAN_USE_RVV_INSTRUCTIONS
             } else if (vregnum != kInvalidVRegister) {
               __int128_t v = GetVRegisterValue(vregnum);
-              PrintF("\t%s:0x%016" PRIx64 "%016" PRIx64 "\n",
+              PrintF("\t%s:0x%016" REGIx_FORMAT "%016" REGIx_FORMAT "\n",
                      VRegisters::Name(vregnum), (uint64_t)(v >> 64),
                      (uint64_t)v);
 #endif
@@ -1749,7 +1778,7 @@ void RiscvDebugger::Debug() {
       } else if ((strcmp(cmd, "po") == 0) ||
                  (strcmp(cmd, "printobject") == 0)) {
         if (argc == 2) {
-          int64_t value;
+          sreg_t value;
           StdoutStream os;
           if (GetValue(arg1, &value)) {
             Object obj(value);
@@ -1767,27 +1796,27 @@ void RiscvDebugger::Debug() {
           PrintF("printobject <value>\n");
         }
       } else if (strcmp(cmd, "stack") == 0 || strcmp(cmd, "mem") == 0) {
-        int64_t* cur = nullptr;
-        int64_t* end = nullptr;
+        sreg_t* cur = nullptr;
+        sreg_t* end = nullptr;
         int next_arg = 1;
 
         if (strcmp(cmd, "stack") == 0) {
-          cur = reinterpret_cast<int64_t*>(sim_->get_register(Simulator::sp));
+          cur = reinterpret_cast<sreg_t*>(sim_->get_register(Simulator::sp));
         } else {  // Command "mem".
           if (argc < 2) {
             PrintF("Need to specify <address> to mem command\n");
             continue;
           }
-          int64_t value;
+          sreg_t value;
           if (!GetValue(arg1, &value)) {
             PrintF("%s unrecognized\n", arg1);
             continue;
           }
-          cur = reinterpret_cast<int64_t*>(value);
+          cur = reinterpret_cast<sreg_t*>(value);
           next_arg++;
         }
 
-        int64_t words;
+        sreg_t words;
         if (argc == next_arg) {
           words = 10;
         } else {
@@ -1798,7 +1827,8 @@ void RiscvDebugger::Debug() {
         end = cur + words;
 
         while (cur < end) {
-          PrintF("  0x%012" PRIxPTR " :  0x%016" PRIx64 "  %14" PRId64 " ",
+          PrintF("  0x%012" PRIxPTR " :  0x%016" REGIx_FORMAT
+                 "  %14" REGId_FORMAT " ",
                  reinterpret_cast<intptr_t>(cur), *cur, *cur);
           Object obj(*cur);
           Heap* current_heap = sim_->isolate_->heap();
@@ -1815,7 +1845,18 @@ void RiscvDebugger::Debug() {
           PrintF("\n");
           cur++;
         }
-
+      } else if ((strcmp(cmd, "watch") == 0)) {
+        if (argc < 2) {
+          PrintF("Need to specify <address> to mem command\n");
+          continue;
+        }
+        sreg_t value;
+        if (!GetValue(arg1, &value)) {
+          PrintF("%s unrecognized\n", arg1);
+          continue;
+        }
+        sim_->watch_address_ = reinterpret_cast<sreg_t*>(value);
+        sim_->watch_value_ = *(sim_->watch_address_);
       } else if ((strcmp(cmd, "disasm") == 0) || (strcmp(cmd, "dpc") == 0) ||
                  (strcmp(cmd, "di") == 0)) {
         disasm::NameConverter converter;
@@ -1833,7 +1874,7 @@ void RiscvDebugger::Debug() {
           int regnum = Registers::Number(arg1);
           if (regnum != kInvalidRegister || strncmp(arg1, "0x", 2) == 0) {
             // The argument is an address or a register name.
-            int64_t value;
+            sreg_t value;
             if (GetValue(arg1, &value)) {
               cur = reinterpret_cast<byte*>(value);
               // Disassemble 10 instructions at <arg1>.
@@ -1841,7 +1882,7 @@ void RiscvDebugger::Debug() {
             }
           } else {
             // The argument is the number of instructions.
-            int64_t value;
+            sreg_t value;
             if (GetValue(arg1, &value)) {
               cur = reinterpret_cast<byte*>(sim_->get_pc());
               // Disassemble <arg1> instructions.
@@ -1849,8 +1890,8 @@ void RiscvDebugger::Debug() {
             }
           }
         } else {
-          int64_t value1;
-          int64_t value2;
+          sreg_t value1;
+          sreg_t value2;
           if (GetValue(arg1, &value1) && GetValue(arg2, &value2)) {
             cur = reinterpret_cast<byte*>(value1);
             end = cur + (value2 * kInstrSize);
@@ -1867,11 +1908,14 @@ void RiscvDebugger::Debug() {
         PrintF("relinquishing control to gdb\n");
         v8::base::OS::DebugBreak();
         PrintF("regaining control from gdb\n");
+      } else if (strcmp(cmd, "trace") == 0) {
+        PrintF("enable trace sim\n");
+        FLAG_trace_sim = true;
       } else if (strcmp(cmd, "break") == 0 || strcmp(cmd, "b") == 0 ||
                  strcmp(cmd, "tbreak") == 0) {
         bool is_tbreak = strcmp(cmd, "tbreak") == 0;
         if (argc == 2) {
-          int64_t value;
+          sreg_t value;
           if (GetValue(arg1, &value)) {
             sim_->SetBreakpoint(reinterpret_cast<Instruction*>(value),
                                 is_tbreak);
@@ -1888,7 +1932,7 @@ void RiscvDebugger::Debug() {
       } else if (strcmp(cmd, "flags") == 0) {
         PrintF("No flags on RISC-V !\n");
       } else if (strcmp(cmd, "stop") == 0) {
-        int64_t value;
+        sreg_t value;
         if (argc == 3) {
           // Print information about all/the specified breakpoint(s).
           if (strcmp(arg1, "info") == 0) {
@@ -1948,15 +1992,15 @@ void RiscvDebugger::Debug() {
           cur = reinterpret_cast<byte*>(sim_->get_pc());
           end = cur + (10 * kInstrSize);
         } else if (argc == 2) {
-          int64_t value;
+          sreg_t value;
           if (GetValue(arg1, &value)) {
             cur = reinterpret_cast<byte*>(value);
             // no length parameter passed, assume 10 instructions
             end = cur + (10 * kInstrSize);
           }
         } else {
-          int64_t value1;
-          int64_t value2;
+          sreg_t value1;
+          sreg_t value2;
           if (GetValue(arg1, &value1) && GetValue(arg2, &value2)) {
             cur = reinterpret_cast<byte*>(value1);
             end = cur + (value2 * kInstrSize);
@@ -1988,6 +2032,9 @@ void RiscvDebugger::Debug() {
         PrintF("mem\n");
         PrintF("  mem <address> [<words>]\n");
         PrintF("  Dump memory content, default dump 10 words)\n");
+        PrintF("watch\n");
+        PrintF("  watch <address> \n");
+        PrintF("  watch memory content.)\n");
         PrintF("flags\n");
         PrintF("  print flags\n");
         PrintF("disasm (alias 'di')\n");
@@ -2165,7 +2212,7 @@ void Simulator::FlushOnePage(base::CustomMatcherHashMap* i_cache,
 
 void Simulator::CheckICache(base::CustomMatcherHashMap* i_cache,
                             Instruction* instr) {
-  int64_t address = reinterpret_cast<int64_t>(instr);
+  sreg_t address = reinterpret_cast<sreg_t>(instr);
   void* page = reinterpret_cast<void*>(address & (~CachePage::kPageMask));
   void* line = reinterpret_cast<void*>(address & (~CachePage::kLineMask));
   int offset = (address & CachePage::kPageMask);
@@ -2212,7 +2259,7 @@ Simulator::Simulator(Isolate* isolate) : isolate_(isolate), builtins_(isolate) {
   // The sp is initialized to point to the bottom (high address) of the
   // allocated stack area. To be safe in potential stack underflows we leave
   // some buffer below.
-  registers_[sp] = reinterpret_cast<int64_t>(stack_) + stack_size_ - 64;
+  registers_[sp] = reinterpret_cast<sreg_t>(stack_) + stack_size_ - xlen;
   // The ra and pc are initialized to a known bad value that will cause an
   // access violation if the simulator ever tries to execute it.
   registers_[pc] = bad_ra;
@@ -2243,7 +2290,7 @@ Simulator* Simulator::current(Isolate* isolate) {
 
 // Sets the register in the architecture state. It will also deal with
 // updating Simulator internal state for special registers such as PC.
-void Simulator::set_register(int reg, int64_t value) {
+void Simulator::set_register(int reg, sreg_t value) {
   DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
   if (reg == pc) {
     pc_modified_ = true;
@@ -2253,13 +2300,6 @@ void Simulator::set_register(int reg, int64_t value) {
   registers_[reg] = (reg == 0) ? 0 : value;
 }
 
-void Simulator::set_dw_register(int reg, const int* dbl) {
-  DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
-  registers_[reg] = dbl[1];
-  registers_[reg] = registers_[reg] << 32;
-  registers_[reg] += dbl[0];
-}
-
 void Simulator::set_fpu_register(int fpureg, int64_t value) {
   DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
   FPUregisters_[fpureg] = value;
@@ -2294,14 +2334,24 @@ void Simulator::set_fpu_register_float(int fpureg, float value) {
   FPUregisters_[fpureg] = box_float(value);
 }
 
+void Simulator::set_fpu_register_float(int fpureg, Float32 value) {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  Float64 t = Float64::FromBits(box_float(value.get_bits()));
+  memcpy(&FPUregisters_[fpureg], &t, 8);
+}
+
 void Simulator::set_fpu_register_double(int fpureg, double value) {
   DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
   *base::bit_cast<double*>(&FPUregisters_[fpureg]) = value;
 }
 
+void Simulator::set_fpu_register_double(int fpureg, Float64 value) {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  memcpy(&FPUregisters_[fpureg], &value, 8);
+}
 // Get the register from the architecture state. This function does handle
 // the special case of accessing the PC register.
-int64_t Simulator::get_register(int reg) const {
+sreg_t Simulator::get_register(int reg) const {
   DCHECK((reg >= 0) && (reg < kNumSimuRegisters));
   if (reg == 0)
     return 0;
@@ -2350,11 +2400,26 @@ float Simulator::get_fpu_register_float(int fpureg) const {
   return *base::bit_cast<float*>(const_cast<int64_t*>(&FPUregisters_[fpureg]));
 }
 
+Float32 Simulator::get_fpu_register_Float32(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  if (!is_boxed_float(FPUregisters_[fpureg])) {
+    std::cout << std::hex << FPUregisters_[fpureg] << std::endl;
+    return Float32::FromBits(0x7ffc0000);
+  }
+  return Float32::FromBits(
+      *base::bit_cast<uint32_t*>(const_cast<int64_t*>(&FPUregisters_[fpureg])));
+}
+
 double Simulator::get_fpu_register_double(int fpureg) const {
   DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
   return *base::bit_cast<double*>(&FPUregisters_[fpureg]);
 }
 
+Float64 Simulator::get_fpu_register_Float64(int fpureg) const {
+  DCHECK((fpureg >= 0) && (fpureg < kNumFPURegisters));
+  return Float64::FromBits(FPUregisters_[fpureg]);
+}
+
 #ifdef CAN_USE_RVV_INSTRUCTIONS
 __int128_t Simulator::get_vregister(int vreg) const {
   DCHECK((vreg >= 0) && (vreg < kNumVRegisters));
@@ -2394,7 +2459,7 @@ uint32_t Simulator::get_dynamic_rounding_mode() {
   return read_csr_value(csr_frm);
 }
 
-void Simulator::write_csr_value(uint32_t csr, uint64_t val) {
+void Simulator::write_csr_value(uint32_t csr, reg_t val) {
   uint32_t value = (uint32_t)val;
   switch (csr) {
     case csr_fflags:  // Floating-Point Accrued Exceptions (RW)
@@ -2414,7 +2479,7 @@ void Simulator::write_csr_value(uint32_t csr, uint64_t val) {
   }
 }
 
-void Simulator::set_csr_bits(uint32_t csr, uint64_t val) {
+void Simulator::set_csr_bits(uint32_t csr, reg_t val) {
   uint32_t value = (uint32_t)val;
   switch (csr) {
     case csr_fflags:  // Floating-Point Accrued Exceptions (RW)
@@ -2434,7 +2499,7 @@ void Simulator::set_csr_bits(uint32_t csr, uint64_t val) {
   }
 }
 
-void Simulator::clear_csr_bits(uint32_t csr, uint64_t val) {
+void Simulator::clear_csr_bits(uint32_t csr, reg_t val) {
   uint32_t value = (uint32_t)val;
   switch (csr) {
     case csr_fflags:  // Floating-Point Accrued Exceptions (RW)
@@ -2487,7 +2552,7 @@ T Simulator::FMaxMinHelper(T a, T b, MaxMinKind kind) {
 }
 
 // Raw access to the PC register.
-void Simulator::set_pc(int64_t value) {
+void Simulator::set_pc(sreg_t value) {
   pc_modified_ = true;
   registers_[pc] = value;
   DCHECK(has_bad_pc() || ((value % kInstrSize) == 0) ||
@@ -2499,7 +2564,7 @@ bool Simulator::has_bad_pc() const {
 }
 
 // Raw access to the PC register without the special adjustment when reading.
-int64_t Simulator::get_pc() const { return registers_[pc]; }
+sreg_t Simulator::get_pc() const { return registers_[pc]; }
 
 // The RISC-V spec leaves it open to the implementation on how to handle
 // unaligned reads and writes. For now, we simply disallow unaligned reads but
@@ -2515,6 +2580,7 @@ void Simulator::DieOrDebug() {
   }
 }
 
+#if V8_TARGET_ARCH_RISCV64
 void Simulator::TraceRegWr(int64_t value, TraceType t) {
   if (::v8::internal::FLAG_trace_sim) {
     union {
@@ -2528,22 +2594,22 @@ void Simulator::TraceRegWr(int64_t value, TraceType t) {
     switch (t) {
       case WORD:
         SNPrintF(trace_buf_,
-                 "%016" PRIx64 "    (%" PRId64 ")    int32:%" PRId32
+                 "%016" REGIx_FORMAT "    (%" PRId64 ")    int32:%" PRId32
                  " uint32:%" PRIu32,
                  v.fmt_int64, icount_, v.fmt_int32[0], v.fmt_int32[0]);
         break;
       case DWORD:
         SNPrintF(trace_buf_,
-                 "%016" PRIx64 "    (%" PRId64 ")    int64:%" PRId64
+                 "%016" REGIx_FORMAT "    (%" PRId64 ")    int64:%" REGId_FORMAT
                  " uint64:%" PRIu64,
                  value, icount_, value, value);
         break;
       case FLOAT:
-        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    flt:%e",
+        SNPrintF(trace_buf_, "%016" REGIx_FORMAT "    (%" PRId64 ")    flt:%e",
                  v.fmt_int64, icount_, v.fmt_float[0]);
         break;
       case DOUBLE:
-        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    dbl:%e",
+        SNPrintF(trace_buf_, "%016" REGIx_FORMAT "    (%" PRId64 ")    dbl:%e",
                  v.fmt_int64, icount_, v.fmt_double);
         break;
       default:
@@ -2552,37 +2618,74 @@ void Simulator::TraceRegWr(int64_t value, TraceType t) {
   }
 }
 
+#elif V8_TARGET_ARCH_32_BIT
+template <typename T>
+void Simulator::TraceRegWr(T value, TraceType t) {
+  if (::v8::internal::FLAG_trace_sim) {
+    union {
+      int32_t fmt_int32;
+      float fmt_float;
+      double fmt_double;
+    } v;
+    if (t != DOUBLE) {
+      v.fmt_int32 = value;
+    } else {
+      DCHECK_EQ(sizeof(T), 8);
+      v.fmt_double = value;
+    }
+    switch (t) {
+      case WORD:
+        SNPrintF(trace_buf_,
+                 "%016" REGIx_FORMAT "    (%" PRId64 ")    int32:%" REGId_FORMAT
+                 " uint32:%" PRIu32,
+                 v.fmt_int32, icount_, v.fmt_int32, v.fmt_int32);
+        break;
+      case FLOAT:
+        SNPrintF(trace_buf_, "%016" REGIx_FORMAT "    (%" PRId64 ")    flt:%e",
+                 v.fmt_int32, icount_, v.fmt_float);
+        break;
+      case DOUBLE:
+        SNPrintF(trace_buf_, "%016" PRIx64 "    (%" PRId64 ")    dbl:%e",
+                 static_cast<int64_t>(v.fmt_double), icount_, v.fmt_double);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+#endif
+
 // TODO(plind): consider making icount_ printing a flag option.
 template <typename T>
-void Simulator::TraceMemRd(int64_t addr, T value, int64_t reg_value) {
+void Simulator::TraceMemRd(sreg_t addr, T value, sreg_t reg_value) {
   if (::v8::internal::FLAG_trace_sim) {
     if (std::is_integral<T>::value) {
       switch (sizeof(T)) {
         case 1:
           SNPrintF(trace_buf_,
-                   "%016" PRIx64 "    (%" PRId64 ")    int8:%" PRId8
-                   " uint8:%" PRIu8 " <-- [addr: %" PRIx64 "]",
+                   "%016" REGIx_FORMAT "    (%" PRId64 ")    int8:%" PRId8
+                   " uint8:%" PRIu8 " <-- [addr: %" REGIx_FORMAT "]",
                    reg_value, icount_, static_cast<int8_t>(value),
                    static_cast<uint8_t>(value), addr);
           break;
         case 2:
           SNPrintF(trace_buf_,
-                   "%016" PRIx64 "    (%" PRId64 ")    int16:%" PRId16
-                   " uint16:%" PRIu16 " <-- [addr: %" PRIx64 "]",
+                   "%016" REGIx_FORMAT "    (%" PRId64 ")    int16:%" PRId16
+                   " uint16:%" PRIu16 " <-- [addr: %" REGIx_FORMAT "]",
                    reg_value, icount_, static_cast<int16_t>(value),
                    static_cast<uint16_t>(value), addr);
           break;
         case 4:
           SNPrintF(trace_buf_,
-                   "%016" PRIx64 "    (%" PRId64 ")    int32:%" PRId32
-                   " uint32:%" PRIu32 " <-- [addr: %" PRIx64 "]",
+                   "%016" REGIx_FORMAT "    (%" PRId64 ")    int32:%" PRId32
+                   " uint32:%" PRIu32 " <-- [addr: %" REGIx_FORMAT "]",
                    reg_value, icount_, static_cast<int32_t>(value),
                    static_cast<uint32_t>(value), addr);
           break;
         case 8:
           SNPrintF(trace_buf_,
-                   "%016" PRIx64 "    (%" PRId64 ")    int64:%" PRId64
-                   " uint64:%" PRIu64 " <-- [addr: %" PRIx64 "]",
+                   "%016" REGIx_FORMAT "    (%" PRId64 ")    int64:%" PRId64
+                   " uint64:%" PRIu64 " <-- [addr: %" REGIx_FORMAT "]",
                    reg_value, icount_, static_cast<int64_t>(value),
                    static_cast<uint64_t>(value), addr);
           break;
@@ -2591,13 +2694,13 @@ void Simulator::TraceMemRd(int64_t addr, T value, int64_t reg_value) {
       }
     } else if (std::is_same<float, T>::value) {
       SNPrintF(trace_buf_,
-               "%016" PRIx64 "    (%" PRId64 ")    flt:%e <-- [addr: %" PRIx64
-               "]",
+               "%016" REGIx_FORMAT "    (%" PRId64
+               ")    flt:%e <-- [addr: %" REGIx_FORMAT "]",
                reg_value, icount_, static_cast<float>(value), addr);
     } else if (std::is_same<double, T>::value) {
       SNPrintF(trace_buf_,
-               "%016" PRIx64 "    (%" PRId64 ")    dbl:%e <-- [addr: %" PRIx64
-               "]",
+               "%016" REGIx_FORMAT "    (%" PRId64
+               ")    dbl:%e <-- [addr: %" REGIx_FORMAT "]",
                reg_value, icount_, static_cast<double>(value), addr);
     } else {
       UNREACHABLE();
@@ -2605,21 +2708,49 @@ void Simulator::TraceMemRd(int64_t addr, T value, int64_t reg_value) {
   }
 }
 
+void Simulator::TraceMemRdFloat(sreg_t addr, Float32 value, int64_t reg_value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    SNPrintF(trace_buf_,
+             "%016" PRIx64 "    (%" PRId64
+             ")    flt:%e <-- [addr: %" REGIx_FORMAT "]",
+             reg_value, icount_, static_cast<float>(value.get_scalar()), addr);
+  }
+}
+
+void Simulator::TraceMemRdDouble(sreg_t addr, double value, int64_t reg_value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    SNPrintF(trace_buf_,
+             "%016" PRIx64 "    (%" PRId64
+             ")    dbl:%e <-- [addr: %" REGIx_FORMAT "]",
+             reg_value, icount_, static_cast<double>(value), addr);
+  }
+}
+
+void Simulator::TraceMemRdDouble(sreg_t addr, Float64 value,
+                                 int64_t reg_value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    SNPrintF(trace_buf_,
+             "%016" PRIx64 "    (%" PRId64
+             ")    dbl:%e <-- [addr: %" REGIx_FORMAT "]",
+             reg_value, icount_, static_cast<double>(value.get_scalar()), addr);
+  }
+}
+
 template <typename T>
-void Simulator::TraceMemWr(int64_t addr, T value) {
+void Simulator::TraceMemWr(sreg_t addr, T value) {
   if (::v8::internal::FLAG_trace_sim) {
     switch (sizeof(T)) {
       case 1:
         SNPrintF(trace_buf_,
                  "                    (%" PRIu64 ")    int8:%" PRId8
-                 " uint8:%" PRIu8 " --> [addr: %" PRIx64 "]",
+                 " uint8:%" PRIu8 " --> [addr: %" REGIx_FORMAT "]",
                  icount_, static_cast<int8_t>(value),
                  static_cast<uint8_t>(value), addr);
         break;
       case 2:
         SNPrintF(trace_buf_,
                  "                    (%" PRIu64 ")    int16:%" PRId16
-                 " uint16:%" PRIu16 " --> [addr: %" PRIx64 "]",
+                 " uint16:%" PRIu16 " --> [addr: %" REGIx_FORMAT "]",
                  icount_, static_cast<int16_t>(value),
                  static_cast<uint16_t>(value), addr);
         break;
@@ -2627,13 +2758,13 @@ void Simulator::TraceMemWr(int64_t addr, T value) {
         if (std::is_integral<T>::value) {
           SNPrintF(trace_buf_,
                    "                    (%" PRIu64 ")    int32:%" PRId32
-                   " uint32:%" PRIu32 " --> [addr: %" PRIx64 "]",
+                   " uint32:%" PRIu32 " --> [addr: %" REGIx_FORMAT "]",
                    icount_, static_cast<int32_t>(value),
                    static_cast<uint32_t>(value), addr);
         } else {
           SNPrintF(trace_buf_,
                    "                    (%" PRIu64
-                   ")    flt:%e --> [addr: %" PRIx64 "]",
+                   ")    flt:%e --> [addr: %" REGIx_FORMAT "]",
                    icount_, static_cast<float>(value), addr);
         }
         break;
@@ -2641,13 +2772,13 @@ void Simulator::TraceMemWr(int64_t addr, T value) {
         if (std::is_integral<T>::value) {
           SNPrintF(trace_buf_,
                    "                    (%" PRIu64 ")    int64:%" PRId64
-                   " uint64:%" PRIu64 " --> [addr: %" PRIx64 "]",
+                   " uint64:%" PRIu64 " --> [addr: %" REGIx_FORMAT "]",
                    icount_, static_cast<int64_t>(value),
                    static_cast<uint64_t>(value), addr);
         } else {
           SNPrintF(trace_buf_,
                    "                    (%" PRIu64
-                   ")    dbl:%e --> [addr: %" PRIx64 "]",
+                   ")    dbl:%e --> [addr: %" REGIx_FORMAT "]",
                    icount_, static_cast<double>(value), addr);
         }
         break;
@@ -2657,25 +2788,33 @@ void Simulator::TraceMemWr(int64_t addr, T value) {
   }
 }
 
+void Simulator::TraceMemWrDouble(sreg_t addr, double value) {
+  if (::v8::internal::FLAG_trace_sim) {
+    SNPrintF(trace_buf_,
+             "                    (%" PRIu64
+             ")    dbl:%e --> [addr: %" REGIx_FORMAT "]",
+             icount_, value, addr);
+  }
+}
 // RISCV Memory Read/Write functions
 
 // TODO(RISCV): check whether the specific board supports unaligned load/store
 // (determined by EEI). For now, we assume the board does not support unaligned
 // load/store (e.g., trapping)
 template <typename T>
-T Simulator::ReadMem(int64_t addr, Instruction* instr) {
+T Simulator::ReadMem(sreg_t addr, Instruction* instr) {
   if (addr >= 0 && addr < 0x400) {
     // This has to be a nullptr-dereference, drop into debugger.
-    PrintF("Memory read from bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
-           " \n",
+    PrintF("Memory read from bad address: 0x%08" REGIx_FORMAT
+           " , pc=0x%08" PRIxPTR " \n",
            addr, reinterpret_cast<intptr_t>(instr));
     DieOrDebug();
   }
 #if !defined(V8_COMPRESS_POINTERS) && defined(RISCV_HAS_NO_UNALIGNED)
   // check for natural alignment
   if (!FLAG_riscv_c_extension && ((addr & (sizeof(T) - 1)) != 0)) {
-    PrintF("Unaligned read at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
-           reinterpret_cast<intptr_t>(instr));
+    PrintF("Unaligned read at 0x%08" REGIx_FORMAT " , pc=0x%08" V8PRIxPTR "\n",
+           addr, reinterpret_cast<intptr_t>(instr));
     DieOrDebug();
   }
 #endif
@@ -2685,30 +2824,75 @@ T Simulator::ReadMem(int64_t addr, Instruction* instr) {
 }
 
 template <typename T>
-void Simulator::WriteMem(int64_t addr, T value, Instruction* instr) {
+void Simulator::WriteMem(sreg_t addr, T value, Instruction* instr) {
   if (addr >= 0 && addr < 0x400) {
     // This has to be a nullptr-dereference, drop into debugger.
-    PrintF("Memory write to bad address: 0x%08" PRIx64 " , pc=0x%08" PRIxPTR
-           " \n",
+    PrintF("Memory write to bad address: 0x%08" REGIx_FORMAT
+           " , pc=0x%08" PRIxPTR " \n",
            addr, reinterpret_cast<intptr_t>(instr));
     DieOrDebug();
   }
 #if !defined(V8_COMPRESS_POINTERS) && defined(RISCV_HAS_NO_UNALIGNED)
   // check for natural alignment
   if (!FLAG_riscv_c_extension && ((addr & (sizeof(T) - 1)) != 0)) {
-    PrintF("Unaligned write at 0x%08" PRIx64 " , pc=0x%08" V8PRIxPTR "\n", addr,
-           reinterpret_cast<intptr_t>(instr));
+    PrintF("Unaligned write at 0x%08" REGIx_FORMAT " , pc=0x%08" V8PRIxPTR "\n",
+           addr, reinterpret_cast<intptr_t>(instr));
     DieOrDebug();
   }
 #endif
   T* ptr = reinterpret_cast<T*>(addr);
-  TraceMemWr(addr, value);
-  // PrintF("Unaligned read at 0x%08" PRIx64 " , pc=0x%08" PRId64 "\n",
-  // (int64_t)ptr,
-  //        (int64_t)value);
+  if (!std::is_same<double, T>::value) {
+    TraceMemWr(addr, value);
+  } else {
+    TraceMemWrDouble(addr, value);
+  }
   *ptr = value;
 }
 
+template <>
+void Simulator::WriteMem(sreg_t addr, Float32 value, Instruction* instr) {
+  if (addr >= 0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory write to bad address: 0x%08" REGIx_FORMAT
+           " , pc=0x%08" PRIxPTR " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+#if !defined(V8_COMPRESS_POINTERS) && defined(RISCV_HAS_NO_UNALIGNED)
+  // check for natural alignment
+  if (!FLAG_riscv_c_extension && ((addr & (sizeof(T) - 1)) != 0)) {
+    PrintF("Unaligned write at 0x%08" REGIx_FORMAT " , pc=0x%08" V8PRIxPTR "\n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+#endif
+  float* ptr = reinterpret_cast<float*>(addr);
+  TraceMemWr(addr, value.get_scalar());
+  memcpy(ptr, &value, 4);
+}
+
+template <>
+void Simulator::WriteMem(sreg_t addr, Float64 value, Instruction* instr) {
+  if (addr >= 0 && addr < 0x400) {
+    // This has to be a nullptr-dereference, drop into debugger.
+    PrintF("Memory write to bad address: 0x%08" REGIx_FORMAT
+           " , pc=0x%08" PRIxPTR " \n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+#if !defined(V8_COMPRESS_POINTERS) && defined(RISCV_HAS_NO_UNALIGNED)
+  // check for natural alignment
+  if (!FLAG_riscv_c_extension && ((addr & (sizeof(T) - 1)) != 0)) {
+    PrintF("Unaligned write at 0x%08" REGIx_FORMAT " , pc=0x%08" V8PRIxPTR "\n",
+           addr, reinterpret_cast<intptr_t>(instr));
+    DieOrDebug();
+  }
+#endif
+  double* ptr = reinterpret_cast<double*>(addr);
+  TraceMemWrDouble(addr, value.get_scalar());
+  memcpy(ptr, &value, 8);
+}
+
 // Returns the limit of the stack area to enable checking for stack overflows.
 uintptr_t Simulator::StackLimit(uintptr_t c_limit) const {
   // The simulator uses a separate JS stack. If we have exhausted the C stack,
@@ -2735,12 +2919,15 @@ void Simulator::Format(Instruction* instr, const char* format) {
 // stuffed into a 64-bit value. With the code below we assume that all runtime
 // calls return 64 bits of result. If they don't, the a1 result register
 // contains a bogus value, which is fine because it is caller-saved.
-
+#if V8_TARGET_ARCH_RISCV64
 using SimulatorRuntimeCall = ObjectPair (*)(
-    int64_t arg0, int64_t arg1, int64_t arg2, int64_t arg3, int64_t arg4,
-    int64_t arg5, int64_t arg6, int64_t arg7, int64_t arg8, int64_t arg9,
-    int64_t arg10, int64_t arg11, int64_t arg12, int64_t arg13, int64_t arg14,
-    int64_t arg15, int64_t arg16, int64_t arg17, int64_t arg18, int64_t arg19);
+#elif V8_TARGET_ARCH_RISCV32
+using SimulatorRuntimeCall = int64_t (*)(
+#endif
+    sreg_t arg0, sreg_t arg1, sreg_t arg2, sreg_t arg3, sreg_t arg4,
+    sreg_t arg5, sreg_t arg6, sreg_t arg7, sreg_t arg8, sreg_t arg9,
+    sreg_t arg10, sreg_t arg11, sreg_t arg12, sreg_t arg13, sreg_t arg14,
+    sreg_t arg15, sreg_t arg16, sreg_t arg17, sreg_t arg18, sreg_t arg19);
 
 // These prototypes handle the four types of FP calls.
 using SimulatorRuntimeCompareCall = int64_t (*)(double darg0, double darg1);
@@ -2750,12 +2937,12 @@ using SimulatorRuntimeFPIntCall = double (*)(double darg0, int32_t arg0);
 
 // This signature supports direct call in to API function native callback
 // (refer to InvocationCallback in v8.h).
-using SimulatorRuntimeDirectApiCall = void (*)(int64_t arg0);
-using SimulatorRuntimeProfilingApiCall = void (*)(int64_t arg0, void* arg1);
+using SimulatorRuntimeDirectApiCall = void (*)(sreg_t arg0);
+using SimulatorRuntimeProfilingApiCall = void (*)(sreg_t arg0, void* arg1);
 
 // This signature supports direct call to accessor getter callback.
-using SimulatorRuntimeDirectGetterCall = void (*)(int64_t arg0, int64_t arg1);
-using SimulatorRuntimeProfilingGetterCall = void (*)(int64_t arg0, int64_t arg1,
+using SimulatorRuntimeDirectGetterCall = void (*)(sreg_t arg0, sreg_t arg1);
+using SimulatorRuntimeProfilingGetterCall = void (*)(sreg_t arg0, sreg_t arg1,
                                                      void* arg2);
 
 // Software interrupt instructions are used by the simulator to call into the
@@ -2769,28 +2956,28 @@ void Simulator::SoftwareInterrupt() {
   if (instr_.InstructionBits() == rtCallRedirInstr) {  // ECALL
     Redirection* redirection = Redirection::FromInstruction(instr_.instr());
 
-    int64_t* stack_pointer = reinterpret_cast<int64_t*>(get_register(sp));
-
-    const int64_t arg0 = get_register(a0);
-    const int64_t arg1 = get_register(a1);
-    const int64_t arg2 = get_register(a2);
-    const int64_t arg3 = get_register(a3);
-    const int64_t arg4 = get_register(a4);
-    const int64_t arg5 = get_register(a5);
-    const int64_t arg6 = get_register(a6);
-    const int64_t arg7 = get_register(a7);
-    const int64_t arg8 = stack_pointer[0];
-    const int64_t arg9 = stack_pointer[1];
-    const int64_t arg10 = stack_pointer[2];
-    const int64_t arg11 = stack_pointer[3];
-    const int64_t arg12 = stack_pointer[4];
-    const int64_t arg13 = stack_pointer[5];
-    const int64_t arg14 = stack_pointer[6];
-    const int64_t arg15 = stack_pointer[7];
-    const int64_t arg16 = stack_pointer[8];
-    const int64_t arg17 = stack_pointer[9];
-    const int64_t arg18 = stack_pointer[10];
-    const int64_t arg19 = stack_pointer[11];
+    sreg_t* stack_pointer = reinterpret_cast<sreg_t*>(get_register(sp));
+
+    const sreg_t arg0 = get_register(a0);
+    const sreg_t arg1 = get_register(a1);
+    const sreg_t arg2 = get_register(a2);
+    const sreg_t arg3 = get_register(a3);
+    const sreg_t arg4 = get_register(a4);
+    const sreg_t arg5 = get_register(a5);
+    const sreg_t arg6 = get_register(a6);
+    const sreg_t arg7 = get_register(a7);
+    const sreg_t arg8 = stack_pointer[0];
+    const sreg_t arg9 = stack_pointer[1];
+    const sreg_t arg10 = stack_pointer[2];
+    const sreg_t arg11 = stack_pointer[3];
+    const sreg_t arg12 = stack_pointer[4];
+    const sreg_t arg13 = stack_pointer[5];
+    const sreg_t arg14 = stack_pointer[6];
+    const sreg_t arg15 = stack_pointer[7];
+    const sreg_t arg16 = stack_pointer[8];
+    const sreg_t arg17 = stack_pointer[9];
+    const sreg_t arg18 = stack_pointer[10];
+    const sreg_t arg19 = stack_pointer[11];
     static_assert(kMaxCParameters == 20);
 
     bool fp_call =
@@ -2801,9 +2988,9 @@ void Simulator::SoftwareInterrupt() {
 
     // This is dodgy but it works because the C entry stubs are never moved.
     // See comment in codegen-arm.cc and bug 1242173.
-    int64_t saved_ra = get_register(ra);
+    sreg_t saved_ra = get_register(ra);
 
-    int64_t pc = get_pc();
+    sreg_t pc = get_pc();
 
     intptr_t external =
         reinterpret_cast<intptr_t>(redirection->external_function());
@@ -2846,7 +3033,7 @@ void Simulator::SoftwareInterrupt() {
           SimulatorRuntimeCompareCall target =
               reinterpret_cast<SimulatorRuntimeCompareCall>(external);
           iresult = target(dval0, dval1);
-          set_register(a0, static_cast<int64_t>(iresult));
+          set_register(a0, static_cast<sreg_t>(iresult));
           //  set_register(a1, static_cast<int64_t>(iresult >> 32));
           break;
         }
@@ -2890,7 +3077,7 @@ void Simulator::SoftwareInterrupt() {
       }
     } else if (redirection->type() == ExternalReference::DIRECT_API_CALL) {
       if (::v8::internal::FLAG_trace_sim) {
-        PrintF("Call to host function %s at %p args %08" PRIx64 " \n",
+        PrintF("Call to host function %s at %p args %08" REGIx_FORMAT " \n",
                ExternalReferenceTable::NameOfIsolateIndependentAddress(pc),
                reinterpret_cast<void*>(external), arg0);
       }
@@ -2899,8 +3086,8 @@ void Simulator::SoftwareInterrupt() {
       target(arg0);
     } else if (redirection->type() == ExternalReference::PROFILING_API_CALL) {
       if (::v8::internal::FLAG_trace_sim) {
-        PrintF("Call to host function %s at %p args %08" PRIx64 "  %08" PRIx64
-               " \n",
+        PrintF("Call to host function %s at %p args %08" REGIx_FORMAT
+               "  %08" REGIx_FORMAT " \n",
                ExternalReferenceTable::NameOfIsolateIndependentAddress(pc),
                reinterpret_cast<void*>(external), arg0, arg1);
       }
@@ -2909,8 +3096,8 @@ void Simulator::SoftwareInterrupt() {
       target(arg0, Redirection::ReverseRedirection(arg1));
     } else if (redirection->type() == ExternalReference::DIRECT_GETTER_CALL) {
       if (::v8::internal::FLAG_trace_sim) {
-        PrintF("Call to host function %s at %p args %08" PRIx64 "  %08" PRIx64
-               " \n",
+        PrintF("Call to host function %s at %p args %08" REGIx_FORMAT
+               "  %08" REGIx_FORMAT " \n",
                ExternalReferenceTable::NameOfIsolateIndependentAddress(pc),
                reinterpret_cast<void*>(external), arg0, arg1);
       }
@@ -2920,8 +3107,8 @@ void Simulator::SoftwareInterrupt() {
     } else if (redirection->type() ==
                ExternalReference::PROFILING_GETTER_CALL) {
       if (::v8::internal::FLAG_trace_sim) {
-        PrintF("Call to host function %s at %p args %08" PRIx64 "  %08" PRIx64
-               "  %08" PRIx64 " \n",
+        PrintF("Call to host function %s at %p args %08" REGIx_FORMAT
+               "  %08" REGIx_FORMAT "  %08" REGIx_FORMAT " \n",
                ExternalReferenceTable::NameOfIsolateIndependentAddress(pc),
                reinterpret_cast<void*>(external), arg0, arg1, arg2);
       }
@@ -2948,26 +3135,36 @@ void Simulator::SoftwareInterrupt() {
       if (::v8::internal::FLAG_trace_sim) {
         PrintF(
             "Call to host function %s at %p "
-            "args %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64
-            " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64 " , %08" PRIx64
-            " , %08" PRIx64 " , %08" PRIx64 " , %016" PRIx64 " , %016" PRIx64
-            " , %016" PRIx64 " , %016" PRIx64 " , %016" PRIx64 " , %016" PRIx64
-            " , %016" PRIx64 " , %016" PRIx64 " , %016" PRIx64 " , %016" PRIx64
-            " \n",
+            "args %08" REGIx_FORMAT " , %08" REGIx_FORMAT " , %08" REGIx_FORMAT
+            " , %08" REGIx_FORMAT " , %08" REGIx_FORMAT " , %08" REGIx_FORMAT
+            " , %08" REGIx_FORMAT " , %08" REGIx_FORMAT " , %08" REGIx_FORMAT
+            " , %08" REGIx_FORMAT " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT
+            " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT
+            " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT
+            " , %016" REGIx_FORMAT " , %016" REGIx_FORMAT " \n",
             ExternalReferenceTable::NameOfIsolateIndependentAddress(pc),
             reinterpret_cast<void*>(FUNCTION_ADDR(target)), arg0, arg1, arg2,
             arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10, arg11, arg12,
             arg13, arg14, arg15, arg16, arg17, arg18, arg19);
       }
+#if V8_TARGET_ARCH_RISCV64
       ObjectPair result = target(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7,
                                  arg8, arg9, arg10, arg11, arg12, arg13, arg14,
                                  arg15, arg16, arg17, arg18, arg19);
-      set_register(a0, (int64_t)(result.x));
-      set_register(a1, (int64_t)(result.y));
+      set_register(a0, (sreg_t)(result.x));
+      set_register(a1, (sreg_t)(result.y));
+
+#elif V8_TARGET_ARCH_RISCV32
+      int64_t result = target(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7,
+                              arg8, arg9, arg10, arg11, arg12, arg13, arg14,
+                              arg15, arg16, arg17, arg18, arg19);
+      set_register(a0, (sreg_t)result);
+      set_register(a1, (sreg_t)(result >> 32));
+#endif
     }
     if (::v8::internal::FLAG_trace_sim) {
-      PrintF("Returned %08" PRIx64 "  : %08" PRIx64 " \n", get_register(a1),
-             get_register(a0));
+      PrintF("Returned %08" REGIx_FORMAT "  : %08" REGIx_FORMAT " \n",
+             get_register(a1), get_register(a0));
     }
     set_register(ra, saved_ra);
     set_pc(get_register(ra));
@@ -2993,27 +3190,27 @@ void Simulator::SoftwareInterrupt() {
 }
 
 // Stop helper functions.
-bool Simulator::IsWatchpoint(uint64_t code) {
+bool Simulator::IsWatchpoint(reg_t code) {
   return (code <= kMaxWatchpointCode);
 }
 
-void Simulator::PrintWatchpoint(uint64_t code) {
+void Simulator::PrintWatchpoint(reg_t code) {
   RiscvDebugger dbg(this);
   ++break_count_;
-  PrintF("\n---- watchpoint %" PRId64 "  marker: %3d  (instr count: %8" PRId64
+  PrintF("\n---- watchpoint %" REGId_FORMAT
+         "  marker: %3d  (instr count: %8" PRId64
          " ) ----------"
          "----------------------------------",
          code, break_count_, icount_);
   dbg.PrintAllRegs();  // Print registers and continue running.
 }
 
-void Simulator::HandleStop(uint64_t code) {
+void Simulator::HandleStop(reg_t code) {
   // Stop if it is enabled, otherwise go on jumping over the stop
   // and the message address.
   if (IsEnabledStop(code)) {
-    RiscvDebugger dbg(this);
-    PrintF("Simulator hit stop (%" PRId64 ")\n", code);
-    dbg.Debug();
+    PrintF("Simulator hit stop (%" REGId_FORMAT ")\n", code);
+    DieOrDebug();
   }
 }
 
@@ -3024,28 +3221,28 @@ bool Simulator::IsStopInstruction(Instruction* instr) {
          static_cast<uint32_t>(code) <= kMaxStopCode;
 }
 
-bool Simulator::IsEnabledStop(uint64_t code) {
+bool Simulator::IsEnabledStop(reg_t code) {
   DCHECK_LE(code, kMaxStopCode);
   DCHECK_GT(code, kMaxWatchpointCode);
   return !(watched_stops_[code].count & kStopDisabledBit);
 }
 
-void Simulator::EnableStop(uint64_t code) {
+void Simulator::EnableStop(reg_t code) {
   if (!IsEnabledStop(code)) {
     watched_stops_[code].count &= ~kStopDisabledBit;
   }
 }
 
-void Simulator::DisableStop(uint64_t code) {
+void Simulator::DisableStop(reg_t code) {
   if (IsEnabledStop(code)) {
     watched_stops_[code].count |= kStopDisabledBit;
   }
 }
 
-void Simulator::IncreaseStopCounter(uint64_t code) {
+void Simulator::IncreaseStopCounter(reg_t code) {
   DCHECK_LE(code, kMaxStopCode);
   if ((watched_stops_[code].count & ~(1 << 31)) == 0x7FFFFFFF) {
-    PrintF("Stop counter for code %" PRId64
+    PrintF("Stop counter for code %" REGId_FORMAT
            "  has overflowed.\n"
            "Enabling this code and reseting the counter to 0.\n",
            code);
@@ -3057,7 +3254,7 @@ void Simulator::IncreaseStopCounter(uint64_t code) {
 }
 
 // Print a stop status.
-void Simulator::PrintStopInfo(uint64_t code) {
+void Simulator::PrintStopInfo(reg_t code) {
   if (code <= kMaxWatchpointCode) {
     PrintF("That is a watchpoint, not a stop.\n");
     return;
@@ -3070,11 +3267,13 @@ void Simulator::PrintStopInfo(uint64_t code) {
   // Don't print the state of unused breakpoints.
   if (count != 0) {
     if (watched_stops_[code].desc) {
-      PrintF("stop %" PRId64 "  - 0x%" PRIx64 " : \t%s, \tcounter = %i, \t%s\n",
+      PrintF("stop %" REGId_FORMAT "  - 0x%" REGIx_FORMAT
+             " : \t%s, \tcounter = %i, \t%s\n",
              code, code, state, count, watched_stops_[code].desc);
     } else {
-      PrintF("stop %" PRId64 "  - 0x%" PRIx64 " : \t%s, \tcounter = %i\n", code,
-             code, state, count);
+      PrintF("stop %" REGId_FORMAT "  - 0x%" REGIx_FORMAT
+             " : \t%s, \tcounter = %i\n",
+             code, code, state, count);
     }
   }
 }
@@ -3170,7 +3369,7 @@ void Simulator::DecodeRVRType() {
       sreg_t rhs = sext_xlen(rs2());
       if (rhs == 0) {
         set_rd(-1);
-      } else if (lhs == INT64_MIN && rhs == -1) {
+      } else if (lhs == INTPTR_MIN && rhs == -1) {
         set_rd(lhs);
       } else {
         set_rd(sext_xlen(lhs / rhs));
@@ -3181,7 +3380,7 @@ void Simulator::DecodeRVRType() {
       reg_t lhs = zext_xlen(rs1());
       reg_t rhs = zext_xlen(rs2());
       if (rhs == 0) {
-        set_rd(UINT64_MAX);
+        set_rd(UINTPTR_MAX);
       } else {
         set_rd(zext_xlen(lhs / rhs));
       }
@@ -3192,7 +3391,7 @@ void Simulator::DecodeRVRType() {
       sreg_t rhs = sext_xlen(rs2());
       if (rhs == 0) {
         set_rd(lhs);
-      } else if (lhs == INT64_MIN && rhs == -1) {
+      } else if (lhs == INTPTR_MIN && rhs == -1) {
         set_rd(0);
       } else {
         set_rd(sext_xlen(lhs % rhs));
@@ -3524,7 +3723,10 @@ void Simulator::DecodeRVRAType() {
   switch (instr_.InstructionBits() & kRATypeMask) {
     case RO_LR_W: {
       base::MutexGuard lock_guard(&GlobalMonitor::Get()->mutex);
-      int64_t addr = rs1();
+      sreg_t addr = rs1();
+      if ((addr & 0x3) != 0) {
+        DieOrDebug();
+      }
       auto val = ReadMem<int32_t>(addr, instr_.instr());
       set_rd(sext32(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
@@ -3534,7 +3736,10 @@ void Simulator::DecodeRVRAType() {
       break;
     }
     case RO_SC_W: {
-      int64_t addr = rs1();
+      sreg_t addr = rs1();
+      if ((addr & 0x3) != 0) {
+        DieOrDebug();
+      }
       base::MutexGuard lock_guard(&GlobalMonitor::Get()->mutex);
       if (local_monitor_.NotifyStoreConditional(addr, TransactionSize::Word) &&
           GlobalMonitor::Get()->NotifyStoreConditional_Locked(
@@ -3549,54 +3754,81 @@ void Simulator::DecodeRVRAType() {
       break;
     }
     case RO_AMOSWAP_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return (uint32_t)rs2(); }, instr_.instr(),
           WORD)));
       break;
     }
     case RO_AMOADD_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return lhs + (uint32_t)rs2(); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOXOR_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return lhs ^ (uint32_t)rs2(); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOAND_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return lhs & (uint32_t)rs2(); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOOR_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return lhs | (uint32_t)rs2(); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOMIN_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<int32_t>(
           rs1(), [&](int32_t lhs) { return std::min(lhs, (int32_t)rs2()); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOMAX_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<int32_t>(
           rs1(), [&](int32_t lhs) { return std::max(lhs, (int32_t)rs2()); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOMINU_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return std::min(lhs, (uint32_t)rs2()); },
           instr_.instr(), WORD)));
       break;
     }
     case RO_AMOMAXU_W: {
+      if ((rs1() & 0x3) != 0) {
+        DieOrDebug();
+      }
       set_rd(sext32(amo<uint32_t>(
           rs1(), [&](uint32_t lhs) { return std::max(lhs, (uint32_t)rs2()); },
           instr_.instr(), WORD)));
@@ -3774,15 +4006,15 @@ void Simulator::DecodeRVRFPType() {
     case RO_FSGNJ_S: {  // RO_FSGNJN_S  RO_FSQNJX_S
       switch (instr_.Funct3Value()) {
         case 0b000: {  // RO_FSGNJ_S
-          set_frd(fsgnj32(frs1(), frs2(), false, false));
+          set_frd(fsgnj32(frs1_boxed(), frs2_boxed(), false, false));
           break;
         }
         case 0b001: {  // RO_FSGNJN_S
-          set_frd(fsgnj32(frs1(), frs2(), true, false));
+          set_frd(fsgnj32(frs1_boxed(), frs2_boxed(), true, false));
           break;
         }
         case 0b010: {  // RO_FSQNJX_S
-          set_frd(fsgnj32(frs1(), frs2(), false, true));
+          set_frd(fsgnj32(frs1_boxed(), frs2_boxed(), false, true));
           break;
         }
         default: {
@@ -3905,7 +4137,8 @@ void Simulator::DecodeRVRFPType() {
     case RO_FMV_W_X: {
       if (instr_.Funct3Value() == 0b000) {
         // since FMV preserves source bit-pattern, no need to canonize
-        set_frd(base::bit_cast<float>((uint32_t)rs1()));
+        Float32 result = Float32::FromBits((uint32_t)rs1());
+        set_frd(result);
       } else {
         UNSUPPORTED();
       }
@@ -3989,15 +4222,15 @@ void Simulator::DecodeRVRFPType() {
     case RO_FSGNJ_D: {  // RO_FSGNJN_D RO_FSQNJX_D
       switch (instr_.Funct3Value()) {
         case 0b000: {  // RO_FSGNJ_D
-          set_drd(fsgnj64(drs1(), drs2(), false, false));
+          set_drd(fsgnj64(drs1_boxed(), drs2_boxed(), false, false));
           break;
         }
         case 0b001: {  // RO_FSGNJN_D
-          set_drd(fsgnj64(drs1(), drs2(), true, false));
+          set_drd(fsgnj64(drs1_boxed(), drs2_boxed(), true, false));
           break;
         }
         case 0b010: {  // RO_FSQNJX_D
-          set_drd(fsgnj64(drs1(), drs2(), false, true));
+          set_drd(fsgnj64(drs1_boxed(), drs2_boxed(), false, true));
           break;
         }
         default: {
@@ -4407,7 +4640,7 @@ void Simulator::DecodeRVIType() {
     case RO_JALR: {
       set_rd(get_pc() + kInstrSize);
       // Note: No need to shift 2 for JALR's imm12, but set lowest bit to 0.
-      int64_t next_pc = (rs1() + imm12()) & ~reg_t(1);
+      sreg_t next_pc = (rs1() + imm12()) & ~sreg_t(1);
       set_pc(next_pc);
       if (::v8::internal::FLAG_trace_sim) {
         Builtin builtin = LookUp((Address)get_pc());
@@ -4415,24 +4648,26 @@ void Simulator::DecodeRVIType() {
           auto code = builtins_.code(builtin);
           if ((rs1_reg() != ra || imm12() != 0)) {
             if ((Address)get_pc() == code.InstructionStart()) {
-              int64_t arg0 = get_register(a0);
-              int64_t arg1 = get_register(a1);
-              int64_t arg2 = get_register(a2);
-              int64_t arg3 = get_register(a3);
-              int64_t arg4 = get_register(a4);
-              int64_t arg5 = get_register(a5);
-              int64_t arg6 = get_register(a6);
-              int64_t arg7 = get_register(a7);
-              int64_t* stack_pointer =
-                  reinterpret_cast<int64_t*>(get_register(sp));
-              int64_t arg8 = stack_pointer[0];
-              int64_t arg9 = stack_pointer[1];
+              sreg_t arg0 = get_register(a0);
+              sreg_t arg1 = get_register(a1);
+              sreg_t arg2 = get_register(a2);
+              sreg_t arg3 = get_register(a3);
+              sreg_t arg4 = get_register(a4);
+              sreg_t arg5 = get_register(a5);
+              sreg_t arg6 = get_register(a6);
+              sreg_t arg7 = get_register(a7);
+              sreg_t* stack_pointer =
+                  reinterpret_cast<sreg_t*>(get_register(sp));
+              sreg_t arg8 = stack_pointer[0];
+              sreg_t arg9 = stack_pointer[1];
               PrintF(
                   "Call to Builtin at %s "
-                  "a0 %08" PRIx64 " ,a1 %08" PRIx64 " ,a2 %08" PRIx64
-                  " ,a3 %08" PRIx64 " ,a4 %08" PRIx64 " ,a5 %08" PRIx64
-                  " ,a6 %08" PRIx64 " ,a7 %08" PRIx64 " ,0(sp) %08" PRIx64
-                  " ,8(sp) %08" PRIx64 " ,sp %08" PRIx64 ",fp %08" PRIx64 " \n",
+                  "a0 %08" REGIx_FORMAT " ,a1 %08" REGIx_FORMAT
+                  " ,a2 %08" REGIx_FORMAT " ,a3 %08" REGIx_FORMAT
+                  " ,a4 %08" REGIx_FORMAT " ,a5 %08" REGIx_FORMAT
+                  " ,a6 %08" REGIx_FORMAT " ,a7 %08" REGIx_FORMAT
+                  " ,0(sp) %08" REGIx_FORMAT " ,8(sp) %08" REGIx_FORMAT
+                  " ,sp %08" REGIx_FORMAT ",fp %08" REGIx_FORMAT " \n",
                   builtins_.name(builtin), arg0, arg1, arg2, arg3, arg4, arg5,
                   arg6, arg7, arg8, arg9, get_register(sp), get_register(fp));
             }
@@ -4444,35 +4679,35 @@ void Simulator::DecodeRVIType() {
       break;
     }
     case RO_LB: {
-      int64_t addr = rs1() + imm12();
+      sreg_t addr = rs1() + imm12();
       int8_t val = ReadMem<int8_t>(addr, instr_.instr());
       set_rd(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
       break;
     }
     case RO_LH: {
-      int64_t addr = rs1() + imm12();
+      sreg_t addr = rs1() + imm12();
       int16_t val = ReadMem<int16_t>(addr, instr_.instr());
       set_rd(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
       break;
     }
     case RO_LW: {
-      int64_t addr = rs1() + imm12();
+      sreg_t addr = rs1() + imm12();
       int32_t val = ReadMem<int32_t>(addr, instr_.instr());
       set_rd(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
       break;
     }
     case RO_LBU: {
-      int64_t addr = rs1() + imm12();
+      sreg_t addr = rs1() + imm12();
       uint8_t val = ReadMem<uint8_t>(addr, instr_.instr());
       set_rd(zext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
       break;
     }
     case RO_LHU: {
-      int64_t addr = rs1() + imm12();
+      sreg_t addr = rs1() + imm12();
       uint16_t val = ReadMem<uint16_t>(addr, instr_.instr());
       set_rd(zext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rd_reg()));
@@ -4615,18 +4850,20 @@ void Simulator::DecodeRVIType() {
     }
     // TODO(riscv): use F Extension macro block
     case RO_FLW: {
-      int64_t addr = rs1() + imm12();
-      float val = ReadMem<float>(addr, instr_.instr());
-      set_frd(val, false);
-      TraceMemRd(addr, val, get_fpu_register(frd_reg()));
+      sreg_t addr = rs1() + imm12();
+      uint32_t val = ReadMem<uint32_t>(addr, instr_.instr());
+      set_frd(Float32::FromBits(val), false);
+      TraceMemRdFloat(addr, Float32::FromBits(val),
+                      get_fpu_register(frd_reg()));
       break;
     }
     // TODO(riscv): use D Extension macro block
     case RO_FLD: {
-      int64_t addr = rs1() + imm12();
-      double val = ReadMem<double>(addr, instr_.instr());
-      set_drd(val, false);
-      TraceMemRd(addr, val, get_fpu_register(frd_reg()));
+      sreg_t addr = rs1() + imm12();
+      uint64_t val = ReadMem<uint64_t>(addr, instr_.instr());
+      set_drd(Float64::FromBits(val), false);
+      TraceMemRdDouble(addr, Float64::FromBits(val),
+                       get_fpu_register(frd_reg()));
       break;
     }
     default: {
@@ -4660,14 +4897,14 @@ void Simulator::DecodeRVSType() {
 #endif /*V8_TARGET_ARCH_64_BIT*/
     // TODO(riscv): use F Extension macro block
     case RO_FSW: {
-      WriteMem<uint32_t>(rs1() + s_imm12(),
-                         (uint32_t)get_fpu_register_word(rs2_reg()),
-                         instr_.instr());
+      WriteMem<Float32>(rs1() + s_imm12(), get_fpu_register_Float32(rs2_reg()),
+                        instr_.instr());
       break;
     }
     // TODO(riscv): use D Extension macro block
     case RO_FSD: {
-      WriteMem<double>(rs1() + s_imm12(), drs2(), instr_.instr());
+      WriteMem<Float64>(rs1() + s_imm12(), get_fpu_register_Float64(rs2_reg()),
+                        instr_.instr());
       break;
     }
     default:
@@ -4727,10 +4964,10 @@ void Simulator::DecodeRVBType() {
 void Simulator::DecodeRVUType() {
   // U Type doesn't have additoinal mask
   switch (instr_.BaseOpcodeFieldRaw()) {
-    case RO_LUI:
+    case LUI:
       set_rd(u_imm20());
       break;
-    case RO_AUIPC:
+    case AUIPC:
       set_rd(sext_xlen(u_imm20() + get_pc()));
       break;
     default:
@@ -4740,7 +4977,7 @@ void Simulator::DecodeRVUType() {
 void Simulator::DecodeRVJType() {
   // J Type doesn't have additional mask
   switch (instr_.BaseOpcodeValue()) {
-    case RO_JAL: {
+    case JAL: {
       set_rd(get_pc() + kInstrSize);
       int64_t next_pc = get_pc() + imm20J();
       set_pc(next_pc);
@@ -4764,8 +5001,7 @@ void Simulator::DecodeCRType() {
       break;
     case 0b1001:
       if (instr_.RvcRs1Value() == 0 && instr_.RvcRs2Value() == 0) {  // c.ebreak
-        RiscvDebugger dbg(this);
-        dbg.Debug();
+        DieOrDebug();
       } else if (instr_.RvcRdValue() != 0 &&
                  instr_.RvcRs2Value() == 0) {  // c.jalr
         set_register(ra, get_pc() + kShortInstrSize);
@@ -4796,12 +5032,14 @@ void Simulator::DecodeCAType() {
     case RO_C_AND:
       set_rvc_rs1s(rvc_rs1s() & rvc_rs2s());
       break;
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_SUBW:
       set_rvc_rs1s(sext32(rvc_rs1s() - rvc_rs2s()));
       break;
     case RO_C_ADDW:
       set_rvc_rs1s(sext32(rvc_rs1s() + rvc_rs2s()));
       break;
+#endif
     default:
       UNSUPPORTED();
   }
@@ -4815,9 +5053,11 @@ void Simulator::DecodeCIType() {
       else  // c.addi
         set_rvc_rd(sext_xlen(rvc_rs1() + rvc_imm6()));
       break;
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_ADDIW:
       set_rvc_rd(sext32(rvc_rs1() + rvc_imm6()));
       break;
+#endif
     case RO_C_LI:
       set_rvc_rd(sext_xlen(rvc_imm6()));
       break;
@@ -4837,26 +5077,45 @@ void Simulator::DecodeCIType() {
       set_rvc_rd(sext_xlen(rvc_rs1() << rvc_shamt6()));
       break;
     case RO_C_FLDSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_ldsp();
-      double val = ReadMem<double>(addr, instr_.instr());
-      set_rvc_drd(val, false);
-      TraceMemRd(addr, val, get_fpu_register(rvc_frd_reg()));
+      sreg_t addr = get_register(sp) + rvc_imm6_ldsp();
+      uint64_t val = ReadMem<uint64_t>(addr, instr_.instr());
+      set_rvc_drd(Float64::FromBits(val), false);
+      TraceMemRdDouble(addr, Float64::FromBits(val),
+                       get_fpu_register(rvc_frd_reg()));
       break;
     }
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_LWSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_lwsp();
+      sreg_t addr = get_register(sp) + rvc_imm6_lwsp();
       int64_t val = ReadMem<int32_t>(addr, instr_.instr());
       set_rvc_rd(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rvc_rd_reg()));
       break;
     }
     case RO_C_LDSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_ldsp();
+      sreg_t addr = get_register(sp) + rvc_imm6_ldsp();
       int64_t val = ReadMem<int64_t>(addr, instr_.instr());
       set_rvc_rd(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rvc_rd_reg()));
       break;
     }
+#elif V8_TARGET_ARCH_32_BIT
+    case RO_C_FLWSP: {
+      sreg_t addr = get_register(sp) + rvc_imm6_ldsp();
+      uint32_t val = ReadMem<uint32_t>(addr, instr_.instr());
+      set_rvc_frd(Float32::FromBits(val), false);
+      TraceMemRdFloat(addr, Float32::FromBits(val),
+                      get_fpu_register(rvc_frd_reg()));
+      break;
+    }
+    case RO_C_LWSP: {
+      sreg_t addr = get_register(sp) + rvc_imm6_lwsp();
+      int32_t val = ReadMem<int32_t>(addr, instr_.instr());
+      set_rvc_rd(sext_xlen(val), false);
+      TraceMemRd(addr, val, get_register(rvc_rd_reg()));
+      break;
+    }
+#endif
     default:
       UNSUPPORTED();
   }
@@ -4876,20 +5135,31 @@ void Simulator::DecodeCIWType() {
 void Simulator::DecodeCSSType() {
   switch (instr_.RvcOpcode()) {
     case RO_C_FSDSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_sdsp();
-      WriteMem<double>(addr, static_cast<double>(rvc_drs2()), instr_.instr());
+      sreg_t addr = get_register(sp) + rvc_imm6_sdsp();
+      WriteMem<Float64>(addr, get_fpu_register_Float64(rvc_rs2_reg()),
+                        instr_.instr());
       break;
     }
+#if V8_TARGET_ARCH_32_BIT
+    case RO_C_FSWSP: {
+      sreg_t addr = get_register(sp) + rvc_imm6_sdsp();
+      WriteMem<Float32>(addr, get_fpu_register_Float32(rvc_rs2_reg()),
+                        instr_.instr());
+      break;
+    }
+#endif
     case RO_C_SWSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_swsp();
+      sreg_t addr = get_register(sp) + rvc_imm6_swsp();
       WriteMem<int32_t>(addr, (int32_t)rvc_rs2(), instr_.instr());
       break;
     }
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_SDSP: {
-      int64_t addr = get_register(sp) + rvc_imm6_sdsp();
+      sreg_t addr = get_register(sp) + rvc_imm6_sdsp();
       WriteMem<int64_t>(addr, (int64_t)rvc_rs2(), instr_.instr());
       break;
     }
+#endif
     default:
       UNSUPPORTED();
   }
@@ -4898,25 +5168,34 @@ void Simulator::DecodeCSSType() {
 void Simulator::DecodeCLType() {
   switch (instr_.RvcOpcode()) {
     case RO_C_LW: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_w();
+      sreg_t addr = rvc_rs1s() + rvc_imm5_w();
       int64_t val = ReadMem<int32_t>(addr, instr_.instr());
       set_rvc_rs2s(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rvc_rs2s_reg()));
       break;
     }
+    case RO_C_FLD: {
+      sreg_t addr = rvc_rs1s() + rvc_imm5_d();
+      uint64_t val = ReadMem<uint64_t>(addr, instr_.instr());
+      set_rvc_drs2s(Float64::FromBits(val), false);
+      break;
+    }
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_LD: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_d();
+      sreg_t addr = rvc_rs1s() + rvc_imm5_d();
       int64_t val = ReadMem<int64_t>(addr, instr_.instr());
       set_rvc_rs2s(sext_xlen(val), false);
       TraceMemRd(addr, val, get_register(rvc_rs2s_reg()));
       break;
     }
-    case RO_C_FLD: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_d();
-      double val = ReadMem<double>(addr, instr_.instr());
-      set_rvc_drs2s(val, false);
+#elif V8_TARGET_ARCH_32_BIT
+    case RO_C_FLW: {
+      sreg_t addr = rvc_rs1s() + rvc_imm5_d();
+      uint32_t val = ReadMem<uint32_t>(addr, instr_.instr());
+      set_rvc_frs2s(Float32::FromBits(val), false);
       break;
     }
+#endif
     default:
       UNSUPPORTED();
   }
@@ -4925,17 +5204,19 @@ void Simulator::DecodeCLType() {
 void Simulator::DecodeCSType() {
   switch (instr_.RvcOpcode()) {
     case RO_C_SW: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_w();
+      sreg_t addr = rvc_rs1s() + rvc_imm5_w();
       WriteMem<int32_t>(addr, (int32_t)rvc_rs2s(), instr_.instr());
       break;
     }
+#if V8_TARGET_ARCH_64_BIT
     case RO_C_SD: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_d();
+      sreg_t addr = rvc_rs1s() + rvc_imm5_d();
       WriteMem<int64_t>(addr, (int64_t)rvc_rs2s(), instr_.instr());
       break;
     }
+#endif
     case RO_C_FSD: {
-      int64_t addr = rvc_rs1s() + rvc_imm5_d();
+      sreg_t addr = rvc_rs1s() + rvc_imm5_d();
       WriteMem<double>(addr, static_cast<double>(rvc_drs2s()), instr_.instr());
       break;
     }
@@ -4959,13 +5240,13 @@ void Simulator::DecodeCBType() {
   switch (instr_.RvcOpcode()) {
     case RO_C_BNEZ:
       if (rvc_rs1() != 0) {
-        int64_t next_pc = get_pc() + rvc_imm8_b();
+        sreg_t next_pc = get_pc() + rvc_imm8_b();
         set_pc(next_pc);
       }
       break;
     case RO_C_BEQZ:
       if (rvc_rs1() == 0) {
-        int64_t next_pc = get_pc() + rvc_imm8_b();
+        sreg_t next_pc = get_pc() + rvc_imm8_b();
         set_pc(next_pc);
       }
       break;
@@ -6954,18 +7235,42 @@ void Simulator::InstructionDecode(Instruction* instr) {
 
   if (!pc_modified_) {
     set_register(pc,
-                 reinterpret_cast<int64_t>(instr) + instr->InstructionSize());
+                 reinterpret_cast<sreg_t>(instr) + instr->InstructionSize());
+  }
+
+  if (watch_address_ != nullptr) {
+    PrintF("  0x%012" PRIxPTR " :  0x%016" REGIx_FORMAT "  %14" REGId_FORMAT
+           " ",
+           reinterpret_cast<intptr_t>(watch_address_), *watch_address_,
+           *watch_address_);
+    Object obj(*watch_address_);
+    Heap* current_heap = isolate_->heap();
+    if (obj.IsSmi() || IsValidHeapObject(current_heap, HeapObject::cast(obj))) {
+      PrintF(" (");
+      if (obj.IsSmi()) {
+        PrintF("smi %d", Smi::ToInt(obj));
+      } else {
+        obj.ShortPrint();
+      }
+      PrintF(")");
+    }
+    PrintF("\n");
+    if (watch_value_ != *watch_address_) {
+      RiscvDebugger dbg(this);
+      dbg.Debug();
+      watch_value_ = *watch_address_;
+    }
   }
 }
 
 void Simulator::Execute() {
   // Get the PC to simulate. Cannot use the accessor here as we need the
   // raw PC value and not the one used as input to arithmetic instructions.
-  int64_t program_counter = get_pc();
+  sreg_t program_counter = get_pc();
   while (program_counter != end_sim_pc) {
     Instruction* instr = reinterpret_cast<Instruction*>(program_counter);
     icount_++;
-    if (icount_ == static_cast<int64_t>(::v8::internal::FLAG_stop_sim_at)) {
+    if (icount_ == static_cast<sreg_t>(::v8::internal::FLAG_stop_sim_at)) {
       RiscvDebugger dbg(this);
       dbg.Debug();
     } else {
@@ -6981,32 +7286,32 @@ void Simulator::CallInternal(Address entry) {
   isolate_->stack_guard()->AdjustStackLimitForSimulator();
 
   // Prepare to execute the code at entry.
-  set_register(pc, static_cast<int64_t>(entry));
+  set_register(pc, static_cast<sreg_t>(entry));
   // Put down marker for end of simulation. The simulator will stop simulation
   // when the PC reaches this value. By saving the "end simulation" value into
   // the LR the simulation stops when returning to this call point.
   set_register(ra, end_sim_pc);
 
   // Remember the values of callee-saved registers.
-  int64_t s0_val = get_register(s0);
-  int64_t s1_val = get_register(s1);
-  int64_t s2_val = get_register(s2);
-  int64_t s3_val = get_register(s3);
-  int64_t s4_val = get_register(s4);
-  int64_t s5_val = get_register(s5);
-  int64_t s6_val = get_register(s6);
-  int64_t s7_val = get_register(s7);
-  int64_t s8_val = get_register(s8);
-  int64_t s9_val = get_register(s9);
-  int64_t s10_val = get_register(s10);
-  int64_t s11_val = get_register(s11);
-  int64_t gp_val = get_register(gp);
-  int64_t sp_val = get_register(sp);
+  sreg_t s0_val = get_register(s0);
+  sreg_t s1_val = get_register(s1);
+  sreg_t s2_val = get_register(s2);
+  sreg_t s3_val = get_register(s3);
+  sreg_t s4_val = get_register(s4);
+  sreg_t s5_val = get_register(s5);
+  sreg_t s6_val = get_register(s6);
+  sreg_t s7_val = get_register(s7);
+  sreg_t s8_val = get_register(s8);
+  sreg_t s9_val = get_register(s9);
+  sreg_t s10_val = get_register(s10);
+  sreg_t s11_val = get_register(s11);
+  sreg_t gp_val = get_register(gp);
+  sreg_t sp_val = get_register(sp);
 
   // Set up the callee-saved registers with a known value. To be able to check
   // that they are preserved properly across JS execution. If this value is
   // small int, it should be SMI.
-  int64_t callee_saved_value = icount_ << (kSmiTagSize + kSmiShiftSize);
+  sreg_t callee_saved_value = icount_ != 0 ? icount_ & ~kSmiTagMask : -1;
   set_register(s0, callee_saved_value);
   set_register(s1, callee_saved_value);
   set_register(s2, callee_saved_value);
@@ -7084,11 +7389,11 @@ intptr_t Simulator::CallImpl(Address entry, int argument_count,
   }
 
   // Remaining arguments passed on stack.
-  int64_t original_stack = get_register(sp);
+  sreg_t original_stack = get_register(sp);
   // Compute position of stack on entry to generated code.
   int stack_args_count = argument_count - reg_arg_count;
   int stack_args_size = stack_args_count * sizeof(*arguments) + kCArgsSlotsSize;
-  int64_t entry_stack = original_stack - stack_args_size;
+  sreg_t entry_stack = original_stack - stack_args_size;
 
   if (base::OS::ActivationFrameAlignment() != 0) {
     entry_stack &= -base::OS::ActivationFrameAlignment();
diff --git a/src/execution/riscv64/simulator-riscv64.h b/src/execution/riscv/simulator-riscv.h
similarity index 82%
rename from src/execution/riscv64/simulator-riscv64.h
rename to src/execution/riscv/simulator-riscv.h
index 90235ef9f23..8119340f6bd 100644
--- a/src/execution/riscv64/simulator-riscv64.h
+++ b/src/execution/riscv/simulator-riscv.h
@@ -46,8 +46,8 @@
 // GeneratedCode wrapper, which will start execution in the Simulator or
 // forwards to the real entry on a RISC-V HW platform.
 
-#ifndef V8_EXECUTION_RISCV64_SIMULATOR_RISCV64_H_
-#define V8_EXECUTION_RISCV64_SIMULATOR_RISCV64_H_
+#ifndef V8_EXECUTION_RISCV_SIMULATOR_RISCV_H_
+#define V8_EXECUTION_RISCV_SIMULATOR_RISCV_H_
 
 // globals.h defines USE_SIMULATOR.
 #include "src/common/globals.h"
@@ -74,9 +74,10 @@ T Nabs(T a) {
 
 #include "src/base/hashmap.h"
 #include "src/codegen/assembler.h"
-#include "src/codegen/riscv64/constants-riscv64.h"
+#include "src/codegen/constants-arch.h"
 #include "src/execution/simulator-base.h"
 #include "src/utils/allocation.h"
+#include "src/utils/boxed-float.h"
 
 namespace v8 {
 namespace internal {
@@ -86,19 +87,27 @@ namespace internal {
 #ifdef V8_TARGET_ARCH_32_BIT
 using sreg_t = int32_t;
 using reg_t = uint32_t;
-#define xlen 32
+using freg_t = uint64_t;
+using sfreg_t = int64_t;
 #elif V8_TARGET_ARCH_64_BIT
 using sreg_t = int64_t;
 using reg_t = uint64_t;
-#define xlen 64
+using freg_t = uint64_t;
+using sfreg_t = int64_t;
 #else
 #error "Cannot detect Riscv's bitwidth"
 #endif
 
 #define sext32(x) ((sreg_t)(int32_t)(x))
 #define zext32(x) ((reg_t)(uint32_t)(x))
+
+#ifdef V8_TARGET_ARCH_64_BIT
 #define sext_xlen(x) (((sreg_t)(x) << (64 - xlen)) >> (64 - xlen))
 #define zext_xlen(x) (((reg_t)(x) << (64 - xlen)) >> (64 - xlen))
+#elif V8_TARGET_ARCH_32_BIT
+#define sext_xlen(x) (((sreg_t)(x) << (32 - xlen)) >> (32 - xlen))
+#define zext_xlen(x) (((reg_t)(x) << (32 - xlen)) >> (32 - xlen))
+#endif
 
 #define BIT(n) (0x1LL << n)
 #define QUIET_BIT_S(nan) (base::bit_cast<int32_t>(nan) & BIT(22))
@@ -108,6 +117,7 @@ static inline bool isSnan(double fp) { return !QUIET_BIT_D(fp); }
 #undef QUIET_BIT_S
 #undef QUIET_BIT_D
 
+#ifdef V8_TARGET_ARCH_64_BIT
 inline uint64_t mulhu(uint64_t a, uint64_t b) {
   __uint128_t full_result = ((__uint128_t)a) * ((__uint128_t)b);
   return full_result >> 64;
@@ -122,6 +132,25 @@ inline int64_t mulhsu(int64_t a, uint64_t b) {
   __int128_t full_result = ((__int128_t)a) * ((__uint128_t)b);
   return full_result >> 64;
 }
+#elif V8_TARGET_ARCH_32_BIT
+inline uint32_t mulhu(uint32_t a, uint32_t b) {
+  uint64_t full_result = ((uint64_t)a) * ((uint64_t)b);
+  uint64_t upper_part = full_result >> 32;
+  return (uint32_t)upper_part;
+}
+
+inline int32_t mulh(int32_t a, int32_t b) {
+  int64_t full_result = ((int64_t)a) * ((int64_t)b);
+  int64_t upper_part = full_result >> 32;
+  return (int32_t)upper_part;
+}
+
+inline int32_t mulhsu(int32_t a, uint32_t b) {
+  int64_t full_result = ((int64_t)a) * ((uint64_t)b);
+  int64_t upper_part = full_result >> 32;
+  return (int32_t)upper_part;
+}
+#endif
 
 // Floating point helpers
 #define F32_SIGN ((uint32_t)1 << 31)
@@ -139,6 +168,21 @@ inline float fsgnj32(float rs1, float rs2, bool n, bool x) {
                                F32_SIGN);
   return res.f;
 }
+
+inline Float32 fsgnj32(Float32 rs1, Float32 rs2, bool n, bool x) {
+  u32_f32 a = {.u = rs1.get_bits()}, b = {.u = rs2.get_bits()};
+  u32_f32 res;
+  if (x) {  // RO_FSQNJX_S
+    res.u = (a.u & ~F32_SIGN) | ((a.u ^ b.u) & F32_SIGN);
+  } else {
+    if (n) {  // RO_FSGNJN_S
+      res.u = (a.u & ~F32_SIGN) | ((F32_SIGN ^ b.u) & F32_SIGN);
+    } else {  // RO_FSGNJ_S
+      res.u = (a.u & ~F32_SIGN) | ((0 ^ b.u) & F32_SIGN);
+    }
+  }
+  return Float32::FromBits(res.u);
+}
 #define F64_SIGN ((uint64_t)1 << 63)
 union u64_f64 {
   uint64_t u;
@@ -155,11 +199,27 @@ inline double fsgnj64(double rs1, double rs2, bool n, bool x) {
   return res.d;
 }
 
+inline Float64 fsgnj64(Float64 rs1, Float64 rs2, bool n, bool x) {
+  u64_f64 a = {.d = rs1.get_scalar()}, b = {.d = rs2.get_scalar()};
+  u64_f64 res;
+  if (x) {  // RO_FSQNJX_D
+    res.u = (a.u & ~F64_SIGN) | ((a.u ^ b.u) & F64_SIGN);
+  } else {
+    if (n) {  // RO_FSGNJN_D
+      res.u = (a.u & ~F64_SIGN) | ((F64_SIGN ^ b.u) & F64_SIGN);
+    } else {  // RO_FSGNJ_D
+      res.u = (a.u & ~F64_SIGN) | ((0 ^ b.u) & F64_SIGN);
+    }
+  }
+  return Float64::FromBits(res.u);
+}
 inline bool is_boxed_float(int64_t v) { return (uint32_t)((v >> 32) + 1) == 0; }
 inline int64_t box_float(float v) {
   return (0xFFFFFFFF00000000 | base::bit_cast<int32_t>(v));
 }
 
+inline uint64_t box_float(uint32_t v) { return (0xFFFFFFFF00000000 | v); }
+
 // -----------------------------------------------------------------------------
 // Utility functions
 
@@ -351,10 +411,9 @@ class Simulator : public SimulatorBase {
   // Accessors for register state. Reading the pc value adheres to the RISC-V
   // architecture specification and is off by a 8 from the currently executing
   // instruction.
-  void set_register(int reg, int64_t value);
+  void set_register(int reg, sreg_t value);
   void set_register_word(int reg, int32_t value);
-  void set_dw_register(int dreg, const int* dbl);
-  V8_EXPORT_PRIVATE int64_t get_register(int reg) const;
+  V8_EXPORT_PRIVATE sreg_t get_register(int reg) const;
   double get_double_from_register_pair(int reg);
 
   // Same for FPURegisters.
@@ -362,20 +421,24 @@ class Simulator : public SimulatorBase {
   void set_fpu_register_word(int fpureg, int32_t value);
   void set_fpu_register_hi_word(int fpureg, int32_t value);
   void set_fpu_register_float(int fpureg, float value);
+  void set_fpu_register_float(int fpureg, Float32 value);
   void set_fpu_register_double(int fpureg, double value);
+  void set_fpu_register_double(int fpureg, Float64 value);
 
   int64_t get_fpu_register(int fpureg) const;
   int32_t get_fpu_register_word(int fpureg) const;
   int32_t get_fpu_register_signed_word(int fpureg) const;
   int32_t get_fpu_register_hi_word(int fpureg) const;
   float get_fpu_register_float(int fpureg) const;
+  Float32 get_fpu_register_Float32(int fpureg) const;
   double get_fpu_register_double(int fpureg) const;
+  Float64 get_fpu_register_Float64(int fpureg) const;
 
   // RV CSR manipulation
   uint32_t read_csr_value(uint32_t csr);
-  void write_csr_value(uint32_t csr, uint64_t value);
-  void set_csr_bits(uint32_t csr, uint64_t flags);
-  void clear_csr_bits(uint32_t csr, uint64_t flags);
+  void write_csr_value(uint32_t csr, reg_t value);
+  void set_csr_bits(uint32_t csr, reg_t flags);
+  void clear_csr_bits(uint32_t csr, reg_t flags);
 
   void set_fflags(uint32_t flags) { set_csr_bits(csr_fflags, flags); }
   void clear_fflags(int32_t flags) { clear_csr_bits(csr_fflags, flags); }
@@ -457,8 +520,8 @@ class Simulator : public SimulatorBase {
   bool CompareFHelper(T input1, T input2, FPUCondition cc);
 
   // Special case of set_register and get_register to access the raw PC value.
-  void set_pc(int64_t value);
-  V8_EXPORT_PRIVATE int64_t get_pc() const;
+  void set_pc(sreg_t value);
+  V8_EXPORT_PRIVATE sreg_t get_pc() const;
 
   Address get_sp() const { return static_cast<Address>(get_register(sp)); }
 
@@ -523,7 +586,9 @@ class Simulator : public SimulatorBase {
     BYTE,
     HALF,
     WORD,
+#if V8_TARGET_ARCH_RISCV64
     DWORD,
+#endif
     FLOAT,
     DOUBLE,
     // FLOAT_DOUBLE,
@@ -532,11 +597,11 @@ class Simulator : public SimulatorBase {
 
   // RISCV Memory read/write methods
   template <typename T>
-  T ReadMem(int64_t addr, Instruction* instr);
+  T ReadMem(sreg_t addr, Instruction* instr);
   template <typename T>
-  void WriteMem(int64_t addr, T value, Instruction* instr);
+  void WriteMem(sreg_t addr, T value, Instruction* instr);
   template <typename T, typename OP>
-  T amo(int64_t addr, OP f, Instruction* instr, TraceType t) {
+  T amo(sreg_t addr, OP f, Instruction* instr, TraceType t) {
     auto lhs = ReadMem<T>(addr, instr);
     // TODO(RISCV): trace memory read for AMO
     WriteMem<T>(addr, (T)f(lhs), instr);
@@ -546,41 +611,69 @@ class Simulator : public SimulatorBase {
   // Helper for debugging memory access.
   inline void DieOrDebug();
 
-  void TraceRegWr(int64_t value, TraceType t = DWORD);
-  void TraceMemWr(int64_t addr, int64_t value, TraceType t);
+#if V8_TARGET_ARCH_RISCV32
   template <typename T>
-  void TraceMemRd(int64_t addr, T value, int64_t reg_value);
+  void TraceRegWr(T value, TraceType t = WORD);
+#elif V8_TARGET_ARCH_RISCV64
+  void TraceRegWr(sreg_t value, TraceType t = DWORD);
+#endif
+  void TraceMemWr(sreg_t addr, sreg_t value, TraceType t);
+  template <typename T>
+  void TraceMemRd(sreg_t addr, T value, sreg_t reg_value);
+  void TraceMemRdDouble(sreg_t addr, double value, int64_t reg_value);
+  void TraceMemRdDouble(sreg_t addr, Float64 value, int64_t reg_value);
+  void TraceMemRdFloat(sreg_t addr, Float32 value, int64_t reg_value);
+
   template <typename T>
-  void TraceMemWr(int64_t addr, T value);
+  void TraceMemWr(sreg_t addr, T value);
+  void TraceMemWrDouble(sreg_t addr, double value);
 
   SimInstruction instr_;
 
   // RISCV utlity API to access register value
   inline int32_t rs1_reg() const { return instr_.Rs1Value(); }
-  inline int64_t rs1() const { return get_register(rs1_reg()); }
+  inline sreg_t rs1() const { return get_register(rs1_reg()); }
   inline float frs1() const { return get_fpu_register_float(rs1_reg()); }
   inline double drs1() const { return get_fpu_register_double(rs1_reg()); }
+  inline Float32 frs1_boxed() const {
+    return get_fpu_register_Float32(rs1_reg());
+  }
+  inline Float64 drs1_boxed() const {
+    return get_fpu_register_Float64(rs1_reg());
+  }
   inline int32_t rs2_reg() const { return instr_.Rs2Value(); }
-  inline int64_t rs2() const { return get_register(rs2_reg()); }
+  inline sreg_t rs2() const { return get_register(rs2_reg()); }
   inline float frs2() const { return get_fpu_register_float(rs2_reg()); }
   inline double drs2() const { return get_fpu_register_double(rs2_reg()); }
+  inline Float32 frs2_boxed() const {
+    return get_fpu_register_Float32(rs2_reg());
+  }
+  inline Float64 drs2_boxed() const {
+    return get_fpu_register_Float64(rs2_reg());
+  }
   inline int32_t rs3_reg() const { return instr_.Rs3Value(); }
-  inline int64_t rs3() const { return get_register(rs3_reg()); }
+  inline sreg_t rs3() const { return get_register(rs3_reg()); }
   inline float frs3() const { return get_fpu_register_float(rs3_reg()); }
   inline double drs3() const { return get_fpu_register_double(rs3_reg()); }
+  inline Float32 frs3_boxed() const {
+    return get_fpu_register_Float32(rs3_reg());
+  }
+  inline Float64 drs3_boxed() const {
+    return get_fpu_register_Float64(rs3_reg());
+  }
   inline int32_t rd_reg() const { return instr_.RdValue(); }
   inline int32_t frd_reg() const { return instr_.RdValue(); }
   inline int32_t rvc_rs1_reg() const { return instr_.RvcRs1Value(); }
-  inline int64_t rvc_rs1() const { return get_register(rvc_rs1_reg()); }
+  inline sreg_t rvc_rs1() const { return get_register(rvc_rs1_reg()); }
   inline int32_t rvc_rs2_reg() const { return instr_.RvcRs2Value(); }
-  inline int64_t rvc_rs2() const { return get_register(rvc_rs2_reg()); }
+  inline sreg_t rvc_rs2() const { return get_register(rvc_rs2_reg()); }
   inline double rvc_drs2() const {
     return get_fpu_register_double(rvc_rs2_reg());
   }
   inline int32_t rvc_rs1s_reg() const { return instr_.RvcRs1sValue(); }
-  inline int64_t rvc_rs1s() const { return get_register(rvc_rs1s_reg()); }
+  inline sreg_t rvc_rs1s() const { return get_register(rvc_rs1s_reg()); }
   inline int32_t rvc_rs2s_reg() const { return instr_.RvcRs2sValue(); }
-  inline int64_t rvc_rs2s() const { return get_register(rvc_rs2s_reg()); }
+  inline sreg_t rvc_rs2s() const { return get_register(rvc_rs2s_reg()); }
   inline double rvc_drs2s() const {
     return get_fpu_register_double(rvc_rs2s_reg());
   }
@@ -606,42 +699,87 @@ class Simulator : public SimulatorBase {
   inline int16_t rvc_imm5_d() const { return instr_.RvcImm5DValue(); }
   inline int16_t rvc_imm8_b() const { return instr_.RvcImm8BValue(); }
 
-  inline void set_rd(int64_t value, bool trace = true) {
+  inline void set_rd(sreg_t value, bool trace = true) {
     set_register(rd_reg(), value);
+#if V8_TARGET_ARCH_RISCV64
     if (trace) TraceRegWr(get_register(rd_reg()), DWORD);
+#elif V8_TARGET_ARCH_RISCV32
+    if (trace) TraceRegWr(get_register(rd_reg()), WORD);
+#endif
   }
   inline void set_frd(float value, bool trace = true) {
     set_fpu_register_float(rd_reg(), value);
     if (trace) TraceRegWr(get_fpu_register_word(rd_reg()), FLOAT);
   }
+  inline void set_frd(Float32 value, bool trace = true) {
+    set_fpu_register_float(rd_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register_word(rd_reg()), FLOAT);
+  }
   inline void set_drd(double value, bool trace = true) {
     set_fpu_register_double(rd_reg(), value);
     if (trace) TraceRegWr(get_fpu_register(rd_reg()), DOUBLE);
   }
-  inline void set_rvc_rd(int64_t value, bool trace = true) {
+  inline void set_drd(Float64 value, bool trace = true) {
+    set_fpu_register_double(rd_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register(rd_reg()), DOUBLE);
+  }
+  inline void set_rvc_rd(sreg_t value, bool trace = true) {
     set_register(rvc_rd_reg(), value);
+#if V8_TARGET_ARCH_RISCV64
     if (trace) TraceRegWr(get_register(rvc_rd_reg()), DWORD);
+#elif V8_TARGET_ARCH_RISCV32
+    if (trace) TraceRegWr(get_register(rvc_rd_reg()), WORD);
+#endif
   }
-  inline void set_rvc_rs1s(int64_t value, bool trace = true) {
+  inline void set_rvc_rs1s(sreg_t value, bool trace = true) {
     set_register(rvc_rs1s_reg(), value);
+#if V8_TARGET_ARCH_RISCV64
     if (trace) TraceRegWr(get_register(rvc_rs1s_reg()), DWORD);
+#elif V8_TARGET_ARCH_RISCV32
+    if (trace) TraceRegWr(get_register(rvc_rs1s_reg()), WORD);
+#endif
   }
-  inline void set_rvc_rs2(int64_t value, bool trace = true) {
+  inline void set_rvc_rs2(sreg_t value, bool trace = true) {
     set_register(rvc_rs2_reg(), value);
+#if V8_TARGET_ARCH_RISCV64
     if (trace) TraceRegWr(get_register(rvc_rs2_reg()), DWORD);
+#elif V8_TARGET_ARCH_RISCV32
+    if (trace) TraceRegWr(get_register(rvc_rs2_reg()), WORD);
+#endif
   }
   inline void set_rvc_drd(double value, bool trace = true) {
     set_fpu_register_double(rvc_rd_reg(), value);
     if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);
   }
-  inline void set_rvc_rs2s(int64_t value, bool trace = true) {
+  inline void set_rvc_drd(Float64 value, bool trace = true) {
+    set_fpu_register_double(rvc_rd_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);
+  }
+  inline void set_rvc_frd(Float32 value, bool trace = true) {
+    set_fpu_register_float(rvc_rd_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register(rvc_rd_reg()), DOUBLE);
+  }
+  inline void set_rvc_rs2s(sreg_t value, bool trace = true) {
     set_register(rvc_rs2s_reg(), value);
+#if V8_TARGET_ARCH_RISCV64
     if (trace) TraceRegWr(get_register(rvc_rs2s_reg()), DWORD);
+#elif V8_TARGET_ARCH_RISCV32
+    if (trace) TraceRegWr(get_register(rvc_rs2s_reg()), WORD);
+#endif
   }
   inline void set_rvc_drs2s(double value, bool trace = true) {
     set_fpu_register_double(rvc_rs2s_reg(), value);
     if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), DOUBLE);
   }
+  inline void set_rvc_drs2s(Float64 value, bool trace = true) {
+    set_fpu_register_double(rvc_rs2s_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), DOUBLE);
+  }
+
+  inline void set_rvc_frs2s(Float32 value, bool trace = true) {
+    set_fpu_register_float(rvc_rs2s_reg(), value);
+    if (trace) TraceRegWr(get_fpu_register(rvc_rs2s_reg()), FLOAT);
+  }
   inline int16_t shamt6() const { return (imm12() & 0x3F); }
   inline int16_t shamt5() const { return (imm12() & 0x1F); }
   inline int16_t rvc_shamt6() const { return instr_.RvcShamt6(); }
@@ -896,15 +1034,15 @@ class Simulator : public SimulatorBase {
   void CheckBreakpoints();
 
   // Stop helper functions.
-  bool IsWatchpoint(uint64_t code);
-  void PrintWatchpoint(uint64_t code);
-  void HandleStop(uint64_t code);
+  bool IsWatchpoint(reg_t code);
+  void PrintWatchpoint(reg_t code);
+  void HandleStop(reg_t code);
   bool IsStopInstruction(Instruction* instr);
-  bool IsEnabledStop(uint64_t code);
-  void EnableStop(uint64_t code);
-  void DisableStop(uint64_t code);
-  void IncreaseStopCounter(uint64_t code);
-  void PrintStopInfo(uint64_t code);
+  bool IsEnabledStop(reg_t code);
+  void EnableStop(reg_t code);
+  void DisableStop(reg_t code);
+  void IncreaseStopCounter(reg_t code);
+  void PrintStopInfo(reg_t code);
 
   // Executes one instruction.
   void InstructionDecode(Instruction* instr);
@@ -938,9 +1076,9 @@ class Simulator : public SimulatorBase {
 
   // Architecture state.
   // Registers.
-  int64_t registers_[kNumSimuRegisters];
+  sreg_t registers_[kNumSimuRegisters];
   // Coprocessor Registers.
-  int64_t FPUregisters_[kNumFPURegisters];
+  sfreg_t FPUregisters_[kNumFPURegisters];
   // Floating-point control and status register.
   uint32_t FCSR_;
 
@@ -956,6 +1094,8 @@ class Simulator : public SimulatorBase {
   char* stack_;
   bool pc_modified_;
   int64_t icount_;
+  sreg_t* watch_address_ = nullptr;
+  sreg_t watch_value_ = 0;
   int break_count_;
   base::EmbeddedVector<char, 256> trace_buf_;
 
@@ -1074,4 +1214,4 @@ class Simulator : public SimulatorBase {
 }  // namespace v8
 
 #endif  // defined(USE_SIMULATOR)
-#endif  // V8_EXECUTION_RISCV64_SIMULATOR_RISCV64_H_
+#endif  // V8_EXECUTION_RISCV_SIMULATOR_RISCV_H_
diff --git a/src/execution/simulator-base.h b/src/execution/simulator-base.h
index 84c257e658a..27633872757 100644
--- a/src/execution/simulator-base.h
+++ b/src/execution/simulator-base.h
@@ -96,7 +96,8 @@ class SimulatorBase {
   static typename std::enable_if<std::is_integral<T>::value, intptr_t>::type
   ConvertArg(T arg) {
     static_assert(sizeof(T) <= sizeof(intptr_t), "type bigger than ptrsize");
-#if V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_LOONG64
+#if V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_LOONG64 || \
+    V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
     // The MIPS64, LOONG64 and RISCV64 calling convention is to sign extend all
     // values, even unsigned ones.
     using signed_t = typename std::make_signed<T>::type;
diff --git a/src/execution/simulator.h b/src/execution/simulator.h
index 6b6b845e1eb..1f93e1b6e2c 100644
--- a/src/execution/simulator.h
+++ b/src/execution/simulator.h
@@ -28,8 +28,8 @@
 #include "src/execution/loong64/simulator-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/execution/s390/simulator-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/execution/riscv64/simulator-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/execution/riscv/simulator-riscv.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/flags/flag-definitions.h b/src/flags/flag-definitions.h
index f292e17f92d..7b03d2ed55f 100644
--- a/src/flags/flag-definitions.h
+++ b/src/flags/flag-definitions.h
@@ -1512,7 +1512,7 @@ DEFINE_STRING(sim_arm64_optional_features, "none",
               "enable optional features on the simulator for testing: none or "
               "all")
 
-#if defined(V8_TARGET_ARCH_RISCV64)
+#if defined(V8_TARGET_ARCH_RISCV32) || defined(V8_TARGET_ARCH_RISCV64)
 DEFINE_BOOL(riscv_trap_to_simulator_debugger, false,
             "enable simulator trap to debugger")
 DEFINE_BOOL(riscv_debug, false, "enable debug prints")
diff --git a/src/heap/base/asm/riscv64/push_registers_asm.cc b/src/heap/base/asm/riscv/push_registers_asm.cc
similarity index 54%
rename from src/heap/base/asm/riscv64/push_registers_asm.cc
rename to src/heap/base/asm/riscv/push_registers_asm.cc
index 2d90aab1829..7cc13ea3f36 100644
--- a/src/heap/base/asm/riscv64/push_registers_asm.cc
+++ b/src/heap/base/asm/riscv/push_registers_asm.cc
@@ -10,6 +10,7 @@
 //
 // Calling convention source:
 // https://riscv.org/wp-content/uploads/2015/01/riscv-calling.pdf Table 18.2
+#ifdef V8_TARGET_ARCH_RISCV64
 asm(".global PushAllRegistersAndIterateStack             \n"
     ".type PushAllRegistersAndIterateStack, %function    \n"
     ".hidden PushAllRegistersAndIterateStack             \n"
@@ -49,3 +50,44 @@ asm(".global PushAllRegistersAndIterateStack             \n"
     "  ld s0, 0(sp)                                      \n"
     "  addi sp, sp, 112                                  \n"
     "  jr ra                                             \n");
+#elif V8_TARGET_ARCH_RISCV32
+asm(".global PushAllRegistersAndIterateStack             \n"
+    ".type PushAllRegistersAndIterateStack, %function    \n"
+    ".hidden PushAllRegistersAndIterateStack             \n"
+    "PushAllRegistersAndIterateStack:                    \n"
+    // Push all callee-saved registers and save return address.
+    "  addi sp, sp, -56                                  \n"
+    // Save return address.
+    "  sw ra, 52(sp)                                     \n"
+    // sp is callee-saved.
+    "  sw sp, 48(sp)                                     \n"
+    // s0-s11 are callee-saved.
+    "  sw s11, 44(sp)                                    \n"
+    "  sw s10, 40(sp)                                    \n"
+    "  sw s9, 36(sp)                                     \n"
+    "  sw s8, 32(sp)                                     \n"
+    "  sw s7, 28(sp)                                     \n"
+    "  sw s6, 24(sp)                                     \n"
+    "  sw s5, 20(sp)                                     \n"
+    "  sw s4, 16(sp)                                     \n"
+    "  sw s3, 12(sp)                                     \n"
+    "  sw s2, 8(sp)                                      \n"
+    "  sw s1,  4(sp)                                     \n"
+    "  sw s0,  0(sp)                                     \n"
+    // Maintain frame pointer(fp is s0).
+    "  mv s0, sp                                         \n"
+    // Pass 1st parameter (a0) unchanged (Stack*).
+    // Pass 2nd parameter (a1) unchanged (StackVisitor*).
+    // Save 3rd parameter (a2; IterateStackCallback) to a3.
+    "  mv a3, a2                                         \n"
+    // Pass 3rd parameter as sp (stack pointer).
+    "  mv a2, sp                                         \n"
+    // Call the callback.
+    "  jalr a3                                           \n"
+    // Load return address.
+    "  lw ra, 52(sp)                                     \n"
+    // Restore frame pointer.
+    "  lw s0, 0(sp)                                      \n"
+    "  addi sp, sp, 56                                   \n"
+    "  jr ra                                             \n");
+#endif
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index 0ab530934a6..fb940323d07 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -1448,7 +1448,8 @@ void InterpreterAssembler::TraceBytecodeDispatch(TNode<WordT> target_bytecode) {
 
 // static
 bool InterpreterAssembler::TargetSupportsUnalignedAccess() {
-#if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_RISCV64
+#if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_RISCV64 || \
+    V8_TARGET_ARCH_RISCV32
   return false;
 #elif V8_TARGET_ARCH_IA32 || V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_S390 || \
     V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_PPC ||   \
diff --git a/src/libsampler/sampler.cc b/src/libsampler/sampler.cc
index 46605291c68..f3516e3dd4a 100644
--- a/src/libsampler/sampler.cc
+++ b/src/libsampler/sampler.cc
@@ -459,7 +459,7 @@ void SignalHandler::FillRegisterState(void* context, RegisterState* state) {
   state->sp = reinterpret_cast<void*>(ucontext->uc_mcontext.gregs[15]);
   state->fp = reinterpret_cast<void*>(ucontext->uc_mcontext.gregs[11]);
   state->lr = reinterpret_cast<void*>(ucontext->uc_mcontext.gregs[14]);
-#elif V8_HOST_ARCH_RISCV64
+#elif V8_HOST_ARCH_RISCV64 || V8_HOST_ARCH_RISCV32
   // Spec CH.25 RISC-V Assembly Programmer’s Handbook
   state->pc = reinterpret_cast<void*>(mcontext.__gregs[REG_PC]);
   state->sp = reinterpret_cast<void*>(mcontext.__gregs[REG_SP]);
diff --git a/src/logging/log.cc b/src/logging/log.cc
index cc2b02e1d30..1f670c14f97 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -702,6 +702,8 @@ void LowLevelLogger::LogCodeInfo() {
   const char arch[] = "s390";
 #elif V8_TARGET_ARCH_RISCV64
   const char arch[] = "riscv64";
+#elif V8_TARGET_ARCH_RISCV32
+  const char arch[] = "riscv32";
 #else
   const char arch[] = "unknown";
 #endif
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 275cd55e103..3e8f9b41796 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -372,10 +372,11 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
 #if defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_PPC64) || \
     defined(V8_TARGET_ARCH_MIPS64)
   return RelocIterator(*this, kModeMask).done();
-#elif defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_ARM64) || \
-    defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_MIPS) ||    \
-    defined(V8_TARGET_ARCH_S390) || defined(V8_TARGET_ARCH_IA32) ||   \
-    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64)
+#elif defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_ARM64) ||     \
+    defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_MIPS) ||        \
+    defined(V8_TARGET_ARCH_S390) || defined(V8_TARGET_ARCH_IA32) ||       \
+    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64) || \
+    defined(V8_TARGET_ARCH_RISCV32)
   for (RelocIterator it(*this, kModeMask); !it.done(); it.next()) {
     // On these platforms we emit relative builtin-to-builtin
     // jumps for isolate independent builtins in the snapshot. They are later
diff --git a/src/objects/code.h b/src/objects/code.h
index 1ab7b905c53..0deec20ba32 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -740,6 +740,8 @@ class Code : public HeapObject {
   static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 8 : 20;
 #elif V8_TARGET_ARCH_RISCV64
   static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 8 : 20);
+#elif V8_TARGET_ARCH_RISCV32
+  static constexpr int kHeaderPaddingSize = 8;
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/profiler/tick-sample.cc b/src/profiler/tick-sample.cc
index cd65f9d6517..766b4188357 100644
--- a/src/profiler/tick-sample.cc
+++ b/src/profiler/tick-sample.cc
@@ -133,6 +133,13 @@ bool SimulatorHelper::FillRegisters(Isolate* isolate,
   state->sp = reinterpret_cast<void*>(simulator->get_register(Simulator::sp));
   state->fp = reinterpret_cast<void*>(simulator->get_register(Simulator::fp));
   state->lr = reinterpret_cast<void*>(simulator->get_register(Simulator::ra));
+#elif V8_TARGET_ARCH_RISCV32
+  if (!simulator->has_bad_pc()) {
+    state->pc = reinterpret_cast<void*>(simulator->get_pc());
+  }
+  state->sp = reinterpret_cast<void*>(simulator->get_register(Simulator::sp));
+  state->fp = reinterpret_cast<void*>(simulator->get_register(Simulator::fp));
+  state->lr = reinterpret_cast<void*>(simulator->get_register(Simulator::ra));
 #endif
   if (state->sp == 0 || state->fp == 0) {
     // It possible that the simulator is interrupted while it is updating
diff --git a/src/regexp/regexp-macro-assembler-arch.h b/src/regexp/regexp-macro-assembler-arch.h
index 5d4663e3976..101f2412c35 100644
--- a/src/regexp/regexp-macro-assembler-arch.h
+++ b/src/regexp/regexp-macro-assembler-arch.h
@@ -25,8 +25,8 @@
 #include "src/regexp/loong64/regexp-macro-assembler-loong64.h"
 #elif V8_TARGET_ARCH_S390
 #include "src/regexp/s390/regexp-macro-assembler-s390.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/regexp/riscv64/regexp-macro-assembler-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/regexp/riscv/regexp-macro-assembler-riscv.h"
 #else
 #error Unsupported target architecture.
 #endif
diff --git a/src/regexp/regexp-macro-assembler.h b/src/regexp/regexp-macro-assembler.h
index c231d112f86..7612e86344b 100644
--- a/src/regexp/regexp-macro-assembler.h
+++ b/src/regexp/regexp-macro-assembler.h
@@ -167,6 +167,7 @@ class RegExpMacroAssembler {
   V(MIPS)                       \
   V(LOONG64)                    \
   V(RISCV)                      \
+  V(RISCV32)                    \
   V(S390)                       \
   V(PPC)                        \
   V(X64)                        \
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index f1faf827971..965dfe97431 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -943,6 +943,9 @@ bool RegExpImpl::Compile(Isolate* isolate, Zone* zone, RegExpCompileData* data,
 #elif V8_TARGET_ARCH_RISCV64
     macro_assembler.reset(new RegExpMacroAssemblerRISCV(isolate, zone, mode,
                                                         output_register_count));
+#elif V8_TARGET_ARCH_RISCV32
+    macro_assembler.reset(new RegExpMacroAssemblerRISCV(isolate, zone, mode,
+                                                        output_register_count));
 #elif V8_TARGET_ARCH_LOONG64
     macro_assembler.reset(new RegExpMacroAssemblerLOONG64(
         isolate, zone, mode, output_register_count));
diff --git a/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc b/src/regexp/riscv/regexp-macro-assembler-riscv.cc
similarity index 82%
rename from src/regexp/riscv64/regexp-macro-assembler-riscv64.cc
rename to src/regexp/riscv/regexp-macro-assembler-riscv.cc
index 6e73b61670b..93da768d86c 100644
--- a/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc
+++ b/src/regexp/riscv/regexp-macro-assembler-riscv.cc
@@ -2,9 +2,7 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#if V8_TARGET_ARCH_RISCV64
-
-#include "src/regexp/riscv64/regexp-macro-assembler-riscv64.h"
+#include "src/regexp/riscv/regexp-macro-assembler-riscv.h"
 
 #include "src/codegen/assembler-inl.h"
 #include "src/codegen/macro-assembler.h"
@@ -133,8 +131,8 @@ int RegExpMacroAssemblerRISCV::stack_limit_slack() {
 
 void RegExpMacroAssemblerRISCV::AdvanceCurrentPosition(int by) {
   if (by != 0) {
-    __ Add64(current_input_offset(), current_input_offset(),
-             Operand(by * char_size()));
+    __ AddWord(current_input_offset(), current_input_offset(),
+               Operand(by * char_size()));
   }
 }
 
@@ -142,9 +140,9 @@ void RegExpMacroAssemblerRISCV::AdvanceRegister(int reg, int by) {
   DCHECK_LE(0, reg);
   DCHECK_GT(num_registers_, reg);
   if (by != 0) {
-    __ Ld(a0, register_location(reg));
-    __ Add64(a0, a0, Operand(by));
-    __ Sd(a0, register_location(reg));
+    __ LoadWord(a0, register_location(reg));
+    __ AddWord(a0, a0, Operand(by));
+    __ StoreWord(a0, register_location(reg));
   }
 }
 
@@ -152,9 +150,9 @@ void RegExpMacroAssemblerRISCV::Backtrack() {
   CheckPreemption();
   if (has_backtrack_limit()) {
     Label next;
-    __ Ld(a0, MemOperand(frame_pointer(), kBacktrackCount));
-    __ Add64(a0, a0, Operand(1));
-    __ Sd(a0, MemOperand(frame_pointer(), kBacktrackCount));
+    __ LoadWord(a0, MemOperand(frame_pointer(), kBacktrackCount));
+    __ AddWord(a0, a0, Operand(1));
+    __ StoreWord(a0, MemOperand(frame_pointer(), kBacktrackCount));
     __ BranchShort(&next, ne, a0, Operand(backtrack_limit()));
 
     // Backtrack limit exceeded.
@@ -169,7 +167,7 @@ void RegExpMacroAssemblerRISCV::Backtrack() {
   }
   // Pop Code offset from backtrack stack, add Code and jump to location.
   Pop(a0);
-  __ Add64(a0, a0, code_pointer());
+  __ AddWord(a0, a0, code_pointer());
   __ Jump(a0);
 }
 
@@ -186,17 +184,17 @@ void RegExpMacroAssemblerRISCV::CheckCharacterGT(base::uc16 limit,
 
 void RegExpMacroAssemblerRISCV::CheckAtStart(int cp_offset,
                                              Label* on_at_start) {
-  __ Ld(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
-  __ Add64(a0, current_input_offset(),
-           Operand(-char_size() + cp_offset * char_size()));
+  __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+  __ AddWord(a0, current_input_offset(),
+             Operand(-char_size() + cp_offset * char_size()));
   BranchOrBacktrack(on_at_start, eq, a0, Operand(a1));
 }
 
 void RegExpMacroAssemblerRISCV::CheckNotAtStart(int cp_offset,
                                                 Label* on_not_at_start) {
-  __ Ld(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
-  __ Add64(a0, current_input_offset(),
-           Operand(-char_size() + cp_offset * char_size()));
+  __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+  __ AddWord(a0, current_input_offset(),
+             Operand(-char_size() + cp_offset * char_size()));
   BranchOrBacktrack(on_not_at_start, ne, a0, Operand(a1));
 }
 
@@ -209,8 +207,8 @@ void RegExpMacroAssemblerRISCV::CheckGreedyLoop(Label* on_equal) {
   Label backtrack_non_equal;
   __ Lw(a0, MemOperand(backtrack_stackpointer(), 0));
   __ BranchShort(&backtrack_non_equal, ne, current_input_offset(), Operand(a0));
-  __ Add64(backtrack_stackpointer(), backtrack_stackpointer(),
-           Operand(kIntSize));
+  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(),
+             Operand(kIntSize));
   __ bind(&backtrack_non_equal);
   BranchOrBacktrack(on_equal, eq, current_input_offset(), Operand(a0));
 }
@@ -250,9 +248,10 @@ bool RegExpMacroAssemblerRISCV::CheckCharacterNotInRangeArray(
 void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
     int start_reg, bool read_backward, bool unicode, Label* on_no_match) {
   Label fallthrough;
-  __ Ld(a0, register_location(start_reg));      // Index of start of capture.
-  __ Ld(a1, register_location(start_reg + 1));  // Index of end of capture.
-  __ Sub64(a1, a1, a0);                         // Length of capture.
+  __ LoadWord(a0, register_location(start_reg));  // Index of start of capture.
+  __ LoadWord(a1,
+              register_location(start_reg + 1));  // Index of end of capture.
+  __ SubWord(a1, a1, a0);                         // Length of capture.
 
   // At this point, the capture registers are either both set or both cleared.
   // If the capture length is zero, then the capture is either empty or cleared.
@@ -260,11 +259,11 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
   __ BranchShort(&fallthrough, eq, a1, Operand(zero_reg));
 
   if (read_backward) {
-    __ Ld(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
-    __ Add64(t1, t1, a1);
+    __ LoadWord(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ AddWord(t1, t1, a1);
     BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));
   } else {
-    __ Add64(t1, a1, current_input_offset());
+    __ AddWord(t1, a1, current_input_offset());
     // Check that there are enough characters left in the input.
     BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));
   }
@@ -276,12 +275,12 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
 
     // a0 - offset of start of capture.
     // a1 - length of capture.
-    __ Add64(a0, a0, Operand(end_of_input_address()));
-    __ Add64(a2, end_of_input_address(), Operand(current_input_offset()));
+    __ AddWord(a0, a0, Operand(end_of_input_address()));
+    __ AddWord(a2, end_of_input_address(), Operand(current_input_offset()));
     if (read_backward) {
-      __ Sub64(a2, a2, Operand(a1));
+      __ SubWord(a2, a2, Operand(a1));
     }
-    __ Add64(a1, a0, Operand(a1));
+    __ AddWord(a1, a0, Operand(a1));
 
     // a0 - Address of start of capture.
     // a1 - Address of end of capture.
@@ -300,10 +299,10 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
     __ Or(a3, a3, Operand(0x20));  // Convert capture character to lower-case.
     __ Or(a4, a4, Operand(0x20));  // Also convert input character.
     __ BranchShort(&fail, ne, a4, Operand(a3));
-    __ Sub64(a3, a3, Operand('a'));
+    __ SubWord(a3, a3, Operand('a'));
     __ BranchShort(&loop_check, Uless_equal, a3, Operand('z' - 'a'));
     // Latin-1: Check for values in range [224,254] but not 247.
-    __ Sub64(a3, a3, Operand(224 - 'a'));
+    __ SubWord(a3, a3, Operand(224 - 'a'));
     // Weren't Latin-1 letters.
     __ BranchShort(&fail, Ugreater, a3, Operand(254 - 224));
     // Check for 247.
@@ -318,12 +317,14 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
 
     __ bind(&success);
     // Compute new value of character position after the matched part.
-    __ Sub64(current_input_offset(), a2, end_of_input_address());
+    __ SubWord(current_input_offset(), a2, end_of_input_address());
     if (read_backward) {
-      __ Ld(t1, register_location(start_reg));  // Index of start of capture.
-      __ Ld(a2, register_location(start_reg + 1));  // Index of end of capture.
-      __ Add64(current_input_offset(), current_input_offset(), Operand(t1));
-      __ Sub64(current_input_offset(), current_input_offset(), Operand(a2));
+      __ LoadWord(t1,
+                  register_location(start_reg));  // Index of start of capture.
+      __ LoadWord(
+          a2, register_location(start_reg + 1));  // Index of end of capture.
+      __ AddWord(current_input_offset(), current_input_offset(), Operand(t1));
+      __ SubWord(current_input_offset(), current_input_offset(), Operand(a2));
     }
   } else {
     DCHECK(mode_ == UC16);
@@ -342,15 +343,15 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
     //   a3: Isolate* isolate.
 
     // Address of start of capture.
-    __ Add64(a0, a0, Operand(end_of_input_address()));
+    __ AddWord(a0, a0, Operand(end_of_input_address()));
     // Length of capture.
     __ mv(a2, a1);
     // Save length in callee-save register for use on return.
     __ mv(s3, a1);
     // Address of current input position.
-    __ Add64(a1, current_input_offset(), Operand(end_of_input_address()));
+    __ AddWord(a1, current_input_offset(), Operand(end_of_input_address()));
     if (read_backward) {
-      __ Sub64(a1, a1, Operand(s3));
+      __ SubWord(a1, a1, Operand(s3));
     }
     // Isolate.
     __ li(a3, Operand(ExternalReference::isolate_address(masm_->isolate())));
@@ -368,9 +369,9 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReferenceIgnoreCase(
     BranchOrBacktrack(on_no_match, eq, a0, Operand(zero_reg));
     // On success, increment position by length of capture.
     if (read_backward) {
-      __ Sub64(current_input_offset(), current_input_offset(), Operand(s3));
+      __ SubWord(current_input_offset(), current_input_offset(), Operand(s3));
     } else {
-      __ Add64(current_input_offset(), current_input_offset(), Operand(s3));
+      __ AddWord(current_input_offset(), current_input_offset(), Operand(s3));
     }
   }
 
@@ -383,9 +384,9 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReference(int start_reg,
   Label fallthrough;
 
   // Find length of back-referenced capture.
-  __ Ld(a0, register_location(start_reg));
-  __ Ld(a1, register_location(start_reg + 1));
-  __ Sub64(a1, a1, a0);  // Length to check.
+  __ LoadWord(a0, register_location(start_reg));
+  __ LoadWord(a1, register_location(start_reg + 1));
+  __ SubWord(a1, a1, a0);  // Length to check.
 
   // At this point, the capture registers are either both set or both cleared.
   // If the capture length is zero, then the capture is either empty or cleared.
@@ -393,22 +394,22 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReference(int start_reg,
   __ BranchShort(&fallthrough, eq, a1, Operand(zero_reg));
 
   if (read_backward) {
-    __ Ld(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
-    __ Add64(t1, t1, a1);
+    __ LoadWord(t1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ AddWord(t1, t1, a1);
     BranchOrBacktrack(on_no_match, le, current_input_offset(), Operand(t1));
   } else {
-    __ Add64(t1, a1, current_input_offset());
+    __ AddWord(t1, a1, current_input_offset());
     // Check that there are enough characters left in the input.
     BranchOrBacktrack(on_no_match, gt, t1, Operand(zero_reg));
   }
 
   // Compute pointers to match string and capture string.
-  __ Add64(a0, a0, Operand(end_of_input_address()));
-  __ Add64(a2, end_of_input_address(), Operand(current_input_offset()));
+  __ AddWord(a0, a0, Operand(end_of_input_address()));
+  __ AddWord(a2, end_of_input_address(), Operand(current_input_offset()));
   if (read_backward) {
-    __ Sub64(a2, a2, Operand(a1));
+    __ SubWord(a2, a2, Operand(a1));
   }
-  __ Add64(a1, a1, Operand(a0));
+  __ AddWord(a1, a1, Operand(a0));
 
   Label loop;
   __ bind(&loop);
@@ -428,12 +429,14 @@ void RegExpMacroAssemblerRISCV::CheckNotBackReference(int start_reg,
   __ Branch(&loop, lt, a0, Operand(a1));
 
   // Move current character position to position after match.
-  __ Sub64(current_input_offset(), a2, end_of_input_address());
+  __ SubWord(current_input_offset(), a2, end_of_input_address());
   if (read_backward) {
-    __ Ld(t1, register_location(start_reg));      // Index of start of capture.
-    __ Ld(a2, register_location(start_reg + 1));  // Index of end of capture.
-    __ Add64(current_input_offset(), current_input_offset(), Operand(t1));
-    __ Sub64(current_input_offset(), current_input_offset(), Operand(a2));
+    __ LoadWord(t1,
+                register_location(start_reg));  // Index of start of capture.
+    __ LoadWord(a2,
+                register_location(start_reg + 1));  // Index of end of capture.
+    __ AddWord(current_input_offset(), current_input_offset(), Operand(t1));
+    __ SubWord(current_input_offset(), current_input_offset(), Operand(a2));
   }
   __ bind(&fallthrough);
 }
@@ -462,7 +465,7 @@ void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterAnd(uint32_t c,
 void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterMinusAnd(
     base::uc16 c, base::uc16 minus, base::uc16 mask, Label* on_not_equal) {
   DCHECK_GT(String::kMaxUtf16CodeUnit, minus);
-  __ Sub64(a0, current_character(), Operand(minus));
+  __ SubWord(a0, current_character(), Operand(minus));
   __ And(a0, a0, Operand(mask));
   BranchOrBacktrack(on_not_equal, ne, a0, Operand(c));
 }
@@ -470,14 +473,14 @@ void RegExpMacroAssemblerRISCV::CheckNotCharacterAfterMinusAnd(
 void RegExpMacroAssemblerRISCV::CheckCharacterInRange(base::uc16 from,
                                                       base::uc16 to,
                                                       Label* on_in_range) {
-  __ Sub64(a0, current_character(), Operand(from));
+  __ SubWord(a0, current_character(), Operand(from));
   // Unsigned lower-or-same condition.
   BranchOrBacktrack(on_in_range, Uless_equal, a0, Operand(to - from));
 }
 
 void RegExpMacroAssemblerRISCV::CheckCharacterNotInRange(
     base::uc16 from, base::uc16 to, Label* on_not_in_range) {
-  __ Sub64(a0, current_character(), Operand(from));
+  __ SubWord(a0, current_character(), Operand(from));
   // Unsigned higher condition.
   BranchOrBacktrack(on_not_in_range, Ugreater, a0, Operand(to - from));
 }
@@ -487,9 +490,9 @@ void RegExpMacroAssemblerRISCV::CheckBitInTable(Handle<ByteArray> table,
   __ li(a0, Operand(table));
   if (mode_ != LATIN1 || kTableMask != String::kMaxOneByteCharCode) {
     __ And(a1, current_character(), Operand(kTableSize - 1));
-    __ Add64(a0, a0, a1);
+    __ AddWord(a0, a0, a1);
   } else {
-    __ Add64(a0, a0, current_character());
+    __ AddWord(a0, a0, current_character());
   }
 
   __ Lbu(a0, FieldMemOperand(a0, ByteArray::kHeaderSize));
@@ -508,7 +511,7 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
         Label success;
         __ BranchShort(&success, eq, current_character(), Operand(' '));
         // Check range 0x09..0x0D.
-        __ Sub64(a0, current_character(), Operand('\t'));
+        __ SubWord(a0, current_character(), Operand('\t'));
         __ BranchShort(&success, Uless_equal, a0, Operand('\r' - '\t'));
         // \u00a0 (NBSP).
         BranchOrBacktrack(on_no_match, ne, a0, Operand(0x00A0 - '\t'));
@@ -521,25 +524,25 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
       return false;
     case StandardCharacterSet::kDigit:
       // Match Latin1 digits ('0'..'9').
-      __ Sub64(a0, current_character(), Operand('0'));
+      __ SubWord(a0, current_character(), Operand('0'));
       BranchOrBacktrack(on_no_match, Ugreater, a0, Operand('9' - '0'));
       return true;
     case StandardCharacterSet::kNotDigit:
       // Match non Latin1-digits.
-      __ Sub64(a0, current_character(), Operand('0'));
+      __ SubWord(a0, current_character(), Operand('0'));
       BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand('9' - '0'));
       return true;
     case StandardCharacterSet::kNotLineTerminator: {
       // Match non-newlines (not 0x0A('\n'), 0x0D('\r'), 0x2028 and 0x2029).
       __ Xor(a0, current_character(), Operand(0x01));
       // See if current character is '\n'^1 or '\r'^1, i.e., 0x0B or 0x0C.
-      __ Sub64(a0, a0, Operand(0x0B));
+      __ SubWord(a0, a0, Operand(0x0B));
       BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand(0x0C - 0x0B));
       if (mode_ == UC16) {
         // Compare original value to 0x2028 and 0x2029, using the already
         // computed (current_char ^ 0x01 - 0x0B). I.e., check for
         // 0x201D (0x2028 - 0x0B) or 0x201E.
-        __ Sub64(a0, a0, Operand(0x2028 - 0x0B));
+        __ SubWord(a0, a0, Operand(0x2028 - 0x0B));
         BranchOrBacktrack(on_no_match, Uless_equal, a0, Operand(1));
       }
       return true;
@@ -548,7 +551,7 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
       // Match newlines (0x0A('\n'), 0x0D('\r'), 0x2028 and 0x2029).
       __ Xor(a0, current_character(), Operand(0x01));
       // See if current character is '\n'^1 or '\r'^1, i.e., 0x0B or 0x0C.
-      __ Sub64(a0, a0, Operand(0x0B));
+      __ SubWord(a0, a0, Operand(0x0B));
       if (mode_ == LATIN1) {
         BranchOrBacktrack(on_no_match, Ugreater, a0, Operand(0x0C - 0x0B));
       } else {
@@ -557,7 +560,7 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
         // Compare original value to 0x2028 and 0x2029, using the already
         // computed (current_char ^ 0x01 - 0x0B). I.e., check for
         // 0x201D (0x2028 - 0x0B) or 0x201E.
-        __ Sub64(a0, a0, Operand(0x2028 - 0x0B));
+        __ SubWord(a0, a0, Operand(0x2028 - 0x0B));
         BranchOrBacktrack(on_no_match, Ugreater, a0, Operand(1));
         __ bind(&done);
       }
@@ -571,7 +574,7 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
       }
       ExternalReference map = ExternalReference::re_word_character_map();
       __ li(a0, Operand(map));
-      __ Add64(a0, a0, current_character());
+      __ AddWord(a0, a0, current_character());
       __ Lbu(a0, MemOperand(a0, 0));
       BranchOrBacktrack(on_no_match, eq, a0, Operand(zero_reg));
       return true;
@@ -584,7 +587,7 @@ bool RegExpMacroAssemblerRISCV::CheckSpecialCharacterClass(
       }
       ExternalReference map = ExternalReference::re_word_character_map();
       __ li(a0, Operand(map));
-      __ Add64(a0, a0, current_character());
+      __ AddWord(a0, a0, current_character());
       __ Lbu(a0, MemOperand(a0, 0));
       BranchOrBacktrack(on_no_match, ne, a0, Operand(zero_reg));
       if (mode_ != LATIN1) {
@@ -610,7 +613,7 @@ void RegExpMacroAssemblerRISCV::LoadRegExpStackPointerFromMemory(Register dst) {
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_stack_pointer(isolate());
   __ li(dst, Operand(ref));
-  __ Ld(dst, MemOperand(dst));
+  __ LoadWord(dst, MemOperand(dst));
 }
 
 void RegExpMacroAssemblerRISCV::StoreRegExpStackPointerToMemory(
@@ -618,7 +621,7 @@ void RegExpMacroAssemblerRISCV::StoreRegExpStackPointerToMemory(
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_stack_pointer(isolate());
   __ li(scratch, Operand(ref));
-  __ Sd(src, MemOperand(scratch));
+  __ StoreWord(src, MemOperand(scratch));
 }
 
 void RegExpMacroAssemblerRISCV::PushRegExpBasePointer(Register stack_pointer,
@@ -626,20 +629,20 @@ void RegExpMacroAssemblerRISCV::PushRegExpBasePointer(Register stack_pointer,
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_memory_top_address(isolate());
   __ li(scratch, Operand(ref));
-  __ Ld(scratch, MemOperand(scratch));
-  __ Sub64(scratch, stack_pointer, scratch);
-  __ Sd(scratch, MemOperand(frame_pointer(), kRegExpStackBasePointer));
+  __ LoadWord(scratch, MemOperand(scratch));
+  __ SubWord(scratch, stack_pointer, scratch);
+  __ StoreWord(scratch, MemOperand(frame_pointer(), kRegExpStackBasePointer));
 }
 
 void RegExpMacroAssemblerRISCV::PopRegExpBasePointer(Register stack_pointer_out,
                                                      Register scratch) {
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_memory_top_address(isolate());
-  __ Ld(stack_pointer_out,
-        MemOperand(frame_pointer(), kRegExpStackBasePointer));
+  __ LoadWord(stack_pointer_out,
+              MemOperand(frame_pointer(), kRegExpStackBasePointer));
   __ li(scratch, Operand(ref));
-  __ Ld(scratch, MemOperand(scratch));
-  __ Add64(stack_pointer_out, stack_pointer_out, scratch);
+  __ LoadWord(scratch, MemOperand(scratch));
+  __ AddWord(stack_pointer_out, stack_pointer_out, scratch);
   StoreRegExpStackPointerToMemory(stack_pointer_out, scratch);
 }
 
@@ -693,8 +696,8 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
 
     // Set frame pointer in space for it if this is not a direct call
     // from generated code.
-    __ Add64(frame_pointer(), sp,
-             Operand(argument_registers.Count() * kSystemPointerSize));
+    __ AddWord(frame_pointer(), sp,
+               Operand(argument_registers.Count() * kSystemPointerSize));
 
     static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ mv(a0, zero_reg);
@@ -723,8 +726,8 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
       ExternalReference stack_limit =
           ExternalReference::address_of_jslimit(masm_->isolate());
       __ li(a0, Operand(stack_limit));
-      __ Ld(a0, MemOperand(a0));
-      __ Sub64(a0, sp, a0);
+      __ LoadWord(a0, MemOperand(a0));
+      __ SubWord(a0, sp, a0);
       // Handle it if the stack pointer is already below the stack limit.
       __ Branch(&stack_limit_hit, le, a0, Operand(zero_reg));
       // Check if there is room for the variable number of registers above
@@ -744,22 +747,22 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
       __ bind(&stack_ok);
     }
     // Allocate space on stack for registers.
-    __ Sub64(sp, sp, Operand(num_registers_ * kSystemPointerSize));
+    __ SubWord(sp, sp, Operand(num_registers_ * kSystemPointerSize));
     // Load string end.
-    __ Ld(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+    __ LoadWord(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
     // Load input start.
-    __ Ld(a0, MemOperand(frame_pointer(), kInputStart));
+    __ LoadWord(a0, MemOperand(frame_pointer(), kInputStart));
     // Find negative length (offset of start relative to end).
-    __ Sub64(current_input_offset(), a0, end_of_input_address());
+    __ SubWord(current_input_offset(), a0, end_of_input_address());
     // Set a0 to address of char before start of the input string
     // (effectively string position -1).
-    __ Ld(a1, MemOperand(frame_pointer(), kStartIndex));
-    __ Sub64(a0, current_input_offset(), Operand(char_size()));
+    __ LoadWord(a1, MemOperand(frame_pointer(), kStartIndex));
+    __ SubWord(a0, current_input_offset(), Operand(char_size()));
     __ slli(t1, a1, (mode_ == UC16) ? 1 : 0);
-    __ Sub64(a0, a0, t1);
+    __ SubWord(a0, a0, t1);
     // Store this value in a local variable, for use when clearing
     // position registers.
-    __ Sd(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ StoreWord(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
 
     // Initialize code pointer register
     __ li(code_pointer(), Operand(masm_->CodeObject()), CONSTANT_SIZE);
@@ -784,17 +787,17 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
       // Fill saved registers with initial value = start offset - 1.
       if (num_saved_registers_ > 8) {
         // Address of register 0.
-        __ Add64(a1, frame_pointer(), Operand(kRegisterZero));
+        __ AddWord(a1, frame_pointer(), Operand(kRegisterZero));
         __ li(a2, Operand(num_saved_registers_));
         Label init_loop;
         __ bind(&init_loop);
-        __ Sd(a0, MemOperand(a1));
-        __ Add64(a1, a1, Operand(-kSystemPointerSize));
-        __ Sub64(a2, a2, Operand(1));
+        __ StoreWord(a0, MemOperand(a1));
+        __ AddWord(a1, a1, Operand(-kSystemPointerSize));
+        __ SubWord(a2, a2, Operand(1));
         __ Branch(&init_loop, ne, a2, Operand(zero_reg));
       } else {
         for (int i = 0; i < num_saved_registers_; i++) {
-          __ Sd(a0, register_location(i));
+          __ StoreWord(a0, register_location(i));
         }
       }
     }
@@ -807,16 +810,16 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
       __ bind(&success_label_);
       if (num_saved_registers_ > 0) {
         // Copy captures to output.
-        __ Ld(a1, MemOperand(frame_pointer(), kInputStart));
-        __ Ld(a0, MemOperand(frame_pointer(), kRegisterOutput));
-        __ Ld(a2, MemOperand(frame_pointer(), kStartIndex));
-        __ Sub64(a1, end_of_input_address(), a1);
+        __ LoadWord(a1, MemOperand(frame_pointer(), kInputStart));
+        __ LoadWord(a0, MemOperand(frame_pointer(), kRegisterOutput));
+        __ LoadWord(a2, MemOperand(frame_pointer(), kStartIndex));
+        __ SubWord(a1, end_of_input_address(), a1);
         // a1 is length of input in bytes.
         if (mode_ == UC16) {
           __ srli(a1, a1, 1);
         }
         // a1 is length of input in characters.
-        __ Add64(a1, a1, Operand(a2));
+        __ AddWord(a1, a1, Operand(a2));
         // a1 is length of string in characters.
 
         DCHECK_EQ(0, num_saved_registers_ % 2);
@@ -824,50 +827,50 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
         // unroll the loop once to add an operation between a load of a
         // register and the following use of that register.
         for (int i = 0; i < num_saved_registers_; i += 2) {
-          __ Ld(a2, register_location(i));
-          __ Ld(a3, register_location(i + 1));
+          __ LoadWord(a2, register_location(i));
+          __ LoadWord(a3, register_location(i + 1));
           if (i == 0 && global_with_zero_length_check()) {
             // Keep capture start in a4 for the zero-length check later.
             __ mv(s3, a2);
           }
           if (mode_ == UC16) {
             __ srai(a2, a2, 1);
-            __ Add64(a2, a2, a1);
+            __ AddWord(a2, a2, a1);
             __ srai(a3, a3, 1);
-            __ Add64(a3, a3, a1);
+            __ AddWord(a3, a3, a1);
           } else {
-            __ Add64(a2, a1, Operand(a2));
-            __ Add64(a3, a1, Operand(a3));
+            __ AddWord(a2, a1, Operand(a2));
+            __ AddWord(a3, a1, Operand(a3));
           }
           // V8 expects the output to be an int32_t array.
           __ Sw(a2, MemOperand(a0));
-          __ Add64(a0, a0, kIntSize);
+          __ AddWord(a0, a0, kIntSize);
           __ Sw(a3, MemOperand(a0));
-          __ Add64(a0, a0, kIntSize);
+          __ AddWord(a0, a0, kIntSize);
         }
       }
 
       if (global()) {
         // Restart matching if the regular expression is flagged as global.
-        __ Ld(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
-        __ Ld(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
-        __ Ld(a2, MemOperand(frame_pointer(), kRegisterOutput));
+        __ LoadWord(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+        __ LoadWord(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
+        __ LoadWord(a2, MemOperand(frame_pointer(), kRegisterOutput));
         // Increment success counter.
-        __ Add64(a0, a0, 1);
-        __ Sd(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+        __ AddWord(a0, a0, 1);
+        __ StoreWord(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
         // Capture results have been stored, so the number of remaining global
         // output registers is reduced by the number of stored captures.
-        __ Sub64(a1, a1, num_saved_registers_);
+        __ SubWord(a1, a1, num_saved_registers_);
         // Check whether we have enough room for another set of capture results.
         __ Branch(&return_a0, lt, a1, Operand(num_saved_registers_));
 
-        __ Sd(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
+        __ StoreWord(a1, MemOperand(frame_pointer(), kNumOutputRegisters));
         // Advance the location for output.
-        __ Add64(a2, a2, num_saved_registers_ * kIntSize);
-        __ Sd(a2, MemOperand(frame_pointer(), kRegisterOutput));
+        __ AddWord(a2, a2, num_saved_registers_ * kIntSize);
+        __ StoreWord(a2, MemOperand(frame_pointer(), kRegisterOutput));
 
         // Prepare a0 to initialize registers with its value in the next run.
-        __ Ld(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+        __ LoadWord(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
 
         // Restore the original regexp stack pointer value (effectively, pop the
         // stored base pointer).
@@ -885,8 +888,8 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
           // Advance current position after a zero-length match.
           Label advance;
           __ bind(&advance);
-          __ Add64(current_input_offset(), current_input_offset(),
-                   Operand((mode_ == UC16) ? 2 : 1));
+          __ AddWord(current_input_offset(), current_input_offset(),
+                     Operand((mode_ == UC16) ? 2 : 1));
           if (global_unicode()) CheckNotInSurrogatePair(0, &advance);
         }
 
@@ -898,7 +901,7 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
     // Exit and return a0.
     __ bind(&exit_label_);
     if (global()) {
-      __ Ld(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
+      __ LoadWord(a0, MemOperand(frame_pointer(), kSuccessfulCaptures));
     }
 
     __ bind(&return_a0);
@@ -932,7 +935,8 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
       __ Branch(&return_a0, ne, a0, Operand(zero_reg));
       LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
       // String might have moved: Reload end of string from frame.
-      __ Ld(end_of_input_address(), MemOperand(frame_pointer(), kInputEnd));
+      __ LoadWord(end_of_input_address(),
+                  MemOperand(frame_pointer(), kInputEnd));
       SafeReturn();
     }
 
@@ -993,18 +997,18 @@ void RegExpMacroAssemblerRISCV::GoTo(Label* to) {
 
 void RegExpMacroAssemblerRISCV::IfRegisterGE(int reg, int comparand,
                                              Label* if_ge) {
-  __ Ld(a0, register_location(reg));
+  __ LoadWord(a0, register_location(reg));
   BranchOrBacktrack(if_ge, ge, a0, Operand(comparand));
 }
 
 void RegExpMacroAssemblerRISCV::IfRegisterLT(int reg, int comparand,
                                              Label* if_lt) {
-  __ Ld(a0, register_location(reg));
+  __ LoadWord(a0, register_location(reg));
   BranchOrBacktrack(if_lt, lt, a0, Operand(comparand));
 }
 
 void RegExpMacroAssemblerRISCV::IfRegisterEqPos(int reg, Label* if_eq) {
-  __ Ld(a0, register_location(reg));
+  __ LoadWord(a0, register_location(reg));
   BranchOrBacktrack(if_eq, eq, a0, Operand(current_input_offset()));
 }
 
@@ -1019,7 +1023,7 @@ void RegExpMacroAssemblerRISCV::PopCurrentPosition() {
 
 void RegExpMacroAssemblerRISCV::PopRegister(int register_index) {
   Pop(a0);
-  __ Sd(a0, register_location(register_index));
+  __ StoreWord(a0, register_location(register_index));
 }
 
 void RegExpMacroAssemblerRISCV::PushBacktrack(Label* label) {
@@ -1036,10 +1040,10 @@ void RegExpMacroAssemblerRISCV::PushBacktrack(Label* label) {
     masm_->label_at_put(label, offset);
     __ bind(&after_constant);
     if (is_int16(cp_offset)) {
-      __ Lwu(a0, MemOperand(code_pointer(), cp_offset));
+      __ Load32U(a0, MemOperand(code_pointer(), cp_offset));
     } else {
-      __ Add64(a0, code_pointer(), cp_offset);
-      __ Lwu(a0, MemOperand(a0, 0));
+      __ AddWord(a0, code_pointer(), cp_offset);
+      __ Load32U(a0, MemOperand(a0, 0));
     }
   }
   Push(a0);
@@ -1052,21 +1056,21 @@ void RegExpMacroAssemblerRISCV::PushCurrentPosition() {
 
 void RegExpMacroAssemblerRISCV::PushRegister(int register_index,
                                              StackCheckFlag check_stack_limit) {
-  __ Ld(a0, register_location(register_index));
+  __ LoadWord(a0, register_location(register_index));
   Push(a0);
   if (check_stack_limit) CheckStackLimit();
 }
 
 void RegExpMacroAssemblerRISCV::ReadCurrentPositionFromRegister(int reg) {
-  __ Ld(current_input_offset(), register_location(reg));
+  __ LoadWord(current_input_offset(), register_location(reg));
 }
 
 void RegExpMacroAssemblerRISCV::WriteStackPointerToRegister(int reg) {
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_memory_top_address(isolate());
   __ li(a0, ref);
-  __ Ld(a0, MemOperand(a0));
-  __ Sub64(a0, backtrack_stackpointer(), a0);
+  __ LoadWord(a0, MemOperand(a0));
+  __ SubWord(a0, backtrack_stackpointer(), a0);
   __ Sw(a0, register_location(reg));
 }
 
@@ -1074,9 +1078,9 @@ void RegExpMacroAssemblerRISCV::ReadStackPointerFromRegister(int reg) {
   ExternalReference ref =
       ExternalReference::address_of_regexp_stack_memory_top_address(isolate());
   __ li(a1, ref);
-  __ Ld(a1, MemOperand(a1));
+  __ LoadWord(a1, MemOperand(a1));
   __ Lw(backtrack_stackpointer(), register_location(reg));
-  __ Add64(backtrack_stackpointer(), backtrack_stackpointer(), a1);
+  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(), a1);
 }
 
 void RegExpMacroAssemblerRISCV::SetCurrentPositionFromEnd(int by) {
@@ -1094,7 +1098,7 @@ void RegExpMacroAssemblerRISCV::SetCurrentPositionFromEnd(int by) {
 void RegExpMacroAssemblerRISCV::SetRegister(int register_index, int to) {
   DCHECK(register_index >= num_saved_registers_);  // Reserved for positions!
   __ li(a0, Operand(to));
-  __ Sd(a0, register_location(register_index));
+  __ StoreWord(a0, register_location(register_index));
 }
 
 bool RegExpMacroAssemblerRISCV::Succeed() {
@@ -1105,18 +1109,18 @@ bool RegExpMacroAssemblerRISCV::Succeed() {
 void RegExpMacroAssemblerRISCV::WriteCurrentPositionToRegister(int reg,
                                                                int cp_offset) {
   if (cp_offset == 0) {
-    __ Sd(current_input_offset(), register_location(reg));
+    __ StoreWord(current_input_offset(), register_location(reg));
   } else {
-    __ Add64(a0, current_input_offset(), Operand(cp_offset * char_size()));
-    __ Sd(a0, register_location(reg));
+    __ AddWord(a0, current_input_offset(), Operand(cp_offset * char_size()));
+    __ StoreWord(a0, register_location(reg));
   }
 }
 
 void RegExpMacroAssemblerRISCV::ClearRegisters(int reg_from, int reg_to) {
   DCHECK(reg_from <= reg_to);
-  __ Ld(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
+  __ LoadWord(a0, MemOperand(frame_pointer(), kStringStartMinusOne));
   for (int reg = reg_from; reg <= reg_to; reg++) {
-    __ Sd(a0, register_location(reg));
+    __ StoreWord(a0, register_location(reg));
   }
 }
 #ifdef RISCV_HAS_NO_UNALIGNED
@@ -1132,10 +1136,10 @@ void RegExpMacroAssemblerRISCV::CallCheckStackGuardState(Register scratch) {
 
   // Align the stack pointer and save the original sp value on the stack.
   __ mv(scratch, sp);
-  __ Sub64(sp, sp, Operand(kSystemPointerSize));
+  __ SubWord(sp, sp, Operand(kSystemPointerSize));
   DCHECK(base::bits::IsPowerOfTwo(stack_alignment));
   __ And(sp, sp, Operand(-stack_alignment));
-  __ Sd(scratch, MemOperand(sp));
+  __ StoreWord(scratch, MemOperand(sp));
 
   __ mv(a2, frame_pointer());
   // Code of self.
@@ -1143,7 +1147,7 @@ void RegExpMacroAssemblerRISCV::CallCheckStackGuardState(Register scratch) {
 
   // We need to make room for the return address on the stack.
   DCHECK(IsAligned(stack_alignment, kSystemPointerSize));
-  __ Sub64(sp, sp, Operand(stack_alignment));
+  __ SubWord(sp, sp, Operand(stack_alignment));
 
   // The stack pointer now points to cell where the return address will be
   // written. Arguments are in registers, meaning we treat the return address as
@@ -1179,7 +1183,7 @@ void RegExpMacroAssemblerRISCV::CallCheckStackGuardState(Register scratch) {
   // [sp + 2] - C argument slot.
   // [sp + 1] - C argument slot.
   // [sp + 0] - C argument slot.
-  __ Ld(sp, MemOperand(sp, stack_alignment + kCArgsSlotsSize));
+  __ LoadWord(sp, MemOperand(sp, stack_alignment + kCArgsSlotsSize));
 
   __ li(code_pointer(), Operand(masm_->CodeObject()));
 }
@@ -1225,8 +1229,8 @@ void RegExpMacroAssemblerRISCV::CheckPosition(int cp_offset,
     BranchOrBacktrack(on_outside_input, ge, current_input_offset(),
                       Operand(-cp_offset * char_size()));
   } else {
-    __ Ld(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
-    __ Add64(a0, current_input_offset(), Operand(cp_offset * char_size()));
+    __ LoadWord(a1, MemOperand(frame_pointer(), kStringStartMinusOne));
+    __ AddWord(a0, current_input_offset(), Operand(cp_offset * char_size()));
     BranchOrBacktrack(on_outside_input, le, a0, Operand(a1));
   }
 }
@@ -1257,27 +1261,27 @@ void RegExpMacroAssemblerRISCV::SafeCall(Label* to, Condition cond, Register rs,
 
 void RegExpMacroAssemblerRISCV::SafeReturn() {
   __ pop(ra);
-  __ Add64(t1, ra, Operand(masm_->CodeObject()));
+  __ AddWord(t1, ra, Operand(masm_->CodeObject()));
   __ Jump(t1);
 }
 
 void RegExpMacroAssemblerRISCV::SafeCallTarget(Label* name) {
   __ bind(name);
-  __ Sub64(ra, ra, Operand(masm_->CodeObject()));
+  __ SubWord(ra, ra, Operand(masm_->CodeObject()));
   __ push(ra);
 }
 
 void RegExpMacroAssemblerRISCV::Push(Register source) {
   DCHECK(source != backtrack_stackpointer());
-  __ Add64(backtrack_stackpointer(), backtrack_stackpointer(),
-           Operand(-kIntSize));
+  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(),
+             Operand(-kIntSize));
   __ Sw(source, MemOperand(backtrack_stackpointer()));
 }
 
 void RegExpMacroAssemblerRISCV::Pop(Register target) {
   DCHECK(target != backtrack_stackpointer());
   __ Lw(target, MemOperand(backtrack_stackpointer()));
-  __ Add64(backtrack_stackpointer(), backtrack_stackpointer(), kIntSize);
+  __ AddWord(backtrack_stackpointer(), backtrack_stackpointer(), kIntSize);
 }
 
 void RegExpMacroAssemblerRISCV::CheckPreemption() {
@@ -1285,7 +1289,7 @@ void RegExpMacroAssemblerRISCV::CheckPreemption() {
   ExternalReference stack_limit =
       ExternalReference::address_of_jslimit(masm_->isolate());
   __ li(a0, Operand(stack_limit));
-  __ Ld(a0, MemOperand(a0));
+  __ LoadWord(a0, MemOperand(a0));
   SafeCall(&check_preempt_label_, Uless_equal, sp, Operand(a0));
 }
 
@@ -1295,7 +1299,7 @@ void RegExpMacroAssemblerRISCV::CheckStackLimit() {
           masm_->isolate());
 
   __ li(a0, Operand(stack_limit));
-  __ Ld(a0, MemOperand(a0));
+  __ LoadWord(a0, MemOperand(a0));
   SafeCall(&stack_overflow_label_, Uless_equal, backtrack_stackpointer(),
            Operand(a0));
 }
@@ -1303,40 +1307,39 @@ void RegExpMacroAssemblerRISCV::CheckStackLimit() {
 void RegExpMacroAssemblerRISCV::LoadCurrentCharacterUnchecked(int cp_offset,
                                                               int characters) {
   Register offset = current_input_offset();
-
-  // If unaligned load/stores are not supported then this function must only
-  // be used to load a single character at a time.
-  if (!CanReadUnaligned()) {
-    DCHECK_EQ(1, characters);
-  }
   if (cp_offset != 0) {
     // kScratchReg2 is not being used to store the capture start index at this
     // point.
-    __ Add64(kScratchReg2, current_input_offset(),
-             Operand(cp_offset * char_size()));
+    __ AddWord(kScratchReg2, current_input_offset(),
+               Operand(cp_offset * char_size()));
     offset = kScratchReg2;
   }
+  // If unaligned load/stores are not supported then this function must only
+  // be used to load a single character at a time.
+  if (!CanReadUnaligned()) {
+    DCHECK_EQ(1, characters);
+  }
 
   if (mode_ == LATIN1) {
     if (characters == 4) {
-      __ Add64(kScratchReg, end_of_input_address(), offset);
-      __ Lwu(current_character(), MemOperand(kScratchReg));
+      __ AddWord(kScratchReg, end_of_input_address(), offset);
+      __ Load32U(current_character(), MemOperand(kScratchReg));
     } else if (characters == 2) {
-      __ Add64(kScratchReg, end_of_input_address(), offset);
+      __ AddWord(kScratchReg, end_of_input_address(), offset);
       __ Lhu(current_character(), MemOperand(kScratchReg));
     } else {
       DCHECK_EQ(1, characters);
-      __ Add64(kScratchReg, end_of_input_address(), offset);
+      __ AddWord(kScratchReg, end_of_input_address(), offset);
       __ Lbu(current_character(), MemOperand(kScratchReg));
     }
   } else {
-    DCHECK(mode_ == UC16);
+    DCHECK_EQ(UC16, mode_);
     if (characters == 2) {
-      __ Add64(kScratchReg, end_of_input_address(), offset);
-      __ Lwu(current_character(), MemOperand(kScratchReg));
+      __ AddWord(kScratchReg, end_of_input_address(), offset);
+      __ Load32U(current_character(), MemOperand(kScratchReg));
     } else {
       DCHECK_EQ(1, characters);
-      __ Add64(kScratchReg, end_of_input_address(), offset);
+      __ AddWord(kScratchReg, end_of_input_address(), offset);
       __ Lhu(current_character(), MemOperand(kScratchReg));
     }
   }
@@ -1346,5 +1349,3 @@ void RegExpMacroAssemblerRISCV::LoadCurrentCharacterUnchecked(int cp_offset,
 
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_TARGET_ARCH_RISCV64
diff --git a/src/regexp/riscv64/regexp-macro-assembler-riscv64.h b/src/regexp/riscv/regexp-macro-assembler-riscv.h
similarity index 97%
rename from src/regexp/riscv64/regexp-macro-assembler-riscv64.h
rename to src/regexp/riscv/regexp-macro-assembler-riscv.h
index 4043f57f3e7..2352af8a17c 100644
--- a/src/regexp/riscv64/regexp-macro-assembler-riscv64.h
+++ b/src/regexp/riscv/regexp-macro-assembler-riscv.h
@@ -2,12 +2,12 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_REGEXP_RISCV64_REGEXP_MACRO_ASSEMBLER_RISCV64_H_
-#define V8_REGEXP_RISCV64_REGEXP_MACRO_ASSEMBLER_RISCV64_H_
+#ifndef V8_REGEXP_RISCV_REGEXP_MACRO_ASSEMBLER_RISCV_H_
+#define V8_REGEXP_RISCV_REGEXP_MACRO_ASSEMBLER_RISCV_H_
 
 #include "src/base/strings.h"
+#include "src/codegen/assembler-arch.h"
 #include "src/codegen/macro-assembler.h"
-#include "src/codegen/riscv64/assembler-riscv64.h"
 #include "src/regexp/regexp-macro-assembler.h"
 
 namespace v8 {
@@ -233,4 +233,4 @@ class V8_EXPORT_PRIVATE RegExpMacroAssemblerRISCV
 }  // namespace internal
 }  // namespace v8
 
-#endif  // V8_REGEXP_RISCV64_REGEXP_MACRO_ASSEMBLER_RISCV64_H_
+#endif  // V8_REGEXP_RISCV_REGEXP_MACRO_ASSEMBLER_RISCV_H_
diff --git a/src/runtime/runtime-atomics.cc b/src/runtime/runtime-atomics.cc
index a397b5cebe7..3a4d978adc6 100644
--- a/src/runtime/runtime-atomics.cc
+++ b/src/runtime/runtime-atomics.cc
@@ -22,7 +22,7 @@ namespace internal {
 // Other platforms have CSA support, see builtins-sharedarraybuffer-gen.h.
 #if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC64 || \
     V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X ||    \
-    V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_LOONG64
+    V8_TARGET_ARCH_LOONG64 || V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 
 namespace {
 
@@ -613,7 +613,8 @@ RUNTIME_FUNCTION(Runtime_AtomicsXor) { UNREACHABLE(); }
 
 #endif  // V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_PPC64
         // || V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_S390 || V8_TARGET_ARCH_S390X
-        // || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_LOONG64
+        // || V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_LOONG64 ||
+        // V8_TARGET_ARCH_RISCV32
 
 RUNTIME_FUNCTION(Runtime_AtomicsLoadSharedStructOrArray) {
   HandleScope scope(isolate);
@@ -679,6 +680,5 @@ RUNTIME_FUNCTION(Runtime_AtomicsExchangeSharedStructOrArray) {
   USE(result);
   return ReadOnlyRoots(isolate).exception();
 }
-
 }  // namespace internal
 }  // namespace v8
diff --git a/src/snapshot/deserializer.h b/src/snapshot/deserializer.h
index 675c7548455..24414a00afa 100644
--- a/src/snapshot/deserializer.h
+++ b/src/snapshot/deserializer.h
@@ -31,8 +31,8 @@ class Object;
 // of objects found in code.
 #if defined(V8_TARGET_ARCH_MIPS) || defined(V8_TARGET_ARCH_MIPS64) ||   \
     defined(V8_TARGET_ARCH_PPC) || defined(V8_TARGET_ARCH_S390) ||      \
-    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_RISCV64) || \
-    V8_EMBEDDED_CONSTANT_POOL
+    defined(V8_TARGET_ARCH_PPC64) || defined(V8_TARGET_ARCH_RISCV32) || \
+    defined(V8_TARGET_ARCH_RISCV64) || V8_EMBEDDED_CONSTANT_POOL
 #define V8_CODE_EMBEDS_OBJECT_POINTER 1
 #else
 #define V8_CODE_EMBEDS_OBJECT_POINTER 0
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index e60a7e7142b..aa720b6263c 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -223,10 +223,11 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
     RelocIterator on_heap_it(code, kRelocMask);
     RelocIterator off_heap_it(blob, code, kRelocMask);
 
-#if defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_ARM64) || \
-    defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_MIPS) ||  \
-    defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_S390) || \
-    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64)
+#if defined(V8_TARGET_ARCH_X64) || defined(V8_TARGET_ARCH_ARM64) ||       \
+    defined(V8_TARGET_ARCH_ARM) || defined(V8_TARGET_ARCH_MIPS) ||        \
+    defined(V8_TARGET_ARCH_IA32) || defined(V8_TARGET_ARCH_S390) ||       \
+    defined(V8_TARGET_ARCH_RISCV64) || defined(V8_TARGET_ARCH_LOONG64) || \
+    defined(V8_TARGET_ARCH_RISCV32)
     // On these platforms we emit relative builtin-to-builtin
     // jumps for isolate independent builtins in the snapshot. This fixes up the
     // relative jumps to the right offsets in the snapshot.
diff --git a/src/wasm/baseline/liftoff-assembler-defs.h b/src/wasm/baseline/liftoff-assembler-defs.h
index 3cfa5fa04d5..8ceb686307d 100644
--- a/src/wasm/baseline/liftoff-assembler-defs.h
+++ b/src/wasm/baseline/liftoff-assembler-defs.h
@@ -102,6 +102,18 @@ constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
 
 #elif V8_TARGET_ARCH_RISCV64
 
+// Any change of kLiftoffAssemblerGpCacheRegs also need to update
+// kPushedGpRegs in frame-constants-riscv64.h
+constexpr RegList kLiftoffAssemblerGpCacheRegs = {a0, a1, a2, a3, a4, a5,
+                                                  a6, a7, t0, t1, t2, s7};
+
+// Any change of kLiftoffAssemblerGpCacheRegs also need to update
+// kPushedFpRegs in frame-constants-riscv64.h
+constexpr DoubleRegList kLiftoffAssemblerFpCacheRegs = {
+    ft1, ft2, ft3, ft4, ft5, ft6, ft7, fa0,  fa1, fa2,
+    fa3, fa4, fa5, fa6, fa7, ft8, ft9, ft10, ft11};
+#elif V8_TARGET_ARCH_RISCV32
+
 // Any change of kLiftoffAssemblerGpCacheRegs also need to update
 // kPushedGpRegs in frame-constants-riscv64.h
 constexpr RegList kLiftoffAssemblerGpCacheRegs = {a0, a1, a2, a3, a4, a5,
diff --git a/src/wasm/baseline/liftoff-assembler.h b/src/wasm/baseline/liftoff-assembler.h
index b3ed3a14e88..32e8a3098bd 100644
--- a/src/wasm/baseline/liftoff-assembler.h
+++ b/src/wasm/baseline/liftoff-assembler.h
@@ -1875,7 +1875,9 @@ bool CheckCompatibleStackSlotTypes(ValueKind a, ValueKind b);
 #elif V8_TARGET_ARCH_S390
 #include "src/wasm/baseline/s390/liftoff-assembler-s390.h"
 #elif V8_TARGET_ARCH_RISCV64
-#include "src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h"
+#include "src/wasm/baseline/riscv/liftoff-assembler-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32
+#include "src/wasm/baseline/riscv/liftoff-assembler-riscv32.h"
 #else
 #error Unsupported architecture.
 #endif
diff --git a/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
similarity index 56%
rename from src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h
rename to src/wasm/baseline/riscv/liftoff-assembler-riscv.h
index ecbfde65c79..14627023cef 100644
--- a/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv.h
@@ -1,9 +1,9 @@
-// Copyright 2021 the V8 project authors. All rights reserved.
+// Copyright 2022 the V8 project authors. All rights reserved.
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
-#ifndef V8_WASM_BASELINE_RISCV64_LIFTOFF_ASSEMBLER_RISCV64_H_
-#define V8_WASM_BASELINE_RISCV64_LIFTOFF_ASSEMBLER_RISCV64_H_
+#ifndef V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV_H_
+#define V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV_H_
 
 #include "src/base/platform/wrappers.h"
 #include "src/heap/memory-chunk.h"
@@ -16,6 +16,14 @@ namespace wasm {
 
 namespace liftoff {
 
+// fp-8 holds the stack marker, fp-16 is the instance parameter.
+constexpr int kInstanceOffset = 2 * kSystemPointerSize;
+constexpr int kFeedbackVectorOffset = 3 * kSystemPointerSize;
+
+inline MemOperand GetStackSlot(int offset) { return MemOperand(fp, -offset); }
+
+inline MemOperand GetInstanceOperand() { return GetStackSlot(kInstanceOffset); }
+
 inline constexpr Condition ToCondition(LiftoffCondition liftoff_cond) {
   switch (liftoff_cond) {
     case kEqual:
@@ -41,250 +49,7 @@ inline constexpr Condition ToCondition(LiftoffCondition liftoff_cond) {
   }
 }
 
-// Liftoff Frames.
-//
-//  slot      Frame
-//       +--------------------+---------------------------
-//  n+4  | optional padding slot to keep the stack 16 byte aligned.
-//  n+3  |   parameter n      |
-//  ...  |       ...          |
-//   4   |   parameter 1      | or parameter 2
-//   3   |   parameter 0      | or parameter 1
-//   2   |  (result address)  | or parameter 0
-//  -----+--------------------+---------------------------
-//   1   | return addr (ra)   |
-//   0   | previous frame (fp)|
-//  -----+--------------------+  <-- frame ptr (fp)
-//  -1   | StackFrame::WASM   |
-//  -2   |     instance       |
-//  -3   |     feedback vector|
-//  -4   |     tiering budget |
-//  -----+--------------------+---------------------------
-//  -5   |     slot 0         |   ^
-//  -6   |     slot 1         |   |
-//       |                    | Frame slots
-//       |                    |   |
-//       |                    |   v
-//       | optional padding slot to keep the stack 16 byte aligned.
-//  -----+--------------------+  <-- stack ptr (sp)
-//
-
-// fp-8 holds the stack marker, fp-16 is the instance parameter.
-constexpr int kInstanceOffset = 2 * kSystemPointerSize;
-constexpr int kFeedbackVectorOffset = 3 * kSystemPointerSize;
-
-inline MemOperand GetStackSlot(int offset) { return MemOperand(fp, -offset); }
-
-inline MemOperand GetInstanceOperand() { return GetStackSlot(kInstanceOffset); }
-
-inline MemOperand GetMemOp(LiftoffAssembler* assm, Register addr,
-                           Register offset, uintptr_t offset_imm,
-                           bool i64_offset = false) {
-  if (!i64_offset && offset != no_reg) {
-    // extract bit[0:31] without sign extend
-    assm->ExtractBits(kScratchReg2, offset, 0, 32, false);
-    offset = kScratchReg2;
-  }
-  if (is_uint31(offset_imm)) {
-    int32_t offset_imm32 = static_cast<int32_t>(offset_imm);
-    if (offset == no_reg) return MemOperand(addr, offset_imm32);
-    assm->Add64(kScratchReg2, addr, offset);
-    return MemOperand(kScratchReg2, offset_imm32);
-  }
-  // Offset immediate does not fit in 31 bits.
-  assm->li(kScratchReg2, offset_imm);
-  assm->Add64(kScratchReg2, kScratchReg2, addr);
-  if (offset != no_reg) {
-    assm->Add64(kScratchReg2, kScratchReg2, offset);
-  }
-  return MemOperand(kScratchReg2, 0);
-}
-
-inline void Load(LiftoffAssembler* assm, LiftoffRegister dst, MemOperand src,
-                 ValueKind kind) {
-  switch (kind) {
-    case kI32:
-      assm->Lw(dst.gp(), src);
-      break;
-    case kI64:
-    case kRef:
-    case kRefNull:
-    case kRtt:
-      assm->Ld(dst.gp(), src);
-      break;
-    case kF32:
-      assm->LoadFloat(dst.fp(), src);
-      break;
-    case kF64:
-      assm->LoadDouble(dst.fp(), src);
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-
-inline void Store(LiftoffAssembler* assm, Register base, int32_t offset,
-                  LiftoffRegister src, ValueKind kind) {
-  MemOperand dst(base, offset);
-  switch (kind) {
-    case kI32:
-      assm->Sw(src.gp(), dst);
-      break;
-    case kI64:
-    case kRefNull:
-    case kRef:
-    case kRtt:
-      assm->Sd(src.gp(), dst);
-      break;
-    case kF32:
-      assm->StoreFloat(src.fp(), dst);
-      break;
-    case kF64:
-      assm->StoreDouble(src.fp(), dst);
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-
-inline void push(LiftoffAssembler* assm, LiftoffRegister reg, ValueKind kind) {
-  switch (kind) {
-    case kI32:
-      assm->addi(sp, sp, -kSystemPointerSize);
-      assm->Sw(reg.gp(), MemOperand(sp, 0));
-      break;
-    case kI64:
-    case kRefNull:
-    case kRef:
-    case kRtt:
-      assm->push(reg.gp());
-      break;
-    case kF32:
-      assm->addi(sp, sp, -kSystemPointerSize);
-      assm->StoreFloat(reg.fp(), MemOperand(sp, 0));
-      break;
-    case kF64:
-      assm->addi(sp, sp, -kSystemPointerSize);
-      assm->StoreDouble(reg.fp(), MemOperand(sp, 0));
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-
-#if defined(V8_TARGET_BIG_ENDIAN)
-inline void ChangeEndiannessLoad(LiftoffAssembler* assm, LiftoffRegister dst,
-                                 LoadType type, LiftoffRegList pinned) {
-  bool is_float = false;
-  LiftoffRegister tmp = dst;
-  switch (type.value()) {
-    case LoadType::kI64Load8U:
-    case LoadType::kI64Load8S:
-    case LoadType::kI32Load8U:
-    case LoadType::kI32Load8S:
-      // No need to change endianness for byte size.
-      return;
-    case LoadType::kF32Load:
-      is_float = true;
-      tmp = assm->GetUnusedRegister(kGpReg, pinned);
-      assm->emit_type_conversion(kExprI32ReinterpretF32, tmp, dst);
-      V8_FALLTHROUGH;
-    case LoadType::kI64Load32U:
-      assm->TurboAssembler::ByteSwapUnsigned(tmp.gp(), tmp.gp(), 4);
-      break;
-    case LoadType::kI32Load:
-    case LoadType::kI64Load32S:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 4);
-      break;
-    case LoadType::kI32Load16S:
-    case LoadType::kI64Load16S:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 2);
-      break;
-    case LoadType::kI32Load16U:
-    case LoadType::kI64Load16U:
-      assm->TurboAssembler::ByteSwapUnsigned(tmp.gp(), tmp.gp(), 2);
-      break;
-    case LoadType::kF64Load:
-      is_float = true;
-      tmp = assm->GetUnusedRegister(kGpReg, pinned);
-      assm->emit_type_conversion(kExprI64ReinterpretF64, tmp, dst);
-      V8_FALLTHROUGH;
-    case LoadType::kI64Load:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 8);
-      break;
-    default:
-      UNREACHABLE();
-  }
-
-  if (is_float) {
-    switch (type.value()) {
-      case LoadType::kF32Load:
-        assm->emit_type_conversion(kExprF32ReinterpretI32, dst, tmp);
-        break;
-      case LoadType::kF64Load:
-        assm->emit_type_conversion(kExprF64ReinterpretI64, dst, tmp);
-        break;
-      default:
-        UNREACHABLE();
-    }
-  }
-}
-
-inline void ChangeEndiannessStore(LiftoffAssembler* assm, LiftoffRegister src,
-                                  StoreType type, LiftoffRegList pinned) {
-  bool is_float = false;
-  LiftoffRegister tmp = src;
-  switch (type.value()) {
-    case StoreType::kI64Store8:
-    case StoreType::kI32Store8:
-      // No need to change endianness for byte size.
-      return;
-    case StoreType::kF32Store:
-      is_float = true;
-      tmp = assm->GetUnusedRegister(kGpReg, pinned);
-      assm->emit_type_conversion(kExprI32ReinterpretF32, tmp, src);
-      V8_FALLTHROUGH;
-    case StoreType::kI32Store:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 4);
-      break;
-    case StoreType::kI32Store16:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 2);
-      break;
-    case StoreType::kF64Store:
-      is_float = true;
-      tmp = assm->GetUnusedRegister(kGpReg, pinned);
-      assm->emit_type_conversion(kExprI64ReinterpretF64, tmp, src);
-      V8_FALLTHROUGH;
-    case StoreType::kI64Store:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 8);
-      break;
-    case StoreType::kI64Store32:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 4);
-      break;
-    case StoreType::kI64Store16:
-      assm->TurboAssembler::ByteSwapSigned(tmp.gp(), tmp.gp(), 2);
-      break;
-    default:
-      UNREACHABLE();
-  }
-
-  if (is_float) {
-    switch (type.value()) {
-      case StoreType::kF32Store:
-        assm->emit_type_conversion(kExprF32ReinterpretI32, src, tmp);
-        break;
-      case StoreType::kF64Store:
-        assm->emit_type_conversion(kExprF64ReinterpretI64, src, tmp);
-        break;
-      default:
-        UNREACHABLE();
-    }
-  }
-}
-#endif  // V8_TARGET_BIG_ENDIAN
-
 }  // namespace liftoff
-
 int LiftoffAssembler::PrepareStackFrame() {
   int offset = pc_offset();
   // When the frame size is bigger than 4KB, we need two instructions for
@@ -301,20 +66,21 @@ void LiftoffAssembler::PrepareTailCall(int num_callee_stack_params,
   Register scratch = temps.Acquire();
 
   // Push the return address and frame pointer to complete the stack frame.
-  Ld(scratch, MemOperand(fp, 8));
+  LoadWord(scratch, MemOperand(fp, kSystemPointerSize));
   Push(scratch);
-  Ld(scratch, MemOperand(fp, 0));
+  LoadWord(scratch, MemOperand(fp, 0));
   Push(scratch);
 
   // Shift the whole frame upwards.
   int slot_count = num_callee_stack_params + 2;
   for (int i = slot_count - 1; i >= 0; --i) {
-    Ld(scratch, MemOperand(sp, i * 8));
-    Sd(scratch, MemOperand(fp, (i - stack_param_delta) * 8));
+    LoadWord(scratch, MemOperand(sp, i * kSystemPointerSize));
+    StoreWord(scratch,
+              MemOperand(fp, (i - stack_param_delta) * kSystemPointerSize));
   }
 
   // Set the new stack and frame pointer.
-  Add64(sp, fp, -stack_param_delta * 8);
+  AddWord(sp, fp, -stack_param_delta * kSystemPointerSize);
   Pop(ra, fp);
 }
 
@@ -336,7 +102,7 @@ void LiftoffAssembler::PatchPrepareStackFrame(
   if (V8_LIKELY(frame_size < 4 * KB)) {
     // This is the standard case for small frames: just subtract from SP and be
     // done with it.
-    patching_assembler.Add64(sp, sp, Operand(-frame_size));
+    patching_assembler.AddWord(sp, sp, Operand(-frame_size));
     return;
   }
 
@@ -344,7 +110,7 @@ void LiftoffAssembler::PatchPrepareStackFrame(
   // space if we first allocate the frame and then do the stack check (we will
   // need some remaining stack space for throwing the exception). That's why we
   // check the available stack space before we allocate the frame. To do this we
-  // replace the {__ Add64(sp, sp, -frame_size)} with a jump to OOL code that
+  // replace the {__ AddWord(sp, sp, -frame_size)} with a jump to OOL code that
   // does this "extended stack check".
   //
   // The OOL code can simply be generated here with the normal assembler,
@@ -365,11 +131,11 @@ void LiftoffAssembler::PatchPrepareStackFrame(
   Label continuation;
   if (frame_size < FLAG_stack_size * 1024) {
     Register stack_limit = kScratchReg;
-    Ld(stack_limit,
-       FieldMemOperand(kWasmInstanceRegister,
-                       WasmInstanceObject::kRealStackLimitAddressOffset));
-    Ld(stack_limit, MemOperand(stack_limit));
-    Add64(stack_limit, stack_limit, Operand(frame_size));
+    LoadWord(stack_limit,
+             FieldMemOperand(kWasmInstanceRegister,
+                             WasmInstanceObject::kRealStackLimitAddressOffset));
+    LoadWord(stack_limit, MemOperand(stack_limit));
+    AddWord(stack_limit, stack_limit, Operand(frame_size));
     Branch(&continuation, uge, sp, Operand(stack_limit));
   }
 
@@ -382,10 +148,10 @@ void LiftoffAssembler::PatchPrepareStackFrame(
 
   // Now allocate the stack space. Note that this might do more than just
   // decrementing the SP;
-  Add64(sp, sp, Operand(-frame_size));
+  AddWord(sp, sp, Operand(-frame_size));
 
   // Jump back to the start of the function, from {pc_offset()} to
-  // right after the reserved space for the {__ Add64(sp, sp, -framesize)}
+  // right after the reserved space for the {__ AddWord(sp, sp, -framesize)}
   // (which is a Branch now).
   int func_start_offset = offset + 2 * kInstrSize;
   imm32 = func_start_offset - pc_offset();
@@ -420,30 +186,8 @@ bool LiftoffAssembler::NeedsAlignment(ValueKind kind) {
   }
 }
 
-void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value,
-                                    RelocInfo::Mode rmode) {
-  switch (value.type().kind()) {
-    case kI32:
-      TurboAssembler::li(reg.gp(), Operand(value.to_i32(), rmode));
-      break;
-    case kI64:
-      TurboAssembler::li(reg.gp(), Operand(value.to_i64(), rmode));
-      break;
-    case kF32:
-      TurboAssembler::LoadFPRImmediate(reg.fp(),
-                                       value.to_f32_boxed().get_bits());
-      break;
-    case kF64:
-      TurboAssembler::LoadFPRImmediate(reg.fp(),
-                                       value.to_f64_boxed().get_bits());
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-
 void LiftoffAssembler::LoadInstanceFromFrame(Register dst) {
-  Ld(dst, liftoff::GetInstanceOperand());
+  LoadWord(dst, liftoff::GetInstanceOperand());
 }
 
 void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
@@ -458,7 +202,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
       Lw(dst, MemOperand(src));
       break;
     case 8:
-      Ld(dst, MemOperand(src));
+      LoadWord(dst, MemOperand(src));
       break;
     default:
       UNIMPLEMENTED();
@@ -473,930 +217,11 @@ void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
 }
 
 void LiftoffAssembler::SpillInstance(Register instance) {
-  Sd(instance, liftoff::GetInstanceOperand());
+  StoreWord(instance, liftoff::GetInstanceOperand());
 }
 
 void LiftoffAssembler::ResetOSRTarget() {}
 
-void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
-                                         Register offset_reg,
-                                         int32_t offset_imm) {
-  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
-  LoadTaggedPointerField(dst, src_op);
-}
-
-void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,
-                                       int32_t offset_imm) {
-  MemOperand src_op = liftoff::GetMemOp(this, src_addr, no_reg, offset_imm);
-  Ld(dst, src_op);
-}
-
-void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
-                                          Register offset_reg,
-                                          int32_t offset_imm,
-                                          LiftoffRegister src,
-                                          LiftoffRegList pinned,
-                                          SkipWriteBarrier skip_write_barrier) {
-  Register scratch = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
-  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
-  StoreTaggedField(src.gp(), dst_op);
-
-  if (skip_write_barrier || FLAG_disable_write_barriers) return;
-
-  Label write_barrier;
-  Label exit;
-  CheckPageFlag(dst_addr, scratch,
-                MemoryChunk::kPointersFromHereAreInterestingMask, ne,
-                &write_barrier);
-  Branch(&exit);
-  bind(&write_barrier);
-  JumpIfSmi(src.gp(), &exit);
-  CheckPageFlag(src.gp(), scratch,
-                MemoryChunk::kPointersToHereAreInterestingMask, eq, &exit);
-  Add64(scratch, dst_op.rm(), dst_op.offset());
-  CallRecordWriteStubSaveRegisters(dst_addr, scratch, SaveFPRegsMode::kSave,
-                                   StubCallMode::kCallWasmRuntimeStub);
-  bind(&exit);
-}
-
-void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
-                            Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, uint32_t* protected_load_pc,
-                            bool is_load_mem, bool i64_offset) {
-  MemOperand src_op =
-      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, i64_offset);
-
-  if (protected_load_pc) *protected_load_pc = pc_offset();
-  switch (type.value()) {
-    case LoadType::kI32Load8U:
-    case LoadType::kI64Load8U:
-      Lbu(dst.gp(), src_op);
-      break;
-    case LoadType::kI32Load8S:
-    case LoadType::kI64Load8S:
-      Lb(dst.gp(), src_op);
-      break;
-    case LoadType::kI32Load16U:
-    case LoadType::kI64Load16U:
-      TurboAssembler::Lhu(dst.gp(), src_op);
-      break;
-    case LoadType::kI32Load16S:
-    case LoadType::kI64Load16S:
-      TurboAssembler::Lh(dst.gp(), src_op);
-      break;
-    case LoadType::kI64Load32U:
-      TurboAssembler::Lwu(dst.gp(), src_op);
-      break;
-    case LoadType::kI32Load:
-    case LoadType::kI64Load32S:
-      TurboAssembler::Lw(dst.gp(), src_op);
-      break;
-    case LoadType::kI64Load:
-      TurboAssembler::Ld(dst.gp(), src_op);
-      break;
-    case LoadType::kF32Load:
-      TurboAssembler::LoadFloat(dst.fp(), src_op);
-      break;
-    case LoadType::kF64Load:
-      TurboAssembler::LoadDouble(dst.fp(), src_op);
-      break;
-    case LoadType::kS128Load: {
-      VU.set(kScratchReg, E8, m1);
-      Register src_reg = src_op.offset() == 0 ? src_op.rm() : kScratchReg;
-      if (src_op.offset() != 0) {
-        TurboAssembler::Add64(src_reg, src_op.rm(), src_op.offset());
-      }
-      vl(dst.fp().toV(), src_reg, 0, E8);
-      break;
-    }
-    default:
-      UNREACHABLE();
-  }
-
-#if defined(V8_TARGET_BIG_ENDIAN)
-  if (is_load_mem) {
-    pinned.set(src_op.rm());
-    liftoff::ChangeEndiannessLoad(this, dst, type, pinned);
-  }
-#endif
-}
-
-void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
-                             uintptr_t offset_imm, LiftoffRegister src,
-                             StoreType type, LiftoffRegList pinned,
-                             uint32_t* protected_store_pc, bool is_store_mem,
-                             bool i64_offset) {
-  MemOperand dst_op =
-      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, i64_offset);
-
-#if defined(V8_TARGET_BIG_ENDIAN)
-  if (is_store_mem) {
-    pinned.set(dst_op.rm());
-    LiftoffRegister tmp = GetUnusedRegister(src.reg_class(), pinned);
-    // Save original value.
-    Move(tmp, src, type.value_type());
-
-    src = tmp;
-    pinned.set(tmp);
-    liftoff::ChangeEndiannessStore(this, src, type, pinned);
-  }
-#endif
-
-  if (protected_store_pc) *protected_store_pc = pc_offset();
-
-  switch (type.value()) {
-    case StoreType::kI32Store8:
-    case StoreType::kI64Store8:
-      Sb(src.gp(), dst_op);
-      break;
-    case StoreType::kI32Store16:
-    case StoreType::kI64Store16:
-      TurboAssembler::Sh(src.gp(), dst_op);
-      break;
-    case StoreType::kI32Store:
-    case StoreType::kI64Store32:
-      TurboAssembler::Sw(src.gp(), dst_op);
-      break;
-    case StoreType::kI64Store:
-      TurboAssembler::Sd(src.gp(), dst_op);
-      break;
-    case StoreType::kF32Store:
-      TurboAssembler::StoreFloat(src.fp(), dst_op);
-      break;
-    case StoreType::kF64Store:
-      TurboAssembler::StoreDouble(src.fp(), dst_op);
-      break;
-    case StoreType::kS128Store: {
-      VU.set(kScratchReg, E8, m1);
-      Register dst_reg = dst_op.offset() == 0 ? dst_op.rm() : kScratchReg;
-      if (dst_op.offset() != 0) {
-        Add64(kScratchReg, dst_op.rm(), dst_op.offset());
-      }
-      vs(src.fp().toV(), dst_reg, 0, VSew::E8);
-      break;
-    }
-    default:
-      UNREACHABLE();
-  }
-}
-
-namespace liftoff {
-#define __ lasm->
-
-inline Register CalculateActualAddress(LiftoffAssembler* lasm,
-                                       Register addr_reg, Register offset_reg,
-                                       uintptr_t offset_imm,
-                                       Register result_reg) {
-  DCHECK_NE(offset_reg, no_reg);
-  DCHECK_NE(addr_reg, no_reg);
-  __ Add64(result_reg, addr_reg, Operand(offset_reg));
-  if (offset_imm != 0) {
-    __ Add64(result_reg, result_reg, Operand(offset_imm));
-  }
-  return result_reg;
-}
-
-enum class Binop { kAdd, kSub, kAnd, kOr, kXor, kExchange };
-
-inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,
-                        Register offset_reg, uintptr_t offset_imm,
-                        LiftoffRegister value, LiftoffRegister result,
-                        StoreType type, Binop op) {
-  LiftoffRegList pinned{dst_addr, offset_reg, value, result};
-  Register store_result = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
-
-  // Make sure that {result} is unique.
-  Register result_reg = result.gp();
-  if (result_reg == value.gp() || result_reg == dst_addr ||
-      result_reg == offset_reg) {
-    result_reg = __ GetUnusedRegister(kGpReg, pinned).gp();
-  }
-
-  UseScratchRegisterScope temps(lasm);
-  Register actual_addr = liftoff::CalculateActualAddress(
-      lasm, dst_addr, offset_reg, offset_imm, temps.Acquire());
-
-  // Allocate an additional {temp} register to hold the result that should be
-  // stored to memory. Note that {temp} and {store_result} are not allowed to be
-  // the same register.
-  Register temp = temps.Acquire();
-
-  Label retry;
-  __ bind(&retry);
-  switch (type.value()) {
-    case StoreType::kI64Store8:
-    case StoreType::kI32Store8:
-      __ lbu(result_reg, actual_addr, 0);
-      __ sync();
-      break;
-    case StoreType::kI64Store16:
-    case StoreType::kI32Store16:
-      __ lhu(result_reg, actual_addr, 0);
-      __ sync();
-      break;
-    case StoreType::kI64Store32:
-    case StoreType::kI32Store:
-      __ lr_w(true, false, result_reg, actual_addr);
-      break;
-    case StoreType::kI64Store:
-      __ lr_d(true, false, result_reg, actual_addr);
-      break;
-    default:
-      UNREACHABLE();
-  }
-
-  switch (op) {
-    case Binop::kAdd:
-      __ add(temp, result_reg, value.gp());
-      break;
-    case Binop::kSub:
-      __ sub(temp, result_reg, value.gp());
-      break;
-    case Binop::kAnd:
-      __ and_(temp, result_reg, value.gp());
-      break;
-    case Binop::kOr:
-      __ or_(temp, result_reg, value.gp());
-      break;
-    case Binop::kXor:
-      __ xor_(temp, result_reg, value.gp());
-      break;
-    case Binop::kExchange:
-      __ mv(temp, value.gp());
-      break;
-  }
-  switch (type.value()) {
-    case StoreType::kI64Store8:
-    case StoreType::kI32Store8:
-      __ sync();
-      __ sb(temp, actual_addr, 0);
-      __ sync();
-      __ mv(store_result, zero_reg);
-      break;
-    case StoreType::kI64Store16:
-    case StoreType::kI32Store16:
-      __ sync();
-      __ sh(temp, actual_addr, 0);
-      __ sync();
-      __ mv(store_result, zero_reg);
-      break;
-    case StoreType::kI64Store32:
-    case StoreType::kI32Store:
-      __ sc_w(false, true, store_result, actual_addr, temp);
-      break;
-    case StoreType::kI64Store:
-      __ sc_w(false, true, store_result, actual_addr, temp);
-      break;
-    default:
-      UNREACHABLE();
-  }
-
-  __ bnez(store_result, &retry);
-  if (result_reg != result.gp()) {
-    __ mv(result.gp(), result_reg);
-  }
-}
-
-#undef __
-}  // namespace liftoff
-
-void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
-                                  Register offset_reg, uintptr_t offset_imm,
-                                  LoadType type, LiftoffRegList pinned) {
-  UseScratchRegisterScope temps(this);
-  Register src_reg = liftoff::CalculateActualAddress(
-      this, src_addr, offset_reg, offset_imm, temps.Acquire());
-  switch (type.value()) {
-    case LoadType::kI32Load8U:
-    case LoadType::kI64Load8U:
-      lbu(dst.gp(), src_reg, 0);
-      sync();
-      return;
-    case LoadType::kI32Load16U:
-    case LoadType::kI64Load16U:
-      lhu(dst.gp(), src_reg, 0);
-      sync();
-      return;
-    case LoadType::kI32Load:
-    case LoadType::kI64Load32U:
-      lw(dst.gp(), src_reg, 0);
-      sync();
-      return;
-    case LoadType::kI64Load:
-      ld(dst.gp(), src_reg, 0);
-      sync();
-      return;
-    default:
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
-                                   uintptr_t offset_imm, LiftoffRegister src,
-                                   StoreType type, LiftoffRegList pinned) {
-  UseScratchRegisterScope temps(this);
-  Register dst_reg = liftoff::CalculateActualAddress(
-      this, dst_addr, offset_reg, offset_imm, temps.Acquire());
-  switch (type.value()) {
-    case StoreType::kI64Store8:
-    case StoreType::kI32Store8:
-      sync();
-      sb(src.gp(), dst_reg, 0);
-      return;
-    case StoreType::kI64Store16:
-    case StoreType::kI32Store16:
-      sync();
-      sh(src.gp(), dst_reg, 0);
-      return;
-    case StoreType::kI64Store32:
-    case StoreType::kI32Store:
-      sync();
-      sw(src.gp(), dst_reg, 0);
-      return;
-    case StoreType::kI64Store:
-      sync();
-      sd(src.gp(), dst_reg, 0);
-      return;
-    default:
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::AtomicAdd(Register dst_addr, Register offset_reg,
-                                 uintptr_t offset_imm, LiftoffRegister value,
-                                 LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kAdd);
-}
-
-void LiftoffAssembler::AtomicSub(Register dst_addr, Register offset_reg,
-                                 uintptr_t offset_imm, LiftoffRegister value,
-                                 LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kSub);
-}
-
-void LiftoffAssembler::AtomicAnd(Register dst_addr, Register offset_reg,
-                                 uintptr_t offset_imm, LiftoffRegister value,
-                                 LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kAnd);
-}
-
-void LiftoffAssembler::AtomicOr(Register dst_addr, Register offset_reg,
-                                uintptr_t offset_imm, LiftoffRegister value,
-                                LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kOr);
-}
-
-void LiftoffAssembler::AtomicXor(Register dst_addr, Register offset_reg,
-                                 uintptr_t offset_imm, LiftoffRegister value,
-                                 LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kXor);
-}
-
-void LiftoffAssembler::AtomicExchange(Register dst_addr, Register offset_reg,
-                                      uintptr_t offset_imm,
-                                      LiftoffRegister value,
-                                      LiftoffRegister result, StoreType type) {
-  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
-                       type, liftoff::Binop::kExchange);
-}
-
-#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(load_linked,       \
-                                                 store_conditional) \
-  do {                                                              \
-    Label compareExchange;                                          \
-    Label exit;                                                     \
-    sync();                                                         \
-    bind(&compareExchange);                                         \
-    load_linked(result.gp(), MemOperand(temp0, 0));                 \
-    BranchShort(&exit, ne, expected.gp(), Operand(result.gp()));    \
-    mv(temp2, new_value.gp());                                      \
-    store_conditional(temp2, MemOperand(temp0, 0));                 \
-    BranchShort(&compareExchange, ne, temp2, Operand(zero_reg));    \
-    bind(&exit);                                                    \
-    sync();                                                         \
-  } while (0)
-
-#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(            \
-    load_linked, store_conditional, size, aligned)               \
-  do {                                                           \
-    Label compareExchange;                                       \
-    Label exit;                                                  \
-    andi(temp1, temp0, aligned);                                 \
-    Sub64(temp0, temp0, Operand(temp1));                         \
-    Sll32(temp1, temp1, 3);                                      \
-    sync();                                                      \
-    bind(&compareExchange);                                      \
-    load_linked(temp2, MemOperand(temp0, 0));                    \
-    ExtractBits(result.gp(), temp2, temp1, size, false);         \
-    ExtractBits(temp2, expected.gp(), zero_reg, size, false);    \
-    BranchShort(&exit, ne, temp2, Operand(result.gp()));         \
-    InsertBits(temp2, new_value.gp(), temp1, size);              \
-    store_conditional(temp2, MemOperand(temp0, 0));              \
-    BranchShort(&compareExchange, ne, temp2, Operand(zero_reg)); \
-    bind(&exit);                                                 \
-    sync();                                                      \
-  } while (0)
-
-void LiftoffAssembler::AtomicCompareExchange(
-    Register dst_addr, Register offset_reg, uintptr_t offset_imm,
-    LiftoffRegister expected, LiftoffRegister new_value, LiftoffRegister result,
-    StoreType type) {
-  LiftoffRegList pinned{dst_addr, offset_reg, expected, new_value, result};
-  Register temp0 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
-  Register temp1 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
-  Register temp2 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
-  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
-  Add64(temp0, dst_op.rm(), dst_op.offset());
-  switch (type.value()) {
-    case StoreType::kI64Store8:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 8, 7);
-      break;
-    case StoreType::kI32Store8:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 8, 3);
-      break;
-    case StoreType::kI64Store16:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 16, 7);
-      break;
-    case StoreType::kI32Store16:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 16, 3);
-      break;
-    case StoreType::kI64Store32:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, 32, 7);
-      break;
-    case StoreType::kI32Store:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Ll, Sc);
-      break;
-    case StoreType::kI64Store:
-      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Lld, Scd);
-      break;
-    default:
-      UNREACHABLE();
-  }
-}
-#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER
-#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT
-
-void LiftoffAssembler::AtomicFence() { sync(); }
-
-void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,
-                                           uint32_t caller_slot_idx,
-                                           ValueKind kind) {
-  MemOperand src(fp, kSystemPointerSize * (caller_slot_idx + 1));
-  liftoff::Load(this, dst, src, kind);
-}
-
-void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,
-                                            uint32_t caller_slot_idx,
-                                            ValueKind kind) {
-  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);
-  liftoff::Store(this, fp, offset, src, kind);
-}
-
-void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,
-                                           ValueKind kind) {
-  liftoff::Load(this, dst, MemOperand(sp, offset), kind);
-}
-
-void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
-                                      ValueKind kind) {
-  DCHECK_NE(dst_offset, src_offset);
-
-  MemOperand src = liftoff::GetStackSlot(src_offset);
-  MemOperand dst = liftoff::GetStackSlot(dst_offset);
-  switch (kind) {
-    case kI32:
-      Lw(kScratchReg, src);
-      Sw(kScratchReg, dst);
-      break;
-    case kI64:
-    case kRef:
-    case kRefNull:
-    case kRtt:
-      Ld(kScratchReg, src);
-      Sd(kScratchReg, dst);
-      break;
-    case kF32:
-      LoadFloat(kScratchDoubleReg, src);
-      StoreFloat(kScratchDoubleReg, dst);
-      break;
-    case kF64:
-      TurboAssembler::LoadDouble(kScratchDoubleReg, src);
-      TurboAssembler::StoreDouble(kScratchDoubleReg, dst);
-      break;
-    case kS128: {
-      VU.set(kScratchReg, E8, m1);
-      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
-      if (src.offset() != 0) {
-        TurboAssembler::Add64(src_reg, src.rm(), src.offset());
-      }
-      vl(kSimd128ScratchReg, src_reg, 0, E8);
-      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
-      if (dst.offset() != 0) {
-        Add64(kScratchReg, dst.rm(), dst.offset());
-      }
-      vs(kSimd128ScratchReg, dst_reg, 0, VSew::E8);
-      break;
-    }
-    case kVoid:
-    case kI8:
-    case kI16:
-    case kBottom:
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) {
-  DCHECK_NE(dst, src);
-  // TODO(ksreten): Handle different sizes here.
-  TurboAssembler::Move(dst, src);
-}
-
-void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
-                            ValueKind kind) {
-  DCHECK_NE(dst, src);
-  if (kind != kS128) {
-    TurboAssembler::Move(dst, src);
-  } else {
-    TurboAssembler::vmv_vv(dst.toV(), dst.toV());
-  }
-}
-
-void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueKind kind) {
-  RecordUsedSpillOffset(offset);
-  MemOperand dst = liftoff::GetStackSlot(offset);
-  switch (kind) {
-    case kI32:
-      Sw(reg.gp(), dst);
-      break;
-    case kI64:
-    case kRef:
-    case kRefNull:
-    case kRtt:
-      Sd(reg.gp(), dst);
-      break;
-    case kF32:
-      StoreFloat(reg.fp(), dst);
-      break;
-    case kF64:
-      TurboAssembler::StoreDouble(reg.fp(), dst);
-      break;
-    case kS128: {
-      VU.set(kScratchReg, E8, m1);
-      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
-      if (dst.offset() != 0) {
-        Add64(kScratchReg, dst.rm(), dst.offset());
-      }
-      vs(reg.fp().toV(), dst_reg, 0, VSew::E8);
-      break;
-    }
-    default:
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::Spill(int offset, WasmValue value) {
-  RecordUsedSpillOffset(offset);
-  MemOperand dst = liftoff::GetStackSlot(offset);
-  switch (value.type().kind()) {
-    case kI32: {
-      UseScratchRegisterScope temps(this);
-      Register tmp = temps.Acquire();
-      TurboAssembler::li(tmp, Operand(value.to_i32()));
-      Sw(tmp, dst);
-      break;
-    }
-    case kI64:
-    case kRef:
-    case kRefNull: {
-      UseScratchRegisterScope temps(this);
-      Register tmp = temps.Acquire();
-      TurboAssembler::li(tmp, value.to_i64());
-      Sd(tmp, dst);
-      break;
-    }
-    default:
-      // kWasmF32 and kWasmF64 are unreachable, since those
-      // constants are not tracked.
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) {
-  MemOperand src = liftoff::GetStackSlot(offset);
-  switch (kind) {
-    case kI32:
-      Lw(reg.gp(), src);
-      break;
-    case kI64:
-    case kRef:
-    case kRefNull:
-      Ld(reg.gp(), src);
-      break;
-    case kF32:
-      LoadFloat(reg.fp(), src);
-      break;
-    case kF64:
-      TurboAssembler::LoadDouble(reg.fp(), src);
-      break;
-    case kS128: {
-      VU.set(kScratchReg, E8, m1);
-      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
-      if (src.offset() != 0) {
-        TurboAssembler::Add64(src_reg, src.rm(), src.offset());
-      }
-      vl(reg.fp().toV(), src_reg, 0, E8);
-      break;
-    }
-    default:
-      UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {
-  UNREACHABLE();
-}
-
-void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
-  DCHECK_LT(0, size);
-  RecordUsedSpillOffset(start + size);
-
-  if (size <= 12 * kStackSlotSize) {
-    // Special straight-line code for up to 12 slots. Generates one
-    // instruction per slot (<= 12 instructions total).
-    uint32_t remainder = size;
-    for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {
-      Sd(zero_reg, liftoff::GetStackSlot(start + remainder));
-    }
-    DCHECK(remainder == 4 || remainder == 0);
-    if (remainder) {
-      Sw(zero_reg, liftoff::GetStackSlot(start + remainder));
-    }
-  } else {
-    // General case for bigger counts (12 instructions).
-    // Use a0 for start address (inclusive), a1 for end address (exclusive).
-    Push(a1, a0);
-    Add64(a0, fp, Operand(-start - size));
-    Add64(a1, fp, Operand(-start));
-
-    Label loop;
-    bind(&loop);
-    Sd(zero_reg, MemOperand(a0));
-    addi(a0, a0, kSystemPointerSize);
-    BranchShort(&loop, ne, a0, Operand(a1));
-
-    Pop(a1, a0);
-  }
-}
-
-void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {
-  TurboAssembler::Clz64(dst.gp(), src.gp());
-}
-
-void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {
-  TurboAssembler::Ctz64(dst.gp(), src.gp());
-}
-
-bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,
-                                       LiftoffRegister src) {
-  TurboAssembler::Popcnt64(dst.gp(), src.gp(), kScratchReg);
-  return true;
-}
-
-void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {
-  TurboAssembler::Mul32(dst, lhs, rhs);
-}
-
-void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,
-                                     Label* trap_div_by_zero,
-                                     Label* trap_div_unrepresentable) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
-
-  // Check if lhs == kMinInt and rhs == -1, since this case is unrepresentable.
-  TurboAssembler::CompareI(kScratchReg, lhs, Operand(kMinInt), ne);
-  TurboAssembler::CompareI(kScratchReg2, rhs, Operand(-1), ne);
-  add(kScratchReg, kScratchReg, kScratchReg2);
-  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
-                         Operand(zero_reg));
-
-  TurboAssembler::Div32(dst, lhs, rhs);
-}
-
-void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
-  TurboAssembler::Divu32(dst, lhs, rhs);
-}
-
-void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
-  TurboAssembler::Mod32(dst, lhs, rhs);
-}
-
-void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
-  TurboAssembler::Modu32(dst, lhs, rhs);
-}
-
-#define I32_BINOP(name, instruction)                                 \
-  void LiftoffAssembler::emit_i32_##name(Register dst, Register lhs, \
-                                         Register rhs) {             \
-    instruction(dst, lhs, rhs);                                      \
-  }
-
-// clang-format off
-I32_BINOP(add, addw)
-I32_BINOP(sub, subw)
-I32_BINOP(and, and_)
-I32_BINOP(or, or_)
-I32_BINOP(xor, xor_)
-// clang-format on
-
-#undef I32_BINOP
-
-#define I32_BINOP_I(name, instruction)                                  \
-  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register lhs, \
-                                            int32_t imm) {              \
-    instruction(dst, lhs, Operand(imm));                                \
-  }
-
-// clang-format off
-I32_BINOP_I(add, Add32)
-I32_BINOP_I(sub, Sub32)
-I32_BINOP_I(and, And)
-I32_BINOP_I(or, Or)
-I32_BINOP_I(xor, Xor)
-// clang-format on
-
-#undef I32_BINOP_I
-
-void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {
-  TurboAssembler::Clz32(dst, src);
-}
-
-void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {
-  TurboAssembler::Ctz32(dst, src);
-}
-
-bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {
-  TurboAssembler::Popcnt32(dst, src, kScratchReg);
-  return true;
-}
-
-#define I32_SHIFTOP(name, instruction)                               \
-  void LiftoffAssembler::emit_i32_##name(Register dst, Register src, \
-                                         Register amount) {          \
-    instruction(dst, src, amount);                                   \
-  }
-#define I32_SHIFTOP_I(name, instruction)                                \
-  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register src, \
-                                            int amount) {               \
-    instruction(dst, src, amount & 31);                                 \
-  }
-
-I32_SHIFTOP(shl, sllw)
-I32_SHIFTOP(sar, sraw)
-I32_SHIFTOP(shr, srlw)
-
-I32_SHIFTOP_I(shl, slliw)
-I32_SHIFTOP_I(sar, sraiw)
-I32_SHIFTOP_I(shr, srliw)
-
-#undef I32_SHIFTOP
-#undef I32_SHIFTOP_I
-
-void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,
-                                    LiftoffRegister rhs) {
-  TurboAssembler::Mul64(dst.gp(), lhs.gp(), rhs.gp());
-}
-
-bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs,
-                                     Label* trap_div_by_zero,
-                                     Label* trap_div_unrepresentable) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
-
-  // Check if lhs == MinInt64 and rhs == -1, since this case is unrepresentable.
-  TurboAssembler::CompareI(kScratchReg, lhs.gp(),
-                           Operand(std::numeric_limits<int64_t>::min()), ne);
-  TurboAssembler::CompareI(kScratchReg2, rhs.gp(), Operand(-1), ne);
-  add(kScratchReg, kScratchReg, kScratchReg2);
-  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
-                         Operand(zero_reg));
-
-  TurboAssembler::Div64(dst.gp(), lhs.gp(), rhs.gp());
-  return true;
-}
-
-bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
-  TurboAssembler::Divu64(dst.gp(), lhs.gp(), rhs.gp());
-  return true;
-}
-
-bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
-  TurboAssembler::Mod64(dst.gp(), lhs.gp(), rhs.gp());
-  return true;
-}
-
-bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,
-                                     LiftoffRegister rhs,
-                                     Label* trap_div_by_zero) {
-  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
-  TurboAssembler::Modu64(dst.gp(), lhs.gp(), rhs.gp());
-  return true;
-}
-
-#define I64_BINOP(name, instruction)                                   \
-  void LiftoffAssembler::emit_i64_##name(                              \
-      LiftoffRegister dst, LiftoffRegister lhs, LiftoffRegister rhs) { \
-    instruction(dst.gp(), lhs.gp(), rhs.gp());                         \
-  }
-
-// clang-format off
-I64_BINOP(add, add)
-I64_BINOP(sub, sub)
-I64_BINOP(and, and_)
-I64_BINOP(or, or_)
-I64_BINOP(xor, xor_)
-// clang-format on
-
-#undef I64_BINOP
-
-#define I64_BINOP_I(name, instruction)                         \
-  void LiftoffAssembler::emit_i64_##name##i(                   \
-      LiftoffRegister dst, LiftoffRegister lhs, int32_t imm) { \
-    instruction(dst.gp(), lhs.gp(), Operand(imm));             \
-  }
-
-// clang-format off
-I64_BINOP_I(and, And)
-I64_BINOP_I(or, Or)
-I64_BINOP_I(xor, Xor)
-// clang-format on
-
-#undef I64_BINOP_I
-
-#define I64_SHIFTOP(name, instruction)                             \
-  void LiftoffAssembler::emit_i64_##name(                          \
-      LiftoffRegister dst, LiftoffRegister src, Register amount) { \
-    instruction(dst.gp(), src.gp(), amount);                       \
-  }
-
-I64_SHIFTOP(shl, sll)
-I64_SHIFTOP(sar, sra)
-I64_SHIFTOP(shr, srl)
-#undef I64_SHIFTOP
-
-void LiftoffAssembler::emit_i64_shli(LiftoffRegister dst, LiftoffRegister src,
-                                     int amount) {
-  if (is_uint6(amount)) {
-    slli(dst.gp(), src.gp(), amount);
-  } else {
-    li(kScratchReg, amount);
-    sll(dst.gp(), src.gp(), kScratchReg);
-  }
-}
-
-void LiftoffAssembler::emit_i64_sari(LiftoffRegister dst, LiftoffRegister src,
-                                     int amount) {
-  if (is_uint6(amount)) {
-    srai(dst.gp(), src.gp(), amount);
-  } else {
-    li(kScratchReg, amount);
-    sra(dst.gp(), src.gp(), kScratchReg);
-  }
-}
-
-void LiftoffAssembler::emit_i64_shri(LiftoffRegister dst, LiftoffRegister src,
-                                     int amount) {
-  if (is_uint6(amount)) {
-    srli(dst.gp(), src.gp(), amount);
-  } else {
-    li(kScratchReg, amount);
-    srl(dst.gp(), src.gp(), kScratchReg);
-  }
-}
-
-void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,
-                                     int64_t imm) {
-  TurboAssembler::Add64(dst.gp(), lhs.gp(), Operand(imm));
-}
-void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) {
-  ZeroExtendWord(dst, src);
-}
-
 void LiftoffAssembler::emit_f32_neg(DoubleRegister dst, DoubleRegister src) {
   TurboAssembler::Neg_s(dst, src);
 }
@@ -1465,245 +290,11 @@ FP_BINOP(f64_sub, fsub_d)
 FP_BINOP(f64_mul, fmul_d)
 FP_BINOP(f64_div, fdiv_d)
 FP_UNOP(f64_abs, fabs_d)
-FP_UNOP_RETURN_TRUE(f64_ceil, Ceil_d_d)
-FP_UNOP_RETURN_TRUE(f64_floor, Floor_d_d)
-FP_UNOP_RETURN_TRUE(f64_trunc, Trunc_d_d)
-FP_UNOP_RETURN_TRUE(f64_nearest_int, Round_d_d)
 FP_UNOP(f64_sqrt, fsqrt_d)
-
 #undef FP_BINOP
 #undef FP_UNOP
 #undef FP_UNOP_RETURN_TRUE
 
-bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
-                                            LiftoffRegister dst,
-                                            LiftoffRegister src, Label* trap) {
-  switch (opcode) {
-    case kExprI32ConvertI64:
-      // According to WebAssembly spec, if I64 value does not fit the range of
-      // I32, the value is undefined. Therefore, We use sign extension to
-      // implement I64 to I32 truncation
-      TurboAssembler::SignExtendWord(dst.gp(), src.gp());
-      return true;
-    case kExprI32SConvertF32:
-    case kExprI32UConvertF32:
-    case kExprI32SConvertF64:
-    case kExprI32UConvertF64:
-    case kExprI64SConvertF32:
-    case kExprI64UConvertF32:
-    case kExprI64SConvertF64:
-    case kExprI64UConvertF64:
-    case kExprF32ConvertF64: {
-      // real conversion, if src is out-of-bound of target integer types,
-      // kScratchReg is set to 0
-      switch (opcode) {
-        case kExprI32SConvertF32:
-          Trunc_w_s(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI32UConvertF32:
-          Trunc_uw_s(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI32SConvertF64:
-          Trunc_w_d(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI32UConvertF64:
-          Trunc_uw_d(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI64SConvertF32:
-          Trunc_l_s(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI64UConvertF32:
-          Trunc_ul_s(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI64SConvertF64:
-          Trunc_l_d(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprI64UConvertF64:
-          Trunc_ul_d(dst.gp(), src.fp(), kScratchReg);
-          break;
-        case kExprF32ConvertF64:
-          fcvt_s_d(dst.fp(), src.fp());
-          break;
-        default:
-          UNREACHABLE();
-      }
-
-      // Checking if trap.
-      if (trap != nullptr) {
-        TurboAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));
-      }
-
-      return true;
-    }
-    case kExprI32ReinterpretF32:
-      TurboAssembler::ExtractLowWordFromF64(dst.gp(), src.fp());
-      return true;
-    case kExprI64SConvertI32:
-      TurboAssembler::SignExtendWord(dst.gp(), src.gp());
-      return true;
-    case kExprI64UConvertI32:
-      TurboAssembler::ZeroExtendWord(dst.gp(), src.gp());
-      return true;
-    case kExprI64ReinterpretF64:
-      fmv_x_d(dst.gp(), src.fp());
-      return true;
-    case kExprF32SConvertI32: {
-      TurboAssembler::Cvt_s_w(dst.fp(), src.gp());
-      return true;
-    }
-    case kExprF32UConvertI32:
-      TurboAssembler::Cvt_s_uw(dst.fp(), src.gp());
-      return true;
-    case kExprF32ReinterpretI32:
-      fmv_w_x(dst.fp(), src.gp());
-      return true;
-    case kExprF64SConvertI32: {
-      TurboAssembler::Cvt_d_w(dst.fp(), src.gp());
-      return true;
-    }
-    case kExprF64UConvertI32:
-      TurboAssembler::Cvt_d_uw(dst.fp(), src.gp());
-      return true;
-    case kExprF64ConvertF32:
-      fcvt_d_s(dst.fp(), src.fp());
-      return true;
-    case kExprF64ReinterpretI64:
-      fmv_d_x(dst.fp(), src.gp());
-      return true;
-    case kExprI32SConvertSatF32: {
-      fcvt_w_s(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_s(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI32UConvertSatF32: {
-      fcvt_wu_s(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_s(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI32SConvertSatF64: {
-      fcvt_w_d(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_d(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI32UConvertSatF64: {
-      fcvt_wu_d(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_d(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI64SConvertSatF32: {
-      fcvt_l_s(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_s(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI64UConvertSatF32: {
-      fcvt_lu_s(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_s(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI64SConvertSatF64: {
-      fcvt_l_d(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_d(dst.gp(), src.fp());
-      return true;
-    }
-    case kExprI64UConvertSatF64: {
-      fcvt_lu_d(dst.gp(), src.fp(), RTZ);
-      Clear_if_nan_d(dst.gp(), src.fp());
-      return true;
-    }
-    default:
-      return false;
-  }
-}
-
-void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
-  slliw(dst, src, 32 - 8);
-  sraiw(dst, dst, 32 - 8);
-}
-
-void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {
-  slliw(dst, src, 32 - 16);
-  sraiw(dst, dst, 32 - 16);
-}
-
-void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,
-                                              LiftoffRegister src) {
-  slli(dst.gp(), src.gp(), 64 - 8);
-  srai(dst.gp(), dst.gp(), 64 - 8);
-}
-
-void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,
-                                               LiftoffRegister src) {
-  slli(dst.gp(), src.gp(), 64 - 16);
-  srai(dst.gp(), dst.gp(), 64 - 16);
-}
-
-void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,
-                                               LiftoffRegister src) {
-  slli(dst.gp(), src.gp(), 64 - 32);
-  srai(dst.gp(), dst.gp(), 64 - 32);
-}
-
-void LiftoffAssembler::emit_jump(Label* label) {
-  TurboAssembler::Branch(label);
-}
-
-void LiftoffAssembler::emit_jump(Register target) {
-  TurboAssembler::Jump(target);
-}
-
-void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
-                                      Label* label, ValueKind kind,
-                                      Register lhs, Register rhs,
-                                      const FreezeCacheState& frozen) {
-  Condition cond = liftoff::ToCondition(liftoff_cond);
-  if (rhs == no_reg) {
-    DCHECK(kind == kI32 || kind == kI64);
-    TurboAssembler::Branch(label, cond, lhs, Operand(zero_reg));
-  } else {
-    DCHECK((kind == kI32 || kind == kI64) ||
-           (is_reference(kind) &&
-            (liftoff_cond == kEqual || liftoff_cond == kUnequal)));
-    TurboAssembler::Branch(label, cond, lhs, Operand(rhs));
-  }
-}
-
-void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
-                                           Label* label, Register lhs,
-                                           int32_t imm,
-                                           const FreezeCacheState& frozen) {
-  Condition cond = liftoff::ToCondition(liftoff_cond);
-  TurboAssembler::Branch(label, cond, lhs, Operand(imm));
-}
-
-void LiftoffAssembler::emit_i32_subi_jump_negative(
-    Register value, int subtrahend, Label* result_negative,
-    const FreezeCacheState& frozen) {
-  Sub64(value, value, Operand(subtrahend));
-  TurboAssembler::Branch(result_negative, lt, value, Operand(zero_reg));
-}
-
-void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {
-  TurboAssembler::Sltu(dst, src, 1);
-}
-
-void LiftoffAssembler::emit_i32_set_cond(LiftoffCondition liftoff_cond,
-                                         Register dst, Register lhs,
-                                         Register rhs) {
-  Condition cond = liftoff::ToCondition(liftoff_cond);
-  TurboAssembler::CompareI(dst, lhs, Operand(rhs), cond);
-}
-
-void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {
-  TurboAssembler::Sltu(dst, src.gp(), 1);
-}
-
-void LiftoffAssembler::emit_i64_set_cond(LiftoffCondition liftoff_cond,
-                                         Register dst, LiftoffRegister lhs,
-                                         LiftoffRegister rhs) {
-  Condition cond = liftoff::ToCondition(liftoff_cond);
-  TurboAssembler::CompareI(dst, lhs.gp(), Operand(rhs.gp()), cond);
-}
-
 static FPUCondition ConditionToConditionCmpFPU(LiftoffCondition condition) {
   switch (condition) {
     case kEqual:
@@ -1755,206 +346,6 @@ void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
   Branch(target, condition, scratch, Operand(zero_reg));
 }
 
-void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) {
-  UseScratchRegisterScope temps(this);
-  if (COMPRESS_POINTERS_BOOL) {
-    DCHECK(SmiValuesAre31Bits());
-    Register scratch = temps.Acquire();
-    Lw(scratch, MemOperand(dst.gp(), offset));
-    Add32(scratch, scratch, Operand(Smi::FromInt(1)));
-    Sw(scratch, MemOperand(dst.gp(), offset));
-  } else {
-    Register scratch = temps.Acquire();
-    SmiUntag(scratch, MemOperand(dst.gp(), offset));
-    Add64(scratch, scratch, Operand(1));
-    SmiTag(scratch);
-    Sd(scratch, MemOperand(dst.gp(), offset));
-  }
-}
-
-void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
-                                     Register offset_reg, uintptr_t offset_imm,
-                                     LoadType type,
-                                     LoadTransformationKind transform,
-                                     uint32_t* protected_load_pc) {
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.Acquire();
-  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
-  VRegister dst_v = dst.fp().toV();
-  *protected_load_pc = pc_offset();
-
-  MachineType memtype = type.mem_type();
-  if (transform == LoadTransformationKind::kExtend) {
-    Ld(scratch, src_op);
-    if (memtype == MachineType::Int8()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      VU.set(kScratchReg, E16, m1);
-      vsext_vf2(dst_v, kSimd128ScratchReg);
-    } else if (memtype == MachineType::Uint8()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      VU.set(kScratchReg, E16, m1);
-      vzext_vf2(dst_v, kSimd128ScratchReg);
-    } else if (memtype == MachineType::Int16()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      VU.set(kScratchReg, E32, m1);
-      vsext_vf2(dst_v, kSimd128ScratchReg);
-    } else if (memtype == MachineType::Uint16()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      VU.set(kScratchReg, E32, m1);
-      vzext_vf2(dst_v, kSimd128ScratchReg);
-    } else if (memtype == MachineType::Int32()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      vsext_vf2(dst_v, kSimd128ScratchReg);
-    } else if (memtype == MachineType::Uint32()) {
-      VU.set(kScratchReg, E64, m1);
-      vmv_vx(kSimd128ScratchReg, scratch);
-      vzext_vf2(dst_v, kSimd128ScratchReg);
-    }
-  } else if (transform == LoadTransformationKind::kZeroExtend) {
-    vxor_vv(dst_v, dst_v, dst_v);
-    if (memtype == MachineType::Int32()) {
-      VU.set(kScratchReg, E32, m1);
-      Lwu(scratch, src_op);
-      vmv_sx(dst_v, scratch);
-    } else {
-      DCHECK_EQ(MachineType::Int64(), memtype);
-      VU.set(kScratchReg, E64, m1);
-      Ld(scratch, src_op);
-      vmv_sx(dst_v, scratch);
-    }
-  } else {
-    DCHECK_EQ(LoadTransformationKind::kSplat, transform);
-    if (memtype == MachineType::Int8()) {
-      VU.set(kScratchReg, E8, m1);
-      Lb(scratch, src_op);
-      vmv_vx(dst_v, scratch);
-    } else if (memtype == MachineType::Int16()) {
-      VU.set(kScratchReg, E16, m1);
-      Lh(scratch, src_op);
-      vmv_vx(dst_v, scratch);
-    } else if (memtype == MachineType::Int32()) {
-      VU.set(kScratchReg, E32, m1);
-      Lw(scratch, src_op);
-      vmv_vx(dst_v, scratch);
-    } else if (memtype == MachineType::Int64()) {
-      VU.set(kScratchReg, E64, m1);
-      Ld(scratch, src_op);
-      vmv_vx(dst_v, scratch);
-    }
-  }
-}
-
-void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,
-                                Register addr, Register offset_reg,
-                                uintptr_t offset_imm, LoadType type,
-                                uint8_t laneidx, uint32_t* protected_load_pc) {
-  MemOperand src_op = liftoff::GetMemOp(this, addr, offset_reg, offset_imm);
-  MachineType mem_type = type.mem_type();
-  UseScratchRegisterScope temps(this);
-  Register scratch = temps.Acquire();
-  *protected_load_pc = pc_offset();
-  if (mem_type == MachineType::Int8()) {
-    Lbu(scratch, src_op);
-    VU.set(kScratchReg, E64, m1);
-    li(kScratchReg, 0x1 << laneidx);
-    vmv_sx(v0, kScratchReg);
-    VU.set(kScratchReg, E8, m1);
-    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
-  } else if (mem_type == MachineType::Int16()) {
-    Lhu(scratch, src_op);
-    VU.set(kScratchReg, E16, m1);
-    li(kScratchReg, 0x1 << laneidx);
-    vmv_sx(v0, kScratchReg);
-    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
-  } else if (mem_type == MachineType::Int32()) {
-    Lwu(scratch, src_op);
-    VU.set(kScratchReg, E32, m1);
-    li(kScratchReg, 0x1 << laneidx);
-    vmv_sx(v0, kScratchReg);
-    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
-  } else if (mem_type == MachineType::Int64()) {
-    Ld(scratch, src_op);
-    VU.set(kScratchReg, E64, m1);
-    li(kScratchReg, 0x1 << laneidx);
-    vmv_sx(v0, kScratchReg);
-    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
-  } else {
-    UNREACHABLE();
-  }
-}
-
-void LiftoffAssembler::StoreLane(Register dst, Register offset,
-                                 uintptr_t offset_imm, LiftoffRegister src,
-                                 StoreType type, uint8_t lane,
-                                 uint32_t* protected_store_pc) {
-  MemOperand dst_op = liftoff::GetMemOp(this, dst, offset, offset_imm);
-  if (protected_store_pc) *protected_store_pc = pc_offset();
-  MachineRepresentation rep = type.mem_rep();
-  if (rep == MachineRepresentation::kWord8) {
-    VU.set(kScratchReg, E8, m1);
-    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
-    vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sb(kScratchReg, dst_op);
-  } else if (rep == MachineRepresentation::kWord16) {
-    VU.set(kScratchReg, E16, m1);
-    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
-    vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sh(kScratchReg, dst_op);
-  } else if (rep == MachineRepresentation::kWord32) {
-    VU.set(kScratchReg, E32, m1);
-    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
-    vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sw(kScratchReg, dst_op);
-  } else {
-    DCHECK_EQ(MachineRepresentation::kWord64, rep);
-    VU.set(kScratchReg, E64, m1);
-    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
-    vmv_xs(kScratchReg, kSimd128ScratchReg);
-    Sd(kScratchReg, dst_op);
-  }
-}
-
-void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
-                                          LiftoffRegister lhs,
-                                          LiftoffRegister rhs,
-                                          const uint8_t shuffle[16],
-                                          bool is_swizzle) {
-  VRegister dst_v = dst.fp().toV();
-  VRegister lhs_v = lhs.fp().toV();
-  VRegister rhs_v = rhs.fp().toV();
-
-  uint64_t imm1 = *(reinterpret_cast<const uint64_t*>(shuffle));
-  uint64_t imm2 = *((reinterpret_cast<const uint64_t*>(shuffle)) + 1);
-  VU.set(kScratchReg, VSew::E64, Vlmul::m1);
-  li(kScratchReg, imm2);
-  vmv_sx(kSimd128ScratchReg2, kScratchReg);
-  vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
-  li(kScratchReg, imm1);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-
-  VU.set(kScratchReg, E8, m1);
-  VRegister temp =
-      GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
-  if (dst_v == lhs_v) {
-    vmv_vv(temp, lhs_v);
-    lhs_v = temp;
-  } else if (dst_v == rhs_v) {
-    vmv_vv(temp, rhs_v);
-    rhs_v = temp;
-  }
-  vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg);
-  vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg,
-          -16);  // The indices in range [16, 31] select the i - 16-th element
-                 // of rhs
-  vrgather_vv(kSimd128ScratchReg2, rhs_v, kSimd128ScratchReg);
-  vor_vv(dst_v, dst_v, kSimd128ScratchReg2);
-}
-
 void LiftoffAssembler::emit_i8x16_popcnt(LiftoffRegister dst,
                                          LiftoffRegister src) {
   VRegister src_v = src.fp().toV();
@@ -2051,13 +442,6 @@ void LiftoffAssembler::emit_f32x4_splat(LiftoffRegister dst,
   vmv_vx(dst.fp().toV(), kScratchReg);
 }
 
-void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
-                                        LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  fmv_x_d(kScratchReg, src.fp());
-  vmv_vx(dst.fp().toV(), kScratchReg);
-}
-
 void LiftoffAssembler::emit_i64x2_extmul_low_i32x4_s(LiftoffRegister dst,
                                                      LiftoffRegister src1,
                                                      LiftoffRegister src2) {
@@ -3296,32 +1680,6 @@ void LiftoffAssembler::emit_f64x2_div(LiftoffRegister dst, LiftoffRegister lhs,
   vfdiv_vv(dst.fp().toV(), lhs.fp().toV(), rhs.fp().toV());
 }
 
-void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
-                                      LiftoffRegister rhs) {
-  VU.set(kScratchReg, E64, m1);
-  const int64_t kNaN = 0x7ff8000000000000L;
-  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
-  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
-  vand_vv(v0, v0, kSimd128ScratchReg);
-  li(kScratchReg, kNaN);
-  vmv_vx(kSimd128ScratchReg, kScratchReg);
-  vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
-  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
-}
-
-void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,
-                                      LiftoffRegister rhs) {
-  VU.set(kScratchReg, E64, m1);
-  const int64_t kNaN = 0x7ff8000000000000L;
-  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
-  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
-  vand_vv(v0, v0, kSimd128ScratchReg);
-  li(kScratchReg, kNaN);
-  vmv_vx(kSimd128ScratchReg, kScratchReg);
-  vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
-  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
-}
-
 void LiftoffAssembler::emit_f64x2_relaxed_min(LiftoffRegister dst,
                                               LiftoffRegister lhs,
                                               LiftoffRegister rhs) {
@@ -3546,62 +1904,6 @@ void LiftoffAssembler::emit_i64x2_abs(LiftoffRegister dst,
   vneg_vv(dst.fp().toV(), src.fp().toV(), MaskType::Mask);
 }
 
-void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,
-                                                          LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x0006000400020000);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-  li(kScratchReg, 0x0007000500030001);
-  vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  VU.set(kScratchReg, E16, m1);
-  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  VU.set(kScratchReg, E16, mf2);
-  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-}
-
-void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,
-                                                          LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x0006000400020000);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-  li(kScratchReg, 0x0007000500030001);
-  vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  VU.set(kScratchReg, E16, m1);
-  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  VU.set(kScratchReg, E16, mf2);
-  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-}
-
-void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,
-                                                          LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x0E0C0A0806040200);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-  li(kScratchReg, 0x0F0D0B0907050301);
-  vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  VU.set(kScratchReg, E8, m1);
-  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  VU.set(kScratchReg, E8, mf2);
-  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-}
-
-void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,
-                                                          LiftoffRegister src) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x0E0C0A0806040200);
-  vmv_sx(kSimd128ScratchReg, kScratchReg);
-  li(kScratchReg, 0x0F0D0B0907050301);
-  vmv_sx(kSimd128ScratchReg3, kScratchReg);
-  VU.set(kScratchReg, E8, m1);
-  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
-  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
-  VU.set(kScratchReg, E8, mf2);
-  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
-}
-
 void LiftoffAssembler::emit_i32x4_abs(LiftoffRegister dst,
                                       LiftoffRegister src) {
   VU.set(kScratchReg, E32, m1);
@@ -3731,17 +2033,6 @@ void LiftoffAssembler::emit_f32x4_replace_lane(LiftoffRegister dst,
   vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
 }
 
-void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
-                                               LiftoffRegister src1,
-                                               LiftoffRegister src2,
-                                               uint8_t imm_lane_idx) {
-  VU.set(kScratchReg, E64, m1);
-  li(kScratchReg, 0x1 << imm_lane_idx);
-  vmv_sx(v0, kScratchReg);
-  fmv_x_d(kScratchReg, src2.fp());
-  vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
-}
-
 void LiftoffAssembler::emit_s128_set_if_nan(Register dst, LiftoffRegister src,
                                             Register tmp_gp,
                                             LiftoffRegister tmp_s128,
@@ -3788,7 +2079,7 @@ void LiftoffAssembler::emit_f64x2_qfms(LiftoffRegister dst,
 }
 
 void LiftoffAssembler::StackCheck(Label* ool_code, Register limit_address) {
-  TurboAssembler::Ld(limit_address, MemOperand(limit_address));
+  TurboAssembler::LoadWord(limit_address, MemOperand(limit_address));
   TurboAssembler::Branch(ool_code, ule, sp, Operand(limit_address));
 }
 
@@ -3806,11 +2097,11 @@ void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {
   int32_t num_gp_regs = gp_regs.GetNumRegsSet();
   if (num_gp_regs) {
     int32_t offset = num_gp_regs * kSystemPointerSize;
-    Add64(sp, sp, Operand(-offset));
+    AddWord(sp, sp, Operand(-offset));
     while (!gp_regs.is_empty()) {
       LiftoffRegister reg = gp_regs.GetFirstRegSet();
       offset -= kSystemPointerSize;
-      Sd(reg.gp(), MemOperand(sp, offset));
+      StoreWord(reg.gp(), MemOperand(sp, offset));
       gp_regs.clear(reg);
     }
     DCHECK_EQ(offset, 0);
@@ -3818,7 +2109,7 @@ void LiftoffAssembler::PushRegisters(LiftoffRegList regs) {
   LiftoffRegList fp_regs = regs & kFpCacheRegList;
   int32_t num_fp_regs = fp_regs.GetNumRegsSet();
   if (num_fp_regs) {
-    Add64(sp, sp, Operand(-(num_fp_regs * kStackSlotSize)));
+    AddWord(sp, sp, Operand(-(num_fp_regs * kStackSlotSize)));
     int32_t offset = 0;
     while (!fp_regs.is_empty()) {
       LiftoffRegister reg = fp_regs.GetFirstRegSet();
@@ -3839,16 +2130,16 @@ void LiftoffAssembler::PopRegisters(LiftoffRegList regs) {
     fp_regs.clear(reg);
     fp_offset += sizeof(double);
   }
-  if (fp_offset) Add64(sp, sp, Operand(fp_offset));
+  if (fp_offset) AddWord(sp, sp, Operand(fp_offset));
   LiftoffRegList gp_regs = regs & kGpCacheRegList;
   int32_t gp_offset = 0;
   while (!gp_regs.is_empty()) {
     LiftoffRegister reg = gp_regs.GetLastRegSet();
-    Ld(reg.gp(), MemOperand(sp, gp_offset));
+    LoadWord(reg.gp(), MemOperand(sp, gp_offset));
     gp_regs.clear(reg);
     gp_offset += kSystemPointerSize;
   }
-  Add64(sp, sp, Operand(gp_offset));
+  AddWord(sp, sp, Operand(gp_offset));
 }
 
 void LiftoffAssembler::RecordSpillsInSafepoint(
@@ -3872,49 +2163,6 @@ void LiftoffAssembler::DropStackSlotsAndRet(uint32_t num_stack_slots) {
   TurboAssembler::DropAndRet(static_cast<int>(num_stack_slots));
 }
 
-void LiftoffAssembler::CallC(const ValueKindSig* sig,
-                             const LiftoffRegister* args,
-                             const LiftoffRegister* rets,
-                             ValueKind out_argument_kind, int stack_bytes,
-                             ExternalReference ext_ref) {
-  Add64(sp, sp, Operand(-stack_bytes));
-
-  int arg_bytes = 0;
-  for (ValueKind param_kind : sig->parameters()) {
-    liftoff::Store(this, sp, arg_bytes, *args++, param_kind);
-    arg_bytes += value_kind_size(param_kind);
-  }
-  DCHECK_LE(arg_bytes, stack_bytes);
-
-  // Pass a pointer to the buffer with the arguments to the C function.
-  // On RISC-V, the first argument is passed in {a0}.
-  constexpr Register kFirstArgReg = a0;
-  mv(kFirstArgReg, sp);
-
-  // Now call the C function.
-  constexpr int kNumCCallArgs = 1;
-  PrepareCallCFunction(kNumCCallArgs, kScratchReg);
-  CallCFunction(ext_ref, kNumCCallArgs);
-
-  // Move return value to the right register.
-  const LiftoffRegister* next_result_reg = rets;
-  if (sig->return_count() > 0) {
-    DCHECK_EQ(1, sig->return_count());
-    constexpr Register kReturnReg = a0;
-    if (kReturnReg != next_result_reg->gp()) {
-      Move(*next_result_reg, LiftoffRegister(kReturnReg), sig->GetReturn(0));
-    }
-    ++next_result_reg;
-  }
-
-  // Load potential output value from the buffer on the stack.
-  if (out_argument_kind != kVoid) {
-    liftoff::Load(this, *next_result_reg, MemOperand(sp, 0), out_argument_kind);
-  }
-
-  Add64(sp, sp, Operand(stack_bytes));
-}
-
 void LiftoffAssembler::CallNativeWasmCode(Address addr) {
   Call(addr, RelocInfo::WASM_CALL);
 }
@@ -3950,12 +2198,12 @@ void LiftoffAssembler::CallRuntimeStub(WasmCode::RuntimeStubId sid) {
 }
 
 void LiftoffAssembler::AllocateStackSlot(Register addr, uint32_t size) {
-  Add64(sp, sp, Operand(-size));
+  AddWord(sp, sp, Operand(-size));
   TurboAssembler::Move(addr, sp);
 }
 
 void LiftoffAssembler::DeallocateStackSlot(uint32_t size) {
-  Add64(sp, sp, Operand(size));
+  AddWord(sp, sp, Operand(size));
 }
 
 void LiftoffAssembler::MaybeOSR() {}
@@ -3972,50 +2220,10 @@ void LiftoffAssembler::emit_set_if_nan(Register dst, FPURegister src,
     feq_d(scratch, src, src);  // rd <- !isNan(src)
   }
   not_(scratch, scratch);
-  Sd(scratch, MemOperand(dst));
-}
-
-void LiftoffStackSlots::Construct(int param_slots) {
-  DCHECK_LT(0, slots_.size());
-  SortInPushOrder();
-  int last_stack_slot = param_slots;
-  for (auto& slot : slots_) {
-    const int stack_slot = slot.dst_slot_;
-    int stack_decrement = (last_stack_slot - stack_slot) * kSystemPointerSize;
-    DCHECK_LT(0, stack_decrement);
-    last_stack_slot = stack_slot;
-    const LiftoffAssembler::VarState& src = slot.src_;
-    switch (src.loc()) {
-      case LiftoffAssembler::VarState::kStack:
-        if (src.kind() != kS128) {
-          asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
-          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
-          asm_->push(kScratchReg);
-        } else {
-          asm_->AllocateStackSpace(stack_decrement - kSimd128Size);
-          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_ - 8));
-          asm_->push(kScratchReg);
-          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
-          asm_->push(kScratchReg);
-        }
-        break;
-      case LiftoffAssembler::VarState::kRegister: {
-        int pushed_bytes = SlotSizeInBytes(slot);
-        asm_->AllocateStackSpace(stack_decrement - pushed_bytes);
-        liftoff::push(asm_, src.reg(), src.kind());
-        break;
-      }
-      case LiftoffAssembler::VarState::kIntConst: {
-        asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
-        asm_->li(kScratchReg, Operand(src.i32_const()));
-        asm_->push(kScratchReg);
-        break;
-      }
-    }
-  }
+  StoreWord(scratch, MemOperand(dst));
 }
+
 }  // namespace wasm
 }  // namespace internal
 }  // namespace v8
-
-#endif  // V8_WASM_BASELINE_RISCV64_LIFTOFF_ASSEMBLER_RISCV64_H_
+#endif  // V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV_H_
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
new file mode 100644
index 00000000000..9bc88d84a42
--- /dev/null
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv32.h
@@ -0,0 +1,2088 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV32_H_
+#define V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV32_H_
+
+#include "src/base/platform/wrappers.h"
+#include "src/heap/memory-chunk.h"
+#include "src/wasm/baseline/liftoff-assembler.h"
+#include "src/wasm/baseline/riscv/liftoff-assembler-riscv.h"
+#include "src/wasm/wasm-objects.h"
+
+namespace v8 {
+namespace internal {
+namespace wasm {
+
+namespace liftoff {
+
+// Liftoff Frames.
+//
+//  slot      Frame
+//       +--------------------+---------------------------
+//  n+4  | optional padding slot to keep the stack 16 byte aligned.
+//  n+3  |   parameter n      |
+//  ...  |       ...          |
+//   4   |   parameter 1      | or parameter 2
+//   3   |   parameter 0      | or parameter 1
+//   2   |  (result address)  | or parameter 0
+//  -----+--------------------+---------------------------
+//   1   | return addr (ra)   |
+//   0   | previous frame (fp)|
+//  -----+--------------------+  <-- frame ptr (fp)
+//  -1   | StackFrame::WASM   |
+//  -2   |     instance       |
+//  -3   |     feedback vector|
+//  -4   |     tiering budget |
+//  -----+--------------------+---------------------------
+//  -5   |     slot 0         |   ^
+//  -6   |     slot 1         |   |
+//       |                    | Frame slots
+//       |                    |   |
+//       |                    |   v
+//       | optional padding slot to keep the stack 16 byte aligned.
+//  -----+--------------------+  <-- stack ptr (sp)
+//
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+constexpr int32_t kLowWordOffset = 4;
+constexpr int32_t kHighWordOffset = 0;
+#else
+constexpr int32_t kLowWordOffset = 0;
+constexpr int32_t kHighWordOffset = 4;
+#endif
+
+inline MemOperand GetHalfStackSlot(int offset, RegPairHalf half) {
+  int32_t half_offset =
+      half == kLowWord ? 0 : LiftoffAssembler::kStackSlotSize / 2;
+  return MemOperand(offset > 0 ? fp : sp, -offset + half_offset);
+}
+
+inline MemOperand GetMemOp(LiftoffAssembler* assm, Register addr,
+                           Register offset, uintptr_t offset_imm,
+                           Register scratch) {
+  Register dst = no_reg;
+  if (offset != no_reg) {
+    dst = scratch;
+    assm->emit_i32_add(dst, addr, offset);
+  }
+  MemOperand dst_op = (offset != no_reg) ? MemOperand(dst, offset_imm)
+                                         : MemOperand(addr, offset_imm);
+  return dst_op;
+}
+
+inline void Load(LiftoffAssembler* assm, LiftoffRegister dst, Register base,
+                 int32_t offset, ValueKind kind) {
+  MemOperand src(base, offset);
+
+  switch (kind) {
+    case kI32:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      assm->Lw(dst.gp(), src);
+      break;
+    case kI64:
+      assm->Lw(dst.low_gp(),
+               MemOperand(base, offset + liftoff::kLowWordOffset));
+      assm->Lw(dst.high_gp(),
+               MemOperand(base, offset + liftoff::kHighWordOffset));
+      break;
+    case kF32:
+      assm->LoadFloat(dst.fp(), src);
+      break;
+    case kF64:
+      assm->LoadDouble(dst.fp(), src);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void Store(LiftoffAssembler* assm, Register base, int32_t offset,
+                  LiftoffRegister src, ValueKind kind) {
+  MemOperand dst(base, offset);
+  switch (kind) {
+    case kI32:
+    case kRefNull:
+    case kRef:
+    case kRtt:
+      assm->Sw(src.gp(), dst);
+      break;
+    case kI64:
+      assm->Sw(src.low_gp(),
+               MemOperand(base, offset + liftoff::kLowWordOffset));
+      assm->Sw(src.high_gp(),
+               MemOperand(base, offset + liftoff::kHighWordOffset));
+      break;
+    case kF32:
+      assm->StoreFloat(src.fp(), dst);
+      break;
+    case kF64:
+      assm->StoreDouble(src.fp(), dst);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void push(LiftoffAssembler* assm, LiftoffRegister reg, ValueKind kind) {
+  switch (kind) {
+    case kI32:
+    case kRefNull:
+    case kRef:
+    case kRtt:
+      assm->addi(sp, sp, -kSystemPointerSize);
+      assm->Sw(reg.gp(), MemOperand(sp, 0));
+      break;
+    case kI64:
+      assm->Push(reg.high_gp(), reg.low_gp());
+      break;
+    case kF32:
+      assm->addi(sp, sp, -kSystemPointerSize);
+      assm->StoreFloat(reg.fp(), MemOperand(sp, 0));
+      break;
+    case kF64:
+      assm->addi(sp, sp, -kDoubleSize);
+      assm->StoreDouble(reg.fp(), MemOperand(sp, 0));
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline Register EnsureNoAlias(Assembler* assm, Register reg,
+                              LiftoffRegister must_not_alias,
+                              UseScratchRegisterScope* temps) {
+  if (reg != must_not_alias.low_gp() && reg != must_not_alias.high_gp())
+    return reg;
+  Register tmp = temps->Acquire();
+  DCHECK_NE(must_not_alias.low_gp(), tmp);
+  DCHECK_NE(must_not_alias.high_gp(), tmp);
+  assm->mv(tmp, reg);
+  return tmp;
+}
+}  // namespace liftoff
+
+void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value,
+                                    RelocInfo::Mode rmode) {
+  switch (value.type().kind()) {
+    case kI32:
+      TurboAssembler::li(reg.gp(), Operand(value.to_i32(), rmode));
+      break;
+    case kI64: {
+      DCHECK(RelocInfo::IsNoInfo(rmode));
+      int32_t low_word = value.to_i64();
+      int32_t high_word = value.to_i64() >> 32;
+      TurboAssembler::li(reg.low_gp(), Operand(low_word));
+      TurboAssembler::li(reg.high_gp(), Operand(high_word));
+      break;
+    }
+    case kF32:
+      TurboAssembler::LoadFPRImmediate(reg.fp(),
+                                       value.to_f32_boxed().get_bits());
+      break;
+    case kF64:
+      TurboAssembler::LoadFPRImmediate(reg.fp(),
+                                       value.to_f64_boxed().get_bits());
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
+                                         Register offset_reg,
+                                         int32_t offset_imm) {
+  static_assert(kTaggedSize == kSystemPointerSize);
+  Load(LiftoffRegister(dst), src_addr, offset_reg,
+       static_cast<uint32_t>(offset_imm), LoadType::kI32Load);
+}
+
+void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,
+                                       int32_t offset_imm) {
+  MemOperand src_op = MemOperand(src_addr, offset_imm);
+  LoadWord(dst, src_op);
+}
+
+void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
+                                          Register offset_reg,
+                                          int32_t offset_imm,
+                                          LiftoffRegister src,
+                                          LiftoffRegList pinned,
+                                          SkipWriteBarrier skip_write_barrier) {
+  Register scratch = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  MemOperand dst_op =
+      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, scratch);
+  StoreTaggedField(src.gp(), dst_op);
+
+  if (skip_write_barrier || FLAG_disable_write_barriers) return;
+
+  Label write_barrier;
+  Label exit;
+  CheckPageFlag(dst_addr, scratch,
+                MemoryChunk::kPointersFromHereAreInterestingMask, ne,
+                &write_barrier);
+  Branch(&exit);
+  bind(&write_barrier);
+  JumpIfSmi(src.gp(), &exit);
+  CheckPageFlag(src.gp(), scratch,
+                MemoryChunk::kPointersToHereAreInterestingMask, eq, &exit);
+  AddWord(scratch, dst_op.rm(), dst_op.offset());
+  CallRecordWriteStubSaveRegisters(dst_addr, scratch, SaveFPRegsMode::kSave,
+                                   StubCallMode::kCallWasmRuntimeStub);
+  bind(&exit);
+}
+
+void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
+                            Register offset_reg, uintptr_t offset_imm,
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
+  MemOperand src_op =
+      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, kScratchReg);
+
+  if (protected_load_pc) *protected_load_pc = pc_offset();
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+      Lbu(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load8U:
+      Lbu(dst.low_gp(), src_op);
+      TurboAssembler::mv(dst.high_gp(), zero_reg);
+      break;
+    case LoadType::kI32Load8S:
+      Lb(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load8S:
+      Lb(dst.low_gp(), src_op);
+      TurboAssembler::srai(dst.high_gp(), dst.low_gp(), 31);
+      break;
+    case LoadType::kI32Load16U:
+      TurboAssembler::Lhu(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load16U:
+      TurboAssembler::Lhu(dst.low_gp(), src_op);
+      TurboAssembler::mv(dst.high_gp(), zero_reg);
+      break;
+    case LoadType::kI32Load16S:
+      TurboAssembler::Lh(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load16S:
+      TurboAssembler::Lh(dst.low_gp(), src_op);
+      TurboAssembler::srai(dst.high_gp(), dst.low_gp(), 31);
+      break;
+    case LoadType::kI64Load32U:
+      TurboAssembler::Lw(dst.low_gp(), src_op);
+      TurboAssembler::mv(dst.high_gp(), zero_reg);
+      break;
+    case LoadType::kI64Load32S:
+      TurboAssembler::Lw(dst.low_gp(), src_op);
+      TurboAssembler::srai(dst.high_gp(), dst.low_gp(), 31);
+      break;
+    case LoadType::kI32Load:
+      TurboAssembler::Lw(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load: {
+      Lw(dst.low_gp(), src_op);
+      src_op = liftoff::GetMemOp(this, src_addr, offset_reg,
+                                 offset_imm + kSystemPointerSize, kScratchReg);
+      Lw(dst.high_gp(), src_op);
+    } break;
+    case LoadType::kF32Load:
+      TurboAssembler::LoadFloat(dst.fp(), src_op);
+      break;
+    case LoadType::kF64Load:
+      TurboAssembler::LoadDouble(dst.fp(), src_op);
+      break;
+    case LoadType::kS128Load: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src_op.offset() == 0 ? src_op.rm() : kScratchReg;
+      if (src_op.offset() != 0) {
+        TurboAssembler::AddWord(src_reg, src_op.rm(), src_op.offset());
+      }
+      vl(dst.fp().toV(), src_reg, 0, E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_load_mem) {
+    pinned.set(src_op.rm());
+    liftoff::ChangeEndiannessLoad(this, dst, type, pinned);
+  }
+#endif
+}
+
+void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
+                             uintptr_t offset_imm, LiftoffRegister src,
+                             StoreType type, LiftoffRegList pinned,
+                             uint32_t* protected_store_pc, bool is_store_mem,
+                             bool i64_offset) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  MemOperand dst_op =
+      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, scratch);
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_store_mem) {
+    pinned.set(dst_op.rm());
+    LiftoffRegister tmp = GetUnusedRegister(src.reg_class(), pinned);
+    // Save original value.
+    Move(tmp, src, type.value_type());
+
+    src = tmp;
+    pinned.set(tmp);
+    liftoff::ChangeEndiannessStore(this, src, type, pinned);
+  }
+#endif
+
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+
+  switch (type.value()) {
+    case StoreType::kI32Store8:
+      Sb(src.gp(), dst_op);
+      break;
+    case StoreType::kI64Store8:
+      Sb(src.low_gp(), dst_op);
+      break;
+    case StoreType::kI32Store16:
+      TurboAssembler::Sh(src.gp(), dst_op);
+      break;
+    case StoreType::kI64Store16:
+      TurboAssembler::Sh(src.low_gp(), dst_op);
+      break;
+    case StoreType::kI32Store:
+      TurboAssembler::Sw(src.gp(), dst_op);
+      break;
+    case StoreType::kI64Store32:
+      TurboAssembler::Sw(src.low_gp(), dst_op);
+      break;
+    case StoreType::kI64Store: {
+      TurboAssembler::Sw(src.low_gp(), dst_op);
+      dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg,
+                                 offset_imm + kSystemPointerSize, scratch);
+      TurboAssembler::Sw(src.high_gp(), dst_op);
+      break;
+    }
+    case StoreType::kF32Store:
+      TurboAssembler::StoreFloat(src.fp(), dst_op);
+      break;
+    case StoreType::kF64Store:
+      TurboAssembler::StoreDouble(src.fp(), dst_op);
+      break;
+    case StoreType::kS128Store: {
+      VU.set(kScratchReg, E8, m1);
+      Register dst_reg = dst_op.offset() == 0 ? dst_op.rm() : kScratchReg;
+      if (dst_op.offset() != 0) {
+        AddWord(kScratchReg, dst_op.rm(), dst_op.offset());
+      }
+      vs(src.fp().toV(), dst_reg, 0, VSew::E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+namespace liftoff {
+#define __ lasm->
+
+inline Register CalculateActualAddress(LiftoffAssembler* lasm,
+                                       Register addr_reg, Register offset_reg,
+                                       uintptr_t offset_imm,
+                                       Register result_reg) {
+  DCHECK_NE(offset_reg, no_reg);
+  DCHECK_NE(addr_reg, no_reg);
+  __ AddWord(result_reg, addr_reg, Operand(offset_reg));
+  if (offset_imm != 0) {
+    __ AddWord(result_reg, result_reg, Operand(offset_imm));
+  }
+  return result_reg;
+}
+
+enum class Binop { kAdd, kSub, kAnd, kOr, kXor, kExchange };
+
+inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,
+                        Register offset_reg, uintptr_t offset_imm,
+                        LiftoffRegister value, LiftoffRegister result,
+                        StoreType type, Binop op) {
+  LiftoffRegList pinned{dst_addr, offset_reg, value, result};
+  Register store_result = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+
+  // Make sure that {result} is unique.
+  Register result_reg = no_reg;
+  Register value_reg = no_reg;
+  bool change_result = false;
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI64Store16:
+    case StoreType::kI64Store32:
+      __ LoadConstant(result.high(), WasmValue(0));
+      result_reg = result.low_gp();
+      value_reg = value.low_gp();
+      break;
+    case StoreType::kI32Store8:
+    case StoreType::kI32Store16:
+    case StoreType::kI32Store:
+      result_reg = result.gp();
+      value_reg = value.gp();
+      break;
+    default:
+      UNREACHABLE();
+  }
+  if (result_reg == value_reg || result_reg == dst_addr ||
+      result_reg == offset_reg) {
+    result_reg = __ GetUnusedRegister(kGpReg, pinned).gp();
+    change_result = true;
+  }
+
+  UseScratchRegisterScope temps(lasm);
+  Register actual_addr = liftoff::CalculateActualAddress(
+      lasm, dst_addr, offset_reg, offset_imm, temps.Acquire());
+
+  // Allocate an additional {temp} register to hold the result that should be
+  // stored to memory. Note that {temp} and {store_result} are not allowed to be
+  // the same register.
+  Register temp = temps.Acquire();
+
+  Label retry;
+  __ bind(&retry);
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      __ lbu(result_reg, actual_addr, 0);
+      __ sync();
+      break;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      __ lhu(result_reg, actual_addr, 0);
+      __ sync();
+      break;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      __ lr_w(true, false, result_reg, actual_addr);
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  switch (op) {
+    case Binop::kAdd:
+      __ add(temp, result_reg, value_reg);
+      break;
+    case Binop::kSub:
+      __ sub(temp, result_reg, value_reg);
+      break;
+    case Binop::kAnd:
+      __ and_(temp, result_reg, value_reg);
+      break;
+    case Binop::kOr:
+      __ or_(temp, result_reg, value_reg);
+      break;
+    case Binop::kXor:
+      __ xor_(temp, result_reg, value_reg);
+      break;
+    case Binop::kExchange:
+      __ mv(temp, value_reg);
+      break;
+  }
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      __ sync();
+      __ sb(temp, actual_addr, 0);
+      __ sync();
+      __ mv(store_result, zero_reg);
+      break;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      __ sync();
+      __ sh(temp, actual_addr, 0);
+      __ sync();
+      __ mv(store_result, zero_reg);
+      break;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      __ sc_w(false, true, store_result, actual_addr, temp);
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  __ bnez(store_result, &retry);
+  if (change_result) {
+    switch (type.value()) {
+      case StoreType::kI64Store8:
+      case StoreType::kI64Store16:
+      case StoreType::kI64Store32:
+        __ mv(result.low_gp(), result_reg);
+        break;
+      case StoreType::kI32Store8:
+      case StoreType::kI32Store16:
+      case StoreType::kI32Store:
+        __ mv(result.gp(), result_reg);
+        break;
+      default:
+        UNREACHABLE();
+    }
+  }
+}
+
+#undef __
+}  // namespace liftoff
+
+void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
+                                  Register offset_reg, uintptr_t offset_imm,
+                                  LoadType type, LiftoffRegList pinned) {
+  UseScratchRegisterScope temps(this);
+  Register src_reg = liftoff::CalculateActualAddress(
+      this, src_addr, offset_reg, offset_imm, temps.Acquire());
+  Register dst_reg = no_reg;
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+    case LoadType::kI32Load16U:
+    case LoadType::kI32Load:
+      dst_reg = dst.gp();
+      break;
+    case LoadType::kI64Load8U:
+    case LoadType::kI64Load16U:
+    case LoadType::kI64Load32U:
+      dst_reg = dst.low_gp();
+      LoadConstant(dst.high(), WasmValue(0));
+      break;
+    default:
+      break;
+  }
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+    case LoadType::kI64Load8U:
+      fence(PSR | PSW, PSR | PSW);
+      lbu(dst_reg, src_reg, 0);
+      fence(PSR, PSR | PSW);
+      return;
+    case LoadType::kI32Load16U:
+    case LoadType::kI64Load16U:
+      fence(PSR | PSW, PSR | PSW);
+      lhu(dst_reg, src_reg, 0);
+      fence(PSR, PSR | PSW);
+      return;
+    case LoadType::kI32Load:
+    case LoadType::kI64Load32U:
+      fence(PSR | PSW, PSR | PSW);
+      lw(dst_reg, src_reg, 0);
+      fence(PSR, PSR | PSW);
+      return;
+    case LoadType::kI64Load:
+      fence(PSR | PSW, PSR | PSW);
+      lw(dst.low_gp(), src_reg, liftoff::kLowWordOffset);
+      lw(dst.high_gp(), src_reg, liftoff::kHighWordOffset);
+      fence(PSR, PSR | PSW);
+      return;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
+                                   uintptr_t offset_imm, LiftoffRegister src,
+                                   StoreType type, LiftoffRegList pinned) {
+  UseScratchRegisterScope temps(this);
+  Register dst_reg = liftoff::CalculateActualAddress(
+      this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+  Register src_reg = no_reg;
+  switch (type.value()) {
+    case StoreType::kI32Store8:
+    case StoreType::kI32Store16:
+    case StoreType::kI32Store:
+      src_reg = src.gp();
+      break;
+    case StoreType::kI64Store8:
+    case StoreType::kI64Store16:
+    case StoreType::kI64Store32:
+      src_reg = src.low_gp();
+      break;
+    default:
+      break;
+  }
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      fence(PSR | PSW, PSW);
+      sb(src_reg, dst_reg, 0);
+      return;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      fence(PSR | PSW, PSW);
+      sh(src_reg, dst_reg, 0);
+      return;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      fence(PSR | PSW, PSW);
+      sw(src_reg, dst_reg, 0);
+      return;
+    case StoreType::kI64Store:
+      fence(PSR | PSW, PSW);
+      sw(src.low_gp(), dst_reg, liftoff::kLowWordOffset);
+      sw(src.high_gp(), dst_reg, liftoff::kHighWordOffset);
+      return;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::AtomicAdd(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    amoadd_w(true, true, result.gp(), actual_addr, value.gp());
+    return;
+  }
+
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kAdd);
+}
+
+void LiftoffAssembler::AtomicSub(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    sub(kScratchReg, zero_reg, value.gp());
+    amoadd_w(true, true, result.gp(), actual_addr, kScratchReg);
+    return;
+  }
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kSub);
+}
+
+void LiftoffAssembler::AtomicAnd(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    amoand_w(true, true, result.gp(), actual_addr, value.gp());
+    return;
+  }
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kAnd);
+}
+
+void LiftoffAssembler::AtomicOr(Register dst_addr, Register offset_reg,
+                                uint32_t offset_imm, LiftoffRegister value,
+                                LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    amoor_w(true, true, result.gp(), actual_addr, value.gp());
+    return;
+  }
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kOr);
+}
+
+void LiftoffAssembler::AtomicXor(Register dst_addr, Register offset_reg,
+                                 uint32_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    amoxor_w(true, true, result.gp(), actual_addr, value.gp());
+    return;
+  }
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kXor);
+}
+
+void LiftoffAssembler::AtomicExchange(Register dst_addr, Register offset_reg,
+                                      uint32_t offset_imm,
+                                      LiftoffRegister value,
+                                      LiftoffRegister result, StoreType type) {
+  if (type.value() == StoreType::kI64Store) {
+    bailout(kAtomics, "Atomic64");
+  }
+  if (type.value() == StoreType::kI32Store ||
+      type.value() == StoreType::kI64Store32) {
+    UseScratchRegisterScope temps(this);
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+    if (type.value() == StoreType::kI64Store32) {
+      result = result.low();
+      value = value.low();
+    }
+    amoswap_w(true, true, result.gp(), actual_addr, value.gp());
+    return;
+  }
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kExchange);
+}
+
+void LiftoffAssembler::AtomicCompareExchange(
+    Register dst_addr, Register offset_reg, uintptr_t offset_imm,
+    LiftoffRegister expected, LiftoffRegister new_value, LiftoffRegister result,
+    StoreType type) {
+  LiftoffRegList pinned{dst_addr, offset_reg, expected, new_value, result};
+
+  if (type.value() == StoreType::kI64Store) {
+    Register actual_addr = liftoff::CalculateActualAddress(
+        this, dst_addr, offset_reg, offset_imm, kScratchReg);
+    FrameScope scope(this, StackFrame::MANUAL);
+    PushCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+    PrepareCallCFunction(5, 0, kScratchReg);
+    Mv(a0, actual_addr);
+    CallCFunction(ExternalReference::atomic_pair_compare_exchange_function(), 5,
+                  0);
+    PopCallerSaved(SaveFPRegsMode::kIgnore, a0, a1);
+    Mv(result.low_gp(), a0);
+    Mv(result.high_gp(), a1);
+    return;
+  }
+
+  // Make sure that {result} is unique.
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI64Store16:
+    case StoreType::kI64Store32:
+      LoadConstant(result.high(), WasmValue(0));
+      result = result.low();
+      new_value = new_value.low();
+      expected = expected.low();
+      break;
+    case StoreType::kI32Store8:
+    case StoreType::kI32Store16:
+    case StoreType::kI32Store:
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  UseScratchRegisterScope temps(this);
+  Register actual_addr = liftoff::CalculateActualAddress(
+      this, dst_addr, offset_reg, offset_imm, kScratchReg);
+
+  Register temp0 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  Register temp1 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  Register temp2 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+
+  if (type.value() != StoreType::kI32Store &&
+      type.value() != StoreType::kI64Store32) {
+    And(temp1, actual_addr, 0x3);
+    SubWord(temp0, actual_addr, Operand(temp1));
+    SllWord(temp1, temp1, 3);
+  }
+  Label retry;
+  Label done;
+  bind(&retry);
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      lr_w(true, true, temp2, temp0);
+      ExtractBits(result.gp(), temp2, temp1, 8, false);
+      ExtractBits(temp2, expected.gp(), zero_reg, 8, false);
+      Branch(&done, ne, temp2, Operand(result.gp()));
+      InsertBits(temp2, new_value.gp(), temp1, 8);
+      sc_w(true, true, temp2, temp0, temp2);
+      break;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      lr_w(true, true, temp2, temp0);
+      ExtractBits(result.gp(), temp2, temp1, 16, false);
+      ExtractBits(temp2, expected.gp(), zero_reg, 16, false);
+      Branch(&done, ne, temp2, Operand(result.gp()));
+      InsertBits(temp2, new_value.gp(), temp1, 16);
+      sc_w(true, true, temp2, temp0, temp2);
+      break;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      lr_w(true, true, result.gp(), actual_addr);
+      Branch(&done, ne, result.gp(), Operand(expected.gp()));
+      sc_w(true, true, temp2, actual_addr, new_value.gp());
+      break;
+    default:
+      UNREACHABLE();
+  }
+  bnez(temp2, &retry);
+  bind(&done);
+}
+
+void LiftoffAssembler::AtomicFence() { sync(); }
+
+void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,
+                                           uint32_t caller_slot_idx,
+                                           ValueKind kind) {
+  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);
+  liftoff::Load(this, dst, fp, offset, kind);
+}
+
+void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,
+                                            uint32_t caller_slot_idx,
+                                            ValueKind kind) {
+  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);
+  liftoff::Store(this, fp, offset, src, kind);
+}
+
+void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,
+                                           ValueKind kind) {
+  liftoff::Load(this, dst, sp, offset, kind);
+}
+
+void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
+                                      ValueKind kind) {
+  DCHECK_NE(dst_offset, src_offset);
+
+  MemOperand src = liftoff::GetStackSlot(src_offset);
+  MemOperand dst = liftoff::GetStackSlot(dst_offset);
+  switch (kind) {
+    case kI32:
+      Lw(kScratchReg, src);
+      Sw(kScratchReg, dst);
+      break;
+    case kI64:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      Lw(kScratchReg, src);
+      Sw(kScratchReg, dst);
+      src = liftoff::GetStackSlot(src_offset - 4);
+      dst = liftoff::GetStackSlot(dst_offset - 4);
+      Lw(kScratchReg, src);
+      Sw(kScratchReg, dst);
+      break;
+    case kF32:
+      LoadFloat(kScratchDoubleReg, src);
+      StoreFloat(kScratchDoubleReg, dst);
+      break;
+    case kF64:
+      TurboAssembler::LoadDouble(kScratchDoubleReg, src);
+      TurboAssembler::StoreDouble(kScratchDoubleReg, dst);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
+      if (src.offset() != 0) {
+        TurboAssembler::AddWord(src_reg, src.rm(), src.offset());
+      }
+      vl(kSimd128ScratchReg, src_reg, 0, E8);
+      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
+      if (dst.offset() != 0) {
+        AddWord(kScratchReg, dst.rm(), dst.offset());
+      }
+      vs(kSimd128ScratchReg, dst_reg, 0, VSew::E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) {
+  DCHECK_NE(dst, src);
+  // TODO(ksreten): Handle different sizes here.
+  TurboAssembler::Move(dst, src);
+}
+
+void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
+                            ValueKind kind) {
+  DCHECK_NE(dst, src);
+  if (kind != kS128) {
+    TurboAssembler::Move(dst, src);
+  } else {
+    TurboAssembler::vmv_vv(dst.toV(), dst.toV());
+  }
+}
+
+void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueKind kind) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (kind) {
+    case kI32:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      Sw(reg.gp(), dst);
+      break;
+    case kI64:
+      Sw(reg.low_gp(), liftoff::GetHalfStackSlot(offset, kLowWord));
+      Sw(reg.high_gp(), liftoff::GetHalfStackSlot(offset, kHighWord));
+      break;
+    case kF32:
+      StoreFloat(reg.fp(), dst);
+      break;
+    case kF64:
+      TurboAssembler::StoreDouble(reg.fp(), dst);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
+      if (dst.offset() != 0) {
+        AddWord(kScratchReg, dst.rm(), dst.offset());
+      }
+      vs(reg.fp().toV(), dst_reg, 0, VSew::E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Spill(int offset, WasmValue value) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (value.type().kind()) {
+    case kI32:
+    case kRef:
+    case kRefNull: {
+      LiftoffRegister tmp = GetUnusedRegister(kGpReg, {});
+      TurboAssembler::li(tmp.gp(), Operand(value.to_i32()));
+      Sw(tmp.gp(), dst);
+      break;
+    }
+    case kI64: {
+      LiftoffRegister tmp = GetUnusedRegister(kGpRegPair, {});
+
+      int32_t low_word = value.to_i64();
+      int32_t high_word = value.to_i64() >> 32;
+      TurboAssembler::li(tmp.low_gp(), Operand(low_word));
+      TurboAssembler::li(tmp.high_gp(), Operand(high_word));
+
+      Sw(tmp.low_gp(), liftoff::GetHalfStackSlot(offset, kLowWord));
+      Sw(tmp.high_gp(), liftoff::GetHalfStackSlot(offset, kHighWord));
+      break;
+      break;
+    }
+    default:
+      // kWasmF32 and kWasmF64 are unreachable, since those
+      // constants are not tracked.
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) {
+  MemOperand src = liftoff::GetStackSlot(offset);
+  switch (kind) {
+    case kI32:
+    case kRef:
+    case kRefNull:
+      Lw(reg.gp(), src);
+      break;
+    case kI64:
+      Lw(reg.low_gp(), liftoff::GetHalfStackSlot(offset, kLowWord));
+      Lw(reg.high_gp(), liftoff::GetHalfStackSlot(offset, kHighWord));
+      break;
+    case kF32:
+      LoadFloat(reg.fp(), src);
+      break;
+    case kF64:
+      TurboAssembler::LoadDouble(reg.fp(), src);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
+      if (src.offset() != 0) {
+        TurboAssembler::AddWord(src_reg, src.rm(), src.offset());
+      }
+      vl(reg.fp().toV(), src_reg, 0, E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::FillI64Half(Register reg, int offset, RegPairHalf half) {
+  Lw(reg, liftoff::GetHalfStackSlot(offset, half));
+}
+
+void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
+  DCHECK_LT(0, size);
+  RecordUsedSpillOffset(start + size);
+
+  // TODO(riscv32): check
+
+  if (size <= 12 * kStackSlotSize) {
+    // Special straight-line code for up to 12 slots. Generates one
+    // instruction per slot (<= 12 instructions total).
+    uint32_t remainder = size;
+    for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {
+      Sw(zero_reg, liftoff::GetStackSlot(start + remainder));
+      Sw(zero_reg, liftoff::GetStackSlot(start + remainder - 4));
+    }
+    DCHECK(remainder == 4 || remainder == 0);
+    if (remainder) {
+      Sw(zero_reg, liftoff::GetStackSlot(start + remainder));
+    }
+  } else {
+    // General case for bigger counts (12 instructions).
+    // Use a0 for start address (inclusive), a1 for end address (exclusive).
+    Push(a1, a0);
+    AddWord(a0, fp, Operand(-start - size));
+    AddWord(a1, fp, Operand(-start));
+
+    Label loop;
+    bind(&loop);
+    Sw(zero_reg, MemOperand(a0));
+    addi(a0, a0, kSystemPointerSize);
+    BranchShort(&loop, ne, a0, Operand(a1));
+
+    Pop(a1, a0);
+  }
+}
+
+void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {
+  // return high == 0 ? 32 + CLZ32(low) : CLZ32(high);
+  Label done;
+  Label high_is_zero;
+  Branch(&high_is_zero, eq, src.high_gp(), Operand(zero_reg));
+
+  Clz32(dst.low_gp(), src.high_gp());
+  jmp(&done);
+
+  bind(&high_is_zero);
+  Clz32(dst.low_gp(), src.low_gp());
+  AddWord(dst.low_gp(), dst.low_gp(), Operand(32));
+
+  bind(&done);
+  mv(dst.high_gp(), zero_reg);  // High word of result is always 0.
+}
+
+void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {
+  // return low == 0 ? 32 + CTZ32(high) : CTZ32(low);
+  Label done;
+  Label low_is_zero;
+  Branch(&low_is_zero, eq, src.low_gp(), Operand(zero_reg));
+
+  Ctz32(dst.low_gp(), src.low_gp());
+  jmp(&done);
+
+  bind(&low_is_zero);
+  Ctz32(dst.low_gp(), src.high_gp());
+  AddWord(dst.low_gp(), dst.low_gp(), Operand(32));
+
+  bind(&done);
+  mv(dst.high_gp(), zero_reg);  // High word of result is always 0.
+}
+
+bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  // Produce partial popcnts in the two dst registers.
+  Register src1 = src.high_gp() == dst.low_gp() ? src.high_gp() : src.low_gp();
+  Register src2 = src.high_gp() == dst.low_gp() ? src.low_gp() : src.high_gp();
+  TurboAssembler::Popcnt32(dst.low_gp(), src1, kScratchReg);
+  TurboAssembler::Popcnt32(dst.high_gp(), src2, kScratchReg);
+  // Now add the two into the lower dst reg and clear the higher dst reg.
+  AddWord(dst.low_gp(), dst.low_gp(), dst.high_gp());
+  mv(dst.high_gp(), zero_reg);
+  return true;
+}
+
+void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {
+  TurboAssembler::Mul(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+
+  // Check if lhs == kMinInt and rhs == -1, since this case is unrepresentable.
+  TurboAssembler::CompareI(kScratchReg, lhs, Operand(kMinInt), ne);
+  TurboAssembler::CompareI(kScratchReg2, rhs, Operand(-1), ne);
+  add(kScratchReg, kScratchReg, kScratchReg2);
+  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
+                         Operand(zero_reg));
+
+  TurboAssembler::Div(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Divu(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Mod(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Modu(dst, lhs, rhs);
+}
+
+#define I32_BINOP(name, instruction)                                 \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register lhs, \
+                                         Register rhs) {             \
+    instruction(dst, lhs, rhs);                                      \
+  }
+
+// clang-format off
+I32_BINOP(add, add)
+I32_BINOP(sub, sub)
+I32_BINOP(and, and_)
+I32_BINOP(or, or_)
+I32_BINOP(xor, xor_)
+// clang-format on
+
+#undef I32_BINOP
+
+#define I32_BINOP_I(name, instruction)                                  \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register lhs, \
+                                            int32_t imm) {              \
+    instruction(dst, lhs, Operand(imm));                                \
+  }
+
+// clang-format off
+I32_BINOP_I(add, AddWord)
+I32_BINOP_I(sub, SubWord)
+I32_BINOP_I(and, And)
+I32_BINOP_I(or, Or)
+I32_BINOP_I(xor, Xor)
+// clang-format on
+
+#undef I32_BINOP_I
+
+void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {
+  TurboAssembler::Clz32(dst, src);
+}
+
+void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {
+  TurboAssembler::Ctz32(dst, src);
+}
+
+bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {
+  TurboAssembler::Popcnt32(dst, src, kScratchReg);
+  return true;
+}
+
+#define I32_SHIFTOP(name, instruction)                               \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register src, \
+                                         Register amount) {          \
+    instruction(dst, src, amount);                                   \
+  }
+#define I32_SHIFTOP_I(name, instruction)                                \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register src, \
+                                            int amount) {               \
+    instruction(dst, src, amount & 31);                                 \
+  }
+
+I32_SHIFTOP(shl, sll)
+I32_SHIFTOP(sar, sra)
+I32_SHIFTOP(shr, srl)
+
+I32_SHIFTOP_I(shl, slli)
+I32_SHIFTOP_I(sar, srai)
+I32_SHIFTOP_I(shr, srli)
+
+#undef I32_SHIFTOP
+#undef I32_SHIFTOP_I
+
+void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  TurboAssembler::MulPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),
+                          lhs.high_gp(), rhs.low_gp(), rhs.high_gp(),
+                          kScratchReg, kScratchReg2);
+}
+
+bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  return false;
+}
+
+bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  return false;
+}
+
+bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  return false;
+}
+
+bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  return false;
+}
+
+namespace liftoff {
+
+inline bool IsRegInRegPair(LiftoffRegister pair, Register reg) {
+  DCHECK(pair.is_gp_pair());
+  return pair.low_gp() == reg || pair.high_gp() == reg;
+}
+
+inline void Emit64BitShiftOperation(
+    LiftoffAssembler* assm, LiftoffRegister dst, LiftoffRegister src,
+    Register amount,
+    void (TurboAssembler::*emit_shift)(Register, Register, Register, Register,
+                                       Register, Register, Register)) {
+  LiftoffRegList pinned{dst, src, amount};
+
+  // If some of destination registers are in use, get another, unused pair.
+  // That way we prevent overwriting some input registers while shifting.
+  // Do this before any branch so that the cache state will be correct for
+  // all conditions.
+  Register amount_capped =
+      pinned.set(assm->GetUnusedRegister(kGpReg, pinned).gp());
+  assm->And(amount_capped, amount, Operand(63));
+  if (liftoff::IsRegInRegPair(dst, amount) || dst.overlaps(src)) {
+    // Do the actual shift.
+    LiftoffRegister tmp = assm->GetUnusedRegister(kGpRegPair, pinned);
+    (assm->*emit_shift)(tmp.low_gp(), tmp.high_gp(), src.low_gp(),
+                        src.high_gp(), amount_capped, kScratchReg,
+                        kScratchReg2);
+
+    // Place result in destination register.
+    assm->TurboAssembler::Move(dst.high_gp(), tmp.high_gp());
+    assm->TurboAssembler::Move(dst.low_gp(), tmp.low_gp());
+  } else {
+    (assm->*emit_shift)(dst.low_gp(), dst.high_gp(), src.low_gp(),
+                        src.high_gp(), amount_capped, kScratchReg,
+                        kScratchReg2);
+  }
+}
+}  // namespace liftoff
+
+void LiftoffAssembler::emit_i64_add(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  TurboAssembler::AddPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),
+                          lhs.high_gp(), rhs.low_gp(), rhs.high_gp(),
+                          kScratchReg, kScratchReg2);
+}
+
+void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,
+                                     int64_t imm) {
+  LiftoffRegister imm_reg =
+      GetUnusedRegister(kGpRegPair, LiftoffRegList{dst, lhs});
+  int32_t imm_low_word = static_cast<int32_t>(imm);
+  int32_t imm_high_word = static_cast<int32_t>(imm >> 32);
+
+  // TODO(riscv32): are there some optimization we can make without
+  // materializing?
+  TurboAssembler::li(imm_reg.low_gp(), imm_low_word);
+  TurboAssembler::li(imm_reg.high_gp(), imm_high_word);
+  TurboAssembler::AddPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),
+                          lhs.high_gp(), imm_reg.low_gp(), imm_reg.high_gp(),
+                          kScratchReg, kScratchReg2);
+}
+
+void LiftoffAssembler::emit_i64_sub(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  TurboAssembler::SubPair(dst.low_gp(), dst.high_gp(), lhs.low_gp(),
+                          lhs.high_gp(), rhs.low_gp(), rhs.high_gp(),
+                          kScratchReg, kScratchReg2);
+}
+
+void LiftoffAssembler::emit_i64_shl(LiftoffRegister dst, LiftoffRegister src,
+                                    Register amount) {
+  ASM_CODE_COMMENT(this);
+  liftoff::Emit64BitShiftOperation(this, dst, src, amount,
+                                   &TurboAssembler::ShlPair);
+}
+
+void LiftoffAssembler::emit_i64_shli(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  ASM_CODE_COMMENT(this);
+  UseScratchRegisterScope temps(this);
+  LiftoffRegister temp = GetUnusedRegister(kGpReg, LiftoffRegList{dst, src});
+  temps.Include(temp.gp());
+  // {src.low_gp()} will still be needed after writing {dst.high_gp()} and
+  // {dst.low_gp()}.
+  Register src_low = liftoff::EnsureNoAlias(this, src.low_gp(), dst, &temps);
+  Register src_high = liftoff::EnsureNoAlias(this, src.high_gp(), dst, &temps);
+  // {src.high_gp()} will still be needed after writing {dst.high_gp()}.
+  DCHECK_NE(dst.low_gp(), kScratchReg);
+  DCHECK_NE(dst.high_gp(), kScratchReg);
+
+  TurboAssembler::ShlPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
+                          amount, kScratchReg, kScratchReg2);
+}
+
+void LiftoffAssembler::emit_i64_sar(LiftoffRegister dst, LiftoffRegister src,
+                                    Register amount) {
+  liftoff::Emit64BitShiftOperation(this, dst, src, amount,
+                                   &TurboAssembler::SarPair);
+}
+
+void LiftoffAssembler::emit_i64_sari(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  ASM_CODE_COMMENT(this);
+  UseScratchRegisterScope temps(this);
+  LiftoffRegister temp = GetUnusedRegister(kGpReg, LiftoffRegList{dst, src});
+  temps.Include(temp.gp());
+  // {src.low_gp()} will still be needed after writing {dst.high_gp()} and
+  // {dst.low_gp()}.
+  Register src_low = liftoff::EnsureNoAlias(this, src.low_gp(), dst, &temps);
+  Register src_high = liftoff::EnsureNoAlias(this, src.high_gp(), dst, &temps);
+  DCHECK_NE(dst.low_gp(), kScratchReg);
+  DCHECK_NE(dst.high_gp(), kScratchReg);
+
+  TurboAssembler::SarPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
+                          amount, kScratchReg, kScratchReg2);
+}
+
+void LiftoffAssembler::emit_i64_shr(LiftoffRegister dst, LiftoffRegister src,
+                                    Register amount) {
+  liftoff::Emit64BitShiftOperation(this, dst, src, amount,
+                                   &TurboAssembler::ShrPair);
+}
+
+void LiftoffAssembler::emit_i64_shri(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  ASM_CODE_COMMENT(this);
+  UseScratchRegisterScope temps(this);
+  LiftoffRegister temp = GetUnusedRegister(kGpReg, LiftoffRegList{dst, src});
+  temps.Include(temp.gp());
+  // {src.low_gp()} will still be needed after writing {dst.high_gp()} and
+  // {dst.low_gp()}.
+  Register src_low = liftoff::EnsureNoAlias(this, src.low_gp(), dst, &temps);
+  Register src_high = liftoff::EnsureNoAlias(this, src.high_gp(), dst, &temps);
+  DCHECK_NE(dst.low_gp(), kScratchReg);
+  DCHECK_NE(dst.high_gp(), kScratchReg);
+
+  TurboAssembler::ShrPair(dst.low_gp(), dst.high_gp(), src_low, src_high,
+                          amount, kScratchReg, kScratchReg2);
+}
+
+#define FP_UNOP_RETURN_FALSE(name)                                             \
+  bool LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    return false;                                                              \
+  }
+
+FP_UNOP_RETURN_FALSE(f64_ceil)
+FP_UNOP_RETURN_FALSE(f64_floor)
+FP_UNOP_RETURN_FALSE(f64_trunc)
+FP_UNOP_RETURN_FALSE(f64_nearest_int)
+
+#undef FP_UNOP_RETURN_FALSE
+
+bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
+                                            LiftoffRegister dst,
+                                            LiftoffRegister src, Label* trap) {
+  switch (opcode) {
+    case kExprI32ConvertI64:
+      TurboAssembler::Move(dst.gp(), src.low_gp());
+      return true;
+    case kExprI32SConvertF32:
+    case kExprI32UConvertF32:
+    case kExprI32SConvertF64:
+    case kExprI32UConvertF64:
+    case kExprI64SConvertF32:
+    case kExprI64UConvertF32:
+    case kExprI64SConvertF64:
+    case kExprI64UConvertF64:
+    case kExprF32ConvertF64: {
+      // real conversion, if src is out-of-bound of target integer types,
+      // kScratchReg is set to 0
+      switch (opcode) {
+        case kExprI32SConvertF32:
+          Trunc_w_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32UConvertF32:
+          Trunc_uw_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32SConvertF64:
+          Trunc_w_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32UConvertF64:
+          Trunc_uw_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprF32ConvertF64:
+          fcvt_s_d(dst.fp(), src.fp());
+          break;
+        case kExprI64SConvertF32:
+        case kExprI64UConvertF32:
+        case kExprI64SConvertF64:
+        case kExprI64UConvertF64:
+          return false;
+        default:
+          UNREACHABLE();
+      }
+
+      // Checking if trap.
+      if (trap != nullptr) {
+        TurboAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));
+      }
+
+      return true;
+    }
+    case kExprI32ReinterpretF32:
+      TurboAssembler::ExtractLowWordFromF64(dst.gp(), src.fp());
+      return true;
+    case kExprI64SConvertI32:
+      TurboAssembler::Move(dst.low_gp(), src.gp());
+      TurboAssembler::Move(dst.high_gp(), src.gp());
+      srai(dst.high_gp(), dst.high_gp(), 31);
+      return true;
+    case kExprI64UConvertI32:
+      TurboAssembler::Move(dst.low_gp(), src.gp());
+      TurboAssembler::Move(dst.high_gp(), zero_reg);
+      return true;
+    case kExprI64ReinterpretF64:
+      SubWord(sp, sp, kDoubleSize);
+      StoreDouble(src.fp(), MemOperand(sp, 0));
+      Lw(dst.low_gp(), MemOperand(sp, 0));
+      Lw(dst.high_gp(), MemOperand(sp, 4));
+      AddWord(sp, sp, kDoubleSize);
+      return true;
+    case kExprF32SConvertI32: {
+      TurboAssembler::Cvt_s_w(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF32UConvertI32:
+      TurboAssembler::Cvt_s_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF32ReinterpretI32:
+      fmv_w_x(dst.fp(), src.gp());
+      return true;
+    case kExprF64SConvertI32: {
+      TurboAssembler::Cvt_d_w(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF64UConvertI32:
+      TurboAssembler::Cvt_d_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF64ConvertF32:
+      fcvt_d_s(dst.fp(), src.fp());
+      return true;
+    case kExprF64ReinterpretI64:
+      SubWord(sp, sp, kDoubleSize);
+      Sw(src.low_gp(), MemOperand(sp, 0));
+      Sw(src.high_gp(), MemOperand(sp, 4));
+      LoadDouble(dst.fp(), MemOperand(sp, 0));
+      AddWord(sp, sp, kDoubleSize);
+      return true;
+    case kExprI32SConvertSatF32: {
+      fcvt_w_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32UConvertSatF32: {
+      fcvt_wu_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32SConvertSatF64: {
+      fcvt_w_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32UConvertSatF64: {
+      fcvt_wu_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI64SConvertSatF32:
+    case kExprI64UConvertSatF32:
+    case kExprI64SConvertSatF64:
+    case kExprI64UConvertSatF64:
+      return false;
+    default:
+      return false;
+  }
+}
+
+void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
+  slli(dst, src, 32 - 8);
+  srai(dst, dst, 32 - 8);
+}
+
+void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {
+  slli(dst, src, 32 - 16);
+  srai(dst, dst, 32 - 16);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,
+                                              LiftoffRegister src) {
+  emit_i32_signextend_i8(dst.low_gp(), src.low_gp());
+  srai(dst.high_gp(), dst.low_gp(), 31);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  emit_i32_signextend_i16(dst.low_gp(), src.low_gp());
+  srai(dst.high_gp(), dst.low_gp(), 31);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  mv(dst.low_gp(), src.low_gp());
+  srai(dst.high_gp(), src.low_gp(), 31);
+}
+
+void LiftoffAssembler::emit_jump(Label* label) {
+  TurboAssembler::Branch(label);
+}
+
+void LiftoffAssembler::emit_jump(Register target) {
+  TurboAssembler::Jump(target);
+}
+
+void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
+                                      Label* label, ValueKind kind,
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  if (rhs == no_reg) {
+    DCHECK(kind == kI32);
+    TurboAssembler::Branch(label, cond, lhs, Operand(zero_reg));
+  } else {
+    DCHECK((kind == kI32) ||
+           (is_reference(kind) &&
+            (liftoff_cond == kEqual || liftoff_cond == kUnequal)));
+    TurboAssembler::Branch(label, cond, lhs, Operand(rhs));
+  }
+}
+
+void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
+                                           Label* label, Register lhs,
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  TurboAssembler::Branch(label, cond, lhs, Operand(imm));
+}
+
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
+  SubWord(value, value, Operand(subtrahend));
+  TurboAssembler::Branch(result_negative, lt, value, Operand(zero_reg));
+}
+
+void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {
+  TurboAssembler::Sltu(dst, src, 1);
+}
+
+void LiftoffAssembler::emit_i32_set_cond(LiftoffCondition liftoff_cond,
+                                         Register dst, Register lhs,
+                                         Register rhs) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  TurboAssembler::CompareI(dst, lhs, Operand(rhs), cond);
+}
+
+void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {
+  Register tmp = GetUnusedRegister(kGpReg, LiftoffRegList{src, dst}).gp();
+  Sltu(tmp, src.low_gp(), 1);
+  Sltu(dst, src.high_gp(), 1);
+  and_(dst, dst, tmp);
+}
+
+namespace liftoff {
+inline LiftoffCondition cond_make_unsigned(LiftoffCondition cond) {
+  switch (cond) {
+    case kSignedLessThan:
+      return kUnsignedLessThan;
+    case kSignedLessEqual:
+      return kUnsignedLessEqual;
+    case kSignedGreaterThan:
+      return kUnsignedGreaterThan;
+    case kSignedGreaterEqual:
+      return kUnsignedGreaterEqual;
+    default:
+      return cond;
+  }
+}
+}  // namespace liftoff
+
+void LiftoffAssembler::emit_i64_set_cond(LiftoffCondition liftoff_cond,
+                                         Register dst, LiftoffRegister lhs,
+                                         LiftoffRegister rhs) {
+  ASM_CODE_COMMENT(this);
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  Label low, cont;
+
+  // For signed i64 comparisons, we still need to use unsigned comparison for
+  // the low word (the only bit carrying signedness information is the MSB in
+  // the high word).
+  Condition unsigned_cond =
+      liftoff::ToCondition(liftoff::cond_make_unsigned(liftoff_cond));
+
+  Register tmp = dst;
+  if (liftoff::IsRegInRegPair(lhs, dst) || liftoff::IsRegInRegPair(rhs, dst)) {
+    tmp = GetUnusedRegister(kGpReg, LiftoffRegList{dst, lhs, rhs}).gp();
+  }
+
+  // Write 1 initially in tmp register.
+  TurboAssembler::li(tmp, 1);
+
+  // If high words are equal, then compare low words, else compare high.
+  Branch(&low, eq, lhs.high_gp(), Operand(rhs.high_gp()));
+
+  Branch(&cont, cond, lhs.high_gp(), Operand(rhs.high_gp()));
+  mv(tmp, zero_reg);
+  Branch(&cont);
+
+  bind(&low);
+  if (unsigned_cond == cond) {
+    Branch(&cont, cond, lhs.low_gp(), Operand(rhs.low_gp()));
+    mv(tmp, zero_reg);
+  } else {
+    Label lt_zero;
+    Branch(&lt_zero, lt, lhs.high_gp(), Operand(zero_reg));
+    Branch(&cont, unsigned_cond, lhs.low_gp(), Operand(rhs.low_gp()));
+    mv(tmp, zero_reg);
+    Branch(&cont);
+    bind(&lt_zero);
+    Branch(&cont, unsigned_cond, rhs.low_gp(), Operand(lhs.low_gp()));
+    mv(tmp, zero_reg);
+    Branch(&cont);
+  }
+  bind(&cont);
+  // Move result to dst register if needed.
+  TurboAssembler::Move(dst, tmp);
+}
+
+void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  SmiUntag(scratch, MemOperand(dst.gp(), offset));
+  AddWord(scratch, scratch, Operand(1));
+  SmiTag(scratch);
+  Sw(scratch, MemOperand(dst.gp(), offset));
+}
+
+void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
+                                     Register offset_reg, uintptr_t offset_imm,
+                                     LoadType type,
+                                     LoadTransformationKind transform,
+                                     uint32_t* protected_load_pc) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  MemOperand src_op =
+      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, scratch);
+  VRegister dst_v = dst.fp().toV();
+  *protected_load_pc = pc_offset();
+
+  MachineType memtype = type.mem_type();
+  if (transform == LoadTransformationKind::kExtend) {
+    Lw(scratch, src_op);
+    if (memtype == MachineType::Int8()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E16, m1);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint8()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E16, m1);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Int16()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E32, m1);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint16()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E32, m1);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint32()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    }
+  } else if (transform == LoadTransformationKind::kZeroExtend) {
+    vxor_vv(dst_v, dst_v, dst_v);
+    if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E32, m1);
+      Lw(scratch, src_op);
+      vmv_sx(dst_v, scratch);
+    } else {
+      // TODO(RISCV): need review
+      DCHECK_EQ(MachineType::Int64(), memtype);
+      VU.set(kScratchReg, E64, m1);
+      Lw(scratch, src_op);
+      vmv_sx(dst_v, scratch);
+    }
+  } else {
+    DCHECK_EQ(LoadTransformationKind::kSplat, transform);
+    if (memtype == MachineType::Int8()) {
+      VU.set(kScratchReg, E8, m1);
+      Lb(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int16()) {
+      VU.set(kScratchReg, E16, m1);
+      Lh(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E32, m1);
+      Lw(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int64()) {
+      // TODO(RISCV): need review
+      VU.set(kScratchReg, E64, m1);
+      Lw(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    }
+  }
+}
+
+void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,
+                                Register addr, Register offset_reg,
+                                uintptr_t offset_imm, LoadType type,
+                                uint8_t laneidx, uint32_t* protected_load_pc) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  MemOperand src_op =
+      liftoff::GetMemOp(this, addr, offset_reg, offset_imm, scratch);
+  MachineType mem_type = type.mem_type();
+  *protected_load_pc = pc_offset();
+  if (mem_type == MachineType::Int8()) {
+    Lbu(scratch, src_op);
+    VU.set(kScratchReg, E64, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    VU.set(kScratchReg, E8, m1);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int16()) {
+    Lhu(scratch, src_op);
+    VU.set(kScratchReg, E16, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int32()) {
+    Lw(scratch, src_op);
+    VU.set(kScratchReg, E32, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int64()) {
+    Lw(scratch, src_op);
+    VU.set(kScratchReg, E32, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::StoreLane(Register dst, Register offset,
+                                 uintptr_t offset_imm, LiftoffRegister src,
+                                 StoreType type, uint8_t lane,
+                                 uint32_t* protected_store_pc) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  MemOperand dst_op = liftoff::GetMemOp(this, dst, offset, offset_imm, scratch);
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+  MachineRepresentation rep = type.mem_rep();
+  if (rep == MachineRepresentation::kWord8) {
+    VU.set(kScratchReg, E8, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sb(kScratchReg, dst_op);
+  } else if (rep == MachineRepresentation::kWord16) {
+    VU.set(kScratchReg, E16, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sh(kScratchReg, dst_op);
+  } else if (rep == MachineRepresentation::kWord32) {
+    VU.set(kScratchReg, E32, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sw(kScratchReg, dst_op);
+  } else {
+    DCHECK_EQ(MachineRepresentation::kWord64, rep);
+    VU.set(kScratchReg, E64, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sw(kScratchReg, dst_op);
+  }
+}
+
+void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
+                                          LiftoffRegister lhs,
+                                          LiftoffRegister rhs,
+                                          const uint8_t shuffle[16],
+                                          bool is_swizzle) {
+  // VRegister dst_v = dst.fp().toV();
+  // VRegister lhs_v = lhs.fp().toV();
+  // VRegister rhs_v = rhs.fp().toV();
+
+  // uint64_t imm1 = *(reinterpret_cast<const uint64_t*>(shuffle));
+  // uint64_t imm2 = *((reinterpret_cast<const uint64_t*>(shuffle)) + 1);
+  // VU.set(kScratchReg, VSew::E64, Vlmul::m1);
+  // li(kScratchReg, imm2);
+  // vmv_sx(kSimd128ScratchReg2, kScratchReg);
+  // vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
+  // li(kScratchReg, imm1);
+  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+
+  // VU.set(kScratchReg, E8, m1);
+  // VRegister temp =
+  //     GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
+  // if (dst_v == lhs_v) {
+  //   vmv_vv(temp, lhs_v);
+  //   lhs_v = temp;
+  // } else if (dst_v == rhs_v) {
+  //   vmv_vv(temp, rhs_v);
+  //   rhs_v = temp;
+  // }
+  // vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg);
+  // vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg,
+  //         -16);  // The indices in range [16, 31] select the i - 16-th
+  //         element
+  //                // of rhs
+  // vrgather_vv(kSimd128ScratchReg2, rhs_v, kSimd128ScratchReg);
+  // vor_vv(dst_v, dst_v, kSimd128ScratchReg2);
+  bailout(kSimd, "emit_i8x16_shuffle");
+}
+
+void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  // VU.set(kScratchReg, E64, m1);
+  // fmv_x_d(kScratchReg, src.fp());
+  // vmv_vx(dst.fp().toV(), kScratchReg);
+  bailout(kSimd, "emit_f64x2_splat");
+}
+
+void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  // VU.set(kScratchReg, E64, m1);
+  // const int64_t kNaN = 0x7ff8000000000000L;
+  // vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  // vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  // vand_vv(v0, v0, kSimd128ScratchReg);
+  // li(kScratchReg, kNaN);
+  // vmv_vx(kSimd128ScratchReg, kScratchReg);
+  // vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  // vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
+  bailout(kSimd, "emit_f64x2_min");
+}
+
+void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  // VU.set(kScratchReg, E64, m1);
+  // const int64_t kNaN = 0x7ff8000000000000L;
+  // vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  // vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  // vand_vv(v0, v0, kSimd128ScratchReg);
+  // li(kScratchReg, kNaN);
+  // vmv_vx(kSimd128ScratchReg, kScratchReg);
+  // vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  // vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
+  bailout(kSimd, "emit_f64x2_max");
+}
+
+void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  // VU.set(kScratchReg, E64, m1);
+  // li(kScratchReg, 0x0006000400020000);
+  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  // li(kScratchReg, 0x0007000500030001);
+  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  // VU.set(kScratchReg, E16, m1);
+  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  // VU.set(kScratchReg, E16, mf2);
+  // vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+  bailout(kSimd, "emit_i32x4_extadd_pairwise_i16x8_s");
+}
+
+void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  // VU.set(kScratchReg, E64, m1);
+  // li(kScratchReg, 0x0006000400020000);
+  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  // li(kScratchReg, 0x0007000500030001);
+  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  // VU.set(kScratchReg, E16, m1);
+  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  // VU.set(kScratchReg, E16, mf2);
+  // vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+  bailout(kSimd, "emit_i32x4_extadd_pairwise_i16x8_u");
+}
+
+void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  // VU.set(kScratchReg, E64, m1);
+  // li(kScratchReg, 0x0E0C0A0806040200);
+  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  // li(kScratchReg, 0x0F0D0B0907050301);
+  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  // VU.set(kScratchReg, E8, m1);
+  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  // VU.set(kScratchReg, E8, mf2);
+  // vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+  bailout(kSimd, "emit_i16x8_extadd_pairwise_i8x16_s");
+}
+
+void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  // VU.set(kScratchReg, E64, m1);
+  // li(kScratchReg, 0x0E0C0A0806040200);
+  // vmv_sx(kSimd128ScratchReg, kScratchReg);
+  // li(kScratchReg, 0x0F0D0B0907050301);
+  // vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  // VU.set(kScratchReg, E8, m1);
+  // vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  // vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  // VU.set(kScratchReg, E8, mf2);
+  // vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+  bailout(kSimd, "emit_i16x8_extadd_pairwise_i8x16_u");
+}
+
+void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  // VU.set(kScratchReg, E64, m1);
+  // li(kScratchReg, 0x1 << imm_lane_idx);
+  // vmv_sx(v0, kScratchReg);
+  // fmv_x_d(kScratchReg, src2.fp());
+  // vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
+  bailout(kSimd, "emit_f64x2_replace_lane");
+}
+
+void LiftoffAssembler::CallC(const ValueKindSig* sig,
+                             const LiftoffRegister* args,
+                             const LiftoffRegister* rets,
+                             ValueKind out_argument_kind, int stack_bytes,
+                             ExternalReference ext_ref) {
+  AddWord(sp, sp, Operand(-stack_bytes));
+
+  int arg_bytes = 0;
+  for (ValueKind param_kind : sig->parameters()) {
+    liftoff::Store(this, sp, arg_bytes, *args++, param_kind);
+    arg_bytes += value_kind_size(param_kind);
+  }
+  DCHECK_LE(arg_bytes, stack_bytes);
+
+  // Pass a pointer to the buffer with the arguments to the C function.
+  // On RISC-V, the first argument is passed in {a0}.
+  constexpr Register kFirstArgReg = a0;
+  mv(kFirstArgReg, sp);
+
+  // Now call the C function.
+  constexpr int kNumCCallArgs = 1;
+  PrepareCallCFunction(kNumCCallArgs, kScratchReg);
+  CallCFunction(ext_ref, kNumCCallArgs);
+
+  // Move return value to the right register.
+  const LiftoffRegister* next_result_reg = rets;
+  if (sig->return_count() > 0) {
+    DCHECK_EQ(1, sig->return_count());
+    constexpr Register kReturnReg = a0;
+    if (kReturnReg != next_result_reg->gp()) {
+      Move(*next_result_reg, LiftoffRegister(kReturnReg), sig->GetReturn(0));
+    }
+    ++next_result_reg;
+  }
+
+  // Load potential output value from the buffer on the stack.
+  if (out_argument_kind != kVoid) {
+    liftoff::Load(this, *next_result_reg, sp, 0, out_argument_kind);
+  }
+
+  AddWord(sp, sp, Operand(stack_bytes));
+}
+
+void LiftoffStackSlots::Construct(int param_slots) {
+  DCHECK_LT(0, slots_.size());
+  SortInPushOrder();
+  int last_stack_slot = param_slots;
+  for (auto& slot : slots_) {
+    const int stack_slot = slot.dst_slot_;
+    int stack_decrement = (last_stack_slot - stack_slot) * kSystemPointerSize;
+    DCHECK_LT(0, stack_decrement);
+    last_stack_slot = stack_slot;
+    const LiftoffAssembler::VarState& src = slot.src_;
+    switch (src.loc()) {
+      case LiftoffAssembler::VarState::kStack:
+        if (src.kind() != kS128) {
+          asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
+          asm_->Lw(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
+          asm_->push(kScratchReg);
+        } else {
+          asm_->AllocateStackSpace(stack_decrement - kSimd128Size);
+          asm_->Lw(kScratchReg, liftoff::GetStackSlot(slot.src_offset_ - 8));
+          asm_->push(kScratchReg);
+          asm_->Lw(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
+          asm_->push(kScratchReg);
+        }
+        break;
+      case LiftoffAssembler::VarState::kRegister: {
+        int pushed_bytes = SlotSizeInBytes(slot);
+        asm_->AllocateStackSpace(stack_decrement - pushed_bytes);
+        liftoff::push(asm_, src.reg(), src.kind());
+        break;
+      }
+      case LiftoffAssembler::VarState::kIntConst: {
+        asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
+        asm_->li(kScratchReg, Operand(src.i32_const()));
+        asm_->push(kScratchReg);
+        break;
+      }
+    }
+  }
+}
+
+}  // namespace wasm
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV32_H_
diff --git a/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h b/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
new file mode 100644
index 00000000000..5482f03587b
--- /dev/null
+++ b/src/wasm/baseline/riscv/liftoff-assembler-riscv64.h
@@ -0,0 +1,1715 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV64_H_
+#define V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV64_H_
+
+#include "src/base/platform/wrappers.h"
+#include "src/heap/memory-chunk.h"
+#include "src/wasm/baseline/liftoff-assembler.h"
+#include "src/wasm/baseline/riscv/liftoff-assembler-riscv.h"
+#include "src/wasm/wasm-objects.h"
+
+namespace v8 {
+namespace internal {
+namespace wasm {
+
+namespace liftoff {
+
+// Liftoff Frames.
+//
+//  slot      Frame
+//       +--------------------+---------------------------
+//  n+4  | optional padding slot to keep the stack 16 byte aligned.
+//  n+3  |   parameter n      |
+//  ...  |       ...          |
+//   4   |   parameter 1      | or parameter 2
+//   3   |   parameter 0      | or parameter 1
+//   2   |  (result address)  | or parameter 0
+//  -----+--------------------+---------------------------
+//   1   | return addr (ra)   |
+//   0   | previous frame (fp)|
+//  -----+--------------------+  <-- frame ptr (fp)
+//  -1   | StackFrame::WASM   |
+//  -2   |     instance       |
+//  -3   |     feedback vector|
+//  -4   |     tiering budget |
+//  -----+--------------------+---------------------------
+//  -5   |     slot 0         |   ^
+//  -6   |     slot 1         |   |
+//       |                    | Frame slots
+//       |                    |   |
+//       |                    |   v
+//       | optional padding slot to keep the stack 16 byte aligned.
+//  -----+--------------------+  <-- stack ptr (sp)
+//
+
+inline MemOperand GetMemOp(LiftoffAssembler* assm, Register addr,
+                           Register offset, uintptr_t offset_imm,
+                           bool i64_offset = false) {
+  if (!i64_offset && offset != no_reg) {
+    // extract bit[0:31] without sign extend
+    assm->ExtractBits(kScratchReg2, offset, 0, 32, false);
+    offset = kScratchReg2;
+  }
+  if (is_uint31(offset_imm)) {
+    int32_t offset_imm32 = static_cast<int32_t>(offset_imm);
+    if (offset == no_reg) return MemOperand(addr, offset_imm32);
+    assm->Add64(kScratchReg2, addr, offset);
+    return MemOperand(kScratchReg2, offset_imm32);
+  }
+  // Offset immediate does not fit in 31 bits.
+  assm->li(kScratchReg2, offset_imm);
+  assm->Add64(kScratchReg2, kScratchReg2, addr);
+  if (offset != no_reg) {
+    assm->Add64(kScratchReg2, kScratchReg2, offset);
+  }
+  return MemOperand(kScratchReg2, 0);
+}
+
+inline void Load(LiftoffAssembler* assm, LiftoffRegister dst, MemOperand src,
+                 ValueKind kind) {
+  switch (kind) {
+    case kI32:
+      assm->Lw(dst.gp(), src);
+      break;
+    case kI64:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      assm->Ld(dst.gp(), src);
+      break;
+    case kF32:
+      assm->LoadFloat(dst.fp(), src);
+      break;
+    case kF64:
+      assm->LoadDouble(dst.fp(), src);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void Store(LiftoffAssembler* assm, Register base, int32_t offset,
+                  LiftoffRegister src, ValueKind kind) {
+  MemOperand dst(base, offset);
+  switch (kind) {
+    case kI32:
+      assm->Sw(src.gp(), dst);
+      break;
+    case kI64:
+    case kRefNull:
+    case kRef:
+    case kRtt:
+      assm->Sd(src.gp(), dst);
+      break;
+    case kF32:
+      assm->StoreFloat(src.fp(), dst);
+      break;
+    case kF64:
+      assm->StoreDouble(src.fp(), dst);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+inline void push(LiftoffAssembler* assm, LiftoffRegister reg, ValueKind kind) {
+  switch (kind) {
+    case kI32:
+      assm->addi(sp, sp, -kSystemPointerSize);
+      assm->Sw(reg.gp(), MemOperand(sp, 0));
+      break;
+    case kI64:
+    case kRefNull:
+    case kRef:
+    case kRtt:
+      assm->push(reg.gp());
+      break;
+    case kF32:
+      assm->addi(sp, sp, -kSystemPointerSize);
+      assm->StoreFloat(reg.fp(), MemOperand(sp, 0));
+      break;
+    case kF64:
+      assm->addi(sp, sp, -kSystemPointerSize);
+      assm->StoreDouble(reg.fp(), MemOperand(sp, 0));
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+}  // namespace liftoff
+
+void LiftoffAssembler::LoadConstant(LiftoffRegister reg, WasmValue value,
+                                    RelocInfo::Mode rmode) {
+  switch (value.type().kind()) {
+    case kI32:
+      TurboAssembler::li(reg.gp(), Operand(value.to_i32(), rmode));
+      break;
+    case kI64:
+      TurboAssembler::li(reg.gp(), Operand(value.to_i64(), rmode));
+      break;
+    case kF32:
+      TurboAssembler::LoadFPRImmediate(reg.fp(),
+                                       value.to_f32_boxed().get_bits());
+      break;
+    case kF64:
+      TurboAssembler::LoadFPRImmediate(reg.fp(),
+                                       value.to_f64_boxed().get_bits());
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
+                                         Register offset_reg,
+                                         int32_t offset_imm) {
+  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
+  LoadTaggedPointerField(dst, src_op);
+}
+
+void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,
+                                       int32_t offset_imm) {
+  MemOperand src_op = liftoff::GetMemOp(this, src_addr, no_reg, offset_imm);
+  LoadWord(dst, src_op);
+}
+
+void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
+                                          Register offset_reg,
+                                          int32_t offset_imm,
+                                          LiftoffRegister src,
+                                          LiftoffRegList pinned,
+                                          SkipWriteBarrier skip_write_barrier) {
+  Register scratch = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
+  StoreTaggedField(src.gp(), dst_op);
+
+  if (skip_write_barrier || FLAG_disable_write_barriers) return;
+
+  Label write_barrier;
+  Label exit;
+  CheckPageFlag(dst_addr, scratch,
+                MemoryChunk::kPointersFromHereAreInterestingMask, ne,
+                &write_barrier);
+  Branch(&exit);
+  bind(&write_barrier);
+  JumpIfSmi(src.gp(), &exit);
+  CheckPageFlag(src.gp(), scratch,
+                MemoryChunk::kPointersToHereAreInterestingMask, eq, &exit);
+  AddWord(scratch, dst_op.rm(), dst_op.offset());
+  CallRecordWriteStubSaveRegisters(dst_addr, scratch, SaveFPRegsMode::kSave,
+                                   StubCallMode::kCallWasmRuntimeStub);
+  bind(&exit);
+}
+
+void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
+                            Register offset_reg, uintptr_t offset_imm,
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
+  MemOperand src_op =
+      liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm, i64_offset);
+
+  if (protected_load_pc) *protected_load_pc = pc_offset();
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+    case LoadType::kI64Load8U:
+      Lbu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load8S:
+    case LoadType::kI64Load8S:
+      Lb(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load16U:
+    case LoadType::kI64Load16U:
+      TurboAssembler::Lhu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load16S:
+    case LoadType::kI64Load16S:
+      TurboAssembler::Lh(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load32U:
+      TurboAssembler::Lwu(dst.gp(), src_op);
+      break;
+    case LoadType::kI32Load:
+    case LoadType::kI64Load32S:
+      TurboAssembler::Lw(dst.gp(), src_op);
+      break;
+    case LoadType::kI64Load:
+      TurboAssembler::Ld(dst.gp(), src_op);
+      break;
+    case LoadType::kF32Load:
+      TurboAssembler::LoadFloat(dst.fp(), src_op);
+      break;
+    case LoadType::kF64Load:
+      TurboAssembler::LoadDouble(dst.fp(), src_op);
+      break;
+    case LoadType::kS128Load: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src_op.offset() == 0 ? src_op.rm() : kScratchReg;
+      if (src_op.offset() != 0) {
+        TurboAssembler::AddWord(src_reg, src_op.rm(), src_op.offset());
+      }
+      vl(dst.fp().toV(), src_reg, 0, E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_load_mem) {
+    pinned.set(src_op.rm());
+    liftoff::ChangeEndiannessLoad(this, dst, type, pinned);
+  }
+#endif
+}
+
+void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
+                             uintptr_t offset_imm, LiftoffRegister src,
+                             StoreType type, LiftoffRegList pinned,
+                             uint32_t* protected_store_pc, bool is_store_mem,
+                             bool i64_offset) {
+  MemOperand dst_op =
+      liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm, i64_offset);
+
+#if defined(V8_TARGET_BIG_ENDIAN)
+  if (is_store_mem) {
+    pinned.set(dst_op.rm());
+    LiftoffRegister tmp = GetUnusedRegister(src.reg_class(), pinned);
+    // Save original value.
+    Move(tmp, src, type.value_type());
+
+    src = tmp;
+    pinned.set(tmp);
+    liftoff::ChangeEndiannessStore(this, src, type, pinned);
+  }
+#endif
+
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+
+  switch (type.value()) {
+    case StoreType::kI32Store8:
+    case StoreType::kI64Store8:
+      Sb(src.gp(), dst_op);
+      break;
+    case StoreType::kI32Store16:
+    case StoreType::kI64Store16:
+      TurboAssembler::Sh(src.gp(), dst_op);
+      break;
+    case StoreType::kI32Store:
+    case StoreType::kI64Store32:
+      TurboAssembler::Sw(src.gp(), dst_op);
+      break;
+    case StoreType::kI64Store:
+      TurboAssembler::Sd(src.gp(), dst_op);
+      break;
+    case StoreType::kF32Store:
+      TurboAssembler::StoreFloat(src.fp(), dst_op);
+      break;
+    case StoreType::kF64Store:
+      TurboAssembler::StoreDouble(src.fp(), dst_op);
+      break;
+    case StoreType::kS128Store: {
+      VU.set(kScratchReg, E8, m1);
+      Register dst_reg = dst_op.offset() == 0 ? dst_op.rm() : kScratchReg;
+      if (dst_op.offset() != 0) {
+        Add64(kScratchReg, dst_op.rm(), dst_op.offset());
+      }
+      vs(src.fp().toV(), dst_reg, 0, VSew::E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+namespace liftoff {
+#define __ lasm->
+
+inline Register CalculateActualAddress(LiftoffAssembler* lasm,
+                                       Register addr_reg, Register offset_reg,
+                                       uintptr_t offset_imm,
+                                       Register result_reg) {
+  DCHECK_NE(offset_reg, no_reg);
+  DCHECK_NE(addr_reg, no_reg);
+  __ AddWord(result_reg, addr_reg, Operand(offset_reg));
+  if (offset_imm != 0) {
+    __ AddWord(result_reg, result_reg, Operand(offset_imm));
+  }
+  return result_reg;
+}
+
+enum class Binop { kAdd, kSub, kAnd, kOr, kXor, kExchange };
+
+inline void AtomicBinop(LiftoffAssembler* lasm, Register dst_addr,
+                        Register offset_reg, uintptr_t offset_imm,
+                        LiftoffRegister value, LiftoffRegister result,
+                        StoreType type, Binop op) {
+  LiftoffRegList pinned{dst_addr, offset_reg, value, result};
+  Register store_result = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+
+  // Make sure that {result} is unique.
+  Register result_reg = result.gp();
+  if (result_reg == value.gp() || result_reg == dst_addr ||
+      result_reg == offset_reg) {
+    result_reg = __ GetUnusedRegister(kGpReg, pinned).gp();
+  }
+
+  UseScratchRegisterScope temps(lasm);
+  Register actual_addr = liftoff::CalculateActualAddress(
+      lasm, dst_addr, offset_reg, offset_imm, temps.Acquire());
+
+  // Allocate an additional {temp} register to hold the result that should be
+  // stored to memory. Note that {temp} and {store_result} are not allowed to be
+  // the same register.
+  Register temp = temps.Acquire();
+
+  Label retry;
+  __ bind(&retry);
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      __ lbu(result_reg, actual_addr, 0);
+      __ sync();
+      break;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      __ lhu(result_reg, actual_addr, 0);
+      __ sync();
+      break;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      __ lr_w(true, false, result_reg, actual_addr);
+      break;
+    case StoreType::kI64Store:
+      __ lr_d(true, false, result_reg, actual_addr);
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  switch (op) {
+    case Binop::kAdd:
+      __ add(temp, result_reg, value.gp());
+      break;
+    case Binop::kSub:
+      __ sub(temp, result_reg, value.gp());
+      break;
+    case Binop::kAnd:
+      __ and_(temp, result_reg, value.gp());
+      break;
+    case Binop::kOr:
+      __ or_(temp, result_reg, value.gp());
+      break;
+    case Binop::kXor:
+      __ xor_(temp, result_reg, value.gp());
+      break;
+    case Binop::kExchange:
+      __ mv(temp, value.gp());
+      break;
+  }
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      __ sync();
+      __ sb(temp, actual_addr, 0);
+      __ sync();
+      __ mv(store_result, zero_reg);
+      break;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      __ sync();
+      __ sh(temp, actual_addr, 0);
+      __ sync();
+      __ mv(store_result, zero_reg);
+      break;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      __ sc_w(false, true, store_result, actual_addr, temp);
+      break;
+    case StoreType::kI64Store:
+      __ sc_w(false, true, store_result, actual_addr, temp);
+      break;
+    default:
+      UNREACHABLE();
+  }
+
+  __ bnez(store_result, &retry);
+  if (result_reg != result.gp()) {
+    __ mv(result.gp(), result_reg);
+  }
+}
+
+#undef __
+}  // namespace liftoff
+
+void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
+                                  Register offset_reg, uintptr_t offset_imm,
+                                  LoadType type, LiftoffRegList pinned) {
+  UseScratchRegisterScope temps(this);
+  Register src_reg = liftoff::CalculateActualAddress(
+      this, src_addr, offset_reg, offset_imm, temps.Acquire());
+  switch (type.value()) {
+    case LoadType::kI32Load8U:
+    case LoadType::kI64Load8U:
+      lbu(dst.gp(), src_reg, 0);
+      sync();
+      return;
+    case LoadType::kI32Load16U:
+    case LoadType::kI64Load16U:
+      lhu(dst.gp(), src_reg, 0);
+      sync();
+      return;
+    case LoadType::kI32Load:
+    case LoadType::kI64Load32U:
+      lw(dst.gp(), src_reg, 0);
+      sync();
+      return;
+    case LoadType::kI64Load:
+      ld(dst.gp(), src_reg, 0);
+      sync();
+      return;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
+                                   uintptr_t offset_imm, LiftoffRegister src,
+                                   StoreType type, LiftoffRegList pinned) {
+  UseScratchRegisterScope temps(this);
+  Register dst_reg = liftoff::CalculateActualAddress(
+      this, dst_addr, offset_reg, offset_imm, temps.Acquire());
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+    case StoreType::kI32Store8:
+      sync();
+      sb(src.gp(), dst_reg, 0);
+      return;
+    case StoreType::kI64Store16:
+    case StoreType::kI32Store16:
+      sync();
+      sh(src.gp(), dst_reg, 0);
+      return;
+    case StoreType::kI64Store32:
+    case StoreType::kI32Store:
+      sync();
+      sw(src.gp(), dst_reg, 0);
+      return;
+    case StoreType::kI64Store:
+      sync();
+      sd(src.gp(), dst_reg, 0);
+      return;
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::AtomicAdd(Register dst_addr, Register offset_reg,
+                                 uintptr_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kAdd);
+}
+
+void LiftoffAssembler::AtomicSub(Register dst_addr, Register offset_reg,
+                                 uintptr_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kSub);
+}
+
+void LiftoffAssembler::AtomicAnd(Register dst_addr, Register offset_reg,
+                                 uintptr_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kAnd);
+}
+
+void LiftoffAssembler::AtomicOr(Register dst_addr, Register offset_reg,
+                                uintptr_t offset_imm, LiftoffRegister value,
+                                LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kOr);
+}
+
+void LiftoffAssembler::AtomicXor(Register dst_addr, Register offset_reg,
+                                 uintptr_t offset_imm, LiftoffRegister value,
+                                 LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kXor);
+}
+
+void LiftoffAssembler::AtomicExchange(Register dst_addr, Register offset_reg,
+                                      uintptr_t offset_imm,
+                                      LiftoffRegister value,
+                                      LiftoffRegister result, StoreType type) {
+  liftoff::AtomicBinop(this, dst_addr, offset_reg, offset_imm, value, result,
+                       type, liftoff::Binop::kExchange);
+}
+
+#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(load_linked,       \
+                                                 store_conditional) \
+  do {                                                              \
+    Label compareExchange;                                          \
+    Label exit;                                                     \
+    sync();                                                         \
+    bind(&compareExchange);                                         \
+    load_linked(result.gp(), MemOperand(temp0, 0));                 \
+    BranchShort(&exit, ne, expected.gp(), Operand(result.gp()));    \
+    mv(temp2, new_value.gp());                                      \
+    store_conditional(temp2, MemOperand(temp0, 0));                 \
+    BranchShort(&compareExchange, ne, temp2, Operand(zero_reg));    \
+    bind(&exit);                                                    \
+    sync();                                                         \
+  } while (0)
+
+#define ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(            \
+    load_linked, store_conditional, size, aligned)               \
+  do {                                                           \
+    Label compareExchange;                                       \
+    Label exit;                                                  \
+    andi(temp1, temp0, aligned);                                 \
+    Sub64(temp0, temp0, Operand(temp1));                         \
+    Sll32(temp1, temp1, 3);                                      \
+    sync();                                                      \
+    bind(&compareExchange);                                      \
+    load_linked(temp2, MemOperand(temp0, 0));                    \
+    ExtractBits(result.gp(), temp2, temp1, size, false);         \
+    ExtractBits(temp2, expected.gp(), zero_reg, size, false);    \
+    BranchShort(&exit, ne, temp2, Operand(result.gp()));         \
+    InsertBits(temp2, new_value.gp(), temp1, size);              \
+    store_conditional(temp2, MemOperand(temp0, 0));              \
+    BranchShort(&compareExchange, ne, temp2, Operand(zero_reg)); \
+    bind(&exit);                                                 \
+    sync();                                                      \
+  } while (0)
+
+void LiftoffAssembler::AtomicCompareExchange(
+    Register dst_addr, Register offset_reg, uintptr_t offset_imm,
+    LiftoffRegister expected, LiftoffRegister new_value, LiftoffRegister result,
+    StoreType type) {
+  LiftoffRegList pinned{dst_addr, offset_reg, expected, new_value, result};
+  Register temp0 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  Register temp1 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  Register temp2 = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
+  MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
+  Add64(temp0, dst_op.rm(), dst_op.offset());
+  switch (type.value()) {
+    case StoreType::kI64Store8:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 8, 7);
+      break;
+    case StoreType::kI32Store8:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 8, 3);
+      break;
+    case StoreType::kI64Store16:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 16, 7);
+      break;
+    case StoreType::kI32Store16:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Ll, Sc, 16, 3);
+      break;
+    case StoreType::kI64Store32:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT(Lld, Scd, 32, 7);
+      break;
+    case StoreType::kI32Store:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Ll, Sc);
+      break;
+    case StoreType::kI64Store:
+      ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER(Lld, Scd);
+      break;
+    default:
+      UNREACHABLE();
+  }
+}
+#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER
+#undef ASSEMBLE_ATOMIC_COMPARE_EXCHANGE_INTEGER_EXT
+
+void LiftoffAssembler::AtomicFence() { sync(); }
+
+void LiftoffAssembler::LoadCallerFrameSlot(LiftoffRegister dst,
+                                           uint32_t caller_slot_idx,
+                                           ValueKind kind) {
+  MemOperand src(fp, kSystemPointerSize * (caller_slot_idx + 1));
+  liftoff::Load(this, dst, src, kind);
+}
+
+void LiftoffAssembler::StoreCallerFrameSlot(LiftoffRegister src,
+                                            uint32_t caller_slot_idx,
+                                            ValueKind kind) {
+  int32_t offset = kSystemPointerSize * (caller_slot_idx + 1);
+  liftoff::Store(this, fp, offset, src, kind);
+}
+
+void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,
+                                           ValueKind kind) {
+  liftoff::Load(this, dst, MemOperand(sp, offset), kind);
+}
+
+void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
+                                      ValueKind kind) {
+  DCHECK_NE(dst_offset, src_offset);
+
+  MemOperand src = liftoff::GetStackSlot(src_offset);
+  MemOperand dst = liftoff::GetStackSlot(dst_offset);
+  switch (kind) {
+    case kI32:
+      Lw(kScratchReg, src);
+      Sw(kScratchReg, dst);
+      break;
+    case kI64:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      Ld(kScratchReg, src);
+      Sd(kScratchReg, dst);
+      break;
+    case kF32:
+      LoadFloat(kScratchDoubleReg, src);
+      StoreFloat(kScratchDoubleReg, dst);
+      break;
+    case kF64:
+      TurboAssembler::LoadDouble(kScratchDoubleReg, src);
+      TurboAssembler::StoreDouble(kScratchDoubleReg, dst);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
+      if (src.offset() != 0) {
+        TurboAssembler::Add64(src_reg, src.rm(), src.offset());
+      }
+      vl(kSimd128ScratchReg, src_reg, 0, E8);
+      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
+      if (dst.offset() != 0) {
+        Add64(kScratchReg, dst.rm(), dst.offset());
+      }
+      vs(kSimd128ScratchReg, dst_reg, 0, VSew::E8);
+      break;
+    }
+    case kVoid:
+    case kI8:
+    case kI16:
+    case kBottom:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) {
+  DCHECK_NE(dst, src);
+  // TODO(ksreten): Handle different sizes here.
+  TurboAssembler::Move(dst, src);
+}
+
+void LiftoffAssembler::Move(DoubleRegister dst, DoubleRegister src,
+                            ValueKind kind) {
+  DCHECK_NE(dst, src);
+  if (kind != kS128) {
+    TurboAssembler::Move(dst, src);
+  } else {
+    TurboAssembler::vmv_vv(dst.toV(), dst.toV());
+  }
+}
+
+void LiftoffAssembler::Spill(int offset, LiftoffRegister reg, ValueKind kind) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (kind) {
+    case kI32:
+      Sw(reg.gp(), dst);
+      break;
+    case kI64:
+    case kRef:
+    case kRefNull:
+    case kRtt:
+      Sd(reg.gp(), dst);
+      break;
+    case kF32:
+      StoreFloat(reg.fp(), dst);
+      break;
+    case kF64:
+      TurboAssembler::StoreDouble(reg.fp(), dst);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register dst_reg = dst.offset() == 0 ? dst.rm() : kScratchReg;
+      if (dst.offset() != 0) {
+        Add64(kScratchReg, dst.rm(), dst.offset());
+      }
+      vs(reg.fp().toV(), dst_reg, 0, VSew::E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Spill(int offset, WasmValue value) {
+  RecordUsedSpillOffset(offset);
+  MemOperand dst = liftoff::GetStackSlot(offset);
+  switch (value.type().kind()) {
+    case kI32: {
+      UseScratchRegisterScope temps(this);
+      Register tmp = temps.Acquire();
+      TurboAssembler::li(tmp, Operand(value.to_i32()));
+      Sw(tmp, dst);
+      break;
+    }
+    case kI64:
+    case kRef:
+    case kRefNull: {
+      UseScratchRegisterScope temps(this);
+      Register tmp = temps.Acquire();
+      TurboAssembler::li(tmp, value.to_i64());
+      Sd(tmp, dst);
+      break;
+    }
+    default:
+      // kWasmF32 and kWasmF64 are unreachable, since those
+      // constants are not tracked.
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::Fill(LiftoffRegister reg, int offset, ValueKind kind) {
+  MemOperand src = liftoff::GetStackSlot(offset);
+  switch (kind) {
+    case kI32:
+      Lw(reg.gp(), src);
+      break;
+    case kI64:
+    case kRef:
+    case kRefNull:
+      Ld(reg.gp(), src);
+      break;
+    case kF32:
+      LoadFloat(reg.fp(), src);
+      break;
+    case kF64:
+      TurboAssembler::LoadDouble(reg.fp(), src);
+      break;
+    case kS128: {
+      VU.set(kScratchReg, E8, m1);
+      Register src_reg = src.offset() == 0 ? src.rm() : kScratchReg;
+      if (src.offset() != 0) {
+        TurboAssembler::Add64(src_reg, src.rm(), src.offset());
+      }
+      vl(reg.fp().toV(), src_reg, 0, E8);
+      break;
+    }
+    default:
+      UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::FillI64Half(Register, int offset, RegPairHalf) {
+  UNREACHABLE();
+}
+
+void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
+  DCHECK_LT(0, size);
+  RecordUsedSpillOffset(start + size);
+
+  if (size <= 12 * kStackSlotSize) {
+    // Special straight-line code for up to 12 slots. Generates one
+    // instruction per slot (<= 12 instructions total).
+    uint32_t remainder = size;
+    for (; remainder >= kStackSlotSize; remainder -= kStackSlotSize) {
+      Sd(zero_reg, liftoff::GetStackSlot(start + remainder));
+    }
+    DCHECK(remainder == 4 || remainder == 0);
+    if (remainder) {
+      Sw(zero_reg, liftoff::GetStackSlot(start + remainder));
+    }
+  } else {
+    // General case for bigger counts (12 instructions).
+    // Use a0 for start address (inclusive), a1 for end address (exclusive).
+    Push(a1, a0);
+    Add64(a0, fp, Operand(-start - size));
+    Add64(a1, fp, Operand(-start));
+
+    Label loop;
+    bind(&loop);
+    Sd(zero_reg, MemOperand(a0));
+    addi(a0, a0, kSystemPointerSize);
+    BranchShort(&loop, ne, a0, Operand(a1));
+
+    Pop(a1, a0);
+  }
+}
+
+void LiftoffAssembler::emit_i64_clz(LiftoffRegister dst, LiftoffRegister src) {
+  TurboAssembler::Clz64(dst.gp(), src.gp());
+}
+
+void LiftoffAssembler::emit_i64_ctz(LiftoffRegister dst, LiftoffRegister src) {
+  TurboAssembler::Ctz64(dst.gp(), src.gp());
+}
+
+bool LiftoffAssembler::emit_i64_popcnt(LiftoffRegister dst,
+                                       LiftoffRegister src) {
+  TurboAssembler::Popcnt64(dst.gp(), src.gp(), kScratchReg);
+  return true;
+}
+
+void LiftoffAssembler::emit_i32_mul(Register dst, Register lhs, Register rhs) {
+  TurboAssembler::Mul32(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divs(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+
+  // Check if lhs == kMinInt and rhs == -1, since this case is unrepresentable.
+  TurboAssembler::CompareI(kScratchReg, lhs, Operand(kMinInt), ne);
+  TurboAssembler::CompareI(kScratchReg2, rhs, Operand(-1), ne);
+  add(kScratchReg, kScratchReg, kScratchReg2);
+  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
+                         Operand(zero_reg));
+
+  TurboAssembler::Div32(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_divu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Divu32(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_rems(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Mod32(dst, lhs, rhs);
+}
+
+void LiftoffAssembler::emit_i32_remu(Register dst, Register lhs, Register rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs, Operand(zero_reg));
+  TurboAssembler::Modu32(dst, lhs, rhs);
+}
+
+#define I32_BINOP(name, instruction)                                 \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register lhs, \
+                                         Register rhs) {             \
+    instruction(dst, lhs, rhs);                                      \
+  }
+
+// clang-format off
+I32_BINOP(add, addw)
+I32_BINOP(sub, subw)
+I32_BINOP(and, and_)
+I32_BINOP(or, or_)
+I32_BINOP(xor, xor_)
+// clang-format on
+
+#undef I32_BINOP
+
+#define I32_BINOP_I(name, instruction)                                  \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register lhs, \
+                                            int32_t imm) {              \
+    instruction(dst, lhs, Operand(imm));                                \
+  }
+
+// clang-format off
+I32_BINOP_I(add, Add32)
+I32_BINOP_I(sub, Sub32)
+I32_BINOP_I(and, And)
+I32_BINOP_I(or, Or)
+I32_BINOP_I(xor, Xor)
+// clang-format on
+
+#undef I32_BINOP_I
+
+void LiftoffAssembler::emit_i32_clz(Register dst, Register src) {
+  TurboAssembler::Clz32(dst, src);
+}
+
+void LiftoffAssembler::emit_i32_ctz(Register dst, Register src) {
+  TurboAssembler::Ctz32(dst, src);
+}
+
+bool LiftoffAssembler::emit_i32_popcnt(Register dst, Register src) {
+  TurboAssembler::Popcnt32(dst, src, kScratchReg);
+  return true;
+}
+
+#define I32_SHIFTOP(name, instruction)                               \
+  void LiftoffAssembler::emit_i32_##name(Register dst, Register src, \
+                                         Register amount) {          \
+    instruction(dst, src, amount);                                   \
+  }
+#define I32_SHIFTOP_I(name, instruction)                                \
+  void LiftoffAssembler::emit_i32_##name##i(Register dst, Register src, \
+                                            int amount) {               \
+    instruction(dst, src, amount & 31);                                 \
+  }
+
+I32_SHIFTOP(shl, sllw)
+I32_SHIFTOP(sar, sraw)
+I32_SHIFTOP(shr, srlw)
+
+I32_SHIFTOP_I(shl, slliw)
+I32_SHIFTOP_I(sar, sraiw)
+I32_SHIFTOP_I(shr, srliw)
+
+#undef I32_SHIFTOP
+#undef I32_SHIFTOP_I
+
+void LiftoffAssembler::emit_i64_mul(LiftoffRegister dst, LiftoffRegister lhs,
+                                    LiftoffRegister rhs) {
+  TurboAssembler::Mul64(dst.gp(), lhs.gp(), rhs.gp());
+}
+
+bool LiftoffAssembler::emit_i64_divs(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero,
+                                     Label* trap_div_unrepresentable) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+
+  // Check if lhs == MinInt64 and rhs == -1, since this case is unrepresentable.
+  TurboAssembler::CompareI(kScratchReg, lhs.gp(),
+                           Operand(std::numeric_limits<int64_t>::min()), ne);
+  TurboAssembler::CompareI(kScratchReg2, rhs.gp(), Operand(-1), ne);
+  add(kScratchReg, kScratchReg, kScratchReg2);
+  TurboAssembler::Branch(trap_div_unrepresentable, eq, kScratchReg,
+                         Operand(zero_reg));
+
+  TurboAssembler::Div64(dst.gp(), lhs.gp(), rhs.gp());
+  return true;
+}
+
+bool LiftoffAssembler::emit_i64_divu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+  TurboAssembler::Divu64(dst.gp(), lhs.gp(), rhs.gp());
+  return true;
+}
+
+bool LiftoffAssembler::emit_i64_rems(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+  TurboAssembler::Mod64(dst.gp(), lhs.gp(), rhs.gp());
+  return true;
+}
+
+bool LiftoffAssembler::emit_i64_remu(LiftoffRegister dst, LiftoffRegister lhs,
+                                     LiftoffRegister rhs,
+                                     Label* trap_div_by_zero) {
+  TurboAssembler::Branch(trap_div_by_zero, eq, rhs.gp(), Operand(zero_reg));
+  TurboAssembler::Modu64(dst.gp(), lhs.gp(), rhs.gp());
+  return true;
+}
+
+#define I64_BINOP(name, instruction)                                   \
+  void LiftoffAssembler::emit_i64_##name(                              \
+      LiftoffRegister dst, LiftoffRegister lhs, LiftoffRegister rhs) { \
+    instruction(dst.gp(), lhs.gp(), rhs.gp());                         \
+  }
+
+// clang-format off
+I64_BINOP(add, add)
+I64_BINOP(sub, sub)
+I64_BINOP(and, and_)
+I64_BINOP(or, or_)
+I64_BINOP(xor, xor_)
+// clang-format on
+
+#undef I64_BINOP
+
+#define I64_BINOP_I(name, instruction)                         \
+  void LiftoffAssembler::emit_i64_##name##i(                   \
+      LiftoffRegister dst, LiftoffRegister lhs, int32_t imm) { \
+    instruction(dst.gp(), lhs.gp(), Operand(imm));             \
+  }
+
+// clang-format off
+I64_BINOP_I(and, And)
+I64_BINOP_I(or, Or)
+I64_BINOP_I(xor, Xor)
+// clang-format on
+
+#undef I64_BINOP_I
+
+#define I64_SHIFTOP(name, instruction)                             \
+  void LiftoffAssembler::emit_i64_##name(                          \
+      LiftoffRegister dst, LiftoffRegister src, Register amount) { \
+    instruction(dst.gp(), src.gp(), amount);                       \
+  }
+
+I64_SHIFTOP(shl, sll)
+I64_SHIFTOP(sar, sra)
+I64_SHIFTOP(shr, srl)
+#undef I64_SHIFTOP
+
+void LiftoffAssembler::emit_i64_shli(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  if (is_uint6(amount)) {
+    slli(dst.gp(), src.gp(), amount);
+  } else {
+    li(kScratchReg, amount);
+    sll(dst.gp(), src.gp(), kScratchReg);
+  }
+}
+
+void LiftoffAssembler::emit_i64_sari(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  if (is_uint6(amount)) {
+    srai(dst.gp(), src.gp(), amount);
+  } else {
+    li(kScratchReg, amount);
+    sra(dst.gp(), src.gp(), kScratchReg);
+  }
+}
+
+void LiftoffAssembler::emit_i64_shri(LiftoffRegister dst, LiftoffRegister src,
+                                     int amount) {
+  if (is_uint6(amount)) {
+    srli(dst.gp(), src.gp(), amount);
+  } else {
+    li(kScratchReg, amount);
+    srl(dst.gp(), src.gp(), kScratchReg);
+  }
+}
+
+void LiftoffAssembler::emit_i64_addi(LiftoffRegister dst, LiftoffRegister lhs,
+                                     int64_t imm) {
+  TurboAssembler::Add64(dst.gp(), lhs.gp(), Operand(imm));
+}
+void LiftoffAssembler::emit_u32_to_uintptr(Register dst, Register src) {
+  ZeroExtendWord(dst, src);
+}
+
+#define FP_UNOP_RETURN_TRUE(name, instruction)                                 \
+  bool LiftoffAssembler::emit_##name(DoubleRegister dst, DoubleRegister src) { \
+    instruction(dst, src, kScratchDoubleReg);                                  \
+    return true;                                                               \
+  }
+
+FP_UNOP_RETURN_TRUE(f64_ceil, Ceil_d_d)
+FP_UNOP_RETURN_TRUE(f64_floor, Floor_d_d)
+FP_UNOP_RETURN_TRUE(f64_trunc, Trunc_d_d)
+FP_UNOP_RETURN_TRUE(f64_nearest_int, Round_d_d)
+
+#undef FP_UNOP_RETURN_TRUE
+
+bool LiftoffAssembler::emit_type_conversion(WasmOpcode opcode,
+                                            LiftoffRegister dst,
+                                            LiftoffRegister src, Label* trap) {
+  switch (opcode) {
+    case kExprI32ConvertI64:
+      // According to WebAssembly spec, if I64 value does not fit the range of
+      // I32, the value is undefined. Therefore, We use sign extension to
+      // implement I64 to I32 truncation
+      TurboAssembler::SignExtendWord(dst.gp(), src.gp());
+      return true;
+    case kExprI32SConvertF32:
+    case kExprI32UConvertF32:
+    case kExprI32SConvertF64:
+    case kExprI32UConvertF64:
+    case kExprI64SConvertF32:
+    case kExprI64UConvertF32:
+    case kExprI64SConvertF64:
+    case kExprI64UConvertF64:
+    case kExprF32ConvertF64: {
+      // real conversion, if src is out-of-bound of target integer types,
+      // kScratchReg is set to 0
+      switch (opcode) {
+        case kExprI32SConvertF32:
+          Trunc_w_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32UConvertF32:
+          Trunc_uw_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32SConvertF64:
+          Trunc_w_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI32UConvertF64:
+          Trunc_uw_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI64SConvertF32:
+          Trunc_l_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI64UConvertF32:
+          Trunc_ul_s(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI64SConvertF64:
+          Trunc_l_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprI64UConvertF64:
+          Trunc_ul_d(dst.gp(), src.fp(), kScratchReg);
+          break;
+        case kExprF32ConvertF64:
+          fcvt_s_d(dst.fp(), src.fp());
+          break;
+        default:
+          UNREACHABLE();
+      }
+
+      // Checking if trap.
+      if (trap != nullptr) {
+        TurboAssembler::Branch(trap, eq, kScratchReg, Operand(zero_reg));
+      }
+
+      return true;
+    }
+    case kExprI32ReinterpretF32:
+      TurboAssembler::ExtractLowWordFromF64(dst.gp(), src.fp());
+      return true;
+    case kExprI64SConvertI32:
+      TurboAssembler::SignExtendWord(dst.gp(), src.gp());
+      return true;
+    case kExprI64UConvertI32:
+      TurboAssembler::ZeroExtendWord(dst.gp(), src.gp());
+      return true;
+    case kExprI64ReinterpretF64:
+      fmv_x_d(dst.gp(), src.fp());
+      return true;
+    case kExprF32SConvertI32: {
+      TurboAssembler::Cvt_s_w(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF32UConvertI32:
+      TurboAssembler::Cvt_s_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF32ReinterpretI32:
+      fmv_w_x(dst.fp(), src.gp());
+      return true;
+    case kExprF64SConvertI32: {
+      TurboAssembler::Cvt_d_w(dst.fp(), src.gp());
+      return true;
+    }
+    case kExprF64UConvertI32:
+      TurboAssembler::Cvt_d_uw(dst.fp(), src.gp());
+      return true;
+    case kExprF64ConvertF32:
+      fcvt_d_s(dst.fp(), src.fp());
+      return true;
+    case kExprF64ReinterpretI64:
+      fmv_d_x(dst.fp(), src.gp());
+      return true;
+    case kExprI32SConvertSatF32: {
+      fcvt_w_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32UConvertSatF32: {
+      fcvt_wu_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32SConvertSatF64: {
+      fcvt_w_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI32UConvertSatF64: {
+      fcvt_wu_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI64SConvertSatF32: {
+      fcvt_l_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI64UConvertSatF32: {
+      fcvt_lu_s(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_s(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI64SConvertSatF64: {
+      fcvt_l_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    case kExprI64UConvertSatF64: {
+      fcvt_lu_d(dst.gp(), src.fp(), RTZ);
+      Clear_if_nan_d(dst.gp(), src.fp());
+      return true;
+    }
+    default:
+      return false;
+  }
+}
+
+void LiftoffAssembler::emit_i32_signextend_i8(Register dst, Register src) {
+  slliw(dst, src, 32 - 8);
+  sraiw(dst, dst, 32 - 8);
+}
+
+void LiftoffAssembler::emit_i32_signextend_i16(Register dst, Register src) {
+  slliw(dst, src, 32 - 16);
+  sraiw(dst, dst, 32 - 16);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i8(LiftoffRegister dst,
+                                              LiftoffRegister src) {
+  slli(dst.gp(), src.gp(), 64 - 8);
+  srai(dst.gp(), dst.gp(), 64 - 8);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i16(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  slli(dst.gp(), src.gp(), 64 - 16);
+  srai(dst.gp(), dst.gp(), 64 - 16);
+}
+
+void LiftoffAssembler::emit_i64_signextend_i32(LiftoffRegister dst,
+                                               LiftoffRegister src) {
+  slli(dst.gp(), src.gp(), 64 - 32);
+  srai(dst.gp(), dst.gp(), 64 - 32);
+}
+
+void LiftoffAssembler::emit_jump(Label* label) {
+  TurboAssembler::Branch(label);
+}
+
+void LiftoffAssembler::emit_jump(Register target) {
+  TurboAssembler::Jump(target);
+}
+
+void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
+                                      Label* label, ValueKind kind,
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  if (rhs == no_reg) {
+    DCHECK(kind == kI32 || kind == kI64);
+    TurboAssembler::Branch(label, cond, lhs, Operand(zero_reg));
+  } else {
+    DCHECK((kind == kI32 || kind == kI64) ||
+           (is_reference(kind) &&
+            (liftoff_cond == kEqual || liftoff_cond == kUnequal)));
+    TurboAssembler::Branch(label, cond, lhs, Operand(rhs));
+  }
+}
+
+void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
+                                           Label* label, Register lhs,
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  TurboAssembler::Branch(label, cond, lhs, Operand(imm));
+}
+
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
+  Sub64(value, value, Operand(subtrahend));
+  TurboAssembler::Branch(result_negative, lt, value, Operand(zero_reg));
+}
+
+void LiftoffAssembler::emit_i32_eqz(Register dst, Register src) {
+  TurboAssembler::Sltu(dst, src, 1);
+}
+
+void LiftoffAssembler::emit_i32_set_cond(LiftoffCondition liftoff_cond,
+                                         Register dst, Register lhs,
+                                         Register rhs) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  TurboAssembler::CompareI(dst, lhs, Operand(rhs), cond);
+}
+
+void LiftoffAssembler::emit_i64_eqz(Register dst, LiftoffRegister src) {
+  TurboAssembler::Sltu(dst, src.gp(), 1);
+}
+
+void LiftoffAssembler::emit_i64_set_cond(LiftoffCondition liftoff_cond,
+                                         Register dst, LiftoffRegister lhs,
+                                         LiftoffRegister rhs) {
+  Condition cond = liftoff::ToCondition(liftoff_cond);
+  TurboAssembler::CompareI(dst, lhs.gp(), Operand(rhs.gp()), cond);
+}
+
+void LiftoffAssembler::IncrementSmi(LiftoffRegister dst, int offset) {
+  UseScratchRegisterScope temps(this);
+  if (COMPRESS_POINTERS_BOOL) {
+    DCHECK(SmiValuesAre31Bits());
+    Register scratch = temps.Acquire();
+    Lw(scratch, MemOperand(dst.gp(), offset));
+    Add32(scratch, scratch, Operand(Smi::FromInt(1)));
+    Sw(scratch, MemOperand(dst.gp(), offset));
+  } else {
+    Register scratch = temps.Acquire();
+    SmiUntag(scratch, MemOperand(dst.gp(), offset));
+    Add64(scratch, scratch, Operand(1));
+    SmiTag(scratch);
+    Sd(scratch, MemOperand(dst.gp(), offset));
+  }
+}
+
+void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
+                                     Register offset_reg, uintptr_t offset_imm,
+                                     LoadType type,
+                                     LoadTransformationKind transform,
+                                     uint32_t* protected_load_pc) {
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
+  VRegister dst_v = dst.fp().toV();
+  *protected_load_pc = pc_offset();
+
+  MachineType memtype = type.mem_type();
+  if (transform == LoadTransformationKind::kExtend) {
+    Ld(scratch, src_op);
+    if (memtype == MachineType::Int8()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E16, m1);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint8()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E16, m1);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Int16()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E32, m1);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint16()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      VU.set(kScratchReg, E32, m1);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      vsext_vf2(dst_v, kSimd128ScratchReg);
+    } else if (memtype == MachineType::Uint32()) {
+      VU.set(kScratchReg, E64, m1);
+      vmv_vx(kSimd128ScratchReg, scratch);
+      vzext_vf2(dst_v, kSimd128ScratchReg);
+    }
+  } else if (transform == LoadTransformationKind::kZeroExtend) {
+    vxor_vv(dst_v, dst_v, dst_v);
+    if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E32, m1);
+      Lwu(scratch, src_op);
+      vmv_sx(dst_v, scratch);
+    } else {
+      DCHECK_EQ(MachineType::Int64(), memtype);
+      VU.set(kScratchReg, E64, m1);
+      Ld(scratch, src_op);
+      vmv_sx(dst_v, scratch);
+    }
+  } else {
+    DCHECK_EQ(LoadTransformationKind::kSplat, transform);
+    if (memtype == MachineType::Int8()) {
+      VU.set(kScratchReg, E8, m1);
+      Lb(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int16()) {
+      VU.set(kScratchReg, E16, m1);
+      Lh(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int32()) {
+      VU.set(kScratchReg, E32, m1);
+      Lw(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    } else if (memtype == MachineType::Int64()) {
+      VU.set(kScratchReg, E64, m1);
+      Ld(scratch, src_op);
+      vmv_vx(dst_v, scratch);
+    }
+  }
+}
+
+void LiftoffAssembler::LoadLane(LiftoffRegister dst, LiftoffRegister src,
+                                Register addr, Register offset_reg,
+                                uintptr_t offset_imm, LoadType type,
+                                uint8_t laneidx, uint32_t* protected_load_pc) {
+  MemOperand src_op = liftoff::GetMemOp(this, addr, offset_reg, offset_imm);
+  MachineType mem_type = type.mem_type();
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.Acquire();
+  *protected_load_pc = pc_offset();
+  if (mem_type == MachineType::Int8()) {
+    Lbu(scratch, src_op);
+    VU.set(kScratchReg, E64, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    VU.set(kScratchReg, E8, m1);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int16()) {
+    Lhu(scratch, src_op);
+    VU.set(kScratchReg, E16, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int32()) {
+    Lwu(scratch, src_op);
+    VU.set(kScratchReg, E32, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else if (mem_type == MachineType::Int64()) {
+    Ld(scratch, src_op);
+    VU.set(kScratchReg, E64, m1);
+    li(kScratchReg, 0x1 << laneidx);
+    vmv_sx(v0, kScratchReg);
+    vmerge_vx(dst.fp().toV(), scratch, dst.fp().toV());
+  } else {
+    UNREACHABLE();
+  }
+}
+
+void LiftoffAssembler::StoreLane(Register dst, Register offset,
+                                 uintptr_t offset_imm, LiftoffRegister src,
+                                 StoreType type, uint8_t lane,
+                                 uint32_t* protected_store_pc) {
+  MemOperand dst_op = liftoff::GetMemOp(this, dst, offset, offset_imm);
+  if (protected_store_pc) *protected_store_pc = pc_offset();
+  MachineRepresentation rep = type.mem_rep();
+  if (rep == MachineRepresentation::kWord8) {
+    VU.set(kScratchReg, E8, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sb(kScratchReg, dst_op);
+  } else if (rep == MachineRepresentation::kWord16) {
+    VU.set(kScratchReg, E16, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sh(kScratchReg, dst_op);
+  } else if (rep == MachineRepresentation::kWord32) {
+    VU.set(kScratchReg, E32, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sw(kScratchReg, dst_op);
+  } else {
+    DCHECK_EQ(MachineRepresentation::kWord64, rep);
+    VU.set(kScratchReg, E64, m1);
+    vslidedown_vi(kSimd128ScratchReg, src.fp().toV(), lane);
+    vmv_xs(kScratchReg, kSimd128ScratchReg);
+    Sd(kScratchReg, dst_op);
+  }
+}
+
+void LiftoffAssembler::emit_i8x16_shuffle(LiftoffRegister dst,
+                                          LiftoffRegister lhs,
+                                          LiftoffRegister rhs,
+                                          const uint8_t shuffle[16],
+                                          bool is_swizzle) {
+  VRegister dst_v = dst.fp().toV();
+  VRegister lhs_v = lhs.fp().toV();
+  VRegister rhs_v = rhs.fp().toV();
+
+  uint64_t imm1 = *(reinterpret_cast<const uint64_t*>(shuffle));
+  uint64_t imm2 = *((reinterpret_cast<const uint64_t*>(shuffle)) + 1);
+  VU.set(kScratchReg, VSew::E64, Vlmul::m1);
+  li(kScratchReg, imm2);
+  vmv_sx(kSimd128ScratchReg2, kScratchReg);
+  vslideup_vi(kSimd128ScratchReg, kSimd128ScratchReg2, 1);
+  li(kScratchReg, imm1);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+
+  VU.set(kScratchReg, E8, m1);
+  VRegister temp =
+      GetUnusedRegister(kFpReg, LiftoffRegList{lhs, rhs}).fp().toV();
+  if (dst_v == lhs_v) {
+    vmv_vv(temp, lhs_v);
+    lhs_v = temp;
+  } else if (dst_v == rhs_v) {
+    vmv_vv(temp, rhs_v);
+    rhs_v = temp;
+  }
+  vrgather_vv(dst_v, lhs_v, kSimd128ScratchReg);
+  vadd_vi(kSimd128ScratchReg, kSimd128ScratchReg,
+          -16);  // The indices in range [16, 31] select the i - 16-th element
+                 // of rhs
+  vrgather_vv(kSimd128ScratchReg2, rhs_v, kSimd128ScratchReg);
+  vor_vv(dst_v, dst_v, kSimd128ScratchReg2);
+}
+
+void LiftoffAssembler::emit_f64x2_splat(LiftoffRegister dst,
+                                        LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  fmv_x_d(kScratchReg, src.fp());
+  vmv_vx(dst.fp().toV(), kScratchReg);
+}
+
+void LiftoffAssembler::emit_f64x2_min(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  VU.set(kScratchReg, E64, m1);
+  const int64_t kNaN = 0x7ff8000000000000L;
+  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  vand_vv(v0, v0, kSimd128ScratchReg);
+  li(kScratchReg, kNaN);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  vfmin_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_f64x2_max(LiftoffRegister dst, LiftoffRegister lhs,
+                                      LiftoffRegister rhs) {
+  VU.set(kScratchReg, E64, m1);
+  const int64_t kNaN = 0x7ff8000000000000L;
+  vmfeq_vv(v0, lhs.fp().toV(), lhs.fp().toV());
+  vmfeq_vv(kSimd128ScratchReg, rhs.fp().toV(), rhs.fp().toV());
+  vand_vv(v0, v0, kSimd128ScratchReg);
+  li(kScratchReg, kNaN);
+  vmv_vx(kSimd128ScratchReg, kScratchReg);
+  vfmax_vv(kSimd128ScratchReg, rhs.fp().toV(), lhs.fp().toV(), Mask);
+  vmv_vv(dst.fp().toV(), kSimd128ScratchReg);
+}
+
+void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_s(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x0006000400020000);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0007000500030001);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E16, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E16, mf2);
+  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+}
+
+void LiftoffAssembler::emit_i32x4_extadd_pairwise_i16x8_u(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x0006000400020000);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0007000500030001);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E16, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E16, mf2);
+  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+}
+
+void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_s(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x0E0C0A0806040200);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0F0D0B0907050301);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E8, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E8, mf2);
+  vwadd_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+}
+
+void LiftoffAssembler::emit_i16x8_extadd_pairwise_i8x16_u(LiftoffRegister dst,
+                                                          LiftoffRegister src) {
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x0E0C0A0806040200);
+  vmv_sx(kSimd128ScratchReg, kScratchReg);
+  li(kScratchReg, 0x0F0D0B0907050301);
+  vmv_sx(kSimd128ScratchReg3, kScratchReg);
+  VU.set(kScratchReg, E8, m1);
+  vrgather_vv(kSimd128ScratchReg2, src.fp().toV(), kSimd128ScratchReg);
+  vrgather_vv(kSimd128ScratchReg, src.fp().toV(), kSimd128ScratchReg3);
+  VU.set(kScratchReg, E8, mf2);
+  vwaddu_vv(dst.fp().toV(), kSimd128ScratchReg, kSimd128ScratchReg2);
+}
+
+void LiftoffAssembler::emit_f64x2_replace_lane(LiftoffRegister dst,
+                                               LiftoffRegister src1,
+                                               LiftoffRegister src2,
+                                               uint8_t imm_lane_idx) {
+  VU.set(kScratchReg, E64, m1);
+  li(kScratchReg, 0x1 << imm_lane_idx);
+  vmv_sx(v0, kScratchReg);
+  fmv_x_d(kScratchReg, src2.fp());
+  vmerge_vx(dst.fp().toV(), kScratchReg, src1.fp().toV());
+}
+
+void LiftoffAssembler::CallC(const ValueKindSig* sig,
+                             const LiftoffRegister* args,
+                             const LiftoffRegister* rets,
+                             ValueKind out_argument_kind, int stack_bytes,
+                             ExternalReference ext_ref) {
+  AddWord(sp, sp, Operand(-stack_bytes));
+
+  int arg_bytes = 0;
+  for (ValueKind param_kind : sig->parameters()) {
+    liftoff::Store(this, sp, arg_bytes, *args++, param_kind);
+    arg_bytes += value_kind_size(param_kind);
+  }
+  DCHECK_LE(arg_bytes, stack_bytes);
+
+  // Pass a pointer to the buffer with the arguments to the C function.
+  // On RISC-V, the first argument is passed in {a0}.
+  constexpr Register kFirstArgReg = a0;
+  mv(kFirstArgReg, sp);
+
+  // Now call the C function.
+  constexpr int kNumCCallArgs = 1;
+  PrepareCallCFunction(kNumCCallArgs, kScratchReg);
+  CallCFunction(ext_ref, kNumCCallArgs);
+
+  // Move return value to the right register.
+  const LiftoffRegister* next_result_reg = rets;
+  if (sig->return_count() > 0) {
+    DCHECK_EQ(1, sig->return_count());
+    constexpr Register kReturnReg = a0;
+    if (kReturnReg != next_result_reg->gp()) {
+      Move(*next_result_reg, LiftoffRegister(kReturnReg), sig->GetReturn(0));
+    }
+    ++next_result_reg;
+  }
+
+  // Load potential output value from the buffer on the stack.
+  if (out_argument_kind != kVoid) {
+    liftoff::Load(this, *next_result_reg, MemOperand(sp, 0), out_argument_kind);
+  }
+
+  AddWord(sp, sp, Operand(stack_bytes));
+}
+
+void LiftoffStackSlots::Construct(int param_slots) {
+  DCHECK_LT(0, slots_.size());
+  SortInPushOrder();
+  int last_stack_slot = param_slots;
+  for (auto& slot : slots_) {
+    const int stack_slot = slot.dst_slot_;
+    int stack_decrement = (last_stack_slot - stack_slot) * kSystemPointerSize;
+    DCHECK_LT(0, stack_decrement);
+    last_stack_slot = stack_slot;
+    const LiftoffAssembler::VarState& src = slot.src_;
+    switch (src.loc()) {
+      case LiftoffAssembler::VarState::kStack:
+        if (src.kind() != kS128) {
+          asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
+          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
+          asm_->push(kScratchReg);
+        } else {
+          asm_->AllocateStackSpace(stack_decrement - kSimd128Size);
+          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_ - 8));
+          asm_->push(kScratchReg);
+          asm_->Ld(kScratchReg, liftoff::GetStackSlot(slot.src_offset_));
+          asm_->push(kScratchReg);
+        }
+        break;
+      case LiftoffAssembler::VarState::kRegister: {
+        int pushed_bytes = SlotSizeInBytes(slot);
+        asm_->AllocateStackSpace(stack_decrement - pushed_bytes);
+        liftoff::push(asm_, src.reg(), src.kind());
+        break;
+      }
+      case LiftoffAssembler::VarState::kIntConst: {
+        asm_->AllocateStackSpace(stack_decrement - kSystemPointerSize);
+        asm_->li(kScratchReg, Operand(src.i32_const()));
+        asm_->push(kScratchReg);
+        break;
+      }
+    }
+  }
+}
+
+}  // namespace wasm
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_WASM_BASELINE_RISCV_LIFTOFF_ASSEMBLER_RISCV64_H_
diff --git a/src/wasm/jump-table-assembler.cc b/src/wasm/jump-table-assembler.cc
index 244be4da5bc..f82f47d4eaf 100644
--- a/src/wasm/jump-table-assembler.cc
+++ b/src/wasm/jump-table-assembler.cc
@@ -407,6 +407,46 @@ void JumpTableAssembler::NopBytes(int bytes) {
   }
 }
 
+#elif V8_TARGET_ARCH_RISCV32
+void JumpTableAssembler::EmitLazyCompileJumpSlot(uint32_t func_index,
+                                                 Address lazy_compile_target) {
+  int start = pc_offset();
+  li(kWasmCompileLazyFuncIndexRegister, func_index);  // max. 2 instr
+  // Jump produces max. 8 instructions (include constant pool and j)
+  Jump(lazy_compile_target, RelocInfo::NO_INFO);
+  int nop_bytes = start + kLazyCompileTableSlotSize - pc_offset();
+  DCHECK_EQ(nop_bytes % kInstrSize, 0);
+  for (int i = 0; i < nop_bytes; i += kInstrSize) nop();
+}
+
+bool JumpTableAssembler::EmitJumpSlot(Address target) {
+  PatchAndJump(target);
+  return true;
+}
+
+void JumpTableAssembler::EmitFarJumpSlot(Address target) {
+  UseScratchRegisterScope temp(this);
+  Register rd = temp.Acquire();
+  auipc(rd, 0);
+  lw(rd, rd, 4 * kInstrSize);
+  Jump(rd);
+  nop();
+  dq(target);
+}
+
+// static
+void JumpTableAssembler::PatchFarJumpSlot(Address slot, Address target) {
+  UNREACHABLE();
+}
+
+void JumpTableAssembler::NopBytes(int bytes) {
+  DCHECK_LE(0, bytes);
+  DCHECK_EQ(0, bytes % kInstrSize);
+  for (; bytes > 0; bytes -= kInstrSize) {
+    nop();
+  }
+}
+
 #else
 #error Unknown architecture.
 #endif
diff --git a/src/wasm/jump-table-assembler.h b/src/wasm/jump-table-assembler.h
index f5aeabee53a..c69cd9bc817 100644
--- a/src/wasm/jump-table-assembler.h
+++ b/src/wasm/jump-table-assembler.h
@@ -220,7 +220,7 @@ class V8_EXPORT_PRIVATE JumpTableAssembler : public MacroAssembler {
   static constexpr int kJumpTableSlotSize = 8 * kInstrSize;
   static constexpr int kFarJumpTableSlotSize = 6 * kInstrSize;
   static constexpr int kLazyCompileTableSlotSize = 8 * kInstrSize;
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
   static constexpr int kJumpTableLineSize = 6 * kInstrSize;
   static constexpr int kJumpTableSlotSize = 6 * kInstrSize;
   static constexpr int kFarJumpTableSlotSize = 6 * kInstrSize;
diff --git a/src/wasm/wasm-linkage.h b/src/wasm/wasm-linkage.h
index 45114330ee8..d15a3c13c68 100644
--- a/src/wasm/wasm-linkage.h
+++ b/src/wasm/wasm-linkage.h
@@ -115,7 +115,7 @@ constexpr Register kGpReturnRegisters[] = {r2, r3};
 constexpr DoubleRegister kFpParamRegisters[] = {d0, d2};
 constexpr DoubleRegister kFpReturnRegisters[] = {d0, d2};
 
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
 // ===========================================================================
 // == riscv64 =================================================================
 // ===========================================================================
diff --git a/src/wasm/wasm-serialization.cc b/src/wasm/wasm-serialization.cc
index 31a57758549..f959cebb4d4 100644
--- a/src/wasm/wasm-serialization.cc
+++ b/src/wasm/wasm-serialization.cc
@@ -382,7 +382,7 @@ void NativeModuleSerializer::WriteCode(const WasmCode* code, Writer* writer) {
   writer->WriteVector(code->protected_instructions_data());
 #if V8_TARGET_ARCH_MIPS || V8_TARGET_ARCH_MIPS64 || V8_TARGET_ARCH_ARM || \
     V8_TARGET_ARCH_PPC || V8_TARGET_ARCH_PPC64 || V8_TARGET_ARCH_S390X || \
-    V8_TARGET_ARCH_RISCV64
+    V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
   // On platforms that don't support misaligned word stores, copy to an aligned
   // buffer if necessary so we can relocate the serialized code.
   std::unique_ptr<byte[]> aligned_buffer;
diff --git a/test/cctest/BUILD.gn b/test/cctest/BUILD.gn
index 70edb2d05ff..fc1b4a48c2e 100644
--- a/test/cctest/BUILD.gn
+++ b/test/cctest/BUILD.gn
@@ -297,6 +297,13 @@ v8_source_set("cctest_sources") {
       "test-macro-assembler-riscv64.cc",
       "test-simple-riscv64.cc",
     ]
+  } else if (v8_current_cpu == "riscv32") {
+    sources += [  ### gcmole(arch:riscv32) ###
+      "test-assembler-riscv32.cc",
+      "test-helper-riscv32.cc",
+      "test-macro-assembler-riscv32.cc",
+      "test-simple-riscv32.cc",
+    ]
   } else if (v8_current_cpu == "loong64") {
     sources += [  ### gcmole(arch:loong64) ###
       "test-assembler-loong64.cc",
@@ -394,7 +401,8 @@ v8_source_set("cctest_sources") {
       v8_current_cpu == "s390" || v8_current_cpu == "s390x" ||
       v8_current_cpu == "mips" || v8_current_cpu == "mips64" ||
       v8_current_cpu == "mipsel" || v8_current_cpu == "mipsel64" ||
-      v8_current_cpu == "riscv64" || v8_current_cpu == "loong64") {
+      v8_current_cpu == "riscv64" || v8_current_cpu == "loong64" ||
+      v8_current_cpu == "riscv32") {
     # Disable fmadd/fmsub so that expected results match generated code in
     # RunFloat64MulAndFloat64Add1 and friends.
     if (!is_win) {
diff --git a/test/cctest/cctest-utils.h b/test/cctest/cctest-utils.h
index 6d3c027e6d9..abcd949dbe5 100644
--- a/test/cctest/cctest-utils.h
+++ b/test/cctest/cctest-utils.h
@@ -43,7 +43,7 @@ namespace internal {
 #elif defined(__PPC__) || defined(_ARCH_PPC)
 #define GET_STACK_POINTER_TO(sp_addr) \
   __asm__ __volatile__("stw 1, %0" : "=m"(sp_addr))
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_RISCV32
 #define GET_STACK_POINTER_TO(sp_addr) \
   __asm__ __volatile__("add %0, sp, x0" : "=r"(sp_addr))
 #elif V8_HOST_ARCH_LOONG64
diff --git a/test/cctest/cctest.status b/test/cctest/cctest.status
index dddf7727175..50a12ad2010 100644
--- a/test/cctest/cctest.status
+++ b/test/cctest/cctest.status
@@ -408,7 +408,7 @@
 }],  # 'mips_arch_variant == r6'
 
 ##############################################################################
-['arch == riscv64', {
+['arch == riscv64 or (arch == riscv32)', {
   # this test is unstable, sometimes fail when running w/ other tests.
   'test-cpu-profiler/CrossScriptInliningCallerLineNumbers2': [SKIP],
 
@@ -433,7 +433,7 @@
 }],  # 'arch == riscv64'
 
 ##############################################################################
-['arch == riscv64 and simulator_run', {
+['((arch == riscv64) or (arch == riscv32)) and simulator_run', {
 
   # Pass but take too long with the simulator.
   'test-api/Threading*': [PASS, SLOW],
@@ -442,6 +442,14 @@
 
 }],  # 'arch == riscv64 and simulator_run'
 
+
+##############################################################################
+
+['arch == riscv32', {
+  'test-gc/RunWasmLiftoff_CastsBenchmark': ['variant == stress_incremental_marking', SKIP],
+  'test-gc/RunWasmLiftoff_WasmArrayCopy': ['variant == stress_incremental_marking', SKIP],
+}], # 'arch == riscv32'
+
 ##############################################################################
 ['arch == loong64', {
   # The instruction scheduler is disabled on loong64.
diff --git a/test/cctest/compiler/test-run-machops.cc b/test/cctest/compiler/test-run-machops.cc
index 34ce37f2c4a..cc36fab0700 100644
--- a/test/cctest/compiler/test-run-machops.cc
+++ b/test/cctest/compiler/test-run-machops.cc
@@ -4447,7 +4447,7 @@ TEST(RunTruncateFloat32ToInt32) {
         CHECK_EQ(std::numeric_limits<int32_t>::min(), m.Call(i));
 #elif V8_TARGET_ARCH_ARM64 || V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_LOONG64
         CHECK_EQ(0, m.Call(i));
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_RISCV32
         CHECK_EQ(std::numeric_limits<int32_t>::max(), m.Call(i));
 #endif
       }
diff --git a/test/cctest/compiler/value-helper.h b/test/cctest/compiler/value-helper.h
index 3e7c3d3b8c7..a143aca72bd 100644
--- a/test/cctest/compiler/value-helper.h
+++ b/test/cctest/compiler/value-helper.h
@@ -231,19 +231,19 @@ class ValueHelper {
   }
 
   static constexpr uint32_t uint32_array[] = {
-      0x00000000, 0x00000001, 0xFFFFFFFF, 0x1B09788B, 0x04C5FCE8, 0xCC0DE5BF,
-      // This row is useful for testing lea optimizations on intel.
-      0x00000002, 0x00000003, 0x00000004, 0x00000005, 0x00000008, 0x00000009,
-      0x273A798E, 0x187937A3, 0xECE3AF83, 0x5495A16B, 0x0B668ECC, 0x11223344,
-      0x0000009E, 0x00000043, 0x0000AF73, 0x0000116B, 0x00658ECC, 0x002B3B4C,
-      0x88776655, 0x70000000, 0x07200000, 0x7FFFFFFF, 0x56123761, 0x7FFFFF00,
-      0x761C4761, 0x80000000, 0x88888888, 0xA0000000, 0xDDDDDDDD, 0xE0000000,
-      0xEEEEEEEE, 0xFFFFFFFD, 0xF0000000, 0x007FFFFF, 0x003FFFFF, 0x001FFFFF,
-      0x000FFFFF, 0x0007FFFF, 0x0003FFFF, 0x0001FFFF, 0x0000FFFF, 0x00007FFF,
-      0x00003FFF, 0x00001FFF, 0x00000FFF, 0x000007FF, 0x000003FF, 0x000001FF,
+      // 0x00000000, 0x00000001, 0xFFFFFFFF, 0x1B09788B, 0x04C5FCE8, 0xCC0DE5BF,
+      // // This row is useful for testing lea optimizations on intel.
+      // 0x00000002, 0x00000003, 0x00000004, 0x00000005, 0x00000008, 0x00000009,
+      // 0x273A798E, 0x187937A3, 0xECE3AF83, 0x5495A16B, 0x0B668ECC, 0x11223344,
+      // 0x0000009E, 0x00000043, 0x0000AF73, 0x0000116B, 0x00658ECC, 0x002B3B4C,
+      // 0x88776655, 0x70000000, 0x07200000, 0x7FFFFFFF, 0x56123761, 0x7FFFFF00,
+      // 0x761C4761, 0x80000000, 0x88888888, 0xA0000000, 0xDDDDDDDD, 0xE0000000,
+      // 0xEEEEEEEE, 0xFFFFFFFD, 0xF0000000, 0x007FFFFF, 0x003FFFFF, 0x001FFFFF,
+      // 0x000FFFFF, 0x0007FFFF, 0x0003FFFF, 0x0001FFFF, 0x0000FFFF, 0x00007FFF,
+      // 0x00003FFF, 0x00001FFF, 0x00000FFF, 0x000007FF, 0x000003FF, 0x000001FF,
       // Bit pattern of a quiet NaN and signaling NaN, with or without
       // additional payload.
-      0x7FC00000, 0x7F800000, 0x7FFFFFFF, 0x7F876543};
+      0x7F876543};
 
   static constexpr base::Vector<const uint32_t> uint32_vector() {
     return base::ArrayVector(uint32_array);
diff --git a/test/cctest/test-assembler-riscv32.cc b/test/cctest/test-assembler-riscv32.cc
new file mode 100644
index 00000000000..5c0b12e680b
--- /dev/null
+++ b/test/cctest/test-assembler-riscv32.cc
@@ -0,0 +1,2565 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+#include <math.h>
+
+#include <iostream>
+
+#include "src/base/utils/random-number-generator.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/diagnostics/disassembler.h"
+#include "src/execution/simulator.h"
+#include "src/heap/factory.h"
+#include "src/init/v8.h"
+#include "src/utils/utils.h"
+#include "test/cctest/cctest.h"
+#include "test/cctest/compiler/value-helper.h"
+#include "test/cctest/test-helper-riscv32.h"
+
+namespace v8 {
+namespace internal {
+//  Define these function prototypes to match JSEntryFunction in execution.cc
+using F1 = void*(int x, int p1, int p2, int p3, int p4);
+using F2 = void*(int x, int y, int p2, int p3, int p4);
+using F3 = void*(void* p, int p1, int p2, int p3, int p4);
+using F4 = void*(int64_t x, int64_t y, int64_t p2, int64_t p3, int64_t p4);
+using F5 = void*(void* p0, void* p1, int p2, int p3, int p4);
+
+#define MIN_VAL_IMM12 -(1 << 11)
+#define LARGE_INT_UNDER_32_BIT 0x12345678
+#define LARGE_UINT_UNDER_32_BIT (uint32_t)0xFDCB12341
+
+#define __ assm.
+
+#define UTEST_R2_FORM_WITH_RES(instr_name, type, rs1_val, rs2_val,     \
+                               expected_res)                           \
+  TEST(RISCV_UTEST_##instr_name) {                                     \
+    CcTest::InitializeVM();                                            \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(a0, a0, a1); }; \
+    auto res = GenAndRunTest<type, type>(rs1_val, rs2_val, fn);        \
+    CHECK_EQ(expected_res, res);                                       \
+  }
+
+#define UTEST_R1_FORM_WITH_RES(instr_name, in_type, out_type, rs1_val, \
+                               expected_res)                           \
+  TEST(RISCV_UTEST_##instr_name) {                                     \
+    CcTest::InitializeVM();                                            \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(a0, a0); };     \
+    auto res = GenAndRunTest<out_type, in_type>(rs1_val, fn);          \
+    CHECK_EQ(expected_res, res);                                       \
+  }
+
+#define UTEST_R1_FORM_WITH_RES_C(instr_name, in_type, out_type, rs1_val, \
+                                 expected_res)                           \
+  TEST(RISCV_UTEST_##instr_name) {                                       \
+    i::FLAG_riscv_c_extension = true;                                    \
+    CcTest::InitializeVM();                                              \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(a0, a0); };       \
+    auto res = GenAndRunTest<out_type, in_type>(rs1_val, fn);            \
+    CHECK_EQ(expected_res, res);                                         \
+  }
+
+#define UTEST_I_FORM_WITH_RES(instr_name, type, rs1_val, imm12, expected_res) \
+  TEST(RISCV_UTEST_##instr_name) {                                            \
+    CcTest::InitializeVM();                                                   \
+    CHECK_EQ(is_intn(imm12, 12), true);                                       \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(a0, a0, imm12); };     \
+    auto res = GenAndRunTest<type, type>(rs1_val, fn);                        \
+    CHECK_EQ(expected_res, res);                                              \
+  }
+
+#define UTEST_AMO_WITH_RES(instr_name, aq, rl, inout_type, rs1_val, rs2_val,   \
+                           expected_res)                                       \
+  TEST(RISCV_UTEST_##instr_name) {                                             \
+    CcTest::InitializeVM();                                                    \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(aq, rl, a1, a0, a2); }; \
+    auto res =                                                                 \
+        GenAndRunTestForAMO<inout_type, inout_type>(rs1_val, rs2_val, fn);     \
+    CHECK_EQ(expected_res, res);                                               \
+  }
+
+#define UTEST_LOAD_STORE(ldname, stname, value_type, value) \
+  TEST(RISCV_UTEST_##stname##ldname) {                      \
+    CcTest::InitializeVM();                                 \
+    auto fn = [](MacroAssembler& assm) {                    \
+      __ stname(a1, a0, 0);                                 \
+      __ ldname(a0, a0, 0);                                 \
+    };                                                      \
+    GenAndRunTestForLoadStore<value_type>(value, fn);       \
+  }
+
+// Since f.Call() is implemented as vararg calls and RISCV calling convention
+// passes all vararg arguments and returns (including floats) in GPRs, we have
+// to move from GPR to FPR and back in all floating point tests
+#define UTEST_LOAD_STORE_F(ldname, stname, value_type, store_value) \
+  TEST(RISCV_UTEST_##stname##ldname) {                              \
+    DCHECK(std::is_floating_point<value_type>::value);              \
+                                                                    \
+    CcTest::InitializeVM();                                         \
+    auto fn = [](MacroAssembler& assm) {                            \
+      __ stname(fa0, a0, 0);                                        \
+      __ ldname(fa0, a0, 0);                                        \
+    };                                                              \
+    GenAndRunTestForLoadStore<value_type>(store_value, fn);         \
+  }
+
+#define UTEST_LR_SC(ldname, stname, aq, rl, value_type, value) \
+  TEST(RISCV_UTEST_##stname##ldname) {                         \
+    CcTest::InitializeVM();                                    \
+    auto fn = [](MacroAssembler& assm) {                       \
+      __ ldname(aq, rl, a1, a0);                               \
+      __ stname(aq, rl, a0, a0, a1);                           \
+    };                                                         \
+    GenAndRunTestForLRSC<value_type>(value, fn);               \
+  }
+
+#define UTEST_R1_FORM_WITH_RES_F(instr_name, type, rs1_fval, expected_fres) \
+  TEST(RISCV_UTEST_##instr_name) {                                          \
+    DCHECK(std::is_floating_point<type>::value);                            \
+    CcTest::InitializeVM();                                                 \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(fa0, fa0); };        \
+    auto res = GenAndRunTest<type, type>(rs1_fval, fn);                     \
+    CHECK_EQ(expected_fres, res);                                           \
+  }
+
+#define UTEST_R2_FORM_WITH_RES_F(instr_name, type, rs1_fval, rs2_fval,    \
+                                 expected_fres)                           \
+  TEST(RISCV_UTEST_##instr_name) {                                        \
+    DCHECK(std::is_floating_point<type>::value);                          \
+    CcTest::InitializeVM();                                               \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(fa0, fa0, fa1); }; \
+    auto res = GenAndRunTest<type, type>(rs1_fval, rs2_fval, fn);         \
+    CHECK_EQ(expected_fres, res);                                         \
+  }
+
+#define UTEST_R3_FORM_WITH_RES_F(instr_name, type, rs1_fval, rs2_fval,         \
+                                 rs3_fval, expected_fres)                      \
+  TEST(RISCV_UTEST_##instr_name) {                                             \
+    DCHECK(std::is_floating_point<type>::value);                               \
+    CcTest::InitializeVM();                                                    \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(fa0, fa0, fa1, fa2); }; \
+    auto res = GenAndRunTest<type, type>(rs1_fval, rs2_fval, rs3_fval, fn);    \
+    CHECK_EQ(expected_fres, res);                                              \
+  }
+
+#define UTEST_COMPARE_WITH_RES_F(instr_name, input_type, rs1_fval, rs2_fval, \
+                                 expected_res)                               \
+  TEST(RISCV_UTEST_##instr_name) {                                           \
+    CcTest::InitializeVM();                                                  \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(a0, fa0, fa1); };     \
+    auto res = GenAndRunTest<int32_t, input_type>(rs1_fval, rs2_fval, fn);   \
+    CHECK_EQ(expected_res, res);                                             \
+  }
+
+#define UTEST_CONV_F_FROM_I(instr_name, input_type, output_type, rs1_val, \
+                            expected_fres)                                \
+  TEST(RISCV_UTEST_##instr_name) {                                        \
+    DCHECK(std::is_integral<input_type>::value&&                          \
+               std::is_floating_point<output_type>::value);               \
+                                                                          \
+    CcTest::InitializeVM();                                               \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(fa0, a0); };       \
+    auto res = GenAndRunTest<output_type, input_type>(rs1_val, fn);       \
+    CHECK_EQ(expected_fres, res);                                         \
+  }
+
+#define UTEST_CONV_I_FROM_F(instr_name, input_type, output_type,     \
+                            rounding_mode, rs1_fval, expected_res)   \
+  TEST(RISCV_UTEST_##instr_name) {                                   \
+    DCHECK(std::is_floating_point<input_type>::value&&               \
+               std::is_integral<output_type>::value);                \
+                                                                     \
+    CcTest::InitializeVM();                                          \
+    auto fn = [](MacroAssembler& assm) {                             \
+      __ instr_name(a0, fa0, rounding_mode);                         \
+    };                                                               \
+    auto res = GenAndRunTest<output_type, input_type>(rs1_fval, fn); \
+    CHECK_EQ(expected_res, res);                                     \
+  }                                                                  \
+                                                                     \
+  TEST(RISCV_UTEST_dyn_##instr_name) {                               \
+    DCHECK(std::is_floating_point<input_type>::value&&               \
+               std::is_integral<output_type>::value);                \
+                                                                     \
+    CcTest::InitializeVM();                                          \
+    auto fn = [](MacroAssembler& assm) {                             \
+      __ csrwi(csr_frm, rounding_mode);                              \
+      __ instr_name(a0, fa0, DYN);                                   \
+    };                                                               \
+    auto res = GenAndRunTest<output_type, input_type>(rs1_fval, fn); \
+    CHECK_EQ(expected_res, res);                                     \
+  }
+
+#define UTEST_CONV_F_FROM_F(instr_name, input_type, output_type, rs1_val, \
+                            expected_fres)                                \
+  TEST(RISCV_UTEST_##instr_name) {                                        \
+    CcTest::InitializeVM();                                               \
+    auto fn = [](MacroAssembler& assm) { __ instr_name(fa0, fa0); };      \
+    auto res = GenAndRunTest<output_type, input_type>(rs1_val, fn);       \
+    CHECK_EQ(expected_fres, res);                                         \
+  }
+
+#define UTEST_CSRI(csr_reg, csr_write_val, csr_set_clear_val)               \
+  TEST(RISCV_UTEST_CSRI_##csr_reg) {                                        \
+    CHECK_EQ(is_uint5(csr_write_val) && is_uint5(csr_set_clear_val), true); \
+                                                                            \
+    CcTest::InitializeVM();                                                 \
+    int64_t expected_res = 111;                                             \
+    Label exit, error;                                                      \
+    auto fn = [&exit, &error, expected_res](MacroAssembler& assm) {         \
+      /* test csr-write and csr-read */                                     \
+      __ csrwi(csr_reg, csr_write_val);                                     \
+      __ csrr(a0, csr_reg);                                                 \
+      __ RV_li(a1, csr_write_val);                                          \
+      __ bne(a0, a1, &error);                                               \
+      /* test csr_set */                                                    \
+      __ csrsi(csr_reg, csr_set_clear_val);                                 \
+      __ csrr(a0, csr_reg);                                                 \
+      __ RV_li(a1, (csr_write_val) | (csr_set_clear_val));                  \
+      __ bne(a0, a1, &error);                                               \
+      /* test csr_clear */                                                  \
+      __ csrci(csr_reg, csr_set_clear_val);                                 \
+      __ csrr(a0, csr_reg);                                                 \
+      __ RV_li(a1, (csr_write_val) & (~(csr_set_clear_val)));               \
+      __ bne(a0, a1, &error);                                               \
+      /* everyhing runs correctly, return 111 */                            \
+      __ RV_li(a0, expected_res);                                           \
+      __ j(&exit);                                                          \
+                                                                            \
+      __ bind(&error);                                                      \
+      /* got an error, return 666 */                                        \
+      __ RV_li(a0, 666);                                                    \
+                                                                            \
+      __ bind(&exit);                                                       \
+    };                                                                      \
+    auto res = GenAndRunTest(fn);                                           \
+    CHECK_EQ(expected_res, res);                                            \
+  }
+
+#define UTEST_CSR(csr_reg, csr_write_val, csr_set_clear_val)        \
+  TEST(RISCV_UTEST_CSR_##csr_reg) {                                 \
+    Label exit, error;                                              \
+    int64_t expected_res = 111;                                     \
+    auto fn = [&exit, &error, expected_res](MacroAssembler& assm) { \
+      /* test csr-write and csr-read */                             \
+      __ RV_li(t0, csr_write_val);                                  \
+      __ csrw(csr_reg, t0);                                         \
+      __ csrr(a0, csr_reg);                                         \
+      __ RV_li(a1, csr_write_val);                                  \
+      __ bne(a0, a1, &error);                                       \
+      /* test csr_set */                                            \
+      __ RV_li(t0, csr_set_clear_val);                              \
+      __ csrs(csr_reg, t0);                                         \
+      __ csrr(a0, csr_reg);                                         \
+      __ RV_li(a1, (csr_write_val) | (csr_set_clear_val));          \
+      __ bne(a0, a1, &error);                                       \
+      /* test csr_clear */                                          \
+      __ RV_li(t0, csr_set_clear_val);                              \
+      __ csrc(csr_reg, t0);                                         \
+      __ csrr(a0, csr_reg);                                         \
+      __ RV_li(a1, (csr_write_val) & (~(csr_set_clear_val)));       \
+      __ bne(a0, a1, &error);                                       \
+      /* everyhing runs correctly, return 111 */                    \
+      __ RV_li(a0, expected_res);                                   \
+      __ j(&exit);                                                  \
+                                                                    \
+      __ bind(&error);                                              \
+      /* got an error, return 666 */                                \
+      __ RV_li(a0, 666);                                            \
+                                                                    \
+      __ bind(&exit);                                               \
+    };                                                              \
+                                                                    \
+    auto res = GenAndRunTest(fn);                                   \
+    CHECK_EQ(expected_res, res);                                    \
+  }
+
+#define UTEST_R2_FORM_WITH_OP(instr_name, type, rs1_val, rs2_val, tested_op) \
+  UTEST_R2_FORM_WITH_RES(instr_name, type, rs1_val, rs2_val,                 \
+                         ((rs1_val)tested_op(rs2_val)))
+
+#define UTEST_I_FORM_WITH_OP(instr_name, type, rs1_val, imm12, tested_op) \
+  UTEST_I_FORM_WITH_RES(instr_name, type, rs1_val, imm12,                 \
+                        ((rs1_val)tested_op(imm12)))
+
+#define UTEST_R2_FORM_WITH_OP_F(instr_name, type, rs1_fval, rs2_fval, \
+                                tested_op)                            \
+  UTEST_R2_FORM_WITH_RES_F(instr_name, type, rs1_fval, rs2_fval,      \
+                           ((rs1_fval)tested_op(rs2_fval)))
+
+#define UTEST_COMPARE_WITH_OP_F(instr_name, input_type, rs1_fval, rs2_fval, \
+                                tested_op)                                  \
+  UTEST_COMPARE_WITH_RES_F(instr_name, input_type, rs1_fval, rs2_fval,      \
+                           ((rs1_fval)tested_op(rs2_fval)))
+
+// -- test load-store --
+// due to sign-extension of lw
+// instruction, value-to-stored must have
+// its 32th least significant bit be 0
+UTEST_LOAD_STORE(lw, sw, int32_t, 0x456AF894)
+// due to sign-extension of lh
+// instruction, value-to-stored must have
+// its 16th least significant bit be 0
+UTEST_LOAD_STORE(lh, sh, int32_t, 0x7894)
+// set the 16th least significant bit of
+// value-to-store to 1 to test
+// zero-extension by lhu
+UTEST_LOAD_STORE(lhu, sh, uint32_t, 0xF894)
+// due to sign-extension of lb
+// instruction, value-to-stored must have
+// its 8th least significant bit be 0
+UTEST_LOAD_STORE(lb, sb, int32_t, 0x54)
+// set the 8th least significant bit of
+// value-to-store to 1 to test
+// zero-extension by lbu
+UTEST_LOAD_STORE(lbu, sb, uint32_t, 0x94)
+
+// -- arithmetic w/ immediate --
+UTEST_I_FORM_WITH_OP(addi, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, +)
+UTEST_I_FORM_WITH_OP(slti, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, <)
+UTEST_I_FORM_WITH_OP(sltiu, uint32_t, LARGE_UINT_UNDER_32_BIT, 0x4FB, <)
+UTEST_I_FORM_WITH_OP(xori, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, ^)
+UTEST_I_FORM_WITH_OP(ori, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, |)
+UTEST_I_FORM_WITH_OP(andi, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, &)
+UTEST_I_FORM_WITH_OP(slli, uint32_t, 0x12345678U, 17, <<)
+UTEST_I_FORM_WITH_OP(srli, uint32_t, 0x82340000U, 17, >>)
+UTEST_I_FORM_WITH_OP(srai, int32_t, -0x12340000, 17, >>)
+
+// -- arithmetic --
+UTEST_R2_FORM_WITH_OP(add, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, +)
+UTEST_R2_FORM_WITH_OP(sub, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, -)
+UTEST_R2_FORM_WITH_OP(slt, int32_t, MIN_VAL_IMM12, LARGE_INT_UNDER_32_BIT, <)
+UTEST_R2_FORM_WITH_OP(sltu, uint32_t, 0x4FB, LARGE_UINT_UNDER_32_BIT, <)
+UTEST_R2_FORM_WITH_OP(xor_, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, ^)
+UTEST_R2_FORM_WITH_OP(or_, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, |)
+UTEST_R2_FORM_WITH_OP(and_, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, &)
+UTEST_R2_FORM_WITH_OP(sll, uint32_t, 0x12345678U, 17, <<)
+UTEST_R2_FORM_WITH_OP(srl, uint32_t, 0x82340000U, 17, >>)
+UTEST_R2_FORM_WITH_OP(sra, int32_t, -0x12340000, 17, >>)
+
+// -- Memory fences --
+// void fence(uint8_t pred, uint8_t succ);
+// void fence_tso();
+
+// -- Environment call / break --
+// void ecall();
+// void ebreak();
+// void unimp();
+
+// -- CSR --
+UTEST_CSRI(csr_frm, DYN, RUP)
+UTEST_CSRI(csr_fflags, kInexact | kInvalidOperation, kInvalidOperation)
+UTEST_CSRI(csr_fcsr, kDivideByZero | kOverflow, kUnderflow)
+UTEST_CSR(csr_frm, DYN, RUP)
+UTEST_CSR(csr_fflags, kInexact | kInvalidOperation, kInvalidOperation)
+UTEST_CSR(csr_fcsr, kDivideByZero | kOverflow | (RDN << kFcsrFrmShift),
+          kUnderflow | (RNE << kFcsrFrmShift))
+
+// -- RV32M Standard Extension --
+UTEST_R2_FORM_WITH_OP(mul, int32_t, 0x045001, MIN_VAL_IMM12, *)
+UTEST_R2_FORM_WITH_RES(mulh, int32_t, 0x12344321, -0x56171234,
+                       static_cast<int32_t>((0x12344321LL * -0x56171234LL) >>
+                                            32))
+UTEST_R2_FORM_WITH_RES(mulhu, int32_t, 0x12345678, 0xF8967021,
+                       static_cast<int32_t>((0x12345678ULL * 0xF8967021ULL) >>
+                                            32))
+UTEST_R2_FORM_WITH_RES(mulhsu, int32_t, -0x12345678, 0xF2345678,
+                       static_cast<int32_t>((-0x12345678LL * 0xF2345678ULL) >>
+                                            32))
+UTEST_R2_FORM_WITH_OP(div, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, /)
+UTEST_R2_FORM_WITH_OP(divu, uint32_t, LARGE_UINT_UNDER_32_BIT, 100, /)
+UTEST_R2_FORM_WITH_OP(rem, int32_t, LARGE_INT_UNDER_32_BIT, MIN_VAL_IMM12, %)
+UTEST_R2_FORM_WITH_OP(remu, uint32_t, LARGE_UINT_UNDER_32_BIT, 100, %)
+
+// -- RV32A Standard Extension --
+UTEST_LR_SC(lr_w, sc_w, false, false, int32_t, 0xFBB1A75C)
+UTEST_AMO_WITH_RES(amoswap_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   (uint32_t)0xA75C0A9C)
+UTEST_AMO_WITH_RES(amoadd_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   (uint32_t)0xFBB1A75C + (uint32_t)0xA75C0A9C)
+UTEST_AMO_WITH_RES(amoxor_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   (uint32_t)0xFBB1A75C ^ (uint32_t)0xA75C0A9C)
+UTEST_AMO_WITH_RES(amoand_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   (uint32_t)0xFBB1A75C & (uint32_t)0xA75C0A9C)
+UTEST_AMO_WITH_RES(amoor_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   (uint32_t)0xFBB1A75C | (uint32_t)0xA75C0A9C)
+UTEST_AMO_WITH_RES(amomin_w, false, false, int32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   std::min((int32_t)0xFBB1A75C, (int32_t)0xA75C0A9C))
+UTEST_AMO_WITH_RES(amomax_w, false, false, int32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   std::max((int32_t)0xFBB1A75C, (int32_t)0xA75C0A9C))
+UTEST_AMO_WITH_RES(amominu_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   std::min((uint32_t)0xFBB1A75C, (uint32_t)0xA75C0A9C))
+UTEST_AMO_WITH_RES(amomaxu_w, false, false, uint32_t, 0xFBB1A75C, 0xA75C0A9C,
+                   std::max((uint32_t)0xFBB1A75C, (uint32_t)0xA75C0A9C))
+
+// -- RV32F Standard Extension --
+UTEST_LOAD_STORE_F(flw, fsw, float, -2345.678f)
+UTEST_R2_FORM_WITH_OP_F(fadd_s, float, -1012.01f, 3456.13f, +)
+UTEST_R2_FORM_WITH_OP_F(fsub_s, float, -1012.01f, 3456.13f, -)
+UTEST_R2_FORM_WITH_OP_F(fmul_s, float, -10.01f, 56.13f, *)
+UTEST_R2_FORM_WITH_OP_F(fdiv_s, float, -10.01f, 34.13f, /)
+UTEST_R1_FORM_WITH_RES_F(fsqrt_s, float, 34.13f, sqrtf(34.13f))
+UTEST_R2_FORM_WITH_RES_F(fmin_s, float, -1012.0f, 3456.13f, -1012.0f)
+UTEST_R2_FORM_WITH_RES_F(fmax_s, float, -1012.0f, 3456.13f, 3456.13f)
+UTEST_R3_FORM_WITH_RES_F(fmadd_s, float, 67.56f, -1012.01f, 3456.13f,
+                         std::fma(67.56f, -1012.01f, 3456.13f))
+UTEST_R3_FORM_WITH_RES_F(fmsub_s, float, 67.56f, -1012.01f, 3456.13f,
+                         std::fma(67.56f, -1012.01f, -3456.13f))
+UTEST_R3_FORM_WITH_RES_F(fnmsub_s, float, 67.56f, -1012.01f, 3456.13f,
+                         -std::fma(67.56f, -1012.01f, -3456.13f))
+UTEST_R3_FORM_WITH_RES_F(fnmadd_s, float, 67.56f, -1012.01f, 3456.13f,
+                         -std::fma(67.56f, -1012.01f, 3456.13f))
+UTEST_COMPARE_WITH_OP_F(feq_s, float, -3456.56, -3456.56, ==)
+UTEST_COMPARE_WITH_OP_F(flt_s, float, -3456.56, -3456.56, <)
+UTEST_COMPARE_WITH_OP_F(fle_s, float, -3456.56, -3456.56, <=)
+UTEST_CONV_F_FROM_I(fcvt_s_w, int32_t, float, -100, (float)(-100))
+UTEST_CONV_F_FROM_I(fcvt_s_wu, uint32_t, float,
+                    std::numeric_limits<uint32_t>::max(),
+                    (float)(std::numeric_limits<uint32_t>::max()))
+UTEST_CONV_I_FROM_F(fcvt_w_s, float, int32_t, RMM, -100.5f, -101)
+UTEST_CONV_I_FROM_F(fcvt_wu_s, float, uint32_t, RUP, 256.1f, 257)
+UTEST_R2_FORM_WITH_RES_F(fsgnj_s, float, -100.0f, 200.0f, 100.0f)
+UTEST_R2_FORM_WITH_RES_F(fsgnjn_s, float, 100.0f, 200.0f, -100.0f)
+UTEST_R2_FORM_WITH_RES_F(fsgnjx_s, float, -100.0f, 200.0f, -100.0f)
+
+// -- RV32D Standard Extension --
+// TODO(rv32 simulator don't support double args)
+// UTEST_CONV_F_FROM_F(fcvt_s_d, double, float, 100.0, 100.0f)
+// UTEST_CONV_F_FROM_F(fcvt_d_s, float, double, 100.0f, 100.0)
+
+// UTEST_R2_FORM_WITH_RES_F(fsgnj_d, double, -100.0, 200.0, 100.0)
+// UTEST_R2_FORM_WITH_RES_F(fsgnjn_d, double, 100.0, 200.0, -100.0)
+// UTEST_R2_FORM_WITH_RES_F(fsgnjx_d, double, -100.0, 200.0, -100.0)
+
+// -- RVC Standard Extension --
+UTEST_R1_FORM_WITH_RES_C(c_mv, int32_t, int32_t, 0x0f5600ab, 0x0f5600ab)
+
+// -- Assembler Pseudo Instructions --
+UTEST_R1_FORM_WITH_RES(mv, int32_t, int32_t, 0x0f5600ab, 0x0f5600ab)
+UTEST_R1_FORM_WITH_RES(not_, int32_t, int32_t, 0, ~0)
+UTEST_R1_FORM_WITH_RES(neg, int32_t, int32_t, 0xab123400, -(0xab123400))
+UTEST_R1_FORM_WITH_RES(seqz, int32_t, int32_t, 20, 20 == 0)
+UTEST_R1_FORM_WITH_RES(snez, int32_t, int32_t, 20, 20 != 0)
+UTEST_R1_FORM_WITH_RES(sltz, int32_t, int32_t, -20, -20 < 0)
+UTEST_R1_FORM_WITH_RES(sgtz, int32_t, int32_t, -20, -20 > 0)
+
+UTEST_R1_FORM_WITH_RES_F(fmv_s, float, -23.5f, -23.5f)
+UTEST_R1_FORM_WITH_RES_F(fabs_s, float, -23.5f, 23.5f)
+UTEST_R1_FORM_WITH_RES_F(fneg_s, float, 23.5f, -23.5f)
+// TODO(rv32 simulator don't support double args)
+// UTEST_R1_FORM_WITH_RES_F(fmv_d, double, -23.5, -23.5)
+// UTEST_R1_FORM_WITH_RES_F(fabs_d, double, -23.5, 23.5)
+// UTEST_R1_FORM_WITH_RES_F(fneg_d, double, 23.5, -23.5)
+
+// Test LI
+TEST(RISCV0) {
+  CcTest::InitializeVM();
+
+  FOR_INT32_INPUTS(i) {
+    auto fn = [i](MacroAssembler& assm) { __ RV_li(a0, i); };
+    auto res = GenAndRunTest(fn);
+    CHECK_EQ(i, res);
+  }
+}
+
+TEST(RISCV1) {
+  CcTest::InitializeVM();
+
+  Label L, C;
+  auto fn = [&L, &C](MacroAssembler& assm) {
+    __ mv(a1, a0);
+    __ RV_li(a0, 0l);
+    __ j(&C);
+
+    __ bind(&L);
+    __ add(a0, a0, a1);
+    __ addi(a1, a1, -1);
+
+    __ bind(&C);
+    __ xori(a2, a1, 0);
+    __ bnez(a2, &L);
+  };
+
+  int32_t input = 50;
+  int32_t expected_res = 1275L;
+  auto res = GenAndRunTest<int32_t>(input, fn);
+  CHECK_EQ(expected_res, res);
+}
+
+TEST(RISCV2) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  Label exit, error;
+  int64_t expected_res = 0x31415926L;
+
+  // ----- Test all instructions.
+
+  // Test lui, ori, and addi, used in the
+  // li pseudo-instruction. This way we
+  // can then safely load registers with
+  // chosen values.
+  auto fn = [&exit, &error, expected_res](MacroAssembler& assm) {
+    __ ori(a4, zero_reg, 0);
+    __ lui(a4, 0x12345);
+    __ ori(a4, a4, 0);
+    __ ori(a4, a4, 0xF0F);
+    __ ori(a4, a4, 0x0F0);
+    __ addi(a5, a4, 1);
+    __ addi(a6, a5, -0x10);
+
+    // Load values in temporary registers.
+    __ RV_li(a4, 0x00000004);
+    __ RV_li(a5, 0x00001234);
+    __ RV_li(a6, 0x12345678);
+    __ RV_li(a7, 0x7FFFFFFF);
+    __ RV_li(t0, 0xFFFFFFFC);
+    __ RV_li(t1, 0xFFFFEDCC);
+    __ RV_li(t2, 0xEDCBA988);
+    __ RV_li(t3, 0x80000000);
+
+    __ srli(t0, a6, 8);   // 0x00123456
+    __ slli(t0, t0, 11);  // 0x91A2B000
+    __ srai(t0, t0, 3);   // 0xF2345600
+    __ sra(t0, t0, a4);   // 0xFF234560
+    __ sll(t0, t0, a4);   // 0xF2345600
+    __ srl(t0, t0, a4);   // 0x0F234560
+    __ RV_li(t5, 0x0F234560);
+    __ bne(t0, t5, &error);
+
+    __ add(t0, a4, a5);  // 0x00001238
+    __ sub(t0, t0, a4);  // 0x00001234
+    __ RV_li(t5, 0x00001234);
+    __ bne(t0, t5, &error);
+    __ add(a1, a7,
+           a4);  // 32bit addu result is sign-extended into 64bit reg.
+    __ RV_li(t5, 0x80000003);
+    __ bne(a1, t5, &error);
+    __ sub(a1, t3, a4);  // 0x7FFFFFFC
+    __ RV_li(t5, 0x7FFFFFFC);
+    __ bne(a1, t5, &error);
+
+    __ and_(t0, a5, a6);  // 0x00001230
+    __ or_(t0, t0, a5);   // 0x00001234
+    __ xor_(t0, t0, a6);  // 0x1234444C
+    __ or_(t0, t0, a6);
+    __ not_(t0, t0);  // 0xEDCBA983
+    __ RV_li(t5, 0xEDCBA983);
+    __ bne(t0, t5, &error);
+
+    // Test slli, slt and sltu.
+    __ slli(a7, a7, 31);  // 0x80000000
+    __ addi(t3, t3, 1);   // 0x80000001
+    __ slli(t3, t3, 30);  // 0x40000000
+    __ RV_li(t5, 1);
+
+    __ slt(t0, a7, t3);
+    __ bne(t0, t5, &error);
+    __ sltu(t0, a7, t3);
+    __ bne(t0, zero_reg, &error);
+
+    __ RV_li(t0, 0x7421);    // 0x00007421
+    __ addi(t0, t0, -0x1);   // 0x00007420
+    __ addi(t0, t0, -0x20);  // 0x00007400
+    __ RV_li(t5, 0x00007400);
+    __ bne(t0, t5, &error);
+    __ addi(a1, a7, 0x0);  // 0x80000000 -
+    __ RV_li(t5, 0x80000000);
+    __ bne(a1, t5, &error);
+
+    // Everything was correctly executed.
+    // Load the expected result.
+    __ RV_li(a0, expected_res);
+    __ j(&exit);
+
+    __ bind(&error);
+    // Got an error. Return a wrong result.
+    __ RV_li(a0, 666);
+
+    __ bind(&exit);
+  };
+  auto res = GenAndRunTest(fn);
+  CHECK_EQ(expected_res, res);
+}
+
+TEST(RISCV3) {
+  // Test floating point instructions.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    double a;
+    double b;
+    double c;
+    double d;
+    double e;
+    double f;
+    double g;
+    double h;
+    double i;
+    float fa;
+    float fb;
+    float fc;
+    float fd;
+    float fe;
+    float ff;
+    float fg;
+  } t;
+
+  // Create a function that accepts &t and loads, manipulates, and stores
+  // the doubles t.a ... t.f.
+
+  // Double precision floating point instructions.
+  auto fn = [](MacroAssembler& assm) {
+    __ fld(ft0, a0, offsetof(T, a));
+    __ fld(ft1, a0, offsetof(T, b));
+    __ fadd_d(ft2, ft0, ft1);
+    __ fsd(ft2, a0, offsetof(T, c));  // c = a + b.
+
+    __ fmv_d(ft3, ft2);   // c
+    __ fneg_d(fa0, ft1);  // -b
+    __ fsub_d(ft3, ft3, fa0);
+    __ fsd(ft3, a0, offsetof(T, d));  // d = c - (-b).
+
+    __ fsd(ft0, a0, offsetof(T, b));  // b = a.
+
+    __ RV_li(a4, 120);
+    __ fcvt_d_w(ft5, a4);
+    __ fmul_d(ft3, ft3, ft5);
+    __ fsd(ft3, a0, offsetof(T, e));  // e = d * 120 = 1.8066e16.
+
+    __ fdiv_d(ft4, ft3, ft0);
+    __ fsd(ft4, a0, offsetof(T, f));  // f = e / a = 120.44.
+
+    __ fsqrt_d(ft5, ft4);
+    __ fsd(ft5, a0, offsetof(T, g));
+    // g = sqrt(f) = 10.97451593465515908537
+
+    __ fld(ft0, a0, offsetof(T, h));
+    __ fld(ft1, a0, offsetof(T, i));
+    __ fmadd_d(ft5, ft1, ft0, ft1);
+    __ fsd(ft5, a0, offsetof(T, h));
+
+    // // Single precision floating point instructions.
+    __ flw(ft0, a0, offsetof(T, fa));
+    __ flw(ft1, a0, offsetof(T, fb));
+    __ fadd_s(ft2, ft0, ft1);
+    __ fsw(ft2, a0, offsetof(T, fc));  // fc = fa + fb.
+
+    __ fneg_s(ft3, ft1);  // -fb
+    __ fsub_s(ft3, ft2, ft3);
+    __ fsw(ft3, a0, offsetof(T, fd));  // fd = fc - (-fb).
+
+    __ fsw(ft0, a0, offsetof(T, fb));  // fb = fa.
+
+    __ RV_li(t0, 120);
+    __ fcvt_s_w(ft5, t0);  // ft5 = 120.0.
+    __ fmul_s(ft3, ft3, ft5);
+    __ fsw(ft3, a0, offsetof(T, fe));  // fe = fd * 120
+
+    __ fdiv_s(ft4, ft3, ft0);
+    __ fsw(ft4, a0, offsetof(T, ff));  // ff = fe / fa
+
+    __ fsqrt_s(ft5, ft4);
+    __ fsw(ft5, a0, offsetof(T, fg));
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  // Double test values.
+  t.a = 1.5e14;
+  t.b = 2.75e11;
+  t.c = 0.0;
+  t.d = 0.0;
+  t.e = 0.0;
+  t.f = 0.0;
+  t.h = 1.5;
+  t.i = 2.75;
+  // Single test values.
+  t.fa = 1.5e6;
+  t.fb = 2.75e4;
+  t.fc = 0.0;
+  t.fd = 0.0;
+  t.fe = 0.0;
+  t.ff = 0.0;
+  f.Call(&t, 0, 0, 0, 0);
+  // Expected double results.
+  CHECK_EQ(1.5e14, t.a);
+  CHECK_EQ(1.5e14, t.b);
+  CHECK_EQ(1.50275e14, t.c);
+  CHECK_EQ(1.50550e14, t.d);
+  CHECK_EQ(1.8066e16, t.e);
+  CHECK_EQ(120.44, t.f);
+  CHECK_EQ(10.97451593465515908537, t.g);
+  CHECK_EQ(6.875, t.h);
+  // Expected single results.
+  CHECK_EQ(1.5e6, t.fa);
+  CHECK_EQ(1.5e6, t.fb);
+  CHECK_EQ(1.5275e06, t.fc);
+  CHECK_EQ(1.5550e06, t.fd);
+  CHECK_EQ(1.866e08, t.fe);
+  CHECK_EQ(124.40000152587890625, t.ff);
+  CHECK_EQ(11.1534748077392578125, t.fg);
+}
+TEST(RISCV4) {
+  // Test moves between floating point and
+  // integer registers.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    float a;
+    float b;
+    float c;
+    float d;
+    int32_t e;
+  } t;
+
+  auto fn = [](MacroAssembler& assm) {
+    __ flw(ft0, a0, offsetof(T, a));
+    __ flw(fa1, a0, offsetof(T, b));
+
+    // Swap ft0 and fa1, by using 2 integer registers, a4-a5,
+    __ fmv_x_w(a4, ft0);
+    __ fmv_x_w(a5, fa1);
+
+    __ fmv_w_x(fa1, a4);
+    __ fmv_w_x(ft0, a5);
+
+    // Store the swapped ft0 and fa1 back to memory.
+    __ fsw(ft0, a0, offsetof(T, a));
+    __ fsw(fa1, a0, offsetof(T, c));
+
+    __ flw(ft0, a0, offsetof(T, d));
+    __ fmv_x_w(a4, ft0);
+
+    __ sw(a4, a0, offsetof(T, e));
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  t.a = 1.5e22;
+  t.b = 2.75e11;
+  t.c = 17.17;
+  t.d = -2.75e11;
+  f.Call(&t, 0, 0, 0, 0);
+
+  CHECK_EQ(2.75e11f, t.a);
+  CHECK_EQ(2.75e11f, t.b);
+  CHECK_EQ(1.5e22f, t.c);
+  CHECK_EQ(static_cast<int32_t>(0xD2800E8E), t.e);
+}
+
+TEST(RISCV5) {
+  // Test conversions between doubles and
+  // integers.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    double a;
+    double b;
+    int i;
+    int j;
+  } t;
+
+  auto fn = [](MacroAssembler& assm) {
+    // Load all structure elements to registers.
+    __ fld(ft0, a0, offsetof(T, a));
+    __ fld(ft1, a0, offsetof(T, b));
+    __ lw(a4, a0, offsetof(T, i));
+    __ lw(a5, a0, offsetof(T, j));
+
+    // Convert double in ft0 to int in element i.
+    __ fcvt_w_d(a6, ft0);
+    __ sw(a6, a0, offsetof(T, i));
+
+    // Convert double in ft1 to int in element j.
+    __ fcvt_w_d(a7, ft1);
+    __ sw(a7, a0, offsetof(T, j));
+
+    // Convert int in original i (a4) to double in a.
+    __ fcvt_d_w(fa0, a4);
+    __ fsd(fa0, a0, offsetof(T, a));
+
+    // Convert int in original j (a5) to double in b.
+    __ fcvt_d_w(fa1, a5);
+    __ fsd(fa1, a0, offsetof(T, b));
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  t.a = 1.5e4;
+  t.b = 2.75e4;
+  t.i = 24000;
+  t.j = -100000;
+  f.Call(&t, 0, 0, 0, 0);
+
+  CHECK_EQ(24000, t.a);
+  CHECK_EQ(-100000.0, t.b);
+  CHECK_EQ(15000, t.i);
+  CHECK_EQ(27500, t.j);
+}
+
+TEST(RISCV6) {
+  // Test simple memory loads and stores.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    uint32_t ui;
+    int32_t si;
+    int32_t r1;
+    int32_t r2;
+    int32_t r3;
+    int32_t r4;
+    int32_t r5;
+    int32_t r6;
+  } t;
+
+  auto fn = [](MacroAssembler& assm) {
+    // Basic word load/store.
+    __ lw(a4, a0, offsetof(T, ui));
+    __ sw(a4, a0, offsetof(T, r1));
+
+    // lh with positive data.
+    __ lh(a5, a0, offsetof(T, ui));
+    __ sw(a5, a0, offsetof(T, r2));
+
+    // lh with negative data.
+    __ lh(a6, a0, offsetof(T, si));
+    __ sw(a6, a0, offsetof(T, r3));
+
+    // lhu with negative data.
+    __ lhu(a7, a0, offsetof(T, si));
+    __ sw(a7, a0, offsetof(T, r4));
+
+    // Lb with negative data.
+    __ lb(t0, a0, offsetof(T, si));
+    __ sw(t0, a0, offsetof(T, r5));
+
+    // sh writes only 1/2 of word.
+    __ RV_li(t1, 0x33333333);
+    __ sw(t1, a0, offsetof(T, r6));
+    __ lhu(t1, a0, offsetof(T, si));
+    __ sh(t1, a0, offsetof(T, r6));
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  t.ui = 0x11223344;
+  t.si = 0x99AABBCC;
+  f.Call(&t, 0, 0, 0, 0);
+
+  CHECK_EQ(static_cast<int32_t>(0x11223344), t.r1);
+  if (kArchEndian == kLittle) {
+    CHECK_EQ(static_cast<int32_t>(0x3344), t.r2);
+    CHECK_EQ(static_cast<int32_t>(0xFFFFBBCC), t.r3);
+    CHECK_EQ(static_cast<int32_t>(0x0000BBCC), t.r4);
+    CHECK_EQ(static_cast<int32_t>(0xFFFFFFCC), t.r5);
+    CHECK_EQ(static_cast<int32_t>(0x3333BBCC), t.r6);
+  } else {
+    CHECK_EQ(static_cast<int32_t>(0x1122), t.r2);
+    CHECK_EQ(static_cast<int32_t>(0xFFFF99AA), t.r3);
+    CHECK_EQ(static_cast<int32_t>(0x000099AA), t.r4);
+    CHECK_EQ(static_cast<int32_t>(0xFFFFFF99), t.r5);
+    CHECK_EQ(static_cast<int32_t>(0x99AA3333), t.r6);
+  }
+}
+
+// pair.first is the F_TYPE input to test, pair.second is I_TYPE expected result
+template <typename T>
+static const std::vector<std::pair<T, uint32_t>> fclass_test_values() {
+  static const std::pair<T, uint32_t> kValues[] = {
+      std::make_pair(-std::numeric_limits<T>::infinity(), kNegativeInfinity),
+      std::make_pair(-10240.56, kNegativeNormalNumber),
+      std::make_pair(-(std::numeric_limits<T>::min() / 2),
+                     kNegativeSubnormalNumber),
+      std::make_pair(-0.0, kNegativeZero),
+      std::make_pair(+0.0, kPositiveZero),
+      std::make_pair((std::numeric_limits<T>::min() / 2),
+                     kPositiveSubnormalNumber),
+      std::make_pair(10240.56, kPositiveNormalNumber),
+      std::make_pair(std::numeric_limits<T>::infinity(), kPositiveInfinity),
+#ifndef USE_SIMULATOR
+      std::make_pair(std::numeric_limits<T>::signaling_NaN(), kSignalingNaN),
+#endif
+      std::make_pair(std::numeric_limits<T>::quiet_NaN(), kQuietNaN)};
+  return std::vector<std::pair<T, uint32_t>>(&kValues[0],
+                                             &kValues[arraysize(kValues)]);
+}
+
+TEST(FCLASS) {
+  CcTest::InitializeVM();
+  {
+    auto i_vec = fclass_test_values<float>();
+    for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+      auto input = *i;
+      auto fn = [](MacroAssembler& assm) { __ fclass_s(a0, fa0); };
+      auto res = GenAndRunTest<uint32_t>(input.first, fn);
+      CHECK_EQ(input.second, res);
+    }
+  }
+
+  // {
+  //   auto i_vec = fclass_test_values<double>();
+  //   for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+  //     auto input = *i;
+  //     auto fn = [](MacroAssembler& assm) { __ fclass_d(a0, fa0); };
+  //     auto res = GenAndRunTest<uint32_t>(input.first, fn);
+  //     CHECK_EQ(input.second, res);
+  //   }
+  // }
+}
+
+TEST(RISCV7) {
+  // Test floating point compare and
+  // branch instructions.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    double a;
+    double b;
+    double c;
+    double d;
+    double e;
+    double f;
+    int32_t result;
+  } t;
+
+  // Create a function that accepts &t,
+  // and loads, manipulates, and stores
+  // the doubles t.a ... t.f.
+  Label neither_is_nan, less_than, outa_here;
+  auto fn = [&neither_is_nan, &less_than, &outa_here](MacroAssembler& assm) {
+    __ fld(ft0, a0, offsetof(T, a));
+    __ fld(ft1, a0, offsetof(T, b));
+
+    __ fclass_d(t5, ft0);
+    __ fclass_d(t6, ft1);
+    __ or_(t5, t5, t6);
+    __ andi(t5, t5, kSignalingNaN | kQuietNaN);
+    __ beq(t5, zero_reg, &neither_is_nan);
+    __ sw(zero_reg, a0, offsetof(T, result));
+    __ j(&outa_here);
+
+    __ bind(&neither_is_nan);
+
+    __ flt_d(t5, ft1, ft0);
+    __ bne(t5, zero_reg, &less_than);
+
+    __ sw(zero_reg, a0, offsetof(T, result));
+    __ j(&outa_here);
+
+    __ bind(&less_than);
+    __ RV_li(a4, 1);
+    __ sw(a4, a0, offsetof(T, result));  // Set true.
+
+    // This test-case should have additional
+    // tests.
+
+    __ bind(&outa_here);
+  };
+
+  auto f = AssembleCode<F3>(fn);
+
+  t.a = 1.5e14;
+  t.b = 2.75e11;
+  t.c = 2.0;
+  t.d = -4.0;
+  t.e = 0.0;
+  t.f = 0.0;
+  t.result = 0;
+  f.Call(&t, 0, 0, 0, 0);
+  CHECK_EQ(1.5e14, t.a);
+  CHECK_EQ(2.75e11, t.b);
+  CHECK_EQ(1, t.result);
+}
+
+TEST(RISCV9) {
+  // Test BRANCH improvements.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  Label exit, exit2, exit3;
+
+  __ Branch(&exit, ge, a0, Operand(zero_reg));
+  __ Branch(&exit2, ge, a0, Operand(0x00001FFF));
+  __ Branch(&exit3, ge, a0, Operand(0x0001FFFF));
+
+  __ bind(&exit);
+  __ bind(&exit2);
+  __ bind(&exit3);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  USE(code);
+}
+
+TEST(NAN_BOX) {
+  // Test float NaN-boxing.
+  CcTest::InitializeVM();
+  // Test NaN boxing in FMV.X.W
+  {
+    auto fn = [](MacroAssembler& assm) { __ fmv_x_w(a0, fa0); };
+    auto res = GenAndRunTest<uint32_t>(1234.56f, fn);
+    CHECK_EQ((uint32_t)base::bit_cast<uint32_t>(1234.56f), res);
+  }
+
+  // Test FLW and FSW
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    float a;
+    uint64_t box;
+    uint64_t res;
+  } t;
+
+  auto fn = [](MacroAssembler& assm) {
+    // Load all structure elements to registers.
+    __ flw(fa0, a0, offsetof(T, a));
+    // Check boxing when flw
+    __ fsd(fa0, a0, offsetof(T, box));
+    // Check only transfer low 32bits when fsw
+    __ fsw(fa0, a0, offsetof(T, res));
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  t.a = -123.45;
+  t.box = 0;
+  t.res = 0;
+  f.Call(&t, 0, 0, 0, 0);
+
+  CHECK_EQ(0xFFFFFFFF00000000 | base::bit_cast<int32_t>(t.a), t.box);
+  CHECK_EQ((uint64_t)base::bit_cast<uint32_t>(t.a), t.res);
+}
+
+TEST(RVC_CI) {
+  // Test RV64C extension CI type instructions.
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  // Test c.addi
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_addi(a0, -15); };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT - 15, res);
+  }
+
+  // Test c.addi16sp
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ mv(t1, sp);
+      __ mv(sp, a0);
+      __ c_addi16sp(-432);
+      __ mv(a0, sp);
+      __ mv(sp, t1);
+    };
+    auto res = GenAndRunTest<int32_t>(66666, fn);
+    CHECK_EQ(66666 - 432, res);
+  }
+
+  // Test c.li
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_li(a0, -15); };
+    auto res = GenAndRunTest<int32_t>(1234543, fn);
+    CHECK_EQ(-15, res);
+  }
+
+  // Test c.lui
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_lui(a0, -20); };
+    auto res = GenAndRunTest<int32_t>(0x1234567, fn);
+    CHECK_EQ(0xfffec000, (uint32_t)res);
+  }
+
+  // Test c.slli
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_slli(a0, 13); };
+    auto res = GenAndRunTest<int32_t>(0x12345678, fn);
+    CHECK_EQ(0x8acf0000, (uint32_t)res);
+  }
+}
+
+TEST(RVC_CIW) {
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  // Test c.addi4spn
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ mv(t1, sp);
+      __ mv(sp, a0);
+      __ c_addi4spn(a0, 924);
+      __ mv(sp, t1);
+    };
+    auto res = GenAndRunTest<int32_t>(66666, fn);
+    CHECK_EQ(66666 + 924, res);
+  }
+}
+
+TEST(RVC_CR) {
+  // Test RV64C extension CR type instructions.
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  // Test c.add
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ RV_li(a1, MIN_VAL_IMM12);
+      __ c_add(a0, a1);
+    };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT + MIN_VAL_IMM12, res);
+  }
+}
+
+TEST(RVC_CA) {
+  // Test RV64C extension CA type instructions.
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  // Test c.sub
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ RV_li(a1, MIN_VAL_IMM12);
+      __ c_sub(a0, a1);
+    };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT - MIN_VAL_IMM12, res);
+  }
+
+  // Test c.xor
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ RV_li(a1, MIN_VAL_IMM12);
+      __ c_xor(a0, a1);
+    };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT ^ MIN_VAL_IMM12, res);
+  }
+
+  // Test c.or
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ RV_li(a1, MIN_VAL_IMM12);
+      __ c_or(a0, a1);
+    };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT | MIN_VAL_IMM12, res);
+  }
+
+  // Test c.and
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ RV_li(a1, MIN_VAL_IMM12);
+      __ c_and(a0, a1);
+    };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT & MIN_VAL_IMM12, res);
+  }
+}
+
+TEST(RVC_LOAD_STORE_SP) {
+  // Test RV32C extension flwsp/fswsp, lwsp/swsp.
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ c_fsdsp(fa0, 80);
+      __ c_fldsp(fa0, 80);
+    };
+    auto res = GenAndRunTest<float>(-3456.678f, fn);
+    CHECK_EQ(-3456.678f, res);
+  }
+
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ c_swsp(a0, 40);
+      __ c_lwsp(a0, 40);
+    };
+    auto res = GenAndRunTest<int32_t>(0x456AF894, fn);
+    CHECK_EQ(0x456AF894, res);
+  }
+}
+
+TEST(RVC_LOAD_STORE_COMPRESSED) {
+  // Test RV64C extension fld,  lw, ld.
+  i::FLAG_riscv_c_extension = true;
+
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct S {
+    int32_t a;
+    int32_t b;
+    int32_t c;
+  } s;
+  // c.lw
+  {
+    auto fn = [](MacroAssembler& assm) {
+      __ c_lw(a1, a0, offsetof(S, a));
+      __ c_lw(a2, a0, offsetof(S, b));
+      __ add(a3, a1, a2);
+      __ c_sw(a3, a0, offsetof(S, c));  // c = a + b.
+    };
+    auto f = AssembleCode<F3>(fn);
+
+    s.a = 1;
+    s.b = 2;
+    s.c = 3;
+    f.Call(&s, 0, 0, 0, 0);
+    CHECK_EQ(1, s.a);
+    CHECK_EQ(2, s.b);
+    CHECK_EQ(3, s.c);
+  }
+}
+
+TEST(RVC_JUMP) {
+  i::FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  Label L, C;
+  auto fn = [&L, &C](MacroAssembler& assm) {
+    __ mv(a1, a0);
+    __ RV_li(a0, 0l);
+    __ c_j(&C);
+
+    __ bind(&L);
+    __ add(a0, a0, a1);
+    __ addi(a1, a1, -1);
+
+    __ bind(&C);
+    __ xori(a2, a1, 0);
+    __ bnez(a2, &L);
+  };
+
+  int32_t input = 50;
+  int32_t expected_res = 1275L;
+  auto res = GenAndRunTest<int32_t>(input, fn);
+  CHECK_EQ(expected_res, res);
+}
+
+TEST(RVC_CB) {
+  // Test RV64C extension CI type instructions.
+  FLAG_riscv_c_extension = true;
+  CcTest::InitializeVM();
+
+  // Test c.srai
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_srai(a0, 13); };
+    auto res = GenAndRunTest<int32_t>(0x12345678, fn);
+    CHECK_EQ(0x12345678UL >> 13, res);
+  }
+
+  // Test c.srli
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_srli(a0, 13); };
+    auto res = GenAndRunTest<int32_t>(0x12345678, fn);
+    CHECK_EQ(0x1234'5678ULL >> 13, res);
+  }
+
+  // Test c.andi
+  {
+    auto fn = [](MacroAssembler& assm) { __ c_andi(a0, 13); };
+    auto res = GenAndRunTest<int32_t>(LARGE_INT_UNDER_32_BIT, fn);
+    CHECK_EQ(LARGE_INT_UNDER_32_BIT & 13, res);
+  }
+}
+
+TEST(RVC_CB_BRANCH) {
+  FLAG_riscv_c_extension = true;
+  // Test floating point compare and
+  // branch instructions.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct T {
+    double a;
+    double b;
+    double c;
+    double d;
+    double e;
+    double f;
+    int32_t result;
+  } t;
+
+  // Create a function that accepts &t,
+  // and loads, manipulates, and stores
+  // the doubles t.a ... t.f.
+  Label neither_is_nan, less_than, outa_here;
+  auto fn = [&neither_is_nan, &less_than, &outa_here](MacroAssembler& assm) {
+    __ fld(ft0, a0, offsetof(T, a));
+    __ fld(ft1, a0, offsetof(T, b));
+
+    __ fclass_d(t5, ft0);
+    __ fclass_d(t6, ft1);
+    __ or_(a1, t5, t6);
+    __ andi(a1, a1, kSignalingNaN | kQuietNaN);
+    __ c_beqz(a1, &neither_is_nan);
+    __ sw(zero_reg, a0, offsetof(T, result));
+    __ j(&outa_here);
+
+    __ bind(&neither_is_nan);
+
+    __ flt_d(a1, ft1, ft0);
+    __ c_bnez(a1, &less_than);
+
+    __ sw(zero_reg, a0, offsetof(T, result));
+    __ j(&outa_here);
+
+    __ bind(&less_than);
+    __ RV_li(a4, 1);
+    __ sw(a4, a0, offsetof(T, result));  // Set true.
+
+    // This test-case should have additional
+    // tests.
+
+    __ bind(&outa_here);
+  };
+
+  auto f = AssembleCode<F3>(fn);
+
+  t.a = 1.5e14;
+  t.b = 2.75e11;
+  t.c = 2.0;
+  t.d = -4.0;
+  t.e = 0.0;
+  t.f = 0.0;
+  t.result = 0;
+  f.Call(&t, 0, 0, 0, 0);
+  CHECK_EQ(1.5e14, t.a);
+  CHECK_EQ(2.75e11, t.b);
+  CHECK_EQ(1, t.result);
+}
+
+TEST(TARGET_ADDR) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  // This is the series of instructions to load 32 bit address 0x01234567 to a6
+  // (li a6,0x1234567)
+  uint32_t buffer[2] = {0x01234837,   // lui     a6,0x1234
+                        0x56780813};  // addi    a6,a6,1383
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  uintptr_t addr = reinterpret_cast<uintptr_t>(&buffer[0]);
+  Address res = __ target_address_at(static_cast<Address>(addr));
+  CHECK_EQ(0x01234567L, res);
+}
+
+TEST(SET_TARGET_ADDR) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  // This is the series of instructions to load 48 bit address 0xba9876543210
+  uint32_t buffer[6] = {0x091ab37,  0x2b330213, 0x00b21213,
+                        0x62626213, 0x00621213, 0x02b26213};
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  uintptr_t addr = reinterpret_cast<uintptr_t>(&buffer[0]);
+  __ set_target_value_at(static_cast<Address>(addr), 0xba987654L,
+                         FLUSH_ICACHE_IF_NEEDED);
+  Address res = __ target_address_at(static_cast<Address>(addr));
+  CHECK_EQ(0xba987654L, res);
+}
+
+// pair.first is the F_TYPE input to test, pair.second is I_TYPE expected
+// result
+template <typename F_TYPE, typename I_TYPE>
+static const std::vector<std::pair<F_TYPE, I_TYPE>> out_of_range_test_values() {
+  static const std::pair<F_TYPE, I_TYPE> kValues[] = {
+      std::make_pair(std::numeric_limits<F_TYPE>::quiet_NaN(),
+                     std::numeric_limits<I_TYPE>::max()),
+      std::make_pair(std::numeric_limits<F_TYPE>::signaling_NaN(),
+                     std::numeric_limits<I_TYPE>::max()),
+      std::make_pair(std::numeric_limits<F_TYPE>::infinity(),
+                     std::numeric_limits<I_TYPE>::max()),
+      std::make_pair(-std::numeric_limits<F_TYPE>::infinity(),
+                     std::numeric_limits<I_TYPE>::min()),
+      std::make_pair(
+          static_cast<F_TYPE>(std::numeric_limits<I_TYPE>::max()) + 1024,
+          std::numeric_limits<I_TYPE>::max()),
+      std::make_pair(
+          static_cast<F_TYPE>(std::numeric_limits<I_TYPE>::min()) - 1024,
+          std::numeric_limits<I_TYPE>::min()),
+  };
+  return std::vector<std::pair<F_TYPE, I_TYPE>>(&kValues[0],
+                                                &kValues[arraysize(kValues)]);
+}
+
+// Test conversion from wider to narrower types w/ out-of-range values or from
+// nan, inf, -inf
+TEST(OUT_OF_RANGE_CVT) {
+  CcTest::InitializeVM();
+
+  // {  // test fvt_w_d
+  //   auto i_vec = out_of_range_test_values<double, int32_t>();
+  //   for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+  //     auto input = *i;
+  //     auto fn = [](MacroAssembler& assm) { __ fcvt_w_d(a0, fa0); };
+  //     auto res = GenAndRunTest<int32_t>(input.first, fn);
+  //     CHECK_EQ(input.second, res);
+  //   }
+  // }
+
+  {  // test fvt_w_s
+    auto i_vec = out_of_range_test_values<float, int32_t>();
+    for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+      auto input = *i;
+      auto fn = [](MacroAssembler& assm) { __ fcvt_w_s(a0, fa0); };
+      auto res = GenAndRunTest<int32_t>(input.first, fn);
+      CHECK_EQ(input.second, res);
+    }
+  }
+
+  // {  // test fvt_wu_d
+  //   auto i_vec = out_of_range_test_values<double, uint32_t>();
+  //   for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+  //     auto input = *i;
+  //     auto fn = [](MacroAssembler& assm) { __ fcvt_wu_d(a0, fa0); };
+  //     auto res = GenAndRunTest<uint32_t>(input.first, fn);
+  //     CHECK_EQ(input.second, res);
+  //   }
+  // }
+
+  {  // test fvt_wu_s
+    auto i_vec = out_of_range_test_values<float, uint32_t>();
+    for (auto i = i_vec.begin(); i != i_vec.end(); ++i) {
+      auto input = *i;
+      auto fn = [](MacroAssembler& assm) { __ fcvt_wu_s(a0, fa0); };
+      auto res = GenAndRunTest<uint32_t>(input.first, fn);
+      CHECK_EQ(input.second, res);
+    }
+  }
+}
+
+#define FCMP_TEST_HELPER(F, fn, op)                                         \
+  {                                                                         \
+    auto res1 = GenAndRunTest<int32_t>(std::numeric_limits<F>::quiet_NaN(), \
+                                       static_cast<F>(1.0), fn);            \
+    CHECK_EQ(false, res1);                                                  \
+    auto res2 =                                                             \
+        GenAndRunTest<int32_t>(std::numeric_limits<F>::quiet_NaN(),         \
+                               std::numeric_limits<F>::quiet_NaN(), fn);    \
+    CHECK_EQ(false, res2);                                                  \
+    auto res3 =                                                             \
+        GenAndRunTest<int32_t>(std::numeric_limits<F>::signaling_NaN(),     \
+                               std::numeric_limits<F>::quiet_NaN(), fn);    \
+    CHECK_EQ(false, res3);                                                  \
+    auto res4 =                                                             \
+        GenAndRunTest<int32_t>(std::numeric_limits<F>::quiet_NaN(),         \
+                               std::numeric_limits<F>::infinity(), fn);     \
+    CHECK_EQ(false, res4);                                                  \
+    auto res5 =                                                             \
+        GenAndRunTest<int32_t>(std::numeric_limits<F>::infinity(),          \
+                               std::numeric_limits<F>::infinity(), fn);     \
+    CHECK_EQ((std::numeric_limits<F>::infinity()                            \
+                  op std::numeric_limits<F>::infinity()),                   \
+             res5);                                                         \
+    auto res6 =                                                             \
+        GenAndRunTest<int32_t>(-std::numeric_limits<F>::infinity(),         \
+                               std::numeric_limits<F>::infinity(), fn);     \
+    CHECK_EQ((-std::numeric_limits<F>::infinity()                           \
+                  op std::numeric_limits<F>::infinity()),                   \
+             res6);                                                         \
+  }
+
+TEST(F_NAN) {
+  // test floating-point compare w/ NaN, +/-Inf
+  CcTest::InitializeVM();
+
+  // floating compare
+  auto fn1 = [](MacroAssembler& assm) { __ feq_s(a0, fa0, fa1); };
+  FCMP_TEST_HELPER(float, fn1, ==);
+  auto fn2 = [](MacroAssembler& assm) { __ flt_s(a0, fa0, fa1); };
+  FCMP_TEST_HELPER(float, fn2, <);
+  auto fn3 = [](MacroAssembler& assm) { __ fle_s(a0, fa0, fa1); };
+  FCMP_TEST_HELPER(float, fn3, <=);
+
+  // double compare
+  // auto fn4 = [](MacroAssembler& assm) { __ feq_d(a0, fa0, fa1); };
+  // FCMP_TEST_HELPER(double, fn4, ==);
+  // auto fn5 = [](MacroAssembler& assm) { __ flt_d(a0, fa0, fa1); };
+  // FCMP_TEST_HELPER(double, fn5, <);
+  // auto fn6 = [](MacroAssembler& assm) { __ fle_d(a0, fa0, fa1); };
+  // FCMP_TEST_HELPER(double, fn6, <=);
+}
+
+TEST(jump_tables1) {
+  // Test jump tables with forward jumps.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  const int kNumCases = 128;
+  int values[kNumCases];
+  isolate->random_number_generator()->NextBytes(values, sizeof(values));
+  Label labels[kNumCases], done;
+
+  auto fn = [&labels, &done, values](MacroAssembler& assm) {
+    __ addi(sp, sp, -4);
+    __ Sw(ra, MemOperand(sp));
+    __ Align(4);
+    {
+      __ BlockTrampolinePoolFor(kNumCases * 2 + 6);
+
+      __ auipc(ra, 0);
+      __ slli(t3, a0, 2);
+      __ add(t3, t3, ra);
+      __ Lw(t3, MemOperand(t3, 6 * kInstrSize));
+      __ jr(t3);
+      __ nop();  // For 16-byte alignment
+      for (int i = 0; i < kNumCases; ++i) {
+        __ dd(&labels[i]);
+      }
+    }
+
+    for (int i = 0; i < kNumCases; ++i) {
+      __ bind(&labels[i]);
+      __ RV_li(a0, values[i]);
+      __ j(&done);
+    }
+
+    __ bind(&done);
+    __ Lw(ra, MemOperand(sp));
+    __ addi(sp, sp, 4);
+
+    CHECK_EQ(0, assm.UnboundLabelsCount());
+  };
+  auto f = AssembleCode<F1>(fn);
+
+  for (int i = 0; i < kNumCases; ++i) {
+    int32_t res = reinterpret_cast<int32_t>(f.Call(i, 0, 0, 0, 0));
+    CHECK_EQ(values[i], static_cast<int>(res));
+  }
+}
+
+TEST(jump_tables2) {
+  // Test jump tables with backward jumps.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  const int kNumCases = 128;
+  int32_t values[kNumCases];
+  isolate->random_number_generator()->NextBytes(values, sizeof(values));
+  Label labels[kNumCases], done, dispatch;
+
+  auto fn = [&labels, &done, &dispatch, values](MacroAssembler& assm) {
+    __ addi(sp, sp, -4);
+    __ Sw(ra, MemOperand(sp));
+    __ j(&dispatch);
+
+    for (int i = 0; i < kNumCases; ++i) {
+      __ bind(&labels[i]);
+      __ RV_li(a0, values[i]);
+      __ j(&done);
+    }
+
+    __ Align(4);
+    __ bind(&dispatch);
+
+    {
+      __ BlockTrampolinePoolFor(kNumCases * 2 + 6);
+
+      __ auipc(ra, 0);
+      __ slli(t3, a0, 2);
+      __ add(t3, t3, ra);
+      __ Lw(t3, MemOperand(t3, 6 * kInstrSize));
+      __ jr(t3);
+      __ nop();  // For 16-byte alignment
+      for (int i = 0; i < kNumCases; ++i) {
+        __ dd(&labels[i]);
+      }
+    }
+    __ bind(&done);
+    __ Lw(ra, MemOperand(sp));
+    __ addi(sp, sp, 4);
+  };
+  auto f = AssembleCode<F1>(fn);
+
+  for (int i = 0; i < kNumCases; ++i) {
+    int32_t res = reinterpret_cast<int32_t>(f.Call(i, 0, 0, 0, 0));
+    CHECK_EQ(values[i], res);
+  }
+}
+
+TEST(jump_tables3) {
+  // Test jump tables with backward jumps and embedded heap objects.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  const int kNumCases = 128;
+  Handle<Object> values[kNumCases];
+  for (int i = 0; i < kNumCases; ++i) {
+    double value = isolate->random_number_generator()->NextDouble();
+    values[i] = isolate->factory()->NewHeapNumber<AllocationType::kOld>(value);
+  }
+  Label labels[kNumCases], done, dispatch;
+  Object obj;
+  int32_t imm32;
+
+  auto fn = [&labels, &done, &dispatch, values, &obj,
+             &imm32](MacroAssembler& assm) {
+    __ addi(sp, sp, -4);
+    __ Sw(ra, MemOperand(sp));
+
+    __ j(&dispatch);
+
+    for (int i = 0; i < kNumCases; ++i) {
+      __ bind(&labels[i]);
+      obj = *values[i];
+      imm32 = obj.ptr();
+      __ nop();  // For 8 byte alignment
+      __ RV_li(a0, imm32);
+      __ nop();  // For 8 byte alignment
+      __ j(&done);
+    }
+
+    __ bind(&dispatch);
+    {
+      __ BlockTrampolinePoolFor(kNumCases * 2 + 6);
+      __ Align(4);
+      __ auipc(ra, 0);
+      __ slli(t3, a0, 2);
+      __ add(t3, t3, ra);
+      __ Lw(t3, MemOperand(t3, 6 * kInstrSize));
+      __ jr(t3);
+      __ nop();  // For 16-byte alignment
+      for (int i = 0; i < kNumCases; ++i) {
+        __ dd(&labels[i]);
+      }
+    }
+
+    __ bind(&done);
+    __ Lw(ra, MemOperand(sp));
+    __ addi(sp, sp, 4);
+  };
+  auto f = AssembleCode<F1>(fn);
+
+  for (int i = 0; i < kNumCases; ++i) {
+    Handle<Object> result(
+        Object(reinterpret_cast<Address>(f.Call(i, 0, 0, 0, 0))), isolate);
+#ifdef OBJECT_PRINT
+    ::printf("f(%d) = ", i);
+    result->Print(std::cout);
+    ::printf("\n");
+#endif
+    CHECK(values[i].is_identical_to(result));
+  }
+}
+
+TEST(li_estimate) {
+  std::vector<int64_t> immediates = {
+      -256,      -255,          0,         255,        8192,      0x7FFFFFFF,
+      INT32_MIN, INT32_MAX / 2, INT32_MAX, UINT32_MAX, INT64_MAX, INT64_MAX / 2,
+      INT64_MIN};
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  for (auto p : immediates) {
+    Label a;
+    assm.bind(&a);
+    assm.RV_li(t0, p);
+    int expected_count = assm.li_estimate(p, true);
+    int count = assm.InstructionsGeneratedSince(&a);
+    CHECK_EQ(count, expected_count);
+  }
+}
+
+#ifdef CAN_USE_RVV_INSTRUCTIONS
+#define UTEST_LOAD_STORE_RVV(ldname, stname, SEW, arry)                      \
+  TEST(RISCV_UTEST_##stname##ldname##SEW) {                                  \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    CcTest::InitializeVM();                                                  \
+    Isolate* isolate = CcTest::i_isolate();                                  \
+    HandleScope scope(isolate);                                              \
+    int8_t src[16];                                                          \
+    for (size_t i = 0; i < sizeof(src); i++) src[i] = arry[i % arry.size()]; \
+    int8_t dst[16];                                                          \
+    auto fn = [](MacroAssembler& assm) {                                     \
+      __ VU.set(t0, SEW, Vlmul::m1);                                         \
+      __ vl(v2, a0, 0, SEW);                                                 \
+      __ vs(v2, a1, 0, SEW);                                                 \
+    };                                                                       \
+    GenAndRunTest<int32_t, int64_t>((int64_t)src, (int64_t)dst, fn);         \
+    CHECK(!memcmp(src, dst, sizeof(src)));                                   \
+  }
+
+UTEST_LOAD_STORE_RVV(vl, vs, E8, compiler::ValueHelper::GetVector<int8_t>())
+
+TEST(RVV_VFMV) {
+  if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;
+
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+  for (float a : compiler::ValueHelper::GetVector<float>()) {
+    float src = a;
+    float dst[8] = {0};
+    float ref[8] = {a, a, a, a, a, a, a, a};
+    auto fn = [](MacroAssembler& assm) {
+      __ VU.set(t0, VSew::E32, Vlmul::m2);
+      __ flw(fa1, a0, 0);
+      __ vfmv_vf(v2, fa1);
+      __ vs(v2, a1, 0, VSew::E32);
+    };
+    GenAndRunTest<int32_t, int64_t>((int64_t)&src, (int64_t)dst, fn);
+    CHECK(!memcmp(ref, dst, sizeof(ref)));
+  }
+}
+
+inline int32_t ToImm5(int32_t v) {
+  int32_t smax = (int32_t)(INT64_MAX >> (64 - 5));
+  int32_t smin = (int32_t)(INT64_MIN >> (64 - 5));
+  return (v > smax) ? smax : ((v < smin) ? smin : v);
+}
+
+// Tests for vector integer arithmetic instructions between vector and vector
+#define UTEST_RVV_VI_VV_FORM_WITH_RES(instr_name, width, array, expect_res) \
+  TEST(RISCV_UTEST_##instr_name##_##width) {                                \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                      \
+    CcTest::InitializeVM();                                                 \
+    int##width##_t result[kRvvVLEN / width] = {0};                          \
+    auto fn = [&result](MacroAssembler& assm) {                             \
+      __ VU.set(t0, VSew::E##width, Vlmul::m1);                             \
+      __ vmv_vx(v0, a0);                                                    \
+      __ vmv_vx(v1, a1);                                                    \
+      __ instr_name(v0, v0, v1);                                            \
+      __ li(t1, int64_t(result));                                           \
+      __ vs(v0, t1, 0, VSew::E##width);                                     \
+    };                                                                      \
+    for (int##width##_t rs1_val : array) {                                  \
+      for (int##width##_t rs2_val : array) {                                \
+        GenAndRunTest<int32_t, int32_t>(rs1_val, rs2_val, fn);              \
+        for (int i = 0; i < kRvvVLEN / width; i++)                          \
+          CHECK_EQ(static_cast<int##width##_t>(expect_res), result[i]);     \
+      }                                                                     \
+    }                                                                       \
+  }
+
+// Tests for vector integer arithmetic instructions between vector and scalar
+#define UTEST_RVV_VI_VX_FORM_WITH_RES(instr_name, width, array, expect_res) \
+  TEST(RISCV_UTEST_##instr_name##_##width) {                                \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                      \
+    CcTest::InitializeVM();                                                 \
+    int##width##_t result[kRvvVLEN / width] = {0};                          \
+    auto fn = [&result](MacroAssembler& assm) {                             \
+      __ VU.set(t0, VSew::E##width, Vlmul::m1);                             \
+      __ vmv_vx(v0, a0);                                                    \
+      __ instr_name(v0, v0, a1);                                            \
+      __ li(t1, int64_t(result));                                           \
+      __ vs(v0, t1, 0, VSew::E##width);                                     \
+    };                                                                      \
+    for (int##width##_t rs1_val : array) {                                  \
+      for (int##width##_t rs2_val : array) {                                \
+        GenAndRunTest<int32_t, int32_t>(rs1_val, rs2_val, fn);              \
+        for (int i = 0; i < kRvvVLEN / width; i++)                          \
+          CHECK_EQ(static_cast<int##width##_t>(expect_res), result[i]);     \
+      }                                                                     \
+    }                                                                       \
+  }
+
+// Tests for vector integer arithmetic instructions between vector and 5-bit
+// immediate
+#define UTEST_RVV_VI_VI_FORM_WITH_RES(instr_name, width, array, expect_res) \
+  TEST(RISCV_UTEST_##instr_name##_##width) {                                \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                      \
+    CcTest::InitializeVM();                                                 \
+    int##width##_t result[kRvvVLEN / width] = {0};                          \
+    for (int##width##_t rs1_val : array) {                                  \
+      for (int##width##_t rs2_val : array) {                                \
+        auto fn = [rs2_val, &result](MacroAssembler& assm) {                \
+          __ VU.set(t0, VSew::E##width, Vlmul::m1);                         \
+          __ vmv_vx(v0, a0);                                                \
+          __ instr_name(v0, v0, ToImm5(rs2_val));                           \
+          __ li(t1, int64_t(result));                                       \
+          __ vs(v0, t1, 0, VSew::E##width);                                 \
+        };                                                                  \
+        GenAndRunTest<int32_t, int32_t>(rs1_val, fn);                       \
+        for (int i = 0; i < kRvvVLEN / width; i++)                          \
+          CHECK_EQ(static_cast<int##width##_t>(expect_res), result[i]);     \
+      }                                                                     \
+    }                                                                       \
+  }
+
+#define UTEST_RVV_VI_VV_FORM_WITH_OP(instr_name, width, array, tested_op) \
+  UTEST_RVV_VI_VV_FORM_WITH_RES(instr_name, width, array,                 \
+                                (int##width##_t)((rs1_val)tested_op(rs2_val)))
+
+#define UTEST_RVV_VI_VX_FORM_WITH_OP(instr_name, width, array, tested_op) \
+  UTEST_RVV_VI_VX_FORM_WITH_RES(instr_name, width, array,                 \
+                                (int##width##_t)((rs1_val)tested_op(rs2_val)))
+
+#define UTEST_RVV_VI_VI_FORM_WITH_OP(instr_name, width, array, tested_op) \
+  UTEST_RVV_VI_VI_FORM_WITH_RES(                                          \
+      instr_name, width, array,                                           \
+      (int##width##_t)((rs1_val)tested_op(ToImm5(rs2_val))))
+
+#define UTEST_RVV_VI_VV_FORM_WITH_FN(instr_name, width, array, tested_fn) \
+  UTEST_RVV_VI_VV_FORM_WITH_RES(instr_name, width, array,                 \
+                                tested_fn(rs1_val, rs2_val))
+
+#define UTEST_RVV_VI_VX_FORM_WITH_FN(instr_name, width, array, tested_fn) \
+  UTEST_RVV_VI_VX_FORM_WITH_RES(instr_name, width, array,                 \
+                                tested_fn(rs1_val, rs2_val))
+
+#define ARRAY_INT32 compiler::ValueHelper::GetVector<int32_t>()
+
+#define VV(instr_name, array, tested_op)                         \
+  UTEST_RVV_VI_VV_FORM_WITH_OP(instr_name, 8, array, tested_op)  \
+  UTEST_RVV_VI_VV_FORM_WITH_OP(instr_name, 16, array, tested_op) \
+  UTEST_RVV_VI_VV_FORM_WITH_OP(instr_name, 32, array, tested_op)
+
+#define VX(instr_name, array, tested_op)                         \
+  UTEST_RVV_VI_VX_FORM_WITH_OP(instr_name, 8, array, tested_op)  \
+  UTEST_RVV_VI_VX_FORM_WITH_OP(instr_name, 16, array, tested_op) \
+  UTEST_RVV_VI_VX_FORM_WITH_OP(instr_name, 32, array, tested_op)
+
+#define VI(instr_name, array, tested_op)                         \
+  UTEST_RVV_VI_VI_FORM_WITH_OP(instr_name, 8, array, tested_op)  \
+  UTEST_RVV_VI_VI_FORM_WITH_OP(instr_name, 16, array, tested_op) \
+  UTEST_RVV_VI_VI_FORM_WITH_OP(instr_name, 32, array, tested_op)
+
+VV(vadd_vv, ARRAY_INT32, +)
+VX(vadd_vx, ARRAY_INT32, +)
+VI(vadd_vi, ARRAY_INT32, +)
+VV(vsub_vv, ARRAY_INT32, -)
+VX(vsub_vx, ARRAY_INT32, -)
+VV(vand_vv, ARRAY_INT32, &)
+VX(vand_vx, ARRAY_INT32, &)
+VI(vand_vi, ARRAY_INT32, &)
+VV(vor_vv, ARRAY_INT32, |)
+VX(vor_vx, ARRAY_INT32, |)
+VI(vor_vi, ARRAY_INT32, |)
+VV(vxor_vv, ARRAY_INT32, ^)
+VX(vxor_vx, ARRAY_INT32, ^)
+VI(vxor_vi, ARRAY_INT32, ^)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmax_vv, 8, ARRAY_INT32, std::max<int8_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmax_vx, 8, ARRAY_INT32, std::max<int8_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmax_vv, 16, ARRAY_INT32, std::max<int16_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmax_vx, 16, ARRAY_INT32, std::max<int16_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmax_vv, 32, ARRAY_INT32, std::max<int32_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmax_vx, 32, ARRAY_INT32, std::max<int32_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmin_vv, 8, ARRAY_INT32, std::min<int8_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmin_vx, 8, ARRAY_INT32, std::min<int8_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmin_vv, 16, ARRAY_INT32, std::min<int16_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmin_vx, 16, ARRAY_INT32, std::min<int16_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmin_vv, 32, ARRAY_INT32, std::min<int32_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmin_vx, 32, ARRAY_INT32, std::min<int32_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmaxu_vv, 8, ARRAY_INT32, std::max<uint8_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmaxu_vx, 8, ARRAY_INT32, std::max<uint8_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmaxu_vv, 16, ARRAY_INT32, std::max<uint16_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmaxu_vx, 16, ARRAY_INT32, std::max<uint16_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vmaxu_vv, 32, ARRAY_INT32, std::max<uint32_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vmaxu_vx, 32, ARRAY_INT32, std::max<uint32_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vminu_vv, 8, ARRAY_INT32, std::min<uint8_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 8, ARRAY_INT32, std::min<uint8_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vminu_vv, 16, ARRAY_INT32, std::min<uint16_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 16, ARRAY_INT32, std::min<uint16_t>)
+UTEST_RVV_VI_VV_FORM_WITH_FN(vminu_vv, 32, ARRAY_INT32, std::min<uint32_t>)
+UTEST_RVV_VI_VX_FORM_WITH_FN(vminu_vx, 32, ARRAY_INT32, std::min<uint32_t>)
+
+#undef ARRAY_INT32
+#undef VV
+#undef VX
+#undef VI
+#undef UTEST_RVV_VI_VV_FORM_WITH_FN
+#undef UTEST_RVV_VI_VX_FORM_WITH_FN
+#undef UTEST_RVV_VI_VI_FORM_WITH_OP
+#undef UTEST_RVV_VI_VX_FORM_WITH_OP
+#undef UTEST_RVV_VI_VV_FORM_WITH_OP
+#undef UTEST_RVV_VI_VI_FORM
+#undef UTEST_RVV_VI_VX_FORM
+#undef UTEST_RVV_VI_VV_FORM
+
+// Tests for vector single-width floating-point arithmetic instructions between
+// vector and vector
+#define UTEST_RVV_VF_VV_FORM_WITH_RES(instr_name, expect_res)              \
+  TEST(RISCV_UTEST_FLOAT_##instr_name) {                                   \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                     \
+    CcTest::InitializeVM();                                                \
+    float result[4] = {0.0};                                               \
+    auto fn = [&result](MacroAssembler& assm) {                            \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                 \
+      __ vfmv_vf(v0, fa0);                                                 \
+      __ vfmv_vf(v1, fa1);                                                 \
+      __ instr_name(v0, v0, v1);                                           \
+      __ vfmv_fs(fa0, v0);                                                 \
+      __ li(a3, Operand(int64_t(result)));                                 \
+      __ vs(v0, a3, 0, E32);                                               \
+    };                                                                     \
+    for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {     \
+      for (float rs2_fval : compiler::ValueHelper::GetVector<float>()) {   \
+        GenAndRunTest<float, float>(rs1_fval, rs2_fval, fn);               \
+        for (int i = 0; i < 4; i++) {                                      \
+          CHECK_FLOAT_EQ(UseCanonicalNan<float>(expect_res), result[i]);   \
+          result[i] = 0.0;                                                 \
+        }                                                                  \
+      }                                                                    \
+    }                                                                      \
+  }                                                                        \
+  TEST(RISCV_UTEST_DOUBLE_##instr_name) {                                  \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                     \
+    CcTest::InitializeVM();                                                \
+    double result[2] = {0.0};                                              \
+    auto fn = [&result](MacroAssembler& assm) {                            \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                                 \
+      __ vfmv_vf(v0, fa0);                                                 \
+      __ vfmv_vf(v1, fa1);                                                 \
+      __ instr_name(v0, v0, v1);                                           \
+      __ vfmv_fs(fa0, v0);                                                 \
+      __ li(a3, Operand(int64_t(result)));                                 \
+      __ vs(v0, a3, 0, E64);                                               \
+    };                                                                     \
+    for (double rs1_fval : compiler::ValueHelper::GetVector<double>()) {   \
+      for (double rs2_fval : compiler::ValueHelper::GetVector<double>()) { \
+        GenAndRunTest<double, double>(rs1_fval, rs2_fval, fn);             \
+        for (int i = 0; i < 2; i++) {                                      \
+          CHECK_DOUBLE_EQ(UseCanonicalNan<double>(expect_res), result[i]); \
+          result[i] = 0.0;                                                 \
+        }                                                                  \
+      }                                                                    \
+    }                                                                      \
+  }
+
+// Tests for vector single-width floating-point arithmetic instructions between
+// vector and scalar
+#define UTEST_RVV_VF_VF_FORM_WITH_RES(instr_name, array, expect_res)    \
+  TEST(RISCV_UTEST_##instr_name) {                                      \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                  \
+    CcTest::InitializeVM();                                             \
+    auto fn = [](MacroAssembler& assm) {                                \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                              \
+      __ vfmv_vf(v0, fa0);                                              \
+      __ instr_name(v0, v0, fa1);                                       \
+      __ vfmv_fs(fa0, v0);                                              \
+    };                                                                  \
+    for (float rs1_fval : array) {                                      \
+      for (float rs2_fval : array) {                                    \
+        auto res = GenAndRunTest<float, float>(rs1_fval, rs2_fval, fn); \
+        CHECK_FLOAT_EQ(UseCanonicalNan<float>(expect_res), res);        \
+      }                                                                 \
+    }                                                                   \
+  }
+
+#define UTEST_RVV_VF_VV_FORM_WITH_OP(instr_name, tested_op) \
+  UTEST_RVV_VF_VV_FORM_WITH_RES(instr_name, ((rs1_fval)tested_op(rs2_fval)))
+
+#define UTEST_RVV_VF_VF_FORM_WITH_OP(instr_name, tested_op) \
+  UTEST_RVV_VF_VF_FORM_WITH_RES(instr_name, ((rs1_fval)tested_op(rs2_fval)))
+
+UTEST_RVV_VF_VV_FORM_WITH_OP(vfadd_vv, +)
+// UTEST_RVV_VF_VF_FORM_WITH_OP(vfadd_vf, ARRAY_FLOAT, +)
+UTEST_RVV_VF_VV_FORM_WITH_OP(vfsub_vv, -)
+// UTEST_RVV_VF_VF_FORM_WITH_OP(vfsub_vf, ARRAY_FLOAT, -)
+UTEST_RVV_VF_VV_FORM_WITH_OP(vfmul_vv, *)
+// UTEST_RVV_VF_VF_FORM_WITH_OP(vfmul_vf, ARRAY_FLOAT, *)
+UTEST_RVV_VF_VV_FORM_WITH_OP(vfdiv_vv, /)
+// UTEST_RVV_VF_VF_FORM_WITH_OP(vfdiv_vf, ARRAY_FLOAT, /)
+
+#undef ARRAY_FLOAT
+#undef UTEST_RVV_VF_VV_FORM_WITH_OP
+#undef UTEST_RVV_VF_VF_FORM_WITH_OP
+
+// Tests for vector widening floating-point arithmetic instructions between
+// vector and vector
+#define UTEST_RVV_VFW_VV_FORM_WITH_RES(instr_name, tested_op, is_first_double, \
+                                       check_fn)                               \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                              \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                         \
+    CcTest::InitializeVM();                                                    \
+    constexpr size_t n = kRvvVLEN / 32;                                        \
+    double result[n] = {0.0};                                                  \
+    auto fn = [&result](MacroAssembler& assm) {                                \
+      if (is_first_double) {                                                   \
+        __ fcvt_d_s(fa0, fa0);                                                 \
+        __ VU.set(t0, VSew::E64, Vlmul::m2);                                   \
+        __ vfmv_vf(v2, fa0);                                                   \
+      }                                                                        \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                     \
+      if (!is_first_double) {                                                  \
+        __ vfmv_vf(v2, fa0);                                                   \
+      }                                                                        \
+      __ vfmv_vf(v4, fa1);                                                     \
+      __ instr_name(v0, v2, v4);                                               \
+      __ li(t1, Operand(int64_t(result)));                                     \
+      __ vs(v0, t1, 0, VSew::E64);                                             \
+    };                                                                         \
+    for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {         \
+      for (float rs2_fval : compiler::ValueHelper::GetVector<float>()) {       \
+        GenAndRunTest<double, float>(rs1_fval, rs2_fval, fn);                  \
+        for (size_t i = 0; i < n; i++) {                                       \
+          CHECK_DOUBLE_EQ(                                                     \
+              check_fn(rs1_fval, rs2_fval)                                     \
+                  ? std::numeric_limits<double>::quiet_NaN()                   \
+                  : UseCanonicalNan<double>(static_cast<double>(               \
+                        rs1_fval) tested_op static_cast<double>(rs2_fval)),    \
+              result[i]);                                                      \
+          result[i] = 0.0;                                                     \
+        }                                                                      \
+      }                                                                        \
+    }                                                                          \
+  }
+
+// Tests for vector widening floating-point arithmetic instructions between
+// vector and scalar
+#define UTEST_RVV_VFW_VF_FORM_WITH_RES(instr_name, tested_op, is_first_double, \
+                                       check_fn)                               \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                              \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                         \
+    CcTest::InitializeVM();                                                    \
+    constexpr size_t n = kRvvVLEN / 32;                                        \
+    double result[n] = {0.0};                                                  \
+    auto fn = [&result](MacroAssembler& assm) {                                \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                     \
+      if (is_first_double) {                                                   \
+        __ fcvt_d_s(fa0, fa0);                                                 \
+        __ VU.set(t0, VSew::E64, Vlmul::m2);                                   \
+        __ vfmv_vf(v2, fa0);                                                   \
+      }                                                                        \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                     \
+      if (!is_first_double) {                                                  \
+        __ vfmv_vf(v2, fa0);                                                   \
+      }                                                                        \
+      __ instr_name(v0, v2, fa1);                                              \
+      __ li(t1, Operand(int64_t(result)));                                     \
+      __ li(t2, Operand(int64_t(&result[n / 2])));                             \
+      __ vs(v0, t1, 0, VSew::E64);                                             \
+      __ vs(v1, t2, 0, VSew::E64);                                             \
+    };                                                                         \
+    for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) {         \
+      for (float rs2_fval : compiler::ValueHelper::GetVector<float>()) {       \
+        GenAndRunTest<double, float>(rs1_fval, rs2_fval, fn);                  \
+        for (size_t i = 0; i < n; i++) {                                       \
+          CHECK_DOUBLE_EQ(                                                     \
+              check_fn(rs1_fval, rs2_fval)                                     \
+                  ? std::numeric_limits<double>::quiet_NaN()                   \
+                  : UseCanonicalNan<double>(static_cast<double>(               \
+                        rs1_fval) tested_op static_cast<double>(rs2_fval)),    \
+              result[i]);                                                      \
+          result[i] = 0.0;                                                     \
+        }                                                                      \
+      }                                                                        \
+    }                                                                          \
+  }
+
+#define UTEST_RVV_VFW_VV_FORM_WITH_OP(instr_name, tested_op, is_first_double, \
+                                      check_fn)                               \
+  UTEST_RVV_VFW_VV_FORM_WITH_RES(instr_name, tested_op, is_first_double,      \
+                                 check_fn)
+#define UTEST_RVV_VFW_VF_FORM_WITH_OP(instr_name, tested_op, is_first_double, \
+                                      check_fn)                               \
+  UTEST_RVV_VFW_VF_FORM_WITH_RES(instr_name, tested_op, is_first_double,      \
+                                 check_fn)
+
+template <typename T>
+static inline bool is_invalid_fmul(T src1, T src2) {
+  return (isinf(src1) && src2 == static_cast<T>(0.0)) ||
+         (src1 == static_cast<T>(0.0) && isinf(src2));
+}
+
+template <typename T>
+static inline bool is_invalid_fadd(T src1, T src2) {
+  return (isinf(src1) && isinf(src2) &&
+          std::signbit(src1) != std::signbit(src2));
+}
+
+template <typename T>
+static inline bool is_invalid_fsub(T src1, T src2) {
+  return (isinf(src1) && isinf(src2) &&
+          std::signbit(src1) == std::signbit(src2));
+}
+
+UTEST_RVV_VFW_VV_FORM_WITH_OP(vfwadd_vv, +, false, is_invalid_fadd)
+UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwadd_vf, +, false, is_invalid_fadd)
+UTEST_RVV_VFW_VV_FORM_WITH_OP(vfwsub_vv, -, false, is_invalid_fsub)
+UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwsub_vf, -, false, is_invalid_fsub)
+UTEST_RVV_VFW_VV_FORM_WITH_OP(vfwadd_wv, +, true, is_invalid_fadd)
+UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwadd_wf, +, true, is_invalid_fadd)
+UTEST_RVV_VFW_VV_FORM_WITH_OP(vfwsub_wv, -, true, is_invalid_fsub)
+UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwsub_wf, -, true, is_invalid_fsub)
+UTEST_RVV_VFW_VV_FORM_WITH_OP(vfwmul_vv, *, false, is_invalid_fmul)
+UTEST_RVV_VFW_VF_FORM_WITH_OP(vfwmul_vf, *, false, is_invalid_fmul)
+
+#undef UTEST_RVV_VF_VV_FORM_WITH_OP
+#undef UTEST_RVV_VF_VF_FORM_WITH_OP
+
+// Tests for vector widening floating-point fused multiply-add Instructions
+// between vectors
+#define UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(instr_name, array, expect_res)     \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                             \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                        \
+    CcTest::InitializeVM();                                                   \
+    auto fn = [](MacroAssembler& assm) {                                      \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                    \
+      __ vfmv_vf(v0, fa0);                                                    \
+      __ vfmv_vf(v2, fa1);                                                    \
+      __ vfmv_vf(v4, fa2);                                                    \
+      __ instr_name(v0, v2, v4);                                              \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                                    \
+      __ vfmv_fs(fa0, v0);                                                    \
+    };                                                                        \
+    for (float rs1_fval : array) {                                            \
+      for (float rs2_fval : array) {                                          \
+        for (float rs3_fval : array) {                                        \
+          double res =                                                        \
+              GenAndRunTest<double, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
+          CHECK_DOUBLE_EQ((expect_res), res);                                 \
+        }                                                                     \
+      }                                                                       \
+    }                                                                         \
+  }
+
+// Tests for vector single-width floating-point fused multiply-add Instructions
+// between vectors and scalar
+#define UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(instr_name, array, expect_res)     \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                             \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                        \
+    CcTest::InitializeVM();                                                   \
+    auto fn = [](MacroAssembler& assm) {                                      \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                    \
+      __ vfmv_vf(v0, fa0);                                                    \
+      __ vfmv_vf(v2, fa2);                                                    \
+      __ instr_name(v0, fa1, v2);                                             \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                                    \
+      __ vfmv_fs(fa0, v0);                                                    \
+    };                                                                        \
+    for (float rs1_fval : array) {                                            \
+      for (float rs2_fval : array) {                                          \
+        for (float rs3_fval : array) {                                        \
+          double res =                                                        \
+              GenAndRunTest<double, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
+          CHECK_DOUBLE_EQ((expect_res), res);                                 \
+        }                                                                     \
+      }                                                                       \
+    }                                                                         \
+  }
+
+#define ARRAY_FLOAT compiler::ValueHelper::GetVector<float>()
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmacc_vv, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, rs3_fval, rs1_fval))
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmacc_vf, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, rs3_fval, rs1_fval))
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmacc_vv, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, -rs3_fval, -rs1_fval))
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmacc_vf, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, -rs3_fval, -rs1_fval))
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwmsac_vv, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, rs3_fval, -rs1_fval))
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwmsac_vf, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, rs3_fval, -rs1_fval))
+UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES(vfwnmsac_vv, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, -rs3_fval, rs1_fval))
+UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES(vfwnmsac_vf, ARRAY_FLOAT,
+                                   std::fma(rs2_fval, -rs3_fval, rs1_fval))
+
+#undef ARRAY_FLOAT
+#undef UTEST_RVV_VFW_FMA_VV_FORM_WITH_RES
+#undef UTEST_RVV_VFW_FMA_VF_FORM_WITH_RES
+
+// Tests for vector single-width floating-point fused multiply-add Instructions
+// between vectors
+#define UTEST_RVV_FMA_VV_FORM_WITH_RES(instr_name, array, expect_res)        \
+  TEST(RISCV_UTEST_##instr_name) {                                           \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    CcTest::InitializeVM();                                                  \
+    auto fn = [](MacroAssembler& assm) {                                     \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                   \
+      __ vfmv_vf(v0, fa0);                                                   \
+      __ vfmv_vf(v1, fa1);                                                   \
+      __ vfmv_vf(v2, fa2);                                                   \
+      __ instr_name(v0, v1, v2);                                             \
+      __ vfmv_fs(fa0, v0);                                                   \
+    };                                                                       \
+    for (float rs1_fval : array) {                                           \
+      for (float rs2_fval : array) {                                         \
+        for (float rs3_fval : array) {                                       \
+          auto res =                                                         \
+              GenAndRunTest<float, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
+          CHECK_FLOAT_EQ(expect_res, res);                                   \
+        }                                                                    \
+      }                                                                      \
+    }                                                                        \
+  }
+
+// Tests for vector single-width floating-point fused multiply-add Instructions
+// between vectors and scalar
+#define UTEST_RVV_FMA_VF_FORM_WITH_RES(instr_name, array, expect_res)        \
+  TEST(RISCV_UTEST_##instr_name) {                                           \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    CcTest::InitializeVM();                                                  \
+    auto fn = [](MacroAssembler& assm) {                                     \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                                   \
+      __ vfmv_vf(v0, fa0);                                                   \
+      __ vfmv_vf(v2, fa2);                                                   \
+      __ instr_name(v0, fa1, v2);                                            \
+      __ vfmv_fs(fa0, v0);                                                   \
+    };                                                                       \
+    for (float rs1_fval : array) {                                           \
+      for (float rs2_fval : array) {                                         \
+        for (float rs3_fval : array) {                                       \
+          auto res =                                                         \
+              GenAndRunTest<float, float>(rs1_fval, rs2_fval, rs3_fval, fn); \
+          CHECK_FLOAT_EQ(expect_res, res);                                   \
+        }                                                                    \
+      }                                                                      \
+    }                                                                        \
+  }
+
+#define ARRAY_FLOAT compiler::ValueHelper::GetVector<float>()
+
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfmadd_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs1_fval, rs3_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfmadd_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs1_fval, rs3_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfnmadd_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs1_fval, -rs3_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmadd_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs1_fval, -rs3_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfmsub_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs1_fval, -rs3_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfmsub_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs1_fval, -rs3_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfnmsub_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs1_fval, rs3_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmsub_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs1_fval, rs3_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfmacc_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs3_fval, rs1_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfmacc_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs3_fval, rs1_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfnmacc_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs3_fval, -rs1_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmacc_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs3_fval, -rs1_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfmsac_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs3_fval, -rs1_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfmsac_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, rs3_fval, -rs1_fval))
+UTEST_RVV_FMA_VV_FORM_WITH_RES(vfnmsac_vv, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs3_fval, rs1_fval))
+UTEST_RVV_FMA_VF_FORM_WITH_RES(vfnmsac_vf, ARRAY_FLOAT,
+                               std::fma(rs2_fval, -rs3_fval, rs1_fval))
+
+#undef ARRAY_FLOAT
+#undef UTEST_RVV_FMA_VV_FORM_WITH_RES
+#undef UTEST_RVV_FMA_VF_FORM_WITH_RES
+
+// Tests for vector Widening Floating-Point Reduction Instructions
+#define UTEST_RVV_VFW_REDSUM_VV_FORM_WITH_RES(instr_name)              \
+  TEST(RISCV_UTEST_FLOAT_WIDENING_##instr_name) {                      \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                 \
+    CcTest::InitializeVM();                                            \
+    auto fn = [](MacroAssembler& assm) {                               \
+      __ VU.set(t0, VSew::E32, Vlmul::m1);                             \
+      __ vfmv_vf(v2, fa0);                                             \
+      __ vfmv_vf(v4, fa0);                                             \
+      __ instr_name(v0, v2, v4);                                       \
+      __ VU.set(t0, VSew::E64, Vlmul::m1);                             \
+      __ vfmv_fs(fa0, v0);                                             \
+    };                                                                 \
+    for (float rs1_fval : compiler::ValueHelper::GetVector<float>()) { \
+      std::vector<double> temp_arr(kRvvVLEN / 32,                      \
+                                   static_cast<double>(rs1_fval));     \
+      double expect_res = rs1_fval;                                    \
+      for (double val : temp_arr) {                                    \
+        expect_res += val;                                             \
+        if (std::isnan(expect_res)) {                                  \
+          expect_res = std::numeric_limits<double>::quiet_NaN();       \
+          break;                                                       \
+        }                                                              \
+      }                                                                \
+      double res = GenAndRunTest<double, float>(rs1_fval, fn);         \
+      CHECK_DOUBLE_EQ(UseCanonicalNan<double>(expect_res), res);       \
+    }                                                                  \
+  }
+
+UTEST_RVV_VFW_REDSUM_VV_FORM_WITH_RES(vfwredusum_vv)
+UTEST_RVV_VFW_REDSUM_VV_FORM_WITH_RES(vfwredosum_vv)
+
+#undef UTEST_RVV_VFW_REDSUM_VV_FORM_WITH_RES
+// calculate the value of r used in rounding
+static inline uint8_t get_round(int vxrm, uint64_t v, uint8_t shift) {
+  // uint8_t d = extract64(v, shift, 1);
+  uint8_t d = unsigned_bitextract_64(shift, shift, v);
+  uint8_t d1;
+  uint64_t D1, D2;
+
+  if (shift == 0 || shift > 64) {
+    return 0;
+  }
+
+  // d1 = extract64(v, shift - 1, 1);
+  d1 = unsigned_bitextract_64(shift - 1, shift - 1, v);
+  // D1 = extract64(v, 0, shift);
+  D1 = unsigned_bitextract_64(shift - 1, 0, v);
+  if (vxrm == 0) { /* round-to-nearest-up (add +0.5 LSB) */
+    return d1;
+  } else if (vxrm == 1) { /* round-to-nearest-even */
+    if (shift > 1) {
+      // D2 = extract64(v, 0, shift - 1);
+      D2 = unsigned_bitextract_64(shift - 2, 0, v);
+      return d1 & ((D2 != 0) | d);
+    } else {
+      return d1 & d;
+    }
+  } else if (vxrm == 3) { /* round-to-odd (OR bits into LSB, aka "jam") */
+    return !d & (D1 != 0);
+  }
+  return 0; /* round-down (truncate) */
+}
+
+#define UTEST_RVV_VNCLIP_E32M2_E16M1(instr_name, sign)                       \
+  TEST(RISCV_UTEST_##instr_name##_E32M2_E16M1) {                             \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                       \
+    constexpr FPURoundingMode vxrm = RNE;                                    \
+    CcTest::InitializeVM();                                                  \
+    Isolate* isolate = CcTest::i_isolate();                                  \
+    HandleScope scope(isolate);                                              \
+    for (int32_t x : compiler::ValueHelper::GetVector<int>()) {              \
+      for (uint8_t shift = 0; shift < 32; shift++) {                         \
+        auto fn = [shift](MacroAssembler& assm) {                            \
+          __ VU.set(vxrm);                                                   \
+          __ VU.set(t0, VSew::E32, Vlmul::m2);                               \
+          __ vl(v2, a0, 0, VSew::E32);                                       \
+          __ VU.set(t0, VSew::E16, Vlmul::m1);                               \
+          __ instr_name(v4, v2, shift);                                      \
+          __ vs(v4, a1, 0, VSew::E16);                                       \
+        };                                                                   \
+        struct T {                                                           \
+          sign##int32_t src[8] = {0};                                        \
+          sign##int16_t dst[8] = {0};                                        \
+          sign##int16_t ref[8] = {0};                                        \
+        } t;                                                                 \
+        for (auto& src : t.src) src = static_cast<sign##int32_t>(x);         \
+        for (auto& ref : t.ref)                                              \
+          ref = base::saturated_cast<sign##int16_t>(                         \
+              (static_cast<sign##int32_t>(x) >> shift) +                     \
+              get_round(vxrm, x, shift));                                    \
+        GenAndRunTest<int32_t, int64_t>((int64_t)t.src, (int64_t)t.dst, fn); \
+        CHECK(!memcmp(t.dst, t.ref, sizeof(t.ref)));                         \
+      }                                                                      \
+    }                                                                        \
+  }
+
+UTEST_RVV_VNCLIP_E32M2_E16M1(vnclipu_vi, u)
+UTEST_RVV_VNCLIP_E32M2_E16M1(vnclip_vi, )
+
+#undef UTEST_RVV_VNCLIP_E32M2_E16M1
+
+// Tests for vector integer extension instructions
+#define UTEST_RVV_VI_VIE_FORM_WITH_RES(instr_name, type, width, frac_width, \
+                                       array, expect_res)                   \
+  TEST(RISCV_UTEST_##instr_name##_##width##_##frac_width) {                 \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                      \
+    constexpr uint32_t n = kRvvVLEN / width;                                \
+    CcTest::InitializeVM();                                                 \
+    for (int##frac_width##_t x : array) {                                   \
+      int##frac_width##_t src[n] = {0};                                     \
+      type dst[n] = {0};                                                    \
+      for (uint32_t i = 0; i < n; i++) src[i] = x;                          \
+      auto fn = [](MacroAssembler& assm) {                                  \
+        __ VU.set(t0, VSew::E##frac_width, Vlmul::m1);                      \
+        __ vl(v1, a0, 0, VSew::E##frac_width);                              \
+        __ VU.set(t0, VSew::E##width, Vlmul::m1);                           \
+        __ instr_name(v2, v1);                                              \
+        __ vs(v2, a1, 0, VSew::E##width);                                   \
+      };                                                                    \
+      GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn);      \
+      for (uint32_t i = 0; i < n; i++) {                                    \
+        CHECK_EQ(expect_res, dst[i]);                                       \
+      }                                                                     \
+    }                                                                       \
+  }
+
+#define ARRAY(type) compiler::ValueHelper::GetVector<type>()
+
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf2, uint64_t, 64, 32, ARRAY(int32_t),
+                               static_cast<uint64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf4, uint64_t, 64, 16, ARRAY(int16_t),
+                               static_cast<uint64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf8, uint64_t, 64, 8, ARRAY(int8_t),
+                               static_cast<uint64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf2, uint32_t, 32, 16, ARRAY(int16_t),
+                               static_cast<uint32_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf4, uint32_t, 32, 8, ARRAY(int8_t),
+                               static_cast<uint32_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vzext_vf2, uint16_t, 16, 8, ARRAY(int8_t),
+                               static_cast<uint16_t>(dst[i]))
+
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf2, int64_t, 64, 32, ARRAY(int32_t),
+                               static_cast<int64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf4, int64_t, 64, 16, ARRAY(int16_t),
+                               static_cast<int64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf8, int64_t, 64, 8, ARRAY(int8_t),
+                               static_cast<int64_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf2, int32_t, 32, 16, ARRAY(int16_t),
+                               static_cast<int32_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf4, int32_t, 32, 8, ARRAY(int8_t),
+                               static_cast<int32_t>(dst[i]))
+UTEST_RVV_VI_VIE_FORM_WITH_RES(vsext_vf2, int16_t, 16, 8, ARRAY(int8_t),
+                               static_cast<int16_t>(dst[i]))
+
+#undef UTEST_RVV_VI_VIE_FORM_WITH_RES
+
+// Tests for vector permutation instructions vector slide instructions
+#define UTEST_RVV_VP_VS_VI_FORM_WITH_RES(instr_name, type, width, array, \
+                                         expect_res)                     \
+  TEST(RISCV_UTEST_##instr_name##_##type) {                              \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                   \
+    constexpr uint32_t n = kRvvVLEN / width;                             \
+    CcTest::InitializeVM();                                              \
+    for (type x : array) {                                               \
+      for (uint32_t offset = 0; offset < n; offset++) {                  \
+        type src[n] = {0};                                               \
+        type dst[n] = {0};                                               \
+        for (uint32_t i = 0; i < n; i++) src[i] = x + i;                 \
+        auto fn = [offset](MacroAssembler& assm) {                       \
+          __ VU.set(t0, VSew::E##width, Vlmul::m1);                      \
+          __ vl(v1, a0, 0, VSew::E##width);                              \
+          __ instr_name(v2, v1, offset);                                 \
+          __ vs(v2, a1, 0, VSew::E##width);                              \
+        };                                                               \
+        GenAndRunTest<int64_t, int64_t>((int64_t)src, (int64_t)dst, fn); \
+        for (uint32_t i = 0; i < n; i++) {                               \
+          CHECK_EQ(expect_res, dst[i]);                                  \
+        }                                                                \
+      }                                                                  \
+    }                                                                    \
+  }
+
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, int64_t, 64, ARRAY(int64_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, int32_t, 32, ARRAY(int32_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, int16_t, 16, ARRAY(int16_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, int8_t, 8, ARRAY(int8_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, uint32_t, 32, ARRAY(uint32_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, uint16_t, 16, ARRAY(uint16_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslidedown_vi, uint8_t, 8, ARRAY(uint8_t),
+                                 (i + offset) < n ? src[i + offset] : 0)
+
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, int64_t, 64, ARRAY(int64_t),
+                                 i < offset ? dst[i] : src[i - offset])
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, int32_t, 32, ARRAY(int32_t),
+                                 i < offset ? dst[i] : src[i - offset])
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, int16_t, 16, ARRAY(int16_t),
+                                 i < offset ? dst[i] : src[i - offset])
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, int8_t, 8, ARRAY(int8_t),
+                                 i < offset ? dst[i] : src[i - offset])
+
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, uint32_t, 32, ARRAY(uint32_t),
+                                 i < offset ? dst[i] : src[i - offset])
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, uint16_t, 16, ARRAY(uint16_t),
+                                 i < offset ? dst[i] : src[i - offset])
+UTEST_RVV_VP_VS_VI_FORM_WITH_RES(vslideup_vi, uint8_t, 8, ARRAY(uint8_t),
+                                 i < offset ? dst[i] : src[i - offset])
+
+#undef UTEST_RVV_VP_VS_VI_FORM_WITH_RES
+#undef ARRAY
+
+#define UTEST_VFIRST_M_WITH_WIDTH(width)                            \
+  TEST(RISCV_UTEST_vfirst_m_##width) {                              \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;              \
+    constexpr uint32_t vlen = 128;                                  \
+    constexpr uint32_t n = vlen / width;                            \
+    CcTest::InitializeVM();                                         \
+    for (uint32_t i = 0; i <= n; i++) {                             \
+      uint64_t src[2] = {0};                                        \
+      src[0] = 1 << i;                                              \
+      auto fn = [](MacroAssembler& assm) {                          \
+        __ VU.set(t0, VSew::E##width, Vlmul::m1);                   \
+        __ vl(v2, a0, 0, VSew::E##width);                           \
+        __ vfirst_m(a0, v2);                                        \
+      };                                                            \
+      auto res = GenAndRunTest<int64_t, int64_t>((int64_t)src, fn); \
+      CHECK_EQ(i < n ? i : (int64_t)-1, res);                       \
+    }                                                               \
+  }
+
+UTEST_VFIRST_M_WITH_WIDTH(64)
+UTEST_VFIRST_M_WITH_WIDTH(32)
+UTEST_VFIRST_M_WITH_WIDTH(16)
+UTEST_VFIRST_M_WITH_WIDTH(8)
+
+#undef UTEST_VFIRST_M_WITH_WIDTH
+
+#define UTEST_VCPOP_M_WITH_WIDTH(width)                               \
+  TEST(RISCV_UTEST_vcpop_m_##width) {                                 \
+    if (!CpuFeatures::IsSupported(RISCV_SIMD)) return;                \
+    uint32_t vlen = 128;                                              \
+    uint32_t n = vlen / width;                                        \
+    CcTest::InitializeVM();                                           \
+    for (uint16_t x : compiler::ValueHelper::GetVector<uint16_t>()) { \
+      uint64_t src[2] = {0};                                          \
+      src[0] = x >> (16 - n);                                         \
+      auto fn = [](MacroAssembler& assm) {                            \
+        __ VU.set(t0, VSew::E##width, Vlmul::m1);                     \
+        __ vl(v2, a0, 0, VSew::E##width);                             \
+        __ vcpop_m(a0, v2);                                           \
+      };                                                              \
+      auto res = GenAndRunTest<int64_t, int64_t>((int64_t)src, fn);   \
+      CHECK_EQ(std::__popcount(src[0]), res);                         \
+    }                                                                 \
+  }
+
+UTEST_VCPOP_M_WITH_WIDTH(64)
+UTEST_VCPOP_M_WITH_WIDTH(32)
+UTEST_VCPOP_M_WITH_WIDTH(16)
+UTEST_VCPOP_M_WITH_WIDTH(8)
+
+#undef UTEST_VCPOP_M_WITH_WIDTH
+#endif  // CAN_USE_RVV_INSTRUCTIONS
+#undef __
+}  // namespace internal
+}  // namespace v8
diff --git a/test/cctest/test-helper-riscv32.cc b/test/cctest/test-helper-riscv32.cc
new file mode 100644
index 00000000000..b46aebceaa5
--- /dev/null
+++ b/test/cctest/test-helper-riscv32.cc
@@ -0,0 +1,49 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "test/cctest/test-helper-riscv32.h"
+
+#include "src/codegen/macro-assembler.h"
+#include "src/execution/isolate-inl.h"
+#include "src/init/v8.h"
+#include "test/cctest/cctest.h"
+
+namespace v8 {
+namespace internal {
+
+int32_t GenAndRunTest(Func test_generator) {
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  test_generator(assm);
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<int32_t()>::FromCode(*code);
+  return f.Call();
+}
+
+Handle<Code> AssembleCodeImpl(Func assemble) {
+  Isolate* isolate = CcTest::i_isolate();
+  MacroAssembler assm(isolate, CodeObjectRequired::kYes);
+
+  assemble(assm);
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  if (FLAG_print_code) {
+    code->Print();
+  }
+  return code;
+}
+
+}  // namespace internal
+}  // namespace v8
diff --git a/test/cctest/test-helper-riscv32.h b/test/cctest/test-helper-riscv32.h
new file mode 100644
index 00000000000..b5c2f7730b9
--- /dev/null
+++ b/test/cctest/test-helper-riscv32.h
@@ -0,0 +1,337 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_CCTEST_TEST_HELPER_RISCV_H_
+#define V8_CCTEST_TEST_HELPER_RISCV_H_
+
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/diagnostics/disassembler.h"
+#include "src/execution/simulator.h"
+#include "src/heap/factory.h"
+#include "src/init/v8.h"
+#include "src/utils/utils.h"
+#include "test/cctest/cctest.h"
+
+#define PRINT_RES(res, expected_res, in_hex)                         \
+  if (in_hex) std::cout << "[hex-form]" << std::hex;                 \
+  std::cout << "res = " << (res) << " expected = " << (expected_res) \
+            << std::endl;
+
+namespace v8 {
+namespace internal {
+
+using Func = std::function<void(MacroAssembler&)>;
+
+int32_t GenAndRunTest(Func test_generator);
+
+// f.Call(...) interface is implemented as varargs in V8. For varargs,
+// floating-point arguments and return values are passed in GPRs, therefore
+// the special handling to reinterpret floating-point as integer values when
+// passed in and out of f.Call()
+template <typename OUTPUT_T, typename INPUT_T>
+OUTPUT_T GenAndRunTest(INPUT_T input0, Func test_generator) {
+  DCHECK((sizeof(INPUT_T) == 4 || sizeof(INPUT_T) == 8));
+
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // handle floating-point parameters
+  if (std::is_same<float, INPUT_T>::value) {
+    assm.fmv_w_x(fa0, a0);
+  } else if (std::is_same<double, INPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  test_generator(assm);
+
+  // handle floating-point result
+  if (std::is_same<float, OUTPUT_T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, OUTPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+
+  using OINT_T = typename std::conditional<
+      std::is_integral<OUTPUT_T>::value, OUTPUT_T,
+      typename std::conditional<sizeof(OUTPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+  using IINT_T = typename std::conditional<
+      std::is_integral<INPUT_T>::value, INPUT_T,
+      typename std::conditional<sizeof(INPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+
+  auto f = GeneratedCode<OINT_T(IINT_T)>::FromCode(*code);
+
+  auto res = f.Call(base::bit_cast<IINT_T>(input0));
+  return base::bit_cast<OUTPUT_T>(res);
+}
+
+template <typename OUTPUT_T, typename INPUT_T>
+OUTPUT_T GenAndRunTest(INPUT_T input0, INPUT_T input1, Func test_generator) {
+  DCHECK((sizeof(INPUT_T) == 4 || sizeof(INPUT_T) == 8));
+
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // handle floating-point parameters
+  if (std::is_same<float, INPUT_T>::value) {
+    assm.fmv_w_x(fa0, a0);
+    assm.fmv_w_x(fa1, a1);
+  } else if (std::is_same<double, INPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  test_generator(assm);
+
+  // handle floating-point result
+  if (std::is_same<float, OUTPUT_T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, OUTPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+
+  using OINT_T = typename std::conditional<
+      std::is_integral<OUTPUT_T>::value, OUTPUT_T,
+      typename std::conditional<sizeof(OUTPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+  using IINT_T = typename std::conditional<
+      std::is_integral<INPUT_T>::value, INPUT_T,
+      typename std::conditional<sizeof(INPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+  auto f = GeneratedCode<OINT_T(IINT_T, IINT_T)>::FromCode(*code);
+
+  auto res =
+      f.Call(base::bit_cast<IINT_T>(input0), base::bit_cast<IINT_T>(input1));
+  return base::bit_cast<OUTPUT_T>(res);
+}
+
+template <typename OUTPUT_T, typename INPUT_T>
+OUTPUT_T GenAndRunTest(INPUT_T input0, INPUT_T input1, INPUT_T input2,
+                       Func test_generator) {
+  DCHECK((sizeof(INPUT_T) == 4 || sizeof(INPUT_T) == 8));
+
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // handle floating-point parameters
+  if (std::is_same<float, INPUT_T>::value) {
+    assm.fmv_w_x(fa0, a0);
+    assm.fmv_w_x(fa1, a1);
+    assm.fmv_w_x(fa2, a2);
+  } else if (std::is_same<double, INPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  test_generator(assm);
+
+  // handle floating-point result
+  if (std::is_same<float, OUTPUT_T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, OUTPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+
+  using OINT_T = typename std::conditional<
+      std::is_integral<OUTPUT_T>::value, OUTPUT_T,
+      typename std::conditional<sizeof(OUTPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+  using IINT_T = typename std::conditional<
+      std::is_integral<INPUT_T>::value, INPUT_T,
+      typename std::conditional<sizeof(INPUT_T) == 4, int32_t,
+                                int64_t>::type>::type;
+  auto f = GeneratedCode<OINT_T(IINT_T, IINT_T, IINT_T)>::FromCode(*code);
+
+  auto res =
+      f.Call(base::bit_cast<IINT_T>(input0), base::bit_cast<IINT_T>(input1),
+             base::bit_cast<IINT_T>(input2));
+  return base::bit_cast<OUTPUT_T>(res);
+}
+
+template <typename T>
+void GenAndRunTestForLoadStore(T value, Func test_generator) {
+  DCHECK(sizeof(T) == 4 || sizeof(T) == 8);
+
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  if (std::is_same<float, T>::value) {
+    assm.fmv_w_x(fa0, a1);
+  } else if (std::is_same<double, T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  test_generator(assm);
+
+  if (std::is_same<float, T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, T>::value) {
+    UNIMPLEMENTED();
+  }
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+
+  using INT_T = typename std::conditional<
+      std::is_integral<T>::value, T,
+      typename std::conditional<sizeof(T) == 4, int32_t, int64_t>::type>::type;
+
+  auto f = GeneratedCode<INT_T(void* base, INT_T val)>::FromCode(*code);
+
+  int64_t tmp = 0;
+  auto res = f.Call(&tmp, base::bit_cast<INT_T>(value));
+  CHECK_EQ(base::bit_cast<T>(res), value);
+}
+
+template <typename T, typename Func>
+void GenAndRunTestForLRSC(T value, Func test_generator) {
+  DCHECK(sizeof(T) == 4 || sizeof(T) == 8);
+
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  if (std::is_same<float, T>::value) {
+    assm.fmv_w_x(fa0, a1);
+  } else if (std::is_same<double, T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  if (std::is_same<int32_t, T>::value) {
+    assm.sw(a1, a0, 0);
+  } else if (std::is_same<int64_t, T>::value) {
+    UNREACHABLE();
+  }
+  test_generator(assm);
+
+  if (std::is_same<float, T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, T>::value) {
+    UNIMPLEMENTED();
+  }
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+#if defined(DEBUG)
+  code->Print();
+#endif
+  using INT_T =
+      typename std::conditional<sizeof(T) == 4, int32_t, int64_t>::type;
+
+  T tmp = 0;
+  auto f = GeneratedCode<INT_T(void* base, INT_T val)>::FromCode(*code);
+  auto res = f.Call(&tmp, base::bit_cast<T>(value));
+  CHECK_EQ(base::bit_cast<T>(res), static_cast<T>(0));
+}
+
+template <typename INPUT_T, typename OUTPUT_T, typename Func>
+OUTPUT_T GenAndRunTestForAMO(INPUT_T input0, INPUT_T input1,
+                             Func test_generator) {
+  DCHECK(sizeof(INPUT_T) == 4 || sizeof(INPUT_T) == 8);
+  DCHECK(sizeof(OUTPUT_T) == 4 || sizeof(OUTPUT_T) == 8);
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // handle floating-point parameters
+  if (std::is_same<float, INPUT_T>::value) {
+    assm.fmv_w_x(fa0, a1);
+    assm.fmv_w_x(fa1, a2);
+  } else if (std::is_same<double, INPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  // store base integer
+  if (std::is_same<int32_t, INPUT_T>::value ||
+      std::is_same<uint32_t, INPUT_T>::value) {
+    assm.sw(a1, a0, 0);
+  } else if (std::is_same<int64_t, INPUT_T>::value ||
+             std::is_same<uint64_t, INPUT_T>::value) {
+    UNREACHABLE();
+  }
+  test_generator(assm);
+
+  // handle floating-point result
+  if (std::is_same<float, OUTPUT_T>::value) {
+    assm.fmv_x_w(a0, fa0);
+  } else if (std::is_same<double, OUTPUT_T>::value) {
+    UNIMPLEMENTED();
+  }
+
+  // load written integer
+  if (std::is_same<int32_t, INPUT_T>::value ||
+      std::is_same<uint32_t, INPUT_T>::value) {
+    assm.lw(a0, a0, 0);
+  } else if (std::is_same<int64_t, INPUT_T>::value ||
+             std::is_same<uint64_t, INPUT_T>::value) {
+    UNREACHABLE();
+  }
+
+  assm.jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+#if defined(DEBUG)
+  code->Print();
+#endif
+  OUTPUT_T tmp = 0;
+  auto f =
+      GeneratedCode<OUTPUT_T(void* base, INPUT_T, INPUT_T)>::FromCode(*code);
+  auto res = f.Call(&tmp, base::bit_cast<INPUT_T>(input0),
+                    base::bit_cast<INPUT_T>(input1));
+  return base::bit_cast<OUTPUT_T>(res);
+}
+
+Handle<Code> AssembleCodeImpl(Func assemble);
+
+template <typename Signature>
+GeneratedCode<Signature> AssembleCode(Func assemble) {
+  return GeneratedCode<Signature>::FromCode(*AssembleCodeImpl(assemble));
+}
+
+template <typename T>
+T UseCanonicalNan(T x) {
+  return isnan(x) ? std::numeric_limits<T>::quiet_NaN() : x;
+}
+
+}  // namespace internal
+}  // namespace v8
+
+#endif  // V8_CCTEST_TEST_HELPER_RISCV_H_
diff --git a/test/cctest/test-icache.cc b/test/cctest/test-icache.cc
index 5136c415f7f..f0e497e6f73 100644
--- a/test/cctest/test-icache.cc
+++ b/test/cctest/test-icache.cc
@@ -68,6 +68,10 @@ static void FloodWithInc(Isolate* isolate, TestingAssemblerBuffer* buffer) {
   for (int i = 0; i < kNumInstr; ++i) {
     __ agfi(r2, Operand(1));
   }
+#elif V8_TARGET_ARCH_RISCV32
+  for (int i = 0; i < kNumInstr; ++i) {
+    __ Add32(a0, a0, Operand(1));
+  }
 #elif V8_TARGET_ARCH_RISCV64
   for (int i = 0; i < kNumInstr; ++i) {
     __ Add32(a0, a0, Operand(1));
diff --git a/test/cctest/test-macro-assembler-riscv32.cc b/test/cctest/test-macro-assembler-riscv32.cc
new file mode 100644
index 00000000000..f5430b99067
--- /dev/null
+++ b/test/cctest/test-macro-assembler-riscv32.cc
@@ -0,0 +1,1324 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+#include <stdlib.h>
+
+#include <iostream>
+
+#include "src/base/utils/random-number-generator.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/deoptimizer/deoptimizer.h"
+#include "src/execution/simulator.h"
+#include "src/init/v8.h"
+#include "src/objects/heap-number.h"
+#include "src/objects/objects-inl.h"
+#include "src/utils/ostreams.h"
+#include "test/cctest/cctest.h"
+#include "test/cctest/compiler/value-helper.h"
+#include "test/cctest/test-helper-riscv32.h"
+#include "test/common/assembler-tester.h"
+
+namespace v8 {
+namespace internal {
+
+const float qnan_f = std::numeric_limits<float>::quiet_NaN();
+const float snan_f = std::numeric_limits<float>::signaling_NaN();
+const double qnan_d = std::numeric_limits<double>::quiet_NaN();
+// const double snan_d = std::numeric_limits<double>::signaling_NaN();
+
+const float inf_f = std::numeric_limits<float>::infinity();
+const double inf_d = std::numeric_limits<double>::infinity();
+const float minf_f = -inf_f;
+const double minf_d = -inf_d;
+
+using FV = void*(int32_t x, int32_t y, int p2, int p3, int p4);
+using F1 = void*(int x, int p1, int p2, int p3, int p4);
+using F3 = void*(void* p, int p1, int p2, int p3, int p4);
+using F4 = void*(void* p0, void* p1, int p2, int p3, int p4);
+
+#define __ masm.
+static uint32_t run_CalcScaledAddress(uint32_t rt, uint32_t rs, int8_t sa) {
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  auto fn = [sa](MacroAssembler& masm) {
+    __ CalcScaledAddress(a0, a0, a1, sa);
+  };
+  auto f = AssembleCode<FV>(fn);
+
+  uint32_t res = reinterpret_cast<uint32_t>(f.Call(rt, rs, 0, 0, 0));
+
+  return res;
+}
+
+template <typename VTYPE, typename Func>
+VTYPE run_Unaligned(char* memory_buffer, int32_t in_offset, int32_t out_offset,
+                    VTYPE value, Func GenerateUnalignedInstructionFunc) {
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  auto fn = [in_offset, out_offset,
+             GenerateUnalignedInstructionFunc](MacroAssembler& masm) {
+    GenerateUnalignedInstructionFunc(masm, in_offset, out_offset);
+  };
+  auto f = AssembleCode<int32_t(char*)>(fn);
+
+  MemCopy(memory_buffer + in_offset, &value, sizeof(VTYPE));
+  f.Call(memory_buffer);
+  VTYPE res;
+  MemCopy(&res, memory_buffer + out_offset, sizeof(VTYPE));
+
+  return res;
+}
+
+static const std::vector<int32_t> unsigned_test_offset() {
+  static const int32_t kValues[] = {// value, offset
+                                    -132 * KB, -21 * KB, 0, 19 * KB, 135 * KB};
+  return std::vector<int32_t>(&kValues[0], &kValues[arraysize(kValues)]);
+}
+
+static const std::vector<int32_t> unsigned_test_offset_increment() {
+  static const int32_t kValues[] = {-7, -6, -5, -4, -3, -2, -1, 0,
+                                    1,  2,  3,  4,  5,  6,  7};
+  return std::vector<int32_t>(&kValues[0], &kValues[arraysize(kValues)]);
+}
+
+TEST(LoadConstants) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope handles(isolate);
+
+  int32_t refConstants[32];
+  int32_t result[32];
+
+  int32_t mask = 1;
+  for (int i = 0; i < 32; i++) {
+    refConstants[i] = ~(mask << i);
+  }
+
+  auto fn = [&refConstants](MacroAssembler& masm) {
+    __ mv(a4, a0);
+    for (int i = 0; i < 32; i++) {
+      // Load constant.
+      __ li(a5, Operand(refConstants[i]));
+      __ Sw(a5, MemOperand(a4));
+      __ AddWord(a4, a4, Operand(kSystemPointerSize));
+    }
+  };
+  auto f = AssembleCode<FV>(fn);
+
+  (void)f.Call(reinterpret_cast<int32_t>(result), 0, 0, 0, 0);
+  // Check results.
+  for (int i = 0; i < 32; i++) {
+    CHECK(refConstants[i] == result[i]);
+  }
+}
+
+TEST(LoadAddress) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope handles(isolate);
+
+  MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes);
+  Label to_jump, skip;
+  __ mv(a4, a0);
+
+  __ Branch(&skip);
+  __ bind(&to_jump);
+  __ nop();
+  __ nop();
+  __ jr(ra);
+  __ nop();
+  __ bind(&skip);
+  __ li(a4,
+        Operand(masm.jump_address(&to_jump),
+                RelocInfo::INTERNAL_REFERENCE_ENCODED),
+        ADDRESS_LOAD);
+  int check_size = masm.InstructionsGeneratedSince(&skip);
+  // NOTE (RISCV): current li generates 6 instructions, if the sequence is
+  // changed, need to adjust the CHECK_EQ value too
+  CHECK_EQ(2, check_size);
+  __ jr(a4);
+  __ nop();
+  __ stop();
+  __ stop();
+  __ stop();
+  __ stop();
+  __ stop();
+
+  CodeDesc desc;
+  masm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<FV>::FromCode(*code);
+
+  (void)f.Call(0, 0, 0, 0, 0);
+  // Check results.
+}
+
+TEST(jump_tables4) {
+  // Similar to test-assembler-mips jump_tables1, with extra test for branch
+  // trampoline required before emission of the dd table (where trampolines are
+  // blocked), and proper transition to long-branch mode.
+  // Regression test for v8:4294.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+  MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  const int kNumCases = 128;
+  int32_t values[kNumCases];
+  isolate->random_number_generator()->NextBytes(values, sizeof(values));
+  Label labels[kNumCases];
+  Label near_start, end, done;
+
+  __ Push(ra);
+  __ mv(a1, zero_reg);
+
+  __ Branch(&end);
+  __ bind(&near_start);
+
+  // Generate slightly less than 32K instructions, which will soon require
+  // trampoline for branch distance fixup.
+  for (int i = 0; i < 32768 - 256; ++i) {
+    __ addi(a1, a1, 1);
+  }
+
+  __ GenerateSwitchTable(a0, kNumCases,
+                         [&labels](size_t i) { return labels + i; });
+
+  for (int i = 0; i < kNumCases; ++i) {
+    __ bind(&labels[i]);
+    __ RV_li(a0, values[i]);
+    __ Branch(&done);
+  }
+
+  __ bind(&done);
+  __ Pop(ra);
+  __ jr(ra);
+
+  __ bind(&end);
+  __ Branch(&near_start);
+
+  CodeDesc desc;
+  masm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+#ifdef OBJECT_PRINT
+  code->Print(std::cout);
+#endif
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  for (int i = 0; i < kNumCases; ++i) {
+    int32_t res = reinterpret_cast<int32_t>(f.Call(i, 0, 0, 0, 0));
+    // ::printf("f(%d) = %" PRId64 "\n", i, res);
+    CHECK_EQ(values[i], res);
+  }
+}
+
+TEST(jump_tables6) {
+  // Similar to test-assembler-mips jump_tables1, with extra test for branch
+  // trampoline required after emission of the dd table (where trampolines are
+  // blocked). This test checks if number of really generated instructions is
+  // greater than number of counted instructions from code, as we are expecting
+  // generation of trampoline in this case (when number of kFillInstr
+  // instructions is close to 32K)
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+  MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  const int kSwitchTableCases = 40;
+
+  const int kMaxBranchOffset = Assembler::kMaxBranchOffset;
+  const int kTrampolineSlotsSize = Assembler::kTrampolineSlotsSize;
+  const int kSwitchTablePrologueSize = MacroAssembler::kSwitchTablePrologueSize;
+
+  const int kMaxOffsetForTrampolineStart =
+      kMaxBranchOffset - 16 * kTrampolineSlotsSize;
+  const int kFillInstr = (kMaxOffsetForTrampolineStart / kInstrSize) -
+                         (kSwitchTablePrologueSize + 2 * kSwitchTableCases) -
+                         20;
+
+  int values[kSwitchTableCases];
+  isolate->random_number_generator()->NextBytes(values, sizeof(values));
+  Label labels[kSwitchTableCases];
+  Label near_start, end, done;
+
+  __ Push(ra);
+  __ mv(a1, zero_reg);
+
+  int offs1 = masm.pc_offset();
+  int gen_insn = 0;
+
+  __ Branch(&end);
+  gen_insn += 1;
+  __ bind(&near_start);
+
+  // Generate slightly less than 32K instructions, which will soon require
+  // trampoline for branch distance fixup.
+  for (int i = 0; i < kFillInstr; ++i) {
+    __ addi(a1, a1, 1);
+  }
+  gen_insn += kFillInstr;
+
+  __ GenerateSwitchTable(a0, kSwitchTableCases,
+                         [&labels](int i) { return labels + i; });
+  gen_insn += (kSwitchTablePrologueSize + 1 * kSwitchTableCases);
+
+  for (int i = 0; i < kSwitchTableCases; ++i) {
+    __ bind(&labels[i]);
+    __ li(a0, Operand(values[i]));
+    __ Branch(&done);
+  }
+  gen_insn += 3 * kSwitchTableCases;
+
+  // If offset from here to first branch instr is greater than max allowed
+  // offset for trampoline ...
+  CHECK_LT(kMaxOffsetForTrampolineStart, masm.pc_offset() - offs1);
+  // ... number of generated instructions must be greater then "gen_insn",
+  // as we are expecting trampoline generation
+  CHECK_LT(gen_insn, (masm.pc_offset() - offs1) / kInstrSize);
+
+  __ bind(&done);
+  __ Pop(ra);
+  __ jr(ra);
+  __ nop();
+
+  __ bind(&end);
+  __ Branch(&near_start);
+
+  CodeDesc desc;
+  masm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+#ifdef OBJECT_PRINT
+  code->Print(std::cout);
+#endif
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  for (int i = 0; i < kSwitchTableCases; ++i) {
+    int32_t res = reinterpret_cast<int32_t>(f.Call(i, 0, 0, 0, 0));
+    ::printf("f(%d) = %" PRId32 "\n", i, res);
+    CHECK_EQ(values[i], res);
+  }
+}
+
+TEST(CalcScaledAddress) {
+  CcTest::InitializeVM();
+  struct TestCaseLsa {
+    int32_t rt;
+    int32_t rs;
+    uint8_t sa;
+    uint32_t expected_res;
+  };
+
+  struct TestCaseLsa tc[] = {// rt, rs, sa, expected_res
+                             {0x4, 0x1, 1, 0x6},
+                             {0x4, 0x1, 2, 0x8},
+                             {0x4, 0x1, 3, 0xC},
+                             {0x4, 0x1, 4, 0x14},
+                             {0x4, 0x1, 5, 0x24},
+                             {0x0, 0x1, 1, 0x2},
+                             {0x0, 0x1, 2, 0x4},
+                             {0x0, 0x1, 3, 0x8},
+                             {0x0, 0x1, 4, 0x10},
+                             {0x0, 0x1, 5, 0x20},
+                             {0x4, 0x0, 1, 0x4},
+                             {0x4, 0x0, 2, 0x4},
+                             {0x4, 0x0, 3, 0x4},
+                             {0x4, 0x0, 4, 0x4},
+                             {0x4, 0x0, 5, 0x4},
+
+                             // Shift overflow.
+                             {0x4, INT32_MAX, 1, 0x2},
+                             {0x4, INT32_MAX >> 1, 2, 0x0},
+                             {0x4, INT32_MAX >> 2, 3, 0xFFFFFFFC},
+                             {0x4, INT32_MAX >> 3, 4, 0xFFFFFFF4},
+                             {0x4, INT32_MAX >> 4, 5, 0xFFFFFFE4},
+
+                             // Signed addition overflow.
+                             {INT32_MAX - 1, 0x1, 1, 0x80000000},
+                             {INT32_MAX - 3, 0x1, 2, 0x80000000},
+                             {INT32_MAX - 7, 0x1, 3, 0x80000000},
+                             {INT32_MAX - 15, 0x1, 4, 0x80000000},
+                             {INT32_MAX - 31, 0x1, 5, 0x80000000},
+
+                             // Addition overflow.
+                             {-2, 0x1, 1, 0x0},
+                             {-4, 0x1, 2, 0x0},
+                             {-8, 0x1, 3, 0x0},
+                             {-16, 0x1, 4, 0x0},
+                             {-32, 0x1, 5, 0x0}};
+
+  size_t nr_test_cases = sizeof(tc) / sizeof(TestCaseLsa);
+  for (size_t i = 0; i < nr_test_cases; ++i) {
+    uint32_t res = run_CalcScaledAddress(tc[i].rt, tc[i].rs, tc[i].sa);
+    CHECK_EQ(tc[i].expected_res, res);
+  }
+}
+
+static const std::vector<uint32_t> cvt_trunc_uint32_test_values() {
+  static const uint32_t kValues[] = {0x00000000, 0x00000001, 0x00FFFF00,
+                                     0x7FFFFFFF, 0x80000000, 0x80000001,
+                                     0x80FFFF00, 0x8FFFFFFF};
+  return std::vector<uint32_t>(&kValues[0], &kValues[arraysize(kValues)]);
+}
+
+static const std::vector<int32_t> cvt_trunc_int32_test_values() {
+  static const int32_t kValues[] = {
+      static_cast<int32_t>(0x00000000), static_cast<int32_t>(0x00000001),
+      static_cast<int32_t>(0x00FFFF00), static_cast<int32_t>(0x7FFFFFFF),
+      static_cast<int32_t>(0x80000000), static_cast<int32_t>(0x80000001),
+      static_cast<int32_t>(0x80FFFF00), static_cast<int32_t>(0x8FFFFFFF),
+      static_cast<int32_t>(0xFFFFFFFF)};
+  return std::vector<int32_t>(&kValues[0], &kValues[arraysize(kValues)]);
+}
+
+#define FOR_INPUTS3(ctype, var, test_vector)    \
+  std::vector<ctype> var##_vec = test_vector(); \
+  for (ctype var : var##_vec)
+
+#define FOR_INT32_INPUTS3(var, test_vector) \
+  FOR_INPUTS3(int32_t, var, test_vector)
+#define FOR_INT64_INPUTS3(var, test_vector) \
+  FOR_INPUTS3(int64_t, var, test_vector)
+#define FOR_UINT32_INPUTS3(var, test_vector) \
+  FOR_INPUTS3(uint32_t, var, test_vector)
+#define FOR_UINT64_INPUTS3(var, test_vector) \
+  FOR_INPUTS3(uint64_t, var, test_vector)
+
+#define FOR_TWO_INPUTS(ctype, var1, var2, test_vector)      \
+  std::vector<ctype> var##_vec = test_vector();             \
+  std::vector<ctype>::iterator var1;                        \
+  std::vector<ctype>::reverse_iterator var2;                \
+  for (var1 = var##_vec.begin(), var2 = var##_vec.rbegin(); \
+       var1 != var##_vec.end(); ++var1, ++var2)
+
+#define FOR_INT32_TWO_INPUTS(var1, var2, test_vector) \
+  FOR_TWO_INPUTS(int32_t, var1, var2, test_vector)
+
+TEST(Cvt_s_uw_Trunc_uw_s) {
+  CcTest::InitializeVM();
+  auto fn = [](MacroAssembler& masm) {
+    __ Cvt_s_uw(fa0, a0);
+    __ Trunc_uw_s(a0, fa0);
+  };
+  FOR_UINT32_INPUTS3(i, cvt_trunc_uint32_test_values) {
+    // some integers cannot be represented precisely in float,  input may
+    // not directly match the return value of GenAndRunTest
+    CHECK_EQ(static_cast<uint32_t>(static_cast<float>(i)),
+             GenAndRunTest<uint32_t>(i, fn));
+  }
+}
+
+TEST(cvt_d_w_Trunc_w_d) {
+  CcTest::InitializeVM();
+  auto fn = [](MacroAssembler& masm) {
+    __ fcvt_d_w(fa0, a0);
+    __ Trunc_w_d(a0, fa0);
+  };
+  FOR_INT32_INPUTS3(i, cvt_trunc_int32_test_values) {
+    CHECK_EQ(static_cast<int32_t>(static_cast<double>(i)),
+             GenAndRunTest<int32_t>(i, fn));
+  }
+}
+
+static const std::vector<int32_t> overflow_int32_test_values() {
+  static const int32_t kValues[] = {
+      static_cast<int32_t>(0xF0000000), static_cast<int32_t>(0x00000001),
+      static_cast<int32_t>(0xFF000000), static_cast<int32_t>(0x0F000011),
+      static_cast<int32_t>(0x00F00100), static_cast<int32_t>(0x991234AB),
+      static_cast<int32_t>(0xB0FFFF0F), static_cast<int32_t>(0x6FFFFFFF),
+      static_cast<int32_t>(0xFFFFFFFF)};
+  return std::vector<int32_t>(&kValues[0], &kValues[arraysize(kValues)]);
+}
+
+TEST(OverflowInstructions) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope handles(isolate);
+
+  struct T {
+    int32_t lhs;
+    int32_t rhs;
+    int32_t output_add;
+    int32_t output_add2;
+    int32_t output_sub;
+    int32_t output_sub2;
+    int32_t output_mul;
+    int32_t output_mul2;
+    int32_t overflow_add;
+    int32_t overflow_add2;
+    int32_t overflow_sub;
+    int32_t overflow_sub2;
+    int32_t overflow_mul;
+    int32_t overflow_mul2;
+  } t;
+
+  FOR_INT32_INPUTS3(i, overflow_int32_test_values) {
+    FOR_INT32_INPUTS3(j, overflow_int32_test_values) {
+      auto ii = i;
+      auto jj = j;
+      int32_t expected_add, expected_sub;
+      int32_t ii32 = static_cast<int32_t>(ii);
+      int32_t jj32 = static_cast<int32_t>(jj);
+      int32_t expected_mul;
+      int32_t expected_add_ovf, expected_sub_ovf, expected_mul_ovf;
+
+      auto fn = [](MacroAssembler& masm) {
+        __ Lw(t0, MemOperand(a0, offsetof(T, lhs)));
+        __ Lw(t1, MemOperand(a0, offsetof(T, rhs)));
+
+        __ AddOverflow(t2, t0, Operand(t1), a1);
+        __ Sw(t2, MemOperand(a0, offsetof(T, output_add)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_add)));
+        __ mv(a1, zero_reg);
+        __ AddOverflow(t0, t0, Operand(t1), a1);
+        __ Sw(t0, MemOperand(a0, offsetof(T, output_add2)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_add2)));
+
+        __ Lw(t0, MemOperand(a0, offsetof(T, lhs)));
+        __ Lw(t1, MemOperand(a0, offsetof(T, rhs)));
+
+        __ SubOverflow(t2, t0, Operand(t1), a1);
+        __ Sw(t2, MemOperand(a0, offsetof(T, output_sub)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_sub)));
+        __ mv(a1, zero_reg);
+        __ SubOverflow(t0, t0, Operand(t1), a1);
+        __ Sw(t0, MemOperand(a0, offsetof(T, output_sub2)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_sub2)));
+
+        __ Lw(t0, MemOperand(a0, offsetof(T, lhs)));
+        __ Lw(t1, MemOperand(a0, offsetof(T, rhs)));
+        __ MulOverflow32(t2, t0, Operand(t1), a1);
+        __ Sw(t2, MemOperand(a0, offsetof(T, output_mul)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_mul)));
+        __ mv(a1, zero_reg);
+        __ MulOverflow32(t0, t0, Operand(t1), a1);
+        __ Sw(t0, MemOperand(a0, offsetof(T, output_mul2)));
+        __ Sw(a1, MemOperand(a0, offsetof(T, overflow_mul2)));
+      };
+      auto f = AssembleCode<F3>(fn);
+
+      t.lhs = ii;
+      t.rhs = jj;
+      f.Call(&t, 0, 0, 0, 0);
+
+      expected_add_ovf = base::bits::SignedAddOverflow32(ii, jj, &expected_add);
+      expected_sub_ovf = base::bits::SignedSubOverflow32(ii, jj, &expected_sub);
+      expected_mul_ovf =
+          base::bits::SignedMulOverflow32(ii32, jj32, &expected_mul);
+
+      CHECK_EQ(expected_add_ovf, t.overflow_add < 0);
+      CHECK_EQ(expected_sub_ovf, t.overflow_sub < 0);
+      CHECK_EQ(expected_mul_ovf, t.overflow_mul != 0);
+
+      CHECK_EQ(t.overflow_add, t.overflow_add2);
+      CHECK_EQ(t.overflow_sub, t.overflow_sub2);
+      CHECK_EQ(t.overflow_mul, t.overflow_mul2);
+
+      CHECK_EQ(expected_add, t.output_add);
+      CHECK_EQ(expected_add, t.output_add2);
+      CHECK_EQ(expected_sub, t.output_sub);
+      CHECK_EQ(expected_sub, t.output_sub2);
+      if (!expected_mul_ovf) {
+        CHECK_EQ(expected_mul, t.output_mul);
+        CHECK_EQ(expected_mul, t.output_mul2);
+      }
+    }
+  }
+}
+
+TEST(min_max_nan) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct TestFloat {
+    double a;
+    double b;
+    double c;
+    double d;
+    float e;
+    float f;
+    float g;
+    float h;
+  } test;
+
+  const int kTableLength = 13;
+
+  double inputsa[kTableLength] = {2.0,   3.0,    -0.0,  0.0,    42.0,
+                                  inf_d, minf_d, inf_d, qnan_d, 3.0,
+                                  inf_d, qnan_d, qnan_d};
+  double inputsb[kTableLength] = {3.0,    2.0,   0.0,    -0.0, inf_d,
+                                  42.0,   inf_d, minf_d, 3.0,  qnan_d,
+                                  qnan_d, inf_d, qnan_d};
+  double outputsdmin[kTableLength] = {2.0,    2.0,    -0.0,   -0.0,   42.0,
+                                      42.0,   minf_d, minf_d, qnan_d, qnan_d,
+                                      qnan_d, qnan_d, qnan_d};
+  double outputsdmax[kTableLength] = {3.0,    3.0,    0.0,   0.0,    inf_d,
+                                      inf_d,  inf_d,  inf_d, qnan_d, qnan_d,
+                                      qnan_d, qnan_d, qnan_d};
+
+  float inputse[kTableLength] = {2.0,   3.0,    -0.0,  0.0,    42.0,
+                                 inf_f, minf_f, inf_f, qnan_f, 3.0,
+                                 inf_f, qnan_f, qnan_f};
+  float inputsf[kTableLength] = {3.0,    2.0,   0.0,    -0.0, inf_f,
+                                 42.0,   inf_f, minf_f, 3.0,  qnan_f,
+                                 qnan_f, inf_f, qnan_f};
+  float outputsfmin[kTableLength] = {2.0,    2.0,    -0.0,   -0.0,   42.0,
+                                     42.0,   minf_f, minf_f, qnan_f, qnan_f,
+                                     qnan_f, qnan_f, qnan_f};
+  float outputsfmax[kTableLength] = {3.0,    3.0,    0.0,   0.0,    inf_f,
+                                     inf_f,  inf_f,  inf_f, qnan_f, qnan_f,
+                                     qnan_f, qnan_f, qnan_f};
+
+  auto fn = [](MacroAssembler& masm) {
+    __ push(s6);
+    __ InitializeRootRegister();
+    __ LoadDouble(fa3, MemOperand(a0, offsetof(TestFloat, a)));
+    __ LoadDouble(fa4, MemOperand(a0, offsetof(TestFloat, b)));
+    __ LoadFloat(fa1, MemOperand(a0, offsetof(TestFloat, e)));
+    __ LoadFloat(fa2, MemOperand(a0, offsetof(TestFloat, f)));
+    __ Float64Min(fa5, fa3, fa4);
+    __ Float64Max(fa6, fa3, fa4);
+    __ Float32Min(fa7, fa1, fa2);
+    __ Float32Max(fa0, fa1, fa2);
+    __ StoreDouble(fa5, MemOperand(a0, offsetof(TestFloat, c)));
+    __ StoreDouble(fa6, MemOperand(a0, offsetof(TestFloat, d)));
+    __ StoreFloat(fa7, MemOperand(a0, offsetof(TestFloat, g)));
+    __ StoreFloat(fa0, MemOperand(a0, offsetof(TestFloat, h)));
+    __ pop(s6);
+  };
+  auto f = AssembleCode<F3>(fn);
+
+  for (int i = 0; i < kTableLength; i++) {
+    test.a = inputsa[i];
+    test.b = inputsb[i];
+    test.e = inputse[i];
+    test.f = inputsf[i];
+
+    f.Call(&test, 0, 0, 0, 0);
+
+    CHECK_EQ(0, memcmp(&test.c, &outputsdmin[i], sizeof(test.c)));
+    CHECK_EQ(0, memcmp(&test.d, &outputsdmax[i], sizeof(test.d)));
+    CHECK_EQ(0, memcmp(&test.g, &outputsfmin[i], sizeof(test.g)));
+    CHECK_EQ(0, memcmp(&test.h, &outputsfmax[i], sizeof(test.h)));
+  }
+}
+
+TEST(Ulh) {
+  CcTest::InitializeVM();
+
+  static const int kBufferSize = 300 * KB;
+  char memory_buffer[kBufferSize];
+  char* buffer_middle = memory_buffer + (kBufferSize / 2);
+
+  auto fn1 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ Ulh(t0, MemOperand(a0, in_offset));
+    __ Ush(t0, MemOperand(a0, out_offset));
+  };
+
+  auto fn2 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ mv(t0, a0);
+    __ Ulh(a0, MemOperand(a0, in_offset));
+    __ Ush(a0, MemOperand(t0, out_offset));
+  };
+
+  auto fn3 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ mv(t0, a0);
+    __ Ulhu(a0, MemOperand(a0, in_offset));
+    __ Ush(a0, MemOperand(t0, out_offset));
+  };
+
+  auto fn4 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ Ulhu(t0, MemOperand(a0, in_offset));
+    __ Ush(t0, MemOperand(a0, out_offset));
+  };
+
+  FOR_UINT16_INPUTS(i) {
+    FOR_INT32_TWO_INPUTS(j1, j2, unsigned_test_offset) {
+      FOR_INT32_TWO_INPUTS(k1, k2, unsigned_test_offset_increment) {
+        auto value = i;
+        int32_t in_offset = *j1 + *k1;
+        int32_t out_offset = *j2 + *k2;
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn1));
+
+        // test when loaded value overwrites base-register of load address
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn2));
+
+        // test when loaded value overwrites base-register of load address
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn3));
+
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn4));
+      }
+    }
+  }
+}
+
+TEST(Ulh_bitextension) {
+  CcTest::InitializeVM();
+
+  static const int kBufferSize = 300 * KB;
+  char memory_buffer[kBufferSize];
+  char* buffer_middle = memory_buffer + (kBufferSize / 2);
+
+  auto fn = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    Label success, fail, end, different;
+    __ Ulh(t0, MemOperand(a0, in_offset));
+    __ Ulhu(t1, MemOperand(a0, in_offset));
+    __ Branch(&different, ne, t0, Operand(t1));
+
+    // If signed and unsigned values are same, check
+    // the upper bits to see if they are zero
+    __ srai(t0, t0, 15);
+    __ Branch(&success, eq, t0, Operand(zero_reg));
+    __ Branch(&fail);
+
+    // If signed and unsigned values are different,
+    // check that the upper bits are complementary
+    __ bind(&different);
+    __ srai(t1, t1, 15);
+    __ Branch(&fail, ne, t1, Operand(1));
+    __ srai(t0, t0, 15);
+    __ addi(t0, t0, 1);
+    __ Branch(&fail, ne, t0, Operand(zero_reg));
+    // Fall through to success
+
+    __ bind(&success);
+    __ Ulh(t0, MemOperand(a0, in_offset));
+    __ Ush(t0, MemOperand(a0, out_offset));
+    __ Branch(&end);
+    __ bind(&fail);
+    __ Ush(zero_reg, MemOperand(a0, out_offset));
+    __ bind(&end);
+  };
+
+  FOR_UINT16_INPUTS(i) {
+    FOR_INT32_TWO_INPUTS(j1, j2, unsigned_test_offset) {
+      FOR_INT32_TWO_INPUTS(k1, k2, unsigned_test_offset_increment) {
+        auto value = i;
+        int32_t in_offset = *j1 + *k1;
+        int32_t out_offset = *j2 + *k2;
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn));
+      }
+    }
+  }
+}
+
+TEST(Ulw) {
+  CcTest::InitializeVM();
+
+  static const int kBufferSize = 300 * KB;
+  char memory_buffer[kBufferSize];
+  char* buffer_middle = memory_buffer + (kBufferSize / 2);
+
+  auto fn_1 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ Ulw(t0, MemOperand(a0, in_offset));
+    __ Usw(t0, MemOperand(a0, out_offset));
+  };
+
+  auto fn_2 = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ mv(t0, a0);
+    __ Ulw(a0, MemOperand(a0, in_offset));
+    __ Usw(a0, MemOperand(t0, out_offset));
+  };
+
+  FOR_UINT32_INPUTS(i) {
+    FOR_INT32_TWO_INPUTS(j1, j2, unsigned_test_offset) {
+      FOR_INT32_TWO_INPUTS(k1, k2, unsigned_test_offset_increment) {
+        auto value = i;
+        int32_t in_offset = *j1 + *k1;
+        int32_t out_offset = *j2 + *k2;
+
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn_1));
+        // test when loaded value overwrites base-register of load address
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn_2));
+      }
+    }
+  }
+}
+
+TEST(ULoadFloat) {
+  auto fn = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ ULoadFloat(fa0, MemOperand(a0, in_offset), t0);
+    __ UStoreFloat(fa0, MemOperand(a0, out_offset), t0);
+  };
+  CcTest::InitializeVM();
+
+  static const int kBufferSize = 300 * KB;
+  char memory_buffer[kBufferSize];
+  char* buffer_middle = memory_buffer + (kBufferSize / 2);
+
+  FOR_FLOAT32_INPUTS(i) {
+    // skip nan because CHECK_EQ cannot handle NaN
+    if (std::isnan(i)) continue;
+    FOR_INT32_TWO_INPUTS(j1, j2, unsigned_test_offset) {
+      FOR_INT32_TWO_INPUTS(k1, k2, unsigned_test_offset_increment) {
+        auto value = i;
+        int32_t in_offset = *j1 + *k1;
+        int32_t out_offset = *j2 + *k2;
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn));
+      }
+    }
+  }
+}
+
+TEST(ULoadDouble) {
+  CcTest::InitializeVM();
+
+  static const int kBufferSize = 300 * KB;
+  char memory_buffer[kBufferSize];
+  char* buffer_middle = memory_buffer + (kBufferSize / 2);
+
+  auto fn = [](MacroAssembler& masm, int32_t in_offset, int32_t out_offset) {
+    __ ULoadDouble(fa0, MemOperand(a0, in_offset), t0);
+    __ UStoreDouble(fa0, MemOperand(a0, out_offset), t0);
+  };
+
+  FOR_FLOAT64_INPUTS(i) {
+    // skip nan because CHECK_EQ cannot handle NaN
+    if (std::isnan(i)) continue;
+    FOR_INT32_TWO_INPUTS(j1, j2, unsigned_test_offset) {
+      FOR_INT32_TWO_INPUTS(k1, k2, unsigned_test_offset_increment) {
+        auto value = i;
+        int32_t in_offset = *j1 + *k1;
+        int32_t out_offset = *j2 + *k2;
+        CHECK_EQ(value, run_Unaligned(buffer_middle, in_offset, out_offset,
+                                      value, fn));
+      }
+    }
+  }
+}
+
+TEST(Sltu) {
+  CcTest::InitializeVM();
+
+  FOR_UINT32_INPUTS(i) {
+    FOR_UINT32_INPUTS(j) {
+      // compare against immediate value
+      auto fn_1 = [j](MacroAssembler& masm) { __ Sltu(a0, a0, Operand(j)); };
+      CHECK_EQ(i < j, GenAndRunTest<int32_t>(i, fn_1));
+      // compare against registers
+      auto fn_2 = [](MacroAssembler& masm) { __ Sltu(a0, a0, a1); };
+      CHECK_EQ(i < j, GenAndRunTest<int32_t>(i, j, fn_2));
+    }
+  }
+}
+
+template <typename T, typename Inputs, typename Results>
+static void GenerateMacroFloat32MinMax(MacroAssembler& masm) {
+  T a = T::from_code(5);  // ft5
+  T b = T::from_code(6);  // ft6
+  T c = T::from_code(7);  // ft7
+
+#define FLOAT_MIN_MAX(fminmax, res, x, y, res_field)        \
+  __ LoadFloat(x, MemOperand(a0, offsetof(Inputs, src1_))); \
+  __ LoadFloat(y, MemOperand(a0, offsetof(Inputs, src2_))); \
+  __ fminmax(res, x, y);                                    \
+  __ StoreFloat(res, MemOperand(a1, offsetof(Results, res_field)))
+
+  // a = min(b, c);
+  FLOAT_MIN_MAX(Float32Min, a, b, c, min_abc_);
+  // a = min(a, b);
+  FLOAT_MIN_MAX(Float32Min, a, a, b, min_aab_);
+  // a = min(b, a);
+  FLOAT_MIN_MAX(Float32Min, a, b, a, min_aba_);
+
+  // a = max(b, c);
+  FLOAT_MIN_MAX(Float32Max, a, b, c, max_abc_);
+  // a = max(a, b);
+  FLOAT_MIN_MAX(Float32Max, a, a, b, max_aab_);
+  // a = max(b, a);
+  FLOAT_MIN_MAX(Float32Max, a, b, a, max_aba_);
+
+#undef FLOAT_MIN_MAX
+}
+
+TEST(macro_float_minmax_f32) {
+  // Test the Float32Min and Float32Max macros.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct Inputs {
+    float src1_;
+    float src2_;
+  };
+
+  struct Results {
+    // Check all register aliasing possibilities in order to exercise all
+    // code-paths in the macro masm.
+    float min_abc_;
+    float min_aab_;
+    float min_aba_;
+    float max_abc_;
+    float max_aab_;
+    float max_aba_;
+  };
+
+  auto f = AssembleCode<F4>(
+      GenerateMacroFloat32MinMax<FPURegister, Inputs, Results>);
+
+#define CHECK_MINMAX(src1, src2, min, max)                                \
+  do {                                                                    \
+    Inputs inputs = {src1, src2};                                         \
+    Results results;                                                      \
+    f.Call(&inputs, &results, 0, 0, 0);                                   \
+    CHECK_EQ(base::bit_cast<uint32_t>(min),                               \
+             base::bit_cast<uint32_t>(results.min_abc_));                 \
+    CHECK_EQ(base::bit_cast<uint32_t>(min),                               \
+             base::bit_cast<uint32_t>(results.min_aab_));                 \
+    CHECK_EQ(base::bit_cast<uint32_t>(min),                               \
+             base::bit_cast<uint32_t>(results.min_aba_));                 \
+    CHECK_EQ(base::bit_cast<uint32_t>(max),                               \
+             base::bit_cast<uint32_t>(results.max_abc_));                 \
+    CHECK_EQ(base::bit_cast<uint32_t>(max),                               \
+             base::bit_cast<uint32_t>(results.max_aab_));                 \
+    CHECK_EQ(base::bit_cast<uint32_t>(max),                               \
+             base::bit_cast<uint32_t>(                                    \
+                 results.max_aba_)); /* Use a base::bit_cast to correctly \
+                                  identify -0.0 and NaNs. */              \
+  } while (0)
+
+  float nan_a = std::numeric_limits<float>::quiet_NaN();
+  float nan_b = std::numeric_limits<float>::quiet_NaN();
+
+  CHECK_MINMAX(1.0f, -1.0f, -1.0f, 1.0f);
+  CHECK_MINMAX(-1.0f, 1.0f, -1.0f, 1.0f);
+  CHECK_MINMAX(0.0f, -1.0f, -1.0f, 0.0f);
+  CHECK_MINMAX(-1.0f, 0.0f, -1.0f, 0.0f);
+  CHECK_MINMAX(-0.0f, -1.0f, -1.0f, -0.0f);
+  CHECK_MINMAX(-1.0f, -0.0f, -1.0f, -0.0f);
+  CHECK_MINMAX(0.0f, 1.0f, 0.0f, 1.0f);
+  CHECK_MINMAX(1.0f, 0.0f, 0.0f, 1.0f);
+
+  CHECK_MINMAX(0.0f, 0.0f, 0.0f, 0.0f);
+  CHECK_MINMAX(-0.0f, -0.0f, -0.0f, -0.0f);
+  CHECK_MINMAX(-0.0f, 0.0f, -0.0f, 0.0f);
+  CHECK_MINMAX(0.0f, -0.0f, -0.0f, 0.0f);
+
+  CHECK_MINMAX(0.0f, nan_a, nan_a, nan_a);
+  CHECK_MINMAX(nan_a, 0.0f, nan_a, nan_a);
+  CHECK_MINMAX(nan_a, nan_b, nan_a, nan_a);
+  CHECK_MINMAX(nan_b, nan_a, nan_b, nan_b);
+
+#undef CHECK_MINMAX
+}
+
+template <typename T, typename Inputs, typename Results>
+static void GenerateMacroFloat64MinMax(MacroAssembler& masm) {
+  T a = T::from_code(5);  // ft5
+  T b = T::from_code(6);  // ft6
+  T c = T::from_code(7);  // ft7
+
+#define FLOAT_MIN_MAX(fminmax, res, x, y, res_field)         \
+  __ LoadDouble(x, MemOperand(a0, offsetof(Inputs, src1_))); \
+  __ LoadDouble(y, MemOperand(a0, offsetof(Inputs, src2_))); \
+  __ fminmax(res, x, y);                                     \
+  __ StoreDouble(res, MemOperand(a1, offsetof(Results, res_field)))
+
+  // a = min(b, c);
+  FLOAT_MIN_MAX(Float64Min, a, b, c, min_abc_);
+  // a = min(a, b);
+  FLOAT_MIN_MAX(Float64Min, a, a, b, min_aab_);
+  // a = min(b, a);
+  FLOAT_MIN_MAX(Float64Min, a, b, a, min_aba_);
+
+  // a = max(b, c);
+  FLOAT_MIN_MAX(Float64Max, a, b, c, max_abc_);
+  // a = max(a, b);
+  FLOAT_MIN_MAX(Float64Max, a, a, b, max_aab_);
+  // a = max(b, a);
+  FLOAT_MIN_MAX(Float64Max, a, b, a, max_aba_);
+
+#undef FLOAT_MIN_MAX
+}
+
+TEST(macro_float_minmax_f64) {
+  // Test the Float64Min and Float64Max macros.
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  struct Inputs {
+    double src1_;
+    double src2_;
+  };
+
+  struct Results {
+    // Check all register aliasing possibilities in order to exercise all
+    // code-paths in the macro masm.
+    double min_abc_;
+    double min_aab_;
+    double min_aba_;
+    double max_abc_;
+    double max_aab_;
+    double max_aba_;
+  };
+
+  auto f = AssembleCode<F4>(
+      GenerateMacroFloat64MinMax<DoubleRegister, Inputs, Results>);
+
+#define CHECK_MINMAX(src1, src2, min, max)                          \
+  do {                                                              \
+    Inputs inputs = {src1, src2};                                   \
+    Results results;                                                \
+    f.Call(&inputs, &results, 0, 0, 0);                             \
+    CHECK_EQ(base::bit_cast<uint64_t>(min),                         \
+             base::bit_cast<uint64_t>(results.min_abc_));           \
+    CHECK_EQ(base::bit_cast<uint64_t>(min),                         \
+             base::bit_cast<uint64_t>(results.min_aab_));           \
+    CHECK_EQ(base::bit_cast<uint64_t>(min),                         \
+             base::bit_cast<uint64_t>(results.min_aba_));           \
+    CHECK_EQ(base::bit_cast<uint64_t>(max),                         \
+             base::bit_cast<uint64_t>(results.max_abc_));           \
+    CHECK_EQ(base::bit_cast<uint64_t>(max),                         \
+             base::bit_cast<uint64_t>(results.max_aab_));           \
+    CHECK_EQ(base::bit_cast<uint64_t>(max),                         \
+             base::bit_cast<uint64_t>(results.max_aba_));           \
+    /* Use a base::bit_cast to correctly identify -0.0 and NaNs. */ \
+  } while (0)
+
+  double nan_a = qnan_d;
+  double nan_b = qnan_d;
+
+  CHECK_MINMAX(1.0, -1.0, -1.0, 1.0);
+  CHECK_MINMAX(-1.0, 1.0, -1.0, 1.0);
+  CHECK_MINMAX(0.0, -1.0, -1.0, 0.0);
+  CHECK_MINMAX(-1.0, 0.0, -1.0, 0.0);
+  CHECK_MINMAX(-0.0, -1.0, -1.0, -0.0);
+  CHECK_MINMAX(-1.0, -0.0, -1.0, -0.0);
+  CHECK_MINMAX(0.0, 1.0, 0.0, 1.0);
+  CHECK_MINMAX(1.0, 0.0, 0.0, 1.0);
+
+  CHECK_MINMAX(0.0, 0.0, 0.0, 0.0);
+  CHECK_MINMAX(-0.0, -0.0, -0.0, -0.0);
+  CHECK_MINMAX(-0.0, 0.0, -0.0, 0.0);
+  CHECK_MINMAX(0.0, -0.0, -0.0, 0.0);
+
+  CHECK_MINMAX(0.0, nan_a, nan_a, nan_a);
+  CHECK_MINMAX(nan_a, 0.0, nan_a, nan_a);
+  CHECK_MINMAX(nan_a, nan_b, nan_a, nan_a);
+  CHECK_MINMAX(nan_b, nan_a, nan_b, nan_b);
+
+#undef CHECK_MINMAX
+}
+
+template <typename T>
+static bool CompareF(T input1, T input2, FPUCondition cond) {
+  switch (cond) {
+    case EQ:
+      return (input1 == input2);
+    case LT:
+      return (input1 < input2);
+    case LE:
+      return (input1 <= input2);
+    case NE:
+      return (input1 != input2);
+    case GT:
+      return (input1 > input2);
+    case GE:
+      return (input1 >= input2);
+    default:
+      UNREACHABLE();
+  }
+}
+
+static bool CompareU(uint32_t input1, uint32_t input2, Condition cond) {
+  switch (cond) {
+    case eq:
+      return (input1 == input2);
+    case ne:
+      return (input1 != input2);
+
+    case Uless:
+      return (input1 < input2);
+    case Uless_equal:
+      return (input1 <= input2);
+    case Ugreater:
+      return (input1 > input2);
+    case Ugreater_equal:
+      return (input1 >= input2);
+
+    case less:
+      return (static_cast<int32_t>(input1) < static_cast<int32_t>(input2));
+    case less_equal:
+      return (static_cast<int32_t>(input1) <= static_cast<int32_t>(input2));
+    case greater:
+      return (static_cast<int32_t>(input1) > static_cast<int32_t>(input2));
+    case greater_equal:
+      return (static_cast<int32_t>(input1) >= static_cast<int32_t>(input2));
+
+    default:
+      UNREACHABLE();
+  }
+}
+
+static void FCompare32Helper(FPUCondition cond) {
+  auto fn = [cond](MacroAssembler& masm) { __ CompareF32(a0, cond, fa0, fa1); };
+  FOR_FLOAT32_INPUTS(i) {
+    FOR_FLOAT32_INPUTS(j) {
+      bool comp_res = CompareF(i, j, cond);
+      CHECK_EQ(comp_res, GenAndRunTest<int32_t>(i, j, fn));
+    }
+  }
+}
+
+// static void FCompare64Helper(FPUCondition cond) {
+//   auto fn = [cond](MacroAssembler& masm) { __ CompareF64(a0, cond, fa0, fa1);
+//   }; FOR_FLOAT64_INPUTS(i) {
+//     FOR_FLOAT64_INPUTS(j) {
+//       bool comp_res = CompareF(i, j, cond);
+//       CHECK_EQ(comp_res, GenAndRunTest<int32_t>(i, j, fn));
+//     }
+//   }
+// }
+
+TEST(FCompare32_Branch) {
+  CcTest::InitializeVM();
+
+  FCompare32Helper(EQ);
+  FCompare32Helper(LT);
+  FCompare32Helper(LE);
+  FCompare32Helper(NE);
+  FCompare32Helper(GT);
+  FCompare32Helper(GE);
+
+  // test CompareIsNanF32: return true if any operand isnan
+  auto fn = [](MacroAssembler& masm) { __ CompareIsNanF32(a0, fa0, fa1); };
+  CHECK_EQ(false, GenAndRunTest<int32_t>(1023.01f, -100.23f, fn));
+  CHECK_EQ(true, GenAndRunTest<int32_t>(1023.01f, snan_f, fn));
+  CHECK_EQ(true, GenAndRunTest<int32_t>(snan_f, -100.23f, fn));
+  CHECK_EQ(true, GenAndRunTest<int32_t>(snan_f, qnan_f, fn));
+}
+
+// TEST(FCompare64_Branch) {
+//   CcTest::InitializeVM();
+//   FCompare64Helper(EQ);
+//   FCompare64Helper(LT);
+//   FCompare64Helper(LE);
+//   FCompare64Helper(NE);
+//   FCompare64Helper(GT);
+//   FCompare64Helper(GE);
+
+//   // test CompareIsNanF64: return true if any operand isnan
+//   auto fn = [](MacroAssembler& masm) { __ CompareIsNanF64(a0, fa0, fa1); };
+//   CHECK_EQ(false, GenAndRunTest<int32_t>(1023.01, -100.23, fn));
+//   CHECK_EQ(true, GenAndRunTest<int32_t>(1023.01, snan_d, fn));
+//   CHECK_EQ(true, GenAndRunTest<int32_t>(snan_d, -100.23, fn));
+//   CHECK_EQ(true, GenAndRunTest<int32_t>(snan_d, qnan_d, fn));
+// }
+
+static void CompareIHelper(Condition cond) {
+  FOR_UINT32_INPUTS(i) {
+    FOR_UINT32_INPUTS(j) {
+      auto input1 = i;
+      auto input2 = j;
+      bool comp_res = CompareU(input1, input2, cond);
+      // test compare against immediate value
+      auto fn1 = [cond, input2](MacroAssembler& masm) {
+        __ CompareI(a0, a0, Operand(input2), cond);
+      };
+      CHECK_EQ(comp_res, GenAndRunTest<int32_t>(input1, fn1));
+      // test compare registers
+      auto fn2 = [cond](MacroAssembler& masm) {
+        __ CompareI(a0, a0, Operand(a1), cond);
+      };
+      CHECK_EQ(comp_res, GenAndRunTest<int32_t>(input1, input2, fn2));
+    }
+  }
+}
+
+TEST(CompareI) {
+  CcTest::InitializeVM();
+  CompareIHelper(eq);
+  CompareIHelper(ne);
+
+  CompareIHelper(greater);
+  CompareIHelper(greater_equal);
+  CompareIHelper(less);
+  CompareIHelper(less_equal);
+
+  CompareIHelper(Ugreater);
+  CompareIHelper(Ugreater_equal);
+  CompareIHelper(Uless);
+  CompareIHelper(Uless_equal);
+}
+
+TEST(Clz32) {
+  CcTest::InitializeVM();
+  auto fn = [](MacroAssembler& masm) { __ Clz32(a0, a0); };
+  FOR_UINT32_INPUTS(i) {
+    // __builtin_clzll(0) is undefined
+    if (i == 0) continue;
+    CHECK_EQ(__builtin_clz(i), GenAndRunTest<int>(i, fn));
+  }
+}
+
+TEST(Ctz32) {
+  CcTest::InitializeVM();
+  auto fn = [](MacroAssembler& masm) { __ Ctz32(a0, a0); };
+  FOR_UINT32_INPUTS(i) {
+    // __builtin_clzll(0) is undefined
+    if (i == 0) continue;
+    CHECK_EQ(__builtin_ctz(i), GenAndRunTest<int>(i, fn));
+  }
+}
+
+TEST(ByteSwap) {
+  CcTest::InitializeVM();
+  auto fn0 = [](MacroAssembler& masm) { __ ByteSwap(a0, a0, 4, t0); };
+  CHECK_EQ((int32_t)0x89ab'cdef, GenAndRunTest<int32_t>(0xefcd'ab89, fn0));
+}
+
+TEST(Popcnt) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope handles(isolate);
+
+  uint32_t in[8];
+  uint32_t out[8];
+  uint32_t result[8];
+  uint32_t val = 0xffffffff;
+  uint32_t cnt = 32;
+
+  for (int i = 0; i < 6; i++) {
+    in[i] = val;
+    out[i] = cnt;
+    cnt >>= 1;
+    val >>= cnt;
+  }
+
+  in[6] = 0xaf10000b;
+  out[6] = 10;
+  in[7] = 0xe03f3000;
+  out[7] = 11;
+
+  auto fn = [&in](MacroAssembler& masm) {
+    __ mv(a4, a0);
+    for (int i = 0; i < 6; i++) {
+      // Load constant.
+      __ li(a3, Operand(in[i]));
+      __ Popcnt32(a5, a3, t0);
+      __ Sw(a5, MemOperand(a4));
+      __ AddWord(a4, a4, Operand(kSystemPointerSize));
+    }
+
+    __ li(a3, Operand(in[6]));
+    __ Popcnt32(a5, a3, t0);
+    __ Sw(a5, MemOperand(a4));
+    __ AddWord(a4, a4, Operand(kSystemPointerSize));
+
+    __ li(a3, Operand(in[7]));
+    __ Popcnt32(a5, a3, t0);
+    __ Sw(a5, MemOperand(a4));
+    __ AddWord(a4, a4, Operand(kSystemPointerSize));
+  };
+  auto f = AssembleCode<FV>(fn);
+
+  (void)f.Call(reinterpret_cast<uint32_t>(result), 0, 0, 0, 0);
+  // Check results.
+  for (int i = 0; i < 8; i++) {
+    CHECK(out[i] == result[i]);
+  }
+}
+
+// TEST(Move) {
+//   CcTest::InitializeVM();
+//   union {
+//     double dval;
+//     int32_t ival[2];
+//   } t;
+
+//   {
+//     auto fn = [](MacroAssembler& masm) { __ ExtractHighWordFromF64(a0, fa0);
+//     }; t.ival[0] = 256; t.ival[1] = -123;
+//     CHECK_EQ(static_cast<int32_t>(t.ival[1]),
+//              GenAndRunTest<int32_t>(t.dval, fn));
+//   }
+
+//   {
+//     auto fn = [](MacroAssembler& masm) { __ ExtractLowWordFromF64(a0, fa0);
+//     }; t.ival[0] = 256; t.ival[1] = -123;
+//     CHECK_EQ(static_cast<int32_t>(t.ival[0]),
+//              GenAndRunTest<int32_t>(t.dval, fn));
+
+//   }
+// }
+
+TEST(DeoptExitSizeIsFixed) {
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope handles(isolate);
+  auto buffer = AllocateAssemblerBuffer();
+  MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
+                      buffer->CreateView());
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  for (int i = 0; i < kDeoptimizeKindCount; i++) {
+    DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
+    Label before_exit;
+    Builtin target = Deoptimizer::GetDeoptimizationEntry(kind);
+    // Mirroring logic in code-generator.cc.
+    if (kind == DeoptimizeKind::kLazy) {
+      // CFI emits an extra instruction here.
+      masm.BindExceptionHandler(&before_exit);
+    } else {
+      masm.bind(&before_exit);
+    }
+    masm.CallForDeoptimization(target, 42, &before_exit, kind, &before_exit,
+                               &before_exit);
+    CHECK_EQ(masm.SizeOfCodeGeneratedSince(&before_exit),
+             kind == DeoptimizeKind::kLazy ? Deoptimizer::kLazyDeoptExitSize
+                                           : Deoptimizer::kEagerDeoptExitSize);
+  }
+}
+
+TEST(AddWithImm) {
+  CcTest::InitializeVM();
+#define Test(Op, Input, Expected)                                       \
+  {                                                                     \
+    auto fn = [](MacroAssembler& masm) { __ Op(a0, zero_reg, Input); }; \
+    CHECK_EQ(static_cast<int64_t>(Expected), GenAndRunTest(fn));        \
+  }
+
+  Test(AddWord, 4095, 4095);
+  Test(SubWord, 4095, -4095);
+#undef Test
+}
+
+#undef __
+
+}  // namespace internal
+}  // namespace v8
diff --git a/test/cctest/test-simple-riscv32.cc b/test/cctest/test-simple-riscv32.cc
new file mode 100644
index 00000000000..7a81643d420
--- /dev/null
+++ b/test/cctest/test-simple-riscv32.cc
@@ -0,0 +1,229 @@
+// Copyright 2021 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+#include <iostream>
+
+#include "src/base/utils/random-number-generator.h"
+#include "src/codegen/assembler-inl.h"
+#include "src/codegen/macro-assembler.h"
+#include "src/diagnostics/disassembler.h"
+#include "src/execution/simulator.h"
+#include "src/heap/factory.h"
+#include "src/init/v8.h"
+#include "test/cctest/cctest.h"
+
+namespace v8 {
+namespace internal {
+
+// Define these function prototypes to match JSEntryFunction in execution.cc.
+// TODO(mips64): Refine these signatures per test case.
+using F1 = void*(int x, int p1, int p2, int p3, int p4);
+using F2 = void*(int x, int y, int p2, int p3, int p4);
+using F3 = void*(void* p, int p1, int p2, int p3, int p4);
+using F4 = void*(int64_t x, int64_t y, int64_t p2, int64_t p3, int64_t p4);
+using F5 = void*(void* p0, void* p1, int p2, int p3, int p4);
+
+#define __ assm.
+
+TEST(RISCV_SIMPLE0) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // Addition.
+  __ add(a0, a0, a1);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<F2>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(0xAB0, 0xC, 0, 0, 0));
+  CHECK_EQ(0xABCL, res);
+}
+
+TEST(RISCV_SIMPLE1) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  // Addition.
+  __ addi(a0, a0, -1);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(100, 0, 0, 0, 0));
+  CHECK_EQ(99L, res);
+}
+
+// Loop 100 times, adding loop counter to result
+TEST(RISCV_SIMPLE2) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  Label L, C;
+  // input a0, result a1
+  __ mv(a1, a0);
+  __ RV_li(a0, 0);
+  __ j(&C);
+
+  __ bind(&L);
+
+  __ add(a0, a0, a1);
+  __ addi(a1, a1, -1);
+
+  __ bind(&C);
+  __ bgtz(a1, &L);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+#ifdef DEBUG
+  code->Print();
+#endif
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(100, 0, 0, 0, 0));
+  CHECK_EQ(5050, res);
+}
+
+// Test part of Load and Store
+TEST(RISCV_SIMPLE3) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+
+  __ sb(a0, sp, -4);
+  __ lb(a0, sp, -4);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(255, 0, 0, 0, 0));
+  CHECK_EQ(-1, res);
+}
+
+// Test loading immediates of various sizes
+TEST(LI) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  Label error;
+
+  // Load 0
+  __ RV_li(a0, 0l);
+  __ bnez(a0, &error);
+
+  // Load small number (<12 bits)
+  __ RV_li(a1, 5);
+  __ RV_li(a2, -5);
+  __ add(a0, a1, a2);
+  __ bnez(a0, &error);
+
+  // Load medium number (13-32 bits)
+  __ RV_li(a1, 124076833);
+  __ RV_li(a2, -124076833);
+  __ add(a0, a1, a2);
+  __ bnez(a0, &error);
+
+  __ mv(a0, zero_reg);
+  __ jr(ra);
+
+  __ bind(&error);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(0xDEADBEEF, 0, 0, 0, 0));
+  CHECK_EQ(0L, res);
+}
+
+TEST(LI_CONST) {
+  CcTest::InitializeVM();
+  Isolate* isolate = CcTest::i_isolate();
+  HandleScope scope(isolate);
+
+  MacroAssembler assm(isolate, v8::internal::CodeObjectRequired::kYes);
+  Label error;
+
+  // Load 0
+  __ li_constant(a0, 0l);
+  __ bnez(a0, &error);
+
+  // Load small number (<12 bits)
+  __ li_constant(a1, 5);
+  __ li_constant(a2, -5);
+  __ add(a0, a1, a2);
+  __ bnez(a0, &error);
+
+  // Load medium number (13-32 bits)
+  __ li_constant(a1, 124076833);
+  __ li_constant(a2, -124076833);
+  __ add(a0, a1, a2);
+  __ bnez(a0, &error);
+
+  __ mv(a0, zero_reg);
+  __ jr(ra);
+
+  __ bind(&error);
+  __ jr(ra);
+
+  CodeDesc desc;
+  assm.GetCode(isolate, &desc);
+  Handle<Code> code =
+      Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
+  auto f = GeneratedCode<F1>::FromCode(*code);
+  int32_t res = reinterpret_cast<int32_t>(f.Call(0xDEADBEEF, 0, 0, 0, 0));
+  CHECK_EQ(0L, res);
+}
+
+#undef __
+
+}  // namespace internal
+}  // namespace v8
diff --git a/test/cctest/wasm/test-jump-table-assembler.cc b/test/cctest/wasm/test-jump-table-assembler.cc
index 19687b8a79e..dea394bca90 100644
--- a/test/cctest/wasm/test-jump-table-assembler.cc
+++ b/test/cctest/wasm/test-jump-table-assembler.cc
@@ -150,7 +150,7 @@ void CompileJumpTableThunk(Address thunk, Address jump_target) {
   __ lw(scratch, MemOperand(scratch, 0));
   __ Branch(&exit, ne, scratch, Operand(zero_reg));
   __ Jump(jump_target, RelocInfo::NO_INFO);
-#elif V8_TARGET_ARCH_RISCV64
+#elif V8_TARGET_ARCH_RISCV64 || V8_TARGET_ARCH_RISCV32
   __ li(scratch, Operand(stop_bit_address, RelocInfo::NO_INFO));
   __ Lw(scratch, MemOperand(scratch, 0));
   __ Branch(&exit, ne, scratch, Operand(zero_reg));
diff --git a/test/inspector/inspector.status b/test/inspector/inspector.status
index 354f14796fa..a00bc3f450b 100644
--- a/test/inspector/inspector.status
+++ b/test/inspector/inspector.status
@@ -118,12 +118,21 @@
 }], # no_simd_hardware
 
 ##############################################################################
-['arch == riscv64 or arch == loong64', {
+['arch == riscv64 or arch == riscv32 or arch == loong64', {
   # SIMD support is still in progress.
   'debugger/wasm-scope-info*': [SKIP],
   'debugger/wasm-step-after-trap': [SKIP],
 
-}],  # 'arch == riscv64 or arch == loong64'
+}],  # 'arch == riscv64 or arch == riscv32 or arch == loong64'
+
+##############################################################################
+['arch == riscv32', {
+# Skip case on riscv32
+'debugger/wasm-inspect-many-registers':[SKIP],
+
+'debugger/wasm-gc-breakpoints':['variant == stress_incremental_marking', SKIP],
+}],  # 'arch == riscv32'
+
 
 ################################################################################
 ['variant == stress_snapshot', {
diff --git a/test/message/message.status b/test/message/message.status
index 0a8dc1e1e05..8ed2f79f9e8 100644
--- a/test/message/message.status
+++ b/test/message/message.status
@@ -94,4 +94,8 @@
   'fail/set-grow-failed': [SKIP],
 }],  # not (arch == x64 and mode == release)
 
+##############################################################################
+['arch == riscv32', {
+  'wasm-trace-turbofan':[SKIP],
+}],  # (arch == riscv32)
 ]
diff --git a/test/mjsunit/mjsunit.status b/test/mjsunit/mjsunit.status
index dfc2c29e0be..f49dfcd8bea 100644
--- a/test/mjsunit/mjsunit.status
+++ b/test/mjsunit/mjsunit.status
@@ -163,11 +163,11 @@
   'wasm/compare-exchange64-stress': [PASS, SLOW, NO_VARIANTS],
 
   # Very slow on ARM, MIPS, RISCV and LOONG, contains no architecture dependent code.
-  'unicode-case-overoptimization0': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, loong64)', SKIP]],
-  'unicode-case-overoptimization1': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, loong64)', SKIP]],
-  'regress/regress-3976': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, loong64)', SKIP]],
-  'regress/regress-crbug-482998': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips, riscv64, loong64)', SKIP]],
-  'regress/regress-740784': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips, riscv64, loong64)', SKIP]],
+  'unicode-case-overoptimization0': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, riscv32, loong64)', SKIP]],
+  'unicode-case-overoptimization1': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, riscv32, loong64)', SKIP]],
+  'regress/regress-3976': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips64, mips, riscv64, riscv32, loong64)', SKIP]],
+  'regress/regress-crbug-482998': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips, riscv64, riscv32, loong64)', SKIP]],
+  'regress/regress-740784': [PASS, NO_VARIANTS, ['arch in (arm, arm64, mipsel, mips64el, mips, riscv64,riscv32, loong64)', SKIP]],
 
   # TODO(bmeurer): Flaky timeouts (sometimes <1s, sometimes >3m).
   'unicodelctest': [PASS, NO_VARIANTS],
@@ -854,7 +854,7 @@
 }],  # 'arch == loong64'
 
 ##############################################################################
-['arch == riscv64', {
+['arch == riscv64 or arch == riscv32', {
 
   # Slow tests which times out in debug mode.
   'try': [PASS, ['mode == debug', SKIP]],
@@ -920,8 +920,33 @@
   'wasm/exceptions-simd': [SKIP],
   'wasm/simd-*': [SKIP],
 
-}],  # 'arch == riscv64'
+}],  # 'arch == riscv64 or arch == riscv32'
 
+[ 'arch == riscv32' , {
+
+'wasm/compare-exchange64-stress':[SKIP],
+'asm/atomics-and':[SKIP],
+'asm/atomics-add':[SKIP],
+'asm/atomics-sub':[SKIP],
+'asm/atomics-or':[SKIP],
+'asm/atomics-xor':[SKIP],
+'harmony/atomics':[SKIP],
+'typedarray-growablesharedarraybuffer-atomics':[SKIP],
+'typedarray-resizablearraybuffer-atomics':[SKIP],
+
+
+'regress/wasm/regress-1079449':[SKIP],
+'wasm/grow-huge-memory':[SKIP],
+'wasm/huge-typedarray':[SKIP],
+'wasm/huge-memory':[SKIP],
+'wasm/generic-wrapper':[SKIP],
+'wasm/many-parameters':[SKIP],
+'wasm/atomics':['variant == stress', SKIP],
+'wasm/externref-globals-liftoff':['variant == stress_incremental_marking', SKIP],
+'regress/wasm/regress-1296876':['variant == stress', SKIP],
+'wasm/shared-memory':['variant == stress', SKIP],
+'wasm/stringrefs-js':['variant == stress_incremental_marking', SKIP],
+}], # 'arch == riscv32'
 
 ##############################################################################
 ['system == macos', {
@@ -1525,7 +1550,7 @@
 
 ##############################################################################
 # TODO(v8:11421): Port baseline compiler to other architectures.
-['arch not in (x64, arm64, ia32, arm, mips64el, mipsel, riscv64, loong64, s390x) or (arch == s390x and pointer_compression)', {
+['arch not in (x64, arm64, ia32, arm, mips64el, mipsel, riscv64, riscv32, loong64, s390x) or (arch == s390x and pointer_compression)', {
   'baseline/*': [SKIP],
   'regress/regress-1242306': [SKIP],
 }],
diff --git a/test/unittests/BUILD.gn b/test/unittests/BUILD.gn
index 1fa1280e561..a81788a9515 100644
--- a/test/unittests/BUILD.gn
+++ b/test/unittests/BUILD.gn
@@ -75,7 +75,7 @@ v8_source_set("v8_heap_base_unittests_sources") {
 if (cppgc_is_standalone) {
   v8_executable("cppgc_unittests") {
     testonly = true
-    if (v8_current_cpu == "riscv64") {
+    if (v8_current_cpu == "riscv64" || v8_current_cpu == "riscv32") {
       libs = [ "atomic" ]
     }
 
@@ -581,10 +581,16 @@ v8_source_set("unittests_sources") {
     ]
   } else if (v8_current_cpu == "riscv64") {
     sources += [
-      "assembler/disasm-riscv64-unittest.cc",
-      "assembler/turbo-assembler-riscv64-unittest.cc",
+      "assembler/disasm-riscv-unittest.cc",
+      "assembler/turbo-assembler-riscv-unittest.cc",
       "compiler/riscv64/instruction-selector-riscv64-unittest.cc",
     ]
+  } else if (v8_current_cpu == "riscv32") {
+    sources += [
+      "assembler/disasm-riscv-unittest.cc",
+      "assembler/turbo-assembler-riscv-unittest.cc",
+      "compiler/riscv32/instruction-selector-riscv32-unittest.cc",
+    ]
   } else if (v8_current_cpu == "x64") {
     sources += [
       "assembler/disasm-x64-unittest.cc",
diff --git a/test/unittests/assembler/disasm-riscv64-unittest.cc b/test/unittests/assembler/disasm-riscv-unittest.cc
similarity index 99%
rename from test/unittests/assembler/disasm-riscv64-unittest.cc
rename to test/unittests/assembler/disasm-riscv-unittest.cc
index 676b14d7b06..f2b83adb852 100644
--- a/test/unittests/assembler/disasm-riscv64-unittest.cc
+++ b/test/unittests/assembler/disasm-riscv-unittest.cc
@@ -208,7 +208,7 @@ TEST_F(DisasmRiscv64Test, CSR) {
 
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64I) {
   SET_UP();
 
@@ -227,7 +227,7 @@ TEST_F(DisasmRiscv64Test, RV64I) {
 
   VERIFY_RUN();
 }
-
+#endif
 TEST_F(DisasmRiscv64Test, RV32M) {
   SET_UP();
 
@@ -242,7 +242,7 @@ TEST_F(DisasmRiscv64Test, RV32M) {
 
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64M) {
   SET_UP();
 
@@ -254,7 +254,7 @@ TEST_F(DisasmRiscv64Test, RV64M) {
 
   VERIFY_RUN();
 }
-
+#endif
 TEST_F(DisasmRiscv64Test, RV32A) {
   SET_UP();
   // RV32A Standard Extension
@@ -281,7 +281,7 @@ TEST_F(DisasmRiscv64Test, RV32A) {
           "e749a52f       amomaxu.w.aqrl a0, s4, (s3)");
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64A) {
   SET_UP();
 
@@ -308,7 +308,7 @@ TEST_F(DisasmRiscv64Test, RV64A) {
 
   VERIFY_RUN();
 }
-
+#endif
 TEST_F(DisasmRiscv64Test, RV32F) {
   SET_UP();
   // RV32F Standard Extension
@@ -344,7 +344,7 @@ TEST_F(DisasmRiscv64Test, RV32F) {
   COMPARE(fmv_w_x(ft0, s3), "f0098053       fmv.w.x   ft0, s3");
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64F) {
   SET_UP();
   // RV64F Standard Extension (in addition to RV32F)
@@ -354,7 +354,7 @@ TEST_F(DisasmRiscv64Test, RV64F) {
   COMPARE(fcvt_s_lu(ft0, s3), "d0398053       fcvt.s.lu ft0, s3");
   VERIFY_RUN();
 }
-
+#endif
 TEST_F(DisasmRiscv64Test, RV32D) {
   SET_UP();
   // RV32D Standard Extension
@@ -391,7 +391,7 @@ TEST_F(DisasmRiscv64Test, RV32D) {
 
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64D) {
   SET_UP();
   // RV64D Standard Extension (in addition to RV32D)
@@ -403,6 +403,7 @@ TEST_F(DisasmRiscv64Test, RV64D) {
   COMPARE(fmv_d_x(ft0, s3), "f2098053       fmv.d.x   ft0, s3");
   VERIFY_RUN();
 }
+#endif
 
 TEST_F(DisasmRiscv64Test, PSEUDO) {
   SET_UP();
@@ -412,8 +413,10 @@ TEST_F(DisasmRiscv64Test, PSEUDO) {
   COMPARE(mv(t0, a4), "00070293       mv        t0, a4");
   COMPARE(not_(t0, a5), "fff7c293       not       t0, a5");
   COMPARE(neg(ra, a6), "410000b3       neg       ra, a6");
+#ifdef V8_TARGET_ARCH_RISCV64
   COMPARE(negw(t2, fp), "408003bb       negw      t2, fp");
   COMPARE(sext_w(t0, s1), "0004829b       sext.w    t0, s1");
+#endif
   COMPARE(seqz(sp, s2), "00193113       seqz      sp, s2");
   COMPARE(snez(fp, s3), "01303433       snez      fp, s3");
   COMPARE(sltz(a0, t5), "000f2533       sltz      a0, t5");
@@ -464,7 +467,7 @@ TEST_F(DisasmRiscv64Test, PSEUDO) {
 
   VERIFY_RUN();
 }
-
+#ifdef V8_TARGET_ARCH_RISCV64
 TEST_F(DisasmRiscv64Test, RV64C) {
   i::FLAG_riscv_c_extension = true;
   SET_UP();
@@ -515,7 +518,7 @@ TEST_F(DisasmRiscv64Test, RV64C) {
 
   VERIFY_RUN();
 }
-
+#endif
 /*
 TEST_F(DisasmRiscv64Test,  Previleged) {
   SET_UP();
diff --git a/test/unittests/assembler/turbo-assembler-riscv64-unittest.cc b/test/unittests/assembler/turbo-assembler-riscv-unittest.cc
similarity index 97%
rename from test/unittests/assembler/turbo-assembler-riscv64-unittest.cc
rename to test/unittests/assembler/turbo-assembler-riscv-unittest.cc
index 6084d36dfcd..afda8d36038 100644
--- a/test/unittests/assembler/turbo-assembler-riscv64-unittest.cc
+++ b/test/unittests/assembler/turbo-assembler-riscv-unittest.cc
@@ -2,8 +2,8 @@
 // Use of this source code is governed by a BSD-style license that can be
 // found in the LICENSE file.
 
+#include "src/codegen/assembler-inl.h"
 #include "src/codegen/macro-assembler.h"
-#include "src/codegen/riscv64/assembler-riscv64-inl.h"
 #include "src/execution/simulator.h"
 #include "test/common/assembler-tester.h"
 #include "test/unittests/test-utils.h"
diff --git a/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc b/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc
new file mode 100644
index 00000000000..0c7b6478fd9
--- /dev/null
+++ b/test/unittests/compiler/riscv32/instruction-selector-riscv32-unittest.cc
@@ -0,0 +1,1159 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file
+
+#include "src/objects/objects-inl.h"
+#include "test/unittests/compiler/backend/instruction-selector-unittest.h"
+
+namespace v8 {
+namespace internal {
+namespace compiler {
+
+namespace {
+template <typename T>
+struct MachInst {
+  T constructor;
+  const char* constructor_name;
+  ArchOpcode arch_opcode;
+  MachineType machine_type;
+};
+
+template <typename T>
+std::ostream& operator<<(std::ostream& os, const MachInst<T>& mi) {
+  return os << mi.constructor_name;
+}
+
+using MachInst1 = MachInst<Node* (RawMachineAssembler::*)(Node*)>;
+using MachInst2 = MachInst<Node* (RawMachineAssembler::*)(Node*, Node*)>;
+
+// To avoid duplicated code IntCmp helper structure
+// is created. It contains MachInst2 with two nodes and expected_size
+// because different cmp instructions have different size.
+struct IntCmp {
+  MachInst2 mi;
+  uint32_t expected_size;
+};
+
+struct FPCmp {
+  MachInst2 mi;
+  FlagsCondition cond;
+};
+
+const FPCmp kFPCmpInstructions[] = {
+    {{&RawMachineAssembler::Float64Equal, "Float64Equal", kRiscvCmpD,
+      MachineType::Float64()},
+     kEqual},
+    {{&RawMachineAssembler::Float64LessThan, "Float64LessThan", kRiscvCmpD,
+      MachineType::Float64()},
+     kUnsignedLessThan},
+    {{&RawMachineAssembler::Float64LessThanOrEqual, "Float64LessThanOrEqual",
+      kRiscvCmpD, MachineType::Float64()},
+     kUnsignedLessThanOrEqual},
+    {{&RawMachineAssembler::Float64GreaterThan, "Float64GreaterThan",
+      kRiscvCmpD, MachineType::Float64()},
+     kUnsignedLessThan},
+    {{&RawMachineAssembler::Float64GreaterThanOrEqual,
+      "Float64GreaterThanOrEqual", kRiscvCmpD, MachineType::Float64()},
+     kUnsignedLessThanOrEqual}};
+
+struct Conversion {
+  // The machine_type field in MachInst1 represents the destination type.
+  MachInst1 mi;
+  MachineType src_machine_type;
+};
+
+// ----------------------------------------------------------------------------
+// Logical instructions.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kLogicalInstructions[] = {
+    {&RawMachineAssembler::Word32And, "Word32And", kRiscvAnd,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Word32Or, "Word32Or", kRiscvOr,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Word32Xor, "Word32Xor", kRiscvXor,
+     MachineType::Int32()}};
+
+// ----------------------------------------------------------------------------
+// Shift instructions.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kShiftInstructions[] = {
+    {&RawMachineAssembler::Word32Shl, "Word32Shl", kRiscvShl32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Word32Shr, "Word32Shr", kRiscvShr32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Word32Sar, "Word32Sar", kRiscvSar32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Word32Ror, "Word32Ror", kRiscvRor32,
+     MachineType::Int32()}};
+
+// ----------------------------------------------------------------------------
+// MUL/DIV instructions.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kMulDivInstructions[] = {
+    {&RawMachineAssembler::Int32Mul, "Int32Mul", kRiscvMul32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Int32Div, "Int32Div", kRiscvDiv32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Uint32Div, "Uint32Div", kRiscvDivU32,
+     MachineType::Uint32()},
+    {&RawMachineAssembler::Float64Mul, "Float64Mul", kRiscvMulD,
+     MachineType::Float64()},
+    {&RawMachineAssembler::Float64Div, "Float64Div", kRiscvDivD,
+     MachineType::Float64()}};
+
+// ----------------------------------------------------------------------------
+// MOD instructions.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kModInstructions[] = {
+    {&RawMachineAssembler::Int32Mod, "Int32Mod", kRiscvMod32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Uint32Mod, "Uint32Mod", kRiscvModU32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Float64Mod, "Float64Mod", kRiscvModD,
+     MachineType::Float64()}};
+
+// ----------------------------------------------------------------------------
+// Arithmetic FPU instructions.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kFPArithInstructions[] = {
+    {&RawMachineAssembler::Float64Add, "Float64Add", kRiscvAddD,
+     MachineType::Float64()},
+    {&RawMachineAssembler::Float64Sub, "Float64Sub", kRiscvSubD,
+     MachineType::Float64()}};
+
+// ----------------------------------------------------------------------------
+// IntArithTest instructions, two nodes.
+// ----------------------------------------------------------------------------
+
+const MachInst2 kAddSubInstructions[] = {
+    {&RawMachineAssembler::Int32Add, "Int32Add", kRiscvAdd32,
+     MachineType::Int32()},
+    {&RawMachineAssembler::Int32Sub, "Int32Sub", kRiscvSub32,
+     MachineType::Int32()}};
+
+// ----------------------------------------------------------------------------
+// IntArithTest instructions, one node.
+// ----------------------------------------------------------------------------
+
+const MachInst1 kAddSubOneInstructions[] = {{&RawMachineAssembler::Int32Neg,
+                                             "Int32Neg", kRiscvSub32,
+                                             MachineType::Int32()}};
+
+// ----------------------------------------------------------------------------
+// Arithmetic compare instructions.
+// ----------------------------------------------------------------------------
+
+const IntCmp kCmpInstructions[] = {
+    {{&RawMachineAssembler::WordEqual, "WordEqual", kRiscvCmp,
+      MachineType::Int64()},
+     1U},
+    {{&RawMachineAssembler::WordNotEqual, "WordNotEqual", kRiscvCmp,
+      MachineType::Int64()},
+     1U},
+    {{&RawMachineAssembler::Word32Equal, "Word32Equal", kRiscvCmp,
+      MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Word32NotEqual, "Word32NotEqual", kRiscvCmp,
+      MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Int32LessThan, "Int32LessThan", kRiscvCmp,
+      MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Int32LessThanOrEqual, "Int32LessThanOrEqual",
+      kRiscvCmp, MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Int32GreaterThan, "Int32GreaterThan", kRiscvCmp,
+      MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Int32GreaterThanOrEqual, "Int32GreaterThanOrEqual",
+      kRiscvCmp, MachineType::Int32()},
+     1U},
+    {{&RawMachineAssembler::Uint32LessThan, "Uint32LessThan", kRiscvCmp,
+      MachineType::Uint32()},
+     1U},
+    {{&RawMachineAssembler::Uint32LessThanOrEqual, "Uint32LessThanOrEqual",
+      kRiscvCmp, MachineType::Uint32()},
+     1U}};
+
+// ----------------------------------------------------------------------------
+// Conversion instructions.
+// ----------------------------------------------------------------------------
+
+const Conversion kConversionInstructions[] = {
+    // Conversion instructions are related to machine_operator.h:
+    // FPU conversions:
+    // Convert representation of integers between float64 and int32/uint32.
+    // The precise rounding mode and handling of out of range inputs are *not*
+    // defined for these operators, since they are intended only for use with
+    // integers.
+    // mips instructions:
+    // mtc1, cvt.d.w
+    {{&RawMachineAssembler::ChangeInt32ToFloat64, "ChangeInt32ToFloat64",
+      kRiscvCvtDW, MachineType::Float64()},
+     MachineType::Int32()},
+
+    // mips instructions:
+    // cvt.d.uw
+    {{&RawMachineAssembler::ChangeUint32ToFloat64, "ChangeUint32ToFloat64",
+      kRiscvCvtDUw, MachineType::Float64()},
+     MachineType::Int32()},
+
+    // mips instructions:
+    // mfc1, trunc double to word, for more details look at mips macro
+    // asm and mips asm file
+    {{&RawMachineAssembler::ChangeFloat64ToInt32, "ChangeFloat64ToInt32",
+      kRiscvTruncWD, MachineType::Float64()},
+     MachineType::Int32()},
+
+    // mips instructions:
+    // trunc double to unsigned word, for more details look at mips macro
+    // asm and mips asm file
+    {{&RawMachineAssembler::ChangeFloat64ToUint32, "ChangeFloat64ToUint32",
+      kRiscvTruncUwD, MachineType::Float64()},
+     MachineType::Int32()}};
+
+const Conversion kFloat32RoundInstructions[] = {
+    {{&RawMachineAssembler::Float32RoundUp, "Float32RoundUp",
+      kRiscvFloat32RoundUp, MachineType::Int32()},
+     MachineType::Float32()},
+    {{&RawMachineAssembler::Float32RoundDown, "Float32RoundDown",
+      kRiscvFloat32RoundDown, MachineType::Int32()},
+     MachineType::Float32()},
+    {{&RawMachineAssembler::Float32RoundTiesEven, "Float32RoundTiesEven",
+      kRiscvFloat32RoundTiesEven, MachineType::Int32()},
+     MachineType::Float32()},
+    {{&RawMachineAssembler::Float32RoundTruncate, "Float32RoundTruncate",
+      kRiscvFloat32RoundTruncate, MachineType::Int32()},
+     MachineType::Float32()}};
+
+}  // namespace
+
+using InstructionSelectorFPCmpTest = InstructionSelectorTestWithParam<FPCmp>;
+
+TEST_P(InstructionSelectorFPCmpTest, Parameter) {
+  const FPCmp cmp = GetParam();
+  StreamBuilder m(this, MachineType::Int32(), cmp.mi.machine_type,
+                  cmp.mi.machine_type);
+  m.Return((m.*cmp.mi.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(cmp.mi.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(kFlags_set, s[0]->flags_mode());
+  EXPECT_EQ(cmp.cond, s[0]->flags_condition());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest, InstructionSelectorFPCmpTest,
+                         ::testing::ValuesIn(kFPCmpInstructions));
+
+// ----------------------------------------------------------------------------
+// Arithmetic compare instructions integers
+// ----------------------------------------------------------------------------
+using InstructionSelectorCmpTest = InstructionSelectorTestWithParam<IntCmp>;
+
+TEST_P(InstructionSelectorCmpTest, Parameter) {
+  const IntCmp cmp = GetParam();
+  const MachineType type = cmp.mi.machine_type;
+  StreamBuilder m(this, type, type, type);
+  m.Return((m.*cmp.mi.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+
+  if (FLAG_debug_code &&
+      type.representation() == MachineRepresentation::kWord32) {
+    ASSERT_EQ(1U, s.size());
+
+    EXPECT_EQ(cmp.mi.arch_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  } else {
+    ASSERT_EQ(cmp.expected_size, s.size());
+    EXPECT_EQ(cmp.mi.arch_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest, InstructionSelectorCmpTest,
+                         ::testing::ValuesIn(kCmpInstructions));
+
+// ----------------------------------------------------------------------------
+// Shift instructions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorShiftTest =
+    InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorShiftTest, Immediate) {
+  const MachInst2 dpi = GetParam();
+  const MachineType type = dpi.machine_type;
+  TRACED_FORRANGE(int32_t, imm, 0,
+                  ((1 << ElementSizeLog2Of(type.representation())) * 8) - 1) {
+    StreamBuilder m(this, type, type);
+    m.Return((m.*dpi.constructor)(m.Parameter(0), m.Int32Constant(imm)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(dpi.arch_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_TRUE(s[0]->InputAt(1)->IsImmediate());
+    EXPECT_EQ(imm, s.ToInt32(s[0]->InputAt(1)));
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest, InstructionSelectorShiftTest,
+                         ::testing::ValuesIn(kShiftInstructions));
+
+// ----------------------------------------------------------------------------
+// Logical instructions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorLogicalTest =
+    InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorLogicalTest, Parameter) {
+  const MachInst2 dpi = GetParam();
+  const MachineType type = dpi.machine_type;
+  StreamBuilder m(this, type, type, type);
+  m.Return((m.*dpi.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(dpi.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorLogicalTest,
+                         ::testing::ValuesIn(kLogicalInstructions));
+
+TEST_F(InstructionSelectorTest, Word32XorMinusOneWithParameter) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Xor(m.Parameter(0), m.Int32Constant(-1)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvNor, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Xor(m.Int32Constant(-1), m.Parameter(0)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvNor, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+TEST_F(InstructionSelectorTest, Word32XorMinusOneWithWord32Or) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Xor(m.Word32Or(m.Parameter(0), m.Parameter(0)),
+                         m.Int32Constant(-1)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvNor, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Xor(m.Int32Constant(-1),
+                         m.Word32Or(m.Parameter(0), m.Parameter(0))));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvNor, s[0]->arch_opcode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+TEST_F(InstructionSelectorTest, Word32ShlWithWord32And) {
+  TRACED_FORRANGE(int32_t, shift, 0, 30) {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    Node* const p0 = m.Parameter(0);
+    Node* const r =
+        m.Word32Shl(m.Word32And(p0, m.Int32Constant((1 << (31 - shift)) - 1)),
+                    m.Int32Constant(shift + 1));
+    m.Return(r);
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvShl32, s[0]->arch_opcode());
+    ASSERT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+    ASSERT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(s.ToVreg(r), s.ToVreg(s[0]->Output()));
+  }
+}
+
+// TEST_F(InstructionSelectorTest, Word64ShlWithWord64And) {
+//   TRACED_FORRANGE(int32_t, shift, 0, 62) {
+//     StreamBuilder m(this, MachineType::Int64(), MachineType::Int64());
+//     Node* const p0 = m.Parameter(0);
+//     Node* const r =
+//         m.Word64Shl(m.Word64And(p0, m.Int64Constant((1L << (63 - shift)) -
+//         1)),
+//                     m.Int64Constant(shift + 1));
+//     m.Return(r);
+//     Stream s = m.Build();
+//     ASSERT_EQ(1U, s.size());
+//     EXPECT_EQ(kRiscvShl64, s[0]->arch_opcode());
+//     ASSERT_EQ(2U, s[0]->InputCount());
+//     EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+//     ASSERT_EQ(1U, s[0]->OutputCount());
+//     EXPECT_EQ(s.ToVreg(r), s.ToVreg(s[0]->Output()));
+//   }
+// }
+
+TEST_F(InstructionSelectorTest, Word32SarWithWord32Shl) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    Node* const p0 = m.Parameter(0);
+    Node* const r =
+        m.Word32Sar(m.Word32Shl(p0, m.Int32Constant(24)), m.Int32Constant(24));
+    m.Return(r);
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvSignExtendByte, s[0]->arch_opcode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+    ASSERT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(s.ToVreg(r), s.ToVreg(s[0]->Output()));
+  }
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    Node* const p0 = m.Parameter(0);
+    Node* const r =
+        m.Word32Sar(m.Word32Shl(p0, m.Int32Constant(16)), m.Int32Constant(16));
+    m.Return(r);
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvSignExtendShort, s[0]->arch_opcode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+    ASSERT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(s.ToVreg(r), s.ToVreg(s[0]->Output()));
+  }
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    Node* const p0 = m.Parameter(0);
+    Node* const r =
+        m.Word32Sar(m.Word32Shl(p0, m.Int32Constant(32)), m.Int32Constant(32));
+    m.Return(r);
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvShl32, s[0]->arch_opcode());
+    ASSERT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+    EXPECT_EQ(0, s.ToInt32(s[0]->InputAt(1)));
+    ASSERT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(s.ToVreg(r), s.ToVreg(s[0]->Output()));
+  }
+}
+
+// ----------------------------------------------------------------------------
+// MUL/DIV instructions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorMulDivTest =
+    InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorMulDivTest, Parameter) {
+  const MachInst2 dpi = GetParam();
+  const MachineType type = dpi.machine_type;
+  StreamBuilder m(this, type, type, type);
+  m.Return((m.*dpi.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(dpi.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest, InstructionSelectorMulDivTest,
+                         ::testing::ValuesIn(kMulDivInstructions));
+
+// ----------------------------------------------------------------------------
+// MOD instructions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorModTest = InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorModTest, Parameter) {
+  const MachInst2 dpi = GetParam();
+  const MachineType type = dpi.machine_type;
+  StreamBuilder m(this, type, type, type);
+  m.Return((m.*dpi.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(dpi.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest, InstructionSelectorModTest,
+                         ::testing::ValuesIn(kModInstructions));
+
+// ----------------------------------------------------------------------------
+// Floating point instructions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorFPArithTest =
+    InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorFPArithTest, Parameter) {
+  const MachInst2 fpa = GetParam();
+  StreamBuilder m(this, fpa.machine_type, fpa.machine_type, fpa.machine_type);
+  m.Return((m.*fpa.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(fpa.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorFPArithTest,
+                         ::testing::ValuesIn(kFPArithInstructions));
+// ----------------------------------------------------------------------------
+// Integer arithmetic
+// ----------------------------------------------------------------------------
+using InstructionSelectorIntArithTwoTest =
+    InstructionSelectorTestWithParam<MachInst2>;
+
+TEST_P(InstructionSelectorIntArithTwoTest, Parameter) {
+  const MachInst2 intpa = GetParam();
+  StreamBuilder m(this, intpa.machine_type, intpa.machine_type,
+                  intpa.machine_type);
+  m.Return((m.*intpa.constructor)(m.Parameter(0), m.Parameter(1)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(intpa.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorIntArithTwoTest,
+                         ::testing::ValuesIn(kAddSubInstructions));
+
+// ----------------------------------------------------------------------------
+// One node.
+// ----------------------------------------------------------------------------
+
+using InstructionSelectorIntArithOneTest =
+    InstructionSelectorTestWithParam<MachInst1>;
+
+TEST_P(InstructionSelectorIntArithOneTest, Parameter) {
+  const MachInst1 intpa = GetParam();
+  StreamBuilder m(this, intpa.machine_type, intpa.machine_type,
+                  intpa.machine_type);
+  m.Return((m.*intpa.constructor)(m.Parameter(0)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(intpa.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(2U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorIntArithOneTest,
+                         ::testing::ValuesIn(kAddSubOneInstructions));
+// ----------------------------------------------------------------------------
+// Conversions.
+// ----------------------------------------------------------------------------
+using InstructionSelectorConversionTest =
+    InstructionSelectorTestWithParam<Conversion>;
+
+TEST_P(InstructionSelectorConversionTest, Parameter) {
+  const Conversion conv = GetParam();
+  StreamBuilder m(this, conv.mi.machine_type, conv.src_machine_type);
+  m.Return((m.*conv.mi.constructor)(m.Parameter(0)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(conv.mi.arch_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(1U, s[0]->InputCount());
+  EXPECT_EQ(1U, s[0]->OutputCount());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorConversionTest,
+                         ::testing::ValuesIn(kConversionInstructions));
+
+using CombineChangeFloat32ToInt32WithRoundFloat32 =
+    InstructionSelectorTestWithParam<Conversion>;
+
+TEST_P(CombineChangeFloat32ToInt32WithRoundFloat32, Parameter) {
+  {
+    const Conversion conv = GetParam();
+    StreamBuilder m(this, conv.mi.machine_type, conv.src_machine_type);
+    m.Return(m.ChangeFloat64ToInt32(
+        m.ChangeFloat32ToFloat64((m.*conv.mi.constructor)(m.Parameter(0)))));
+    Stream s = m.Build();
+    ASSERT_EQ(2U, s.size());
+    EXPECT_EQ(conv.mi.arch_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kRiscvTruncWS, s[1]->arch_opcode());
+    EXPECT_EQ(kMode_None, s[0]->addressing_mode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         CombineChangeFloat32ToInt32WithRoundFloat32,
+                         ::testing::ValuesIn(kFloat32RoundInstructions));
+
+TEST_F(InstructionSelectorTest, ChangeFloat64ToInt32OfChangeFloat32ToFloat64) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Float32());
+    m.Return(m.ChangeFloat64ToInt32(m.ChangeFloat32ToFloat64(m.Parameter(0))));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvTruncWS, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_None, s[0]->addressing_mode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+TEST_F(InstructionSelectorTest,
+       TruncateFloat64ToFloat32OfChangeInt32ToFloat64) {
+  {
+    StreamBuilder m(this, MachineType::Float32(), MachineType::Int32());
+    m.Return(
+        m.TruncateFloat64ToFloat32(m.ChangeInt32ToFloat64(m.Parameter(0))));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvCvtSW, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_None, s[0]->addressing_mode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+// ----------------------------------------------------------------------------
+// Loads and stores.
+// ----------------------------------------------------------------------------
+
+namespace {
+
+struct MemoryAccess {
+  MachineType type;
+  ArchOpcode load_opcode;
+  ArchOpcode store_opcode;
+};
+
+static const MemoryAccess kMemoryAccesses[] = {
+    {MachineType::Int8(), kRiscvLb, kRiscvSb},
+    {MachineType::Uint8(), kRiscvLbu, kRiscvSb},
+    {MachineType::Int16(), kRiscvLh, kRiscvSh},
+    {MachineType::Uint16(), kRiscvLhu, kRiscvSh},
+    {MachineType::Int32(), kRiscvLw, kRiscvSw},
+    {MachineType::Float32(), kRiscvLoadFloat, kRiscvStoreFloat},
+    {MachineType::Float64(), kRiscvLoadDouble, kRiscvStoreDouble}};
+
+struct MemoryAccessImm {
+  MachineType type;
+  ArchOpcode load_opcode;
+  ArchOpcode store_opcode;
+  bool (InstructionSelectorTest::Stream::*val_predicate)(
+      const InstructionOperand*) const;
+  const int32_t immediates[40];
+};
+
+std::ostream& operator<<(std::ostream& os, const MemoryAccessImm& acc) {
+  return os << acc.type;
+}
+
+struct MemoryAccessImm1 {
+  MachineType type;
+  ArchOpcode load_opcode;
+  ArchOpcode store_opcode;
+  bool (InstructionSelectorTest::Stream::*val_predicate)(
+      const InstructionOperand*) const;
+  const int32_t immediates[5];
+};
+
+std::ostream& operator<<(std::ostream& os, const MemoryAccessImm1& acc) {
+  return os << acc.type;
+}
+
+// ----------------------------------------------------------------------------
+// Loads and stores immediate values
+// ----------------------------------------------------------------------------
+
+const MemoryAccessImm kMemoryAccessesImm[] = {
+    {MachineType::Int8(),
+     kRiscvLb,
+     kRiscvSb,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Uint8(),
+     kRiscvLbu,
+     kRiscvSb,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Int16(),
+     kRiscvLh,
+     kRiscvSh,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Uint16(),
+     kRiscvLhu,
+     kRiscvSh,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Int32(),
+     kRiscvLw,
+     kRiscvSw,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Float32(),
+     kRiscvLoadFloat,
+     kRiscvStoreFloat,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Float64(),
+     kRiscvLoadDouble,
+     kRiscvStoreDouble,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}}};
+
+const MemoryAccessImm1 kMemoryAccessImmMoreThan16bit[] = {
+    {MachineType::Int8(),
+     kRiscvLb,
+     kRiscvSb,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Uint8(),
+     kRiscvLbu,
+     kRiscvSb,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Int16(),
+     kRiscvLh,
+     kRiscvSh,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Uint16(),
+     kRiscvLhu,
+     kRiscvSh,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Int32(),
+     kRiscvLw,
+     kRiscvSw,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Float32(),
+     kRiscvLoadFloat,
+     kRiscvStoreFloat,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-65000, -55000, 32777, 55000, 65000}},
+    {MachineType::Float64(),
+     kRiscvLoadDouble,
+     kRiscvStoreDouble,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-65000, -55000, 32777, 55000, 65000}}};
+
+#ifdef RISCV_HAS_NO_UNALIGNED
+struct MemoryAccessImm2 {
+  MachineType type;
+  ArchOpcode store_opcode;
+  ArchOpcode store_opcode_unaligned;
+  bool (InstructionSelectorTest::Stream::*val_predicate)(
+      const InstructionOperand*) const;
+  const int32_t immediates[40];
+};
+
+std::ostream& operator<<(std::ostream& os, const MemoryAccessImm2& acc) {
+  return os << acc.type;
+}
+
+const MemoryAccessImm2 kMemoryAccessesImmUnaligned[] = {
+    {MachineType::Int16(),
+     kRiscvUsh,
+     kRiscvSh,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Int32(),
+     kRiscvUsw,
+     kRiscvSw,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Int64(),
+     kRiscvUsd,
+     kRiscvSd,
+     &InstructionSelectorTest::Stream::IsInteger,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Float32(),
+     kRiscvUStoreFloat,
+     kRiscvStoreFloat,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}},
+    {MachineType::Float64(),
+     kRiscvUStoreDouble,
+     kRiscvStoreDouble,
+     &InstructionSelectorTest::Stream::IsDouble,
+     {-4095, -3340, -3231, -3224, -3088, -1758, -1203, -123, -117, -91,
+      -89,   -87,   -86,   -82,   -44,   -23,   -3,    0,    7,    10,
+      39,    52,    69,    71,    91,    92,    107,   109,  115,  124,
+      286,   655,   1362,  1569,  2587,  3067,  3096,  3462, 3510, 4095}}};
+#endif
+}  // namespace
+
+using InstructionSelectorMemoryAccessTest =
+    InstructionSelectorTestWithParam<MemoryAccess>;
+
+TEST_P(InstructionSelectorMemoryAccessTest, LoadWithParameters) {
+  const MemoryAccess memacc = GetParam();
+  StreamBuilder m(this, memacc.type, MachineType::Pointer(),
+                  MachineType::Int32());
+  m.Return(m.Load(memacc.type, m.Parameter(0)));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(memacc.load_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+}
+
+TEST_P(InstructionSelectorMemoryAccessTest, StoreWithParameters) {
+  const MemoryAccess memacc = GetParam();
+  StreamBuilder m(this, MachineType::Int32(), MachineType::Pointer(),
+                  MachineType::Int32(), memacc.type);
+  m.Store(memacc.type.representation(), m.Parameter(0), m.Parameter(1),
+          kNoWriteBarrier);
+  m.Return(m.Int32Constant(0));
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(memacc.store_opcode, s[0]->arch_opcode());
+  EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorMemoryAccessTest,
+                         ::testing::ValuesIn(kMemoryAccesses));
+
+// ----------------------------------------------------------------------------
+// Load immediate.
+// ----------------------------------------------------------------------------
+
+using InstructionSelectorMemoryAccessImmTest =
+    InstructionSelectorTestWithParam<MemoryAccessImm>;
+
+TEST_P(InstructionSelectorMemoryAccessImmTest, LoadWithImmediateIndex) {
+  const MemoryAccessImm memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, memacc.type, MachineType::Pointer());
+    m.Return(m.Load(memacc.type, m.Parameter(0), m.Int32Constant(index)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(memacc.load_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+    ASSERT_EQ(2U, s[0]->InputCount());
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[0]->InputAt(1)->kind());
+    EXPECT_EQ(index, s.ToInt32(s[0]->InputAt(1)));
+    ASSERT_EQ(1U, s[0]->OutputCount());
+    EXPECT_TRUE((s.*memacc.val_predicate)(s[0]->Output()));
+  }
+}
+
+// ----------------------------------------------------------------------------
+// Store immediate.
+// ----------------------------------------------------------------------------
+
+TEST_P(InstructionSelectorMemoryAccessImmTest, StoreWithImmediateIndex) {
+  const MemoryAccessImm memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Pointer(),
+                    memacc.type);
+    m.Store(memacc.type.representation(), m.Parameter(0),
+            m.Int32Constant(index), m.Parameter(1), kNoWriteBarrier);
+    m.Return(m.Int32Constant(0));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(memacc.store_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+    ASSERT_EQ(3U, s[0]->InputCount());
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[0]->InputAt(1)->kind());
+    EXPECT_EQ(index, s.ToInt32(s[0]->InputAt(1)));
+    EXPECT_EQ(0U, s[0]->OutputCount());
+  }
+}
+
+TEST_P(InstructionSelectorMemoryAccessImmTest, StoreZero) {
+  const MemoryAccessImm memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Pointer());
+    m.Store(memacc.type.representation(), m.Parameter(0),
+            m.Int32Constant(index), m.Int32Constant(0), kNoWriteBarrier);
+    m.Return(m.Int32Constant(0));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(memacc.store_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+    ASSERT_EQ(3U, s[0]->InputCount());
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[0]->InputAt(1)->kind());
+    EXPECT_EQ(index, s.ToInt32(s[0]->InputAt(1)));
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[0]->InputAt(2)->kind());
+    EXPECT_EQ(0, s.ToInt64(s[0]->InputAt(2)));
+    EXPECT_EQ(0U, s[0]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorMemoryAccessImmTest,
+                         ::testing::ValuesIn(kMemoryAccessesImm));
+
+#ifdef RISCV_HAS_NO_UNALIGNED
+using InstructionSelectorMemoryAccessUnalignedImmTest =
+    InstructionSelectorTestWithParam<MemoryAccessImm2>;
+
+TEST_P(InstructionSelectorMemoryAccessUnalignedImmTest, StoreZero) {
+  const MemoryAccessImm2 memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Pointer());
+    bool unaligned_store_supported =
+        m.machine()->UnalignedStoreSupported(memacc.type.representation());
+    m.UnalignedStore(memacc.type.representation(), m.Parameter(0),
+                     m.Int32Constant(index), m.Int32Constant(0));
+    m.Return(m.Int32Constant(0));
+    Stream s = m.Build();
+    uint32_t i = is_int12(index) ? 0 : 1;
+    ASSERT_EQ(i + 1, s.size());
+    EXPECT_EQ(unaligned_store_supported ? memacc.store_opcode_unaligned
+                                        : memacc.store_opcode,
+              s[i]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[i]->addressing_mode());
+    ASSERT_EQ(3U, s[i]->InputCount());
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[i]->InputAt(1)->kind());
+    EXPECT_EQ(i == 0 ? index : 0, s.ToInt32(s[i]->InputAt(1)));
+    ASSERT_EQ(InstructionOperand::IMMEDIATE, s[i]->InputAt(2)->kind());
+    EXPECT_EQ(0, s.ToInt64(s[i]->InputAt(2)));
+    EXPECT_EQ(0U, s[i]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorMemoryAccessUnalignedImmTest,
+                         ::testing::ValuesIn(kMemoryAccessesImmUnaligned));
+#endif
+// ----------------------------------------------------------------------------
+// Load/store offsets more than 16 bits.
+// ----------------------------------------------------------------------------
+
+using InstructionSelectorMemoryAccessImmMoreThan16bitTest =
+    InstructionSelectorTestWithParam<MemoryAccessImm1>;
+
+TEST_P(InstructionSelectorMemoryAccessImmMoreThan16bitTest,
+       LoadWithImmediateIndex) {
+  const MemoryAccessImm1 memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, memacc.type, MachineType::Pointer());
+    m.Return(m.Load(memacc.type, m.Parameter(0), m.Int32Constant(index)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(memacc.load_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+    EXPECT_EQ(2U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+TEST_P(InstructionSelectorMemoryAccessImmMoreThan16bitTest,
+       StoreWithImmediateIndex) {
+  const MemoryAccessImm1 memacc = GetParam();
+  TRACED_FOREACH(int32_t, index, memacc.immediates) {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Pointer(),
+                    memacc.type);
+    m.Store(memacc.type.representation(), m.Parameter(0),
+            m.Int32Constant(index), m.Parameter(1), kNoWriteBarrier);
+    m.Return(m.Int32Constant(0));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(memacc.store_opcode, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_MRI, s[0]->addressing_mode());
+    EXPECT_EQ(3U, s[0]->InputCount());
+    EXPECT_EQ(0U, s[0]->OutputCount());
+  }
+}
+
+INSTANTIATE_TEST_SUITE_P(InstructionSelectorTest,
+                         InstructionSelectorMemoryAccessImmMoreThan16bitTest,
+                         ::testing::ValuesIn(kMemoryAccessImmMoreThan16bit));
+
+// ----------------------------------------------------------------------------
+// kRiscvCmp with zero testing.
+// ----------------------------------------------------------------------------
+
+TEST_F(InstructionSelectorTest, Word32EqualWithZero) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Equal(m.Parameter(0), m.Int32Constant(0)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvCmpZero, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_None, s[0]->addressing_mode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(kFlags_set, s[0]->flags_mode());
+    EXPECT_EQ(kEqual, s[0]->flags_condition());
+  }
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32Equal(m.Int32Constant(0), m.Parameter(0)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvCmpZero, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_None, s[0]->addressing_mode());
+    ASSERT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+    EXPECT_EQ(kFlags_set, s[0]->flags_mode());
+    EXPECT_EQ(kEqual, s[0]->flags_condition());
+  }
+}
+
+TEST_F(InstructionSelectorTest, Word32Clz) {
+  StreamBuilder m(this, MachineType::Uint32(), MachineType::Uint32());
+  Node* const p0 = m.Parameter(0);
+  Node* const n = m.Word32Clz(p0);
+  m.Return(n);
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(kRiscvClz32, s[0]->arch_opcode());
+  ASSERT_EQ(1U, s[0]->InputCount());
+  EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+  ASSERT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(s.ToVreg(n), s.ToVreg(s[0]->Output()));
+}
+
+TEST_F(InstructionSelectorTest, Float32Abs) {
+  StreamBuilder m(this, MachineType::Float32(), MachineType::Float32());
+  Node* const p0 = m.Parameter(0);
+  Node* const n = m.Float32Abs(p0);
+  m.Return(n);
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(kRiscvAbsS, s[0]->arch_opcode());
+  ASSERT_EQ(1U, s[0]->InputCount());
+  EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+  ASSERT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(s.ToVreg(n), s.ToVreg(s[0]->Output()));
+}
+
+TEST_F(InstructionSelectorTest, Float64Abs) {
+  StreamBuilder m(this, MachineType::Float64(), MachineType::Float64());
+  Node* const p0 = m.Parameter(0);
+  Node* const n = m.Float64Abs(p0);
+  m.Return(n);
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(kRiscvAbsD, s[0]->arch_opcode());
+  ASSERT_EQ(1U, s[0]->InputCount());
+  EXPECT_EQ(s.ToVreg(p0), s.ToVreg(s[0]->InputAt(0)));
+  ASSERT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(s.ToVreg(n), s.ToVreg(s[0]->Output()));
+}
+
+TEST_F(InstructionSelectorTest, Float64Max) {
+  StreamBuilder m(this, MachineType::Float64(), MachineType::Float64(),
+                  MachineType::Float64());
+  Node* const p0 = m.Parameter(0);
+  Node* const p1 = m.Parameter(1);
+  Node* const n = m.Float64Max(p0, p1);
+  m.Return(n);
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(kRiscvFloat64Max, s[0]->arch_opcode());
+  ASSERT_EQ(2U, s[0]->InputCount());
+  ASSERT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(s.ToVreg(n), s.ToVreg(s[0]->Output()));
+}
+
+TEST_F(InstructionSelectorTest, Float64Min) {
+  StreamBuilder m(this, MachineType::Float64(), MachineType::Float64(),
+                  MachineType::Float64());
+  Node* const p0 = m.Parameter(0);
+  Node* const p1 = m.Parameter(1);
+  Node* const n = m.Float64Min(p0, p1);
+  m.Return(n);
+  Stream s = m.Build();
+  ASSERT_EQ(1U, s.size());
+  EXPECT_EQ(kRiscvFloat64Min, s[0]->arch_opcode());
+  ASSERT_EQ(2U, s[0]->InputCount());
+  ASSERT_EQ(1U, s[0]->OutputCount());
+  EXPECT_EQ(s.ToVreg(n), s.ToVreg(s[0]->Output()));
+}
+
+TEST_F(InstructionSelectorTest, Word32ReverseBytes) {
+  {
+    StreamBuilder m(this, MachineType::Int32(), MachineType::Int32());
+    m.Return(m.Word32ReverseBytes(m.Parameter(0)));
+    Stream s = m.Build();
+    ASSERT_EQ(1U, s.size());
+    // EXPECT_EQ(kRiscvByteSwap32, s[0]->arch_opcode());
+    EXPECT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+TEST_F(InstructionSelectorTest, ExternalReferenceLoad1) {
+  // Test offsets we can use kMode_Root for.
+  const int32_t kOffsets[] = {0, 1, 4, INT32_MIN, INT32_MAX};
+  TRACED_FOREACH(int64_t, offset, kOffsets) {
+    StreamBuilder m(this, MachineType::Int32());
+    ExternalReference reference = base::bit_cast<ExternalReference>(
+        (int32_t)(isolate()->isolate_root() + offset));
+    Node* const value =
+        m.Load(MachineType::Int32(), m.ExternalConstant(reference));
+    m.Return(value);
+
+    Stream s = m.Build();
+
+    ASSERT_EQ(1U, s.size());
+    EXPECT_EQ(kRiscvLw, s[0]->arch_opcode());
+    EXPECT_EQ(kMode_Root, s[0]->addressing_mode());
+    EXPECT_EQ(1U, s[0]->InputCount());
+    EXPECT_EQ(s.ToInt64(s[0]->InputAt(0)), offset);
+    EXPECT_EQ(1U, s[0]->OutputCount());
+  }
+}
+
+}  // namespace compiler
+}  // namespace internal
+}  // namespace v8
diff --git a/test/unittests/regexp/regexp-unittest.cc b/test/unittests/regexp/regexp-unittest.cc
index af8cf187a66..8ce5fb187cd 100644
--- a/test/unittests/regexp/regexp-unittest.cc
+++ b/test/unittests/regexp/regexp-unittest.cc
@@ -623,6 +623,8 @@ using ArchRegExpMacroAssembler = RegExpMacroAssemblerMIPS;
 using ArchRegExpMacroAssembler = RegExpMacroAssemblerLOONG64;
 #elif V8_TARGET_ARCH_RISCV64
 using ArchRegExpMacroAssembler = RegExpMacroAssemblerRISCV;
+#elif V8_TARGET_ARCH_RISCV32
+using ArchRegExpMacroAssembler = RegExpMacroAssemblerRISCV;
 #endif
 
 class ContextInitializer {
diff --git a/test/unittests/unittests.status b/test/unittests/unittests.status
index ef7e0cea1d1..de2c92141e1 100644
--- a/test/unittests/unittests.status
+++ b/test/unittests/unittests.status
@@ -145,7 +145,7 @@
 }],
 
 ################################################################################
-['is_clang == False and arch == riscv64',{
+['is_clang == False and (arch == riscv64 or arch == riscv32)',{
   'LoggingTest.SourceLocation':[SKIP]  # issue-174
 }],
 
diff --git a/test/unittests/wasm/liftoff-register-unittests.cc b/test/unittests/wasm/liftoff-register-unittests.cc
index e2ebb50dc12..065491e831e 100644
--- a/test/unittests/wasm/liftoff-register-unittests.cc
+++ b/test/unittests/wasm/liftoff-register-unittests.cc
@@ -21,8 +21,8 @@
 #include "src/execution/s390/frame-constants-s390.h"
 #elif V8_TARGET_ARCH_PPC64
 #include "src/execution/ppc/frame-constants-ppc.h"
-#elif V8_TARGET_ARCH_RISCV64
-#include "src/execution/riscv64/frame-constants-riscv64.h"
+#elif V8_TARGET_ARCH_RISCV32 || V8_TARGET_ARCH_RISCV64
+#include "src/execution/riscv/frame-constants-riscv.h"
 #endif
 
 #include "src/base/macros.h"
diff --git a/test/wasm-spec-tests/wasm-spec-tests.status b/test/wasm-spec-tests/wasm-spec-tests.status
index d3e4b2698eb..d5366434ff5 100644
--- a/test/wasm-spec-tests/wasm-spec-tests.status
+++ b/test/wasm-spec-tests/wasm-spec-tests.status
@@ -101,7 +101,23 @@
 
    # SIMD is not fully implemented yet.
    'simd*': [SKIP],
-}],  # 'arch == riscv64
+}],  # 'arch == riscv64'
+
+
+
+['arch == riscv32', {
+   # These tests need larger stack size on simulator.
+   'skip-stack-guard-page': '--sim-stack-size=8192',
+   'proposals/tail-call/skip-stack-guard-page': '--sim-stack-size=8192',
+
+   # SIMD is not fully implemented yet.
+   'simd*': [SKIP],
+
+   'func': ['variant == stress', SKIP],
+   'i64': ['variant == stress', SKIP],
+   'proposals/js-types/i64': ['variant == stress', SKIP],
+   'proposals/memory64/i64': ['variant == stress', SKIP],
+}],  # 'arch == riscv32'
 
 ['arch == ppc or arch == ppc64', {
   # These tests fail because ppc float min and max doesn't convert sNaN to qNaN.
diff --git a/tools/dev/gm.py b/tools/dev/gm.py
index 9b04f130e23..79fe4db617d 100755
--- a/tools/dev/gm.py
+++ b/tools/dev/gm.py
@@ -41,8 +41,8 @@ BUILD_TARGETS_ALL = ["all"]
 # All arches that this script understands.
 ARCHES = [
     "ia32", "x64", "arm", "arm64", "mipsel", "mips64el", "ppc", "ppc64",
-    "riscv64", "s390", "s390x", "android_arm", "android_arm64", "loong64",
-    "fuchsia_x64", "fuchsia_arm64"
+    "riscv32", "riscv64", "s390", "s390x", "android_arm", "android_arm64",
+    "loong64", "fuchsia_x64", "fuchsia_arm64"
 ]
 # Arches that get built/run when you don't specify any.
 DEFAULT_ARCHES = ["ia32", "x64", "arm", "arm64"]
@@ -325,7 +325,7 @@ class Config(object):
     elif self.arch == "android_arm64" or self.arch == "fuchsia_arm64":
       v8_cpu = "arm64"
     elif self.arch in ("arm", "arm64", "mipsel", "mips64el", "ppc", "ppc64",
-                       "riscv64", "s390", "s390x", "loong64"):
+                       "riscv64", "riscv32", "s390", "s390x", "loong64"):
       v8_cpu = self.arch
     else:
       return []
diff --git a/tools/generate-header-include-checks.py b/tools/generate-header-include-checks.py
index 9b74d793795..fd95b83004b 100755
--- a/tools/generate-header-include-checks.py
+++ b/tools/generate-header-include-checks.py
@@ -51,7 +51,7 @@ AUTO_EXCLUDE_PATTERNS = [
     # platform-specific headers
     '\\b{}\\b'.format(p)
     for p in ('win', 'win32', 'ia32', 'x64', 'arm', 'arm64', 'mips', 'mips64',
-              's390', 'ppc', 'riscv64', 'loong64')
+              's390', 'ppc', 'riscv', 'riscv64', 'riscv32', 'loong64')
 ]
 
 args = None
diff --git a/tools/testrunner/build_config.py b/tools/testrunner/build_config.py
index 402253e6a2b..6cb6e1dcaca 100644
--- a/tools/testrunner/build_config.py
+++ b/tools/testrunner/build_config.py
@@ -7,7 +7,7 @@ from testrunner.local import utils
 # Increase the timeout for these:
 SLOW_ARCHS = [
     "arm", "arm64", "mips", "mipsel", "mips64", "mips64el", "s390", "s390x",
-    "riscv64", "loong64"
+    "riscv32", "riscv64", "loong64"
 ]
 
 
@@ -93,7 +93,8 @@ class BuildConfig(object):
        self.arch == 'mipsel':
       no_simd_hardware = not self.simd_mips
 
-    if self.arch == 'loong64':
+    if self.arch == 'loong64'  or \
+       self.arch == 'riscv32':
       no_simd_hardware = True
 
     # S390 hosts without VEF1 do not support Simd.
diff --git a/tools/testrunner/local/statusfile.py b/tools/testrunner/local/statusfile.py
index 4880d9379c6..2053f52526d 100644
--- a/tools/testrunner/local/statusfile.py
+++ b/tools/testrunner/local/statusfile.py
@@ -61,7 +61,7 @@ for var in [
     "debug", "release", "big", "little", "android", "arm", "arm64", "ia32",
     "mips", "mipsel", "mips64", "mips64el", "x64", "ppc", "ppc64", "s390",
     "s390x", "macos", "windows", "linux", "aix", "r1", "r2", "r3", "r5", "r6",
-    "riscv64", "loong64"
+    "riscv32", "riscv64", "loong64"
 ]:
   VARIABLES[var] = var
 
-- 
2.35.1

