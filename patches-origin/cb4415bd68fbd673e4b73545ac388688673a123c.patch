From cb4415bd68fbd673e4b73545ac388688673a123c Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Stephen=20R=C3=B6ttger?= <sroettger@google.com>
Date: Thu, 16 Mar 2023 10:09:40 +0000
Subject: [PATCH] Revert "Move data fields from InstructionStream to Code"
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This reverts commit ff909db74eefcf68656e190bd6e3fc0edc0f7790.

Reason for revert: breaks build on arm no ptr compression

Original change's description:
> Move data fields from InstructionStream to Code
>
> and use the Code object directly wherever possible, effectively treating
> the Code+IStream tuple as a single object.
>
> This cl includes:
> * moving nearly all data fields from IStream to Code
> * passing around/storing the code object instead of istream in various places
> * remove unnecessary loads (e.g. in the arch specific code, we don't need to load the istream anymore)
> * add a code member to reloc_info besides istream
> * add a RelocIterator constructor that takes code + istream (during serialization / gc, the ptrs between are not always up to date)
> * remove InstructionStream from various visit* functions since it's passed in RelocInfo anyway
>
> Submitting with `No-Try: true` due to infra issues with linux-rel. (crbug.com/1423329#c12). I tested linux-rel separately in crbug.com/4338946 and it passes, so the assumption is that it will also pass in the v8 autoroll tests.
>
> No-Try: true
> Cq-Include-Trybots: luci.v8.try:v8_linux64_gc_stress_custom_snapshot_dbg,v8_linux64_gc_stress_dbg,v8_linux_gc_stress_dbg,v8_mac64_gc_stress_dbg
> Bug: v8:13784
> Change-Id: I91716f0dc0059517746e271c9fe70403c20a01eb
> Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4295670
> Reviewed-by: Jakob Linke <jgruber@chromium.org>
> Reviewed-by: Clemens Backes <clemensb@chromium.org>
> Commit-Queue: Stephen Röttger <sroettger@google.com>
> Reviewed-by: Dominik Inführ <dinfuehr@chromium.org>
> Cr-Commit-Position: refs/heads/main@{#86478}

Bug: v8:13784
Change-Id: I75053c668a5a61cace436f625bcbab4a331a9b80
Cq-Include-Trybots: luci.v8.try:v8_linux64_gc_stress_custom_snapshot_dbg,v8_linux64_gc_stress_dbg,v8_linux_gc_stress_dbg,v8_mac64_gc_stress_dbg
No-Presubmit: true
No-Tree-Checks: true
No-Try: true
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4342402
Auto-Submit: Stephen Röttger <sroettger@google.com>
Owners-Override: Manos Koukoutos <manoskouk@chromium.org>
Commit-Queue: Manos Koukoutos <manoskouk@chromium.org>
Reviewed-by: Manos Koukoutos <manoskouk@chromium.org>
Bot-Commit: Rubber Stamper <rubber-stamper@appspot.gserviceaccount.com>
Cr-Commit-Position: refs/heads/main@{#86480}
---
 src/api/api.cc                                |   7 +-
 src/builtins/arm/builtins-arm.cc              |  20 +-
 src/builtins/arm64/builtins-arm64.cc          |  20 +-
 src/builtins/builtins.cc                      |   2 +-
 src/builtins/builtins.h                       |   2 +-
 src/builtins/ia32/builtins-ia32.cc            |  20 +-
 src/builtins/setup-builtins-internal.cc       |  16 +-
 src/builtins/x64/builtins-x64.cc              |  16 +-
 src/codegen/arm/assembler-arm-inl.h           |   8 +-
 src/codegen/arm/assembler-arm.cc              |   2 +-
 src/codegen/arm/assembler-arm.h               |   2 +-
 src/codegen/arm/macro-assembler-arm.cc        |  11 +-
 src/codegen/arm/macro-assembler-arm.h         |   6 +
 src/codegen/arm64/assembler-arm64-inl.h       |   6 +-
 src/codegen/arm64/assembler-arm64.cc          |   4 +-
 src/codegen/arm64/assembler-arm64.h           |   5 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  14 +-
 src/codegen/arm64/macro-assembler-arm64.h     |   6 +
 src/codegen/code-stub-assembler.cc            |   7 +-
 src/codegen/compiler.cc                       |  15 +-
 src/codegen/external-reference.cc             |   4 +-
 src/codegen/handler-table.cc                  |   4 +
 src/codegen/ia32/assembler-ia32-inl.h         |   6 +-
 src/codegen/ia32/assembler-ia32.cc            |   2 +-
 src/codegen/ia32/assembler-ia32.h             |   2 +-
 src/codegen/ia32/macro-assembler-ia32.cc      |  10 +-
 src/codegen/ia32/macro-assembler-ia32.h       |   6 +
 src/codegen/maglev-safepoint-table.cc         |   7 +
 src/codegen/maglev-safepoint-table.h          |   2 +
 src/codegen/reloc-info.cc                     |  85 +--
 src/codegen/reloc-info.h                      |  47 +-
 src/codegen/safepoint-table.cc                |   5 +
 src/codegen/source-position.cc                |   2 +-
 src/codegen/source-position.h                 |   2 +-
 src/codegen/x64/assembler-x64-inl.h           |   6 +-
 src/codegen/x64/assembler-x64.cc              |   2 +-
 src/codegen/x64/assembler-x64.h               |   2 +-
 src/codegen/x64/macro-assembler-x64.cc        |  12 +-
 src/codegen/x64/macro-assembler-x64.h         |   6 +
 .../backend/arm/code-generator-arm.cc         |   2 +-
 .../backend/ia32/code-generator-ia32.cc       |   2 +-
 src/compiler/heap-refs.cc                     |  22 +-
 src/compiler/heap-refs.h                      |  10 +
 src/compiler/pipeline.cc                      |   5 +-
 src/debug/debug-evaluate.cc                   |   2 +-
 src/deoptimizer/deoptimizer.cc                |  33 +-
 src/deoptimizer/deoptimizer.h                 |   8 +-
 src/diagnostics/disassembler.cc               |  18 +-
 src/diagnostics/objects-debug.cc              |  21 +-
 src/diagnostics/objects-printer.cc            |   4 +
 src/execution/frames.cc                       |   6 +-
 src/execution/isolate.cc                      |  38 +-
 src/extensions/statistics-extension.cc        |   4 +-
 src/heap/concurrent-marking.cc                |   8 +-
 src/heap/evacuation-verifier.cc               |  15 +-
 src/heap/evacuation-verifier.h                |  10 +-
 src/heap/factory-base.cc                      |  47 +-
 src/heap/factory-base.h                       |  23 +-
 src/heap/factory.cc                           | 122 ++--
 src/heap/heap-verifier.cc                     |  37 +-
 src/heap/heap-write-barrier-inl.h             |  13 +-
 src/heap/heap-write-barrier.cc                |  17 +-
 src/heap/heap-write-barrier.h                 |   5 +-
 src/heap/heap.cc                              |  69 +-
 src/heap/heap.h                               |   8 +-
 src/heap/mark-compact-inl.h                   |   5 +-
 src/heap/mark-compact.cc                      | 135 ++--
 src/heap/mark-compact.h                       |  12 +-
 src/heap/marking-barrier.cc                   |  13 +-
 src/heap/marking-barrier.h                    |   3 +-
 src/heap/marking-visitor-inl.h                |  33 +-
 src/heap/marking-visitor.h                    |  23 +-
 src/heap/object-stats.cc                      |  20 +-
 src/heap/read-only-spaces.cc                  |   7 +-
 src/heap/remembered-set-inl.h                 |  14 +-
 src/heap/scavenger-inl.h                      |  16 +-
 src/heap/scavenger.cc                         |  16 +-
 src/heap/sweeper.cc                           |  16 +-
 src/heap/weak-object-worklists.h              |   2 +-
 src/logging/code-events.h                     |  13 +-
 src/logging/log.cc                            |  19 +-
 src/logging/log.h                             |  23 +-
 src/objects/code-inl.h                        | 602 +++++++++++++-----
 src/objects/code.cc                           |  86 ++-
 src/objects/code.h                            | 529 +++++++++------
 src/objects/js-function-inl.h                 |   5 +
 src/objects/js-function.h                     |   6 +
 src/objects/js-regexp.cc                      |   4 +-
 src/objects/js-regexp.h                       |   3 +-
 src/objects/objects-body-descriptors-inl.h    |  22 +-
 src/objects/objects.cc                        |   3 +-
 src/objects/visitors.h                        |  25 +-
 src/profiler/heap-snapshot-generator.cc       |  41 +-
 src/profiler/heap-snapshot-generator.h        |   2 +-
 src/profiler/profiler-listener.cc             |  11 +-
 src/profiler/profiler-listener.h              |   9 +-
 src/regexp/arm/regexp-macro-assembler-arm.cc  |   3 +-
 .../arm64/regexp-macro-assembler-arm64.cc     |   3 +-
 .../ia32/regexp-macro-assembler-ia32.cc       |   3 +-
 src/regexp/regexp-macro-assembler.cc          |   2 +-
 src/regexp/regexp.cc                          |   9 +-
 src/regexp/x64/regexp-macro-assembler-x64.cc  |   3 +-
 src/runtime/runtime-compiler.cc               |   6 +-
 src/runtime/runtime-test.cc                   |   6 +-
 src/snapshot/code-serializer.cc               |   2 +-
 src/snapshot/deserializer.cc                  |  58 +-
 src/snapshot/embedded/embedded-data.cc        |  24 +-
 src/snapshot/embedded/embedded-file-writer.cc |   2 +-
 src/snapshot/serializer.cc                    |  81 ++-
 src/snapshot/serializer.h                     |  16 +-
 src/snapshot/startup-serializer.cc            |  14 +-
 src/wasm/module-compiler.cc                   |   7 +-
 src/wasm/wasm-code-manager.cc                 |   6 +-
 src/wasm/wasm-code-manager.h                  |   2 +-
 test/cctest/compiler/codegen-tester.h         |   4 +
 test/cctest/compiler/function-tester.cc       |  15 +
 test/cctest/compiler/function-tester.h        |   2 +
 test/cctest/compiler/test-code-generator.cc   |   4 +-
 test/cctest/compiler/test-multiple-return.cc  |  24 +-
 .../cctest/heap/test-concurrent-allocation.cc |  13 +-
 test/cctest/heap/test-heap.cc                 |  21 +-
 test/cctest/test-cpu-profiler.cc              |   2 +-
 test/cctest/test-serialize.cc                 |   4 +-
 test/cctest/test-unwinder-code-pages.cc       |  21 +-
 test/fuzzer/multi-return.cc                   |   3 +-
 .../assembler/macro-assembler-x64-unittest.cc |   3 +-
 test/unittests/compiler/codegen-tester.h      |   4 +
 test/unittests/compiler/function-tester.cc    |  15 +
 test/unittests/regexp/regexp-unittest.cc      |  29 +-
 tools/gen-postmortem-metadata.py              |   7 +-
 130 files changed, 1867 insertions(+), 1219 deletions(-)

diff --git a/src/api/api.cc b/src/api/api.cc
index 05846797e99..2e8aba78461 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -6946,9 +6946,10 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
   void VisitMapPointer(i::HeapObject host) final {
     VisitPointer(host, host.map_slot());
   }
-  void VisitCodePointer(i::Code host, i::CodeObjectSlot slot) final {}
-  void VisitCodeTarget(i::RelocInfo* rinfo) final {}
-  void VisitEmbeddedPointer(i::RelocInfo* rinfo) final {}
+  void VisitCodePointer(i::HeapObject host, i::CodeObjectSlot slot) final {}
+  void VisitCodeTarget(i::InstructionStream host, i::RelocInfo* rinfo) final {}
+  void VisitEmbeddedPointer(i::InstructionStream host,
+                            i::RelocInfo* rinfo) final {}
   void VisitCustomWeakPointers(i::HeapObject host, i::ObjectSlot start,
                                i::ObjectSlot end) final {}
 
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 6a928db24b8..aecf2e5219a 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -1774,15 +1774,20 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ LeaveFrame(StackFrame::STUB);
   }
 
+  __ LoadCodeInstructionStreamNonBuiltin(r0, r0);
+
   // Load deoptimization data from the code object.
   // <deopt_data> = <code>[#deoptimization_data_offset]
-  __ ldr(r1,
-         FieldMemOperand(r0, Code::kDeoptimizationDataOrInterpreterDataOffset));
-
-  __ LoadCodeEntry(r0, r0);
+  __ ldr(
+      r1,
+      FieldMemOperand(
+          r0, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
 
   {
     ConstantPoolUnavailableScope constant_pool_unavailable(masm);
+    __ add(r0, r0,
+           Operand(InstructionStream::kHeaderSize -
+                   kHeapObjectTag));  // InstructionStream start
 
     // Load the OSR entrypoint offset from the deoptimization data.
     // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
@@ -3603,6 +3608,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, r3);
   }
+  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = r2;
@@ -3670,15 +3676,17 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ PrepareCallCFunction(3, 0);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
   __ add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
     UseScratchRegisterScope temps(masm);
     ResetBytecodeAge(masm, kInterpreterBytecodeArrayRegister, temps.Acquire());
-    Generate_OSREntry(masm, code_obj);
+    Generate_OSREntry(masm, code_obj,
+                      Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
   } else {
+    __ add(code_obj, code_obj,
+           Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
     __ Jump(code_obj);
   }
   __ Trap();  // Unreachable.
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index b6eaf6e330a..c16655c918d 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -2005,11 +2005,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ LeaveFrame(StackFrame::STUB);
   }
 
+  __ LoadCodeInstructionStreamNonBuiltin(x0, x0);
+
   // Load deoptimization data from the code object.
   // <deopt_data> = <code>[#deoptimization_data_offset]
   __ LoadTaggedField(
       x1,
-      FieldMemOperand(x0, Code::kDeoptimizationDataOrInterpreterDataOffset));
+      FieldMemOperand(
+          x0, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   // <osr_offset> = <deopt_data>[#header_size + #osr_pc_offset]
@@ -2017,11 +2020,10 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       x1, FieldMemOperand(x1, FixedArray::OffsetOfElementAt(
                                   DeoptimizationData::kOsrPcOffsetIndex)));
 
-  __ LoadCodeEntry(x0, x0);
-
-  // Compute the target address = code_entry + osr_offset
-  // <entry_addr> = <code_entry> + <osr_offset>
-  Generate_OSREntry(masm, x0, x1);
+  // Compute the target address = code_obj + header_size + osr_offset
+  // <entry_addr> = <code_obj> + #header_size + <osr_offset>
+  __ Add(x0, x0, x1);
+  Generate_OSREntry(masm, x0, InstructionStream::kHeaderSize - kHeapObjectTag);
 }
 
 }  // namespace
@@ -5734,6 +5736,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, x3);
   }
+  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = x2;
@@ -5800,14 +5803,15 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallCFunction(get_baseline_pc, 3, 0);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
   __ Add(code_obj, code_obj, kReturnRegister0);
   __ Pop(kInterpreterAccumulatorRegister, padreg);
 
   if (is_osr) {
     ResetBytecodeAge(masm, kInterpreterBytecodeArrayRegister);
-    Generate_OSREntry(masm, code_obj);
+    Generate_OSREntry(masm, code_obj,
+                      InstructionStream::kHeaderSize - kHeapObjectTag);
   } else {
+    __ Add(code_obj, code_obj, InstructionStream::kHeaderSize - kHeapObjectTag);
     __ Jump(code_obj);
   }
   __ Trap();  // Unreachable.
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index 2c84a811bc2..6992ffd9f3d 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -283,7 +283,7 @@ Address Builtins::CppEntryOf(Builtin builtin) {
 }
 
 // static
-bool Builtins::IsBuiltin(const Code code) {
+bool Builtins::IsBuiltin(const InstructionStream code) {
   return Builtins::IsBuiltinId(code.builtin_id());
 }
 
diff --git a/src/builtins/builtins.h b/src/builtins/builtins.h
index 91811adb8bb..b58147027c0 100644
--- a/src/builtins/builtins.h
+++ b/src/builtins/builtins.h
@@ -174,7 +174,7 @@ class Builtins {
 
   // True, iff the given code object is a builtin. Note that this does not
   // necessarily mean that its kind is InstructionStream::BUILTIN.
-  static bool IsBuiltin(const Code code);
+  static bool IsBuiltin(const InstructionStream code);
 
   // As above, but safe to access off the main thread since the check is done
   // by handle location. Similar to Heap::IsRootHandle.
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index bd02ab53e63..0e3c4fc6345 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -2741,9 +2741,13 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ leave();
   }
 
+  __ LoadCodeInstructionStreamNonBuiltin(eax, eax);
+
   // Load deoptimization data from the code object.
-  __ mov(ecx, Operand(eax, Code::kDeoptimizationDataOrInterpreterDataOffset -
-                               kHeapObjectTag));
+  __ mov(ecx,
+         Operand(eax,
+                 InstructionStream::kDeoptimizationDataOrInterpreterDataOffset -
+                     kHeapObjectTag));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   __ mov(ecx, Operand(ecx, FixedArray::OffsetOfElementAt(
@@ -2751,10 +2755,9 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
                                kHeapObjectTag));
   __ SmiUntag(ecx);
 
-  __ LoadCodeEntry(eax, eax);
-
-  // Compute the target address = code_entry + osr_offset
-  __ add(eax, ecx);
+  // Compute the target address = code_obj + header_size + osr_offset
+  __ lea(eax, Operand(eax, ecx, times_1,
+                      InstructionStream::kHeaderSize - kHeapObjectTag));
 
   Generate_OSREntry(masm, eax);
 }
@@ -4247,6 +4250,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, ecx);
   }
+  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = ecx;
@@ -4312,8 +4316,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
            kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
-  __ add(code_obj, kReturnRegister0);
+  __ lea(code_obj, FieldOperand(code_obj, kReturnRegister0, times_1,
+                                InstructionStream::kHeaderSize));
   __ pop(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index ca7b4ca57cd..11d08c8391c 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -243,19 +243,19 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
   PtrComprCageBase cage_base(isolate);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    InstructionStream code = FromCode(builtins->code(builtin));
     isolate->heap()->UnprotectAndRegisterMemoryChunk(
         code, UnprotectMemoryOrigin::kMainThread);
     bool flush_icache = false;
     for (RelocIterator it(code, kRelocMask); !it.done(); it.next()) {
       RelocInfo* rinfo = it.rinfo();
       if (RelocInfo::IsCodeTargetMode(rinfo->rmode())) {
-        Code target_code = Code::FromTargetAddress(rinfo->target_address());
-        DCHECK_IMPLIES(
-            RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
-            Builtins::IsIsolateIndependent(target_code.builtin_id()));
-        if (!target_code.is_builtin()) continue;
-        Code new_target = builtins->code(target_code.builtin_id());
+        InstructionStream target =
+            InstructionStream::FromTargetAddress(rinfo->target_address());
+        DCHECK_IMPLIES(RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
+                       Builtins::IsIsolateIndependent(target.builtin_id()));
+        if (!target.is_builtin()) continue;
+        Code new_target = builtins->code(target.builtin_id());
         rinfo->set_target_address(new_target.InstructionStart(),
                                   UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       } else {
@@ -271,7 +271,7 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
       flush_icache = true;
     }
     if (flush_icache) {
-      FlushInstructionCache(code.InstructionStart(), code.instruction_size());
+      FlushInstructionCache(code.instruction_start(), code.instruction_size());
     }
   }
 }
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 846e3eac9b3..cc06b159381 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -2686,11 +2686,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
     __ leave();
   }
 
+  __ LoadCodeInstructionStreamNonBuiltin(rax, rax);
+
   // Load deoptimization data from the code object.
   const TaggedRegister deopt_data(rbx);
   __ LoadTaggedField(
       deopt_data,
-      FieldOperand(rax, Code::kDeoptimizationDataOrInterpreterDataOffset));
+      FieldOperand(
+          rax, InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
 
   // Load the OSR entrypoint offset from the deoptimization data.
   __ SmiUntagField(
@@ -2698,10 +2701,8 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
       FieldOperand(deopt_data, FixedArray::OffsetOfElementAt(
                                    DeoptimizationData::kOsrPcOffsetIndex)));
 
-  __ LoadCodeEntry(rax, rax);
-
-  // Compute the target address = code_entry + osr_offset
-  __ addq(rax, rbx);
+  // Compute the target address = code_obj + header_size + osr_offset
+  __ leaq(rax, FieldOperand(rax, rbx, times_1, InstructionStream::kHeaderSize));
 
   Generate_OSREntry(masm, rax);
 }
@@ -5180,6 +5181,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (v8_flags.debug_code) {
     AssertCodeIsBaseline(masm, code_obj, r11);
   }
+  __ LoadCodeInstructionStreamNonBuiltin(code_obj, code_obj);
 
   // Load the feedback vector.
   Register feedback_vector = r11;
@@ -5246,8 +5248,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ movq(arg_reg_3, kInterpreterBytecodeArrayRegister);
     __ CallCFunction(get_baseline_pc, 3);
   }
-  __ LoadCodeEntry(code_obj, code_obj);
-  __ addq(code_obj, kReturnRegister0);
+  __ leaq(code_obj, FieldOperand(code_obj, kReturnRegister0, times_1,
+                                 InstructionStream::kHeaderSize));
   __ popq(kInterpreterAccumulatorRegister);
 
   if (is_osr) {
diff --git a/src/codegen/arm/assembler-arm-inl.h b/src/codegen/arm/assembler-arm-inl.h
index 0a0880db06d..7b912318a8b 100644
--- a/src/codegen/arm/assembler-arm-inl.h
+++ b/src/codegen/arm/assembler-arm-inl.h
@@ -112,8 +112,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   DCHECK(IsCodeTarget(rmode_) || IsFullEmbeddedObject(rmode_));
   Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                    icache_flush_mode);
-  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
 
@@ -191,8 +191,8 @@ void Assembler::emit(Instr x) {
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address constant_pool_entry, Code code, Address target) {
-  DCHECK(!Builtins::IsIsolateIndependentBuiltin(code));
+    Address constant_pool_entry, InstructionStream code, Address target) {
+  DCHECK(!Builtins::IsIsolateIndependentBuiltin(code.code(kAcquireLoad)));
   Memory<Address>(constant_pool_entry) = target;
 }
 
diff --git a/src/codegen/arm/assembler-arm.cc b/src/codegen/arm/assembler-arm.cc
index 179c309acbc..8e4e1171e93 100644
--- a/src/codegen/arm/assembler-arm.cc
+++ b/src/codegen/arm/assembler-arm.cc
@@ -5254,7 +5254,7 @@ void Assembler::dq(uint64_t value, RelocInfo::Mode rmode) {
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
   DCHECK_GE(buffer_space(), kMaxRelocSize);  // too late to grow buffer here
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/arm/assembler-arm.h b/src/codegen/arm/assembler-arm.h
index 9444abea188..cd68628b249 100644
--- a/src/codegen/arm/assembler-arm.h
+++ b/src/codegen/arm/assembler-arm.h
@@ -367,7 +367,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the constant pool on ARM).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address constant_pool_entry, Code code, Address target);
+      Address constant_pool_entry, InstructionStream code, Address target);
 
   // Get the size of the special target encoded at 'location'.
   inline static int deserialization_special_target_size(Address location);
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index 9be1d37e030..55958db53bf 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -344,6 +344,15 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
+void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                                         Register code_object) {
+  ASM_CODE_COMMENT(this);
+  // Compute the InstructionStream object pointer from the code entry point.
+  ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
+  sub(destination, destination,
+      Operand(InstructionStream::kHeaderSize - kHeapObjectTag));
+}
+
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadCodeEntry(code_object, code_object);
@@ -390,7 +399,7 @@ void MacroAssembler::Drop(Register count, Condition cond) {
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code,
                                                        Register scratch) {
   ldr(scratch, FieldMemOperand(code, Code::kKindSpecificFlagsOffset));
-  tst(scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
+  tst(scratch, Operand(1 << InstructionStream::kMarkedForDeoptimizationBit));
 }
 
 Operand MacroAssembler::ClearedValue() const {
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 971b21661b4..53b2406e3f4 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -336,6 +336,12 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
+  // Load code entry point from the Code object and compute
+  // InstructionStream object pointer out of it. Must not be used for
+  // Codes corresponding to builtins, because their entry points
+  // values point to the embedded instruction stream in .text section.
+  void LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index d564d062745..d6801ac7f33 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -548,7 +548,7 @@ int Assembler::deserialization_special_target_size(Address location) {
 }
 
 void Assembler::deserialization_set_special_target_at(Address location,
-                                                      Code code,
+                                                      InstructionStream code,
                                                       Address target) {
   Instruction* instr = reinterpret_cast<Instruction*>(location);
   if (instr->IsBranchAndLink() || instr->IsUnconditionalBranch()) {
@@ -696,8 +696,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
 
diff --git a/src/codegen/arm64/assembler-arm64.cc b/src/codegen/arm64/assembler-arm64.cc
index f753e0bcc82..fbaa2ad36e8 100644
--- a/src/codegen/arm64/assembler-arm64.cc
+++ b/src/codegen/arm64/assembler-arm64.cc
@@ -4533,7 +4533,7 @@ void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data,
   DCHECK(constpool_.IsBlocked());
 
   // We do not try to reuse pool constants.
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
                   InstructionStream());
 
   DCHECK_GE(buffer_space(), kMaxRelocSize);  // too late to grow buffer here
@@ -4660,7 +4660,7 @@ intptr_t Assembler::MaxPCOffsetAfterVeneerPoolIfEmittedNow(size_t margin) {
 void Assembler::RecordVeneerPool(int location_offset, int size) {
   Assembler::BlockPoolsScope block_pools(this, PoolEmissionCheck::kSkip);
   RelocInfo rinfo(reinterpret_cast<Address>(buffer_start_) + location_offset,
-                  RelocInfo::VENEER_POOL, static_cast<intptr_t>(size), Code(),
+                  RelocInfo::VENEER_POOL, static_cast<intptr_t>(size),
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/arm64/assembler-arm64.h b/src/codegen/arm64/assembler-arm64.h
index 5c10dd86979..431a1abba7a 100644
--- a/src/codegen/arm64/assembler-arm64.h
+++ b/src/codegen/arm64/assembler-arm64.h
@@ -277,9 +277,8 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination. 'location' here can be either the pc of
   // an immediate branch or the address of an entry in the constant pool.
   // This is for calls and branches within generated code.
-  inline static void deserialization_set_special_target_at(Address location,
-                                                           Code code,
-                                                           Address target);
+  inline static void deserialization_set_special_target_at(
+      Address location, InstructionStream code, Address target);
 
   // Get the size of the special target encoded at 'location'.
   inline static int deserialization_special_target_size(Address location);
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index ee7f5a37bee..b79bf1a4332 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -2372,6 +2372,15 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   Ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
+void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                                         Register code_object) {
+  ASM_CODE_COMMENT(this);
+  // Compute the InstructionStream object pointer from the code entry point.
+  Ldr(destination, FieldMemOperand(code_object, Code::kCodeEntryPointOffset));
+  Sub(destination, destination,
+      Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
+}
+
 void MacroAssembler::CallCodeObject(Register code_object) {
   ASM_CODE_COMMENT(this);
   LoadCodeEntry(code_object, code_object);
@@ -2451,7 +2460,8 @@ void MacroAssembler::BailoutIfDeoptimized() {
                   MemOperand(kJavaScriptCallCodeStartRegister, offset));
   Ldr(scratch.W(), FieldMemOperand(scratch, Code::kKindSpecificFlagsOffset));
   Label not_deoptimized;
-  Tbz(scratch.W(), Code::kMarkedForDeoptimizationBit, &not_deoptimized);
+  Tbz(scratch.W(), InstructionStream::kMarkedForDeoptimizationBit,
+      &not_deoptimized);
   Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
        RelocInfo::CODE_TARGET);
   Bind(&not_deoptimized);
@@ -2685,7 +2695,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
 void MacroAssembler::JumpIfCodeIsMarkedForDeoptimization(
     Register code, Register scratch, Label* if_marked_for_deoptimization) {
   Ldr(scratch.W(), FieldMemOperand(code, Code::kKindSpecificFlagsOffset));
-  Tbnz(scratch.W(), Code::kMarkedForDeoptimizationBit,
+  Tbnz(scratch.W(), InstructionStream::kMarkedForDeoptimizationBit,
        if_marked_for_deoptimization);
 }
 
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 9fc898ad125..7ccfa59818b 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1050,6 +1050,12 @@ class V8_EXPORT_PRIVATE MacroAssembler : public MacroAssemblerBase {
 
   // Load code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
+  // Load code entry point from the Code object and compute
+  // InstructionStream object pointer out of it. Must not be used for
+  // Codes corresponding to builtins, because their entry points
+  // values point to the embedded instruction stream in .text section.
+  void LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 75ffdee22dc..11a9934ee13 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -3179,7 +3179,8 @@ TNode<BytecodeArray> CodeStubAssembler::LoadSharedFunctionInfoBytecodeArray(
                           Int32Constant(static_cast<int>(CodeKind::BASELINE))));
 #endif  // DEBUG
     TNode<HeapObject> baseline_data = LoadObjectField<HeapObject>(
-        code, Code::kDeoptimizationDataOrInterpreterDataOffset);
+        FromCodeNonBuiltin(code),
+        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
     var_result = baseline_data;
   }
   Goto(&check_for_interpreter_data);
@@ -15686,8 +15687,8 @@ TNode<RawPtrT> CodeStubAssembler::GetCodeEntry(TNode<Code> code) {
 }
 
 TNode<BoolT> CodeStubAssembler::IsMarkedForDeoptimization(TNode<Code> code) {
-  return IsSetWord32<Code::MarkedForDeoptimizationField>(
-      LoadObjectField<Int16T>(code, Code::kKindSpecificFlagsOffset));
+  return IsSetWord32<InstructionStream::MarkedForDeoptimizationField>(
+      LoadObjectField<Int32T>(code, Code::kKindSpecificFlagsOffset));
 }
 
 TNode<JSFunction> CodeStubAssembler::AllocateFunctionWithMapAndContext(
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index 22f743a4a4b..d33fa418901 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -1731,13 +1731,18 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   }
 
   // The object graph for a newly compiled Script shouldn't yet contain any
-  // Code. If any of these functions are called, then that would indicate that
-  // the graph was not disjoint from the rest of the heap as expected.
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  // InstructionStream. If any of these functions are called, then that would
+  // indicate that the graph was not disjoint from the rest of the heap as
+  // expected.
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+    UNREACHABLE();
+  }
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+    UNREACHABLE();
+  }
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     UNREACHABLE();
   }
-  void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
  private:
   enum ObjectKind {
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 5b9981e9238..27e860dbfca 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -748,7 +748,7 @@ namespace {
 static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
                                              int bytecode_offset,
                                              Address raw_bytecode_array) {
-  Code code_obj = Code::cast(Object(raw_code_obj));
+  InstructionStream code_obj = InstructionStream::cast(Object(raw_code_obj));
   BytecodeArray bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj.GetBaselineStartPCForBytecodeOffset(bytecode_offset,
@@ -758,7 +758,7 @@ static uintptr_t BaselinePCForBytecodeOffset(Address raw_code_obj,
 static uintptr_t BaselinePCForNextExecutedBytecode(Address raw_code_obj,
                                                    int bytecode_offset,
                                                    Address raw_bytecode_array) {
-  Code code_obj = Code::cast(Object(raw_code_obj));
+  InstructionStream code_obj = InstructionStream::cast(Object(raw_code_obj));
   BytecodeArray bytecode_array =
       BytecodeArray::cast(Object(raw_bytecode_array));
   return code_obj.GetBaselinePCForNextExecutedBytecode(bytecode_offset,
diff --git a/src/codegen/handler-table.cc b/src/codegen/handler-table.cc
index a00f9113ffb..f2147a171ba 100644
--- a/src/codegen/handler-table.cc
+++ b/src/codegen/handler-table.cc
@@ -19,6 +19,10 @@
 namespace v8 {
 namespace internal {
 
+HandlerTable::HandlerTable(InstructionStream code)
+    : HandlerTable(code.handler_table_address(), code.handler_table_size(),
+                   kReturnAddressBasedEncoding) {}
+
 HandlerTable::HandlerTable(Code code)
     : HandlerTable(code.HandlerTableAddress(), code.handler_table_size(),
                    kReturnAddressBasedEncoding) {}
diff --git a/src/codegen/ia32/assembler-ia32-inl.h b/src/codegen/ia32/assembler-ia32-inl.h
index d5040b8aa4d..5ecbc5416bd 100644
--- a/src/codegen/ia32/assembler-ia32-inl.h
+++ b/src/codegen/ia32/assembler-ia32-inl.h
@@ -98,8 +98,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
 
@@ -224,7 +224,7 @@ void Assembler::set_target_address_at(Address pc, Address constant_pool,
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, Code code, Address target) {
+    Address instruction_payload, InstructionStream code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code.constant_pool() : kNullAddress,
                         target);
diff --git a/src/codegen/ia32/assembler-ia32.cc b/src/codegen/ia32/assembler-ia32.cc
index bb76c82a6f5..a9ee1ffccf7 100644
--- a/src/codegen/ia32/assembler-ia32.cc
+++ b/src/codegen/ia32/assembler-ia32.cc
@@ -3407,7 +3407,7 @@ void Assembler::dd(Label* label) {
 
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/ia32/assembler-ia32.h b/src/codegen/ia32/assembler-ia32.h
index 54cbbf6886d..918ef36e91e 100644
--- a/src/codegen/ia32/assembler-ia32.h
+++ b/src/codegen/ia32/assembler-ia32.h
@@ -421,7 +421,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x86).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, Code code, Address target);
+      Address instruction_payload, InstructionStream code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index 77d07785f5e..965d68f57e7 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -708,7 +708,7 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
 
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code) {
   test(FieldOperand(code, Code::kKindSpecificFlagsOffset),
-       Immediate(1 << Code::kMarkedForDeoptimizationBit));
+       Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
 }
 
 Immediate MacroAssembler::ClearedValue() const {
@@ -2005,6 +2005,14 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   mov(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
+void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                                         Register code_object) {
+  ASM_CODE_COMMENT(this);
+  // Compute the InstructionStream object pointer from the code entry point.
+  mov(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
+  sub(destination, Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
+}
+
 void MacroAssembler::CallCodeObject(Register code_object) {
   LoadCodeEntry(code_object, code_object);
   call(code_object);
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 39ee0f106e1..13946396f99 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -160,6 +160,12 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
+  // Load code entry point from the Code object and compute
+  // InstructionStream object pointer out of it. Must not be used for
+  // Codes corresponding to builtins, because their entry points
+  // values point to the embedded instruction stream in .text section.
+  void LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/codegen/maglev-safepoint-table.cc b/src/codegen/maglev-safepoint-table.cc
index 50adb61efd9..24374fdb5d8 100644
--- a/src/codegen/maglev-safepoint-table.cc
+++ b/src/codegen/maglev-safepoint-table.cc
@@ -12,6 +12,13 @@
 namespace v8 {
 namespace internal {
 
+MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
+                                           InstructionStream code)
+    : MaglevSafepointTable(code.instruction_start(),
+                           code.safepoint_table_address()) {
+  DCHECK(code.is_maglevved());
+}
+
 MaglevSafepointTable::MaglevSafepointTable(Isolate* isolate, Address pc,
                                            Code code)
     : MaglevSafepointTable(code.InstructionStart(isolate, pc),
diff --git a/src/codegen/maglev-safepoint-table.h b/src/codegen/maglev-safepoint-table.h
index 492346917f2..6d9569bbb72 100644
--- a/src/codegen/maglev-safepoint-table.h
+++ b/src/codegen/maglev-safepoint-table.h
@@ -66,6 +66,8 @@ class MaglevSafepointTable {
  public:
   // The isolate and pc arguments are used for figuring out whether pc
   // belongs to the embedded or un-embedded code blob.
+  explicit MaglevSafepointTable(Isolate* isolate, Address pc,
+                                InstructionStream code);
   explicit MaglevSafepointTable(Isolate* isolate, Address pc, Code code);
   MaglevSafepointTable(const MaglevSafepointTable&) = delete;
   MaglevSafepointTable& operator=(const MaglevSafepointTable&) = delete;
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index a7c4be40c4c..1a4b24fe141 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -253,64 +253,56 @@ void RelocIterator::next() {
   done_ = true;
 }
 
-RelocIterator::RelocIterator(Code code, int mode_mask)
+RelocIterator::RelocIterator(InstructionStream code, int mode_mask)
     : RelocIterator(code, code.unchecked_relocation_info(), mode_mask) {}
 
-RelocIterator::RelocIterator(Code code, ByteArray relocation_info,
-                             int mode_mask)
-    : RelocIterator(
-          code,
-          InstructionStream::unchecked_cast(code.raw_instruction_stream()),
-          InstructionStream::unchecked_cast(code.raw_instruction_stream())
-              .instruction_start(),
-          code.constant_pool(), relocation_info.GetDataEndAddress(),
-          relocation_info.GetDataStartAddress(), mode_mask) {}
-
-RelocIterator::RelocIterator(Code code, InstructionStream instruction_stream,
-                             ByteArray relocation_info, Address constant_pool,
+RelocIterator::RelocIterator(Code code, int mode_mask)
+    : RelocIterator(code.instruction_stream(),
+                    code.instruction_stream().unchecked_relocation_info(),
+                    mode_mask) {}
+
+RelocIterator::RelocIterator(InstructionStream code, ByteArray relocation_info,
                              int mode_mask)
-    : RelocIterator(code, instruction_stream,
-                    instruction_stream.instruction_start(), constant_pool,
+    : RelocIterator(code, code.instruction_start(), code.constant_pool(),
                     relocation_info.GetDataEndAddress(),
                     relocation_info.GetDataStartAddress(), mode_mask) {}
 
 RelocIterator::RelocIterator(const CodeReference code_reference, int mode_mask)
-    : RelocIterator(
-          Code(), InstructionStream(), code_reference.instruction_start(),
-          code_reference.constant_pool(), code_reference.relocation_end(),
-          code_reference.relocation_start(), mode_mask) {}
-
-RelocIterator::RelocIterator(EmbeddedData* embedded_data, Code code,
-                             int mode_mask)
-    : RelocIterator(code, code.instruction_stream(),
+    : RelocIterator(InstructionStream(), code_reference.instruction_start(),
+                    code_reference.constant_pool(),
+                    code_reference.relocation_end(),
+                    code_reference.relocation_start(), mode_mask) {}
+
+RelocIterator::RelocIterator(EmbeddedData* embedded_data,
+                             InstructionStream code, int mode_mask)
+    : RelocIterator(code,
                     embedded_data->InstructionStartOfBuiltin(code.builtin_id()),
                     code.constant_pool(),
                     code.relocation_start() + code.relocation_size(),
                     code.relocation_start(), mode_mask) {}
 
 RelocIterator::RelocIterator(const CodeDesc& desc, int mode_mask)
-    : RelocIterator(
-          Code(), InstructionStream(), reinterpret_cast<Address>(desc.buffer),
-          0, desc.buffer + desc.buffer_size,
-          desc.buffer + desc.buffer_size - desc.reloc_size, mode_mask) {}
+    : RelocIterator(InstructionStream(), reinterpret_cast<Address>(desc.buffer),
+                    0, desc.buffer + desc.buffer_size,
+                    desc.buffer + desc.buffer_size - desc.reloc_size,
+                    mode_mask) {}
 
 RelocIterator::RelocIterator(base::Vector<byte> instructions,
                              base::Vector<const byte> reloc_info,
                              Address const_pool, int mode_mask)
-    : RelocIterator(Code(), InstructionStream(),
+    : RelocIterator(InstructionStream(),
                     reinterpret_cast<Address>(instructions.begin()), const_pool,
                     reloc_info.begin() + reloc_info.size(), reloc_info.begin(),
                     mode_mask) {}
 
-RelocIterator::RelocIterator(Code code, InstructionStream instruction_stream,
-                             Address pc, Address constant_pool, const byte* pos,
+RelocIterator::RelocIterator(InstructionStream host, Address pc,
+                             Address constant_pool, const byte* pos,
                              const byte* end, int mode_mask)
     : pos_(pos), end_(end), mode_mask_(mode_mask) {
   // Relocation info is read backwards.
   DCHECK_GE(pos_, end_);
-  rinfo_.code_ = code;
+  rinfo_.host_ = host;
   rinfo_.pc_ = pc;
-  rinfo_.instruction_stream_ = instruction_stream;
   rinfo_.constant_pool_ = constant_pool;
   if (mode_mask_ == 0) pos_ = end_;
   next();
@@ -363,12 +355,11 @@ void RelocInfo::set_target_address(Address target,
          IsWasmCall(rmode_));
   Assembler::set_target_address_at(pc_, constant_pool_, target,
                                    icache_flush_mode);
-  if (!instruction_stream().is_null() && IsCodeTargetMode(rmode_) &&
+  if (!host().is_null() && IsCodeTargetMode(rmode_) &&
       !v8_flags.disable_write_barriers) {
     InstructionStream target_code =
         InstructionStream::FromTargetAddress(target);
-    WriteBarrierForCode(instruction_stream(), this, target_code,
-                        write_barrier_mode);
+    WriteBarrierForCode(host(), this, target_code, write_barrier_mode);
   }
 }
 
@@ -397,6 +388,16 @@ bool RelocInfo::HasTargetAddressAddress() const {
   return (ModeMask(rmode_) & kTargetAddressAddressModeMask) != 0;
 }
 
+bool RelocInfo::RequiresRelocationAfterCodegen(const CodeDesc& desc) {
+  RelocIterator it(desc, RelocInfo::PostCodegenRelocationMask());
+  return !it.done();
+}
+
+bool RelocInfo::RequiresRelocation(InstructionStream code) {
+  RelocIterator it(code, RelocInfo::kApplyMask);
+  return !it.done();
+}
+
 #ifdef ENABLE_DISASSEMBLER
 const char* RelocInfo::RelocModeName(RelocInfo::Mode rmode) {
   switch (rmode) {
@@ -469,10 +470,11 @@ void RelocInfo::Print(Isolate* isolate, std::ostream& os) {
        << ")";
   } else if (IsCodeTargetMode(rmode_)) {
     const Address code_target = target_address();
-    Code target_code = Code::FromTargetAddress(code_target);
-    os << " (" << CodeKindToString(target_code.kind());
-    if (Builtins::IsBuiltin(target_code)) {
-      os << " " << Builtins::name(target_code.builtin_id());
+    InstructionStream code = InstructionStream::FromTargetAddress(code_target);
+    DCHECK(code.IsInstructionStream());
+    os << " (" << CodeKindToString(code.kind());
+    if (Builtins::IsBuiltin(code)) {
+      os << " " << Builtins::name(code.builtin_id());
     }
     os << ")  (" << reinterpret_cast<const void*>(target_address()) << ")";
   } else if (IsConstPool(rmode_)) {
@@ -518,8 +520,9 @@ void RelocInfo::Verify(Isolate* isolate) {
       Address target = target_internal_reference();
       Address pc = target_internal_reference_address();
       Code lookup_result = isolate->heap()->FindCodeForInnerPointer(pc);
-      CHECK_GE(target, lookup_result.InstructionStart());
-      CHECK_LT(target, lookup_result.InstructionEnd());
+      InstructionStream code = lookup_result.instruction_stream();
+      CHECK(target >= code.instruction_start());
+      CHECK(target < code.instruction_end());
       break;
     }
     case OFF_HEAP_TARGET: {
diff --git a/src/codegen/reloc-info.h b/src/codegen/reloc-info.h
index 0f5b64757c9..169cb6a6502 100644
--- a/src/codegen/reloc-info.h
+++ b/src/codegen/reloc-info.h
@@ -114,14 +114,12 @@ class RelocInfo {
 
   RelocInfo() = default;
 
-  RelocInfo(Address pc, Mode rmode, intptr_t data, Code code,
-            InstructionStream instruction_stream,
+  RelocInfo(Address pc, Mode rmode, intptr_t data, InstructionStream host,
             Address constant_pool = kNullAddress)
       : pc_(pc),
         rmode_(rmode),
         data_(data),
-        code_(code),
-        instruction_stream_(instruction_stream),
+        host_(host),
         constant_pool_(constant_pool) {
     DCHECK_IMPLIES(!COMPRESS_POINTERS_BOOL,
                    rmode != COMPRESSED_EMBEDDED_OBJECT);
@@ -215,8 +213,7 @@ class RelocInfo {
   Address pc() const { return pc_; }
   Mode rmode() const { return rmode_; }
   intptr_t data() const { return data_; }
-  Code code() const { return code_; }
-  InstructionStream instruction_stream() const { return instruction_stream_; }
+  InstructionStream host() const { return host_; }
   Address constant_pool() const { return constant_pool_; }
 
   // Apply a relocation by delta bytes. When the code object is moved, PC
@@ -320,18 +317,23 @@ class RelocInfo {
   void Visit(ObjectVisitor* visitor) {
     Mode mode = rmode();
     if (IsEmbeddedObjectMode(mode)) {
-      visitor->VisitEmbeddedPointer(this);
+      visitor->VisitEmbeddedPointer(host(), this);
     } else if (IsCodeTargetMode(mode)) {
-      visitor->VisitCodeTarget(this);
+      visitor->VisitCodeTarget(host(), this);
     } else if (IsExternalReference(mode)) {
-      visitor->VisitExternalReference(this);
+      visitor->VisitExternalReference(host(), this);
     } else if (IsInternalReference(mode) || IsInternalReferenceEncoded(mode)) {
-      visitor->VisitInternalReference(this);
+      visitor->VisitInternalReference(host(), this);
     } else if (IsBuiltinEntryMode(mode)) {
-      visitor->VisitOffHeapTarget(this);
+      visitor->VisitOffHeapTarget(host(), this);
     }
   }
 
+  // Check whether the given code contains relocation information that
+  // either is position-relative or movable by the garbage collector.
+  static bool RequiresRelocationAfterCodegen(const CodeDesc& desc);
+  static bool RequiresRelocation(InstructionStream code);
+
 #ifdef ENABLE_DISASSEMBLER
   // Printing
   static const char* RelocModeName(Mode rmode);
@@ -373,8 +375,7 @@ class RelocInfo {
   Address pc_;
   Mode rmode_;
   intptr_t data_ = 0;
-  Code code_;
-  InstructionStream instruction_stream_;
+  InstructionStream host_;
   Address constant_pool_ = kNullAddress;
   friend class RelocIterator;
 };
@@ -428,15 +429,16 @@ class RelocInfoWriter {
 // A mask can be specified to skip unwanted modes.
 class V8_EXPORT_PRIVATE RelocIterator : public Malloced {
  public:
-  // Create a new iterator positioned at the beginning of the reloc info.
-  // Relocation information with mode k is included in the iteration iff bit k
-  // of mode_mask is set.
+  // Create a new iterator positioned at
+  // the beginning of the reloc info.
+  // Relocation information with mode k is included in the
+  // iteration iff bit k of mode_mask is set.
+  explicit RelocIterator(InstructionStream code, int mode_mask = -1);
   explicit RelocIterator(Code code, int mode_mask = -1);
-  explicit RelocIterator(Code code, ByteArray relocation_info, int mode_mask);
-  explicit RelocIterator(Code code, InstructionStream instruction_stream,
-                         ByteArray relocation_info, Address constant_pool,
+  explicit RelocIterator(InstructionStream code, ByteArray relocation_info,
+                         int mode_mask);
+  explicit RelocIterator(EmbeddedData* embedded_data, InstructionStream code,
                          int mode_mask);
-  explicit RelocIterator(EmbeddedData* embedded_data, Code code, int mode_mask);
   explicit RelocIterator(const CodeDesc& desc, int mode_mask = -1);
   explicit RelocIterator(const CodeReference code_reference,
                          int mode_mask = -1);
@@ -459,9 +461,8 @@ class V8_EXPORT_PRIVATE RelocIterator : public Malloced {
   }
 
  private:
-  RelocIterator(Code code, InstructionStream instruction_stream, Address pc,
-                Address constant_pool, const byte* pos, const byte* end,
-                int mode_mask);
+  RelocIterator(InstructionStream host, Address pc, Address constant_pool,
+                const byte* pos, const byte* end, int mode_mask);
 
   // Advance* moves the position before/after reading.
   // *Read* reads from current byte(s) into rinfo_.
diff --git a/src/codegen/safepoint-table.cc b/src/codegen/safepoint-table.cc
index b1ae0d8a7eb..ef3793a0644 100644
--- a/src/codegen/safepoint-table.cc
+++ b/src/codegen/safepoint-table.cc
@@ -20,6 +20,11 @@
 namespace v8 {
 namespace internal {
 
+SafepointTable::SafepointTable(Isolate* isolate, Address pc,
+                               InstructionStream code)
+    : SafepointTable(code.instruction_start(), code.safepoint_table_address()) {
+}
+
 SafepointTable::SafepointTable(Isolate* isolate, Address pc, Code code)
     : SafepointTable(code.InstructionStart(isolate, pc),
                      code.SafepointTableAddress()) {}
diff --git a/src/codegen/source-position.cc b/src/codegen/source-position.cc
index 60ff9f8f980..d705ebd2f6f 100644
--- a/src/codegen/source-position.cc
+++ b/src/codegen/source-position.cc
@@ -126,7 +126,7 @@ void SourcePosition::PrintJson(std::ostream& out) const {
   }
 }
 
-void SourcePosition::Print(std::ostream& out, Code code) const {
+void SourcePosition::Print(std::ostream& out, InstructionStream code) const {
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(code.deoptimization_data());
   if (!isInlined()) {
diff --git a/src/codegen/source-position.h b/src/codegen/source-position.h
index 5d6b1a93f76..0f9922b6a3c 100644
--- a/src/codegen/source-position.h
+++ b/src/codegen/source-position.h
@@ -85,7 +85,7 @@ class SourcePosition final {
       Isolate* isolate, OptimizedCompilationInfo* cinfo) const;
   SourcePositionInfo FirstInfo(Isolate* isolate, Code code) const;
 
-  void Print(std::ostream& out, Code code) const;
+  void Print(std::ostream& out, InstructionStream code) const;
   void PrintJson(std::ostream& out) const;
 
   int ScriptOffset() const {
diff --git a/src/codegen/x64/assembler-x64-inl.h b/src/codegen/x64/assembler-x64-inl.h
index dc51e84ff4e..1aabc0c7b33 100644
--- a/src/codegen/x64/assembler-x64-inl.h
+++ b/src/codegen/x64/assembler-x64-inl.h
@@ -215,7 +215,7 @@ void Assembler::deserialization_set_target_internal_reference_at(
 }
 
 void Assembler::deserialization_set_special_target_at(
-    Address instruction_payload, Code code, Address target) {
+    Address instruction_payload, InstructionStream code, Address target) {
   set_target_address_at(instruction_payload,
                         !code.is_null() ? code.constant_pool() : kNullAddress,
                         target);
@@ -349,8 +349,8 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!instruction_stream().is_null() && !v8_flags.disable_write_barriers) {
-    WriteBarrierForCode(instruction_stream(), this, target, write_barrier_mode);
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
+    WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
 
diff --git a/src/codegen/x64/assembler-x64.cc b/src/codegen/x64/assembler-x64.cc
index b271271de58..ff650d91e42 100644
--- a/src/codegen/x64/assembler-x64.cc
+++ b/src/codegen/x64/assembler-x64.cc
@@ -4492,7 +4492,7 @@ void Assembler::dq(Label* label) {
 
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   if (!ShouldRecordRelocInfo(rmode)) return;
-  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data, Code(),
+  RelocInfo rinfo(reinterpret_cast<Address>(pc_), rmode, data,
                   InstructionStream());
   reloc_info_writer.Write(&rinfo);
 }
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index a8b7d775361..6ede04b50ad 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -530,7 +530,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // This sets the branch destination (which is in the instruction on x64).
   // This is for calls and branches within generated code.
   inline static void deserialization_set_special_target_at(
-      Address instruction_payload, Code code, Address target);
+      Address instruction_payload, InstructionStream code, Address target);
 
   // Get the size of the special target encoded at 'instruction_payload'.
   inline static int deserialization_special_target_size(
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 6c1c25db829..68434a1992d 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -2264,6 +2264,14 @@ void MacroAssembler::LoadCodeEntry(Register destination, Register code_object) {
   movq(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
 }
 
+void MacroAssembler::LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                                         Register code_object) {
+  ASM_CODE_COMMENT(this);
+  // Compute the InstructionStream object pointer from the code entry point.
+  movq(destination, FieldOperand(code_object, Code::kCodeEntryPointOffset));
+  subq(destination, Immediate(InstructionStream::kHeaderSize - kHeapObjectTag));
+}
+
 void MacroAssembler::CallCodeObject(Register code_object) {
   LoadCodeEntry(code_object, code_object);
   call(code_object);
@@ -2564,7 +2572,7 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
 
 void MacroAssembler::TestCodeIsMarkedForDeoptimization(Register code) {
   testl(FieldOperand(code, Code::kKindSpecificFlagsOffset),
-        Immediate(1 << Code::kMarkedForDeoptimizationBit));
+        Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
 }
 
 Immediate MacroAssembler::ClearedValue() const {
@@ -3322,7 +3330,7 @@ void MacroAssembler::BailoutIfDeoptimized(Register scratch) {
   int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;
   LoadTaggedField(scratch, Operand(kJavaScriptCallCodeStartRegister, offset));
   testl(FieldOperand(scratch, Code::kKindSpecificFlagsOffset),
-        Immediate(1 << Code::kMarkedForDeoptimizationBit));
+        Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
   Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
        RelocInfo::CODE_TARGET, not_zero);
 }
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 24cf7fcfabc..674cd8e331f 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -411,6 +411,12 @@ class V8_EXPORT_PRIVATE MacroAssembler
 
   // Load the code entry point from the Code object.
   void LoadCodeEntry(Register destination, Register code_object);
+  // Load code entry point from the Code object and compute
+  // InstructionStream object pointer out of it. Must not be used for
+  // Codes corresponding to builtins, because their entry points
+  // values point to the embedded instruction stream in .text section.
+  void LoadCodeInstructionStreamNonBuiltin(Register destination,
+                                           Register code_object);
   void CallCodeObject(Register code_object);
   void JumpCodeObject(Register code_object,
                       JumpMode jump_mode = JumpMode::kJump);
diff --git a/src/compiler/backend/arm/code-generator-arm.cc b/src/compiler/backend/arm/code-generator-arm.cc
index 43c94b3e9db..1fa882fc4ee 100644
--- a/src/compiler/backend/arm/code-generator-arm.cc
+++ b/src/compiler/backend/arm/code-generator-arm.cc
@@ -649,7 +649,7 @@ void CodeGenerator::BailoutIfDeoptimized() {
   int offset = InstructionStream::kCodeOffset - InstructionStream::kHeaderSize;
   __ ldr(scratch, MemOperand(kJavaScriptCallCodeStartRegister, offset));
   __ ldr(scratch, FieldMemOperand(scratch, Code::kKindSpecificFlagsOffset));
-  __ tst(scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
+  __ tst(scratch, Operand(1 << InstructionStream::kMarkedForDeoptimizationBit));
   __ Jump(BUILTIN_CODE(isolate(), CompileLazyDeoptimizedCode),
           RelocInfo::CODE_TARGET, ne);
 }
diff --git a/src/compiler/backend/ia32/code-generator-ia32.cc b/src/compiler/backend/ia32/code-generator-ia32.cc
index fe1df09b985..3f97b94d9c5 100644
--- a/src/compiler/backend/ia32/code-generator-ia32.cc
+++ b/src/compiler/backend/ia32/code-generator-ia32.cc
@@ -664,7 +664,7 @@ void CodeGenerator::BailoutIfDeoptimized() {
   __ push(eax);  // Push eax so we can use it as a scratch register.
   __ mov(eax, Operand(kJavaScriptCallCodeStartRegister, offset));
   __ test(FieldOperand(eax, Code::kKindSpecificFlagsOffset),
-          Immediate(1 << Code::kMarkedForDeoptimizationBit));
+          Immediate(1 << InstructionStream::kMarkedForDeoptimizationBit));
   __ pop(eax);  // Restore eax.
 
   Label skip;
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index dbde14e3fce..c82abacf7a9 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -2270,10 +2270,9 @@ std::ostream& operator<<(std::ostream& os, const ObjectRef& ref) {
   }
 }
 
-unsigned CodeRef::GetInlinedBytecodeSize() const {
-  Code code = *object();
-  if (!code.has_instruction_stream()) return 0;
+namespace {
 
+unsigned GetInlinedBytecodeSizeImpl(InstructionStream code) {
   unsigned value = code.inlined_bytecode_size();
   if (value > 0) {
     // Don't report inlined bytecode size if the code object was already
@@ -2283,6 +2282,23 @@ unsigned CodeRef::GetInlinedBytecodeSize() const {
   return value;
 }
 
+}  // namespace
+
+unsigned InstructionStreamRef::GetInlinedBytecodeSize() const {
+  return GetInlinedBytecodeSizeImpl(*object());
+}
+
+unsigned CodeRef::GetInlinedBytecodeSize() const {
+  Code code = *object();
+  Object maybe_istream = code.raw_instruction_stream(kRelaxedLoad);
+  if (maybe_istream == Smi::zero()) return 0;
+
+  // Safe to do a relaxed conversion to InstructionStream here since
+  // Code::instruction_stream field is modified only by GC and the Code was
+  // acquire-loaded.
+  return GetInlinedBytecodeSizeImpl(InstructionStream::cast(maybe_istream));
+}
+
 #undef BIMODAL_ACCESSOR
 #undef BIMODAL_ACCESSOR_B
 #undef BIMODAL_ACCESSOR_C
diff --git a/src/compiler/heap-refs.h b/src/compiler/heap-refs.h
index 6695e684b71..231494ad620 100644
--- a/src/compiler/heap-refs.h
+++ b/src/compiler/heap-refs.h
@@ -113,6 +113,7 @@ enum class RefSerializationKind {
   BACKGROUND_SERIALIZED(BigInt)                                               \
   NEVER_SERIALIZED(CallHandlerInfo)                                           \
   NEVER_SERIALIZED(Cell)                                                      \
+  NEVER_SERIALIZED(InstructionStream)                                         \
   NEVER_SERIALIZED(Code)                                                      \
   NEVER_SERIALIZED(Context)                                                   \
   NEVER_SERIALIZED(DescriptorArray)                                           \
@@ -1104,6 +1105,15 @@ class JSGlobalProxyRef : public JSObjectRef {
   Handle<JSGlobalProxy> object() const;
 };
 
+class InstructionStreamRef : public HeapObjectRef {
+ public:
+  DEFINE_REF_CONSTRUCTOR(InstructionStream, HeapObjectRef)
+
+  Handle<InstructionStream> object() const;
+
+  unsigned GetInlinedBytecodeSize() const;
+};
+
 class CodeRef : public HeapObjectRef {
  public:
   DEFINE_REF_CONSTRUCTOR(Code, HeapObjectRef)
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index 58d4b9ae95e..c169835e679 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -1332,16 +1332,17 @@ PipelineCompilationJob::Status PipelineCompilationJob::FinalizeJobImpl(
 
 void PipelineCompilationJob::RegisterWeakObjectsInOptimizedCode(
     Isolate* isolate, Handle<NativeContext> context, Handle<Code> code) {
+  Handle<InstructionStream> istream(code->instruction_stream(), isolate);
   std::vector<Handle<Map>> maps;
   DCHECK(code->is_optimized_code());
   {
     DisallowGarbageCollection no_gc;
     PtrComprCageBase cage_base(isolate);
     int const mode_mask = RelocInfo::EmbeddedObjectModeMask();
-    for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
+    for (RelocIterator it(*istream, mode_mask); !it.done(); it.next()) {
       DCHECK(RelocInfo::IsEmbeddedObjectMode(it.rinfo()->rmode()));
       HeapObject target_object = it.rinfo()->target_object(cage_base);
-      if (code->IsWeakObjectInOptimizedCode(target_object)) {
+      if (istream->IsWeakObjectInOptimizedCode(target_object)) {
         if (target_object.IsMap(cage_base)) {
           maps.push_back(handle(Map::cast(target_object), isolate));
         }
diff --git a/src/debug/debug-evaluate.cc b/src/debug/debug-evaluate.cc
index 48349f193dc..686d43d81fa 100644
--- a/src/debug/debug-evaluate.cc
+++ b/src/debug/debug-evaluate.cc
@@ -1234,7 +1234,7 @@ void DebugEvaluate::VerifyTransitiveBuiltins(Isolate* isolate) {
   for (Builtin caller = Builtins::kFirst; caller <= Builtins::kLast; ++caller) {
     DebugInfo::SideEffectState state = BuiltinGetSideEffectState(caller);
     if (state != DebugInfo::kHasNoSideEffect) continue;
-    Code code = isolate->builtins()->code(caller);
+    InstructionStream code = FromCode(isolate->builtins()->code(caller));
     int mode = RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
                RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
 
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index ee5ee898a09..a6ab2452cc5 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -338,8 +338,9 @@ void Deoptimizer::DeoptimizeAll(Isolate* isolate) {
 
   // Mark all code, then deoptimize.
   {
-    Code::OptimizedCodeIterator it(isolate);
-    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
+    InstructionStream::OptimizedCodeIterator it(isolate);
+    for (InstructionStream code = it.Next(); !code.is_null();
+         code = it.Next()) {
       code.set_marked_for_deoptimization(true);
     }
   }
@@ -383,8 +384,9 @@ void Deoptimizer::DeoptimizeAllOptimizedCodeWithFunction(
   // Mark all code that inlines this function, then deoptimize.
   bool any_marked = false;
   {
-    Code::OptimizedCodeIterator it(isolate);
-    for (Code code = it.Next(); !code.is_null(); code = it.Next()) {
+    InstructionStream::OptimizedCodeIterator it(isolate);
+    for (InstructionStream code = it.Next(); !code.is_null();
+         code = it.Next()) {
       if (code.Inlines(*function)) {
         code.set_marked_for_deoptimization(true);
         any_marked = true;
@@ -447,9 +449,10 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   }
 
   DCHECK_NE(from, kNullAddress);
-  compiled_code_ = isolate_->heap()->FindCodeForInnerPointer(from);
+  compiled_code_ =
+      isolate_->heap()->FindCodeForInnerPointer(from).instruction_stream();
   DCHECK(!compiled_code_.is_null());
-  DCHECK(compiled_code_.IsCode());
+  DCHECK(compiled_code_.IsInstructionStream());
 
   DCHECK(function.IsJSFunction());
 #ifdef DEBUG
@@ -474,7 +477,7 @@ Deoptimizer::Deoptimizer(Isolate* isolate, JSFunction function,
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(compiled_code_.deoptimization_data());
   Address deopt_start =
-      compiled_code_.InstructionStart() + deopt_data.DeoptExitStart().value();
+      compiled_code_.instruction_start() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * kEagerDeoptExitSize;
@@ -503,8 +506,8 @@ Handle<JSFunction> Deoptimizer::function() const {
   return Handle<JSFunction>(function_, isolate());
 }
 
-Handle<Code> Deoptimizer::compiled_code() const {
-  return Handle<Code>(compiled_code_, isolate());
+Handle<InstructionStream> Deoptimizer::compiled_code() const {
+  return Handle<InstructionStream>(compiled_code_, isolate());
 }
 
 Deoptimizer::~Deoptimizer() {
@@ -601,11 +604,12 @@ void Deoptimizer::TraceDeoptEnd(double deopt_duration) {
 }
 
 // static
-void Deoptimizer::TraceMarkForDeoptimization(Isolate* isolate, Code code,
+void Deoptimizer::TraceMarkForDeoptimization(InstructionStream code,
                                              const char* reason) {
   if (!v8_flags.trace_deopt && !v8_flags.log_deopt) return;
 
   DisallowGarbageCollection no_gc;
+  Isolate* isolate = code.GetIsolate();
   Object maybe_data = code.deoptimization_data();
   if (maybe_data == ReadOnlyRoots(isolate).empty_fixed_array()) return;
 
@@ -1891,13 +1895,13 @@ namespace {
 // points to immediately after the deopt call).
 //
 // See also the Deoptimizer constructor.
-Address GetDeoptCallPCFromReturnPC(Address return_pc, Code code) {
+Address GetDeoptCallPCFromReturnPC(Address return_pc, InstructionStream code) {
   DCHECK_GT(Deoptimizer::kEagerDeoptExitSize, 0);
   DCHECK_GT(Deoptimizer::kLazyDeoptExitSize, 0);
   DeoptimizationData deopt_data =
       DeoptimizationData::cast(code.deoptimization_data());
   Address deopt_start =
-      code.InstructionStart() + deopt_data.DeoptExitStart().value();
+      code.instruction_start() + deopt_data.DeoptExitStart().value();
   int eager_deopt_count = deopt_data.EagerDeoptCount().value();
   Address lazy_deopt_start =
       deopt_start + eager_deopt_count * Deoptimizer::kEagerDeoptExitSize;
@@ -1956,8 +1960,9 @@ unsigned Deoptimizer::ComputeIncomingArgumentSize(SharedFunctionInfo shared) {
   return parameter_slots * kSystemPointerSize;
 }
 
-Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(Code code, Address pc) {
-  CHECK(code.InstructionStart() <= pc && pc <= code.InstructionEnd());
+Deoptimizer::DeoptInfo Deoptimizer::GetDeoptInfo(InstructionStream code,
+                                                 Address pc) {
+  CHECK(code.instruction_start() <= pc && pc <= code.instruction_end());
   SourcePosition last_position = SourcePosition::Unknown();
   DeoptimizeReason last_reason = DeoptimizeReason::kUnknown;
   uint32_t last_node_id = 0;
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index 02eec6d87be..0cdf66460e6 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -43,7 +43,7 @@ class Deoptimizer : public Malloced {
     const int deopt_id;
   };
 
-  static DeoptInfo GetDeoptInfo(Code code, Address from);
+  static DeoptInfo GetDeoptInfo(InstructionStream code, Address from);
   DeoptInfo GetDeoptInfo() const {
     return Deoptimizer::GetDeoptInfo(compiled_code_, from_);
   }
@@ -51,7 +51,7 @@ class Deoptimizer : public Malloced {
   static const char* MessageFor(DeoptimizeKind kind);
 
   Handle<JSFunction> function() const;
-  Handle<Code> compiled_code() const;
+  Handle<InstructionStream> compiled_code() const;
   DeoptimizeKind deopt_kind() const { return deopt_kind_; }
 
   // Where the deopt exit occurred *in the outermost frame*, i.e in the
@@ -130,7 +130,7 @@ class Deoptimizer : public Malloced {
   V8_EXPORT_PRIVATE static const int kLazyDeoptExitSize;
 
   // Tracing.
-  static void TraceMarkForDeoptimization(Isolate* isolate, Code code,
+  static void TraceMarkForDeoptimization(InstructionStream code,
                                          const char* reason);
   static void TraceEvictFromOptimizedCodeCache(Isolate* isolate,
                                                SharedFunctionInfo sfi,
@@ -189,7 +189,7 @@ class Deoptimizer : public Malloced {
 
   Isolate* isolate_;
   JSFunction function_;
-  Code compiled_code_;
+  InstructionStream compiled_code_;
   unsigned deopt_exit_index_;
   BytecodeOffset bytecode_offset_in_outermost_frame_ = BytecodeOffset::None();
   DeoptimizeKind deopt_kind_;
diff --git a/src/diagnostics/disassembler.cc b/src/diagnostics/disassembler.cc
index 6f0b60f41a0..241251b1562 100644
--- a/src/diagnostics/disassembler.cc
+++ b/src/diagnostics/disassembler.cc
@@ -380,17 +380,17 @@ static int DecodeIt(Isolate* isolate, ExternalReferenceEncoder* ref_encoder,
       const CodeReference& host = code;
       Address constant_pool =
           host.is_null() ? kNullAddress : host.constant_pool();
-      Handle<Code> code_handle;
+      InstructionStream instruction_stream;
       if (host.is_code()) {
-        code_handle = host.as_code();
+        instruction_stream = host.as_code()->instruction_stream();
+      }
 
-        RelocInfo relocinfo(pcs[i], rmodes[i], datas[i], *code_handle,
-                            code_handle->instruction_stream(), constant_pool);
+      RelocInfo relocinfo(pcs[i], rmodes[i], datas[i], instruction_stream,
+                          constant_pool);
 
-        bool first_reloc_info = (i == 0);
-        PrintRelocInfo(out, isolate, ref_encoder, os, code, &relocinfo,
-                       first_reloc_info);
-      }
+      bool first_reloc_info = (i == 0);
+      PrintRelocInfo(out, isolate, ref_encoder, os, code, &relocinfo,
+                     first_reloc_info);
     }
 
     // If this is a constant pool load and we haven't found any RelocInfo
@@ -402,7 +402,7 @@ static int DecodeIt(Isolate* isolate, ExternalReferenceEncoder* ref_encoder,
     // by IsInConstantPool() below.
     if (pcs.empty() && !code.is_null() && !decoding_constant_pool) {
       RelocInfo dummy_rinfo(reinterpret_cast<Address>(prev_pc),
-                            RelocInfo::NO_INFO, 0, Code(), InstructionStream());
+                            RelocInfo::NO_INFO, 0, InstructionStream());
       if (dummy_rinfo.IsInConstantPool()) {
         Address constant_pool_entry_address =
             dummy_rinfo.constant_pool_entry_address();
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index cdce7778c44..ce9a3fb1172 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -1102,15 +1102,9 @@ void Code::CodeVerify(Isolate* isolate) {
   CHECK(IsCode());
   if (raw_instruction_stream() != Smi::zero()) {
     InstructionStream istream = instruction_stream();
+    CHECK_EQ(istream.kind(), kind());
+    CHECK_EQ(istream.builtin_id(), builtin_id());
     CHECK_EQ(istream.code(kAcquireLoad), *this);
-    CHECK_EQ(safepoint_table_offset(), 0);
-    CHECK_LE(safepoint_table_offset(), handler_table_offset());
-    CHECK_LE(handler_table_offset(), constant_pool_offset());
-    CHECK_LE(constant_pool_offset(), code_comments_offset());
-    CHECK_LE(code_comments_offset(), unwinding_info_offset());
-    CHECK_LE(unwinding_info_offset(), metadata_size());
-
-    relocation_info().ObjectVerify(isolate);
 
     // Ensure the cached code entry point corresponds to the InstructionStream
     // object associated with this Code.
@@ -1138,8 +1132,14 @@ void Code::CodeVerify(Isolate* isolate) {
 
 void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
   CHECK(
-      IsAligned(code(kAcquireLoad).instruction_size(),
+      IsAligned(instruction_size(),
                 static_cast<unsigned>(InstructionStream::kMetadataAlignment)));
+  CHECK_EQ(safepoint_table_offset(), 0);
+  CHECK_LE(safepoint_table_offset(), handler_table_offset());
+  CHECK_LE(handler_table_offset(), constant_pool_offset());
+  CHECK_LE(constant_pool_offset(), code_comments_offset());
+  CHECK_LE(code_comments_offset(), unwinding_info_offset());
+  CHECK_LE(unwinding_info_offset(), metadata_size());
 #if !defined(_MSC_VER) || defined(__clang__)
   // See also: PlatformEmbeddedFileWriterWin::AlignToCodeAlignment.
   CHECK_IMPLIES(!ReadOnlyHeap::Contains(*this),
@@ -1148,12 +1148,13 @@ void InstructionStream::InstructionStreamVerify(Isolate* isolate) {
   CHECK_IMPLIES(!ReadOnlyHeap::Contains(*this),
                 IsAligned(instruction_start(), kCodeAlignment));
   CHECK_EQ(*this, code(kAcquireLoad).instruction_stream());
+  relocation_info().ObjectVerify(isolate);
   CHECK(V8_ENABLE_THIRD_PARTY_HEAP_BOOL ||
         CodeSize() <= MemoryChunkLayout::MaxRegularCodeObjectSize() ||
         isolate->heap()->InSpace(*this, CODE_LO_SPACE));
   Address last_gc_pc = kNullAddress;
 
-  for (RelocIterator it(code(kAcquireLoad)); !it.done(); it.next()) {
+  for (RelocIterator it(*this); !it.done(); it.next()) {
     it.rinfo()->Verify(isolate);
     // Ensure that GC will not iterate twice over the same pointer.
     if (RelocInfo::IsGCRelocMode(it.rinfo()->rmode())) {
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index d652609a7c1..d4b3d731dfc 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -1879,6 +1879,10 @@ void InstructionStream::InstructionStreamPrint(std::ostream& os) {
   PrintHeader(os, "InstructionStream");
   Code the_code = code(kAcquireLoad);
   os << "\n - code: " << Brief(the_code);
+  if (is_builtin()) {
+    os << "\n - builtin_id: " << Builtins::name(builtin_id());
+  }
+  os << "\n";
 #ifdef ENABLE_DISASSEMBLER
   the_code.Disassemble(nullptr, os, GetIsolate());
 #endif
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index e78b7d3954a..b54322f8a4a 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -610,7 +610,7 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
   // TODO(v8:10026): avoid replacing a signed pointer.
   PointerAuthentication::ReplacePC(pc_address, new_pc, kSystemPointerSize);
   if (V8_EMBEDDED_CONSTANT_POOL_BOOL && constant_pool_address != nullptr) {
-    *constant_pool_address = visited_holder.constant_pool();
+    *constant_pool_address = istream.constant_pool();
   }
 }
 
@@ -2448,12 +2448,12 @@ void InterpretedFrame::PatchBytecodeArray(BytecodeArray bytecode_array) {
 }
 
 int BaselineFrame::GetBytecodeOffset() const {
-  Code code = LookupCode();
+  InstructionStream code = LookupCode().instruction_stream();
   return code.GetBytecodeOffsetForBaselinePC(this->pc(), GetBytecodeArray());
 }
 
 intptr_t BaselineFrame::GetPCForBytecodeOffset(int bytecode_offset) const {
-  Code code = LookupCode();
+  InstructionStream code = LookupCode().instruction_stream();
   return code.GetBaselineStartPCForBytecodeOffset(bytecode_offset,
                                                   GetBytecodeArray());
 }
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index cec3c3d931a..de5e74240a6 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -445,9 +445,9 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     DCHECK(Internals::HasHeapObjectTag(code.ptr()));
     uint8_t* const code_ptr = reinterpret_cast<uint8_t*>(code.address());
 
-    // These static asserts ensure we don't miss relevant fields. We don't hash
-    // code cage base and code entry point. Other data fields must remain the
-    // same.
+    // These static asserts ensure we don't miss relevant fields. We don't
+    // hash code cage base and code entry point. Other data fields must
+    // remain the same.
     static_assert(Code::kCodePointerFieldsStrongEndOffset ==
                   Code::kCodeEntryPointOffset);
 
@@ -456,22 +456,6 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     static_assert(Code::kBuiltinIdOffsetEnd + 1 ==
                   Code::kKindSpecificFlagsOffset);
     static_assert(Code::kKindSpecificFlagsOffsetEnd + 1 ==
-                  Code::kInstructionSizeOffset);
-    static_assert(Code::kInstructionSizeOffsetEnd + 1 ==
-                  Code::kMetadataSizeOffset);
-    static_assert(Code::kMetadataSizeOffsetEnd + 1 ==
-                  Code::kInlinedBytecodeSizeOffset);
-    static_assert(Code::kInlinedBytecodeSizeOffsetEnd + 1 ==
-                  Code::kOsrOffsetOffset);
-    static_assert(Code::kOsrOffsetOffsetEnd + 1 ==
-                  Code::kHandlerTableOffsetOffset);
-    static_assert(Code::kHandlerTableOffsetOffsetEnd + 1 ==
-                  Code::kUnwindingInfoOffsetOffset);
-    static_assert(Code::kUnwindingInfoOffsetOffsetEnd + 1 ==
-                  Code::kConstantPoolOffsetOffset);
-    static_assert(Code::kConstantPoolOffsetOffsetEnd + 1 ==
-                  Code::kCodeCommentsOffsetOffset);
-    static_assert(Code::kCodeCommentsOffsetOffsetEnd + 1 ==
                   Code::kUnalignedSize);
     constexpr int kStartOffset = Code::kFlagsOffset;
 
@@ -2002,7 +1986,7 @@ Object Isolate::UnwindAndFindHandler() {
       CHECK(frame->is_java_script());
 
       if (frame->is_turbofan()) {
-        Code code = frame->LookupCode();
+        InstructionStream code = frame->LookupCode().instruction_stream();
         // The debugger triggers lazy deopt for the "to-be-restarted" frame
         // immediately when the CDP event arrives while paused.
         CHECK(code.marked_for_deoptimization());
@@ -2011,7 +1995,7 @@ Object Isolate::UnwindAndFindHandler() {
         // Jump directly to the optimized frames return, to immediately fall
         // into the deoptimizer.
         const int offset =
-            static_cast<int>(frame->pc() - code.InstructionStart());
+            static_cast<int>(frame->pc() - code.instruction_start());
 
         // Compute the stack pointer from the frame pointer. This ensures that
         // argument slots on the stack are dropped as returning would.
@@ -2019,7 +2003,7 @@ Object Isolate::UnwindAndFindHandler() {
         Address return_sp = frame->fp() +
                             StandardFrameConstants::kFixedFrameSizeAboveFp -
                             code.stack_slots() * kSystemPointerSize;
-        return FoundHandler(Context(), code.InstructionStart(), offset,
+        return FoundHandler(Context(), code.instruction_start(), offset,
                             code.constant_pool(), return_sp, frame->fp(),
                             visited_frames);
       }
@@ -2054,9 +2038,9 @@ Object Isolate::UnwindAndFindHandler() {
       case StackFrame::C_WASM_ENTRY: {
         StackHandler* handler = frame->top_handler();
         thread_local_top()->handler_ = handler->next_address();
-        Code code = frame->LookupCode();
+        InstructionStream code = frame->LookupCode().instruction_stream();
         HandlerTable table(code);
-        Address instruction_start = code.InstructionStart();
+        Address instruction_start = code.instruction_start();
         int return_offset = static_cast<int>(frame->pc() - instruction_start);
         int handler_offset = table.LookupReturn(return_offset);
         DCHECK_NE(-1, handler_offset);
@@ -2198,12 +2182,12 @@ Object Isolate::UnwindAndFindHandler() {
 
         if (frame->is_baseline()) {
           BaselineFrame* sp_frame = BaselineFrame::cast(js_frame);
-          Code code = sp_frame->LookupCode();
+          InstructionStream code = sp_frame->LookupCode().instruction_stream();
           intptr_t pc_offset = sp_frame->GetPCForBytecodeOffset(offset);
           // Patch the context register directly on the frame, so that we don't
           // need to have a context read + write in the baseline code.
           sp_frame->PatchContext(context);
-          return FoundHandler(Context(), code.InstructionStart(), pc_offset,
+          return FoundHandler(Context(), code.instruction_start(), pc_offset,
                               code.constant_pool(), return_sp, sp_frame->fp(),
                               visited_frames);
         } else {
@@ -4750,7 +4734,7 @@ bool Isolate::use_optimizer() {
 
 void Isolate::IncreaseTotalRegexpCodeGenerated(Handle<HeapObject> code) {
   PtrComprCageBase cage_base(this);
-  DCHECK(code->IsCode(cage_base) || code->IsByteArray(cage_base));
+  DCHECK(code->IsInstructionStream(cage_base) || code->IsByteArray(cage_base));
   total_regexp_code_generated_ += code->Size(cage_base);
 }
 
diff --git a/src/extensions/statistics-extension.cc b/src/extensions/statistics-extension.cc
index ea333762c9a..b1299da7da7 100644
--- a/src/extensions/statistics-extension.cc
+++ b/src/extensions/statistics-extension.cc
@@ -144,8 +144,8 @@ void StatisticsExtension::GetCounters(
     for (HeapObject obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       Object maybe_source_positions;
-      if (obj.IsCode()) {
-        Code code = Code::cast(obj);
+      if (obj.IsInstructionStream()) {
+        InstructionStream code = InstructionStream::cast(obj);
         reloc_info_total += code.relocation_info().Size();
         // Baseline code doesn't have source positions since it uses
         // interpreter code positions.
diff --git a/src/heap/concurrent-marking.cc b/src/heap/concurrent-marking.cc
index 73a43c249c0..54a971a70fc 100644
--- a/src/heap/concurrent-marking.cc
+++ b/src/heap/concurrent-marking.cc
@@ -235,11 +235,13 @@ class ConcurrentMarkingVisitor final
   ConcurrentMarkingState* marking_state() { return &marking_state_; }
 
  private:
-  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target) {
-    if (!MarkCompactCollector::ShouldRecordRelocSlot(rinfo, target)) return;
+  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                       HeapObject target) {
+    if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target))
+      return;
 
     MarkCompactCollector::RecordRelocSlotInfo info =
-        MarkCompactCollector::ProcessRelocInfo(rinfo, target);
+        MarkCompactCollector::ProcessRelocInfo(host, rinfo, target);
 
     MemoryChunkData& data = (*memory_chunk_data_)[info.memory_chunk];
     if (!data.typed_slots) {
diff --git a/src/heap/evacuation-verifier.cc b/src/heap/evacuation-verifier.cc
index 5134b7f02f8..973eabc6d0d 100644
--- a/src/heap/evacuation-verifier.cc
+++ b/src/heap/evacuation-verifier.cc
@@ -25,7 +25,8 @@ void EvacuationVerifier::VisitPointers(HeapObject host, MaybeObjectSlot start,
   VerifyPointers(start, end);
 }
 
-void EvacuationVerifier::VisitCodePointer(Code host, CodeObjectSlot slot) {
+void EvacuationVerifier::VisitCodePointer(HeapObject host,
+                                          CodeObjectSlot slot) {
   VerifyCodePointer(slot);
 }
 
@@ -111,12 +112,14 @@ void FullEvacuationVerifier::VerifyCodePointer(CodeObjectSlot slot) {
     VerifyHeapObjectImpl(code);
   }
 }
-void FullEvacuationVerifier::VisitCodeTarget(RelocInfo* rinfo) {
+void FullEvacuationVerifier::VisitCodeTarget(InstructionStream host,
+                                             RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
-void FullEvacuationVerifier::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void FullEvacuationVerifier::VisitEmbeddedPointer(InstructionStream host,
+                                                  RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 void FullEvacuationVerifier::VerifyRootPointers(FullObjectSlot start,
@@ -155,12 +158,14 @@ void YoungGenerationEvacuationVerifier::VerifyCodePointer(CodeObjectSlot slot) {
     VerifyHeapObjectImpl(code);
   }
 }
-void YoungGenerationEvacuationVerifier::VisitCodeTarget(RelocInfo* rinfo) {
+void YoungGenerationEvacuationVerifier::VisitCodeTarget(InstructionStream host,
+                                                        RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
-void YoungGenerationEvacuationVerifier::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void YoungGenerationEvacuationVerifier::VisitEmbeddedPointer(
+    InstructionStream host, RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 void YoungGenerationEvacuationVerifier::VerifyRootPointers(FullObjectSlot start,
diff --git a/src/heap/evacuation-verifier.h b/src/heap/evacuation-verifier.h
index 039d793f33d..764b9397667 100644
--- a/src/heap/evacuation-verifier.h
+++ b/src/heap/evacuation-verifier.h
@@ -26,7 +26,7 @@ class EvacuationVerifier : public ObjectVisitorWithCageBases,
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
@@ -70,8 +70,8 @@ class FullEvacuationVerifier : public EvacuationVerifier {
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override;
   void VerifyPointers(MaybeObjectSlot start, MaybeObjectSlot end) override;
   void VerifyCodePointer(CodeObjectSlot slot) override;
-  void VisitCodeTarget(RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override;
 };
 
@@ -91,8 +91,8 @@ class YoungGenerationEvacuationVerifier : public EvacuationVerifier {
   void VerifyPointers(ObjectSlot start, ObjectSlot end) override;
   void VerifyPointers(MaybeObjectSlot start, MaybeObjectSlot end) override;
   void VerifyCodePointer(CodeObjectSlot slot) override;
-  void VisitCodeTarget(RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override;
 };
 
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index 3242976926a..c903c328b22 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -74,43 +74,19 @@ Handle<AccessorPair> FactoryBase<Impl>::NewAccessorPair() {
 }
 
 template <typename Impl>
-Handle<Code> FactoryBase<Impl>::NewCode(const NewCodeOptions& options) {
+Handle<Code> FactoryBase<Impl>::NewCode(int flags, AllocationType allocation) {
   Map map = read_only_roots().code_map();
   int size = map.instance_size();
-  DCHECK_NE(options.allocation, AllocationType::kYoung);
-  Code code =
-      Code::cast(AllocateRawWithImmortalMap(size, options.allocation, map));
+  DCHECK_NE(allocation, AllocationType::kYoung);
+  Code data_container =
+      Code::cast(AllocateRawWithImmortalMap(size, allocation, map));
   DisallowGarbageCollection no_gc;
-  code.initialize_flags(options.kind, options.builtin, options.is_turbofanned,
-                        options.stack_slots);
-  code.set_kind_specific_flags(options.kind_specific_flags, kRelaxedStore);
+  data_container.set_kind_specific_flags(flags, kRelaxedStore);
   Isolate* isolate_for_sandbox = impl()->isolate_for_sandbox();
-  code.set_raw_instruction_stream(Smi::zero(), SKIP_WRITE_BARRIER);
-  code.init_code_entry_point(isolate_for_sandbox, kNullAddress);
-  code.set_instruction_size(options.instruction_size);
-  code.set_metadata_size(options.metadata_size);
-  code.set_relocation_info(*options.reloc_info);
-  code.set_inlined_bytecode_size(options.inlined_bytecode_size);
-  code.set_osr_offset(options.osr_offset);
-  code.set_handler_table_offset(options.handler_table_offset);
-  code.set_constant_pool_offset(options.constant_pool_offset);
-  code.set_code_comments_offset(options.code_comments_offset);
-  code.set_unwinding_info_offset(options.unwinding_info_offset);
-
-  if (options.kind == CodeKind::BASELINE) {
-    code.set_bytecode_or_interpreter_data(
-        *options.bytecode_or_deoptimization_data);
-    code.set_bytecode_offset_table(
-        *options.bytecode_offsets_or_source_position_table);
-  } else {
-    code.set_deoptimization_data(
-        FixedArray::cast(*options.bytecode_or_deoptimization_data));
-    code.set_source_position_table(
-        *options.bytecode_offsets_or_source_position_table);
-  }
-
-  code.clear_padding();
-  return handle(code, isolate());
+  data_container.set_raw_instruction_stream(Smi::zero(), SKIP_WRITE_BARRIER);
+  data_container.init_code_entry_point(isolate_for_sandbox, kNullAddress);
+  data_container.clear_padding();
+  return handle(data_container, isolate());
 }
 
 template <typename Impl>
@@ -349,7 +325,7 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfoForLiteral(
     FunctionLiteral* literal, Handle<Script> script, bool is_toplevel) {
   FunctionKind kind = literal->kind();
   Handle<SharedFunctionInfo> shared = NewSharedFunctionInfo(
-      literal->GetName(isolate()), MaybeHandle<HeapObject>(),
+      literal->GetName(isolate()), MaybeHandle<InstructionStream>(),
       Builtin::kCompileLazy, kind);
   SharedFunctionInfo::InitFromFunctionLiteral(isolate(), shared, literal,
                                               is_toplevel);
@@ -453,7 +429,8 @@ Handle<SharedFunctionInfo> FactoryBase<Impl>::NewSharedFunctionInfo(
     // If we pass function_data then we shouldn't pass a builtin index, and
     // the function_data should not be code with a builtin.
     DCHECK(!Builtins::IsBuiltinId(builtin));
-    DCHECK(!function_data->IsInstructionStream());
+    DCHECK_IMPLIES(function_data->IsInstructionStream(),
+                   !InstructionStream::cast(*function_data).is_builtin());
     raw.set_function_data(*function_data, kReleaseStore);
   } else if (Builtins::IsBuiltinId(builtin)) {
     raw.set_builtin_id(builtin);
diff --git a/src/heap/factory-base.h b/src/heap/factory-base.h
index 9739d52e408..d4b66b07b96 100644
--- a/src/heap/factory-base.h
+++ b/src/heap/factory-base.h
@@ -8,7 +8,6 @@
 #include "src/base/export-template.h"
 #include "src/base/strings.h"
 #include "src/common/globals.h"
-#include "src/objects/code-kind.h"
 #include "src/objects/fixed-array.h"
 #include "src/objects/function-kind.h"
 #include "src/objects/instance-type.h"
@@ -61,26 +60,6 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) TorqueGeneratedFactory {
 #include "torque-generated/factory.inc"
 };
 
-struct NewCodeOptions {
-  CodeKind kind;
-  Builtin builtin;
-  bool is_turbofanned;
-  int stack_slots;
-  int kind_specific_flags;
-  AllocationType allocation;
-  int instruction_size;
-  int metadata_size;
-  unsigned int inlined_bytecode_size;
-  BytecodeOffset osr_offset;
-  int handler_table_offset;
-  int constant_pool_offset;
-  int code_comments_offset;
-  int32_t unwinding_info_offset;
-  Handle<ByteArray> reloc_info;
-  Handle<HeapObject> bytecode_or_deoptimization_data;
-  Handle<ByteArray> bytecode_offsets_or_source_position_table;
-};
-
 template <typename Impl>
 class FactoryBase : public TorqueGeneratedFactory<Impl> {
  public:
@@ -120,7 +99,7 @@ class FactoryBase : public TorqueGeneratedFactory<Impl> {
   Handle<AccessorPair> NewAccessorPair();
 
   // Creates a new Code for a InstructionStream object.
-  Handle<Code> NewCode(const NewCodeOptions& options);
+  Handle<Code> NewCode(int flags, AllocationType allocation);
 
   // Allocates a fixed array initialized with undefined values.
   Handle<FixedArray> NewFixedArray(
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 4c17596e7dc..1f88a8e0657 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -115,34 +115,15 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
           : factory->NewByteArray(code_desc_.reloc_size, AllocationType::kOld);
 
   Handle<Code> code;
-
-  NewCodeOptions new_code_options = {
-      /*kind=*/kind_,
-      /*builtin=*/builtin_,
-      /*is_turbofanned=*/is_turbofanned_,
-      /*stack_slots=*/stack_slots_,
-      /*kind_specific_flags=*/kind_specific_flags_,
-      /*allocation=*/AllocationType::kOld,
-      /*instruction_size=*/code_desc_.instruction_size(),
-      /*metadata_size=*/code_desc_.metadata_size(),
-      /*inlined_bytecode_size=*/inlined_bytecode_size_,
-      /*osr_offset=*/osr_offset_,
-      /*handler_table_offset=*/code_desc_.handler_table_offset_relative(),
-      /*constant_pool_offset=*/code_desc_.constant_pool_offset_relative(),
-      /*code_comments_offset=*/code_desc_.code_comments_offset_relative(),
-      /*unwinding_info_offset=*/code_desc_.unwinding_info_offset_relative(),
-      /*reloc_info=*/reloc_info,
-      /*bytecode_or_deoptimization_data=*/kind_ == CodeKind::BASELINE
-          ? interpreter_data_
-          : deoptimization_data_,
-      /*bytecode_offsets_or_source_position_table=*/position_table_};
-
   if (CompiledWithConcurrentBaseline()) {
-    code = local_isolate_->factory()->NewCode(new_code_options);
+    code = local_isolate_->factory()->NewCode(0, AllocationType::kOld);
   } else {
-    code = factory->NewCode(new_code_options);
+    code = factory->NewCode(0, AllocationType::kOld);
   }
 
+  code->initialize_flags(kind_, builtin_, is_turbofanned_);
+  code->set_kind_specific_flags(kind_specific_flags_, kRelaxedStore);
+
   // Basic block profiling data for builtins is stored in the JS heap rather
   // than in separately-allocated C++ objects. Allocate that data now if
   // appropriate.
@@ -178,10 +159,32 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     InstructionStream raw_istream = *instruction_stream;
     DisallowGarbageCollection no_gc;
 
+    raw_istream.set_instruction_size(code_desc_.instruction_size());
+    raw_istream.set_metadata_size(code_desc_.metadata_size());
+    raw_istream.set_relocation_info(*reloc_info);
+    raw_istream.initialize_flags(kind_, is_turbofanned_, stack_slots_);
+    raw_istream.set_builtin_id(builtin_);
     // This might impact direct concurrent reads from TF if we are resetting
     // this field. We currently assume it's immutable thus a relaxed read (after
     // passing IsPendingAllocation).
+    raw_istream.set_inlined_bytecode_size(inlined_bytecode_size_);
+    raw_istream.set_osr_offset(osr_offset_);
     raw_istream.set_code(*code, kReleaseStore);
+    if (kind_ == CodeKind::BASELINE) {
+      raw_istream.set_bytecode_or_interpreter_data(*interpreter_data_);
+      raw_istream.set_bytecode_offset_table(*position_table_);
+    } else {
+      raw_istream.set_deoptimization_data(*deoptimization_data_);
+      raw_istream.set_source_position_table(*position_table_);
+    }
+    raw_istream.set_handler_table_offset(
+        code_desc_.handler_table_offset_relative());
+    raw_istream.set_constant_pool_offset(
+        code_desc_.constant_pool_offset_relative());
+    raw_istream.set_code_comments_offset(
+        code_desc_.code_comments_offset_relative());
+    raw_istream.set_unwinding_info_offset(
+        code_desc_.unwinding_info_offset_relative());
 
     // Allow self references to created code object by patching the handle to
     // point to the newly allocated InstructionStream object.
@@ -206,21 +209,20 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
               handle(on_heap_profiler_data->counts(), isolate_));
     }
 
-    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
-    }
-    code->SetInstructionStreamAndEntryPoint(isolate_, raw_istream);
-
     // Migrate generated code.
     // The generated code can contain embedded objects (typically from
     // handles) in a pointer-to-tagged-value format (i.e. with indirection
     // like a handle) that are dereferenced during the copy to point directly
     // to the actual heap objects. These pointers can include references to
     // the code object itself, through the self_reference parameter.
-    code->CopyFromNoFlush(*reloc_info, heap, code_desc_);
+    raw_istream.CopyFromNoFlush(*reloc_info, heap, code_desc_);
 
-    code->ClearInstructionStreamPadding();
+    raw_istream.clear_padding();
 
+    if (V8_EXTERNAL_CODE_SPACE_BOOL) {
+      raw_istream.set_main_cage_base(isolate_->cage_base(), kRelaxedStore);
+    }
+    code->SetInstructionStreamAndEntryPoint(isolate_, raw_istream);
 #ifdef VERIFY_HEAP
     if (v8_flags.verify_heap) {
       HeapObject::VerifyCodePointer(isolate_, raw_istream);
@@ -232,7 +234,7 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     // some older ARM kernels there is a bug which causes an access error on
     // cache flush instructions to trigger access error on non-writable memory.
     // See https://bugs.chromium.org/p/v8/issues/detail?id=8157
-    code->FlushICache();
+    raw_istream.FlushICache();
   }
 
   if (V8_UNLIKELY(profiler_data_ && v8_flags.turbo_profiling_verbose)) {
@@ -276,13 +278,13 @@ MaybeHandle<InstructionStream> Factory::CodeBuilder::AllocateInstructionStream(
   DisallowGarbageCollection no_gc;
   result.set_map_after_allocation(
       *isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
-  Handle<InstructionStream> istream =
+  Handle<InstructionStream> code =
       handle(InstructionStream::cast(result), isolate_);
-  DCHECK(IsAligned(istream->address(), kCodeAlignment));
+  DCHECK(IsAligned(code->address(), kCodeAlignment));
   DCHECK_IMPLIES(
       !V8_ENABLE_THIRD_PARTY_HEAP_BOOL && !heap->code_region().is_empty(),
-      heap->code_region().contains(istream->address()));
-  return istream;
+      heap->code_region().contains(code->address()));
+  return code;
 }
 
 MaybeHandle<InstructionStream>
@@ -301,10 +303,10 @@ Factory::CodeBuilder::AllocateConcurrentSparkplugInstructionStream(
   DisallowGarbageCollection no_gc;
   result.set_map_after_allocation(
       *local_isolate_->factory()->instruction_stream_map(), SKIP_WRITE_BARRIER);
-  Handle<InstructionStream> istream =
+  Handle<InstructionStream> code =
       handle(InstructionStream::cast(result), local_isolate_);
-  DCHECK(IsAligned(istream->address(), kCodeAlignment));
-  return istream;
+  DCHECK(IsAligned(code->address(), kCodeAlignment));
+  return code;
 }
 
 MaybeHandle<Code> Factory::CodeBuilder::TryBuild() {
@@ -2500,38 +2502,16 @@ Handle<Code> Factory::NewOffHeapTrampolineFor(Handle<Code> code,
   CHECK_NE(0, isolate()->embedded_blob_code_size());
   CHECK(Builtins::IsIsolateIndependentBuiltin(*code));
 
-  NewCodeOptions new_code_options = {
-      /*kind=*/code->kind(),
-      /*builtin=*/code->builtin_id(),
-      /*is_turbofanned=*/code->is_turbofanned(),
-      /*stack_slots=*/code->stack_slots(),
-      /*kind_specific_flags=*/code->kind_specific_flags(kRelaxedLoad),
-      /*allocation=*/AllocationType::kOld,
-      /*instruction_size=*/code->instruction_size(),
-      /*metadata_size=*/code->metadata_size(),
-      /*inlined_bytecode_size=*/code->inlined_bytecode_size(),
-      /*osr_offset=*/code->osr_offset(),
-      /*handler_table_offset=*/code->handler_table_offset(),
-      /*constant_pool_offset=*/code->constant_pool_offset(),
-      /*code_comments_offset=*/code->code_comments_offset(),
-      /*unwinding_info_offset=*/code->unwinding_info_offset(),
-      /*reloc_info=*/
-      Handle<ByteArray>(read_only_roots().empty_byte_array(), isolate()),
-      /*bytecode_or_deoptimization_data=*/
-      Handle<FixedArray>(read_only_roots().empty_fixed_array(), isolate()),
-      /*bytecode_offsets_or_source_position_table=*/
-      Handle<ByteArray>(read_only_roots().empty_byte_array(), isolate())};
-
-  Handle<Code> off_heap_trampoline = NewCode(new_code_options);
+  const int no_flags = 0;
+  Handle<Code> off_heap_trampoline = NewCode(no_flags, AllocationType::kOld);
+
+  off_heap_trampoline->initialize_flags(code->kind(), code->builtin_id(),
+                                        code->is_turbofanned());
+  off_heap_trampoline->set_kind_specific_flags(
+      code->kind_specific_flags(kRelaxedLoad), kRelaxedStore);
   off_heap_trampoline->set_code_entry_point(isolate(),
                                             code->code_entry_point());
-
-  DCHECK_EQ(code->instruction_size(), code->OffHeapInstructionSize());
-  DCHECK_EQ(code->metadata_size(), code->OffHeapMetadataSize());
-  DCHECK_EQ(code->inlined_bytecode_size(), 0);
-  DCHECK_EQ(code->osr_offset(), BytecodeOffset::None());
-
-  return off_heap_trampoline;
+  return Handle<Code>::cast(off_heap_trampoline);
 }
 
 Handle<BytecodeArray> Factory::CopyBytecodeArray(Handle<BytecodeArray> source) {
@@ -3400,8 +3380,8 @@ Handle<SharedFunctionInfo> Factory::NewSharedFunctionInfoForApiFunction(
 
 Handle<SharedFunctionInfo> Factory::NewSharedFunctionInfoForBuiltin(
     MaybeHandle<String> maybe_name, Builtin builtin, FunctionKind kind) {
-  return NewSharedFunctionInfo(maybe_name, MaybeHandle<HeapObject>(), builtin,
-                               kind);
+  return NewSharedFunctionInfo(maybe_name, MaybeHandle<InstructionStream>(),
+                               builtin, kind);
 }
 
 int Factory::NumberToStringCacheHash(Smi number) {
diff --git a/src/heap/heap-verifier.cc b/src/heap/heap-verifier.cc
index b550d621e77..f52f6203f00 100644
--- a/src/heap/heap-verifier.cc
+++ b/src/heap/heap-verifier.cc
@@ -62,9 +62,9 @@ class VerifyPointersVisitor : public ObjectVisitorWithCageBases,
                      ObjectSlot end) override;
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
-  void VisitCodeTarget(RelocInfo* rinfo) override;
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override;
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override;
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override;
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
@@ -97,7 +97,8 @@ void VerifyPointersVisitor::VisitPointers(HeapObject host,
   VerifyPointers(host, start, end);
 }
 
-void VerifyPointersVisitor::VisitCodePointer(Code host, CodeObjectSlot slot) {
+void VerifyPointersVisitor::VisitCodePointer(HeapObject host,
+                                             CodeObjectSlot slot) {
   Object maybe_code = slot.load(code_cage_base());
   HeapObject code;
   // The slot might contain smi during Code creation.
@@ -163,13 +164,15 @@ void VerifyPointersVisitor::VerifyPointers(HeapObject host,
   VerifyPointersImpl(start, end);
 }
 
-void VerifyPointersVisitor::VisitCodeTarget(RelocInfo* rinfo) {
+void VerifyPointersVisitor::VisitCodeTarget(InstructionStream host,
+                                            RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
   VerifyHeapObjectImpl(target);
 }
 
-void VerifyPointersVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void VerifyPointersVisitor::VisitEmbeddedPointer(InstructionStream host,
+                                                 RelocInfo* rinfo) {
   VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
 }
 
@@ -474,18 +477,17 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     if (ShouldHaveBeenRecorded(
             host, MaybeObject::FromObject(slot.load(code_cage_base())))) {
       CHECK_GT(untyped_->count(slot.address()), 0);
     }
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     Object target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    if (ShouldHaveBeenRecorded(rinfo->instruction_stream(),
-                               MaybeObject::FromObject(target))) {
+    if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kCodeEntry, rinfo->pc()) ||
             (rinfo->IsInConstantPool() &&
              InTypedSet(SlotType::kConstPoolCodeEntry,
@@ -493,10 +495,9 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     Object target = rinfo->target_object(cage_base());
-    if (ShouldHaveBeenRecorded(rinfo->instruction_stream(),
-                               MaybeObject::FromObject(target))) {
+    if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kEmbeddedObjectFull, rinfo->pc()) ||
             InTypedSet(SlotType::kEmbeddedObjectCompressed, rinfo->pc()) ||
             (rinfo->IsInConstantPool() &&
@@ -606,16 +607,20 @@ class SlotCollectingVisitor final : public ObjectVisitor {
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
 #ifdef V8_EXTERNAL_CODE_SPACE
     code_slots_.push_back(slot);
 #endif
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) final { UNREACHABLE(); }
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
+    UNREACHABLE();
+  }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+    UNREACHABLE();
+  }
 
   void VisitMapPointer(HeapObject object) override {}  // do nothing by default
 
diff --git a/src/heap/heap-write-barrier-inl.h b/src/heap/heap-write-barrier-inl.h
index dd850bbf013..6e60ad03edc 100644
--- a/src/heap/heap-write-barrier-inl.h
+++ b/src/heap/heap-write-barrier-inl.h
@@ -30,8 +30,8 @@ V8_EXPORT_PRIVATE void Heap_CombinedGenerationalAndSharedBarrierSlow(
 V8_EXPORT_PRIVATE void Heap_CombinedGenerationalAndSharedEphemeronBarrierSlow(
     EphemeronHashTable table, Address slot, HeapObject value);
 
-V8_EXPORT_PRIVATE void Heap_GenerationalBarrierForCodeSlow(RelocInfo* rinfo,
-                                                           HeapObject object);
+V8_EXPORT_PRIVATE void Heap_GenerationalBarrierForCodeSlow(
+    InstructionStream host, RelocInfo* rinfo, HeapObject object);
 
 V8_EXPORT_PRIVATE void Heap_GenerationalEphemeronKeyBarrierSlow(
     Heap* heap, HeapObject table, Address slot);
@@ -146,7 +146,7 @@ inline void WriteBarrierForCode(InstructionStream host, RelocInfo* rinfo,
   }
 
   DCHECK_EQ(mode, UPDATE_WRITE_BARRIER);
-  GenerationalBarrierForCode(rinfo, value);
+  GenerationalBarrierForCode(host, rinfo, value);
   WriteBarrier::Shared(host, rinfo, value);
   WriteBarrier::Marking(host, rinfo, value);
 }
@@ -214,12 +214,13 @@ inline void CombinedEphemeronWriteBarrier(EphemeronHashTable host,
   }
 }
 
-inline void GenerationalBarrierForCode(RelocInfo* rinfo, HeapObject object) {
+inline void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
+                                       HeapObject object) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) return;
   heap_internals::MemoryChunk* object_chunk =
       heap_internals::MemoryChunk::FromHeapObject(object);
   if (!object_chunk->InYoungGeneration()) return;
-  Heap_GenerationalBarrierForCodeSlow(rinfo, object);
+  Heap_GenerationalBarrierForCodeSlow(host, rinfo, object);
 }
 
 inline WriteBarrierMode GetWriteBarrierModeForObject(
@@ -305,7 +306,7 @@ void WriteBarrier::Shared(InstructionStream host, RelocInfo* reloc_info,
       heap_internals::MemoryChunk::FromHeapObject(value);
   if (!value_chunk->InWritableSharedSpace()) return;
 
-  SharedSlow(reloc_info, value);
+  SharedSlow(host, reloc_info, value);
 }
 
 void WriteBarrier::Marking(JSArrayBuffer host,
diff --git a/src/heap/heap-write-barrier.cc b/src/heap/heap-write-barrier.cc
index b730ce261ed..25e533d9695 100644
--- a/src/heap/heap-write-barrier.cc
+++ b/src/heap/heap-write-barrier.cc
@@ -68,9 +68,10 @@ void WriteBarrier::MarkingSlow(InstructionStream host, RelocInfo* reloc_info,
   marking_barrier->Write(host, reloc_info, value);
 }
 
-void WriteBarrier::SharedSlow(RelocInfo* reloc_info, HeapObject value) {
+void WriteBarrier::SharedSlow(InstructionStream host, RelocInfo* reloc_info,
+                              HeapObject value) {
   MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(reloc_info, value);
+      MarkCompactCollector::ProcessRelocInfo(host, reloc_info, value);
 
   base::MutexGuard write_scope(info.memory_chunk->mutex());
   RememberedSet<OLD_TO_SHARED>::InsertTyped(info.memory_chunk, info.slot_type,
@@ -163,7 +164,17 @@ int WriteBarrier::SharedFromCode(Address raw_host, Address raw_slot) {
 bool WriteBarrier::IsImmortalImmovableHeapObject(HeapObject object) {
   BasicMemoryChunk* basic_chunk = BasicMemoryChunk::FromHeapObject(object);
   // All objects in readonly space are immortal and immovable.
-  return basic_chunk->InReadOnlySpace();
+  if (basic_chunk->InReadOnlySpace()) return true;
+  MemoryChunk* chunk = MemoryChunk::FromHeapObject(object);
+  // There are also objects in "regular" spaces which are immortal and
+  // immovable. Objects on a page that can get compacted are movable and can be
+  // filtered out.
+  if (!chunk->IsFlagSet(MemoryChunk::NEVER_EVACUATE)) return false;
+  // Builtins don't have InstructionStream objects (instead, they point
+  // directly into off-heap code streams).
+  DCHECK_IMPLIES(object.IsInstructionStream(),
+                 !InstructionStream::cast(object).is_builtin());
+  return false;
 }
 #endif
 
diff --git a/src/heap/heap-write-barrier.h b/src/heap/heap-write-barrier.h
index f44af9a9f17..0ea460ac2df 100644
--- a/src/heap/heap-write-barrier.h
+++ b/src/heap/heap-write-barrier.h
@@ -44,7 +44,8 @@ void CombinedEphemeronWriteBarrier(EphemeronHashTable object, ObjectSlot slot,
                                    Object value, WriteBarrierMode mode);
 
 // Generational write barrier.
-void GenerationalBarrierForCode(RelocInfo* rinfo, HeapObject object);
+void GenerationalBarrierForCode(InstructionStream host, RelocInfo* rinfo,
+                                HeapObject object);
 
 inline bool IsReadOnlyHeapObject(HeapObject object);
 
@@ -104,7 +105,7 @@ class V8_EXPORT_PRIVATE WriteBarrier {
                                                            size_t argc,
                                                            void** values);
 
-  static void SharedSlow(RelocInfo*, HeapObject value);
+  static void SharedSlow(InstructionStream host, RelocInfo*, HeapObject value);
 
   friend class Heap;
 };
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index 1b59a213fb3..2aef8cb7247 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -158,8 +158,9 @@ void Heap_CombinedGenerationalAndSharedEphemeronBarrierSlow(
   Heap::CombinedGenerationalAndSharedEphemeronBarrierSlow(table, slot, value);
 }
 
-void Heap_GenerationalBarrierForCodeSlow(RelocInfo* rinfo, HeapObject object) {
-  Heap::GenerationalBarrierForCodeSlow(rinfo, object);
+void Heap_GenerationalBarrierForCodeSlow(InstructionStream host,
+                                         RelocInfo* rinfo, HeapObject object) {
+  Heap::GenerationalBarrierForCodeSlow(host, rinfo, object);
 }
 
 void Heap::SetConstructStubCreateDeoptPCOffset(int pc_offset) {
@@ -1260,14 +1261,19 @@ void Heap::PublishPendingAllocations() {
   code_lo_space_->ResetPendingObject();
 }
 
+void Heap::InvalidateCodeDeoptimizationData(InstructionStream code) {
+  CodePageMemoryModificationScope modification_scope(code);
+  code.set_deoptimization_data(ReadOnlyRoots(this).empty_fixed_array());
+}
+
 void Heap::DeoptMarkedAllocationSites() {
   // TODO(hpayer): If iterating over the allocation sites list becomes a
   // performance issue, use a cache data structure in heap instead.
 
-  ForeachAllocationSite(allocation_sites_list(), [this](AllocationSite site) {
+  ForeachAllocationSite(allocation_sites_list(), [](AllocationSite site) {
     if (site.deopt_dependent_code()) {
       DependentCode::MarkCodeForDeoptimization(
-          isolate_, site, DependentCode::kAllocationSiteTenuringChangedGroup);
+          site, DependentCode::kAllocationSiteTenuringChangedGroup);
       site.set_deopt_dependent_code(false);
     }
   });
@@ -6290,7 +6296,7 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
       MarkPointers(start, end);
     }
 
-    void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+    void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
       Object maybe_code = slot.load(code_cage_base());
       HeapObject heap_object;
       if (maybe_code.GetHeapObject(&heap_object)) {
@@ -6298,12 +6304,12 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
       }
     }
 
-    void VisitCodeTarget(RelocInfo* rinfo) final {
+    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
       InstructionStream target =
           InstructionStream::FromTargetAddress(rinfo->target_address());
       MarkHeapObject(target);
     }
-    void VisitEmbeddedPointer(RelocInfo* rinfo) final {
+    void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final {
       MarkHeapObject(rinfo->target_object(cage_base()));
     }
 
@@ -6824,35 +6830,43 @@ GcSafeCode Heap::GcSafeGetCodeFromInstructionStream(
   return GcSafeCode::unchecked_cast(istream.raw_code(kAcquireLoad));
 }
 
-bool Heap::GcSafeInstructionStreamContains(InstructionStream istream,
+bool Heap::GcSafeInstructionStreamContains(InstructionStream instruction_stream,
                                            Address addr) {
-  Map map = GcSafeMapOfHeapObject(istream);
+  Map map = GcSafeMapOfHeapObject(instruction_stream);
   DCHECK_EQ(map, ReadOnlyRoots(this).instruction_stream_map());
 
   Builtin builtin_lookup_result =
       OffHeapInstructionStream::TryLookupCode(isolate(), addr);
   if (Builtins::IsBuiltinId(builtin_lookup_result)) {
     // Builtins don't have InstructionStream objects.
-    DCHECK(!Builtins::IsBuiltinId(istream.code(kAcquireLoad).builtin_id()));
+    DCHECK(!Builtins::IsBuiltinId(instruction_stream.builtin_id()));
     return false;
   }
 
-  Address start = istream.address();
-  Address end = start + istream.SizeFromMap(map);
+  Address start = instruction_stream.address();
+  Address end = start + instruction_stream.SizeFromMap(map);
   return start <= addr && addr < end;
 }
 
-base::Optional<InstructionStream>
-Heap::GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer) {
+base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
+    Address inner_pointer) {
+  Builtin maybe_builtin =
+      OffHeapInstructionStream::TryLookupCode(isolate(), inner_pointer);
+  if (Builtins::IsBuiltinId(maybe_builtin)) {
+    return GcSafeCode::cast(isolate()->builtins()->code(maybe_builtin));
+  }
+
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     Address start = tp_heap_->GetObjectFromInnerPointer(inner_pointer);
-    return InstructionStream::unchecked_cast(HeapObject::FromAddress(start));
+    return GcSafeGetCodeFromInstructionStream(HeapObject::FromAddress(start),
+                                              inner_pointer);
   }
 
   // Check if the inner pointer points into a large object chunk.
   LargePage* large_page = code_lo_space()->FindPage(inner_pointer);
   if (large_page != nullptr) {
-    return InstructionStream::unchecked_cast(large_page->GetObject());
+    return GcSafeGetCodeFromInstructionStream(large_page->GetObject(),
+                                              inner_pointer);
   }
 
   if (V8_LIKELY(code_space()->Contains(inner_pointer))) {
@@ -6863,27 +6877,13 @@ Heap::GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer) {
     Address start =
         page->GetCodeObjectRegistry()->GetCodeObjectStartFromInnerAddress(
             inner_pointer);
-    return InstructionStream::unchecked_cast(HeapObject::FromAddress(start));
+    return GcSafeGetCodeFromInstructionStream(HeapObject::FromAddress(start),
+                                              inner_pointer);
   }
 
   return {};
 }
 
-base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
-    Address inner_pointer) {
-  Builtin maybe_builtin =
-      OffHeapInstructionStream::TryLookupCode(isolate(), inner_pointer);
-  if (Builtins::IsBuiltinId(maybe_builtin)) {
-    return GcSafeCode::cast(isolate()->builtins()->code(maybe_builtin));
-  }
-
-  base::Optional<InstructionStream> maybe_istream =
-      GcSafeTryFindInstructionStreamForInnerPointer(inner_pointer);
-  if (!maybe_istream) return {};
-
-  return GcSafeGetCodeFromInstructionStream(*maybe_istream, inner_pointer);
-}
-
 Code Heap::FindCodeForInnerPointer(Address inner_pointer) {
   return GcSafeFindCodeForInnerPointer(inner_pointer).UnsafeCastToCode();
 }
@@ -7086,10 +7086,11 @@ void Heap::WriteBarrierForRange(HeapObject object, TSlot start_slot,
   }
 }
 
-void Heap::GenerationalBarrierForCodeSlow(RelocInfo* rinfo, HeapObject object) {
+void Heap::GenerationalBarrierForCodeSlow(InstructionStream host,
+                                          RelocInfo* rinfo, HeapObject object) {
   DCHECK(InYoungGeneration(object));
   const MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(rinfo, object);
+      MarkCompactCollector::ProcessRelocInfo(host, rinfo, object);
 
   RememberedSet<OLD_TO_NEW>::InsertTyped(info.memory_chunk, info.slot_type,
                                          info.offset);
diff --git a/src/heap/heap.h b/src/heap/heap.h
index d6b0a138e36..70f7c892a17 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -485,7 +485,7 @@ class Heap {
   V8_EXPORT_PRIVATE static void EphemeronKeyWriteBarrierFromCode(
       Address raw_object, Address address, Isolate* isolate);
   V8_EXPORT_PRIVATE static void GenerationalBarrierForCodeSlow(
-      RelocInfo* rinfo, HeapObject value);
+      InstructionStream host, RelocInfo* rinfo, HeapObject value);
   V8_EXPORT_PRIVATE static bool PageFlagsAreConsistent(HeapObject object);
 
   // Notifies the heap that is ok to start marking or other activities that
@@ -1168,6 +1168,10 @@ class Heap {
   void SetConstructStubInvokeDeoptPCOffset(int pc_offset);
   void SetInterpreterEntryReturnPCOffset(int pc_offset);
 
+  // Invalidates references in the given {code} object that are referenced
+  // transitively from the deoptimization data. Mutates write-protected code.
+  void InvalidateCodeDeoptimizationData(InstructionStream code);
+
   void DeoptMarkedAllocationSites();
 
   // ===========================================================================
@@ -1567,8 +1571,6 @@ class Heap {
   GcSafeCode GcSafeFindCodeForInnerPointer(Address inner_pointer);
   base::Optional<GcSafeCode> GcSafeTryFindCodeForInnerPointer(
       Address inner_pointer);
-  base::Optional<InstructionStream>
-  GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer);
   // Only intended for use from the `jco` gdb macro.
   base::Optional<Code> TryFindCodeForInnerPointerForPrinting(
       Address inner_pointer);
diff --git a/src/heap/mark-compact-inl.h b/src/heap/mark-compact-inl.h
index 1556fc0d641..29aa49a397d 100644
--- a/src/heap/mark-compact-inl.h
+++ b/src/heap/mark-compact-inl.h
@@ -100,9 +100,10 @@ void MainMarkingVisitor<MarkingState>::RecordSlot(HeapObject object, TSlot slot,
 }
 
 template <typename MarkingState>
-void MainMarkingVisitor<MarkingState>::RecordRelocSlot(RelocInfo* rinfo,
+void MainMarkingVisitor<MarkingState>::RecordRelocSlot(InstructionStream host,
+                                                       RelocInfo* rinfo,
                                                        HeapObject target) {
-  MarkCompactCollector::RecordRelocSlot(rinfo, target);
+  MarkCompactCollector::RecordRelocSlot(host, rinfo, target);
 }
 
 template <LiveObjectIterationMode mode>
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index a28b338f54d..cb2091c7ade 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -131,7 +131,7 @@ class MarkingVerifier : public ObjectVisitorWithCageBases, public RootVisitor {
     VerifyPointers(start, end);
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     VerifyCodePointer(slot);
   }
 
@@ -273,16 +273,16 @@ class FullMarkingVerifier : public MarkingVerifier {
     VerifyPointersImpl(start, end);
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
     HeapObject target_object = rinfo->target_object(cage_base());
-    if (!rinfo->code().IsWeakObject(target_object)) {
+    if (!host.IsWeakObject(target_object)) {
       VerifyHeapObjectImpl(target_object);
     }
   }
@@ -1046,14 +1046,14 @@ class MarkCompactCollector::RootMarkingVisitor final : public RootVisitor {
     Object istream_or_smi_zero = *istream_or_smi_zero_slot;
     DCHECK(istream_or_smi_zero == Smi::zero() ||
            istream_or_smi_zero.IsInstructionStream());
-    Code code = Code::cast(*code_slot);
-    DCHECK_EQ(code.raw_instruction_stream(), istream_or_smi_zero);
-
-    // We must not remove deoptimization literals which may be needed in
-    // order to successfully deoptimize.
-    code.IterateDeoptimizationLiterals(this);
+    DCHECK_EQ(Code::cast(*code_slot).raw_instruction_stream(),
+              istream_or_smi_zero);
 
     if (istream_or_smi_zero != Smi::zero()) {
+      InstructionStream istream = InstructionStream::cast(istream_or_smi_zero);
+      // We must not remove deoptimization literals which may be needed in
+      // order to successfully deoptimize.
+      istream.IterateDeoptimizationLiterals(this);
       VisitRootPointer(Root::kStackRoots, nullptr, istream_or_smi_zero_slot);
     }
 
@@ -1108,7 +1108,7 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     MarkObject(host, slot.load(code_cage_base()));
   }
 
@@ -1118,14 +1118,14 @@ class MarkCompactCollector::CustomRootBodyMarkingVisitor final
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    MarkObject(rinfo->instruction_stream(), target);
+    MarkObject(host, target);
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
-    MarkObject(rinfo->instruction_stream(), rinfo->target_object(cage_base()));
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+    MarkObject(host, rinfo->target_object(cage_base()));
   }
 
  private:
@@ -1163,7 +1163,7 @@ class MarkCompactCollector::ClientCustomRootBodyMarkingVisitor final
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     MarkObject(host, slot.load(code_cage_base()));
   }
 
@@ -1173,14 +1173,14 @@ class MarkCompactCollector::ClientCustomRootBodyMarkingVisitor final
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    MarkObject(rinfo->instruction_stream(), target);
+    MarkObject(host, target);
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
-    MarkObject(rinfo->instruction_stream(), rinfo->target_object(cage_base()));
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+    MarkObject(host, rinfo->target_object(cage_base()));
   }
 
  private:
@@ -1225,7 +1225,7 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     UNREACHABLE();
   }
 
@@ -1238,9 +1238,13 @@ class MarkCompactCollector::SharedHeapObjectVisitor final
     }
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+    UNREACHABLE();
+  }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
+    UNREACHABLE();
+  }
 
  private:
   V8_INLINE void CheckForSharedObject(HeapObject host, ObjectSlot slot,
@@ -1382,11 +1386,16 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
                        MaybeObjectSlot end) override {
       UNREACHABLE();
     }
-    void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+    void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
+      UNREACHABLE();
+    }
+    void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
+      UNREACHABLE();
+    }
+    void VisitEmbeddedPointer(InstructionStream host,
+                              RelocInfo* rinfo) override {
       UNREACHABLE();
     }
-    void VisitCodeTarget(RelocInfo* rinfo) override { UNREACHABLE(); }
-    void VisitEmbeddedPointer(RelocInfo* rinfo) override { UNREACHABLE(); }
 
    private:
     ExternalPointerTable* table_;
@@ -1472,7 +1481,7 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitCodePointer(Code host, CodeObjectSlot slot) final {
+  inline void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
     // This code is similar to the implementation of VisitPointer() modulo
     // new kind of slot.
     DCHECK(!HasWeakHeapObjectTag(slot.load(code_cage_base())));
@@ -1505,7 +1514,9 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     }
   }
 
-  inline void VisitCodeTarget(RelocInfo* rinfo) override {
+  inline void VisitCodeTarget(InstructionStream host,
+                              RelocInfo* rinfo) override {
+    DCHECK_EQ(host, rinfo->host());
     DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
@@ -1513,20 +1524,24 @@ class RecordMigratedSlotVisitor : public ObjectVisitorWithCageBases {
     // the old-to-new remembered set.
     DCHECK(!Heap::InYoungGeneration(target));
     DCHECK(!target.InWritableSharedSpace());
-    heap_->mark_compact_collector()->RecordRelocSlot(rinfo, target);
+    heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, target);
   }
 
-  inline void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  inline void VisitEmbeddedPointer(InstructionStream host,
+                                   RelocInfo* rinfo) override {
+    DCHECK_EQ(host, rinfo->host());
     DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
     HeapObject object = rinfo->target_object(cage_base());
-    GenerationalBarrierForCode(rinfo, object);
-    WriteBarrier::Shared(rinfo->instruction_stream(), rinfo, object);
-    heap_->mark_compact_collector()->RecordRelocSlot(rinfo, object);
+    GenerationalBarrierForCode(host, rinfo, object);
+    WriteBarrier::Shared(host, rinfo, object);
+    heap_->mark_compact_collector()->RecordRelocSlot(host, rinfo, object);
   }
 
   // Entries that are skipped for recording.
-  inline void VisitExternalReference(RelocInfo* rinfo) final {}
-  inline void VisitInternalReference(RelocInfo* rinfo) final {}
+  inline void VisitExternalReference(InstructionStream host,
+                                     RelocInfo* rinfo) final {}
+  inline void VisitInternalReference(InstructionStream host,
+                                     RelocInfo* rinfo) final {}
   inline void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                    ExternalPointerTag tag) final {}
 
@@ -1675,8 +1690,8 @@ class EvacuateVisitorBase : public HeapObjectVisitor {
     } else if (dest == CODE_SPACE) {
       DCHECK_CODEOBJECT_SIZE(size, base->heap_->code_space());
       base->heap_->CopyBlock(dst_addr, src_addr, size);
-      InstructionStream istream = InstructionStream::cast(dst);
-      istream.Relocate(dst_addr - src_addr);
+      InstructionStream code = InstructionStream::cast(dst);
+      code.Relocate(dst_addr - src_addr);
       if (mode != MigrationMode::kFast)
         base->ExecuteMigrationObservers(dest, src, dst, size);
       // In case the object's map gets relocated during GC we load the old map
@@ -2458,9 +2473,10 @@ void MarkCompactCollector::ProcessTopOptimizedFrame(ObjectVisitor* visitor,
     if (it.frame()->is_optimized()) {
       GcSafeCode lookup_result = it.frame()->GcSafeLookupCode();
       if (!lookup_result.has_instruction_stream()) return;
-      if (!lookup_result.CanDeoptAt(isolate, it.frame()->pc())) {
-        InstructionStream istream = InstructionStream::unchecked_cast(
-            lookup_result.raw_instruction_stream());
+      InstructionStream istream = InstructionStream::unchecked_cast(
+          lookup_result.raw_instruction_stream());
+      DCHECK_NE(istream, Smi::zero());
+      if (!istream.CanDeoptAt(isolate, it.frame()->pc())) {
         PtrComprCageBase cage_base(isolate);
         InstructionStream::BodyDescriptor::IterateBody(istream.map(cage_base),
                                                        istream, visitor);
@@ -3016,15 +3032,15 @@ void MarkCompactCollector::ClearNonLiveReferences() {
 }
 
 void MarkCompactCollector::MarkDependentCodeForDeoptimization() {
-  std::pair<HeapObject, Code> weak_object_in_code;
+  std::pair<HeapObject, InstructionStream> weak_object_in_code;
   while (local_weak_objects()->weak_objects_in_code_local.Pop(
       &weak_object_in_code)) {
     HeapObject object = weak_object_in_code.first;
-    Code code = weak_object_in_code.second;
+    InstructionStream code = weak_object_in_code.second;
     if (!non_atomic_marking_state()->IsBlackOrGrey(object) &&
         !code.embedded_objects_cleared()) {
       if (!code.marked_for_deoptimization()) {
-        code.SetMarkedForDeoptimization(isolate(), "weak objects");
+        code.SetMarkedForDeoptimization("weak objects");
         have_code_to_deoptimize_ = true;
       }
       code.ClearEmbeddedObjects(heap_);
@@ -3149,7 +3165,7 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
       // acquire-loaded.
       baseline_istream = FromCode(baseline_code, isolate(), kRelaxedLoad);
       baseline_bytecode_or_interpreter_data =
-          baseline_code.bytecode_or_interpreter_data();
+          baseline_istream.bytecode_or_interpreter_data(isolate());
     }
     // During flushing a BytecodeArray is transformed into an UncompiledData in
     // place. Seeing an UncompiledData here implies that another
@@ -3595,10 +3611,10 @@ bool MarkCompactCollector::IsOnEvacuationCandidate(MaybeObject obj) {
 }
 
 // static
-bool MarkCompactCollector::ShouldRecordRelocSlot(RelocInfo* rinfo,
+bool MarkCompactCollector::ShouldRecordRelocSlot(InstructionStream host,
+                                                 RelocInfo* rinfo,
                                                  HeapObject target) {
-  MemoryChunk* source_chunk =
-      MemoryChunk::FromHeapObject(rinfo->instruction_stream());
+  MemoryChunk* source_chunk = MemoryChunk::FromHeapObject(host);
   BasicMemoryChunk* target_chunk = BasicMemoryChunk::FromHeapObject(target);
   return target_chunk->IsEvacuationCandidate() &&
          !source_chunk->ShouldSkipEvacuationSlotRecording();
@@ -3606,7 +3622,10 @@ bool MarkCompactCollector::ShouldRecordRelocSlot(RelocInfo* rinfo,
 
 // static
 MarkCompactCollector::RecordRelocSlotInfo
-MarkCompactCollector::ProcessRelocInfo(RelocInfo* rinfo, HeapObject target) {
+MarkCompactCollector::ProcessRelocInfo(InstructionStream host, RelocInfo* rinfo,
+                                       HeapObject target) {
+  DCHECK_EQ(host, rinfo->host());
+
   RecordRelocSlotInfo result;
   const RelocInfo::Mode rmode = rinfo->rmode();
   Address addr;
@@ -3636,8 +3655,7 @@ MarkCompactCollector::ProcessRelocInfo(RelocInfo* rinfo, HeapObject target) {
     }
   }
 
-  MemoryChunk* const source_chunk =
-      MemoryChunk::FromHeapObject(rinfo->instruction_stream());
+  MemoryChunk* const source_chunk = MemoryChunk::FromHeapObject(host);
   const uintptr_t offset = addr - source_chunk->address();
   DCHECK_LT(offset, static_cast<uintptr_t>(TypedSlotSet::kMaxOffset));
   result.memory_chunk = source_chunk;
@@ -3648,10 +3666,11 @@ MarkCompactCollector::ProcessRelocInfo(RelocInfo* rinfo, HeapObject target) {
 }
 
 // static
-void MarkCompactCollector::RecordRelocSlot(RelocInfo* rinfo,
+void MarkCompactCollector::RecordRelocSlot(InstructionStream host,
+                                           RelocInfo* rinfo,
                                            HeapObject target) {
-  if (!ShouldRecordRelocSlot(rinfo, target)) return;
-  RecordRelocSlotInfo info = ProcessRelocInfo(rinfo, target);
+  if (!ShouldRecordRelocSlot(host, rinfo, target)) return;
+  RecordRelocSlotInfo info = ProcessRelocInfo(host, rinfo, target);
 
   // Access to TypeSlots need to be protected, since LocalHeaps might
   // publish code in the background thread.
@@ -3867,7 +3886,7 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     UpdateStrongCodeSlot<AccessMode::NON_ATOMIC>(host, cage_base(),
                                                  code_cage_base(), slot);
   }
@@ -3893,12 +3912,12 @@ class PointersUpdatingVisitor final : public ObjectVisitorWithCageBases,
     }
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     // This visitor nevers visits code objects.
     UNREACHABLE();
   }
@@ -5478,12 +5497,12 @@ class YoungGenerationMarkingVerifier : public MarkingVerifier {
     UNREACHABLE();
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VerifyHeapObjectImpl(target);
   }
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     VerifyHeapObjectImpl(rinfo->target_object(cage_base()));
   }
   void VerifyRootPointers(FullObjectSlot start, FullObjectSlot end) override {
diff --git a/src/heap/mark-compact.h b/src/heap/mark-compact.h
index 77fc1e1886b..14a75424f8f 100644
--- a/src/heap/mark-compact.h
+++ b/src/heap/mark-compact.h
@@ -213,7 +213,8 @@ class MainMarkingVisitor final
   template <typename TSlot>
   void RecordSlot(HeapObject object, TSlot slot, HeapObject target);
 
-  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
+  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                       HeapObject target);
 
   MarkingState* marking_state() { return marking_state_; }
 
@@ -381,11 +382,14 @@ class MarkCompactCollector final : public CollectorBase {
 
   static V8_EXPORT_PRIVATE bool IsMapOrForwarded(Map map);
 
-  static bool ShouldRecordRelocSlot(RelocInfo* rinfo, HeapObject target);
-  static RecordRelocSlotInfo ProcessRelocInfo(RelocInfo* rinfo,
+  static bool ShouldRecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                                    HeapObject target);
+  static RecordRelocSlotInfo ProcessRelocInfo(InstructionStream host,
+                                              RelocInfo* rinfo,
                                               HeapObject target);
 
-  static void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
+  static void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                              HeapObject target);
   V8_INLINE static void RecordSlot(HeapObject object, ObjectSlot slot,
                                    HeapObject target);
   V8_INLINE static void RecordSlot(HeapObject object, HeapObjectSlot slot,
diff --git a/src/heap/marking-barrier.cc b/src/heap/marking-barrier.cc
index 8b746d2751e..741acd08480 100644
--- a/src/heap/marking-barrier.cc
+++ b/src/heap/marking-barrier.cc
@@ -77,9 +77,9 @@ void MarkingBarrier::Write(InstructionStream host, RelocInfo* reloc_info,
     if (is_main_thread_barrier_) {
       // An optimization to avoid allocating additional typed slots for the
       // main thread.
-      major_collector_->RecordRelocSlot(reloc_info, value);
+      major_collector_->RecordRelocSlot(host, reloc_info, value);
     } else {
-      RecordRelocSlot(reloc_info, value);
+      RecordRelocSlot(host, reloc_info, value);
     }
   }
 }
@@ -146,12 +146,13 @@ void MarkingBarrier::Write(DescriptorArray descriptor_array,
   }
 }
 
-void MarkingBarrier::RecordRelocSlot(RelocInfo* rinfo, HeapObject target) {
-  DCHECK(IsCurrentMarkingBarrier(rinfo->instruction_stream()));
-  if (!MarkCompactCollector::ShouldRecordRelocSlot(rinfo, target)) return;
+void MarkingBarrier::RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                                     HeapObject target) {
+  DCHECK(IsCurrentMarkingBarrier(host));
+  if (!MarkCompactCollector::ShouldRecordRelocSlot(host, rinfo, target)) return;
 
   MarkCompactCollector::RecordRelocSlotInfo info =
-      MarkCompactCollector::ProcessRelocInfo(rinfo, target);
+      MarkCompactCollector::ProcessRelocInfo(host, rinfo, target);
 
   auto& typed_slots = typed_slots_map_[info.memory_chunk];
   if (!typed_slots) {
diff --git a/src/heap/marking-barrier.h b/src/heap/marking-barrier.h
index 382267edf6b..35d59d3bcb1 100644
--- a/src/heap/marking-barrier.h
+++ b/src/heap/marking-barrier.h
@@ -66,7 +66,8 @@ class MarkingBarrier {
 
   inline bool WhiteToGreyAndPush(HeapObject value);
 
-  void RecordRelocSlot(RelocInfo* rinfo, HeapObject target);
+  void RecordRelocSlot(InstructionStream host, RelocInfo* rinfo,
+                       HeapObject target);
 
   bool IsCurrentMarkingBarrier(HeapObject verification_candidate);
 
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index 03038f6c14e..4241fabdf67 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -103,7 +103,7 @@ MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitPointersImpl(
 template <typename ConcreteVisitor, typename MarkingState>
 V8_INLINE void
 MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodePointerImpl(
-    Code host, CodeObjectSlot slot) {
+    HeapObject host, CodeObjectSlot slot) {
   Object object =
       slot.Relaxed_Load(ObjectVisitorWithCageBases::code_cage_base());
   HeapObject heap_object;
@@ -117,35 +117,34 @@ MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodePointerImpl(
 
 template <typename ConcreteVisitor, typename MarkingState>
 void MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitEmbeddedPointer(
-    RelocInfo* rinfo) {
+    InstructionStream host, RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
   HeapObject object =
       rinfo->target_object(ObjectVisitorWithCageBases::cage_base());
   if (!ShouldMarkObject(object)) return;
 
   if (!concrete_visitor()->marking_state()->IsBlackOrGrey(object)) {
-    if (rinfo->code().IsWeakObject(object)) {
+    if (host.IsWeakObject(object)) {
       local_weak_objects_->weak_objects_in_code_local.Push(
-          std::make_pair(object, rinfo->code()));
-      AddWeakReferenceForReferenceSummarizer(rinfo->instruction_stream(),
-                                             object);
+          std::make_pair(object, host));
+      AddWeakReferenceForReferenceSummarizer(host, object);
     } else {
-      MarkObject(rinfo->instruction_stream(), object);
+      MarkObject(host, object);
     }
   }
-  concrete_visitor()->RecordRelocSlot(rinfo, object);
+  concrete_visitor()->RecordRelocSlot(host, rinfo, object);
 }
 
 template <typename ConcreteVisitor, typename MarkingState>
 void MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitCodeTarget(
-    RelocInfo* rinfo) {
+    InstructionStream host, RelocInfo* rinfo) {
   DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
 
   if (!ShouldMarkObject(target)) return;
-  MarkObject(rinfo->instruction_stream(), target);
-  concrete_visitor()->RecordRelocSlot(rinfo, target);
+  MarkObject(host, target);
+  concrete_visitor()->RecordRelocSlot(host, rinfo, target);
 }
 
 template <typename ConcreteVisitor, typename MarkingState>
@@ -217,10 +216,16 @@ int MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitSharedFunctionInfo(
     // then we have to visit the bytecode but not the baseline code.
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
     Code baseline_code = Code::cast(shared_info.function_data(kAcquireLoad));
+    // Safe to do a relaxed load here since the Code was
+    // acquire-loaded.
+    InstructionStream baseline_istream =
+        FromCode(baseline_code, ObjectVisitorWithCageBases::code_cage_base(),
+                 kRelaxedLoad);
     // Visit the bytecode hanging off baseline code.
-    VisitPointer(baseline_code,
-                 baseline_code.RawField(
-                     Code::kDeoptimizationDataOrInterpreterDataOffset));
+    VisitPointer(
+        baseline_istream,
+        baseline_istream.RawField(
+            InstructionStream::kDeoptimizationDataOrInterpreterDataOffset));
     local_weak_objects_->code_flushing_candidates_local.Push(shared_info);
   } else {
     // In other cases, record as a flushing candidate since we have old
diff --git a/src/heap/marking-visitor.h b/src/heap/marking-visitor.h
index f2c069e5a61..876e30048ef 100644
--- a/src/heap/marking-visitor.h
+++ b/src/heap/marking-visitor.h
@@ -98,11 +98,13 @@ class MarkingVisitorBase : public HeapVisitor<int, ConcreteVisitor> {
                                MaybeObjectSlot end) final {
     VisitPointersImpl(host, start, end);
   }
-  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final {
+  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
     VisitCodePointerImpl(host, slot);
   }
-  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final;
-  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final;
+  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
+                                      RelocInfo* rinfo) final;
+  V8_INLINE void VisitCodeTarget(InstructionStream host,
+                                 RelocInfo* rinfo) final;
   void VisitCustomWeakPointers(HeapObject host, ObjectSlot start,
                                ObjectSlot end) final {
     // Weak list pointers should be ignored during marking. The lists are
@@ -147,7 +149,7 @@ class MarkingVisitorBase : public HeapVisitor<int, ConcreteVisitor> {
 
   // Similar to VisitPointersImpl() but using code cage base for loading from
   // the slot.
-  V8_INLINE void VisitCodePointerImpl(Code host, CodeObjectSlot slot);
+  V8_INLINE void VisitCodePointerImpl(HeapObject host, CodeObjectSlot slot);
 
   V8_INLINE void VisitDescriptorsForMap(Map map);
 
@@ -214,7 +216,8 @@ class YoungGenerationMarkingVisitorBase
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  V8_INLINE void VisitCodePointer(HeapObject host,
+                                  CodeObjectSlot slot) override {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // InstructionStream slots never appear in new space because
     // Code objects, the only object that can contain code pointers, are
@@ -230,13 +233,15 @@ class YoungGenerationMarkingVisitorBase
     VisitPointerImpl(host, slot);
   }
 
-  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final {
-    // Code objects are not expected in new space.
+  V8_INLINE void VisitCodeTarget(InstructionStream host,
+                                 RelocInfo* rinfo) final {
+    // InstructionStream objects are not expected in new space.
     UNREACHABLE();
   }
 
-  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final {
-    // Code objects are not expected in new space.
+  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
+                                      RelocInfo* rinfo) final {
+    // InstructionStream objects are not expected in new space.
     UNREACHABLE();
   }
 
diff --git a/src/heap/object-stats.cc b/src/heap/object-stats.cc
index c533834ee18..1531f39734a 100644
--- a/src/heap/object-stats.cc
+++ b/src/heap/object-stats.cc
@@ -97,16 +97,17 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
     *tagged_fields_count_ += (end - start);
   }
 
-  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  V8_INLINE void VisitCodePointer(HeapObject host,
+                                  CodeObjectSlot slot) override {
     *tagged_fields_count_ += 1;
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     // InstructionStream target is most likely encoded as a relative 32-bit
     // offset and not as a full tagged value, so there's nothing to count.
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     *tagged_fields_count_ += 1;
   }
 
@@ -1036,20 +1037,19 @@ ObjectStats::VirtualInstanceType CodeKindToVirtualInstanceType(CodeKind kind) {
 }  // namespace
 
 void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
-    InstructionStream istream) {
-  Code code = istream.code(kAcquireLoad);
-  RecordSimpleVirtualObjectStats(HeapObject(), istream,
+    InstructionStream code) {
+  RecordSimpleVirtualObjectStats(HeapObject(), code,
                                  CodeKindToVirtualInstanceType(code.kind()));
-  RecordSimpleVirtualObjectStats(istream, code.relocation_info(),
+  RecordSimpleVirtualObjectStats(code, code.relocation_info(),
                                  ObjectStats::RELOC_INFO_TYPE);
   if (CodeKindIsOptimizedJSFunction(code.kind())) {
     Object source_position_table = code.source_position_table();
     if (source_position_table.IsHeapObject()) {
-      RecordSimpleVirtualObjectStats(istream,
+      RecordSimpleVirtualObjectStats(code,
                                      HeapObject::cast(source_position_table),
                                      ObjectStats::SOURCE_POSITION_TABLE_TYPE);
     }
-    RecordSimpleVirtualObjectStats(istream, code.deoptimization_data(),
+    RecordSimpleVirtualObjectStats(code, code.deoptimization_data(),
                                    ObjectStats::DEOPTIMIZATION_DATA_TYPE);
     DeoptimizationData input_data =
         DeoptimizationData::cast(code.deoptimization_data());
@@ -1065,7 +1065,7 @@ void ObjectStatsCollectorImpl::RecordVirtualCodeDetails(
     Object target = it.rinfo()->target_object(cage_base());
     if (target.IsFixedArrayExact(cage_base())) {
       RecordVirtualObjectsForConstantPoolOrEmbeddedObjects(
-          istream, HeapObject::cast(target), ObjectStats::EMBEDDED_OBJECT_TYPE);
+          code, HeapObject::cast(target), ObjectStats::EMBEDDED_OBJECT_TYPE);
     }
   }
 }
diff --git a/src/heap/read-only-spaces.cc b/src/heap/read-only-spaces.cc
index 869668245aa..77016649830 100644
--- a/src/heap/read-only-spaces.cc
+++ b/src/heap/read-only-spaces.cc
@@ -442,7 +442,12 @@ class ReadOnlySpaceObjectIterator : public ObjectIterator {
       cur_addr_ += ALIGN_TO_ALLOCATION_ALIGNMENT(obj_size);
       DCHECK_LE(cur_addr_, cur_end_);
       if (!obj.IsFreeSpaceOrFiller()) {
-        DCHECK_OBJECT_SIZE(obj_size);
+        if (obj.IsInstructionStream()) {
+          DCHECK(InstructionStream::cast(obj).is_builtin());
+          DCHECK_CODEOBJECT_SIZE(obj_size, space_);
+        } else {
+          DCHECK_OBJECT_SIZE(obj_size);
+        }
         return obj;
       }
     }
diff --git a/src/heap/remembered-set-inl.h b/src/heap/remembered-set-inl.h
index d30f8871221..2de82cc6af8 100644
--- a/src/heap/remembered-set-inl.h
+++ b/src/heap/remembered-set-inl.h
@@ -20,20 +20,19 @@ SlotCallbackResult UpdateTypedSlotHelper::UpdateTypedSlot(Heap* heap,
                                                           Callback callback) {
   switch (slot_type) {
     case SlotType::kCodeEntry: {
-      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, Code(),
-                      InstructionStream());
+      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, InstructionStream());
       return UpdateCodeTarget(&rinfo, callback);
     }
     case SlotType::kConstPoolCodeEntry: {
       return UpdateCodeEntry(addr, callback);
     }
     case SlotType::kEmbeddedObjectCompressed: {
-      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0, Code(),
+      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0,
                       InstructionStream());
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
     case SlotType::kEmbeddedObjectFull: {
-      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0, Code(),
+      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0,
                       InstructionStream());
       return UpdateEmbeddedPointer(heap, &rinfo, callback);
     }
@@ -64,20 +63,19 @@ HeapObject UpdateTypedSlotHelper::GetTargetObject(Heap* heap,
                                                   Address addr) {
   switch (slot_type) {
     case SlotType::kCodeEntry: {
-      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, Code(),
-                      InstructionStream());
+      RelocInfo rinfo(addr, RelocInfo::CODE_TARGET, 0, InstructionStream());
       return InstructionStream::FromTargetAddress(rinfo.target_address());
     }
     case SlotType::kConstPoolCodeEntry: {
       return InstructionStream::FromEntryAddress(addr);
     }
     case SlotType::kEmbeddedObjectCompressed: {
-      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0, Code(),
+      RelocInfo rinfo(addr, RelocInfo::COMPRESSED_EMBEDDED_OBJECT, 0,
                       InstructionStream());
       return rinfo.target_object(heap->isolate());
     }
     case SlotType::kEmbeddedObjectFull: {
-      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0, Code(),
+      RelocInfo rinfo(addr, RelocInfo::FULL_EMBEDDED_OBJECT, 0,
                       InstructionStream());
       return rinfo.target_object(heap->isolate());
     }
diff --git a/src/heap/scavenger-inl.h b/src/heap/scavenger-inl.h
index dacbda3c307..ea827b163fd 100644
--- a/src/heap/scavenger-inl.h
+++ b/src/heap/scavenger-inl.h
@@ -478,10 +478,12 @@ class ScavengeVisitor final : public NewSpaceVisitor<ScavengeVisitor> {
 
   V8_INLINE void VisitPointers(HeapObject host, MaybeObjectSlot start,
                                MaybeObjectSlot end) final;
-  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final;
+  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final;
 
-  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final;
-  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final;
+  V8_INLINE void VisitCodeTarget(InstructionStream host,
+                                 RelocInfo* rinfo) final;
+  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
+                                      RelocInfo* rinfo) final;
   V8_INLINE int VisitEphemeronHashTable(Map map, EphemeronHashTable object);
   V8_INLINE int VisitJSArrayBuffer(Map map, JSArrayBuffer object);
   V8_INLINE int VisitJSApiObject(Map map, JSObject object);
@@ -506,7 +508,7 @@ void ScavengeVisitor::VisitPointers(HeapObject host, MaybeObjectSlot start,
   return VisitPointersImpl(host, start, end);
 }
 
-void ScavengeVisitor::VisitCodePointer(Code host, CodeObjectSlot slot) {
+void ScavengeVisitor::VisitCodePointer(HeapObject host, CodeObjectSlot slot) {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // InstructionStream slots never appear in new space because
   // Code objects, the only object that can contain code pointers, are
@@ -514,7 +516,8 @@ void ScavengeVisitor::VisitCodePointer(Code host, CodeObjectSlot slot) {
   UNREACHABLE();
 }
 
-void ScavengeVisitor::VisitCodeTarget(RelocInfo* rinfo) {
+void ScavengeVisitor::VisitCodeTarget(InstructionStream host,
+                                      RelocInfo* rinfo) {
   InstructionStream target =
       InstructionStream::FromTargetAddress(rinfo->target_address());
 #ifdef DEBUG
@@ -527,7 +530,8 @@ void ScavengeVisitor::VisitCodeTarget(RelocInfo* rinfo) {
   DCHECK_EQ(old_target, target);
 }
 
-void ScavengeVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void ScavengeVisitor::VisitEmbeddedPointer(InstructionStream host,
+                                           RelocInfo* rinfo) {
   HeapObject heap_object = rinfo->target_object(cage_base());
 #ifdef DEBUG
   HeapObject old_heap_object = heap_object;
diff --git a/src/heap/scavenger.cc b/src/heap/scavenger.cc
index 36cf21adb9d..5111ee66415 100644
--- a/src/heap/scavenger.cc
+++ b/src/heap/scavenger.cc
@@ -61,7 +61,7 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
     VisitPointersImpl(host, start, end);
   }
 
-  V8_INLINE void VisitCodePointer(Code host, CodeObjectSlot slot) final {
+  V8_INLINE void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // InstructionStream slots never appear in new space because
     // Code objects, the only object that can contain code pointers, are
@@ -69,17 +69,17 @@ class IterateAndScavengePromotedObjectsVisitor final : public ObjectVisitor {
     UNREACHABLE();
   }
 
-  V8_INLINE void VisitCodeTarget(RelocInfo* rinfo) final {
+  V8_INLINE void VisitCodeTarget(InstructionStream host,
+                                 RelocInfo* rinfo) final {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
-    HandleSlot(rinfo->instruction_stream(), FullHeapObjectSlot(&target),
-               target);
+    HandleSlot(host, FullHeapObjectSlot(&target), target);
   }
-  V8_INLINE void VisitEmbeddedPointer(RelocInfo* rinfo) final {
-    PtrComprCageBase cage_base = GetPtrComprCageBase(rinfo->code());
+  V8_INLINE void VisitEmbeddedPointer(InstructionStream host,
+                                      RelocInfo* rinfo) final {
+    PtrComprCageBase cage_base = host.main_cage_base();
     HeapObject heap_object = rinfo->target_object(cage_base);
-    HandleSlot(rinfo->instruction_stream(), FullHeapObjectSlot(&heap_object),
-               heap_object);
+    HandleSlot(host, FullHeapObjectSlot(&heap_object), heap_object);
   }
 
   inline void VisitEphemeron(HeapObject obj, int entry, ObjectSlot key,
diff --git a/src/heap/sweeper.cc b/src/heap/sweeper.cc
index 6d578752310..c260c8f47eb 100644
--- a/src/heap/sweeper.cc
+++ b/src/heap/sweeper.cc
@@ -817,7 +817,7 @@ class PromotedPageRecordMigratedSlotVisitor
     }
   }
 
-  inline void VisitCodePointer(Code host, CodeObjectSlot slot) final {
+  inline void VisitCodePointer(HeapObject host, CodeObjectSlot slot) final {
     CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
     // This code is similar to the implementation of VisitPointer() modulo
     // new kind of slot.
@@ -835,12 +835,18 @@ class PromotedPageRecordMigratedSlotVisitor
     VisitPointer(host, key);
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) final { UNREACHABLE(); }
-  void VisitEmbeddedPointer(RelocInfo* rinfo) final { UNREACHABLE(); }
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) final {
+    UNREACHABLE();
+  }
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) final {
+    UNREACHABLE();
+  }
 
   // Entries that are skipped for recording.
-  inline void VisitExternalReference(RelocInfo* rinfo) final {}
-  inline void VisitInternalReference(RelocInfo* rinfo) final {}
+  inline void VisitExternalReference(InstructionStream host,
+                                     RelocInfo* rinfo) final {}
+  inline void VisitInternalReference(InstructionStream host,
+                                     RelocInfo* rinfo) final {}
   inline void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                    ExternalPointerTag tag) final {}
 
diff --git a/src/heap/weak-object-worklists.h b/src/heap/weak-object-worklists.h
index a921a415079..68749b9bc46 100644
--- a/src/heap/weak-object-worklists.h
+++ b/src/heap/weak-object-worklists.h
@@ -19,7 +19,7 @@ struct Ephemeron {
 };
 
 using HeapObjectAndSlot = std::pair<HeapObject, HeapObjectSlot>;
-using HeapObjectAndCode = std::pair<HeapObject, Code>;
+using HeapObjectAndCode = std::pair<HeapObject, InstructionStream>;
 class EphemeronHashTable;
 class JSFunction;
 class SharedFunctionInfo;
diff --git a/src/logging/code-events.h b/src/logging/code-events.h
index 9faee22ab69..562eaff77ad 100644
--- a/src/logging/code-events.h
+++ b/src/logging/code-events.h
@@ -95,11 +95,12 @@ class LogEventListener {
   virtual void CodeMovingGCEvent() = 0;
   virtual void CodeDisableOptEvent(Handle<AbstractCode> code,
                                    Handle<SharedFunctionInfo> shared) = 0;
-  virtual void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
-                              Address pc, int fp_to_sp_delta) = 0;
+  virtual void CodeDeoptEvent(Handle<InstructionStream> code,
+                              DeoptimizeKind kind, Address pc,
+                              int fp_to_sp_delta) = 0;
   // These events can happen when 1. an assumption made by optimized code fails
   // or 2. a weakly embedded object dies.
-  virtual void CodeDependencyChangeEvent(Handle<Code> code,
+  virtual void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                          Handle<SharedFunctionInfo> shared,
                                          const char* reason) = 0;
   // Called during GC shortly after any weak references to code objects are
@@ -242,14 +243,14 @@ class Logger {
       listener->CodeDisableOptEvent(code, shared);
     }
   }
-  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                      int fp_to_sp_delta) {
+  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                      Address pc, int fp_to_sp_delta) {
     base::MutexGuard guard(&mutex_);
     for (auto listener : listeners_) {
       listener->CodeDeoptEvent(code, kind, pc, fp_to_sp_delta);
     }
   }
-  void CodeDependencyChangeEvent(Handle<Code> code,
+  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) {
     base::MutexGuard guard(&mutex_);
diff --git a/src/logging/log.cc b/src/logging/log.cc
index 0d1aa645e86..3b4caa6fa06 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -625,8 +625,7 @@ void ExternalLogEventListener::CodeMoveEvent(InstructionStream from,
                                              InstructionStream to) {
   CodeEvent code_event;
   InitializeCodeEvent(isolate_, &code_event, from.instruction_start(),
-                      to.instruction_start(),
-                      to.code(kAcquireLoad).instruction_size());
+                      to.instruction_start(), to.instruction_size());
   code_event_handler_->Handle(reinterpret_cast<v8::CodeEvent*>(&code_event));
 }
 
@@ -915,7 +914,7 @@ void JitLogger::CodeMoveEvent(InstructionStream from, InstructionStream to) {
   event.type = JitCodeEvent::CODE_MOVED;
   event.code_type = JitCodeEvent::JIT_CODE;
   event.code_start = reinterpret_cast<void*>(from.instruction_start());
-  event.code_len = from.unchecked_code().instruction_size();
+  event.code_len = from.instruction_size();
   event.new_code_start = reinterpret_cast<void*>(to.instruction_start());
   event.isolate = reinterpret_cast<v8::Isolate*>(isolate_);
 
@@ -1624,11 +1623,12 @@ void V8FileLogger::CodeDisableOptEvent(Handle<AbstractCode> code,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
-                                     const char* kind, const char* reason) {
+void V8FileLogger::ProcessDeoptEvent(Handle<InstructionStream> code,
+                                     SourcePosition position, const char* kind,
+                                     const char* reason) {
   MSG_BUILDER();
   msg << Event::kCodeDeopt << kNext << Time() << kNext << code->CodeSize()
-      << kNext << reinterpret_cast<void*>(code->InstructionStart());
+      << kNext << reinterpret_cast<void*>(code->instruction_start());
 
   std::ostringstream deopt_location;
   int inlining_id = -1;
@@ -1646,15 +1646,16 @@ void V8FileLogger::ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
   msg.WriteToLogFile();
 }
 
-void V8FileLogger::CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
-                                  Address pc, int fp_to_sp_delta) {
+void V8FileLogger::CodeDeoptEvent(Handle<InstructionStream> code,
+                                  DeoptimizeKind kind, Address pc,
+                                  int fp_to_sp_delta) {
   if (!is_logging() || !v8_flags.log_deopt) return;
   Deoptimizer::DeoptInfo info = Deoptimizer::GetDeoptInfo(*code, pc);
   ProcessDeoptEvent(code, info.position, Deoptimizer::MessageFor(kind),
                     DeoptimizeReasonToString(info.deopt_reason));
 }
 
-void V8FileLogger::CodeDependencyChangeEvent(Handle<Code> code,
+void V8FileLogger::CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                              Handle<SharedFunctionInfo> sfi,
                                              const char* reason) {
   if (!is_logging() || !v8_flags.log_deopt) return;
diff --git a/src/logging/log.h b/src/logging/log.h
index c0720c9f27a..d411da512bc 100644
--- a/src/logging/log.h
+++ b/src/logging/log.h
@@ -197,16 +197,17 @@ class V8FileLogger : public LogEventListener {
   void CodeMovingGCEvent() override;
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override;
-  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                      int fp_to_sp_delta) override;
-  void CodeDependencyChangeEvent(Handle<Code> code,
+  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                      Address pc, int fp_to_sp_delta) override;
+  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override;
   void FeedbackVectorEvent(FeedbackVector vector, AbstractCode code);
   void WeakCodeClearEvent() override {}
 
-  void ProcessDeoptEvent(Handle<Code> code, SourcePosition position,
-                         const char* kind, const char* reason);
+  void ProcessDeoptEvent(Handle<InstructionStream> code,
+                         SourcePosition position, const char* kind,
+                         const char* reason);
 
   // Emits a code line info record event.
   void CodeLinePosInfoRecordEvent(Address code_start,
@@ -432,9 +433,9 @@ class V8_EXPORT_PRIVATE CodeEventLogger : public LogEventListener {
   void SharedFunctionInfoMoveEvent(Address from, Address to) override {}
   void NativeContextMoveEvent(Address from, Address to) override {}
   void CodeMovingGCEvent() override {}
-  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                      int fp_to_sp_delta) override {}
-  void CodeDependencyChangeEvent(Handle<Code> code,
+  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                      Address pc, int fp_to_sp_delta) override {}
+  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override {}
@@ -504,9 +505,9 @@ class ExternalLogEventListener : public LogEventListener {
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override {}
   void CodeMovingGCEvent() override {}
-  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                      int fp_to_sp_delta) override {}
-  void CodeDependencyChangeEvent(Handle<Code> code,
+  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                      Address pc, int fp_to_sp_delta) override {}
+  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override {}
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 2878dd65595..2bcf2d1ad62 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -69,9 +69,6 @@ GCSAFE_CODE_FWD_ACCESSOR(bool, is_turbofanned)
 GCSAFE_CODE_FWD_ACCESSOR(bool, has_tagged_outgoing_params)
 GCSAFE_CODE_FWD_ACCESSOR(bool, marked_for_deoptimization)
 GCSAFE_CODE_FWD_ACCESSOR(Object, raw_instruction_stream)
-GCSAFE_CODE_FWD_ACCESSOR(int, stack_slots)
-GCSAFE_CODE_FWD_ACCESSOR(Address, constant_pool)
-GCSAFE_CODE_FWD_ACCESSOR(Address, SafepointTableAddress)
 #undef GCSAFE_CODE_FWD_ACCESSOR
 
 int GcSafeCode::GetOffsetFromInstructionStart(Isolate* isolate,
@@ -84,22 +81,10 @@ Address GcSafeCode::InstructionStart(Isolate* isolate, Address pc) const {
 }
 
 Address GcSafeCode::InstructionEnd(Isolate* isolate, Address pc) const {
-  return UnsafeCastToCode().InstructionEnd(isolate, pc);
-}
-
-bool GcSafeCode::CanDeoptAt(Isolate* isolate, Address pc) const {
-  DeoptimizationData deopt_data = DeoptimizationData::unchecked_cast(
-      UnsafeCastToCode().unchecked_deoptimization_data());
-  Address code_start_address = InstructionStart();
-  for (int i = 0; i < deopt_data.DeoptCount(); i++) {
-    if (deopt_data.Pc(i).value() == -1) continue;
-    Address address = code_start_address + deopt_data.Pc(i).value();
-    if (address == pc &&
-        deopt_data.GetBytecodeOffset(i) != BytecodeOffset::None()) {
-      return true;
-    }
-  }
-  return false;
+  return V8_LIKELY(has_instruction_stream())
+             ? InstructionStream::unchecked_cast(raw_instruction_stream())
+                   .instruction_end()
+             : UnsafeCastToCode().OffHeapInstructionEnd(isolate, pc);
 }
 
 int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
@@ -141,7 +126,10 @@ ByteArray AbstractCode::SourcePositionTable(Isolate* isolate,
 int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
   Map map_object = map(cage_base);
   if (InstanceTypeChecker::IsCode(map_object)) {
-    return GetCode().SizeIncludingMetadata();
+    Code code = GetCode();
+    return code.has_instruction_stream()
+               ? FromCode(code).SizeIncludingMetadata(cage_base)
+               : 0;
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(map_object));
     return GetBytecodeArray().SizeIncludingMetadata();
@@ -233,30 +221,33 @@ BytecodeArray AbstractCode::GetBytecodeArray() {
 OBJECT_CONSTRUCTORS_IMPL(InstructionStream, HeapObject)
 NEVER_READ_ONLY_SPACE_IMPL(InstructionStream)
 
-INT_ACCESSORS(Code, instruction_size, kInstructionSizeOffset)
-INT_ACCESSORS(Code, metadata_size, kMetadataSizeOffset)
-INT_ACCESSORS(Code, handler_table_offset, kHandlerTableOffsetOffset)
-INT_ACCESSORS(Code, code_comments_offset, kCodeCommentsOffsetOffset)
-INT32_ACCESSORS(Code, unwinding_info_offset, kUnwindingInfoOffsetOffset)
-ACCESSORS(Code, relocation_info, ByteArray, kRelocationInfoOffset)
-ACCESSORS_CHECKED2(Code, deoptimization_data, FixedArray,
-                   kDeoptimizationDataOrInterpreterDataOffset,
-                   kind() != CodeKind::BASELINE,
-                   kind() != CodeKind::BASELINE &&
-                       !ObjectInYoungGeneration(value))
-ACCESSORS_CHECKED2(Code, bytecode_or_interpreter_data, HeapObject,
-                   kDeoptimizationDataOrInterpreterDataOffset,
-                   kind() == CodeKind::BASELINE,
-                   kind() == CodeKind::BASELINE &&
-                       !ObjectInYoungGeneration(value))
-ACCESSORS_CHECKED2(Code, source_position_table, ByteArray, kPositionTableOffset,
-                   kind() != CodeKind::BASELINE,
-                   kind() != CodeKind::BASELINE &&
-                       !ObjectInYoungGeneration(value))
-ACCESSORS_CHECKED2(Code, bytecode_offset_table, ByteArray, kPositionTableOffset,
-                   kind() == CodeKind::BASELINE,
-                   kind() == CodeKind::BASELINE &&
-                       !ObjectInYoungGeneration(value))
+INT_ACCESSORS(InstructionStream, instruction_size, kInstructionSizeOffset)
+INT_ACCESSORS(InstructionStream, metadata_size, kMetadataSizeOffset)
+INT_ACCESSORS(InstructionStream, handler_table_offset,
+              kHandlerTableOffsetOffset)
+INT_ACCESSORS(InstructionStream, code_comments_offset,
+              kCodeCommentsOffsetOffset)
+INT32_ACCESSORS(InstructionStream, unwinding_info_offset,
+                kUnwindingInfoOffsetOffset)
+
+// Same as ACCESSORS_CHECKED2 macro but with InstructionStream as a host and
+// using main_cage_base() for computing the base.
+#define INSTRUCTION_STREAM_ACCESSORS_CHECKED2(name, type, offset,           \
+                                              get_condition, set_condition) \
+  type InstructionStream::name() const {                                    \
+    PtrComprCageBase cage_base = main_cage_base();                          \
+    return InstructionStream::name(cage_base);                              \
+  }                                                                         \
+  type InstructionStream::name(PtrComprCageBase cage_base) const {          \
+    type value = TaggedField<type, offset>::load(cage_base, *this);         \
+    DCHECK(get_condition);                                                  \
+    return value;                                                           \
+  }                                                                         \
+  void InstructionStream::set_##name(type value, WriteBarrierMode mode) {   \
+    DCHECK(set_condition);                                                  \
+    TaggedField<type, offset>::store(*this, value);                         \
+    CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);                  \
+  }
 
 // Same as RELEASE_ACQUIRE_ACCESSORS_CHECKED2 macro but with InstructionStream
 // as a host and using main_cage_base(kRelaxedLoad) for computing the base.
@@ -279,14 +270,41 @@ ACCESSORS_CHECKED2(Code, bytecode_offset_table, ByteArray, kPositionTableOffset,
     CONDITIONAL_WRITE_BARRIER(*this, offset, value, mode);                  \
   }
 
+#define INSTRUCTION_STREAM_ACCESSORS(name, type, offset) \
+  INSTRUCTION_STREAM_ACCESSORS_CHECKED2(name, type, offset, true, true)
+
 #define RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(name, type, offset) \
   RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS_CHECKED2(                 \
       name, type, offset, !ObjectInYoungGeneration(value),               \
       !ObjectInYoungGeneration(value))
 
+INSTRUCTION_STREAM_ACCESSORS(relocation_info, ByteArray, kRelocationInfoOffset)
+
+INSTRUCTION_STREAM_ACCESSORS_CHECKED2(
+    deoptimization_data, FixedArray, kDeoptimizationDataOrInterpreterDataOffset,
+    kind() != CodeKind::BASELINE,
+    kind() != CodeKind::BASELINE && !ObjectInYoungGeneration(value))
+INSTRUCTION_STREAM_ACCESSORS_CHECKED2(
+    bytecode_or_interpreter_data, HeapObject,
+    kDeoptimizationDataOrInterpreterDataOffset, kind() == CodeKind::BASELINE,
+    kind() == CodeKind::BASELINE && !ObjectInYoungGeneration(value))
+
+INSTRUCTION_STREAM_ACCESSORS_CHECKED2(source_position_table, ByteArray,
+                                      kPositionTableOffset,
+                                      kind() != CodeKind::BASELINE,
+                                      kind() != CodeKind::BASELINE &&
+                                          !ObjectInYoungGeneration(value))
+INSTRUCTION_STREAM_ACCESSORS_CHECKED2(bytecode_offset_table, ByteArray,
+                                      kPositionTableOffset,
+                                      kind() == CodeKind::BASELINE,
+                                      kind() == CodeKind::BASELINE &&
+                                          !ObjectInYoungGeneration(value))
+
 // Concurrent marker needs to access kind specific flags in code.
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(code, Code, kCodeOffset)
 RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS(raw_code, HeapObject, kCodeOffset)
+#undef INSTRUCTION_STREAM_ACCESSORS
+#undef INSTRUCTION_STREAM_ACCESSORS_CHECKED2
 #undef RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS
 #undef RELEASE_ACQUIRE_INSTRUCTION_STREAM_ACCESSORS_CHECKED2
 
@@ -331,6 +349,17 @@ Code InstructionStream::GCSafeCode(AcquireLoadTag) const {
 // Code and back.
 inline Code ToCode(InstructionStream code) { return code.code(kAcquireLoad); }
 
+inline Handle<Code> ToCode(Handle<InstructionStream> code, Isolate* isolate) {
+  return handle(ToCode(*code), isolate);
+}
+
+inline MaybeHandle<Code> ToCode(MaybeHandle<InstructionStream> maybe_code,
+                                Isolate* isolate) {
+  Handle<InstructionStream> code;
+  if (maybe_code.ToHandle(&code)) return ToCode(code, isolate);
+  return {};
+}
+
 inline InstructionStream FromCode(Code code) {
   DCHECK(code.has_instruction_stream());
   // Compute the InstructionStream object pointer from the code entry point.
@@ -357,20 +386,22 @@ inline InstructionStream FromCode(Code code, Isolate* isolate,
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-// TODO(jgruber): Remove this method once main_cage_base is gone.
 void InstructionStream::WipeOutHeader() {
+  WRITE_FIELD(*this, kRelocationInfoOffset, Smi::FromInt(0));
+  WRITE_FIELD(*this, kDeoptimizationDataOrInterpreterDataOffset,
+              Smi::FromInt(0));
+  WRITE_FIELD(*this, kPositionTableOffset, Smi::FromInt(0));
   WRITE_FIELD(*this, kCodeOffset, Smi::FromInt(0));
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     set_main_cage_base(kNullAddress, kRelaxedStore);
   }
 }
 
-void Code::ClearInstructionStreamPadding() {
+void InstructionStream::clear_padding() {
   // Clear the padding between the header and `body_start`.
-  if (FIELD_SIZE(InstructionStream::kOptionalPaddingOffset) != 0) {
-    memset(reinterpret_cast<void*>(instruction_stream().address() +
-                                   InstructionStream::kOptionalPaddingOffset),
-           0, FIELD_SIZE(InstructionStream::kOptionalPaddingOffset));
+  if (FIELD_SIZE(kOptionalPaddingOffset) != 0) {
+    memset(reinterpret_cast<void*>(address() + kOptionalPaddingOffset), 0,
+           FIELD_SIZE(kOptionalPaddingOffset));
   }
 
   // Clear the padding after `body_end`.
@@ -384,7 +415,11 @@ ByteArray Code::SourcePositionTable(Isolate* isolate,
   if (!has_instruction_stream()) {
     return GetReadOnlyRoots().empty_byte_array();
   }
+  return instruction_stream().SourcePositionTable(isolate, sfi);
+}
 
+ByteArray InstructionStream::SourcePositionTable(Isolate* isolate,
+                                                 SharedFunctionInfo sfi) const {
   DisallowGarbageCollection no_gc;
   if (kind() == CodeKind::BASELINE) {
     return sfi.GetBytecodeArray(isolate).SourcePositionTable(isolate);
@@ -392,25 +427,38 @@ ByteArray Code::SourcePositionTable(Isolate* isolate,
   return source_position_table(isolate);
 }
 
-Address Code::body_start() const { return InstructionStart(); }
+Address InstructionStream::body_start() const { return instruction_start(); }
 
-Address Code::body_end() const { return body_start() + body_size(); }
+Address InstructionStream::body_end() const {
+  return body_start() + body_size();
+}
 
-int Code::body_size() const { return instruction_size() + metadata_size(); }
+int InstructionStream::body_size() const {
+  return instruction_size() + metadata_size();
+}
 
-// TODO(jgruber): Remove instruction_size.
-int Code::InstructionSize() const { return instruction_size(); }
+int Code::InstructionSize() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().instruction_size()
+             : OffHeapInstructionSize();
+}
 
 Address InstructionStream::instruction_start() const {
   return field_address(kHeaderSize);
 }
 
+Address InstructionStream::instruction_end() const {
+  return instruction_start() + instruction_size();
+}
+
 Address Code::InstructionEnd() const {
-  return InstructionStart() + instruction_size();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().instruction_end()
+             : OffHeapInstructionEnd();
 }
 
-Address Code::metadata_start() const {
-  return InstructionStart() + instruction_size();
+Address InstructionStream::metadata_start() const {
+  return instruction_start() + instruction_size();
 }
 
 Address Code::InstructionStart(Isolate* isolate, Address pc) const {
@@ -421,7 +469,7 @@ Address Code::InstructionStart(Isolate* isolate, Address pc) const {
 
 Address Code::InstructionEnd(Isolate* isolate, Address pc) const {
   return V8_LIKELY(has_instruction_stream())
-             ? InstructionEnd()
+             ? instruction_stream().instruction_end()
              : OffHeapInstructionEnd(isolate, pc);
 }
 
@@ -432,52 +480,83 @@ int Code::GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const {
   return static_cast<int>(offset);
 }
 
-Address Code::metadata_end() const {
+Address InstructionStream::metadata_end() const {
   return metadata_start() + metadata_size();
 }
 
-int Code::SizeIncludingMetadata() const {
+DEF_GETTER(InstructionStream, SizeIncludingMetadata, int) {
   int size = CodeSize();
-  size += relocation_info().Size();
+  size += relocation_info(cage_base).Size();
   if (kind() != CodeKind::BASELINE) {
-    size += deoptimization_data().Size();
+    size += deoptimization_data(cage_base).Size();
   }
   return size;
 }
 
-Address Code::safepoint_table_address() const {
+Address InstructionStream::safepoint_table_address() const {
   return metadata_start() + safepoint_table_offset();
 }
 
+int InstructionStream::safepoint_table_size() const {
+  DCHECK_GE(handler_table_offset() - safepoint_table_offset(), 0);
+  return handler_table_offset() - safepoint_table_offset();
+}
+
+bool InstructionStream::has_safepoint_table() const {
+  return safepoint_table_size() > 0;
+}
+
 Address Code::SafepointTableAddress() const {
-  return V8_LIKELY(has_instruction_stream()) ? safepoint_table_address()
-                                             : OffHeapSafepointTableAddress();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().safepoint_table_address()
+             : OffHeapSafepointTableAddress();
+}
+
+Address GcSafeCode::SafepointTableAddress() const {
+  Code unsafe_this = UnsafeCastToCode();
+  return V8_LIKELY(has_instruction_stream())
+             ? InstructionStream::unchecked_cast(
+                   unsafe_this.raw_instruction_stream(kRelaxedLoad))
+                   .safepoint_table_address()
+             : unsafe_this.OffHeapSafepointTableAddress();
 }
 
 int Code::safepoint_table_size() const {
-  return handler_table_offset() - safepoint_table_offset();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().safepoint_table_size()
+             : OffHeapSafepointTableSize();
 }
 
 bool Code::has_safepoint_table() const { return safepoint_table_size() > 0; }
 
-Address Code::handler_table_address() const {
+Address InstructionStream::handler_table_address() const {
   return metadata_start() + handler_table_offset();
 }
 
+int InstructionStream::handler_table_size() const {
+  DCHECK_GE(constant_pool_offset() - handler_table_offset(), 0);
+  return constant_pool_offset() - handler_table_offset();
+}
+
+bool InstructionStream::has_handler_table() const {
+  return handler_table_size() > 0;
+}
+
 Address Code::HandlerTableAddress() const {
-  return V8_LIKELY(has_instruction_stream()) ? handler_table_address()
-                                             : OffHeapHandlerTableAddress();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().handler_table_address()
+             : OffHeapHandlerTableAddress();
 }
 
 int Code::handler_table_size() const {
-  return constant_pool_offset() - handler_table_offset();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().handler_table_size()
+             : OffHeapHandlerTableSize();
 }
 
 bool Code::has_handler_table() const { return handler_table_size() > 0; }
 
-int Code::constant_pool_size() const {
-  if V8_UNLIKELY (!has_instruction_stream()) return OffHeapConstantPoolSize();
-
+int InstructionStream::constant_pool_size() const {
   const int size = code_comments_offset() - constant_pool_offset();
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     DCHECK_EQ(size, 0);
@@ -487,39 +566,52 @@ int Code::constant_pool_size() const {
   return size;
 }
 
+bool InstructionStream::has_constant_pool() const {
+  return constant_pool_size() > 0;
+}
+
+int Code::constant_pool_size() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().constant_pool_size()
+             : OffHeapConstantPoolSize();
+}
+
 bool Code::has_constant_pool() const { return constant_pool_size() > 0; }
 
-ByteArray Code::unchecked_relocation_info() const {
+ByteArray InstructionStream::unchecked_relocation_info() const {
+  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
   return ByteArray::unchecked_cast(
-      TaggedField<HeapObject, kRelocationInfoOffset>::load(*this));
+      TaggedField<HeapObject, kRelocationInfoOffset>::load(cage_base, *this));
 }
 
-FixedArray Code::unchecked_deoptimization_data() const {
-  return FixedArray::unchecked_cast(
-      TaggedField<HeapObject, kDeoptimizationDataOrInterpreterDataOffset>::load(
-          *this));
+byte* InstructionStream::relocation_start() const {
+  return unchecked_relocation_info().GetDataStartAddress();
 }
 
-Code InstructionStream::unchecked_code() const {
-  PtrComprCageBase cage_base = main_cage_base(kRelaxedLoad);
-  return Code::unchecked_cast(
-      TaggedField<HeapObject, kCodeOffset>::Acquire_Load(cage_base, *this));
+byte* InstructionStream::relocation_end() const {
+  return unchecked_relocation_info().GetDataEndAddress();
+}
+
+int InstructionStream::relocation_size() const {
+  return unchecked_relocation_info().length();
 }
 
 byte* Code::relocation_start() const {
   return V8_LIKELY(has_instruction_stream())
-             ? relocation_info().GetDataStartAddress()
+             ? instruction_stream().relocation_start()
              : nullptr;
 }
 
 byte* Code::relocation_end() const {
   return V8_LIKELY(has_instruction_stream())
-             ? relocation_info().GetDataEndAddress()
+             ? instruction_stream().relocation_end()
              : nullptr;
 }
 
 int Code::relocation_size() const {
-  return V8_LIKELY(has_instruction_stream()) ? relocation_info().length() : 0;
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().relocation_size()
+             : 0;
 }
 
 Address InstructionStream::entry() const { return instruction_start(); }
@@ -536,42 +628,40 @@ bool Code::contains(Isolate* isolate, Address inner_pointer) {
 }
 
 // static
-void Code::CopyRelocInfoToByteArray(ByteArray dest, const CodeDesc& desc) {
+void InstructionStream::CopyRelocInfoToByteArray(ByteArray dest,
+                                                 const CodeDesc& desc) {
   DCHECK_EQ(dest.length(), desc.reloc_size);
   CopyBytes(dest.GetDataStartAddress(),
             desc.buffer + desc.buffer_size - desc.reloc_size,
             static_cast<size_t>(desc.reloc_size));
 }
 
-int InstructionStream::CodeSize() const {
-  return SizeFor(Code::unchecked_cast(raw_code(kAcquireLoad)).body_size());
-}
-int Code::CodeSize() const { return InstructionStream::SizeFor(body_size()); }
+int InstructionStream::CodeSize() const { return SizeFor(body_size()); }
 
 DEF_GETTER(InstructionStream, Size, int) { return CodeSize(); }
 
-CodeKind Code::kind() const {
+CodeKind InstructionStream::kind() const {
   static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return KindField::decode(flags);
 }
 
-int Code::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                         BytecodeArray bytecodes) {
+int InstructionStream::GetBytecodeOffsetForBaselinePC(Address baseline_pc,
+                                                      BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK(!is_baseline_trampoline_builtin());
   if (is_baseline_leave_frame_builtin()) return kFunctionExitBytecodeOffset;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
       ByteArray::cast(bytecode_offset_table()), bytecodes);
-  Address pc = baseline_pc - InstructionStart();
+  Address pc = baseline_pc - instruction_start();
   offset_iterator.AdvanceToPCOffset(pc);
   return offset_iterator.current_bytecode_offset();
 }
 
-uintptr_t Code::GetBaselinePCForBytecodeOffset(int bytecode_offset,
-                                               BytecodeToPCPosition position,
-                                               BytecodeArray bytecodes) {
+uintptr_t InstructionStream::GetBaselinePCForBytecodeOffset(
+    int bytecode_offset, BytecodeToPCPosition position,
+    BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -587,20 +677,20 @@ uintptr_t Code::GetBaselinePCForBytecodeOffset(int bytecode_offset,
   return pc;
 }
 
-uintptr_t Code::GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
-                                                    BytecodeArray bytecodes) {
+uintptr_t InstructionStream::GetBaselineStartPCForBytecodeOffset(
+    int bytecode_offset, BytecodeArray bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtStartOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t Code::GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
-                                                  BytecodeArray bytecodes) {
+uintptr_t InstructionStream::GetBaselineEndPCForBytecodeOffset(
+    int bytecode_offset, BytecodeArray bytecodes) {
   return GetBaselinePCForBytecodeOffset(bytecode_offset, kPcAtEndOfBytecode,
                                         bytecodes);
 }
 
-uintptr_t Code::GetBaselinePCForNextExecutedBytecode(int bytecode_offset,
-                                                     BytecodeArray bytecodes) {
+uintptr_t InstructionStream::GetBaselinePCForNextExecutedBytecode(
+    int bytecode_offset, BytecodeArray bytecodes) {
   DisallowGarbageCollection no_gc;
   CHECK_EQ(kind(), CodeKind::BASELINE);
   baseline::BytecodeOffsetIterator offset_iterator(
@@ -621,6 +711,31 @@ uintptr_t Code::GetBaselinePCForNextExecutedBytecode(int bytecode_offset,
   }
 }
 
+void InstructionStream::initialize_flags(CodeKind kind, bool is_turbofanned,
+                                         int stack_slots) {
+  CHECK(0 <= stack_slots && stack_slots < StackSlotsField::kMax);
+  DCHECK(!CodeKindIsInterpretedJSFunction(kind));
+  uint32_t flags = KindField::encode(kind) |
+                   IsTurbofannedField::encode(is_turbofanned) |
+                   StackSlotsField::encode(stack_slots);
+  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
+  RELAXED_WRITE_UINT32_FIELD(*this, kFlagsOffset, flags);
+  DCHECK_IMPLIES(stack_slots != 0, uses_safepoint_table());
+  DCHECK_IMPLIES(!uses_safepoint_table(), stack_slots == 0);
+}
+
+inline bool InstructionStream::is_interpreter_trampoline_builtin() const {
+  return IsInterpreterTrampolineBuiltin(builtin_id());
+}
+
+inline bool InstructionStream::is_baseline_trampoline_builtin() const {
+  return IsBaselineTrampolineBuiltin(builtin_id());
+}
+
+inline bool InstructionStream::is_baseline_leave_frame_builtin() const {
+  return builtin_id() == Builtin::kBaselineLeaveFrame;
+}
+
 inline bool Code::checks_tiering_state() const {
   bool checks_state = (builtin_id() == Builtin::kCompileLazy ||
                        builtin_id() == Builtin::kInterpreterEntryTrampoline ||
@@ -634,6 +749,15 @@ inline constexpr bool CodeKindHasTaggedOutgoingParams(CodeKind kind) {
          kind != CodeKind::C_WASM_ENTRY && kind != CodeKind::WASM_FUNCTION;
 }
 
+inline bool InstructionStream::has_tagged_outgoing_params() const {
+#if V8_ENABLE_WEBASSEMBLY
+  return CodeKindHasTaggedOutgoingParams(kind()) &&
+         builtin_id() != Builtin::kWasmCompileLazy;
+#else
+  return CodeKindHasTaggedOutgoingParams(kind());
+#endif
+}
+
 inline bool Code::has_tagged_outgoing_params() const {
 #if V8_ENABLE_WEBASSEMBLY
   return CodeKindHasTaggedOutgoingParams(kind()) &&
@@ -643,39 +767,67 @@ inline bool Code::has_tagged_outgoing_params() const {
 #endif
 }
 
-inline bool Code::is_turbofanned() const {
+inline bool InstructionStream::is_turbofanned() const {
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return IsTurbofannedField::decode(flags);
 }
 
+inline bool Code::is_turbofanned() const {
+  return IsTurbofannedField::decode(flags(kRelaxedLoad));
+}
+
+bool InstructionStream::is_maglevved() const {
+  return kind() == CodeKind::MAGLEV;
+}
+
 inline bool Code::is_maglevved() const { return kind() == CodeKind::MAGLEV; }
 
 inline bool Code::can_have_weak_objects() const {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int16_t flags = kind_specific_flags(kRelaxedLoad);
-  return CanHaveWeakObjectsField::decode(flags);
+  int32_t flags = kind_specific_flags(kRelaxedLoad);
+  return InstructionStream::CanHaveWeakObjectsField::decode(flags);
 }
 
 inline void Code::set_can_have_weak_objects(bool value) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int16_t previous = kind_specific_flags(kRelaxedLoad);
-  int16_t updated = CanHaveWeakObjectsField::update(previous, value);
+  int32_t previous = kind_specific_flags(kRelaxedLoad);
+  int32_t updated =
+      InstructionStream::CanHaveWeakObjectsField::update(previous, value);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
+inline bool InstructionStream::can_have_weak_objects() const {
+  DCHECK(CodeKindIsOptimizedJSFunction(kind()));
+  Code container = code(kAcquireLoad);
+  return container.can_have_weak_objects();
+}
+
+inline void InstructionStream::set_can_have_weak_objects(bool value) {
+  DCHECK(CodeKindIsOptimizedJSFunction(kind()));
+  Code container = code(kAcquireLoad);
+  container.set_can_have_weak_objects(value);
+}
+
 inline bool Code::is_promise_rejection() const {
   DCHECK_EQ(kind(), CodeKind::BUILTIN);
-  int16_t flags = kind_specific_flags(kRelaxedLoad);
-  return IsPromiseRejectionField::decode(flags);
+  int32_t flags = kind_specific_flags(kRelaxedLoad);
+  return InstructionStream::IsPromiseRejectionField::decode(flags);
 }
 
 inline void Code::set_is_promise_rejection(bool value) {
   DCHECK_EQ(kind(), CodeKind::BUILTIN);
-  int16_t previous = kind_specific_flags(kRelaxedLoad);
-  int16_t updated = IsPromiseRejectionField::update(previous, value);
+  int32_t previous = kind_specific_flags(kRelaxedLoad);
+  int32_t updated =
+      InstructionStream::IsPromiseRejectionField::update(previous, value);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
+inline bool InstructionStream::is_promise_rejection() const {
+  DCHECK_EQ(kind(), CodeKind::BUILTIN);
+  Code container = code(kAcquireLoad);
+  return container.is_promise_rejection();
+}
+
 inline HandlerTable::CatchPrediction
 InstructionStream::GetBuiltinCatchPrediction() const {
   if (is_promise_rejection()) return HandlerTable::PROMISE;
@@ -687,67 +839,122 @@ inline HandlerTable::CatchPrediction Code::GetBuiltinCatchPrediction() const {
   return HandlerTable::UNCAUGHT;
 }
 
-unsigned Code::inlined_bytecode_size() const {
+Builtin InstructionStream::builtin_id() const {
+  int index = RELAXED_READ_INT_FIELD(*this, kBuiltinIndexOffset);
+  DCHECK(index == static_cast<int>(Builtin::kNoBuiltinId) ||
+         Builtins::IsBuiltinId(index));
+  return static_cast<Builtin>(index);
+}
+
+void InstructionStream::set_builtin_id(Builtin builtin) {
+  DCHECK(builtin == Builtin::kNoBuiltinId || Builtins::IsBuiltinId(builtin));
+  RELAXED_WRITE_INT_FIELD(*this, kBuiltinIndexOffset,
+                          static_cast<int>(builtin));
+}
+
+bool InstructionStream::is_builtin() const {
+  return builtin_id() != Builtin::kNoBuiltinId;
+}
+
+unsigned InstructionStream::inlined_bytecode_size() const {
   unsigned size = RELAXED_READ_UINT_FIELD(*this, kInlinedBytecodeSizeOffset);
   DCHECK(CodeKindIsOptimizedJSFunction(kind()) || size == 0);
   return size;
 }
 
-void Code::set_inlined_bytecode_size(unsigned size) {
+void InstructionStream::set_inlined_bytecode_size(unsigned size) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()) || size == 0);
   RELAXED_WRITE_UINT_FIELD(*this, kInlinedBytecodeSizeOffset, size);
 }
 
-BytecodeOffset Code::osr_offset() const {
+BytecodeOffset InstructionStream::osr_offset() const {
   return BytecodeOffset(RELAXED_READ_INT32_FIELD(*this, kOsrOffsetOffset));
 }
 
-void Code::set_osr_offset(BytecodeOffset offset) {
+void InstructionStream::set_osr_offset(BytecodeOffset offset) {
   RELAXED_WRITE_INT32_FIELD(*this, kOsrOffsetOffset, offset.ToInt());
 }
 
+bool InstructionStream::uses_safepoint_table() const {
+  return is_turbofanned() || is_maglevved() || is_wasm_code();
+}
+
 bool Code::uses_safepoint_table() const {
   return is_turbofanned() || is_maglevved() || is_wasm_code();
 }
 
-int Code::stack_slots() const {
+int InstructionStream::stack_slots() const {
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   const int slots = StackSlotsField::decode(flags);
   DCHECK_IMPLIES(!uses_safepoint_table(), slots == 0);
   return slots;
 }
 
+int Code::stack_slots() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().stack_slots()
+             : OffHeapStackSlots();
+}
+
+int GcSafeCode::stack_slots() const {
+  Code unsafe_this = UnsafeCastToCode();
+  return V8_LIKELY(has_instruction_stream())
+             ? InstructionStream::unchecked_cast(
+                   unsafe_this.raw_instruction_stream(kRelaxedLoad))
+                   .stack_slots()
+             : unsafe_this.OffHeapStackSlots();
+}
+
 bool Code::marked_for_deoptimization() const {
   DCHECK(CodeKindCanDeoptimize(kind()));
-  int16_t flags = kind_specific_flags(kRelaxedLoad);
-  return MarkedForDeoptimizationField::decode(flags);
+  int32_t flags = kind_specific_flags(kRelaxedLoad);
+  return InstructionStream::MarkedForDeoptimizationField::decode(flags);
+}
+
+bool InstructionStream::marked_for_deoptimization() const {
+  DCHECK(CodeKindCanDeoptimize(kind()));
+  return code(kAcquireLoad).marked_for_deoptimization();
 }
 
 void Code::set_marked_for_deoptimization(bool flag) {
   DCHECK(CodeKindCanDeoptimize(kind()));
   DCHECK_IMPLIES(flag, AllowDeoptimization::IsAllowed(GetIsolate()));
-  int16_t previous = kind_specific_flags(kRelaxedLoad);
-  int16_t updated = MarkedForDeoptimizationField::update(previous, flag);
+  int32_t previous = kind_specific_flags(kRelaxedLoad);
+  int32_t updated =
+      InstructionStream::MarkedForDeoptimizationField::update(previous, flag);
   set_kind_specific_flags(updated, kRelaxedStore);
 }
 
-bool Code::embedded_objects_cleared() const {
+void InstructionStream::set_marked_for_deoptimization(bool flag) {
+  code(kAcquireLoad).set_marked_for_deoptimization(flag);
+}
+
+bool InstructionStream::embedded_objects_cleared() const {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
-  int16_t flags = kind_specific_flags(kRelaxedLoad);
-  return Code::EmbeddedObjectsClearedField::decode(flags);
+  int32_t flags = code(kAcquireLoad).kind_specific_flags(kRelaxedLoad);
+  return EmbeddedObjectsClearedField::decode(flags);
 }
 
-void Code::set_embedded_objects_cleared(bool flag) {
+void InstructionStream::set_embedded_objects_cleared(bool flag) {
   DCHECK(CodeKindIsOptimizedJSFunction(kind()));
   DCHECK_IMPLIES(flag, marked_for_deoptimization());
-  int16_t previous = kind_specific_flags(kRelaxedLoad);
-  int16_t updated = Code::EmbeddedObjectsClearedField::update(previous, flag);
-  set_kind_specific_flags(updated, kRelaxedStore);
+  Code container = code(kAcquireLoad);
+  int32_t previous = container.kind_specific_flags(kRelaxedLoad);
+  int32_t updated = EmbeddedObjectsClearedField::update(previous, flag);
+  container.set_kind_specific_flags(updated, kRelaxedStore);
+}
+
+bool InstructionStream::is_optimized_code() const {
+  return CodeKindIsOptimizedJSFunction(kind());
+}
+
+bool InstructionStream::is_wasm_code() const {
+  return kind() == CodeKind::WASM_FUNCTION;
 }
 
 bool Code::is_wasm_code() const { return kind() == CodeKind::WASM_FUNCTION; }
 
-int Code::constant_pool_offset() const {
+int InstructionStream::constant_pool_offset() const {
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     // Redirection needed since the field doesn't exist in this case.
     return code_comments_offset();
@@ -755,7 +962,7 @@ int Code::constant_pool_offset() const {
   return ReadField<int>(kConstantPoolOffsetOffset);
 }
 
-void Code::set_constant_pool_offset(int value) {
+void InstructionStream::set_constant_pool_offset(int value) {
   if (!V8_EMBEDDED_CONSTANT_POOL_BOOL) {
     // Redirection needed since the field doesn't exist in this case.
     return;
@@ -764,31 +971,78 @@ void Code::set_constant_pool_offset(int value) {
   WriteField<int>(kConstantPoolOffsetOffset, value);
 }
 
+Address InstructionStream::constant_pool() const {
+  if (!has_constant_pool()) return kNullAddress;
+  return metadata_start() + constant_pool_offset();
+}
+
 Address Code::constant_pool() const {
   if (!has_constant_pool()) return kNullAddress;
-  return V8_LIKELY(has_instruction_stream()) ? constant_pool()
-                                             : OffHeapConstantPoolAddress();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().constant_pool()
+             : OffHeapConstantPoolAddress();
+}
+
+Address InstructionStream::code_comments() const {
+  return metadata_start() + code_comments_offset();
+}
+
+int InstructionStream::code_comments_size() const {
+  DCHECK_GE(unwinding_info_offset() - code_comments_offset(), 0);
+  return unwinding_info_offset() - code_comments_offset();
+}
+
+bool InstructionStream::has_code_comments() const {
+  return code_comments_size() > 0;
 }
 
 Address Code::code_comments() const {
   return V8_LIKELY(has_instruction_stream())
-             ? metadata_start() + code_comments_offset()
+             ? instruction_stream().code_comments()
              : OffHeapCodeCommentsAddress();
 }
 
 int Code::code_comments_size() const {
-  return unwinding_info_offset() - code_comments_offset();
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().code_comments_size()
+             : OffHeapCodeCommentsSize();
 }
 
 bool Code::has_code_comments() const { return code_comments_size() > 0; }
 
-Address Code::unwinding_info_start() const {
+Address InstructionStream::unwinding_info_start() const {
   return metadata_start() + unwinding_info_offset();
 }
 
-Address Code::unwinding_info_end() const { return metadata_end(); }
+Address InstructionStream::unwinding_info_end() const { return metadata_end(); }
+
+int InstructionStream::unwinding_info_size() const {
+  DCHECK_GE(unwinding_info_end(), unwinding_info_start());
+  return static_cast<int>(unwinding_info_end() - unwinding_info_start());
+}
+
+bool InstructionStream::has_unwinding_info() const {
+  return unwinding_info_size() > 0;
+}
+
+Address Code::unwinding_info_start() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().unwinding_info_start()
+             : OffHeapUnwindingInfoAddress();
+}
+
+Address Code::unwinding_info_end() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().metadata_end()
+             : OffHeapMetadataEnd();
+}
 
 int Code::unwinding_info_size() const {
+  return V8_LIKELY(has_instruction_stream())
+             ? instruction_stream().unwinding_info_size()
+             : OffHeapUnwindingInfoSize();
+
+  DCHECK_GE(unwinding_info_end(), unwinding_info_start());
   return static_cast<int>(unwinding_info_end() - unwinding_info_start());
 }
 
@@ -812,11 +1066,6 @@ InstructionStream InstructionStream::FromTargetAddress(Address address) {
   return InstructionStream::unchecked_cast(code);
 }
 
-// static
-Code Code::FromTargetAddress(Address address) {
-  return InstructionStream::FromTargetAddress(address).code(kAcquireLoad);
-}
-
 // static
 InstructionStream InstructionStream::FromEntryAddress(
     Address location_of_address) {
@@ -828,15 +1077,15 @@ InstructionStream InstructionStream::FromEntryAddress(
   return InstructionStream::unchecked_cast(code);
 }
 
-bool Code::CanContainWeakObjects() {
+bool InstructionStream::CanContainWeakObjects() {
   return is_optimized_code() && can_have_weak_objects();
 }
 
-bool Code::IsWeakObject(HeapObject object) {
+bool InstructionStream::IsWeakObject(HeapObject object) {
   return (CanContainWeakObjects() && IsWeakObjectInOptimizedCode(object));
 }
 
-bool Code::IsWeakObjectInOptimizedCode(HeapObject object) {
+bool InstructionStream::IsWeakObjectInOptimizedCode(HeapObject object) {
   Map map_object = object.map(kAcquireLoad);
   if (InstanceTypeChecker::IsMap(map_object)) {
     return Map::cast(object).CanTransition();
@@ -846,16 +1095,18 @@ bool Code::IsWeakObjectInOptimizedCode(HeapObject object) {
          InstanceTypeChecker::IsContext(map_object);
 }
 
-bool Code::IsWeakObjectInDeoptimizationLiteralArray(Object object) {
+bool InstructionStream::IsWeakObjectInDeoptimizationLiteralArray(
+    Object object) {
   // Maps must be strong because they can be used as part of the description for
   // how to materialize an object upon deoptimization, in which case it is
   // possible to reach the code that requires the Map without anything else
   // holding a strong pointer to that Map.
   return object.IsHeapObject() && !object.IsMap() &&
-         Code::IsWeakObjectInOptimizedCode(HeapObject::cast(object));
+         InstructionStream::IsWeakObjectInOptimizedCode(
+             HeapObject::cast(object));
 }
 
-void Code::IterateDeoptimizationLiterals(RootVisitor* v) {
+void InstructionStream::IterateDeoptimizationLiterals(RootVisitor* v) {
   if (kind() == CodeKind::BASELINE) return;
 
   auto deopt_data = DeoptimizationData::cast(deoptimization_data());
@@ -875,8 +1126,8 @@ void Code::IterateDeoptimizationLiterals(RootVisitor* v) {
 
 // This field has to have relaxed atomic accessors because it is accessed in the
 // concurrent marker.
-static_assert(FIELD_SIZE(Code::kKindSpecificFlagsOffset) == kInt16Size);
-RELAXED_UINT16_ACCESSORS(Code, kind_specific_flags, kKindSpecificFlagsOffset)
+static_assert(FIELD_SIZE(Code::kKindSpecificFlagsOffset) == kInt32Size);
+RELAXED_INT32_ACCESSORS(Code, kind_specific_flags, kKindSpecificFlagsOffset)
 
 Object Code::raw_instruction_stream() const {
   PtrComprCageBase cage_base = code_cage_base();
@@ -972,6 +1223,8 @@ void Code::UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
 
 Address Code::InstructionStart() const { return code_entry_point(); }
 
+Address Code::body_size() const { return instruction_stream().body_size(); }
+
 void Code::clear_padding() {
   memset(reinterpret_cast<void*>(address() + kUnalignedSize), 0,
          kSize - kUnalignedSize);
@@ -986,21 +1239,16 @@ static_assert(static_cast<int>(Builtin::kNoBuiltinId) == -1);
 static_assert(Builtins::kBuiltinCount < std::numeric_limits<int16_t>::max());
 
 void Code::initialize_flags(CodeKind kind, Builtin builtin_id,
-                            bool is_turbofanned, int stack_slots) {
-  CHECK(0 <= stack_slots && stack_slots < StackSlotsField::kMax);
-  DCHECK(!CodeKindIsInterpretedJSFunction(kind));
-  uint32_t value = KindField::encode(kind) |
-                   IsTurbofannedField::encode(is_turbofanned) |
-                   StackSlotsField::encode(stack_slots);
-  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
-  RELAXED_WRITE_UINT32_FIELD(*this, kFlagsOffset, value);
+                            bool is_turbofanned) {
+  uint16_t value =
+      KindField::encode(kind) | IsTurbofannedField::encode(is_turbofanned);
   set_flags(value, kRelaxedStore);
-  DCHECK_IMPLIES(stack_slots != 0, uses_safepoint_table());
-  DCHECK_IMPLIES(!uses_safepoint_table(), stack_slots == 0);
 
   WriteField<int16_t>(kBuiltinIdOffset, static_cast<int16_t>(builtin_id));
 }
 
+CodeKind Code::kind() const { return KindField::decode(flags(kRelaxedLoad)); }
+
 Builtin Code::builtin_id() const {
   // Rely on sign-extension when converting int16_t to int to preserve
   // kNoBuiltinId value.
@@ -1028,6 +1276,32 @@ inline bool Code::is_baseline_leave_frame_builtin() const {
   return builtin_id() == Builtin::kBaselineLeaveFrame;
 }
 
+//
+// A collection of getters and predicates that forward queries to associated
+// InstructionStream object.
+//
+
+#define DEF_PRIMITIVE_FORWARDING_CODE_GETTER(name, type) \
+  type Code::name() const { return FromCode(*this).name(); }
+
+#define DEF_FORWARDING_CODE_GETTER(name, type,                      \
+                                   result_if_no_instruction_stream) \
+  DEF_GETTER(Code, name, type) {                                    \
+    if (!has_instruction_stream()) {                                \
+      return GetReadOnlyRoots().result_if_no_instruction_stream();  \
+    }                                                               \
+    return FromCode(*this).name(cage_base);                         \
+  }
+
+DEF_FORWARDING_CODE_GETTER(deoptimization_data, FixedArray, empty_fixed_array)
+DEF_FORWARDING_CODE_GETTER(bytecode_or_interpreter_data, HeapObject,
+                           empty_fixed_array)
+DEF_FORWARDING_CODE_GETTER(source_position_table, ByteArray, empty_byte_array)
+DEF_FORWARDING_CODE_GETTER(bytecode_offset_table, ByteArray, empty_byte_array)
+
+#undef DEF_PRIMITIVE_FORWARDING_CODE_GETTER
+#undef DEF_FORWARDING_CODE_GETTER
+
 byte BytecodeArray::get(int index) const {
   DCHECK(index >= 0 && index < this->length());
   return ReadField<byte>(kHeaderSize + index * kCharSize);
@@ -1240,7 +1514,7 @@ inline Object DeoptimizationLiteralArray::get(PtrComprCageBase cage_base,
 
 inline void DeoptimizationLiteralArray::set(int index, Object value) {
   MaybeObject maybe = MaybeObject::FromObject(value);
-  if (Code::IsWeakObjectInDeoptimizationLiteralArray(value)) {
+  if (InstructionStream::IsWeakObjectInDeoptimizationLiteralArray(value)) {
     maybe = MaybeObject::MakeWeak(maybe);
   }
   Set(index, maybe);
@@ -1257,11 +1531,11 @@ void DependentCode::DeoptimizeDependencyGroups(Isolate* isolate, ObjectT object,
 
 // static
 template <typename ObjectT>
-bool DependentCode::MarkCodeForDeoptimization(Isolate* isolate, ObjectT object,
+bool DependentCode::MarkCodeForDeoptimization(ObjectT object,
                                               DependencyGroups groups) {
   // Shared objects are designed to never invalidate code.
   DCHECK(!object.InSharedHeap());
-  return object.dependent_code().MarkCodeForDeoptimization(isolate, groups);
+  return object.dependent_code().MarkCodeForDeoptimization(groups);
 }
 
 }  // namespace internal
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 1ca28beb309..8bc05ceebaf 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -172,7 +172,7 @@ int Code::OffHeapStackSlots() const {
   return d.StackSlotsOf(builtin);
 }
 
-void Code::ClearEmbeddedObjects(Heap* heap) {
+void InstructionStream::ClearEmbeddedObjects(Heap* heap) {
   HeapObject undefined = ReadOnlyRoots(heap).undefined_value();
   int mode_mask = RelocInfo::EmbeddedObjectModeMask();
   for (RelocIterator it(*this, mode_mask); !it.done(); it.next()) {
@@ -183,30 +183,24 @@ void Code::ClearEmbeddedObjects(Heap* heap) {
 }
 
 void InstructionStream::Relocate(intptr_t delta) {
-  Code code = unchecked_code();
-  // This is called during evacuation and code.instruction_stream() will point
-  // to the old object. So pass *this directly to the RelocIterator and use a
-  // dummy Code() since it's not needed.
-  for (RelocIterator it(Code(), *this, code.unchecked_relocation_info(),
-                        code.constant_pool(), RelocInfo::kApplyMask);
-       !it.done(); it.next()) {
+  for (RelocIterator it(*this, RelocInfo::kApplyMask); !it.done(); it.next()) {
     it.rinfo()->apply(delta);
   }
-  FlushInstructionCache(instruction_start(), code.instruction_size());
+  FlushICache();
 }
 
-void Code::FlushICache() const {
-  FlushInstructionCache(InstructionStart(), instruction_size());
+void InstructionStream::FlushICache() const {
+  FlushInstructionCache(instruction_start(), instruction_size());
 }
 
-void Code::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
-                           const CodeDesc& desc) {
+void InstructionStream::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
+                                        const CodeDesc& desc) {
   // Copy code.
-  static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-  CopyBytes(reinterpret_cast<byte*>(InstructionStart()), desc.buffer,
+  static_assert(kOnHeapBodyIsContiguous);
+  CopyBytes(reinterpret_cast<byte*>(instruction_start()), desc.buffer,
             static_cast<size_t>(desc.instr_size));
   // TODO(jgruber,v8:11036): Merge with the above.
-  CopyBytes(reinterpret_cast<byte*>(InstructionStart() + desc.instr_size),
+  CopyBytes(reinterpret_cast<byte*>(instruction_start() + desc.instr_size),
             desc.unwinding_info, static_cast<size_t>(desc.unwinding_info_size));
 
   // Copy reloc info.
@@ -216,8 +210,8 @@ void Code::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
   RelocateFromDesc(reloc_info, heap, desc);
 }
 
-void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
-                            const CodeDesc& desc) {
+void InstructionStream::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
+                                         const CodeDesc& desc) {
   // Unbox handles and relocate.
   Assembler* origin = desc.origin;
   const int mode_mask = RelocInfo::PostCodegenRelocationMask();
@@ -259,7 +253,7 @@ void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
 #endif
     } else {
       intptr_t delta =
-          InstructionStart() - reinterpret_cast<Address>(desc.buffer);
+          instruction_start() - reinterpret_cast<Address>(desc.buffer);
       it.rinfo()->apply(delta);
     }
   }
@@ -336,7 +330,22 @@ int AbstractCode::SourceStatementPosition(PtrComprCageBase cage_base,
   return statement_position;
 }
 
-bool Code::IsIsolateIndependent(Isolate* isolate) {
+bool InstructionStream::CanDeoptAt(Isolate* isolate, Address pc) {
+  DeoptimizationData deopt_data =
+      DeoptimizationData::cast(deoptimization_data());
+  Address code_start_address = instruction_start();
+  for (int i = 0; i < deopt_data.DeoptCount(); i++) {
+    if (deopt_data.Pc(i).value() == -1) continue;
+    Address address = code_start_address + deopt_data.Pc(i).value();
+    if (address == pc &&
+        deopt_data.GetBytecodeOffset(i) != BytecodeOffset::None()) {
+      return true;
+    }
+  }
+  return false;
+}
+
+bool InstructionStream::IsIsolateIndependent(Isolate* isolate) {
   static constexpr int kModeMask =
       RelocInfo::AllRealModesMask() &
       ~RelocInfo::ModeMask(RelocInfo::CONST_POOL) &
@@ -371,8 +380,10 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
       if (OffHeapInstructionStream::PcIsOffHeap(isolate, target_address))
         continue;
 
-      Code target = Code::FromTargetAddress(target_address);
-      if (Builtins::IsIsolateIndependentBuiltin(target)) {
+      InstructionStream target =
+          InstructionStream::FromTargetAddress(target_address);
+      CHECK(target.IsInstructionStream());
+      if (Builtins::IsIsolateIndependentBuiltin(target.code(kAcquireLoad))) {
         continue;
       }
     }
@@ -384,7 +395,7 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
 #endif
 }
 
-bool Code::Inlines(SharedFunctionInfo sfi) {
+bool InstructionStream::Inlines(SharedFunctionInfo sfi) {
   // We can only check for inlining for optimized code.
   DCHECK(is_optimized_code());
   DisallowGarbageCollection no_gc;
@@ -400,7 +411,8 @@ bool Code::Inlines(SharedFunctionInfo sfi) {
   return false;
 }
 
-Code::OptimizedCodeIterator::OptimizedCodeIterator(Isolate* isolate)
+InstructionStream::OptimizedCodeIterator::OptimizedCodeIterator(
+    Isolate* isolate)
     : isolate_(isolate),
       safepoint_scope_(std::make_unique<SafepointScope>(
           isolate, isolate->is_shared_space_isolate()
@@ -410,7 +422,7 @@ Code::OptimizedCodeIterator::OptimizedCodeIterator(Isolate* isolate)
           isolate->heap()->code_space()->GetObjectIterator(isolate->heap())),
       state_(kIteratingCodeSpace) {}
 
-Code Code::OptimizedCodeIterator::Next() {
+InstructionStream InstructionStream::OptimizedCodeIterator::Next() {
   while (true) {
     HeapObject object = object_iterator_->Next();
     if (object.is_null()) {
@@ -434,11 +446,10 @@ Code Code::OptimizedCodeIterator::Next() {
           state_ = kDone;
           V8_FALLTHROUGH;
         case kDone:
-          return Code();
+          return InstructionStream();
       }
     }
-    InstructionStream istream = InstructionStream::cast(object);
-    Code code = istream.code(kAcquireLoad);
+    InstructionStream code = InstructionStream::cast(object);
     if (!CodeKindCanDeoptimize(code.kind())) continue;
     return code;
   }
@@ -637,8 +648,10 @@ void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
   }
 
   os << "RelocInfo (size = " << code.relocation_size() << ")\n";
+  // TODO(jgruber): Update this once relocations are based on Code, not
+  // InstructionStream objects.
   if (code.has_instruction_stream()) {
-    for (RelocIterator it(code); !it.done(); it.next()) {
+    for (RelocIterator it(code.instruction_stream()); !it.done(); it.next()) {
       it.rinfo()->Print(isolate, os);
     }
   }
@@ -938,7 +951,7 @@ void DependentCode::IterateAndCompact(const IterateAndCompactFn& fn) {
 }
 
 bool DependentCode::MarkCodeForDeoptimization(
-    Isolate* isolate, DependentCode::DependencyGroups deopt_groups) {
+    DependentCode::DependencyGroups deopt_groups) {
   DisallowGarbageCollection no_gc;
 
   bool marked_something = false;
@@ -946,7 +959,7 @@ bool DependentCode::MarkCodeForDeoptimization(
     if ((groups & deopt_groups) == 0) return false;
 
     if (!code.marked_for_deoptimization()) {
-      code.SetMarkedForDeoptimization(isolate, "code dependencies");
+      code.SetMarkedForDeoptimization("code dependencies");
       marked_something = true;
     }
 
@@ -974,7 +987,7 @@ int DependentCode::FillEntryFromBack(int index, int length) {
 void DependentCode::DeoptimizeDependencyGroups(
     Isolate* isolate, DependentCode::DependencyGroups groups) {
   DisallowGarbageCollection no_gc_scope;
-  bool marked_something = MarkCodeForDeoptimization(isolate, groups);
+  bool marked_something = MarkCodeForDeoptimization(groups);
   if (marked_something) {
     DCHECK(AllowCodeDependencyChange::IsAllowed());
     Deoptimizer::DeoptimizeMarkedCode(isolate);
@@ -986,9 +999,14 @@ DependentCode DependentCode::empty_dependent_code(const ReadOnlyRoots& roots) {
   return DependentCode::cast(roots.empty_weak_array_list());
 }
 
-void Code::SetMarkedForDeoptimization(Isolate* isolate, const char* reason) {
+void InstructionStream::SetMarkedForDeoptimization(const char* reason) {
+  set_marked_for_deoptimization(true);
+  Deoptimizer::TraceMarkForDeoptimization(*this, reason);
+}
+
+void Code::SetMarkedForDeoptimization(const char* reason) {
   set_marked_for_deoptimization(true);
-  Deoptimizer::TraceMarkForDeoptimization(isolate, *this, reason);
+  Deoptimizer::TraceMarkForDeoptimization(FromCode(*this), reason);
 }
 
 const char* DependentCode::DependencyGroupName(DependencyGroup group) {
diff --git a/src/objects/code.h b/src/objects/code.h
index e929bbecd9f..b3444294460 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -118,22 +118,18 @@ class Code : public HeapObject {
   inline void UpdateCodeEntryPoint(Isolate* isolate_for_sandbox,
                                    InstructionStream code);
 
-  DECL_RELAXED_UINT16_ACCESSORS(kind_specific_flags)
+  DECL_RELAXED_INT32_ACCESSORS(kind_specific_flags)
 
   // Initializes internal flags field which stores cached values of some
   // properties of the respective InstructionStream object.
   inline void initialize_flags(CodeKind kind, Builtin builtin_id,
-                               bool is_turbofanned, int stack_slots);
+                               bool is_turbofanned);
+
+  inline Address body_size() const;
 
   // Clear uninitialized padding space. This ensures that the snapshot content
   // is deterministic.
   inline void clear_padding();
-  // Clear the padding in the InstructionStream
-  inline void ClearInstructionStreamPadding();
-
-  // Flushes the instruction cache for the executable instructions of this code
-  // object. Make sure to call this while the code is still writable.
-  void FlushICache() const;
 
   DECL_PRIMITIVE_ACCESSORS(can_have_weak_objects, bool)
   DECL_PRIMITIVE_ACCESSORS(marked_for_deoptimization, bool)
@@ -146,38 +142,6 @@ class Code : public HeapObject {
 
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
-  DECL_PRIMITIVE_ACCESSORS(instruction_size, int)
-  DECL_PRIMITIVE_ACCESSORS(metadata_size, int)
-  // [handler_table_offset]: The offset where the exception handler table
-  // starts.
-  DECL_PRIMITIVE_ACCESSORS(handler_table_offset, int)
-  // [unwinding_info_offset]: Offset of the unwinding info section.
-  DECL_PRIMITIVE_ACCESSORS(unwinding_info_offset, int32_t)
-  // [deoptimization_data]: Array containing data for deopt for non-baseline
-  // code.
-  DECL_ACCESSORS(deoptimization_data, FixedArray)
-  // [bytecode_or_interpreter_data]: BytecodeArray or InterpreterData for
-  // baseline code.
-  DECL_ACCESSORS(bytecode_or_interpreter_data, HeapObject)
-  // [source_position_table]: ByteArray for the source positions table for
-  // non-baseline code.
-  DECL_ACCESSORS(source_position_table, ByteArray)
-  // [bytecode_offset_table]: ByteArray for the bytecode offset for baseline
-  // code.
-  DECL_ACCESSORS(bytecode_offset_table, ByteArray)
-  // [relocation_info]: InstructionStream relocation information
-  DECL_ACCESSORS(relocation_info, ByteArray)
-  DECL_PRIMITIVE_ACCESSORS(inlined_bytecode_size, unsigned)
-  DECL_PRIMITIVE_ACCESSORS(osr_offset, BytecodeOffset)
-  // [code_comments_offset]: Offset of the code comment section.
-  DECL_PRIMITIVE_ACCESSORS(code_comments_offset, int)
-  // [constant_pool offset]: Offset of the constant pool.
-  DECL_PRIMITIVE_ACCESSORS(constant_pool_offset, int)
-
-  // Unchecked accessors to be used during GC.
-  inline ByteArray unchecked_relocation_info() const;
-  inline FixedArray unchecked_deoptimization_data() const;
-
   inline CodeKind kind() const;
   inline Builtin builtin_id() const;
   inline bool is_builtin() const;
@@ -213,6 +177,11 @@ class Code : public HeapObject {
   // reserved in the code prologue; otherwise 0.
   inline int stack_slots() const;
 
+  DECL_GETTER(deoptimization_data, FixedArray)
+  DECL_GETTER(bytecode_or_interpreter_data, HeapObject)
+  DECL_GETTER(source_position_table, ByteArray)
+  DECL_GETTER(bytecode_offset_table, ByteArray)
+
   inline ByteArray SourcePositionTable(Isolate* isolate,
                                        SharedFunctionInfo sfi) const;
 
@@ -244,23 +213,6 @@ class Code : public HeapObject {
   inline byte* relocation_end() const;
   inline int relocation_size() const;
 
-  // [safepoint_table_offset]: The offset where the safepoint table starts.
-  inline int safepoint_table_offset() const { return 0; }
-
-  inline Address body_start() const;
-  inline Address body_end() const;
-  inline int body_size() const;
-
-  inline Address metadata_start() const;
-  inline Address metadata_end() const;
-
-  inline Address handler_table_address() const;
-
-  inline Address safepoint_table_address() const;
-
-  inline int CodeSize() const;
-  inline int SizeIncludingMetadata() const;
-
   // When builtins un-embedding is enabled for the Isolate
   // (see Isolate::is_short_builtin_calls_enabled()) then both embedded and
   // un-embedded builtins might be exeuted and thus two kinds of |pc|s might
@@ -291,60 +243,7 @@ class Code : public HeapObject {
 
   inline int GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const;
 
-  void SetMarkedForDeoptimization(Isolate* isolate, const char* reason);
-
-  inline bool CanContainWeakObjects();
-
-  inline bool IsWeakObject(HeapObject object);
-
-  static inline bool IsWeakObjectInOptimizedCode(HeapObject object);
-
-  static inline bool IsWeakObjectInDeoptimizationLiteralArray(Object object);
-
-  // This function should be called only from GC.
-  void ClearEmbeddedObjects(Heap* heap);
-
-  // [embedded_objects_cleared]: If CodeKindIsOptimizedJSFunction(kind), tells
-  // whether the embedded objects in the code marked for deoptimization were
-  // cleared. Note that embedded_objects_cleared() implies
-  // marked_for_deoptimization().
-  inline bool embedded_objects_cleared() const;
-  inline void set_embedded_objects_cleared(bool flag);
-
-  // Migrate code from desc without flushing the instruction cache.
-  void CopyFromNoFlush(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
-  void RelocateFromDesc(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
-
-  // Copy the RelocInfo portion of |desc| to |dest|. The ByteArray must be
-  // exactly the same size as the RelocInfo in |desc|.
-  static inline void CopyRelocInfoToByteArray(ByteArray dest,
-                                              const CodeDesc& desc);
-
-  bool IsIsolateIndependent(Isolate* isolate);
-
-  inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
-                                                       BytecodeArray bytecodes);
-
-  inline uintptr_t GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
-                                                     BytecodeArray bytecodes);
-
-  // Returns true if the function is inlined in the code.
-  bool Inlines(SharedFunctionInfo sfi);
-
-  // Returns the PC of the next bytecode in execution order.
-  // If the bytecode at the given offset is JumpLoop, the PC of the jump target
-  // is returned. Other jumps are not allowed.
-  // For other bytecodes this is equivalent to
-  // GetBaselineEndPCForBytecodeOffset.
-  inline uintptr_t GetBaselinePCForNextExecutedBytecode(
-      int bytecode_offset, BytecodeArray bytecodes);
-
-  inline int GetBytecodeOffsetForBaselinePC(Address baseline_pc,
-                                            BytecodeArray bytecodes);
-
-  inline void IterateDeoptimizationLiterals(RootVisitor* v);
-
-  static inline Code FromTargetAddress(Address address);
+  void SetMarkedForDeoptimization(const char* reason);
 
 #ifdef ENABLE_DISASSEMBLER
   V8_EXPORT_PRIVATE void Disassemble(const char* name, std::ostream& os,
@@ -359,35 +258,19 @@ class Code : public HeapObject {
   DECL_VERIFIER(Code)
 
 // Layout description.
-#define CODE_DATA_FIELDS(V)                                                   \
-  /* Strong pointer fields. */                                                \
-  V(kRelocationInfoOffset, kTaggedSize)                                       \
-  V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
-  V(kPositionTableOffset, kTaggedSize)                                        \
-  V(kPointerFieldsStrongEndOffset, 0)                                         \
-  /* Strong InstructionStream pointer fields. */                              \
-  V(kInstructionStreamOffset, kTaggedSize)                                    \
-  V(kCodePointerFieldsStrongEndOffset, 0)                                     \
-  /* Raw data fields. */                                                      \
-  /* Data or code not directly visited by GC directly starts here. */         \
-  V(kDataStart, 0)                                                            \
-  V(kCodeEntryPointOffset, kSystemPointerSize)                                \
-  /* The serializer needs to copy bytes starting from here verbatim. */       \
-  /* Objects embedded into code is visited via reloc info. */                 \
-  V(kFlagsOffset, kInt32Size)                                                 \
-  V(kBuiltinIdOffset, kInt16Size)                                             \
-  V(kKindSpecificFlagsOffset, kInt16Size)                                     \
-  V(kInstructionSizeOffset, kIntSize)                                         \
-  V(kMetadataSizeOffset, kIntSize)                                            \
-  V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
-  V(kOsrOffsetOffset, kInt32Size)                                             \
-  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
-  V(kHandlerTableOffsetOffset, kIntSize)                                      \
-  V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
-  V(kConstantPoolOffsetOffset, V8_EMBEDDED_CONSTANT_POOL_BOOL ? kIntSize : 0) \
-  V(kCodeCommentsOffsetOffset, kIntSize)                                      \
-  V(kUnalignedSize, OBJECT_POINTER_PADDING(kUnalignedSize))                   \
-  /* Total size. */                                                           \
+#define CODE_DATA_FIELDS(V)                                 \
+  /* Strong pointer fields. */                              \
+  V(kPointerFieldsStrongEndOffset, 0)                       \
+  /* Strong InstructionStream pointer fields. */            \
+  V(kInstructionStreamOffset, kTaggedSize)                  \
+  V(kCodePointerFieldsStrongEndOffset, 0)                   \
+  /* Raw data fields. */                                    \
+  V(kCodeEntryPointOffset, kSystemPointerSize)              \
+  V(kFlagsOffset, kUInt16Size)                              \
+  V(kBuiltinIdOffset, kInt16Size)                           \
+  V(kKindSpecificFlagsOffset, kInt32Size)                   \
+  V(kUnalignedSize, OBJECT_POINTER_PADDING(kUnalignedSize)) \
+  /* Total size. */                                         \
   V(kSize, 0)
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, CODE_DATA_FIELDS)
@@ -405,38 +288,16 @@ class Code : public HeapObject {
   class BodyDescriptor;
 
   // Flags layout.
-#define FLAGS_BIT_FIELDS(V, _)      \
-  V(KindField, CodeKind, 4, _)      \
-  V(IsTurbofannedField, bool, 1, _) \
-  V(StackSlotsField, int, 24, _)
-  /* The other 3 bits are still free. */
-  // TODO(v8:13784): merge this with KindSpecificFlags by dropping the
-  // IsPromiseRejection field or taking one bit from the StackSlots field.
+#define FLAGS_BIT_FIELDS(V, _) \
+  V(KindField, CodeKind, 4, _) \
+  V(IsTurbofannedField, bool, 1, _)
+  /* The other 10 bits are still free. */
 
   DEFINE_BIT_FIELDS(FLAGS_BIT_FIELDS)
 #undef FLAGS_BIT_FIELDS
-  static_assert(kCodeKindCount <= KindField::kNumValues);
-  static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 29);
+  static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 5);
   static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
-                FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
-
-  // KindSpecificFlags layout.
-#define KIND_SPECIFIC_FLAGS_BIT_FIELDS(V, _)  \
-  V(MarkedForDeoptimizationField, bool, 1, _) \
-  V(EmbeddedObjectsClearedField, bool, 1, _)  \
-  V(CanHaveWeakObjectsField, bool, 1, _)      \
-  V(IsPromiseRejectionField, bool, 1, _)
-  DEFINE_BIT_FIELDS(KIND_SPECIFIC_FLAGS_BIT_FIELDS)
-#undef CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS
-  static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
-  static_assert(KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
-                FIELD_SIZE(Code::kKindSpecificFlagsOffset) * kBitsPerByte);
-
-  // The {marked_for_deoptimization} field is accessed from generated code.
-  static const int kMarkedForDeoptimizationBit =
-      MarkedForDeoptimizationField::kShift;
-
-  class OptimizedCodeIterator;
+                FIELD_SIZE(Code::kFlagsOffset) * kBitsPerByte);
 
  private:
   inline void init_code_entry_point(Isolate* isolate, Address initial_value);
@@ -463,17 +324,6 @@ class Code : public HeapObject {
   V8_EXPORT_PRIVATE int OffHeapUnwindingInfoSize() const;
   V8_EXPORT_PRIVATE int OffHeapStackSlots() const;
 
-  enum BytecodeToPCPosition {
-    kPcAtStartOfBytecode,
-    // End of bytecode equals the start of the next bytecode.
-    // We need it when we deoptimize to the next bytecode (lazy deopt or deopt
-    // of non-topmost frame).
-    kPcAtEndOfBytecode
-  };
-  inline uintptr_t GetBaselinePCForBytecodeOffset(int bytecode_offset,
-                                                  BytecodeToPCPosition position,
-                                                  BytecodeArray bytecodes);
-
   template <typename IsolateT>
   friend class Deserializer;
   friend class GcSafeCode;  // For OffHeapFoo functions.
@@ -526,16 +376,14 @@ class GcSafeCode : public HeapObject {
   inline bool has_tagged_outgoing_params() const;
   inline bool marked_for_deoptimization() const;
   inline Object raw_instruction_stream() const;
-  inline Address constant_pool() const;
-  inline int stack_slots() const;
 
   inline int GetOffsetFromInstructionStart(Isolate* isolate, Address pc) const;
   inline Address InstructionStart(Isolate* isolate, Address pc) const;
   inline Address InstructionEnd(Isolate* isolate, Address pc) const;
-  inline bool CanDeoptAt(Isolate* isolate, Address pc) const;
 
   // Accessors that had to be modified to be used in GC settings.
   inline Address SafepointTableAddress() const;
+  inline int stack_slots() const;
 
  private:
   OBJECT_CONSTRUCTORS(GcSafeCode, HeapObject);
@@ -547,6 +395,10 @@ class InstructionStream : public HeapObject {
  public:
   NEVER_READ_ONLY_SPACE
 
+  // Opaque data type for encapsulating code flags like kind, inline cache
+  // state, and arguments count.
+  using Flags = uint32_t;
+
   // All InstructionStream objects have the following layout:
   //
   //  +--------------------------+
@@ -578,18 +430,153 @@ class InstructionStream : public HeapObject {
   static constexpr bool kBodyIsContiguous =
       kOnHeapBodyIsContiguous && kOffHeapBodyIsContiguous;
 
+  inline Address body_start() const;
+  inline Address body_end() const;
+  inline int body_size() const;
+
   inline Address instruction_start() const;
+  inline Address instruction_end() const;
+
+  inline int instruction_size() const;
+  inline void set_instruction_size(int value);
+
+  inline Address metadata_start() const;
+  inline Address metadata_end() const;
+  inline int metadata_size() const;
+  inline void set_metadata_size(int value);
 
   // The metadata section is aligned to this value.
   static constexpr int kMetadataAlignment = kIntSize;
 
+  // [safepoint_table_offset]: The offset where the safepoint table starts.
+  inline int safepoint_table_offset() const { return 0; }
+  inline Address safepoint_table_address() const;
+  inline int safepoint_table_size() const;
+  inline bool has_safepoint_table() const;
+
+  // [handler_table_offset]: The offset where the exception handler table
+  // starts.
+  inline int handler_table_offset() const;
+  inline void set_handler_table_offset(int offset);
+  inline Address handler_table_address() const;
+  inline int handler_table_size() const;
+  inline bool has_handler_table() const;
+
+  // [constant_pool offset]: Offset of the constant pool.
+  inline int constant_pool_offset() const;
+  inline void set_constant_pool_offset(int offset);
+  inline Address constant_pool() const;
+  inline int constant_pool_size() const;
+  inline bool has_constant_pool() const;
+
+  // [code_comments_offset]: Offset of the code comment section.
+  inline int code_comments_offset() const;
+  inline void set_code_comments_offset(int offset);
+  inline Address code_comments() const;
+  inline int code_comments_size() const;
+  inline bool has_code_comments() const;
+
+  // [unwinding_info_offset]: Offset of the unwinding info section.
+  inline int32_t unwinding_info_offset() const;
+  inline void set_unwinding_info_offset(int32_t offset);
+  inline Address unwinding_info_start() const;
+  inline Address unwinding_info_end() const;
+  inline int unwinding_info_size() const;
+  inline bool has_unwinding_info() const;
+
+  // [relocation_info]: InstructionStream relocation information
+  DECL_ACCESSORS(relocation_info, ByteArray)
+
+  // This function should be called only from GC.
+  void ClearEmbeddedObjects(Heap* heap);
+
+  // [deoptimization_data]: Array containing data for deopt for non-baseline
+  // code.
+  DECL_ACCESSORS(deoptimization_data, FixedArray)
+  // [bytecode_or_interpreter_data]: BytecodeArray or InterpreterData for
+  // baseline code.
+  DECL_ACCESSORS(bytecode_or_interpreter_data, HeapObject)
+
+  // [source_position_table]: ByteArray for the source positions table for
+  // non-baseline code.
+  DECL_ACCESSORS(source_position_table, ByteArray)
+  // [bytecode_offset_table]: ByteArray for the bytecode offset for baseline
+  // code.
+  DECL_ACCESSORS(bytecode_offset_table, ByteArray)
+
+  // If source positions have not been collected or an exception has been thrown
+  // this will return empty_byte_array.
+  inline ByteArray SourcePositionTable(Isolate* isolate,
+                                       SharedFunctionInfo sfi) const;
+
   // [code]: The associated Code object.
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
   DECL_RELEASE_ACQUIRE_ACCESSORS(raw_code, HeapObject)
 
-  // A convenience wrapper around raw_code that will do an unchecked cast for
-  // you.
-  inline Code unchecked_code() const;
+  // Unchecked accessors to be used during GC.
+  inline ByteArray unchecked_relocation_info() const;
+
+  inline int relocation_size() const;
+
+  // [kind]: Access to specific code kind.
+  inline CodeKind kind() const;
+
+  inline bool is_optimized_code() const;
+  inline bool is_wasm_code() const;
+
+  inline bool is_interpreter_trampoline_builtin() const;
+  inline bool is_baseline_trampoline_builtin() const;
+  inline bool is_baseline_leave_frame_builtin() const;
+
+  // Tells whether the outgoing parameters of this code are tagged pointers.
+  inline bool has_tagged_outgoing_params() const;
+
+  // [is_turbofanned]: Tells whether the code object was generated by the
+  // TurboFan optimizing compiler.
+  inline bool is_turbofanned() const;
+
+  // TODO(jgruber): Reconsider these predicates; we should probably merge them
+  // and rename to something appropriate.
+  inline bool is_maglevved() const;
+
+  // [can_have_weak_objects]: If CodeKindIsOptimizedJSFunction(kind), tells
+  // whether the embedded objects in code should be treated weakly.
+  inline bool can_have_weak_objects() const;
+  inline void set_can_have_weak_objects(bool value);
+
+  // [builtin]: For builtins, tells which builtin index the code object
+  // has. The builtin index is a non-negative integer for builtins, and
+  // Builtin::kNoBuiltinId (-1) otherwise.
+  inline Builtin builtin_id() const;
+  inline void set_builtin_id(Builtin builtin);
+  inline bool is_builtin() const;
+
+  inline unsigned inlined_bytecode_size() const;
+  inline void set_inlined_bytecode_size(unsigned size);
+
+  inline BytecodeOffset osr_offset() const;
+  inline void set_osr_offset(BytecodeOffset offset);
+
+  // [uses_safepoint_table]: Whether this InstructionStream object uses
+  // safepoint tables (note the table may still be empty, see
+  // has_safepoint_table).
+  inline bool uses_safepoint_table() const;
+
+  // [stack_slots]: If {uses_safepoint_table()}, the number of stack slots
+  // reserved in the code prologue; otherwise 0.
+  inline int stack_slots() const;
+
+  // [marked_for_deoptimization]: If CodeKindCanDeoptimize(kind), tells whether
+  // the code is going to be deoptimized.
+  inline bool marked_for_deoptimization() const;
+  inline void set_marked_for_deoptimization(bool flag);
+
+  // [embedded_objects_cleared]: If CodeKindIsOptimizedJSFunction(kind), tells
+  // whether the embedded objects in the code marked for deoptimization were
+  // cleared. Note that embedded_objects_cleared() implies
+  // marked_for_deoptimization().
+  inline bool embedded_objects_cleared() const;
+  inline void set_embedded_objects_cleared(bool flag);
 
   // The entire code object including its header is copied verbatim to the
   // snapshot so that it can be written in one, fast, memcpy during
@@ -609,9 +596,27 @@ class InstructionStream : public HeapObject {
   inline PtrComprCageBase main_cage_base(RelaxedLoadTag) const;
   inline void set_main_cage_base(Address cage_base, RelaxedStoreTag);
 
+  // Clear uninitialized padding space. This ensures that the snapshot content
+  // is deterministic. Depending on the V8 build mode there could be no padding.
+  inline void clear_padding();
+  // Initialize the flags field. Similar to clear_padding above this ensure that
+  // the snapshot content is deterministic.
+  inline void initialize_flags(CodeKind kind, bool is_turbofanned,
+                               int stack_slots);
+
   static inline InstructionStream FromTargetAddress(Address address);
   static inline InstructionStream FromEntryAddress(Address location_of_address);
 
+  // Returns the size of code and its metadata. This includes the size of code
+  // relocation information, deoptimization data.
+  DECL_GETTER(SizeIncludingMetadata, int)
+
+  // Returns the address of the first relocation info (read backwards!).
+  inline byte* relocation_start() const;
+
+  // Returns the address right after the relocation info (read backwards!).
+  inline byte* relocation_end() const;
+
   // InstructionStream entry point.
   inline Address entry() const;
 
@@ -622,6 +627,36 @@ class InstructionStream : public HeapObject {
   // object has been moved by delta bytes.
   void Relocate(intptr_t delta);
 
+  // Migrate code from desc without flushing the instruction cache.
+  void CopyFromNoFlush(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
+  void RelocateFromDesc(ByteArray reloc_info, Heap* heap, const CodeDesc& desc);
+
+  // Copy the RelocInfo portion of |desc| to |dest|. The ByteArray must be
+  // exactly the same size as the RelocInfo in |desc|.
+  static inline void CopyRelocInfoToByteArray(ByteArray dest,
+                                              const CodeDesc& desc);
+
+  inline uintptr_t GetBaselineStartPCForBytecodeOffset(int bytecode_offset,
+                                                       BytecodeArray bytecodes);
+
+  inline uintptr_t GetBaselineEndPCForBytecodeOffset(int bytecode_offset,
+                                                     BytecodeArray bytecodes);
+
+  // Returns the PC of the next bytecode in execution order.
+  // If the bytecode at the given offset is JumpLoop, the PC of the jump target
+  // is returned. Other jumps are not allowed.
+  // For other bytecodes this is equivalent to
+  // GetBaselineEndPCForBytecodeOffset.
+  inline uintptr_t GetBaselinePCForNextExecutedBytecode(
+      int bytecode_offset, BytecodeArray bytecodes);
+
+  inline int GetBytecodeOffsetForBaselinePC(Address baseline_pc,
+                                            BytecodeArray bytecodes);
+
+  // Flushes the instruction cache for the executable instructions of this code
+  // object. Make sure to call this while the code is still writable.
+  void FlushICache() const;
+
   // Returns the object size for a given body (used for allocation).
   static int SizeFor(int body_size) {
     return RoundUp(kHeaderSize + body_size, kCodeAlignment);
@@ -638,19 +673,56 @@ class InstructionStream : public HeapObject {
   DECL_PRINTER(InstructionStream)
   DECL_VERIFIER(InstructionStream)
 
+  bool CanDeoptAt(Isolate* isolate, Address pc);
+
+  void SetMarkedForDeoptimization(const char* reason);
+
   inline HandlerTable::CatchPrediction GetBuiltinCatchPrediction() const;
 
+  bool IsIsolateIndependent(Isolate* isolate);
+
+  inline bool CanContainWeakObjects();
+
+  inline bool IsWeakObject(HeapObject object);
+
+  static inline bool IsWeakObjectInOptimizedCode(HeapObject object);
+
+  static inline bool IsWeakObjectInDeoptimizationLiteralArray(Object object);
+
+  inline void IterateDeoptimizationLiterals(RootVisitor* v);
+
+  // Returns true if the function is inlined in the code.
+  bool Inlines(SharedFunctionInfo sfi);
+
+  class OptimizedCodeIterator;
+
   // Layout description.
-#define ISTREAM_FIELDS(V)                                                 \
-  V(kCodeOffset, kTaggedSize)                                             \
-  /* Data or code not directly visited by GC directly starts here. */     \
-  V(kDataStart, 0)                                                        \
-  V(kMainCageBaseUpper32BitsOffset,                                       \
-    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                        \
-  V(kUnalignedHeaderSize, 0)                                              \
-  /* Add padding to align the instruction start following right after */  \
-  /* the InstructionStream object header. */                              \
-  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset)) \
+#define ISTREAM_FIELDS(V)                                                     \
+  V(kRelocationInfoOffset, kTaggedSize)                                       \
+  V(kDeoptimizationDataOrInterpreterDataOffset, kTaggedSize)                  \
+  V(kPositionTableOffset, kTaggedSize)                                        \
+  V(kCodeOffset, kTaggedSize)                                                 \
+  /* Data or code not directly visited by GC directly starts here. */         \
+  /* The serializer needs to copy bytes starting from here verbatim. */       \
+  /* Objects embedded into code is visited via reloc info. */                 \
+  V(kDataStart, 0)                                                            \
+  V(kMainCageBaseUpper32BitsOffset,                                           \
+    V8_EXTERNAL_CODE_SPACE_BOOL ? kTaggedSize : 0)                            \
+  V(kInstructionSizeOffset, kIntSize)                                         \
+  V(kMetadataSizeOffset, kIntSize)                                            \
+  V(kFlagsOffset, kInt32Size)                                                 \
+  V(kBuiltinIndexOffset, kIntSize)                                            \
+  V(kInlinedBytecodeSizeOffset, kIntSize)                                     \
+  V(kOsrOffsetOffset, kInt32Size)                                             \
+  /* Offsets describing inline metadata tables, relative to MetadataStart. */ \
+  V(kHandlerTableOffsetOffset, kIntSize)                                      \
+  V(kConstantPoolOffsetOffset, V8_EMBEDDED_CONSTANT_POOL_BOOL ? kIntSize : 0) \
+  V(kCodeCommentsOffsetOffset, kIntSize)                                      \
+  V(kUnwindingInfoOffsetOffset, kInt32Size)                                   \
+  V(kUnalignedHeaderSize, 0)                                                  \
+  /* Add padding to align the instruction start following right after */      \
+  /* the InstructionStream object header. */                                  \
+  V(kOptionalPaddingOffset, CODE_POINTER_PADDING(kOptionalPaddingOffset))     \
   V(kHeaderSize, 0)
 
   DEFINE_FIELD_OFFSET_CONSTANTS(HeapObject::kHeaderSize, ISTREAM_FIELDS)
@@ -660,30 +732,30 @@ class InstructionStream : public HeapObject {
   // object header due to padding for code alignment.
 #if V8_TARGET_ARCH_ARM64
   static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 20 : (COMPRESS_POINTERS_BOOL ? 24 : 20);
+      V8_EXTERNAL_CODE_SPACE_BOOL ? 4 : (COMPRESS_POINTERS_BOOL ? 8 : 20);
 #elif V8_TARGET_ARCH_MIPS64
   static constexpr int kHeaderPaddingSize = 20;
 #elif V8_TARGET_ARCH_LOONG64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 20);
+  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 8 : 20);
 #elif V8_TARGET_ARCH_X64
   static constexpr int kHeaderPaddingSize =
-      V8_EXTERNAL_CODE_SPACE_BOOL ? 52 : (COMPRESS_POINTERS_BOOL ? 56 : 48);
+      V8_EXTERNAL_CODE_SPACE_BOOL ? 4 : (COMPRESS_POINTERS_BOOL ? 8 : 52);
 #elif V8_TARGET_ARCH_ARM
-  static constexpr int kHeaderPaddingSize = 24;
+  static constexpr int kHeaderPaddingSize = 8;
 #elif V8_TARGET_ARCH_IA32
-  static constexpr int kHeaderPaddingSize = 24;
+  static constexpr int kHeaderPaddingSize = 8;
 #elif V8_TARGET_ARCH_MIPS
-  static constexpr int kHeaderPaddingSize = 24;
+  static constexpr int kHeaderPaddingSize = 8;
 #elif V8_TARGET_ARCH_PPC64
   static constexpr int kHeaderPaddingSize =
       V8_EMBEDDED_CONSTANT_POOL_BOOL ? (COMPRESS_POINTERS_BOOL ? 4 : 48)
                                      : (COMPRESS_POINTERS_BOOL ? 8 : 52);
 #elif V8_TARGET_ARCH_S390X
-  static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 24 : 20;
+  static constexpr int kHeaderPaddingSize = COMPRESS_POINTERS_BOOL ? 8 : 20;
 #elif V8_TARGET_ARCH_RISCV64
-  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 24 : 20);
+  static constexpr int kHeaderPaddingSize = (COMPRESS_POINTERS_BOOL ? 8 : 20);
 #elif V8_TARGET_ARCH_RISCV32
-  static constexpr int kHeaderPaddingSize = 24;
+  static constexpr int kHeaderPaddingSize = 8;
 #else
 #error Unknown architecture.
 #endif
@@ -691,6 +763,34 @@ class InstructionStream : public HeapObject {
 
   class BodyDescriptor;
 
+  // Flags layout.  base::BitField<type, shift, size>.
+#define ISTREAM_FLAGS_BIT_FIELDS(V, _) \
+  V(KindField, CodeKind, 4, _)         \
+  V(IsTurbofannedField, bool, 1, _)    \
+  V(StackSlotsField, int, 24, _)
+  DEFINE_BIT_FIELDS(ISTREAM_FLAGS_BIT_FIELDS)
+#undef ISTREAM_FLAGS_BIT_FIELDS
+  static_assert(kCodeKindCount <= KindField::kNumValues);
+  static_assert(ISTREAM_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 29);
+  static_assert(ISTREAM_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
+                FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
+
+  // KindSpecificFlags layout.
+#define ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS(V, _) \
+  V(MarkedForDeoptimizationField, bool, 1, _)        \
+  V(EmbeddedObjectsClearedField, bool, 1, _)         \
+  V(CanHaveWeakObjectsField, bool, 1, _)             \
+  V(IsPromiseRejectionField, bool, 1, _)
+  DEFINE_BIT_FIELDS(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS)
+#undef ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS
+  static_assert(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
+  static_assert(ISTREAM_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
+                FIELD_SIZE(Code::kKindSpecificFlagsOffset) * kBitsPerByte);
+
+  // The {marked_for_deoptimization} field is accessed from generated code.
+  static const int kMarkedForDeoptimizationBit =
+      MarkedForDeoptimizationField::kShift;
+
   static const int kArgumentsBits = 16;
   // Reserve one argument count value as the "don't adapt arguments" sentinel.
   static const int kMaxArguments = (1 << kArgumentsBits) - 2;
@@ -703,15 +803,26 @@ class InstructionStream : public HeapObject {
 
   bool is_promise_rejection() const;
 
+  enum BytecodeToPCPosition {
+    kPcAtStartOfBytecode,
+    // End of bytecode equals the start of the next bytecode.
+    // We need it when we deoptimize to the next bytecode (lazy deopt or deopt
+    // of non-topmost frame).
+    kPcAtEndOfBytecode
+  };
+  inline uintptr_t GetBaselinePCForBytecodeOffset(int bytecode_offset,
+                                                  BytecodeToPCPosition position,
+                                                  BytecodeArray bytecodes);
+
   OBJECT_CONSTRUCTORS(InstructionStream, HeapObject);
 };
 
-class Code::OptimizedCodeIterator {
+class InstructionStream::OptimizedCodeIterator {
  public:
   explicit OptimizedCodeIterator(Isolate* isolate);
   OptimizedCodeIterator(const OptimizedCodeIterator&) = delete;
   OptimizedCodeIterator& operator=(const OptimizedCodeIterator&) = delete;
-  Code Next();
+  InstructionStream Next();
 
  private:
   Isolate* isolate_;
@@ -725,6 +836,7 @@ class Code::OptimizedCodeIterator {
 // Helper functions for converting InstructionStream objects to
 // Code and back.
 inline Code ToCode(InstructionStream code);
+inline Handle<Code> ToCode(Handle<InstructionStream> code, Isolate* isolate);
 inline InstructionStream FromCode(Code code);
 inline InstructionStream FromCode(Code code, Isolate* isolate, RelaxedLoadTag);
 inline InstructionStream FromCode(Code code, PtrComprCageBase, RelaxedLoadTag);
@@ -841,7 +953,7 @@ class DependentCode : public WeakArrayList {
                                          DependencyGroups groups);
 
   template <typename ObjectT>
-  static bool MarkCodeForDeoptimization(Isolate* isolate, ObjectT object,
+  static bool MarkCodeForDeoptimization(ObjectT object,
                                         DependencyGroups groups);
 
   V8_EXPORT_PRIVATE static DependentCode empty_dependent_code(
@@ -866,8 +978,7 @@ class DependentCode : public WeakArrayList {
                                               DependencyGroups groups,
                                               Handle<Code> code);
 
-  bool MarkCodeForDeoptimization(Isolate* isolate,
-                                 DependencyGroups deopt_groups);
+  bool MarkCodeForDeoptimization(DependencyGroups deopt_groups);
 
   void DeoptimizeDependencyGroups(Isolate* isolate, DependencyGroups groups);
 
diff --git a/src/objects/js-function-inl.h b/src/objects/js-function-inl.h
index 152c00d1b17..08c67974530 100644
--- a/src/objects/js-function-inl.h
+++ b/src/objects/js-function-inl.h
@@ -80,6 +80,11 @@ void JSFunction::set_code(Code value, ReleaseStoreTag, WriteBarrierMode mode) {
 }
 RELEASE_ACQUIRE_ACCESSORS(JSFunction, context, Context, kContextOffset)
 
+void JSFunction::set_code(InstructionStream code, ReleaseStoreTag,
+                          WriteBarrierMode mode) {
+  set_code(ToCode(code), kReleaseStore, mode);
+}
+
 Address JSFunction::code_entry_point() const {
   return Code::cast(code()).code_entry_point();
 }
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index da353d2a66e..e24882d1a85 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -120,6 +120,12 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // are fully initialized.
   DECL_ACCESSORS(code, Code)
   DECL_RELEASE_ACQUIRE_ACCESSORS(code, Code)
+  // Convenient overloads to avoid unnecessary InstructionStream <->
+  // Code conversions.
+  // TODO(v8:11880): remove once |code| accessors are migrated to
+  // Code.
+  inline void set_code(InstructionStream code, ReleaseStoreTag,
+                       WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
   // Returns the address of the function code's instruction start.
   inline Address code_entry_point() const;
diff --git a/src/objects/js-regexp.cc b/src/objects/js-regexp.cc
index 2fc3c9149db..715ed0db044 100644
--- a/src/objects/js-regexp.cc
+++ b/src/objects/js-regexp.cc
@@ -157,8 +157,8 @@ Object JSRegExp::code(bool is_latin1) const {
   return value;
 }
 
-void JSRegExp::set_code(bool is_latin1, Handle<Code> code) {
-  SetDataAt(code_index(is_latin1), *code);
+void JSRegExp::set_code(bool is_latin1, Handle<InstructionStream> code) {
+  SetDataAt(code_index(is_latin1), ToCode(*code));
 }
 
 Object JSRegExp::bytecode(bool is_latin1) const {
diff --git a/src/objects/js-regexp.h b/src/objects/js-regexp.h
index b1a8c1336a4..1a2eab8bf58 100644
--- a/src/objects/js-regexp.h
+++ b/src/objects/js-regexp.h
@@ -68,7 +68,8 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
   inline String atom_pattern() const;
   // This could be a Smi kUninitializedValue or InstructionStream.
   V8_EXPORT_PRIVATE Object code(bool is_latin1) const;
-  V8_EXPORT_PRIVATE void set_code(bool is_unicode, Handle<Code> code);
+  V8_EXPORT_PRIVATE void set_code(bool is_unicode,
+                                  Handle<InstructionStream> code);
   // This could be a Smi kUninitializedValue or ByteArray.
   V8_EXPORT_PRIVATE Object bytecode(bool is_latin1) const;
   // Sets the bytecode as well as initializing trampoline slots to the
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 769fe4272b8..886f62513ed 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -940,8 +940,11 @@ class CoverageInfo::BodyDescriptor final : public BodyDescriptorBase {
 
 class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
  public:
-  static_assert(static_cast<int>(HeapObject::kHeaderSize) ==
-                static_cast<int>(kCodeOffset));
+  static_assert(kRelocationInfoOffset + kTaggedSize ==
+                kDeoptimizationDataOrInterpreterDataOffset);
+  static_assert(kDeoptimizationDataOrInterpreterDataOffset + kTaggedSize ==
+                kPositionTableOffset);
+  static_assert(kPositionTableOffset + kTaggedSize == kCodeOffset);
   static_assert(kCodeOffset + kTaggedSize == kDataStart);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
@@ -963,12 +966,9 @@ class InstructionStream::BodyDescriptor final : public BodyDescriptorBase {
   template <typename ObjectVisitor>
   static inline void IterateBody(Map map, HeapObject obj, ObjectVisitor* v) {
     // GC does not visit data/code in the header and in the body directly.
-    IteratePointers(obj, kCodeOffset, kDataStart, v);
+    IteratePointers(obj, kRelocationInfoOffset, kDataStart, v);
 
-    InstructionStream istream = InstructionStream::cast(obj);
-    Code code = istream.unchecked_code();
-    RelocIterator it(code, istream, code.unchecked_relocation_info(),
-                     code.constant_pool(), kRelocModeMask);
+    RelocIterator it(InstructionStream::cast(obj), kRelocModeMask);
     v->VisitRelocInfo(&it);
   }
 
@@ -1060,11 +1060,11 @@ class Code::BodyDescriptor final : public BodyDescriptorBase {
   template <typename ObjectVisitor>
   static inline void IterateBody(Map map, HeapObject obj, int object_size,
                                  ObjectVisitor* v) {
-    IteratePointers(obj, HeapObject::kHeaderSize,
-                    Code::kPointerFieldsStrongEndOffset, v);
+    // No strong pointers to iterate.
+    static_assert(static_cast<int>(HeapObject::kHeaderSize) ==
+                  static_cast<int>(Code::kPointerFieldsStrongEndOffset));
 
-    v->VisitCodePointer(Code::cast(obj),
-                        obj.RawCodeField(kInstructionStreamOffset));
+    v->VisitCodePointer(obj, obj.RawCodeField(kInstructionStreamOffset));
   }
 
   static inline int SizeOf(Map map, HeapObject object) { return Code::kSize; }
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index 43ea4a73502..77279dc4b14 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -2036,8 +2036,7 @@ void HeapObject::HeapObjectShortPrint(std::ostream& os) {
       break;
     }
     case INSTRUCTION_STREAM_TYPE: {
-      InstructionStream istream = InstructionStream::cast(*this);
-      Code code = istream.code(kAcquireLoad);
+      InstructionStream code = InstructionStream::cast(*this);
       os << "<InstructionStream " << CodeKindToString(code.kind());
       if (code.is_builtin()) {
         os << " " << Builtins::name(code.builtin_id());
diff --git a/src/objects/visitors.h b/src/objects/visitors.h
index ab16adf31d8..651ee080d1e 100644
--- a/src/objects/visitors.h
+++ b/src/objects/visitors.h
@@ -136,7 +136,7 @@ class ObjectVisitor {
   // slot. The values may be modified on return. Not used when
   // V8_EXTERNAL_CODE_SPACE is not enabled (the InstructionStream pointer slots
   // are visited as a part of on-heap slot visitation - via VisitPointers()).
-  virtual void VisitCodePointer(Code host, CodeObjectSlot slot) = 0;
+  virtual void VisitCodePointer(HeapObject host, CodeObjectSlot slot) = 0;
 
   // Custom weak pointers must be ignored by the GC but not other
   // visitors. They're used for e.g., lists that are recreated after GC. The
@@ -164,23 +164,36 @@ class ObjectVisitor {
     VisitPointer(host, value);
   }
 
-  virtual void VisitCodeTarget(RelocInfo* rinfo) = 0;
+  // To allow lazy clearing of inline caches the visitor has
+  // a rich interface for iterating over InstructionStream objects ...
 
-  virtual void VisitEmbeddedPointer(RelocInfo* rinfo) = 0;
+  // Visits a code target in the instruction stream.
+  virtual void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) = 0;
 
-  virtual void VisitExternalReference(RelocInfo* rinfo) {}
+  // Visit pointer embedded into a code object.
+  virtual void VisitEmbeddedPointer(InstructionStream host,
+                                    RelocInfo* rinfo) = 0;
 
+  // Visits an external reference embedded into a code object.
+  virtual void VisitExternalReference(InstructionStream host,
+                                      RelocInfo* rinfo) {}
+
+  // Visits an external pointer.
   virtual void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                                     ExternalPointerTag tag) {}
 
-  virtual void VisitInternalReference(RelocInfo* rinfo) {}
+  // Visits an (encoded) internal reference.
+  virtual void VisitInternalReference(InstructionStream host,
+                                      RelocInfo* rinfo) {}
 
+  // Visits an off-heap target or near builtin entry in the instruction stream.
   // TODO(ishell): rename to VisitBuiltinEntry.
-  virtual void VisitOffHeapTarget(RelocInfo* rinfo) {}
+  virtual void VisitOffHeapTarget(InstructionStream host, RelocInfo* rinfo) {}
 
   // Visits the relocation info using the given iterator.
   void VisitRelocInfo(RelocIterator* it);
 
+  // Visits the object's map pointer, decoding as necessary
   virtual void VisitMapPointer(HeapObject host) { UNREACHABLE(); }
 };
 
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index efc9bfdcab9..82c35c5815c 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -188,8 +188,7 @@ class HeapEntryVerifier {
   // Objects that have been checked via a call to CheckStrongReference or
   // CheckWeakReference, or deliberately skipped via a call to
   // MarkReferenceCheckedWithoutChecking.
-  std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>
-      checked_objects_;
+  std::unordered_set<HeapObject, Object::Hasher> checked_objects_;
 
   // Objects transitively retained by the primary object. The objects in the set
   // at index i are retained by the primary object via a chain of i+1
@@ -1076,19 +1075,19 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
     }
   }
 
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override {
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override {
     VisitSlotImpl(code_cage_base(), slot);
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo) override {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo) override {
     InstructionStream target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     VisitHeapObjectImpl(target, -1);
   }
 
-  void VisitEmbeddedPointer(RelocInfo* rinfo) override {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo) override {
     HeapObject object = rinfo->target_object(cage_base());
-    if (rinfo->code().IsWeakObject(object)) {
+    if (host.IsWeakObject(object)) {
       generator_->SetWeakReference(parent_, next_index_++, object, {});
     } else {
       VisitHeapObjectImpl(object, -1);
@@ -1170,8 +1169,8 @@ void V8HeapExplorer::ExtractReferences(HeapEntry* entry, HeapObject obj) {
     ExtractAccessorInfoReferences(entry, AccessorInfo::cast(obj));
   } else if (obj.IsAccessorPair()) {
     ExtractAccessorPairReferences(entry, AccessorPair::cast(obj));
-  } else if (obj.IsCode()) {
-    ExtractCodeReferences(entry, Code::cast(obj));
+  } else if (obj.IsInstructionStream()) {
+    ExtractCodeReferences(entry, InstructionStream::cast(obj));
   } else if (obj.IsCell()) {
     ExtractCellReferences(entry, Cell::cast(obj));
   } else if (obj.IsFeedbackCell()) {
@@ -1581,29 +1580,29 @@ void V8HeapExplorer::TagBuiltinCodeObject(Code code, const char* name) {
   }
 }
 
-void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
-  if (!code.has_instruction_stream()) return;
-
+void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry,
+                                           InstructionStream code) {
   TagObject(code.relocation_info(), "(code relocation info)", HeapEntry::kCode);
   SetInternalReference(entry, "relocation_info", code.relocation_info(),
-                       Code::kRelocationInfoOffset);
+                       InstructionStream::kRelocationInfoOffset);
 
   if (code.kind() == CodeKind::BASELINE) {
     TagObject(code.bytecode_or_interpreter_data(), "(interpreter data)");
-    SetInternalReference(entry, "interpreter_data",
-                         code.bytecode_or_interpreter_data(),
-                         Code::kDeoptimizationDataOrInterpreterDataOffset);
+    SetInternalReference(
+        entry, "interpreter_data", code.bytecode_or_interpreter_data(),
+        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
     TagObject(code.bytecode_offset_table(), "(bytecode offset table)",
               HeapEntry::kCode);
     SetInternalReference(entry, "bytecode_offset_table",
                          code.bytecode_offset_table(),
-                         Code::kPositionTableOffset);
+                         InstructionStream::kPositionTableOffset);
   } else {
     DeoptimizationData deoptimization_data =
         DeoptimizationData::cast(code.deoptimization_data());
     TagObject(deoptimization_data, "(code deopt data)", HeapEntry::kCode);
-    SetInternalReference(entry, "deoptimization_data", deoptimization_data,
-                         Code::kDeoptimizationDataOrInterpreterDataOffset);
+    SetInternalReference(
+        entry, "deoptimization_data", deoptimization_data,
+        InstructionStream::kDeoptimizationDataOrInterpreterDataOffset);
     if (deoptimization_data.length() > 0) {
       TagObject(deoptimization_data.TranslationByteArray(), "(code deopt data)",
                 HeapEntry::kCode);
@@ -1616,7 +1615,7 @@ void V8HeapExplorer::ExtractCodeReferences(HeapEntry* entry, Code code) {
               HeapEntry::kCode);
     SetInternalReference(entry, "source_position_table",
                          code.source_position_table(),
-                         Code::kPositionTableOffset);
+                         InstructionStream::kPositionTableOffset);
   }
 }
 
@@ -2071,8 +2070,8 @@ class RootsReferencesExtractor : public RootVisitor {
                         FullObjectSlot istream_or_smi_zero_slot) final {
     Object istream_or_smi_zero = *istream_or_smi_zero_slot;
     if (istream_or_smi_zero != Smi::zero()) {
-      Code code = Code::cast(*code_slot);
-      code.IterateDeoptimizationLiterals(this);
+      InstructionStream istream = InstructionStream::cast(istream_or_smi_zero);
+      istream.IterateDeoptimizationLiterals(this);
       VisitRootPointer(Root::kStackRoots, nullptr, istream_or_smi_zero_slot);
     }
     VisitRootPointer(Root::kStackRoots, nullptr, code_slot);
diff --git a/src/profiler/heap-snapshot-generator.h b/src/profiler/heap-snapshot-generator.h
index 216833e2fd6..d8f6628f1d7 100644
--- a/src/profiler/heap-snapshot-generator.h
+++ b/src/profiler/heap-snapshot-generator.h
@@ -433,7 +433,7 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   void ExtractAccessorInfoReferences(HeapEntry* entry,
                                      AccessorInfo accessor_info);
   void ExtractAccessorPairReferences(HeapEntry* entry, AccessorPair accessors);
-  void ExtractCodeReferences(HeapEntry* entry, Code code);
+  void ExtractCodeReferences(HeapEntry* entry, InstructionStream code);
   void ExtractCellReferences(HeapEntry* entry, Cell cell);
   void ExtractJSWeakRefReferences(HeapEntry* entry, JSWeakRef js_weak_ref);
   void ExtractWeakCellReferences(HeapEntry* entry, WeakCell weak_cell);
diff --git a/src/profiler/profiler-listener.cc b/src/profiler/profiler-listener.cc
index 688bdc1027f..dd0631664a1 100644
--- a/src/profiler/profiler-listener.cc
+++ b/src/profiler/profiler-listener.cc
@@ -331,12 +331,13 @@ void ProfilerListener::CodeDisableOptEvent(Handle<AbstractCode> code,
   DispatchCodeEvent(evt_rec);
 }
 
-void ProfilerListener::CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind,
-                                      Address pc, int fp_to_sp_delta) {
+void ProfilerListener::CodeDeoptEvent(Handle<InstructionStream> code,
+                                      DeoptimizeKind kind, Address pc,
+                                      int fp_to_sp_delta) {
   CodeEventsContainer evt_rec(CodeEventRecord::Type::kCodeDeopt);
   CodeDeoptEventRecord* rec = &evt_rec.CodeDeoptEventRecord_;
   Deoptimizer::DeoptInfo info = Deoptimizer::GetDeoptInfo(*code, pc);
-  rec->instruction_start = code->InstructionStart();
+  rec->instruction_start = code->instruction_start();
   rec->deopt_reason = DeoptimizeReasonToString(info.deopt_reason);
   rec->deopt_id = info.deopt_id;
   rec->pc = pc;
@@ -385,7 +386,7 @@ const char* ProfilerListener::GetFunctionName(SharedFunctionInfo shared) {
   }
 }
 
-void ProfilerListener::AttachDeoptInlinedFrames(Handle<Code> code,
+void ProfilerListener::AttachDeoptInlinedFrames(Handle<InstructionStream> code,
                                                 CodeDeoptEventRecord* rec) {
   int deopt_id = rec->deopt_id;
   SourcePosition last_position = SourcePosition::Unknown();
@@ -415,7 +416,7 @@ void ProfilerListener::AttachDeoptInlinedFrames(Handle<Code> code,
       // scope limits their lifetime.
       HandleScope scope(isolate_);
       std::vector<SourcePositionInfo> stack =
-          last_position.InliningStack(isolate_, *code);
+          last_position.InliningStack(isolate_, code->code(kAcquireLoad));
       CpuProfileDeoptFrame* deopt_frames =
           new CpuProfileDeoptFrame[stack.size()];
 
diff --git a/src/profiler/profiler-listener.h b/src/profiler/profiler-listener.h
index eec7b08bfa8..3517ab56b5f 100644
--- a/src/profiler/profiler-listener.h
+++ b/src/profiler/profiler-listener.h
@@ -63,9 +63,9 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
   void CodeMovingGCEvent() override {}
   void CodeDisableOptEvent(Handle<AbstractCode> code,
                            Handle<SharedFunctionInfo> shared) override;
-  void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                      int fp_to_sp_delta) override;
-  void CodeDependencyChangeEvent(Handle<Code> code,
+  void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                      Address pc, int fp_to_sp_delta) override;
+  void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                  Handle<SharedFunctionInfo> sfi,
                                  const char* reason) override {}
   void WeakCodeClearEvent() override;
@@ -94,7 +94,8 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
  private:
   const char* GetFunctionName(SharedFunctionInfo);
 
-  void AttachDeoptInlinedFrames(Handle<Code> code, CodeDeoptEventRecord* rec);
+  void AttachDeoptInlinedFrames(Handle<InstructionStream> code,
+                                CodeDeoptEventRecord* rec);
   Name InferScriptName(Name name, SharedFunctionInfo info);
   V8_INLINE void DispatchCodeEvent(const CodeEventsContainer& evt_rec) {
     observer_->CodeEventHandler(evt_rec);
diff --git a/src/regexp/arm/regexp-macro-assembler-arm.cc b/src/regexp/arm/regexp-macro-assembler-arm.cc
index 89d3ea62529..46ae0aaa7fe 100644
--- a/src/regexp/arm/regexp-macro-assembler-arm.cc
+++ b/src/regexp/arm/regexp-macro-assembler-arm.cc
@@ -1010,9 +1010,10 @@ Handle<HeapObject> RegExpMacroAssemblerARM::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
+  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(code);
+  return Handle<HeapObject>::cast(istream);
 }
 
 
diff --git a/src/regexp/arm64/regexp-macro-assembler-arm64.cc b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
index fe1b0f6e041..d453922f6bf 100644
--- a/src/regexp/arm64/regexp-macro-assembler-arm64.cc
+++ b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
@@ -1198,9 +1198,10 @@ Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
+  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(code);
+  return Handle<HeapObject>::cast(istream);
 }
 
 
diff --git a/src/regexp/ia32/regexp-macro-assembler-ia32.cc b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
index 33f79d30502..0d448569fa6 100644
--- a/src/regexp/ia32/regexp-macro-assembler-ia32.cc
+++ b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
@@ -1075,9 +1075,10 @@ Handle<HeapObject> RegExpMacroAssemblerIA32::GetCode(Handle<String> source) {
       Factory::CodeBuilder(isolate(), code_desc, CodeKind::REGEXP)
           .set_self_reference(masm_->CodeObject())
           .Build();
+  Handle<InstructionStream> istream(code->instruction_stream(), isolate());
   PROFILE(masm_->isolate(),
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(code);
+  return Handle<HeapObject>::cast(istream);
 }
 
 void RegExpMacroAssemblerIA32::GoTo(Label* to) { BranchOrBacktrack(to); }
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index 912175258af..2fcb0a425ec 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -288,7 +288,7 @@ int NativeRegExpMacroAssembler::CheckStackGuardState(
   DisallowGarbageCollection no_gc;
   Address old_pc = PointerAuthentication::AuthenticatePC(return_address, 0);
   DCHECK_LE(re_code.instruction_start(), old_pc);
-  DCHECK_LE(old_pc, re_code.code(kAcquireLoad).InstructionEnd());
+  DCHECK_LE(old_pc, re_code.instruction_end());
 
   StackLimitCheck check(isolate);
   bool js_has_overflowed = check.JsHasOverflowed();
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index e6e7d140a7a..1b7b4b23f4c 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -579,8 +579,8 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
   Handle<FixedArray> data =
       Handle<FixedArray>(FixedArray::cast(re->data()), isolate);
   if (compile_data.compilation_target == RegExpCompilationTarget::kNative) {
-    Code code = Code::cast(*compile_data.code);
-    data->set(JSRegExp::code_index(is_one_byte), code);
+    InstructionStream code = InstructionStream::cast(*compile_data.code);
+    data->set(JSRegExp::code_index(is_one_byte), ToCode(code));
 
     // Reset bytecode to uninitialized. In case we use tier-up we know that
     // tier-up has happened this way.
@@ -1023,9 +1023,10 @@ bool RegExpImpl::Compile(Isolate* isolate, Zone* zone, RegExpCompileData* data,
         data->compilation_target == RegExpCompilationTarget::kNative) {
       CodeTracer::Scope trace_scope(isolate->GetCodeTracer());
       OFStream os(trace_scope.file());
-      Handle<Code> code = Handle<Code>::cast(result.code);
+      Code code =
+          Handle<InstructionStream>::cast(result.code)->code(kAcquireLoad);
       std::unique_ptr<char[]> pattern_cstring = pattern->ToCString();
-      code->Disassemble(pattern_cstring.get(), os, isolate);
+      code.Disassemble(pattern_cstring.get(), os, isolate);
     }
 #endif
     if (v8_flags.print_regexp_bytecode &&
diff --git a/src/regexp/x64/regexp-macro-assembler-x64.cc b/src/regexp/x64/regexp-macro-assembler-x64.cc
index 90ff2696a50..3377858ee22 100644
--- a/src/regexp/x64/regexp-macro-assembler-x64.cc
+++ b/src/regexp/x64/regexp-macro-assembler-x64.cc
@@ -1123,9 +1123,10 @@ Handle<HeapObject> RegExpMacroAssemblerX64::GetCode(Handle<String> source) {
   Handle<Code> code = Factory::CodeBuilder(isolate, code_desc, CodeKind::REGEXP)
                           .set_self_reference(masm_.CodeObject())
                           .Build();
+  Handle<InstructionStream> istream(code->instruction_stream(), isolate);
   PROFILE(isolate,
           RegExpCodeCreateEvent(Handle<AbstractCode>::cast(code), source));
-  return Handle<HeapObject>::cast(code);
+  return Handle<HeapObject>::cast(istream);
 }
 
 void RegExpMacroAssemblerX64::GoTo(Label* to) { BranchOrBacktrack(to); }
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 591f5c8d9d5..1503f110053 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -369,7 +369,7 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   Handle<JSFunction> function = deoptimizer->function();
   // For OSR the optimized code isn't installed on the function, so get the
   // code object from deoptimizer.
-  Handle<Code> optimized_code = deoptimizer->compiled_code();
+  Handle<InstructionStream> optimized_code = deoptimizer->compiled_code();
   const DeoptimizeKind deopt_kind = deoptimizer->deopt_kind();
   const DeoptimizeReason deopt_reason =
       deoptimizer->GetDeoptInfo().deopt_reason;
@@ -415,11 +415,11 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   // the loop should pay for the deoptimization costs.
   const BytecodeOffset osr_offset = optimized_code->osr_offset();
   if (osr_offset.IsNone()) {
-    Deoptimizer::DeoptimizeFunction(*function, *optimized_code);
+    Deoptimizer::DeoptimizeFunction(*function, ToCode(*optimized_code));
     DeoptAllOsrLoopsContainingDeoptExit(isolate, *function, deopt_exit_offset);
   } else if (DeoptExitIsInsideOsrLoop(isolate, *function, deopt_exit_offset,
                                       osr_offset)) {
-    Deoptimizer::DeoptimizeFunction(*function, *optimized_code);
+    Deoptimizer::DeoptimizeFunction(*function, ToCode(*optimized_code));
   }
 
   return ReadOnlyRoots(isolate).undefined_value();
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index 819e22ef7d0..ce84ce10c75 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -1667,9 +1667,9 @@ RUNTIME_FUNCTION(Runtime_EnableCodeLoggingForTesting) {
     void CodeMovingGCEvent() final {}
     void CodeDisableOptEvent(Handle<AbstractCode> code,
                              Handle<SharedFunctionInfo> shared) final {}
-    void CodeDeoptEvent(Handle<Code> code, DeoptimizeKind kind, Address pc,
-                        int fp_to_sp_delta) final {}
-    void CodeDependencyChangeEvent(Handle<Code> code,
+    void CodeDeoptEvent(Handle<InstructionStream> code, DeoptimizeKind kind,
+                        Address pc, int fp_to_sp_delta) final {}
+    void CodeDependencyChangeEvent(Handle<InstructionStream> code,
                                    Handle<SharedFunctionInfo> shared,
                                    const char* reason) final {}
     void WeakCodeClearEvent() final {}
diff --git a/src/snapshot/code-serializer.cc b/src/snapshot/code-serializer.cc
index c83e8e4581a..9765bc80264 100644
--- a/src/snapshot/code-serializer.cc
+++ b/src/snapshot/code-serializer.cc
@@ -276,7 +276,7 @@ void CreateInterpreterDataForDeserializedCode(Isolate* isolate,
     interpreter_data->set_bytecode_array(info->GetBytecodeArray(isolate));
     interpreter_data->set_interpreter_trampoline(*code);
     if (info->HasBaselineCode()) {
-      info->baseline_code(kAcquireLoad)
+      FromCode(info->baseline_code(kAcquireLoad))
           .set_bytecode_or_interpreter_data(*interpreter_data);
     } else {
       info->set_interpreter_data(*interpreter_data);
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index 91037ae6e81..a3f0f86f5db 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -730,11 +730,11 @@ class DeserializerRelocInfoVisitor {
     DCHECK_EQ(current_object_, objects_->size());
   }
 
-  void VisitCodeTarget(RelocInfo* rinfo);
-  void VisitEmbeddedPointer(RelocInfo* rinfo);
-  void VisitExternalReference(RelocInfo* rinfo);
-  void VisitInternalReference(RelocInfo* rinfo);
-  void VisitOffHeapTarget(RelocInfo* rinfo);
+  void VisitCodeTarget(InstructionStream host, RelocInfo* rinfo);
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* rinfo);
+  void VisitExternalReference(InstructionStream host, RelocInfo* rinfo);
+  void VisitInternalReference(InstructionStream host, RelocInfo* rinfo);
+  void VisitOffHeapTarget(InstructionStream host, RelocInfo* rinfo);
 
  private:
   Isolate* isolate() { return deserializer_->isolate(); }
@@ -745,19 +745,22 @@ class DeserializerRelocInfoVisitor {
   int current_object_;
 };
 
-void DeserializerRelocInfoVisitor::VisitCodeTarget(RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitCodeTarget(InstructionStream host,
+                                                   RelocInfo* rinfo) {
   HeapObject object = *objects_->at(current_object_++);
   rinfo->set_target_address(
       InstructionStream::cast(object).instruction_start());
 }
 
-void DeserializerRelocInfoVisitor::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitEmbeddedPointer(InstructionStream host,
+                                                        RelocInfo* rinfo) {
   HeapObject object = *objects_->at(current_object_++);
   // Embedded object reference must be a strong one.
   rinfo->set_target_object(isolate()->heap(), object);
 }
 
-void DeserializerRelocInfoVisitor::VisitExternalReference(RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitExternalReference(
+    InstructionStream host, RelocInfo* rinfo) {
   byte data = source().Get();
   CHECK_EQ(data, Deserializer<Isolate>::kExternalReference);
 
@@ -766,13 +769,14 @@ void DeserializerRelocInfoVisitor::VisitExternalReference(RelocInfo* rinfo) {
   if (rinfo->IsCodedSpecially()) {
     Address location_of_branch_data = rinfo->pc();
     Assembler::deserialization_set_special_target_at(location_of_branch_data,
-                                                     rinfo->code(), address);
+                                                     host, address);
   } else {
     WriteUnalignedValue(rinfo->target_address_address(), address);
   }
 }
 
-void DeserializerRelocInfoVisitor::VisitInternalReference(RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitInternalReference(
+    InstructionStream host, RelocInfo* rinfo) {
   byte data = source().Get();
   CHECK_EQ(data, Deserializer<Isolate>::kInternalReference);
 
@@ -780,13 +784,14 @@ void DeserializerRelocInfoVisitor::VisitInternalReference(RelocInfo* rinfo) {
   int target_offset = source().GetInt();
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
   DCHECK_LT(static_cast<unsigned>(target_offset),
-            static_cast<unsigned>(rinfo->code().instruction_size()));
-  Address target = rinfo->code().InstructionStart() + target_offset;
+            static_cast<unsigned>(host.instruction_size()));
+  Address target = host.entry() + target_offset;
   Assembler::deserialization_set_target_internal_reference_at(
       rinfo->pc(), target, rinfo->rmode());
 }
 
-void DeserializerRelocInfoVisitor::VisitOffHeapTarget(RelocInfo* rinfo) {
+void DeserializerRelocInfoVisitor::VisitOffHeapTarget(InstructionStream host,
+                                                      RelocInfo* rinfo) {
   // Currently we don't serialize code that contains near builtin entries.
   DCHECK_NE(rinfo->rmode(), RelocInfo::NEAR_BUILTIN_ENTRY);
 
@@ -804,7 +809,7 @@ void DeserializerRelocInfoVisitor::VisitOffHeapTarget(RelocInfo* rinfo) {
   if (RelocInfo::OffHeapTargetIsCodedSpecially()) {
     Address location_of_branch_data = rinfo->pc();
     Assembler::deserialization_set_special_target_at(location_of_branch_data,
-                                                     rinfo->code(), address);
+                                                     host, address);
   } else {
     WriteUnalignedValue(rinfo->target_address_address(), address);
   }
@@ -1170,8 +1175,7 @@ int Deserializer<IsolateT>::ReadVariableRawData(byte data,
   return size_in_tagged;
 }
 
-// Custom deserialization for a Code object and its associated InstructionStream
-// object.
+// Deserialize raw code directly into the body of the code object.
 template <typename IsolateT>
 template <typename SlotAccessor>
 int Deserializer<IsolateT>::ReadCodeBody(byte data,
@@ -1187,16 +1191,15 @@ int Deserializer<IsolateT>::ReadCodeBody(byte data,
 
   {
     DisallowGarbageCollection no_gc;
-    InstructionStream istream =
-        InstructionStream::cast(*slot_accessor.object());
+    InstructionStream code = InstructionStream::cast(*slot_accessor.object());
 
-    // First deserialize the untagged region of the InstructionStream object.
-    source_.CopyRaw(reinterpret_cast<void*>(istream.address() +
-                                            InstructionStream::kDataStart),
-                    size_in_bytes);
+    // First deserialize the code itself.
+    source_.CopyRaw(
+        reinterpret_cast<void*>(code.address() + InstructionStream::kDataStart),
+        size_in_bytes);
   }
 
-  // Then deserialize the InstructionStream header
+  // Then deserialize the code header
   ReadData(slot_accessor.object(), HeapObject::kHeaderSize / kTaggedSize,
            InstructionStream::kDataStart / kTaggedSize);
 
@@ -1217,15 +1220,12 @@ int Deserializer<IsolateT>::ReadCodeBody(byte data,
   {
     DisallowGarbageCollection no_gc;
 
-    InstructionStream istream =
-        InstructionStream::cast(*slot_accessor.object());
+    InstructionStream code = InstructionStream::cast(*slot_accessor.object());
     if (V8_EXTERNAL_CODE_SPACE_BOOL) {
-      istream.set_main_cage_base(isolate()->cage_base(), kRelaxedStore);
+      code.set_main_cage_base(isolate()->cage_base(), kRelaxedStore);
     }
-    Code code = istream.code(kAcquireLoad);
     DeserializerRelocInfoVisitor visitor(this, &preserialized_objects);
-    for (RelocIterator it(code, istream, code.relocation_info(),
-                          code.constant_pool(),
+    for (RelocIterator it(code,
                           InstructionStream::BodyDescriptor::kRelocModeMask);
          !it.done(); it.next()) {
       it.rinfo()->Visit(&visitor);
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index d7193d02d23..201c30fa77e 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -187,7 +187,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = isolate->builtins()->code(builtin);
+    InstructionStream code = FromCode(isolate->builtins()->code(builtin));
     RelocIterator on_heap_it(code, kRelocMask);
     RelocIterator off_heap_it(blob, code, kRelocMask);
 
@@ -204,12 +204,13 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
 
       RelocInfo* rinfo = on_heap_it.rinfo();
       DCHECK_EQ(rinfo->rmode(), off_heap_it.rinfo()->rmode());
-      Code target_code = Code::FromTargetAddress(rinfo->target_address());
-      CHECK(Builtins::IsIsolateIndependentBuiltin(target_code));
+      InstructionStream target =
+          InstructionStream::FromTargetAddress(rinfo->target_address());
+      CHECK(Builtins::IsIsolateIndependentBuiltin(target.code(kAcquireLoad)));
 
       // Do not emit write-barrier for off-heap writes.
       off_heap_it.rinfo()->set_off_heap_target_address(
-          blob->InstructionStartOfBuiltin(target_code.builtin_id()));
+          blob->InstructionStartOfBuiltin(target.builtin_id()));
 
       on_heap_it.next();
       off_heap_it.next();
@@ -226,14 +227,15 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
 }
 
 void EnsureRelocatable(Code code) {
-  if (code.relocation_size() == 0) return;
+  InstructionStream instruction_stream = FromCode(code);
+  if (instruction_stream.relocation_size() == 0) return;
 
   // On some architectures (arm) the builtin might have a non-empty reloc
   // info containing a CONST_POOL entry. These entries don't have to be
   // updated when InstructionStream object is relocated, so it's safe to drop
   // the reloc info alltogether. If it wasn't the case then we'd have to store
   // it in the metadata.
-  for (RelocIterator it(code); !it.done(); it.next()) {
+  for (RelocIterator it(instruction_stream); !it.done(); it.next()) {
     CHECK_EQ(it.rinfo()->rmode(), RelocInfo::CONST_POOL);
   }
 }
@@ -253,7 +255,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    InstructionStream code = FromCode(builtins->code(builtin));
 
     // Sanity-check that the given builtin is isolate-independent.
     if (!code.IsIsolateIndependent(isolate)) {
@@ -332,7 +334,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    InstructionStream code = FromCode(builtins->code(builtin));
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].metadata_offset;
     uint8_t* dst = raw_metadata_start + offset;
@@ -350,13 +352,13 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins->code(builtin);
+    InstructionStream code = FromCode(builtins->code(builtin));
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].instruction_offset;
     uint8_t* dst = raw_code_start + offset;
     DCHECK_LE(RawCodeOffset() + offset + code.instruction_size(),
               blob_code_size);
-    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.InstructionStart()),
+    std::memcpy(dst, reinterpret_cast<uint8_t*>(code.instruction_start()),
                 code.instruction_size());
   }
 
@@ -386,7 +388,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   if (DEBUG_BOOL) {
     for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
          ++builtin) {
-      Code code = builtins->code(builtin);
+      InstructionStream code = FromCode(builtins->code(builtin));
 
       CHECK_EQ(d.InstructionSizeOfBuiltin(builtin), code.instruction_size());
       CHECK_EQ(d.MetadataSizeOfBuiltin(builtin), code.metadata_size());
diff --git a/src/snapshot/embedded/embedded-file-writer.cc b/src/snapshot/embedded/embedded-file-writer.cc
index 9ce32f64e89..ba982259b63 100644
--- a/src/snapshot/embedded/embedded-file-writer.cc
+++ b/src/snapshot/embedded/embedded-file-writer.cc
@@ -269,7 +269,7 @@ void EmbeddedFileWriter::PrepareBuiltinSourcePositionMap(Builtins* builtins) {
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     // Retrieve the SourcePositionTable and copy it.
-    Code code = builtins->code(builtin);
+    InstructionStream code = FromCode(builtins->code(builtin));
     ByteArray source_position_table = code.source_position_table();
     std::vector<unsigned char> data(source_position_table.GetDataStartAddress(),
                                     source_position_table.GetDataEndAddress());
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index bf4b8503302..a2f3547147c 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -260,8 +260,8 @@ bool Serializer::SerializePendingObject(HeapObject obj) {
 }
 
 bool Serializer::ObjectIsBytecodeHandler(HeapObject obj) const {
-  if (!obj.IsCode()) return false;
-  return (Code::cast(obj).kind() == CodeKind::BYTECODE_HANDLER);
+  if (!obj.IsInstructionStream()) return false;
+  return (InstructionStream::cast(obj).kind() == CodeKind::BYTECODE_HANDLER);
 }
 
 void Serializer::PutRoot(RootIndex root) {
@@ -870,8 +870,8 @@ void Serializer::ObjectSerializer::SerializeContent(Map map, int size) {
   HeapObject raw = *object_;
   UnlinkWeakNextScope unlink_weak_next(isolate()->heap(), raw);
   if (raw.IsInstructionStream()) {
-    // For InstructionStream objects, perform a custom serialization.
-    SerializeInstructionStream(map, size);
+    // For code objects, perform a custom serialization.
+    SerializeCode(map, size);
   } else {
     // For other objects, iterate references first.
     raw.IterateBody(map, size, this);
@@ -954,7 +954,7 @@ void Serializer::ObjectSerializer::VisitPointers(HeapObject host,
   }
 }
 
-void Serializer::ObjectSerializer::VisitCodePointer(Code host,
+void Serializer::ObjectSerializer::VisitCodePointer(HeapObject host,
                                                     CodeObjectSlot slot) {
   // A version of VisitPointers() customized for CodeObjectSlot.
   HandleScope scope(isolate());
@@ -1046,12 +1046,12 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
   explicit RelocInfoObjectPreSerializer(Serializer* serializer)
       : serializer_(serializer) {}
 
-  void VisitEmbeddedPointer(RelocInfo* target) {
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* target) {
     HeapObject object = target->target_object(isolate());
     serializer_->SerializeObject(handle(object, isolate()));
     num_serialized_objects_++;
   }
-  void VisitCodeTarget(RelocInfo* target) {
+  void VisitCodeTarget(InstructionStream host, RelocInfo* target) {
 #ifdef V8_TARGET_ARCH_ARM
     DCHECK(!RelocInfo::IsRelativeCodeTarget(target->rmode()));
 #endif
@@ -1061,9 +1061,9 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
     num_serialized_objects_++;
   }
 
-  void VisitExternalReference(RelocInfo* rinfo) {}
-  void VisitInternalReference(RelocInfo* rinfo) {}
-  void VisitOffHeapTarget(RelocInfo* target) {}
+  void VisitExternalReference(InstructionStream host, RelocInfo* rinfo) {}
+  void VisitInternalReference(InstructionStream host, RelocInfo* rinfo) {}
+  void VisitOffHeapTarget(InstructionStream host, RelocInfo* target) {}
 
   int num_serialized_objects() const { return num_serialized_objects_; }
 
@@ -1074,7 +1074,8 @@ class Serializer::ObjectSerializer::RelocInfoObjectPreSerializer {
   int num_serialized_objects_ = 0;
 };
 
-void Serializer::ObjectSerializer::VisitEmbeddedPointer(RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitEmbeddedPointer(InstructionStream host,
+                                                        RelocInfo* rinfo) {
   // Target object should be pre-serialized by RelocInfoObjectPreSerializer, so
   // just track the pointer's existence as kTaggedSize in
   // bytes_processed_so_far_.
@@ -1083,7 +1084,8 @@ void Serializer::ObjectSerializer::VisitEmbeddedPointer(RelocInfo* rinfo) {
   bytes_processed_so_far_ += kTaggedSize;
 }
 
-void Serializer::ObjectSerializer::VisitExternalReference(RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitExternalReference(
+    InstructionStream host, RelocInfo* rinfo) {
   Address target = rinfo->target_external_reference();
   DCHECK_NE(target,
             kNullAddress);  // InstructionStream does not reference null.
@@ -1094,12 +1096,14 @@ void Serializer::ObjectSerializer::VisitExternalReference(RelocInfo* rinfo) {
                           kExternalPointerNullTag);
 }
 
-void Serializer::ObjectSerializer::VisitInternalReference(RelocInfo* rinfo) {
-  Address entry = rinfo->code().InstructionStart();
+void Serializer::ObjectSerializer::VisitInternalReference(
+    InstructionStream host, RelocInfo* rinfo) {
+  Handle<InstructionStream> istream = Handle<InstructionStream>::cast(object_);
+  Address entry = istream->entry();
   DCHECK_GE(rinfo->target_internal_reference(), entry);
   uintptr_t target_offset = rinfo->target_internal_reference() - entry;
   static_assert(InstructionStream::kOnHeapBodyIsContiguous);
-  DCHECK_LT(target_offset, rinfo->code().instruction_size());
+  DCHECK_LT(target_offset, istream->instruction_size());
   sink_->Put(kInternalReference, "InternalRef");
   sink_->PutInt(target_offset, "internal ref value");
 }
@@ -1143,7 +1147,8 @@ void Serializer::ObjectSerializer::VisitExternalPointer(
   }
 }
 
-void Serializer::ObjectSerializer::VisitOffHeapTarget(RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitOffHeapTarget(InstructionStream host,
+                                                      RelocInfo* rinfo) {
   static_assert(EmbeddedData::kTableSize == Builtins::kBuiltinCount);
 
   // Currently we don't serialize code that contains near builtin entries.
@@ -1160,7 +1165,8 @@ void Serializer::ObjectSerializer::VisitOffHeapTarget(RelocInfo* rinfo) {
   sink_->PutInt(static_cast<int>(builtin), "builtin index");
 }
 
-void Serializer::ObjectSerializer::VisitCodeTarget(RelocInfo* rinfo) {
+void Serializer::ObjectSerializer::VisitCodeTarget(InstructionStream host,
+                                                   RelocInfo* rinfo) {
   // Target object should be pre-serialized by RelocInfoObjectPreSerializer, so
   // just track the pointer's existence as kTaggedSize in
   // bytes_processed_so_far_.
@@ -1261,8 +1267,7 @@ void Serializer::ObjectSerializer::OutputRawData(Address up_to) {
   }
 }
 
-void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
-                                                              int size) {
+void Serializer::ObjectSerializer::SerializeCode(Map map, int size) {
   static const int kWipeOutModeMask =
       RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
       RelocInfo::ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT) |
@@ -1273,38 +1278,33 @@ void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
       RelocInfo::ModeMask(RelocInfo::OFF_HEAP_TARGET);
 
   DCHECK_EQ(HeapObject::kHeaderSize, bytes_processed_so_far_);
-  Handle<InstructionStream> on_heap_istream =
+  Handle<InstructionStream> on_heap_code =
       Handle<InstructionStream>::cast(object_);
-  Handle<Code> code = handle(on_heap_istream->code(kAcquireLoad), isolate_);
 
   // With enabled pointer compression normal accessors no longer work for
   // off-heap objects, so we have to get the relocation info data via the
-  // on-heap InstructionStream object.
-  // TODO(v8:13784): we can clean this up since we moved all data fields from
-  // InstructionStream to Code
-  ByteArray relocation_info = code->unchecked_relocation_info();
-
-  // To make snapshots reproducible, we make a copy of the InstructionStream
-  // object and wipe all pointers in the copy, which we then serialize.
-  InstructionStream off_heap_istream = serializer_->CopyCode(*on_heap_istream);
-  for (RelocIterator it(*code, off_heap_istream, relocation_info,
-                        code->constant_pool(), kWipeOutModeMask);
+  // on-heap code object.
+  ByteArray relocation_info = on_heap_code->unchecked_relocation_info();
+
+  // To make snapshots reproducible, we make a copy of the code object
+  // and wipe all pointers in the copy, which we then serialize.
+  InstructionStream off_heap_code = serializer_->CopyCode(*on_heap_code);
+  for (RelocIterator it(off_heap_code, relocation_info, kWipeOutModeMask);
        !it.done(); it.next()) {
     RelocInfo* rinfo = it.rinfo();
     rinfo->WipeOut();
   }
   // We need to wipe out the header fields *after* wiping out the
   // relocations, because some of these fields are needed for the latter.
-  off_heap_istream.WipeOutHeader();
+  off_heap_code.WipeOutHeader();
 
   // Initially skip serializing the code header. We'll serialize it after the
   // InstructionStream body, so that the various fields the InstructionStream
   // needs for iteration are already valid.
-  // TODO(v8:13784): rename to kInstructionStreamBody
   sink_->Put(kCodeBody, "kCodeBody");
 
   // Now serialize the wiped off-heap InstructionStream, as length + data.
-  Address start = off_heap_istream.address() + InstructionStream::kDataStart;
+  Address start = off_heap_code.address() + InstructionStream::kDataStart;
   int bytes_to_output = size - InstructionStream::kDataStart;
   DCHECK(IsAligned(bytes_to_output, kTaggedSize));
   int tagged_to_output = bytes_to_output / kTaggedSize;
@@ -1323,13 +1323,12 @@ void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
   // InstructionStream::BodyDescriptor here as we don't yet want to walk the
   // RelocInfos.
   DCHECK_EQ(HeapObject::kHeaderSize, bytes_processed_so_far_);
-  VisitPointers(*on_heap_istream,
-                on_heap_istream->RawField(HeapObject::kHeaderSize),
-                on_heap_istream->RawField(InstructionStream::kDataStart));
+  VisitPointers(*on_heap_code, on_heap_code->RawField(HeapObject::kHeaderSize),
+                on_heap_code->RawField(InstructionStream::kDataStart));
   DCHECK_EQ(bytes_processed_so_far_, InstructionStream::kDataStart);
 
   // Now serialize RelocInfos. We can't allocate during a RelocInfo walk during
-  // deserialization, so we have two passes for RelocInfo serialization:
+  // deserualization, so we have two passes for RelocInfo serialization:
   //   1. A pre-serializer which serializes all allocatable objects in the
   //      RelocInfo, followed by a kSynchronize bytecode, and
   //   2. A walk the RelocInfo with this serializer, serializing any objects
@@ -1340,8 +1339,7 @@ void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
   // TODO(leszeks): We only really need to pre-serialize objects which need
   // serialization, i.e. no backrefs or roots.
   RelocInfoObjectPreSerializer pre_serializer(serializer_);
-  for (RelocIterator it(*code, *on_heap_istream, relocation_info,
-                        code->constant_pool(),
+  for (RelocIterator it(*on_heap_code, relocation_info,
                         InstructionStream::BodyDescriptor::kRelocModeMask);
        !it.done(); it.next()) {
     it.rinfo()->Visit(&pre_serializer);
@@ -1352,8 +1350,7 @@ void Serializer::ObjectSerializer::SerializeInstructionStream(Map map,
   // Finally serialize all RelocInfo objects in the on-heap InstructionStream,
   // knowing that we will not do a recursive serialization.
   // TODO(leszeks): Add a scope that DCHECKs this.
-  for (RelocIterator it(*code, *on_heap_istream, relocation_info,
-                        code->constant_pool(),
+  for (RelocIterator it(*on_heap_code, relocation_info,
                         InstructionStream::BodyDescriptor::kRelocModeMask);
        !it.done(); it.next()) {
     it.rinfo()->Visit(this);
diff --git a/src/snapshot/serializer.h b/src/snapshot/serializer.h
index 86a8cec8bd5..733567ccadd 100644
--- a/src/snapshot/serializer.h
+++ b/src/snapshot/serializer.h
@@ -462,12 +462,14 @@ class Serializer::ObjectSerializer : public ObjectVisitor {
                      ObjectSlot end) override;
   void VisitPointers(HeapObject host, MaybeObjectSlot start,
                      MaybeObjectSlot end) override;
-  void VisitCodePointer(Code host, CodeObjectSlot slot) override;
-  void VisitEmbeddedPointer(RelocInfo* target) override;
-  void VisitExternalReference(RelocInfo* rinfo) override;
-  void VisitInternalReference(RelocInfo* rinfo) override;
-  void VisitCodeTarget(RelocInfo* target) override;
-  void VisitOffHeapTarget(RelocInfo* target) override;
+  void VisitCodePointer(HeapObject host, CodeObjectSlot slot) override;
+  void VisitEmbeddedPointer(InstructionStream host, RelocInfo* target) override;
+  void VisitExternalReference(InstructionStream host,
+                              RelocInfo* rinfo) override;
+  void VisitInternalReference(InstructionStream host,
+                              RelocInfo* rinfo) override;
+  void VisitCodeTarget(InstructionStream host, RelocInfo* target) override;
+  void VisitOffHeapTarget(InstructionStream host, RelocInfo* target) override;
 
   void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                             ExternalPointerTag tag) override;
@@ -485,7 +487,7 @@ class Serializer::ObjectSerializer : public ObjectVisitor {
   void OutputExternalReference(Address target, int target_size, bool sandboxify,
                                ExternalPointerTag tag);
   void OutputRawData(Address up_to);
-  void SerializeInstructionStream(Map map, int size);
+  void SerializeCode(Map map, int size);
   uint32_t SerializeBackingStore(void* backing_store, int32_t byte_length,
                                  Maybe<int32_t> max_byte_length);
   void SerializeJSTypedArray();
diff --git a/src/snapshot/startup-serializer.cc b/src/snapshot/startup-serializer.cc
index f7af6a4ee60..981c910ea49 100644
--- a/src/snapshot/startup-serializer.cc
+++ b/src/snapshot/startup-serializer.cc
@@ -85,11 +85,15 @@ StartupSerializer::~StartupSerializer() {
 #ifdef DEBUG
 namespace {
 
-bool IsUnexpectedInstructionStreamObject(Isolate* isolate, HeapObject obj) {
+bool IsUnexpectedCodeObject(Isolate* isolate, HeapObject obj) {
   if (!obj.IsInstructionStream()) return false;
-  // TODO(jgruber): Is REGEXP code still fully supported?
-  return InstructionStream::cast(obj).code(kAcquireLoad).kind() !=
-         CodeKind::REGEXP;
+
+  InstructionStream code = InstructionStream::cast(obj);
+  if (code.kind() == CodeKind::REGEXP) return false;
+  if (!code.is_builtin()) return true;
+
+  // An on-heap builtin.
+  return true;
 }
 
 }  // namespace
@@ -109,7 +113,7 @@ void StartupSerializer::SerializeObjectImpl(Handle<HeapObject> obj) {
   {
     DisallowGarbageCollection no_gc;
     HeapObject raw = *obj;
-    DCHECK(!IsUnexpectedInstructionStreamObject(isolate(), raw));
+    DCHECK(!IsUnexpectedCodeObject(isolate(), raw));
     if (SerializeHotObject(raw)) return;
     if (IsRootAndHasBeenSerialized(raw) && SerializeRoot(raw)) return;
   }
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index 3c4820ba582..8737c1c68ed 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1455,8 +1455,11 @@ namespace {
 
 void RecordStats(Code code, Counters* counters) {
   if (!code.has_instruction_stream()) return;
-  counters->wasm_generated_code_size()->Increment(code.body_size());
-  counters->wasm_reloc_size()->Increment(code.relocation_info().length());
+  InstructionStream instruction_stream = FromCode(code);
+  counters->wasm_generated_code_size()->Increment(
+      instruction_stream.body_size());
+  counters->wasm_reloc_size()->Increment(
+      instruction_stream.relocation_info().length());
 }
 
 enum CompilationExecutionResult : int8_t { kNoMoreUnits, kYield };
diff --git a/src/wasm/wasm-code-manager.cc b/src/wasm/wasm-code-manager.cc
index 69847eb05f0..bdb55457c0c 100644
--- a/src/wasm/wasm-code-manager.cc
+++ b/src/wasm/wasm-code-manager.cc
@@ -907,7 +907,7 @@ CompilationEnv NativeModule::CreateCompilationEnv() const {
           compilation_state()->dynamic_tiering()};
 }
 
-WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
+WasmCode* NativeModule::AddCodeForTesting(Handle<InstructionStream> code) {
   CodeSpaceWriteScope code_space_write_scope(this);
   const size_t relocation_size = code->relocation_size();
   base::OwnedVector<byte> reloc_info;
@@ -916,7 +916,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
         base::Vector<byte>{code->relocation_start(), relocation_size});
   }
   Handle<ByteArray> source_pos_table(code->source_position_table(),
-                                     code->instruction_stream().GetIsolate());
+                                     code->GetIsolate());
   base::OwnedVector<byte> source_pos =
       base::OwnedVector<byte>::NewForOverwrite(source_pos_table->length());
   if (source_pos_table->length() > 0) {
@@ -950,7 +950,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
 
   // Apply the relocation delta by iterating over the RelocInfo.
   intptr_t delta = reinterpret_cast<Address>(dst_code_bytes.begin()) -
-                   code->InstructionStart();
+                   code->instruction_start();
   int mode_mask =
       RelocInfo::kApplyMask | RelocInfo::ModeMask(RelocInfo::WASM_STUB_CALL);
   auto jump_tables_ref =
diff --git a/src/wasm/wasm-code-manager.h b/src/wasm/wasm-code-manager.h
index 9760c22dea7..2b0d32af8b0 100644
--- a/src/wasm/wasm-code-manager.h
+++ b/src/wasm/wasm-code-manager.h
@@ -656,7 +656,7 @@ class V8_EXPORT_PRIVATE NativeModule final {
       ExecutionTier tier);
 
   // Adds anonymous code for testing purposes.
-  WasmCode* AddCodeForTesting(Handle<Code> code);
+  WasmCode* AddCodeForTesting(Handle<InstructionStream> code);
 
   // Allocates and initializes the {lazy_compile_table_} and initializes the
   // first jump table with jumps to the {lazy_compile_table_}.
diff --git a/test/cctest/compiler/codegen-tester.h b/test/cctest/compiler/codegen-tester.h
index 219650def31..44eabde6111 100644
--- a/test/cctest/compiler/codegen-tester.h
+++ b/test/cctest/compiler/codegen-tester.h
@@ -72,6 +72,10 @@ class RawMachineAssemblerTester : public HandleAndZoneScope,
 
   void GenerateCode() { Generate(); }
 
+  Handle<InstructionStream> GetInstructionStream() {
+    return handle(GetCode()->instruction_stream(), main_isolate());
+  }
+
   Handle<Code> GetCode() {
     Generate();
     return code_.ToHandleChecked();
diff --git a/test/cctest/compiler/function-tester.cc b/test/cctest/compiler/function-tester.cc
index f3bfd5bb11b..614c799556c 100644
--- a/test/cctest/compiler/function-tester.cc
+++ b/test/cctest/compiler/function-tester.cc
@@ -38,6 +38,18 @@ FunctionTester::FunctionTester(Graph* graph, int param_count)
   CompileGraph(graph);
 }
 
+FunctionTester::FunctionTester(Handle<InstructionStream> code, int param_count)
+    : isolate(main_isolate()),
+      canonical(isolate),
+      function((v8_flags.allow_natives_syntax = true,
+                NewFunction(BuildFunction(param_count).c_str()))),
+      flags_(0) {
+  CHECK(!code.is_null());
+  CHECK(code->IsInstructionStream());
+  Compile(function);
+  function->set_code(ToCode(*code), kReleaseStore);
+}
+
 FunctionTester::FunctionTester(Handle<Code> code, int param_count)
     : isolate(main_isolate()),
       canonical(isolate),
@@ -50,6 +62,9 @@ FunctionTester::FunctionTester(Handle<Code> code, int param_count)
   function->set_code(*code, kReleaseStore);
 }
 
+FunctionTester::FunctionTester(Handle<InstructionStream> code)
+    : FunctionTester(code, 0) {}
+
 FunctionTester::FunctionTester(Handle<Code> code) : FunctionTester(code, 0) {}
 
 void FunctionTester::CheckThrows(Handle<Object> a) {
diff --git a/test/cctest/compiler/function-tester.h b/test/cctest/compiler/function-tester.h
index 09249f40b09..6f589979829 100644
--- a/test/cctest/compiler/function-tester.h
+++ b/test/cctest/compiler/function-tester.h
@@ -25,9 +25,11 @@ class FunctionTester : public InitializedHandleScope {
 
   FunctionTester(Graph* graph, int param_count);
 
+  FunctionTester(Handle<InstructionStream> code, int param_count);
   FunctionTester(Handle<Code> code, int param_count);
 
   // Assumes VoidDescriptor call interface.
+  explicit FunctionTester(Handle<InstructionStream> code);
   explicit FunctionTester(Handle<Code> code);
 
   Isolate* isolate;
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index 7d86c2e3592..7242134dc16 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -1666,7 +1666,9 @@ TEST(Regress_1171759) {
   std::shared_ptr<wasm::NativeModule> module =
       AllocateNativeModule(handles.main_isolate(), code->InstructionSize());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
-  byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
+  Handle<InstructionStream> istream(code->instruction_stream(),
+                                    handles.main_isolate());
+  byte* code_start = module->AddCodeForTesting(istream)->instructions().begin();
 
   // Generate a minimal calling function, to push stack arguments.
   RawMachineAssemblerTester<int32_t> mt;
diff --git a/test/cctest/compiler/test-multiple-return.cc b/test/cctest/compiler/test-multiple-return.cc
index 9c65ee1582b..afc36d47c8e 100644
--- a/test/cctest/compiler/test-multiple-return.cc
+++ b/test/cctest/compiler/test-multiple-return.cc
@@ -174,6 +174,8 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
         code->Disassemble("multi_value", os, handles.main_isolate());
       }
 #endif
+      Handle<InstructionStream> istream(code->instruction_stream(),
+                                        handles.main_isolate());
 
       const int a = 47, b = 12;
       int expect = 0;
@@ -185,10 +187,10 @@ void TestReturnMultipleValues(MachineType type, int min_count, int max_count) {
       }
 
       std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
-          handles.main_isolate(), code->instruction_size());
+          handles.main_isolate(), istream->instruction_size());
       wasm::WasmCodeRefScope wasm_code_ref_scope;
       byte* code_start =
-          module->AddCodeForTesting(code)->instructions().begin();
+          module->AddCodeForTesting(istream)->instructions().begin();
 
       RawMachineAssemblerTester<int32_t> mt(CodeKind::JS_TO_WASM_FUNCTION);
       const int input_count = 2 + param_count;
@@ -280,11 +282,14 @@ void ReturnLastValue(MachineType type) {
                             AssemblerOptions::Default(handles.main_isolate()),
                             m.ExportForTest())
                             .ToHandleChecked();
+    Handle<InstructionStream> istream(code->instruction_stream(),
+                                      handles.main_isolate());
 
-    std::shared_ptr<wasm::NativeModule> module =
-        AllocateNativeModule(handles.main_isolate(), code->instruction_size());
+    std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
+        handles.main_isolate(), istream->instruction_size());
     wasm::WasmCodeRefScope wasm_code_ref_scope;
-    byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
+    byte* code_start =
+        module->AddCodeForTesting(istream)->instructions().begin();
 
     // Generate caller.
     int expect = return_count - 1;
@@ -343,11 +348,14 @@ void ReturnSumOfReturns(MachineType type) {
                             AssemblerOptions::Default(handles.main_isolate()),
                             m.ExportForTest())
                             .ToHandleChecked();
+    Handle<InstructionStream> istream(code->instruction_stream(),
+                                      handles.main_isolate());
 
-    std::shared_ptr<wasm::NativeModule> module =
-        AllocateNativeModule(handles.main_isolate(), code->instruction_size());
+    std::shared_ptr<wasm::NativeModule> module = AllocateNativeModule(
+        handles.main_isolate(), istream->instruction_size());
     wasm::WasmCodeRefScope wasm_code_ref_scope;
-    byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
+    byte* code_start =
+        module->AddCodeForTesting(istream)->instructions().begin();
 
     // Generate caller.
     RawMachineAssemblerTester<int32_t> mt;
diff --git a/test/cctest/heap/test-concurrent-allocation.cc b/test/cctest/heap/test-concurrent-allocation.cc
index b2ea8e238dc..fdef32ed33b 100644
--- a/test/cctest/heap/test-concurrent-allocation.cc
+++ b/test/cctest/heap/test-concurrent-allocation.cc
@@ -470,7 +470,7 @@ UNINITIALIZED_TEST(ConcurrentWriteBarrier) {
 
 class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
  public:
-  explicit ConcurrentRecordRelocSlotThread(Heap* heap, Code code,
+  explicit ConcurrentRecordRelocSlotThread(Heap* heap, InstructionStream code,
                                            HeapObject value)
       : v8::base::Thread(base::Thread::Options("ThreadWithLocalHeap")),
         heap_(heap),
@@ -491,7 +491,7 @@ class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
   }
 
   Heap* heap_;
-  Code code_;
+  InstructionStream code_;
   HeapObject value_;
 };
 
@@ -510,7 +510,7 @@ UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
   Isolate* i_isolate = reinterpret_cast<Isolate*>(isolate);
   Heap* heap = i_isolate->heap();
   {
-    Code code;
+    InstructionStream code;
     HeapObject value;
     CodePageCollectionMemoryModificationScopeForTesting code_scope(heap);
     {
@@ -529,8 +529,11 @@ UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
 #endif
       CodeDesc desc;
       masm.GetCode(i_isolate, &desc);
-      Handle<Code> code_handle =
-          Factory::CodeBuilder(i_isolate, desc, CodeKind::FOR_TESTING).Build();
+      Handle<InstructionStream> code_handle(
+          Factory::CodeBuilder(i_isolate, desc, CodeKind::FOR_TESTING)
+              .Build()
+              ->instruction_stream(),
+          i_isolate);
       heap::AbandonCurrentlyFreeMemory(heap->old_space());
       Handle<HeapNumber> value_handle(
           i_isolate->factory()->NewHeapNumber<AllocationType::kOld>(1.1));
diff --git a/test/cctest/heap/test-heap.cc b/test/cctest/heap/test-heap.cc
index 9da3f7a67d1..9cfc4af5f5f 100644
--- a/test/cctest/heap/test-heap.cc
+++ b/test/cctest/heap/test-heap.cc
@@ -185,8 +185,9 @@ static void CheckNumber(Isolate* isolate, double value, const char* string) {
   CHECK(String::cast(*print_string).IsOneByteEqualTo(base::CStrVector(string)));
 }
 
-void CheckEmbeddedObjectsAreEqual(Isolate* isolate, Handle<Code> lhs,
-                                  Handle<Code> rhs) {
+void CheckEmbeddedObjectsAreEqual(Isolate* isolate,
+                                  Handle<InstructionStream> lhs,
+                                  Handle<InstructionStream> rhs) {
   int mode_mask = RelocInfo::ModeMask(RelocInfo::FULL_EMBEDDED_OBJECT);
   PtrComprCageBase cage_base(isolate);
   RelocIterator lhs_it(*lhs, mode_mask);
@@ -4338,7 +4339,7 @@ TEST(CellsInOptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<Code> code;
+  Handle<InstructionStream> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4362,7 +4363,7 @@ TEST(CellsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(bar->code(), isolate);
+    code = handle(FromCode(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4387,7 +4388,7 @@ TEST(ObjectsInOptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<Code> code;
+  Handle<InstructionStream> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4409,7 +4410,7 @@ TEST(ObjectsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(bar->code(), isolate);
+    code = handle(FromCode(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4434,7 +4435,7 @@ TEST(NewSpaceObjectsInOptimizedCode) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(isolate);
-  Handle<Code> code;
+  Handle<InstructionStream> code;
   {
     LocalContext context;
     HandleScope scope(isolate);
@@ -4475,7 +4476,7 @@ TEST(NewSpaceObjectsInOptimizedCode) {
     HeapVerifier::VerifyHeap(CcTest::heap());
 #endif
     CHECK(!bar->code().marked_for_deoptimization());
-    code = handle(bar->code(), isolate);
+    code = handle(FromCode(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4499,7 +4500,7 @@ TEST(ObjectsInEagerlyDeoptimizedCodeAreWeak) {
 
   if (!isolate->use_optimizer()) return;
   HandleScope outer_scope(heap->isolate());
-  Handle<Code> code;
+  Handle<InstructionStream> code;
   {
     LocalContext context;
     HandleScope scope(heap->isolate());
@@ -4522,7 +4523,7 @@ TEST(ObjectsInEagerlyDeoptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(bar->code(), isolate);
+    code = handle(FromCode(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
diff --git a/test/cctest/test-cpu-profiler.cc b/test/cctest/test-cpu-profiler.cc
index 09b0d0541ff..dc44d60eadd 100644
--- a/test/cctest/test-cpu-profiler.cc
+++ b/test/cctest/test-cpu-profiler.cc
@@ -4286,7 +4286,7 @@ int GetSourcePositionEntryCount(i::Isolate* isolate, const char* source,
   i::Handle<i::JSFunction> function = i::Handle<i::JSFunction>::cast(
       v8::Utils::OpenHandle(*CompileRun(source)));
   if (function->ActiveTierIsIgnition()) return -1;
-  i::Handle<i::Code> code(function->code(), isolate);
+  i::Handle<i::InstructionStream> code(i::FromCode(function->code()), isolate);
   i::SourcePositionTableIterator iterator(
       ByteArray::cast(code->source_position_table()));
 
diff --git a/test/cctest/test-serialize.cc b/test/cctest/test-serialize.cc
index eb4a9db6314..49e09e60c2a 100644
--- a/test/cctest/test-serialize.cc
+++ b/test/cctest/test-serialize.cc
@@ -1597,7 +1597,9 @@ int CountBuiltins() {
   int counter = 0;
   for (HeapObject obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
-    if (obj.IsCode() && Code::cast(obj).kind() == CodeKind::BUILTIN) counter++;
+    if (obj.IsInstructionStream() &&
+        InstructionStream::cast(obj).kind() == CodeKind::BUILTIN)
+      counter++;
   }
   return counter;
 }
diff --git a/test/cctest/test-unwinder-code-pages.cc b/test/cctest/test-unwinder-code-pages.cc
index fe72a45d894..795126b8e4f 100644
--- a/test/cctest/test-unwinder-code-pages.cc
+++ b/test/cctest/test-unwinder-code-pages.cc
@@ -301,12 +301,13 @@ TEST(Unwind_CodeObjectPCInMiddle_Success_CodePagesAPI) {
   // --no-maglev.
   if (!code.is_optimized_code()) return;
 
+  InstructionStream instruction_stream = FromCode(code);
   // We don't want the offset too early or it could be the `push rbp`
   // instruction (which is not at the start of generated code, because the lazy
   // deopt check happens before frame setup).
-  const uintptr_t offset = code.instruction_size() - 20;
-  CHECK_LT(offset, code.instruction_size());
-  Address pc = code.InstructionStart() + offset;
+  const uintptr_t offset = instruction_stream.instruction_size() - 20;
+  CHECK_LT(offset, instruction_stream.instruction_size());
+  Address pc = instruction_stream.instruction_start() + offset;
   register_state.pc = reinterpret_cast<void*>(pc);
 
   // Get code pages from the API now that the code obejct exists and check that
@@ -672,12 +673,14 @@ TEST(PCIsInV8_LargeCodeObject_CodePagesAPI) {
   desc.unwinding_info = nullptr;
   desc.unwinding_info_size = 0;
   desc.origin = nullptr;
-  Handle<Code> foo_code =
-      Factory::CodeBuilder(i_isolate, desc, CodeKind::WASM_FUNCTION).Build();
-
-  CHECK(i_isolate->heap()->InSpace(foo_code->instruction_stream(),
-                                   CODE_LO_SPACE));
-  byte* start = reinterpret_cast<byte*>(foo_code->InstructionStart());
+  Handle<InstructionStream> foo_code(
+      Factory::CodeBuilder(i_isolate, desc, CodeKind::WASM_FUNCTION)
+          .Build()
+          ->instruction_stream(),
+      i_isolate);
+
+  CHECK(i_isolate->heap()->InSpace(*foo_code, CODE_LO_SPACE));
+  byte* start = reinterpret_cast<byte*>(foo_code->instruction_start());
 
   MemoryRange code_pages[v8::Isolate::kMinCodePagesBufferSize];
   size_t pages_length =
diff --git a/test/fuzzer/multi-return.cc b/test/fuzzer/multi-return.cc
index 09c49ec9f82..11746b4a1b6 100644
--- a/test/fuzzer/multi-return.cc
+++ b/test/fuzzer/multi-return.cc
@@ -244,11 +244,12 @@ extern "C" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {
                                        AssemblerOptions::Default(i_isolate),
                                        callee.ExportForTest())
           .ToHandleChecked();
+  Handle<InstructionStream> istream(code->instruction_stream(), i_isolate);
 
   std::shared_ptr<wasm::NativeModule> module =
       AllocateNativeModule(i_isolate, code->InstructionSize());
   wasm::WasmCodeRefScope wasm_code_ref_scope;
-  byte* code_start = module->AddCodeForTesting(code)->instructions().begin();
+  byte* code_start = module->AddCodeForTesting(istream)->instructions().begin();
   // Generate wrapper.
   int expect = 0;
 
diff --git a/test/unittests/assembler/macro-assembler-x64-unittest.cc b/test/unittests/assembler/macro-assembler-x64-unittest.cc
index 9924b620eee..8ea56c12c4e 100644
--- a/test/unittests/assembler/macro-assembler-x64-unittest.cc
+++ b/test/unittests/assembler/macro-assembler-x64-unittest.cc
@@ -530,7 +530,8 @@ TEST_F(MacroAssemblerX64Test, EmbeddedObj) {
 
   // Test the user-facing reloc interface.
   const int mode_mask = RelocInfo::EmbeddedObjectModeMask();
-  for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
+  for (RelocIterator it(code->instruction_stream(), mode_mask); !it.done();
+       it.next()) {
     RelocInfo::Mode mode = it.rinfo()->rmode();
     if (RelocInfo::IsCompressedEmbeddedObject(mode)) {
       CHECK_EQ(*my_array, it.rinfo()->target_object(cage_base));
diff --git a/test/unittests/compiler/codegen-tester.h b/test/unittests/compiler/codegen-tester.h
index 146d9907f7e..68248381b4c 100644
--- a/test/unittests/compiler/codegen-tester.h
+++ b/test/unittests/compiler/codegen-tester.h
@@ -72,6 +72,10 @@ class RawMachineAssemblerTester : public CallHelper<ReturnType>,
 
   void GenerateCode() { Generate(); }
 
+  Handle<InstructionStream> GetInstructionStream() {
+    return handle(GetCode()->instruction_stream(), isolate_);
+  }
+
   Handle<Code> GetCode() {
     Generate();
     return code_.ToHandleChecked();
diff --git a/test/unittests/compiler/function-tester.cc b/test/unittests/compiler/function-tester.cc
index d7e7356c767..3f9b582df98 100644
--- a/test/unittests/compiler/function-tester.cc
+++ b/test/unittests/compiler/function-tester.cc
@@ -52,6 +52,18 @@ FunctionTester::FunctionTester(Isolate* isolate, Graph* graph, int param_count)
   CompileGraph(graph);
 }
 
+FunctionTester::FunctionTester(Isolate* isolate, Handle<InstructionStream> code,
+                               int param_count)
+    : isolate(isolate),
+      canonical(isolate),
+      function((v8_flags.allow_natives_syntax = true,
+                NewFunction(BuildFunction(param_count).c_str()))),
+      flags_(0) {
+  CHECK(!code.is_null());
+  Compile(function);
+  function->set_code(ToCode(*code), kReleaseStore);
+}
+
 FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
                                int param_count)
     : isolate(isolate),
@@ -64,6 +76,9 @@ FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
   function->set_code(*code, kReleaseStore);
 }
 
+FunctionTester::FunctionTester(Isolate* isolate, Handle<InstructionStream> code)
+    : FunctionTester(isolate, code, 0) {}
+
 void FunctionTester::CheckThrows(Handle<Object> a) {
   TryCatch try_catch(reinterpret_cast<v8::Isolate*>(isolate));
   MaybeHandle<Object> no_result = Call(a);
diff --git a/test/unittests/regexp/regexp-unittest.cc b/test/unittests/regexp/regexp-unittest.cc
index 28799cdcebe..f42b455b39c 100644
--- a/test/unittests/regexp/regexp-unittest.cc
+++ b/test/unittests/regexp/regexp-unittest.cc
@@ -641,7 +641,8 @@ class ContextInitializer {
 
 // Create new JSRegExp object with only necessary fields (for this tests)
 // initialized.
-static Handle<JSRegExp> CreateJSRegExp(Handle<String> source, Handle<Code> code,
+static Handle<JSRegExp> CreateJSRegExp(Handle<String> source,
+                                       Handle<InstructionStream> code,
                                        bool is_unicode = false) {
   Isolate* isolate = reinterpret_cast<i::Isolate*>(v8::Isolate::GetCurrent());
   Factory* factory = isolate->factory();
@@ -681,7 +682,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSuccess) {
 
   Handle<String> source = factory->NewStringFromStaticChars("");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   int captures[4] = {42, 37, 87, 117};
@@ -728,7 +729,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSimple) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^foo");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   int captures[4] = {42, 37, 87, 117};
@@ -784,7 +785,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeSimpleUC16) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^foo");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code, true);
 
   int captures[4] = {42, 37, 87, 117};
@@ -842,7 +843,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBacktrack) {
 
   Handle<String> source = factory->NewStringFromStaticChars("..........");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("foofoo");
@@ -880,7 +881,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackReferenceLATIN1) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^(..)..\1");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("fooofo");
@@ -923,7 +924,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackReferenceUC16) {
 
   Handle<String> source = factory->NewStringFromStaticChars("^(..)..\1");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code, true);
 
   const base::uc16 input_data[6] = {'f', 0x2028, 'o', 'o', 'f', 0x2028};
@@ -976,7 +977,7 @@ TEST_F(RegExpTest, MacroAssemblernativeAtStart) {
 
   Handle<String> source = factory->NewStringFromStaticChars("(^f|ob)");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("foobar");
@@ -1027,7 +1028,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeBackRefNoCase) {
   Handle<String> source =
       factory->NewStringFromStaticChars("^(abc)\1\1(?!\1)...(?!\1)");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   Handle<String> input = factory->NewStringFromStaticChars("aBcAbCABCxYzab");
@@ -1119,7 +1120,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeRegisters) {
 
   Handle<String> source = factory->NewStringFromStaticChars("<loop test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -1156,7 +1157,7 @@ TEST_F(RegExpTest, MacroAssemblerStackOverflow) {
   Handle<String> source =
       factory->NewStringFromStaticChars("<stack overflow test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -1196,7 +1197,7 @@ TEST_F(RegExpTest, MacroAssemblerNativeLotsOfRegisters) {
   Handle<String> source =
       factory->NewStringFromStaticChars("<huge register space test>");
   Handle<Object> code_object = m.GetCode(source);
-  Handle<Code> code = Handle<Code>::cast(code_object);
+  Handle<InstructionStream> code = Handle<InstructionStream>::cast(code_object);
   Handle<JSRegExp> regexp = CreateJSRegExp(source, code);
 
   // String long enough for test (content doesn't matter).
@@ -2360,8 +2361,8 @@ TEST_F(RegExpTestWithContext, UnicodePropertyEscapeCodeSize) {
     CHECK_LT(ByteArray::cast(maybe_bytecode).Size(), kMaxSize);
   } else if (maybe_code.IsCode()) {
     // On x64, excessive inlining produced >360KB.
-    CHECK_LT(Code::cast(maybe_code).Size(), kMaxSize);
-    CHECK_EQ(Code::cast(maybe_code).kind(), CodeKind::REGEXP);
+    CHECK_LT(FromCode(Code::cast(maybe_code)).Size(), kMaxSize);
+    CHECK_EQ(FromCode(Code::cast(maybe_code)).kind(), CodeKind::REGEXP);
   } else {
     UNREACHABLE();
   }
diff --git a/tools/gen-postmortem-metadata.py b/tools/gen-postmortem-metadata.py
index 4d171267a96..69b0f1ff1b7 100644
--- a/tools/gen-postmortem-metadata.py
+++ b/tools/gen-postmortem-metadata.py
@@ -164,11 +164,11 @@ consts_misc = [
     },
     {
         'name': 'CodeKindFieldMask',
-        'value': 'Code::KindField::kMask'
+        'value': 'InstructionStream::KindField::kMask'
     },
     {
         'name': 'CodeKindFieldShift',
-        'value': 'Code::KindField::kShift'
+        'value': 'InstructionStream::KindField::kShift'
     },
     {
         'name': 'DeoptimizationDataInlinedFunctionCountIndex',
@@ -549,7 +549,10 @@ extras_accessors = [
     'SharedFunctionInfo, flags, int, kFlagsOffset',
     'SharedFunctionInfo, length, uint16_t, kLengthOffset',
     'SlicedString, parent, String, kParentOffset',
+    'InstructionStream, flags, uint32_t, kFlagsOffset',
     'InstructionStream, instruction_start, uintptr_t, kHeaderSize',
+    'InstructionStream, instruction_size, int, kInstructionSizeOffset',
+    'InstructionStream, deoptimization_data, FixedArray, kDeoptimizationDataOrInterpreterDataOffset',
     'String, length, int32_t, kLengthOffset',
     'DescriptorArray, header_size, uintptr_t, kHeaderSize',
     'ConsString, first, String, kFirstOffset',
-- 
2.35.1

