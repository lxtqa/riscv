From 7ab965538a16f61db26b3e32cebf4a8467502d83 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Olivier=20Fl=C3=BCckiger?= <olivf@chromium.org>
Date: Thu, 13 Jul 2023 09:58:29 +0200
Subject: [PATCH] [maglev-osr] Add a maybe_has_maglev_osr_code flag to the fv
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

Sharing the maybe_has_optimized_osr_code flag on the feedback vector for
maglev and turbofan osr caused the osr checks in osr-from-maglev to
trigger unneccessarily often, since the flag was already set by
maglev-osr.

With this change we add an additional flag to distinguish the two cases.
For interpreter and baseline code nothing changes as these tiers will
happily tier up to either maglev or turbofan.

Bug: v8:7700
Change-Id: I89d5ad0eec7f7b53a7a8850f48d563d179324850
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4672408
Commit-Queue: Olivier Flückiger <olivf@chromium.org>
Reviewed-by: Jakob Linke <jgruber@chromium.org>
Auto-Submit: Olivier Flückiger <olivf@chromium.org>
Reviewed-by: Toon Verwaest <verwaest@chromium.org>
Cr-Commit-Position: refs/heads/main@{#88894}
---
 src/base/bit-field.h                     | 21 +++++++++++++++++++++
 src/baseline/baseline-compiler.cc        |  4 +++-
 src/builtins/arm/builtins-arm.cc         |  3 +--
 src/builtins/arm64/builtins-arm64.cc     |  3 +--
 src/builtins/ia32/builtins-ia32.cc       |  3 +--
 src/builtins/loong64/builtins-loong64.cc |  3 +--
 src/builtins/mips64/builtins-mips64.cc   |  3 +--
 src/builtins/ppc/builtins-ppc.cc         |  3 +--
 src/builtins/riscv/builtins-riscv.cc     |  3 +--
 src/builtins/s390/builtins-s390.cc       |  3 +--
 src/builtins/x64/builtins-x64.cc         |  3 +--
 src/interpreter/interpreter-generator.cc |  5 ++++-
 src/maglev/maglev-ir.cc                  | 14 ++++++++------
 src/objects/feedback-vector-inl.h        | 21 ++++++++++++++++++---
 src/objects/feedback-vector.cc           |  5 +++--
 src/objects/feedback-vector.h            |  9 +++++----
 src/objects/feedback-vector.tq           | 10 ++++++----
 17 files changed, 77 insertions(+), 39 deletions(-)

diff --git a/src/base/bit-field.h b/src/base/bit-field.h
index 06db44e3b2e..59e786bdd1f 100644
--- a/src/base/bit-field.h
+++ b/src/base/bit-field.h
@@ -7,6 +7,8 @@
 
 #include <stdint.h>
 
+#include <algorithm>
+
 #include "src/base/macros.h"
 
 namespace v8 {
@@ -29,6 +31,7 @@ class BitField final {
   static_assert(size > 0);
 
   using FieldType = T;
+  using BaseType = U;
 
   // A type U mask of bit field.  To use all bits of a type U of x bits
   // in a bitfield without compiler warnings we have to compute 2^x
@@ -72,6 +75,24 @@ class BitField final {
   }
 };
 
+// ----------------------------------------------------------------------------
+// BitFieldUnion can be used to combine two linear BitFields.
+// So far only the static mask is computed. Encoding and decoding tbd.
+// Can be used for example as a quick combined check:
+//   `if (BitFieldUnion<BFA, BFB>::kMask & bitfield) ...`
+
+template <typename A, typename B>
+class BitFieldUnion final {
+ public:
+  static_assert(
+      std::is_same<typename A::BaseType, typename B::BaseType>::value);
+  static_assert((A::kMask & B::kMask) == 0);
+  static constexpr int kShift = std::min(A::kShift, B::kShift);
+  static constexpr int kMask = A::kMask | B::kMask;
+  static constexpr int kSize =
+      A::kSize + B::kSize + (std::max(A::kShift, B::kShift) - kShift);
+};
+
 template <class T, int shift, int size>
 using BitField8 = BitField<T, shift, size, uint8_t>;
 
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index 1c971ff04cf..217fb226142 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -1908,7 +1908,9 @@ void BaselineCompiler::VisitJumpLoop() {
     LoadFeedbackVector(feedback_vector);
     __ LoadWord8Field(osr_state, feedback_vector,
                       FeedbackVector::kOsrStateOffset);
-    static_assert(FeedbackVector::MaybeHasOptimizedOsrCodeBit::encode(true) >
+    static_assert(FeedbackVector::MaybeHasMaglevOsrCodeBit::encode(true) >
+                  FeedbackVector::kMaxOsrUrgency);
+    static_assert(FeedbackVector::MaybeHasTurbofanOsrCodeBit::encode(true) >
                   FeedbackVector::kMaxOsrUrgency);
     __ JumpIfByte(kUnsignedGreaterThan, osr_state, loop_depth, &osr_armed,
                   Label::kNear);
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 67181dbc2c8..eee0e1f3e80 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -937,8 +937,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ ldrb(scratch,
           FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ and_(scratch, scratch,
-          Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ and_(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ strb(scratch,
           FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index e1de804f8ac..7c6142019ed 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -1104,8 +1104,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ Ldrb(scratch,
           FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ And(scratch, scratch,
-         Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ Strb(scratch,
           FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index 8cae1a0e182..be736200fa8 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -880,8 +880,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
                                    Register feedback_vector, Register scratch) {
   __ mov_b(scratch,
            FieldOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ and_(scratch,
-          Immediate(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ and_(scratch, Immediate(~FeedbackVector::OsrUrgencyBits::kMask));
   __ mov_b(FieldOperand(feedback_vector, FeedbackVector::kOsrStateOffset),
            scratch);
 }
diff --git a/src/builtins/loong64/builtins-loong64.cc b/src/builtins/loong64/builtins-loong64.cc
index c274e481054..4aafd868e44 100644
--- a/src/builtins/loong64/builtins-loong64.cc
+++ b/src/builtins/loong64/builtins-loong64.cc
@@ -926,8 +926,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ Ld_bu(scratch,
            FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ And(scratch, scratch,
-         Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ St_b(scratch,
           FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index ae0bece22aa..0795b37b334 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -908,8 +908,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ Lbu(scratch,
          FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ And(scratch, scratch,
-         Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ Sb(scratch,
         FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index 53a6d7d785c..18b763957cd 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -111,8 +111,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   __ LoadU8(scratch1,
             FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset),
             scratch2);
-  __ andi(scratch1, scratch1,
-          Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ andi(scratch1, scratch1, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ StoreU8(scratch1,
              FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset),
              scratch2);
diff --git a/src/builtins/riscv/builtins-riscv.cc b/src/builtins/riscv/builtins-riscv.cc
index 24adaf43376..47388e96804 100644
--- a/src/builtins/riscv/builtins-riscv.cc
+++ b/src/builtins/riscv/builtins-riscv.cc
@@ -960,8 +960,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ Lbu(scratch,
          FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ And(scratch, scratch,
-         Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ And(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ Sb(scratch,
         FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index d48f5630142..b83347b3914 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -113,8 +113,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
   DCHECK(!AreAliased(feedback_vector, scratch));
   __ LoadU8(scratch,
             FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ AndP(scratch, scratch,
-          Operand(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ AndP(scratch, scratch, Operand(~FeedbackVector::OsrUrgencyBits::kMask));
   __ StoreU8(scratch,
              FieldMemOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
 }
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index e828c11f3a1..83318369e60 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -1011,8 +1011,7 @@ void ResetFeedbackVectorOsrUrgency(MacroAssembler* masm,
                                    Register feedback_vector, Register scratch) {
   __ movb(scratch,
           FieldOperand(feedback_vector, FeedbackVector::kOsrStateOffset));
-  __ andb(scratch,
-          Immediate(FeedbackVector::MaybeHasOptimizedOsrCodeBit::kMask));
+  __ andb(scratch, Immediate(~FeedbackVector::OsrUrgencyBits::kMask));
   __ movb(FieldOperand(feedback_vector, FeedbackVector::kOsrStateOffset),
           scratch);
 }
diff --git a/src/interpreter/interpreter-generator.cc b/src/interpreter/interpreter-generator.cc
index 4879c545dab..e25402dbecd 100644
--- a/src/interpreter/interpreter-generator.cc
+++ b/src/interpreter/interpreter-generator.cc
@@ -2206,8 +2206,11 @@ IGNITION_HANDLER(JumpLoop, InterpreterAssembler) {
   Label maybe_osr_because_osr_state(this, Label::kDeferred);
   // The quick initial OSR check. If it passes, we proceed on to more expensive
   // OSR logic.
-  static_assert(FeedbackVector::MaybeHasOptimizedOsrCodeBit::encode(true) >
+  static_assert(FeedbackVector::MaybeHasMaglevOsrCodeBit::encode(true) >
                 FeedbackVector::kMaxOsrUrgency);
+  static_assert(FeedbackVector::MaybeHasTurbofanOsrCodeBit::encode(true) >
+                FeedbackVector::kMaxOsrUrgency);
+
   GotoIfNot(Uint32GreaterThanOrEqual(loop_depth, osr_state),
             &maybe_osr_because_osr_state);
 
diff --git a/src/maglev/maglev-ir.cc b/src/maglev/maglev-ir.cc
index 6dba1e10e9a..c142f1eda6e 100644
--- a/src/maglev/maglev-ir.cc
+++ b/src/maglev/maglev-ir.cc
@@ -5466,16 +5466,18 @@ void TryOnStackReplacement::GenerateCode(MaglevAssembler* masm,
 
   ZoneLabelRef no_code_for_osr(masm);
 
-  if (masm->compilation_info()->toplevel_is_osr()) {
-    // TODO(olivf) The maybe_has_optimized_code bit is guaranteed to be set
-    // since we have optimized maglev code -- but are waiting for TF. Thus we
-    // have to go into the slow case more than neccessary.
-    __ DecodeField<FeedbackVector::OsrUrgencyBits>(osr_state);
+  if (v8_flags.maglev_osr) {
+    // In case we use maglev_osr, we need to explicitly know if there is
+    // turbofan code waiting for us (i.e., ignore the MaybeHasMaglevOsrCodeBit).
+    __ DecodeField<
+        base::BitFieldUnion<FeedbackVector::OsrUrgencyBits,
+                            FeedbackVector::MaybeHasTurbofanOsrCodeBit>>(
+        osr_state);
   }
 
   // The quick initial OSR check. If it passes, we proceed on to more
   // expensive OSR logic.
-  static_assert(FeedbackVector::MaybeHasOptimizedOsrCodeBit::encode(true) >
+  static_assert(FeedbackVector::MaybeHasTurbofanOsrCodeBit::encode(true) >
                 FeedbackVector::kMaxOsrUrgency);
   __ CompareInt32(osr_state, loop_depth_);
   __ JumpToDeferredIf(kUnsignedGreaterThan, AttemptOnStackReplacement,
diff --git a/src/objects/feedback-vector-inl.h b/src/objects/feedback-vector-inl.h
index 978bab1e4b5..f7fe5deb6cf 100644
--- a/src/objects/feedback-vector-inl.h
+++ b/src/objects/feedback-vector-inl.h
@@ -139,11 +139,26 @@ void FeedbackVector::RequestOsrAtNextOpportunity() {
 void FeedbackVector::reset_osr_state() { set_osr_state(0); }
 
 bool FeedbackVector::maybe_has_optimized_osr_code() const {
-  return MaybeHasOptimizedOsrCodeBit::decode(osr_state());
+  return maybe_has_maglev_osr_code() || maybe_has_turbofan_osr_code();
 }
 
-void FeedbackVector::set_maybe_has_optimized_osr_code(bool value) {
-  set_osr_state(MaybeHasOptimizedOsrCodeBit::update(osr_state(), value));
+bool FeedbackVector::maybe_has_maglev_osr_code() const {
+  return MaybeHasMaglevOsrCodeBit::decode(osr_state());
+}
+
+bool FeedbackVector::maybe_has_turbofan_osr_code() const {
+  return MaybeHasTurbofanOsrCodeBit::decode(osr_state());
+}
+
+void FeedbackVector::set_maybe_has_optimized_osr_code(bool value,
+                                                      CodeKind code_kind) {
+  if (code_kind == CodeKind::MAGLEV) {
+    CHECK(v8_flags.maglev_osr);
+    set_osr_state(MaybeHasMaglevOsrCodeBit::update(osr_state(), value));
+  } else {
+    CHECK_EQ(code_kind, CodeKind::TURBOFAN);
+    set_osr_state(MaybeHasTurbofanOsrCodeBit::update(osr_state(), value));
+  }
 }
 
 Code FeedbackVector::optimized_code() const {
diff --git a/src/objects/feedback-vector.cc b/src/objects/feedback-vector.cc
index ebfd4bbb642..224de27f10c 100644
--- a/src/objects/feedback-vector.cc
+++ b/src/objects/feedback-vector.cc
@@ -400,7 +400,7 @@ void FeedbackVector::SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot,
     return;
   }
   Set(slot, HeapObjectReference::Weak(code));
-  set_maybe_has_optimized_osr_code(true);
+  set_maybe_has_optimized_osr_code(true, code.kind());
 }
 
 void FeedbackVector::reset_tiering_state() {
@@ -419,7 +419,8 @@ void FeedbackVector::reset_flags() {
             MaybeHasMaglevCodeBit::encode(false) |
             MaybeHasTurbofanCodeBit::encode(false) |
             OsrTieringStateBit::encode(TieringState::kNone) |
-            MaybeHasOptimizedOsrCodeBit::encode(false));
+            MaybeHasMaglevOsrCodeBit::encode(false) |
+            MaybeHasTurbofanOsrCodeBit::encode(false));
 }
 
 TieringState FeedbackVector::osr_tiering_state() {
diff --git a/src/objects/feedback-vector.h b/src/objects/feedback-vector.h
index 1772a42ee45..9a4fa7e07bc 100644
--- a/src/objects/feedback-vector.h
+++ b/src/objects/feedback-vector.h
@@ -241,12 +241,13 @@ class FeedbackVector
   inline void RequestOsrAtNextOpportunity();
 
   // Whether this vector may contain cached optimized osr code for *any* slot.
-  // Represented internally as a bit that can be efficiently checked by
-  // generated code. May diverge from the state of the world; the invariant is
-  // that if `maybe_has_optimized_osr_code` is false, no optimized osr code
+  // May diverge from the state of the world; the invariant is that if
+  // `maybe_has_(maglev|turbofan)_osr_code` is false, no optimized osr code
   // exists.
+  inline bool maybe_has_maglev_osr_code() const;
+  inline bool maybe_has_turbofan_osr_code() const;
   inline bool maybe_has_optimized_osr_code() const;
-  inline void set_maybe_has_optimized_osr_code(bool value);
+  inline void set_maybe_has_optimized_osr_code(bool value, CodeKind code_kind);
 
   // The `osr_state` contains the osr_urgency and maybe_has_optimized_osr_code.
   inline void reset_osr_state();
diff --git a/src/objects/feedback-vector.tq b/src/objects/feedback-vector.tq
index a7238971dac..b1287c57e4b 100644
--- a/src/objects/feedback-vector.tq
+++ b/src/objects/feedback-vector.tq
@@ -20,14 +20,16 @@ bitfield struct FeedbackVectorFlags extends uint16 {
 }
 
 bitfield struct OsrState extends uint8 {
-  // The layout is chosen s.t. osr_urgency and maybe_has_optimized_osr_code can
-  // be loaded with a single load (i.e. no masking required).
+  // The layout is chosen s.t. osr_urgency and
+  // maybe_has_(maglev|turbofan)_osr_code can be loaded with a single load
+  // (i.e. no masking required).
   osr_urgency: uint32: 3 bit;
-  maybe_has_optimized_osr_code: bool: 1 bit;
+  maybe_has_maglev_osr_code: bool: 1 bit;
+  maybe_has_turbofan_osr_code: bool: 1 bit;
   // In order to have fast OSR checks in Ignition and Sparkplug, these bits
   // should remain 0. That way, the OSR check can be implemented as a single
   // comparison.
-  dont_use_these_bits_unless_beneficial: uint32: 4 bit;
+  dont_use_these_bits_unless_beneficial: uint32: 3 bit;
 }
 
 @generateBodyDescriptor
-- 
2.35.1

