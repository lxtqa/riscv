From 1dcaf48c94e838d3815d4cb8ca43ba4e4fcce3e1 Mon Sep 17 00:00:00 2001
From: Lu Yahan <yahan@iscas.ac.cn>
Date: Wed, 15 Feb 2023 09:45:22 +0800
Subject: [PATCH] [riscv] Remove duplicate declaration in regexp macro
 assembler

Also, move the code introduced by https://chromium-review.googlesource.com/c/v8/v8/+/4230306 to the end of the file. It references to LLVM so need to be distinguished from other part.

Change-Id: Ib8b6efd64920bdd9618264d732fb3818a75a4fd7
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4252497
Commit-Queue: Yahan Lu <yahan@iscas.ac.cn>
Reviewed-by: ji qiu <qiuji@iscas.ac.cn>
Auto-Submit: Yahan Lu <yahan@iscas.ac.cn>
Cr-Commit-Position: refs/heads/main@{#85823}
---
 src/codegen/riscv/assembler-riscv.cc          | 819 +++++++++---------
 .../riscv/regexp-macro-assembler-riscv.h      |   3 -
 2 files changed, 403 insertions(+), 419 deletions(-)

diff --git a/src/codegen/riscv/assembler-riscv.cc b/src/codegen/riscv/assembler-riscv.cc
index 06bdbe39c74..add788ebe9d 100644
--- a/src/codegen/riscv/assembler-riscv.cc
+++ b/src/codegen/riscv/assembler-riscv.cc
@@ -1062,440 +1062,106 @@ void Assembler::GeneralLi(Register rd, int64_t imm) {
   }
 }
 
-//===- RISCVMatInt.cpp - Immediate materialisation -------------*- C++
-//-*--===//
-//
-//  Part of the LLVM Project, under the Apache License v2.0 with LLVM
-//  Exceptions. See https://llvm.org/LICENSE.txt for license information.
-//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
-//
-//===----------------------------------------------------------------------===//
-void Assembler::RecursiveLi(Register rd, int64_t val) {
-  if (val > 0 && RecursiveLiImplCount(val) > 2) {
-    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
-    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
-    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
-    if (countFillZero < RecursiveLiImplCount(val)) {
-      RecursiveLiImpl(rd, ShiftedVal);
-      srli(rd, rd, LeadingZeros);
-      return;
-    }
-  }
-  RecursiveLiImpl(rd, val);
+void Assembler::li_ptr(Register rd, int64_t imm) {
+  // Initialize rd with an address
+  // Pointers are 48 bits
+  // 6 fixed instructions are generated
+  DCHECK_EQ((imm & 0xfff0000000000000ll), 0);
+  int64_t a6 = imm & 0x3f;                      // bits 0:5. 6 bits
+  int64_t b11 = (imm >> 6) & 0x7ff;             // bits 6:11. 11 bits
+  int64_t high_31 = (imm >> 17) & 0x7fffffff;   // 31 bits
+  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
+  int64_t low_12 = high_31 & 0xfff;             // 12 bits
+  lui(rd, (int32_t)high_20);
+  addi(rd, rd, low_12);  // 31 bits in rd.
+  slli(rd, rd, 11);      // Space for next 11 bis
+  ori(rd, rd, b11);      // 11 bits are put in. 42 bit in rd
+  slli(rd, rd, 6);       // Space for next 6 bits
+  ori(rd, rd, a6);       // 6 bits are put in. 48 bis in rd
 }
 
-int Assembler::RecursiveLiCount(int64_t val) {
-  if (val > 0 && RecursiveLiImplCount(val) > 2) {
-    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
-    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
-    // Fill in the bits that will be shifted out with 1s. An example where
-    // this helps is trailing one masks with 32 or more ones. This will
-    // generate ADDI -1 and an SRLI.
-    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
-    if (countFillZero < RecursiveLiImplCount(val)) {
-      return countFillZero;
-    }
-  }
-  return RecursiveLiImplCount(val);
+void Assembler::li_constant(Register rd, int64_t imm) {
+  DEBUG_PRINTF("\tli_constant(%d, %lx <%ld>)\n", ToNumber(rd), imm, imm);
+  lui(rd, (imm + (1LL << 47) + (1LL << 35) + (1LL << 23) + (1LL << 11)) >>
+              48);  // Bits 63:48
+  addiw(rd, rd,
+        (imm + (1LL << 35) + (1LL << 23) + (1LL << 11)) << 16 >>
+            52);  // Bits 47:36
+  slli(rd, rd, 12);
+  addi(rd, rd, (imm + (1LL << 23) + (1LL << 11)) << 28 >> 52);  // Bits 35:24
+  slli(rd, rd, 12);
+  addi(rd, rd, (imm + (1LL << 11)) << 40 >> 52);  // Bits 23:12
+  slli(rd, rd, 12);
+  addi(rd, rd, imm << 52 >> 52);  // Bits 11:0
 }
 
-void Assembler::RecursiveLiImpl(Register rd, int64_t Val) {
-  if (is_int32(Val)) {
-    // Depending on the active bits in the immediate Value v, the following
-    // instruction sequences are emitted:
-    //
-    // v == 0                        : ADDI
-    // v[0,12) != 0 && v[12,32) == 0 : ADDI
-    // v[0,12) == 0 && v[12,32) != 0 : LUI
-    // v[0,32) != 0                  : LUI+ADDI(W)
-    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
-    int64_t Lo12 = Val << 52 >> 52;
-
-    if (Hi20) {
-      lui(rd, (int32_t)Hi20);
-    }
-
-    if (Lo12 || Hi20 == 0) {
-      if (Hi20) {
-        addiw(rd, rd, Lo12);
-      } else {
-        addi(rd, zero_reg, Lo12);
-      }
-    }
-    return;
-  }
-
-  // In the worst case, for a full 64-bit constant, a sequence of 8
-  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
-  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
-  // up to 32 bits while the following ADDI instructions contribute up to 12
-  // bits each.
-  //
-  // On the first glance, implementing this seems to be possible by simply
-  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
-  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
-  // to the fact that ADDI performs a sign extended addition, doing it like
-  // that would only be possible when at most 11 bits of the ADDI instructions
-  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
-  // actually requires that the constant is processed starting with the least
-  // significant bit.
-  //
-  // In the following, constants are processed from LSB to MSB but instruction
-  // emission is performed from MSB to LSB by recursively calling
-  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
-  // from the constant and the optimal shift amount, which can be greater than
-  // 12 bits if the constant is sparse, is determined. Then, the shifted
-  // remaining constant is processed recursively and gets emitted as soon as
-  // it fits into 32 bits. The emission of the shifts and additions is
-  // subsequently performed when the recursion returns.
-
-  int64_t Lo12 = Val << 52 >> 52;
-  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
-  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
-  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);
-
-  // If the remaining bits don't fit in 12 bits, we might be able to reduce
-  // the shift amount in order to use LUI which will zero the lower 12 bits.
-  bool Unsigned = false;
-  if (ShiftAmount > 12 && !is_int12(Hi52)) {
-    if (is_int32((uint64_t)Hi52 << 12)) {
-      // Reduce the shift amount and add zeros to the LSBs so it will match
-      // LUI.
-      ShiftAmount -= 12;
-      Hi52 = (uint64_t)Hi52 << 12;
+#elif V8_TARGET_ARCH_RISCV32
+void Assembler::RV_li(Register rd, int32_t imm) {
+  int32_t high_20 = ((imm + 0x800) >> 12);
+  int32_t low_12 = imm & 0xfff;
+  if (high_20) {
+    lui(rd, high_20);
+    if (low_12) {
+      addi(rd, rd, low_12);
     }
-  }
-  RecursiveLi(rd, Hi52);
-
-  if (Unsigned) {
   } else {
-    slli(rd, rd, ShiftAmount);
-  }
-  if (Lo12) {
-    addi(rd, rd, Lo12);
+    addi(rd, zero_reg, low_12);
   }
 }
 
-int Assembler::RecursiveLiImplCount(int64_t Val) {
+int Assembler::RV_li_count(int32_t imm, bool is_get_temp_reg) {
   int count = 0;
-  if (is_int32(Val)) {
-    // Depending on the active bits in the immediate Value v, the following
-    // instruction sequences are emitted:
-    //
-    // v == 0                        : ADDI
-    // v[0,12) != 0 && v[12,32) == 0 : ADDI
-    // v[0,12) == 0 && v[12,32) != 0 : LUI
-    // v[0,32) != 0                  : LUI+ADDI(W)
-    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
-    int64_t Lo12 = Val << 52 >> 52;
-
-    if (Hi20) {
-      // lui(rd, (int32_t)Hi20);
-      count++;
-    }
-
-    if (Lo12 || Hi20 == 0) {
-      //   unsigned AddiOpc = (IsRV64 && Hi20) ? RISCV::ADDIW : RISCV::ADDI;
-      //   Res.push_back(RISCVMatInt::Inst(AddiOpc, Lo12));
+  // imitate Assembler::RV_li
+  int32_t high_20 = ((imm + 0x800) >> 12);
+  int32_t low_12 = imm & 0xfff;
+  if (high_20) {
+    count++;
+    if (low_12) {
       count++;
     }
-    return count;
+  } else {
+    // if high_20 is 0, always need one instruction to load the low_12 bit
+    count++;
   }
 
-  // In the worst case, for a full 64-bit constant, a sequence of 8
-  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
-  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
-  // up to 32 bits while the following ADDI instructions contribute up to 12
-  // bits each.
-  //
-  // On the first glance, implementing this seems to be possible by simply
-  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
-  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
-  // to the fact that ADDI performs a sign extended addition, doing it like
-  // that would only be possible when at most 11 bits of the ADDI instructions
-  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
-  // actually requires that the constant is processed starting with the least
-  // significant bit.
-  //
-  // In the following, constants are processed from LSB to MSB but instruction
-  // emission is performed from MSB to LSB by recursively calling
-  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
-  // from the constant and the optimal shift amount, which can be greater than
-  // 12 bits if the constant is sparse, is determined. Then, the shifted
-  // remaining constant is processed recursively and gets emitted as soon as
-  // it fits into 32 bits. The emission of the shifts and additions is
-  // subsequently performed when the recursion returns.
+  return count;
+}
 
-  int64_t Lo12 = Val << 52 >> 52;
-  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
-  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
-  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);
+void Assembler::li_ptr(Register rd, int32_t imm) {
+  // Initialize rd with an address
+  // Pointers are 32 bits
+  // 2 fixed instructions are generated
+  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
+  int32_t low_12 = imm & 0xfff;             // bits11:0
+  lui(rd, high_20);
+  addi(rd, rd, low_12);
+}
 
-  // If the remaining bits don't fit in 12 bits, we might be able to reduce
-  // the shift amount in order to use LUI which will zero the lower 12 bits.
-  bool Unsigned = false;
-  if (ShiftAmount > 12 && !is_int12(Hi52)) {
-    if (is_int32((uint64_t)Hi52 << 12)) {
-      // Reduce the shift amount and add zeros to the LSBs so it will match
-      // LUI.
-      ShiftAmount -= 12;
-      Hi52 = (uint64_t)Hi52 << 12;
-    }
-    // else if (isUInt<32>((uint64_t)Hi52 << 12) &&
-    //            ActiveFeatures[RISCV::FeatureStdExtZba]) {
-    //   // Reduce the shift amount and add zeros to the LSBs so it will match
-    //   // LUI, then shift left with SLLI.UW to clear the upper 32 set bits.
-    //   ShiftAmount -= 12;
-    //   Hi52 = ((uint64_t)Hi52 << 12) | (0xffffffffull << 32);
-    //   Unsigned = true;
-    // }
-  }
+void Assembler::li_constant(Register rd, int32_t imm) {
+  DEBUG_PRINTF("\tli_constant(%d, %x <%d>)\n", ToNumber(rd), imm, imm);
+  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
+  int32_t low_12 = imm & 0xfff;             // bits11:0
+  lui(rd, high_20);
+  addi(rd, rd, low_12);
+}
+#endif
 
-  //   // Try to use SLLIUW for Hi52 when it is uint32 but not int32.
-  //   if (isUInt<32>((uint64_t)Hi52) && !isInt<32>((uint64_t)Hi52) &&
-  //       ActiveFeatures[RISCV::FeatureStdExtZba]) {
-  //     // Use LUI+ADDI or LUI to compose, then clear the upper 32 bits with
-  //     SLLIUW. Hi52 = ((uint64_t)Hi52) | (0xffffffffull << 32); Unsigned =
-  //     true;
-  //   }
+// Break / Trap instructions.
+void Assembler::break_(uint32_t code, bool break_as_stop) {
+  // We need to invalidate breaks that could be stops as well because the
+  // simulator expects a char pointer after the stop instruction.
+  // See constants-mips.h for explanation.
+  DCHECK(
+      (break_as_stop && code <= kMaxStopCode && code > kMaxTracepointCode) ||
+      (!break_as_stop && (code > kMaxStopCode || code <= kMaxTracepointCode)));
 
-  count += RecursiveLiImplCount(Hi52);
-
-  if (Unsigned) {
-  } else {
-    // slli(rd, rd, ShiftAmount);
-    count++;
-  }
-  if (Lo12) {
-    // addi(rd, rd, Lo12);
-    count++;
-  }
-  return count;
-}
-
-int Assembler::GeneralLiCount(int64_t imm, bool is_get_temp_reg) {
-  int count = 0;
-  // imitate Assembler::RV_li
-  if (is_int32(imm + 0x800)) {
-    // 32-bit case. Maximum of 2 instructions generated
-    int64_t high_20 = ((imm + 0x800) >> 12);
-    int64_t low_12 = imm << 52 >> 52;
-    if (high_20) {
-      count++;
-      if (low_12) {
-        count++;
-      }
-    } else {
-      count++;
-    }
-    return count;
-  } else {
-    // 64-bit case: divide imm into two 32-bit parts, upper and lower
-    int64_t up_32 = imm >> 32;
-    int64_t low_32 = imm & 0xffffffffull;
-    // Check if a temporary register is available
-    if (is_get_temp_reg) {
-      // keep track of hardware behavior for lower part in sim_low
-      int64_t sim_low = 0;
-      // Build lower part
-      if (low_32 != 0) {
-        int64_t high_20 = ((low_32 + 0x800) >> 12);
-        int64_t low_12 = low_32 & 0xfff;
-        if (high_20) {
-          // Adjust to 20 bits for the case of overflow
-          high_20 &= 0xfffff;
-          sim_low = ((high_20 << 12) << 32) >> 32;
-          count++;
-          if (low_12) {
-            sim_low += (low_12 << 52 >> 52) | low_12;
-            count++;
-          }
-        } else {
-          sim_low = low_12;
-          count++;
-        }
-      }
-      if (sim_low & 0x100000000) {
-        // Bit 31 is 1. Either an overflow or a negative 64 bit
-        if (up_32 == 0) {
-          // Positive number, but overflow because of the add 0x800
-          count++;
-          count++;
-          return count;
-        }
-        // low_32 is a negative 64 bit after the build
-        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
-      }
-      if (up_32 == 0) {
-        return count;
-      }
-      int64_t high_20 = (up_32 + 0x800) >> 12;
-      int64_t low_12 = up_32 & 0xfff;
-      if (high_20) {
-        // Adjust to 20 bits for the case of overflow
-        high_20 &= 0xfffff;
-        count++;
-        if (low_12) {
-          count++;
-        }
-      } else {
-        count++;
-      }
-      // Put it at the bgining of register
-      count++;
-      if (low_32 != 0) {
-        count++;
-      }
-      return count;
-    }
-    // No temp register. Build imm in rd.
-    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
-    // parts to the upper part by doing shift and add.
-    // First build upper part in rd.
-    int64_t high_20 = (up_32 + 0x800) >> 12;
-    int64_t low_12 = up_32 & 0xfff;
-    if (high_20) {
-      // Adjust to 20 bits for the case of overflow
-      high_20 &= 0xfffff;
-      count++;
-      if (low_12) {
-        count++;
-      }
-    } else {
-      count++;
-    }
-    // upper part already in rd. Each part to be added to rd, has maximum of 11
-    // bits, and always starts with a 1. rd is shifted by the size of the part
-    // plus the number of zeros between the parts. Each part is added after the
-    // left shift.
-    uint32_t mask = 0x80000000;
-    int32_t i;
-    for (i = 0; i < 32; i++) {
-      if ((low_32 & mask) == 0) {
-        mask >>= 1;
-        if (i == 31) {
-          // rest is zero
-          count++;
-        }
-        continue;
-      }
-      // The first 1 seen
-      if ((i + 11) < 32) {
-        // Pick 11 bits
-        count++;
-        count++;
-        i += 10;
-        mask >>= 11;
-      } else {
-        count++;
-        count++;
-        break;
-      }
-    }
-  }
-  return count;
-}
-
-void Assembler::li_ptr(Register rd, int64_t imm) {
-  // Initialize rd with an address
-  // Pointers are 48 bits
-  // 6 fixed instructions are generated
-  DCHECK_EQ((imm & 0xfff0000000000000ll), 0);
-  int64_t a6 = imm & 0x3f;                      // bits 0:5. 6 bits
-  int64_t b11 = (imm >> 6) & 0x7ff;             // bits 6:11. 11 bits
-  int64_t high_31 = (imm >> 17) & 0x7fffffff;   // 31 bits
-  int64_t high_20 = ((high_31 + 0x800) >> 12);  // 19 bits
-  int64_t low_12 = high_31 & 0xfff;             // 12 bits
-  lui(rd, (int32_t)high_20);
-  addi(rd, rd, low_12);  // 31 bits in rd.
-  slli(rd, rd, 11);      // Space for next 11 bis
-  ori(rd, rd, b11);      // 11 bits are put in. 42 bit in rd
-  slli(rd, rd, 6);       // Space for next 6 bits
-  ori(rd, rd, a6);       // 6 bits are put in. 48 bis in rd
-}
-
-void Assembler::li_constant(Register rd, int64_t imm) {
-  DEBUG_PRINTF("\tli_constant(%d, %lx <%ld>)\n", ToNumber(rd), imm, imm);
-  lui(rd, (imm + (1LL << 47) + (1LL << 35) + (1LL << 23) + (1LL << 11)) >>
-              48);  // Bits 63:48
-  addiw(rd, rd,
-        (imm + (1LL << 35) + (1LL << 23) + (1LL << 11)) << 16 >>
-            52);  // Bits 47:36
-  slli(rd, rd, 12);
-  addi(rd, rd, (imm + (1LL << 23) + (1LL << 11)) << 28 >> 52);  // Bits 35:24
-  slli(rd, rd, 12);
-  addi(rd, rd, (imm + (1LL << 11)) << 40 >> 52);  // Bits 23:12
-  slli(rd, rd, 12);
-  addi(rd, rd, imm << 52 >> 52);  // Bits 11:0
-}
-
-#elif V8_TARGET_ARCH_RISCV32
-void Assembler::RV_li(Register rd, int32_t imm) {
-  int32_t high_20 = ((imm + 0x800) >> 12);
-  int32_t low_12 = imm & 0xfff;
-  if (high_20) {
-    lui(rd, high_20);
-    if (low_12) {
-      addi(rd, rd, low_12);
-    }
-  } else {
-    addi(rd, zero_reg, low_12);
-  }
-}
-
-int Assembler::RV_li_count(int32_t imm, bool is_get_temp_reg) {
-  int count = 0;
-  // imitate Assembler::RV_li
-  int32_t high_20 = ((imm + 0x800) >> 12);
-  int32_t low_12 = imm & 0xfff;
-  if (high_20) {
-    count++;
-    if (low_12) {
-      count++;
-    }
-  } else {
-    // if high_20 is 0, always need one instruction to load the low_12 bit
-    count++;
-  }
-
-  return count;
-}
-
-void Assembler::li_ptr(Register rd, int32_t imm) {
-  // Initialize rd with an address
-  // Pointers are 32 bits
-  // 2 fixed instructions are generated
-  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
-  int32_t low_12 = imm & 0xfff;             // bits11:0
-  lui(rd, high_20);
-  addi(rd, rd, low_12);
-}
-
-void Assembler::li_constant(Register rd, int32_t imm) {
-  DEBUG_PRINTF("\tli_constant(%d, %x <%d>)\n", ToNumber(rd), imm, imm);
-  int32_t high_20 = ((imm + 0x800) >> 12);  // bits31:12
-  int32_t low_12 = imm & 0xfff;             // bits11:0
-  lui(rd, high_20);
-  addi(rd, rd, low_12);
-}
-#endif
-
-// Break / Trap instructions.
-void Assembler::break_(uint32_t code, bool break_as_stop) {
-  // We need to invalidate breaks that could be stops as well because the
-  // simulator expects a char pointer after the stop instruction.
-  // See constants-mips.h for explanation.
-  DCHECK(
-      (break_as_stop && code <= kMaxStopCode && code > kMaxTracepointCode) ||
-      (!break_as_stop && (code > kMaxStopCode || code <= kMaxTracepointCode)));
-
-  // since ebreak does not allow additional immediate field, we use the
-  // immediate field of lui instruction immediately following the ebreak to
-  // encode the "code" info
-  ebreak();
-  DCHECK(is_uint20(code));
-  lui(zero_reg, code);
-}
+  // since ebreak does not allow additional immediate field, we use the
+  // immediate field of lui instruction immediately following the ebreak to
+  // encode the "code" info
+  ebreak();
+  DCHECK(is_uint20(code));
+  lui(zero_reg, code);
+}
 
 void Assembler::stop(uint32_t code) {
   DCHECK_GT(code, kMaxWatchpointCode);
@@ -2146,5 +1812,326 @@ const size_t ConstantPool::kOpportunityDistToPool32 = 64 * KB;
 const size_t ConstantPool::kOpportunityDistToPool64 = 64 * KB;
 const size_t ConstantPool::kApproxMaxEntryCount = 512;
 
+#if defined(V8_TARGET_ARCH_RISCV64)
+// LLVM Code
+//===- RISCVMatInt.cpp - Immediate materialisation -------------*- C++
+//-*--===//
+//
+//  Part of the LLVM Project, under the Apache License v2.0 with LLVM
+//  Exceptions. See https://llvm.org/LICENSE.txt for license information.
+//  SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+void Assembler::RecursiveLi(Register rd, int64_t val) {
+  if (val > 0 && RecursiveLiImplCount(val) > 2) {
+    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
+    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
+    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
+    if (countFillZero < RecursiveLiImplCount(val)) {
+      RecursiveLiImpl(rd, ShiftedVal);
+      srli(rd, rd, LeadingZeros);
+      return;
+    }
+  }
+  RecursiveLiImpl(rd, val);
+}
+
+int Assembler::RecursiveLiCount(int64_t val) {
+  if (val > 0 && RecursiveLiImplCount(val) > 2) {
+    unsigned LeadingZeros = base::bits::CountLeadingZeros((uint64_t)val);
+    uint64_t ShiftedVal = (uint64_t)val << LeadingZeros;
+    // Fill in the bits that will be shifted out with 1s. An example where
+    // this helps is trailing one masks with 32 or more ones. This will
+    // generate ADDI -1 and an SRLI.
+    int countFillZero = RecursiveLiImplCount(ShiftedVal) + 1;
+    if (countFillZero < RecursiveLiImplCount(val)) {
+      return countFillZero;
+    }
+  }
+  return RecursiveLiImplCount(val);
+}
+
+void Assembler::RecursiveLiImpl(Register rd, int64_t Val) {
+  if (is_int32(Val)) {
+    // Depending on the active bits in the immediate Value v, the following
+    // instruction sequences are emitted:
+    //
+    // v == 0                        : ADDI
+    // v[0,12) != 0 && v[12,32) == 0 : ADDI
+    // v[0,12) == 0 && v[12,32) != 0 : LUI
+    // v[0,32) != 0                  : LUI+ADDI(W)
+    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
+    int64_t Lo12 = Val << 52 >> 52;
+
+    if (Hi20) {
+      lui(rd, (int32_t)Hi20);
+    }
+
+    if (Lo12 || Hi20 == 0) {
+      if (Hi20) {
+        addiw(rd, rd, Lo12);
+      } else {
+        addi(rd, zero_reg, Lo12);
+      }
+    }
+    return;
+  }
+
+  // In the worst case, for a full 64-bit constant, a sequence of 8
+  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
+  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
+  // up to 32 bits while the following ADDI instructions contribute up to 12
+  // bits each.
+  //
+  // On the first glance, implementing this seems to be possible by simply
+  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
+  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
+  // to the fact that ADDI performs a sign extended addition, doing it like
+  // that would only be possible when at most 11 bits of the ADDI instructions
+  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
+  // actually requires that the constant is processed starting with the least
+  // significant bit.
+  //
+  // In the following, constants are processed from LSB to MSB but instruction
+  // emission is performed from MSB to LSB by recursively calling
+  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
+  // from the constant and the optimal shift amount, which can be greater than
+  // 12 bits if the constant is sparse, is determined. Then, the shifted
+  // remaining constant is processed recursively and gets emitted as soon as
+  // it fits into 32 bits. The emission of the shifts and additions is
+  // subsequently performed when the recursion returns.
+
+  int64_t Lo12 = Val << 52 >> 52;
+  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
+  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
+  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);
+
+  // If the remaining bits don't fit in 12 bits, we might be able to reduce
+  // the shift amount in order to use LUI which will zero the lower 12 bits.
+  bool Unsigned = false;
+  if (ShiftAmount > 12 && !is_int12(Hi52)) {
+    if (is_int32((uint64_t)Hi52 << 12)) {
+      // Reduce the shift amount and add zeros to the LSBs so it will match
+      // LUI.
+      ShiftAmount -= 12;
+      Hi52 = (uint64_t)Hi52 << 12;
+    }
+  }
+  RecursiveLi(rd, Hi52);
+
+  if (Unsigned) {
+  } else {
+    slli(rd, rd, ShiftAmount);
+  }
+  if (Lo12) {
+    addi(rd, rd, Lo12);
+  }
+}
+
+int Assembler::RecursiveLiImplCount(int64_t Val) {
+  int count = 0;
+  if (is_int32(Val)) {
+    // Depending on the active bits in the immediate Value v, the following
+    // instruction sequences are emitted:
+    //
+    // v == 0                        : ADDI
+    // v[0,12) != 0 && v[12,32) == 0 : ADDI
+    // v[0,12) == 0 && v[12,32) != 0 : LUI
+    // v[0,32) != 0                  : LUI+ADDI(W)
+    int64_t Hi20 = ((Val + 0x800) >> 12) & 0xFFFFF;
+    int64_t Lo12 = Val << 52 >> 52;
+
+    if (Hi20) {
+      // lui(rd, (int32_t)Hi20);
+      count++;
+    }
+
+    if (Lo12 || Hi20 == 0) {
+      //   unsigned AddiOpc = (IsRV64 && Hi20) ? RISCV::ADDIW : RISCV::ADDI;
+      //   Res.push_back(RISCVMatInt::Inst(AddiOpc, Lo12));
+      count++;
+    }
+    return count;
+  }
+
+  // In the worst case, for a full 64-bit constant, a sequence of 8
+  // instructions (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be
+  // emitted. Note that the first two instructions (LUI+ADDIW) can contribute
+  // up to 32 bits while the following ADDI instructions contribute up to 12
+  // bits each.
+  //
+  // On the first glance, implementing this seems to be possible by simply
+  // emitting the most significant 32 bits (LUI+ADDIW) followed by as many
+  // left shift (SLLI) and immediate additions (ADDI) as needed. However, due
+  // to the fact that ADDI performs a sign extended addition, doing it like
+  // that would only be possible when at most 11 bits of the ADDI instructions
+  // are used. Using all 12 bits of the ADDI instructions, like done by GAS,
+  // actually requires that the constant is processed starting with the least
+  // significant bit.
+  //
+  // In the following, constants are processed from LSB to MSB but instruction
+  // emission is performed from MSB to LSB by recursively calling
+  // generateInstSeq. In each recursion, first the lowest 12 bits are removed
+  // from the constant and the optimal shift amount, which can be greater than
+  // 12 bits if the constant is sparse, is determined. Then, the shifted
+  // remaining constant is processed recursively and gets emitted as soon as
+  // it fits into 32 bits. The emission of the shifts and additions is
+  // subsequently performed when the recursion returns.
+
+  int64_t Lo12 = Val << 52 >> 52;
+  int64_t Hi52 = ((uint64_t)Val + 0x800ull) >> 12;
+  int ShiftAmount = 12 + base::bits::CountTrailingZeros((uint64_t)Hi52);
+  Hi52 = signExtend(Hi52 >> (ShiftAmount - 12), 64 - ShiftAmount);
+
+  // If the remaining bits don't fit in 12 bits, we might be able to reduce
+  // the shift amount in order to use LUI which will zero the lower 12 bits.
+  bool Unsigned = false;
+  if (ShiftAmount > 12 && !is_int12(Hi52)) {
+    if (is_int32((uint64_t)Hi52 << 12)) {
+      // Reduce the shift amount and add zeros to the LSBs so it will match
+      // LUI.
+      ShiftAmount -= 12;
+      Hi52 = (uint64_t)Hi52 << 12;
+    }
+  }
+
+  count += RecursiveLiImplCount(Hi52);
+
+  if (Unsigned) {
+  } else {
+    // slli(rd, rd, ShiftAmount);
+    count++;
+  }
+  if (Lo12) {
+    // addi(rd, rd, Lo12);
+    count++;
+  }
+  return count;
+}
+
+int Assembler::GeneralLiCount(int64_t imm, bool is_get_temp_reg) {
+  int count = 0;
+  // imitate Assembler::RV_li
+  if (is_int32(imm + 0x800)) {
+    // 32-bit case. Maximum of 2 instructions generated
+    int64_t high_20 = ((imm + 0x800) >> 12);
+    int64_t low_12 = imm << 52 >> 52;
+    if (high_20) {
+      count++;
+      if (low_12) {
+        count++;
+      }
+    } else {
+      count++;
+    }
+    return count;
+  } else {
+    // 64-bit case: divide imm into two 32-bit parts, upper and lower
+    int64_t up_32 = imm >> 32;
+    int64_t low_32 = imm & 0xffffffffull;
+    // Check if a temporary register is available
+    if (is_get_temp_reg) {
+      // keep track of hardware behavior for lower part in sim_low
+      int64_t sim_low = 0;
+      // Build lower part
+      if (low_32 != 0) {
+        int64_t high_20 = ((low_32 + 0x800) >> 12);
+        int64_t low_12 = low_32 & 0xfff;
+        if (high_20) {
+          // Adjust to 20 bits for the case of overflow
+          high_20 &= 0xfffff;
+          sim_low = ((high_20 << 12) << 32) >> 32;
+          count++;
+          if (low_12) {
+            sim_low += (low_12 << 52 >> 52) | low_12;
+            count++;
+          }
+        } else {
+          sim_low = low_12;
+          count++;
+        }
+      }
+      if (sim_low & 0x100000000) {
+        // Bit 31 is 1. Either an overflow or a negative 64 bit
+        if (up_32 == 0) {
+          // Positive number, but overflow because of the add 0x800
+          count++;
+          count++;
+          return count;
+        }
+        // low_32 is a negative 64 bit after the build
+        up_32 = (up_32 - 0xffffffff) & 0xffffffff;
+      }
+      if (up_32 == 0) {
+        return count;
+      }
+      int64_t high_20 = (up_32 + 0x800) >> 12;
+      int64_t low_12 = up_32 & 0xfff;
+      if (high_20) {
+        // Adjust to 20 bits for the case of overflow
+        high_20 &= 0xfffff;
+        count++;
+        if (low_12) {
+          count++;
+        }
+      } else {
+        count++;
+      }
+      // Put it at the bgining of register
+      count++;
+      if (low_32 != 0) {
+        count++;
+      }
+      return count;
+    }
+    // No temp register. Build imm in rd.
+    // Build upper 32 bits first in rd. Divide lower 32 bits parts and add
+    // parts to the upper part by doing shift and add.
+    // First build upper part in rd.
+    int64_t high_20 = (up_32 + 0x800) >> 12;
+    int64_t low_12 = up_32 & 0xfff;
+    if (high_20) {
+      // Adjust to 20 bits for the case of overflow
+      high_20 &= 0xfffff;
+      count++;
+      if (low_12) {
+        count++;
+      }
+    } else {
+      count++;
+    }
+    // upper part already in rd. Each part to be added to rd, has maximum of
+    // 11 bits, and always starts with a 1. rd is shifted by the size of the
+    // part plus the number of zeros between the parts. Each part is added
+    // after the left shift.
+    uint32_t mask = 0x80000000;
+    int32_t i;
+    for (i = 0; i < 32; i++) {
+      if ((low_32 & mask) == 0) {
+        mask >>= 1;
+        if (i == 31) {
+          // rest is zero
+          count++;
+        }
+        continue;
+      }
+      // The first 1 seen
+      if ((i + 11) < 32) {
+        // Pick 11 bits
+        count++;
+        count++;
+        i += 10;
+        mask >>= 11;
+      } else {
+        count++;
+        count++;
+        break;
+      }
+    }
+  }
+  return count;
+}
+#endif
+
 }  // namespace internal
 }  // namespace v8
diff --git a/src/regexp/riscv/regexp-macro-assembler-riscv.h b/src/regexp/riscv/regexp-macro-assembler-riscv.h
index 18511516345..90a1d314cc7 100644
--- a/src/regexp/riscv/regexp-macro-assembler-riscv.h
+++ b/src/regexp/riscv/regexp-macro-assembler-riscv.h
@@ -159,9 +159,6 @@ class V8_EXPORT_PRIVATE RegExpMacroAssemblerRISCV
   void PushCallerSavedRegisters();
   void PopCallerSavedRegisters();
 
-  void CallCFunctionFromIrregexpCode(ExternalReference function,
-                                     int num_arguments);
-
   // Check whether preemption has been requested.
   void CheckPreemption();
 
-- 
2.35.1

