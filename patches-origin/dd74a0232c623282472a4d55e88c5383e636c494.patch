From dd74a0232c623282472a4d55e88c5383e636c494 Mon Sep 17 00:00:00 2001
From: Clemens Backes <clemensb@chromium.org>
Date: Fri, 13 May 2022 11:19:09 +0200
Subject: [PATCH] Replace STATIC_ASSERT with static_assert

Now that we require C++17 support, we can just use the standard
static_assert without message, instead of our STATIC_ASSERT macro.

R=leszeks@chromium.org

Bug: v8:12425
Change-Id: I1d4e39c310b533bcd3a4af33d027827e6c083afe
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3647353
Reviewed-by: Leszek Swirski <leszeks@chromium.org>
Reviewed-by: Hannes Payer <hpayer@chromium.org>
Commit-Queue: Clemens Backes <clemensb@chromium.org>
Cr-Commit-Position: refs/heads/main@{#80524}
---
 src/api/api-inl.h                             |   4 +-
 src/api/api.cc                                |  22 +--
 src/ast/ast-source-ranges.h                   |   2 +-
 src/ast/ast.h                                 |   8 +-
 src/ast/scopes.h                              |   2 +-
 src/base/address-region.h                     |   4 +-
 src/base/atomic-utils.h                       |  32 ++---
 src/base/atomicops.h                          |   2 +-
 src/base/bit-field.h                          |  10 +-
 src/base/bits-iterator.h                      |   2 +-
 src/base/bits.h                               |   6 +-
 src/base/bounds.h                             |   2 +-
 src/base/division-by-constant.cc              |   4 +-
 src/base/hashmap-entry.h                      |   4 +-
 src/base/hashmap.h                            |   4 +-
 src/base/macros.h                             |  15 +-
 src/base/platform/platform-posix.cc           |   8 +-
 src/base/platform/platform-win32.cc           |  24 ++--
 src/base/small-vector.h                       |   2 +-
 src/base/vector.h                             |   6 +-
 src/baseline/arm/baseline-assembler-arm-inl.h |   2 +-
 src/baseline/baseline-compiler.cc             |  12 +-
 .../loong64/baseline-assembler-loong64-inl.h  |   2 +-
 .../mips/baseline-assembler-mips-inl.h        |   2 +-
 .../mips64/baseline-assembler-mips64-inl.h    |   2 +-
 src/baseline/ppc/baseline-assembler-ppc-inl.h |   2 +-
 .../s390/baseline-assembler-s390-inl.h        |   2 +-
 src/builtins/arm/builtins-arm.cc              |  54 +++----
 src/builtins/arm64/builtins-arm64.cc          |  56 ++++----
 src/builtins/builtins-array-gen.cc            |  20 +--
 src/builtins/builtins-array.cc                |   4 +-
 src/builtins/builtins-async-gen.cc            |   2 +-
 src/builtins/builtins-bigint-gen.h            |   2 +-
 src/builtins/builtins-call-gen.cc             |  14 +-
 src/builtins/builtins-collections-gen.cc      |  14 +-
 src/builtins/builtins-constructor-gen.cc      |  16 +--
 src/builtins/builtins-constructor.h           |   2 +-
 src/builtins/builtins-handler-gen.cc          |   6 +-
 src/builtins/builtins-internal-gen.cc         |   2 +-
 src/builtins/builtins-intl-gen.cc             |   2 +-
 src/builtins/builtins-regexp-gen.cc           |  12 +-
 .../builtins-sharedarraybuffer-gen.cc         |  32 ++---
 src/builtins/builtins-string-gen.cc           |  14 +-
 src/builtins/builtins-typed-array-gen.cc      |   4 +-
 src/builtins/builtins.cc                      |   8 +-
 src/builtins/builtins.h                       |   6 +-
 src/builtins/ia32/builtins-ia32.cc            |  58 ++++----
 src/builtins/loong64/builtins-loong64.cc      |  48 +++----
 src/builtins/mips/builtins-mips.cc            |  48 +++----
 src/builtins/mips64/builtins-mips64.cc        |  50 +++----
 src/builtins/ppc/builtins-ppc.cc              |  52 +++----
 src/builtins/riscv64/builtins-riscv64.cc      |  50 +++----
 src/builtins/s390/builtins-s390.cc            |  52 +++----
 src/builtins/x64/builtins-x64.cc              |  56 ++++----
 src/codegen/arm/assembler-arm.h               |   8 +-
 .../arm/interface-descriptors-arm-inl.h       |   2 +-
 src/codegen/arm/macro-assembler-arm.cc        |  28 ++--
 src/codegen/arm64/assembler-arm64-inl.h       |   8 +-
 src/codegen/arm64/assembler-arm64.h           |   6 +-
 src/codegen/arm64/constants-arm64.h           |  12 +-
 .../arm64/interface-descriptors-arm64-inl.h   |   2 +-
 src/codegen/arm64/macro-assembler-arm64-inl.h |   2 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  26 ++--
 src/codegen/arm64/macro-assembler-arm64.h     |   2 +-
 src/codegen/arm64/register-arm64.h            |   2 +-
 src/codegen/arm64/utils-arm64.h               |   4 +-
 src/codegen/code-stub-assembler.cc            | 132 +++++++++---------
 src/codegen/code-stub-assembler.h             |  14 +-
 src/codegen/compilation-cache.cc              |   4 +-
 src/codegen/compiler.cc                       |   2 +-
 src/codegen/cpu-features.h                    |   2 +-
 src/codegen/external-reference-table.cc       |   2 +-
 src/codegen/external-reference-table.h        |   4 +-
 src/codegen/external-reference.cc             |   4 +-
 src/codegen/ia32/assembler-ia32.h             |   2 +-
 .../ia32/interface-descriptors-ia32-inl.h     |   2 +-
 src/codegen/ia32/macro-assembler-ia32.cc      |  20 +--
 src/codegen/ia32/macro-assembler-ia32.h       |   4 +-
 src/codegen/interface-descriptors-inl.h       |   4 +-
 src/codegen/interface-descriptors.h           |   2 +-
 src/codegen/loong64/assembler-loong64.h       |   2 +-
 src/codegen/loong64/constants-loong64.h       |   2 +-
 .../interface-descriptors-loong64-inl.h       |   2 +-
 .../loong64/macro-assembler-loong64.cc        |  34 ++---
 src/codegen/loong64/macro-assembler-loong64.h |   2 +-
 src/codegen/machine-type.h                    |   4 +-
 src/codegen/mips/assembler-mips.h             |   2 +-
 src/codegen/mips/constants-mips.h             |   2 +-
 .../mips/interface-descriptors-mips-inl.h     |   2 +-
 src/codegen/mips/macro-assembler-mips.cc      |  36 ++---
 src/codegen/mips64/assembler-mips64.h         |   2 +-
 src/codegen/mips64/constants-mips64.h         |   2 +-
 .../mips64/interface-descriptors-mips64-inl.h |   2 +-
 src/codegen/mips64/macro-assembler-mips64.cc  |  34 ++---
 src/codegen/mips64/macro-assembler-mips64.h   |   2 +-
 src/codegen/ppc/assembler-ppc.h               |   2 +-
 .../ppc/interface-descriptors-ppc-inl.h       |   2 +-
 src/codegen/ppc/macro-assembler-ppc.cc        |  36 ++---
 src/codegen/ppc/macro-assembler-ppc.h         |   8 +-
 src/codegen/register-base.h                   |   4 +-
 src/codegen/register-configuration.cc         |  12 +-
 src/codegen/reglist-base.h                    |   2 +-
 src/codegen/reloc-info.cc                     |   2 +-
 src/codegen/reloc-info.h                      |   2 +-
 src/codegen/riscv64/assembler-riscv64.h       |   2 +-
 src/codegen/riscv64/constants-riscv64.h       |   2 +-
 .../interface-descriptors-riscv64-inl.h       |   2 +-
 .../riscv64/macro-assembler-riscv64.cc        |  34 ++---
 src/codegen/riscv64/macro-assembler-riscv64.h |   2 +-
 src/codegen/s390/assembler-s390.h             |   2 +-
 .../s390/interface-descriptors-s390-inl.h     |   2 +-
 src/codegen/s390/macro-assembler-s390.cc      |  32 ++---
 src/codegen/s390/macro-assembler-s390.h       |   8 +-
 src/codegen/safepoint-table.cc                |  18 +--
 src/codegen/safepoint-table.h                 |   4 +-
 src/codegen/signature.h                       |   2 +-
 src/codegen/source-position.h                 |   2 +-
 src/codegen/x64/assembler-x64.h               |   2 +-
 .../x64/interface-descriptors-x64-inl.h       |   2 +-
 src/codegen/x64/macro-assembler-x64.cc        |  22 +--
 src/common/globals.h                          |  42 +++---
 src/compiler/access-builder.cc                |  10 +-
 src/compiler/access-info.cc                   |   2 +-
 src/compiler/allocation-builder-inl.h         |   2 +-
 .../backend/arm/code-generator-arm.cc         |   6 +-
 .../backend/arm64/code-generator-arm64.cc     |   6 +-
 .../arm64/instruction-selector-arm64.cc       |   4 +-
 .../backend/ia32/code-generator-ia32.cc       |   8 +-
 src/compiler/backend/instruction-selector.cc  |   4 +-
 src/compiler/backend/instruction.cc           |   2 +-
 src/compiler/backend/instruction.h            |  14 +-
 src/compiler/backend/spill-placer.cc          |   4 +-
 .../backend/x64/code-generator-x64.cc         |   8 +-
 src/compiler/bytecode-graph-builder.cc        |  60 ++++----
 src/compiler/code-assembler.h                 |   2 +-
 src/compiler/common-operator.h                |   2 +-
 src/compiler/compilation-dependencies.cc      |   2 +-
 src/compiler/effect-control-linearizer.cc     |  18 +--
 src/compiler/heap-refs.cc                     |   6 +-
 src/compiler/js-call-reducer.cc               |  28 ++--
 src/compiler/js-create-lowering.cc            |  48 +++----
 src/compiler/js-generic-lowering.cc           |  56 ++++----
 src/compiler/js-heap-broker.h                 |   4 +-
 src/compiler/js-inlining-heuristic.cc         |   2 +-
 src/compiler/js-inlining.cc                   |   2 +-
 src/compiler/js-intrinsic-lowering.cc         |   2 +-
 .../js-native-context-specialization.cc       |  12 +-
 src/compiler/js-operator.h                    |  12 +-
 src/compiler/js-typed-lowering.cc             |  10 +-
 src/compiler/memory-lowering.cc               |   2 +-
 src/compiler/node.cc                          |   4 +-
 src/compiler/operation-typer.cc               |   2 +-
 src/compiler/scheduler.cc                     |   2 +-
 src/compiler/turboshaft/assembler.h           |   4 +-
 src/compiler/turboshaft/graph.h               |   4 +-
 src/compiler/turboshaft/operations.h          |  12 +-
 src/compiler/typer.cc                         |   4 +-
 src/compiler/types.cc                         |   4 +-
 src/compiler/wasm-compiler.cc                 |  10 +-
 src/d8/d8.h                                   |   2 +-
 src/debug/debug-interface.cc                  |   2 +-
 src/debug/debug-wasm-objects.cc               |   2 +-
 src/debug/debug.cc                            |   2 +-
 src/debug/liveedit.cc                         |   2 +-
 src/deoptimizer/arm/deoptimizer-arm.cc        |   2 +-
 src/deoptimizer/deoptimize-reason.h           |   2 +-
 src/deoptimizer/deoptimizer.cc                |   2 +-
 src/deoptimizer/ppc/deoptimizer-ppc.cc        |   2 +-
 src/deoptimizer/s390/deoptimizer-s390.cc      |   2 +-
 src/deoptimizer/x64/deoptimizer-x64.cc        |   2 +-
 src/diagnostics/objects-debug.cc              |   4 +-
 src/diagnostics/unwinding-info-win64.cc       |   4 +-
 src/execution/arm/simulator-arm.cc            |   2 +-
 src/execution/arm64/frame-constants-arm64.cc  |   2 +-
 src/execution/arm64/simulator-arm64.cc        |   4 +-
 src/execution/arm64/simulator-arm64.h         |   8 +-
 src/execution/execution.cc                    |  14 +-
 src/execution/frames.cc                       |   2 +-
 src/execution/frames.h                        |  12 +-
 src/execution/isolate-data.h                  |  16 +--
 src/execution/isolate.cc                      |  22 +--
 src/execution/loong64/simulator-loong64.cc    |   2 +-
 src/execution/mips/simulator-mips.cc          |   2 +-
 src/execution/mips64/simulator-mips64.cc      |   2 +-
 src/execution/ppc/simulator-ppc.cc            |   4 +-
 src/execution/protectors.cc                   |   2 +-
 src/execution/riscv64/simulator-riscv64.cc    |   2 +-
 src/execution/riscv64/simulator-riscv64.h     |   8 +-
 src/execution/s390/simulator-s390.cc          |   4 +-
 src/execution/stack-guard.h                   |   2 +-
 src/execution/thread-local-top.h              |   2 +-
 src/execution/tiering-manager.cc              |   2 +-
 src/handles/global-handles.cc                 |   4 +-
 src/heap/allocation-result.h                  |   2 +-
 src/heap/basic-memory-chunk.cc                |  38 ++---
 src/heap/basic-memory-chunk.h                 |   2 +-
 src/heap/cppgc/allocation.cc                  |   2 +-
 src/heap/cppgc/caged-heap.cc                  |   4 +-
 src/heap/cppgc/heap-object-header.cc          |   6 +-
 src/heap/cppgc/heap-page.cc                   |   4 +-
 src/heap/cppgc/object-allocator.h             |   6 +-
 src/heap/cppgc/trace-event.h                  |   2 +-
 src/heap/factory-base.cc                      |   6 +-
 src/heap/factory.cc                           |  28 ++--
 src/heap/free-list.h                          |   4 +-
 src/heap/gc-tracer-inl.h                      |   2 +-
 src/heap/gc-tracer.cc                         |   4 +-
 src/heap/heap-inl.h                           |   2 +-
 src/heap/heap.cc                              |  22 +--
 src/heap/heap.h                               |  16 +--
 src/heap/large-spaces.cc                      |   6 +-
 src/heap/large-spaces.h                       |   2 +-
 src/heap/mark-compact.cc                      |   4 +-
 src/heap/marking-visitor-inl.h                |   2 +-
 src/heap/marking.h                            |   4 +-
 src/heap/remembered-set.h                     |   2 +-
 src/heap/setup-heap-internal.cc               |   2 +-
 src/heap/slot-set.h                           |  10 +-
 src/heap/spaces.cc                            |   4 +-
 src/heap/spaces.h                             |   8 +-
 src/ic/accessor-assembler.cc                  |  10 +-
 src/ic/handler-configuration.h                |  14 +-
 src/ic/ic.cc                                  |   2 +-
 src/ic/keyed-store-generic.cc                 |  12 +-
 src/ic/stub-cache.h                           |   4 +-
 src/ic/unary-op-assembler.cc                  |   2 +-
 src/init/bootstrapper.cc                      |  10 +-
 src/interpreter/bytecode-flags.h              |   2 +-
 src/interpreter/bytecode-generator.cc         |  10 +-
 src/interpreter/bytecodes.cc                  |   2 +-
 src/interpreter/bytecodes.h                   |  10 +-
 src/interpreter/interpreter-assembler.cc      |   6 +-
 src/interpreter/interpreter-intrinsics.h      |   2 +-
 src/json/json-parser.cc                       |   6 +-
 src/json/json-parser.h                        |   2 +-
 src/json/json-stringifier.cc                  |   2 +-
 src/maglev/maglev-ir.cc                       |   4 +-
 src/maglev/maglev-ir.h                        |  12 +-
 src/numbers/conversions.cc                    |   2 +-
 src/objects/allocation-site.h                 |   2 +-
 src/objects/arguments.h                       |   2 +-
 src/objects/bigint.cc                         |  26 ++--
 src/objects/bigint.h                          |   6 +-
 src/objects/code-inl.h                        |  12 +-
 src/objects/code-kind.h                       |  14 +-
 src/objects/code.cc                           |   4 +-
 src/objects/code.h                            |  16 +--
 src/objects/compilation-cache-table-inl.h     |   4 +-
 src/objects/compilation-cache-table.cc        |   4 +-
 src/objects/contexts-inl.h                    |   2 +-
 src/objects/contexts.cc                       |  18 +--
 src/objects/contexts.h                        |   2 +-
 src/objects/descriptor-array.h                |   4 +-
 src/objects/dictionary-inl.h                  |   4 +-
 src/objects/elements-kind.cc                  |  10 +-
 src/objects/elements-kind.h                   |  10 +-
 src/objects/elements.cc                       |  16 +--
 src/objects/embedder-data-array.h             |   2 +-
 src/objects/embedder-data-slot-inl.h          |  12 +-
 src/objects/feedback-vector-inl.h             |   2 +-
 src/objects/feedback-vector.cc                |   4 +-
 src/objects/feedback-vector.h                 |  28 ++--
 src/objects/field-index.h                     |   2 +-
 src/objects/fixed-array.h                     |  12 +-
 src/objects/foreign.h                         |   4 +-
 src/objects/function-kind.h                   |   2 +-
 src/objects/hash-table.h                      |   8 +-
 src/objects/heap-object.h                     |   2 +-
 src/objects/instance-type-inl.h               |   2 +-
 src/objects/instance-type.h                   |  38 ++---
 src/objects/intl-objects.cc                   |   2 +-
 src/objects/js-array-buffer-inl.h             |   2 +-
 src/objects/js-array-buffer.h                 |  10 +-
 src/objects/js-array.h                        |   6 +-
 src/objects/js-collection.h                   |   4 +-
 src/objects/js-date-time-format.h             |  34 ++---
 src/objects/js-display-names.h                |  14 +-
 src/objects/js-function.cc                    |   6 +-
 src/objects/js-function.h                     |   2 +-
 src/objects/js-list-format.h                  |  12 +-
 src/objects/js-objects-inl.h                  |   2 +-
 src/objects/js-objects.cc                     |   2 +-
 src/objects/js-objects.h                      |  14 +-
 src/objects/js-plural-rules.h                 |   4 +-
 src/objects/js-promise.h                      |   6 +-
 src/objects/js-proxy.h                        |   2 +-
 src/objects/js-regexp.h                       |  10 +-
 src/objects/js-relative-time-format.h         |   4 +-
 src/objects/js-segment-iterator.h             |   6 +-
 src/objects/js-segmenter.h                    |   6 +-
 src/objects/js-segments.h                     |   6 +-
 src/objects/keys.cc                           |   2 +-
 src/objects/literal-objects.cc                |  16 +--
 src/objects/lookup-inl.h                      |   2 +-
 src/objects/lookup.cc                         |   2 +-
 src/objects/map-inl.h                         |   6 +-
 src/objects/map.cc                            |   8 +-
 src/objects/map.h                             |  10 +-
 src/objects/name.h                            |  10 +-
 src/objects/objects-body-descriptors-inl.h    |  46 +++---
 src/objects/objects-body-descriptors.h        |   2 +-
 src/objects/objects-inl.h                     |   6 +-
 src/objects/objects.cc                        |  18 +--
 src/objects/oddball.h                         |   6 +-
 src/objects/ordered-hash-table.h              |   6 +-
 src/objects/primitive-heap-object.h           |   2 +-
 src/objects/promise.h                         |   4 +-
 src/objects/property-details.h                |  42 +++---
 src/objects/scope-info.cc                     |   2 +-
 src/objects/scope-info.h                      |   4 +-
 src/objects/shared-function-info-inl.h        |  12 +-
 src/objects/shared-function-info.cc           |   4 +-
 src/objects/shared-function-info.h            |   8 +-
 src/objects/smi.h                             |   2 +-
 src/objects/source-text-module.h              |   6 +-
 src/objects/string-inl.h                      |  22 +--
 src/objects/string-table.cc                   |  12 +-
 src/objects/string-table.h                    |   2 +-
 src/objects/string.cc                         |  18 +--
 src/objects/string.h                          |  12 +-
 src/objects/struct.h                          |   2 +-
 src/objects/swiss-hash-table-helpers.h        |   4 +-
 src/objects/swiss-name-dictionary-inl.h       |  16 +--
 src/objects/swiss-name-dictionary.cc          |   8 +-
 src/objects/tagged-index.h                    |   2 +-
 src/objects/transitions.h                     |   2 +-
 src/parsing/preparse-data.cc                  |   2 +-
 src/parsing/preparser.h                       |   4 +-
 src/parsing/scanner-inl.h                     |   6 +-
 src/parsing/scanner.cc                        |   2 +-
 src/parsing/scanner.h                         |   2 +-
 src/profiler/heap-snapshot-generator.cc       |  24 ++--
 src/regexp/arm/regexp-macro-assembler-arm.cc  |  10 +-
 .../arm64/regexp-macro-assembler-arm64.cc     |  10 +-
 .../experimental/experimental-bytecode.h      |   4 +-
 .../experimental/experimental-compiler.cc     |   4 +-
 .../experimental/experimental-interpreter.cc  |   2 +-
 src/regexp/experimental/experimental.cc       |   2 +-
 .../ia32/regexp-macro-assembler-ia32.cc       |  18 +--
 .../loong64/regexp-macro-assembler-loong64.cc |  10 +-
 .../mips/regexp-macro-assembler-mips.cc       |  10 +-
 .../mips64/regexp-macro-assembler-mips64.cc   |  10 +-
 src/regexp/ppc/regexp-macro-assembler-ppc.cc  |  10 +-
 src/regexp/regexp-bytecodes.h                 |   4 +-
 src/regexp/regexp-compiler-tonode.cc          |  26 ++--
 src/regexp/regexp-compiler.cc                 |  16 +--
 src/regexp/regexp-interpreter.cc              |   8 +-
 src/regexp/regexp-stack.h                     |   2 +-
 src/regexp/regexp.cc                          |   8 +-
 .../riscv64/regexp-macro-assembler-riscv64.cc |  10 +-
 .../s390/regexp-macro-assembler-s390.cc       |  10 +-
 src/regexp/x64/regexp-macro-assembler-x64.cc  |  20 +--
 src/roots/roots.h                             |   4 +-
 src/runtime/runtime-classes.cc                |   2 +-
 src/runtime/runtime-literals.cc               |   2 +-
 src/runtime/runtime-regexp.cc                 |   4 +-
 src/runtime/runtime-wasm.cc                   |   2 +-
 src/sandbox/external-pointer-inl.h            |   4 +-
 src/sandbox/external-pointer-table-inl.h      |   4 +-
 src/sandbox/external-pointer-table.cc         |   2 +-
 src/snapshot/deserializer.cc                  |  14 +-
 src/snapshot/deserializer.h                   |   2 +-
 src/snapshot/embedded/embedded-data-inl.h     |   2 +-
 src/snapshot/embedded/embedded-data.cc        |  28 ++--
 src/snapshot/embedded/embedded-data.h         |  24 ++--
 src/snapshot/embedded/embedded-file-writer.cc |   2 +-
 .../platform-embedded-file-writer-aix.cc      |   8 +-
 .../platform-embedded-file-writer-generic.cc  |   8 +-
 .../platform-embedded-file-writer-mac.cc      |   8 +-
 .../platform-embedded-file-writer-win.cc      |  10 +-
 src/snapshot/read-only-serializer.cc          |   2 +-
 src/snapshot/references.h                     |   2 +-
 src/snapshot/serializer-deserializer.h        |   4 +-
 src/snapshot/serializer.cc                    |  10 +-
 src/snapshot/serializer.h                     |   2 +-
 src/snapshot/snapshot.cc                      |   2 +-
 src/strings/string-builder-inl.h              |   6 +-
 src/strings/string-hasher-inl.h               |   6 +-
 src/tracing/trace-event.h                     |   2 +-
 src/utils/identity-map.h                      |   6 +-
 src/utils/memcopy.h                           |   6 +-
 src/utils/utils.h                             |  10 +-
 src/wasm/baseline/arm/liftoff-assembler-arm.h |   6 +-
 .../baseline/arm64/liftoff-assembler-arm64.h  |   2 +-
 .../baseline/ia32/liftoff-assembler-ia32.h    |   6 +-
 src/wasm/baseline/liftoff-compiler.cc         |   6 +-
 src/wasm/baseline/liftoff-register.h          |   2 +-
 .../loong64/liftoff-assembler-loong64.h       |   4 +-
 .../baseline/mips/liftoff-assembler-mips.h    |   6 +-
 .../mips64/liftoff-assembler-mips64.h         |   6 +-
 src/wasm/function-body-decoder-impl.h         |   8 +-
 src/wasm/function-compiler.h                  |   2 +-
 src/wasm/jump-table-assembler.cc              |   8 +-
 src/wasm/jump-table-assembler.h               |   2 +-
 src/wasm/memory-protection-key.cc             |   2 +-
 src/wasm/memory-protection-key.h              |   6 +-
 src/wasm/module-compiler.cc                   |   2 +-
 src/wasm/module-decoder.cc                    |   2 +-
 src/wasm/wasm-code-manager.cc                 |  12 +-
 src/wasm/wasm-code-manager.h                  |   2 +-
 src/wasm/wasm-engine.cc                       |   2 +-
 src/wasm/wasm-module-builder.cc               |  14 +-
 src/wasm/wasm-module.h                        |   8 +-
 src/wasm/wasm-objects-inl.h                   |   4 +-
 src/wasm/wasm-objects.cc                      |   2 +-
 src/wasm/wasm-objects.h                       |   6 +-
 src/wasm/wasm-value.h                         |   4 +-
 src/web-snapshot/web-snapshot.cc              |  20 +--
 src/web-snapshot/web-snapshot.h               |   2 +-
 src/zone/zone-chunk-list.h                    |   2 +-
 src/zone/zone-handle-set.h                    |   2 +-
 .../test-atomic-load-store-codegen.cc         |   2 +-
 test/cctest/compiler/test-codegen.h           |   2 +-
 test/cctest/compiler/test-run-load-store.cc   |   2 +-
 .../interpreter/test-bytecode-generator.cc    |   2 +-
 test/cctest/scope-test-helper.h               |   2 +-
 test/cctest/test-api.cc                       |   6 +-
 test/cctest/test-assembler-arm.cc             |   2 +-
 test/cctest/test-assembler-arm64.cc           |   4 +-
 test/cctest/test-assembler-ia32.cc            |   2 +-
 test/cctest/test-code-stub-assembler.cc       |   6 +-
 test/cctest/test-descriptor-array.cc          |   4 +-
 test/cctest/test-factory.cc                   |   2 +-
 test/cctest/test-field-type-tracking.cc       |   4 +-
 test/cctest/test-macro-assembler-arm.cc       |   2 +-
 test/cctest/test-macro-assembler-arm64.cc     |   2 +-
 test/cctest/test-macro-assembler-loong64.cc   |   2 +-
 test/cctest/test-macro-assembler-mips.cc      |   2 +-
 test/cctest/test-macro-assembler-mips64.cc    |   2 +-
 test/cctest/test-macro-assembler-riscv64.cc   |   2 +-
 test/cctest/test-macro-assembler-x64.cc       |   2 +-
 test/cctest/test-parsing.cc                   |   2 +-
 test/cctest/test-strings.cc                   |   2 +-
 test/cctest/test-swiss-name-dictionary-csa.cc |  16 +--
 .../test-swiss-name-dictionary-shared-tests.h |   4 +-
 test/cctest/test-transitions.cc               |   4 +-
 test/cctest/test-utils.cc                     |   2 +-
 test/cctest/wasm/test-jump-table-assembler.cc |   2 +-
 test/fuzzer/regexp-builtins.cc                |   4 +-
 test/fuzzer/wasm-compile.cc                   |   6 +-
 test/unittests/base/vector-unittest.cc        |   6 +-
 .../heap/cppgc/garbage-collected-unittest.cc  |  60 ++++----
 test/unittests/objects/object-unittest.cc     |   4 +-
 .../wasm/liftoff-register-unittests.cc        |   4 +-
 .../unittests/wasm/module-decoder-unittest.cc |   6 +-
 tools/debug_helper/get-object-properties.cc   |   2 +-
 446 files changed, 1945 insertions(+), 1950 deletions(-)

diff --git a/src/api/api-inl.h b/src/api/api-inl.h
index 9a507fc1355..54b17c97f3d 100644
--- a/src/api/api-inl.h
+++ b/src/api/api-inl.h
@@ -20,7 +20,7 @@ namespace v8 {
 
 template <typename T>
 inline T ToCData(v8::internal::Object obj) {
-  STATIC_ASSERT(sizeof(T) == sizeof(v8::internal::Address));
+  static_assert(sizeof(T) == sizeof(v8::internal::Address));
   if (obj == v8::internal::Smi::zero()) return nullptr;
   return reinterpret_cast<T>(
       v8::internal::Foreign::cast(obj).foreign_address());
@@ -35,7 +35,7 @@ inline v8::internal::Address ToCData(v8::internal::Object obj) {
 template <typename T>
 inline v8::internal::Handle<v8::internal::Object> FromCData(
     v8::internal::Isolate* isolate, T obj) {
-  STATIC_ASSERT(sizeof(T) == sizeof(v8::internal::Address));
+  static_assert(sizeof(T) == sizeof(v8::internal::Address));
   if (obj == nullptr) return handle(v8::internal::Smi::zero(), isolate);
   return isolate->factory()->NewForeign(
       reinterpret_cast<v8::internal::Address>(obj));
diff --git a/src/api/api.cc b/src/api/api.cc
index 8cd0b7fc025..044f389819a 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -1078,7 +1078,7 @@ Context::BackupIncumbentScope::~BackupIncumbentScope() {
   i_isolate->set_top_backup_incumbent_scope(prev_);
 }
 
-STATIC_ASSERT(i::Internals::kEmbedderDataSlotSize == i::kEmbedderDataSlotSize);
+static_assert(i::Internals::kEmbedderDataSlotSize == i::kEmbedderDataSlotSize);
 
 static i::Handle<i::EmbedderDataArray> EmbedderDataFor(Context* context,
                                                        int index, bool can_grow,
@@ -5157,7 +5157,7 @@ MaybeLocal<Value> Object::CallAsFunction(Local<Context> context,
                                              i_isolate);
   auto self = Utils::OpenHandle(this);
   auto recv_obj = Utils::OpenHandle(*recv);
-  STATIC_ASSERT(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
+  static_assert(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
   i::Handle<i::Object>* args = reinterpret_cast<i::Handle<i::Object>*>(argv);
   Local<Value> result;
   has_pending_exception = !ToLocal<Value>(
@@ -5176,7 +5176,7 @@ MaybeLocal<Value> Object::CallAsConstructor(Local<Context> context, int argc,
   i::NestedTimedHistogramScope execute_timer(i_isolate->counters()->execute(),
                                              i_isolate);
   auto self = Utils::OpenHandle(this);
-  STATIC_ASSERT(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
+  static_assert(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
   i::Handle<i::Object>* args = reinterpret_cast<i::Handle<i::Object>*>(argv);
   Local<Value> result;
   has_pending_exception = !ToLocal<Value>(
@@ -5215,7 +5215,7 @@ MaybeLocal<Object> Function::NewInstanceWithSideEffectType(
   i::NestedTimedHistogramScope execute_timer(i_isolate->counters()->execute(),
                                              i_isolate);
   auto self = Utils::OpenHandle(this);
-  STATIC_ASSERT(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
+  static_assert(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
   bool should_set_has_no_side_effect =
       side_effect_type == SideEffectType::kHasNoSideEffect &&
       i_isolate->debug_execution_mode() == i::DebugInfo::kSideEffects;
@@ -5269,7 +5269,7 @@ MaybeLocal<v8::Value> Function::Call(Local<Context> context,
   Utils::ApiCheck(!self.is_null(), "v8::Function::Call",
                   "Function to be called is a null pointer");
   i::Handle<i::Object> recv_obj = Utils::OpenHandle(*recv);
-  STATIC_ASSERT(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
+  static_assert(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
   i::Handle<i::Object>* args = reinterpret_cast<i::Handle<i::Object>*>(argv);
   Local<Value> result;
   has_pending_exception = !ToLocal<Value>(
@@ -5587,7 +5587,7 @@ static int WriteUtf8Impl(base::Vector<const Char> string, char* write_start,
   int prev_char = unibrow::Utf16::kNoPreviousCharacter;
   // Do a fast loop where there is no exit capacity check.
   // Need enough space to write everything but one character.
-  STATIC_ASSERT(unibrow::Utf16::kMaxExtraUtf8BytesForOneUtf16CodeUnit == 3);
+  static_assert(unibrow::Utf16::kMaxExtraUtf8BytesForOneUtf16CodeUnit == 3);
   static const int kMaxSizePerChar = sizeof(Char) == 1 ? 2 : 3;
   while (read_index < read_length) {
     int up_to = read_length;
@@ -6768,7 +6768,7 @@ bool FunctionTemplate::IsLeafTemplateForApiObject(
 }
 
 Local<External> v8::External::New(Isolate* v8_isolate, void* value) {
-  STATIC_ASSERT(sizeof(value) == sizeof(i::Address));
+  static_assert(sizeof(value) == sizeof(i::Address));
   // Nullptr is not allowed here because serialization/deserialization of
   // nullptr external api references is not possible as nullptr is used as an
   // external_references table terminator, see v8::SnapshotCreator()
@@ -6836,7 +6836,7 @@ inline i::MaybeHandle<i::String> NewString(
   return factory->NewStringFromTwoByte(string);
 }
 
-STATIC_ASSERT(v8::String::kMaxLength == i::String::kMaxLength);
+static_assert(v8::String::kMaxLength == i::String::kMaxLength);
 
 }  // anonymous namespace
 
@@ -7267,7 +7267,7 @@ double v8::Date::ValueOf() const {
 // Assert that the static TimeZoneDetection cast in
 // DateTimeConfigurationChangeNotification is valid.
 #define TIME_ZONE_DETECTION_ASSERT_EQ(value)                     \
-  STATIC_ASSERT(                                                 \
+  static_assert(                                                 \
       static_cast<int>(v8::Isolate::TimeZoneDetection::value) == \
       static_cast<int>(base::TimezoneCache::TimeZoneDetection::value));
 TIME_ZONE_DETECTION_ASSERT_EQ(kSkip)
@@ -7313,7 +7313,7 @@ Local<v8::String> v8::RegExp::GetSource() const {
 
 // Assert that the static flags cast in GetFlags is valid.
 #define REGEXP_FLAG_ASSERT_EQ(flag)                   \
-  STATIC_ASSERT(static_cast<int>(v8::RegExp::flag) == \
+  static_assert(static_cast<int>(v8::RegExp::flag) == \
                 static_cast<int>(i::JSRegExp::flag))
 REGEXP_FLAG_ASSERT_EQ(kNone);
 REGEXP_FLAG_ASSERT_EQ(kGlobal);
@@ -9077,7 +9077,7 @@ int64_t Isolate::AdjustAmountOfExternalAllocatedMemory(
   // Try to check for unreasonably large or small values from the embedder.
   const int64_t kMaxReasonableBytes = int64_t(1) << 60;
   const int64_t kMinReasonableBytes = -kMaxReasonableBytes;
-  STATIC_ASSERT(kMaxReasonableBytes >= i::JSArrayBuffer::kMaxByteLength);
+  static_assert(kMaxReasonableBytes >= i::JSArrayBuffer::kMaxByteLength);
 
   CHECK(kMinReasonableBytes <= change_in_bytes &&
         change_in_bytes < kMaxReasonableBytes);
diff --git a/src/ast/ast-source-ranges.h b/src/ast/ast-source-ranges.h
index 1e96ec4c27c..7edd07658be 100644
--- a/src/ast/ast-source-ranges.h
+++ b/src/ast/ast-source-ranges.h
@@ -27,7 +27,7 @@ struct SourceRange {
   }
 
   static constexpr int kFunctionLiteralSourcePosition = -2;
-  STATIC_ASSERT(kFunctionLiteralSourcePosition == kNoSourcePosition - 1);
+  static_assert(kFunctionLiteralSourcePosition == kNoSourcePosition - 1);
 
   // Source ranges associated with a function literal do not contain real
   // source positions; instead, they are created with special marker values.
diff --git a/src/ast/ast.h b/src/ast/ast.h
index a348ccf82a2..32438f9b249 100644
--- a/src/ast/ast.h
+++ b/src/ast/ast.h
@@ -256,7 +256,7 @@ class Expression : public AstNode {
   bool IsCompileTimeValue();
 
   bool IsPattern() {
-    STATIC_ASSERT(kObjectLiteral + 1 == kArrayLiteral);
+    static_assert(kObjectLiteral + 1 == kArrayLiteral);
     return base::IsInRange(node_type(), kObjectLiteral, kArrayLiteral);
   }
 
@@ -635,7 +635,7 @@ class ReturnStatement final : public JumpStatement {
   // This constant is used to indicate that the return position
   // from the FunctionLiteral should be used when emitting code.
   static constexpr int kFunctionLiteralReturnPosition = -2;
-  STATIC_ASSERT(kFunctionLiteralReturnPosition == kNoSourcePosition - 1);
+  static_assert(kFunctionLiteralReturnPosition == kNoSourcePosition - 1);
 
   int end_position() const { return end_position_; }
 
@@ -1107,7 +1107,7 @@ class LiteralBoilerplateBuilder {
   enum DepthKind { kUninitialized, kShallow, kNotShallow };
 
   static constexpr int kDepthKindBits = 2;
-  STATIC_ASSERT((1 << kDepthKindBits) > kNotShallow);
+  static_assert((1 << kDepthKindBits) > kNotShallow);
 
   bool is_initialized() const {
     return kUninitialized != DepthField::decode(bit_field_);
@@ -1354,7 +1354,7 @@ class ObjectLiteral final : public AggregateLiteral {
     kFastElements = 1 << 3,
     kHasNullPrototype = 1 << 4,
   };
-  STATIC_ASSERT(
+  static_assert(
       static_cast<int>(AggregateLiteral::kNeedsInitialAllocationSite) <
       static_cast<int>(kFastElements));
 
diff --git a/src/ast/scopes.h b/src/ast/scopes.h
index 6f701ead0b1..9f497093145 100644
--- a/src/ast/scopes.h
+++ b/src/ast/scopes.h
@@ -818,7 +818,7 @@ class V8_EXPORT_PRIVATE Scope : public NON_EXPORTED_BASE(ZoneObject) {
   // Scope-specific information computed during parsing.
   //
   // The language mode of this scope.
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   bool is_strict_ : 1;
   // This scope contains an 'eval' call.
   bool calls_eval_ : 1;
diff --git a/src/base/address-region.h b/src/base/address-region.h
index 1d37d8a863c..517f5935e0f 100644
--- a/src/base/address-region.h
+++ b/src/base/address-region.h
@@ -39,12 +39,12 @@ class AddressRegion {
   bool is_empty() const { return size_ == 0; }
 
   bool contains(Address address) const {
-    STATIC_ASSERT(std::is_unsigned<Address>::value);
+    static_assert(std::is_unsigned<Address>::value);
     return (address - begin()) < size();
   }
 
   bool contains(Address address, size_t size) const {
-    STATIC_ASSERT(std::is_unsigned<Address>::value);
+    static_assert(std::is_unsigned<Address>::value);
     Address offset = address - begin();
     return (offset < size_) && (offset + size <= size_);
   }
diff --git a/src/base/atomic-utils.h b/src/base/atomic-utils.h
index 76fc6449b99..e81176e582a 100644
--- a/src/base/atomic-utils.h
+++ b/src/base/atomic-utils.h
@@ -35,7 +35,7 @@ class AtomicValue {
   }
 
  private:
-  STATIC_ASSERT(sizeof(T) <= sizeof(base::AtomicWord));
+  static_assert(sizeof(T) <= sizeof(base::AtomicWord));
 
   template <typename S>
   struct cast_helper {
@@ -68,21 +68,21 @@ class AsAtomicImpl {
 
   template <typename T>
   static T SeqCst_Load(T* addr) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(
         base::SeqCst_Load(to_storage_addr(addr)));
   }
 
   template <typename T>
   static T Acquire_Load(T* addr) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(
         base::Acquire_Load(to_storage_addr(addr)));
   }
 
   template <typename T>
   static T Relaxed_Load(T* addr) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(
         base::Relaxed_Load(to_storage_addr(addr)));
   }
@@ -90,7 +90,7 @@ class AsAtomicImpl {
   template <typename T>
   static void SeqCst_Store(T* addr,
                            typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     base::SeqCst_Store(to_storage_addr(addr),
                        cast_helper<T>::to_storage_type(new_value));
   }
@@ -98,7 +98,7 @@ class AsAtomicImpl {
   template <typename T>
   static void Release_Store(T* addr,
                             typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     base::Release_Store(to_storage_addr(addr),
                         cast_helper<T>::to_storage_type(new_value));
   }
@@ -106,7 +106,7 @@ class AsAtomicImpl {
   template <typename T>
   static void Relaxed_Store(T* addr,
                             typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     base::Relaxed_Store(to_storage_addr(addr),
                         cast_helper<T>::to_storage_type(new_value));
   }
@@ -114,7 +114,7 @@ class AsAtomicImpl {
   template <typename T>
   static T SeqCst_Swap(T* addr,
                        typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return base::SeqCst_AtomicExchange(
         to_storage_addr(addr), cast_helper<T>::to_storage_type(new_value));
   }
@@ -123,7 +123,7 @@ class AsAtomicImpl {
   static T Release_CompareAndSwap(
       T* addr, typename std::remove_reference<T>::type old_value,
       typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(base::Release_CompareAndSwap(
         to_storage_addr(addr), cast_helper<T>::to_storage_type(old_value),
         cast_helper<T>::to_storage_type(new_value)));
@@ -133,7 +133,7 @@ class AsAtomicImpl {
   static T Relaxed_CompareAndSwap(
       T* addr, typename std::remove_reference<T>::type old_value,
       typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(base::Relaxed_CompareAndSwap(
         to_storage_addr(addr), cast_helper<T>::to_storage_type(old_value),
         cast_helper<T>::to_storage_type(new_value)));
@@ -143,7 +143,7 @@ class AsAtomicImpl {
   static T AcquireRelease_CompareAndSwap(
       T* addr, typename std::remove_reference<T>::type old_value,
       typename std::remove_reference<T>::type new_value) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     return cast_helper<T>::to_return_type(base::AcquireRelease_CompareAndSwap(
         to_storage_addr(addr), cast_helper<T>::to_storage_type(old_value),
         cast_helper<T>::to_storage_type(new_value)));
@@ -153,7 +153,7 @@ class AsAtomicImpl {
   // Returns false if the bits are already set as needed.
   template <typename T>
   static bool SetBits(T* addr, T bits, T mask) {
-    STATIC_ASSERT(sizeof(T) <= sizeof(AtomicStorageType));
+    static_assert(sizeof(T) <= sizeof(AtomicStorageType));
     DCHECK_EQ(bits & ~mask, static_cast<T>(0));
     T old_value = Relaxed_Load(addr);
     T new_value, old_value_before_cas;
@@ -256,15 +256,15 @@ inline void CheckedDecrement(
 
 template <typename T>
 V8_INLINE std::atomic<T>* AsAtomicPtr(T* t) {
-  STATIC_ASSERT(sizeof(T) == sizeof(std::atomic<T>));
-  STATIC_ASSERT(alignof(T) >= alignof(std::atomic<T>));
+  static_assert(sizeof(T) == sizeof(std::atomic<T>));
+  static_assert(alignof(T) >= alignof(std::atomic<T>));
   return reinterpret_cast<std::atomic<T>*>(t);
 }
 
 template <typename T>
 V8_INLINE const std::atomic<T>* AsAtomicPtr(const T* t) {
-  STATIC_ASSERT(sizeof(T) == sizeof(std::atomic<T>));
-  STATIC_ASSERT(alignof(T) >= alignof(std::atomic<T>));
+  static_assert(sizeof(T) == sizeof(std::atomic<T>));
+  static_assert(alignof(T) >= alignof(std::atomic<T>));
   return reinterpret_cast<const std::atomic<T>*>(t);
 }
 
diff --git a/src/base/atomicops.h b/src/base/atomicops.h
index 93a390380be..8ffcfac2b83 100644
--- a/src/base/atomicops.h
+++ b/src/base/atomicops.h
@@ -75,7 +75,7 @@ using AtomicWord = Atomic64;
 #else
 using AtomicWord = Atomic32;
 #endif
-STATIC_ASSERT(sizeof(void*) == sizeof(AtomicWord));
+static_assert(sizeof(void*) == sizeof(AtomicWord));
 
 namespace helper {
 template <typename T>
diff --git a/src/base/bit-field.h b/src/base/bit-field.h
index 63142a20fa2..9a66468d4e0 100644
--- a/src/base/bit-field.h
+++ b/src/base/bit-field.h
@@ -22,11 +22,11 @@ namespace base {
 template <class T, int shift, int size, class U = uint32_t>
 class BitField final {
  public:
-  STATIC_ASSERT(std::is_unsigned<U>::value);
-  STATIC_ASSERT(shift < 8 * sizeof(U));  // Otherwise shifts by {shift} are UB.
-  STATIC_ASSERT(size < 8 * sizeof(U));   // Otherwise shifts by {size} are UB.
-  STATIC_ASSERT(shift + size <= 8 * sizeof(U));
-  STATIC_ASSERT(size > 0);
+  static_assert(std::is_unsigned<U>::value);
+  static_assert(shift < 8 * sizeof(U));  // Otherwise shifts by {shift} are UB.
+  static_assert(size < 8 * sizeof(U));   // Otherwise shifts by {size} are UB.
+  static_assert(shift + size <= 8 * sizeof(U));
+  static_assert(size > 0);
 
   using FieldType = T;
 
diff --git a/src/base/bits-iterator.h b/src/base/bits-iterator.h
index 6ce656e6d71..9056ae4da1f 100644
--- a/src/base/bits-iterator.h
+++ b/src/base/bits-iterator.h
@@ -16,7 +16,7 @@ namespace bits {
 
 template <typename T, bool kMSBFirst = false>
 class BitsIterator : public iterator<std::forward_iterator_tag, int> {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
 
  public:
   explicit BitsIterator(T bits) : bits_(bits) {}
diff --git a/src/base/bits.h b/src/base/bits.h
index 2f919d3f85e..49d45570e95 100644
--- a/src/base/bits.h
+++ b/src/base/bits.h
@@ -27,7 +27,7 @@ constexpr inline
     typename std::enable_if<std::is_unsigned<T>::value && sizeof(T) <= 8,
                             unsigned>::type
     CountPopulation(T value) {
-  STATIC_ASSERT(sizeof(T) <= 8);
+  static_assert(sizeof(T) <= 8);
 #if V8_HAS_BUILTIN_POPCOUNT
   return sizeof(T) == 8 ? __builtin_popcountll(static_cast<uint64_t>(value))
                         : __builtin_popcount(static_cast<uint32_t>(value));
@@ -60,7 +60,7 @@ constexpr inline
 // ReverseBits(value) returns |value| in reverse bit order.
 template <typename T>
 T ReverseBits(T value) {
-  STATIC_ASSERT((sizeof(value) == 1) || (sizeof(value) == 2) ||
+  static_assert((sizeof(value) == 1) || (sizeof(value) == 2) ||
                 (sizeof(value) == 4) || (sizeof(value) == 8));
   T result = 0;
   for (unsigned i = 0; i < (sizeof(value) * 8); i++) {
@@ -167,7 +167,7 @@ template <typename T,
 inline constexpr int WhichPowerOfTwo(T value) {
   DCHECK(IsPowerOfTwo(value));
 #if V8_HAS_BUILTIN_CTZ
-  STATIC_ASSERT(sizeof(T) <= 8);
+  static_assert(sizeof(T) <= 8);
   return sizeof(T) == 8 ? __builtin_ctzll(static_cast<uint64_t>(value))
                         : __builtin_ctz(static_cast<uint32_t>(value));
 #else
diff --git a/src/base/bounds.h b/src/base/bounds.h
index 0fe141b3097..143ea82c578 100644
--- a/src/base/bounds.h
+++ b/src/base/bounds.h
@@ -16,7 +16,7 @@ namespace base {
 template <typename T, typename U>
 inline constexpr bool IsInRange(T value, U lower_limit, U higher_limit) {
   DCHECK_LE(lower_limit, higher_limit);
-  STATIC_ASSERT(sizeof(U) <= sizeof(T));
+  static_assert(sizeof(U) <= sizeof(T));
   using unsigned_T = typename std::make_unsigned<T>::type;
   // Use static_cast to support enum classes.
   return static_cast<unsigned_T>(static_cast<unsigned_T>(value) -
diff --git a/src/base/division-by-constant.cc b/src/base/division-by-constant.cc
index 7aa3a690149..97dfd5680b0 100644
--- a/src/base/division-by-constant.cc
+++ b/src/base/division-by-constant.cc
@@ -14,7 +14,7 @@ namespace base {
 
 template <class T>
 MagicNumbersForDivision<T> SignedDivisionByConstant(T d) {
-  STATIC_ASSERT(static_cast<T>(0) < static_cast<T>(-1));
+  static_assert(static_cast<T>(0) < static_cast<T>(-1));
   DCHECK(d != static_cast<T>(-1) && d != 0 && d != 1);
   const unsigned bits = static_cast<unsigned>(sizeof(T)) * 8;
   const T min = (static_cast<T>(1) << (bits - 1));
@@ -52,7 +52,7 @@ MagicNumbersForDivision<T> SignedDivisionByConstant(T d) {
 template <class T>
 MagicNumbersForDivision<T> UnsignedDivisionByConstant(T d,
                                                       unsigned leading_zeros) {
-  STATIC_ASSERT(static_cast<T>(0) < static_cast<T>(-1));
+  static_assert(static_cast<T>(0) < static_cast<T>(-1));
   DCHECK_NE(d, 0);
   const unsigned bits = static_cast<unsigned>(sizeof(T)) * 8;
   const T ones = ~static_cast<T>(0) >> leading_zeros;
diff --git a/src/base/hashmap-entry.h b/src/base/hashmap-entry.h
index 2f984f3c2ae..5a7c6fcf7bd 100644
--- a/src/base/hashmap-entry.h
+++ b/src/base/hashmap-entry.h
@@ -23,7 +23,7 @@ struct NoHashMapValue {};
 // should use NoHashMapValue.
 template <typename Key, typename Value>
 struct TemplateHashMapEntry {
-  STATIC_ASSERT((!std::is_same<Value, NoHashMapValue>::value));
+  static_assert((!std::is_same<Value, NoHashMapValue>::value));
 
   Key key;
   Value value;
@@ -43,7 +43,7 @@ struct TemplateHashMapEntry {
 // Specialization for pointer-valued keys
 template <typename Key, typename Value>
 struct TemplateHashMapEntry<Key*, Value> {
-  STATIC_ASSERT((!std::is_same<Value, NoHashMapValue>::value));
+  static_assert((!std::is_same<Value, NoHashMapValue>::value));
 
   Key* key;
   Value value;
diff --git a/src/base/hashmap.h b/src/base/hashmap.h
index 576ffb06812..6648eff6b28 100644
--- a/src/base/hashmap.h
+++ b/src/base/hashmap.h
@@ -530,8 +530,8 @@ class TemplateHashMap
                                    AllocationPolicy>;
 
  public:
-  STATIC_ASSERT(sizeof(Key*) == sizeof(void*));
-  STATIC_ASSERT(sizeof(Value*) == sizeof(void*));
+  static_assert(sizeof(Key*) == sizeof(void*));
+  static_assert(sizeof(Value*) == sizeof(void*));
   struct value_type {
     Key* first;
     Value* second;
diff --git a/src/base/macros.h b/src/base/macros.h
index 76b1e9ebe23..8ecd7f6875a 100644
--- a/src/base/macros.h
+++ b/src/base/macros.h
@@ -190,11 +190,6 @@ V8_INLINE Dest bit_cast(Source const& source) {
 #define DISABLE_CFI_ICALL V8_CLANG_NO_SANITIZE("cfi-icall")
 #endif
 
-// A convenience wrapper around static_assert without a string message argument.
-// Once C++17 becomes the default, this macro can be removed in favor of the
-// new static_assert(condition) overload.
-#define STATIC_ASSERT(test) static_assert(test, #test)
-
 namespace v8 {
 namespace base {
 
@@ -326,28 +321,28 @@ inline uint64_t make_uint64(uint32_t high, uint32_t low) {
 // Return the largest multiple of m which is <= x.
 template <typename T>
 inline T RoundDown(T x, intptr_t m) {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
   // m must be a power of two.
   DCHECK(m != 0 && ((m & (m - 1)) == 0));
   return x & static_cast<T>(-m);
 }
 template <intptr_t m, typename T>
 constexpr inline T RoundDown(T x) {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
   // m must be a power of two.
-  STATIC_ASSERT(m != 0 && ((m & (m - 1)) == 0));
+  static_assert(m != 0 && ((m & (m - 1)) == 0));
   return x & static_cast<T>(-m);
 }
 
 // Return the smallest multiple of m which is >= x.
 template <typename T>
 inline T RoundUp(T x, intptr_t m) {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
   return RoundDown<T>(static_cast<T>(x + m - 1), m);
 }
 template <intptr_t m, typename T>
 constexpr inline T RoundUp(T x) {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
   return RoundDown<m, T>(static_cast<T>(x + (m - 1)));
 }
 
diff --git a/src/base/platform/platform-posix.cc b/src/base/platform/platform-posix.cc
index 0c1c9d08f0c..131ff9614e7 100644
--- a/src/base/platform/platform-posix.cc
+++ b/src/base/platform/platform-posix.cc
@@ -1061,7 +1061,7 @@ static void SetThreadName(const char* name) {
 #if V8_OS_DRAGONFLYBSD || V8_OS_FREEBSD || V8_OS_OPENBSD
   pthread_set_name_np(pthread_self(), name);
 #elif V8_OS_NETBSD
-  STATIC_ASSERT(Thread::kMaxThreadNameLength <= PTHREAD_MAX_NAMELEN_NP);
+  static_assert(Thread::kMaxThreadNameLength <= PTHREAD_MAX_NAMELEN_NP);
   pthread_setname_np(pthread_self(), "%s", name);
 #elif V8_OS_DARWIN
   // pthread_setname_np is only available in 10.6 or later, so test
@@ -1073,7 +1073,7 @@ static void SetThreadName(const char* name) {
 
   // Mac OS X does not expose the length limit of the name, so hardcode it.
   static const int kMaxNameLength = 63;
-  STATIC_ASSERT(Thread::kMaxThreadNameLength <= kMaxNameLength);
+  static_assert(Thread::kMaxThreadNameLength <= kMaxNameLength);
   dynamic_pthread_setname_np(name);
 #elif defined(PR_SET_NAME)
   prctl(PR_SET_NAME,
@@ -1139,7 +1139,7 @@ static Thread::LocalStorageKey PthreadKeyToLocalKey(pthread_key_t pthread_key) {
   // We need to cast pthread_key_t to Thread::LocalStorageKey in two steps
   // because pthread_key_t is a pointer type on Cygwin. This will probably not
   // work on 64-bit platforms, but Cygwin doesn't support 64-bit anyway.
-  STATIC_ASSERT(sizeof(Thread::LocalStorageKey) == sizeof(pthread_key_t));
+  static_assert(sizeof(Thread::LocalStorageKey) == sizeof(pthread_key_t));
   intptr_t ptr_key = reinterpret_cast<intptr_t>(pthread_key);
   return static_cast<Thread::LocalStorageKey>(ptr_key);
 #else
@@ -1150,7 +1150,7 @@ static Thread::LocalStorageKey PthreadKeyToLocalKey(pthread_key_t pthread_key) {
 
 static pthread_key_t LocalKeyToPthreadKey(Thread::LocalStorageKey local_key) {
 #if V8_OS_CYGWIN
-  STATIC_ASSERT(sizeof(Thread::LocalStorageKey) == sizeof(pthread_key_t));
+  static_assert(sizeof(Thread::LocalStorageKey) == sizeof(pthread_key_t));
   intptr_t ptr_key = static_cast<intptr_t>(local_key);
   return reinterpret_cast<pthread_key_t>(ptr_key);
 #else
diff --git a/src/base/platform/platform-win32.cc b/src/base/platform/platform-win32.cc
index 5df9c3fa0d9..4857a25027f 100644
--- a/src/base/platform/platform-win32.cc
+++ b/src/base/platform/platform-win32.cc
@@ -39,25 +39,25 @@
 #endif               // defined(_MSC_VER)
 
 // Check that type sizes and alignments match.
-STATIC_ASSERT(sizeof(V8_CONDITION_VARIABLE) == sizeof(CONDITION_VARIABLE));
-STATIC_ASSERT(alignof(V8_CONDITION_VARIABLE) == alignof(CONDITION_VARIABLE));
-STATIC_ASSERT(sizeof(V8_SRWLOCK) == sizeof(SRWLOCK));
-STATIC_ASSERT(alignof(V8_SRWLOCK) == alignof(SRWLOCK));
-STATIC_ASSERT(sizeof(V8_CRITICAL_SECTION) == sizeof(CRITICAL_SECTION));
-STATIC_ASSERT(alignof(V8_CRITICAL_SECTION) == alignof(CRITICAL_SECTION));
+static_assert(sizeof(V8_CONDITION_VARIABLE) == sizeof(CONDITION_VARIABLE));
+static_assert(alignof(V8_CONDITION_VARIABLE) == alignof(CONDITION_VARIABLE));
+static_assert(sizeof(V8_SRWLOCK) == sizeof(SRWLOCK));
+static_assert(alignof(V8_SRWLOCK) == alignof(SRWLOCK));
+static_assert(sizeof(V8_CRITICAL_SECTION) == sizeof(CRITICAL_SECTION));
+static_assert(alignof(V8_CRITICAL_SECTION) == alignof(CRITICAL_SECTION));
 
 // Check that CRITICAL_SECTION offsets match.
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, DebugInfo) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, DebugInfo) ==
               offsetof(CRITICAL_SECTION, DebugInfo));
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, LockCount) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, LockCount) ==
               offsetof(CRITICAL_SECTION, LockCount));
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, RecursionCount) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, RecursionCount) ==
               offsetof(CRITICAL_SECTION, RecursionCount));
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, OwningThread) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, OwningThread) ==
               offsetof(CRITICAL_SECTION, OwningThread));
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, LockSemaphore) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, LockSemaphore) ==
               offsetof(CRITICAL_SECTION, LockSemaphore));
-STATIC_ASSERT(offsetof(V8_CRITICAL_SECTION, SpinCount) ==
+static_assert(offsetof(V8_CRITICAL_SECTION, SpinCount) ==
               offsetof(CRITICAL_SECTION, SpinCount));
 
 // Extra functions for MinGW. Most of these are the _s functions which are in
diff --git a/src/base/small-vector.h b/src/base/small-vector.h
index 8bbbbebf981..0591b2a2f21 100644
--- a/src/base/small-vector.h
+++ b/src/base/small-vector.h
@@ -23,7 +23,7 @@ class SmallVector {
   // Currently only support trivially copyable and trivially destructible data
   // types, as it uses memcpy to copy elements and never calls destructors.
   ASSERT_TRIVIALLY_COPYABLE(T);
-  STATIC_ASSERT(std::is_trivially_destructible<T>::value);
+  static_assert(std::is_trivially_destructible<T>::value);
 
  public:
   static constexpr size_t kInlineSize = kSize;
diff --git a/src/base/vector.h b/src/base/vector.h
index 28f8eba6919..1f5e103d4cd 100644
--- a/src/base/vector.h
+++ b/src/base/vector.h
@@ -142,8 +142,8 @@ class Vector {
   static Vector<T> cast(Vector<S> input) {
     // Casting is potentially dangerous, so be really restrictive here. This
     // might be lifted once we have use cases for that.
-    STATIC_ASSERT(std::is_pod<S>::value);
-    STATIC_ASSERT(std::is_pod<T>::value);
+    static_assert(std::is_pod<S>::value);
+    static_assert(std::is_pod<T>::value);
     DCHECK_EQ(0, (input.size() * sizeof(S)) % sizeof(T));
     DCHECK_EQ(0, reinterpret_cast<uintptr_t>(input.begin()) % alignof(T));
     return Vector<T>(reinterpret_cast<T*>(input.begin()),
@@ -207,7 +207,7 @@ class OwnedVector {
                 std::unique_ptr<U>, std::unique_ptr<T>>::value>::type>
   OwnedVector(OwnedVector<U>&& other)
       : data_(std::move(other.data_)), length_(other.length_) {
-    STATIC_ASSERT(sizeof(U) == sizeof(T));
+    static_assert(sizeof(U) == sizeof(T));
     other.length_ = 0;
   }
 
diff --git a/src/baseline/arm/baseline-assembler-arm-inl.h b/src/baseline/arm/baseline-assembler-arm-inl.h
index 0c5f74a8a0e..f8c171608d9 100644
--- a/src/baseline/arm/baseline-assembler-arm-inl.h
+++ b/src/baseline/arm/baseline-assembler-arm-inl.h
@@ -63,7 +63,7 @@ enum class Condition : uint32_t {
 inline internal::Condition AsMasmCondition(Condition cond) {
   // This is important for arm, where the internal::Condition where each value
   // represents an encoded bit field value.
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   return static_cast<internal::Condition>(cond);
 }
 
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index d0063321521..c8da2dc93da 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -185,14 +185,14 @@ template <typename Descriptor, int ArgIndex, bool kIsRegister>
 struct ArgumentSettingHelper<Descriptor, ArgIndex, kIsRegister> {
   static void Set(BaselineAssembler* masm) {
     // Should only ever be called for the end of register arguments.
-    STATIC_ASSERT(ArgIndex == Descriptor::GetRegisterParameterCount());
+    static_assert(ArgIndex == Descriptor::GetRegisterParameterCount());
   }
 };
 
 template <typename Descriptor, int ArgIndex, typename Arg, typename... Args>
 struct ArgumentSettingHelper<Descriptor, ArgIndex, true, Arg, Args...> {
   static void Set(BaselineAssembler* masm, Arg arg, Args... args) {
-    STATIC_ASSERT(ArgIndex < Descriptor::GetRegisterParameterCount());
+    static_assert(ArgIndex < Descriptor::GetRegisterParameterCount());
     Register target = Descriptor::GetRegisterParameter(ArgIndex);
     CheckSettingDoesntClobber(target, args...);
     masm->Move(target, arg);
@@ -207,7 +207,7 @@ template <typename Descriptor, int ArgIndex>
 struct ArgumentSettingHelper<Descriptor, ArgIndex, true,
                              interpreter::RegisterList> {
   static void Set(BaselineAssembler* masm, interpreter::RegisterList list) {
-    STATIC_ASSERT(ArgIndex < Descriptor::GetRegisterParameterCount());
+    static_assert(ArgIndex < Descriptor::GetRegisterParameterCount());
     DCHECK_EQ(ArgIndex + list.register_count(),
               Descriptor::GetRegisterParameterCount());
     for (int i = 0; ArgIndex + i < Descriptor::GetRegisterParameterCount();
@@ -652,7 +652,7 @@ void BaselineCompiler::JumpIfToBoolean(bool do_jump_if_true, Label* label,
   // ToBooleanForBaselineJump returns the ToBoolean value into return reg 1, and
   // the original value into kInterpreterAccumulatorRegister, so we don't have
   // to worry about it getting clobbered.
-  STATIC_ASSERT(kReturnRegister0 == kInterpreterAccumulatorRegister);
+  static_assert(kReturnRegister0 == kInterpreterAccumulatorRegister);
   __ JumpIfSmi(do_jump_if_true ? Condition::kNotEqual : Condition::kEqual,
                kReturnRegister1, Smi::FromInt(0), label, distance);
 }
@@ -1595,7 +1595,7 @@ void BaselineCompiler::VisitTestTypeOf() {
     case interpreter::TestTypeOfFlags::LiteralFlag::kString: {
       Label is_smi, bad_instance_type;
       __ JumpIfSmi(kInterpreterAccumulatorRegister, &is_smi, Label::kNear);
-      STATIC_ASSERT(INTERNALIZED_STRING_TYPE == FIRST_TYPE);
+      static_assert(INTERNALIZED_STRING_TYPE == FIRST_TYPE);
       __ JumpIfObjectType(Condition::kGreaterThanEqual,
                           kInterpreterAccumulatorRegister, FIRST_NONSTRING_TYPE,
                           scratch_scope.AcquireScratch(), &bad_instance_type,
@@ -1709,7 +1709,7 @@ void BaselineCompiler::VisitTestTypeOf() {
                     &is_null, Label::kNear);
 
       // If the object's instance type isn't within the range, return false.
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       Register map = scratch_scope.AcquireScratch();
       __ JumpIfObjectType(Condition::kLessThan, kInterpreterAccumulatorRegister,
                           FIRST_JS_RECEIVER_TYPE, map, &bad_instance_type,
diff --git a/src/baseline/loong64/baseline-assembler-loong64-inl.h b/src/baseline/loong64/baseline-assembler-loong64-inl.h
index 4330a0a0059..81265530a0f 100644
--- a/src/baseline/loong64/baseline-assembler-loong64-inl.h
+++ b/src/baseline/loong64/baseline-assembler-loong64-inl.h
@@ -58,7 +58,7 @@ enum class Condition : uint32_t {
 };
 
 inline internal::Condition AsMasmCondition(Condition cond) {
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   return static_cast<internal::Condition>(cond);
 }
 
diff --git a/src/baseline/mips/baseline-assembler-mips-inl.h b/src/baseline/mips/baseline-assembler-mips-inl.h
index 1ebd48c2a27..a25c1a0a715 100644
--- a/src/baseline/mips/baseline-assembler-mips-inl.h
+++ b/src/baseline/mips/baseline-assembler-mips-inl.h
@@ -60,7 +60,7 @@ enum class Condition : uint32_t {
 inline internal::Condition AsMasmCondition(Condition cond) {
   // This is important for arm, where the internal::Condition where each value
   // represents an encoded bit field value.
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   return static_cast<internal::Condition>(cond);
 }
 
diff --git a/src/baseline/mips64/baseline-assembler-mips64-inl.h b/src/baseline/mips64/baseline-assembler-mips64-inl.h
index 47cb71b77d8..aee26c391ed 100644
--- a/src/baseline/mips64/baseline-assembler-mips64-inl.h
+++ b/src/baseline/mips64/baseline-assembler-mips64-inl.h
@@ -58,7 +58,7 @@ enum class Condition : uint32_t {
 };
 
 inline internal::Condition AsMasmCondition(Condition cond) {
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   return static_cast<internal::Condition>(cond);
 }
 
diff --git a/src/baseline/ppc/baseline-assembler-ppc-inl.h b/src/baseline/ppc/baseline-assembler-ppc-inl.h
index 332043f1f5d..2c0780eb940 100644
--- a/src/baseline/ppc/baseline-assembler-ppc-inl.h
+++ b/src/baseline/ppc/baseline-assembler-ppc-inl.h
@@ -70,7 +70,7 @@ enum class Condition : uint32_t {
 };
 
 inline internal::Condition AsMasmCondition(Condition cond) {
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   switch (cond) {
     case Condition::kEqual:
       return eq;
diff --git a/src/baseline/s390/baseline-assembler-s390-inl.h b/src/baseline/s390/baseline-assembler-s390-inl.h
index f52552b1f2d..1c069ccded6 100644
--- a/src/baseline/s390/baseline-assembler-s390-inl.h
+++ b/src/baseline/s390/baseline-assembler-s390-inl.h
@@ -70,7 +70,7 @@ enum class Condition : uint32_t {
 };
 
 inline internal::Condition AsMasmCondition(Condition cond) {
-  STATIC_ASSERT(sizeof(internal::Condition) == sizeof(Condition));
+  static_assert(sizeof(internal::Condition) == sizeof(Condition));
   switch (cond) {
     case Condition::kEqual:
       return eq;
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 525d948024f..e93fc85878e 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -315,7 +315,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ CompareObjectType(r0, r4, r5, FIRST_JS_RECEIVER_TYPE);
   __ b(ge, &leave_and_return);
   __ b(&use_receiver);
@@ -971,10 +971,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ cmp(bytecode, Operand(0x3));
   __ b(hi, &process_bytecode);
@@ -1067,7 +1067,7 @@ namespace {
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array,
                       Register scratch) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   DCHECK(!AreAliased(bytecode_array, scratch));
   __ mov(scratch, Operand(0));
   __ strh(scratch,
@@ -1908,14 +1908,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ ldr(kContextRegister,
          MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2316,7 +2316,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ ldr(r3, __ ReceiverOperand(r0));
       __ JumpIfSmi(r3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CompareObjectType(r3, r4, r4, FIRST_JS_RECEIVER_TYPE);
       __ b(hs, &done_convert);
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -2721,7 +2721,7 @@ void Builtins::Generate_WasmDebugBreak(MacroAssembler* masm) {
   {
     FrameAndConstantPoolScope scope(masm, StackFrame::WASM_DEBUG_BREAK);
 
-    STATIC_ASSERT(DwVfpRegister::kNumRegisters == 32);
+    static_assert(DwVfpRegister::kNumRegisters == 32);
     constexpr DwVfpRegister last =
         WasmDebugBreakFrameConstants::kPushedFpRegs.last();
     constexpr DwVfpRegister first =
@@ -2958,7 +2958,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
           HeapNumber::kExponentBits);
   // Load scratch with exponent - 1. This is faster than loading
   // with exponent because Bias + 1 = 1024 which is an *ARM* immediate value.
-  STATIC_ASSERT(HeapNumber::kExponentBias + 1 == 1024);
+  static_assert(HeapNumber::kExponentBias + 1 == 1024);
   __ sub(scratch, scratch, Operand(HeapNumber::kExponentBias + 1));
   // If exponent is greater than or equal to 84, the 32 less significant
   // bits are 0s (2^84 = 1, 52 significant bits, 32 uncoded bits),
@@ -3161,13 +3161,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3253,14 +3253,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
@@ -3383,7 +3383,7 @@ void Generate_DeoptimizationEntry(MacroAssembler* masm,
 
   // Save all general purpose registers before messing with them.
   static constexpr int kNumberOfRegisters = Register::kNumRegisters;
-  STATIC_ASSERT(kNumberOfRegisters == 16);
+  static_assert(kNumberOfRegisters == 16);
 
   // Everything but pc, lr and ip which will be saved but not restored.
   RegList restored_regs = kJSCallerSaved | kCalleeSaved | RegList{ip};
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index 1a9ddc320f6..2cb666aad6a 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -387,7 +387,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ JumpIfObjectType(x0, x4, x5, FIRST_JS_RECEIVER_TYPE, &leave_and_return,
                       ge);
   __ B(&use_receiver);
@@ -675,9 +675,9 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Set up fp. It points to the {fp, lr} pair pushed as the last step in
   // PushCalleeSavedRegisters.
-  STATIC_ASSERT(
+  static_assert(
       EntryFrameConstants::kCalleeSavedRegisterBytesPushedAfterFpLrPair == 0);
-  STATIC_ASSERT(EntryFrameConstants::kOffsetToCalleeSavedRegisters == 0);
+  static_assert(EntryFrameConstants::kOffsetToCalleeSavedRegisters == 0);
   __ Mov(fp, sp);
 
   // Build an entry frame (see layout below).
@@ -1140,10 +1140,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ Cmp(bytecode, Operand(0x3));
   __ B(hi, &process_bytecode);
@@ -1233,7 +1233,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 namespace {
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ Strh(wzr,
           FieldMemOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset));
 }
@@ -1481,7 +1481,7 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
 
   // Push actual argument count, bytecode array, Smi tagged bytecode array
   // offset and an undefined (to properly align the stack pointer).
-  STATIC_ASSERT(TurboAssembler::kExtraSlotClaimedByPrologue == 1);
+  static_assert(TurboAssembler::kExtraSlotClaimedByPrologue == 1);
   __ SmiTag(x6, kInterpreterBytecodeOffsetRegister);
   __ Push(kJavaScriptCallArgCountRegister, kInterpreterBytecodeArrayRegister);
   __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
@@ -2147,14 +2147,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ ldr(kContextRegister,
          MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2644,7 +2644,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ Peek(x3, __ ReceiverOperand(x0));
       __ JumpIfSmi(x3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CompareObjectType(x3, x4, x4, FIRST_JS_RECEIVER_TYPE);
       __ B(hs, &done_convert);
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3496,7 +3496,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
 
   // Save the callee-save registers we are going to use.
   // TODO(all): Is this necessary? ARM doesn't do it.
-  STATIC_ASSERT(kCallApiFunctionSpillSpace == 4);
+  static_assert(kCallApiFunctionSpillSpace == 4);
   __ Poke(x19, (spill_offset + 0) * kXRegSize);
   __ Poke(x20, (spill_offset + 1) * kXRegSize);
   __ Poke(x21, (spill_offset + 2) * kXRegSize);
@@ -3614,13 +3614,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3714,14 +3714,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 }
 
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/builtins-array-gen.cc b/src/builtins/builtins-array-gen.cc
index 52cde978351..04747162769 100644
--- a/src/builtins/builtins-array-gen.cc
+++ b/src/builtins/builtins-array-gen.cc
@@ -685,10 +685,10 @@ void ArrayIncludesIndexofAssembler::Generate(SearchVariant variant,
 
   TNode<Int32T> elements_kind = LoadElementsKind(array);
   TNode<FixedArrayBase> elements = LoadElements(array);
-  STATIC_ASSERT(PACKED_SMI_ELEMENTS == 0);
-  STATIC_ASSERT(HOLEY_SMI_ELEMENTS == 1);
-  STATIC_ASSERT(PACKED_ELEMENTS == 2);
-  STATIC_ASSERT(HOLEY_ELEMENTS == 3);
+  static_assert(PACKED_SMI_ELEMENTS == 0);
+  static_assert(HOLEY_SMI_ELEMENTS == 1);
+  static_assert(PACKED_ELEMENTS == 2);
+  static_assert(HOLEY_ELEMENTS == 3);
   GotoIf(IsElementsKindLessThanOrEqual(elements_kind, HOLEY_ELEMENTS),
          &if_smiorobjects);
   GotoIf(
@@ -1740,12 +1740,12 @@ void ArrayBuiltinsAssembler::CreateArrayDispatchSingleArgument(
     TNode<Smi> transition_info = LoadTransitionInfo(*allocation_site);
 
     // Least significant bit in fast array elements kind means holeyness.
-    STATIC_ASSERT(PACKED_SMI_ELEMENTS == 0);
-    STATIC_ASSERT(HOLEY_SMI_ELEMENTS == 1);
-    STATIC_ASSERT(PACKED_ELEMENTS == 2);
-    STATIC_ASSERT(HOLEY_ELEMENTS == 3);
-    STATIC_ASSERT(PACKED_DOUBLE_ELEMENTS == 4);
-    STATIC_ASSERT(HOLEY_DOUBLE_ELEMENTS == 5);
+    static_assert(PACKED_SMI_ELEMENTS == 0);
+    static_assert(HOLEY_SMI_ELEMENTS == 1);
+    static_assert(PACKED_ELEMENTS == 2);
+    static_assert(HOLEY_ELEMENTS == 3);
+    static_assert(PACKED_DOUBLE_ELEMENTS == 4);
+    static_assert(HOLEY_DOUBLE_ELEMENTS == 5);
 
     Label normal_sequence(this);
     TVARIABLE(Int32T, var_elements_kind,
diff --git a/src/builtins/builtins-array.cc b/src/builtins/builtins-array.cc
index 020a7f4dd88..28e3494593c 100644
--- a/src/builtins/builtins-array.cc
+++ b/src/builtins/builtins-array.cc
@@ -1464,8 +1464,8 @@ MaybeHandle<JSArray> Fast_ArrayConcat(Isolate* isolate,
   }
   // We shouldn't overflow when adding another len.
   const int kHalfOfMaxInt = 1 << (kBitsPerInt - 2);
-  STATIC_ASSERT(FixedArray::kMaxLength < kHalfOfMaxInt);
-  STATIC_ASSERT(FixedDoubleArray::kMaxLength < kHalfOfMaxInt);
+  static_assert(FixedArray::kMaxLength < kHalfOfMaxInt);
+  static_assert(FixedDoubleArray::kMaxLength < kHalfOfMaxInt);
   USE(kHalfOfMaxInt);
 
   int n_arguments = args->length();
diff --git a/src/builtins/builtins-async-gen.cc b/src/builtins/builtins-async-gen.cc
index 03e4b32d06b..6336654391d 100644
--- a/src/builtins/builtins-async-gen.cc
+++ b/src/builtins/builtins-async-gen.cc
@@ -160,7 +160,7 @@ void AsyncBuiltinsAssembler::InitializeNativeClosure(
              IntPtrEqual(LoadMapInstanceSizeInWords(function_map),
                          IntPtrConstant(JSFunction::kSizeWithoutPrototype /
                                         kTaggedSize)));
-  STATIC_ASSERT(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
+  static_assert(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
   StoreMapNoWriteBarrier(function, function_map);
   StoreObjectFieldRoot(function, JSObject::kPropertiesOrHashOffset,
                        RootIndex::kEmptyFixedArray);
diff --git a/src/builtins/builtins-bigint-gen.h b/src/builtins/builtins-bigint-gen.h
index 288418258bf..327c666c67a 100644
--- a/src/builtins/builtins-bigint-gen.h
+++ b/src/builtins/builtins-bigint-gen.h
@@ -29,7 +29,7 @@ class BigIntBuiltinsAssembler : public CodeStubAssembler {
 
   void WriteBigIntSignAndLength(TNode<BigInt> bigint, TNode<Uint32T> sign,
                                 TNode<IntPtrT> length) {
-    STATIC_ASSERT(BigIntBase::SignBits::kShift == 0);
+    static_assert(BigIntBase::SignBits::kShift == 0);
     TNode<Uint32T> bitfield = Unsigned(
         Word32Or(Word32Shl(TruncateIntPtrToInt32(length),
                            Int32Constant(BigIntBase::LengthBits::kShift)),
diff --git a/src/builtins/builtins-call-gen.cc b/src/builtins/builtins-call-gen.cc
index 8b7b364375f..834a6371867 100644
--- a/src/builtins/builtins-call-gen.cc
+++ b/src/builtins/builtins-call-gen.cc
@@ -226,13 +226,13 @@ void CallOrConstructBuiltinsAssembler::CallOrConstructWithArrayLike(
         LoadAndUntagToWord32ObjectField(js_object, JSArray::kLengthOffset);
 
     // Holey arrays and double backing stores need special treatment.
-    STATIC_ASSERT(PACKED_SMI_ELEMENTS == 0);
-    STATIC_ASSERT(HOLEY_SMI_ELEMENTS == 1);
-    STATIC_ASSERT(PACKED_ELEMENTS == 2);
-    STATIC_ASSERT(HOLEY_ELEMENTS == 3);
-    STATIC_ASSERT(PACKED_DOUBLE_ELEMENTS == 4);
-    STATIC_ASSERT(HOLEY_DOUBLE_ELEMENTS == 5);
-    STATIC_ASSERT(LAST_FAST_ELEMENTS_KIND == HOLEY_DOUBLE_ELEMENTS);
+    static_assert(PACKED_SMI_ELEMENTS == 0);
+    static_assert(HOLEY_SMI_ELEMENTS == 1);
+    static_assert(PACKED_ELEMENTS == 2);
+    static_assert(HOLEY_ELEMENTS == 3);
+    static_assert(PACKED_DOUBLE_ELEMENTS == 4);
+    static_assert(HOLEY_DOUBLE_ELEMENTS == 5);
+    static_assert(LAST_FAST_ELEMENTS_KIND == HOLEY_DOUBLE_ELEMENTS);
 
     Branch(Word32And(kind, Int32Constant(1)), &if_holey_array, &if_done);
   }
diff --git a/src/builtins/builtins-collections-gen.cc b/src/builtins/builtins-collections-gen.cc
index d590ffdce77..78b02290117 100644
--- a/src/builtins/builtins-collections-gen.cc
+++ b/src/builtins/builtins-collections-gen.cc
@@ -359,7 +359,7 @@ RootIndex BaseCollectionsAssembler::GetAddFunctionNameIndex(Variant variant) {
 void BaseCollectionsAssembler::GotoIfInitialAddFunctionModified(
     Variant variant, TNode<NativeContext> native_context,
     TNode<HeapObject> collection, Label* if_modified) {
-  STATIC_ASSERT(JSCollection::kAddFunctionDescriptorIndex ==
+  static_assert(JSCollection::kAddFunctionDescriptorIndex ==
                 JSWeakCollection::kAddFunctionDescriptorIndex);
 
   // TODO(jgruber): Investigate if this should also fall back to full prototype
@@ -368,7 +368,7 @@ void BaseCollectionsAssembler::GotoIfInitialAddFunctionModified(
       PrototypeCheckAssembler::kCheckPrototypePropertyConstness};
 
   static constexpr int kNoContextIndex = -1;
-  STATIC_ASSERT(
+  static_assert(
       (flags & PrototypeCheckAssembler::kCheckPrototypePropertyIdentity) == 0);
 
   using DescriptorIndexNameValue =
@@ -1438,11 +1438,11 @@ TF_BUILTIN(OrderedHashTableHealIndex, CollectionsBuiltinsAssembler) {
   GotoIfNot(SmiLessThan(SmiConstant(0), index), &return_zero);
 
   // Check if the {table} was cleared.
-  STATIC_ASSERT(OrderedHashMap::NumberOfDeletedElementsOffset() ==
+  static_assert(OrderedHashMap::NumberOfDeletedElementsOffset() ==
                 OrderedHashSet::NumberOfDeletedElementsOffset());
   TNode<IntPtrT> number_of_deleted_elements = LoadAndUntagObjectField(
       table, OrderedHashMap::NumberOfDeletedElementsOffset());
-  STATIC_ASSERT(OrderedHashMap::kClearedTableSentinel ==
+  static_assert(OrderedHashMap::kClearedTableSentinel ==
                 OrderedHashSet::kClearedTableSentinel);
   GotoIf(IntPtrEqual(number_of_deleted_elements,
                      IntPtrConstant(OrderedHashMap::kClearedTableSentinel)),
@@ -1456,7 +1456,7 @@ TF_BUILTIN(OrderedHashTableHealIndex, CollectionsBuiltinsAssembler) {
   {
     TNode<IntPtrT> i = var_i.value();
     GotoIfNot(IntPtrLessThan(i, number_of_deleted_elements), &return_index);
-    STATIC_ASSERT(OrderedHashMap::RemovedHolesIndex() ==
+    static_assert(OrderedHashMap::RemovedHolesIndex() ==
                   OrderedHashSet::RemovedHolesIndex());
     TNode<Smi> removed_index = CAST(LoadFixedArrayElement(
         CAST(table), i, OrderedHashMap::RemovedHolesIndex() * kTaggedSize));
@@ -1685,7 +1685,7 @@ TF_BUILTIN(MapPrototypeSet, CollectionsBuiltinsAssembler) {
     number_of_buckets = SmiUntag(CAST(UnsafeLoadFixedArrayElement(
         table, OrderedHashMap::NumberOfBucketsIndex())));
 
-    STATIC_ASSERT(OrderedHashMap::kLoadFactor == 2);
+    static_assert(OrderedHashMap::kLoadFactor == 2);
     const TNode<WordT> capacity = WordShl(number_of_buckets.value(), 1);
     const TNode<IntPtrT> number_of_elements = SmiUntag(
         CAST(LoadObjectField(table, OrderedHashMap::NumberOfElementsOffset())));
@@ -1860,7 +1860,7 @@ TF_BUILTIN(SetPrototypeAdd, CollectionsBuiltinsAssembler) {
     number_of_buckets = SmiUntag(CAST(UnsafeLoadFixedArrayElement(
         table, OrderedHashSet::NumberOfBucketsIndex())));
 
-    STATIC_ASSERT(OrderedHashSet::kLoadFactor == 2);
+    static_assert(OrderedHashSet::kLoadFactor == 2);
     const TNode<WordT> capacity = WordShl(number_of_buckets.value(), 1);
     const TNode<IntPtrT> number_of_elements = SmiUntag(
         CAST(LoadObjectField(table, OrderedHashSet::NumberOfElementsOffset())));
diff --git a/src/builtins/builtins-constructor-gen.cc b/src/builtins/builtins-constructor-gen.cc
index 65ee03778c8..ccab4b7afbd 100644
--- a/src/builtins/builtins-constructor-gen.cc
+++ b/src/builtins/builtins-constructor-gen.cc
@@ -246,7 +246,7 @@ TF_BUILTIN(FastNewClosure, ConstructorBuiltinsAssembler) {
     BIND(&done);
   }
 
-  STATIC_ASSERT(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
+  static_assert(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
   StoreObjectFieldNoWriteBarrier(result, JSFunction::kFeedbackCellOffset,
                                  feedback_cell);
   StoreObjectFieldNoWriteBarrier(result, JSFunction::kSharedFunctionInfoOffset,
@@ -399,14 +399,14 @@ TNode<JSRegExp> ConstructorBuiltinsAssembler::CreateRegExpLiteral(
       CAST(LoadFeedbackVectorSlot(feedback_vector, slot));
   GotoIfNot(HasBoilerplate(literal_site), &call_runtime);
   {
-    STATIC_ASSERT(JSRegExp::kDataOffset == JSObject::kHeaderSize);
-    STATIC_ASSERT(JSRegExp::kSourceOffset ==
+    static_assert(JSRegExp::kDataOffset == JSObject::kHeaderSize);
+    static_assert(JSRegExp::kSourceOffset ==
                   JSRegExp::kDataOffset + kTaggedSize);
-    STATIC_ASSERT(JSRegExp::kFlagsOffset ==
+    static_assert(JSRegExp::kFlagsOffset ==
                   JSRegExp::kSourceOffset + kTaggedSize);
-    STATIC_ASSERT(JSRegExp::kHeaderSize ==
+    static_assert(JSRegExp::kHeaderSize ==
                   JSRegExp::kFlagsOffset + kTaggedSize);
-    STATIC_ASSERT(JSRegExp::kLastIndexOffset == JSRegExp::kHeaderSize);
+    static_assert(JSRegExp::kLastIndexOffset == JSRegExp::kHeaderSize);
     DCHECK_EQ(JSRegExp::Size(), JSRegExp::kLastIndexOffset + kTaggedSize);
 
     TNode<RegExpBoilerplateDescription> boilerplate = CAST(literal_site);
@@ -593,7 +593,7 @@ TNode<HeapObject> ConstructorBuiltinsAssembler::CreateShallowObjectLiteral(
 
   // Ensure new-space allocation for a fresh JSObject so we can skip write
   // barriers when copying all object fields.
-  STATIC_ASSERT(JSObject::kMaxInstanceSize < kMaxRegularHeapObjectSize);
+  static_assert(JSObject::kMaxInstanceSize < kMaxRegularHeapObjectSize);
   TNode<IntPtrT> instance_size =
       TimesTaggedSize(LoadMapInstanceSizeInWords(boilerplate_map));
   TNode<IntPtrT> allocation_size = instance_size;
@@ -676,7 +676,7 @@ TNode<JSObject> ConstructorBuiltinsAssembler::CreateEmptyObjectLiteral(
   TNode<NativeContext> native_context = LoadNativeContext(context);
   TNode<Map> map = LoadObjectFunctionInitialMap(native_context);
   // Ensure that slack tracking is disabled for the map.
-  STATIC_ASSERT(Map::kNoSlackTracking == 0);
+  static_assert(Map::kNoSlackTracking == 0);
   CSA_DCHECK(this, IsClearWord32<Map::Bits3::ConstructionCounterBits>(
                        LoadMapBitField3(map)));
   TNode<FixedArray> empty_fixed_array = EmptyFixedArrayConstant();
diff --git a/src/builtins/builtins-constructor.h b/src/builtins/builtins-constructor.h
index e3fc416771a..6e0dbd412ed 100644
--- a/src/builtins/builtins-constructor.h
+++ b/src/builtins/builtins-constructor.h
@@ -37,7 +37,7 @@ class ConstructorBuiltins {
 
   // FastNewFunctionContext can only allocate closures which fit in the
   // new space.
-  STATIC_ASSERT(Context::SizeFor(kMaximumSlots + Context::MIN_CONTEXT_SLOTS) <
+  static_assert(Context::SizeFor(kMaximumSlots + Context::MIN_CONTEXT_SLOTS) <
                 kMaxRegularHeapObjectSize);
 };
 
diff --git a/src/builtins/builtins-handler-gen.cc b/src/builtins/builtins-handler-gen.cc
index cc94990e842..5c6c29396be 100644
--- a/src/builtins/builtins-handler-gen.cc
+++ b/src/builtins/builtins-handler-gen.cc
@@ -92,7 +92,7 @@ void Builtins::Generate_DefineNamedOwnIC_NoFeedback(
 void HandlerBuiltinsAssembler::DispatchForElementsKindTransition(
     TNode<Int32T> from_kind, TNode<Int32T> to_kind,
     const ElementsKindTransitionSwitchCase& case_function) {
-  STATIC_ASSERT(sizeof(ElementsKind) == sizeof(uint8_t));
+  static_assert(sizeof(ElementsKind) == sizeof(uint8_t));
 
   Label next(this), if_unknown_type(this, Label::kDeferred);
 
@@ -111,7 +111,7 @@ void HandlerBuiltinsAssembler::DispatchForElementsKindTransition(
       ELEMENTS_KIND_TRANSITIONS(ELEMENTS_KINDS_CASE)
 #undef ELEMENTS_KINDS_CASE
   };
-  STATIC_ASSERT(arraysize(combined_elements_kinds) ==
+  static_assert(arraysize(combined_elements_kinds) ==
                 arraysize(elements_kind_labels));
 
   TNode<Int32T> combined_elements_kind =
@@ -247,7 +247,7 @@ void HandlerBuiltinsAssembler::DispatchByElementsKind(
       ELEMENTS_KINDS(ELEMENTS_KINDS_CASE)
 #undef ELEMENTS_KINDS_CASE
   };
-  STATIC_ASSERT(arraysize(elements_kinds) == arraysize(elements_kind_labels));
+  static_assert(arraysize(elements_kinds) == arraysize(elements_kind_labels));
 
   // TODO(mythria): Do not emit cases for typed elements kind when
   // handle_typed_elements is false to decrease the size of the jump table.
diff --git a/src/builtins/builtins-internal-gen.cc b/src/builtins/builtins-internal-gen.cc
index 8538c518230..c2fbfc60c94 100644
--- a/src/builtins/builtins-internal-gen.cc
+++ b/src/builtins/builtins-internal-gen.cc
@@ -717,7 +717,7 @@ TF_BUILTIN(DeleteProperty, DeletePropertyBaseAssembler) {
 
     BIND(&dont_delete);
     {
-      STATIC_ASSERT(LanguageModeSize == 2);
+      static_assert(LanguageModeSize == 2);
       GotoIf(SmiNotEqual(language_mode, SmiConstant(LanguageMode::kSloppy)),
              &slow);
       Return(FalseConstant());
diff --git a/src/builtins/builtins-intl-gen.cc b/src/builtins/builtins-intl-gen.cc
index dd0410ccd21..217c41e7b85 100644
--- a/src/builtins/builtins-intl-gen.cc
+++ b/src/builtins/builtins-intl-gen.cc
@@ -31,7 +31,7 @@ class IntlBuiltinsAssembler : public CodeStubAssembler {
   TNode<IntPtrT> PointerToSeqStringData(TNode<String> seq_string) {
     CSA_DCHECK(this,
                IsSequentialStringInstanceType(LoadInstanceType(seq_string)));
-    STATIC_ASSERT(SeqOneByteString::kHeaderSize ==
+    static_assert(SeqOneByteString::kHeaderSize ==
                   SeqTwoByteString::kHeaderSize);
     return IntPtrAdd(
         BitcastTaggedToWord(seq_string),
diff --git a/src/builtins/builtins-regexp-gen.cc b/src/builtins/builtins-regexp-gen.cc
index 38dc47a122f..69c6cd747e5 100644
--- a/src/builtins/builtins-regexp-gen.cc
+++ b/src/builtins/builtins-regexp-gen.cc
@@ -478,7 +478,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
       };
       Label* labels[] = {&next, &atom, &next};
 
-      STATIC_ASSERT(arraysize(values) == arraysize(labels));
+      static_assert(arraysize(values) == arraysize(labels));
       Switch(tag, &unreachable, values, labels, arraysize(values));
 
       BIND(&unreachable);
@@ -493,7 +493,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
         data, JSRegExp::kIrregexpCaptureCountIndex));
 
     const int kOffsetsSize = Isolate::kJSRegexpStaticOffsetsVectorSize;
-    STATIC_ASSERT(kOffsetsSize >= 2);
+    static_assert(kOffsetsSize >= 2);
     GotoIf(SmiAbove(capture_count, SmiConstant(kOffsetsSize / 2 - 1)),
            &runtime);
   }
@@ -603,7 +603,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
         data, JSRegExp::kIrregexpCaptureCountIndex));
     // capture_count is the number of captures without the match itself.
     // Required registers = (capture_count + 1) * 2.
-    STATIC_ASSERT(Internals::IsValidSmi((JSRegExp::kMaxCaptures + 1) * 2));
+    static_assert(Internals::IsValidSmi((JSRegExp::kMaxCaptures + 1) * 2));
     TNode<Smi> register_count =
         SmiShl(SmiAdd(capture_count, SmiConstant(1)), 1);
 
@@ -672,7 +672,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
 
     // Check that the last match info has space for the capture registers and
     // the additional information. Ensure no overflow in add.
-    STATIC_ASSERT(FixedArray::kMaxLength < kMaxInt - FixedArray::kLengthOffset);
+    static_assert(FixedArray::kMaxLength < kMaxInt - FixedArray::kLengthOffset);
     TNode<Smi> available_slots =
         SmiSub(LoadFixedArrayBaseLength(match_info),
                SmiConstant(RegExpMatchInfo::kLastMatchOverhead));
@@ -974,7 +974,7 @@ TF_BUILTIN(RegExpExecAtom, RegExpBuiltinsAssembler) {
                                      LoadStringLengthAsWord(subject_string)));
 
     const int kNumRegisters = 2;
-    STATIC_ASSERT(RegExpMatchInfo::kInitialCaptureIndices >= kNumRegisters);
+    static_assert(RegExpMatchInfo::kInitialCaptureIndices >= kNumRegisters);
 
     const TNode<Smi> match_to =
         SmiAdd(match_from, LoadStringLengthAsSmi(needle_string));
@@ -1430,7 +1430,7 @@ TNode<Number> RegExpBuiltinsAssembler::AdvanceStringIndex(
   if (is_fastpath) {
     // Must be in Smi range on the fast path. We control the value of {index}
     // on all call-sites and can never exceed the length of the string.
-    STATIC_ASSERT(String::kMaxLength + 2 < Smi::kMaxValue);
+    static_assert(String::kMaxLength + 2 < Smi::kMaxValue);
     CSA_DCHECK(this, TaggedIsPositiveSmi(index_plus_one));
   }
 
diff --git a/src/builtins/builtins-sharedarraybuffer-gen.cc b/src/builtins/builtins-sharedarraybuffer-gen.cc
index 42dd79604bd..acda3b31a1e 100644
--- a/src/builtins/builtins-sharedarraybuffer-gen.cc
+++ b/src/builtins/builtins-sharedarraybuffer-gen.cc
@@ -73,18 +73,18 @@ void SharedArrayBufferBuiltinsAssembler::ValidateIntegerTypedArray(
   GotoIf(IsDetachedBuffer(array_buffer), detached);
 
   // Fail if the array's element type is float32, float64 or clamped.
-  STATIC_ASSERT(INT8_ELEMENTS < FLOAT32_ELEMENTS);
-  STATIC_ASSERT(INT16_ELEMENTS < FLOAT32_ELEMENTS);
-  STATIC_ASSERT(INT32_ELEMENTS < FLOAT32_ELEMENTS);
-  STATIC_ASSERT(UINT8_ELEMENTS < FLOAT32_ELEMENTS);
-  STATIC_ASSERT(UINT16_ELEMENTS < FLOAT32_ELEMENTS);
-  STATIC_ASSERT(UINT32_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(INT8_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(INT16_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(INT32_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(UINT8_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(UINT16_ELEMENTS < FLOAT32_ELEMENTS);
+  static_assert(UINT32_ELEMENTS < FLOAT32_ELEMENTS);
   TNode<Int32T> elements_kind =
       GetNonRabGsabElementsKind(LoadMapElementsKind(map));
   GotoIf(Int32LessThan(elements_kind, Int32Constant(FLOAT32_ELEMENTS)),
          &not_float_or_clamped);
-  STATIC_ASSERT(BIGINT64_ELEMENTS > UINT8_CLAMPED_ELEMENTS);
-  STATIC_ASSERT(BIGUINT64_ELEMENTS > UINT8_CLAMPED_ELEMENTS);
+  static_assert(BIGINT64_ELEMENTS > UINT8_CLAMPED_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS > UINT8_CLAMPED_ELEMENTS);
   Branch(Int32GreaterThan(elements_kind, Int32Constant(UINT8_CLAMPED_ELEMENTS)),
          &not_float_or_clamped, &invalid);
 
@@ -308,8 +308,8 @@ TF_BUILTIN(AtomicsStore, SharedArrayBufferBuiltinsAssembler) {
   // 3. Let arrayTypeName be typedArray.[[TypedArrayName]].
   // 4. If arrayTypeName is "BigUint64Array" or "BigInt64Array",
   //    let v be ? ToBigInt(value).
-  STATIC_ASSERT(BIGINT64_ELEMENTS > INT32_ELEMENTS);
-  STATIC_ASSERT(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
   GotoIf(Int32GreaterThan(elements_kind, Int32Constant(INT32_ELEMENTS)), &u64);
 
   // 5. Otherwise, let v be ? ToInteger(value).
@@ -432,8 +432,8 @@ TF_BUILTIN(AtomicsExchange, SharedArrayBufferBuiltinsAssembler) {
 
   // 3. Let arrayTypeName be typedArray.[[TypedArrayName]].
   // 4. If typedArray.[[ContentType]] is BigInt, let v be ? ToBigInt(value).
-  STATIC_ASSERT(BIGINT64_ELEMENTS > INT32_ELEMENTS);
-  STATIC_ASSERT(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
   GotoIf(Int32GreaterThan(elements_kind, Int32Constant(INT32_ELEMENTS)), &big);
 
   // 5. Otherwise, let v be ? ToInteger(value).
@@ -569,8 +569,8 @@ TF_BUILTIN(AtomicsCompareExchange, SharedArrayBufferBuiltinsAssembler) {
   // 4. If typedArray.[[ContentType]] is BigInt, then
   //   a. Let expected be ? ToBigInt(expectedValue).
   //   b. Let replacement be ? ToBigInt(replacementValue).
-  STATIC_ASSERT(BIGINT64_ELEMENTS > INT32_ELEMENTS);
-  STATIC_ASSERT(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
   GotoIf(Int32GreaterThan(elements_kind, Int32Constant(INT32_ELEMENTS)), &big);
 
   // 5. Else,
@@ -736,8 +736,8 @@ void SharedArrayBufferBuiltinsAssembler::AtomicBinopBuiltinCommon(
 
   // 3. Let arrayTypeName be typedArray.[[TypedArrayName]].
   // 4. If typedArray.[[ContentType]] is BigInt, let v be ? ToBigInt(value).
-  STATIC_ASSERT(BIGINT64_ELEMENTS > INT32_ELEMENTS);
-  STATIC_ASSERT(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGINT64_ELEMENTS > INT32_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS > INT32_ELEMENTS);
   GotoIf(Int32GreaterThan(elements_kind, Int32Constant(INT32_ELEMENTS)), &big);
 
   // 5. Otherwise, let v be ? ToInteger(value).
diff --git a/src/builtins/builtins-string-gen.cc b/src/builtins/builtins-string-gen.cc
index 882c9be6aa1..c54769c3361 100644
--- a/src/builtins/builtins-string-gen.cc
+++ b/src/builtins/builtins-string-gen.cc
@@ -189,8 +189,8 @@ void StringBuiltinsAssembler::StringEqual_Core(
 
   // Check if both {lhs} and {rhs} are direct strings, and that in case of
   // ExternalStrings the data pointer is cached.
-  STATIC_ASSERT(kUncachedExternalStringTag != 0);
-  STATIC_ASSERT(kIsIndirectStringTag != 0);
+  static_assert(kUncachedExternalStringTag != 0);
+  static_assert(kIsIndirectStringTag != 0);
   int const kBothDirectStringMask =
       kIsIndirectStringMask | kUncachedExternalStringMask |
       ((kIsIndirectStringMask | kUncachedExternalStringMask) << 8);
@@ -320,8 +320,8 @@ TNode<String> StringBuiltinsAssembler::AllocateConsString(TNode<Uint32T> length,
 
   // Determine the resulting ConsString map to use depending on whether
   // any of {left} or {right} has two byte encoding.
-  STATIC_ASSERT(kOneByteStringTag != 0);
-  STATIC_ASSERT(kTwoByteStringTag == 0);
+  static_assert(kOneByteStringTag != 0);
+  static_assert(kTwoByteStringTag == 0);
   TNode<Int32T> combined_instance_type =
       Word32And(left_instance_type, right_instance_type);
   TNode<Map> result_map = CAST(Select<Object>(
@@ -473,7 +473,7 @@ void StringBuiltinsAssembler::DerefIndirectString(TVariable<String>* var_string,
   BIND(&can_deref);
 #endif  // DEBUG
 
-  STATIC_ASSERT(static_cast<int>(ThinString::kActualOffset) ==
+  static_assert(static_cast<int>(ThinString::kActualOffset) ==
                 static_cast<int>(ConsString::kFirstOffset));
   *var_string =
       LoadObjectField<String>(var_string->value(), ThinString::kActualOffset);
@@ -523,7 +523,7 @@ TNode<String> StringBuiltinsAssembler::DerefIndirectString(
   Label deref(this);
   BranchIfCanDerefIndirectString(string, instance_type, &deref, cannot_deref);
   BIND(&deref);
-  STATIC_ASSERT(static_cast<int>(ThinString::kActualOffset) ==
+  static_assert(static_cast<int>(ThinString::kActualOffset) ==
                 static_cast<int>(ConsString::kFirstOffset));
   return LoadObjectField<String>(string, ThinString::kActualOffset);
 }
@@ -1539,7 +1539,7 @@ void StringBuiltinsAssembler::CopyStringCharacters(
 
   ElementsKind from_kind = from_one_byte ? UINT8_ELEMENTS : UINT16_ELEMENTS;
   ElementsKind to_kind = to_one_byte ? UINT8_ELEMENTS : UINT16_ELEMENTS;
-  STATIC_ASSERT(SeqOneByteString::kHeaderSize == SeqTwoByteString::kHeaderSize);
+  static_assert(SeqOneByteString::kHeaderSize == SeqTwoByteString::kHeaderSize);
   int header_size = SeqOneByteString::kHeaderSize - kHeapObjectTag;
   TNode<IntPtrT> from_offset =
       ElementOffsetFromIndex(from_index, from_kind, header_size);
diff --git a/src/builtins/builtins-typed-array-gen.cc b/src/builtins/builtins-typed-array-gen.cc
index 7d1633fe3c2..e3806f4353c 100644
--- a/src/builtins/builtins-typed-array-gen.cc
+++ b/src/builtins/builtins-typed-array-gen.cc
@@ -196,7 +196,7 @@ TNode<BoolT> TypedArrayBuiltinsAssembler::IsUint8ElementsKind(
 
 TNode<BoolT> TypedArrayBuiltinsAssembler::IsBigInt64ElementsKind(
     TNode<Int32T> kind) {
-  STATIC_ASSERT(BIGUINT64_ELEMENTS + 1 == BIGINT64_ELEMENTS);
+  static_assert(BIGUINT64_ELEMENTS + 1 == BIGINT64_ELEMENTS);
   return Word32Or(
       IsElementsKindInRange(kind, BIGUINT64_ELEMENTS, BIGINT64_ELEMENTS),
       IsElementsKindInRange(kind, RAB_GSAB_BIGUINT64_ELEMENTS,
@@ -399,7 +399,7 @@ void TypedArrayBuiltinsAssembler::DispatchTypedArrayByElementsKind(
       TYPED_ARRAYS(TYPED_ARRAY_CASE) RAB_GSAB_TYPED_ARRAYS(TYPED_ARRAY_CASE)
 #undef TYPED_ARRAY_CASE
   };
-  STATIC_ASSERT(arraysize(elements_kinds) == arraysize(elements_kind_labels));
+  static_assert(arraysize(elements_kinds) == arraysize(elements_kind_labels));
 
   Switch(elements_kind, &if_unknown_type, elements_kinds, elements_kind_labels,
          arraysize(elements_kinds));
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index e1b216b602a..ac70aacd08e 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -42,9 +42,9 @@ struct BuiltinMetadata {
     interpreter::OperandScale scale : 8;
   };
 
-  STATIC_ASSERT(sizeof(interpreter::Bytecode) == 1);
-  STATIC_ASSERT(sizeof(interpreter::OperandScale) == 1);
-  STATIC_ASSERT(sizeof(BytecodeAndScale) <= sizeof(Address));
+  static_assert(sizeof(interpreter::Bytecode) == 1);
+  static_assert(sizeof(interpreter::OperandScale) == 1);
+  static_assert(sizeof(BytecodeAndScale) <= sizeof(Address));
 
   // The `data` field has kind-specific contents.
   union KindSpecificData {
@@ -347,7 +347,7 @@ void Builtins::EmitCodeCreateEvents(Isolate* isolate) {
                                      Builtins::name(FromInt(i))));
   }
 
-  STATIC_ASSERT(kLastBytecodeHandlerPlusOne == kBuiltinCount);
+  static_assert(kLastBytecodeHandlerPlusOne == kBuiltinCount);
   for (; i < kBuiltinCount; i++) {
     Code builtin_code = FromCodeT(CodeT::cast(Object(builtins[i])));
     Handle<AbstractCode> code(AbstractCode::cast(builtin_code), isolate);
diff --git a/src/builtins/builtins.h b/src/builtins/builtins.h
index f1922a037f4..4e8e90d5dc1 100644
--- a/src/builtins/builtins.h
+++ b/src/builtins/builtins.h
@@ -90,13 +90,13 @@ class Builtins {
       kFirstWideBytecodeHandler + kNumberOfWideBytecodeHandlers;
   static constexpr int kLastBytecodeHandlerPlusOne =
       kFirstExtraWideBytecodeHandler + kNumberOfWideBytecodeHandlers;
-  STATIC_ASSERT(kLastBytecodeHandlerPlusOne == kBuiltinCount);
+  static_assert(kLastBytecodeHandlerPlusOne == kBuiltinCount);
 
   static constexpr bool IsBuiltinId(Builtin builtin) {
     return builtin != Builtin::kNoBuiltinId;
   }
   static constexpr bool IsBuiltinId(int maybe_id) {
-    STATIC_ASSERT(static_cast<int>(Builtin::kNoBuiltinId) == -1);
+    static_assert(static_cast<int>(Builtin::kNoBuiltinId) == -1);
     return static_cast<uint32_t>(maybe_id) <
            static_cast<uint32_t>(kBuiltinCount);
   }
@@ -201,7 +201,7 @@ class Builtins {
     return kAllBuiltinsAreIsolateIndependent;
   }
   static constexpr bool IsIsolateIndependent(Builtin builtin) {
-    STATIC_ASSERT(kAllBuiltinsAreIsolateIndependent);
+    static_assert(kAllBuiltinsAreIsolateIndependent);
     return kAllBuiltinsAreIsolateIndependent;
   }
 
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index c631d833aaa..0d433ac9e05 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -326,7 +326,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ CmpObjectType(eax, FIRST_JS_RECEIVER_TYPE, ecx);
   __ j(above_equal, &leave_and_return, Label::kNear);
   __ jmp(&use_receiver, Label::kNear);
@@ -928,10 +928,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ cmp(bytecode, Immediate(0x3));
   __ j(above, &process_bytecode, Label::kNear);
@@ -1035,7 +1035,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 namespace {
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ mov_w(FieldOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset),
            Immediate(0));
 }
@@ -2430,7 +2430,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ mov(ecx, args.GetReceiverOperand());
       __ JumpIfSmi(ecx, &convert_to_object, Label::kNear);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CmpObjectType(ecx, FIRST_JS_RECEIVER_TYPE, ecx);  // Clobbers ecx.
       __ j(above_equal, &done_convert);
       // Reload the receiver (it was clobbered by CmpObjectType).
@@ -2890,14 +2890,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ mov(kContextRegister,
          MemOperand(ebp, BaselineFrameConstants::kContextOffset));
@@ -3043,11 +3043,11 @@ void Builtins::Generate_CEntry(MacroAssembler* masm, int result_size,
   // If argv_mode == ArgvMode::kRegister:
   // ecx: pointer to the first argument
 
-  STATIC_ASSERT(eax == kRuntimeCallArgCountRegister);
-  STATIC_ASSERT(ecx == kRuntimeCallArgvRegister);
-  STATIC_ASSERT(edx == kRuntimeCallFunctionRegister);
-  STATIC_ASSERT(esi == kContextRegister);
-  STATIC_ASSERT(edi == kJSFunctionRegister);
+  static_assert(eax == kRuntimeCallArgCountRegister);
+  static_assert(ecx == kRuntimeCallArgvRegister);
+  static_assert(edx == kRuntimeCallFunctionRegister);
+  static_assert(esi == kContextRegister);
+  static_assert(edi == kJSFunctionRegister);
 
   DCHECK(!AreAliased(kRuntimeCallArgCountRegister, kRuntimeCallArgvRegister,
                      kRuntimeCallFunctionRegister, kContextRegister,
@@ -3463,13 +3463,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3556,14 +3556,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/loong64/builtins-loong64.cc b/src/builtins/loong64/builtins-loong64.cc
index f16ebb9a9f5..aa6ab5a0d72 100644
--- a/src/builtins/loong64/builtins-loong64.cc
+++ b/src/builtins/loong64/builtins-loong64.cc
@@ -304,7 +304,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
   __ GetObjectType(a0, t2, t2);
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ Branch(&leave_and_return, greater_equal, t2,
             Operand(FIRST_JS_RECEIVER_TYPE));
   __ Branch(&use_receiver);
@@ -942,10 +942,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ Branch(&process_bytecode, hi, bytecode, Operand(3));
   __ And(scratch2, bytecode, Operand(1));
@@ -1042,7 +1042,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 
 namespace {
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ St_h(zero_reg,
           FieldMemOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset));
 }
@@ -1855,14 +1855,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ Ld_d(kContextRegister,
           MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2305,7 +2305,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(a3, a0);
       __ JumpIfSmi(a3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ GetObjectType(a3, a4, a4);
       __ Branch(&done_convert, hs, a4, Operand(FIRST_JS_RECEIVER_TYPE));
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3184,13 +3184,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3287,14 +3287,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/mips/builtins-mips.cc b/src/builtins/mips/builtins-mips.cc
index 176e21d2f5f..26fb28df973 100644
--- a/src/builtins/mips/builtins-mips.cc
+++ b/src/builtins/mips/builtins-mips.cc
@@ -305,7 +305,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
   __ GetObjectType(v0, t2, t2);
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ Branch(&leave_and_return, greater_equal, t2,
             Operand(FIRST_JS_RECEIVER_TYPE));
   __ Branch(&use_receiver);
@@ -942,10 +942,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ Branch(&process_bytecode, hi, bytecode, Operand(3));
   __ And(scratch2, bytecode, Operand(1));
@@ -1040,7 +1040,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 namespace {
 void ResetBytecodeAge(MacroAssembler* masm,
                                  Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ sh(zero_reg,
         FieldMemOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset));
 }
@@ -1848,14 +1848,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ Lw(kContextRegister,
         MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2249,7 +2249,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(a3, a0);
       __ JumpIfSmi(a3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ GetObjectType(a3, t0, t0);
       __ Branch(&done_convert, hs, t0, Operand(FIRST_JS_RECEIVER_TYPE));
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3122,13 +3122,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3220,14 +3220,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index 346bb73964c..f6b956d8bd4 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -304,7 +304,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
   __ GetObjectType(v0, t2, t2);
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ Branch(&leave_and_return, greater_equal, t2,
             Operand(FIRST_JS_RECEIVER_TYPE));
   __ Branch(&use_receiver);
@@ -951,10 +951,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ Branch(&process_bytecode, hi, bytecode, Operand(3));
   __ And(scratch2, bytecode, Operand(1));
@@ -1047,7 +1047,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 
 namespace {
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ Sh(zero_reg,
         FieldMemOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset));
 }
@@ -1849,14 +1849,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ Ld(kContextRegister,
         MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2301,7 +2301,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(a3, a0);
       __ JumpIfSmi(a3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ GetObjectType(a3, a4, a4);
       __ Branch(&done_convert, hs, a4, Operand(FIRST_JS_RECEIVER_TYPE));
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3145,7 +3145,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
     __ li(s0, Operand(stack_space));
   } else {
     DCHECK_EQ(stack_space, 0);
-    STATIC_ASSERT(kCArgSlotCount == 0);
+    static_assert(kCArgSlotCount == 0);
     __ Ld(s0, *stack_space_operand);
   }
 
@@ -3205,13 +3205,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3308,14 +3308,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index 1201a20d8ed..72408a0bad6 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -81,7 +81,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array,
                       Register scratch) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   DCHECK(!AreAliased(bytecode_array, scratch));
   __ mov(scratch, Operand(0));
   __ StoreU16(
@@ -638,7 +638,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ CompareObjectType(r3, r7, r7, FIRST_JS_RECEIVER_TYPE);
   __ bge(&leave_and_return);
   __ b(&use_receiver);
@@ -1256,10 +1256,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ cmpi(bytecode, Operand(0x3));
   __ bgt(&process_bytecode);
@@ -2106,7 +2106,7 @@ void Builtins::Generate_NotifyDeoptimized(MacroAssembler* masm) {
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
@@ -2114,7 +2114,7 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 #if ENABLE_SPARKPLUG
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ LoadU64(kContextRegister,
              MemOperand(fp, BaselineFrameConstants::kContextOffset), r0);
@@ -2546,7 +2546,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(r6, r3);
       __ JumpIfSmi(r6, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CompareObjectType(r6, r7, r7, FIRST_JS_RECEIVER_TYPE);
       __ bge(&done_convert);
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3224,7 +3224,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   __ ExtractBitMask(scratch, scratch_high, HeapNumber::kExponentMask);
   // Load scratch with exponent - 1. This is faster than loading
   // with exponent because Bias + 1 = 1024 which is a *PPC* immediate value.
-  STATIC_ASSERT(HeapNumber::kExponentBias + 1 == 1024);
+  static_assert(HeapNumber::kExponentBias + 1 == 1024);
   __ subi(scratch, scratch, Operand(HeapNumber::kExponentBias + 1));
   // If exponent is greater than or equal to 84, the 32 less significant
   // bits are 0s (2^84 = 1, 52 significant bits, 32 uncoded bits),
@@ -3250,7 +3250,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   __ subfic(scratch, scratch, Operand(32));
   __ ExtractBitMask(result_reg, scratch_high, HeapNumber::kMantissaMask);
   // Set the implicit 1 before the mantissa part in scratch_high.
-  STATIC_ASSERT(HeapNumber::kMantissaBitsInTopWord >= 16);
+  static_assert(HeapNumber::kMantissaBitsInTopWord >= 16);
   __ oris(result_reg, result_reg,
           Operand(1 << ((HeapNumber::kMantissaBitsInTopWord)-16)));
   __ ShiftLeftU32(r0, result_reg, scratch);
@@ -3445,13 +3445,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3556,14 +3556,14 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   int apiStackSpace = 0;
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/riscv64/builtins-riscv64.cc b/src/builtins/riscv64/builtins-riscv64.cc
index 99ec54a67b8..b6a1f6dcb04 100644
--- a/src/builtins/riscv64/builtins-riscv64.cc
+++ b/src/builtins/riscv64/builtins-riscv64.cc
@@ -327,7 +327,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
     Register map = temps.Acquire(), type = temps.Acquire();
     __ GetObjectType(a0, map, type);
 
-    STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+    static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
     __ Branch(&leave_and_return, greater_equal, type,
               Operand(FIRST_JS_RECEIVER_TYPE));
     __ Branch(&use_receiver);
@@ -1000,10 +1000,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ Branch(&process_bytecode, Ugreater, bytecode, Operand(3),
             Label::Distance::kNear);
@@ -1106,7 +1106,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 
 namespace {
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ Sh(zero_reg,
         FieldMemOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset));
 }
@@ -1916,14 +1916,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ Ld(kContextRegister,
         MemOperand(fp, BaselineFrameConstants::kContextOffset));
@@ -2421,7 +2421,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(a3, a0);
       __ JumpIfSmi(a3, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ GetObjectType(a3, a4, a4);
       __ Branch(&done_convert, Ugreater_equal, a4,
                 Operand(FIRST_JS_RECEIVER_TYPE));
@@ -3217,7 +3217,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
     __ li(s3, Operand(stack_space));
   } else {
     DCHECK_EQ(stack_space, 0);
-    STATIC_ASSERT(kCArgSlotCount == 0);
+    static_assert(kCArgSlotCount == 0);
     __ Ld(s3, *stack_space_operand);
   }
 
@@ -3279,13 +3279,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3381,14 +3381,14 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index 7c30d9439a2..f1ca347c1a6 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -85,7 +85,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array,
                       Register scratch) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   DCHECK(!AreAliased(bytecode_array, scratch));
   __ mov(r0, Operand(0));
   __ StoreU16(
@@ -616,7 +616,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ CompareObjectType(r2, r6, r6, FIRST_JS_RECEIVER_TYPE);
   __ bge(&leave_and_return);
   __ b(&use_receiver);
@@ -1294,10 +1294,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ CmpS64(bytecode, Operand(0x3));
   __ bgt(&process_bytecode);
@@ -2559,7 +2559,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ LoadReceiver(r5, r2);
       __ JumpIfSmi(r5, &convert_to_object);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CompareObjectType(r5, r6, r6, FIRST_JS_RECEIVER_TYPE);
       __ bge(&done_convert);
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -3215,7 +3215,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   __ ExtractBitMask(scratch, scratch_high, HeapNumber::kExponentMask);
   // Load scratch with exponent - 1. This is faster than loading
   // with exponent because Bias + 1 = 1024 which is a *S390* immediate value.
-  STATIC_ASSERT(HeapNumber::kExponentBias + 1 == 1024);
+  static_assert(HeapNumber::kExponentBias + 1 == 1024);
   __ SubS64(scratch, Operand(HeapNumber::kExponentBias + 1));
   // If exponent is greater than or equal to 84, the 32 less significant
   // bits are 0s (2^84 = 1, 52 significant bits, 32 uncoded bits),
@@ -3243,7 +3243,7 @@ void Builtins::Generate_DoubleToI(MacroAssembler* masm) {
   __ SubS64(scratch, r0, scratch);
   __ ExtractBitMask(result_reg, scratch_high, HeapNumber::kMantissaMask);
   // Set the implicit 1 before the mantissa part in scratch_high.
-  STATIC_ASSERT(HeapNumber::kMantissaBitsInTopWord >= 16);
+  static_assert(HeapNumber::kMantissaBitsInTopWord >= 16);
   __ mov(r0, Operand(1 << ((HeapNumber::kMantissaBitsInTopWord)-16)));
   __ ShiftLeftU64(r0, r0, Operand(16));
   __ OrP(result_reg, result_reg, r0);
@@ -3433,13 +3433,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -3543,14 +3543,14 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
   int apiStackSpace = 0;
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   Register receiver = ApiGetterDescriptor::ReceiverRegister();
   Register holder = ApiGetterDescriptor::HolderRegister();
@@ -3871,7 +3871,7 @@ void Builtins::Generate_DeoptimizationEntry_Lazy(MacroAssembler* masm) {
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
@@ -3879,7 +3879,7 @@ void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
 #if ENABLE_SPARKPLUG
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
 
   __ LoadU64(kContextRegister,
          MemOperand(fp, BaselineFrameConstants::kContextOffset));
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 66901efe9f1..57d06c0bd82 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -319,7 +319,7 @@ void Builtins::Generate_JSConstructStubGeneric(MacroAssembler* masm) {
 
   // If the type of the result (stored in its map) is less than
   // FIRST_JS_RECEIVER_TYPE, it is not an object in the ECMA sense.
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   __ CmpObjectType(rax, FIRST_JS_RECEIVER_TYPE, rcx);
   __ j(above_equal, &leave_and_return, Label::kNear);
   __ jmp(&use_receiver);
@@ -399,8 +399,8 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
     __ movdqu(Operand(rsp, EntryFrameConstants::kXMMRegisterSize * 7), xmm13);
     __ movdqu(Operand(rsp, EntryFrameConstants::kXMMRegisterSize * 8), xmm14);
     __ movdqu(Operand(rsp, EntryFrameConstants::kXMMRegisterSize * 9), xmm15);
-    STATIC_ASSERT(EntryFrameConstants::kCalleeSaveXMMRegisters == 10);
-    STATIC_ASSERT(EntryFrameConstants::kXMMRegistersBlockSize ==
+    static_assert(EntryFrameConstants::kCalleeSaveXMMRegisters == 10);
+    static_assert(EntryFrameConstants::kXMMRegistersBlockSize ==
                   EntryFrameConstants::kXMMRegisterSize *
                       EntryFrameConstants::kCalleeSaveXMMRegisters);
 #endif
@@ -1038,10 +1038,10 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 
   // Check if the bytecode is a Wide or ExtraWide prefix bytecode.
   Label process_bytecode, extra_wide;
-  STATIC_ASSERT(0 == static_cast<int>(interpreter::Bytecode::kWide));
-  STATIC_ASSERT(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
-  STATIC_ASSERT(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
-  STATIC_ASSERT(3 ==
+  static_assert(0 == static_cast<int>(interpreter::Bytecode::kWide));
+  static_assert(1 == static_cast<int>(interpreter::Bytecode::kExtraWide));
+  static_assert(2 == static_cast<int>(interpreter::Bytecode::kDebugBreakWide));
+  static_assert(3 ==
                 static_cast<int>(interpreter::Bytecode::kDebugBreakExtraWide));
   __ cmpb(bytecode, Immediate(0x3));
   __ j(above, &process_bytecode, Label::kNear);
@@ -1135,7 +1135,7 @@ static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 namespace {
 
 void ResetBytecodeAge(MacroAssembler* masm, Register bytecode_array) {
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+  static_assert(BytecodeArray::kNoAgeBytecodeAge == 0);
   __ movw(FieldOperand(bytecode_array, BytecodeArray::kBytecodeAgeOffset),
           Immediate(0));
 }
@@ -2358,7 +2358,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
       Label convert_to_object, convert_receiver;
       __ movq(rcx, args.GetReceiverOperand());
       __ JumpIfSmi(rcx, &convert_to_object, Label::kNear);
-      STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+      static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
       __ CmpObjectType(rcx, FIRST_JS_RECEIVER_TYPE, rbx);
       __ j(above_equal, &done_convert);
       if (mode != ConvertReceiverMode::kNotNullOrUndefined) {
@@ -2796,14 +2796,14 @@ void OnStackReplacement(MacroAssembler* masm, OsrSourceTier source,
 
 void Builtins::Generate_InterpreterOnStackReplacement(MacroAssembler* masm) {
   using D = InterpreterOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   OnStackReplacement(masm, OsrSourceTier::kInterpreter,
                      D::MaybeTargetCodeRegister());
 }
 
 void Builtins::Generate_BaselineOnStackReplacement(MacroAssembler* masm) {
   using D = BaselineOnStackReplacementDescriptor;
-  STATIC_ASSERT(D::kParameterCount == 1);
+  static_assert(D::kParameterCount == 1);
   __ movq(kContextRegister,
           MemOperand(rbp, BaselineFrameConstants::kContextOffset));
   OnStackReplacement(masm, OsrSourceTier::kBaseline,
@@ -2999,7 +2999,7 @@ void AllocateContinuation(MacroAssembler* masm, Register function_data,
   __ CallRuntime(Runtime::kWasmAllocateContinuation);
   __ Pop(function_data);
   __ Pop(wasm_instance);
-  STATIC_ASSERT(kReturnRegister0 == rax);
+  static_assert(kReturnRegister0 == rax);
   suspender = no_reg;
 }
 
@@ -3386,7 +3386,7 @@ void GenericJSToWasmWrapperHelper(MacroAssembler* masm, bool stack_switch) {
   // We have to check the types of the params. The ValueType array contains
   // first the return then the param types.
   constexpr int kValueTypeSize = sizeof(wasm::ValueType);
-  STATIC_ASSERT(kValueTypeSize == 4);
+  static_assert(kValueTypeSize == 4);
   const int32_t kValueTypeSizeLog2 = log2(kValueTypeSize);
   // Set the ValueType array pointer to point to the first parameter.
   Register returns_size = return_count;
@@ -4659,13 +4659,13 @@ void Builtins::Generate_CallApiCallback(MacroAssembler* masm) {
 
   using FCA = FunctionCallbackArguments;
 
-  STATIC_ASSERT(FCA::kArgsLength == 6);
-  STATIC_ASSERT(FCA::kNewTargetIndex == 5);
-  STATIC_ASSERT(FCA::kDataIndex == 4);
-  STATIC_ASSERT(FCA::kReturnValueOffset == 3);
-  STATIC_ASSERT(FCA::kReturnValueDefaultValueIndex == 2);
-  STATIC_ASSERT(FCA::kIsolateIndex == 1);
-  STATIC_ASSERT(FCA::kHolderIndex == 0);
+  static_assert(FCA::kArgsLength == 6);
+  static_assert(FCA::kNewTargetIndex == 5);
+  static_assert(FCA::kDataIndex == 4);
+  static_assert(FCA::kReturnValueOffset == 3);
+  static_assert(FCA::kReturnValueDefaultValueIndex == 2);
+  static_assert(FCA::kIsolateIndex == 1);
+  static_assert(FCA::kHolderIndex == 0);
 
   // Set up FunctionCallbackInfo's implicit_args on the stack as follows:
   //
@@ -4761,14 +4761,14 @@ void Builtins::Generate_CallApiGetter(MacroAssembler* masm) {
 
   // Build v8::PropertyCallbackInfo::args_ array on the stack and push property
   // name below the exit frame to make GC aware of them.
-  STATIC_ASSERT(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
-  STATIC_ASSERT(PropertyCallbackArguments::kHolderIndex == 1);
-  STATIC_ASSERT(PropertyCallbackArguments::kIsolateIndex == 2);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
-  STATIC_ASSERT(PropertyCallbackArguments::kReturnValueOffset == 4);
-  STATIC_ASSERT(PropertyCallbackArguments::kDataIndex == 5);
-  STATIC_ASSERT(PropertyCallbackArguments::kThisIndex == 6);
-  STATIC_ASSERT(PropertyCallbackArguments::kArgsLength == 7);
+  static_assert(PropertyCallbackArguments::kShouldThrowOnErrorIndex == 0);
+  static_assert(PropertyCallbackArguments::kHolderIndex == 1);
+  static_assert(PropertyCallbackArguments::kIsolateIndex == 2);
+  static_assert(PropertyCallbackArguments::kReturnValueDefaultValueIndex == 3);
+  static_assert(PropertyCallbackArguments::kReturnValueOffset == 4);
+  static_assert(PropertyCallbackArguments::kDataIndex == 5);
+  static_assert(PropertyCallbackArguments::kThisIndex == 6);
+  static_assert(PropertyCallbackArguments::kArgsLength == 7);
 
   // Insert additional parameters into the stack frame above return address.
   __ PopReturnAddressTo(scratch);
diff --git a/src/codegen/arm/assembler-arm.h b/src/codegen/arm/assembler-arm.h
index 9408dd07933..f11de5e1206 100644
--- a/src/codegen/arm/assembler-arm.h
+++ b/src/codegen/arm/assembler-arm.h
@@ -104,11 +104,11 @@ class V8_EXPORT_PRIVATE Operand {
     return Operand(rm, ASR, kSmiTagSize);
   }
   V8_INLINE static Operand PointerOffsetFromSmiKey(Register key) {
-    STATIC_ASSERT(kSmiTag == 0 && kSmiTagSize < kPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiTagSize < kPointerSizeLog2);
     return Operand(key, LSL, kPointerSizeLog2 - kSmiTagSize);
   }
   V8_INLINE static Operand DoubleOffsetFromSmiKey(Register key) {
-    STATIC_ASSERT(kSmiTag == 0 && kSmiTagSize < kDoubleSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiTagSize < kDoubleSizeLog2);
     return Operand(key, LSL, kDoubleSizeLog2 - kSmiTagSize);
   }
 
@@ -209,7 +209,7 @@ class V8_EXPORT_PRIVATE MemOperand {
   V8_INLINE static MemOperand PointerAddressFromSmiKey(Register array,
                                                        Register key,
                                                        AddrMode am = Offset) {
-    STATIC_ASSERT(kSmiTag == 0 && kSmiTagSize < kPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiTagSize < kPointerSizeLog2);
     return MemOperand(array, key, LSL, kPointerSizeLog2 - kSmiTagSize, am);
   }
 
@@ -1259,7 +1259,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   // Relocation info generation
   // Each relocation is encoded as a variable size value
diff --git a/src/codegen/arm/interface-descriptors-arm-inl.h b/src/codegen/arm/interface-descriptors-arm-inl.h
index dcfd9e4833b..23d4b31bc01 100644
--- a/src/codegen/arm/interface-descriptors-arm-inl.h
+++ b/src/codegen/arm/interface-descriptors-arm-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(r0, r1, r2, r3, r4);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index c16045ece2d..98067de6899 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -256,10 +256,10 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSystemPointerSize == 4);
-  STATIC_ASSERT(kSmiShiftSize == 0);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 4);
+  static_assert(kSmiShiftSize == 0);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   // Untagging is folded into the indexing operand below.
@@ -1340,7 +1340,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       add(sp, sp, Operand(count, LSL, kPointerSizeLog2 - kSmiTagSize), LeaveCC);
       break;
     }
@@ -1801,8 +1801,8 @@ void MacroAssembler::InvokeFunction(Register function,
 void MacroAssembler::PushStackHandler() {
   ASM_CODE_COMMENT(this);
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
 
   Push(Smi::zero());  // Padding.
   // Link the current handler as the next handler.
@@ -1818,7 +1818,7 @@ void MacroAssembler::PopStackHandler() {
   ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   pop(r1);
   Move(scratch,
        ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate()));
@@ -2142,7 +2142,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 void MacroAssembler::AssertNotSmi(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmi);
 }
@@ -2150,7 +2150,7 @@ void MacroAssembler::AssertNotSmi(Register object) {
 void MacroAssembler::AssertSmi(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(eq, AbortReason::kOperandIsNotASmi);
 }
@@ -2158,7 +2158,7 @@ void MacroAssembler::AssertSmi(Register object) {
 void MacroAssembler::AssertConstructor(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor);
   push(object);
@@ -2172,7 +2172,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 void MacroAssembler::AssertFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotAFunction);
   push(object);
@@ -2186,7 +2186,7 @@ void MacroAssembler::AssertFunction(Register object) {
 void MacroAssembler::AssertCallableFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotAFunction);
   push(object);
@@ -2200,7 +2200,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 void MacroAssembler::AssertBoundFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction);
   push(object);
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index 4b221276961..0f42f0efa94 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -194,8 +194,8 @@ template <typename T>
 struct ImmediateInitializer {
   static inline RelocInfo::Mode rmode_for(T) { return RelocInfo::NO_INFO; }
   static inline int64_t immediate_for(T t) {
-    STATIC_ASSERT(sizeof(T) <= 8);
-    STATIC_ASSERT(std::is_integral<T>::value || std::is_enum<T>::value);
+    static_assert(sizeof(T) <= 8);
+    static_assert(std::is_integral<T>::value || std::is_enum<T>::value);
     return t;
   }
 };
@@ -232,7 +232,7 @@ Immediate::Immediate(T t)
 template <typename T>
 Immediate::Immediate(T t, RelocInfo::Mode rmode)
     : value_(ImmediateInitializer<T>::immediate_for(t)), rmode_(rmode) {
-  STATIC_ASSERT(std::is_integral<T>::value);
+  static_assert(std::is_integral<T>::value);
 }
 
 template <typename T>
@@ -504,7 +504,7 @@ AssemblerBase::EmbeddedObjectIndex
 Assembler::embedded_object_index_referenced_from(Address pc) {
   Instruction* instr = reinterpret_cast<Instruction*>(pc);
   if (instr->IsLdrLiteralX()) {
-    STATIC_ASSERT(sizeof(EmbeddedObjectIndex) == sizeof(intptr_t));
+    static_assert(sizeof(EmbeddedObjectIndex) == sizeof(intptr_t));
     return Memory<EmbeddedObjectIndex>(target_pointer_address_at(pc));
   } else {
     DCHECK(instr->IsLdrLiteralW());
diff --git a/src/codegen/arm64/assembler-arm64.h b/src/codegen/arm64/assembler-arm64.h
index f9e991a57b3..f12e1ef1304 100644
--- a/src/codegen/arm64/assembler-arm64.h
+++ b/src/codegen/arm64/assembler-arm64.h
@@ -2614,8 +2614,8 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 
   // Emit the instruction at pc_.
   void Emit(Instr instruction) {
-    STATIC_ASSERT(sizeof(*pc_) == 1);
-    STATIC_ASSERT(sizeof(instruction) == kInstrSize);
+    static_assert(sizeof(*pc_) == 1);
+    static_assert(sizeof(instruction) == kInstrSize);
     DCHECK_LE(pc_ + sizeof(instruction), buffer_start_ + buffer_->size());
 
     memcpy(pc_, &instruction, sizeof(instruction));
@@ -2660,7 +2660,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // relocation info entries, and debug strings encoded in the instruction
   // stream.
   static constexpr int kGap = 64;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
  public:
 #ifdef DEBUG
diff --git a/src/codegen/arm64/constants-arm64.h b/src/codegen/arm64/constants-arm64.h
index 52790b9faf4..9b877630e4b 100644
--- a/src/codegen/arm64/constants-arm64.h
+++ b/src/codegen/arm64/constants-arm64.h
@@ -9,15 +9,15 @@
 #include "src/common/globals.h"
 
 // Assert that this is an LP64 system, or LLP64 on Windows.
-STATIC_ASSERT(sizeof(int) == sizeof(int32_t));
+static_assert(sizeof(int) == sizeof(int32_t));
 #if defined(V8_OS_WIN)
-STATIC_ASSERT(sizeof(1L) == sizeof(int32_t));
+static_assert(sizeof(1L) == sizeof(int32_t));
 #else
-STATIC_ASSERT(sizeof(long) == sizeof(int64_t));  // NOLINT(runtime/int)
-STATIC_ASSERT(sizeof(1L) == sizeof(int64_t));
+static_assert(sizeof(long) == sizeof(int64_t));  // NOLINT(runtime/int)
+static_assert(sizeof(1L) == sizeof(int64_t));
 #endif
-STATIC_ASSERT(sizeof(void*) == sizeof(int64_t));
-STATIC_ASSERT(sizeof(1) == sizeof(int32_t));
+static_assert(sizeof(void*) == sizeof(int64_t));
+static_assert(sizeof(1) == sizeof(int32_t));
 
 // Get the standard printf format macros for C99 stdint types.
 #ifndef __STDC_FORMAT_MACROS
diff --git a/src/codegen/arm64/interface-descriptors-arm64-inl.h b/src/codegen/arm64/interface-descriptors-arm64-inl.h
index 9ec6f8a88a9..3b1801e0318 100644
--- a/src/codegen/arm64/interface-descriptors-arm64-inl.h
+++ b/src/codegen/arm64/interface-descriptors-arm64-inl.h
@@ -16,7 +16,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(x0, x1, x2, x3, x4);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/arm64/macro-assembler-arm64-inl.h b/src/codegen/arm64/macro-assembler-arm64-inl.h
index e92a65d3cee..066eccbfacc 100644
--- a/src/codegen/arm64/macro-assembler-arm64-inl.h
+++ b/src/codegen/arm64/macro-assembler-arm64-inl.h
@@ -1106,7 +1106,7 @@ void TurboAssembler::SmiToInt32(Register smi) {
 
 void TurboAssembler::JumpIfSmi(Register value, Label* smi_label,
                                Label* not_smi_label) {
-  STATIC_ASSERT((kSmiTagSize == 1) && (kSmiTag == 0));
+  static_assert((kSmiTagSize == 1) && (kSmiTag == 0));
   // Check if the tag bit is set.
   if (smi_label) {
     Tbz(value, 0, smi_label);
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index 31ebcad387f..3b7d27d7b8a 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -1281,7 +1281,7 @@ void MacroAssembler::PushCalleeSavedRegisters() {
   stp(x21, x22, tos);
   stp(x19, x20, tos);
 
-  STATIC_ASSERT(
+  static_assert(
       EntryFrameConstants::kCalleeSavedRegisterBytesPushedBeforeFpLrPair ==
       18 * kSystemPointerSize);
 
@@ -1293,7 +1293,7 @@ void MacroAssembler::PushCalleeSavedRegisters() {
 
     stp(x29, x30, tos);  // fp, lr
 
-    STATIC_ASSERT(
+    static_assert(
         EntryFrameConstants::kCalleeSavedRegisterBytesPushedAfterFpLrPair == 0);
 }
 
@@ -1448,7 +1448,7 @@ void TurboAssembler::AssertFPCRState(Register fpcr) {
     //   - Assert that flush-to-zero is not set.
     Tbnz(fpcr, FZ_offset, &unexpected_mode);
     //   - Assert that the rounding mode is nearest-with-ties-to-even.
-    STATIC_ASSERT(FPTieEven == 0);
+    static_assert(FPTieEven == 0);
     Tst(fpcr, RMode_mask);
     B(eq, &done);
 
@@ -1538,7 +1538,7 @@ void TurboAssembler::Swap(VRegister lhs, VRegister rhs) {
 void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   Tst(object, kSmiTagMask);
   Check(eq, reason);
 }
@@ -1546,7 +1546,7 @@ void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
 void MacroAssembler::AssertNotSmi(Register object, AbortReason reason) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   Tst(object, kSmiTagMask);
   Check(ne, reason);
 }
@@ -2152,7 +2152,7 @@ void TurboAssembler::LoadCodeDataContainerCodeNonBuiltin(
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // Given the fields layout we can read the Code reference as a full word.
-  STATIC_ASSERT(!V8_EXTERNAL_CODE_SPACE_BOOL ||
+  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
                 (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
                  CodeDataContainer::kCodeOffset + kTaggedSize));
   Ldr(destination, FieldMemOperand(code_data_container_object,
@@ -2643,7 +2643,7 @@ void TurboAssembler::Prologue() {
   ASM_CODE_COMMENT(this);
   Push<TurboAssembler::kSignLR>(lr, fp);
   mov(fp, sp);
-  STATIC_ASSERT(kExtraSlotClaimedByPrologue == 1);
+  static_assert(kExtraSlotClaimedByPrologue == 1);
   Push(cp, kJSFunctionRegister, kJavaScriptCallArgCountRegister, padreg);
 }
 
@@ -2752,13 +2752,13 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
   //    fp -> fp[0]: CallerFP (old fp)
   //          fp[-8]: STUB marker
   //    sp -> fp[-16]: Space reserved for SPOffset.
-  STATIC_ASSERT((2 * kSystemPointerSize) ==
+  static_assert((2 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerSPOffset);
-  STATIC_ASSERT((1 * kSystemPointerSize) ==
+  static_assert((1 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerPCOffset);
-  STATIC_ASSERT((0 * kSystemPointerSize) ==
+  static_assert((0 * kSystemPointerSize) ==
                 ExitFrameConstants::kCallerFPOffset);
-  STATIC_ASSERT((-2 * kSystemPointerSize) == ExitFrameConstants::kSPOffset);
+  static_assert((-2 * kSystemPointerSize) == ExitFrameConstants::kSPOffset);
 
   // Save the frame pointer and context pointer in the top frame.
   Mov(scratch,
@@ -2768,7 +2768,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
       ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   Str(cp, MemOperand(scratch));
 
-  STATIC_ASSERT((-2 * kSystemPointerSize) ==
+  static_assert((-2 * kSystemPointerSize) ==
                 ExitFrameConstants::kLastExitFrameField);
   if (save_doubles) {
     ExitFramePreserveFPRegs();
@@ -3179,7 +3179,7 @@ void TurboAssembler::LoadExternalPointerField(Register destination,
   Ldr(destination.W(), field_operand);
   // MemOperand doesn't support LSR currently (only LSL), so here we do the
   // offset computation separately first.
-  STATIC_ASSERT(kExternalPointerIndexShift > kSystemPointerSizeLog2);
+  static_assert(kExternalPointerIndexShift > kSystemPointerSizeLog2);
   int shift_amount = kExternalPointerIndexShift - kSystemPointerSizeLog2;
   Mov(destination, Operand(destination, LSR, shift_amount));
   Ldr(destination, MemOperand(external_table, destination));
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 4870c1e99ca..4d9cc1e259d 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -1552,7 +1552,7 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 #undef DECLARE_FUNCTION
 
   // Branch type inversion relies on these relations.
-  STATIC_ASSERT((reg_zero == (reg_not_zero ^ 1)) &&
+  static_assert((reg_zero == (reg_not_zero ^ 1)) &&
                 (reg_bit_clear == (reg_bit_set ^ 1)) &&
                 (always == (never ^ 1)));
 
diff --git a/src/codegen/arm64/register-arm64.h b/src/codegen/arm64/register-arm64.h
index 83fb23098a7..728ac559d50 100644
--- a/src/codegen/arm64/register-arm64.h
+++ b/src/codegen/arm64/register-arm64.h
@@ -412,7 +412,7 @@ class VRegister : public CPURegister {
   unsigned LaneSizeInBits() const { return LaneSizeInBytes() * 8; }
 
   static constexpr int kMaxNumRegisters = kNumberOfVRegisters;
-  STATIC_ASSERT(kMaxNumRegisters == kDoubleAfterLast);
+  static_assert(kMaxNumRegisters == kDoubleAfterLast);
 
   static constexpr VRegister from_code(int code) {
     // Always return a D register.
diff --git a/src/codegen/arm64/utils-arm64.h b/src/codegen/arm64/utils-arm64.h
index 922298a7f66..266cd525cca 100644
--- a/src/codegen/arm64/utils-arm64.h
+++ b/src/codegen/arm64/utils-arm64.h
@@ -14,8 +14,8 @@ namespace v8 {
 namespace internal {
 
 // These are global assumptions in v8.
-STATIC_ASSERT((static_cast<int32_t>(-1) >> 1) == -1);
-STATIC_ASSERT((static_cast<uint32_t>(-1) >> 1) == 0x7FFFFFFF);
+static_assert((static_cast<int32_t>(-1) >> 1) == -1);
+static_assert((static_cast<uint32_t>(-1) >> 1) == 0x7FFFFFFF);
 
 uint32_t float_sign(float val);
 uint32_t float_exp(float val);
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 5df58426926..400d8cb85ea 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -1142,7 +1142,7 @@ TNode<Int32T> CodeStubAssembler::TruncateIntPtrToInt32(TNode<IntPtrT> value) {
 }
 
 TNode<BoolT> CodeStubAssembler::TaggedIsSmi(TNode<MaybeObject> a) {
-  STATIC_ASSERT(kSmiTagMask < kMaxUInt32);
+  static_assert(kSmiTagMask < kMaxUInt32);
   return Word32Equal(
       Word32And(TruncateIntPtrToInt32(BitcastTaggedToWordForTagAndSmiBits(a)),
                 Int32Constant(kSmiTagMask)),
@@ -1193,7 +1193,7 @@ TNode<Float64T> CodeStubAssembler::LoadDoubleWithHoleCheck(
 void CodeStubAssembler::BranchIfJSReceiver(TNode<Object> object, Label* if_true,
                                            Label* if_false) {
   GotoIf(TaggedIsSmi(object), if_false);
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   Branch(IsJSReceiver(CAST(object)), if_true, if_false);
 }
 
@@ -1717,7 +1717,7 @@ TNode<Int32T> CodeStubAssembler::LoadAndUntagToWord32ObjectField(
 TNode<Float64T> CodeStubAssembler::LoadHeapNumberValue(
     TNode<HeapObject> object) {
   CSA_DCHECK(this, Word32Or(IsHeapNumber(object), IsOddball(object)));
-  STATIC_ASSERT(HeapNumber::kValueOffset == Oddball::kToNumberRawOffset);
+  static_assert(HeapNumber::kValueOffset == Oddball::kToNumberRawOffset);
   return LoadObjectField<Float64T>(object, HeapNumber::kValueOffset);
 }
 
@@ -1829,7 +1829,7 @@ TNode<Object> CodeStubAssembler::LoadJSArgumentsObjectLength(
     TNode<Context> context, TNode<JSArgumentsObject> array) {
   CSA_DCHECK(this, IsJSArgumentsObjectWithLength(context, array));
   constexpr int offset = JSStrictArgumentsObject::kLengthOffset;
-  STATIC_ASSERT(offset == JSSloppyArgumentsObject::kLengthOffset);
+  static_assert(offset == JSSloppyArgumentsObject::kLengthOffset);
   return LoadObjectField(array, offset);
 }
 
@@ -2604,7 +2604,7 @@ TNode<Numeric> CodeStubAssembler::LoadFixedTypedArrayElementAsTagged(
       TYPED_ARRAYS(TYPED_ARRAY_CASE)
 #undef TYPED_ARRAY_CASE
   };
-  STATIC_ASSERT(arraysize(elements_kinds) == arraysize(elements_kind_labels));
+  static_assert(arraysize(elements_kinds) == arraysize(elements_kind_labels));
 
   Switch(elements_kind, &if_unknown_type, elements_kinds, elements_kind_labels,
          arraysize(elements_kinds));
@@ -3108,15 +3108,15 @@ void CodeStubAssembler::StoreFixedArrayOrPropertyArrayElement(
          barrier_mode == UPDATE_WRITE_BARRIER ||
          barrier_mode == UPDATE_EPHEMERON_KEY_WRITE_BARRIER);
   DCHECK(IsAligned(additional_offset, kTaggedSize));
-  STATIC_ASSERT(static_cast<int>(FixedArray::kHeaderSize) ==
+  static_assert(static_cast<int>(FixedArray::kHeaderSize) ==
                 static_cast<int>(PropertyArray::kHeaderSize));
   int header_size =
       FixedArray::kHeaderSize + additional_offset - kHeapObjectTag;
   TNode<IntPtrT> offset =
       ElementOffsetFromIndex(index_node, HOLEY_ELEMENTS, header_size);
-  STATIC_ASSERT(static_cast<int>(FixedArrayBase::kLengthOffset) ==
+  static_assert(static_cast<int>(FixedArrayBase::kLengthOffset) ==
                 static_cast<int>(WeakFixedArray::kLengthOffset));
-  STATIC_ASSERT(static_cast<int>(FixedArrayBase::kLengthOffset) ==
+  static_assert(static_cast<int>(FixedArrayBase::kLengthOffset) ==
                 static_cast<int>(PropertyArray::kLengthAndHashOffset));
   // Check that index_node + additional_offset <= object.length.
   // TODO(cbruni): Use proper LoadXXLength helpers
@@ -3719,7 +3719,7 @@ TNode<CollectionType> CodeStubAssembler::AllocateOrderedHashTableWithCapacity(
              IntPtrLessThanOrEqual(
                  capacity, IntPtrConstant(CollectionType::MaxCapacity())));
 
-  STATIC_ASSERT(CollectionType::kLoadFactor == 2);
+  static_assert(CollectionType::kLoadFactor == 2);
   TNode<IntPtrT> bucket_count = Signed(WordShr(capacity, IntPtrConstant(1)));
   TNode<IntPtrT> data_table_length =
       IntPtrMul(capacity, IntPtrConstant(CollectionType::kEntrySize));
@@ -3749,7 +3749,7 @@ TNode<CollectionType> CodeStubAssembler::AllocateOrderedHashTableWithCapacity(
 
   TNode<IntPtrT> object_address = BitcastTaggedToWord(table);
 
-  STATIC_ASSERT(CollectionType::HashTableStartIndex() ==
+  static_assert(CollectionType::HashTableStartIndex() ==
                 CollectionType::NumberOfBucketsIndex() + 1);
 
   TNode<Smi> not_found_sentinel = SmiConstant(CollectionType::kNotFound);
@@ -3896,7 +3896,7 @@ void CodeStubAssembler::InitializeJSObjectFromMap(
 void CodeStubAssembler::InitializeJSObjectBodyNoSlackTracking(
     TNode<HeapObject> object, TNode<Map> map, TNode<IntPtrT> instance_size,
     int start_offset) {
-  STATIC_ASSERT(Map::kNoSlackTracking == 0);
+  static_assert(Map::kNoSlackTracking == 0);
   CSA_DCHECK(this, IsClearWord32<Map::Bits3::ConstructionCounterBits>(
                        LoadMapBitField3(map)));
   InitializeFieldsWithRoot(object, IntPtrConstant(start_offset), instance_size,
@@ -3911,7 +3911,7 @@ void CodeStubAssembler::InitializeJSObjectBodyWithSlackTracking(
   int start_offset = JSObject::kHeaderSize;
   TNode<Uint32T> bit_field3 = LoadMapBitField3(map);
   Label end(this), slack_tracking(this), complete(this, Label::kDeferred);
-  STATIC_ASSERT(Map::kNoSlackTracking == 0);
+  static_assert(Map::kNoSlackTracking == 0);
   GotoIf(IsSetWord32<Map::Bits3::ConstructionCounterBits>(bit_field3),
          &slack_tracking);
   Comment("No slack tracking");
@@ -3923,12 +3923,12 @@ void CodeStubAssembler::InitializeJSObjectBodyWithSlackTracking(
     Comment("Decrease construction counter");
     // Slack tracking is only done on initial maps.
     CSA_DCHECK(this, IsUndefined(LoadMapBackPointer(map)));
-    STATIC_ASSERT(Map::Bits3::ConstructionCounterBits::kLastUsedBit == 31);
+    static_assert(Map::Bits3::ConstructionCounterBits::kLastUsedBit == 31);
     TNode<Word32T> new_bit_field3 = Int32Sub(
         bit_field3,
         Int32Constant(1 << Map::Bits3::ConstructionCounterBits::kShift));
     StoreObjectFieldNoWriteBarrier(map, Map::kBitField3Offset, new_bit_field3);
-    STATIC_ASSERT(Map::kSlackTrackingCounterEnd == 1);
+    static_assert(Map::kSlackTrackingCounterEnd == 1);
 
     // The object still has in-object slack therefore the |unsed_or_unused|
     // field contain the "used" value.
@@ -3944,7 +3944,7 @@ void CodeStubAssembler::InitializeJSObjectBodyWithSlackTracking(
     InitializeFieldsWithRoot(object, IntPtrConstant(start_offset), used_size,
                              RootIndex::kUndefinedValue);
 
-    STATIC_ASSERT(Map::kNoSlackTracking == 0);
+    static_assert(Map::kNoSlackTracking == 0);
     GotoIf(IsClearWord32<Map::Bits3::ConstructionCounterBits>(new_bit_field3),
            &complete);
     Goto(&end);
@@ -4121,7 +4121,7 @@ CodeStubAssembler::AllocateUninitializedJSArrayWithElements(
                                    elements.value());
 
     // Setup elements object.
-    STATIC_ASSERT(FixedArrayBase::kHeaderSize == 2 * kTaggedSize);
+    static_assert(FixedArrayBase::kHeaderSize == 2 * kTaggedSize);
     RootIndex elements_map_index = IsDoubleElementsKind(kind)
                                        ? RootIndex::kFixedDoubleArrayMap
                                        : RootIndex::kFixedArrayMap;
@@ -4485,7 +4485,7 @@ TNode<FixedArrayBase> CodeStubAssembler::ExtractFixedDoubleArrayFillingHoles(
   // The construction of the loop and the offsets for double elements is
   // extracted from CopyFixedArrayElements.
   CSA_SLOW_DCHECK(this, IsFixedArrayWithKindOrEmpty(from_array, kind));
-  STATIC_ASSERT(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
+  static_assert(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
 
   Comment("[ ExtractFixedDoubleArrayFillingHoles");
 
@@ -4782,7 +4782,7 @@ void CodeStubAssembler::FillFixedArrayWithSmiZero(TNode<FixedArray> array,
   // Call out to memset to perform initialization.
   TNode<ExternalReference> memset =
       ExternalConstant(ExternalReference::libc_memset_function());
-  STATIC_ASSERT(kSizetSize == kIntptrSize);
+  static_assert(kSizetSize == kIntptrSize);
   CallCFunction(memset, MachineType::Pointer(),
                 std::make_pair(MachineType::Pointer(), backing_store),
                 std::make_pair(MachineType::IntPtr(), IntPtrConstant(0)),
@@ -4804,7 +4804,7 @@ void CodeStubAssembler::FillFixedDoubleArrayWithZero(
   // Call out to memset to perform initialization.
   TNode<ExternalReference> memset =
       ExternalConstant(ExternalReference::libc_memset_function());
-  STATIC_ASSERT(kSizetSize == kIntptrSize);
+  static_assert(kSizetSize == kIntptrSize);
   CallCFunction(memset, MachineType::Pointer(),
                 std::make_pair(MachineType::Pointer(), backing_store),
                 std::make_pair(MachineType::IntPtr(), IntPtrConstant(0)),
@@ -5008,7 +5008,7 @@ void CodeStubAssembler::CopyFixedArrayElements(
                  convert_holes == HoleConversionMode::kConvertToUndefined);
   CSA_SLOW_DCHECK(this, IsFixedArrayWithKindOrEmpty(from_array, from_kind));
   CSA_SLOW_DCHECK(this, IsFixedArrayWithKindOrEmpty(to_array, to_kind));
-  STATIC_ASSERT(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
+  static_assert(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
   static_assert(
       std::is_same<TIndex, Smi>::value || std::is_same<TIndex, IntPtrT>::value,
       "Only Smi or IntPtrT indices are allowed");
@@ -6358,7 +6358,7 @@ TNode<BoolT> CodeStubAssembler::IsFunctionWithPrototypeSlotMap(TNode<Map> map) {
 
 TNode<BoolT> CodeStubAssembler::IsSpecialReceiverInstanceType(
     TNode<Int32T> instance_type) {
-  STATIC_ASSERT(JS_GLOBAL_OBJECT_TYPE <= LAST_SPECIAL_RECEIVER_TYPE);
+  static_assert(JS_GLOBAL_OBJECT_TYPE <= LAST_SPECIAL_RECEIVER_TYPE);
   return Int32LessThanOrEqual(instance_type,
                               Int32Constant(LAST_SPECIAL_RECEIVER_TYPE));
 }
@@ -6371,7 +6371,7 @@ TNode<BoolT> CodeStubAssembler::IsCustomElementsReceiverInstanceType(
 
 TNode<BoolT> CodeStubAssembler::IsStringInstanceType(
     TNode<Int32T> instance_type) {
-  STATIC_ASSERT(INTERNALIZED_STRING_TYPE == FIRST_TYPE);
+  static_assert(INTERNALIZED_STRING_TYPE == FIRST_TYPE);
   return Int32LessThan(instance_type, Int32Constant(FIRST_NONSTRING_TYPE));
 }
 
@@ -6416,8 +6416,8 @@ TNode<BoolT> CodeStubAssembler::IsConsStringInstanceType(
 TNode<BoolT> CodeStubAssembler::IsIndirectStringInstanceType(
     TNode<Int32T> instance_type) {
   CSA_DCHECK(this, IsStringInstanceType(instance_type));
-  STATIC_ASSERT(kIsIndirectStringMask == 0x1);
-  STATIC_ASSERT(kIsIndirectStringTag == 0x1);
+  static_assert(kIsIndirectStringMask == 0x1);
+  static_assert(kIsIndirectStringTag == 0x1);
   return UncheckedCast<BoolT>(
       Word32And(instance_type, Int32Constant(kIsIndirectStringMask)));
 }
@@ -6433,13 +6433,13 @@ TNode<BoolT> CodeStubAssembler::IsExternalStringInstanceType(
 TNode<BoolT> CodeStubAssembler::IsUncachedExternalStringInstanceType(
     TNode<Int32T> instance_type) {
   CSA_DCHECK(this, IsStringInstanceType(instance_type));
-  STATIC_ASSERT(kUncachedExternalStringTag != 0);
+  static_assert(kUncachedExternalStringTag != 0);
   return IsSetWord32(instance_type, kUncachedExternalStringMask);
 }
 
 TNode<BoolT> CodeStubAssembler::IsJSReceiverInstanceType(
     TNode<Int32T> instance_type) {
-  STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
   return Int32GreaterThanOrEqual(instance_type,
                                  Int32Constant(FIRST_JS_RECEIVER_TYPE));
 }
@@ -6479,7 +6479,7 @@ TNode<BoolT> CodeStubAssembler::IsJSGeneratorMap(TNode<Map> map) {
 
 TNode<BoolT> CodeStubAssembler::IsJSObjectInstanceType(
     TNode<Int32T> instance_type) {
-  STATIC_ASSERT(LAST_JS_OBJECT_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_OBJECT_TYPE == LAST_TYPE);
   return Int32GreaterThanOrEqual(instance_type,
                                  Int32Constant(FIRST_JS_OBJECT_TYPE));
 }
@@ -6721,7 +6721,7 @@ TNode<BoolT> CodeStubAssembler::IsSymbolInstanceType(
 
 TNode<BoolT> CodeStubAssembler::IsInternalizedStringInstanceType(
     TNode<Int32T> instance_type) {
-  STATIC_ASSERT(kNotInternalizedTag != 0);
+  static_assert(kNotInternalizedTag != 0);
   return Word32Equal(
       Word32And(instance_type,
                 Int32Constant(kIsNotStringMask | kIsNotInternalizedMask)),
@@ -7167,7 +7167,7 @@ TNode<String> ToDirectStringAssembler::TryToDirect(Label* if_bailout) {
     Label* labels[] = {
         &out, &if_iscons, &if_isexternal, &if_issliced, &if_isthin,
     };
-    STATIC_ASSERT(arraysize(values) == arraysize(labels));
+    static_assert(arraysize(values) == arraysize(labels));
 
     const TNode<Int32T> representation = Word32And(
         var_instance_type_.value(), Int32Constant(kStringRepresentationMask));
@@ -7244,7 +7244,7 @@ TNode<RawPtrT> ToDirectStringAssembler::TryToSequential(
 
   BIND(&if_issequential);
   {
-    STATIC_ASSERT(SeqOneByteString::kHeaderSize ==
+    static_assert(SeqOneByteString::kHeaderSize ==
                   SeqTwoByteString::kHeaderSize);
     TNode<RawPtrT> result =
         ReinterpretCast<RawPtrT>(BitcastTaggedToWord(var_string_.value()));
@@ -8073,7 +8073,7 @@ void CodeStubAssembler::TryToName(TNode<Object> key, Label* if_keyisindex,
     GotoIf(IsSymbolInstanceType(var_instance_type.value()), &if_symbol);
 
     // Miss if |key| is not a String.
-    STATIC_ASSERT(FIRST_NAME_TYPE == FIRST_TYPE);
+    static_assert(FIRST_NAME_TYPE == FIRST_TYPE);
     Branch(IsStringInstanceType(var_instance_type.value()), &if_string,
            &if_keyisother);
 
@@ -8112,7 +8112,7 @@ void CodeStubAssembler::TryToName(TNode<Object> key, Label* if_keyisindex,
              &if_forwarding_index);
 
       // Finally, check if |key| is internalized.
-      STATIC_ASSERT(kNotInternalizedTag != 0);
+      static_assert(kNotInternalizedTag != 0);
       GotoIf(IsSetWord32(var_instance_type.value(), kIsNotInternalizedMask),
              if_notinternalized != nullptr ? if_notinternalized : if_bailout);
 
@@ -10044,7 +10044,7 @@ void CodeStubAssembler::TryLookupElement(
       &if_rab_gsab_typedarray,
   };
   // clang-format on
-  STATIC_ASSERT(arraysize(values) == arraysize(labels));
+  static_assert(arraysize(values) == arraysize(labels));
   Switch(elements_kind, if_bailout, values, labels, arraysize(values));
 
   BIND(&if_isobjectorsmi);
@@ -11778,7 +11778,7 @@ void CodeStubAssembler::BuildFastArrayForEach(
     ElementsKind kind, TNode<TIndex> first_element_inclusive,
     TNode<TIndex> last_element_exclusive, const FastArrayForEachBody& body,
     ForEachDirection direction) {
-  STATIC_ASSERT(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
+  static_assert(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
   CSA_SLOW_DCHECK(this, Word32Or(IsFixedArrayWithKind(array, kind),
                                  IsPropertyArray(array)));
 
@@ -12308,7 +12308,7 @@ TNode<Oddball> CodeStubAssembler::RelationalComparison(
             // a BigInt, otherwise call ToPrimitive(right, hint Number) if
             // {right} is a receiver, or ToNumeric(left) and then
             // ToNumeric(right) in the other cases.
-            STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+            static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
             Label if_right_bigint(this),
                 if_right_receiver(this, Label::kDeferred);
             GotoIf(IsBigIntInstanceType(right_instance_type), &if_right_bigint);
@@ -12375,7 +12375,7 @@ TNode<Oddball> CodeStubAssembler::RelationalComparison(
           // If {left} is a receiver, call ToPrimitive(left, hint Number).
           // Otherwise call ToNumeric(right) and then ToNumeric(left), the
           // order here is important as it's observable by user code.
-          STATIC_ASSERT(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
+          static_assert(LAST_JS_RECEIVER_TYPE == LAST_TYPE);
           Label if_left_receiver(this, Label::kDeferred);
           GotoIf(IsJSReceiverInstanceType(left_instance_type),
                  &if_left_receiver);
@@ -13243,7 +13243,7 @@ TNode<Oddball> CodeStubAssembler::StrictEqual(
                 {
                   Label if_rhsisheapnumber(this), if_rhsisnotheapnumber(this);
 
-                  STATIC_ASSERT(LAST_PRIMITIVE_HEAP_OBJECT_TYPE ==
+                  static_assert(LAST_PRIMITIVE_HEAP_OBJECT_TYPE ==
                                 ODDBALL_TYPE);
                   GotoIf(Int32LessThan(rhs_instance_type,
                                        Int32Constant(ODDBALL_TYPE)),
@@ -14515,37 +14515,37 @@ void CodeStubArguments::PopAndReturn(TNode<Object> value) {
 
 TNode<BoolT> CodeStubAssembler::IsFastElementsKind(
     TNode<Int32T> elements_kind) {
-  STATIC_ASSERT(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
+  static_assert(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
   return Uint32LessThanOrEqual(elements_kind,
                                Int32Constant(LAST_FAST_ELEMENTS_KIND));
 }
 
 TNode<BoolT> CodeStubAssembler::IsFastOrNonExtensibleOrSealedElementsKind(
     TNode<Int32T> elements_kind) {
-  STATIC_ASSERT(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
-  STATIC_ASSERT(LAST_FAST_ELEMENTS_KIND + 1 == PACKED_NONEXTENSIBLE_ELEMENTS);
-  STATIC_ASSERT(PACKED_NONEXTENSIBLE_ELEMENTS + 1 ==
+  static_assert(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
+  static_assert(LAST_FAST_ELEMENTS_KIND + 1 == PACKED_NONEXTENSIBLE_ELEMENTS);
+  static_assert(PACKED_NONEXTENSIBLE_ELEMENTS + 1 ==
                 HOLEY_NONEXTENSIBLE_ELEMENTS);
-  STATIC_ASSERT(HOLEY_NONEXTENSIBLE_ELEMENTS + 1 == PACKED_SEALED_ELEMENTS);
-  STATIC_ASSERT(PACKED_SEALED_ELEMENTS + 1 == HOLEY_SEALED_ELEMENTS);
+  static_assert(HOLEY_NONEXTENSIBLE_ELEMENTS + 1 == PACKED_SEALED_ELEMENTS);
+  static_assert(PACKED_SEALED_ELEMENTS + 1 == HOLEY_SEALED_ELEMENTS);
   return Uint32LessThanOrEqual(elements_kind,
                                Int32Constant(HOLEY_SEALED_ELEMENTS));
 }
 
 TNode<BoolT> CodeStubAssembler::IsDoubleElementsKind(
     TNode<Int32T> elements_kind) {
-  STATIC_ASSERT(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
-  STATIC_ASSERT((PACKED_DOUBLE_ELEMENTS & 1) == 0);
-  STATIC_ASSERT(PACKED_DOUBLE_ELEMENTS + 1 == HOLEY_DOUBLE_ELEMENTS);
+  static_assert(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
+  static_assert((PACKED_DOUBLE_ELEMENTS & 1) == 0);
+  static_assert(PACKED_DOUBLE_ELEMENTS + 1 == HOLEY_DOUBLE_ELEMENTS);
   return Word32Equal(Word32Shr(elements_kind, Int32Constant(1)),
                      Int32Constant(PACKED_DOUBLE_ELEMENTS / 2));
 }
 
 TNode<BoolT> CodeStubAssembler::IsFastSmiOrTaggedElementsKind(
     TNode<Int32T> elements_kind) {
-  STATIC_ASSERT(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
-  STATIC_ASSERT(PACKED_DOUBLE_ELEMENTS > TERMINAL_FAST_ELEMENTS_KIND);
-  STATIC_ASSERT(HOLEY_DOUBLE_ELEMENTS > TERMINAL_FAST_ELEMENTS_KIND);
+  static_assert(FIRST_ELEMENTS_KIND == FIRST_FAST_ELEMENTS_KIND);
+  static_assert(PACKED_DOUBLE_ELEMENTS > TERMINAL_FAST_ELEMENTS_KIND);
+  static_assert(HOLEY_DOUBLE_ELEMENTS > TERMINAL_FAST_ELEMENTS_KIND);
   return Uint32LessThanOrEqual(elements_kind,
                                Int32Constant(TERMINAL_FAST_ELEMENTS_KIND));
 }
@@ -14560,9 +14560,9 @@ TNode<BoolT> CodeStubAssembler::IsHoleyFastElementsKind(
     TNode<Int32T> elements_kind) {
   CSA_DCHECK(this, IsFastElementsKind(elements_kind));
 
-  STATIC_ASSERT(HOLEY_SMI_ELEMENTS == (PACKED_SMI_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_ELEMENTS == (PACKED_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_DOUBLE_ELEMENTS == (PACKED_DOUBLE_ELEMENTS | 1));
+  static_assert(HOLEY_SMI_ELEMENTS == (PACKED_SMI_ELEMENTS | 1));
+  static_assert(HOLEY_ELEMENTS == (PACKED_ELEMENTS | 1));
+  static_assert(HOLEY_DOUBLE_ELEMENTS == (PACKED_DOUBLE_ELEMENTS | 1));
   return IsSetWord32(elements_kind, 1);
 }
 
@@ -14572,13 +14572,13 @@ TNode<BoolT> CodeStubAssembler::IsHoleyFastElementsKindForRead(
                        elements_kind,
                        Int32Constant(LAST_ANY_NONEXTENSIBLE_ELEMENTS_KIND)));
 
-  STATIC_ASSERT(HOLEY_SMI_ELEMENTS == (PACKED_SMI_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_ELEMENTS == (PACKED_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_DOUBLE_ELEMENTS == (PACKED_DOUBLE_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_NONEXTENSIBLE_ELEMENTS ==
+  static_assert(HOLEY_SMI_ELEMENTS == (PACKED_SMI_ELEMENTS | 1));
+  static_assert(HOLEY_ELEMENTS == (PACKED_ELEMENTS | 1));
+  static_assert(HOLEY_DOUBLE_ELEMENTS == (PACKED_DOUBLE_ELEMENTS | 1));
+  static_assert(HOLEY_NONEXTENSIBLE_ELEMENTS ==
                 (PACKED_NONEXTENSIBLE_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_SEALED_ELEMENTS == (PACKED_SEALED_ELEMENTS | 1));
-  STATIC_ASSERT(HOLEY_FROZEN_ELEMENTS == (PACKED_FROZEN_ELEMENTS | 1));
+  static_assert(HOLEY_SEALED_ELEMENTS == (PACKED_SEALED_ELEMENTS | 1));
+  static_assert(HOLEY_FROZEN_ELEMENTS == (PACKED_FROZEN_ELEMENTS | 1));
   return IsSetWord32(elements_kind, 1);
 }
 
@@ -14761,7 +14761,7 @@ TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
     &check_is_wasm_on_fulfilled,
 #endif  // V8_ENABLE_WEBASSEMBLY
   };
-  STATIC_ASSERT(arraysize(case_values) == arraysize(case_labels));
+  static_assert(arraysize(case_values) == arraysize(case_labels));
   Switch(data_type, &check_is_interpreter_data, case_values, case_labels,
          arraysize(case_labels));
 
@@ -14853,7 +14853,7 @@ TNode<JSFunction> CodeStubAssembler::AllocateFunctionWithMapAndContext(
   CSA_DCHECK(this, Word32BinaryNot(IsConstructorMap(map)));
   CSA_DCHECK(this, Word32BinaryNot(IsFunctionWithPrototypeSlotMap(map)));
   const TNode<HeapObject> fun = Allocate(JSFunction::kSizeWithoutPrototype);
-  STATIC_ASSERT(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
+  static_assert(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
   StoreMapNoWriteBarrier(fun, map);
   StoreObjectFieldRoot(fun, JSObject::kPropertiesOrHashOffset,
                        RootIndex::kEmptyFixedArray);
@@ -14885,7 +14885,7 @@ void CodeStubAssembler::CheckPrototypeEnumCache(TNode<JSReceiver> receiver,
     // The following relies on the elements only aliasing with JSProxy::target,
     // which is a JavaScript value and hence cannot be confused with an elements
     // backing store.
-    STATIC_ASSERT(static_cast<int>(JSObject::kElementsOffset) ==
+    static_assert(static_cast<int>(JSObject::kElementsOffset) ==
                   static_cast<int>(JSProxy::kTargetOffset));
     TNode<Object> object_elements =
         LoadObjectField(var_object.value(), JSObject::kElementsOffset);
@@ -14949,7 +14949,7 @@ TNode<Map> CodeStubAssembler::CheckEnumCache(TNode<JSReceiver> receiver,
     } else {
       CSA_DCHECK(this, Word32Or(IsNameDictionary(properties),
                                 IsGlobalDictionary(properties)));
-      STATIC_ASSERT(static_cast<int>(NameDictionary::kNumberOfElementsIndex) ==
+      static_assert(static_cast<int>(NameDictionary::kNumberOfElementsIndex) ==
                     static_cast<int>(GlobalDictionary::kNumberOfElementsIndex));
       length = GetNumberOfElements(UncheckedCast<HashTableBase>(properties));
     }
@@ -15616,8 +15616,8 @@ CodeStubAssembler::AllocateSwissNameDictionaryWithCapacity(
       IntPtrAdd(ctrl_table_start_ptr, ctrl_table_size_bytes);
 
   // |ctrl_table_size_bytes| (= capacity + kGroupWidth) is divisble by four:
-  STATIC_ASSERT(SwissNameDictionary::kGroupWidth % 4 == 0);
-  STATIC_ASSERT(SwissNameDictionary::kInitialCapacity % 4 == 0);
+  static_assert(SwissNameDictionary::kGroupWidth % 4 == 0);
+  static_assert(SwissNameDictionary::kInitialCapacity % 4 == 0);
 
   // TODO(v8:11330) For all capacities except 4, we know that
   // |ctrl_table_size_bytes| is divisible by 8. Consider initializing the ctrl
@@ -15896,8 +15896,8 @@ void CodeStubAssembler::StoreSwissNameDictionaryPropertyDetails(
 void CodeStubAssembler::StoreSwissNameDictionaryKeyAndValue(
     TNode<SwissNameDictionary> dict, TNode<IntPtrT> entry, TNode<Object> key,
     TNode<Object> value) {
-  STATIC_ASSERT(SwissNameDictionary::kDataTableKeyEntryIndex == 0);
-  STATIC_ASSERT(SwissNameDictionary::kDataTableValueEntryIndex == 1);
+  static_assert(SwissNameDictionary::kDataTableKeyEntryIndex == 0);
+  static_assert(SwissNameDictionary::kDataTableValueEntryIndex == 1);
 
   // TODO(v8:11330) Consider using StoreObjectField here.
   TNode<IntPtrT> key_offset_minus_tag =
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index 30bbd9c732a..d49403e9326 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -834,7 +834,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 #error "This code requires updating for big-endian architectures"
 #endif
     // Given the fields layout we can read the Code reference as a full word.
-    STATIC_ASSERT(CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
+    static_assert(CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
                   CodeDataContainer::kCodeOffset + kTaggedSize);
     TNode<Object> o = BitcastWordToTagged(Load<RawPtrT>(
         code, IntPtrConstant(CodeDataContainer::kCodeOffset - kHeapObjectTag)));
@@ -1004,7 +1004,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   template <typename U>
   TNode<BoolT> IsInRange(TNode<Word32T> value, U lower_limit, U higher_limit) {
     DCHECK_LE(lower_limit, higher_limit);
-    STATIC_ASSERT(sizeof(U) <= kInt32Size);
+    static_assert(sizeof(U) <= kInt32Size);
     return Uint32LessThanOrEqual(Int32Sub(value, Int32Constant(lower_limit)),
                                  Int32Constant(higher_limit - lower_limit));
   }
@@ -3094,7 +3094,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   void SetNumberOfElements(TNode<Dictionary> dictionary,
                            TNode<Smi> num_elements_smi) {
     // Not supposed to be used for SwissNameDictionary.
-    STATIC_ASSERT(!(std::is_same<Dictionary, SwissNameDictionary>::value));
+    static_assert(!(std::is_same<Dictionary, SwissNameDictionary>::value));
 
     StoreFixedArrayElement(dictionary, Dictionary::kNumberOfElementsIndex,
                            num_elements_smi, SKIP_WRITE_BARRIER);
@@ -3103,7 +3103,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   template <class Dictionary>
   TNode<Smi> GetNumberOfDeletedElements(TNode<Dictionary> dictionary) {
     // Not supposed to be used for SwissNameDictionary.
-    STATIC_ASSERT(!(std::is_same<Dictionary, SwissNameDictionary>::value));
+    static_assert(!(std::is_same<Dictionary, SwissNameDictionary>::value));
 
     return CAST(LoadFixedArrayElement(
         dictionary, Dictionary::kNumberOfDeletedElementsIndex));
@@ -3113,7 +3113,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   void SetNumberOfDeletedElements(TNode<Dictionary> dictionary,
                                   TNode<Smi> num_deleted_smi) {
     // Not supposed to be used for SwissNameDictionary.
-    STATIC_ASSERT(!(std::is_same<Dictionary, SwissNameDictionary>::value));
+    static_assert(!(std::is_same<Dictionary, SwissNameDictionary>::value));
 
     StoreFixedArrayElement(dictionary,
                            Dictionary::kNumberOfDeletedElementsIndex,
@@ -3123,7 +3123,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   template <class Dictionary>
   TNode<Smi> GetCapacity(TNode<Dictionary> dictionary) {
     // Not supposed to be used for SwissNameDictionary.
-    STATIC_ASSERT(!(std::is_same<Dictionary, SwissNameDictionary>::value));
+    static_assert(!(std::is_same<Dictionary, SwissNameDictionary>::value));
 
     return CAST(
         UnsafeLoadFixedArrayElement(dictionary, Dictionary::kCapacityIndex));
@@ -3834,7 +3834,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   template <class... TArgs>
   TNode<HeapObject> MakeTypeError(MessageTemplate message,
                                   TNode<Context> context, TArgs... args) {
-    STATIC_ASSERT(sizeof...(TArgs) <= 3);
+    static_assert(sizeof...(TArgs) <= 3);
     return CAST(CallRuntime(Runtime::kNewTypeError, context,
                             SmiConstant(message), args...));
   }
diff --git a/src/codegen/compilation-cache.cc b/src/codegen/compilation-cache.cc
index 81d456e6185..5c979ca8468 100644
--- a/src/codegen/compilation-cache.cc
+++ b/src/codegen/compilation-cache.cc
@@ -51,7 +51,7 @@ Handle<CompilationCacheTable> CompilationCacheRegExp::GetTable(int generation) {
 }
 
 void CompilationCacheRegExp::Age() {
-  STATIC_ASSERT(kGenerations > 1);
+  static_assert(kGenerations > 1);
 
   // Age the generations implicitly killing off the oldest.
   for (int i = kGenerations - 1; i > 0; i--) {
@@ -102,7 +102,7 @@ void CompilationCacheEval::Age() {
       // Note: The following static assert only establishes an explicit
       // connection between initialization- and use-sites of the smi value
       // field.
-      STATIC_ASSERT(CompilationCacheTable::kHashGenerations);
+      static_assert(CompilationCacheTable::kHashGenerations);
       const int new_count = Smi::ToInt(table.PrimaryValueAt(entry)) - 1;
       if (new_count == 0) {
         table.RemoveEntry(entry);
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index 1e4e452648e..2eb369f1a70 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -3559,7 +3559,7 @@ void Compiler::PostInstantiation(Handle<JSFunction> function) {
 
         // We don't need a release store because the optimized code was
         // stored with release semantics into the vector
-        STATIC_ASSERT(
+        static_assert(
             FeedbackVector::kFeedbackVectorMaybeOptimizedCodeIsStoreRelease);
         function->set_code(code);
       }
diff --git a/src/codegen/cpu-features.h b/src/codegen/cpu-features.h
index 88f28b92388..fc85f064fd6 100644
--- a/src/codegen/cpu-features.h
+++ b/src/codegen/cpu-features.h
@@ -96,7 +96,7 @@ class V8_EXPORT_PRIVATE CpuFeatures : public AllStatic {
   CpuFeatures& operator=(const CpuFeatures&) = delete;
 
   static void Probe(bool cross_compile) {
-    STATIC_ASSERT(NUMBER_OF_CPU_FEATURES <= kBitsPerInt);
+    static_assert(NUMBER_OF_CPU_FEATURES <= kBitsPerInt);
     if (initialized_) return;
     initialized_ = true;
     ProbeImpl(cross_compile);
diff --git a/src/codegen/external-reference-table.cc b/src/codegen/external-reference-table.cc
index d07f021a8bf..8e4852aa350 100644
--- a/src/codegen/external-reference-table.cc
+++ b/src/codegen/external-reference-table.cc
@@ -294,7 +294,7 @@ Address ExternalReferenceTable::GetStatsCounterAddress(StatsCounter* counter) {
     return reinterpret_cast<Address>(&dummy_stats_counter_);
   }
   std::atomic<int>* address = counter->GetInternalPointer();
-  STATIC_ASSERT(sizeof(address) == sizeof(Address));
+  static_assert(sizeof(address) == sizeof(Address));
   return reinterpret_cast<Address>(address);
 }
 
diff --git a/src/codegen/external-reference-table.h b/src/codegen/external-reference-table.h
index 4f8839aa53b..b50b1d534a6 100644
--- a/src/codegen/external-reference-table.h
+++ b/src/codegen/external-reference-table.h
@@ -99,7 +99,7 @@ class ExternalReferenceTable {
   Address GetStatsCounterAddress(StatsCounter* counter);
   void AddNativeCodeStatsCounters(Isolate* isolate, int* index);
 
-  STATIC_ASSERT(sizeof(Address) == kEntrySize);
+  static_assert(sizeof(Address) == kEntrySize);
   Address ref_addr_[kSize];
   static const char* const ref_name_[kSize];
 
@@ -114,7 +114,7 @@ class ExternalReferenceTable {
   uint32_t dummy_stats_counter_ = 0;
 };
 
-STATIC_ASSERT(ExternalReferenceTable::kSizeInBytes ==
+static_assert(ExternalReferenceTable::kSizeInBytes ==
               sizeof(ExternalReferenceTable));
 
 }  // namespace internal
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 28c7c789e65..b22bfed156b 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -319,13 +319,13 @@ struct IsValidExternalReferenceType<Result (Class::*)(Args...)> {
 
 #define FUNCTION_REFERENCE(Name, Target)                                   \
   ExternalReference ExternalReference::Name() {                            \
-    STATIC_ASSERT(IsValidExternalReferenceType<decltype(&Target)>::value); \
+    static_assert(IsValidExternalReferenceType<decltype(&Target)>::value); \
     return ExternalReference(Redirect(FUNCTION_ADDR(Target)));             \
   }
 
 #define FUNCTION_REFERENCE_WITH_TYPE(Name, Target, Type)                   \
   ExternalReference ExternalReference::Name() {                            \
-    STATIC_ASSERT(IsValidExternalReferenceType<decltype(&Target)>::value); \
+    static_assert(IsValidExternalReferenceType<decltype(&Target)>::value); \
     return ExternalReference(Redirect(FUNCTION_ADDR(Target), Type));       \
   }
 
diff --git a/src/codegen/ia32/assembler-ia32.h b/src/codegen/ia32/assembler-ia32.h
index 1c0a68eb9b2..1653369a2ef 100644
--- a/src/codegen/ia32/assembler-ia32.h
+++ b/src/codegen/ia32/assembler-ia32.h
@@ -366,7 +366,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // otherwise valid instructions.)
   // This allows for a single, fast space check per instruction.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
  public:
   // Create an assembler. Instructions and relocation information are emitted
diff --git a/src/codegen/ia32/interface-descriptors-ia32-inl.h b/src/codegen/ia32/interface-descriptors-ia32-inl.h
index 411ec96ed7a..2f05a956350 100644
--- a/src/codegen/ia32/interface-descriptors-ia32-inl.h
+++ b/src/codegen/ia32/interface-descriptors-ia32-inl.h
@@ -14,7 +14,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(eax, ecx, edx, edi);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index f6e092a5781..220f7c23d7b 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -889,7 +889,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       // SMIs are stored shifted left by 1 byte with the tag being 0.
       // This is equivalent to multiplying by 2. To convert SMIs to bytes we
       // can therefore just multiply the stored value by half the system pointer
@@ -1019,8 +1019,8 @@ void MacroAssembler::EnterExitFramePrologue(StackFrame::Type frame_type,
   DCHECK_EQ(-2 * kSystemPointerSize, ExitFrameConstants::kSPOffset);
   push(Immediate(0));  // Saved entry sp, patched before call.
 
-  STATIC_ASSERT(edx == kRuntimeCallFunctionRegister);
-  STATIC_ASSERT(esi == kContextRegister);
+  static_assert(edx == kRuntimeCallFunctionRegister);
+  static_assert(esi == kContextRegister);
 
   // Save the frame pointer and the context in top.
   ExternalReference c_entry_fp_address =
@@ -1141,8 +1141,8 @@ void MacroAssembler::LeaveApiExitFrame() {
 void MacroAssembler::PushStackHandler(Register scratch) {
   ASM_CODE_COMMENT(this);
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
 
   push(Immediate(0));  // Padding.
 
@@ -1157,7 +1157,7 @@ void MacroAssembler::PushStackHandler(Register scratch) {
 
 void MacroAssembler::PopStackHandler(Register scratch) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   ExternalReference handler_address =
       ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate());
   pop(ExternalReferenceAsOperand(handler_address, scratch));
@@ -1888,10 +1888,10 @@ void TurboAssembler::Call(Handle<Code> code_object, RelocInfo::Mode rmode) {
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSystemPointerSize == 4);
-  STATIC_ASSERT(kSmiShiftSize == 0);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 4);
+  static_assert(kSmiShiftSize == 0);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   // Untagging is folded into the indexing operand below (we use
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 0e9e63133f1..74588a1a256 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -533,8 +533,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   // Smi tagging support.
   void SmiTag(Register reg) {
-    STATIC_ASSERT(kSmiTag == 0);
-    STATIC_ASSERT(kSmiTagSize == 1);
+    static_assert(kSmiTag == 0);
+    static_assert(kSmiTagSize == 1);
     add(reg, reg);
   }
 
diff --git a/src/codegen/interface-descriptors-inl.h b/src/codegen/interface-descriptors-inl.h
index 13ea72d5a01..5b8337b6cab 100644
--- a/src/codegen/interface-descriptors-inl.h
+++ b/src/codegen/interface-descriptors-inl.h
@@ -268,7 +268,7 @@ constexpr Register LoadNoFeedbackDescriptor::ICKindRegister() {
 // need to choose a new register here.
 // static
 constexpr Register LoadGlobalWithVectorDescriptor::VectorRegister() {
-  STATIC_ASSERT(!LoadWithVectorDescriptor::VectorRegister().is_valid());
+  static_assert(!LoadWithVectorDescriptor::VectorRegister().is_valid());
   return LoadDescriptor::ReceiverRegister();
 }
 #else
@@ -342,7 +342,7 @@ constexpr auto BaselineOutOfLinePrologueDescriptor::registers() {
       kJavaScriptCallExtraArg1Register, kJavaScriptCallNewTargetRegister,
       kInterpreterBytecodeArrayRegister);
 #elif V8_TARGET_ARCH_IA32
-  STATIC_ASSERT(kJSFunctionRegister == kInterpreterBytecodeArrayRegister);
+  static_assert(kJSFunctionRegister == kInterpreterBytecodeArrayRegister);
   return RegisterArray(
       kContextRegister, kJSFunctionRegister, kJavaScriptCallArgCountRegister,
       kJavaScriptCallExtraArg1Register, kJavaScriptCallNewTargetRegister);
diff --git a/src/codegen/interface-descriptors.h b/src/codegen/interface-descriptors.h
index dfd3fdfe663..35d2a5d1007 100644
--- a/src/codegen/interface-descriptors.h
+++ b/src/codegen/interface-descriptors.h
@@ -308,7 +308,7 @@ constexpr int kMaxTFSBuiltinRegisterParams = 3;
 constexpr int kMaxBuiltinRegisterParams = 5;
 constexpr int kMaxTFSBuiltinRegisterParams = kMaxBuiltinRegisterParams;
 #endif
-STATIC_ASSERT(kMaxTFSBuiltinRegisterParams <= kMaxBuiltinRegisterParams);
+static_assert(kMaxTFSBuiltinRegisterParams <= kMaxBuiltinRegisterParams);
 constexpr int kJSBuiltinRegisterParams = 4;
 
 // Polymorphic base class for call interface descriptors, which defines getters
diff --git a/src/codegen/loong64/assembler-loong64.h b/src/codegen/loong64/assembler-loong64.h
index a25f105c704..e1a38da0723 100644
--- a/src/codegen/loong64/assembler-loong64.h
+++ b/src/codegen/loong64/assembler-loong64.h
@@ -906,7 +906,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 64;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   // Repeated checking whether the trampoline pool should be emitted is rather
   // expensive. By default we only check again once a number of instructions
diff --git a/src/codegen/loong64/constants-loong64.h b/src/codegen/loong64/constants-loong64.h
index 4142e83fdd7..442c7b3e7b2 100644
--- a/src/codegen/loong64/constants-loong64.h
+++ b/src/codegen/loong64/constants-loong64.h
@@ -137,7 +137,7 @@ enum SoftwareInterruptCodes {
 //   debugger.
 const uint32_t kMaxWatchpointCode = 31;
 const uint32_t kMaxStopCode = 127;
-STATIC_ASSERT(kMaxWatchpointCode < kMaxStopCode);
+static_assert(kMaxWatchpointCode < kMaxStopCode);
 
 // ----- Fields offset and length.
 const int kRjShift = 5;
diff --git a/src/codegen/loong64/interface-descriptors-loong64-inl.h b/src/codegen/loong64/interface-descriptors-loong64-inl.h
index 1700b885b12..a260c781908 100644
--- a/src/codegen/loong64/interface-descriptors-loong64-inl.h
+++ b/src/codegen/loong64/interface-descriptors-loong64-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(a0, a1, a2, a3, a4);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/loong64/macro-assembler-loong64.cc b/src/codegen/loong64/macro-assembler-loong64.cc
index 80a2aad66e4..cbf8612bccc 100644
--- a/src/codegen/loong64/macro-assembler-loong64.cc
+++ b/src/codegen/loong64/macro-assembler-loong64.cc
@@ -2711,9 +2711,9 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSystemPointerSize == 8);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   SmiUntag(builtin_index, builtin_index);
@@ -2793,7 +2793,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       DCHECK_NE(scratch, no_reg);
       SmiScale(scratch, count, kPointerSizeLog2);
       Add_d(sp, sp, scratch);
@@ -2908,8 +2908,8 @@ void TurboAssembler::PushArray(Register array, Register size, Register scratch,
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
 
   Push(Smi::zero());  // Padding.
 
@@ -2924,7 +2924,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   Pop(a1);
   Add_d(sp, sp,
         Operand(
@@ -3509,9 +3509,9 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
          frame_type == StackFrame::BUILTIN_EXIT);
 
   // Set up the frame structure on the stack.
-  STATIC_ASSERT(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
-  STATIC_ASSERT(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
-  STATIC_ASSERT(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
+  static_assert(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
+  static_assert(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
+  static_assert(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
 
   // This is how the stack will look:
   // fp + 2 (==kCallerSPDisplacement) - old stack's end
@@ -3702,7 +3702,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 void TurboAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -3713,7 +3713,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 void TurboAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -3725,7 +3725,7 @@ void MacroAssembler::AssertConstructor(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, t8,
           Operand(zero_reg));
@@ -3741,7 +3741,7 @@ void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -3758,7 +3758,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -3776,7 +3776,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, t8,
           Operand(zero_reg));
@@ -3790,7 +3790,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   SmiTst(object, t8);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, t8,
         Operand(zero_reg));
diff --git a/src/codegen/loong64/macro-assembler-loong64.h b/src/codegen/loong64/macro-assembler-loong64.h
index 320871809bb..4ed8d9c1f08 100644
--- a/src/codegen/loong64/macro-assembler-loong64.h
+++ b/src/codegen/loong64/macro-assembler-loong64.h
@@ -418,7 +418,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 #undef DEFINE_INSTRUCTION3
 
   void SmiTag(Register dst, Register src) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     if (SmiValuesAre32Bits()) {
       slli_d(dst, src, 32);
     } else {
diff --git a/src/codegen/machine-type.h b/src/codegen/machine-type.h
index 92969707eb6..ccbfbfd476a 100644
--- a/src/codegen/machine-type.h
+++ b/src/codegen/machine-type.h
@@ -417,9 +417,9 @@ constexpr int kMaximumReprSizeLog2 =
     ElementSizeLog2Of(MachineRepresentation::kSimd128);
 constexpr int kMaximumReprSizeInBytes = 1 << kTaggedSizeLog2;
 
-STATIC_ASSERT(kMaximumReprSizeLog2 >=
+static_assert(kMaximumReprSizeLog2 >=
               ElementSizeLog2Of(MachineRepresentation::kTagged));
-STATIC_ASSERT(kMaximumReprSizeLog2 >=
+static_assert(kMaximumReprSizeLog2 >=
               ElementSizeLog2Of(MachineRepresentation::kWord64));
 
 V8_EXPORT_PRIVATE inline constexpr int ElementSizeInBytes(
diff --git a/src/codegen/mips/assembler-mips.h b/src/codegen/mips/assembler-mips.h
index 628a8bc652c..5c8f8f75887 100644
--- a/src/codegen/mips/assembler-mips.h
+++ b/src/codegen/mips/assembler-mips.h
@@ -1627,7 +1627,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   // Repeated checking whether the trampoline pool should be emitted is rather
   // expensive. By default we only check again once a number of instructions
diff --git a/src/codegen/mips/constants-mips.h b/src/codegen/mips/constants-mips.h
index 3dda85ceda1..4cd67bb176a 100644
--- a/src/codegen/mips/constants-mips.h
+++ b/src/codegen/mips/constants-mips.h
@@ -320,7 +320,7 @@ enum SoftwareInterruptCodes {
 //   debugger.
 const uint32_t kMaxWatchpointCode = 31;
 const uint32_t kMaxStopCode = 127;
-STATIC_ASSERT(kMaxWatchpointCode < kMaxStopCode);
+static_assert(kMaxWatchpointCode < kMaxStopCode);
 
 // ----- Fields offset and length.
 const int kOpcodeShift = 26;
diff --git a/src/codegen/mips/interface-descriptors-mips-inl.h b/src/codegen/mips/interface-descriptors-mips-inl.h
index e3bf41b76a3..c091859955a 100644
--- a/src/codegen/mips/interface-descriptors-mips-inl.h
+++ b/src/codegen/mips/interface-descriptors-mips-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(a0, a1, a2, a3, t0);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/mips/macro-assembler-mips.cc b/src/codegen/mips/macro-assembler-mips.cc
index e284dc4f505..a94591daf41 100644
--- a/src/codegen/mips/macro-assembler-mips.cc
+++ b/src/codegen/mips/macro-assembler-mips.cc
@@ -3912,10 +3912,10 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSystemPointerSize == 4);
-  STATIC_ASSERT(kSmiShiftSize == 0);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 4);
+  static_assert(kSmiShiftSize == 0);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   SmiUntag(builtin_index, builtin_index);
@@ -4051,7 +4051,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       Lsa(sp, sp, count, kPointerSizeLog2 - kSmiTagSize, count);
       break;
     }
@@ -4196,8 +4196,8 @@ void TurboAssembler::PushArray(Register array, Register size, Register scratch,
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
 
   Push(Smi::zero());  // Padding.
 
@@ -4212,7 +4212,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   pop(a1);
   Addu(sp, sp, Operand(StackHandlerConstants::kSize - kPointerSize));
   UseScratchRegisterScope temps(this);
@@ -4853,9 +4853,9 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
          frame_type == StackFrame::BUILTIN_EXIT);
 
   // Set up the frame structure on the stack.
-  STATIC_ASSERT(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
-  STATIC_ASSERT(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
-  STATIC_ASSERT(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
+  static_assert(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
+  static_assert(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
+  static_assert(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
 
   // This is how the stack will look:
   // fp + 2 (==kCallerSPDisplacement) - old stack's end
@@ -5034,7 +5034,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label,
 void MacroAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -5045,7 +5045,7 @@ void MacroAssembler::AssertNotSmi(Register object) {
 void MacroAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -5057,7 +5057,7 @@ void MacroAssembler::AssertConstructor(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, t8,
           Operand(zero_reg));
@@ -5073,7 +5073,7 @@ void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -5090,7 +5090,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -5108,7 +5108,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, t8,
           Operand(zero_reg));
@@ -5122,7 +5122,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   SmiTst(object, t8);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, t8,
         Operand(zero_reg));
diff --git a/src/codegen/mips64/assembler-mips64.h b/src/codegen/mips64/assembler-mips64.h
index 2bfb6e2dfd4..f6eda140812 100644
--- a/src/codegen/mips64/assembler-mips64.h
+++ b/src/codegen/mips64/assembler-mips64.h
@@ -1662,7 +1662,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 64;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   // Repeated checking whether the trampoline pool should be emitted is rather
   // expensive. By default we only check again once a number of instructions
diff --git a/src/codegen/mips64/constants-mips64.h b/src/codegen/mips64/constants-mips64.h
index d584b77d1be..2ca0a43904d 100644
--- a/src/codegen/mips64/constants-mips64.h
+++ b/src/codegen/mips64/constants-mips64.h
@@ -297,7 +297,7 @@ enum SoftwareInterruptCodes {
 //   debugger.
 const uint32_t kMaxWatchpointCode = 31;
 const uint32_t kMaxStopCode = 127;
-STATIC_ASSERT(kMaxWatchpointCode < kMaxStopCode);
+static_assert(kMaxWatchpointCode < kMaxStopCode);
 
 // ----- Fields offset and length.
 const int kOpcodeShift = 26;
diff --git a/src/codegen/mips64/interface-descriptors-mips64-inl.h b/src/codegen/mips64/interface-descriptors-mips64-inl.h
index d6066e09304..3c0e0dc27fa 100644
--- a/src/codegen/mips64/interface-descriptors-mips64-inl.h
+++ b/src/codegen/mips64/interface-descriptors-mips64-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(a0, a1, a2, a3, a4);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/mips64/macro-assembler-mips64.cc b/src/codegen/mips64/macro-assembler-mips64.cc
index c6efc4fc2b8..0025c5c8db4 100644
--- a/src/codegen/mips64/macro-assembler-mips64.cc
+++ b/src/codegen/mips64/macro-assembler-mips64.cc
@@ -4403,9 +4403,9 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSystemPointerSize == 8);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   SmiUntag(builtin_index, builtin_index);
@@ -4572,7 +4572,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       DCHECK_NE(scratch, no_reg);
       SmiScale(scratch, count, kPointerSizeLog2);
       Daddu(sp, sp, scratch);
@@ -4720,8 +4720,8 @@ void TurboAssembler::PushArray(Register array, Register size, Register scratch,
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
 
   Push(Smi::zero());  // Padding.
 
@@ -4736,7 +4736,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   pop(a1);
   Daddu(sp, sp,
         Operand(
@@ -5386,9 +5386,9 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
          frame_type == StackFrame::BUILTIN_EXIT);
 
   // Set up the frame structure on the stack.
-  STATIC_ASSERT(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
-  STATIC_ASSERT(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
-  STATIC_ASSERT(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
+  static_assert(2 * kPointerSize == ExitFrameConstants::kCallerSPDisplacement);
+  static_assert(1 * kPointerSize == ExitFrameConstants::kCallerPCOffset);
+  static_assert(0 * kPointerSize == ExitFrameConstants::kCallerFPOffset);
 
   // This is how the stack will look:
   // fp + 2 (==kCallerSPDisplacement) - old stack's end
@@ -5582,7 +5582,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label,
 void TurboAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -5593,7 +5593,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 void TurboAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     andi(scratch, object, kSmiTagMask);
@@ -5605,7 +5605,7 @@ void MacroAssembler::AssertConstructor(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, t8,
           Operand(zero_reg));
@@ -5621,7 +5621,7 @@ void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -5638,7 +5638,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, t8,
           Operand(zero_reg));
@@ -5656,7 +5656,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, t8);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, t8,
           Operand(zero_reg));
@@ -5670,7 +5670,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   SmiTst(object, t8);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, t8,
         Operand(zero_reg));
diff --git a/src/codegen/mips64/macro-assembler-mips64.h b/src/codegen/mips64/macro-assembler-mips64.h
index e13fa7eac2b..bb7996f205c 100644
--- a/src/codegen/mips64/macro-assembler-mips64.h
+++ b/src/codegen/mips64/macro-assembler-mips64.h
@@ -483,7 +483,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 #undef DEFINE_INSTRUCTION3
 
   void SmiTag(Register dst, Register src) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     if (SmiValuesAre32Bits()) {
       dsll32(dst, src, 0);
     } else {
diff --git a/src/codegen/ppc/assembler-ppc.h b/src/codegen/ppc/assembler-ppc.h
index 54251cdb726..78f0dd9bd2a 100644
--- a/src/codegen/ppc/assembler-ppc.h
+++ b/src/codegen/ppc/assembler-ppc.h
@@ -1388,7 +1388,7 @@ class Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   RelocInfoWriter reloc_info_writer;
 
diff --git a/src/codegen/ppc/interface-descriptors-ppc-inl.h b/src/codegen/ppc/interface-descriptors-ppc-inl.h
index 8a87c97c7ce..b7b1ea515a8 100644
--- a/src/codegen/ppc/interface-descriptors-ppc-inl.h
+++ b/src/codegen/ppc/interface-descriptors-ppc-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(r3, r4, r5, r6, r7);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/ppc/macro-assembler-ppc.cc b/src/codegen/ppc/macro-assembler-ppc.cc
index c96c7a60e0e..cd8e245c7d2 100644
--- a/src/codegen/ppc/macro-assembler-ppc.cc
+++ b/src/codegen/ppc/macro-assembler-ppc.cc
@@ -1150,7 +1150,7 @@ void TurboAssembler::ShiftRightAlgPair(Register dst_low, Register dst_high,
 void TurboAssembler::LoadConstantPoolPointerRegisterFromCodeTargetAddress(
     Register code_target_address) {
   // Builtins do not use the constant pool (see is_constant_pool_available).
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
 
   lwz(r0, MemOperand(code_target_address,
                      Code::kInstructionSizeOffset - Code::kHeaderSize));
@@ -1176,7 +1176,7 @@ void TurboAssembler::ComputeCodeStartAddress(Register dst) {
 void TurboAssembler::LoadConstantPoolPointerRegister() {
   //
   // Builtins do not use the constant pool (see is_constant_pool_available).
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
 
   LoadPC(kConstantPoolRegister);
   int32_t delta = -pc_offset() + 4;
@@ -1216,7 +1216,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       SmiToPtrArrayOffset(count, count);
       add(sp, sp, count);
       break;
@@ -1675,8 +1675,8 @@ void MacroAssembler::InvokeFunction(Register function,
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
 
   Push(Smi::zero());  // Padding.
 
@@ -1692,8 +1692,8 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
 
   pop(r4);
   Move(ip,
@@ -1713,8 +1713,8 @@ void MacroAssembler::CompareObjectType(Register object, Register map,
 
 void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
                                          InstanceType type) {
-  STATIC_ASSERT(Map::kInstanceTypeOffset < 4096);
-  STATIC_ASSERT(LAST_TYPE <= 0xFFFF);
+  static_assert(Map::kInstanceTypeOffset < 4096);
+  static_assert(LAST_TYPE <= 0xFFFF);
   lhz(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
   cmpi(type_reg, Operand(type));
 }
@@ -2138,7 +2138,7 @@ void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
 
 void TurboAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmi, cr0);
   }
@@ -2146,7 +2146,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 
 void TurboAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(eq, AbortReason::kOperandIsNotASmi, cr0);
   }
@@ -2154,7 +2154,7 @@ void TurboAssembler::AssertSmi(Register object) {
 
 void MacroAssembler::AssertConstructor(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, cr0);
     push(object);
@@ -2168,7 +2168,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 
 void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
     push(object);
@@ -2183,7 +2183,7 @@ void MacroAssembler::AssertFunction(Register object) {
 void MacroAssembler::AssertCallableFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   TestIfSmi(object, r0);
   Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
   push(object);
@@ -2196,7 +2196,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 
 void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, cr0);
     push(object);
@@ -3629,9 +3629,9 @@ void TurboAssembler::JumpIfLessThan(Register x, int32_t y, Label* dest) {
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
-  STATIC_ASSERT(kSystemPointerSize == 8);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin_index register contains the builtin index as a Smi.
   if (SmiValuesAre32Bits()) {
diff --git a/src/codegen/ppc/macro-assembler-ppc.h b/src/codegen/ppc/macro-assembler-ppc.h
index 4c74d7eff86..aa7b58e5ceb 100644
--- a/src/codegen/ppc/macro-assembler-ppc.h
+++ b/src/codegen/ppc/macro-assembler-ppc.h
@@ -969,10 +969,10 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   void SmiToPtrArrayOffset(Register dst, Register src) {
 #if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
-    STATIC_ASSERT(kSmiTag == 0 && kSmiShift < kSystemPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiShift < kSystemPointerSizeLog2);
     ShiftLeftU64(dst, src, Operand(kSystemPointerSizeLog2 - kSmiShift));
 #else
-    STATIC_ASSERT(kSmiTag == 0 && kSmiShift > kSystemPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiShift > kSystemPointerSizeLog2);
     ShiftRightS64(dst, src, Operand(kSmiShift - kSystemPointerSizeLog2));
 #endif
   }
@@ -1316,8 +1316,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 #if !defined(V8_COMPRESS_POINTERS) && !defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
   // Ensure it is permissible to read/write int value directly from
   // upper half of the smi.
-  STATIC_ASSERT(kSmiTag == 0);
-  STATIC_ASSERT(kSmiTagSize + kSmiShiftSize == 32);
+  static_assert(kSmiTag == 0);
+  static_assert(kSmiTagSize + kSmiShiftSize == 32);
 #endif
 #if V8_TARGET_ARCH_PPC64 && V8_TARGET_LITTLE_ENDIAN
 #define SmiWordOffset(offset) (offset + kSystemPointerSize / 2)
diff --git a/src/codegen/register-base.h b/src/codegen/register-base.h
index 3a37840af45..f0ff78ad0a7 100644
--- a/src/codegen/register-base.h
+++ b/src/codegen/register-base.h
@@ -62,7 +62,7 @@ class RegisterBase {
 
  private:
   int8_t reg_code_;
-  STATIC_ASSERT(kAfterLastRegister <= kMaxInt8);
+  static_assert(kAfterLastRegister <= kMaxInt8);
 };
 
 template <typename RegType,
@@ -77,7 +77,7 @@ inline std::ostream& operator<<(std::ostream& os, RegType reg) {
 #define DEFINE_REGISTER_NAMES(RegType, LIST)                                   \
   inline const char* RegisterName(RegType reg) {                               \
     static constexpr const char* Names[] = {LIST(DEFINE_REGISTER_NAMES_NAME)}; \
-    STATIC_ASSERT(arraysize(Names) == RegType::kNumRegisters);                 \
+    static_assert(arraysize(Names) == RegType::kNumRegisters);                 \
     return reg.is_valid() ? Names[reg.code()] : "invalid";                     \
   }
 
diff --git a/src/codegen/register-configuration.cc b/src/codegen/register-configuration.cc
index 91f06aec5bf..3f28e35e0b9 100644
--- a/src/codegen/register-configuration.cc
+++ b/src/codegen/register-configuration.cc
@@ -45,13 +45,13 @@ static const int kAllocatableSIMD128Codes[] = {
 #undef REGISTER_CODE
 #endif  // V8_TARGET_ARCH_RISCV64
 
-STATIC_ASSERT(RegisterConfiguration::kMaxGeneralRegisters >=
+static_assert(RegisterConfiguration::kMaxGeneralRegisters >=
               Register::kNumRegisters);
-STATIC_ASSERT(RegisterConfiguration::kMaxFPRegisters >=
+static_assert(RegisterConfiguration::kMaxFPRegisters >=
               FloatRegister::kNumRegisters);
-STATIC_ASSERT(RegisterConfiguration::kMaxFPRegisters >=
+static_assert(RegisterConfiguration::kMaxFPRegisters >=
               DoubleRegister::kNumRegisters);
-STATIC_ASSERT(RegisterConfiguration::kMaxFPRegisters >=
+static_assert(RegisterConfiguration::kMaxFPRegisters >=
               Simd128Register::kNumRegisters);
 
 static int get_num_simd128_registers() {
@@ -296,9 +296,9 @@ RegisterConfiguration::RegisterConfiguration(
 }
 
 // Assert that kFloat32, kFloat64, and kSimd128 are consecutive values.
-STATIC_ASSERT(static_cast<int>(MachineRepresentation::kSimd128) ==
+static_assert(static_cast<int>(MachineRepresentation::kSimd128) ==
               static_cast<int>(MachineRepresentation::kFloat64) + 1);
-STATIC_ASSERT(static_cast<int>(MachineRepresentation::kFloat64) ==
+static_assert(static_cast<int>(MachineRepresentation::kFloat64) ==
               static_cast<int>(MachineRepresentation::kFloat32) + 1);
 
 int RegisterConfiguration::GetAliases(MachineRepresentation rep, int index,
diff --git a/src/codegen/reglist-base.h b/src/codegen/reglist-base.h
index 6fc67cd3046..3bc51c45602 100644
--- a/src/codegen/reglist-base.h
+++ b/src/codegen/reglist-base.h
@@ -23,7 +23,7 @@ class RegListBase {
       RegisterT::kNumRegisters <= 16, uint16_t,
       typename std::conditional<RegisterT::kNumRegisters <= 32, uint32_t,
                                 uint64_t>::type>::type;
-  STATIC_ASSERT(RegisterT::kNumRegisters <= 64);
+  static_assert(RegisterT::kNumRegisters <= 64);
 
  public:
   class Iterator;
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index d110e387b41..107c325c5a7 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -104,7 +104,7 @@ void RelocInfoWriter::WriteShortData(intptr_t data_delta) {
 }
 
 void RelocInfoWriter::WriteMode(RelocInfo::Mode rmode) {
-  STATIC_ASSERT(RelocInfo::NUMBER_OF_MODES <= (1 << kLongTagBits));
+  static_assert(RelocInfo::NUMBER_OF_MODES <= (1 << kLongTagBits));
   *--pos_ = static_cast<int>((rmode << kTagBits) | kDefaultTag);
 }
 
diff --git a/src/codegen/reloc-info.h b/src/codegen/reloc-info.h
index b92907fbf0a..918e060c0aa 100644
--- a/src/codegen/reloc-info.h
+++ b/src/codegen/reloc-info.h
@@ -109,7 +109,7 @@ class RelocInfo {
     FIRST_SHAREABLE_RELOC_MODE = WASM_CALL,
   };
 
-  STATIC_ASSERT(NUMBER_OF_MODES <= kBitsPerInt);
+  static_assert(NUMBER_OF_MODES <= kBitsPerInt);
 
   RelocInfo() = default;
 
diff --git a/src/codegen/riscv64/assembler-riscv64.h b/src/codegen/riscv64/assembler-riscv64.h
index 2b0d262369c..1cf9425a543 100644
--- a/src/codegen/riscv64/assembler-riscv64.h
+++ b/src/codegen/riscv64/assembler-riscv64.h
@@ -1503,7 +1503,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 64;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
   // Repeated checking whether the trampoline pool should be emitted is rather
   // expensive. By default we only check again once a number of instructions
diff --git a/src/codegen/riscv64/constants-riscv64.h b/src/codegen/riscv64/constants-riscv64.h
index 806f457f34f..4e81c849f8f 100644
--- a/src/codegen/riscv64/constants-riscv64.h
+++ b/src/codegen/riscv64/constants-riscv64.h
@@ -176,7 +176,7 @@ enum SoftwareInterruptCodes {
 //   debugger.
 const uint32_t kMaxWatchpointCode = 31;
 const uint32_t kMaxStopCode = 127;
-STATIC_ASSERT(kMaxWatchpointCode < kMaxStopCode);
+static_assert(kMaxWatchpointCode < kMaxStopCode);
 
 // ----- Fields offset and length.
 // RISCV constants
diff --git a/src/codegen/riscv64/interface-descriptors-riscv64-inl.h b/src/codegen/riscv64/interface-descriptors-riscv64-inl.h
index 68f90ef0a83..0559c46a600 100644
--- a/src/codegen/riscv64/interface-descriptors-riscv64-inl.h
+++ b/src/codegen/riscv64/interface-descriptors-riscv64-inl.h
@@ -16,7 +16,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(a0, a1, a2, a3, a4);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/riscv64/macro-assembler-riscv64.cc b/src/codegen/riscv64/macro-assembler-riscv64.cc
index 8cf27979006..8f68f4a5b0c 100644
--- a/src/codegen/riscv64/macro-assembler-riscv64.cc
+++ b/src/codegen/riscv64/macro-assembler-riscv64.cc
@@ -3359,9 +3359,9 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin) {
-  STATIC_ASSERT(kSystemPointerSize == 8);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
 
   // The builtin register contains the builtin index as a Smi.
   SmiUntag(builtin, builtin);
@@ -3598,8 +3598,8 @@ void TurboAssembler::Push(Handle<HeapObject> handle) {
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
 
   Push(Smi::zero());  // Padding.
 
@@ -3617,7 +3617,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   pop(a1);
   Add64(sp, sp,
         Operand(static_cast<int64_t>(StackHandlerConstants::kSize -
@@ -4357,10 +4357,10 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
          frame_type == StackFrame::BUILTIN_EXIT);
 
   // Set up the frame structure on the stack.
-  STATIC_ASSERT(2 * kSystemPointerSize ==
+  static_assert(2 * kSystemPointerSize ==
                 ExitFrameConstants::kCallerSPDisplacement);
-  STATIC_ASSERT(1 * kSystemPointerSize == ExitFrameConstants::kCallerPCOffset);
-  STATIC_ASSERT(0 * kSystemPointerSize == ExitFrameConstants::kCallerFPOffset);
+  static_assert(1 * kSystemPointerSize == ExitFrameConstants::kCallerPCOffset);
+  static_assert(0 * kSystemPointerSize == ExitFrameConstants::kCallerFPOffset);
 
   // This is how the stack will look:
   // fp + 2 (==kCallerSPDisplacement) - old stack's end
@@ -4593,7 +4593,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 void TurboAssembler::AssertNotSmi(Register object, AbortReason reason) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
     andi(kScratchReg, object, kSmiTagMask);
     Check(ne, reason, kScratchReg, Operand(zero_reg));
@@ -4603,7 +4603,7 @@ void TurboAssembler::AssertNotSmi(Register object, AbortReason reason) {
 void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
     andi(kScratchReg, object, kSmiTagMask);
     Check(eq, reason, kScratchReg, Operand(zero_reg));
@@ -4615,7 +4615,7 @@ void MacroAssembler::AssertConstructor(Register object) {
     ASM_CODE_COMMENT(this);
     DCHECK(object != kScratchReg);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     SmiTst(object, kScratchReg);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, kScratchReg,
           Operand(zero_reg));
@@ -4632,7 +4632,7 @@ void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
     SmiTst(object, kScratchReg);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, kScratchReg,
@@ -4651,7 +4651,7 @@ void MacroAssembler::AssertFunction(Register object) {
 void MacroAssembler::AssertCallableFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
   push(object);
   LoadMap(object, object);
@@ -4668,7 +4668,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
     SmiTst(object, kScratchReg);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, kScratchReg,
@@ -4683,7 +4683,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   DCHECK(object != kScratchReg);
   SmiTst(object, kScratchReg);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, kScratchReg,
@@ -5149,7 +5149,7 @@ void MacroAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       DCHECK_NE(scratch, no_reg);
       SmiScale(scratch, count, kPointerSizeLog2);
       Add64(sp, sp, scratch);
diff --git a/src/codegen/riscv64/macro-assembler-riscv64.h b/src/codegen/riscv64/macro-assembler-riscv64.h
index 3ee0b8c8356..4c619e52376 100644
--- a/src/codegen/riscv64/macro-assembler-riscv64.h
+++ b/src/codegen/riscv64/macro-assembler-riscv64.h
@@ -867,7 +867,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // Smi utilities.
 
   void SmiTag(Register dst, Register src) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     if (SmiValuesAre32Bits()) {
       // Smi goes to upper 32
       slli(dst, src, 32);
diff --git a/src/codegen/s390/assembler-s390.h b/src/codegen/s390/assembler-s390.h
index 4df3df75cc4..fe2f05d01a2 100644
--- a/src/codegen/s390/assembler-s390.h
+++ b/src/codegen/s390/assembler-s390.h
@@ -1366,7 +1366,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // not have to check for overflow. The same is true for writes of large
   // relocation info entries.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
  protected:
   int buffer_space() const { return reloc_info_writer.pos() - pc_; }
diff --git a/src/codegen/s390/interface-descriptors-s390-inl.h b/src/codegen/s390/interface-descriptors-s390-inl.h
index 456ca18868f..cf92bbff006 100644
--- a/src/codegen/s390/interface-descriptors-s390-inl.h
+++ b/src/codegen/s390/interface-descriptors-s390-inl.h
@@ -15,7 +15,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(r2, r3, r4, r5, r6);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/s390/macro-assembler-s390.cc b/src/codegen/s390/macro-assembler-s390.cc
index 3b07b994c3a..37d38362a6a 100644
--- a/src/codegen/s390/macro-assembler-s390.cc
+++ b/src/codegen/s390/macro-assembler-s390.cc
@@ -1406,7 +1406,7 @@ void TurboAssembler::DropArguments(Register count, ArgumentsCountType type,
       break;
     }
     case kCountIsSmi: {
-      STATIC_ASSERT(kSmiTagSize == 1 && kSmiTag == 0);
+      static_assert(kSmiTagSize == 1 && kSmiTag == 0);
       SmiToPtrArrayOffset(count, count);
       AddS64(sp, sp, count);
       break;
@@ -1849,8 +1849,8 @@ void MacroAssembler::InvokeFunction(Register function,
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0 * kSystemPointerSize);
 
   // Link the current handler as the next handler.
   Move(r7,
@@ -1871,8 +1871,8 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
 
   // Pop the Next Handler into r3 and store it into Handler Address reference.
   Pop(r3);
@@ -1893,8 +1893,8 @@ void MacroAssembler::CompareObjectType(Register object, Register map,
 
 void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
                                          InstanceType type) {
-  STATIC_ASSERT(Map::kInstanceTypeOffset < 4096);
-  STATIC_ASSERT(LAST_TYPE <= 0xFFFF);
+  static_assert(Map::kInstanceTypeOffset < 4096);
+  static_assert(LAST_TYPE <= 0xFFFF);
   LoadS16(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
   CmpS64(type_reg, Operand(type));
 }
@@ -2132,7 +2132,7 @@ void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
 
 void TurboAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmi, cr0);
   }
@@ -2140,7 +2140,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 
 void TurboAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(eq, AbortReason::kOperandIsNotASmi, cr0);
   }
@@ -2148,7 +2148,7 @@ void TurboAssembler::AssertSmi(Register object) {
 
 void MacroAssembler::AssertConstructor(Register object, Register scratch) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor);
     LoadMap(scratch, object);
@@ -2160,7 +2160,7 @@ void MacroAssembler::AssertConstructor(Register object, Register scratch) {
 
 void MacroAssembler::AssertFunction(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
     push(object);
@@ -2175,7 +2175,7 @@ void MacroAssembler::AssertFunction(Register object) {
 void MacroAssembler::AssertCallableFunction(Register object) {
   if (!FLAG_debug_code) return;
   ASM_CODE_COMMENT(this);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   TestIfSmi(object);
   Check(ne, AbortReason::kOperandIsASmiAndNotAFunction);
   push(object);
@@ -2188,7 +2188,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 
 void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
+    static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, cr0);
     push(object);
@@ -4730,9 +4730,9 @@ void TurboAssembler::JumpIfLessThan(Register x, int32_t y, Label* dest) {
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
-  STATIC_ASSERT(kSystemPointerSize == 8);
-  STATIC_ASSERT(kSmiTagSize == 1);
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSystemPointerSize == 8);
+  static_assert(kSmiTagSize == 1);
+  static_assert(kSmiTag == 0);
   // The builtin_index register contains the builtin index as a Smi.
   if (SmiValuesAre32Bits()) {
     ShiftRightS64(builtin_index, builtin_index,
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index dd95c7a31d5..46a9336df06 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -1434,10 +1434,10 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   void SmiToPtrArrayOffset(Register dst, Register src) {
 #if defined(V8_COMPRESS_POINTERS) || defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
-    STATIC_ASSERT(kSmiTag == 0 && kSmiShift < kSystemPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiShift < kSystemPointerSizeLog2);
     ShiftLeftU64(dst, src, Operand(kSystemPointerSizeLog2 - kSmiShift));
 #else
-    STATIC_ASSERT(kSmiTag == 0 && kSmiShift > kSystemPointerSizeLog2);
+    static_assert(kSmiTag == 0 && kSmiShift > kSystemPointerSizeLog2);
     ShiftRightS64(dst, src, Operand(kSmiShift - kSystemPointerSizeLog2));
 #endif
   }
@@ -1702,8 +1702,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 #if !defined(V8_COMPRESS_POINTERS) && !defined(V8_31BIT_SMIS_ON_64BIT_ARCH)
   // Ensure it is permissible to read/write int value directly from
   // upper half of the smi.
-  STATIC_ASSERT(kSmiTag == 0);
-  STATIC_ASSERT(kSmiTagSize + kSmiShiftSize == 32);
+  static_assert(kSmiTag == 0);
+  static_assert(kSmiTagSize + kSmiShiftSize == 32);
 #endif
 #if V8_TARGET_LITTLE_ENDIAN
 #define SmiWordOffset(offset) (offset + kSystemPointerSize / 2)
diff --git a/src/codegen/safepoint-table.cc b/src/codegen/safepoint-table.cc
index 5b7b618fd61..68bd22ffdc7 100644
--- a/src/codegen/safepoint-table.cc
+++ b/src/codegen/safepoint-table.cc
@@ -170,9 +170,9 @@ void SafepointTableBuilder::Emit(Assembler* assembler, int tagged_slots_size) {
 
   // Compute the required sizes of the fields.
   int used_register_indexes = 0;
-  STATIC_ASSERT(SafepointEntry::kNoTrampolinePC == -1);
+  static_assert(SafepointEntry::kNoTrampolinePC == -1);
   int max_pc = SafepointEntry::kNoTrampolinePC;
-  STATIC_ASSERT(SafepointEntry::kNoDeoptIndex == -1);
+  static_assert(SafepointEntry::kNoDeoptIndex == -1);
   int max_deopt_index = SafepointEntry::kNoDeoptIndex;
   for (const EntryBuilder& entry : entries_) {
     used_register_indexes |= entry.register_indexes;
@@ -193,8 +193,8 @@ void SafepointTableBuilder::Emit(Assembler* assembler, int tagged_slots_size) {
   int register_indexes_size = value_to_bytes(used_register_indexes);
   // Add 1 so all values (including kNoDeoptIndex and kNoTrampolinePC) are
   // non-negative.
-  STATIC_ASSERT(SafepointEntry::kNoDeoptIndex == -1);
-  STATIC_ASSERT(SafepointEntry::kNoTrampolinePC == -1);
+  static_assert(SafepointEntry::kNoDeoptIndex == -1);
+  static_assert(SafepointEntry::kNoTrampolinePC == -1);
   int pc_size = value_to_bytes(max_pc + 1);
   int deopt_index_size = value_to_bytes(max_deopt_index + 1);
   int tagged_slots_bytes =
@@ -216,9 +216,9 @@ void SafepointTableBuilder::Emit(Assembler* assembler, int tagged_slots_size) {
       SafepointTable::TaggedSlotsBytesField::encode(tagged_slots_bytes);
 
   // Emit the table header.
-  STATIC_ASSERT(SafepointTable::kLengthOffset == 0 * kIntSize);
-  STATIC_ASSERT(SafepointTable::kEntryConfigurationOffset == 1 * kIntSize);
-  STATIC_ASSERT(SafepointTable::kHeaderSize == 2 * kIntSize);
+  static_assert(SafepointTable::kLengthOffset == 0 * kIntSize);
+  static_assert(SafepointTable::kEntryConfigurationOffset == 1 * kIntSize);
+  static_assert(SafepointTable::kHeaderSize == 2 * kIntSize);
   int length = static_cast<int>(entries_.size());
   assembler->dd(length);
   assembler->dd(entry_configuration);
@@ -234,8 +234,8 @@ void SafepointTableBuilder::Emit(Assembler* assembler, int tagged_slots_size) {
     if (has_deopt_data) {
       // Add 1 so all values (including kNoDeoptIndex and kNoTrampolinePC) are
       // non-negative.
-      STATIC_ASSERT(SafepointEntry::kNoDeoptIndex == -1);
-      STATIC_ASSERT(SafepointEntry::kNoTrampolinePC == -1);
+      static_assert(SafepointEntry::kNoDeoptIndex == -1);
+      static_assert(SafepointEntry::kNoTrampolinePC == -1);
       emit_bytes(entry.deopt_index + 1, deopt_index_size);
       emit_bytes(entry.trampoline + 1, pc_size);
     }
diff --git a/src/codegen/safepoint-table.h b/src/codegen/safepoint-table.h
index 49848d56ae0..9088ec31af5 100644
--- a/src/codegen/safepoint-table.h
+++ b/src/codegen/safepoint-table.h
@@ -116,8 +116,8 @@ class SafepointTable {
     int deopt_index = SafepointEntry::kNoDeoptIndex;
     int trampoline_pc = SafepointEntry::kNoTrampolinePC;
     if (has_deopt_data()) {
-      STATIC_ASSERT(SafepointEntry::kNoDeoptIndex == -1);
-      STATIC_ASSERT(SafepointEntry::kNoTrampolinePC == -1);
+      static_assert(SafepointEntry::kNoDeoptIndex == -1);
+      static_assert(SafepointEntry::kNoTrampolinePC == -1);
       // `-1` to restore the original value, see also
       // SafepointTableBuilder::Emit.
       deopt_index = read_bytes(&entry_ptr, deopt_index_size()) - 1;
diff --git a/src/codegen/signature.h b/src/codegen/signature.h
index 8098ca8adae..38f79d46291 100644
--- a/src/codegen/signature.h
+++ b/src/codegen/signature.h
@@ -25,7 +25,7 @@ class Signature : public ZoneObject {
     DCHECK_EQ(kReturnCountOffset, offsetof(Signature, return_count_));
     DCHECK_EQ(kParameterCountOffset, offsetof(Signature, parameter_count_));
     DCHECK_EQ(kRepsOffset, offsetof(Signature, reps_));
-    STATIC_ASSERT(std::is_standard_layout<Signature<T>>::value);
+    static_assert(std::is_standard_layout<Signature<T>>::value);
   }
 
   size_t return_count() const { return return_count_; }
diff --git a/src/codegen/source-position.h b/src/codegen/source-position.h
index d8982d64755..826849d7238 100644
--- a/src/codegen/source-position.h
+++ b/src/codegen/source-position.h
@@ -122,7 +122,7 @@ class SourcePosition final {
   }
 
   static const int kNotInlined = -1;
-  STATIC_ASSERT(kNoSourcePosition == -1);
+  static_assert(kNoSourcePosition == -1);
 
   int64_t raw() const { return static_cast<int64_t>(value_); }
   static SourcePosition FromRaw(int64_t raw) {
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index d0a615a3b94..d37520999aa 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -408,7 +408,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // otherwise valid instructions.)
   // This allows for a single, fast space check per instruction.
   static constexpr int kGap = 32;
-  STATIC_ASSERT(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
+  static_assert(AssemblerBase::kMinimalBufferSize >= 2 * kGap);
 
  public:
   // Create an assembler. Instructions and relocation information are emitted
diff --git a/src/codegen/x64/interface-descriptors-x64-inl.h b/src/codegen/x64/interface-descriptors-x64-inl.h
index 5481a78c86b..e3c4cdf00b5 100644
--- a/src/codegen/x64/interface-descriptors-x64-inl.h
+++ b/src/codegen/x64/interface-descriptors-x64-inl.h
@@ -14,7 +14,7 @@ namespace internal {
 
 constexpr auto CallInterfaceDescriptor::DefaultRegisterArray() {
   auto registers = RegisterArray(rax, rbx, rcx, rdx, rdi);
-  STATIC_ASSERT(registers.size() == kMaxBuiltinRegisterParams);
+  static_assert(registers.size() == kMaxBuiltinRegisterParams);
   return registers;
 }
 
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 7f85f2279b1..83e96d3bd38 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -1250,7 +1250,7 @@ void TurboAssembler::Cmp(Register dst, int32_t src) {
 }
 
 void TurboAssembler::SmiTag(Register reg) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
   if (COMPRESS_POINTERS_BOOL) {
     shll(reg, Immediate(kSmiShift));
@@ -1270,7 +1270,7 @@ void TurboAssembler::SmiTag(Register dst, Register src) {
 }
 
 void TurboAssembler::SmiUntag(Register reg) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
   // TODO(v8:7703): Is there a way to avoid this sign extension when pointer
   // compression is enabled?
@@ -1289,7 +1289,7 @@ void TurboAssembler::SmiUntag(Register dst, Register src) {
   }
   // TODO(v8:7703): Call SmiUntag(reg) if we can find a way to avoid the extra
   // mov when pointer compression is enabled.
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
   sarq(dst, Immediate(kSmiShift));
 }
@@ -1311,7 +1311,7 @@ void TurboAssembler::SmiUntag(Register dst, Operand src) {
 }
 
 void TurboAssembler::SmiToInt32(Register reg) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
   if (COMPRESS_POINTERS_BOOL) {
     sarl(reg, Immediate(kSmiShift));
@@ -1371,13 +1371,13 @@ void TurboAssembler::Cmp(Operand dst, Smi src) {
 }
 
 Condition TurboAssembler::CheckSmi(Register src) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   testb(src, Immediate(kSmiTagMask));
   return zero;
 }
 
 Condition TurboAssembler::CheckSmi(Operand src) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   testb(src, Immediate(kSmiTagMask));
   return zero;
 }
@@ -1473,7 +1473,7 @@ void TurboAssembler::Push(Smi source) {
 // ----------------------------------------------------------------------------
 
 void TurboAssembler::Move(Register dst, Smi source) {
-  STATIC_ASSERT(kSmiTag == 0);
+  static_assert(kSmiTag == 0);
   int value = source.value();
   if (value == 0) {
     xorl(dst, dst);
@@ -2022,7 +2022,7 @@ void TurboAssembler::LoadCodeDataContainerCodeNonBuiltin(
   ASM_CODE_COMMENT(this);
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // Given the fields layout we can read the Code reference as a full word.
-  STATIC_ASSERT(!V8_EXTERNAL_CODE_SPACE_BOOL ||
+  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
                 (CodeDataContainer::kCodeCageBaseUpper32BitsOffset ==
                  CodeDataContainer::kCodeOffset + kTaggedSize));
   movq(destination, FieldOperand(code_data_container_object,
@@ -2272,8 +2272,8 @@ void TurboAssembler::Popcntq(Register dst, Operand src) {
 
 void MacroAssembler::PushStackHandler() {
   // Adjust this code if not the case.
-  STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
 
   Push(Immediate(0));  // Padding.
 
@@ -2287,7 +2287,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
-  STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
+  static_assert(StackHandlerConstants::kNextOffset == 0);
   ExternalReference handler_address =
       ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate());
   Pop(ExternalReferenceAsOperand(handler_address));
diff --git a/src/common/globals.h b/src/common/globals.h
index b456f2c3bb6..96f530a0514 100644
--- a/src/common/globals.h
+++ b/src/common/globals.h
@@ -107,7 +107,7 @@ constexpr int kStackSpaceRequiredForCompilation = 40;
 constexpr int kStackLimitSlackForDeoptimizationInBytes = 256;
 
 // Sanity-check, assuming that we aim for a real OS stack size of at least 1MB.
-STATIC_ASSERT(V8_DEFAULT_STACK_SIZE_KB* KB +
+static_assert(V8_DEFAULT_STACK_SIZE_KB * KB +
                   kStackLimitSlackForDeoptimizationInBytes <=
               MB);
 
@@ -341,7 +341,7 @@ constexpr size_t kMinExpectedOSPageSize = 4 * KB;  // OS page.
 constexpr size_t kReservedCodeRangePages = 0;
 #endif
 
-STATIC_ASSERT(kSystemPointerSize == (1 << kSystemPointerSizeLog2));
+static_assert(kSystemPointerSize == (1 << kSystemPointerSizeLog2));
 
 #ifdef V8_COMPRESS_ZONES
 #define COMPRESS_ZONES_BOOL true
@@ -378,36 +378,36 @@ using AtomicTagged_t = base::AtomicWord;
 
 #endif  // V8_COMPRESS_POINTERS
 
-STATIC_ASSERT(kTaggedSize == (1 << kTaggedSizeLog2));
-STATIC_ASSERT((kTaggedSize == 8) == TAGGED_SIZE_8_BYTES);
+static_assert(kTaggedSize == (1 << kTaggedSizeLog2));
+static_assert((kTaggedSize == 8) == TAGGED_SIZE_8_BYTES);
 
 using AsAtomicTagged = base::AsAtomicPointerImpl<AtomicTagged_t>;
-STATIC_ASSERT(sizeof(Tagged_t) == kTaggedSize);
-STATIC_ASSERT(sizeof(AtomicTagged_t) == kTaggedSize);
+static_assert(sizeof(Tagged_t) == kTaggedSize);
+static_assert(sizeof(AtomicTagged_t) == kTaggedSize);
 
-STATIC_ASSERT(kTaggedSize == kApiTaggedSize);
+static_assert(kTaggedSize == kApiTaggedSize);
 
 // TODO(ishell): use kTaggedSize or kSystemPointerSize instead.
 #ifndef V8_COMPRESS_POINTERS
 constexpr int kPointerSize = kSystemPointerSize;
 constexpr int kPointerSizeLog2 = kSystemPointerSizeLog2;
-STATIC_ASSERT(kPointerSize == (1 << kPointerSizeLog2));
+static_assert(kPointerSize == (1 << kPointerSizeLog2));
 #endif
 
 // This type defines raw storage type for external (or off-V8 heap) pointers
 // stored on V8 heap.
 constexpr int kExternalPointerSize = sizeof(ExternalPointer_t);
 #ifdef V8_SANDBOXED_EXTERNAL_POINTERS
-STATIC_ASSERT(kExternalPointerSize == kTaggedSize);
+static_assert(kExternalPointerSize == kTaggedSize);
 #else
-STATIC_ASSERT(kExternalPointerSize == kSystemPointerSize);
+static_assert(kExternalPointerSize == kSystemPointerSize);
 #endif
 
 constexpr int kEmbedderDataSlotSize = kSystemPointerSize;
 
 constexpr int kEmbedderDataSlotSizeInTaggedSlots =
     kEmbedderDataSlotSize / kTaggedSize;
-STATIC_ASSERT(kEmbedderDataSlotSize >= kSystemPointerSize);
+static_assert(kEmbedderDataSlotSize >= kSystemPointerSize);
 
 constexpr int kExternalAllocationSoftLimit =
     internal::Internals::kExternalAllocationSoftLimit;
@@ -536,7 +536,7 @@ inline LanguageMode construct_language_mode(bool strict_bit) {
 // otherwise.
 inline LanguageMode stricter_language_mode(LanguageMode mode1,
                                            LanguageMode mode2) {
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   return static_cast<LanguageMode>(static_cast<int>(mode1) |
                                    static_cast<int>(mode2));
 }
@@ -579,7 +579,7 @@ enum class DeoptimizeKind : uint8_t {
 };
 constexpr DeoptimizeKind kFirstDeoptimizeKind = DeoptimizeKind::kEager;
 constexpr DeoptimizeKind kLastDeoptimizeKind = DeoptimizeKind::kLazy;
-STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
 constexpr int kDeoptimizeKindCount = static_cast<int>(kLastDeoptimizeKind) + 1;
 inline size_t hash_value(DeoptimizeKind kind) {
   return static_cast<size_t>(kind);
@@ -880,7 +880,7 @@ enum AllocationSpace {
   LAST_GROWABLE_PAGED_SPACE = MAP_SPACE
 };
 constexpr int kSpaceTagSize = 4;
-STATIC_ASSERT(FIRST_SPACE == 0);
+static_assert(FIRST_SPACE == 0);
 
 enum class AllocationType : uint8_t {
   kYoung,      // Regular object allocated in NEW_SPACE or NEW_LO_SPACE
@@ -1366,7 +1366,7 @@ inline bool IsDynamicVariableMode(VariableMode mode) {
 }
 
 inline bool IsDeclaredVariableMode(VariableMode mode) {
-  STATIC_ASSERT(static_cast<uint8_t>(VariableMode::kLet) ==
+  static_assert(static_cast<uint8_t>(VariableMode::kLet) ==
                 0);  // Implies that mode >= VariableMode::kLet.
   return mode <= VariableMode::kVar;
 }
@@ -1387,7 +1387,7 @@ inline bool IsConstVariableMode(VariableMode mode) {
 }
 
 inline bool IsLexicalVariableMode(VariableMode mode) {
-  STATIC_ASSERT(static_cast<uint8_t>(VariableMode::kLet) ==
+  static_assert(static_cast<uint8_t>(VariableMode::kLet) ==
                 0);  // Implies that mode >= VariableMode::kLet.
   return mode <= VariableMode::kLastLexicalVariableMode;
 }
@@ -1575,13 +1575,13 @@ enum class ForInFeedback : uint8_t {
   kEnumCacheKeys = 0x3,
   kAny = 0x7
 };
-STATIC_ASSERT((static_cast<int>(ForInFeedback::kNone) |
+static_assert((static_cast<int>(ForInFeedback::kNone) |
                static_cast<int>(ForInFeedback::kEnumCacheKeysAndIndices)) ==
               static_cast<int>(ForInFeedback::kEnumCacheKeysAndIndices));
-STATIC_ASSERT((static_cast<int>(ForInFeedback::kEnumCacheKeysAndIndices) |
+static_assert((static_cast<int>(ForInFeedback::kEnumCacheKeysAndIndices) |
                static_cast<int>(ForInFeedback::kEnumCacheKeys)) ==
               static_cast<int>(ForInFeedback::kEnumCacheKeys));
-STATIC_ASSERT((static_cast<int>(ForInFeedback::kEnumCacheKeys) |
+static_assert((static_cast<int>(ForInFeedback::kEnumCacheKeys) |
                static_cast<int>(ForInFeedback::kAny)) ==
               static_cast<int>(ForInFeedback::kAny));
 
@@ -1691,9 +1691,9 @@ enum class TieringState : int32_t {
 // To efficiently check whether a marker is kNone or kInProgress using a single
 // mask, we expect the kNone to be 0 and kInProgress to be 1 so that we can
 // mask off the lsb for checking.
-STATIC_ASSERT(static_cast<int>(TieringState::kNone) == 0b00 &&
+static_assert(static_cast<int>(TieringState::kNone) == 0b00 &&
               static_cast<int>(TieringState::kInProgress) == 0b01);
-STATIC_ASSERT(static_cast<int>(TieringState::kLastTieringState) <= 0b111);
+static_assert(static_cast<int>(TieringState::kLastTieringState) <= 0b111);
 static constexpr uint32_t kNoneOrInProgressMask = 0b110;
 
 #define V(Name, Value)                          \
diff --git a/src/compiler/access-builder.cc b/src/compiler/access-builder.cc
index cdea6c20959..84d1d60d042 100644
--- a/src/compiler/access-builder.cc
+++ b/src/compiler/access-builder.cc
@@ -854,7 +854,7 @@ FieldAccess AccessBuilder::ForJSStringIteratorIndex() {
 // static
 FieldAccess AccessBuilder::ForArgumentsLength() {
   constexpr int offset = JSStrictArgumentsObject::kLengthOffset;
-  STATIC_ASSERT(offset == JSSloppyArgumentsObject::kLengthOffset);
+  static_assert(offset == JSSloppyArgumentsObject::kLengthOffset);
   FieldAccess access = {kTaggedBase,         offset,
                         Handle<Name>(),      MaybeHandle<Map>(),
                         Type::NonInternal(), MachineType::AnyTagged(),
@@ -1135,7 +1135,7 @@ FieldAccess AccessBuilder::ForHashTableBaseCapacity() {
 FieldAccess AccessBuilder::ForOrderedHashMapOrSetNextTable() {
   // TODO(turbofan): This will be redundant with the HashTableBase
   // methods above once the hash table unification is done.
-  STATIC_ASSERT(OrderedHashMap::NextTableOffset() ==
+  static_assert(OrderedHashMap::NextTableOffset() ==
                 OrderedHashSet::NextTableOffset());
   FieldAccess const access = {
       kTaggedBase,         OrderedHashMap::NextTableOffset(),
@@ -1149,7 +1149,7 @@ FieldAccess AccessBuilder::ForOrderedHashMapOrSetNextTable() {
 FieldAccess AccessBuilder::ForOrderedHashMapOrSetNumberOfBuckets() {
   // TODO(turbofan): This will be redundant with the HashTableBase
   // methods above once the hash table unification is done.
-  STATIC_ASSERT(OrderedHashMap::NumberOfBucketsOffset() ==
+  static_assert(OrderedHashMap::NumberOfBucketsOffset() ==
                 OrderedHashSet::NumberOfBucketsOffset());
   FieldAccess const access = {kTaggedBase,
                               OrderedHashMap::NumberOfBucketsOffset(),
@@ -1165,7 +1165,7 @@ FieldAccess AccessBuilder::ForOrderedHashMapOrSetNumberOfBuckets() {
 FieldAccess AccessBuilder::ForOrderedHashMapOrSetNumberOfDeletedElements() {
   // TODO(turbofan): This will be redundant with the HashTableBase
   // methods above once the hash table unification is done.
-  STATIC_ASSERT(OrderedHashMap::NumberOfDeletedElementsOffset() ==
+  static_assert(OrderedHashMap::NumberOfDeletedElementsOffset() ==
                 OrderedHashSet::NumberOfDeletedElementsOffset());
   FieldAccess const access = {kTaggedBase,
                               OrderedHashMap::NumberOfDeletedElementsOffset(),
@@ -1181,7 +1181,7 @@ FieldAccess AccessBuilder::ForOrderedHashMapOrSetNumberOfDeletedElements() {
 FieldAccess AccessBuilder::ForOrderedHashMapOrSetNumberOfElements() {
   // TODO(turbofan): This will be redundant with the HashTableBase
   // methods above once the hash table unification is done.
-  STATIC_ASSERT(OrderedHashMap::NumberOfElementsOffset() ==
+  static_assert(OrderedHashMap::NumberOfElementsOffset() ==
                 OrderedHashSet::NumberOfElementsOffset());
   FieldAccess const access = {kTaggedBase,
                               OrderedHashMap::NumberOfElementsOffset(),
diff --git a/src/compiler/access-info.cc b/src/compiler/access-info.cc
index 82ebda176cf..307bf44cf42 100644
--- a/src/compiler/access-info.cc
+++ b/src/compiler/access-info.cc
@@ -36,7 +36,7 @@ bool CanInlinePropertyAccess(MapRef map, AccessMode access_mode) {
   // load and the holder is a prototype. The latter ensures a 1:1
   // relationship between the map and the object (and therefore the property
   // dictionary).
-  STATIC_ASSERT(ODDBALL_TYPE == LAST_PRIMITIVE_HEAP_OBJECT_TYPE);
+  static_assert(ODDBALL_TYPE == LAST_PRIMITIVE_HEAP_OBJECT_TYPE);
   if (map.object()->IsBooleanMap()) return true;
   if (map.instance_type() < LAST_PRIMITIVE_HEAP_OBJECT_TYPE) return true;
   if (map.object()->IsJSObjectMap()) {
diff --git a/src/compiler/allocation-builder-inl.h b/src/compiler/allocation-builder-inl.h
index 18651e26e1a..0559dc139b1 100644
--- a/src/compiler/allocation-builder-inl.h
+++ b/src/compiler/allocation-builder-inl.h
@@ -33,7 +33,7 @@ void AllocationBuilder::AllocateContext(int variadic_part_length, MapRef map) {
   int size = Context::SizeFor(variadic_part_length);
   Allocate(size, AllocationType::kYoung, Type::OtherInternal());
   Store(AccessBuilder::ForMap(), map);
-  STATIC_ASSERT(static_cast<int>(Context::kLengthOffset) ==
+  static_assert(static_cast<int>(Context::kLengthOffset) ==
                 static_cast<int>(FixedArray::kLengthOffset));
   Store(AccessBuilder::ForFixedArrayLength(),
         jsgraph()->Constant(variadic_part_length));
diff --git a/src/compiler/backend/arm/code-generator-arm.cc b/src/compiler/backend/arm/code-generator-arm.cc
index 59216cbd727..4138c69042d 100644
--- a/src/compiler/backend/arm/code-generator-arm.cc
+++ b/src/compiler/backend/arm/code-generator-arm.cc
@@ -3665,7 +3665,7 @@ void CodeGenerator::FinishFrame(Frame* frame) {
 
   if (!saves_fp.is_empty()) {
     // Save callee-saved FP registers.
-    STATIC_ASSERT(DwVfpRegister::kNumRegisters == 32);
+    static_assert(DwVfpRegister::kNumRegisters == 32);
     uint32_t last = base::bits::CountLeadingZeros32(saves_fp.bits()) - 1;
     uint32_t first = base::bits::CountTrailingZeros32(saves_fp.bits());
     DCHECK_EQ((last - first + 1), saves_fp.Count());
@@ -3782,7 +3782,7 @@ void CodeGenerator::AssembleConstructFrame() {
 
   if (!saves_fp.is_empty()) {
     // Save callee-saved FP registers.
-    STATIC_ASSERT(DwVfpRegister::kNumRegisters == 32);
+    static_assert(DwVfpRegister::kNumRegisters == 32);
     __ vstm(db_w, sp, saves_fp.first(), saves_fp.last());
   }
 
@@ -3814,7 +3814,7 @@ void CodeGenerator::AssembleReturn(InstructionOperand* additional_pop_count) {
   // Restore FP registers.
   const DoubleRegList saves_fp = call_descriptor->CalleeSavedFPRegisters();
   if (!saves_fp.is_empty()) {
-    STATIC_ASSERT(DwVfpRegister::kNumRegisters == 32);
+    static_assert(DwVfpRegister::kNumRegisters == 32);
     __ vldm(ia_w, sp, saves_fp.first(), saves_fp.last());
   }
 
diff --git a/src/compiler/backend/arm64/code-generator-arm64.cc b/src/compiler/backend/arm64/code-generator-arm64.cc
index 612ac15eb48..91b8f4d747e 100644
--- a/src/compiler/backend/arm64/code-generator-arm64.cc
+++ b/src/compiler/backend/arm64/code-generator-arm64.cc
@@ -3062,11 +3062,11 @@ void CodeGenerator::AssembleConstructFrame() {
   if (frame_access_state()->has_frame()) {
     // Link the frame
     if (call_descriptor->IsJSFunctionCall()) {
-      STATIC_ASSERT(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
+      static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
       DCHECK_EQ(required_slots % 2, 1);
       __ Prologue();
       // Update required_slots count since we have just claimed one extra slot.
-      STATIC_ASSERT(TurboAssembler::kExtraSlotClaimedByPrologue == 1);
+      static_assert(TurboAssembler::kExtraSlotClaimedByPrologue == 1);
       required_slots -= TurboAssembler::kExtraSlotClaimedByPrologue;
     } else {
       __ Push<TurboAssembler::kSignLR>(lr, fp);
@@ -3332,7 +3332,7 @@ void CodeGenerator::PrepareForDeoptimizationExits(
   // Emit the jumps to deoptimization entries.
   UseScratchRegisterScope scope(tasm());
   Register scratch = scope.AcquireX();
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     if (!saw_deopt_kind[i]) continue;
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index ca91ea55543..1874e8bb238 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -3598,7 +3598,7 @@ void InstructionSelector::VisitS128Const(Node* node) {
   Arm64OperandGenerator g(this);
   static const int kUint32Immediates = 4;
   uint32_t val[kUint32Immediates];
-  STATIC_ASSERT(sizeof(val) == kSimd128Size);
+  static_assert(sizeof(val) == kSimd128Size);
   memcpy(val, S128ImmediateParameterOf(node->op()).data(), kSimd128Size);
   // If all bytes are zeros, avoid emitting code for generic constants
   bool all_zeros = !(val[0] || val[1] || val[2] || val[3]);
@@ -3661,7 +3661,7 @@ base::Optional<BicImmParam> BicImm32bitHelper(uint32_t val) {
 base::Optional<BicImmParam> BicImmConstHelper(Node* const_node, bool not_imm) {
   const int kUint32Immediates = 4;
   uint32_t val[kUint32Immediates];
-  STATIC_ASSERT(sizeof(val) == kSimd128Size);
+  static_assert(sizeof(val) == kSimd128Size);
   memcpy(val, S128ImmediateParameterOf(const_node->op()).data(), kSimd128Size);
   // If 4 uint32s are not the same, cannot emit Bic
   if (!(val[0] == val[1] && val[1] == val[2] && val[2] == val[3])) {
diff --git a/src/compiler/backend/ia32/code-generator-ia32.cc b/src/compiler/backend/ia32/code-generator-ia32.cc
index 84c1178acff..03b894b2afe 100644
--- a/src/compiler/backend/ia32/code-generator-ia32.cc
+++ b/src/compiler/backend/ia32/code-generator-ia32.cc
@@ -107,10 +107,10 @@ class IA32OperandConverter : public InstructionOperandConverter {
   }
 
   static ScaleFactor ScaleFor(AddressingMode one, AddressingMode mode) {
-    STATIC_ASSERT(0 == static_cast<int>(times_1));
-    STATIC_ASSERT(1 == static_cast<int>(times_2));
-    STATIC_ASSERT(2 == static_cast<int>(times_4));
-    STATIC_ASSERT(3 == static_cast<int>(times_8));
+    static_assert(0 == static_cast<int>(times_1));
+    static_assert(1 == static_cast<int>(times_2));
+    static_assert(2 == static_cast<int>(times_4));
+    static_assert(3 == static_cast<int>(times_8));
     int scale = static_cast<int>(mode - one);
     DCHECK(scale >= 0 && scale < 4);
     return static_cast<ScaleFactor>(scale);
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index b211728ad80..4365f6faa97 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -2859,7 +2859,7 @@ LinkageLocation ExceptionLocation() {
 constexpr InstructionCode EncodeCallDescriptorFlags(
     InstructionCode opcode, CallDescriptor::Flags flags) {
   // Note: Not all bits of `flags` are preserved.
-  STATIC_ASSERT(CallDescriptor::kFlagsBitsEncodedInInstructionCode ==
+  static_assert(CallDescriptor::kFlagsBitsEncodedInInstructionCode ==
                 MiscField::kSize);
   DCHECK(Instruction::IsCallWithDescriptorFlags(opcode));
   return opcode | MiscField::encode(flags & MiscField::kMax);
@@ -2990,7 +2990,7 @@ void InstructionSelector::VisitCall(Node* node, BasicBlock* handler) {
 #if ABI_USES_FUNCTION_DESCRIPTORS
       // Highest fp_param_count bit is used on AIX to indicate if a CFunction
       // call has function descriptor or not.
-      STATIC_ASSERT(FPParamField::kSize == kHasFunctionDescriptorBitShift + 1);
+      static_assert(FPParamField::kSize == kHasFunctionDescriptorBitShift + 1);
       if (!call_descriptor->NoFunctionDescriptor()) {
         fp_param_count |= 1 << kHasFunctionDescriptorBitShift;
       }
diff --git a/src/compiler/backend/instruction.cc b/src/compiler/backend/instruction.cc
index ba7173c9d7d..d98cdea35d3 100644
--- a/src/compiler/backend/instruction.cc
+++ b/src/compiler/backend/instruction.cc
@@ -335,7 +335,7 @@ Instruction::Instruction(InstructionCode opcode)
   parallel_moves_[1] = nullptr;
 
   // PendingOperands are required to be 8 byte aligned.
-  STATIC_ASSERT(offsetof(Instruction, operands_) % 8 == 0);
+  static_assert(offsetof(Instruction, operands_) % 8 == 0);
 }
 
 Instruction::Instruction(InstructionCode opcode, size_t output_count,
diff --git a/src/compiler/backend/instruction.h b/src/compiler/backend/instruction.h
index 140f422f588..bed43dc6363 100644
--- a/src/compiler/backend/instruction.h
+++ b/src/compiler/backend/instruction.h
@@ -364,7 +364,7 @@ class UnallocatedOperand final : public InstructionOperand {
   // The slot index is a signed value which requires us to decode it manually
   // instead of using the base::BitField utility class.
 
-  STATIC_ASSERT(KindField::kSize == 3);
+  static_assert(KindField::kSize == 3);
 
   using VirtualRegisterField = base::BitField64<uint32_t, 3, 32>;
 
@@ -408,7 +408,7 @@ class ConstantOperand : public InstructionOperand {
 
   INSTRUCTION_OPERAND_CASTS(ConstantOperand, CONSTANT)
 
-  STATIC_ASSERT(KindField::kSize == 3);
+  static_assert(KindField::kSize == 3);
   using VirtualRegisterField = base::BitField64<uint32_t, 3, 32>;
 };
 
@@ -446,7 +446,7 @@ class ImmediateOperand : public InstructionOperand {
 
   INSTRUCTION_OPERAND_CASTS(ImmediateOperand, IMMEDIATE)
 
-  STATIC_ASSERT(KindField::kSize == 3);
+  static_assert(KindField::kSize == 3);
   using TypeField = base::BitField64<ImmediateType, 3, 2>;
   using ValueField = base::BitField64<int32_t, 32, 32>;
 };
@@ -483,9 +483,9 @@ class PendingOperand : public InstructionOperand {
   // Operands are uint64_t values and so are aligned to 8 byte boundaries,
   // therefore we can shift off the bottom three zeros without losing data.
   static const uint64_t kPointerShift = 3;
-  STATIC_ASSERT(alignof(InstructionOperand) >= (1 << kPointerShift));
+  static_assert(alignof(InstructionOperand) >= (1 << kPointerShift));
 
-  STATIC_ASSERT(KindField::kSize == 3);
+  static_assert(KindField::kSize == 3);
   using NextOperandField = base::BitField64<uint64_t, 3, 61>;
 };
 
@@ -590,7 +590,7 @@ class LocationOperand : public InstructionOperand {
     return *static_cast<const LocationOperand*>(&op);
   }
 
-  STATIC_ASSERT(KindField::kSize == 3);
+  static_assert(KindField::kSize == 3);
   using LocationKindField = base::BitField64<LocationKind, 3, 2>;
   using RepresentationField = base::BitField64<MachineRepresentation, 5, 8>;
   using IndexField = base::BitField64<int32_t, 35, 29>;
@@ -984,7 +984,7 @@ class V8_EXPORT_PRIVATE Instruction final {
   }
   bool HasCallDescriptorFlag(CallDescriptor::Flag flag) const {
     DCHECK(IsCallWithDescriptorFlags());
-    STATIC_ASSERT(CallDescriptor::kFlagsBitsEncodedInInstructionCode == 10);
+    static_assert(CallDescriptor::kFlagsBitsEncodedInInstructionCode == 10);
 #ifdef DEBUG
     static constexpr int kInstructionCodeFlagsMask =
         ((1 << CallDescriptor::kFlagsBitsEncodedInInstructionCode) - 1);
diff --git a/src/compiler/backend/spill-placer.cc b/src/compiler/backend/spill-placer.cc
index 55c2b4f7d13..01d130ff596 100644
--- a/src/compiler/backend/spill-placer.cc
+++ b/src/compiler/backend/spill-placer.cc
@@ -174,7 +174,7 @@ class SpillPlacer::Entry {
 
   template <State state>
   uint64_t GetValuesInState() const {
-    STATIC_ASSERT(state < 8);
+    static_assert(state < 8);
     return ((state & 1) ? first_bit_ : ~first_bit_) &
            ((state & 2) ? second_bit_ : ~second_bit_) &
            ((state & 4) ? third_bit_ : ~third_bit_);
@@ -182,7 +182,7 @@ class SpillPlacer::Entry {
 
   template <State state>
   void UpdateValuesToState(uint64_t mask) {
-    STATIC_ASSERT(state < 8);
+    static_assert(state < 8);
     first_bit_ =
         Entry::UpdateBitDataWithMask<(state & 1) != 0>(first_bit_, mask);
     second_bit_ =
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index 966df3edd89..2ea91bf5a8d 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -80,10 +80,10 @@ class X64OperandConverter : public InstructionOperandConverter {
   }
 
   static ScaleFactor ScaleFor(AddressingMode one, AddressingMode mode) {
-    STATIC_ASSERT(0 == static_cast<int>(times_1));
-    STATIC_ASSERT(1 == static_cast<int>(times_2));
-    STATIC_ASSERT(2 == static_cast<int>(times_4));
-    STATIC_ASSERT(3 == static_cast<int>(times_8));
+    static_assert(0 == static_cast<int>(times_1));
+    static_assert(1 == static_cast<int>(times_2));
+    static_assert(2 == static_cast<int>(times_4));
+    static_assert(3 == static_cast<int>(times_8));
     int scale = static_cast<int>(mode - one);
     DCHECK(scale >= 0 && scale < 4);
     return static_cast<ScaleFactor>(scale);
diff --git a/src/compiler/bytecode-graph-builder.cc b/src/compiler/bytecode-graph-builder.cc
index 7fe1b626b1e..da94cdab0fc 100644
--- a/src/compiler/bytecode-graph-builder.cc
+++ b/src/compiler/bytecode-graph-builder.cc
@@ -2019,9 +2019,9 @@ void BytecodeGraphBuilder::VisitGetKeyedProperty() {
     node = lowering.value();
   } else {
     DCHECK(!lowering.Changed());
-    STATIC_ASSERT(JSLoadPropertyNode::ObjectIndex() == 0);
-    STATIC_ASSERT(JSLoadPropertyNode::KeyIndex() == 1);
-    STATIC_ASSERT(JSLoadPropertyNode::FeedbackVectorIndex() == 2);
+    static_assert(JSLoadPropertyNode::ObjectIndex() == 0);
+    static_assert(JSLoadPropertyNode::KeyIndex() == 1);
+    static_assert(JSLoadPropertyNode::FeedbackVectorIndex() == 2);
     DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
     node = NewNode(op, object, key, feedback_vector_node());
   }
@@ -2095,10 +2095,10 @@ void BytecodeGraphBuilder::VisitSetKeyedProperty() {
     node = lowering.value();
   } else {
     DCHECK(!lowering.Changed());
-    STATIC_ASSERT(JSSetKeyedPropertyNode::ObjectIndex() == 0);
-    STATIC_ASSERT(JSSetKeyedPropertyNode::KeyIndex() == 1);
-    STATIC_ASSERT(JSSetKeyedPropertyNode::ValueIndex() == 2);
-    STATIC_ASSERT(JSSetKeyedPropertyNode::FeedbackVectorIndex() == 3);
+    static_assert(JSSetKeyedPropertyNode::ObjectIndex() == 0);
+    static_assert(JSSetKeyedPropertyNode::KeyIndex() == 1);
+    static_assert(JSSetKeyedPropertyNode::ValueIndex() == 2);
+    static_assert(JSSetKeyedPropertyNode::FeedbackVectorIndex() == 3);
     DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
     node = NewNode(op, object, key, value, feedback_vector_node());
   }
@@ -2130,10 +2130,10 @@ void BytecodeGraphBuilder::VisitDefineKeyedOwnProperty() {
     node = lowering.value();
   } else {
     DCHECK(!lowering.Changed());
-    STATIC_ASSERT(JSDefineKeyedOwnPropertyNode::ObjectIndex() == 0);
-    STATIC_ASSERT(JSDefineKeyedOwnPropertyNode::KeyIndex() == 1);
-    STATIC_ASSERT(JSDefineKeyedOwnPropertyNode::ValueIndex() == 2);
-    STATIC_ASSERT(JSDefineKeyedOwnPropertyNode::FeedbackVectorIndex() == 3);
+    static_assert(JSDefineKeyedOwnPropertyNode::ObjectIndex() == 0);
+    static_assert(JSDefineKeyedOwnPropertyNode::KeyIndex() == 1);
+    static_assert(JSDefineKeyedOwnPropertyNode::ValueIndex() == 2);
+    static_assert(JSDefineKeyedOwnPropertyNode::FeedbackVectorIndex() == 3);
     DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
     node = NewNode(op, object, key, value, feedback_vector_node());
   }
@@ -2257,7 +2257,7 @@ void BytecodeGraphBuilder::VisitCreateRegExpLiteral() {
   int const slot_id = bytecode_iterator().GetIndexOperand(1);
   FeedbackSource pair = CreateFeedbackSource(slot_id);
   int literal_flags = bytecode_iterator().GetFlagOperand(2);
-  STATIC_ASSERT(JSCreateLiteralRegExpNode::FeedbackVectorIndex() == 0);
+  static_assert(JSCreateLiteralRegExpNode::FeedbackVectorIndex() == 0);
   const Operator* op =
       javascript()->CreateLiteralRegExp(constant_pattern, pair, literal_flags);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
@@ -2280,7 +2280,7 @@ void BytecodeGraphBuilder::VisitCreateArrayLiteral() {
   literal_flags |= ArrayLiteral::kDisableMementos;
   int number_of_elements =
       array_boilerplate_description.constants_elements_length();
-  STATIC_ASSERT(JSCreateLiteralArrayNode::FeedbackVectorIndex() == 0);
+  static_assert(JSCreateLiteralArrayNode::FeedbackVectorIndex() == 0);
   const Operator* op = javascript()->CreateLiteralArray(
       array_boilerplate_description, pair, literal_flags, number_of_elements);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
@@ -2312,7 +2312,7 @@ void BytecodeGraphBuilder::VisitCreateObjectLiteral() {
   int literal_flags =
       interpreter::CreateObjectLiteralFlags::FlagsBits::decode(bytecode_flags);
   int number_of_properties = constant_properties.size();
-  STATIC_ASSERT(JSCreateLiteralObjectNode::FeedbackVectorIndex() == 0);
+  static_assert(JSCreateLiteralObjectNode::FeedbackVectorIndex() == 0);
   const Operator* op = javascript()->CreateLiteralObject(
       constant_properties, pair, literal_flags, number_of_properties);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
@@ -2333,8 +2333,8 @@ void BytecodeGraphBuilder::VisitCloneObject() {
   int slot = bytecode_iterator().GetIndexOperand(2);
   const Operator* op =
       javascript()->CloneObject(CreateFeedbackSource(slot), flags);
-  STATIC_ASSERT(JSCloneObjectNode::SourceIndex() == 0);
-  STATIC_ASSERT(JSCloneObjectNode::FeedbackVectorIndex() == 1);
+  static_assert(JSCloneObjectNode::SourceIndex() == 0);
+  static_assert(JSCloneObjectNode::FeedbackVectorIndex() == 1);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
   Node* value = NewNode(op, source, feedback_vector_node());
   environment()->BindAccumulator(value, Environment::kAttachFrameState);
@@ -2345,7 +2345,7 @@ void BytecodeGraphBuilder::VisitGetTemplateObject() {
       CreateFeedbackSource(bytecode_iterator().GetIndexOperand(1));
   TemplateObjectDescriptionRef description =
       MakeRefForConstantForIndexOperand<TemplateObjectDescription>(0);
-  STATIC_ASSERT(JSGetTemplateObjectNode::FeedbackVectorIndex() == 0);
+  static_assert(JSGetTemplateObjectNode::FeedbackVectorIndex() == 0);
   const Operator* op =
       javascript()->GetTemplateObject(description, shared_info(), source);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
@@ -2360,10 +2360,10 @@ Node* const* BytecodeGraphBuilder::GetCallArgumentsFromRegisters(
   Node** all = local_zone()->NewArray<Node*>(static_cast<size_t>(arity));
   int cursor = 0;
 
-  STATIC_ASSERT(JSCallNode::TargetIndex() == 0);
-  STATIC_ASSERT(JSCallNode::ReceiverIndex() == 1);
-  STATIC_ASSERT(JSCallNode::FirstArgumentIndex() == 2);
-  STATIC_ASSERT(JSCallNode::kFeedbackVectorIsLastInput);
+  static_assert(JSCallNode::TargetIndex() == 0);
+  static_assert(JSCallNode::ReceiverIndex() == 1);
+  static_assert(JSCallNode::FirstArgumentIndex() == 2);
+  static_assert(JSCallNode::kFeedbackVectorIsLastInput);
 
   all[cursor++] = callee;
   all[cursor++] = receiver;
@@ -2653,10 +2653,10 @@ Node* const* BytecodeGraphBuilder::GetConstructArgumentsFromRegister(
   Node** all = local_zone()->NewArray<Node*>(static_cast<size_t>(arity));
   int cursor = 0;
 
-  STATIC_ASSERT(JSConstructNode::TargetIndex() == 0);
-  STATIC_ASSERT(JSConstructNode::NewTargetIndex() == 1);
-  STATIC_ASSERT(JSConstructNode::FirstArgumentIndex() == 2);
-  STATIC_ASSERT(JSConstructNode::kFeedbackVectorIsLastInput);
+  static_assert(JSConstructNode::TargetIndex() == 0);
+  static_assert(JSConstructNode::NewTargetIndex() == 1);
+  static_assert(JSConstructNode::FirstArgumentIndex() == 2);
+  static_assert(JSConstructNode::kFeedbackVectorIsLastInput);
 
   all[cursor++] = target;
   all[cursor++] = new_target;
@@ -3267,9 +3267,9 @@ void BytecodeGraphBuilder::VisitTestIn() {
       environment()->LookupRegister(bytecode_iterator().GetRegisterOperand(0));
   FeedbackSource feedback =
       CreateFeedbackSource(bytecode_iterator().GetIndexOperand(1));
-  STATIC_ASSERT(JSHasPropertyNode::ObjectIndex() == 0);
-  STATIC_ASSERT(JSHasPropertyNode::KeyIndex() == 1);
-  STATIC_ASSERT(JSHasPropertyNode::FeedbackVectorIndex() == 2);
+  static_assert(JSHasPropertyNode::ObjectIndex() == 0);
+  static_assert(JSHasPropertyNode::KeyIndex() == 1);
+  static_assert(JSHasPropertyNode::FeedbackVectorIndex() == 2);
   const Operator* op = javascript()->HasProperty(feedback);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
   Node* node = NewNode(op, object, key, feedback_vector_node());
@@ -3648,8 +3648,8 @@ void BytecodeGraphBuilder::VisitGetIterator() {
   if (lowering.IsExit()) return;
 
   DCHECK(!lowering.Changed());
-  STATIC_ASSERT(JSGetIteratorNode::ReceiverIndex() == 0);
-  STATIC_ASSERT(JSGetIteratorNode::FeedbackVectorIndex() == 1);
+  static_assert(JSGetIteratorNode::ReceiverIndex() == 0);
+  static_assert(JSGetIteratorNode::FeedbackVectorIndex() == 1);
   DCHECK(IrOpcode::IsFeedbackCollectingOpcode(op->opcode()));
   Node* iterator = NewNode(op, receiver, feedback_vector_node());
   environment()->BindAccumulator(iterator, Environment::kAttachFrameState);
diff --git a/src/compiler/code-assembler.h b/src/compiler/code-assembler.h
index 768f9e3295c..f4ab69e15b9 100644
--- a/src/compiler/code-assembler.h
+++ b/src/compiler/code-assembler.h
@@ -543,7 +543,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   template <typename E,
             typename = typename std::enable_if<std::is_enum<E>::value>::type>
   TNode<Smi> SmiConstant(E value) {
-    STATIC_ASSERT(sizeof(E) <= sizeof(int));
+    static_assert(sizeof(E) <= sizeof(int));
     return SmiConstant(static_cast<int>(value));
   }
   TNode<HeapObject> UntypedHeapConstant(Handle<HeapObject> object);
diff --git a/src/compiler/common-operator.h b/src/compiler/common-operator.h
index a9754f5b095..8faa086029a 100644
--- a/src/compiler/common-operator.h
+++ b/src/compiler/common-operator.h
@@ -665,7 +665,7 @@ class StartNode final : public CommonNodeWrapperBase {
     constexpr int kNewTarget = 1;
     constexpr int kArgCount = 1;
     constexpr int kContext = 1;
-    STATIC_ASSERT(kClosure + kNewTarget + kArgCount + kContext ==
+    static_assert(kClosure + kNewTarget + kArgCount + kContext ==
                   kExtraOutputCount);
     // Checking related linkage methods here since they rely on Start node
     // layout.
diff --git a/src/compiler/compilation-dependencies.cc b/src/compiler/compilation-dependencies.cc
index 6f5514289b7..76209c42595 100644
--- a/src/compiler/compilation-dependencies.cc
+++ b/src/compiler/compilation-dependencies.cc
@@ -508,7 +508,7 @@ class OwnConstantDictionaryPropertyDependency final
         index_(index),
         value_(value) {
     // We depend on map() being cached.
-    STATIC_ASSERT(ref_traits<JSObject>::ref_serialization_kind !=
+    static_assert(ref_traits<JSObject>::ref_serialization_kind !=
                   RefSerializationKind::kNeverSerialized);
   }
 
diff --git a/src/compiler/effect-control-linearizer.cc b/src/compiler/effect-control-linearizer.cc
index d267ae3518e..2e623f7cb6b 100644
--- a/src/compiler/effect-control-linearizer.cc
+++ b/src/compiler/effect-control-linearizer.cc
@@ -1989,7 +1989,7 @@ Node* EffectControlLinearizer::LowerCheckReceiver(Node* node,
   Node* value_instance_type =
       __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
 
-  STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+  static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
   Node* check = __ Uint32LessThanOrEqual(
       __ Uint32Constant(FIRST_JS_RECEIVER_TYPE), value_instance_type);
   __ DeoptimizeIfNot(DeoptimizeReason::kNotAJavaScriptObject, FeedbackSource(),
@@ -2006,8 +2006,8 @@ Node* EffectControlLinearizer::LowerCheckReceiverOrNullOrUndefined(
       __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
 
   // Rule out all primitives except oddballs (true, false, undefined, null).
-  STATIC_ASSERT(LAST_PRIMITIVE_HEAP_OBJECT_TYPE == ODDBALL_TYPE);
-  STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+  static_assert(LAST_PRIMITIVE_HEAP_OBJECT_TYPE == ODDBALL_TYPE);
+  static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
   Node* check0 = __ Uint32LessThanOrEqual(__ Uint32Constant(ODDBALL_TYPE),
                                           value_instance_type);
   __ DeoptimizeIfNot(DeoptimizeReason::kNotAJavaScriptObjectOrNullOrUndefined,
@@ -3388,7 +3388,7 @@ Node* EffectControlLinearizer::LowerObjectIsNonCallable(Node* node) {
   Node* value_map = __ LoadField(AccessBuilder::ForMap(), value);
   Node* value_instance_type =
       __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
-  STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+  static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
   Node* check1 = __ Uint32LessThanOrEqual(
       __ Uint32Constant(FIRST_JS_RECEIVER_TYPE), value_instance_type);
   __ GotoIfNot(check1, &if_primitive);
@@ -3433,7 +3433,7 @@ Node* EffectControlLinearizer::LowerObjectIsReceiver(Node* node) {
 
   __ GotoIf(ObjectIsSmi(value), &if_smi);
 
-  STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+  static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
   Node* value_map = __ LoadField(AccessBuilder::ForMap(), value);
   Node* value_instance_type =
       __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
@@ -3721,8 +3721,8 @@ Node* EffectControlLinearizer::LowerNewConsString(Node* node) {
   auto if_onebyte = __ MakeLabel();
   auto if_twobyte = __ MakeLabel();
   auto done = __ MakeLabel(MachineRepresentation::kTaggedPointer);
-  STATIC_ASSERT(kOneByteStringTag != 0);
-  STATIC_ASSERT(kTwoByteStringTag == 0);
+  static_assert(kOneByteStringTag != 0);
+  static_assert(kTwoByteStringTag == 0);
   Node* instance_type = __ Word32And(first_instance_type, second_instance_type);
   Node* encoding =
       __ Word32And(instance_type, __ Int32Constant(kStringEncodingMask));
@@ -6110,7 +6110,7 @@ Node* EffectControlLinearizer::LowerConvertReceiver(Node* node) {
 
       // Check if {value} is already a JSReceiver.
       __ GotoIf(ObjectIsSmi(value), &convert_to_object);
-      STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+      static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
       Node* value_map = __ LoadField(AccessBuilder::ForMap(), value);
       Node* value_instance_type =
           __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
@@ -6143,7 +6143,7 @@ Node* EffectControlLinearizer::LowerConvertReceiver(Node* node) {
 
       // Check if {value} is already a JSReceiver, or null/undefined.
       __ GotoIf(ObjectIsSmi(value), &convert_to_object);
-      STATIC_ASSERT(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
+      static_assert(LAST_TYPE == LAST_JS_RECEIVER_TYPE);
       Node* value_map = __ LoadField(AccessBuilder::ForMap(), value);
       Node* value_instance_type =
           __ LoadField(AccessBuilder::ForMapInstanceType(), value_map);
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index 638d83ff1f2..c6f865fc397 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -1350,7 +1350,7 @@ base::Optional<ObjectRef> FixedArrayRef::TryGet(int i) const {
 }
 
 Float64 FixedDoubleArrayRef::GetFromImmutableFixedDoubleArray(int i) const {
-  STATIC_ASSERT(ref_traits<FixedDoubleArray>::ref_serialization_kind ==
+  static_assert(ref_traits<FixedDoubleArray>::ref_serialization_kind ==
                 RefSerializationKind::kNeverSerialized);
   CHECK(data_->should_access_heap());
   return Float64::FromBits(object()->get_representation(i));
@@ -1640,7 +1640,7 @@ void* JSTypedArrayRef::data_ptr() const {
   // is_on_heap release/acquire semantics (external_pointer store happens-before
   // base_pointer store, and this external_pointer load happens-after
   // base_pointer load).
-  STATIC_ASSERT(JSTypedArray::kOffHeapDataPtrEqualsExternalPointer);
+  static_assert(JSTypedArray::kOffHeapDataPtrEqualsExternalPointer);
   return object()->DataPtr();
 }
 
@@ -2106,7 +2106,7 @@ Handle<T> TinyRef<T>::object() const {
 #define V(Name)                                  \
   template class TinyRef<Name>;                  \
   /* TinyRef should contain only one pointer. */ \
-  STATIC_ASSERT(sizeof(TinyRef<Name>) == kSystemPointerSize);
+  static_assert(sizeof(TinyRef<Name>) == kSystemPointerSize);
 HEAP_BROKER_OBJECT_LIST(V)
 #undef V
 
diff --git a/src/compiler/js-call-reducer.cc b/src/compiler/js-call-reducer.cc
index 59181a10591..33d917c1f23 100644
--- a/src/compiler/js-call-reducer.cc
+++ b/src/compiler/js-call-reducer.cc
@@ -1220,7 +1220,7 @@ TNode<Boolean> JSCallReducerAssembler::ReduceStringPrototypeStartsWith(
 
   GotoIf(search_string_too_long, &out, BranchHint::kFalse, FalseConstant());
 
-  STATIC_ASSERT(String::kMaxLength <= kSmiMaxValue);
+  static_assert(String::kMaxLength <= kSmiMaxValue);
 
   for (int i = 0; i < search_string_length; i++) {
     TNode<Number> k = NumberConstant(i);
@@ -1263,7 +1263,7 @@ TNode<Boolean> JSCallReducerAssembler::ReduceStringPrototypeStartsWith() {
 
   GotoIf(search_string_too_long, &out, BranchHint::kFalse, FalseConstant());
 
-  STATIC_ASSERT(String::kMaxLength <= kSmiMaxValue);
+  static_assert(String::kMaxLength <= kSmiMaxValue);
 
   ForZeroUntil(search_string_length).Do([&](TNode<Number> k) {
     TNode<Number> receiver_string_position = TNode<Number>::UncheckedCast(
@@ -3056,7 +3056,7 @@ Reduction JSCallReducer::ReduceReflectApply(Node* node) {
   CallParameters const& p = n.Parameters();
   int arity = p.arity_without_implicit_args();
   // Massage value inputs appropriately.
-  STATIC_ASSERT(n.ReceiverIndex() > n.TargetIndex());
+  static_assert(n.ReceiverIndex() > n.TargetIndex());
   node->RemoveInput(n.ReceiverIndex());
   node->RemoveInput(n.TargetIndex());
   while (arity < 3) {
@@ -3082,13 +3082,13 @@ Reduction JSCallReducer::ReduceReflectConstruct(Node* node) {
   Node* arg_argument_list = n.ArgumentOrUndefined(1, jsgraph());
   Node* arg_new_target = n.ArgumentOr(2, arg_target);
 
-  STATIC_ASSERT(n.ReceiverIndex() > n.TargetIndex());
+  static_assert(n.ReceiverIndex() > n.TargetIndex());
   node->RemoveInput(n.ReceiverIndex());
   node->RemoveInput(n.TargetIndex());
 
   // TODO(jgruber): This pattern essentially ensures that we have the correct
   // number of inputs for a given argument count. Wrap it in a helper function.
-  STATIC_ASSERT(JSConstructNode::FirstArgumentIndex() == 2);
+  static_assert(JSConstructNode::FirstArgumentIndex() == 2);
   while (arity < 3) {
     node->InsertInput(graph()->zone(), arity++, jsgraph()->UndefinedConstant());
   }
@@ -3096,9 +3096,9 @@ Reduction JSCallReducer::ReduceReflectConstruct(Node* node) {
     node->RemoveInput(arity);
   }
 
-  STATIC_ASSERT(JSConstructNode::TargetIndex() == 0);
-  STATIC_ASSERT(JSConstructNode::NewTargetIndex() == 1);
-  STATIC_ASSERT(JSConstructNode::kFeedbackVectorIsLastInput);
+  static_assert(JSConstructNode::TargetIndex() == 0);
+  static_assert(JSConstructNode::NewTargetIndex() == 1);
+  static_assert(JSConstructNode::kFeedbackVectorIsLastInput);
   node->ReplaceInput(JSConstructNode::TargetIndex(), arg_target);
   node->ReplaceInput(JSConstructNode::NewTargetIndex(), arg_new_target);
   node->ReplaceInput(JSConstructNode::ArgumentIndex(0), arg_argument_list);
@@ -4029,10 +4029,10 @@ JSCallReducer::ReduceCallOrConstructWithArrayLikeOrSpreadOfCreateArguments(
         FieldAccess const& access = FieldAccessOf(user->op());
         if (access.offset == JSArray::kLengthOffset) {
           // Ignore uses for arguments#length.
-          STATIC_ASSERT(
+          static_assert(
               static_cast<int>(JSArray::kLengthOffset) ==
               static_cast<int>(JSStrictArgumentsObject::kLengthOffset));
-          STATIC_ASSERT(
+          static_assert(
               static_cast<int>(JSArray::kLengthOffset) ==
               static_cast<int>(JSSloppyArgumentsObject::kLengthOffset));
           continue;
@@ -5046,7 +5046,7 @@ Reduction JSCallReducer::ReduceJSConstruct(Node* node) {
 
       // Turn the {node} into a {JSCreateArray} call.
       NodeProperties::ReplaceEffectInput(node, effect);
-      STATIC_ASSERT(JSConstructNode::NewTargetIndex() == 1);
+      static_assert(JSConstructNode::NewTargetIndex() == 1);
       node->ReplaceInput(n.NewTargetIndex(), array_function);
       node->RemoveInput(n.FeedbackVectorIndex());
       NodeProperties::ChangeOp(
@@ -5113,7 +5113,7 @@ Reduction JSCallReducer::ReduceJSConstruct(Node* node) {
         case Builtin::kArrayConstructor: {
           // TODO(bmeurer): Deal with Array subclasses here.
           // Turn the {node} into a {JSCreateArray} call.
-          STATIC_ASSERT(JSConstructNode::NewTargetIndex() == 1);
+          static_assert(JSConstructNode::NewTargetIndex() == 1);
           node->ReplaceInput(n.NewTargetIndex(), new_target);
           node->RemoveInput(n.FeedbackVectorIndex());
           NodeProperties::ChangeOp(
@@ -5951,7 +5951,7 @@ Reduction JSCallReducer::ReduceArrayPrototypeShift(Node* node) {
           // And we need to use index when using NumberLessThan to check
           // terminate and updating index, otherwise which will break inducing
           // variables in LoopVariableOptimizer.
-          STATIC_ASSERT(JSArray::kMaxCopyElements < kSmiMaxValue);
+          static_assert(JSArray::kMaxCopyElements < kSmiMaxValue);
           Node* index_retyped = effect2 =
               graph()->NewNode(common()->TypeGuard(Type::UnsignedSmall()),
                                index, effect2, control2);
@@ -7648,7 +7648,7 @@ Reduction JSCallReducer::ReduceCollectionIteratorPrototypeNext(
       Node* etrue0 = effect;
       {
         // Load the key of the entry.
-        STATIC_ASSERT(OrderedHashMap::HashTableStartIndex() ==
+        static_assert(OrderedHashMap::HashTableStartIndex() ==
                       OrderedHashSet::HashTableStartIndex());
         Node* entry_start_position = graph()->NewNode(
             simplified()->NumberAdd(),
diff --git a/src/compiler/js-create-lowering.cc b/src/compiler/js-create-lowering.cc
index 68d96aed8a1..1b4755e2db2 100644
--- a/src/compiler/js-create-lowering.cc
+++ b/src/compiler/js-create-lowering.cc
@@ -180,7 +180,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
                 : native_context().sloppy_arguments_map());
         // Actually allocate and initialize the arguments object.
         AllocationBuilder a(jsgraph(), effect, control);
-        STATIC_ASSERT(JSSloppyArgumentsObject::kSize == 5 * kTaggedSize);
+        static_assert(JSSloppyArgumentsObject::kSize == 5 * kTaggedSize);
         a.Allocate(JSSloppyArgumentsObject::kSize);
         a.Store(AccessBuilder::ForMap(), arguments_map);
         a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -207,7 +207,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
             jsgraph()->Constant(native_context().strict_arguments_map());
         // Actually allocate and initialize the arguments object.
         AllocationBuilder a(jsgraph(), effect, control);
-        STATIC_ASSERT(JSStrictArgumentsObject::kSize == 4 * kTaggedSize);
+        static_assert(JSStrictArgumentsObject::kSize == 4 * kTaggedSize);
         a.Allocate(JSStrictArgumentsObject::kSize);
         a.Store(AccessBuilder::ForMap(), arguments_map);
         a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -235,7 +235,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
             native_context().js_array_packed_elements_map());
         // Actually allocate and initialize the jsarray.
         AllocationBuilder a(jsgraph(), effect, control);
-        STATIC_ASSERT(JSArray::kHeaderSize == 4 * kTaggedSize);
+        static_assert(JSArray::kHeaderSize == 4 * kTaggedSize);
         a.Allocate(JSArray::kHeaderSize);
         a.Store(AccessBuilder::ForMap(), jsarray_map);
         a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -283,7 +283,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
                                 : native_context().sloppy_arguments_map());
       // Actually allocate and initialize the arguments object.
       AllocationBuilder a(jsgraph(), effect, control);
-      STATIC_ASSERT(JSSloppyArgumentsObject::kSize == 5 * kTaggedSize);
+      static_assert(JSSloppyArgumentsObject::kSize == 5 * kTaggedSize);
       a.Allocate(JSSloppyArgumentsObject::kSize);
       a.Store(AccessBuilder::ForMap(), arguments_map);
       a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -320,7 +320,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
           jsgraph()->Constant(native_context().strict_arguments_map());
       // Actually allocate and initialize the arguments object.
       AllocationBuilder a(jsgraph(), effect, control);
-      STATIC_ASSERT(JSStrictArgumentsObject::kSize == 4 * kTaggedSize);
+      static_assert(JSStrictArgumentsObject::kSize == 4 * kTaggedSize);
       a.Allocate(JSStrictArgumentsObject::kSize);
       a.Store(AccessBuilder::ForMap(), arguments_map);
       a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -362,7 +362,7 @@ Reduction JSCreateLowering::ReduceJSCreateArguments(Node* node) {
       // -1 to minus receiver
       int argument_count = args_state_info.parameter_count() - 1;
       int length = std::max(0, argument_count - start_index);
-      STATIC_ASSERT(JSArray::kHeaderSize == 4 * kTaggedSize);
+      static_assert(JSArray::kHeaderSize == 4 * kTaggedSize);
       a.Allocate(JSArray::kHeaderSize);
       a.Store(AccessBuilder::ForMap(), jsarray_map);
       a.Store(AccessBuilder::ForJSObjectPropertiesOrHashKnownPointer(),
@@ -959,7 +959,7 @@ Reduction JSCreateLowering::ReduceJSCreateClosure(Node* node) {
   AllocationType allocation = AllocationType::kYoung;
 
   // Emit code to allocate the JSFunction instance.
-  STATIC_ASSERT(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
+  static_assert(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
   AllocationBuilder a(jsgraph(), effect, control);
   a.Allocate(function_map.instance_size(), allocation,
              Type::CallableFunction());
@@ -972,11 +972,11 @@ Reduction JSCreateLowering::ReduceJSCreateClosure(Node* node) {
   a.Store(AccessBuilder::ForJSFunctionContext(), context);
   a.Store(AccessBuilder::ForJSFunctionFeedbackCell(), feedback_cell);
   a.Store(AccessBuilder::ForJSFunctionCode(), code);
-  STATIC_ASSERT(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
+  static_assert(JSFunction::kSizeWithoutPrototype == 7 * kTaggedSize);
   if (function_map.has_prototype_slot()) {
     a.Store(AccessBuilder::ForJSFunctionPrototypeOrInitialMap(),
             jsgraph()->TheHoleConstant());
-    STATIC_ASSERT(JSFunction::kSizeWithPrototype == 8 * kTaggedSize);
+    static_assert(JSFunction::kSizeWithPrototype == 8 * kTaggedSize);
   }
   for (int i = 0; i < function_map.GetInObjectProperties(); i++) {
     a.Store(AccessBuilder::ForJSObjectInObjectProperty(function_map, i),
@@ -1006,7 +1006,7 @@ Reduction JSCreateLowering::ReduceJSCreateIterResultObject(Node* node) {
           jsgraph()->EmptyFixedArrayConstant());
   a.Store(AccessBuilder::ForJSIteratorResultValue(), value);
   a.Store(AccessBuilder::ForJSIteratorResultDone(), done);
-  STATIC_ASSERT(JSIteratorResult::kSize == 5 * kTaggedSize);
+  static_assert(JSIteratorResult::kSize == 5 * kTaggedSize);
   a.FinishAndChange(node);
   return Changed(node);
 }
@@ -1029,7 +1029,7 @@ Reduction JSCreateLowering::ReduceJSCreateStringIterator(Node* node) {
           jsgraph()->EmptyFixedArrayConstant());
   a.Store(AccessBuilder::ForJSStringIteratorString(), string);
   a.Store(AccessBuilder::ForJSStringIteratorIndex(), jsgraph()->SmiConstant(0));
-  STATIC_ASSERT(JSIteratorResult::kSize == 5 * kTaggedSize);
+  static_assert(JSIteratorResult::kSize == 5 * kTaggedSize);
   a.FinishAndChange(node);
   return Changed(node);
 }
@@ -1059,7 +1059,7 @@ Reduction JSCreateLowering::ReduceJSCreateKeyValueArray(Node* node) {
           jsgraph()->EmptyFixedArrayConstant());
   a.Store(AccessBuilder::ForJSObjectElements(), elements);
   a.Store(AccessBuilder::ForJSArrayLength(PACKED_ELEMENTS), length);
-  STATIC_ASSERT(JSArray::kHeaderSize == 4 * kTaggedSize);
+  static_assert(JSArray::kHeaderSize == 4 * kTaggedSize);
   a.FinishAndChange(node);
   return Changed(node);
 }
@@ -1080,10 +1080,10 @@ Reduction JSCreateLowering::ReduceJSCreatePromise(Node* node) {
           jsgraph()->EmptyFixedArrayConstant());
   a.Store(AccessBuilder::ForJSObjectOffset(JSPromise::kReactionsOrResultOffset),
           jsgraph()->ZeroConstant());
-  STATIC_ASSERT(v8::Promise::kPending == 0);
+  static_assert(v8::Promise::kPending == 0);
   a.Store(AccessBuilder::ForJSObjectOffset(JSPromise::kFlagsOffset),
           jsgraph()->ZeroConstant());
-  STATIC_ASSERT(JSPromise::kHeaderSize == 5 * kTaggedSize);
+  static_assert(JSPromise::kHeaderSize == 5 * kTaggedSize);
   for (int offset = JSPromise::kHeaderSize;
        offset < JSPromise::kSizeWithEmbedderFields; offset += kTaggedSize) {
     a.Store(AccessBuilder::ForJSObjectOffset(offset),
@@ -1223,7 +1223,7 @@ Reduction JSCreateLowering::ReduceJSCreateFunctionContext(Node* node) {
     Node* control = NodeProperties::GetControlInput(node);
     Node* context = NodeProperties::GetContextInput(node);
     AllocationBuilder a(jsgraph(), effect, control);
-    STATIC_ASSERT(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
+    static_assert(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
     int context_length = slot_count + Context::MIN_CONTEXT_SLOTS;
     switch (scope_type) {
       case EVAL_SCOPE:
@@ -1259,7 +1259,7 @@ Reduction JSCreateLowering::ReduceJSCreateWithContext(Node* node) {
   Node* context = NodeProperties::GetContextInput(node);
 
   AllocationBuilder a(jsgraph(), effect, control);
-  STATIC_ASSERT(Context::MIN_CONTEXT_EXTENDED_SLOTS ==
+  static_assert(Context::MIN_CONTEXT_EXTENDED_SLOTS ==
                 3);  // Ensure fully covered.
   a.AllocateContext(Context::MIN_CONTEXT_EXTENDED_SLOTS,
                     native_context().with_context_map());
@@ -1280,7 +1280,7 @@ Reduction JSCreateLowering::ReduceJSCreateCatchContext(Node* node) {
   Node* context = NodeProperties::GetContextInput(node);
 
   AllocationBuilder a(jsgraph(), effect, control);
-  STATIC_ASSERT(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
+  static_assert(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
   a.AllocateContext(Context::MIN_CONTEXT_SLOTS + 1,
                     native_context().catch_context_map());
   a.Store(AccessBuilder::ForContextSlot(Context::SCOPE_INFO_INDEX), scope_info);
@@ -1305,7 +1305,7 @@ Reduction JSCreateLowering::ReduceJSCreateBlockContext(Node* node) {
     Node* context = NodeProperties::GetContextInput(node);
 
     AllocationBuilder a(jsgraph(), effect, control);
-    STATIC_ASSERT(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
+    static_assert(Context::MIN_CONTEXT_SLOTS == 2);  // Ensure fully covered.
     a.AllocateContext(context_length, native_context().block_context_map());
     a.Store(AccessBuilder::ForContextSlot(Context::SCOPE_INFO_INDEX),
             scope_info);
@@ -1387,7 +1387,7 @@ Reduction JSCreateLowering::ReduceJSCreateObject(Node* node) {
             jsgraph()->SmiConstant(PropertyArray::kNoHashSentinel));
     // Initialize the Properties fields.
     Node* undefined = jsgraph()->UndefinedConstant();
-    STATIC_ASSERT(NameDictionary::kElementsStartIndex ==
+    static_assert(NameDictionary::kElementsStartIndex ==
                   NameDictionary::kObjectHashIndex + 1);
     for (int index = NameDictionary::kElementsStartIndex; index < length;
          index++) {
@@ -1903,12 +1903,12 @@ Node* JSCreateLowering::AllocateLiteralRegExp(
       native_context().regexp_function().initial_map(dependencies());
 
   // Sanity check that JSRegExp object layout hasn't changed.
-  STATIC_ASSERT(JSRegExp::kDataOffset == JSObject::kHeaderSize);
-  STATIC_ASSERT(JSRegExp::kSourceOffset == JSRegExp::kDataOffset + kTaggedSize);
-  STATIC_ASSERT(JSRegExp::kFlagsOffset ==
+  static_assert(JSRegExp::kDataOffset == JSObject::kHeaderSize);
+  static_assert(JSRegExp::kSourceOffset == JSRegExp::kDataOffset + kTaggedSize);
+  static_assert(JSRegExp::kFlagsOffset ==
                 JSRegExp::kSourceOffset + kTaggedSize);
-  STATIC_ASSERT(JSRegExp::kHeaderSize == JSRegExp::kFlagsOffset + kTaggedSize);
-  STATIC_ASSERT(JSRegExp::kLastIndexOffset == JSRegExp::kHeaderSize);
+  static_assert(JSRegExp::kHeaderSize == JSRegExp::kFlagsOffset + kTaggedSize);
+  static_assert(JSRegExp::kLastIndexOffset == JSRegExp::kHeaderSize);
   DCHECK_EQ(JSRegExp::Size(), JSRegExp::kLastIndexOffset + kTaggedSize);
 
   AllocationBuilder builder(jsgraph(), effect, control);
diff --git a/src/compiler/js-generic-lowering.cc b/src/compiler/js-generic-lowering.cc
index 653b812a359..364803ed225 100644
--- a/src/compiler/js-generic-lowering.cc
+++ b/src/compiler/js-generic-lowering.cc
@@ -135,8 +135,8 @@ void JSGenericLowering::ReplaceUnaryOpWithBuiltinCall(
         zone(), descriptor, descriptor.GetStackParameterCount(), flags,
         node->op()->properties());
     Node* stub_code = jsgraph()->HeapConstant(callable.code());
-    STATIC_ASSERT(JSUnaryOpNode::ValueIndex() == 0);
-    STATIC_ASSERT(JSUnaryOpNode::FeedbackVectorIndex() == 1);
+    static_assert(JSUnaryOpNode::ValueIndex() == 0);
+    static_assert(JSUnaryOpNode::FeedbackVectorIndex() == 1);
     DCHECK_EQ(node->op()->ValueInputCount(), 2);
     node->InsertInput(zone(), 0, stub_code);
     node->InsertInput(zone(), 2, slot);
@@ -166,9 +166,9 @@ void JSGenericLowering::ReplaceBinaryOpWithBuiltinCall(
   const FeedbackParameter& p = FeedbackParameterOf(node->op());
   if (CollectFeedbackInGenericLowering() && p.feedback().IsValid()) {
     Node* slot = jsgraph()->UintPtrConstant(p.feedback().slot.ToInt());
-    STATIC_ASSERT(JSBinaryOpNode::LeftIndex() == 0);
-    STATIC_ASSERT(JSBinaryOpNode::RightIndex() == 1);
-    STATIC_ASSERT(JSBinaryOpNode::FeedbackVectorIndex() == 2);
+    static_assert(JSBinaryOpNode::LeftIndex() == 0);
+    static_assert(JSBinaryOpNode::RightIndex() == 1);
+    static_assert(JSBinaryOpNode::FeedbackVectorIndex() == 2);
     DCHECK_EQ(node->op()->ValueInputCount(), 3);
     node->InsertInput(zone(), 2, slot);
     builtin = builtin_with_feedback;
@@ -217,9 +217,9 @@ void JSGenericLowering::LowerJSStrictEqual(Node* node) {
   const FeedbackParameter& p = FeedbackParameterOf(node->op());
   if (CollectFeedbackInGenericLowering() && p.feedback().IsValid()) {
     Node* slot = jsgraph()->UintPtrConstant(p.feedback().slot.ToInt());
-    STATIC_ASSERT(JSStrictEqualNode::LeftIndex() == 0);
-    STATIC_ASSERT(JSStrictEqualNode::RightIndex() == 1);
-    STATIC_ASSERT(JSStrictEqualNode::FeedbackVectorIndex() == 2);
+    static_assert(JSStrictEqualNode::LeftIndex() == 0);
+    static_assert(JSStrictEqualNode::RightIndex() == 1);
+    static_assert(JSStrictEqualNode::FeedbackVectorIndex() == 2);
     DCHECK_EQ(node->op()->ValueInputCount(), 3);
     node->InsertInput(zone(), 2, slot);
     builtin = Builtin::kStrictEqual_WithFeedback;
@@ -263,7 +263,7 @@ void JSGenericLowering::LowerJSHasProperty(Node* node) {
     node->RemoveInput(JSHasPropertyNode::FeedbackVectorIndex());
     ReplaceWithBuiltinCall(node, Builtin::kHasProperty);
   } else {
-    STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+    static_assert(n.FeedbackVectorIndex() == 2);
     n->InsertInput(zone(), 2,
                    jsgraph()->TaggedIndexConstant(p.feedback().index()));
     ReplaceWithBuiltinCall(node, Builtin::kKeyedHasIC);
@@ -275,7 +275,7 @@ void JSGenericLowering::LowerJSLoadProperty(Node* node) {
   const PropertyAccess& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+  static_assert(n.FeedbackVectorIndex() == 2);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     n->InsertInput(zone(), 2,
@@ -299,7 +299,7 @@ void JSGenericLowering::LowerJSLoadNamed(Node* node) {
   NamedAccess const& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 1);
+  static_assert(n.FeedbackVectorIndex() == 1);
   if (!p.feedback().IsValid()) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 1, jsgraph()->Constant(p.name(broker())));
@@ -340,7 +340,7 @@ void JSGenericLowering::LowerJSLoadNamedFromSuper(Node* node) {
       home_object_map, effect, control);
   n->ReplaceInput(n.HomeObjectIndex(), home_object_proto);
   NodeProperties::ReplaceEffectInput(node, effect);
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+  static_assert(n.FeedbackVectorIndex() == 2);
   // If the code below will be used for the invalid feedback case, it needs to
   // be double-checked that the FeedbackVector parameter will be the
   // UndefinedConstant.
@@ -357,7 +357,7 @@ void JSGenericLowering::LowerJSLoadGlobal(Node* node) {
   CallDescriptor::Flags flags = FrameStateFlagForCall(node);
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 0);
+  static_assert(n.FeedbackVectorIndex() == 0);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 0, jsgraph()->Constant(p.name(broker())));
@@ -392,7 +392,7 @@ void JSGenericLowering::LowerJSGetIterator(Node* node) {
       jsgraph()->TaggedIndexConstant(p.loadFeedback().slot.ToInt());
   Node* call_slot =
       jsgraph()->TaggedIndexConstant(p.callFeedback().slot.ToInt());
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 1);
+  static_assert(n.FeedbackVectorIndex() == 1);
   node->InsertInput(zone(), 1, load_slot);
   node->InsertInput(zone(), 2, call_slot);
 
@@ -404,7 +404,7 @@ void JSGenericLowering::LowerJSSetKeyedProperty(Node* node) {
   const PropertyAccess& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 3);
+  static_assert(n.FeedbackVectorIndex() == 3);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 3,
@@ -428,7 +428,7 @@ void JSGenericLowering::LowerJSDefineKeyedOwnProperty(Node* node) {
   const PropertyAccess& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 3);
+  static_assert(n.FeedbackVectorIndex() == 3);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 3,
@@ -446,7 +446,7 @@ void JSGenericLowering::LowerJSSetNamedProperty(Node* node) {
   NamedAccess const& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+  static_assert(n.FeedbackVectorIndex() == 2);
   if (!p.feedback().IsValid()) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 1, jsgraph()->Constant(p.name(broker())));
@@ -476,7 +476,7 @@ void JSGenericLowering::LowerJSDefineNamedOwnProperty(Node* node) {
   DefineNamedOwnPropertyParameters const& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+  static_assert(n.FeedbackVectorIndex() == 2);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 1, jsgraph()->Constant(p.name(broker())));
@@ -498,7 +498,7 @@ void JSGenericLowering::LowerJSStoreGlobal(Node* node) {
   const StoreGlobalParameters& p = n.Parameters();
   FrameState frame_state = n.frame_state();
   Node* outer_state = frame_state.outer_frame_state();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 1);
+  static_assert(n.FeedbackVectorIndex() == 1);
   if (outer_state->opcode() != IrOpcode::kFrameState) {
     n->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(zone(), 0, jsgraph()->Constant(p.name(broker())));
@@ -516,7 +516,7 @@ void JSGenericLowering::LowerJSStoreGlobal(Node* node) {
 void JSGenericLowering::LowerJSDefineKeyedOwnPropertyInLiteral(Node* node) {
   JSDefineKeyedOwnPropertyInLiteralNode n(node);
   FeedbackParameter const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 4);
+  static_assert(n.FeedbackVectorIndex() == 4);
   RelaxControls(node);
   node->InsertInput(zone(), 5,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
@@ -526,7 +526,7 @@ void JSGenericLowering::LowerJSDefineKeyedOwnPropertyInLiteral(Node* node) {
 void JSGenericLowering::LowerJSStoreInArrayLiteral(Node* node) {
   JSStoreInArrayLiteralNode n(node);
   FeedbackParameter const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 3);
+  static_assert(n.FeedbackVectorIndex() == 3);
   RelaxControls(node);
   node->InsertInput(zone(), 3,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
@@ -658,7 +658,7 @@ void JSGenericLowering::LowerJSCreateClosure(Node* node) {
   JSCreateClosureNode n(node);
   CreateClosureParameters const& p = n.Parameters();
   SharedFunctionInfoRef shared_info = p.shared_info(broker());
-  STATIC_ASSERT(n.FeedbackCellIndex() == 0);
+  static_assert(n.FeedbackCellIndex() == 0);
   node->InsertInput(zone(), 0, jsgraph()->Constant(shared_info));
   node->RemoveInput(4);  // control
 
@@ -718,7 +718,7 @@ void JSGenericLowering::LowerJSCreateTypedArray(Node* node) {
 void JSGenericLowering::LowerJSCreateLiteralArray(Node* node) {
   JSCreateLiteralArrayNode n(node);
   CreateLiteralParameters const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 0);
+  static_assert(n.FeedbackVectorIndex() == 0);
   node->InsertInput(zone(), 1,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
   node->InsertInput(zone(), 2, jsgraph()->Constant(p.constant(broker())));
@@ -743,7 +743,7 @@ void JSGenericLowering::LowerJSGetTemplateObject(Node* node) {
   DCHECK_EQ(node->op()->ControlInputCount(), 1);
   node->RemoveInput(NodeProperties::FirstControlIndex(node));
 
-  STATIC_ASSERT(JSGetTemplateObjectNode::FeedbackVectorIndex() == 0);
+  static_assert(JSGetTemplateObjectNode::FeedbackVectorIndex() == 0);
   node->InsertInput(zone(), 0, jsgraph()->Constant(shared));
   node->InsertInput(zone(), 1, jsgraph()->Constant(description));
   node->InsertInput(zone(), 2,
@@ -755,7 +755,7 @@ void JSGenericLowering::LowerJSGetTemplateObject(Node* node) {
 void JSGenericLowering::LowerJSCreateEmptyLiteralArray(Node* node) {
   JSCreateEmptyLiteralArrayNode n(node);
   FeedbackParameter const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 0);
+  static_assert(n.FeedbackVectorIndex() == 0);
   node->InsertInput(zone(), 1,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
   node->RemoveInput(4);  // control
@@ -769,7 +769,7 @@ void JSGenericLowering::LowerJSCreateArrayFromIterable(Node* node) {
 void JSGenericLowering::LowerJSCreateLiteralObject(Node* node) {
   JSCreateLiteralObjectNode n(node);
   CreateLiteralParameters const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 0);
+  static_assert(n.FeedbackVectorIndex() == 0);
   node->InsertInput(zone(), 1,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
   node->InsertInput(zone(), 2, jsgraph()->Constant(p.constant(broker())));
@@ -789,7 +789,7 @@ void JSGenericLowering::LowerJSCreateLiteralObject(Node* node) {
 void JSGenericLowering::LowerJSCloneObject(Node* node) {
   JSCloneObjectNode n(node);
   CloneObjectParameters const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 1);
+  static_assert(n.FeedbackVectorIndex() == 1);
   node->InsertInput(zone(), 1, jsgraph()->SmiConstant(p.flags()));
   node->InsertInput(zone(), 2,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
@@ -803,7 +803,7 @@ void JSGenericLowering::LowerJSCreateEmptyLiteralObject(Node* node) {
 void JSGenericLowering::LowerJSCreateLiteralRegExp(Node* node) {
   JSCreateLiteralRegExpNode n(node);
   CreateLiteralParameters const& p = n.Parameters();
-  STATIC_ASSERT(n.FeedbackVectorIndex() == 0);
+  static_assert(n.FeedbackVectorIndex() == 0);
   node->InsertInput(zone(), 1,
                     jsgraph()->TaggedIndexConstant(p.feedback().index()));
   node->InsertInput(zone(), 2, jsgraph()->Constant(p.constant(broker())));
diff --git a/src/compiler/js-heap-broker.h b/src/compiler/js-heap-broker.h
index 753bdb73d65..04abb6b9c97 100644
--- a/src/compiler/js-heap-broker.h
+++ b/src/compiler/js-heap-broker.h
@@ -466,9 +466,9 @@ class V8_EXPORT_PRIVATE JSHeapBroker {
   int boilerplate_migration_mutex_depth_ = 0;
 
   static constexpr uint32_t kMinimalRefsBucketCount = 8;
-  STATIC_ASSERT(base::bits::IsPowerOfTwo(kMinimalRefsBucketCount));
+  static_assert(base::bits::IsPowerOfTwo(kMinimalRefsBucketCount));
   static constexpr uint32_t kInitialRefsBucketCount = 1024;
-  STATIC_ASSERT(base::bits::IsPowerOfTwo(kInitialRefsBucketCount));
+  static_assert(base::bits::IsPowerOfTwo(kInitialRefsBucketCount));
 };
 
 class V8_NODISCARD TraceScope {
diff --git a/src/compiler/js-inlining-heuristic.cc b/src/compiler/js-inlining-heuristic.cc
index 14e5080c301..b07f4fac6f9 100644
--- a/src/compiler/js-inlining-heuristic.cc
+++ b/src/compiler/js-inlining-heuristic.cc
@@ -657,7 +657,7 @@ void JSInliningHeuristic::CreateOrReuseDispatch(Node* node, Node* callee,
     return;
   }
 
-  STATIC_ASSERT(JSCallOrConstructNode::kHaveIdenticalLayouts);
+  static_assert(JSCallOrConstructNode::kHaveIdenticalLayouts);
 
   Node* fallthrough_control = NodeProperties::GetControlInput(node);
   int const num_calls = candidate.num_functions;
diff --git a/src/compiler/js-inlining.cc b/src/compiler/js-inlining.cc
index a1359e04ec9..5bd7ec0d70b 100644
--- a/src/compiler/js-inlining.cc
+++ b/src/compiler/js-inlining.cc
@@ -616,7 +616,7 @@ Reduction JSInliner::ReduceJSCall(Node* node) {
 
   // Inline {JSConstruct} requires some additional magic.
   if (node->opcode() == IrOpcode::kJSConstruct) {
-    STATIC_ASSERT(JSCallOrConstructNode::kHaveIdenticalLayouts);
+    static_assert(JSCallOrConstructNode::kHaveIdenticalLayouts);
     JSConstructNode n(node);
 
     new_target = n.new_target();
diff --git a/src/compiler/js-intrinsic-lowering.cc b/src/compiler/js-intrinsic-lowering.cc
index 50a1198bb9f..de84d20f3b8 100644
--- a/src/compiler/js-intrinsic-lowering.cc
+++ b/src/compiler/js-intrinsic-lowering.cc
@@ -336,7 +336,7 @@ Reduction JSIntrinsicLowering::ReduceCall(Node* node) {
   int const arity =
       static_cast<int>(CallRuntimeParametersOf(node->op()).arity());
   static constexpr int kTargetAndReceiver = 2;
-  STATIC_ASSERT(JSCallNode::kFeedbackVectorIsLastInput);
+  static_assert(JSCallNode::kFeedbackVectorIsLastInput);
   Node* feedback = jsgraph()->UndefinedConstant();
   node->InsertInput(graph()->zone(), arity, feedback);
   NodeProperties::ChangeOp(
diff --git a/src/compiler/js-native-context-specialization.cc b/src/compiler/js-native-context-specialization.cc
index c94cbc0a529..fc8141de3bc 100644
--- a/src/compiler/js-native-context-specialization.cc
+++ b/src/compiler/js-native-context-specialization.cc
@@ -436,7 +436,7 @@ Reduction JSNativeContextSpecialization::ReduceJSInstanceOf(Node* node) {
     NodeProperties::ReplaceValueInput(node, constructor, 0);
     NodeProperties::ReplaceValueInput(node, object, 1);
     NodeProperties::ReplaceEffectInput(node, effect);
-    STATIC_ASSERT(n.FeedbackVectorIndex() == 2);
+    static_assert(n.FeedbackVectorIndex() == 2);
     node->RemoveInput(n.FeedbackVectorIndex());
     NodeProperties::ChangeOp(node, javascript()->OrdinaryHasInstance());
     return Changed(node).FollowedBy(ReduceJSOrdinaryHasInstance(node));
@@ -482,7 +482,7 @@ Reduction JSNativeContextSpecialization::ReduceJSInstanceOf(Node* node) {
     Node* target = jsgraph()->Constant(*constant);
     Node* feedback = jsgraph()->UndefinedConstant();
     // Value inputs plus context, frame state, effect, control.
-    STATIC_ASSERT(JSCallNode::ArityForArgc(1) + 4 == 8);
+    static_assert(JSCallNode::ArityForArgc(1) + 4 == 8);
     node->EnsureInputCount(graph()->zone(), 8);
     node->ReplaceInput(JSCallNode::TargetIndex(), target);
     node->ReplaceInput(JSCallNode::ReceiverIndex(), constructor);
@@ -1049,7 +1049,7 @@ Reduction JSNativeContextSpecialization::ReduceNamedAccess(
          node->opcode() == IrOpcode::kJSHasProperty ||
          node->opcode() == IrOpcode::kJSLoadNamedFromSuper ||
          node->opcode() == IrOpcode::kJSDefineKeyedOwnProperty);
-  STATIC_ASSERT(JSLoadNamedNode::ObjectIndex() == 0 &&
+  static_assert(JSLoadNamedNode::ObjectIndex() == 0 &&
                 JSSetNamedPropertyNode::ObjectIndex() == 0 &&
                 JSLoadPropertyNode::ObjectIndex() == 0 &&
                 JSSetKeyedPropertyNode::ObjectIndex() == 0 &&
@@ -1058,7 +1058,7 @@ Reduction JSNativeContextSpecialization::ReduceNamedAccess(
                 JSDefineKeyedOwnPropertyInLiteralNode::ObjectIndex() == 0 &&
                 JSHasPropertyNode::ObjectIndex() == 0 &&
                 JSDefineKeyedOwnPropertyNode::ObjectIndex() == 0);
-  STATIC_ASSERT(JSLoadNamedFromSuperNode::ReceiverIndex() == 0);
+  static_assert(JSLoadNamedFromSuperNode::ReceiverIndex() == 0);
 
   Node* context = NodeProperties::GetContextInput(node);
   FrameState frame_state{NodeProperties::GetFrameStateInput(node)};
@@ -1715,7 +1715,7 @@ Reduction JSNativeContextSpecialization::ReduceElementAccess(
          node->opcode() == IrOpcode::kJSDefineKeyedOwnPropertyInLiteral ||
          node->opcode() == IrOpcode::kJSHasProperty ||
          node->opcode() == IrOpcode::kJSDefineKeyedOwnProperty);
-  STATIC_ASSERT(JSLoadPropertyNode::ObjectIndex() == 0 &&
+  static_assert(JSLoadPropertyNode::ObjectIndex() == 0 &&
                 JSSetKeyedPropertyNode::ObjectIndex() == 0 &&
                 JSStoreInArrayLiteralNode::ArrayIndex() == 0 &&
                 JSDefineKeyedOwnPropertyInLiteralNode::ObjectIndex() == 0 &&
@@ -1957,7 +1957,7 @@ Reduction JSNativeContextSpecialization::ReduceElementLoadFromHeapConstant(
   NumberMatcher mkey(key);
   if (mkey.IsInteger() &&
       mkey.IsInRange(0.0, static_cast<double>(JSObject::kMaxElementIndex))) {
-    STATIC_ASSERT(JSObject::kMaxElementIndex <= kMaxUInt32);
+    static_assert(JSObject::kMaxElementIndex <= kMaxUInt32);
     const uint32_t index = static_cast<uint32_t>(mkey.ResolvedValue());
     base::Optional<ObjectRef> element;
 
diff --git a/src/compiler/js-operator.h b/src/compiler/js-operator.h
index 4f3452533b9..42a629322d1 100644
--- a/src/compiler/js-operator.h
+++ b/src/compiler/js-operator.h
@@ -1369,8 +1369,8 @@ class JSCallOrConstructNode : public JSNodeWrapperBase {
   static constexpr int kExtraInputCount = kTargetInputCount +
                                           kReceiverOrNewTargetInputCount +
                                           kFeedbackVectorInputCount;
-  STATIC_ASSERT(kExtraInputCount == CallParameters::kExtraCallInputCount);
-  STATIC_ASSERT(kExtraInputCount ==
+  static_assert(kExtraInputCount == CallParameters::kExtraCallInputCount);
+  static_assert(kExtraInputCount ==
                 ConstructParameters::kExtraConstructInputCount);
 
   // Just for static asserts for spots that rely on node layout.
@@ -1413,7 +1413,7 @@ class JSCallOrConstructNode : public JSNodeWrapperBase {
   virtual int ArgumentCount() const = 0;
 
   static constexpr int FeedbackVectorIndexForArgc(int argc) {
-    STATIC_ASSERT(kFeedbackVectorIsLastInput);
+    static_assert(kFeedbackVectorIsLastInput);
     return ArgumentIndex(argc - 1) + 1;
   }
   int FeedbackVectorIndex() const {
@@ -1467,7 +1467,7 @@ class JSCallNodeBase final : public JSCallOrConstructNode {
 #undef INPUTS
 
   static constexpr int kReceiverInputCount = 1;
-  STATIC_ASSERT(kReceiverInputCount ==
+  static_assert(kReceiverInputCount ==
                 JSCallOrConstructNode::kReceiverOrNewTargetInputCount);
 
   int ArgumentCount() const override {
@@ -1501,7 +1501,7 @@ class JSWasmCallNode final : public JSCallOrConstructNode {
 #undef INPUTS
 
   static constexpr int kReceiverInputCount = 1;
-  STATIC_ASSERT(kReceiverInputCount ==
+  static_assert(kReceiverInputCount ==
                 JSCallOrConstructNode::kReceiverOrNewTargetInputCount);
 
   int ArgumentCount() const override {
@@ -1533,7 +1533,7 @@ class JSConstructNodeBase final : public JSCallOrConstructNode {
 #undef INPUTS
 
   static constexpr int kNewTargetInputCount = 1;
-  STATIC_ASSERT(kNewTargetInputCount ==
+  static_assert(kNewTargetInputCount ==
                 JSCallOrConstructNode::kReceiverOrNewTargetInputCount);
 
   int ArgumentCount() const {
diff --git a/src/compiler/js-typed-lowering.cc b/src/compiler/js-typed-lowering.cc
index 7a7c99368bb..e17e6babe49 100644
--- a/src/compiler/js-typed-lowering.cc
+++ b/src/compiler/js-typed-lowering.cc
@@ -1527,7 +1527,7 @@ void ReduceBuiltin(JSGraph* jsgraph, Node* node, Builtin builtin, int arity,
   Node* new_target;
   Zone* zone = jsgraph->zone();
   if (node->opcode() == IrOpcode::kJSConstruct) {
-    STATIC_ASSERT(JSCallNode::ReceiverIndex() ==
+    static_assert(JSCallNode::ReceiverIndex() ==
                   JSConstructNode::NewTargetIndex());
     new_target = JSConstructNode{node}.new_target();
     node->ReplaceInput(JSConstructNode::NewTargetIndex(),
@@ -1629,8 +1629,8 @@ Reduction JSTypedLowering::ReduceJSConstruct(Node* node) {
         isolate(), function.shared().construct_as_builtin()
                        ? Builtin::kJSBuiltinsConstructStub
                        : Builtin::kJSConstructStubGeneric);
-    STATIC_ASSERT(JSConstructNode::TargetIndex() == 0);
-    STATIC_ASSERT(JSConstructNode::NewTargetIndex() == 1);
+    static_assert(JSConstructNode::TargetIndex() == 0);
+    static_assert(JSConstructNode::NewTargetIndex() == 1);
     node->RemoveInput(n.FeedbackVectorIndex());
     node->InsertInput(graph()->zone(), 0,
                       jsgraph()->HeapConstant(callable.code()));
@@ -1982,7 +1982,7 @@ Reduction JSTypedLowering::ReduceJSForInPrepare(Node* node) {
       Node* bit_field3 = effect = graph()->NewNode(
           simplified()->LoadField(AccessBuilder::ForMapBitField3()), enumerator,
           effect, control);
-      STATIC_ASSERT(Map::Bits3::EnumLengthBits::kShift == 0);
+      static_assert(Map::Bits3::EnumLengthBits::kShift == 0);
       cache_length = graph()->NewNode(
           simplified()->NumberBitwiseAnd(), bit_field3,
           jsgraph()->Constant(Map::Bits3::EnumLengthBits::kMask));
@@ -2017,7 +2017,7 @@ Reduction JSTypedLowering::ReduceJSForInPrepare(Node* node) {
         Node* bit_field3 = etrue = graph()->NewNode(
             simplified()->LoadField(AccessBuilder::ForMapBitField3()),
             enumerator, etrue, if_true);
-        STATIC_ASSERT(Map::Bits3::EnumLengthBits::kShift == 0);
+        static_assert(Map::Bits3::EnumLengthBits::kShift == 0);
         cache_length_true = graph()->NewNode(
             simplified()->NumberBitwiseAnd(), bit_field3,
             jsgraph()->Constant(Map::Bits3::EnumLengthBits::kMask));
diff --git a/src/compiler/memory-lowering.cc b/src/compiler/memory-lowering.cc
index 6b5eb17b334..a2f406ab9c7 100644
--- a/src/compiler/memory-lowering.cc
+++ b/src/compiler/memory-lowering.cc
@@ -420,7 +420,7 @@ Node* MemoryLowering::DecodeExternalPointer(
   // Clone the load node and put it here.
   // TODO(turbofan): consider adding GraphAssembler::Clone() suitable for
   // cloning nodes from arbitrary locaions in effect/control chains.
-  STATIC_ASSERT(kExternalPointerIndexShift > kSystemPointerSizeLog2);
+  static_assert(kExternalPointerIndexShift > kSystemPointerSizeLog2);
   Node* shifted_index = __ AddNode(graph()->CloneNode(node));
   Node* shift_amount =
       __ Int32Constant(kExternalPointerIndexShift - kSystemPointerSizeLog2);
diff --git a/src/compiler/node.cc b/src/compiler/node.cc
index f5ceb7f7e1c..ae24ca7c121 100644
--- a/src/compiler/node.cc
+++ b/src/compiler/node.cc
@@ -162,7 +162,7 @@ void Node::AppendInput(Zone* zone, Node* new_to) {
     bit_field_ = InlineCountField::update(bit_field_, inline_count + 1);
     *GetInputPtr(inline_count) = new_to;
     Use* use = GetUsePtr(inline_count);
-    STATIC_ASSERT(InlineCapacityField::kMax <= Use::InputIndexField::kMax);
+    static_assert(InlineCapacityField::kMax <= Use::InputIndexField::kMax);
     use->bit_field_ = Use::InputIndexField::encode(inline_count) |
                       Use::InlineField::encode(true);
     new_to->AppendUse(use);
@@ -389,7 +389,7 @@ Node::Node(NodeId id, const Operator* op, int inline_count, int inline_capacity)
                  InlineCapacityField::encode(inline_capacity)),
       first_use_(nullptr) {
   // Check that the id didn't overflow.
-  STATIC_ASSERT(IdField::kMax < std::numeric_limits<NodeId>::max());
+  static_assert(IdField::kMax < std::numeric_limits<NodeId>::max());
   CHECK(IdField::is_valid(id));
 
   // Inputs must either be out of line or within the inline capacity.
diff --git a/src/compiler/operation-typer.cc b/src/compiler/operation-typer.cc
index 95c5a62249d..ed51db6a112 100644
--- a/src/compiler/operation-typer.cc
+++ b/src/compiler/operation-typer.cc
@@ -93,7 +93,7 @@ Type OperationTyper::WeakenRange(Type previous_range, Type current_range) {
                                             140737488355327.0,
                                             281474976710655.0,
                                             562949953421311.0};
-  STATIC_ASSERT(arraysize(kWeakenMinLimits) == arraysize(kWeakenMaxLimits));
+  static_assert(arraysize(kWeakenMinLimits) == arraysize(kWeakenMaxLimits));
 
   double current_min = current_range.Min();
   double new_min = current_min;
diff --git a/src/compiler/scheduler.cc b/src/compiler/scheduler.cc
index 5f6e4e51338..90bf8c6d3e0 100644
--- a/src/compiler/scheduler.cc
+++ b/src/compiler/scheduler.cc
@@ -1173,7 +1173,7 @@ BasicBlock* Scheduler::GetCommonDominator(BasicBlock* b1, BasicBlock* b2) {
   // Try to find the common dominator by walking, if there is a chance of
   // finding it quickly.
   constexpr int kCacheGranularity = 63;
-  STATIC_ASSERT((kCacheGranularity & (kCacheGranularity + 1)) == 0);
+  static_assert((kCacheGranularity & (kCacheGranularity + 1)) == 0);
   int depth_difference = b1->dominator_depth() - b2->dominator_depth();
   if (depth_difference > -kCacheGranularity &&
       depth_difference < kCacheGranularity) {
diff --git a/src/compiler/turboshaft/assembler.h b/src/compiler/turboshaft/assembler.h
index d909c46f643..a6439bca915 100644
--- a/src/compiler/turboshaft/assembler.h
+++ b/src/compiler/turboshaft/assembler.h
@@ -191,8 +191,8 @@ class Assembler
 
   template <class Op, class... Args>
   OpIndex Emit(Args... args) {
-    STATIC_ASSERT((std::is_base_of<Operation, Op>::value));
-    STATIC_ASSERT(!(std::is_same<Op, Operation>::value));
+    static_assert((std::is_base_of<Operation, Op>::value));
+    static_assert(!(std::is_same<Op, Operation>::value));
     DCHECK_NOT_NULL(current_block_);
     OpIndex result = graph().Add<Op>(args...);
     if (Op::properties.is_block_terminator) FinalizeBlock();
diff --git a/src/compiler/turboshaft/graph.h b/src/compiler/turboshaft/graph.h
index ab467fd9a0e..d453db1b3ac 100644
--- a/src/compiler/turboshaft/graph.h
+++ b/src/compiler/turboshaft/graph.h
@@ -319,8 +319,8 @@ class Graph {
 
   template <class Op, class... Args>
   void Replace(OpIndex replaced, Args... args) {
-    STATIC_ASSERT((std::is_base_of<Operation, Op>::value));
-    STATIC_ASSERT(std::is_trivially_destructible<Op>::value);
+    static_assert((std::is_base_of<Operation, Op>::value));
+    static_assert(std::is_trivially_destructible<Op>::value);
 
     OperationBuffer::ReplaceScope replace_scope(&operations_, replaced);
     Op::New(this, args...);
diff --git a/src/compiler/turboshaft/operations.h b/src/compiler/turboshaft/operations.h
index 58d355d5c96..3b57c824910 100644
--- a/src/compiler/turboshaft/operations.h
+++ b/src/compiler/turboshaft/operations.h
@@ -333,8 +333,8 @@ struct OperationT : Operation {
     // This is an optimized computation of:
     //   round_up(size_in_bytes / sizeof(StorageSlot))
     constexpr size_t r = sizeof(OperationStorageSlot) / sizeof(OpIndex);
-    STATIC_ASSERT(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
-    STATIC_ASSERT(sizeof(Derived) % sizeof(OpIndex) == 0);
+    static_assert(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
+    static_assert(sizeof(Derived) % sizeof(OpIndex) == 0);
     size_t result = std::max<size_t>(
         2, (r - 1 + sizeof(Derived) / sizeof(OpIndex) + input_count) / r);
     DCHECK_EQ(result, Operation::StorageSlotCount(opcode, input_count));
@@ -360,11 +360,11 @@ struct OperationT : Operation {
   }
 
   explicit OperationT(size_t input_count) : Operation(opcode, input_count) {
-    STATIC_ASSERT((std::is_base_of<OperationT, Derived>::value));
+    static_assert((std::is_base_of<OperationT, Derived>::value));
 #if !V8_CC_MSVC
-    STATIC_ASSERT(std::is_trivially_copyable<Derived>::value);
+    static_assert(std::is_trivially_copyable<Derived>::value);
 #endif  // !V8_CC_MSVC
-    STATIC_ASSERT(std::is_trivially_destructible<Derived>::value);
+    static_assert(std::is_trivially_destructible<Derived>::value);
   }
   explicit OperationT(base::Vector<const OpIndex> inputs)
       : OperationT(inputs.size()) {
@@ -1320,7 +1320,7 @@ inline const OpProperties& Operation::properties() const {
 inline size_t Operation::StorageSlotCount(Opcode opcode, size_t input_count) {
   size_t size = kOperationSizeDividedBySizeofOpIndexTable[OpcodeIndex(opcode)];
   constexpr size_t r = sizeof(OperationStorageSlot) / sizeof(OpIndex);
-  STATIC_ASSERT(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
+  static_assert(sizeof(OperationStorageSlot) % sizeof(OpIndex) == 0);
   return std::max<size_t>(2, (r - 1 + size + input_count) / r);
 }
 
diff --git a/src/compiler/typer.cc b/src/compiler/typer.cc
index 7227dffd051..a3220968afc 100644
--- a/src/compiler/typer.cc
+++ b/src/compiler/typer.cc
@@ -1371,7 +1371,7 @@ Type Typer::Visitor::Weaken(Node* node, Type current_type, Type previous_type) {
       4398046511103.0, 8796093022207.0, 17592186044415.0, 35184372088831.0,
       70368744177663.0, 140737488355327.0, 281474976710655.0,
       562949953421311.0};
-  STATIC_ASSERT(arraysize(kWeakenMinLimits) == arraysize(kWeakenMaxLimits));
+  static_assert(arraysize(kWeakenMinLimits) == arraysize(kWeakenMaxLimits));
 
   // If the types have nothing to do with integers, return the types.
   Type const integer = typer_->cache_->kInteger;
@@ -1863,7 +1863,7 @@ Type Typer::Visitor::TypeJSForInNext(Node* node) {
 }
 
 Type Typer::Visitor::TypeJSForInPrepare(Node* node) {
-  STATIC_ASSERT(Map::Bits3::EnumLengthBits::kMax <= FixedArray::kMaxLength);
+  static_assert(Map::Bits3::EnumLengthBits::kMax <= FixedArray::kMaxLength);
   Type const cache_type =
       Type::Union(Type::SignedSmall(), Type::OtherInternal(), zone());
   Type const cache_array = Type::OtherInternal();
diff --git a/src/compiler/types.cc b/src/compiler/types.cc
index 365eb3c13fc..a6d1f95a6dd 100644
--- a/src/compiler/types.cc
+++ b/src/compiler/types.cc
@@ -1183,10 +1183,10 @@ Handle<TurbofanType> Type::AllocateOnHeap(Factory* factory) {
 }
 
 #define VERIFY_TORQUE_LOW_BITSET_AGREEMENT(Name, _)           \
-  STATIC_ASSERT(static_cast<uint32_t>(BitsetType::k##Name) == \
+  static_assert(static_cast<uint32_t>(BitsetType::k##Name) == \
                 static_cast<uint32_t>(TurbofanTypeLowBits::k##Name));
 #define VERIFY_TORQUE_HIGH_BITSET_AGREEMENT(Name, _)                     \
-  STATIC_ASSERT(static_cast<uint32_t>(                                   \
+  static_assert(static_cast<uint32_t>(                                   \
                     static_cast<uint64_t>(BitsetType::k##Name) >> 32) == \
                 static_cast<uint32_t>(TurbofanTypeHighBits::k##Name));
 INTERNAL_BITSET_TYPE_LIST(VERIFY_TORQUE_LOW_BITSET_AGREEMENT)
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index 5eb7c442076..15ffb574daf 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -378,7 +378,7 @@ void WasmGraphBuilder::StackCheck(
         Operator::kNoThrow | Operator::kNoWrite;
     // If we ever want to mark this call as  kNoDeopt, we'll have to make it
     // non-eliminatable some other way.
-    STATIC_ASSERT((properties & Operator::kEliminatable) !=
+    static_assert((properties & Operator::kEliminatable) !=
                   Operator::kEliminatable);
     auto call_descriptor = Linkage::GetStubCallDescriptor(
         mcgraph()->zone(),                    // zone
@@ -2881,7 +2881,7 @@ Node* WasmGraphBuilder::BuildLoadExternalPointerFromObject(
 #ifdef V8_SANDBOXED_EXTERNAL_POINTERS
   Node* external_pointer = gasm_->LoadFromObject(
       MachineType::Uint32(), object, wasm::ObjectAccess::ToTagged(offset));
-  STATIC_ASSERT(kExternalPointerIndexShift > kSystemPointerSizeLog2);
+  static_assert(kExternalPointerIndexShift > kSystemPointerSizeLog2);
   Node* shift_amount =
       gasm_->Int32Constant(kExternalPointerIndexShift - kSystemPointerSizeLog2);
   Node* scaled_index = gasm_->Word32Shr(external_pointer, shift_amount);
@@ -5060,7 +5060,7 @@ void WasmGraphBuilder::DataDrop(uint32_t data_segment_index,
 
   Node* seg_size_array =
       LOAD_INSTANCE_FIELD(DataSegmentSizes, MachineType::Pointer());
-  STATIC_ASSERT(wasm::kV8MaxWasmDataSegments <= kMaxUInt32 >> 2);
+  static_assert(wasm::kV8MaxWasmDataSegments <= kMaxUInt32 >> 2);
   auto access = ObjectAccess(MachineType::Int32(), kNoWriteBarrier);
   gasm_->StoreToObject(access, seg_size_array, data_segment_index << 2,
                        Int32Constant(0));
@@ -6205,7 +6205,7 @@ class WasmWrapperGraphBuilder : public WasmGraphBuilder {
   void BuildCheckValidRefValue(Node* input, Node* js_context,
                                wasm::ValueType type) {
     // Make sure ValueType fits in a Smi.
-    STATIC_ASSERT(wasm::ValueType::kLastUsedBit + 1 <= kSmiValueSize);
+    static_assert(wasm::ValueType::kLastUsedBit + 1 <= kSmiValueSize);
     // The instance node is always defined: if an instance is not available, it
     // is the undefined value.
     Node* inputs[] = {GetInstance(), input,
@@ -6434,7 +6434,7 @@ class WasmWrapperGraphBuilder : public WasmGraphBuilder {
   Node* BuildCallAllocateJSArray(Node* array_length, Node* context) {
     // Since we don't check that args will fit in an array,
     // we make sure this is true based on statically known limits.
-    STATIC_ASSERT(wasm::kV8MaxWasmFunctionReturns <=
+    static_assert(wasm::kV8MaxWasmFunctionReturns <=
                   JSArray::kInitialMaxFastElementArray);
     return gasm_->CallBuiltin(Builtin::kWasmAllocateJSArray,
                               Operator::kEliminatable, array_length, context);
diff --git a/src/d8/d8.h b/src/d8/d8.h
index aae684dcac0..ddaf6a4046b 100644
--- a/src/d8/d8.h
+++ b/src/d8/d8.h
@@ -48,7 +48,7 @@ class Counter {
   void Bind(const char* name, bool histogram);
   // TODO(12482): Return pointer to an atomic.
   int* ptr() {
-    STATIC_ASSERT(sizeof(int) == sizeof(count_));
+    static_assert(sizeof(int) == sizeof(count_));
     return reinterpret_cast<int*>(&count_);
   }
   int count() const { return count_.load(std::memory_order_relaxed); }
diff --git a/src/debug/debug-interface.cc b/src/debug/debug-interface.cc
index 48cebc79b53..39ec4be5bac 100644
--- a/src/debug/debug-interface.cc
+++ b/src/debug/debug-interface.cc
@@ -1093,7 +1093,7 @@ MaybeLocal<Value> CallFunctionOn(Local<Context> context,
   PREPARE_FOR_DEBUG_INTERFACE_EXECUTION_WITH_ISOLATE(isolate, Value);
   auto self = Utils::OpenHandle(*function);
   auto recv_obj = Utils::OpenHandle(*recv);
-  STATIC_ASSERT(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
+  static_assert(sizeof(v8::Local<v8::Value>) == sizeof(i::Handle<i::Object>));
   auto args = reinterpret_cast<i::Handle<i::Object>*>(argv);
   // Disable breaks in side-effect free mode.
   i::DisableBreak disable_break_scope(isolate->debug(), throw_on_side_effect);
diff --git a/src/debug/debug-wasm-objects.cc b/src/debug/debug-wasm-objects.cc
index 70fef6a9438..11d407b0b94 100644
--- a/src/debug/debug-wasm-objects.cc
+++ b/src/debug/debug-wasm-objects.cc
@@ -519,7 +519,7 @@ Handle<FixedArray> GetOrCreateInstanceProxyCache(
 template <typename Proxy>
 Handle<JSObject> GetOrCreateInstanceProxy(Isolate* isolate,
                                           Handle<WasmInstanceObject> instance) {
-  STATIC_ASSERT(Proxy::kId < kNumInstanceProxies);
+  static_assert(Proxy::kId < kNumInstanceProxies);
   Handle<FixedArray> proxies = GetOrCreateInstanceProxyCache(isolate, instance);
   if (!proxies->is_the_hole(isolate, Proxy::kId)) {
     return handle(JSObject::cast(proxies->get(Proxy::kId)), isolate);
diff --git a/src/debug/debug.cc b/src/debug/debug.cc
index d56f8a110f0..f1cdf7cf8b4 100644
--- a/src/debug/debug.cc
+++ b/src/debug/debug.cc
@@ -2464,7 +2464,7 @@ void Debug::UpdateState() {
 }
 
 void Debug::UpdateHookOnFunctionCall() {
-  STATIC_ASSERT(LastStepAction == StepInto);
+  static_assert(LastStepAction == StepInto);
   hook_on_function_call_ =
       thread_local_.last_step_action_ == StepInto ||
       isolate_->debug_execution_mode() == DebugInfo::kSideEffects ||
diff --git a/src/debug/liveedit.cc b/src/debug/liveedit.cc
index 1cef52ac2f5..3f231a1ae18 100644
--- a/src/debug/liveedit.cc
+++ b/src/debug/liveedit.cc
@@ -210,7 +210,7 @@ class Differencer {
   // This method only holds static assert statement (unfortunately you cannot
   // place one in class scope).
   void StaticAssertHolder() {
-    STATIC_ASSERT(MAX_DIRECTION_FLAG_VALUE < (1 << kDirectionSizeBits));
+    static_assert(MAX_DIRECTION_FLAG_VALUE < (1 << kDirectionSizeBits));
   }
 
   class ResultWriter {
diff --git a/src/deoptimizer/arm/deoptimizer-arm.cc b/src/deoptimizer/arm/deoptimizer-arm.cc
index de83ef1275e..407a3b36b51 100644
--- a/src/deoptimizer/arm/deoptimizer-arm.cc
+++ b/src/deoptimizer/arm/deoptimizer-arm.cc
@@ -11,7 +11,7 @@ namespace internal {
 // The deopt exit sizes below depend on the following IsolateData layout
 // guarantees:
 #define ASSERT_OFFSET(BuiltinName)                                       \
-  STATIC_ASSERT(IsolateData::builtin_tier0_entry_table_offset() +        \
+  static_assert(IsolateData::builtin_tier0_entry_table_offset() +        \
                     Builtins::ToInt(BuiltinName) * kSystemPointerSize <= \
                 0x1000)
 ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
diff --git a/src/deoptimizer/deoptimize-reason.h b/src/deoptimizer/deoptimize-reason.h
index 30c0991079c..9768ace7adb 100644
--- a/src/deoptimizer/deoptimize-reason.h
+++ b/src/deoptimizer/deoptimize-reason.h
@@ -81,7 +81,7 @@ constexpr DeoptimizeReason kFirstDeoptimizeReason =
     DeoptimizeReason::kArrayBufferWasDetached;
 constexpr DeoptimizeReason kLastDeoptimizeReason =
     DeoptimizeReason::kArrayLengthChanged;
-STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeReason) == 0);
+static_assert(static_cast<int>(kFirstDeoptimizeReason) == 0);
 constexpr int kDeoptimizeReasonCount =
     static_cast<int>(kLastDeoptimizeReason) + 1;
 
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index 710e00dee8e..6e244f164fc 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -269,7 +269,7 @@ class ActivationsFinder : public ThreadVisitor {
               code.GetSafepointEntry(isolate, it.frame()->pc());
           int trampoline_pc = safepoint.trampoline_pc();
           DCHECK_IMPLIES(code == topmost_, safe_to_deopt_);
-          STATIC_ASSERT(SafepointEntry::kNoTrampolinePC == -1);
+          static_assert(SafepointEntry::kNoTrampolinePC == -1);
           CHECK_GE(trampoline_pc, 0);
           // Replace the current pc on the stack with the trampoline.
           // TODO(v8:10026): avoid replacing a signed pointer.
diff --git a/src/deoptimizer/ppc/deoptimizer-ppc.cc b/src/deoptimizer/ppc/deoptimizer-ppc.cc
index ab1b122699e..63843b32618 100644
--- a/src/deoptimizer/ppc/deoptimizer-ppc.cc
+++ b/src/deoptimizer/ppc/deoptimizer-ppc.cc
@@ -11,7 +11,7 @@ namespace internal {
 // The deopt exit sizes below depend on the following IsolateData layout
 // guarantees:
 #define ASSERT_OFFSET(BuiltinName)                                       \
-  STATIC_ASSERT(IsolateData::builtin_tier0_entry_table_offset() +        \
+  static_assert(IsolateData::builtin_tier0_entry_table_offset() +        \
                     Builtins::ToInt(BuiltinName) * kSystemPointerSize <= \
                 0x1000)
 ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
diff --git a/src/deoptimizer/s390/deoptimizer-s390.cc b/src/deoptimizer/s390/deoptimizer-s390.cc
index 95028c0e656..92380d6bc80 100644
--- a/src/deoptimizer/s390/deoptimizer-s390.cc
+++ b/src/deoptimizer/s390/deoptimizer-s390.cc
@@ -11,7 +11,7 @@ namespace internal {
 // The deopt exit sizes below depend on the following IsolateData layout
 // guarantees:
 #define ASSERT_OFFSET(BuiltinName)                                       \
-  STATIC_ASSERT(IsolateData::builtin_tier0_entry_table_offset() +        \
+  static_assert(IsolateData::builtin_tier0_entry_table_offset() +        \
                     Builtins::ToInt(BuiltinName) * kSystemPointerSize <= \
                 0x1000)
 ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
diff --git a/src/deoptimizer/x64/deoptimizer-x64.cc b/src/deoptimizer/x64/deoptimizer-x64.cc
index 29c56e61ac6..47c8f60aee0 100644
--- a/src/deoptimizer/x64/deoptimizer-x64.cc
+++ b/src/deoptimizer/x64/deoptimizer-x64.cc
@@ -13,7 +13,7 @@ namespace internal {
 // The deopt exit sizes below depend on the following IsolateData layout
 // guarantees:
 #define ASSERT_OFFSET(BuiltinName)                                       \
-  STATIC_ASSERT(IsolateData::builtin_tier0_entry_table_offset() +        \
+  static_assert(IsolateData::builtin_tier0_entry_table_offset() +        \
                     Builtins::ToInt(BuiltinName) * kSystemPointerSize <= \
                 0x7F)
 ASSERT_OFFSET(Builtin::kDeoptimizationEntry_Eager);
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index 47ece61828b..7507adb4c04 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -878,7 +878,7 @@ void JSFunction::JSFunctionVerify(Isolate* isolate) {
 
   // This assertion exists to encourage updating this verification function if
   // new fields are added in the Torque class layout definition.
-  STATIC_ASSERT(JSFunction::TorqueGeneratedClass::kHeaderSize ==
+  static_assert(JSFunction::TorqueGeneratedClass::kHeaderSize ==
                 8 * kTaggedSize);
 
   JSFunctionOrBoundFunctionOrWrappedFunctionVerify(isolate);
@@ -1797,7 +1797,7 @@ void DataHandler::DataHandlerVerify(Isolate* isolate) {
 
   // This assertion exists to encourage updating this verification function if
   // new fields are added in the Torque class layout definition.
-  STATIC_ASSERT(DataHandler::kHeaderSize == 6 * kTaggedSize);
+  static_assert(DataHandler::kHeaderSize == 6 * kTaggedSize);
 
   StructVerify(isolate);
   CHECK(IsDataHandler());
diff --git a/src/diagnostics/unwinding-info-win64.cc b/src/diagnostics/unwinding-info-win64.cc
index d50767421ab..4a70a284785 100644
--- a/src/diagnostics/unwinding-info-win64.cc
+++ b/src/diagnostics/unwinding-info-win64.cc
@@ -287,7 +287,7 @@ struct V8UnwindData {
     // error is acceptable when the unwinding info for the caller frame also
     // depends on fp rather than sp, as is the case for V8 builtins and runtime-
     // generated code.
-    STATIC_ASSERT(kNumberOfUnwindCodeWords >= 1);
+    static_assert(kNumberOfUnwindCodeWords >= 1);
     unwind_codes[0] = Combine8BitUnwindCodes(
         OpSetFp, MakeOpSaveFpLrX(-CommonFrameConstants::kCallerSPOffset),
         OpEnd);
@@ -346,7 +346,7 @@ std::vector<uint8_t> GetUnwindInfoForBuiltinFunction(
 
   if (fp_adjustment.IsDefault()) {
     // One code word is plenty.
-    STATIC_ASSERT(kDefaultNumberOfUnwindCodeWords <
+    static_assert(kDefaultNumberOfUnwindCodeWords <
                   kMaxNumberOfUnwindCodeWords);
     xdata.unwind_info.CodeWords = kDefaultNumberOfUnwindCodeWords;
   } else {
diff --git a/src/execution/arm/simulator-arm.cc b/src/execution/arm/simulator-arm.cc
index aefb9820ea3..a8fcfcb448f 100644
--- a/src/execution/arm/simulator-arm.cc
+++ b/src/execution/arm/simulator-arm.cc
@@ -1683,7 +1683,7 @@ void Simulator::SoftwareInterrupt(Instruction* instr) {
       int32_t arg17 = stack_pointer[13];
       int32_t arg18 = stack_pointer[14];
       int32_t arg19 = stack_pointer[15];
-      STATIC_ASSERT(kMaxCParameters == 20);
+      static_assert(kMaxCParameters == 20);
 
       bool fp_call =
           (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
diff --git a/src/execution/arm64/frame-constants-arm64.cc b/src/execution/arm64/frame-constants-arm64.cc
index 07aebe48675..96f6f25e757 100644
--- a/src/execution/arm64/frame-constants-arm64.cc
+++ b/src/execution/arm64/frame-constants-arm64.cc
@@ -20,7 +20,7 @@ Register JavaScriptFrame::context_register() { return cp; }
 Register JavaScriptFrame::constant_pool_pointer_register() { UNREACHABLE(); }
 
 int UnoptimizedFrameConstants::RegisterStackSlotCount(int register_count) {
-  STATIC_ASSERT(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
+  static_assert(InterpreterFrameConstants::kFixedFrameSize % 16 == 8);
   // Interpreter frame header size is not 16-bytes aligned, so we'll need at
   // least one register slot to make the frame a multiple of 16 bytes. The code
   // below is equivalent to "RoundUp(register_count - 1, 2) + 1".
diff --git a/src/execution/arm64/simulator-arm64.cc b/src/execution/arm64/simulator-arm64.cc
index 23e3e17678e..5a4f7a7e856 100644
--- a/src/execution/arm64/simulator-arm64.cc
+++ b/src/execution/arm64/simulator-arm64.cc
@@ -678,7 +678,7 @@ void Simulator::DoRuntimeCall(Instruction* instr) {
   const int64_t arg17 = stack_pointer[9];
   const int64_t arg18 = stack_pointer[10];
   const int64_t arg19 = stack_pointer[11];
-  STATIC_ASSERT(kMaxCParameters == 20);
+  static_assert(kMaxCParameters == 20);
 
   switch (redirection->type()) {
     default:
@@ -6050,7 +6050,7 @@ void Simulator::DoPrintf(Instruction* instr) {
   // Read the arguments encoded inline in the instruction stream.
   uint32_t arg_count;
   uint32_t arg_pattern_list;
-  STATIC_ASSERT(sizeof(*instr) == 1);
+  static_assert(sizeof(*instr) == 1);
   memcpy(&arg_count, instr + kPrintfArgCountOffset, sizeof(arg_count));
   memcpy(&arg_pattern_list, instr + kPrintfArgPatternListOffset,
          sizeof(arg_pattern_list));
diff --git a/src/execution/arm64/simulator-arm64.h b/src/execution/arm64/simulator-arm64.h
index e06653c1b9b..44e6abf77af 100644
--- a/src/execution/arm64/simulator-arm64.h
+++ b/src/execution/arm64/simulator-arm64.h
@@ -761,7 +761,7 @@ class Simulator : public DecoderVisitor, public SimulatorBase {
   // Simulation helpers.
   template <typename T>
   void set_pc(T new_pc) {
-    STATIC_ASSERT(sizeof(T) == sizeof(pc_));
+    static_assert(sizeof(T) == sizeof(pc_));
     memcpy(&pc_, &new_pc, sizeof(T));
     pc_modified_ = true;
   }
@@ -1114,7 +1114,7 @@ class Simulator : public DecoderVisitor, public SimulatorBase {
   // As above, but don't automatically log the register update.
   template <typename T>
   void set_vreg_no_log(unsigned code, T value) {
-    STATIC_ASSERT((sizeof(value) == kBRegSize) ||
+    static_assert((sizeof(value) == kBRegSize) ||
                   (sizeof(value) == kHRegSize) ||
                   (sizeof(value) == kSRegSize) ||
                   (sizeof(value) == kDRegSize) || (sizeof(value) == kQRegSize));
@@ -1518,7 +1518,7 @@ class Simulator : public DecoderVisitor, public SimulatorBase {
   template <typename T, typename A>
   T MemoryRead(A address) {
     T value;
-    STATIC_ASSERT((sizeof(value) == 1) || (sizeof(value) == 2) ||
+    static_assert((sizeof(value) == 1) || (sizeof(value) == 2) ||
                   (sizeof(value) == 4) || (sizeof(value) == 8) ||
                   (sizeof(value) == 16));
     memcpy(&value, reinterpret_cast<const void*>(address), sizeof(value));
@@ -1528,7 +1528,7 @@ class Simulator : public DecoderVisitor, public SimulatorBase {
   // Memory write helpers.
   template <typename T, typename A>
   void MemoryWrite(A address, T value) {
-    STATIC_ASSERT((sizeof(value) == 1) || (sizeof(value) == 2) ||
+    static_assert((sizeof(value) == 1) || (sizeof(value) == 2) ||
                   (sizeof(value) == 4) || (sizeof(value) == 8) ||
                   (sizeof(value) == 16));
     memcpy(reinterpret_cast<void*>(address), &value, sizeof(value));
diff --git a/src/execution/execution.cc b/src/execution/execution.cc
index 9372a9ab6c2..6cc635b0b72 100644
--- a/src/execution/execution.cc
+++ b/src/execution/execution.cc
@@ -603,11 +603,11 @@ struct StackHandlerMarker {
   Address next;
   Address padding;
 };
-STATIC_ASSERT(offsetof(StackHandlerMarker, next) ==
+static_assert(offsetof(StackHandlerMarker, next) ==
               StackHandlerConstants::kNextOffset);
-STATIC_ASSERT(offsetof(StackHandlerMarker, padding) ==
+static_assert(offsetof(StackHandlerMarker, padding) ==
               StackHandlerConstants::kPaddingOffset);
-STATIC_ASSERT(sizeof(StackHandlerMarker) == StackHandlerConstants::kSize);
+static_assert(sizeof(StackHandlerMarker) == StackHandlerConstants::kSize);
 
 #if V8_ENABLE_WEBASSEMBLY
 void Execution::CallWasm(Isolate* isolate, Handle<CodeT> wrapper_code,
@@ -641,10 +641,10 @@ void Execution::CallWasm(Isolate* isolate, Handle<CodeT> wrapper_code,
 
   {
     RCS_SCOPE(isolate, RuntimeCallCounterId::kJS_Execution);
-    STATIC_ASSERT(compiler::CWasmEntryParameters::kCodeEntry == 0);
-    STATIC_ASSERT(compiler::CWasmEntryParameters::kObjectRef == 1);
-    STATIC_ASSERT(compiler::CWasmEntryParameters::kArgumentsBuffer == 2);
-    STATIC_ASSERT(compiler::CWasmEntryParameters::kCEntryFp == 3);
+    static_assert(compiler::CWasmEntryParameters::kCodeEntry == 0);
+    static_assert(compiler::CWasmEntryParameters::kObjectRef == 1);
+    static_assert(compiler::CWasmEntryParameters::kArgumentsBuffer == 2);
+    static_assert(compiler::CWasmEntryParameters::kCEntryFp == 3);
     Address result = stub_entry.Call(wasm_call_target, object_ref->ptr(),
                                      packed_args, saved_c_entry_fp);
     if (result != kNullAddress) {
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index 6e76a92d354..14bc3cf0d09 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -420,7 +420,7 @@ SafeStackFrameIterator::SafeStackFrameIterator(Isolate* isolate, Address pc,
     // we check only that kMarkerOffset is within the stack bounds and do
     // compile time check that kContextOffset slot is pushed on the stack before
     // kMarkerOffset.
-    STATIC_ASSERT(StandardFrameConstants::kFunctionOffset <
+    static_assert(StandardFrameConstants::kFunctionOffset <
                   StandardFrameConstants::kContextOffset);
     Address frame_marker = fp + StandardFrameConstants::kFunctionOffset;
     if (IsValidStackAddress(frame_marker)) {
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 1b0c2db112d..f63a1a05ad3 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -143,8 +143,8 @@ class StackFrame {
     INNER_JSENTRY_FRAME = (0 << kSmiTagSize) | kSmiTag,
     OUTERMOST_JSENTRY_FRAME = (1 << kSmiTagSize) | kSmiTag
   };
-  STATIC_ASSERT((INNER_JSENTRY_FRAME & kHeapObjectTagMask) != kHeapObjectTag);
-  STATIC_ASSERT((OUTERMOST_JSENTRY_FRAME & kHeapObjectTagMask) !=
+  static_assert((INNER_JSENTRY_FRAME & kHeapObjectTagMask) != kHeapObjectTag);
+  static_assert((OUTERMOST_JSENTRY_FRAME & kHeapObjectTagMask) !=
                 kHeapObjectTag);
 
   struct State {
@@ -216,7 +216,7 @@ class StackFrame {
   bool is_exit() const { return type() == EXIT; }
   bool is_optimized() const { return type() == OPTIMIZED; }
   bool is_unoptimized() const {
-    STATIC_ASSERT(BASELINE == INTERPRETED + 1);
+    static_assert(BASELINE == INTERPRETED + 1);
     return base::IsInRange(type(), INTERPRETED, BASELINE);
   }
   bool is_interpreted() const { return type() == INTERPRETED; }
@@ -245,9 +245,9 @@ class StackFrame {
   bool is_builtin_exit() const { return type() == BUILTIN_EXIT; }
 
   static bool IsJavaScript(Type t) {
-    STATIC_ASSERT(INTERPRETED + 1 == BASELINE);
-    STATIC_ASSERT(BASELINE + 1 == MAGLEV);
-    STATIC_ASSERT(MAGLEV + 1 == OPTIMIZED);
+    static_assert(INTERPRETED + 1 == BASELINE);
+    static_assert(BASELINE + 1 == MAGLEV);
+    static_assert(MAGLEV + 1 == OPTIMIZED);
     return t >= INTERPRETED && t <= OPTIMIZED;
   }
   bool is_java_script() const { return IsJavaScript(type()); }
diff --git a/src/execution/isolate-data.h b/src/execution/isolate-data.h
index 671492bb959..35309558b9a 100644
--- a/src/execution/isolate-data.h
+++ b/src/execution/isolate-data.h
@@ -137,7 +137,7 @@ class IsolateData final {
   // it's the case then the value can be accessed indirectly through the root
   // register.
   bool contains(Address address) const {
-    STATIC_ASSERT(std::is_unsigned<Address>::value);
+    static_assert(std::is_unsigned<Address>::value);
     Address start = reinterpret_cast<Address>(this);
     return (address - start) < sizeof(*this);
   }
@@ -224,7 +224,7 @@ class IsolateData final {
   // int64_t fields.
   // In order to avoid dealing with zero-size arrays the padding size is always
   // in the range [8, 15).
-  STATIC_ASSERT(kPaddingOffsetEnd + 1 - kPaddingOffset >= 8);
+  static_assert(kPaddingOffsetEnd + 1 - kPaddingOffset >= 8);
   char padding_[kPaddingOffsetEnd + 1 - kPaddingOffset];
 
   V8_INLINE static void AssertPredictableLayout();
@@ -240,15 +240,15 @@ class IsolateData final {
 // issues because of different compilers used for snapshot generator and
 // actual V8 code.
 void IsolateData::AssertPredictableLayout() {
-  STATIC_ASSERT(std::is_standard_layout<RootsTable>::value);
-  STATIC_ASSERT(std::is_standard_layout<ThreadLocalTop>::value);
-  STATIC_ASSERT(std::is_standard_layout<ExternalReferenceTable>::value);
-  STATIC_ASSERT(std::is_standard_layout<IsolateData>::value);
+  static_assert(std::is_standard_layout<RootsTable>::value);
+  static_assert(std::is_standard_layout<ThreadLocalTop>::value);
+  static_assert(std::is_standard_layout<ExternalReferenceTable>::value);
+  static_assert(std::is_standard_layout<IsolateData>::value);
 #define V(Offset, Size, Name) \
-  STATIC_ASSERT(offsetof(IsolateData, Name##_) == Offset);
+  static_assert(offsetof(IsolateData, Name##_) == Offset);
   ISOLATE_DATA_FIELDS(V)
 #undef V
-  STATIC_ASSERT(sizeof(IsolateData) == IsolateData::kSize);
+  static_assert(sizeof(IsolateData) == IsolateData::kSize);
 }
 
 #undef ISOLATE_DATA_FIELDS_SANDBOXED_EXTERNAL_POINTERS
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 779a5315353..390f7e006e0 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -377,7 +377,7 @@ uint32_t Isolate::CurrentEmbeddedBlobDataSize() {
 // static
 base::AddressRegion Isolate::GetShortBuiltinsCallRegion() {
   // Update calculations below if the assert fails.
-  STATIC_ASSERT(kMaxPCRelativeCodeRangeInMB <= 4096);
+  static_assert(kMaxPCRelativeCodeRangeInMB <= 4096);
   if (kMaxPCRelativeCodeRangeInMB == 0) {
     // Return empty region if pc-relative calls/jumps are not supported.
     return base::AddressRegion(kNullAddress, 0);
@@ -430,16 +430,16 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     // they change when creating the off-heap trampolines. Other data fields
     // must remain the same.
 #ifdef V8_EXTERNAL_CODE_SPACE
-    STATIC_ASSERT(Code::kMainCageBaseUpper32BitsOffset == Code::kDataStart);
-    STATIC_ASSERT(Code::kInstructionSizeOffset ==
+    static_assert(Code::kMainCageBaseUpper32BitsOffset == Code::kDataStart);
+    static_assert(Code::kInstructionSizeOffset ==
                   Code::kMainCageBaseUpper32BitsOffsetEnd + 1);
 #else
-    STATIC_ASSERT(Code::kInstructionSizeOffset == Code::kDataStart);
+    static_assert(Code::kInstructionSizeOffset == Code::kDataStart);
 #endif  // V8_EXTERNAL_CODE_SPACE
-    STATIC_ASSERT(Code::kMetadataSizeOffset ==
+    static_assert(Code::kMetadataSizeOffset ==
                   Code::kInstructionSizeOffsetEnd + 1);
-    STATIC_ASSERT(Code::kFlagsOffset == Code::kMetadataSizeOffsetEnd + 1);
-    STATIC_ASSERT(Code::kBuiltinIndexOffset == Code::kFlagsOffsetEnd + 1);
+    static_assert(Code::kFlagsOffset == Code::kMetadataSizeOffsetEnd + 1);
+    static_assert(Code::kBuiltinIndexOffset == Code::kFlagsOffsetEnd + 1);
     static constexpr int kStartOffset = Code::kBuiltinIndexOffset;
 
     for (int j = kStartOffset; j < Code::kUnalignedHeaderSize; j++) {
@@ -3330,10 +3330,10 @@ void Isolate::CheckIsolateLayout() {
   CHECK_EQ(static_cast<int>(OFFSET_OF(Isolate, isolate_data_.roots_table_)),
            Internals::kIsolateRootsOffset);
 
-  STATIC_ASSERT(Internals::kStackGuardSize == sizeof(StackGuard));
-  STATIC_ASSERT(Internals::kBuiltinTier0TableSize ==
+  static_assert(Internals::kStackGuardSize == sizeof(StackGuard));
+  static_assert(Internals::kBuiltinTier0TableSize ==
                 Builtins::kBuiltinTier0Count * kSystemPointerSize);
-  STATIC_ASSERT(Internals::kBuiltinTier0EntryTableSize ==
+  static_assert(Internals::kBuiltinTier0EntryTableSize ==
                 Builtins::kBuiltinTier0Count * kSystemPointerSize);
 
 #ifdef V8_SANDBOXED_EXTERNAL_POINTERS
@@ -3679,7 +3679,7 @@ void CreateOffHeapTrampolines(Isolate* isolate) {
 
   EmbeddedData d = EmbeddedData::FromBlob(isolate);
 
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Address instruction_start = d.InstructionStartOfBuiltin(builtin);
diff --git a/src/execution/loong64/simulator-loong64.cc b/src/execution/loong64/simulator-loong64.cc
index 31f511fc695..a5ddad99a08 100644
--- a/src/execution/loong64/simulator-loong64.cc
+++ b/src/execution/loong64/simulator-loong64.cc
@@ -2213,7 +2213,7 @@ void Simulator::SoftwareInterrupt() {
     int64_t arg17 = stack_pointer[9];
     int64_t arg18 = stack_pointer[10];
     int64_t arg19 = stack_pointer[11];
-    STATIC_ASSERT(kMaxCParameters == 20);
+    static_assert(kMaxCParameters == 20);
 
     bool fp_call =
         (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
diff --git a/src/execution/mips/simulator-mips.cc b/src/execution/mips/simulator-mips.cc
index 7aa5e00fbe9..cc195178b1a 100644
--- a/src/execution/mips/simulator-mips.cc
+++ b/src/execution/mips/simulator-mips.cc
@@ -2245,7 +2245,7 @@ void Simulator::SoftwareInterrupt() {
     int32_t arg17 = stack_pointer[17];
     int32_t arg18 = stack_pointer[18];
     int32_t arg19 = stack_pointer[19];
-    STATIC_ASSERT(kMaxCParameters == 20);
+    static_assert(kMaxCParameters == 20);
 
     bool fp_call =
         (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
diff --git a/src/execution/mips64/simulator-mips64.cc b/src/execution/mips64/simulator-mips64.cc
index 6abe0a74f7b..f6577fb9f61 100644
--- a/src/execution/mips64/simulator-mips64.cc
+++ b/src/execution/mips64/simulator-mips64.cc
@@ -2408,7 +2408,7 @@ void Simulator::SoftwareInterrupt() {
     int64_t arg17 = stack_pointer[9];
     int64_t arg18 = stack_pointer[10];
     int64_t arg19 = stack_pointer[11];
-    STATIC_ASSERT(kMaxCParameters == 20);
+    static_assert(kMaxCParameters == 20);
 
     bool fp_call =
         (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
diff --git a/src/execution/ppc/simulator-ppc.cc b/src/execution/ppc/simulator-ppc.cc
index 134993b39e8..c178bc0e03a 100644
--- a/src/execution/ppc/simulator-ppc.cc
+++ b/src/execution/ppc/simulator-ppc.cc
@@ -1001,8 +1001,8 @@ void Simulator::SoftwareInterrupt(Instruction* instr) {
       for (int i = kRegisterArgCount, j = 0; i < kArgCount; i++, j++) {
         arg[i] = stack_pointer[kStackFrameExtraParamSlot + j];
       }
-      STATIC_ASSERT(kArgCount == kRegisterArgCount + 12);
-      STATIC_ASSERT(kMaxCParameters == kArgCount);
+      static_assert(kArgCount == kRegisterArgCount + 12);
+      static_assert(kMaxCParameters == kArgCount);
       bool fp_call =
           (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
           (redirection->type() == ExternalReference::BUILTIN_COMPARE_CALL) ||
diff --git a/src/execution/protectors.cc b/src/execution/protectors.cc
index cbc6ce0b28e..71d9fa9735b 100644
--- a/src/execution/protectors.cc
+++ b/src/execution/protectors.cc
@@ -38,7 +38,7 @@ void TraceProtectorInvalidation(const char* protector_name) {
 // strictly needed but clarifies the intent of the static assert.
 constexpr bool IsDefined(v8::Isolate::UseCounterFeature) { return true; }
 #define V(Name, ...) \
-  STATIC_ASSERT(IsDefined(v8::Isolate::kInvalidated##Name##Protector));
+  static_assert(IsDefined(v8::Isolate::kInvalidated##Name##Protector));
 
 DECLARED_PROTECTORS_ON_ISOLATE(V)
 #undef V
diff --git a/src/execution/riscv64/simulator-riscv64.cc b/src/execution/riscv64/simulator-riscv64.cc
index be3fafc7d1b..8ae38ca6ae5 100644
--- a/src/execution/riscv64/simulator-riscv64.cc
+++ b/src/execution/riscv64/simulator-riscv64.cc
@@ -2792,7 +2792,7 @@ void Simulator::SoftwareInterrupt() {
     const int64_t arg17 = stack_pointer[9];
     const int64_t arg18 = stack_pointer[10];
     const int64_t arg19 = stack_pointer[11];
-    STATIC_ASSERT(kMaxCParameters == 20);
+    static_assert(kMaxCParameters == 20);
 
     bool fp_call =
         (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
diff --git a/src/execution/riscv64/simulator-riscv64.h b/src/execution/riscv64/simulator-riscv64.h
index 56b8d4673a5..90235ef9f23 100644
--- a/src/execution/riscv64/simulator-riscv64.h
+++ b/src/execution/riscv64/simulator-riscv64.h
@@ -753,7 +753,7 @@ class Simulator : public SimulatorBase {
 
   template <typename T, typename Func>
   inline T CanonicalizeFPUOpFMA(Func fn, T dst, T src1, T src2) {
-    STATIC_ASSERT(std::is_floating_point<T>::value);
+    static_assert(std::is_floating_point<T>::value);
     auto alu_out = fn(dst, src1, src2);
     // if any input or result is NaN, the result is quiet_NaN
     if (std::isnan(alu_out) || std::isnan(src1) || std::isnan(src2) ||
@@ -768,7 +768,7 @@ class Simulator : public SimulatorBase {
 
   template <typename T, typename Func>
   inline T CanonicalizeFPUOp3(Func fn) {
-    STATIC_ASSERT(std::is_floating_point<T>::value);
+    static_assert(std::is_floating_point<T>::value);
     T src1 = std::is_same<float, T>::value ? frs1() : drs1();
     T src2 = std::is_same<float, T>::value ? frs2() : drs2();
     T src3 = std::is_same<float, T>::value ? frs3() : drs3();
@@ -786,7 +786,7 @@ class Simulator : public SimulatorBase {
 
   template <typename T, typename Func>
   inline T CanonicalizeFPUOp2(Func fn) {
-    STATIC_ASSERT(std::is_floating_point<T>::value);
+    static_assert(std::is_floating_point<T>::value);
     T src1 = std::is_same<float, T>::value ? frs1() : drs1();
     T src2 = std::is_same<float, T>::value ? frs2() : drs2();
     auto alu_out = fn(src1, src2);
@@ -802,7 +802,7 @@ class Simulator : public SimulatorBase {
 
   template <typename T, typename Func>
   inline T CanonicalizeFPUOp1(Func fn) {
-    STATIC_ASSERT(std::is_floating_point<T>::value);
+    static_assert(std::is_floating_point<T>::value);
     T src1 = std::is_same<float, T>::value ? frs1() : drs1();
     auto alu_out = fn(src1);
     // if any input or result is NaN, the result is quiet_NaN
diff --git a/src/execution/s390/simulator-s390.cc b/src/execution/s390/simulator-s390.cc
index aa331ec9d51..4e588741267 100644
--- a/src/execution/s390/simulator-s390.cc
+++ b/src/execution/s390/simulator-s390.cc
@@ -2000,8 +2000,8 @@ void Simulator::SoftwareInterrupt(Instruction* instr) {
             stack_pointer[(kCalleeRegisterSaveAreaSize / kSystemPointerSize) +
                           (i - kRegisterArgCount)];
       }
-      STATIC_ASSERT(kArgCount == kRegisterArgCount + 15);
-      STATIC_ASSERT(kMaxCParameters == kArgCount);
+      static_assert(kArgCount == kRegisterArgCount + 15);
+      static_assert(kMaxCParameters == kArgCount);
       bool fp_call =
           (redirection->type() == ExternalReference::BUILTIN_FP_FP_CALL) ||
           (redirection->type() == ExternalReference::BUILTIN_COMPARE_CALL) ||
diff --git a/src/execution/stack-guard.h b/src/execution/stack-guard.h
index 4c70ab15988..5076f47a001 100644
--- a/src/execution/stack-guard.h
+++ b/src/execution/stack-guard.h
@@ -191,7 +191,7 @@ class V8_EXPORT_PRIVATE V8_NODISCARD StackGuard final {
   friend class InterruptsScope;
 };
 
-STATIC_ASSERT(StackGuard::kSizeInBytes == sizeof(StackGuard));
+static_assert(StackGuard::kSizeInBytes == sizeof(StackGuard));
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/execution/thread-local-top.h b/src/execution/thread-local-top.h
index 466e96d91ca..989c817f318 100644
--- a/src/execution/thread-local-top.h
+++ b/src/execution/thread-local-top.h
@@ -98,7 +98,7 @@ class ThreadLocalTop {
   // be cleaner to make it an "Address raw_context_", and construct a Context
   // object in the getter. Same for {pending_handler_context_} below. In the
   // meantime, assert that the memory layout is the same.
-  STATIC_ASSERT(sizeof(Context) == kSystemPointerSize);
+  static_assert(sizeof(Context) == kSystemPointerSize);
   Context context_;
   std::atomic<ThreadId> thread_id_;
   Object pending_exception_;
diff --git a/src/execution/tiering-manager.cc b/src/execution/tiering-manager.cc
index dfd4f965da8..abf0af9a16a 100644
--- a/src/execution/tiering-manager.cc
+++ b/src/execution/tiering-manager.cc
@@ -94,7 +94,7 @@ class OptimizationDecision {
         concurrency_mode(concurrency_mode) {}
 };
 // Since we pass by value:
-STATIC_ASSERT(sizeof(OptimizationDecision) <= kInt32Size);
+static_assert(sizeof(OptimizationDecision) <= kInt32Size);
 
 namespace {
 
diff --git a/src/handles/global-handles.cc b/src/handles/global-handles.cc
index 47a70487216..cdb62a49a28 100644
--- a/src/handles/global-handles.cc
+++ b/src/handles/global-handles.cc
@@ -428,9 +428,9 @@ class GlobalHandles::Node final : public NodeBase<GlobalHandles::Node> {
   };
 
   Node() {
-    STATIC_ASSERT(static_cast<int>(NodeState::kMask) ==
+    static_assert(static_cast<int>(NodeState::kMask) ==
                   Internals::kNodeStateMask);
-    STATIC_ASSERT(WEAK == Internals::kNodeStateIsWeakValue);
+    static_assert(WEAK == Internals::kNodeStateIsWeakValue);
     set_in_young_list(false);
   }
 
diff --git a/src/heap/allocation-result.h b/src/heap/allocation-result.h
index 04a618995b4..df723a77d7d 100644
--- a/src/heap/allocation-result.h
+++ b/src/heap/allocation-result.h
@@ -66,7 +66,7 @@ class AllocationResult final {
   HeapObject object_;
 };
 
-STATIC_ASSERT(sizeof(AllocationResult) == kSystemPointerSize);
+static_assert(sizeof(AllocationResult) == kSystemPointerSize);
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/heap/basic-memory-chunk.cc b/src/heap/basic-memory-chunk.cc
index a456d98cd73..e076d6bc81a 100644
--- a/src/heap/basic-memory-chunk.cc
+++ b/src/heap/basic-memory-chunk.cc
@@ -15,19 +15,19 @@ namespace v8 {
 namespace internal {
 
 // Verify write barrier offsets match the the real offsets.
-STATIC_ASSERT(BasicMemoryChunk::Flag::IS_EXECUTABLE ==
+static_assert(BasicMemoryChunk::Flag::IS_EXECUTABLE ==
               heap_internals::MemoryChunk::kIsExecutableBit);
-STATIC_ASSERT(BasicMemoryChunk::Flag::INCREMENTAL_MARKING ==
+static_assert(BasicMemoryChunk::Flag::INCREMENTAL_MARKING ==
               heap_internals::MemoryChunk::kMarkingBit);
-STATIC_ASSERT(BasicMemoryChunk::Flag::FROM_PAGE ==
+static_assert(BasicMemoryChunk::Flag::FROM_PAGE ==
               heap_internals::MemoryChunk::kFromPageBit);
-STATIC_ASSERT(BasicMemoryChunk::Flag::TO_PAGE ==
+static_assert(BasicMemoryChunk::Flag::TO_PAGE ==
               heap_internals::MemoryChunk::kToPageBit);
-STATIC_ASSERT(BasicMemoryChunk::Flag::READ_ONLY_HEAP ==
+static_assert(BasicMemoryChunk::Flag::READ_ONLY_HEAP ==
               heap_internals::MemoryChunk::kReadOnlySpaceBit);
-STATIC_ASSERT(BasicMemoryChunk::kFlagsOffset ==
+static_assert(BasicMemoryChunk::kFlagsOffset ==
               heap_internals::MemoryChunk::kFlagsOffset);
-STATIC_ASSERT(BasicMemoryChunk::kHeapOffset ==
+static_assert(BasicMemoryChunk::kHeapOffset ==
               heap_internals::MemoryChunk::kHeapOffset);
 
 // static
@@ -84,29 +84,29 @@ void BasicMemoryChunk::SynchronizedHeapLoad() const {
 
 class BasicMemoryChunkValidator {
   // Computed offsets should match the compiler generated ones.
-  STATIC_ASSERT(BasicMemoryChunk::kSizeOffset ==
+  static_assert(BasicMemoryChunk::kSizeOffset ==
                 offsetof(BasicMemoryChunk, size_));
-  STATIC_ASSERT(BasicMemoryChunk::kFlagsOffset ==
+  static_assert(BasicMemoryChunk::kFlagsOffset ==
                 offsetof(BasicMemoryChunk, main_thread_flags_));
-  STATIC_ASSERT(BasicMemoryChunk::kHeapOffset ==
+  static_assert(BasicMemoryChunk::kHeapOffset ==
                 offsetof(BasicMemoryChunk, heap_));
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, size_) ==
+  static_assert(offsetof(BasicMemoryChunk, size_) ==
                 MemoryChunkLayout::kSizeOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, heap_) ==
+  static_assert(offsetof(BasicMemoryChunk, heap_) ==
                 MemoryChunkLayout::kHeapOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, area_start_) ==
+  static_assert(offsetof(BasicMemoryChunk, area_start_) ==
                 MemoryChunkLayout::kAreaStartOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, area_end_) ==
+  static_assert(offsetof(BasicMemoryChunk, area_end_) ==
                 MemoryChunkLayout::kAreaEndOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, allocated_bytes_) ==
+  static_assert(offsetof(BasicMemoryChunk, allocated_bytes_) ==
                 MemoryChunkLayout::kAllocatedBytesOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, wasted_memory_) ==
+  static_assert(offsetof(BasicMemoryChunk, wasted_memory_) ==
                 MemoryChunkLayout::kWastedMemoryOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, high_water_mark_) ==
+  static_assert(offsetof(BasicMemoryChunk, high_water_mark_) ==
                 MemoryChunkLayout::kHighWaterMarkOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, owner_) ==
+  static_assert(offsetof(BasicMemoryChunk, owner_) ==
                 MemoryChunkLayout::kOwnerOffset);
-  STATIC_ASSERT(offsetof(BasicMemoryChunk, reservation_) ==
+  static_assert(offsetof(BasicMemoryChunk, reservation_) ==
                 MemoryChunkLayout::kReservationOffset);
 };
 
diff --git a/src/heap/basic-memory-chunk.h b/src/heap/basic-memory-chunk.h
index 66477e89b1f..abd51302773 100644
--- a/src/heap/basic-memory-chunk.h
+++ b/src/heap/basic-memory-chunk.h
@@ -378,7 +378,7 @@ class BasicMemoryChunk {
 
 DEFINE_OPERATORS_FOR_FLAGS(BasicMemoryChunk::MainThreadFlags)
 
-STATIC_ASSERT(std::is_standard_layout<BasicMemoryChunk>::value);
+static_assert(std::is_standard_layout<BasicMemoryChunk>::value);
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/heap/cppgc/allocation.cc b/src/heap/cppgc/allocation.cc
index 9a29ba161ca..1282c3a164e 100644
--- a/src/heap/cppgc/allocation.cc
+++ b/src/heap/cppgc/allocation.cc
@@ -18,7 +18,7 @@
 namespace cppgc {
 namespace internal {
 
-STATIC_ASSERT(api_constants::kLargeObjectSizeThreshold ==
+static_assert(api_constants::kLargeObjectSizeThreshold ==
               kLargeObjectSizeThreshold);
 
 #if !(defined(V8_TARGET_ARCH_32_BIT) && defined(V8_CC_GNU))
diff --git a/src/heap/cppgc/caged-heap.cc b/src/heap/cppgc/caged-heap.cc
index 4c4b06c1bd0..c1ffd8c748b 100644
--- a/src/heap/cppgc/caged-heap.cc
+++ b/src/heap/cppgc/caged-heap.cc
@@ -21,9 +21,9 @@
 namespace cppgc {
 namespace internal {
 
-STATIC_ASSERT(api_constants::kCagedHeapReservationSize ==
+static_assert(api_constants::kCagedHeapReservationSize ==
               kCagedHeapReservationSize);
-STATIC_ASSERT(api_constants::kCagedHeapReservationAlignment ==
+static_assert(api_constants::kCagedHeapReservationAlignment ==
               kCagedHeapReservationAlignment);
 
 namespace {
diff --git a/src/heap/cppgc/heap-object-header.cc b/src/heap/cppgc/heap-object-header.cc
index fd4422144f2..4b34b4c7adc 100644
--- a/src/heap/cppgc/heap-object-header.cc
+++ b/src/heap/cppgc/heap-object-header.cc
@@ -14,12 +14,12 @@
 namespace cppgc {
 namespace internal {
 
-STATIC_ASSERT((kAllocationGranularity % sizeof(HeapObjectHeader)) == 0);
+static_assert((kAllocationGranularity % sizeof(HeapObjectHeader)) == 0);
 
 void HeapObjectHeader::CheckApiConstants() {
-  STATIC_ASSERT(api_constants::kFullyConstructedBitMask ==
+  static_assert(api_constants::kFullyConstructedBitMask ==
                 FullyConstructedField::kMask);
-  STATIC_ASSERT(api_constants::kFullyConstructedBitFieldOffsetFromPayload ==
+  static_assert(api_constants::kFullyConstructedBitFieldOffsetFromPayload ==
                 (sizeof(encoded_high_) + sizeof(encoded_low_)));
 }
 
diff --git a/src/heap/cppgc/heap-page.cc b/src/heap/cppgc/heap-page.cc
index c7af4e971ee..ae860027f14 100644
--- a/src/heap/cppgc/heap-page.cc
+++ b/src/heap/cppgc/heap-page.cc
@@ -219,9 +219,9 @@ LargePage* LargePage::Create(PageBackend& page_backend, LargePageSpace& space,
                              size_t size) {
   // Ensure that the API-provided alignment guarantees does not violate the
   // internally guaranteed alignment of large page allocations.
-  STATIC_ASSERT(kGuaranteedObjectAlignment <=
+  static_assert(kGuaranteedObjectAlignment <=
                 api_constants::kMaxSupportedAlignment);
-  STATIC_ASSERT(
+  static_assert(
       api_constants::kMaxSupportedAlignment % kGuaranteedObjectAlignment == 0);
 
   DCHECK_LE(kLargeObjectSizeThreshold, size);
diff --git a/src/heap/cppgc/object-allocator.h b/src/heap/cppgc/object-allocator.h
index 4522f94785b..13ac85d57fc 100644
--- a/src/heap/cppgc/object-allocator.h
+++ b/src/heap/cppgc/object-allocator.h
@@ -143,10 +143,10 @@ void* ObjectAllocator::AllocateObjectOnSpace(NormalPageSpace& space,
   // double-world alignment (8 bytes on 32bit and 16 bytes on 64bit
   // architectures). This is enforced on the public API via static_asserts
   // against alignof(T).
-  STATIC_ASSERT(2 * kAllocationGranularity ==
+  static_assert(2 * kAllocationGranularity ==
                 api_constants::kMaxSupportedAlignment);
-  STATIC_ASSERT(kAllocationGranularity == sizeof(HeapObjectHeader));
-  STATIC_ASSERT(kAllocationGranularity ==
+  static_assert(kAllocationGranularity == sizeof(HeapObjectHeader));
+  static_assert(kAllocationGranularity ==
                 api_constants::kAllocationGranularity);
   DCHECK_EQ(2 * sizeof(HeapObjectHeader), static_cast<size_t>(alignment));
   constexpr size_t kAlignment = 2 * kAllocationGranularity;
diff --git a/src/heap/cppgc/trace-event.h b/src/heap/cppgc/trace-event.h
index 6bde1448e39..1ccefbfaf64 100644
--- a/src/heap/cppgc/trace-event.h
+++ b/src/heap/cppgc/trace-event.h
@@ -177,7 +177,7 @@ SetTraceValue(T arg, unsigned char* type, uint64_t* value) {
                                       uint64_t* value) {                    \
     *type = value_type_id;                                                  \
     *value = 0;                                                             \
-    STATIC_ASSERT(sizeof(arg) <= sizeof(*value));                           \
+    static_assert(sizeof(arg) <= sizeof(*value));                           \
     memcpy(value, &arg, sizeof(arg));                                       \
   }
 INTERNAL_DECLARE_SET_TRACE_VALUE(double, TRACE_VALUE_TYPE_DOUBLE)
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index 3c5629b2f14..4ad79cad240 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -34,7 +34,7 @@ namespace internal {
 template <typename Impl>
 template <AllocationType allocation>
 Handle<HeapNumber> FactoryBase<Impl>::NewHeapNumber() {
-  STATIC_ASSERT(HeapNumber::kSize <= kMaxRegularHeapObjectSize);
+  static_assert(HeapNumber::kSize <= kMaxRegularHeapObjectSize);
   Map map = read_only_roots().heap_number_map();
   HeapObject result = AllocateRawWithImmortalMap(HeapNumber::kSize, allocation,
                                                  map, kDoubleUnaligned);
@@ -738,11 +738,11 @@ MaybeHandle<String> FactoryBase<Impl>::NewConsString(
   // If the resulting string is small make a flat string.
   if (length < ConsString::kMinLength) {
     // Note that neither of the two inputs can be a slice because:
-    STATIC_ASSERT(ConsString::kMinLength <= SlicedString::kMinLength);
+    static_assert(ConsString::kMinLength <= SlicedString::kMinLength);
     DCHECK(left->IsFlat());
     DCHECK(right->IsFlat());
 
-    STATIC_ASSERT(ConsString::kMinLength <= String::kMaxLength);
+    static_assert(ConsString::kMinLength <= String::kMaxLength);
     if (is_one_byte) {
       Handle<SeqOneByteString> result =
           NewRawOneByteString(length, allocation).ToHandleChecked();
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index 32e53098812..c44bb4d5656 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -151,7 +151,7 @@ MaybeHandle<Code> Factory::CodeBuilder::BuildInternal(
     isolate_->heap()->SetBasicBlockProfilingData(new_list);
   }
 
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
   Heap* heap = isolate_->heap();
   CodePageCollectionMemoryModificationScope code_allocation(heap);
 
@@ -1110,7 +1110,7 @@ Handle<JSStringIterator> Factory::NewJSStringIterator(Handle<String> string) {
 Symbol Factory::NewSymbolInternal(AllocationType allocation) {
   DCHECK(allocation != AllocationType::kYoung);
   // Statically ensure that it is safe to allocate symbols in paged spaces.
-  STATIC_ASSERT(Symbol::kSize <= kMaxRegularHeapObjectSize);
+  static_assert(Symbol::kSize <= kMaxRegularHeapObjectSize);
 
   Symbol symbol = Symbol::cast(AllocateRawWithImmortalMap(
       Symbol::kSize, allocation, read_only_roots().symbol_map()));
@@ -1259,7 +1259,7 @@ Handle<Context> Factory::NewCatchContext(Handle<Context> previous,
                                          Handle<ScopeInfo> scope_info,
                                          Handle<Object> thrown_object) {
   DCHECK_EQ(scope_info->scope_type(), CATCH_SCOPE);
-  STATIC_ASSERT(Context::MIN_CONTEXT_SLOTS == Context::THROWN_OBJECT_INDEX);
+  static_assert(Context::MIN_CONTEXT_SLOTS == Context::THROWN_OBJECT_INDEX);
   // TODO(ishell): Take the details from CatchContext class.
   int variadic_part_length = Context::MIN_CONTEXT_SLOTS + 1;
   Context context = NewContextInternal(
@@ -1467,7 +1467,7 @@ Handle<PromiseResolveThenableJobTask> Factory::NewPromiseResolveThenableJobTask(
 
 Handle<Foreign> Factory::NewForeign(Address addr) {
   // Statically ensure that it is safe to allocate foreigns in paged spaces.
-  STATIC_ASSERT(Foreign::kSize <= kMaxRegularHeapObjectSize);
+  static_assert(Foreign::kSize <= kMaxRegularHeapObjectSize);
   Map map = *foreign_map();
   Foreign foreign = Foreign::cast(AllocateRawWithImmortalMap(
       map.instance_size(), AllocationType::kYoung, map));
@@ -1735,7 +1735,7 @@ Handle<SharedFunctionInfo> Factory::NewSharedFunctionInfoForWasmCapiFunction(
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 Handle<Cell> Factory::NewCell(Handle<Object> value) {
-  STATIC_ASSERT(Cell::kSize <= kMaxRegularHeapObjectSize);
+  static_assert(Cell::kSize <= kMaxRegularHeapObjectSize);
   Cell result = Cell::cast(AllocateRawWithImmortalMap(
       Cell::kSize, AllocationType::kOld, *cell_map()));
   DisallowGarbageCollection no_gc;
@@ -1781,7 +1781,7 @@ Handle<PropertyCell> Factory::NewPropertyCell(Handle<Name> name,
                                               Handle<Object> value,
                                               AllocationType allocation) {
   DCHECK(name->IsUniqueName());
-  STATIC_ASSERT(PropertyCell::kSize <= kMaxRegularHeapObjectSize);
+  static_assert(PropertyCell::kSize <= kMaxRegularHeapObjectSize);
   PropertyCell cell = PropertyCell::cast(AllocateRawWithImmortalMap(
       PropertyCell::kSize, allocation, *global_property_cell_map()));
   DisallowGarbageCollection no_gc;
@@ -1842,7 +1842,7 @@ Handle<AllocationSite> Factory::NewAllocationSite(bool with_weak_next) {
 Handle<Map> Factory::NewMap(InstanceType type, int instance_size,
                             ElementsKind elements_kind, int inobject_properties,
                             AllocationType allocation_type) {
-  STATIC_ASSERT(LAST_JS_OBJECT_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_OBJECT_TYPE == LAST_TYPE);
   DCHECK_IMPLIES(InstanceTypeChecker::IsJSObject(type) &&
                      !Map::CanHaveFastTransitionableElementsKind(type),
                  IsDictionaryElementsKind(elements_kind) ||
@@ -3028,7 +3028,7 @@ MaybeHandle<JSBoundFunction> Factory::NewJSBoundFunction(
     Handle<JSReceiver> target_function, Handle<Object> bound_this,
     base::Vector<Handle<Object>> bound_args) {
   DCHECK(target_function->IsCallable());
-  STATIC_ASSERT(Code::kMaxArguments <= FixedArray::kMaxLength);
+  static_assert(Code::kMaxArguments <= FixedArray::kMaxLength);
   if (bound_args.length() >= Code::kMaxArguments) {
     THROW_NEW_ERROR(isolate(),
                     NewRangeError(MessageTemplate::kTooManyArguments),
@@ -3333,7 +3333,7 @@ inline Handle<String> Factory::SmiToString(Smi number, NumberCacheMode mode) {
 
   // Compute the hash here (rather than letting the caller take care of it) so
   // that the "cache hit" case above doesn't have to bother with it.
-  STATIC_ASSERT(Smi::kMaxValue <= std::numeric_limits<uint32_t>::max());
+  static_assert(Smi::kMaxValue <= std::numeric_limits<uint32_t>::max());
   {
     DisallowGarbageCollection no_gc;
     String raw = *result;
@@ -3714,7 +3714,7 @@ Handle<Map> Factory::CreateSloppyFunctionMap(
       static_cast<PropertyAttributes>(DONT_ENUM | READ_ONLY);
 
   int field_index = 0;
-  STATIC_ASSERT(
+  static_assert(
       JSFunctionOrBoundFunctionOrWrappedFunction::kLengthDescriptorIndex == 0);
   {  // Add length accessor.
     Descriptor d = Descriptor::AccessorConstant(
@@ -3722,7 +3722,7 @@ Handle<Map> Factory::CreateSloppyFunctionMap(
     map->AppendDescriptor(isolate(), &d);
   }
 
-  STATIC_ASSERT(
+  static_assert(
       JSFunctionOrBoundFunctionOrWrappedFunction::kNameDescriptorIndex == 1);
   if (IsFunctionModeWithName(function_mode)) {
     // Add name field.
@@ -3803,14 +3803,14 @@ Handle<Map> Factory::CreateStrictFunctionMap(
       static_cast<PropertyAttributes>(DONT_ENUM | READ_ONLY);
 
   int field_index = 0;
-  STATIC_ASSERT(JSFunction::kLengthDescriptorIndex == 0);
+  static_assert(JSFunction::kLengthDescriptorIndex == 0);
   {  // Add length accessor.
     Descriptor d = Descriptor::AccessorConstant(
         length_string(), function_length_accessor(), roc_attribs);
     map->AppendDescriptor(isolate(), &d);
   }
 
-  STATIC_ASSERT(JSFunction::kNameDescriptorIndex == 1);
+  static_assert(JSFunction::kNameDescriptorIndex == 1);
   if (IsFunctionModeWithName(function_mode)) {
     // Add name field.
     Handle<Name> name = isolate()->factory()->name_string();
@@ -3864,7 +3864,7 @@ Handle<Map> Factory::CreateClassFunctionMap(Handle<JSFunction> empty_function) {
   PropertyAttributes roc_attribs =
       static_cast<PropertyAttributes>(DONT_ENUM | READ_ONLY);
 
-  STATIC_ASSERT(JSFunction::kLengthDescriptorIndex == 0);
+  static_assert(JSFunction::kLengthDescriptorIndex == 0);
   {  // Add length accessor.
     Descriptor d = Descriptor::AccessorConstant(
         length_string(), function_length_accessor(), roc_attribs);
diff --git a/src/heap/free-list.h b/src/heap/free-list.h
index afa23e051a4..2c9a46c885b 100644
--- a/src/heap/free-list.h
+++ b/src/heap/free-list.h
@@ -448,8 +448,8 @@ class V8_EXPORT_PRIVATE FreeListManyCachedFastPath : public FreeListManyCached {
   // Objects in the 15th category are at least 256 bytes
   static const FreeListCategoryType kFastPathFallBackTiny = 15;
 
-  STATIC_ASSERT(categories_min[kFastPathFirstCategory] == kFastPathStart);
-  STATIC_ASSERT(categories_min[kFastPathFallBackTiny] ==
+  static_assert(categories_min[kFastPathFirstCategory] == kFastPathStart);
+  static_assert(categories_min[kFastPathFallBackTiny] ==
                 kTinyObjectMaxSize * 2);
 
   FreeListCategoryType SelectFastAllocationFreeListCategoryType(
diff --git a/src/heap/gc-tracer-inl.h b/src/heap/gc-tracer-inl.h
index 22fdb1b773a..ab23cd0384e 100644
--- a/src/heap/gc-tracer-inl.h
+++ b/src/heap/gc-tracer-inl.h
@@ -177,7 +177,7 @@ WorkerThreadRuntimeCallStats* GCTracer::worker_thread_runtime_call_stats() {
 }
 
 RuntimeCallCounterId GCTracer::RCSCounterFromScope(Scope::ScopeId id) {
-  STATIC_ASSERT(Scope::FIRST_SCOPE == Scope::MC_INCREMENTAL);
+  static_assert(Scope::FIRST_SCOPE == Scope::MC_INCREMENTAL);
   return static_cast<RuntimeCallCounterId>(
       static_cast<int>(RuntimeCallCounterId::kGC_MC_INCREMENTAL) +
       static_cast<int>(id));
diff --git a/src/heap/gc-tracer.cc b/src/heap/gc-tracer.cc
index 4f325618bee..449e65a6fe7 100644
--- a/src/heap/gc-tracer.cc
+++ b/src/heap/gc-tracer.cc
@@ -167,10 +167,10 @@ GCTracer::GCTracer(Heap* heap)
       previous_mark_compact_end_time_(0) {
   // All accesses to incremental_marking_scope assume that incremental marking
   // scopes come first.
-  STATIC_ASSERT(0 == Scope::FIRST_INCREMENTAL_SCOPE);
+  static_assert(0 == Scope::FIRST_INCREMENTAL_SCOPE);
   // We assume that MC_INCREMENTAL is the first scope so that we can properly
   // map it to RuntimeCallStats.
-  STATIC_ASSERT(0 == Scope::MC_INCREMENTAL);
+  static_assert(0 == Scope::MC_INCREMENTAL);
   current_.end_time = MonotonicallyIncreasingTimeInMs();
   for (int i = 0; i < Scope::NUMBER_OF_SCOPES; i++) {
     background_counter_[i].total_duration_ms = 0;
diff --git a/src/heap/heap-inl.h b/src/heap/heap-inl.h
index bb094e674f7..8ebb2128034 100644
--- a/src/heap/heap-inl.h
+++ b/src/heap/heap-inl.h
@@ -502,7 +502,7 @@ int Heap::NextScriptId() {
   Smi new_id, last_id_before_cas;
   do {
     if (last_id.value() == Smi::kMaxValue) {
-      STATIC_ASSERT(v8::UnboundScript::kNoScriptId == 0);
+      static_assert(v8::UnboundScript::kNoScriptId == 0);
       new_id = Smi::FromInt(1);
     } else {
       new_id = Smi::FromInt(last_id.value() + 1);
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index 778480e3efd..0a48c74b2a9 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -797,7 +797,7 @@ class Heap::AllocationTrackerForDebugging final
     MemoryChunk* memory_chunk = MemoryChunk::FromAddress(object_address);
     AllocationSpace allocation_space = memory_chunk->owner_identity();
 
-    STATIC_ASSERT(kSpaceTagSize + kPageSizeBits <= 32);
+    static_assert(kSpaceTagSize + kPageSizeBits <= 32);
     uint32_t value =
         static_cast<uint32_t>(object_address - memory_chunk->address()) |
         (static_cast<uint32_t>(allocation_space) << kPageSizeBits);
@@ -3120,19 +3120,19 @@ void Heap::VisitExternalResources(v8::ExternalResourceVisitor* visitor) {
   external_string_table_.IterateAll(&external_string_table_visitor);
 }
 
-STATIC_ASSERT(IsAligned(FixedDoubleArray::kHeaderSize, kDoubleAlignment));
+static_assert(IsAligned(FixedDoubleArray::kHeaderSize, kDoubleAlignment));
 
 #ifdef V8_COMPRESS_POINTERS
 // TODO(ishell, v8:8875): When pointer compression is enabled the kHeaderSize
 // is only kTaggedSize aligned but we can keep using unaligned access since
 // both x64 and arm64 architectures (where pointer compression supported)
 // allow unaligned access to doubles.
-STATIC_ASSERT(IsAligned(ByteArray::kHeaderSize, kTaggedSize));
+static_assert(IsAligned(ByteArray::kHeaderSize, kTaggedSize));
 #else
-STATIC_ASSERT(IsAligned(ByteArray::kHeaderSize, kDoubleAlignment));
+static_assert(IsAligned(ByteArray::kHeaderSize, kDoubleAlignment));
 #endif
 
-STATIC_ASSERT(!USE_ALLOCATION_ALIGNMENT_BOOL ||
+static_assert(!USE_ALLOCATION_ALIGNMENT_BOOL ||
               (HeapNumber::kValueOffset & kDoubleAlignmentMask) == kTaggedSize);
 
 int Heap::GetMaximumFillToAlign(AllocationAlignment alignment) {
@@ -3461,9 +3461,9 @@ FixedArrayBase Heap::LeftTrimFixedArray(FixedArrayBase object,
   DCHECK(!IsLargeObject(object));
   DCHECK(object.map() != ReadOnlyRoots(this).fixed_cow_array_map());
 
-  STATIC_ASSERT(FixedArrayBase::kMapOffset == 0);
-  STATIC_ASSERT(FixedArrayBase::kLengthOffset == kTaggedSize);
-  STATIC_ASSERT(FixedArrayBase::kHeaderSize == 2 * kTaggedSize);
+  static_assert(FixedArrayBase::kMapOffset == 0);
+  static_assert(FixedArrayBase::kLengthOffset == kTaggedSize);
+  static_assert(FixedArrayBase::kHeaderSize == 2 * kTaggedSize);
 
   const int len = object.length();
   DCHECK(elements_to_trim <= len);
@@ -5117,7 +5117,7 @@ void Heap::IterateBuiltins(RootVisitor* v) {
   }
 
   // The entry table doesn't need to be updated since all builtins are embedded.
-  STATIC_ASSERT(Builtins::AllBuiltinsAreIsolateIndependent());
+  static_assert(Builtins::AllBuiltinsAreIsolateIndependent());
 }
 
 void Heap::IterateStackRoots(RootVisitor* v) {
@@ -7310,9 +7310,9 @@ template <int kModeMask, typename TSlot>
 void Heap::WriteBarrierForRangeImpl(MemoryChunk* source_page, HeapObject object,
                                     TSlot start_slot, TSlot end_slot) {
   // At least one of generational or marking write barrier should be requested.
-  STATIC_ASSERT(kModeMask & (kDoGenerational | kDoMarking));
+  static_assert(kModeMask & (kDoGenerational | kDoMarking));
   // kDoEvacuationSlotRecording implies kDoMarking.
-  STATIC_ASSERT(!(kModeMask & kDoEvacuationSlotRecording) ||
+  static_assert(!(kModeMask & kDoEvacuationSlotRecording) ||
                 (kModeMask & kDoMarking));
 
   MarkingBarrier* marking_barrier = WriteBarrier::CurrentMarkingBarrier(this);
diff --git a/src/heap/heap.h b/src/heap/heap.h
index 212a143adde..ae92e73406c 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -374,8 +374,8 @@ class Heap {
   static constexpr size_t kMaxSemiSpaceSize = 8192 * KB * kPointerMultiplier;
 #endif
 
-  STATIC_ASSERT(kMinSemiSpaceSize % (1 << kPageSizeBits) == 0);
-  STATIC_ASSERT(kMaxSemiSpaceSize % (1 << kPageSizeBits) == 0);
+  static_assert(kMinSemiSpaceSize % (1 << kPageSizeBits) == 0);
+  static_assert(kMaxSemiSpaceSize % (1 << kPageSizeBits) == 0);
 
   static const int kTraceRingBufferSize = 512;
   static const int kStacktraceBufferSize = 512;
@@ -391,17 +391,17 @@ class Heap {
 
   static const int kMinPromotedPercentForFastPromotionMode = 90;
 
-  STATIC_ASSERT(static_cast<int>(RootIndex::kUndefinedValue) ==
+  static_assert(static_cast<int>(RootIndex::kUndefinedValue) ==
                 Internals::kUndefinedValueRootIndex);
-  STATIC_ASSERT(static_cast<int>(RootIndex::kTheHoleValue) ==
+  static_assert(static_cast<int>(RootIndex::kTheHoleValue) ==
                 Internals::kTheHoleValueRootIndex);
-  STATIC_ASSERT(static_cast<int>(RootIndex::kNullValue) ==
+  static_assert(static_cast<int>(RootIndex::kNullValue) ==
                 Internals::kNullValueRootIndex);
-  STATIC_ASSERT(static_cast<int>(RootIndex::kTrueValue) ==
+  static_assert(static_cast<int>(RootIndex::kTrueValue) ==
                 Internals::kTrueValueRootIndex);
-  STATIC_ASSERT(static_cast<int>(RootIndex::kFalseValue) ==
+  static_assert(static_cast<int>(RootIndex::kFalseValue) ==
                 Internals::kFalseValueRootIndex);
-  STATIC_ASSERT(static_cast<int>(RootIndex::kempty_string) ==
+  static_assert(static_cast<int>(RootIndex::kempty_string) ==
                 Internals::kEmptyStringRootIndex);
 
   // Calculates the maximum amount of filler that could be required by the
diff --git a/src/heap/large-spaces.cc b/src/heap/large-spaces.cc
index a1e2681b2d6..c48c16d1230 100644
--- a/src/heap/large-spaces.cc
+++ b/src/heap/large-spaces.cc
@@ -28,14 +28,14 @@ namespace internal {
 // can't overlap with the lower 32 bits of cleared weak reference value and
 // therefore it's enough to compare only the lower 32 bits of a MaybeObject in
 // order to figure out if it's a cleared weak reference or not.
-STATIC_ASSERT(kClearedWeakHeapObjectLower32 < LargePage::kHeaderSize);
+static_assert(kClearedWeakHeapObjectLower32 < LargePage::kHeaderSize);
 
 LargePage::LargePage(Heap* heap, BaseSpace* space, size_t chunk_size,
                      Address area_start, Address area_end,
                      VirtualMemory reservation, Executability executable)
     : MemoryChunk(heap, space, chunk_size, area_start, area_end,
                   std::move(reservation), executable, PageSize::kLarge) {
-  STATIC_ASSERT(LargePage::kMaxCodePageSize <= TypedSlotSet::kMaxOffset);
+  static_assert(LargePage::kMaxCodePageSize <= TypedSlotSet::kMaxOffset);
 
   if (executable && chunk_size > LargePage::kMaxCodePageSize) {
     FATAL("Code page is too large.");
@@ -48,7 +48,7 @@ LargePage::LargePage(Heap* heap, BaseSpace* space, size_t chunk_size,
 LargePage* LargePage::Initialize(Heap* heap, MemoryChunk* chunk,
                                  Executability executable) {
   if (executable && chunk->size() > LargePage::kMaxCodePageSize) {
-    STATIC_ASSERT(LargePage::kMaxCodePageSize <= TypedSlotSet::kMaxOffset);
+    static_assert(LargePage::kMaxCodePageSize <= TypedSlotSet::kMaxOffset);
     FATAL("Code page is too large.");
   }
 
diff --git a/src/heap/large-spaces.h b/src/heap/large-spaces.h
index 20f44374fda..81703288508 100644
--- a/src/heap/large-spaces.h
+++ b/src/heap/large-spaces.h
@@ -56,7 +56,7 @@ class LargePage : public MemoryChunk {
   friend class MemoryAllocator;
 };
 
-STATIC_ASSERT(sizeof(LargePage) <= MemoryChunk::kHeaderSize);
+static_assert(sizeof(LargePage) <= MemoryChunk::kHeaderSize);
 
 // -----------------------------------------------------------------------------
 // Large objects ( > kMaxRegularHeapObjectSize ) are allocated and managed by
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index 31895ddf16d..cb73dfc5b15 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -81,7 +81,7 @@ const char* Marking::kImpossibleBitPattern = "01";
 
 // The following has to hold in order for {MarkingState::MarkBitFrom} to not
 // produce invalid {kImpossibleBitPattern} in the marking bitmap by overlapping.
-STATIC_ASSERT(Heap::kMinObjectSizeInTaggedWords >= 2);
+static_assert(Heap::kMinObjectSizeInTaggedWords >= 2);
 
 // =============================================================================
 // Verifiers
@@ -2752,7 +2752,7 @@ void MarkCompactCollector::FlushBytecodeFromSFI(
 
   // The size of the bytecode array should always be larger than an
   // UncompiledData object.
-  STATIC_ASSERT(BytecodeArray::SizeFor(0) >=
+  static_assert(BytecodeArray::SizeFor(0) >=
                 UncompiledDataWithoutPreparseData::kSize);
 
   // Replace bytecode array with an uncompiled data array.
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index 756befa539e..2d6c17252b6 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -220,7 +220,7 @@ int MarkingVisitorBase<ConcreteVisitor, MarkingState>::
     VisitFixedArrayWithProgressBar(Map map, FixedArray object,
                                    ProgressBar& progress_bar) {
   const int kProgressBarScanningChunk = kMaxRegularHeapObjectSize;
-  STATIC_ASSERT(kMaxRegularHeapObjectSize % kTaggedSize == 0);
+  static_assert(kMaxRegularHeapObjectSize % kTaggedSize == 0);
   DCHECK(concrete_visitor()->marking_state()->IsBlackOrGrey(object));
   concrete_visitor()->marking_state()->GreyToBlack(object);
   int size = FixedArray::BodyDescriptor::SizeOf(map, object);
diff --git a/src/heap/marking.h b/src/heap/marking.h
index 5ce2f6cd251..7c93ec48a6a 100644
--- a/src/heap/marking.h
+++ b/src/heap/marking.h
@@ -14,7 +14,7 @@ namespace internal {
 class MarkBit {
  public:
   using CellType = uint32_t;
-  STATIC_ASSERT(sizeof(CellType) == sizeof(base::Atomic32));
+  static_assert(sizeof(CellType) == sizeof(base::Atomic32));
 
   inline MarkBit(CellType* cell, CellType mask) : cell_(cell), mask_(mask) {}
 
@@ -400,7 +400,7 @@ class Marking : public AllStatic {
 
   template <AccessMode mode = AccessMode::NON_ATOMIC>
   V8_INLINE static void MarkWhite(MarkBit markbit) {
-    STATIC_ASSERT(mode == AccessMode::NON_ATOMIC);
+    static_assert(mode == AccessMode::NON_ATOMIC);
     markbit.Clear<mode>();
     markbit.Next().Clear<mode>();
   }
diff --git a/src/heap/remembered-set.h b/src/heap/remembered-set.h
index ed7c0a2e369..172803bd702 100644
--- a/src/heap/remembered-set.h
+++ b/src/heap/remembered-set.h
@@ -275,7 +275,7 @@ class RememberedSet : public AllStatic {
 
   // Clear all old to old slots from the remembered set.
   static void ClearAll(Heap* heap) {
-    STATIC_ASSERT(type == OLD_TO_OLD || type == OLD_TO_CODE);
+    static_assert(type == OLD_TO_OLD || type == OLD_TO_CODE);
     OldGenerationMemoryChunkIterator it(heap);
     MemoryChunk* chunk;
     while ((chunk = it.next()) != nullptr) {
diff --git a/src/heap/setup-heap-internal.cc b/src/heap/setup-heap-internal.cc
index 2b005732e8a..d2764bc515a 100644
--- a/src/heap/setup-heap-internal.cc
+++ b/src/heap/setup-heap-internal.cc
@@ -127,7 +127,7 @@ AllocationResult Heap::AllocateMap(InstanceType instance_type,
                                    int instance_size,
                                    ElementsKind elements_kind,
                                    int inobject_properties) {
-  STATIC_ASSERT(LAST_JS_OBJECT_TYPE == LAST_TYPE);
+  static_assert(LAST_JS_OBJECT_TYPE == LAST_TYPE);
   bool is_js_object = InstanceTypeChecker::IsJSObject(instance_type);
   bool is_wasm_object = false;
 #if V8_ENABLE_WEBASSEMBLY
diff --git a/src/heap/slot-set.h b/src/heap/slot-set.h
index 81a48afba70..7da8108f671 100644
--- a/src/heap/slot-set.h
+++ b/src/heap/slot-set.h
@@ -122,8 +122,8 @@ class PossiblyEmptyBuckets {
   FRIEND_TEST(PossiblyEmptyBucketsTest, WordsForBuckets);
 };
 
-STATIC_ASSERT(std::is_standard_layout<PossiblyEmptyBuckets>::value);
-STATIC_ASSERT(sizeof(PossiblyEmptyBuckets) == kSystemPointerSize);
+static_assert(std::is_standard_layout<PossiblyEmptyBuckets>::value);
+static_assert(sizeof(PossiblyEmptyBuckets) == kSystemPointerSize);
 
 // Data structure for maintaining a set of slots in a standard (non-large)
 // page.
@@ -599,8 +599,8 @@ class SlotSet {
 #endif
 };
 
-STATIC_ASSERT(std::is_standard_layout<SlotSet>::value);
-STATIC_ASSERT(std::is_standard_layout<SlotSet::Bucket>::value);
+static_assert(std::is_standard_layout<SlotSet>::value);
+static_assert(std::is_standard_layout<SlotSet::Bucket>::value);
 
 enum class SlotType : uint8_t {
   // Full pointer sized slot storing an object start address.
@@ -699,7 +699,7 @@ class V8_EXPORT_PRIVATE TypedSlotSet : public TypedSlots {
   // This can run concurrently to ClearInvalidSlots().
   template <typename Callback>
   int Iterate(Callback callback, IterationMode mode) {
-    STATIC_ASSERT(static_cast<uint8_t>(SlotType::kLast) < 8);
+    static_assert(static_cast<uint8_t>(SlotType::kLast) < 8);
     Chunk* chunk = head_;
     Chunk* previous = nullptr;
     int new_count = 0;
diff --git a/src/heap/spaces.cc b/src/heap/spaces.cc
index c97b46a8d85..08b96c77556 100644
--- a/src/heap/spaces.cc
+++ b/src/heap/spaces.cc
@@ -43,8 +43,8 @@ namespace internal {
 // object can't overlap with the lower 32 bits of cleared weak reference value
 // and therefore it's enough to compare only the lower 32 bits of a MaybeObject
 // in order to figure out if it's a cleared weak reference or not.
-STATIC_ASSERT(kClearedWeakHeapObjectLower32 > 0);
-STATIC_ASSERT(kClearedWeakHeapObjectLower32 < Page::kHeaderSize);
+static_assert(kClearedWeakHeapObjectLower32 > 0);
+static_assert(kClearedWeakHeapObjectLower32 < Page::kHeaderSize);
 
 // static
 constexpr Page::MainThreadFlags Page::kCopyOnFlipFlagsMask;
diff --git a/src/heap/spaces.h b/src/heap/spaces.h
index 09dc8ff715e..05282d34277 100644
--- a/src/heap/spaces.h
+++ b/src/heap/spaces.h
@@ -209,7 +209,7 @@ class V8_EXPORT_PRIVATE Space : public BaseSpace {
   std::unique_ptr<FreeList> free_list_;
 };
 
-STATIC_ASSERT(sizeof(std::atomic<intptr_t>) == kSystemPointerSize);
+static_assert(sizeof(std::atomic<intptr_t>) == kSystemPointerSize);
 
 // -----------------------------------------------------------------------------
 // A page is a memory chunk of a size 256K. Large object pages may be larger.
@@ -334,9 +334,9 @@ class Page : public MemoryChunk {
 };
 
 // Validate our estimates on the header size.
-STATIC_ASSERT(sizeof(BasicMemoryChunk) <= BasicMemoryChunk::kHeaderSize);
-STATIC_ASSERT(sizeof(MemoryChunk) <= MemoryChunk::kHeaderSize);
-STATIC_ASSERT(sizeof(Page) <= MemoryChunk::kHeaderSize);
+static_assert(sizeof(BasicMemoryChunk) <= BasicMemoryChunk::kHeaderSize);
+static_assert(sizeof(MemoryChunk) <= MemoryChunk::kHeaderSize);
+static_assert(sizeof(Page) <= MemoryChunk::kHeaderSize);
 
 // -----------------------------------------------------------------------------
 // Interface for heap object iterator to be implemented by all object space
diff --git a/src/ic/accessor-assembler.cc b/src/ic/accessor-assembler.cc
index 8c4bc2885ee..a62f44cb5b6 100644
--- a/src/ic/accessor-assembler.cc
+++ b/src/ic/accessor-assembler.cc
@@ -1243,7 +1243,7 @@ void AccessorAssembler::HandleStoreICHandlerCase(
         if_slow(this);
 
 #define ASSERT_CONSECUTIVE(a, b)                                    \
-  STATIC_ASSERT(static_cast<intptr_t>(StoreHandler::Kind::a) + 1 == \
+  static_assert(static_cast<intptr_t>(StoreHandler::Kind::a) + 1 == \
                 static_cast<intptr_t>(StoreHandler::Kind::b));
     ASSERT_CONSECUTIVE(kGlobalProxy, kNormal)
     ASSERT_CONSECUTIVE(kNormal, kInterceptor)
@@ -1282,7 +1282,7 @@ void AccessorAssembler::HandleStoreICHandlerCase(
         const int kTypeAndReadOnlyMask =
             PropertyDetails::KindField::kMask |
             PropertyDetails::kAttributesReadOnlyMask;
-        STATIC_ASSERT(static_cast<int>(PropertyKind::kData) == 0);
+        static_assert(static_cast<int>(PropertyKind::kData) == 0);
         GotoIf(IsSetWord32(details, kTypeAndReadOnlyMask), miss);
 
         if (V8_DICT_PROPERTY_CONST_TRACKING_BOOL) {
@@ -1460,7 +1460,7 @@ void AccessorAssembler::HandleStoreICTransitionMapHandlerCase(
         PropertyDetails::KindField::kMask |
         PropertyDetails::kAttributesDontDeleteMask |
         PropertyDetails::kAttributesReadOnlyMask;
-    STATIC_ASSERT(static_cast<int>(PropertyKind::kData) == 0);
+    static_assert(static_cast<int>(PropertyKind::kData) == 0);
     // Both DontDelete and ReadOnly attributes must not be set and it has to be
     // a kData property.
     GotoIf(IsSetWord32(details, kKindAndAttributesDontDeleteReadOnlyMask),
@@ -1836,7 +1836,7 @@ void AccessorAssembler::HandleStoreICProtoHandler(
         const int kTypeAndReadOnlyMask =
             PropertyDetails::KindField::kMask |
             PropertyDetails::kAttributesReadOnlyMask;
-        STATIC_ASSERT(static_cast<int>(PropertyKind::kData) == 0);
+        static_assert(static_cast<int>(PropertyKind::kData) == 0);
         GotoIf(IsSetWord32(details, kTypeAndReadOnlyMask), miss);
 
         StoreValueByKeyIndex<PropertyDictionary>(properties, name_index,
@@ -2398,7 +2398,7 @@ void AccessorAssembler::EmitElementLoad(
         int16_elements(this), uint32_elements(this), int32_elements(this),
         float32_elements(this), float64_elements(this), bigint64_elements(this),
         biguint64_elements(this);
-    STATIC_ASSERT(LAST_ELEMENTS_KIND ==
+    static_assert(LAST_ELEMENTS_KIND ==
                   LAST_RAB_GSAB_FIXED_TYPED_ARRAY_ELEMENTS_KIND);
     GotoIf(Int32GreaterThanOrEqual(
                elements_kind,
diff --git a/src/ic/handler-configuration.h b/src/ic/handler-configuration.h
index d7a16a9f886..87d0f042b72 100644
--- a/src/ic/handler-configuration.h
+++ b/src/ic/handler-configuration.h
@@ -87,7 +87,7 @@ class LoadHandler final : public DataHandler {
   using DescriptorBits =
       LookupOnLookupStartObjectBits::Next<unsigned, kDescriptorIndexBitCount>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(DescriptorBits::kLastUsedBit < kSmiValueSize);
+  static_assert(DescriptorBits::kLastUsedBit < kSmiValueSize);
 
   //
   // Encoding when KindBits contains kField.
@@ -103,7 +103,7 @@ class LoadHandler final : public DataHandler {
   using FieldIndexBits =
       IsDoubleBits::Next<unsigned, kDescriptorIndexBitCount + 1>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(FieldIndexBits::kLastUsedBit < kSmiValueSize);
+  static_assert(FieldIndexBits::kLastUsedBit < kSmiValueSize);
 
   //
   // Encoding when KindBits contains kField and IsWasmStructBits is 1.
@@ -111,7 +111,7 @@ class LoadHandler final : public DataHandler {
   using WasmFieldTypeBits = IsWasmStructBits::Next<WasmValueType, 4>;
   using WasmFieldOffsetBits = WasmFieldTypeBits::Next<unsigned, 20>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(WasmFieldOffsetBits::kLastUsedBit < kSmiValueSize);
+  static_assert(WasmFieldOffsetBits::kLastUsedBit < kSmiValueSize);
 
   //
   // Encoding when KindBits contains kElement or kIndexedString.
@@ -130,14 +130,14 @@ class LoadHandler final : public DataHandler {
   using ConvertHoleBits = IsJsArrayBits::Next<bool, 1>;
   using ElementsKindBits = ConvertHoleBits::Next<ElementsKind, 8>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(ElementsKindBits::kLastUsedBit < kSmiValueSize);
+  static_assert(ElementsKindBits::kLastUsedBit < kSmiValueSize);
 
   //
   // Encoding when KindBits contains kElement and IsWasmArrayBits is 1.
   //
   using WasmArrayTypeBits = IsWasmArrayBits::Next<WasmValueType, 4>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(WasmArrayTypeBits::kLastUsedBit < kSmiValueSize);
+  static_assert(WasmArrayTypeBits::kLastUsedBit < kSmiValueSize);
 
   //
   // Encoding when KindBits contains kModuleExport.
@@ -145,7 +145,7 @@ class LoadHandler final : public DataHandler {
   using ExportsIndexBits = LookupOnLookupStartObjectBits::Next<
       unsigned,
       kSmiValueSize - LookupOnLookupStartObjectBits::kLastUsedBit - 1>;
-  STATIC_ASSERT(ExportsIndexBits::kLastUsedBit < kSmiValueSize);
+  static_assert(ExportsIndexBits::kLastUsedBit < kSmiValueSize);
 
   // Decodes kind from Smi-handler.
   static inline Kind GetHandlerKind(Smi smi_handler);
@@ -294,7 +294,7 @@ class StoreHandler final : public DataHandler {
   using FieldIndexBits =
       RepresentationBits::Next<unsigned, kDescriptorIndexBitCount + 1>;
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(FieldIndexBits::kLastUsedBit < kSmiValueSize);
+  static_assert(FieldIndexBits::kLastUsedBit < kSmiValueSize);
 
   // Creates a Smi-handler for storing a field to fast object.
   static inline Handle<Smi> StoreField(Isolate* isolate, int descriptor,
diff --git a/src/ic/ic.cc b/src/ic/ic.cc
index 4676fa80b4f..8136d2c318d 100644
--- a/src/ic/ic.cc
+++ b/src/ic/ic.cc
@@ -1494,7 +1494,7 @@ bool IntPtrKeyToSize(intptr_t index, Handle<HeapObject> receiver, size_t* out) {
   }
 #else
   // On 32-bit platforms, any intptr_t is less than kMaxElementIndex.
-  STATIC_ASSERT(
+  static_assert(
       static_cast<double>(std::numeric_limits<decltype(index)>::max()) <=
       static_cast<double>(JSObject::kMaxElementIndex));
 #endif
diff --git a/src/ic/keyed-store-generic.cc b/src/ic/keyed-store-generic.cc
index dbab0d92ca1..e9e90f138cd 100644
--- a/src/ic/keyed-store-generic.cc
+++ b/src/ic/keyed-store-generic.cc
@@ -386,7 +386,7 @@ void KeyedStoreGenericAssembler::StoreElementWithCapacity(
     GotoIf(IsSetWord32(details, PropertyDetails::kAttributesReadOnlyMask),
            slow);
   }
-  STATIC_ASSERT(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
+  static_assert(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
   const int kHeaderSize = FixedArray::kHeaderSize - kHeapObjectTag;
 
   Label check_double_elements(this), check_cow_elements(this);
@@ -435,8 +435,8 @@ void KeyedStoreGenericAssembler::StoreElementWithCapacity(
     // Check if we already have object elements; just do the store if so.
     {
       Label must_transition(this);
-      STATIC_ASSERT(PACKED_SMI_ELEMENTS == 0);
-      STATIC_ASSERT(HOLEY_SMI_ELEMENTS == 1);
+      static_assert(PACKED_SMI_ELEMENTS == 0);
+      static_assert(HOLEY_SMI_ELEMENTS == 1);
       GotoIf(Int32LessThanOrEqual(elements_kind,
                                   Int32Constant(HOLEY_SMI_ELEMENTS)),
              &must_transition);
@@ -626,7 +626,7 @@ void KeyedStoreGenericAssembler::EmitGenericElementStore(
   // dispatch.
   BIND(&if_nonfast);
   {
-    STATIC_ASSERT(LAST_ELEMENTS_KIND ==
+    static_assert(LAST_ELEMENTS_KIND ==
                   LAST_RAB_GSAB_FIXED_TYPED_ARRAY_ELEMENTS_KIND);
     GotoIf(Int32GreaterThanOrEqual(
                elements_kind,
@@ -798,8 +798,8 @@ TNode<Map> KeyedStoreGenericAssembler::FindCandidateStoreICTransitionMapHandler(
       // transition array is expected to be the first among the transitions
       // with the same name.
       // See TransitionArray::CompareDetails() for details.
-      STATIC_ASSERT(static_cast<int>(PropertyKind::kData) == 0);
-      STATIC_ASSERT(NONE == 0);
+      static_assert(static_cast<int>(PropertyKind::kData) == 0);
+      static_assert(NONE == 0);
       const int kKeyToTargetOffset = (TransitionArray::kEntryTargetIndex -
                                       TransitionArray::kEntryKeyIndex) *
                                      kTaggedSize;
diff --git a/src/ic/stub-cache.h b/src/ic/stub-cache.h
index 41f2f6ca68e..7fba00c4556 100644
--- a/src/ic/stub-cache.h
+++ b/src/ic/stub-cache.h
@@ -82,7 +82,7 @@ class V8_EXPORT_PRIVATE StubCache {
   // causes the bit field inside the hash field to get shifted out implicitly.
   // Note that kCacheIndexShift must not get too large, because
   // sizeof(Entry) needs to be a multiple of 1 << kCacheIndexShift (see
-  // the STATIC_ASSERT below, in {entry(...)}).
+  // the static_assert below, in {entry(...)}).
   static const int kCacheIndexShift = Name::HashBits::kShift;
 
   static const int kPrimaryTableBits = 11;
@@ -128,7 +128,7 @@ class V8_EXPORT_PRIVATE StubCache {
   // in the hashed offset computations.
   static Entry* entry(Entry* table, int offset) {
     // The size of {Entry} must be a multiple of 1 << kCacheIndexShift.
-    STATIC_ASSERT((sizeof(*table) >> kCacheIndexShift) << kCacheIndexShift ==
+    static_assert((sizeof(*table) >> kCacheIndexShift) << kCacheIndexShift ==
                   sizeof(*table));
     const int multiplier = sizeof(*table) >> kCacheIndexShift;
     return reinterpret_cast<Entry*>(reinterpret_cast<Address>(table) +
diff --git a/src/ic/unary-op-assembler.cc b/src/ic/unary-op-assembler.cc
index fb5ab7f4221..ee76ef46bf7 100644
--- a/src/ic/unary-op-assembler.cc
+++ b/src/ic/unary-op-assembler.cc
@@ -223,7 +223,7 @@ class UnaryOpAssemblerImpl final : public CodeStubAssembler {
                                      TNode<Object> value, TNode<UintPtrT> slot,
                                      TNode<HeapObject> maybe_feedback_vector,
                                      UpdateFeedbackMode update_feedback_mode) {
-    STATIC_ASSERT(kOperation == Operation::kIncrement ||
+    static_assert(kOperation == Operation::kIncrement ||
                   kOperation == Operation::kDecrement);
     static constexpr int kAddValue =
         (kOperation == Operation::kIncrement) ? 1 : -1;
diff --git a/src/init/bootstrapper.cc b/src/init/bootstrapper.cc
index f7ea4597d86..feafc45417e 100644
--- a/src/init/bootstrapper.cc
+++ b/src/init/bootstrapper.cc
@@ -1720,7 +1720,7 @@ void Genesis::InitializeGlobal(Handle<JSGlobalObject> global_object,
     PropertyAttributes attribs =
         static_cast<PropertyAttributes>(DONT_ENUM | DONT_DELETE);
 
-    STATIC_ASSERT(JSArray::kLengthDescriptorIndex == 0);
+    static_assert(JSArray::kLengthDescriptorIndex == 0);
     {  // Add length.
       Descriptor d = Descriptor::AccessorConstant(
           factory->length_string(), factory->array_length_accessor(), attribs);
@@ -3843,7 +3843,7 @@ void Genesis::InitializeGlobal(Handle<JSGlobalObject> global_object,
     Map::EnsureDescriptorSlack(isolate_, map, 2);
 
     {  // length
-      STATIC_ASSERT(
+      static_assert(
           JSFunctionOrBoundFunctionOrWrappedFunction::kLengthDescriptorIndex ==
           0);
       Descriptor d = Descriptor::AccessorConstant(
@@ -3853,7 +3853,7 @@ void Genesis::InitializeGlobal(Handle<JSGlobalObject> global_object,
     }
 
     {  // name
-      STATIC_ASSERT(
+      static_assert(
           JSFunctionOrBoundFunctionOrWrappedFunction::kNameDescriptorIndex ==
           1);
       Descriptor d = Descriptor::AccessorConstant(
@@ -4528,7 +4528,7 @@ void Genesis::InitializeGlobal_harmony_shadow_realm() {
         static_cast<PropertyAttributes>(DONT_ENUM | READ_ONLY);
     Map::EnsureDescriptorSlack(isolate_, map, 2);
     {  // length
-      STATIC_ASSERT(
+      static_assert(
           JSFunctionOrBoundFunctionOrWrappedFunction::kLengthDescriptorIndex ==
           0);
       Descriptor d = Descriptor::AccessorConstant(
@@ -4538,7 +4538,7 @@ void Genesis::InitializeGlobal_harmony_shadow_realm() {
     }
 
     {  // name
-      STATIC_ASSERT(
+      static_assert(
           JSFunctionOrBoundFunctionOrWrappedFunction::kNameDescriptorIndex ==
           1);
       Descriptor d = Descriptor::AccessorConstant(
diff --git a/src/interpreter/bytecode-flags.h b/src/interpreter/bytecode-flags.h
index 915f500b000..1b98386b251 100644
--- a/src/interpreter/bytecode-flags.h
+++ b/src/interpreter/bytecode-flags.h
@@ -83,7 +83,7 @@ class StoreLookupSlotFlags {
  public:
   using LanguageModeBit = base::BitField8<LanguageMode, 0, 1>;
   using LookupHoistingModeBit = LanguageModeBit::Next<bool, 1>;
-  STATIC_ASSERT(LanguageModeSize <= LanguageModeBit::kNumValues);
+  static_assert(LanguageModeSize <= LanguageModeBit::kNumValues);
 
   static uint8_t Encode(LanguageMode language_mode,
                         LookupHoistingMode lookup_hoisting_mode);
diff --git a/src/interpreter/bytecode-generator.cc b/src/interpreter/bytecode-generator.cc
index 4f66ef5e100..06f29048581 100644
--- a/src/interpreter/bytecode-generator.cc
+++ b/src/interpreter/bytecode-generator.cc
@@ -200,7 +200,7 @@ class V8_NODISCARD BytecodeGenerator::ControlScope::DeferredCommands final {
     // There's always a rethrow path.
     // TODO(leszeks): We could decouple deferred_ index and token to allow us
     // to still push this lazily.
-    STATIC_ASSERT(kRethrowToken == 0);
+    static_assert(kRethrowToken == 0);
     deferred_.push_back({CMD_RETHROW, nullptr, kRethrowToken});
   }
 
@@ -4724,7 +4724,7 @@ void BytecodeGenerator::VisitYield(Yield* expr) {
       Runtime::kInlineGeneratorGetResumeMode, generator_object());
 
   // Now dispatch on resume mode.
-  STATIC_ASSERT(JSGeneratorObject::kNext + 1 == JSGeneratorObject::kReturn);
+  static_assert(JSGeneratorObject::kNext + 1 == JSGeneratorObject::kReturn);
   BytecodeJumpTable* jump_table =
       builder()->AllocateJumpTable(2, JSGeneratorObject::kNext);
 
@@ -4873,7 +4873,7 @@ void BytecodeGenerator::VisitYieldStar(YieldStar* expr) {
         // Fallthrough to default case.
         // TODO(ignition): Add debug code to check that {resume_mode} really is
         // {JSGeneratorObject::kNext} in this case.
-        STATIC_ASSERT(JSGeneratorObject::kNext == 0);
+        static_assert(JSGeneratorObject::kNext == 0);
         {
           FeedbackSlot slot = feedback_spec()->AddCallICSlot();
           builder()->CallProperty(iterator.next(), iterator_and_input,
@@ -4881,7 +4881,7 @@ void BytecodeGenerator::VisitYieldStar(YieldStar* expr) {
           builder()->Jump(after_switch.New());
         }
 
-        STATIC_ASSERT(JSGeneratorObject::kReturn == 1);
+        static_assert(JSGeneratorObject::kReturn == 1);
         builder()->Bind(switch_jump_table, JSGeneratorObject::kReturn);
         {
           const AstRawString* return_string =
@@ -4902,7 +4902,7 @@ void BytecodeGenerator::VisitYieldStar(YieldStar* expr) {
           }
         }
 
-        STATIC_ASSERT(JSGeneratorObject::kThrow == 2);
+        static_assert(JSGeneratorObject::kThrow == 2);
         builder()->Bind(switch_jump_table, JSGeneratorObject::kThrow);
         {
           const AstRawString* throw_string =
diff --git a/src/interpreter/bytecodes.cc b/src/interpreter/bytecodes.cc
index d8ee9aa2766..e43c18c1a23 100644
--- a/src/interpreter/bytecodes.cc
+++ b/src/interpreter/bytecodes.cc
@@ -96,7 +96,7 @@ Bytecodes::kOperandKindSizes[3][BytecodeOperands::kOperandTypeCount] = {
 
 // Make sure kFirstShortStar and kLastShortStar are set correctly.
 #define ASSERT_SHORT_STAR_RANGE(Name, ...)                        \
-  STATIC_ASSERT(Bytecode::k##Name >= Bytecode::kFirstShortStar && \
+  static_assert(Bytecode::k##Name >= Bytecode::kFirstShortStar && \
                 Bytecode::k##Name <= Bytecode::kLastShortStar);
 SHORT_STAR_BYTECODE_LIST(ASSERT_SHORT_STAR_RANGE)
 #undef ASSERT_SHORT_STAR_RANGE
diff --git a/src/interpreter/bytecodes.h b/src/interpreter/bytecodes.h
index 182cd3ba1c6..b76111993ea 100644
--- a/src/interpreter/bytecodes.h
+++ b/src/interpreter/bytecodes.h
@@ -648,7 +648,7 @@ class V8_EXPORT_PRIVATE Bytecodes final : public AllStatic {
   // Return true if |bytecode| is an accumulator load without effects,
   // e.g. LdaConstant, LdaTrue, Ldar.
   static constexpr bool IsAccumulatorLoadWithoutEffects(Bytecode bytecode) {
-    STATIC_ASSERT(Bytecode::kLdar < Bytecode::kLdaImmutableCurrentContextSlot);
+    static_assert(Bytecode::kLdar < Bytecode::kLdaImmutableCurrentContextSlot);
     return bytecode >= Bytecode::kLdar &&
            bytecode <= Bytecode::kLdaImmutableCurrentContextSlot;
   }
@@ -656,7 +656,7 @@ class V8_EXPORT_PRIVATE Bytecodes final : public AllStatic {
   // Returns true if |bytecode| is a compare operation without external effects
   // (e.g., Type cooersion).
   static constexpr bool IsCompareWithoutEffects(Bytecode bytecode) {
-    STATIC_ASSERT(Bytecode::kTestReferenceEqual < Bytecode::kTestTypeOf);
+    static_assert(Bytecode::kTestReferenceEqual < Bytecode::kTestTypeOf);
     return bytecode >= Bytecode::kTestReferenceEqual &&
            bytecode <= Bytecode::kTestTypeOf;
   }
@@ -868,7 +868,7 @@ class V8_EXPORT_PRIVATE Bytecodes final : public AllStatic {
     DCHECK_LE(bytecode, Bytecode::kLast);
     DCHECK_GE(operand_scale, OperandScale::kSingle);
     DCHECK_LE(operand_scale, OperandScale::kLast);
-    STATIC_ASSERT(static_cast<int>(OperandScale::kQuadruple) == 4 &&
+    static_assert(static_cast<int>(OperandScale::kQuadruple) == 4 &&
                   OperandScale::kLast == OperandScale::kQuadruple);
     int scale_index = static_cast<int>(operand_scale) >> 1;
     return kOperandSizes[scale_index][static_cast<size_t>(bytecode)];
@@ -883,7 +883,7 @@ class V8_EXPORT_PRIVATE Bytecodes final : public AllStatic {
   // given |operand_scale|.
   static int Size(Bytecode bytecode, OperandScale operand_scale) {
     DCHECK_LE(bytecode, Bytecode::kLast);
-    STATIC_ASSERT(static_cast<int>(OperandScale::kQuadruple) == 4 &&
+    static_assert(static_cast<int>(OperandScale::kQuadruple) == 4 &&
                   OperandScale::kLast == OperandScale::kQuadruple);
     int scale_index = static_cast<int>(operand_scale) >> 1;
     return kBytecodeSizes[scale_index][static_cast<size_t>(bytecode)];
@@ -974,7 +974,7 @@ class V8_EXPORT_PRIVATE Bytecodes final : public AllStatic {
     DCHECK_LE(operand_type, OperandType::kLast);
     DCHECK_GE(operand_scale, OperandScale::kSingle);
     DCHECK_LE(operand_scale, OperandScale::kLast);
-    STATIC_ASSERT(static_cast<int>(OperandScale::kQuadruple) == 4 &&
+    static_assert(static_cast<int>(OperandScale::kQuadruple) == 4 &&
                   OperandScale::kLast == OperandScale::kQuadruple);
     int scale_index = static_cast<int>(operand_scale) >> 1;
     return kOperandKindSizes[scale_index][static_cast<size_t>(operand_type)];
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index e996cf7d414..42128828758 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -313,7 +313,7 @@ void InterpreterAssembler::StoreRegisterForShortStar(TNode<Object> value,
   constexpr int short_star_to_operand =
       Register(0).ToOperand() - static_cast<int>(Bytecode::kStar0);
   // Make sure the values count in the right direction.
-  STATIC_ASSERT(short_star_to_operand ==
+  static_assert(short_star_to_operand ==
                 Register(1).ToOperand() - static_cast<int>(Bytecode::kStar1));
 
   TNode<IntPtrT> offset =
@@ -1176,9 +1176,9 @@ void InterpreterAssembler::StarDispatchLookahead(TNode<WordT> target_bytecode) {
   // opcodes are never deliberately written, so we can use a one-sided check.
   // This is no less secure than the normal-length Star handler, which performs
   // no validation on its operand.
-  STATIC_ASSERT(static_cast<int>(Bytecode::kLastShortStar) + 1 ==
+  static_assert(static_cast<int>(Bytecode::kLastShortStar) + 1 ==
                 static_cast<int>(Bytecode::kIllegal));
-  STATIC_ASSERT(Bytecode::kIllegal == Bytecode::kLast);
+  static_assert(Bytecode::kIllegal == Bytecode::kLast);
   TNode<Int32T> first_short_star_bytecode =
       Int32Constant(static_cast<int>(Bytecode::kFirstShortStar));
   TNode<BoolT> is_star = Uint32GreaterThanOrEqual(
diff --git a/src/interpreter/interpreter-intrinsics.h b/src/interpreter/interpreter-intrinsics.h
index 89087346ba8..94ff5e88bbb 100644
--- a/src/interpreter/interpreter-intrinsics.h
+++ b/src/interpreter/interpreter-intrinsics.h
@@ -42,7 +42,7 @@ class IntrinsicsHelper {
 #undef DECLARE_INTRINSIC_ID
         kIdCount
   };
-  STATIC_ASSERT(static_cast<uint32_t>(IntrinsicId::kIdCount) <= kMaxUInt8);
+  static_assert(static_cast<uint32_t>(IntrinsicId::kIdCount) <= kMaxUInt8);
 
   V8_EXPORT_PRIVATE static bool IsSupported(Runtime::FunctionId function_id);
   static IntrinsicId FromRuntimeId(Runtime::FunctionId function_id);
diff --git a/src/json/json-parser.cc b/src/json/json-parser.cc
index ac8c82a2940..b682f849d36 100644
--- a/src/json/json-parser.cc
+++ b/src/json/json-parser.cc
@@ -559,7 +559,7 @@ Handle<Object> JsonParser<Char>::BuildJsonObject(
   Handle<ByteArray> mutable_double_buffer;
   // Allocate enough space so we can double-align the payload.
   const int kMutableDoubleSize = sizeof(double) * 2;
-  STATIC_ASSERT(HeapNumber::kSize <= kMutableDoubleSize);
+  static_assert(HeapNumber::kSize <= kMutableDoubleSize);
   if (new_mutable_double > 0) {
     mutable_double_buffer =
         factory()->NewByteArray(kMutableDoubleSize * new_mutable_double);
@@ -937,8 +937,8 @@ Handle<Object> JsonParser<Char>::ParseJsonNumber() {
         return handle(Smi::FromInt(0), isolate_);
       }
       c = CurrentCharacter();
-      STATIC_ASSERT(Smi::IsValid(-999999999));
-      STATIC_ASSERT(Smi::IsValid(999999999));
+      static_assert(Smi::IsValid(-999999999));
+      static_assert(Smi::IsValid(999999999));
       const int kMaxSmiLength = 9;
       if ((cursor_ - smi_start) <= kMaxSmiLength &&
           (!base::IsInRange(c, 0,
diff --git a/src/json/json-parser.h b/src/json/json-parser.h
index 543f24c678d..7e532e2a94b 100644
--- a/src/json/json-parser.h
+++ b/src/json/json-parser.h
@@ -237,7 +237,7 @@ class JsonParser final {
     // There's at least 1 character, we always consume a character and compare
     // the next character. The first character was compared before we jumped
     // to ScanLiteral.
-    STATIC_ASSERT(N > 2);
+    static_assert(N > 2);
     size_t remaining = static_cast<size_t>(end_ - cursor_);
     if (V8_LIKELY(remaining >= N - 1 &&
                   CompareCharsEqual(s + 1, cursor_ + 1, N - 2))) {
diff --git a/src/json/json-stringifier.cc b/src/json/json-stringifier.cc
index c224f26f1ef..ef56a645c1c 100644
--- a/src/json/json-stringifier.cc
+++ b/src/json/json-stringifier.cc
@@ -677,7 +677,7 @@ JsonStringifier::Result JsonStringifier::SerializeJSArray(
     uint32_t limit = std::min(length, kInterruptLength);
     const uint32_t kMaxAllowedFastPackedLength =
         std::numeric_limits<uint32_t>::max() - kInterruptLength;
-    STATIC_ASSERT(FixedArray::kMaxLength < kMaxAllowedFastPackedLength);
+    static_assert(FixedArray::kMaxLength < kMaxAllowedFastPackedLength);
     switch (object->GetElementsKind(cage_base)) {
       case PACKED_SMI_ELEMENTS: {
         Handle<FixedArray> elements(
diff --git a/src/maglev/maglev-ir.cc b/src/maglev/maglev-ir.cc
index 60e8a9f49c9..ad5cb16f2f7 100644
--- a/src/maglev/maglev-ir.cc
+++ b/src/maglev/maglev-ir.cc
@@ -272,7 +272,7 @@ void EmitEagerDeopt(MaglevCodeGenState* code_gen_state,
 
 template <typename NodeT>
 void EmitEagerDeopt(MaglevCodeGenState* code_gen_state, NodeT* node) {
-  STATIC_ASSERT(NodeT::kProperties.can_eager_deopt());
+  static_assert(NodeT::kProperties.can_eager_deopt());
   EmitEagerDeopt(code_gen_state, node->eager_deopt_info());
 }
 
@@ -286,7 +286,7 @@ void EmitEagerDeoptIf(Condition cond, MaglevCodeGenState* code_gen_state,
 template <typename NodeT>
 void EmitEagerDeoptIf(Condition cond, MaglevCodeGenState* code_gen_state,
                       NodeT* node) {
-  STATIC_ASSERT(NodeT::kProperties.can_eager_deopt());
+  static_assert(NodeT::kProperties.can_eager_deopt());
   EmitEagerDeoptIf(cond, code_gen_state, node->eager_deopt_info());
 }
 
diff --git a/src/maglev/maglev-ir.h b/src/maglev/maglev-ir.h
index 691e94ccbe7..7b070bee385 100644
--- a/src/maglev/maglev-ir.h
+++ b/src/maglev/maglev-ir.h
@@ -414,7 +414,7 @@ class NodeBase : public ZoneObject {
  private:
   // Bitfield specification.
   using OpcodeField = base::BitField<Opcode, 0, 6>;
-  STATIC_ASSERT(OpcodeField::is_valid(kLastOpcode));
+  static_assert(OpcodeField::is_valid(kLastOpcode));
   using OpPropertiesField =
       OpcodeField::Next<OpProperties, OpProperties::kSize>;
   using InputCountField = OpPropertiesField::Next<uint16_t, 16>;
@@ -451,7 +451,7 @@ class NodeBase : public ZoneObject {
       new (node->eager_deopt_info_address())
           EagerDeoptInfo(zone, compilation_unit, checkpoint);
     } else {
-      STATIC_ASSERT(Derived::kProperties.can_lazy_deopt());
+      static_assert(Derived::kProperties.can_lazy_deopt());
       new (node->lazy_deopt_info_address())
           LazyDeoptInfo(zone, compilation_unit, checkpoint);
     }
@@ -886,7 +886,7 @@ ValueLocation& Node::result() {
 
 template <class Derived>
 class NodeT : public Node {
-  STATIC_ASSERT(!IsValueNode(opcode_of<Derived>));
+  static_assert(!IsValueNode(opcode_of<Derived>));
 
  public:
   constexpr Opcode opcode() const { return opcode_of<Derived>; }
@@ -918,7 +918,7 @@ class FixedInputNodeT : public NodeT<Derived> {
 
 template <class Derived>
 class ValueNodeT : public ValueNode {
-  STATIC_ASSERT(IsValueNode(opcode_of<Derived>));
+  static_assert(IsValueNode(opcode_of<Derived>));
 
  public:
   constexpr Opcode opcode() const { return opcode_of<Derived>; }
@@ -1784,7 +1784,7 @@ class UnconditionalControlNode : public ControlNode {
 
 template <class Derived>
 class UnconditionalControlNodeT : public UnconditionalControlNode {
-  STATIC_ASSERT(IsUnconditionalControlNode(opcode_of<Derived>));
+  static_assert(IsUnconditionalControlNode(opcode_of<Derived>));
   static constexpr size_t kInputCount = 0;
 
  public:
@@ -1828,7 +1828,7 @@ class ConditionalControlNode : public ControlNode {
 
 template <size_t InputCount, class Derived>
 class ConditionalControlNodeT : public ConditionalControlNode {
-  STATIC_ASSERT(IsConditionalControlNode(opcode_of<Derived>));
+  static_assert(IsConditionalControlNode(opcode_of<Derived>));
   static constexpr size_t kInputCount = InputCount;
 
  public:
diff --git a/src/numbers/conversions.cc b/src/numbers/conversions.cc
index 35062c46074..35a9adde591 100644
--- a/src/numbers/conversions.cc
+++ b/src/numbers/conversions.cc
@@ -462,7 +462,7 @@ void StringToIntHelper<IsolateT>::DetectRadixInternal(Char current,
   }
 
   DCHECK(radix_ >= 2 && radix_ <= 36);
-  STATIC_ASSERT(String::kMaxLength <= INT_MAX);
+  static_assert(String::kMaxLength <= INT_MAX);
   cursor_ = static_cast<int>(current - start);
 }
 
diff --git a/src/objects/allocation-site.h b/src/objects/allocation-site.h
index 9a291b2a29b..01b002c379e 100644
--- a/src/objects/allocation-site.h
+++ b/src/objects/allocation-site.h
@@ -78,7 +78,7 @@ class AllocationSite : public Struct {
   using MementoFoundCountBits = base::BitField<int, 0, 26>;
   using PretenureDecisionBits = base::BitField<PretenureDecision, 26, 3>;
   using DeoptDependentCodeBit = base::BitField<bool, 29, 1>;
-  STATIC_ASSERT(PretenureDecisionBits::kMax >= kLastPretenureDecisionValue);
+  static_assert(PretenureDecisionBits::kMax >= kLastPretenureDecisionValue);
 
   // Increments the mementos found counter and returns true when the first
   // memento was found for a given allocation site.
diff --git a/src/objects/arguments.h b/src/objects/arguments.h
index ddf1ce1489b..69b85a731e9 100644
--- a/src/objects/arguments.h
+++ b/src/objects/arguments.h
@@ -50,7 +50,7 @@ class JSStrictArgumentsObject
  public:
   // Indices of in-object properties.
   static const int kLengthIndex = 0;
-  STATIC_ASSERT(kLengthIndex == JSSloppyArgumentsObject::kLengthIndex);
+  static_assert(kLengthIndex == JSSloppyArgumentsObject::kLengthIndex);
 
  private:
   DISALLOW_IMPLICIT_CONSTRUCTORS(JSStrictArgumentsObject);
diff --git a/src/objects/bigint.cc b/src/objects/bigint.cc
index a296b1406f1..1cc5a1d6a19 100644
--- a/src/objects/bigint.cc
+++ b/src/objects/bigint.cc
@@ -207,7 +207,7 @@ Handle<BigInt> MutableBigInt::NewFromInt(Isolate* isolate, int value) {
     result->set_digit(0, value);
   } else {
     if (value == kMinInt) {
-      STATIC_ASSERT(kMinInt == -kMaxInt - 1);
+      static_assert(kMinInt == -kMaxInt - 1);
       result->set_digit(0, static_cast<BigInt::digit_t>(kMaxInt) + 1);
     } else {
       result->set_digit(0, -value);
@@ -397,7 +397,7 @@ MaybeHandle<BigInt> BigInt::Exponentiate(Isolate* isolate, Handle<BigInt> base,
   }
   // For all bases >= 2, very large exponents would lead to unrepresentable
   // results.
-  STATIC_ASSERT(kMaxLengthBits < std::numeric_limits<digit_t>::max());
+  static_assert(kMaxLengthBits < std::numeric_limits<digit_t>::max());
   if (exponent->length() > 1) {
     return ThrowBigIntTooBig<BigInt>(isolate);
   }
@@ -406,7 +406,7 @@ MaybeHandle<BigInt> BigInt::Exponentiate(Isolate* isolate, Handle<BigInt> base,
   if (exp_value >= kMaxLengthBits) {
     return ThrowBigIntTooBig<BigInt>(isolate);
   }
-  STATIC_ASSERT(kMaxLengthBits <= kMaxInt);
+  static_assert(kMaxLengthBits <= kMaxInt);
   int n = static_cast<int>(exp_value);
   if (base->length() == 1 && base->digit(0) == 2) {
     // Fast path for 2^n.
@@ -777,7 +777,7 @@ bool BigInt::EqualToNumber(Handle<BigInt> x, Handle<Object> y) {
     int value = Smi::ToInt(*y);
     if (value == 0) return x->is_zero();
     // Any multi-digit BigInt is bigger than a Smi.
-    STATIC_ASSERT(sizeof(digit_t) >= sizeof(value));
+    static_assert(sizeof(digit_t) >= sizeof(value));
     return (x->length() == 1) && (x->sign() == (value < 0)) &&
            (x->digit(0) ==
             static_cast<digit_t>(std::abs(static_cast<int64_t>(value))));
@@ -801,7 +801,7 @@ ComparisonResult BigInt::CompareToNumber(Handle<BigInt> x, Handle<Object> y) {
                           : ComparisonResult::kLessThan;
     }
     // Any multi-digit BigInt is bigger than a Smi.
-    STATIC_ASSERT(sizeof(digit_t) >= sizeof(y_value));
+    static_assert(sizeof(digit_t) >= sizeof(y_value));
     if (x->length() > 1) return AbsoluteGreater(x_sign);
 
     digit_t abs_value = std::abs(static_cast<int64_t>(y_value));
@@ -1297,7 +1297,7 @@ Handle<BigInt> MutableBigInt::RightShiftByMaximum(Isolate* isolate, bool sign) {
 Maybe<BigInt::digit_t> MutableBigInt::ToShiftAmount(Handle<BigIntBase> x) {
   if (x->length() > 1) return Nothing<digit_t>();
   digit_t value = x->digit(0);
-  STATIC_ASSERT(kMaxLengthBits < std::numeric_limits<digit_t>::max());
+  static_assert(kMaxLengthBits < std::numeric_limits<digit_t>::max());
   if (value > kMaxLengthBits) return Nothing<digit_t>();
   return Just(value);
 }
@@ -1336,7 +1336,7 @@ uint32_t BigInt::GetBitfieldForSerialization() const {
   // In order to make the serialization format the same on 32/64 bit builds,
   // we convert the length-in-digits to length-in-bytes for serialization.
   // Being able to do this depends on having enough LengthBits:
-  STATIC_ASSERT(kMaxLength * kDigitSize <= LengthBits::kMax);
+  static_assert(kMaxLength * kDigitSize <= LengthBits::kMax);
   int bytelength = length() * kDigitSize;
   return SignBits::encode(sign()) | LengthBits::encode(bytelength);
 }
@@ -1448,7 +1448,7 @@ MaybeHandle<BigInt> BigInt::AsUintN(Isolate* isolate, uint64_t n,
 
 Handle<BigInt> BigInt::FromInt64(Isolate* isolate, int64_t n) {
   if (n == 0) return MutableBigInt::Zero(isolate);
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   int length = 64 / kDigitBits;
   Handle<MutableBigInt> result =
       MutableBigInt::Cast(isolate->factory()->NewBigInt(length));
@@ -1470,7 +1470,7 @@ Handle<BigInt> BigInt::FromInt64(Isolate* isolate, int64_t n) {
 
 Handle<BigInt> BigInt::FromUint64(Isolate* isolate, uint64_t n) {
   if (n == 0) return MutableBigInt::Zero(isolate);
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   int length = 64 / kDigitBits;
   Handle<MutableBigInt> result =
       MutableBigInt::Cast(isolate->factory()->NewBigInt(length));
@@ -1486,7 +1486,7 @@ MaybeHandle<BigInt> BigInt::FromWords64(Isolate* isolate, int sign_bit,
     return ThrowBigIntTooBig<BigInt>(isolate);
   }
   if (words64_count == 0) return MutableBigInt::Zero(isolate);
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   int length = (64 / kDigitBits) * words64_count;
   DCHECK_GT(length, 0);
   if (kDigitBits == 32 && words[words64_count - 1] <= (1ULL << 32)) length--;
@@ -1514,7 +1514,7 @@ MaybeHandle<BigInt> BigInt::FromWords64(Isolate* isolate, int sign_bit,
 }
 
 int BigInt::Words64Count() {
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   return length() / (64 / kDigitBits) +
          (kDigitBits == 32 && length() % 2 == 1 ? 1 : 0);
 }
@@ -1546,7 +1546,7 @@ uint64_t MutableBigInt::GetRawBits(BigIntBase x, bool* lossless) {
   if (lossless != nullptr) *lossless = true;
   if (x.is_zero()) return 0;
   int len = x.length();
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   if (lossless != nullptr && len > 64 / kDigitBits) *lossless = false;
   uint64_t raw = static_cast<uint64_t>(x.digit(0));
   if (kDigitBits == 32 && len > 1) {
@@ -1570,7 +1570,7 @@ uint64_t BigInt::AsUint64(bool* lossless) {
 }
 
 void MutableBigInt::set_64_bits(uint64_t bits) {
-  STATIC_ASSERT(kDigitBits == 64 || kDigitBits == 32);
+  static_assert(kDigitBits == 64 || kDigitBits == 32);
   if (kDigitBits == 64) {
     set_digit(0, static_cast<digit_t>(bits));
   } else {
diff --git a/src/objects/bigint.h b/src/objects/bigint.h
index 016fad90cfb..b58869bc463 100644
--- a/src/objects/bigint.h
+++ b/src/objects/bigint.h
@@ -59,10 +59,10 @@ class BigIntBase : public PrimitiveHeapObject {
   // Sign and length are stored in the same bitfield.  Since the GC needs to be
   // able to read the length concurrently, the getters and setters are atomic.
   static const int kLengthFieldBits = 30;
-  STATIC_ASSERT(kMaxLength <= ((1 << kLengthFieldBits) - 1));
+  static_assert(kMaxLength <= ((1 << kLengthFieldBits) - 1));
   using SignBits = base::BitField<bool, 0, 1>;
   using LengthBits = SignBits::Next<int, kLengthFieldBits>;
-  STATIC_ASSERT(LengthBits::kLastUsedBit < 32);
+  static_assert(LengthBits::kLastUsedBit < 32);
 
   // Layout description.
 #define BIGINT_FIELDS(V)                                                  \
@@ -90,7 +90,7 @@ class BigIntBase : public PrimitiveHeapObject {
   using digit_t = uintptr_t;
   static const int kDigitSize = sizeof(digit_t);
   // kMaxLength definition assumes this:
-  STATIC_ASSERT(kDigitSize == kSystemPointerSize);
+  static_assert(kDigitSize == kSystemPointerSize);
 
   static const int kDigitBits = kDigitSize * kBitsPerByte;
   static const int kHalfDigitBits = kDigitBits / 2;
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index ca919a127c8..a4d7a044870 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -536,7 +536,7 @@ int Code::CodeSize() const { return SizeFor(raw_body_size()); }
 DEF_GETTER(Code, Size, int) { return CodeSize(); }
 
 CodeKind Code::kind() const {
-  STATIC_ASSERT(FIELD_SIZE(kFlagsOffset) == kInt32Size);
+  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
   const uint32_t flags = RELAXED_READ_UINT32_FIELD(*this, kFlagsOffset);
   return KindField::decode(flags);
 }
@@ -612,7 +612,7 @@ void Code::initialize_flags(CodeKind kind, bool is_turbofanned, int stack_slots,
                    IsTurbofannedField::encode(is_turbofanned) |
                    StackSlotsField::encode(stack_slots) |
                    IsOffHeapTrampoline::encode(is_off_heap_trampoline);
-  STATIC_ASSERT(FIELD_SIZE(kFlagsOffset) == kInt32Size);
+  static_assert(FIELD_SIZE(kFlagsOffset) == kInt32Size);
   RELAXED_WRITE_UINT32_FIELD(*this, kFlagsOffset, flags);
   DCHECK_IMPLIES(stack_slots != 0, uses_safepoint_table());
   DCHECK_IMPLIES(!uses_safepoint_table(), stack_slots == 0);
@@ -909,7 +909,7 @@ bool Code::IsExecutable() {
 
 // This field has to have relaxed atomic accessors because it is accessed in the
 // concurrent marker.
-STATIC_ASSERT(FIELD_SIZE(CodeDataContainer::kKindSpecificFlagsOffset) ==
+static_assert(FIELD_SIZE(CodeDataContainer::kKindSpecificFlagsOffset) ==
               kInt32Size);
 RELAXED_INT32_ACCESSORS(CodeDataContainer, kind_specific_flags,
                         kKindSpecificFlagsOffset)
@@ -1072,8 +1072,8 @@ RELAXED_UINT16_ACCESSORS(CodeDataContainer, flags, kFlagsOffset)
 // Ensure builtin_id field fits into int16_t, so that we can rely on sign
 // extension to convert int16_t{-1} to kNoBuiltinId.
 // If the asserts fail, update the code that use kBuiltinIdOffset below.
-STATIC_ASSERT(static_cast<int>(Builtin::kNoBuiltinId) == -1);
-STATIC_ASSERT(Builtins::kBuiltinCount < std::numeric_limits<int16_t>::max());
+static_assert(static_cast<int>(Builtin::kNoBuiltinId) == -1);
+static_assert(Builtins::kBuiltinCount < std::numeric_limits<int16_t>::max());
 
 void CodeDataContainer::initialize_flags(CodeKind kind, Builtin builtin_id) {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
@@ -1093,7 +1093,7 @@ Builtin CodeDataContainer::builtin_id() const {
   CHECK(V8_EXTERNAL_CODE_SPACE_BOOL);
   // Rely on sign-extension when converting int16_t to int to preserve
   // kNoBuiltinId value.
-  STATIC_ASSERT(static_cast<int>(static_cast<int16_t>(Builtin::kNoBuiltinId)) ==
+  static_assert(static_cast<int>(static_cast<int16_t>(Builtin::kNoBuiltinId)) ==
                 static_cast<int>(Builtin::kNoBuiltinId));
   int value = ReadField<int16_t>(kBuiltinIdOffset);
   return static_cast<Builtin>(value);
diff --git a/src/objects/code-kind.h b/src/objects/code-kind.h
index 62465860b9f..f46f5ae1040 100644
--- a/src/objects/code-kind.h
+++ b/src/objects/code-kind.h
@@ -36,14 +36,14 @@ enum class CodeKind : uint8_t {
   CODE_KIND_LIST(DEFINE_CODE_KIND_ENUM)
 #undef DEFINE_CODE_KIND_ENUM
 };
-STATIC_ASSERT(CodeKind::INTERPRETED_FUNCTION < CodeKind::BASELINE);
-STATIC_ASSERT(CodeKind::BASELINE < CodeKind::TURBOFAN);
+static_assert(CodeKind::INTERPRETED_FUNCTION < CodeKind::BASELINE);
+static_assert(CodeKind::BASELINE < CodeKind::TURBOFAN);
 
 #define V(...) +1
 static constexpr int kCodeKindCount = CODE_KIND_LIST(V);
 #undef V
 // Unlikely, but just to be safe:
-STATIC_ASSERT(kCodeKindCount <= std::numeric_limits<uint8_t>::max());
+static_assert(kCodeKindCount <= std::numeric_limits<uint8_t>::max());
 
 const char* CodeKindToString(CodeKind kind);
 
@@ -62,20 +62,20 @@ inline constexpr bool CodeKindIsStaticallyCompiled(CodeKind kind) {
 }
 
 inline constexpr bool CodeKindIsUnoptimizedJSFunction(CodeKind kind) {
-  STATIC_ASSERT(static_cast<int>(CodeKind::INTERPRETED_FUNCTION) + 1 ==
+  static_assert(static_cast<int>(CodeKind::INTERPRETED_FUNCTION) + 1 ==
                 static_cast<int>(CodeKind::BASELINE));
   return base::IsInRange(kind, CodeKind::INTERPRETED_FUNCTION,
                          CodeKind::BASELINE);
 }
 
 inline constexpr bool CodeKindIsOptimizedJSFunction(CodeKind kind) {
-  STATIC_ASSERT(static_cast<int>(CodeKind::MAGLEV) + 1 ==
+  static_assert(static_cast<int>(CodeKind::MAGLEV) + 1 ==
                 static_cast<int>(CodeKind::TURBOFAN));
   return base::IsInRange(kind, CodeKind::MAGLEV, CodeKind::TURBOFAN);
 }
 
 inline constexpr bool CodeKindIsJSFunction(CodeKind kind) {
-  STATIC_ASSERT(static_cast<int>(CodeKind::BASELINE) + 1 ==
+  static_assert(static_cast<int>(CodeKind::BASELINE) + 1 ==
                 static_cast<int>(CodeKind::MAGLEV));
   return base::IsInRange(kind, CodeKind::INTERPRETED_FUNCTION,
                          CodeKind::TURBOFAN);
@@ -112,7 +112,7 @@ enum class CodeKindFlag {
   CODE_KIND_LIST(V)
 #undef V
 };
-STATIC_ASSERT(kCodeKindCount <= kInt32Size * kBitsPerByte);
+static_assert(kCodeKindCount <= kInt32Size * kBitsPerByte);
 
 inline constexpr CodeKindFlag CodeKindToCodeKindFlag(CodeKind kind) {
 #define V(name) kind == CodeKind::name ? CodeKindFlag::name:
diff --git a/src/objects/code.cc b/src/objects/code.cc
index 0a56451b3a4..055f0e609aa 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -169,7 +169,7 @@ void Code::FlushICache() const {
 void Code::CopyFromNoFlush(ByteArray reloc_info, Heap* heap,
                            const CodeDesc& desc) {
   // Copy code.
-  STATIC_ASSERT(kOnHeapBodyIsContiguous);
+  static_assert(kOnHeapBodyIsContiguous);
   CopyBytes(reinterpret_cast<byte*>(raw_instruction_start()), desc.buffer,
             static_cast<size_t>(desc.instr_size));
   // TODO(jgruber,v8:11036): Merge with the above.
@@ -292,7 +292,7 @@ bool Code::IsIsolateIndependent(Isolate* isolate) {
       ~RelocInfo::ModeMask(RelocInfo::CONST_POOL) &
       ~RelocInfo::ModeMask(RelocInfo::OFF_HEAP_TARGET) &
       ~RelocInfo::ModeMask(RelocInfo::VENEER_POOL);
-  STATIC_ASSERT(kModeMask ==
+  static_assert(kModeMask ==
                 (RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
                  RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET) |
                  RelocInfo::ModeMask(RelocInfo::COMPRESSED_EMBEDDED_OBJECT) |
diff --git a/src/objects/code.h b/src/objects/code.h
index eed9454a786..def7cd1196b 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -196,8 +196,8 @@ class CodeDataContainer : public HeapObject {
 
   DEFINE_BIT_FIELDS(FLAGS_BIT_FIELDS)
 #undef FLAGS_BIT_FIELDS
-  STATIC_ASSERT(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
-  STATIC_ASSERT(!V8_EXTERNAL_CODE_SPACE_BOOL ||
+  static_assert(FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
+  static_assert(!V8_EXTERNAL_CODE_SPACE_BOOL ||
                 (FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
                  FIELD_SIZE(CodeDataContainer::kFlagsOffset) * kBitsPerByte));
 
@@ -672,7 +672,7 @@ class Code : public HeapObject {
 #else
 #error Unknown architecture.
 #endif
-  STATIC_ASSERT(FIELD_SIZE(kOptionalPaddingOffset) == kHeaderPaddingSize);
+  static_assert(FIELD_SIZE(kOptionalPaddingOffset) == kHeaderPaddingSize);
 
   class BodyDescriptor;
 
@@ -684,9 +684,9 @@ class Code : public HeapObject {
   V(IsOffHeapTrampoline, bool, 1, _)
   DEFINE_BIT_FIELDS(CODE_FLAGS_BIT_FIELDS)
 #undef CODE_FLAGS_BIT_FIELDS
-  STATIC_ASSERT(kCodeKindCount <= KindField::kNumValues);
-  STATIC_ASSERT(CODE_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 30);
-  STATIC_ASSERT(CODE_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
+  static_assert(kCodeKindCount <= KindField::kNumValues);
+  static_assert(CODE_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 30);
+  static_assert(CODE_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
                 FIELD_SIZE(kFlagsOffset) * kBitsPerByte);
 
   // KindSpecificFlags layout.
@@ -697,8 +697,8 @@ class Code : public HeapObject {
   V(IsPromiseRejectionField, bool, 1, _)
   DEFINE_BIT_FIELDS(CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS)
 #undef CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS
-  STATIC_ASSERT(CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
-  STATIC_ASSERT(CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
+  static_assert(CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount == 4);
+  static_assert(CODE_KIND_SPECIFIC_FLAGS_BIT_FIELDS_Ranges::kBitsCount <=
                 FIELD_SIZE(CodeDataContainer::kKindSpecificFlagsOffset) *
                     kBitsPerByte);
 
diff --git a/src/objects/compilation-cache-table-inl.h b/src/objects/compilation-cache-table-inl.h
index 21483945faa..afc85ba5bf9 100644
--- a/src/objects/compilation-cache-table-inl.h
+++ b/src/objects/compilation-cache-table-inl.h
@@ -36,7 +36,7 @@ void CompilationCacheTable::SetPrimaryValueAt(InternalIndex entry, Object value,
 }
 
 Object CompilationCacheTable::EvalFeedbackValueAt(InternalIndex entry) {
-  STATIC_ASSERT(CompilationCacheShape::kEntrySize == 3);
+  static_assert(CompilationCacheShape::kEntrySize == 3);
   return get(EntryToIndex(entry) + 2);
 }
 
@@ -106,7 +106,7 @@ uint32_t CompilationCacheShape::EvalHash(String source,
     Script script(Script::cast(shared.script()));
     hash ^= String::cast(script.source()).EnsureHash();
   }
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   if (is_strict(language_mode)) hash ^= 0x8000;
   hash += position;
   return hash;
diff --git a/src/objects/compilation-cache-table.cc b/src/objects/compilation-cache-table.cc
index 6167c3d02ce..a874870388a 100644
--- a/src/objects/compilation-cache-table.cc
+++ b/src/objects/compilation-cache-table.cc
@@ -46,7 +46,7 @@ void AddToFeedbackCellsMap(Handle<CompilationCacheTable> cache,
                            Handle<FeedbackCell> feedback_cell) {
   Isolate* isolate = native_context->GetIsolate();
   DCHECK(native_context->IsNativeContext());
-  STATIC_ASSERT(kLiteralEntryLength == 2);
+  static_assert(kLiteralEntryLength == 2);
   Handle<WeakFixedArray> new_literals_map;
   int entry;
 
@@ -333,7 +333,7 @@ InfoCellPair CompilationCacheTable::LookupEval(
   Object obj = table->PrimaryValueAt(entry);
   if (!obj.IsSharedFunctionInfo()) return empty_result;
 
-  STATIC_ASSERT(CompilationCacheShape::kEntrySize == 3);
+  static_assert(CompilationCacheShape::kEntrySize == 3);
   FeedbackCell feedback_cell =
       SearchLiteralsMap(*table, entry, *native_context);
   return InfoCellPair(isolate, SharedFunctionInfo::cast(obj), feedback_cell);
diff --git a/src/objects/contexts-inl.h b/src/objects/contexts-inl.h
index c7e8f4513d1..5faff5547fc 100644
--- a/src/objects/contexts-inl.h
+++ b/src/objects/contexts-inl.h
@@ -202,7 +202,7 @@ bool Context::HasSameSecurityTokenAs(Context that) const {
 NATIVE_CONTEXT_FIELDS(NATIVE_CONTEXT_FIELD_ACCESSORS)
 #undef NATIVE_CONTEXT_FIELD_ACCESSORS
 
-#define CHECK_FOLLOWS2(v1, v2) STATIC_ASSERT((v1 + 1) == (v2))
+#define CHECK_FOLLOWS2(v1, v2) static_assert((v1 + 1) == (v2))
 #define CHECK_FOLLOWS4(v1, v2, v3, v4) \
   CHECK_FOLLOWS2(v1, v2);              \
   CHECK_FOLLOWS2(v2, v3);              \
diff --git a/src/objects/contexts.cc b/src/objects/contexts.cc
index 2f514ac1e7b..eb5d9a711b9 100644
--- a/src/objects/contexts.cc
+++ b/src/objects/contexts.cc
@@ -532,22 +532,22 @@ void NativeContext::IncrementErrorsThrown() {
 
 int NativeContext::GetErrorsThrown() { return errors_thrown().value(); }
 
-STATIC_ASSERT(Context::MIN_CONTEXT_SLOTS == 2);
-STATIC_ASSERT(Context::MIN_CONTEXT_EXTENDED_SLOTS == 3);
-STATIC_ASSERT(NativeContext::kScopeInfoOffset ==
+static_assert(Context::MIN_CONTEXT_SLOTS == 2);
+static_assert(Context::MIN_CONTEXT_EXTENDED_SLOTS == 3);
+static_assert(NativeContext::kScopeInfoOffset ==
               Context::OffsetOfElementAt(NativeContext::SCOPE_INFO_INDEX));
-STATIC_ASSERT(NativeContext::kPreviousOffset ==
+static_assert(NativeContext::kPreviousOffset ==
               Context::OffsetOfElementAt(NativeContext::PREVIOUS_INDEX));
-STATIC_ASSERT(NativeContext::kExtensionOffset ==
+static_assert(NativeContext::kExtensionOffset ==
               Context::OffsetOfElementAt(NativeContext::EXTENSION_INDEX));
 
-STATIC_ASSERT(NativeContext::kStartOfStrongFieldsOffset ==
+static_assert(NativeContext::kStartOfStrongFieldsOffset ==
               Context::OffsetOfElementAt(-1));
-STATIC_ASSERT(NativeContext::kStartOfWeakFieldsOffset ==
+static_assert(NativeContext::kStartOfWeakFieldsOffset ==
               Context::OffsetOfElementAt(NativeContext::FIRST_WEAK_SLOT));
-STATIC_ASSERT(NativeContext::kMicrotaskQueueOffset ==
+static_assert(NativeContext::kMicrotaskQueueOffset ==
               Context::SizeFor(NativeContext::NATIVE_CONTEXT_SLOTS));
-STATIC_ASSERT(NativeContext::kSize ==
+static_assert(NativeContext::kSize ==
               (Context::SizeFor(NativeContext::NATIVE_CONTEXT_SLOTS) +
                kSystemPointerSize));
 
diff --git a/src/objects/contexts.h b/src/objects/contexts.h
index 412c50a8573..447142ff2ad 100644
--- a/src/objects/contexts.h
+++ b/src/objects/contexts.h
@@ -783,7 +783,7 @@ class NativeContext : public Context {
                       Handle<Object> parent);
 
  private:
-  STATIC_ASSERT(OffsetOfElementAt(EMBEDDER_DATA_INDEX) ==
+  static_assert(OffsetOfElementAt(EMBEDDER_DATA_INDEX) ==
                 Internals::kNativeContextEmbedderDataOffset);
 
   OBJECT_CONSTRUCTORS(NativeContext, Context);
diff --git a/src/objects/descriptor-array.h b/src/objects/descriptor-array.h
index 38a18741028..857f672f3f1 100644
--- a/src/objects/descriptor-array.h
+++ b/src/objects/descriptor-array.h
@@ -147,8 +147,8 @@ class DescriptorArray
   // Constant for denoting key was not found.
   static const int kNotFound = -1;
 
-  STATIC_ASSERT(IsAligned(kStartOfWeakFieldsOffset, kTaggedSize));
-  STATIC_ASSERT(IsAligned(kHeaderSize, kTaggedSize));
+  static_assert(IsAligned(kStartOfWeakFieldsOffset, kTaggedSize));
+  static_assert(IsAligned(kHeaderSize, kTaggedSize));
 
   // Garbage collection support.
   DECL_INT16_ACCESSORS(raw_number_of_marked_descriptors)
diff --git a/src/objects/dictionary-inl.h b/src/objects/dictionary-inl.h
index 3b1ed74f64c..a13a6029494 100644
--- a/src/objects/dictionary-inl.h
+++ b/src/objects/dictionary-inl.h
@@ -175,7 +175,7 @@ template <typename Key>
 template <typename Dictionary>
 PropertyDetails BaseDictionaryShape<Key>::DetailsAt(Dictionary dict,
                                                     InternalIndex entry) {
-  STATIC_ASSERT(Dictionary::kEntrySize == 3);
+  static_assert(Dictionary::kEntrySize == 3);
   DCHECK(entry.is_found());
   return PropertyDetails(Smi::cast(dict.get(Dictionary::EntryToIndex(entry) +
                                             Dictionary::kEntryDetailsIndex)));
@@ -186,7 +186,7 @@ template <typename Dictionary>
 void BaseDictionaryShape<Key>::DetailsAtPut(Dictionary dict,
                                             InternalIndex entry,
                                             PropertyDetails value) {
-  STATIC_ASSERT(Dictionary::kEntrySize == 3);
+  static_assert(Dictionary::kEntrySize == 3);
   dict.set(Dictionary::EntryToIndex(entry) + Dictionary::kEntryDetailsIndex,
            value.AsSmi());
 }
diff --git a/src/objects/elements-kind.cc b/src/objects/elements-kind.cc
index 4978e05f7ae..54e71ff2bfd 100644
--- a/src/objects/elements-kind.cc
+++ b/src/objects/elements-kind.cc
@@ -70,7 +70,7 @@ int ElementsKindToByteSize(ElementsKind elements_kind) {
 }
 
 int GetDefaultHeaderSizeForElementsKind(ElementsKind elements_kind) {
-  STATIC_ASSERT(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
+  static_assert(FixedArray::kHeaderSize == FixedDoubleArray::kHeaderSize);
 
   if (IsTypedArrayElementsKind(elements_kind)) {
     return 0;
@@ -138,13 +138,13 @@ const ElementsKind kFastElementsKindSequence[kFastElementsKindCount] = {
     PACKED_ELEMENTS,         // 4
     HOLEY_ELEMENTS           // 5
 };
-STATIC_ASSERT(PACKED_SMI_ELEMENTS == FIRST_FAST_ELEMENTS_KIND);
+static_assert(PACKED_SMI_ELEMENTS == FIRST_FAST_ELEMENTS_KIND);
 // Verify that kFastElementsKindPackedToHoley is correct.
-STATIC_ASSERT(PACKED_SMI_ELEMENTS + kFastElementsKindPackedToHoley ==
+static_assert(PACKED_SMI_ELEMENTS + kFastElementsKindPackedToHoley ==
               HOLEY_SMI_ELEMENTS);
-STATIC_ASSERT(PACKED_DOUBLE_ELEMENTS + kFastElementsKindPackedToHoley ==
+static_assert(PACKED_DOUBLE_ELEMENTS + kFastElementsKindPackedToHoley ==
               HOLEY_DOUBLE_ELEMENTS);
-STATIC_ASSERT(PACKED_ELEMENTS + kFastElementsKindPackedToHoley ==
+static_assert(PACKED_ELEMENTS + kFastElementsKindPackedToHoley ==
               HOLEY_ELEMENTS);
 
 ElementsKind GetFastElementsKindFromSequenceIndex(int sequence_number) {
diff --git a/src/objects/elements-kind.h b/src/objects/elements-kind.h
index a7d8306c3d4..861dfabc41c 100644
--- a/src/objects/elements-kind.h
+++ b/src/objects/elements-kind.h
@@ -158,12 +158,12 @@ constexpr int kFastElementsKindPackedToHoley =
     HOLEY_SMI_ELEMENTS - PACKED_SMI_ELEMENTS;
 
 constexpr int kElementsKindBits = 6;
-STATIC_ASSERT((1 << kElementsKindBits) > LAST_ELEMENTS_KIND);
-STATIC_ASSERT((1 << (kElementsKindBits - 1)) <= LAST_ELEMENTS_KIND);
+static_assert((1 << kElementsKindBits) > LAST_ELEMENTS_KIND);
+static_assert((1 << (kElementsKindBits - 1)) <= LAST_ELEMENTS_KIND);
 
 constexpr int kFastElementsKindBits = 3;
-STATIC_ASSERT((1 << kFastElementsKindBits) > LAST_FAST_ELEMENTS_KIND);
-STATIC_ASSERT((1 << (kFastElementsKindBits - 1)) <= LAST_FAST_ELEMENTS_KIND);
+static_assert((1 << kFastElementsKindBits) > LAST_FAST_ELEMENTS_KIND);
+static_assert((1 << (kFastElementsKindBits - 1)) <= LAST_FAST_ELEMENTS_KIND);
 
 V8_EXPORT_PRIVATE int ElementsKindToShiftSize(ElementsKind elements_kind);
 V8_EXPORT_PRIVATE int ElementsKindToByteSize(ElementsKind elements_kind);
@@ -236,7 +236,7 @@ inline bool IsTerminalElementsKind(ElementsKind kind) {
 }
 
 inline bool IsFastElementsKind(ElementsKind kind) {
-  STATIC_ASSERT(FIRST_FAST_ELEMENTS_KIND == 0);
+  static_assert(FIRST_FAST_ELEMENTS_KIND == 0);
   return kind <= LAST_FAST_ELEMENTS_KIND;
 }
 
diff --git a/src/objects/elements.cc b/src/objects/elements.cc
index 051f6952dd4..276671ad14f 100644
--- a/src/objects/elements.cc
+++ b/src/objects/elements.cc
@@ -1988,7 +1988,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     // normalization frequently enough. At a minimum, it should be large
     // enough to reliably hit the "window" of remaining elements count where
     // normalization would be beneficial.
-    STATIC_ASSERT(kLengthFraction >=
+    static_assert(kLengthFraction >=
                   NumberDictionary::kEntrySize *
                       NumberDictionary::kPreferFastElementsSizeFactor);
     size_t current_counter = isolate->elements_deletion_counter();
@@ -2613,7 +2613,7 @@ class FastSmiOrObjectElementsAccessor
     // elements->get(k) can return the hole, for which the StrictEquals will
     // always fail.
     FixedArray elements = FixedArray::cast(receiver->elements());
-    STATIC_ASSERT(FixedArray::kMaxLength <=
+    static_assert(FixedArray::kMaxLength <=
                   std::numeric_limits<uint32_t>::max());
     for (size_t k = start_from; k < length; ++k) {
       if (value.StrictEquals(elements.get(static_cast<uint32_t>(k)))) {
@@ -3012,7 +3012,7 @@ class FastDoubleElementsAccessor
     double numeric_search_value = value.Number();
     FixedDoubleArray elements = FixedDoubleArray::cast(receiver->elements());
 
-    STATIC_ASSERT(FixedDoubleArray::kMaxLength <=
+    static_assert(FixedDoubleArray::kMaxLength <=
                   std::numeric_limits<int>::max());
     for (size_t k = start_from; k < length; ++k) {
       int k_int = static_cast<int>(k);
@@ -3110,7 +3110,7 @@ class TypedElementsAccessor
     if (IsAligned(reinterpret_cast<uintptr_t>(data_ptr),
                   alignof(std::atomic<ElementType>))) {
       // Use a single relaxed atomic store.
-      STATIC_ASSERT(sizeof(std::atomic<ElementType>) == sizeof(ElementType));
+      static_assert(sizeof(std::atomic<ElementType>) == sizeof(ElementType));
       reinterpret_cast<std::atomic<ElementType>*>(data_ptr)->store(
           value, std::memory_order_relaxed);
       return;
@@ -3133,7 +3133,7 @@ class TypedElementsAccessor
     CHECK_EQ(sizeof(words), sizeof(value));
     memcpy(words, &value, sizeof(value));
     for (size_t word = 0; word < kNumWords; ++word) {
-      STATIC_ASSERT(sizeof(std::atomic<uint32_t>) == sizeof(uint32_t));
+      static_assert(sizeof(std::atomic<uint32_t>) == sizeof(uint32_t));
       reinterpret_cast<std::atomic<uint32_t>*>(data_ptr)[word].store(
           words[word], std::memory_order_relaxed);
     }
@@ -3173,7 +3173,7 @@ class TypedElementsAccessor
     if (IsAligned(reinterpret_cast<uintptr_t>(data_ptr),
                   alignof(std::atomic<ElementType>))) {
       // Use a single relaxed atomic load.
-      STATIC_ASSERT(sizeof(std::atomic<ElementType>) == sizeof(ElementType));
+      static_assert(sizeof(std::atomic<ElementType>) == sizeof(ElementType));
       // Note: acquire semantics are not needed here, but clang seems to merge
       // this atomic load with the non-atomic load above if we use relaxed
       // semantics. This will result in TSan failures.
@@ -3196,7 +3196,7 @@ class TypedElementsAccessor
         std::max(size_t{1}, sizeof(ElementType) / kInt32Size);
     uint32_t words[kNumWords];
     for (size_t word = 0; word < kNumWords; ++word) {
-      STATIC_ASSERT(sizeof(std::atomic<uint32_t>) == sizeof(uint32_t));
+      static_assert(sizeof(std::atomic<uint32_t>) == sizeof(uint32_t));
       words[word] =
           reinterpret_cast<std::atomic<uint32_t>*>(data_ptr)[word].load(
               std::memory_order_relaxed);
@@ -5277,7 +5277,7 @@ void ElementsAccessor::InitializeOncePerProcess() {
 #undef ACCESSOR_ARRAY
   };
 
-  STATIC_ASSERT((sizeof(accessor_array) / sizeof(*accessor_array)) ==
+  static_assert((sizeof(accessor_array) / sizeof(*accessor_array)) ==
                 kElementsKindCount);
 
   elements_accessors_ = accessor_array;
diff --git a/src/objects/embedder-data-array.h b/src/objects/embedder-data-array.h
index 5c4389c16db..d0fd4c3ee2b 100644
--- a/src/objects/embedder-data-array.h
+++ b/src/objects/embedder-data-array.h
@@ -56,7 +56,7 @@ class EmbedderDataArray
       (kMaxSize - kHeaderSize) / kEmbedderDataSlotSize;
 
  private:
-  STATIC_ASSERT(kHeaderSize == Internals::kFixedArrayHeaderSize);
+  static_assert(kHeaderSize == Internals::kFixedArrayHeaderSize);
 
   TQ_OBJECT_CONSTRUCTORS(EmbedderDataArray)
 };
diff --git a/src/objects/embedder-data-slot-inl.h b/src/objects/embedder-data-slot-inl.h
index e4d63e5b32a..2a2ff513837 100644
--- a/src/objects/embedder-data-slot-inl.h
+++ b/src/objects/embedder-data-slot-inl.h
@@ -151,9 +151,9 @@ void EmbedderDataSlot::store_raw(Isolate* isolate,
 
 void EmbedderDataSlot::gc_safe_store(Isolate* isolate, Address value) {
 #ifdef V8_COMPRESS_POINTERS
-  STATIC_ASSERT(kSmiShiftSize == 0);
-  STATIC_ASSERT(SmiValuesAre31Bits());
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kSmiShiftSize == 0);
+  static_assert(SmiValuesAre31Bits());
+  static_assert(kTaggedSize == kInt32Size);
 
   // We have to do two 32-bit stores here because
   // 1) tagged part modifications must be atomic to be properly synchronized
@@ -177,11 +177,11 @@ void EmbedderDataSlot::PopulateEmbedderDataSnapshot(
     Map map, JSObject js_object, int entry_index,
     EmbedderDataSlotSnapshot& snapshot) {
 #ifdef V8_COMPRESS_POINTERS
-  STATIC_ASSERT(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t) * 2);
+  static_assert(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t) * 2);
 #else   // !V8_COMPRESS_POINTERS
-  STATIC_ASSERT(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t));
+  static_assert(sizeof(EmbedderDataSlotSnapshot) == sizeof(AtomicTagged_t));
 #endif  // !V8_COMPRESS_POINTERS
-  STATIC_ASSERT(sizeof(EmbedderDataSlotSnapshot) == kEmbedderDataSlotSize);
+  static_assert(sizeof(EmbedderDataSlotSnapshot) == kEmbedderDataSlotSize);
 
   const Address field_base =
       FIELD_ADDR(js_object, js_object.GetEmbedderFieldOffset(entry_index));
diff --git a/src/objects/feedback-vector-inl.h b/src/objects/feedback-vector-inl.h
index e2445de393c..b3bd3799622 100644
--- a/src/objects/feedback-vector-inl.h
+++ b/src/objects/feedback-vector-inl.h
@@ -129,7 +129,7 @@ int FeedbackVector::osr_urgency() const {
 
 void FeedbackVector::set_osr_urgency(int urgency) {
   DCHECK(0 <= urgency && urgency <= FeedbackVector::kMaxOsrUrgency);
-  STATIC_ASSERT(FeedbackVector::kMaxOsrUrgency <= OsrUrgencyBits::kMax);
+  static_assert(FeedbackVector::kMaxOsrUrgency <= OsrUrgencyBits::kMax);
   set_osr_state(OsrUrgencyBits::update(osr_state(), urgency));
 }
 
diff --git a/src/objects/feedback-vector.cc b/src/objects/feedback-vector.cc
index f22ebbbfecd..871970fe7aa 100644
--- a/src/objects/feedback-vector.cc
+++ b/src/objects/feedback-vector.cc
@@ -445,8 +445,8 @@ TieringState FeedbackVector::osr_tiering_state() {
 
 void FeedbackVector::set_osr_tiering_state(TieringState marker) {
   DCHECK(marker == TieringState::kNone || marker == TieringState::kInProgress);
-  STATIC_ASSERT(TieringState::kNone <= OsrTieringStateBit::kMax);
-  STATIC_ASSERT(TieringState::kInProgress <= OsrTieringStateBit::kMax);
+  static_assert(TieringState::kNone <= OsrTieringStateBit::kMax);
+  static_assert(TieringState::kInProgress <= OsrTieringStateBit::kMax);
   int32_t state = flags();
   state = OsrTieringStateBit::update(state, marker);
   set_flags(state);
diff --git a/src/objects/feedback-vector.h b/src/objects/feedback-vector.h
index eeb8e6376ad..5cd7bcc740c 100644
--- a/src/objects/feedback-vector.h
+++ b/src/objects/feedback-vector.h
@@ -147,11 +147,11 @@ inline LanguageMode GetLanguageModeFromSlotKind(FeedbackSlotKind kind) {
   DCHECK(IsSetNamedICKind(kind) || IsDefineNamedOwnICKind(kind) ||
          IsStoreGlobalICKind(kind) || IsKeyedStoreICKind(kind) ||
          IsDefineKeyedOwnICKind(kind));
-  STATIC_ASSERT(FeedbackSlotKind::kStoreGlobalSloppy <=
+  static_assert(FeedbackSlotKind::kStoreGlobalSloppy <=
                 FeedbackSlotKind::kLastSloppyKind);
-  STATIC_ASSERT(FeedbackSlotKind::kSetKeyedSloppy <=
+  static_assert(FeedbackSlotKind::kSetKeyedSloppy <=
                 FeedbackSlotKind::kLastSloppyKind);
-  STATIC_ASSERT(FeedbackSlotKind::kSetNamedSloppy <=
+  static_assert(FeedbackSlotKind::kSetNamedSloppy <=
                 FeedbackSlotKind::kLastSloppyKind);
   return (kind <= FeedbackSlotKind::kLastSloppyKind) ? LanguageMode::kSloppy
                                                      : LanguageMode::kStrict;
@@ -199,7 +199,7 @@ class FeedbackVector
   NEVER_READ_ONLY_SPACE
   DEFINE_TORQUE_GENERATED_OSR_STATE()
   DEFINE_TORQUE_GENERATED_FEEDBACK_VECTOR_FLAGS()
-  STATIC_ASSERT(TieringState::kLastTieringState <= TieringStateBits::kMax);
+  static_assert(TieringState::kLastTieringState <= TieringStateBits::kMax);
 
   static const bool kFeedbackVectorMaybeOptimizedCodeIsStoreRelease = true;
   using TorqueGeneratedFeedbackVector<FeedbackVector,
@@ -229,7 +229,7 @@ class FeedbackVector
   // the function becomes hotter. When the current loop depth is less than the
   // osr_urgency, JumpLoop calls into runtime to attempt OSR optimization.
   static constexpr int kMaxOsrUrgency = 6;
-  STATIC_ASSERT(kMaxOsrUrgency <= OsrUrgencyBits::kMax);
+  static_assert(kMaxOsrUrgency <= OsrUrgencyBits::kMax);
   inline int osr_urgency() const;
   inline void set_osr_urgency(int urgency);
   inline void reset_osr_urgency();
@@ -436,7 +436,7 @@ class V8_EXPORT_PRIVATE FeedbackVectorSpec {
   }
 
   FeedbackSlotKind GetStoreICSlot(LanguageMode language_mode) {
-    STATIC_ASSERT(LanguageModeSize == 2);
+    static_assert(LanguageModeSize == 2);
     return is_strict(language_mode) ? FeedbackSlotKind::kSetNamedStrict
                                     : FeedbackSlotKind::kSetNamedSloppy;
   }
@@ -456,14 +456,14 @@ class V8_EXPORT_PRIVATE FeedbackVectorSpec {
   }
 
   FeedbackSlot AddStoreGlobalICSlot(LanguageMode language_mode) {
-    STATIC_ASSERT(LanguageModeSize == 2);
+    static_assert(LanguageModeSize == 2);
     return AddSlot(is_strict(language_mode)
                        ? FeedbackSlotKind::kStoreGlobalStrict
                        : FeedbackSlotKind::kStoreGlobalSloppy);
   }
 
   FeedbackSlotKind GetKeyedStoreICSlotKind(LanguageMode language_mode) {
-    STATIC_ASSERT(LanguageModeSize == 2);
+    static_assert(LanguageModeSize == 2);
     return is_strict(language_mode) ? FeedbackSlotKind::kSetKeyedStrict
                                     : FeedbackSlotKind::kSetKeyedSloppy;
   }
@@ -518,7 +518,7 @@ class V8_EXPORT_PRIVATE FeedbackVectorSpec {
 
   void append(FeedbackSlotKind kind) { slot_kinds_.push_back(kind); }
 
-  STATIC_ASSERT(sizeof(FeedbackSlotKind) == sizeof(uint8_t));
+  static_assert(sizeof(FeedbackSlotKind) == sizeof(uint8_t));
   ZoneVector<FeedbackSlotKind> slot_kinds_;
   int create_closure_slot_count_ = 0;
 
@@ -617,7 +617,7 @@ class FeedbackMetadata : public HeapObject {
   inline int length() const;
 
   static const int kFeedbackSlotKindBits = 5;
-  STATIC_ASSERT(kFeedbackSlotKindCount <= (1 << kFeedbackSlotKindBits));
+  static_assert(kFeedbackSlotKindCount <= (1 << kFeedbackSlotKindBits));
 
   void SetKind(FeedbackSlot slot, FeedbackSlotKind kind);
 
@@ -630,10 +630,10 @@ class FeedbackMetadata : public HeapObject {
 
 // Verify that an empty hash field looks like a tagged object, but can't
 // possibly be confused with a pointer.
-STATIC_ASSERT((Name::kEmptyHashField & kHeapObjectTag) == kHeapObjectTag);
-STATIC_ASSERT(Name::kEmptyHashField == 0x3);
+static_assert((Name::kEmptyHashField & kHeapObjectTag) == kHeapObjectTag);
+static_assert(Name::kEmptyHashField == 0x3);
 // Verify that a set hash field will not look like a tagged object.
-STATIC_ASSERT(Name::kHashNotComputedMask == kHeapObjectTag);
+static_assert(Name::kHashNotComputedMask == kHeapObjectTag);
 
 class FeedbackMetadataIterator {
  public:
@@ -871,7 +871,7 @@ class V8_EXPORT_PRIVATE FeedbackNexus final {
 #undef LEXICAL_MODE_BIT_FIELDS
 
   // Make sure we don't overflow the smi.
-  STATIC_ASSERT(LEXICAL_MODE_BIT_FIELDS_Ranges::kBitsCount <= kSmiValueSize);
+  static_assert(LEXICAL_MODE_BIT_FIELDS_Ranges::kBitsCount <= kSmiValueSize);
 
   // For TypeProfile feedback vector slots.
   // ResetTypeProfile will always reset type profile information.
diff --git a/src/objects/field-index.h b/src/objects/field-index.h
index aa1669b0321..fb09734c34b 100644
--- a/src/objects/field-index.h
+++ b/src/objects/field-index.h
@@ -121,7 +121,7 @@ class FieldIndex final {
   // Offset of first inobject property from beginning of object.
   using FirstInobjectPropertyOffsetBits =
       InObjectPropertyBits::Next<int, kFirstInobjectPropertyOffsetBitCount>;
-  STATIC_ASSERT(FirstInobjectPropertyOffsetBits::kLastUsedBit < 64);
+  static_assert(FirstInobjectPropertyOffsetBits::kLastUsedBit < 64);
 
   uint64_t bit_field_;
 };
diff --git a/src/objects/fixed-array.h b/src/objects/fixed-array.h
index f0c3bfc097d..3bc9525e176 100644
--- a/src/objects/fixed-array.h
+++ b/src/objects/fixed-array.h
@@ -89,7 +89,7 @@ class FixedArrayBase
   // which is necessary for being able to create a free space filler for the
   // whole array of kMaxSize.
   static const int kMaxSize = 128 * kTaggedSize * MB - kTaggedSize;
-  STATIC_ASSERT(Smi::IsValid(kMaxSize));
+  static_assert(Smi::IsValid(kMaxSize));
 
  protected:
   TQ_OBJECT_CONSTRUCTORS(FixedArrayBase)
@@ -182,7 +182,7 @@ class FixedArray
 
   // Code Generation support.
   static constexpr int OffsetOfElementAt(int index) {
-    STATIC_ASSERT(kObjectsOffset == SizeFor(0));
+    static_assert(kObjectsOffset == SizeFor(0));
     return SizeFor(index);
   }
 
@@ -195,7 +195,7 @@ class FixedArray
                 "FixedArray maxLength not a Smi");
 
   // Maximally allowed length for regular (non large object space) object.
-  STATIC_ASSERT(kMaxRegularHeapObjectSize < kMaxSize);
+  static_assert(kMaxRegularHeapObjectSize < kMaxSize);
   static const int kMaxRegularLength =
       (kMaxRegularHeapObjectSize - kHeaderSize) / kTaggedSize;
 
@@ -216,7 +216,7 @@ class FixedArray
                                        Object value);
 
  private:
-  STATIC_ASSERT(kHeaderSize == Internals::kFixedArrayHeaderSize);
+  static_assert(kHeaderSize == Internals::kFixedArrayHeaderSize);
 
   TQ_OBJECT_CONSTRUCTORS(FixedArray)
 };
@@ -315,7 +315,7 @@ class WeakFixedArray
   int AllocatedSize();
 
   static int OffsetOfElementAt(int index) {
-    STATIC_ASSERT(kObjectsOffset == SizeFor(0));
+    static_assert(kObjectsOffset == SizeFor(0));
     return SizeFor(index);
   }
 
@@ -483,7 +483,7 @@ class ArrayList : public TorqueGeneratedArrayList<ArrayList, FixedArray> {
 
   static const int kLengthIndex = 0;
   static const int kFirstIndex = 1;
-  STATIC_ASSERT(kHeaderFields == kFirstIndex);
+  static_assert(kHeaderFields == kFirstIndex);
 
   DECL_VERIFIER(ArrayList)
 
diff --git a/src/objects/foreign.h b/src/objects/foreign.h
index ebb219b1531..4cd706de031 100644
--- a/src/objects/foreign.h
+++ b/src/objects/foreign.h
@@ -31,9 +31,9 @@ class Foreign : public TorqueGeneratedForeign<Foreign, HeapObject> {
   // kForeignAddressOffset is only kTaggedSize aligned but we can keep using
   // unaligned access since both x64 and arm64 architectures (where pointer
   // compression is supported) allow unaligned access to full words.
-  STATIC_ASSERT(IsAligned(kForeignAddressOffset, kTaggedSize));
+  static_assert(IsAligned(kForeignAddressOffset, kTaggedSize));
 #else
-  STATIC_ASSERT(IsAligned(kForeignAddressOffset, kExternalPointerSize));
+  static_assert(IsAligned(kForeignAddressOffset, kExternalPointerSize));
 #endif
 
   class BodyDescriptor;
diff --git a/src/objects/function-kind.h b/src/objects/function-kind.h
index da8dfe6f950..ccc3ea35fe8 100644
--- a/src/objects/function-kind.h
+++ b/src/objects/function-kind.h
@@ -66,7 +66,7 @@ enum class FunctionKind : uint8_t {
 };
 
 constexpr int kFunctionKindBitSize = 5;
-STATIC_ASSERT(static_cast<int>(FunctionKind::kLastFunctionKind) <
+static_assert(static_cast<int>(FunctionKind::kLastFunctionKind) <
               (1 << kFunctionKindBitSize));
 
 inline bool IsArrowFunction(FunctionKind kind) {
diff --git a/src/objects/hash-table.h b/src/objects/hash-table.h
index fff52d6f963..2a7d5531bc3 100644
--- a/src/objects/hash-table.h
+++ b/src/objects/hash-table.h
@@ -170,7 +170,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
 
   static const int kElementsStartIndex = kPrefixStartIndex + Shape::kPrefixSize;
   static const int kEntrySize = Shape::kEntrySize;
-  STATIC_ASSERT(kEntrySize > 0);
+  static_assert(kEntrySize > 0);
   static const int kEntryKeyIndex = 0;
   static const int kElementsStartOffset =
       kHeaderSize + kElementsStartIndex * kTaggedSize;
@@ -250,13 +250,13 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
 
  private:
   // Ensure that kMaxRegularCapacity yields a non-large object dictionary.
-  STATIC_ASSERT(EntryToIndex(InternalIndex(kMaxRegularCapacity)) <
+  static_assert(EntryToIndex(InternalIndex(kMaxRegularCapacity)) <
                 kMaxRegularLength);
-  STATIC_ASSERT(v8::base::bits::IsPowerOfTwo(kMaxRegularCapacity));
+  static_assert(v8::base::bits::IsPowerOfTwo(kMaxRegularCapacity));
   static const int kMaxRegularEntry = kMaxRegularCapacity / kEntrySize;
   static const int kMaxRegularIndex =
       EntryToIndex(InternalIndex(kMaxRegularEntry));
-  STATIC_ASSERT(OffsetOfElementAt(kMaxRegularIndex) <
+  static_assert(OffsetOfElementAt(kMaxRegularIndex) <
                 kMaxRegularHeapObjectSize);
 
   // Sets the capacity of the hash table.
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index 87951da0a00..76ffad321ec 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -224,7 +224,7 @@ class HeapObject : public Object {
   DEFINE_FIELD_OFFSET_CONSTANTS(Object::kHeaderSize, HEAP_OBJECT_FIELDS)
 #undef HEAP_OBJECT_FIELDS
 
-  STATIC_ASSERT(kMapOffset == Internals::kHeapObjectMapOffset);
+  static_assert(kMapOffset == Internals::kHeapObjectMapOffset);
 
   using MapField = TaggedField<MapWord, HeapObject::kMapOffset>;
 
diff --git a/src/objects/instance-type-inl.h b/src/objects/instance-type-inl.h
index 146a8f50063..bd96e168d48 100644
--- a/src/objects/instance-type-inl.h
+++ b/src/objects/instance-type-inl.h
@@ -65,7 +65,7 @@ V8_INLINE constexpr bool IsHeapObject(InstanceType instance_type) {
 }
 
 V8_INLINE constexpr bool IsInternalizedString(InstanceType instance_type) {
-  STATIC_ASSERT(kNotInternalizedTag != 0);
+  static_assert(kNotInternalizedTag != 0);
   return (instance_type & (kIsNotStringMask | kIsNotInternalizedMask)) ==
          (kStringTag | kInternalizedTag);
 }
diff --git a/src/objects/instance-type.h b/src/objects/instance-type.h
index 7661561cbbe..cc49535b401 100644
--- a/src/objects/instance-type.h
+++ b/src/objects/instance-type.h
@@ -33,12 +33,12 @@ enum StringRepresentationTag {
 };
 const uint32_t kIsIndirectStringMask = 1 << 0;
 const uint32_t kIsIndirectStringTag = 1 << 0;
-STATIC_ASSERT((kSeqStringTag & kIsIndirectStringMask) == 0);
-STATIC_ASSERT((kExternalStringTag & kIsIndirectStringMask) == 0);
-STATIC_ASSERT((kConsStringTag & kIsIndirectStringMask) == kIsIndirectStringTag);
-STATIC_ASSERT((kSlicedStringTag & kIsIndirectStringMask) ==
+static_assert((kSeqStringTag & kIsIndirectStringMask) == 0);
+static_assert((kExternalStringTag & kIsIndirectStringMask) == 0);
+static_assert((kConsStringTag & kIsIndirectStringMask) == kIsIndirectStringTag);
+static_assert((kSlicedStringTag & kIsIndirectStringMask) ==
               kIsIndirectStringTag);
-STATIC_ASSERT((kThinStringTag & kIsIndirectStringMask) == kIsIndirectStringTag);
+static_assert((kThinStringTag & kIsIndirectStringMask) == kIsIndirectStringTag);
 
 // For strings, bit 3 indicates whether the string consists of two-byte
 // characters or one-byte characters.
@@ -191,26 +191,26 @@ enum InstanceType : uint16_t {
 constexpr InstanceType LAST_STRING_TYPE =
     static_cast<InstanceType>(FIRST_NONSTRING_TYPE - 1);
 
-STATIC_ASSERT((FIRST_NONSTRING_TYPE & kIsNotStringMask) != kStringTag);
-STATIC_ASSERT(JS_OBJECT_TYPE == Internals::kJSObjectType);
-STATIC_ASSERT(FIRST_JS_API_OBJECT_TYPE == Internals::kFirstJSApiObjectType);
-STATIC_ASSERT(LAST_JS_API_OBJECT_TYPE == Internals::kLastJSApiObjectType);
-STATIC_ASSERT(JS_SPECIAL_API_OBJECT_TYPE == Internals::kJSSpecialApiObjectType);
-STATIC_ASSERT(FIRST_NONSTRING_TYPE == Internals::kFirstNonstringType);
-STATIC_ASSERT(ODDBALL_TYPE == Internals::kOddballType);
-STATIC_ASSERT(FOREIGN_TYPE == Internals::kForeignType);
+static_assert((FIRST_NONSTRING_TYPE & kIsNotStringMask) != kStringTag);
+static_assert(JS_OBJECT_TYPE == Internals::kJSObjectType);
+static_assert(FIRST_JS_API_OBJECT_TYPE == Internals::kFirstJSApiObjectType);
+static_assert(LAST_JS_API_OBJECT_TYPE == Internals::kLastJSApiObjectType);
+static_assert(JS_SPECIAL_API_OBJECT_TYPE == Internals::kJSSpecialApiObjectType);
+static_assert(FIRST_NONSTRING_TYPE == Internals::kFirstNonstringType);
+static_assert(ODDBALL_TYPE == Internals::kOddballType);
+static_assert(FOREIGN_TYPE == Internals::kForeignType);
 
 // Verify that string types are all less than other types.
 #define CHECK_STRING_RANGE(TYPE, ...) \
-  STATIC_ASSERT(TYPE < FIRST_NONSTRING_TYPE);
+  static_assert(TYPE < FIRST_NONSTRING_TYPE);
 STRING_TYPE_LIST(CHECK_STRING_RANGE)
 #undef CHECK_STRING_RANGE
-#define CHECK_NONSTRING_RANGE(TYPE) STATIC_ASSERT(TYPE >= FIRST_NONSTRING_TYPE);
+#define CHECK_NONSTRING_RANGE(TYPE) static_assert(TYPE >= FIRST_NONSTRING_TYPE);
 TORQUE_ASSIGNED_INSTANCE_TYPE_LIST(CHECK_NONSTRING_RANGE)
 #undef CHECK_NONSTRING_RANGE
 
 // classConstructor type has to be the last one in the JS Function type range.
-STATIC_ASSERT(JS_CLASS_CONSTRUCTOR_TYPE == LAST_JS_FUNCTION_TYPE);
+static_assert(JS_CLASS_CONSTRUCTOR_TYPE == LAST_JS_FUNCTION_TYPE);
 static_assert(JS_CLASS_CONSTRUCTOR_TYPE < FIRST_CALLABLE_JS_FUNCTION_TYPE ||
                   JS_CLASS_CONSTRUCTOR_TYPE > LAST_CALLABLE_JS_FUNCTION_TYPE,
               "JS_CLASS_CONSTRUCTOR_TYPE must not be in the callable JS "
@@ -226,7 +226,7 @@ static_assert(JS_CLASS_CONSTRUCTOR_TYPE < FIRST_CALLABLE_JS_FUNCTION_TYPE ||
 // that are not also subclasses of JSObject (currently only JSProxy).
 // clang-format off
 #define CHECK_INSTANCE_TYPE(TYPE)                                          \
-  STATIC_ASSERT((TYPE >= FIRST_JS_RECEIVER_TYPE &&                         \
+  static_assert((TYPE >= FIRST_JS_RECEIVER_TYPE &&                         \
                  TYPE <= LAST_SPECIAL_RECEIVER_TYPE) ==                    \
                 (IF_WASM(EXPAND, TYPE == WASM_STRUCT_TYPE ||               \
                                  TYPE == WASM_ARRAY_TYPE ||)               \
@@ -234,7 +234,7 @@ static_assert(JS_CLASS_CONSTRUCTOR_TYPE < FIRST_CALLABLE_JS_FUNCTION_TYPE ||
                  TYPE == JS_GLOBAL_PROXY_TYPE ||                           \
                  TYPE == JS_MODULE_NAMESPACE_TYPE ||                       \
                  TYPE == JS_SPECIAL_API_OBJECT_TYPE));                     \
-  STATIC_ASSERT((TYPE >= FIRST_JS_RECEIVER_TYPE &&                         \
+  static_assert((TYPE >= FIRST_JS_RECEIVER_TYPE &&                         \
                  TYPE <= LAST_CUSTOM_ELEMENTS_RECEIVER) ==                 \
                 (IF_WASM(EXPAND, TYPE == WASM_STRUCT_TYPE ||               \
                                  TYPE == WASM_ARRAY_TYPE ||)               \
@@ -249,7 +249,7 @@ TORQUE_ASSIGNED_INSTANCE_TYPE_LIST(CHECK_INSTANCE_TYPE)
 
 // Make sure it doesn't matter whether we sign-extend or zero-extend these
 // values, because Torque treats InstanceType as signed.
-STATIC_ASSERT(LAST_TYPE < 1 << 15);
+static_assert(LAST_TYPE < 1 << 15);
 
 V8_EXPORT_PRIVATE std::ostream& operator<<(std::ostream& os,
                                            InstanceType instance_type);
diff --git a/src/objects/intl-objects.cc b/src/objects/intl-objects.cc
index 2f551597e09..a93f104bcae 100644
--- a/src/objects/intl-objects.cc
+++ b/src/objects/intl-objects.cc
@@ -1071,7 +1071,7 @@ constexpr uint8_t kCollationWeightsL3[256] = {
     1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  0,
 };
 constexpr int kCollationWeightsLength = arraysize(kCollationWeightsL1);
-STATIC_ASSERT(kCollationWeightsLength == arraysize(kCollationWeightsL3));
+static_assert(kCollationWeightsLength == arraysize(kCollationWeightsL3));
 // clang-format on
 
 // Normalize a comparison delta (usually `lhs - rhs`) to UCollationResult
diff --git a/src/objects/js-array-buffer-inl.h b/src/objects/js-array-buffer-inl.h
index d6633d72bd6..b690c2aff03 100644
--- a/src/objects/js-array-buffer-inl.h
+++ b/src/objects/js-array-buffer-inl.h
@@ -292,7 +292,7 @@ void* JSTypedArray::DataPtr() {
   // so that the addition with |external_pointer| (which already contains
   // compensated offset value) will decompress the tagged value.
   // See JSTypedArray::ExternalPointerCompensationForOnHeapArray() for details.
-  STATIC_ASSERT(kOffHeapDataPtrEqualsExternalPointer);
+  static_assert(kOffHeapDataPtrEqualsExternalPointer);
   return reinterpret_cast<void*>(external_pointer() +
                                  static_cast<Tagged_t>(base_pointer().ptr()));
 }
diff --git a/src/objects/js-array-buffer.h b/src/objects/js-array-buffer.h
index 4b3207c2ed6..4ae300e10c9 100644
--- a/src/objects/js-array-buffer.h
+++ b/src/objects/js-array-buffer.h
@@ -259,8 +259,8 @@ class JSArrayBufferView
 
   static constexpr int kEndOfTaggedFieldsOffset = kByteOffsetOffset;
 
-  STATIC_ASSERT(IsAligned(kByteOffsetOffset, kUIntptrSize));
-  STATIC_ASSERT(IsAligned(kByteLengthOffset, kUIntptrSize));
+  static_assert(IsAligned(kByteOffsetOffset, kUIntptrSize));
+  static_assert(IsAligned(kByteLengthOffset, kUIntptrSize));
 
   TQ_OBJECT_CONSTRUCTORS(JSArrayBufferView)
 };
@@ -353,8 +353,8 @@ class JSTypedArray
   DECL_VERIFIER(JSTypedArray)
 
   // TODO(v8:9287): Re-enable when GCMole stops mixing 32/64 bit configs.
-  // STATIC_ASSERT(IsAligned(kLengthOffset, kTaggedSize));
-  // STATIC_ASSERT(IsAligned(kExternalPointerOffset, kTaggedSize));
+  // static_assert(IsAligned(kLengthOffset, kTaggedSize));
+  // static_assert(IsAligned(kExternalPointerOffset, kTaggedSize));
 
   static const int kSizeWithEmbedderFields =
       kHeaderSize +
@@ -400,7 +400,7 @@ class JSDataView
   DECL_VERIFIER(JSDataView)
 
   // TODO(v8:9287): Re-enable when GCMole stops mixing 32/64 bit configs.
-  // STATIC_ASSERT(IsAligned(kDataPointerOffset, kTaggedSize));
+  // static_assert(IsAligned(kDataPointerOffset, kTaggedSize));
 
   static const int kSizeWithEmbedderFields =
       kHeaderSize +
diff --git a/src/objects/js-array.h b/src/objects/js-array.h
index 58f0964e383..2af61eea0c1 100644
--- a/src/objects/js-array.h
+++ b/src/objects/js-array.h
@@ -123,12 +123,12 @@ class JSArray : public TorqueGeneratedJSArray<JSArray, JSObject> {
   // Valid array indices range from +0 <= i < 2^32 - 1 (kMaxUInt32).
   static constexpr uint32_t kMaxArrayLength = JSObject::kMaxElementCount;
   static constexpr uint32_t kMaxArrayIndex = JSObject::kMaxElementIndex;
-  STATIC_ASSERT(kMaxArrayLength == kMaxUInt32);
-  STATIC_ASSERT(kMaxArrayIndex == kMaxUInt32 - 1);
+  static_assert(kMaxArrayLength == kMaxUInt32);
+  static_assert(kMaxArrayIndex == kMaxUInt32 - 1);
 
   // This constant is somewhat arbitrary. Any large enough value would work.
   static constexpr uint32_t kMaxFastArrayLength = 32 * 1024 * 1024;
-  STATIC_ASSERT(kMaxFastArrayLength <= kMaxArrayLength);
+  static_assert(kMaxFastArrayLength <= kMaxArrayLength);
 
   // Min. stack size for detecting an Array.prototype.join() call cycle.
   static const uint32_t kMinJoinStackSize = 2;
diff --git a/src/objects/js-collection.h b/src/objects/js-collection.h
index 9b3e9d05416..8de079e69cf 100644
--- a/src/objects/js-collection.h
+++ b/src/objects/js-collection.h
@@ -118,7 +118,7 @@ class JSWeakMap : public TorqueGeneratedJSWeakMap<JSWeakMap, JSWeakCollection> {
   DECL_PRINTER(JSWeakMap)
   DECL_VERIFIER(JSWeakMap)
 
-  STATIC_ASSERT(kHeaderSize == kHeaderSizeOfAllWeakCollections);
+  static_assert(kHeaderSize == kHeaderSizeOfAllWeakCollections);
   TQ_OBJECT_CONSTRUCTORS(JSWeakMap)
 };
 
@@ -129,7 +129,7 @@ class JSWeakSet : public TorqueGeneratedJSWeakSet<JSWeakSet, JSWeakCollection> {
   DECL_PRINTER(JSWeakSet)
   DECL_VERIFIER(JSWeakSet)
 
-  STATIC_ASSERT(kHeaderSize == kHeaderSizeOfAllWeakCollections);
+  static_assert(kHeaderSize == kHeaderSizeOfAllWeakCollections);
   TQ_OBJECT_CONSTRUCTORS(JSWeakSet)
 };
 
diff --git a/src/objects/js-date-time-format.h b/src/objects/js-date-time-format.h
index a5465d37cfa..a64c7087ac5 100644
--- a/src/objects/js-date-time-format.h
+++ b/src/objects/js-date-time-format.h
@@ -118,23 +118,23 @@ class JSDateTimeFormat
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_DATE_TIME_FORMAT_FLAGS()
 
-  STATIC_ASSERT(HourCycle::kUndefined <= HourCycleBits::kMax);
-  STATIC_ASSERT(HourCycle::kH11 <= HourCycleBits::kMax);
-  STATIC_ASSERT(HourCycle::kH12 <= HourCycleBits::kMax);
-  STATIC_ASSERT(HourCycle::kH23 <= HourCycleBits::kMax);
-  STATIC_ASSERT(HourCycle::kH24 <= HourCycleBits::kMax);
-
-  STATIC_ASSERT(DateTimeStyle::kUndefined <= DateStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kFull <= DateStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kLong <= DateStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kMedium <= DateStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kShort <= DateStyleBits::kMax);
-
-  STATIC_ASSERT(DateTimeStyle::kUndefined <= TimeStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kFull <= TimeStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kLong <= TimeStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kMedium <= TimeStyleBits::kMax);
-  STATIC_ASSERT(DateTimeStyle::kShort <= TimeStyleBits::kMax);
+  static_assert(HourCycle::kUndefined <= HourCycleBits::kMax);
+  static_assert(HourCycle::kH11 <= HourCycleBits::kMax);
+  static_assert(HourCycle::kH12 <= HourCycleBits::kMax);
+  static_assert(HourCycle::kH23 <= HourCycleBits::kMax);
+  static_assert(HourCycle::kH24 <= HourCycleBits::kMax);
+
+  static_assert(DateTimeStyle::kUndefined <= DateStyleBits::kMax);
+  static_assert(DateTimeStyle::kFull <= DateStyleBits::kMax);
+  static_assert(DateTimeStyle::kLong <= DateStyleBits::kMax);
+  static_assert(DateTimeStyle::kMedium <= DateStyleBits::kMax);
+  static_assert(DateTimeStyle::kShort <= DateStyleBits::kMax);
+
+  static_assert(DateTimeStyle::kUndefined <= TimeStyleBits::kMax);
+  static_assert(DateTimeStyle::kFull <= TimeStyleBits::kMax);
+  static_assert(DateTimeStyle::kLong <= TimeStyleBits::kMax);
+  static_assert(DateTimeStyle::kMedium <= TimeStyleBits::kMax);
+  static_assert(DateTimeStyle::kShort <= TimeStyleBits::kMax);
 
   DECL_ACCESSORS(icu_locale, Managed<icu::Locale>)
   DECL_ACCESSORS(icu_simple_date_format, Managed<icu::SimpleDateFormat>)
diff --git a/src/objects/js-display-names.h b/src/objects/js-display-names.h
index 373b826d20c..f637d91292d 100644
--- a/src/objects/js-display-names.h
+++ b/src/objects/js-display-names.h
@@ -79,13 +79,13 @@ class JSDisplayNames
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_DISPLAY_NAMES_FLAGS()
 
-  STATIC_ASSERT(Style::kLong <= StyleBits::kMax);
-  STATIC_ASSERT(Style::kShort <= StyleBits::kMax);
-  STATIC_ASSERT(Style::kNarrow <= StyleBits::kMax);
-  STATIC_ASSERT(Fallback::kCode <= FallbackBit::kMax);
-  STATIC_ASSERT(Fallback::kNone <= FallbackBit::kMax);
-  STATIC_ASSERT(LanguageDisplay::kDialect <= LanguageDisplayBit::kMax);
-  STATIC_ASSERT(LanguageDisplay::kStandard <= LanguageDisplayBit::kMax);
+  static_assert(Style::kLong <= StyleBits::kMax);
+  static_assert(Style::kShort <= StyleBits::kMax);
+  static_assert(Style::kNarrow <= StyleBits::kMax);
+  static_assert(Fallback::kCode <= FallbackBit::kMax);
+  static_assert(Fallback::kNone <= FallbackBit::kMax);
+  static_assert(LanguageDisplay::kDialect <= LanguageDisplayBit::kMax);
+  static_assert(LanguageDisplay::kStandard <= LanguageDisplayBit::kMax);
 
   DECL_ACCESSORS(internal, Managed<DisplayNamesInternal>)
 
diff --git a/src/objects/js-function.cc b/src/objects/js-function.cc
index 128d8378cef..46a1ea3eea4 100644
--- a/src/objects/js-function.cc
+++ b/src/objects/js-function.cc
@@ -87,7 +87,7 @@ V8_WARN_UNUSED_RESULT bool HighestTierOf(CodeKinds kinds,
                                          CodeKind* highest_tier) {
   DCHECK_EQ((kinds & ~kJSFunctionCodeKindsMask), 0);
   // Higher tiers > lower tiers.
-  STATIC_ASSERT(CodeKind::TURBOFAN > CodeKind::INTERPRETED_FUNCTION);
+  static_assert(CodeKind::TURBOFAN > CodeKind::INTERPRETED_FUNCTION);
   if (kinds == 0) return false;
   const int highest_tier_log2 =
       31 - base::bits::CountLeadingZeros(static_cast<uint32_t>(kinds));
@@ -1043,11 +1043,11 @@ namespace {
 // Assert that the computations in TypedArrayElementsKindToConstructorIndex and
 // TypedArrayElementsKindToRabGsabCtorIndex are sound.
 #define TYPED_ARRAY_CASE(Type, type, TYPE, ctype)                         \
-  STATIC_ASSERT(Context::TYPE##_ARRAY_FUN_INDEX ==                        \
+  static_assert(Context::TYPE##_ARRAY_FUN_INDEX ==                        \
                 Context::FIRST_FIXED_TYPED_ARRAY_FUN_INDEX +              \
                     ElementsKind::TYPE##_ELEMENTS -                       \
                     ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND); \
-  STATIC_ASSERT(Context::RAB_GSAB_##TYPE##_ARRAY_MAP_INDEX ==             \
+  static_assert(Context::RAB_GSAB_##TYPE##_ARRAY_MAP_INDEX ==             \
                 Context::FIRST_RAB_GSAB_TYPED_ARRAY_MAP_INDEX +           \
                     ElementsKind::TYPE##_ELEMENTS -                       \
                     ElementsKind::FIRST_FIXED_TYPED_ARRAY_ELEMENTS_KIND);
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index b787276da7b..501cf33ddf1 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -32,7 +32,7 @@ class JSFunctionOrBoundFunctionOrWrappedFunction
       Handle<JSFunctionOrBoundFunctionOrWrappedFunction> function,
       Handle<JSReceiver> target, Handle<String> prefix, int arg_count);
 
-  STATIC_ASSERT(kHeaderSize == JSObject::kHeaderSize);
+  static_assert(kHeaderSize == JSObject::kHeaderSize);
   TQ_OBJECT_CONSTRUCTORS(JSFunctionOrBoundFunctionOrWrappedFunction)
 };
 
diff --git a/src/objects/js-list-format.h b/src/objects/js-list-format.h
index 0f6de1e2bfc..ea68a313e24 100644
--- a/src/objects/js-list-format.h
+++ b/src/objects/js-list-format.h
@@ -86,12 +86,12 @@ class JSListFormat
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_LIST_FORMAT_FLAGS()
 
-  STATIC_ASSERT(Style::LONG <= StyleBits::kMax);
-  STATIC_ASSERT(Style::SHORT <= StyleBits::kMax);
-  STATIC_ASSERT(Style::NARROW <= StyleBits::kMax);
-  STATIC_ASSERT(Type::CONJUNCTION <= TypeBits::kMax);
-  STATIC_ASSERT(Type::DISJUNCTION <= TypeBits::kMax);
-  STATIC_ASSERT(Type::UNIT <= TypeBits::kMax);
+  static_assert(Style::LONG <= StyleBits::kMax);
+  static_assert(Style::SHORT <= StyleBits::kMax);
+  static_assert(Style::NARROW <= StyleBits::kMax);
+  static_assert(Type::CONJUNCTION <= TypeBits::kMax);
+  static_assert(Type::DISJUNCTION <= TypeBits::kMax);
+  static_assert(Type::UNIT <= TypeBits::kMax);
 
   DECL_PRINTER(JSListFormat)
 
diff --git a/src/objects/js-objects-inl.h b/src/objects/js-objects-inl.h
index c4c4ba6232e..cafc5500bbc 100644
--- a/src/objects/js-objects-inl.h
+++ b/src/objects/js-objects-inl.h
@@ -916,7 +916,7 @@ static inline bool ShouldConvertToSlowElements(JSObject object,
                                                uint32_t capacity,
                                                uint32_t index,
                                                uint32_t* new_capacity) {
-  STATIC_ASSERT(JSObject::kMaxUncheckedOldFastElementsLength <=
+  static_assert(JSObject::kMaxUncheckedOldFastElementsLength <=
                 JSObject::kMaxUncheckedFastElementsLength);
   if (index < capacity) {
     *new_capacity = capacity;
diff --git a/src/objects/js-objects.cc b/src/objects/js-objects.cc
index e4b5f286b4a..6a7a34869b4 100644
--- a/src/objects/js-objects.cc
+++ b/src/objects/js-objects.cc
@@ -4189,7 +4189,7 @@ Handle<NumberDictionary> CreateElementDictionary(Isolate* isolate,
 template <PropertyAttributes attrs>
 Maybe<bool> JSObject::PreventExtensionsWithTransition(
     Handle<JSObject> object, ShouldThrow should_throw) {
-  STATIC_ASSERT(attrs == NONE || attrs == SEALED || attrs == FROZEN);
+  static_assert(attrs == NONE || attrs == SEALED || attrs == FROZEN);
 
   // Sealing/freezing sloppy arguments or namespace objects should be handled
   // elsewhere.
diff --git a/src/objects/js-objects.h b/src/objects/js-objects.h
index 05005e062d7..b093d66a3e3 100644
--- a/src/objects/js-objects.h
+++ b/src/objects/js-objects.h
@@ -833,19 +833,19 @@ class JSObject : public TorqueGeneratedJSObject<JSObject, JSReceiver> {
   // its size by more than the 1 entry necessary, so sequentially adding fields
   // to the same object requires fewer allocations and copies.
   static const int kFieldsAdded = 3;
-  STATIC_ASSERT(kMaxNumberOfDescriptors + kFieldsAdded <=
+  static_assert(kMaxNumberOfDescriptors + kFieldsAdded <=
                 PropertyArray::kMaxLength);
 
-  STATIC_ASSERT(kHeaderSize == Internals::kJSObjectHeaderSize);
+  static_assert(kHeaderSize == Internals::kJSObjectHeaderSize);
   static const int kMaxInObjectProperties =
       (kMaxInstanceSize - kHeaderSize) >> kTaggedSizeLog2;
-  STATIC_ASSERT(kMaxInObjectProperties <= kMaxNumberOfDescriptors);
+  static_assert(kMaxInObjectProperties <= kMaxNumberOfDescriptors);
 
   static const int kMaxFirstInobjectPropertyOffset =
       (1 << kFirstInobjectPropertyOffsetBitCount) - 1;
   static const int kMaxEmbedderFields =
       (kMaxFirstInobjectPropertyOffset - kHeaderSize) / kEmbedderDataSlotSize;
-  STATIC_ASSERT(kHeaderSize +
+  static_assert(kHeaderSize +
                     kMaxEmbedderFields * kEmbedderDataSlotSizeInTaggedSlots <=
                 kMaxInstanceSize);
 
@@ -916,7 +916,7 @@ class JSObjectWithEmbedderSlots
     : public TorqueGeneratedJSObjectWithEmbedderSlots<JSObjectWithEmbedderSlots,
                                                       JSObject> {
  public:
-  STATIC_ASSERT(kHeaderSize == JSObject::kHeaderSize);
+  static_assert(kHeaderSize == JSObject::kHeaderSize);
   TQ_OBJECT_CONSTRUCTORS(JSObjectWithEmbedderSlots)
 };
 
@@ -928,7 +928,7 @@ class JSCustomElementsObject
     : public TorqueGeneratedJSCustomElementsObject<JSCustomElementsObject,
                                                    JSObject> {
  public:
-  STATIC_ASSERT(kHeaderSize == JSObject::kHeaderSize);
+  static_assert(kHeaderSize == JSObject::kHeaderSize);
   TQ_OBJECT_CONSTRUCTORS(JSCustomElementsObject)
 };
 
@@ -941,7 +941,7 @@ class JSSpecialObject
     : public TorqueGeneratedJSSpecialObject<JSSpecialObject,
                                             JSCustomElementsObject> {
  public:
-  STATIC_ASSERT(kHeaderSize == JSObject::kHeaderSize);
+  static_assert(kHeaderSize == JSObject::kHeaderSize);
   TQ_OBJECT_CONSTRUCTORS(JSSpecialObject)
 };
 
diff --git a/src/objects/js-plural-rules.h b/src/objects/js-plural-rules.h
index 0a2b42462b6..df698ab5e8a 100644
--- a/src/objects/js-plural-rules.h
+++ b/src/objects/js-plural-rules.h
@@ -66,8 +66,8 @@ class JSPluralRules
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_PLURAL_RULES_FLAGS()
 
-  STATIC_ASSERT(Type::CARDINAL <= TypeBit::kMax);
-  STATIC_ASSERT(Type::ORDINAL <= TypeBit::kMax);
+  static_assert(Type::CARDINAL <= TypeBit::kMax);
+  static_assert(Type::ORDINAL <= TypeBit::kMax);
 
   DECL_ACCESSORS(icu_plural_rules, Managed<icu::PluralRules>)
   DECL_ACCESSORS(icu_number_formatter,
diff --git a/src/objects/js-promise.h b/src/objects/js-promise.h
index 1d3a94f035c..ffaaab60265 100644
--- a/src/objects/js-promise.h
+++ b/src/objects/js-promise.h
@@ -75,9 +75,9 @@ class JSPromise
   // Flags layout.
   DEFINE_TORQUE_GENERATED_JS_PROMISE_FLAGS()
 
-  STATIC_ASSERT(v8::Promise::kPending == 0);
-  STATIC_ASSERT(v8::Promise::kFulfilled == 1);
-  STATIC_ASSERT(v8::Promise::kRejected == 2);
+  static_assert(v8::Promise::kPending == 0);
+  static_assert(v8::Promise::kFulfilled == 1);
+  static_assert(v8::Promise::kRejected == 2);
 
  private:
   // ES section #sec-triggerpromisereactions
diff --git a/src/objects/js-proxy.h b/src/objects/js-proxy.h
index 12abe9623f6..2567950126c 100644
--- a/src/objects/js-proxy.h
+++ b/src/objects/js-proxy.h
@@ -106,7 +106,7 @@ class JSProxy : public TorqueGeneratedJSProxy<JSProxy, JSReceiver> {
   // JSProxy::target is a Javascript value which cannot be confused with an
   // elements backing store is exploited by loading from this offset from an
   // unknown JSReceiver.
-  STATIC_ASSERT(static_cast<int>(JSObject::kElementsOffset) ==
+  static_assert(static_cast<int>(JSObject::kElementsOffset) ==
                 static_cast<int>(JSProxy::kTargetOffset));
 
   using BodyDescriptor =
diff --git a/src/objects/js-regexp.h b/src/objects/js-regexp.h
index 36e6b791cde..e02a72e1abf 100644
--- a/src/objects/js-regexp.h
+++ b/src/objects/js-regexp.h
@@ -102,15 +102,15 @@ class JSRegExp : public TorqueGeneratedJSRegExp<JSRegExp, JSObject> {
     return f;
   }
 
-  STATIC_ASSERT(static_cast<int>(kNone) == v8::RegExp::kNone);
+  static_assert(static_cast<int>(kNone) == v8::RegExp::kNone);
 #define V(_, Camel, ...)                                             \
-  STATIC_ASSERT(static_cast<int>(k##Camel) == v8::RegExp::k##Camel); \
-  STATIC_ASSERT(static_cast<int>(k##Camel) ==                        \
+  static_assert(static_cast<int>(k##Camel) == v8::RegExp::k##Camel); \
+  static_assert(static_cast<int>(k##Camel) ==                        \
                 static_cast<int>(RegExpFlag::k##Camel));
   REGEXP_FLAG_LIST(V)
 #undef V
-  STATIC_ASSERT(kFlagCount == v8::RegExp::kFlagCount);
-  STATIC_ASSERT(kFlagCount == kRegExpFlagCount);
+  static_assert(kFlagCount == v8::RegExp::kFlagCount);
+  static_assert(kFlagCount == kRegExpFlagCount);
 
   static base::Optional<Flags> FlagsFromString(Isolate* isolate,
                                                Handle<String> flags);
diff --git a/src/objects/js-relative-time-format.h b/src/objects/js-relative-time-format.h
index 444082cf0e2..77c059fdd67 100644
--- a/src/objects/js-relative-time-format.h
+++ b/src/objects/js-relative-time-format.h
@@ -77,8 +77,8 @@ class JSRelativeTimeFormat
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_RELATIVE_TIME_FORMAT_FLAGS()
 
-  STATIC_ASSERT(Numeric::AUTO <= NumericBit::kMax);
-  STATIC_ASSERT(Numeric::ALWAYS <= NumericBit::kMax);
+  static_assert(Numeric::AUTO <= NumericBit::kMax);
+  static_assert(Numeric::ALWAYS <= NumericBit::kMax);
 
   DECL_PRINTER(JSRelativeTimeFormat)
 
diff --git a/src/objects/js-segment-iterator.h b/src/objects/js-segment-iterator.h
index bcbc22df378..b8151dc436d 100644
--- a/src/objects/js-segment-iterator.h
+++ b/src/objects/js-segment-iterator.h
@@ -55,9 +55,9 @@ class JSSegmentIterator
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_SEGMENT_ITERATOR_FLAGS()
 
-  STATIC_ASSERT(JSSegmenter::Granularity::GRAPHEME <= GranularityBits::kMax);
-  STATIC_ASSERT(JSSegmenter::Granularity::WORD <= GranularityBits::kMax);
-  STATIC_ASSERT(JSSegmenter::Granularity::SENTENCE <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::GRAPHEME <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::WORD <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::SENTENCE <= GranularityBits::kMax);
 
   TQ_OBJECT_CONSTRUCTORS(JSSegmentIterator)
 };
diff --git a/src/objects/js-segmenter.h b/src/objects/js-segmenter.h
index 512625d204c..f2e21db5fd7 100644
--- a/src/objects/js-segmenter.h
+++ b/src/objects/js-segmenter.h
@@ -65,9 +65,9 @@ class JSSegmenter : public TorqueGeneratedJSSegmenter<JSSegmenter, JSObject> {
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_SEGMENTER_FLAGS()
 
-  STATIC_ASSERT(Granularity::GRAPHEME <= GranularityBits::kMax);
-  STATIC_ASSERT(Granularity::WORD <= GranularityBits::kMax);
-  STATIC_ASSERT(Granularity::SENTENCE <= GranularityBits::kMax);
+  static_assert(Granularity::GRAPHEME <= GranularityBits::kMax);
+  static_assert(Granularity::WORD <= GranularityBits::kMax);
+  static_assert(Granularity::SENTENCE <= GranularityBits::kMax);
 
   DECL_PRINTER(JSSegmenter)
 
diff --git a/src/objects/js-segments.h b/src/objects/js-segments.h
index 30c387fea60..aa5759ade87 100644
--- a/src/objects/js-segments.h
+++ b/src/objects/js-segments.h
@@ -59,9 +59,9 @@ class JSSegments : public TorqueGeneratedJSSegments<JSSegments, JSObject> {
   // Bit positions in |flags|.
   DEFINE_TORQUE_GENERATED_JS_SEGMENT_ITERATOR_FLAGS()
 
-  STATIC_ASSERT(JSSegmenter::Granularity::GRAPHEME <= GranularityBits::kMax);
-  STATIC_ASSERT(JSSegmenter::Granularity::WORD <= GranularityBits::kMax);
-  STATIC_ASSERT(JSSegmenter::Granularity::SENTENCE <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::GRAPHEME <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::WORD <= GranularityBits::kMax);
+  static_assert(JSSegmenter::Granularity::SENTENCE <= GranularityBits::kMax);
 
   TQ_OBJECT_CONSTRUCTORS(JSSegments)
 };
diff --git a/src/objects/keys.cc b/src/objects/keys.cc
index 44a721d1a52..c631421b010 100644
--- a/src/objects/keys.cc
+++ b/src/objects/keys.cc
@@ -852,7 +852,7 @@ template <typename Dictionary>
 void CopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
                     Handle<FixedArray> storage, KeyCollectionMode mode,
                     KeyAccumulator* accumulator) {
-  STATIC_ASSERT(!Dictionary::kIsOrderedDictionaryType);
+  static_assert(!Dictionary::kIsOrderedDictionaryType);
 
   CommonCopyEnumKeysTo<Dictionary>(isolate, dictionary, storage, mode,
                                    accumulator);
diff --git a/src/objects/literal-objects.cc b/src/objects/literal-objects.cc
index 8a447af7899..e747de86145 100644
--- a/src/objects/literal-objects.cc
+++ b/src/objects/literal-objects.cc
@@ -125,7 +125,7 @@ Handle<NumberDictionary> DictionaryAddNoUpdateNextEnumerationIndex(
 template <typename Dictionary>
 void DictionaryUpdateMaxNumberKey(Handle<Dictionary> dictionary,
                                   Handle<Name> name) {
-  STATIC_ASSERT((std::is_same<Dictionary, SwissNameDictionary>::value ||
+  static_assert((std::is_same<Dictionary, SwissNameDictionary>::value ||
                  std::is_same<Dictionary, NameDictionary>::value));
   // No-op for (ordered) name dictionaries.
 }
@@ -160,7 +160,7 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
 
   const bool is_elements_dictionary =
       std::is_same<Dictionary, NumberDictionary>::value;
-  STATIC_ASSERT(is_elements_dictionary !=
+  static_assert(is_elements_dictionary !=
                 (std::is_same<Dictionary, NameDictionary>::value ||
                  std::is_same<Dictionary, SwissNameDictionary>::value));
 
@@ -215,7 +215,7 @@ void AddToDictionaryTemplate(IsolateT* isolate, Handle<Dictionary> dictionary,
         int existing_setter_index =
             GetExistingValueIndex(current_pair.setter());
         // At least one of the accessors must already be defined.
-        STATIC_ASSERT(kAccessorNotDefined < 0);
+        static_assert(kAccessorNotDefined < 0);
         DCHECK(existing_getter_index >= 0 || existing_setter_index >= 0);
         if (existing_getter_index < key_index &&
             existing_setter_index < key_index) {
@@ -621,7 +621,7 @@ Handle<ClassBoilerplate> ClassBoilerplate::BuildClassBoilerplate(
   // Initialize class object template.
   //
   static_desc.CreateTemplates(isolate);
-  STATIC_ASSERT(JSFunction::kLengthDescriptorIndex == 0);
+  static_assert(JSFunction::kLengthDescriptorIndex == 0);
   {
     // Add length_accessor.
     PropertyAttributes attribs =
@@ -747,11 +747,11 @@ void ArrayBoilerplateDescription::BriefPrintDetails(std::ostream& os) {
 
 void RegExpBoilerplateDescription::BriefPrintDetails(std::ostream& os) {
   // Note: keep boilerplate layout synced with JSRegExp layout.
-  STATIC_ASSERT(JSRegExp::kDataOffset == JSObject::kHeaderSize);
-  STATIC_ASSERT(JSRegExp::kSourceOffset == JSRegExp::kDataOffset + kTaggedSize);
-  STATIC_ASSERT(JSRegExp::kFlagsOffset ==
+  static_assert(JSRegExp::kDataOffset == JSObject::kHeaderSize);
+  static_assert(JSRegExp::kSourceOffset == JSRegExp::kDataOffset + kTaggedSize);
+  static_assert(JSRegExp::kFlagsOffset ==
                 JSRegExp::kSourceOffset + kTaggedSize);
-  STATIC_ASSERT(JSRegExp::kHeaderSize == JSRegExp::kFlagsOffset + kTaggedSize);
+  static_assert(JSRegExp::kHeaderSize == JSRegExp::kFlagsOffset + kTaggedSize);
   os << " " << Brief(data()) << ", " << Brief(source()) << ", " << flags();
 }
 
diff --git a/src/objects/lookup-inl.h b/src/objects/lookup-inl.h
index c5d761aed18..8c6e7b994be 100644
--- a/src/objects/lookup-inl.h
+++ b/src/objects/lookup-inl.h
@@ -116,7 +116,7 @@ PropertyKey::PropertyKey(Isolate* isolate, double index) {
   DCHECK_EQ(index, static_cast<uint64_t>(index));
 #if V8_TARGET_ARCH_32_BIT
   if (index <= JSObject::kMaxElementIndex) {
-    STATIC_ASSERT(JSObject::kMaxElementIndex <=
+    static_assert(JSObject::kMaxElementIndex <=
                   std::numeric_limits<size_t>::max());
     index_ = static_cast<size_t>(index);
   } else {
diff --git a/src/objects/lookup.cc b/src/objects/lookup.cc
index bb81e182f5c..dbd55aed787 100644
--- a/src/objects/lookup.cc
+++ b/src/objects/lookup.cc
@@ -1253,7 +1253,7 @@ bool HasInterceptor(Map map, size_t index) {
 template <bool is_element>
 LookupIterator::State LookupIterator::LookupInSpecialHolder(
     Map const map, JSReceiver const holder) {
-  STATIC_ASSERT(INTERCEPTOR == BEFORE_PROPERTY);
+  static_assert(INTERCEPTOR == BEFORE_PROPERTY);
   switch (state_) {
     case NOT_FOUND:
       if (map.IsJSProxyMap()) {
diff --git a/src/objects/map-inl.h b/src/objects/map-inl.h
index 4e65b2f746a..1145291c9cf 100644
--- a/src/objects/map-inl.h
+++ b/src/objects/map-inl.h
@@ -395,7 +395,7 @@ int Map::UsedInstanceSize() const {
 }
 
 void Map::SetInObjectUnusedPropertyFields(int value) {
-  STATIC_ASSERT(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
+  static_assert(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
   if (!IsJSObjectMap()) {
     CHECK_EQ(0, value);
     set_used_or_unused_instance_size_in_words(0);
@@ -411,7 +411,7 @@ void Map::SetInObjectUnusedPropertyFields(int value) {
 }
 
 void Map::SetOutOfObjectUnusedPropertyFields(int value) {
-  STATIC_ASSERT(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
+  static_assert(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
   CHECK_LT(static_cast<unsigned>(value), JSObject::kFieldsAdded);
   // For out of object properties "used_instance_size_in_words" byte encodes
   // the slack in the property array.
@@ -438,7 +438,7 @@ void Map::CopyUnusedPropertyFieldsAdjustedForInstanceSize(Map map) {
 
 void Map::AccountAddedPropertyField() {
   // Update used instance size and unused property fields number.
-  STATIC_ASSERT(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
+  static_assert(JSObject::kFieldsAdded == JSObject::kHeaderSize / kTaggedSize);
 #ifdef DEBUG
   int new_unused = UnusedPropertyFields() - 1;
   if (new_unused < 0) new_unused += JSObject::kFieldsAdded;
diff --git a/src/objects/map.cc b/src/objects/map.cc
index f8bd209dcf5..cf2a4c73f0e 100644
--- a/src/objects/map.cc
+++ b/src/objects/map.cc
@@ -79,7 +79,7 @@ Map Map::GetInstanceTypeMap(ReadOnlyRoots roots, InstanceType type) {
 }
 
 VisitorId Map::GetVisitorId(Map map) {
-  STATIC_ASSERT(kVisitorIdCount <= 256);
+  static_assert(kVisitorIdCount <= 256);
 
   const int instance_type = map.instance_type();
 
@@ -1202,7 +1202,7 @@ Handle<Map> Map::Normalize(Isolate* isolate, Handle<Map> fast_map,
       Handle<Map> fresh = Map::CopyNormalized(isolate, fast_map, mode);
       fresh->set_elements_kind(new_elements_kind);
 
-      STATIC_ASSERT(Map::kPrototypeValidityCellOffset ==
+      static_assert(Map::kPrototypeValidityCellOffset ==
                     Map::kDependentCodeOffset + kTaggedSize);
       DCHECK_EQ(0, memcmp(reinterpret_cast<void*>(fresh->address()),
                           reinterpret_cast<void*>(new_map->address()),
@@ -1224,7 +1224,7 @@ Handle<Map> Map::Normalize(Isolate* isolate, Handle<Map> fast_map,
       offset = Map::kPrototypeValidityCellOffset + kTaggedSize;
       if (new_map->is_prototype_map()) {
         // For prototype maps, the PrototypeInfo is not copied.
-        STATIC_ASSERT(Map::kTransitionsOrPrototypeInfoOffset ==
+        static_assert(Map::kTransitionsOrPrototypeInfoOffset ==
                       Map::kPrototypeValidityCellOffset + kTaggedSize);
         offset = kTransitionsOrPrototypeInfoOffset + kTaggedSize;
         DCHECK_EQ(fresh->raw_transitions(),
@@ -1596,7 +1596,7 @@ Handle<Map> Map::AsLanguageMode(Isolate* isolate, Handle<Map> initial_map,
                                shared_info->function_map_index())),
                            isolate);
 
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   DCHECK_EQ(LanguageMode::kStrict, shared_info->language_mode());
   Handle<Symbol> transition_symbol =
       isolate->factory()->strict_function_transition_symbol();
diff --git a/src/objects/map.h b/src/objects/map.h
index cc38d4e694d..cebf5a1c96a 100644
--- a/src/objects/map.h
+++ b/src/objects/map.h
@@ -300,17 +300,17 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   };
 
   // Ensure that Torque-defined bit widths for |bit_field3| are as expected.
-  STATIC_ASSERT(Bits3::EnumLengthBits::kSize == kDescriptorIndexBitCount);
-  STATIC_ASSERT(Bits3::NumberOfOwnDescriptorsBits::kSize ==
+  static_assert(Bits3::EnumLengthBits::kSize == kDescriptorIndexBitCount);
+  static_assert(Bits3::NumberOfOwnDescriptorsBits::kSize ==
                 kDescriptorIndexBitCount);
 
-  STATIC_ASSERT(Bits3::NumberOfOwnDescriptorsBits::kMax >=
+  static_assert(Bits3::NumberOfOwnDescriptorsBits::kMax >=
                 kMaxNumberOfDescriptors);
 
   static const int kSlackTrackingCounterStart = 7;
   static const int kSlackTrackingCounterEnd = 1;
   static const int kNoSlackTracking = 0;
-  STATIC_ASSERT(kSlackTrackingCounterStart <=
+  static_assert(kSlackTrackingCounterStart <=
                 Bits3::ConstructionCounterBits::kMax);
 
   // Inobject slack tracking is the way to reclaim unused inobject space.
@@ -816,7 +816,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
 
   static const int kMaxPreAllocatedPropertyFields = 255;
 
-  STATIC_ASSERT(kInstanceTypeOffset == Internals::kMapInstanceTypeOffset);
+  static_assert(kInstanceTypeOffset == Internals::kMapInstanceTypeOffset);
 
   class BodyDescriptor;
 
diff --git a/src/objects/name.h b/src/objects/name.h
index 7f899a10a0d..07c109a6f62 100644
--- a/src/objects/name.h
+++ b/src/objects/name.h
@@ -130,8 +130,8 @@ class Name : public TorqueGeneratedName<Name, PrimitiveHeapObject> {
       HashFieldTypeBits::encode(HashFieldType::kEmpty);
 
   // Empty hash and forwarding indices can not be used as hash.
-  STATIC_ASSERT((kEmptyHashField & kHashNotComputedMask) != 0);
-  STATIC_ASSERT((HashFieldTypeBits::encode(HashFieldType::kForwardingIndex) &
+  static_assert((kEmptyHashField & kHashNotComputedMask) != 0);
+  static_assert((HashFieldTypeBits::encode(HashFieldType::kForwardingIndex) &
                  kHashNotComputedMask) != 0);
 
   // Array index strings this short can keep their index in the hash field.
@@ -155,8 +155,8 @@ class Name : public TorqueGeneratedName<Name, PrimitiveHeapObject> {
   static const int kArrayIndexLengthBits =
       kBitsPerInt - kArrayIndexValueBits - HashFieldTypeBits::kSize;
 
-  STATIC_ASSERT(kArrayIndexLengthBits > 0);
-  STATIC_ASSERT(kMaxArrayIndexSize < (1 << kArrayIndexLengthBits));
+  static_assert(kArrayIndexLengthBits > 0);
+  static_assert(kMaxArrayIndexSize < (1 << kArrayIndexLengthBits));
 
   using ArrayIndexValueBits =
       HashFieldTypeBits::Next<unsigned int, kArrayIndexValueBits>;
@@ -171,7 +171,7 @@ class Name : public TorqueGeneratedName<Name, PrimitiveHeapObject> {
 
   // When any of these bits is set then the hash field does not contain a cached
   // array index.
-  STATIC_ASSERT(HashFieldTypeBits::encode(HashFieldType::kIntegerIndex) == 0);
+  static_assert(HashFieldTypeBits::encode(HashFieldType::kIntegerIndex) == 0);
   static const unsigned int kDoesNotContainCachedArrayIndexMask =
       (~static_cast<unsigned>(kMaxCachedArrayIndexLength)
        << ArrayIndexLengthBits::kShift) |
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 289f1fa053c..edd5445b866 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -56,7 +56,7 @@ int FlexibleWeakBodyDescriptor<start_offset>::SizeOf(Map map,
 bool BodyDescriptorBase::IsValidJSObjectSlotImpl(Map map, HeapObject obj,
                                                  int offset) {
 #ifdef V8_COMPRESS_POINTERS
-  STATIC_ASSERT(kEmbedderDataSlotSize == 2 * kTaggedSize);
+  static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
   int embedder_fields_offset = JSObject::GetEmbedderFieldsStartOffset(map);
   int inobject_fields_offset = map.GetInObjectPropertyOffset(0);
   // |embedder_fields_offset| may be greater than |inobject_fields_offset| if
@@ -65,14 +65,14 @@ bool BodyDescriptorBase::IsValidJSObjectSlotImpl(Map map, HeapObject obj,
   if (embedder_fields_offset <= offset && offset < inobject_fields_offset) {
     // offset points to embedder fields area:
     // [embedder_fields_offset, inobject_fields_offset).
-    STATIC_ASSERT(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
+    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
     return ((offset - embedder_fields_offset) & (kEmbedderDataSlotSize - 1)) ==
            EmbedderDataSlot::kTaggedPayloadOffset;
   }
 #else
   // We store raw aligned pointers as Smis, so it's safe to treat the whole
   // embedder field area as tagged slots.
-  STATIC_ASSERT(kEmbedderDataSlotSize == kTaggedSize);
+  static_assert(kEmbedderDataSlotSize == kTaggedSize);
 #endif
   return true;
 }
@@ -83,7 +83,7 @@ void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
                                                  int end_offset,
                                                  ObjectVisitor* v) {
 #ifdef V8_COMPRESS_POINTERS
-  STATIC_ASSERT(kEmbedderDataSlotSize == 2 * kTaggedSize);
+  static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
   int header_end_offset = JSObject::GetHeaderSize(map);
   int inobject_fields_start_offset = map.GetInObjectPropertyOffset(0);
   // We are always requested to process header and embedder fields.
@@ -106,7 +106,7 @@ void BodyDescriptorBase::IterateJSObjectBodyImpl(Map map, HeapObject obj,
 #else
   // We store raw aligned pointers as Smis, so it's safe to iterate the whole
   // embedder field area as tagged slots.
-  STATIC_ASSERT(kEmbedderDataSlotSize == kTaggedSize);
+  static_assert(kEmbedderDataSlotSize == kTaggedSize);
 #endif
   IteratePointers(obj, start_offset, end_offset, v);
 }
@@ -297,11 +297,11 @@ class JSFinalizationRegistry::BodyDescriptor final : public BodyDescriptorBase {
 
 class AllocationSite::BodyDescriptor final : public BodyDescriptorBase {
  public:
-  STATIC_ASSERT(AllocationSite::kCommonPointerFieldEndOffset ==
+  static_assert(AllocationSite::kCommonPointerFieldEndOffset ==
                 AllocationSite::kPretenureDataOffset);
-  STATIC_ASSERT(AllocationSite::kPretenureDataOffset + kInt32Size ==
+  static_assert(AllocationSite::kPretenureDataOffset + kInt32Size ==
                 AllocationSite::kPretenureCreateCountOffset);
-  STATIC_ASSERT(AllocationSite::kPretenureCreateCountOffset + kInt32Size ==
+  static_assert(AllocationSite::kPretenureCreateCountOffset + kInt32Size ==
                 AllocationSite::kWeakNextOffset);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
@@ -487,7 +487,7 @@ class V8_EXPORT_PRIVATE SwissNameDictionary::BodyDescriptor final
     // address (a Smi) rather than a map.
 
     SwissNameDictionary table = SwissNameDictionary::unchecked_cast(obj);
-    STATIC_ASSERT(MetaTablePointerOffset() + kTaggedSize ==
+    static_assert(MetaTablePointerOffset() + kTaggedSize ==
                   DataTableStartOffset());
     return offset >= MetaTablePointerOffset() &&
            (offset < table.DataTableEndOffset(table.Capacity()));
@@ -497,7 +497,7 @@ class V8_EXPORT_PRIVATE SwissNameDictionary::BodyDescriptor final
   static inline void IterateBody(Map map, HeapObject obj, int object_size,
                                  ObjectVisitor* v) {
     SwissNameDictionary table = SwissNameDictionary::unchecked_cast(obj);
-    STATIC_ASSERT(MetaTablePointerOffset() + kTaggedSize ==
+    static_assert(MetaTablePointerOffset() + kTaggedSize ==
                   DataTableStartOffset());
     int start_offset = MetaTablePointerOffset();
     int end_offset = table.DataTableEndOffset(table.Capacity());
@@ -617,7 +617,7 @@ class PromiseOnStack::BodyDescriptor final : public BodyDescriptorBase {
                                  ObjectVisitor* v) {
     IteratePointers(obj, Struct::kHeaderSize, kPromiseOffset, v);
     IterateMaybeWeakPointer(obj, kPromiseOffset, v);
-    STATIC_ASSERT(kPromiseOffset + kTaggedSize == kHeaderSize);
+    static_assert(kPromiseOffset + kTaggedSize == kHeaderSize);
   }
 
   static inline int SizeOf(Map map, HeapObject obj) {
@@ -636,7 +636,7 @@ class PrototypeInfo::BodyDescriptor final : public BodyDescriptorBase {
                                  ObjectVisitor* v) {
     IteratePointers(obj, HeapObject::kHeaderSize, kObjectCreateMapOffset, v);
     IterateMaybeWeakPointer(obj, kObjectCreateMapOffset, v);
-    STATIC_ASSERT(kObjectCreateMapOffset + kTaggedSize == kHeaderSize);
+    static_assert(kObjectCreateMapOffset + kTaggedSize == kHeaderSize);
   }
 
   static inline int SizeOf(Map map, HeapObject obj) {
@@ -646,7 +646,7 @@ class PrototypeInfo::BodyDescriptor final : public BodyDescriptorBase {
 
 class JSWeakCollection::BodyDescriptorImpl final : public BodyDescriptorBase {
  public:
-  STATIC_ASSERT(kTableOffset + kTaggedSize == kHeaderSizeOfAllWeakCollections);
+  static_assert(kTableOffset + kTaggedSize == kHeaderSizeOfAllWeakCollections);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
     return IsValidJSObjectSlotImpl(map, obj, offset);
@@ -739,7 +739,7 @@ class WasmInstanceObject::BodyDescriptor final : public BodyDescriptorBase {
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
     SLOW_DCHECK(std::is_sorted(std::begin(kTaggedFieldOffsets),
                                std::end(kTaggedFieldOffsets)));
-    STATIC_ASSERT(sizeof(*kTaggedFieldOffsets) == sizeof(uint16_t));
+    static_assert(sizeof(*kTaggedFieldOffsets) == sizeof(uint16_t));
     if (offset < int{8 * sizeof(*kTaggedFieldOffsets)} &&
         std::binary_search(std::begin(kTaggedFieldOffsets),
                            std::end(kTaggedFieldOffsets),
@@ -869,12 +869,12 @@ class CoverageInfo::BodyDescriptor final : public BodyDescriptorBase {
 
 class Code::BodyDescriptor final : public BodyDescriptorBase {
  public:
-  STATIC_ASSERT(kRelocationInfoOffset + kTaggedSize ==
+  static_assert(kRelocationInfoOffset + kTaggedSize ==
                 kDeoptimizationDataOrInterpreterDataOffset);
-  STATIC_ASSERT(kDeoptimizationDataOrInterpreterDataOffset + kTaggedSize ==
+  static_assert(kDeoptimizationDataOrInterpreterDataOffset + kTaggedSize ==
                 kPositionTableOffset);
-  STATIC_ASSERT(kPositionTableOffset + kTaggedSize == kCodeDataContainerOffset);
-  STATIC_ASSERT(kCodeDataContainerOffset + kTaggedSize == kDataStart);
+  static_assert(kPositionTableOffset + kTaggedSize == kCodeDataContainerOffset);
+  static_assert(kCodeDataContainerOffset + kTaggedSize == kDataStart);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
     // Slots in code can't be invalid because we never trim code objects.
@@ -1011,14 +1011,14 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
  public:
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
 #ifdef V8_COMPRESS_POINTERS
-    STATIC_ASSERT(kEmbedderDataSlotSize == 2 * kTaggedSize);
-    STATIC_ASSERT(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
+    static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
+    static_assert(base::bits::IsPowerOfTwo(kEmbedderDataSlotSize));
     return (offset < EmbedderDataArray::kHeaderSize) ||
            (((offset - EmbedderDataArray::kHeaderSize) &
              (kEmbedderDataSlotSize - 1)) ==
             EmbedderDataSlot::kTaggedPayloadOffset);
 #else
-    STATIC_ASSERT(kEmbedderDataSlotSize == kTaggedSize);
+    static_assert(kEmbedderDataSlotSize == kTaggedSize);
     // We store raw aligned pointers as Smis, so it's safe to iterate the whole
     // array.
     return true;
@@ -1029,7 +1029,7 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
   static inline void IterateBody(Map map, HeapObject obj, int object_size,
                                  ObjectVisitor* v) {
 #ifdef V8_COMPRESS_POINTERS
-    STATIC_ASSERT(kEmbedderDataSlotSize == 2 * kTaggedSize);
+    static_assert(kEmbedderDataSlotSize == 2 * kTaggedSize);
     for (int offset = EmbedderDataArray::OffsetOfElementAt(0);
          offset < object_size; offset += kEmbedderDataSlotSize) {
       IteratePointer(obj, offset + EmbedderDataSlot::kTaggedPayloadOffset, v);
@@ -1041,7 +1041,7 @@ class EmbedderDataArray::BodyDescriptor final : public BodyDescriptorBase {
 #else
     // We store raw aligned pointers as Smis, so it's safe to iterate the whole
     // array.
-    STATIC_ASSERT(kEmbedderDataSlotSize == kTaggedSize);
+    static_assert(kEmbedderDataSlotSize == kTaggedSize);
     IteratePointers(obj, EmbedderDataArray::kHeaderSize, object_size, v);
 #endif
   }
diff --git a/src/objects/objects-body-descriptors.h b/src/objects/objects-body-descriptors.h
index 455578940b0..9c863a065f8 100644
--- a/src/objects/objects-body-descriptors.h
+++ b/src/objects/objects-body-descriptors.h
@@ -220,7 +220,7 @@ class SubclassBodyDescriptor final : public BodyDescriptorBase {
  public:
   // The parent must end be before the child's start offset, to make sure that
   // their slots are disjoint.
-  STATIC_ASSERT(ParentBodyDescriptor::kSize <=
+  static_assert(ParentBodyDescriptor::kSize <=
                 ChildBodyDescriptor::kStartOffset);
 
   static bool IsValidSlot(Map map, HeapObject obj, int offset) {
diff --git a/src/objects/objects-inl.h b/src/objects/objects-inl.h
index 82d8776ef34..6d431b63a95 100644
--- a/src/objects/objects-inl.h
+++ b/src/objects/objects-inl.h
@@ -761,18 +761,18 @@ HeapObject MapWord::ToForwardingAddress(PtrComprCageBase host_cage_base) {
 #ifdef VERIFY_HEAP
 void HeapObject::VerifyObjectField(Isolate* isolate, int offset) {
   VerifyPointer(isolate, TaggedField<Object>::load(isolate, *this, offset));
-  STATIC_ASSERT(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
+  static_assert(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
 }
 
 void HeapObject::VerifyMaybeObjectField(Isolate* isolate, int offset) {
   MaybeObject::VerifyMaybeObjectPointer(
       isolate, TaggedField<MaybeObject>::load(isolate, *this, offset));
-  STATIC_ASSERT(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
+  static_assert(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
 }
 
 void HeapObject::VerifySmiField(int offset) {
   CHECK(TaggedField<Object>::load(*this, offset).IsSmi());
-  STATIC_ASSERT(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
+  static_assert(!COMPRESS_POINTERS_BOOL || kTaggedSize == kInt32Size);
 }
 
 #endif
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index b3404cadfc5..78df915d20c 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -1040,7 +1040,7 @@ MaybeHandle<FixedArray> CreateListFromArrayLikeFastPath(
           length > static_cast<size_t>(FixedArray::kMaxLength)) {
         return MaybeHandle<FixedArray>();
       }
-      STATIC_ASSERT(FixedArray::kMaxLength <=
+      static_assert(FixedArray::kMaxLength <=
                     std::numeric_limits<uint32_t>::max());
       return array->GetElementsAccessor()->CreateListFromArrayLike(
           isolate, array, static_cast<uint32_t>(length));
@@ -1330,7 +1330,7 @@ Handle<TemplateList> TemplateList::New(Isolate* isolate, int size) {
 Handle<TemplateList> TemplateList::Add(Isolate* isolate,
                                        Handle<TemplateList> list,
                                        Handle<i::Object> value) {
-  STATIC_ASSERT(kFirstElementIndex == 1);
+  static_assert(kFirstElementIndex == 1);
   int index = list->length() + 1;
   Handle<i::FixedArray> fixed_array = Handle<FixedArray>::cast(list);
   fixed_array = FixedArray::SetAndGrow(isolate, fixed_array, index, value);
@@ -4518,7 +4518,7 @@ void DescriptorArray::Sort() {
 
 int16_t DescriptorArray::UpdateNumberOfMarkedDescriptors(
     unsigned mark_compact_epoch, int16_t new_marked) {
-  STATIC_ASSERT(kMaxNumberOfDescriptors <=
+  static_assert(kMaxNumberOfDescriptors <=
                 NumberOfMarkedDescriptors::kMaxNumberOfMarkedDescriptors);
   int16_t old_raw_marked = raw_number_of_marked_descriptors();
   int16_t old_marked =
@@ -5665,7 +5665,7 @@ Handle<Object> JSPromise::TriggerPromiseReactions(Isolate* isolate,
     }
     if (!has_handler_context) handler_context = isolate->native_context();
 
-    STATIC_ASSERT(
+    static_assert(
         static_cast<int>(PromiseReaction::kSize) ==
         static_cast<int>(
             PromiseReactionJobTask::kSizeOfAllPromiseReactionJobTasks));
@@ -5677,14 +5677,14 @@ Handle<Object> JSPromise::TriggerPromiseReactions(Isolate* isolate,
           *argument);
       Handle<PromiseFulfillReactionJobTask>::cast(task)->set_context(
           *handler_context);
-      STATIC_ASSERT(
+      static_assert(
           static_cast<int>(PromiseReaction::kFulfillHandlerOffset) ==
           static_cast<int>(PromiseFulfillReactionJobTask::kHandlerOffset));
-      STATIC_ASSERT(
+      static_assert(
           static_cast<int>(PromiseReaction::kPromiseOrCapabilityOffset) ==
           static_cast<int>(
               PromiseFulfillReactionJobTask::kPromiseOrCapabilityOffset));
-      STATIC_ASSERT(
+      static_assert(
           static_cast<int>(
               PromiseReaction::kContinuationPreservedEmbedderDataOffset) ==
           static_cast<int>(PromiseFulfillReactionJobTask::
@@ -5699,11 +5699,11 @@ Handle<Object> JSPromise::TriggerPromiseReactions(Isolate* isolate,
           *handler_context);
       Handle<PromiseRejectReactionJobTask>::cast(task)->set_handler(
           *primary_handler);
-      STATIC_ASSERT(
+      static_assert(
           static_cast<int>(PromiseReaction::kPromiseOrCapabilityOffset) ==
           static_cast<int>(
               PromiseRejectReactionJobTask::kPromiseOrCapabilityOffset));
-      STATIC_ASSERT(
+      static_assert(
           static_cast<int>(
               PromiseReaction::kContinuationPreservedEmbedderDataOffset) ==
           static_cast<int>(PromiseRejectReactionJobTask::
diff --git a/src/objects/oddball.h b/src/objects/oddball.h
index 71253aea6b3..2be38f45215 100644
--- a/src/objects/oddball.h
+++ b/src/objects/oddball.h
@@ -76,9 +76,9 @@ class Oddball : public PrimitiveHeapObject {
   using BodyDescriptor =
       FixedBodyDescriptor<kToStringOffset, kKindOffset, kSize>;
 
-  STATIC_ASSERT(kKindOffset == Internals::kOddballKindOffset);
-  STATIC_ASSERT(kNull == Internals::kNullOddballKind);
-  STATIC_ASSERT(kUndefined == Internals::kUndefinedOddballKind);
+  static_assert(kKindOffset == Internals::kOddballKindOffset);
+  static_assert(kNull == Internals::kNullOddballKind);
+  static_assert(kUndefined == Internals::kUndefinedOddballKind);
 
   DECL_PRINTER(Oddball)
 
diff --git a/src/objects/ordered-hash-table.h b/src/objects/ordered-hash-table.h
index ec304170542..60a6343bb06 100644
--- a/src/objects/ordered-hash-table.h
+++ b/src/objects/ordered-hash-table.h
@@ -487,7 +487,7 @@ class SmallOrderedHashTable : public HeapObject {
   // values, which means that this value can't be used a valid
   // index.
   static const int kMaxCapacity = 254;
-  STATIC_ASSERT(kMaxCapacity < kNotFound);
+  static_assert(kMaxCapacity < kNotFound);
 
   // The load factor is used to derive the number of buckets from
   // capacity during Allocation. We also depend on this to calaculate
@@ -674,7 +674,7 @@ class SmallOrderedHashSet : public SmallOrderedHashTable<SmallOrderedHashSet> {
                       SmallOrderedHashTable<SmallOrderedHashSet>);
 };
 
-STATIC_ASSERT(kSmallOrderedHashSetMinCapacity ==
+static_assert(kSmallOrderedHashSetMinCapacity ==
               SmallOrderedHashSet::kMinCapacity);
 
 class SmallOrderedHashMap : public SmallOrderedHashTable<SmallOrderedHashMap> {
@@ -709,7 +709,7 @@ class SmallOrderedHashMap : public SmallOrderedHashTable<SmallOrderedHashMap> {
                       SmallOrderedHashTable<SmallOrderedHashMap>);
 };
 
-STATIC_ASSERT(kSmallOrderedHashMapMinCapacity ==
+static_assert(kSmallOrderedHashMapMinCapacity ==
               SmallOrderedHashMap::kMinCapacity);
 
 // TODO(gsathya): Rename this to OrderedHashTable, after we rename
diff --git a/src/objects/primitive-heap-object.h b/src/objects/primitive-heap-object.h
index 14023c51980..ec327fa97b6 100644
--- a/src/objects/primitive-heap-object.h
+++ b/src/objects/primitive-heap-object.h
@@ -22,7 +22,7 @@ class PrimitiveHeapObject
     : public TorqueGeneratedPrimitiveHeapObject<PrimitiveHeapObject,
                                                 HeapObject> {
  public:
-  STATIC_ASSERT(kHeaderSize == HeapObject::kHeaderSize);
+  static_assert(kHeaderSize == HeapObject::kHeaderSize);
   TQ_OBJECT_CONSTRUCTORS(PrimitiveHeapObject)
 };
 
diff --git a/src/objects/promise.h b/src/objects/promise.h
index f449e416c83..5d36719c5f6 100644
--- a/src/objects/promise.h
+++ b/src/objects/promise.h
@@ -43,7 +43,7 @@ class PromiseFulfillReactionJobTask
     : public TorqueGeneratedPromiseFulfillReactionJobTask<
           PromiseFulfillReactionJobTask, PromiseReactionJobTask> {
  public:
-  STATIC_ASSERT(kSize == kSizeOfAllPromiseReactionJobTasks);
+  static_assert(kSize == kSizeOfAllPromiseReactionJobTasks);
 
   using BodyDescriptor = StructBodyDescriptor;
 
@@ -55,7 +55,7 @@ class PromiseRejectReactionJobTask
     : public TorqueGeneratedPromiseRejectReactionJobTask<
           PromiseRejectReactionJobTask, PromiseReactionJobTask> {
  public:
-  STATIC_ASSERT(kSize == kSizeOfAllPromiseReactionJobTasks);
+  static_assert(kSize == kSizeOfAllPromiseReactionJobTasks);
 
   using BodyDescriptor = StructBodyDescriptor;
 
diff --git a/src/objects/property-details.h b/src/objects/property-details.h
index a354c661973..517c1669619 100644
--- a/src/objects/property-details.h
+++ b/src/objects/property-details.h
@@ -55,28 +55,28 @@ enum PropertyFilter {
   ENUMERABLE_STRINGS = ONLY_ENUMERABLE | SKIP_SYMBOLS,
 };
 // Enable fast comparisons of PropertyAttributes against PropertyFilters.
-STATIC_ASSERT(ALL_PROPERTIES == static_cast<PropertyFilter>(NONE));
-STATIC_ASSERT(ONLY_WRITABLE == static_cast<PropertyFilter>(READ_ONLY));
-STATIC_ASSERT(ONLY_ENUMERABLE == static_cast<PropertyFilter>(DONT_ENUM));
-STATIC_ASSERT(ONLY_CONFIGURABLE == static_cast<PropertyFilter>(DONT_DELETE));
-STATIC_ASSERT(((SKIP_STRINGS | SKIP_SYMBOLS | ONLY_ALL_CAN_READ) &
+static_assert(ALL_PROPERTIES == static_cast<PropertyFilter>(NONE));
+static_assert(ONLY_WRITABLE == static_cast<PropertyFilter>(READ_ONLY));
+static_assert(ONLY_ENUMERABLE == static_cast<PropertyFilter>(DONT_ENUM));
+static_assert(ONLY_CONFIGURABLE == static_cast<PropertyFilter>(DONT_DELETE));
+static_assert(((SKIP_STRINGS | SKIP_SYMBOLS | ONLY_ALL_CAN_READ) &
                ALL_ATTRIBUTES_MASK) == 0);
-STATIC_ASSERT(ALL_PROPERTIES ==
+static_assert(ALL_PROPERTIES ==
               static_cast<PropertyFilter>(v8::PropertyFilter::ALL_PROPERTIES));
-STATIC_ASSERT(ONLY_WRITABLE ==
+static_assert(ONLY_WRITABLE ==
               static_cast<PropertyFilter>(v8::PropertyFilter::ONLY_WRITABLE));
-STATIC_ASSERT(ONLY_ENUMERABLE ==
+static_assert(ONLY_ENUMERABLE ==
               static_cast<PropertyFilter>(v8::PropertyFilter::ONLY_ENUMERABLE));
-STATIC_ASSERT(ONLY_CONFIGURABLE == static_cast<PropertyFilter>(
+static_assert(ONLY_CONFIGURABLE == static_cast<PropertyFilter>(
                                        v8::PropertyFilter::ONLY_CONFIGURABLE));
-STATIC_ASSERT(SKIP_STRINGS ==
+static_assert(SKIP_STRINGS ==
               static_cast<PropertyFilter>(v8::PropertyFilter::SKIP_STRINGS));
-STATIC_ASSERT(SKIP_SYMBOLS ==
+static_assert(SKIP_SYMBOLS ==
               static_cast<PropertyFilter>(v8::PropertyFilter::SKIP_SYMBOLS));
 
 // Assert that kPropertyAttributesBitsCount value matches the definition of
 // ALL_ATTRIBUTES_MASK.
-STATIC_ASSERT((ALL_ATTRIBUTES_MASK == (READ_ONLY | DONT_ENUM | DONT_DELETE)) ==
+static_assert((ALL_ATTRIBUTES_MASK == (READ_ONLY | DONT_ENUM | DONT_DELETE)) ==
               (kPropertyAttributesBitsCount == 3));
 
 class Smi;
@@ -232,7 +232,7 @@ class Representation {
   explicit constexpr Representation(Kind k) : kind_(k) {}
 
   // Make sure kind fits in int8.
-  STATIC_ASSERT(kNumRepresentations <= (1 << kBitsPerByte));
+  static_assert(kNumRepresentations <= (1 << kBitsPerByte));
 
   int8_t kind_;
 };
@@ -418,18 +418,18 @@ class PropertyDetails {
       DescriptorPointer::Next<uint32_t, kDescriptorIndexBitCount>;
 
   // All bits for both fast and slow objects must fit in a smi.
-  STATIC_ASSERT(DictionaryStorageField::kLastUsedBit < 31);
-  STATIC_ASSERT(FieldIndexField::kLastUsedBit < 31);
+  static_assert(DictionaryStorageField::kLastUsedBit < 31);
+  static_assert(FieldIndexField::kLastUsedBit < 31);
 
   // DictionaryStorageField must be the last field, so that overflowing it
   // doesn't overwrite other fields.
-  STATIC_ASSERT(DictionaryStorageField::kLastUsedBit == 30);
+  static_assert(DictionaryStorageField::kLastUsedBit == 30);
 
   // All bits for non-global dictionary mode objects except enumeration index
   // must fit in a byte.
-  STATIC_ASSERT(KindField::kLastUsedBit < 8);
-  STATIC_ASSERT(ConstnessField::kLastUsedBit < 8);
-  STATIC_ASSERT(AttributesField::kLastUsedBit < 8);
+  static_assert(KindField::kLastUsedBit < 8);
+  static_assert(ConstnessField::kLastUsedBit < 8);
+  static_assert(AttributesField::kLastUsedBit < 8);
 
   static const int kInitialIndex = 1;
 
@@ -463,10 +463,10 @@ class PropertyDetails {
     // into a byte together.
 
     DCHECK_EQ(PropertyLocation::kField, location());
-    STATIC_ASSERT(static_cast<int>(PropertyLocation::kField) == 0);
+    static_assert(static_cast<int>(PropertyLocation::kField) == 0);
 
     DCHECK_EQ(PropertyCellType::kNoCell, cell_type());
-    STATIC_ASSERT(static_cast<int>(PropertyCellType::kNoCell) == 0);
+    static_assert(static_cast<int>(PropertyCellType::kNoCell) == 0);
 
     // Only to be used when the enum index isn't actually maintained
     // by the PropertyDetails:
diff --git a/src/objects/scope-info.cc b/src/objects/scope-info.cc
index 70bc3de7b66..c70ac7bb069 100644
--- a/src/objects/scope-info.cc
+++ b/src/objects/scope-info.cc
@@ -164,7 +164,7 @@ Handle<ScopeInfo> ScopeInfo::Create(IsolateT* isolate, Zone* zone, Scope* scope,
 
 // Make sure the Fields enum agrees with Torque-generated offsets.
 #define ASSERT_MATCHED_FIELD(name) \
-  STATIC_ASSERT(OffsetOfElementAt(k##name) == k##name##Offset);
+  static_assert(OffsetOfElementAt(k##name) == k##name##Offset);
   FOR_EACH_SCOPE_INFO_NUMERIC_FIELD(ASSERT_MATCHED_FIELD)
 #undef ASSERT_MATCHED_FIELD
 
diff --git a/src/objects/scope-info.h b/src/objects/scope-info.h
index 06363f23162..3d266606f62 100644
--- a/src/objects/scope-info.h
+++ b/src/objects/scope-info.h
@@ -293,8 +293,8 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
         kVariablePartIndex
   };
 
-  STATIC_ASSERT(LanguageModeSize == 1 << LanguageModeBit::kSize);
-  STATIC_ASSERT(FunctionKind::kLastFunctionKind <= FunctionKindBits::kMax);
+  static_assert(LanguageModeSize == 1 << LanguageModeBit::kSize);
+  static_assert(FunctionKind::kLastFunctionKind <= FunctionKindBits::kMax);
 
   bool IsEmpty() const;
 
diff --git a/src/objects/shared-function-info-inl.h b/src/objects/shared-function-info-inl.h
index 2a5abcb9e04..4a9bd731690 100644
--- a/src/objects/shared-function-info-inl.h
+++ b/src/objects/shared-function-info-inl.h
@@ -317,12 +317,12 @@ void SharedFunctionInfo::set_osr_code_cache_state(
 }
 
 LanguageMode SharedFunctionInfo::language_mode() const {
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   return construct_language_mode(IsStrictBit::decode(flags(kRelaxedLoad)));
 }
 
 void SharedFunctionInfo::set_language_mode(LanguageMode language_mode) {
-  STATIC_ASSERT(LanguageModeSize == 2);
+  static_assert(LanguageModeSize == 2);
   // We only allow language mode transitions that set the same language mode
   // again or go up in the chain:
   DCHECK(is_sloppy(this->language_mode()) || is_strict(language_mode));
@@ -333,7 +333,7 @@ void SharedFunctionInfo::set_language_mode(LanguageMode language_mode) {
 }
 
 FunctionKind SharedFunctionInfo::kind() const {
-  STATIC_ASSERT(FunctionKindBits::kSize == kFunctionKindBitSize);
+  static_assert(FunctionKindBits::kSize == kFunctionKindBitSize);
   return FunctionKindBits::decode(flags(kRelaxedLoad));
 }
 
@@ -378,7 +378,7 @@ int SharedFunctionInfo::function_map_index() const {
 }
 
 void SharedFunctionInfo::set_function_map_index(int index) {
-  STATIC_ASSERT(Context::LAST_FUNCTION_MAP_INDEX <=
+  static_assert(Context::LAST_FUNCTION_MAP_INDEX <=
                 Context::FIRST_FUNCTION_MAP_INDEX + FunctionMapIndexBits::kMax);
   DCHECK_LE(Context::FIRST_FUNCTION_MAP_INDEX, index);
   DCHECK_LE(index, Context::LAST_FUNCTION_MAP_INDEX);
@@ -828,9 +828,9 @@ void SharedFunctionInfo::ClearPreparseData() {
   // We are basically trimming that object to its supertype, so recorded slots
   // within the object don't need to be invalidated.
   heap->NotifyObjectLayoutChange(data, no_gc, InvalidateRecordedSlots::kNo);
-  STATIC_ASSERT(UncompiledDataWithoutPreparseData::kSize <
+  static_assert(UncompiledDataWithoutPreparseData::kSize <
                 UncompiledDataWithPreparseData::kSize);
-  STATIC_ASSERT(UncompiledDataWithoutPreparseData::kSize ==
+  static_assert(UncompiledDataWithoutPreparseData::kSize ==
                 UncompiledData::kHeaderSize);
 
   // Fill the remaining space with filler and clear slots in the trimmed area.
diff --git a/src/objects/shared-function-info.cc b/src/objects/shared-function-info.cc
index 624e7a26e6b..fb1f10c8798 100644
--- a/src/objects/shared-function-info.cc
+++ b/src/objects/shared-function-info.cc
@@ -600,7 +600,7 @@ void SharedFunctionInfo::UpdateExpectedNofPropertiesFromEstimate(
     FunctionLiteral* literal) {
   // Limit actual estimate to fit in a 8 bit field, we will never allocate
   // more than this in any case.
-  STATIC_ASSERT(JSObject::kMaxInObjectProperties <= kMaxUInt8);
+  static_assert(JSObject::kMaxInObjectProperties <= kMaxUInt8);
   int estimate = get_property_estimate_from_literal(literal);
   set_expected_nof_properties(std::min(estimate, kMaxUInt8));
 }
@@ -619,7 +619,7 @@ void SharedFunctionInfo::UpdateAndFinalizeExpectedNofPropertiesFromEstimate(
 
   // Limit actual estimate to fit in a 8 bit field, we will never allocate
   // more than this in any case.
-  STATIC_ASSERT(JSObject::kMaxInObjectProperties <= kMaxUInt8);
+  static_assert(JSObject::kMaxInObjectProperties <= kMaxUInt8);
   estimate = std::min(estimate, kMaxUInt8);
 
   set_expected_nof_properties(estimate);
diff --git a/src/objects/shared-function-info.h b/src/objects/shared-function-info.h
index ed7ceca4223..9aadc5926df 100644
--- a/src/objects/shared-function-info.h
+++ b/src/objects/shared-function-info.h
@@ -664,18 +664,18 @@ class SharedFunctionInfo
   // Constants.
   static const int kMaximumFunctionTokenOffset = kMaxUInt16 - 1;
   static const uint16_t kFunctionTokenOutOfRange = static_cast<uint16_t>(-1);
-  STATIC_ASSERT(kMaximumFunctionTokenOffset + 1 == kFunctionTokenOutOfRange);
+  static_assert(kMaximumFunctionTokenOffset + 1 == kFunctionTokenOutOfRange);
 
   static const int kAlignedSize = OBJECT_POINTER_ALIGN(kSize);
 
   class BodyDescriptor;
 
   // Bailout reasons must fit in the DisabledOptimizationReason bitfield.
-  STATIC_ASSERT(BailoutReason::kLastErrorMessage <=
+  static_assert(BailoutReason::kLastErrorMessage <=
                 DisabledOptimizationReasonBits::kMax);
 
-  STATIC_ASSERT(FunctionKind::kLastFunctionKind <= FunctionKindBits::kMax);
-  STATIC_ASSERT(FunctionSyntaxKind::kLastFunctionSyntaxKind <=
+  static_assert(FunctionKind::kLastFunctionKind <= FunctionKindBits::kMax);
+  static_assert(FunctionSyntaxKind::kLastFunctionSyntaxKind <=
                 FunctionSyntaxKindBits::kMax);
 
   // Sets the bytecode in {shared}'s DebugInfo as the bytecode to
diff --git a/src/objects/smi.h b/src/objects/smi.h
index 49d402ff3f6..e04f6ac0501 100644
--- a/src/objects/smi.h
+++ b/src/objects/smi.h
@@ -63,7 +63,7 @@ class Smi : public Object {
   template <typename E,
             typename = typename std::enable_if<std::is_enum<E>::value>::type>
   static inline Smi FromEnum(E value) {
-    STATIC_ASSERT(sizeof(E) <= sizeof(int));
+    static_assert(sizeof(E) <= sizeof(int));
     return FromInt(static_cast<int>(value));
   }
 
diff --git a/src/objects/source-text-module.h b/src/objects/source-text-module.h
index c91accba0cb..d9fbe38b585 100644
--- a/src/objects/source-text-module.h
+++ b/src/objects/source-text-module.h
@@ -132,9 +132,9 @@ class SourceTextModule
   // If 0, this module is not async or has not been async evaluated.
   static constexpr unsigned kNotAsyncEvaluated = 0;
   static constexpr unsigned kAsyncEvaluateDidFinish = 1;
-  STATIC_ASSERT(kNotAsyncEvaluated < kAsyncEvaluateDidFinish);
-  STATIC_ASSERT(kAsyncEvaluateDidFinish < kFirstAsyncEvaluatingOrdinal);
-  STATIC_ASSERT(kMaxModuleAsyncEvaluatingOrdinal ==
+  static_assert(kNotAsyncEvaluated < kAsyncEvaluateDidFinish);
+  static_assert(kAsyncEvaluateDidFinish < kFirstAsyncEvaluatingOrdinal);
+  static_assert(kMaxModuleAsyncEvaluatingOrdinal ==
                 AsyncEvaluatingOrdinalBits::kMax);
   DECL_PRIMITIVE_ACCESSORS(async_evaluating_ordinal, unsigned)
 
diff --git a/src/objects/string-inl.h b/src/objects/string-inl.h
index 3464fac4dc6..7f30e911f51 100644
--- a/src/objects/string-inl.h
+++ b/src/objects/string-inl.h
@@ -137,7 +137,7 @@ StringShape::StringShape(InstanceType t) : type_(static_cast<uint32_t>(t)) {
 
 bool StringShape::IsInternalized() const {
   DCHECK(valid());
-  STATIC_ASSERT(kNotInternalizedTag != 0);
+  static_assert(kNotInternalizedTag != 0);
   return (type_ & (kIsNotStringMask | kIsNotInternalizedMask)) ==
          (kStringTag | kInternalizedTag);
 }
@@ -196,10 +196,10 @@ uint32_t StringShape::representation_encoding_and_shared_tag() const {
   return (type_ & (kStringRepresentationEncodingAndSharedMask));
 }
 
-STATIC_ASSERT((kStringRepresentationAndEncodingMask) ==
+static_assert((kStringRepresentationAndEncodingMask) ==
               Internals::kStringRepresentationAndEncodingMask);
 
-STATIC_ASSERT(static_cast<uint32_t>(kStringEncodingMask) ==
+static_assert(static_cast<uint32_t>(kStringEncodingMask) ==
               Internals::kStringEncodingMask);
 
 bool StringShape::IsSequentialOneByte() const {
@@ -214,19 +214,19 @@ bool StringShape::IsExternalOneByte() const {
   return representation_and_encoding_tag() == kExternalOneByteStringTag;
 }
 
-STATIC_ASSERT(kExternalOneByteStringTag ==
+static_assert(kExternalOneByteStringTag ==
               Internals::kExternalOneByteRepresentationTag);
 
-STATIC_ASSERT(v8::String::ONE_BYTE_ENCODING == kOneByteStringTag);
+static_assert(v8::String::ONE_BYTE_ENCODING == kOneByteStringTag);
 
 bool StringShape::IsExternalTwoByte() const {
   return representation_and_encoding_tag() == kExternalTwoByteStringTag;
 }
 
-STATIC_ASSERT(kExternalTwoByteStringTag ==
+static_assert(kExternalTwoByteStringTag ==
               Internals::kExternalTwoByteRepresentationTag);
 
-STATIC_ASSERT(v8::String::TWO_BYTE_ENCODING == kTwoByteStringTag);
+static_assert(v8::String::TWO_BYTE_ENCODING == kTwoByteStringTag);
 
 template <typename TDispatcher, typename TResult, typename... TArgs>
 inline TResult StringShape::DispatchToSpecificTypeWithoutCast(TArgs&&... args) {
@@ -301,8 +301,8 @@ DEF_GETTER(String, IsTwoByteRepresentation, bool) {
 bool String::IsOneByteRepresentationUnderneath(String string) {
   while (true) {
     uint32_t type = string.map().instance_type();
-    STATIC_ASSERT(kIsIndirectStringTag != 0);
-    STATIC_ASSERT((kIsIndirectStringMask & kStringEncodingMask) == 0);
+    static_assert(kIsIndirectStringTag != 0);
+    static_assert((kIsIndirectStringMask & kStringEncodingMask) == 0);
     DCHECK(string.IsFlat());
     switch (type & (kIsIndirectStringMask | kStringEncodingMask)) {
       case kOneByteStringTag:
@@ -858,9 +858,9 @@ String String::GetUnderlying() const {
   // wrapping string is already flattened.
   DCHECK(IsFlat());
   DCHECK(StringShape(*this).IsIndirect());
-  STATIC_ASSERT(static_cast<int>(ConsString::kFirstOffset) ==
+  static_assert(static_cast<int>(ConsString::kFirstOffset) ==
                 static_cast<int>(SlicedString::kParentOffset));
-  STATIC_ASSERT(static_cast<int>(ConsString::kFirstOffset) ==
+  static_assert(static_cast<int>(ConsString::kFirstOffset) ==
                 static_cast<int>(ThinString::kActualOffset));
   const int kUnderlyingOffset = SlicedString::kParentOffset;
   return TaggedField<String, kUnderlyingOffset>::load(*this);
diff --git a/src/objects/string-table.cc b/src/objects/string-table.cc
index 7aac6c540aa..b46e3fdaa2b 100644
--- a/src/objects/string-table.cc
+++ b/src/objects/string-table.cc
@@ -187,10 +187,10 @@ void* StringTable::Data::operator new(size_t size, int capacity) {
   DCHECK_EQ(size, sizeof(StringTable::Data));
   // Make sure that the elements_ array is at the end of Data, with no padding,
   // so that subsequent elements can be accessed as offsets from elements_.
-  STATIC_ASSERT(offsetof(StringTable::Data, elements_) ==
+  static_assert(offsetof(StringTable::Data, elements_) ==
                 sizeof(StringTable::Data) - sizeof(Tagged_t));
   // Make sure that elements_ is aligned when StringTable::Data is aligned.
-  STATIC_ASSERT(
+  static_assert(
       (alignof(StringTable::Data) + offsetof(StringTable::Data, elements_)) %
           kTaggedSize ==
       0);
@@ -755,9 +755,9 @@ Address StringTable::TryStringToIndexOrLookupExisting(Isolate* isolate,
 
   // Valid array indices are >= 0, so they cannot be mixed up with any of
   // the result sentinels, which are negative.
-  STATIC_ASSERT(
+  static_assert(
       !String::ArrayIndexValueBits::is_valid(ResultSentinel::kUnsupported));
-  STATIC_ASSERT(
+  static_assert(
       !String::ArrayIndexValueBits::is_valid(ResultSentinel::kNotFound));
 
   size_t start = 0;
@@ -886,10 +886,10 @@ void* StringForwardingTable::Block::operator new(size_t size, int capacity) {
   DCHECK_EQ(size, sizeof(StringForwardingTable::Block));
   // Make sure that the elements_ array is at the end of Block, with no padding,
   // so that subsequent elements can be accessed as offsets from elements_.
-  STATIC_ASSERT(offsetof(StringForwardingTable::Block, elements_) ==
+  static_assert(offsetof(StringForwardingTable::Block, elements_) ==
                 sizeof(StringForwardingTable::Block) - sizeof(Tagged_t) * 1);
   // Make sure that elements_ is aligned when StringTable::Block is aligned.
-  STATIC_ASSERT((alignof(StringForwardingTable::Block) +
+  static_assert((alignof(StringForwardingTable::Block) +
                  offsetof(StringForwardingTable::Block, elements_)) %
                     kTaggedSize ==
                 0);
diff --git a/src/objects/string-table.h b/src/objects/string-table.h
index 6037a9d875d..9d206522794 100644
--- a/src/objects/string-table.h
+++ b/src/objects/string-table.h
@@ -106,7 +106,7 @@ class StringForwardingTable {
  public:
   // Capacity for the first block.
   static constexpr int kInitialBlockSize = 16;
-  STATIC_ASSERT(base::bits::IsPowerOfTwo(kInitialBlockSize));
+  static_assert(base::bits::IsPowerOfTwo(kInitialBlockSize));
   static constexpr int kInitialBlockSizeHighestBit =
       kBitsPerInt - base::bits::CountLeadingZeros32(kInitialBlockSize) - 1;
   // Initial capacity in the block vector.
diff --git a/src/objects/string.cc b/src/objects/string.cc
index 08870b8ebaa..4684bcb6358 100644
--- a/src/objects/string.cc
+++ b/src/objects/string.cc
@@ -1898,18 +1898,18 @@ namespace {
 
 DEFINE_TORQUE_GENERATED_STRING_INSTANCE_TYPE()
 
-STATIC_ASSERT(kStringRepresentationMask == RepresentationBits::kMask);
+static_assert(kStringRepresentationMask == RepresentationBits::kMask);
 
-STATIC_ASSERT(kStringEncodingMask == IsOneByteBit::kMask);
-STATIC_ASSERT(kTwoByteStringTag == IsOneByteBit::encode(false));
-STATIC_ASSERT(kOneByteStringTag == IsOneByteBit::encode(true));
+static_assert(kStringEncodingMask == IsOneByteBit::kMask);
+static_assert(kTwoByteStringTag == IsOneByteBit::encode(false));
+static_assert(kOneByteStringTag == IsOneByteBit::encode(true));
 
-STATIC_ASSERT(kUncachedExternalStringMask == IsUncachedBit::kMask);
-STATIC_ASSERT(kUncachedExternalStringTag == IsUncachedBit::encode(true));
+static_assert(kUncachedExternalStringMask == IsUncachedBit::kMask);
+static_assert(kUncachedExternalStringTag == IsUncachedBit::encode(true));
 
-STATIC_ASSERT(kIsNotInternalizedMask == IsNotInternalizedBit::kMask);
-STATIC_ASSERT(kNotInternalizedTag == IsNotInternalizedBit::encode(true));
-STATIC_ASSERT(kInternalizedTag == IsNotInternalizedBit::encode(false));
+static_assert(kIsNotInternalizedMask == IsNotInternalizedBit::kMask);
+static_assert(kNotInternalizedTag == IsNotInternalizedBit::encode(true));
+static_assert(kInternalizedTag == IsNotInternalizedBit::encode(false));
 }  // namespace
 
 }  // namespace internal
diff --git a/src/objects/string.h b/src/objects/string.h
index 72e063ca460..33f5c8a2f4a 100644
--- a/src/objects/string.h
+++ b/src/objects/string.h
@@ -541,7 +541,7 @@ class String : public TorqueGeneratedString<String, Name> {
       }
 
       // Check aligned words.
-      STATIC_ASSERT(unibrow::Latin1::kMaxChar == 0xFF);
+      static_assert(unibrow::Latin1::kMaxChar == 0xFF);
 #ifdef V8_TARGET_LITTLE_ENDIAN
       const uintptr_t non_one_byte_mask = kUintptrAllBitsSet / 0xFFFF * 0xFF00;
 #else
@@ -743,7 +743,7 @@ class SeqOneByteString
   // Maximal memory usage for a single sequential one-byte string.
   static const int kMaxCharsSize = kMaxLength;
   static const int kMaxSize = OBJECT_POINTER_ALIGN(kMaxCharsSize + kHeaderSize);
-  STATIC_ASSERT((kMaxSize - kHeaderSize) >= String::kMaxLength);
+  static_assert((kMaxSize - kHeaderSize) >= String::kMaxLength);
 
   int AllocatedSize();
 
@@ -789,7 +789,7 @@ class SeqTwoByteString
   // Maximal memory usage for a single sequential two-byte string.
   static const int kMaxCharsSize = kMaxLength * 2;
   static const int kMaxSize = OBJECT_POINTER_ALIGN(kMaxCharsSize + kHeaderSize);
-  STATIC_ASSERT(static_cast<int>((kMaxSize - kHeaderSize) / sizeof(uint16_t)) >=
+  static_assert(static_cast<int>((kMaxSize - kHeaderSize) / sizeof(uint16_t)) >=
                 String::kMaxLength);
 
   int AllocatedSize();
@@ -922,7 +922,7 @@ class ExternalString
   // Disposes string's resource object if it has not already been disposed.
   inline void DisposeResource(Isolate* isolate);
 
-  STATIC_ASSERT(kResourceOffset == Internals::kStringResourceOffset);
+  static_assert(kResourceOffset == Internals::kStringResourceOffset);
   static const int kSizeOfAllExternalStrings = kHeaderSize;
 
  private:
@@ -967,7 +967,7 @@ class ExternalOneByteString
 
   class BodyDescriptor;
 
-  STATIC_ASSERT(kSize == kSizeOfAllExternalStrings);
+  static_assert(kSize == kSizeOfAllExternalStrings);
 
   TQ_OBJECT_CONSTRUCTORS(ExternalOneByteString)
 
@@ -1014,7 +1014,7 @@ class ExternalTwoByteString
 
   class BodyDescriptor;
 
-  STATIC_ASSERT(kSize == kSizeOfAllExternalStrings);
+  static_assert(kSize == kSizeOfAllExternalStrings);
 
   TQ_OBJECT_CONSTRUCTORS(ExternalTwoByteString)
 
diff --git a/src/objects/struct.h b/src/objects/struct.h
index 70c9377dffe..ac7498af1ca 100644
--- a/src/objects/struct.h
+++ b/src/objects/struct.h
@@ -24,7 +24,7 @@ class StructBodyDescriptor;
 class Struct : public TorqueGeneratedStruct<Struct, HeapObject> {
  public:
   void BriefPrintDetails(std::ostream& os);
-  STATIC_ASSERT(kHeaderSize == HeapObject::kHeaderSize);
+  static_assert(kHeaderSize == HeapObject::kHeaderSize);
 
   TQ_OBJECT_CONSTRUCTORS(Struct)
 };
diff --git a/src/objects/swiss-hash-table-helpers.h b/src/objects/swiss-hash-table-helpers.h
index ff2189be3a8..3a3ed89bc49 100644
--- a/src/objects/swiss-hash-table-helpers.h
+++ b/src/objects/swiss-hash-table-helpers.h
@@ -120,8 +120,8 @@ class ProbeSequence {
 //   for (int i : BitMask<uint64_t, 8, 3>(0x0000000080800000)) -> yields 2, 3
 template <class T, int SignificantBits, int Shift = 0>
 class BitMask {
-  STATIC_ASSERT(std::is_unsigned<T>::value);
-  STATIC_ASSERT(Shift == 0 || Shift == 3);
+  static_assert(std::is_unsigned<T>::value);
+  static_assert(Shift == 0 || Shift == 3);
 
  public:
   // These are useful for unit tests (gunit).
diff --git a/src/objects/swiss-name-dictionary-inl.h b/src/objects/swiss-name-dictionary-inl.h
index 5ea4e27e6b1..fb9822b17a5 100644
--- a/src/objects/swiss-name-dictionary-inl.h
+++ b/src/objects/swiss-name-dictionary-inl.h
@@ -436,7 +436,7 @@ int SwissNameDictionary::GetMetaTableField(int field_index) {
 template <typename T>
 void SwissNameDictionary::SetMetaTableField(ByteArray meta_table,
                                             int field_index, int value) {
-  STATIC_ASSERT((std::is_same<T, uint8_t>::value) ||
+  static_assert((std::is_same<T, uint8_t>::value) ||
                 (std::is_same<T, uint16_t>::value) ||
                 (std::is_same<T, uint32_t>::value));
   DCHECK_LE(value, std::numeric_limits<T>::max());
@@ -450,7 +450,7 @@ void SwissNameDictionary::SetMetaTableField(ByteArray meta_table,
 template <typename T>
 int SwissNameDictionary::GetMetaTableField(ByteArray meta_table,
                                            int field_index) {
-  STATIC_ASSERT((std::is_same<T, uint8_t>::value) ||
+  static_assert((std::is_same<T, uint8_t>::value) ||
                 (std::is_same<T, uint16_t>::value) ||
                 (std::is_same<T, uint32_t>::value));
   DCHECK_LT(meta_table.GetDataStartAddress() + field_index * sizeof(T),
@@ -723,9 +723,9 @@ bool SwissNameDictionary::IsEmpty(ctrl_t c) { return c == Ctrl::kEmpty; }
 
 // static
 bool SwissNameDictionary::IsFull(ctrl_t c) {
-  STATIC_ASSERT(Ctrl::kEmpty < 0);
-  STATIC_ASSERT(Ctrl::kDeleted < 0);
-  STATIC_ASSERT(Ctrl::kSentinel < 0);
+  static_assert(Ctrl::kEmpty < 0);
+  static_assert(Ctrl::kDeleted < 0);
+  static_assert(Ctrl::kSentinel < 0);
   return c >= 0;
 }
 
@@ -734,9 +734,9 @@ bool SwissNameDictionary::IsDeleted(ctrl_t c) { return c == Ctrl::kDeleted; }
 
 // static
 bool SwissNameDictionary::IsEmptyOrDeleted(ctrl_t c) {
-  STATIC_ASSERT(Ctrl::kDeleted < Ctrl::kSentinel);
-  STATIC_ASSERT(Ctrl::kEmpty < Ctrl::kSentinel);
-  STATIC_ASSERT(Ctrl::kSentinel < 0);
+  static_assert(Ctrl::kDeleted < Ctrl::kSentinel);
+  static_assert(Ctrl::kEmpty < Ctrl::kSentinel);
+  static_assert(Ctrl::kSentinel < 0);
   return c < Ctrl::kSentinel;
 }
 
diff --git a/src/objects/swiss-name-dictionary.cc b/src/objects/swiss-name-dictionary.cc
index 7579dd26851..5ecdb3a3f79 100644
--- a/src/objects/swiss-name-dictionary.cc
+++ b/src/objects/swiss-name-dictionary.cc
@@ -284,14 +284,14 @@ Object SwissNameDictionary::SlowReverseLookup(Isolate* isolate, Object value) {
 // deleted element count is MaxUsableCapacity(Capacity()). All data in the
 // meta table is unsigned. Using this, we verify the values of the constants
 // |kMax1ByteMetaTableCapacity| and |kMax2ByteMetaTableCapacity|.
-STATIC_ASSERT(SwissNameDictionary::kMax1ByteMetaTableCapacity - 1 <=
+static_assert(SwissNameDictionary::kMax1ByteMetaTableCapacity - 1 <=
               std::numeric_limits<uint8_t>::max());
-STATIC_ASSERT(SwissNameDictionary::MaxUsableCapacity(
+static_assert(SwissNameDictionary::MaxUsableCapacity(
                   SwissNameDictionary::kMax1ByteMetaTableCapacity) <=
               std::numeric_limits<uint8_t>::max());
-STATIC_ASSERT(SwissNameDictionary::kMax2ByteMetaTableCapacity - 1 <=
+static_assert(SwissNameDictionary::kMax2ByteMetaTableCapacity - 1 <=
               std::numeric_limits<uint16_t>::max());
-STATIC_ASSERT(SwissNameDictionary::MaxUsableCapacity(
+static_assert(SwissNameDictionary::MaxUsableCapacity(
                   SwissNameDictionary::kMax2ByteMetaTableCapacity) <=
               std::numeric_limits<uint16_t>::max());
 
diff --git a/src/objects/tagged-index.h b/src/objects/tagged-index.h
index 19812877cb7..eb5992d4776 100644
--- a/src/objects/tagged-index.h
+++ b/src/objects/tagged-index.h
@@ -63,7 +63,7 @@ class TaggedIndex : public Object {
   // Dispatched behavior.
   DECL_VERIFIER(TaggedIndex)
 
-  STATIC_ASSERT(kSmiTagSize == 1);
+  static_assert(kSmiTagSize == 1);
   static constexpr int kTaggedValueSize = 31;
   static constexpr intptr_t kMinValue =
       static_cast<intptr_t>(kUintptrAllBitsSet << (kTaggedValueSize - 1));
diff --git a/src/objects/transitions.h b/src/objects/transitions.h
index bda8c30d1ee..8ff3cfddc75 100644
--- a/src/objects/transitions.h
+++ b/src/objects/transitions.h
@@ -325,7 +325,7 @@ class TransitionArray : public WeakFixedArray {
                                               int value);
 
   static const int kProtoTransitionNumberOfEntriesOffset = 0;
-  STATIC_ASSERT(kProtoTransitionHeaderSize == 1);
+  static_assert(kProtoTransitionHeaderSize == 1);
 
   // Returns the fixed array length required to hold number_of_transitions
   // transitions.
diff --git a/src/parsing/preparse-data.cc b/src/parsing/preparse-data.cc
index 9739c374e73..2ecf7f7979e 100644
--- a/src/parsing/preparse-data.cc
+++ b/src/parsing/preparse-data.cc
@@ -41,7 +41,7 @@ using NumberOfParametersField = LengthEqualsParametersField::Next<uint16_t, 16>;
 
 using LanguageField = base::BitField8<LanguageMode, 0, 1>;
 using UsesSuperField = LanguageField::Next<bool, 1>;
-STATIC_ASSERT(LanguageModeSize <= LanguageField::kNumValues);
+static_assert(LanguageModeSize <= LanguageField::kNumValues);
 
 }  // namespace
 
diff --git a/src/parsing/preparser.h b/src/parsing/preparser.h
index d9476b4c60b..44ea20e919d 100644
--- a/src/parsing/preparser.h
+++ b/src/parsing/preparser.h
@@ -61,7 +61,7 @@ class PreParserIdentifier {
   bool IsAsync() const { return type_ == kAsyncIdentifier; }
   bool IsArguments() const { return type_ == kArgumentsIdentifier; }
   bool IsEvalOrArguments() const {
-    STATIC_ASSERT(kEvalIdentifier + 1 == kArgumentsIdentifier);
+    static_assert(kEvalIdentifier + 1 == kArgumentsIdentifier);
     return base::IsInRange(type_, kEvalIdentifier, kArgumentsIdentifier);
   }
   bool IsConstructor() const { return type_ == kConstructorIdentifier; }
@@ -226,7 +226,7 @@ class PreParserExpression {
   }
 
   bool IsPattern() const {
-    STATIC_ASSERT(kObjectLiteralExpression + 1 == kArrayLiteralExpression);
+    static_assert(kObjectLiteralExpression + 1 == kArrayLiteralExpression);
     return base::IsInRange(TypeField::decode(code_), kObjectLiteralExpression,
                            kArrayLiteralExpression);
   }
diff --git a/src/parsing/scanner-inl.h b/src/parsing/scanner-inl.h
index 0e191330c04..83aa1290fef 100644
--- a/src/parsing/scanner-inl.h
+++ b/src/parsing/scanner-inl.h
@@ -120,7 +120,7 @@ inline constexpr bool CanBeKeywordCharacter(char c) {
 }
 
 // Make sure tokens are stored as a single byte.
-STATIC_ASSERT(sizeof(Token::Value) == 1);
+static_assert(sizeof(Token::Value) == 1);
 
 // Get the shortest token that this character starts, the token may change
 // depending on subsequent characters.
@@ -261,12 +261,12 @@ V8_INLINE Token::Value Scanner::ScanIdentifierOrKeywordInner() {
   bool escaped = false;
   bool can_be_keyword = true;
 
-  STATIC_ASSERT(arraysize(character_scan_flags) == kMaxAscii + 1);
+  static_assert(arraysize(character_scan_flags) == kMaxAscii + 1);
   if (V8_LIKELY(static_cast<uint32_t>(c0_) <= kMaxAscii)) {
     if (V8_LIKELY(c0_ != '\\')) {
       uint8_t scan_flags = character_scan_flags[c0_];
       DCHECK(!TerminatesLiteral(scan_flags));
-      STATIC_ASSERT(static_cast<uint8_t>(ScanFlags::kCannotBeKeywordStart) ==
+      static_assert(static_cast<uint8_t>(ScanFlags::kCannotBeKeywordStart) ==
                     static_cast<uint8_t>(ScanFlags::kCannotBeKeyword) << 1);
       scan_flags >>= 1;
       // Make sure the shifting above doesn't set IdentifierNeedsSlowPath.
diff --git a/src/parsing/scanner.cc b/src/parsing/scanner.cc
index 646970faee8..81894694afb 100644
--- a/src/parsing/scanner.cc
+++ b/src/parsing/scanner.cc
@@ -938,7 +938,7 @@ Token::Value Scanner::ScanIdentifierOrKeywordInnerSlow(bool escaped,
 
     if (!escaped) return token;
 
-    STATIC_ASSERT(Token::LET + 1 == Token::STATIC);
+    static_assert(Token::LET + 1 == Token::STATIC);
     if (base::IsInRange(token, Token::LET, Token::STATIC)) {
       return Token::ESCAPED_STRICT_RESERVED_WORD;
     }
diff --git a/src/parsing/scanner.h b/src/parsing/scanner.h
index 3a38ed243bb..b638185b5b7 100644
--- a/src/parsing/scanner.h
+++ b/src/parsing/scanner.h
@@ -482,7 +482,7 @@ class V8_EXPORT_PRIVATE Scanner {
   // Call this after setting source_ to the input.
   void Init() {
     // Set c0_ (one character ahead)
-    STATIC_ASSERT(kCharacterLookaheadBufferSize == 1);
+    static_assert(kCharacterLookaheadBufferSize == 1);
     Advance();
 
     current_ = &token_storage_[0];
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index aa2a73f7d44..fe3eb974833 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -306,7 +306,7 @@ void HeapEntry::SetNamedAutoIndexReference(HeapGraphEdge::Type type,
 
 void HeapEntry::Print(const char* prefix, const char* edge_name, int max_depth,
                       int indent) const {
-  STATIC_ASSERT(sizeof(unsigned) == sizeof(id()));
+  static_assert(sizeof(unsigned) == sizeof(id()));
   base::OS::Print("%6zu @%6u %*c %s%s: ", self_size(), id(), indent, ' ',
                   prefix, edge_name);
   if (type() != kString) {
@@ -394,13 +394,13 @@ HeapSnapshot::HeapSnapshot(HeapProfiler* profiler,
       numerics_mode_(numerics_mode) {
   // It is very important to keep objects that form a heap snapshot
   // as small as possible. Check assumptions about data structure sizes.
-  STATIC_ASSERT(kSystemPointerSize != 4 || sizeof(HeapGraphEdge) == 12);
-  STATIC_ASSERT(kSystemPointerSize != 8 || sizeof(HeapGraphEdge) == 24);
-  STATIC_ASSERT(kSystemPointerSize != 4 || sizeof(HeapEntry) == 32);
+  static_assert(kSystemPointerSize != 4 || sizeof(HeapGraphEdge) == 12);
+  static_assert(kSystemPointerSize != 8 || sizeof(HeapGraphEdge) == 24);
+  static_assert(kSystemPointerSize != 4 || sizeof(HeapEntry) == 32);
 #if V8_CC_MSVC
-  STATIC_ASSERT(kSystemPointerSize != 8 || sizeof(HeapEntry) == 48);
+  static_assert(kSystemPointerSize != 8 || sizeof(HeapEntry) == 48);
 #else   // !V8_CC_MSVC
-  STATIC_ASSERT(kSystemPointerSize != 8 || sizeof(HeapEntry) == 40);
+  static_assert(kSystemPointerSize != 8 || sizeof(HeapEntry) == 40);
 #endif  // !V8_CC_MSVC
   memset(&gc_subroot_entries_, 0, sizeof(gc_subroot_entries_));
 }
@@ -1220,7 +1220,7 @@ void V8HeapExplorer::ExtractJSObjectReferences(HeapEntry* entry,
                          JSGlobalObject::kNativeContextOffset);
     SetInternalReference(entry, "global_proxy", global_obj.global_proxy(),
                          JSGlobalObject::kGlobalProxyOffset);
-    STATIC_ASSERT(JSGlobalObject::kHeaderSize - JSObject::kHeaderSize ==
+    static_assert(JSGlobalObject::kHeaderSize - JSObject::kHeaderSize ==
                   2 * kTaggedSize);
   } else if (obj.IsJSArrayBufferView()) {
     JSArrayBufferView view = JSArrayBufferView::cast(obj);
@@ -1362,10 +1362,10 @@ void V8HeapExplorer::ExtractContextReferences(HeapEntry* entry,
                      context.get(Context::DEOPTIMIZED_CODE_LIST),
                      Context::OffsetOfElementAt(Context::DEOPTIMIZED_CODE_LIST),
                      HeapEntry::kCustomWeakPointer);
-    STATIC_ASSERT(Context::OPTIMIZED_CODE_LIST == Context::FIRST_WEAK_SLOT);
-    STATIC_ASSERT(Context::NEXT_CONTEXT_LINK + 1 ==
+    static_assert(Context::OPTIMIZED_CODE_LIST == Context::FIRST_WEAK_SLOT);
+    static_assert(Context::NEXT_CONTEXT_LINK + 1 ==
                   Context::NATIVE_CONTEXT_SLOTS);
-    STATIC_ASSERT(Context::FIRST_WEAK_SLOT + 3 ==
+    static_assert(Context::FIRST_WEAK_SLOT + 3 ==
                   Context::NATIVE_CONTEXT_SLOTS);
   }
 }
@@ -2931,7 +2931,7 @@ template<> struct ToUnsigned<8> {
 template <typename T>
 static int utoa_impl(T value, const base::Vector<char>& buffer,
                      int buffer_pos) {
-  STATIC_ASSERT(static_cast<T>(-1) > 0);  // Check that T is unsigned
+  static_assert(static_cast<T>(-1) > 0);  // Check that T is unsigned
   int number_of_digits = 0;
   T t = value;
   do {
@@ -2951,7 +2951,7 @@ static int utoa_impl(T value, const base::Vector<char>& buffer,
 template <typename T>
 static int utoa(T value, const base::Vector<char>& buffer, int buffer_pos) {
   typename ToUnsigned<sizeof(value)>::Type unsigned_value = value;
-  STATIC_ASSERT(sizeof(value) == sizeof(unsigned_value));
+  static_assert(sizeof(value) == sizeof(unsigned_value));
   return utoa_impl(unsigned_value, buffer, buffer_pos);
 }
 
diff --git a/src/regexp/arm/regexp-macro-assembler-arm.cc b/src/regexp/arm/regexp-macro-assembler-arm.cc
index b4c94711310..1988a1759db 100644
--- a/src/regexp/arm/regexp-macro-assembler-arm.cc
+++ b/src/regexp/arm/regexp-macro-assembler-arm.cc
@@ -709,21 +709,21 @@ Handle<HeapObject> RegExpMacroAssemblerARM::GetCode(Handle<String> source) {
   // from generated code.
   __ add(frame_pointer(), sp, Operand(4 * kPointerSize));
 
-  STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+  static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
   __ mov(r0, Operand::Zero());
   __ push(r0);  // Make room for success counter and initialize it to 0.
-  STATIC_ASSERT(kStringStartMinusOne ==
+  static_assert(kStringStartMinusOne ==
                 kSuccessfulCaptures - kSystemPointerSize);
   __ push(r0);  // Make room for "string start - 1" constant.
-  STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+  static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
   __ push(r0);  // The backtrack counter.
-  STATIC_ASSERT(kRegExpStackBasePointer ==
+  static_assert(kRegExpStackBasePointer ==
                 kBacktrackCount - kSystemPointerSize);
   __ push(r0);  // The regexp stack base ptr.
 
   // Initialize backtrack stack pointer. It must not be clobbered from here on.
   // Note the backtrack_stackpointer is callee-saved.
-  STATIC_ASSERT(backtrack_stackpointer() == r8);
+  static_assert(backtrack_stackpointer() == r8);
   LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
   // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/arm64/regexp-macro-assembler-arm64.cc b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
index e1330b02554..3a3ff8ef9c4 100644
--- a/src/regexp/arm64/regexp-macro-assembler-arm64.cc
+++ b/src/regexp/arm64/regexp-macro-assembler-arm64.cc
@@ -122,8 +122,8 @@ RegExpMacroAssemblerARM64::RegExpMacroAssemblerARM64(Isolate* isolate,
       exit_label_() {
   DCHECK_EQ(0, registers_to_save % 2);
   // We can cache at most 16 W registers in x0-x7.
-  STATIC_ASSERT(kNumCachedRegisters <= 16);
-  STATIC_ASSERT((kNumCachedRegisters % 2) == 0);
+  static_assert(kNumCachedRegisters <= 16);
+  static_assert((kNumCachedRegisters % 2) == 0);
   __ CallTarget();
 
   __ B(&entry_label_);   // We'll write the entry code later.
@@ -839,7 +839,7 @@ Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) {
 
   // Initialize backtrack stack pointer. It must not be clobbered from here on.
   // Note the backtrack_stackpointer is callee-saved.
-  STATIC_ASSERT(backtrack_stackpointer() == x23);
+  static_assert(backtrack_stackpointer() == x23);
   LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
   // Store the regexp base pointer - we'll later restore it / write it to
@@ -1008,7 +1008,7 @@ Handle<HeapObject> RegExpMacroAssemblerARM64::GetCode(Handle<String> source) {
 
         // We can unroll the loop here, we should not unroll for less than 2
         // registers.
-        STATIC_ASSERT(kNumRegistersToUnroll > 2);
+        static_assert(kNumRegistersToUnroll > 2);
         if (num_registers_left_on_stack <= kNumRegistersToUnroll) {
           for (int i = 0; i < num_registers_left_on_stack / 2; i++) {
             __ Ldp(capture_end, capture_start,
@@ -1389,7 +1389,7 @@ void RegExpMacroAssemblerARM64::ClearRegisters(int reg_from, int reg_to) {
     reg_from -= kNumCachedRegisters;
     reg_to -= kNumCachedRegisters;
     // We should not unroll the loop for less than 2 registers.
-    STATIC_ASSERT(kNumRegistersToUnroll > 2);
+    static_assert(kNumRegistersToUnroll > 2);
     // We position the base pointer to (reg_from + 1).
     int base_offset = kFirstRegisterOnStack -
         kWRegSize - (kWRegSize * reg_from);
diff --git a/src/regexp/experimental/experimental-bytecode.h b/src/regexp/experimental/experimental-bytecode.h
index e8b33ad76d2..c3fe70ffd93 100644
--- a/src/regexp/experimental/experimental-bytecode.h
+++ b/src/regexp/experimental/experimental-bytecode.h
@@ -176,9 +176,9 @@ struct RegExpInstruction {
     // Payload of ASSERTION:
     RegExpAssertion::Type assertion_type;
   } payload;
-  STATIC_ASSERT(sizeof(payload) == 4);
+  static_assert(sizeof(payload) == 4);
 };
-STATIC_ASSERT(sizeof(RegExpInstruction) == 8);
+static_assert(sizeof(RegExpInstruction) == 8);
 // TODO(mbid,v8:10765): This is rather wasteful.  We can fit the opcode in 2-3
 // bits, so the remaining 29/30 bits can be used as payload.  Problem: The
 // payload of CONSUME_RANGE consists of two 16-bit values `min` and `max`, so
diff --git a/src/regexp/experimental/experimental-compiler.cc b/src/regexp/experimental/experimental-compiler.cc
index ce7862d0fd2..86bcf183f18 100644
--- a/src/regexp/experimental/experimental-compiler.cc
+++ b/src/regexp/experimental/experimental-compiler.cc
@@ -40,7 +40,7 @@ class CanBeHandledVisitor final : private RegExpVisitor {
         RegExpFlag::kGlobal | RegExpFlag::kSticky | RegExpFlag::kMultiline |
         RegExpFlag::kDotAll | RegExpFlag::kLinear;
     // We support Unicode iff kUnicode is among the supported flags.
-    STATIC_ASSERT(ExperimentalRegExp::kSupportsUnicode ==
+    static_assert(ExperimentalRegExp::kSupportsUnicode ==
                   IsUnicode(kAllowedFlags));
     return (flags & ~kAllowedFlags) == 0;
   }
@@ -402,7 +402,7 @@ class CompileVisitor : private RegExpVisitor {
     CompileDisjunction(ranges->length(), [&](int i) {
       // We don't support utf16 for now, so only ranges that can be specified
       // by (complements of) ranges with base::uc16 bounds.
-      STATIC_ASSERT(kMaxSupportedCodepoint <=
+      static_assert(kMaxSupportedCodepoint <=
                     std::numeric_limits<base::uc16>::max());
 
       base::uc32 from = (*ranges)[i].from();
diff --git a/src/regexp/experimental/experimental-interpreter.cc b/src/regexp/experimental/experimental-interpreter.cc
index 078fa25f209..095cbd3a103 100644
--- a/src/regexp/experimental/experimental-interpreter.cc
+++ b/src/regexp/experimental/experimental-interpreter.cc
@@ -200,7 +200,7 @@ class NfaInterpreter {
         // TODO(mbid,v8:10765): If we're in unicode mode, we have to advance to
         // the next codepoint, not to the next code unit. See also
         // `RegExpUtils::AdvanceStringIndex`.
-        STATIC_ASSERT(!ExperimentalRegExp::kSupportsUnicode);
+        static_assert(!ExperimentalRegExp::kSupportsUnicode);
       }
     }
 
diff --git a/src/regexp/experimental/experimental.cc b/src/regexp/experimental/experimental.cc
index 1e745eaa31b..80f88f3822d 100644
--- a/src/regexp/experimental/experimental.cc
+++ b/src/regexp/experimental/experimental.cc
@@ -47,7 +47,7 @@ bool ExperimentalRegExp::IsCompiled(Handle<JSRegExp> re, Isolate* isolate) {
 
 template <class T>
 Handle<ByteArray> VectorToByteArray(Isolate* isolate, base::Vector<T> data) {
-  STATIC_ASSERT(std::is_trivial<T>::value);
+  static_assert(std::is_trivial<T>::value);
 
   int byte_length = sizeof(T) * data.length();
   Handle<ByteArray> byte_array = isolate->factory()->NewByteArray(byte_length);
diff --git a/src/regexp/ia32/regexp-macro-assembler-ia32.cc b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
index bcde61b8229..093931fa9dc 100644
--- a/src/regexp/ia32/regexp-macro-assembler-ia32.cc
+++ b/src/regexp/ia32/regexp-macro-assembler-ia32.cc
@@ -206,8 +206,8 @@ void RegExpMacroAssemblerIA32::CheckGreedyLoop(Label* on_equal) {
 }
 
 void RegExpMacroAssemblerIA32::PushCallerSavedRegisters() {
-  STATIC_ASSERT(backtrack_stackpointer() == ecx);
-  STATIC_ASSERT(current_character() == edx);
+  static_assert(backtrack_stackpointer() == ecx);
+  static_assert(current_character() == edx);
   __ push(ecx);
   __ push(edx);
 }
@@ -689,7 +689,7 @@ bool RegExpMacroAssemblerIA32::CheckSpecialCharacterClass(
 }
 
 void RegExpMacroAssemblerIA32::Fail() {
-  STATIC_ASSERT(FAILURE == 0);  // Return value for failure is zero.
+  static_assert(FAILURE == 0);  // Return value for failure is zero.
   if (!global()) {
     __ Move(eax, Immediate(FAILURE));
   }
@@ -749,23 +749,23 @@ Handle<HeapObject> RegExpMacroAssemblerIA32::GetCode(Handle<String> source) {
   __ push(esi);
   __ push(edi);
   __ push(ebx);  // Callee-save on MacOS.
-  STATIC_ASSERT(kLastCalleeSaveRegister == kBackup_ebx);
+  static_assert(kLastCalleeSaveRegister == kBackup_ebx);
 
-  STATIC_ASSERT(kSuccessfulCaptures ==
+  static_assert(kSuccessfulCaptures ==
                 kLastCalleeSaveRegister - kSystemPointerSize);
   __ push(Immediate(0));  // Number of successful matches in a global regexp.
-  STATIC_ASSERT(kStringStartMinusOne ==
+  static_assert(kStringStartMinusOne ==
                 kSuccessfulCaptures - kSystemPointerSize);
   __ push(Immediate(0));  // Make room for "string start - 1" constant.
-  STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+  static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
   __ push(Immediate(0));  // The backtrack counter.
-  STATIC_ASSERT(kRegExpStackBasePointer ==
+  static_assert(kRegExpStackBasePointer ==
                 kBacktrackCount - kSystemPointerSize);
   __ push(Immediate(0));  // The regexp stack base ptr.
 
   // Initialize backtrack stack pointer. It must not be clobbered from here on.
   // Note the backtrack_stackpointer is *not* callee-saved.
-  STATIC_ASSERT(backtrack_stackpointer() == ecx);
+  static_assert(backtrack_stackpointer() == ecx);
   LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
   // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/loong64/regexp-macro-assembler-loong64.cc b/src/regexp/loong64/regexp-macro-assembler-loong64.cc
index f347f8c1895..a55ea6557a3 100644
--- a/src/regexp/loong64/regexp-macro-assembler-loong64.cc
+++ b/src/regexp/loong64/regexp-macro-assembler-loong64.cc
@@ -677,21 +677,21 @@ Handle<HeapObject> RegExpMacroAssemblerLOONG64::GetCode(Handle<String> source) {
     // from generated code.
     // TODO(plind): this 8 is the # of argument regs, should have definition.
     __ Add_d(frame_pointer(), sp, Operand(8 * kPointerSize));
-    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ mov(a0, zero_reg);
     __ Push(a0);  // Make room for success counter and initialize it to 0.
-    STATIC_ASSERT(kStringStartMinusOne ==
+    static_assert(kStringStartMinusOne ==
                   kSuccessfulCaptures - kSystemPointerSize);
     __ Push(a0);  // Make room for "string start - 1" constant.
-    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
     __ Push(a0);  // The backtrack counter
-    STATIC_ASSERT(kRegExpStackBasePointer ==
+    static_assert(kRegExpStackBasePointer ==
                   kBacktrackCount - kSystemPointerSize);
     __ Push(a0);  // The regexp stack base ptr.
 
     // Initialize backtrack stack pointer. It must not be clobbered from here
     // on. Note the backtrack_stackpointer is callee-saved.
-    STATIC_ASSERT(backtrack_stackpointer() == s7);
+    static_assert(backtrack_stackpointer() == s7);
     LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
     // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/mips/regexp-macro-assembler-mips.cc b/src/regexp/mips/regexp-macro-assembler-mips.cc
index 8904201d023..d884d71e4c2 100644
--- a/src/regexp/mips/regexp-macro-assembler-mips.cc
+++ b/src/regexp/mips/regexp-macro-assembler-mips.cc
@@ -688,21 +688,21 @@ Handle<HeapObject> RegExpMacroAssemblerMIPS::GetCode(Handle<String> source) {
     // from generated code.
     __ Addu(frame_pointer(), sp, Operand(4 * kPointerSize));
 
-    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ mov(a0, zero_reg);
     __ push(a0);  // Make room for success counter and initialize it to 0.
-    STATIC_ASSERT(kStringStartMinusOne ==
+    static_assert(kStringStartMinusOne ==
                   kSuccessfulCaptures - kSystemPointerSize);
     __ push(a0);  // Make room for "string start - 1" constant.
-    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
     __ push(a0);
-    STATIC_ASSERT(kRegExpStackBasePointer ==
+    static_assert(kRegExpStackBasePointer ==
                   kBacktrackCount - kSystemPointerSize);
     __ push(a0);  // The regexp stack base ptr.
 
     // Initialize backtrack stack pointer. It must not be clobbered from here
     // on. Note the backtrack_stackpointer is callee-saved.
-    STATIC_ASSERT(backtrack_stackpointer() == s7);
+    static_assert(backtrack_stackpointer() == s7);
     LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
     // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/mips64/regexp-macro-assembler-mips64.cc b/src/regexp/mips64/regexp-macro-assembler-mips64.cc
index b5c54848f4f..598083dcbb2 100644
--- a/src/regexp/mips64/regexp-macro-assembler-mips64.cc
+++ b/src/regexp/mips64/regexp-macro-assembler-mips64.cc
@@ -725,21 +725,21 @@ Handle<HeapObject> RegExpMacroAssemblerMIPS::GetCode(Handle<String> source) {
     // from generated code.
     // TODO(plind): this 8 is the # of argument regs, should have definition.
     __ Daddu(frame_pointer(), sp, Operand(8 * kPointerSize));
-    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ mov(a0, zero_reg);
     __ push(a0);  // Make room for success counter and initialize it to 0.
-    STATIC_ASSERT(kStringStartMinusOne ==
+    static_assert(kStringStartMinusOne ==
                   kSuccessfulCaptures - kSystemPointerSize);
     __ push(a0);  // Make room for "string start - 1" constant.
-    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
     __ push(a0);  // The backtrack counter
-    STATIC_ASSERT(kRegExpStackBasePointer ==
+    static_assert(kRegExpStackBasePointer ==
                   kBacktrackCount - kSystemPointerSize);
     __ push(a0);  // The regexp stack base ptr.
 
     // Initialize backtrack stack pointer. It must not be clobbered from here
     // on. Note the backtrack_stackpointer is callee-saved.
-    STATIC_ASSERT(backtrack_stackpointer() == s7);
+    static_assert(backtrack_stackpointer() == s7);
     LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
     // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/ppc/regexp-macro-assembler-ppc.cc b/src/regexp/ppc/regexp-macro-assembler-ppc.cc
index da81f893202..b9f441a002b 100644
--- a/src/regexp/ppc/regexp-macro-assembler-ppc.cc
+++ b/src/regexp/ppc/regexp-macro-assembler-ppc.cc
@@ -760,21 +760,21 @@ Handle<HeapObject> RegExpMacroAssemblerPPC::GetCode(Handle<String> source) {
     // from generated code.
     __ addi(frame_pointer(), sp, Operand(8 * kSystemPointerSize));
 
-    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ li(r3, Operand::Zero());
     __ push(r3);  // Make room for success counter and initialize it to 0.
-    STATIC_ASSERT(kStringStartMinusOne ==
+    static_assert(kStringStartMinusOne ==
                   kSuccessfulCaptures - kSystemPointerSize);
     __ push(r3);  // Make room for "string start - 1" constant.
-    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
     __ push(r3);  // The backtrack counter.
-    STATIC_ASSERT(kRegExpStackBasePointer ==
+    static_assert(kRegExpStackBasePointer ==
                   kBacktrackCount - kSystemPointerSize);
     __ push(r3);  // The regexp stack base ptr.
 
     // Initialize backtrack stack pointer. It must not be clobbered from here
     // on. Note the backtrack_stackpointer is callee-saved.
-    STATIC_ASSERT(backtrack_stackpointer() == r29);
+    static_assert(backtrack_stackpointer() == r29);
     LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
     // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/regexp-bytecodes.h b/src/regexp/regexp-bytecodes.h
index c2ca6cbef69..8479b30bd87 100644
--- a/src/regexp/regexp-bytecodes.h
+++ b/src/regexp/regexp-bytecodes.h
@@ -25,7 +25,7 @@ constexpr int BYTECODE_MASK = kRegExpPaddedBytecodeCount - 1;
 // positive values.
 const unsigned int MAX_FIRST_ARG = 0x7fffffu;
 const int BYTECODE_SHIFT = 8;
-STATIC_ASSERT(1 << BYTECODE_SHIFT > BYTECODE_MASK);
+static_assert(1 << BYTECODE_SHIFT > BYTECODE_MASK);
 
 // The list of bytecodes, in format: V(Name, Code, ByteLength).
 // TODO(pthier): Argument offsets of bytecodes should be easily accessible by
@@ -221,7 +221,7 @@ static constexpr int kRegExpBytecodeCount = BYTECODE_ITERATOR(COUNT);
 // contiguous, strictly increasing, and start at 0.
 // TODO(jgruber): Do not explicitly assign values, instead generate them
 // implicitly from the list order.
-STATIC_ASSERT(kRegExpBytecodeCount == 59);
+static_assert(kRegExpBytecodeCount == 59);
 
 #define DECLARE_BYTECODES(name, code, length) \
   static constexpr int BC_##name = code;
diff --git a/src/regexp/regexp-compiler-tonode.cc b/src/regexp/regexp-compiler-tonode.cc
index 4cb9e8e6899..da01f246d97 100644
--- a/src/regexp/regexp-compiler-tonode.cc
+++ b/src/regexp/regexp-compiler-tonode.cc
@@ -150,16 +150,16 @@ void UnicodeRangeSplitter::AddRange(CharacterRange range) {
   static constexpr base::uc32 kBmp2End = kNonBmpStart - 1;
 
   // Ends are all inclusive.
-  STATIC_ASSERT(kBmp1Start == 0);
-  STATIC_ASSERT(kBmp1Start < kBmp1End);
-  STATIC_ASSERT(kBmp1End + 1 == kLeadSurrogateStart);
-  STATIC_ASSERT(kLeadSurrogateStart < kLeadSurrogateEnd);
-  STATIC_ASSERT(kLeadSurrogateEnd + 1 == kTrailSurrogateStart);
-  STATIC_ASSERT(kTrailSurrogateStart < kTrailSurrogateEnd);
-  STATIC_ASSERT(kTrailSurrogateEnd + 1 == kBmp2Start);
-  STATIC_ASSERT(kBmp2Start < kBmp2End);
-  STATIC_ASSERT(kBmp2End + 1 == kNonBmpStart);
-  STATIC_ASSERT(kNonBmpStart < kNonBmpEnd);
+  static_assert(kBmp1Start == 0);
+  static_assert(kBmp1Start < kBmp1End);
+  static_assert(kBmp1End + 1 == kLeadSurrogateStart);
+  static_assert(kLeadSurrogateStart < kLeadSurrogateEnd);
+  static_assert(kLeadSurrogateEnd + 1 == kTrailSurrogateStart);
+  static_assert(kTrailSurrogateStart < kTrailSurrogateEnd);
+  static_assert(kTrailSurrogateEnd + 1 == kBmp2Start);
+  static_assert(kBmp2Start < kBmp2End);
+  static_assert(kBmp2End + 1 == kNonBmpStart);
+  static_assert(kNonBmpStart < kNonBmpEnd);
 
   static constexpr base::uc32 kStarts[] = {
       kBmp1Start, kLeadSurrogateStart, kTrailSurrogateStart,
@@ -175,8 +175,8 @@ void UnicodeRangeSplitter::AddRange(CharacterRange range) {
   };
 
   static constexpr int kCount = arraysize(kStarts);
-  STATIC_ASSERT(kCount == arraysize(kEnds));
-  STATIC_ASSERT(kCount == arraysize(kTargets));
+  static_assert(kCount == arraysize(kEnds));
+  static_assert(kCount == arraysize(kTargets));
 
   for (int i = 0; i < kCount; i++) {
     if (kStarts[i] > range.to()) break;
@@ -1080,7 +1080,7 @@ class AssertionSequenceRewriter final {
 
     // Bitfield of all seen assertions.
     uint32_t seen_assertions = 0;
-    STATIC_ASSERT(static_cast<int>(RegExpAssertion::Type::LAST_ASSERTION_TYPE) <
+    static_assert(static_cast<int>(RegExpAssertion::Type::LAST_ASSERTION_TYPE) <
                   kUInt32Size * kBitsPerByte);
 
     for (int i = from; i < to; i++) {
diff --git a/src/regexp/regexp-compiler.cc b/src/regexp/regexp-compiler.cc
index df3d98644d9..ec115acc1e6 100644
--- a/src/regexp/regexp-compiler.cc
+++ b/src/regexp/regexp-compiler.cc
@@ -175,16 +175,16 @@ using namespace regexp_compiler_constants;  // NOLINT(build/namespaces)
 namespace {
 
 constexpr base::uc32 MaxCodeUnit(const bool one_byte) {
-  STATIC_ASSERT(String::kMaxOneByteCharCodeU <=
+  static_assert(String::kMaxOneByteCharCodeU <=
                 std::numeric_limits<uint16_t>::max());
-  STATIC_ASSERT(String::kMaxUtf16CodeUnitU <=
+  static_assert(String::kMaxUtf16CodeUnitU <=
                 std::numeric_limits<uint16_t>::max());
   return one_byte ? String::kMaxOneByteCharCodeU : String::kMaxUtf16CodeUnitU;
 }
 
 constexpr uint32_t CharMask(const bool one_byte) {
-  STATIC_ASSERT(base::bits::IsPowerOfTwo(String::kMaxOneByteCharCodeU + 1));
-  STATIC_ASSERT(base::bits::IsPowerOfTwo(String::kMaxUtf16CodeUnitU + 1));
+  static_assert(base::bits::IsPowerOfTwo(String::kMaxOneByteCharCodeU + 1));
+  static_assert(base::bits::IsPowerOfTwo(String::kMaxUtf16CodeUnitU + 1));
   return MaxCodeUnit(one_byte);
 }
 
@@ -740,7 +740,7 @@ namespace {
 
 #ifdef DEBUG
 bool ContainsOnlyUtf16CodeUnits(unibrow::uchar* chars, int length) {
-  STATIC_ASSERT(sizeof(unibrow::uchar) == 4);
+  static_assert(sizeof(unibrow::uchar) == 4);
   for (int i = 0; i < length; i++) {
     if (chars[i] > String::kMaxUtf16CodeUnit) return false;
   }
@@ -2723,7 +2723,7 @@ ContainedInLattice AddRange(ContainedInLattice containment, const int* ranges,
 }
 
 int BitsetFirstSetBit(BoyerMoorePositionInfo::Bitset bitset) {
-  STATIC_ASSERT(BoyerMoorePositionInfo::kMapSize ==
+  static_assert(BoyerMoorePositionInfo::kMapSize ==
                 2 * kInt64Size * kBitsPerByte);
 
   // Slight fiddling is needed here, since the bitset is of length 128 while
@@ -2734,7 +2734,7 @@ int BitsetFirstSetBit(BoyerMoorePositionInfo::Bitset bitset) {
   {
     static constexpr BoyerMoorePositionInfo::Bitset mask(~uint64_t{0});
     BoyerMoorePositionInfo::Bitset masked_bitset = bitset & mask;
-    STATIC_ASSERT(kInt64Size >= sizeof(decltype(masked_bitset.to_ullong())));
+    static_assert(kInt64Size >= sizeof(decltype(masked_bitset.to_ullong())));
     uint64_t lsb = masked_bitset.to_ullong();
     if (lsb != 0) return base::bits::CountTrailingZeros(lsb);
   }
@@ -3805,7 +3805,7 @@ void BackReferenceNode::FillInBMInfo(Isolate* isolate, int offset, int budget,
   SaveBMInfo(bm, not_at_start, offset);
 }
 
-STATIC_ASSERT(BoyerMoorePositionInfo::kMapSize ==
+static_assert(BoyerMoorePositionInfo::kMapSize ==
               RegExpMacroAssembler::kTableSize);
 
 void ChoiceNode::FillInBMInfo(Isolate* isolate, int offset, int budget,
diff --git a/src/regexp/regexp-interpreter.cc b/src/regexp/regexp-interpreter.cc
index bf7769b86e5..d3f3ab7f427 100644
--- a/src/regexp/regexp-interpreter.cc
+++ b/src/regexp/regexp-interpreter.cc
@@ -180,7 +180,7 @@ class InterpreterRegisters {
         output_register_count_(output_register_count) {
     // TODO(jgruber): Use int32_t consistently for registers. Currently, CSA
     // uses int32_t while runtime uses int.
-    STATIC_ASSERT(sizeof(int) == sizeof(int32_t));
+    static_assert(sizeof(int) == sizeof(int32_t));
     DCHECK_GE(output_register_count, 2);  // At least 2 for the match itself.
     DCHECK_GE(total_register_count, output_register_count);
     DCHECK_LE(total_register_count, RegExpMacroAssembler::kMaxRegisterCount);
@@ -424,8 +424,8 @@ IrregexpInterpreter::Result RawMatch(
             base::bits::RoundUpToPowerOfTwo32(kRegExpBytecodeCount));
 
   // Make sure every bytecode we get by using BYTECODE_MASK is well defined.
-  STATIC_ASSERT(kRegExpBytecodeCount <= kRegExpPaddedBytecodeCount);
-  STATIC_ASSERT(kRegExpBytecodeCount + kRegExpBytecodeFillerCount ==
+  static_assert(kRegExpBytecodeCount <= kRegExpPaddedBytecodeCount);
+  static_assert(kRegExpBytecodeCount + kRegExpBytecodeFillerCount ==
                 kRegExpPaddedBytecodeCount);
 
 #define DECLARE_DISPATCH_TABLE_ENTRY(name, ...) &&BC_##name,
@@ -522,7 +522,7 @@ IrregexpInterpreter::Result RawMatch(
       DISPATCH();
     }
     BYTECODE(POP_BT) {
-      STATIC_ASSERT(JSRegExp::kNoBacktrackLimit == 0);
+      static_assert(JSRegExp::kNoBacktrackLimit == 0);
       if (++backtrack_count == backtrack_limit) {
         int return_code = LoadPacked24Signed(insn);
         return static_cast<IrregexpInterpreter::Result>(return_code);
diff --git a/src/regexp/regexp-stack.h b/src/regexp/regexp-stack.h
index d52ca3e1d07..5711f7736db 100644
--- a/src/regexp/regexp-stack.h
+++ b/src/regexp/regexp-stack.h
@@ -97,7 +97,7 @@ class RegExpStack final {
       2 * kStackLimitSlack * kSystemPointerSize;
   byte static_stack_[kStaticStackSize] = {0};
 
-  STATIC_ASSERT(kStaticStackSize <= kMaximumStackSize);
+  static_assert(kStaticStackSize <= kMaximumStackSize);
 
   // Structure holding the allocated memory, size and limit. Thread switching
   // archives and restores this struct.
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index df50034b166..8186d756968 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -402,7 +402,7 @@ Handle<Object> RegExpImpl::AtomExec(Isolate* isolate, Handle<JSRegExp> re,
                                     Handle<String> subject, int index,
                                     Handle<RegExpMatchInfo> last_match_info) {
   static const int kNumRegisters = 2;
-  STATIC_ASSERT(kNumRegisters <= Isolate::kJSRegexpStaticOffsetsVectorSize);
+  static_assert(kNumRegisters <= Isolate::kJSRegexpStaticOffsetsVectorSize);
   int32_t* output_registers = isolate->jsregexp_static_offsets_vector();
 
   int res =
@@ -677,11 +677,11 @@ int RegExpImpl::IrregexpExecRaw(Isolate* isolate, Handle<JSRegExp> regexp,
       if (res != NativeRegExpMacroAssembler::RETRY) {
         DCHECK(res != NativeRegExpMacroAssembler::EXCEPTION ||
                isolate->has_pending_exception());
-        STATIC_ASSERT(static_cast<int>(NativeRegExpMacroAssembler::SUCCESS) ==
+        static_assert(static_cast<int>(NativeRegExpMacroAssembler::SUCCESS) ==
                       RegExp::RE_SUCCESS);
-        STATIC_ASSERT(static_cast<int>(NativeRegExpMacroAssembler::FAILURE) ==
+        static_assert(static_cast<int>(NativeRegExpMacroAssembler::FAILURE) ==
                       RegExp::RE_FAILURE);
-        STATIC_ASSERT(static_cast<int>(NativeRegExpMacroAssembler::EXCEPTION) ==
+        static_assert(static_cast<int>(NativeRegExpMacroAssembler::EXCEPTION) ==
                       RegExp::RE_EXCEPTION);
         return res;
       }
diff --git a/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc b/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc
index 2ee1a51c0ef..2c34ac19008 100644
--- a/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc
+++ b/src/regexp/riscv64/regexp-macro-assembler-riscv64.cc
@@ -696,21 +696,21 @@ Handle<HeapObject> RegExpMacroAssemblerRISCV::GetCode(Handle<String> source) {
     __ Add64(frame_pointer(), sp,
              Operand(argument_registers.Count() * kSystemPointerSize));
 
-    STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+    static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
     __ mv(a0, zero_reg);
     __ push(a0);  // Make room for success counter and initialize it to 0.
-    STATIC_ASSERT(kStringStartMinusOne ==
+    static_assert(kStringStartMinusOne ==
                   kSuccessfulCaptures - kSystemPointerSize);
     __ push(a0);  // Make room for "string start - 1" constant.
-    STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+    static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
     __ push(a0);  // The backtrack counter
-    STATIC_ASSERT(kRegExpStackBasePointer ==
+    static_assert(kRegExpStackBasePointer ==
                   kBacktrackCount - kSystemPointerSize);
     __ push(a0);  // The regexp stack base ptr.
 
     // Initialize backtrack stack pointer. It must not be clobbered from here
     // on. Note the backtrack_stackpointer is callee-saved.
-    STATIC_ASSERT(backtrack_stackpointer() == s7);
+    static_assert(backtrack_stackpointer() == s7);
     LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
     // Store the regexp base pointer - we'll later restore it / write it to
     // memory when returning from this irregexp code object.
diff --git a/src/regexp/s390/regexp-macro-assembler-s390.cc b/src/regexp/s390/regexp-macro-assembler-s390.cc
index f4fa2a9ae6d..6afd545f798 100644
--- a/src/regexp/s390/regexp-macro-assembler-s390.cc
+++ b/src/regexp/s390/regexp-macro-assembler-s390.cc
@@ -746,21 +746,21 @@ Handle<HeapObject> RegExpMacroAssemblerS390::GetCode(Handle<String> source) {
   // from generated code.
   __ mov(frame_pointer(), sp);
   __ lay(sp, MemOperand(sp, -10 * kSystemPointerSize));
-  STATIC_ASSERT(kSuccessfulCaptures == kInputString - kSystemPointerSize);
+  static_assert(kSuccessfulCaptures == kInputString - kSystemPointerSize);
   __ mov(r1, Operand::Zero());  // success counter
-  STATIC_ASSERT(kStringStartMinusOne ==
+  static_assert(kStringStartMinusOne ==
                 kSuccessfulCaptures - kSystemPointerSize);
   __ mov(r0, r1);  // offset of location
   __ StoreMultipleP(r0, r9, MemOperand(sp, 0));
-  STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+  static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
   __ Push(r1);  // The backtrack counter.
-  STATIC_ASSERT(kRegExpStackBasePointer ==
+  static_assert(kRegExpStackBasePointer ==
                 kBacktrackCount - kSystemPointerSize);
   __ push(r1);  // The regexp stack base ptr.
 
   // Initialize backtrack stack pointer. It must not be clobbered from here on.
   // Note the backtrack_stackpointer is callee-saved.
-  STATIC_ASSERT(backtrack_stackpointer() == r13);
+  static_assert(backtrack_stackpointer() == r13);
   LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
   // Store the regexp base pointer - we'll later restore it / write it to
diff --git a/src/regexp/x64/regexp-macro-assembler-x64.cc b/src/regexp/x64/regexp-macro-assembler-x64.cc
index d702e210f78..1b4aa566f8f 100644
--- a/src/regexp/x64/regexp-macro-assembler-x64.cc
+++ b/src/regexp/x64/regexp-macro-assembler-x64.cc
@@ -699,7 +699,7 @@ bool RegExpMacroAssemblerX64::CheckSpecialCharacterClass(
 }
 
 void RegExpMacroAssemblerX64::Fail() {
-  STATIC_ASSERT(FAILURE == 0);  // Return value for failure is zero.
+  static_assert(FAILURE == 0);  // Return value for failure is zero.
   if (!global()) {
     __ Move(rax, FAILURE);
   }
@@ -763,7 +763,7 @@ Handle<HeapObject> RegExpMacroAssemblerX64::GetCode(Handle<String> source) {
   __ movq(Operand(rbp, kInputStart), arg_reg_3);
   __ movq(Operand(rbp, kInputEnd), arg_reg_4);
 
-  STATIC_ASSERT(kNumCalleeSaveRegisters == 3);
+  static_assert(kNumCalleeSaveRegisters == 3);
   __ pushq(rsi);
   __ pushq(rdi);
   __ pushq(rbx);
@@ -783,25 +783,25 @@ Handle<HeapObject> RegExpMacroAssemblerX64::GetCode(Handle<String> source) {
   __ pushq(r8);
   __ pushq(r9);
 
-  STATIC_ASSERT(kNumCalleeSaveRegisters == 1);
+  static_assert(kNumCalleeSaveRegisters == 1);
   __ pushq(rbx);
 #endif
 
-  STATIC_ASSERT(kSuccessfulCaptures ==
+  static_assert(kSuccessfulCaptures ==
                 kLastCalleeSaveRegister - kSystemPointerSize);
   __ Push(Immediate(0));  // Number of successful matches in a global regexp.
-  STATIC_ASSERT(kStringStartMinusOne ==
+  static_assert(kStringStartMinusOne ==
                 kSuccessfulCaptures - kSystemPointerSize);
   __ Push(Immediate(0));  // Make room for "string start - 1" constant.
-  STATIC_ASSERT(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
+  static_assert(kBacktrackCount == kStringStartMinusOne - kSystemPointerSize);
   __ Push(Immediate(0));  // The backtrack counter.
-  STATIC_ASSERT(kRegExpStackBasePointer ==
+  static_assert(kRegExpStackBasePointer ==
                 kBacktrackCount - kSystemPointerSize);
   __ Push(Immediate(0));  // The regexp stack base ptr.
 
   // Initialize backtrack stack pointer. It must not be clobbered from here on.
   // Note the backtrack_stackpointer is *not* callee-saved.
-  STATIC_ASSERT(backtrack_stackpointer() == rcx);
+  static_assert(backtrack_stackpointer() == rcx);
   LoadRegExpStackPointerFromMemory(backtrack_stackpointer());
 
   // Store the regexp base pointer - we'll later restore it / write it to
@@ -998,14 +998,14 @@ Handle<HeapObject> RegExpMacroAssemblerX64::GetCode(Handle<String> source) {
 #ifdef V8_TARGET_OS_WIN
   // Restore callee save registers.
   __ leaq(rsp, Operand(rbp, kLastCalleeSaveRegister));
-  STATIC_ASSERT(kNumCalleeSaveRegisters == 3);
+  static_assert(kNumCalleeSaveRegisters == 3);
   __ popq(rbx);
   __ popq(rdi);
   __ popq(rsi);
   // Stack now at rbp.
 #else
   // Restore callee save register.
-  STATIC_ASSERT(kNumCalleeSaveRegisters == 1);
+  static_assert(kNumCalleeSaveRegisters == 1);
   __ movq(rbx, Operand(rbp, kBackup_rbx));
   // Skip rsp to rbp.
   __ movq(rsp, rbp);
diff --git a/src/roots/roots.h b/src/roots/roots.h
index 213c567972d..3e9d36f58cd 100644
--- a/src/roots/roots.h
+++ b/src/roots/roots.h
@@ -477,7 +477,7 @@ class RootsTable {
   // initialization.
   // Generated code can treat direct references to these roots as constants.
   static constexpr bool IsImmortalImmovable(RootIndex root_index) {
-    STATIC_ASSERT(static_cast<int>(RootIndex::kFirstImmortalImmovableRoot) ==
+    static_assert(static_cast<int>(RootIndex::kFirstImmortalImmovableRoot) ==
                   0);
     return static_cast<unsigned>(root_index) <=
            static_cast<unsigned>(RootIndex::kLastImmortalImmovableRoot);
@@ -494,7 +494,7 @@ class RootsTable {
 
   // Used for iterating over all of the read-only and mutable strong roots.
   FullObjectSlot strong_or_read_only_roots_begin() const {
-    STATIC_ASSERT(static_cast<size_t>(RootIndex::kLastReadOnlyRoot) ==
+    static_assert(static_cast<size_t>(RootIndex::kLastReadOnlyRoot) ==
                   static_cast<size_t>(RootIndex::kFirstStrongRoot) - 1);
     return FullObjectSlot(
         &roots_[static_cast<size_t>(RootIndex::kFirstStrongOrReadOnlyRoot)]);
diff --git a/src/runtime/runtime-classes.cc b/src/runtime/runtime-classes.cc
index 02597852c8f..c58bf5d3918 100644
--- a/src/runtime/runtime-classes.cc
+++ b/src/runtime/runtime-classes.cc
@@ -124,7 +124,7 @@ namespace {
 
 template <typename Dictionary>
 Handle<Name> KeyToName(Isolate* isolate, Handle<Object> key) {
-  STATIC_ASSERT((std::is_same<Dictionary, SwissNameDictionary>::value ||
+  static_assert((std::is_same<Dictionary, SwissNameDictionary>::value ||
                  std::is_same<Dictionary, NameDictionary>::value));
   DCHECK(key->IsName());
   return Handle<Name>::cast(key);
diff --git a/src/runtime/runtime-literals.cc b/src/runtime/runtime-literals.cc
index 4ac519c3972..c3c015413b0 100644
--- a/src/runtime/runtime-literals.cc
+++ b/src/runtime/runtime-literals.cc
@@ -567,7 +567,7 @@ MaybeHandle<JSObject> CreateLiteral(Isolate* isolate,
     vector->SynchronizedSet(literals_slot, *site);
   }
 
-  STATIC_ASSERT(static_cast<int>(ObjectLiteral::kDisableMementos) ==
+  static_assert(static_cast<int>(ObjectLiteral::kDisableMementos) ==
                 static_cast<int>(ArrayLiteral::kDisableMementos));
   bool enable_mementos = (flags & ObjectLiteral::kDisableMementos) == 0;
 
diff --git a/src/runtime/runtime-regexp.cc b/src/runtime/runtime-regexp.cc
index 01beb37ba83..d7ca8559cb5 100644
--- a/src/runtime/runtime-regexp.cc
+++ b/src/runtime/runtime-regexp.cc
@@ -43,7 +43,7 @@ uint32_t GetArgcForReplaceCallable(uint32_t num_captures,
   uint32_t argc = has_named_captures
                       ? num_captures + kAdditionalArgsWithNamedCaptures
                       : num_captures + kAdditionalArgsWithoutNamedCaptures;
-  STATIC_ASSERT(Code::kMaxArguments < std::numeric_limits<uint32_t>::max() -
+  static_assert(Code::kMaxArguments < std::numeric_limits<uint32_t>::max() -
                                           kAdditionalArgsWithNamedCaptures);
   return (argc > Code::kMaxArguments) ? -1 : argc;
 }
@@ -567,7 +567,7 @@ V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalAtomRegExpWithString(
                           static_cast<int64_t>(subject_len);
   int result_len;
   if (result_len_64 > static_cast<int64_t>(String::kMaxLength)) {
-    STATIC_ASSERT(String::kMaxLength < kMaxInt);
+    static_assert(String::kMaxLength < kMaxInt);
     result_len = kMaxInt;  // Provoke exception.
   } else {
     result_len = static_cast<int>(result_len_64);
diff --git a/src/runtime/runtime-wasm.cc b/src/runtime/runtime-wasm.cc
index f9d91cdf516..4baf4b5b5f6 100644
--- a/src/runtime/runtime-wasm.cc
+++ b/src/runtime/runtime-wasm.cc
@@ -110,7 +110,7 @@ RUNTIME_FUNCTION(Runtime_WasmIsValidRefValue) {
   Handle<Object> raw_instance = args.at(0);
   Handle<Object> value = args.at(1);
   // Make sure ValueType fits properly in a Smi.
-  STATIC_ASSERT(wasm::ValueType::kLastUsedBit + 1 <= kSmiValueSize);
+  static_assert(wasm::ValueType::kLastUsedBit + 1 <= kSmiValueSize);
   int raw_type = args.smi_value_at(2);
 
   const wasm::WasmModule* module =
diff --git a/src/sandbox/external-pointer-inl.h b/src/sandbox/external-pointer-inl.h
index 61a6cb1a4bf..8e1d2ad8e73 100644
--- a/src/sandbox/external-pointer-inl.h
+++ b/src/sandbox/external-pointer-inl.h
@@ -17,11 +17,11 @@ V8_INLINE Address DecodeExternalPointer(const Isolate* isolate,
                                         ExternalPointer_t encoded_pointer,
                                         ExternalPointerTag tag) {
 #ifdef V8_SANDBOXED_EXTERNAL_POINTERS
-  STATIC_ASSERT(kExternalPointerSize == kInt32Size);
+  static_assert(kExternalPointerSize == kInt32Size);
   uint32_t index = encoded_pointer >> kExternalPointerIndexShift;
   return isolate->external_pointer_table().Get(index, tag);
 #else
-  STATIC_ASSERT(kExternalPointerSize == kSystemPointerSize);
+  static_assert(kExternalPointerSize == kSystemPointerSize);
   return encoded_pointer;
 #endif
 }
diff --git a/src/sandbox/external-pointer-table-inl.h b/src/sandbox/external-pointer-table-inl.h
index cda26fa07ea..e1addc57fc3 100644
--- a/src/sandbox/external-pointer-table-inl.h
+++ b/src/sandbox/external-pointer-table-inl.h
@@ -42,7 +42,7 @@ void ExternalPointerTable::Init(Isolate* isolate) {
 
   // Set up the special null entry. This entry must contain nullptr so that
   // empty EmbedderDataSlots represent nullptr.
-  STATIC_ASSERT(kNullExternalPointer == 0);
+  static_assert(kNullExternalPointer == 0);
   store(kNullExternalPointer, kNullAddress);
 }
 
@@ -125,7 +125,7 @@ uint32_t ExternalPointerTable::Allocate() {
 
 void ExternalPointerTable::Mark(uint32_t index) {
   DCHECK_LT(index, capacity_);
-  STATIC_ASSERT(sizeof(base::Atomic64) == sizeof(Address));
+  static_assert(sizeof(base::Atomic64) == sizeof(Address));
 
   base::Atomic64 old_val = load_atomic(index);
   DCHECK(!is_free(old_val));
diff --git a/src/sandbox/external-pointer-table.cc b/src/sandbox/external-pointer-table.cc
index 5f6c3e2df6f..097410ce316 100644
--- a/src/sandbox/external-pointer-table.cc
+++ b/src/sandbox/external-pointer-table.cc
@@ -15,7 +15,7 @@
 namespace v8 {
 namespace internal {
 
-STATIC_ASSERT(sizeof(ExternalPointerTable) == ExternalPointerTable::kSize);
+static_assert(sizeof(ExternalPointerTable) == ExternalPointerTable::kSize);
 
 // static
 uint32_t ExternalPointerTable::AllocateEntry(ExternalPointerTable* table) {
diff --git a/src/snapshot/deserializer.cc b/src/snapshot/deserializer.cc
index 106c7485d89..679171c1be9 100644
--- a/src/snapshot/deserializer.cc
+++ b/src/snapshot/deserializer.cc
@@ -199,7 +199,7 @@ template <typename TSlot>
 int Deserializer<IsolateT>::WriteAddress(TSlot dest, Address value) {
   DCHECK(!next_reference_is_weak_);
   memcpy(dest.ToVoidPtr(), &value, kSystemPointerSize);
-  STATIC_ASSERT(IsAligned(kSystemPointerSize, TSlot::kSlotDataSize));
+  static_assert(IsAligned(kSystemPointerSize, TSlot::kSlotDataSize));
   return (kSystemPointerSize / TSlot::kSlotDataSize);
 }
 
@@ -252,7 +252,7 @@ Deserializer<IsolateT>::Deserializer(IsolateT* isolate,
   // We start the indices here at 1, so that we can distinguish between an
   // actual index and an empty backing store (serialized as
   // kEmptyBackingStoreRefSentinel) in a deserialized object requiring fix-up.
-  STATIC_ASSERT(kEmptyBackingStoreRefSentinel == 0);
+  static_assert(kEmptyBackingStoreRefSentinel == 0);
   backing_stores_.push_back({});
 
 #ifdef DEBUG
@@ -817,7 +817,7 @@ void DeserializerRelocInfoVisitor::VisitInternalReference(Code host,
   // TODO(jgruber,v8:11036): We are being permissive for this DCHECK, but
   // consider using raw_instruction_size() instead of raw_body_size() in the
   // future.
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
   DCHECK_LT(static_cast<unsigned>(target_offset),
             static_cast<unsigned>(host.raw_body_size()));
   Address target = host.entry() + target_offset;
@@ -868,7 +868,7 @@ namespace {
 // given number of cases matches the number of expected cases for that bytecode.
 template <int byte_code_count, int expected>
 constexpr byte VerifyBytecodeCount(byte bytecode) {
-  STATIC_ASSERT(byte_code_count == expected);
+  static_assert(byte_code_count == expected);
   return bytecode;
 }
 
@@ -1221,9 +1221,9 @@ int Deserializer<IsolateT>::ReadSingleBytecodeData(byte data,
     case CASE_RANGE(kRootArrayConstants, 32): {
       // First kRootArrayConstantsCount roots are guaranteed to be in
       // the old space.
-      STATIC_ASSERT(static_cast<int>(RootIndex::kFirstImmortalImmovableRoot) ==
+      static_assert(static_cast<int>(RootIndex::kFirstImmortalImmovableRoot) ==
                     0);
-      STATIC_ASSERT(kRootArrayConstantsCount <=
+      static_assert(kRootArrayConstantsCount <=
                     static_cast<int>(RootIndex::kLastImmortalImmovableRoot));
 
       RootIndex root_index = RootArrayConstant::Decode(data);
@@ -1241,7 +1241,7 @@ int Deserializer<IsolateT>::ReadSingleBytecodeData(byte data,
     case CASE_RANGE(kFixedRawData, 32): {
       // Deserialize raw data of fixed length from 1 to 32 times kTaggedSize.
       int size_in_tagged = FixedRawDataWithSize::Decode(data);
-      STATIC_ASSERT(TSlot::kSlotDataSize == kTaggedSize ||
+      static_assert(TSlot::kSlotDataSize == kTaggedSize ||
                     TSlot::kSlotDataSize == 2 * kTaggedSize);
       int size_in_slots = size_in_tagged / (TSlot::kSlotDataSize / kTaggedSize);
       // kFixedRawData can have kTaggedSize != TSlot::kSlotDataSize when
diff --git a/src/snapshot/deserializer.h b/src/snapshot/deserializer.h
index 705178df3cd..3170cad2ab9 100644
--- a/src/snapshot/deserializer.h
+++ b/src/snapshot/deserializer.h
@@ -149,7 +149,7 @@ class Deserializer : public SerializerDeserializer {
    private:
     static const int kSize = kHotObjectCount;
     static const int kSizeMask = kSize - 1;
-    STATIC_ASSERT(base::bits::IsPowerOfTwo(kSize));
+    static_assert(base::bits::IsPowerOfTwo(kSize));
     Handle<HeapObject> circular_queue_[kSize];
     int index_ = 0;
   };
diff --git a/src/snapshot/embedded/embedded-data-inl.h b/src/snapshot/embedded/embedded-data-inl.h
index 1817ff62877..6189a7d81b2 100644
--- a/src/snapshot/embedded/embedded-data-inl.h
+++ b/src/snapshot/embedded/embedded-data-inl.h
@@ -136,7 +136,7 @@ Address EmbeddedData::InstructionStartOfBytecodeHandlers() const {
 }
 
 Address EmbeddedData::InstructionEndOfBytecodeHandlers() const {
-  STATIC_ASSERT(static_cast<int>(Builtin::kFirstBytecodeHandler) +
+  static_assert(static_cast<int>(Builtin::kFirstBytecodeHandler) +
                     kNumberOfBytecodeHandlers +
                     2 * kNumberOfWideBytecodeHandlers ==
                 Builtins::kBuiltinCount);
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index 89ce411d3ef..b00ae5a2770 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -201,7 +201,7 @@ bool BuiltinAliasesOffHeapTrampolineRegister(Isolate* isolate, Code code) {
       return false;
   }
 
-  STATIC_ASSERT(CallInterfaceDescriptor::ContextRegister() !=
+  static_assert(CallInterfaceDescriptor::ContextRegister() !=
                 kOffHeapTrampolineRegister);
 
   Callable callable = Builtins::CallableFor(isolate, code.builtin_id());
@@ -220,7 +220,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
       RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
       RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
 
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Code code = FromCodeT(isolate->builtins()->code(builtin));
@@ -274,7 +274,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   bool saw_unsafe_builtin = false;
   uint32_t raw_code_size = 0;
   uint32_t raw_data_size = 0;
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Code code = FromCodeT(builtins->code(builtin));
@@ -328,12 +328,12 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
       "If in doubt, ask jgruber@");
 
   // Allocate space for the code section, value-initialized to 0.
-  STATIC_ASSERT(RawCodeOffset() == 0);
+  static_assert(RawCodeOffset() == 0);
   const uint32_t blob_code_size = RawCodeOffset() + raw_code_size;
   uint8_t* const blob_code = new uint8_t[blob_code_size]();
 
   // Allocate space for the data section, value-initialized to 0.
-  STATIC_ASSERT(IsAligned(FixedDataSize(), Code::kMetadataAlignment));
+  static_assert(IsAligned(FixedDataSize(), Code::kMetadataAlignment));
   const uint32_t blob_data_size = FixedDataSize() + raw_data_size;
   uint8_t* const blob_data = new uint8_t[blob_data_size]();
 
@@ -343,7 +343,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
 
   // Hash relevant parts of the Isolate's heap and store the result.
   {
-    STATIC_ASSERT(IsolateHashSize() == kSizetSize);
+    static_assert(IsolateHashSize() == kSizetSize);
     const size_t hash = isolate->HashIsolateForEmbeddedBlob();
     std::memcpy(blob_data + IsolateHashOffset(), &hash, IsolateHashSize());
   }
@@ -356,7 +356,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
 
   // .. and the variable-size data section.
   uint8_t* const raw_metadata_start = blob_data + RawMetadataOffset();
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Code code = FromCodeT(builtins->code(builtin));
@@ -371,7 +371,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
 
   // .. and the variable-size code section.
   uint8_t* const raw_code_start = blob_code + RawCodeOffset();
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Code code = FromCodeT(builtins->code(builtin));
@@ -391,12 +391,12 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
 
   // Hash the blob and store the result.
   {
-    STATIC_ASSERT(EmbeddedBlobDataHashSize() == kSizetSize);
+    static_assert(EmbeddedBlobDataHashSize() == kSizetSize);
     const size_t data_hash = d.CreateEmbeddedBlobDataHash();
     std::memcpy(blob_data + EmbeddedBlobDataHashOffset(), &data_hash,
                 EmbeddedBlobDataHashSize());
 
-    STATIC_ASSERT(EmbeddedBlobCodeHashSize() == kSizetSize);
+    static_assert(EmbeddedBlobCodeHashSize() == kSizetSize);
     const size_t code_hash = d.CreateEmbeddedBlobCodeHash();
     std::memcpy(blob_data + EmbeddedBlobCodeHashOffset(), &code_hash,
                 EmbeddedBlobCodeHashSize());
@@ -413,9 +413,9 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
 }
 
 size_t EmbeddedData::CreateEmbeddedBlobDataHash() const {
-  STATIC_ASSERT(EmbeddedBlobDataHashOffset() == 0);
-  STATIC_ASSERT(EmbeddedBlobCodeHashOffset() == EmbeddedBlobDataHashSize());
-  STATIC_ASSERT(IsolateHashOffset() ==
+  static_assert(EmbeddedBlobDataHashOffset() == 0);
+  static_assert(EmbeddedBlobCodeHashOffset() == EmbeddedBlobDataHashSize());
+  static_assert(IsolateHashOffset() ==
                 EmbeddedBlobCodeHashOffset() + EmbeddedBlobCodeHashSize());
   static constexpr uint32_t kFirstHashedDataOffset = IsolateHashOffset();
   // Hash the entire data section except the embedded blob hash fields
@@ -436,7 +436,7 @@ void EmbeddedData::PrintStatistics() const {
 
   constexpr int kCount = Builtins::kBuiltinCount;
   int sizes[kCount];
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (int i = 0; i < kCount; i++) {
     sizes[i] = InstructionSizeOfBuiltin(Builtins::FromInt(i));
   }
diff --git a/src/snapshot/embedded/embedded-data.h b/src/snapshot/embedded/embedded-data.h
index ef43e2089ec..fea353dac72 100644
--- a/src/snapshot/embedded/embedded-data.h
+++ b/src/snapshot/embedded/embedded-data.h
@@ -198,30 +198,30 @@ class EmbeddedData final {
     uint32_t code_comments_offset_offset;
     uint32_t unwinding_info_offset_offset;
   };
-  STATIC_ASSERT(offsetof(LayoutDescription, instruction_offset) ==
+  static_assert(offsetof(LayoutDescription, instruction_offset) ==
                 0 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, instruction_length) ==
+  static_assert(offsetof(LayoutDescription, instruction_length) ==
                 1 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, metadata_offset) ==
+  static_assert(offsetof(LayoutDescription, metadata_offset) ==
                 2 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, metadata_length) ==
+  static_assert(offsetof(LayoutDescription, metadata_length) ==
                 3 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, handler_table_offset) ==
+  static_assert(offsetof(LayoutDescription, handler_table_offset) ==
                 4 * kUInt32Size);
 #if V8_EMBEDDED_CONSTANT_POOL
-  STATIC_ASSERT(offsetof(LayoutDescription, constant_pool_offset) ==
+  static_assert(offsetof(LayoutDescription, constant_pool_offset) ==
                 5 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, code_comments_offset_offset) ==
+  static_assert(offsetof(LayoutDescription, code_comments_offset_offset) ==
                 6 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, unwinding_info_offset_offset) ==
+  static_assert(offsetof(LayoutDescription, unwinding_info_offset_offset) ==
                 7 * kUInt32Size);
-  STATIC_ASSERT(sizeof(LayoutDescription) == 8 * kUInt32Size);
+  static_assert(sizeof(LayoutDescription) == 8 * kUInt32Size);
 #else
-  STATIC_ASSERT(offsetof(LayoutDescription, code_comments_offset_offset) ==
+  static_assert(offsetof(LayoutDescription, code_comments_offset_offset) ==
                 5 * kUInt32Size);
-  STATIC_ASSERT(offsetof(LayoutDescription, unwinding_info_offset_offset) ==
+  static_assert(offsetof(LayoutDescription, unwinding_info_offset_offset) ==
                 6 * kUInt32Size);
-  STATIC_ASSERT(sizeof(LayoutDescription) == 7 * kUInt32Size);
+  static_assert(sizeof(LayoutDescription) == 7 * kUInt32Size);
 #endif
 
   // The layout of the blob is as follows:
diff --git a/src/snapshot/embedded/embedded-file-writer.cc b/src/snapshot/embedded/embedded-file-writer.cc
index 3e9a1f63982..66cb66234d0 100644
--- a/src/snapshot/embedded/embedded-file-writer.cc
+++ b/src/snapshot/embedded/embedded-file-writer.cc
@@ -160,7 +160,7 @@ void EmbeddedFileWriter::WriteCodeSection(PlatformEmbeddedFileWriterBase* w,
   w->DeclareSymbolGlobal(EmbeddedBlobCodeSymbol().c_str());
   w->DeclareLabel(EmbeddedBlobCodeSymbol().c_str());
 
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     WriteBuiltin(w, blob, builtin);
diff --git a/src/snapshot/embedded/platform-embedded-file-writer-aix.cc b/src/snapshot/embedded/platform-embedded-file-writer-aix.cc
index 344c7d83c89..6519be7e2dc 100644
--- a/src/snapshot/embedded/platform-embedded-file-writer-aix.cc
+++ b/src/snapshot/embedded/platform-embedded-file-writer-aix.cc
@@ -55,21 +55,21 @@ void PlatformEmbeddedFileWriterAIX::DeclareSymbolGlobal(const char* name) {
 void PlatformEmbeddedFileWriterAIX::AlignToCodeAlignment() {
 #if V8_TARGET_ARCH_X64
   // On x64 use 64-bytes code alignment to allow 64-bytes loop header alignment.
-  STATIC_ASSERT((1 << 6) >= kCodeAlignment);
+  static_assert((1 << 6) >= kCodeAlignment);
   fprintf(fp_, ".align 6\n");
 #elif V8_TARGET_ARCH_PPC64
   // 64 byte alignment is needed on ppc64 to make sure p10 prefixed instructions
   // don't cross 64-byte boundaries.
-  STATIC_ASSERT((1 << 6) >= kCodeAlignment);
+  static_assert((1 << 6) >= kCodeAlignment);
   fprintf(fp_, ".align 6\n");
 #else
-  STATIC_ASSERT((1 << 5) >= kCodeAlignment);
+  static_assert((1 << 5) >= kCodeAlignment);
   fprintf(fp_, ".align 5\n");
 #endif
 }
 
 void PlatformEmbeddedFileWriterAIX::AlignToDataAlignment() {
-  STATIC_ASSERT((1 << 3) >= Code::kMetadataAlignment);
+  static_assert((1 << 3) >= Code::kMetadataAlignment);
   fprintf(fp_, ".align 3\n");
 }
 
diff --git a/src/snapshot/embedded/platform-embedded-file-writer-generic.cc b/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
index 8780de0c3de..d740ed9edf2 100644
--- a/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
+++ b/src/snapshot/embedded/platform-embedded-file-writer-generic.cc
@@ -69,15 +69,15 @@ void PlatformEmbeddedFileWriterGeneric::AlignToCodeAlignment() {
   fprintf(fp_, ".balign 4096\n");
 #elif V8_TARGET_ARCH_X64
   // On x64 use 64-bytes code alignment to allow 64-bytes loop header alignment.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #elif V8_TARGET_ARCH_PPC64
   // 64 byte alignment is needed on ppc64 to make sure p10 prefixed instructions
   // don't cross 64-byte boundaries.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #else
-  STATIC_ASSERT(32 >= kCodeAlignment);
+  static_assert(32 >= kCodeAlignment);
   fprintf(fp_, ".balign 32\n");
 #endif
 }
@@ -95,7 +95,7 @@ void PlatformEmbeddedFileWriterGeneric::AlignToDataAlignment() {
   // instructions are used to retrieve v8_Default_embedded_blob_ and/or
   // v8_Default_embedded_blob_size_. The generated instructions require the
   // load target to be aligned at 8 bytes (2^3).
-  STATIC_ASSERT(8 >= Code::kMetadataAlignment);
+  static_assert(8 >= Code::kMetadataAlignment);
   fprintf(fp_, ".balign 8\n");
 }
 
diff --git a/src/snapshot/embedded/platform-embedded-file-writer-mac.cc b/src/snapshot/embedded/platform-embedded-file-writer-mac.cc
index 81f9b85cb3f..b0cf92b21d2 100644
--- a/src/snapshot/embedded/platform-embedded-file-writer-mac.cc
+++ b/src/snapshot/embedded/platform-embedded-file-writer-mac.cc
@@ -53,19 +53,19 @@ void PlatformEmbeddedFileWriterMac::DeclareSymbolGlobal(const char* name) {
 void PlatformEmbeddedFileWriterMac::AlignToCodeAlignment() {
 #if V8_TARGET_ARCH_X64
   // On x64 use 64-bytes code alignment to allow 64-bytes loop header alignment.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #elif V8_TARGET_ARCH_PPC64
   // 64 byte alignment is needed on ppc64 to make sure p10 prefixed instructions
   // don't cross 64-byte boundaries.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #elif V8_TARGET_ARCH_ARM64
   // ARM64 macOS has a 16kiB page size. Since we want to remap it on the heap,
   // needs to be page-aligned.
   fprintf(fp_, ".balign 16384\n");
 #else
-  STATIC_ASSERT(32 >= kCodeAlignment);
+  static_assert(32 >= kCodeAlignment);
   fprintf(fp_, ".balign 32\n");
 #endif
 }
@@ -80,7 +80,7 @@ void PlatformEmbeddedFileWriterMac::AlignToPageSizeIfNeeded() {
 }
 
 void PlatformEmbeddedFileWriterMac::AlignToDataAlignment() {
-  STATIC_ASSERT(8 >= Code::kMetadataAlignment);
+  static_assert(8 >= Code::kMetadataAlignment);
   fprintf(fp_, ".balign 8\n");
 }
 
diff --git a/src/snapshot/embedded/platform-embedded-file-writer-win.cc b/src/snapshot/embedded/platform-embedded-file-writer-win.cc
index 39418857b16..e0fd99ca233 100644
--- a/src/snapshot/embedded/platform-embedded-file-writer-win.cc
+++ b/src/snapshot/embedded/platform-embedded-file-writer-win.cc
@@ -109,7 +109,7 @@ void EmitUnwindData(PlatformEmbeddedFileWriterWin* w,
   w->Comment("    UnwindInfoAddress");
   w->StartPdataSection();
   {
-    STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+    static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
     Address prev_builtin_end_offset = 0;
     for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
          ++builtin) {
@@ -196,7 +196,7 @@ void EmitUnwindData(PlatformEmbeddedFileWriterWin* w,
   std::vector<int> code_chunks;
   std::vector<win64_unwindinfo::FrameOffsets> fp_adjustments;
 
-  STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+  static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     const int builtin_index = static_cast<int>(builtin);
@@ -611,15 +611,15 @@ void PlatformEmbeddedFileWriterWin::DeclareSymbolGlobal(const char* name) {
 void PlatformEmbeddedFileWriterWin::AlignToCodeAlignment() {
 #if V8_TARGET_ARCH_X64
   // On x64 use 64-bytes code alignment to allow 64-bytes loop header alignment.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #elif V8_TARGET_ARCH_PPC64
   // 64 byte alignment is needed on ppc64 to make sure p10 prefixed instructions
   // don't cross 64-byte boundaries.
-  STATIC_ASSERT(64 >= kCodeAlignment);
+  static_assert(64 >= kCodeAlignment);
   fprintf(fp_, ".balign 64\n");
 #else
-  STATIC_ASSERT(32 >= kCodeAlignment);
+  static_assert(32 >= kCodeAlignment);
   fprintf(fp_, ".balign 32\n");
 #endif
 }
diff --git a/src/snapshot/read-only-serializer.cc b/src/snapshot/read-only-serializer.cc
index d47ff5d5f27..2908fb882ad 100644
--- a/src/snapshot/read-only-serializer.cc
+++ b/src/snapshot/read-only-serializer.cc
@@ -26,7 +26,7 @@ ReadOnlySerializer::ReadOnlySerializer(Isolate* isolate,
       did_serialize_not_mapped_symbol_(false)
 #endif
 {
-  STATIC_ASSERT(RootIndex::kFirstReadOnlyRoot == RootIndex::kFirstRoot);
+  static_assert(RootIndex::kFirstReadOnlyRoot == RootIndex::kFirstRoot);
 }
 
 ReadOnlySerializer::~ReadOnlySerializer() {
diff --git a/src/snapshot/references.h b/src/snapshot/references.h
index ecaedc41d4a..71b62cbd5fe 100644
--- a/src/snapshot/references.h
+++ b/src/snapshot/references.h
@@ -98,7 +98,7 @@ class SerializerReference {
 };
 
 // SerializerReference has to fit in an IdentityMap value field.
-STATIC_ASSERT(sizeof(SerializerReference) <= sizeof(void*));
+static_assert(sizeof(SerializerReference) <= sizeof(void*));
 
 class SerializerReferenceMap {
  public:
diff --git a/src/snapshot/serializer-deserializer.h b/src/snapshot/serializer-deserializer.h
index 99871677b02..e04b8129a10 100644
--- a/src/snapshot/serializer-deserializer.h
+++ b/src/snapshot/serializer-deserializer.h
@@ -68,7 +68,7 @@ class SerializerDeserializer : public RootVisitor {
   // The static assert below will trigger when the number of preallocated spaces
   // changed. If that happens, update the kNewObject and kBackref bytecode
   // ranges in the comments below.
-  STATIC_ASSERT(4 == kNumberOfSnapshotSpaces);
+  static_assert(4 == kNumberOfSnapshotSpaces);
 
   // First 32 root array items.
   static const int kRootArrayConstantsCount = 0x20;
@@ -186,7 +186,7 @@ class SerializerDeserializer : public RootVisitor {
   template <Bytecode kBytecode, int kMinValue, int kMaxValue,
             typename TValue = int>
   struct BytecodeValueEncoder {
-    STATIC_ASSERT((kBytecode + kMaxValue - kMinValue) <= kMaxUInt8);
+    static_assert((kBytecode + kMaxValue - kMinValue) <= kMaxUInt8);
 
     static constexpr bool IsEncodable(TValue value) {
       return base::IsInRange(static_cast<int>(value), kMinValue, kMaxValue);
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index d5f18693a9e..a3d3471eb1c 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -252,7 +252,7 @@ void Serializer::PutRoot(RootIndex root) {
 
   // Assert that the first 32 root array items are a conscious choice. They are
   // chosen so that the most common ones can be encoded more efficiently.
-  STATIC_ASSERT(static_cast<int>(RootIndex::kArgumentsMarker) ==
+  static_assert(static_cast<int>(RootIndex::kArgumentsMarker) ==
                 kRootArrayConstantsCount - 1);
 
   // TODO(ulan): Check that it works with young large objects.
@@ -270,8 +270,8 @@ void Serializer::PutSmiRoot(FullObjectSlot slot) {
   // Serializing a smi root in compressed pointer builds will serialize the
   // full object slot (of kSystemPointerSize) to avoid complications during
   // deserialization (endianness or smi sequences).
-  STATIC_ASSERT(decltype(slot)::kSlotDataSize == sizeof(Address));
-  STATIC_ASSERT(decltype(slot)::kSlotDataSize == kSystemPointerSize);
+  static_assert(decltype(slot)::kSlotDataSize == sizeof(Address));
+  static_assert(decltype(slot)::kSlotDataSize == kSystemPointerSize);
   static constexpr int bytes_to_output = decltype(slot)::kSlotDataSize;
   static constexpr int size_in_tagged = bytes_to_output >> kTaggedSizeLog2;
   sink_.Put(FixedRawDataWithSize::Encode(size_in_tagged), "Smi");
@@ -1100,7 +1100,7 @@ void Serializer::ObjectSerializer::VisitInternalReference(Code host,
   // TODO(jgruber,v8:11036): We are being permissive for this DCHECK, but
   // consider using raw_instruction_size() instead of raw_body_size() in the
   // future.
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
   DCHECK_LE(target_offset, Handle<Code>::cast(object_)->raw_body_size());
   sink_->Put(kInternalReference, "InternalRef");
   sink_->PutInt(target_offset, "internal ref value");
@@ -1114,7 +1114,7 @@ void Serializer::ObjectSerializer::VisitRuntimeEntry(Code host,
 
 void Serializer::ObjectSerializer::VisitOffHeapTarget(Code host,
                                                       RelocInfo* rinfo) {
-  STATIC_ASSERT(EmbeddedData::kTableSize == Builtins::kBuiltinCount);
+  static_assert(EmbeddedData::kTableSize == Builtins::kBuiltinCount);
 
   Address addr = rinfo->target_off_heap_target();
   CHECK_NE(kNullAddress, addr);
diff --git a/src/snapshot/serializer.h b/src/snapshot/serializer.h
index c69eed05e07..39956132d3f 100644
--- a/src/snapshot/serializer.h
+++ b/src/snapshot/serializer.h
@@ -354,7 +354,7 @@ class Serializer : public SerializerDeserializer {
    private:
     static const int kSize = kHotObjectCount;
     static const int kSizeMask = kSize - 1;
-    STATIC_ASSERT(base::bits::IsPowerOfTwo(kSize));
+    static_assert(base::bits::IsPowerOfTwo(kSize));
     Heap* heap_;
     StrongRootsEntry* strong_roots_entry_;
     Address circular_queue_[kSize] = {kNullAddress};
diff --git a/src/snapshot/snapshot.cc b/src/snapshot/snapshot.cc
index 0841d5963f1..240bc88fe91 100644
--- a/src/snapshot/snapshot.cc
+++ b/src/snapshot/snapshot.cc
@@ -100,7 +100,7 @@ class SnapshotImpl : public AllStatic {
 
   static base::Vector<const byte> ChecksummedContent(
       const v8::StartupData* data) {
-    STATIC_ASSERT(kVersionStringOffset == kChecksumOffset + kUInt32Size);
+    static_assert(kVersionStringOffset == kChecksumOffset + kUInt32Size);
     const uint32_t kChecksumStart = kVersionStringOffset;
     return base::Vector<const byte>(
         reinterpret_cast<const byte*>(data->data + kChecksumStart),
diff --git a/src/strings/string-builder-inl.h b/src/strings/string-builder-inl.h
index 60cb3333ec1..8a2daefd6b8 100644
--- a/src/strings/string-builder-inl.h
+++ b/src/strings/string-builder-inl.h
@@ -95,7 +95,7 @@ class ReplacementStringBuilder {
 
   void IncrementCharacterCount(int by) {
     if (character_count_ > String::kMaxLength - by) {
-      STATIC_ASSERT(String::kMaxLength < kMaxInt);
+      static_assert(String::kMaxLength < kMaxInt);
       character_count_ = kMaxInt;
     } else {
       character_count_ += by;
@@ -134,7 +134,7 @@ class IncrementalStringBuilder {
   V8_INLINE void AppendCStringLiteral(const char (&literal)[N]) {
     // Note that the literal contains the zero char.
     const int length = N - 1;
-    STATIC_ASSERT(length > 0);
+    static_assert(length > 0);
     if (length == 1) return AppendCharacter(literal[0]);
     if (encoding_ == String::ONE_BYTE_ENCODING && CurrentPartCanFit(N)) {
       const uint8_t* chars = reinterpret_cast<const uint8_t*>(literal);
@@ -182,7 +182,7 @@ class IncrementalStringBuilder {
   // is a more pessimistic estimate, but faster to calculate.
   V8_INLINE int EscapedLengthIfCurrentPartFits(int length) {
     if (length > kMaxPartLength) return 0;
-    STATIC_ASSERT((kMaxPartLength << 3) <= String::kMaxLength);
+    static_assert((kMaxPartLength << 3) <= String::kMaxLength);
     // This shift will not overflow because length is already less than the
     // maximum part length.
     int worst_case_length = length << 3;
diff --git a/src/strings/string-hasher-inl.h b/src/strings/string-hasher-inl.h
index 07fbac7be62..ce0e2558e6b 100644
--- a/src/strings/string-hasher-inl.h
+++ b/src/strings/string-hasher-inl.h
@@ -42,7 +42,7 @@ uint32_t StringHasher::GetTrivialHash(int length) {
   // The hash of a large string is simply computed from the length.
   // Ensure that the max length is small enough to be encoded without losing
   // information.
-  STATIC_ASSERT(String::kMaxLength <= String::HashBits::kMax);
+  static_assert(String::kMaxLength <= String::HashBits::kMax);
   uint32_t hash = static_cast<uint32_t>(length);
   return String::CreateHashFieldValue(hash, String::HashFieldType::kHash);
 }
@@ -50,8 +50,8 @@ uint32_t StringHasher::GetTrivialHash(int length) {
 template <typename char_t>
 uint32_t StringHasher::HashSequentialString(const char_t* chars_raw, int length,
                                             uint64_t seed) {
-  STATIC_ASSERT(std::is_integral<char_t>::value);
-  STATIC_ASSERT(sizeof(char_t) <= 2);
+  static_assert(std::is_integral<char_t>::value);
+  static_assert(sizeof(char_t) <= 2);
   using uchar = typename std::make_unsigned<char_t>::type;
   const uchar* chars = reinterpret_cast<const uchar*>(chars_raw);
   DCHECK_LE(0, length);
diff --git a/src/tracing/trace-event.h b/src/tracing/trace-event.h
index d5f9fa407f8..35455268610 100644
--- a/src/tracing/trace-event.h
+++ b/src/tracing/trace-event.h
@@ -441,7 +441,7 @@ SetTraceValue(T arg, unsigned char* type, uint64_t* value) {
                                       uint64_t* value) {                    \
     *type = value_type_id;                                                  \
     *value = 0;                                                             \
-    STATIC_ASSERT(sizeof(arg) <= sizeof(*value));                           \
+    static_assert(sizeof(arg) <= sizeof(*value));                           \
     memcpy(value, &arg, sizeof(arg));                                       \
   }
 INTERNAL_DECLARE_SET_TRACE_VALUE(double, TRACE_VALUE_TYPE_DOUBLE)
diff --git a/src/utils/identity-map.h b/src/utils/identity-map.h
index 92538d46bc3..9e36ee139fe 100644
--- a/src/utils/identity-map.h
+++ b/src/utils/identity-map.h
@@ -103,9 +103,9 @@ class V8_EXPORT_PRIVATE IdentityMapBase {
 template <typename V, class AllocationPolicy>
 class IdentityMap : public IdentityMapBase {
  public:
-  STATIC_ASSERT(sizeof(V) <= sizeof(uintptr_t));
-  STATIC_ASSERT(std::is_trivially_copyable<V>::value);
-  STATIC_ASSERT(std::is_trivially_destructible<V>::value);
+  static_assert(sizeof(V) <= sizeof(uintptr_t));
+  static_assert(std::is_trivially_copyable<V>::value);
+  static_assert(std::is_trivially_destructible<V>::value);
 
   explicit IdentityMap(Heap* heap,
                        AllocationPolicy allocator = AllocationPolicy())
diff --git a/src/utils/memcopy.h b/src/utils/memcopy.h
index edc5e9ee4b9..ae4701e7354 100644
--- a/src/utils/memcopy.h
+++ b/src/utils/memcopy.h
@@ -184,7 +184,7 @@ inline void CopyWords(Address dst, const Address src, size_t num_words) {
 // Copies data from |src| to |dst|.  The data spans must not overlap.
 template <typename T>
 inline void CopyBytes(T* dst, const T* src, size_t num_bytes) {
-  STATIC_ASSERT(sizeof(T) == 1);
+  static_assert(sizeof(T) == 1);
   if (num_bytes == 0) return;
   CopyImpl<kMinComplexMemCopy>(dst, src, num_bytes);
 }
@@ -269,8 +269,8 @@ void CopyChars(DstType* dst, const SrcType* src, size_t count) V8_NONNULL(1, 2);
 
 template <typename SrcType, typename DstType>
 void CopyChars(DstType* dst, const SrcType* src, size_t count) {
-  STATIC_ASSERT(std::is_integral<SrcType>::value);
-  STATIC_ASSERT(std::is_integral<DstType>::value);
+  static_assert(std::is_integral<SrcType>::value);
+  static_assert(std::is_integral<DstType>::value);
   using SrcTypeUnsigned = typename std::make_unsigned<SrcType>::type;
   using DstTypeUnsigned = typename std::make_unsigned<DstType>::type;
 
diff --git a/src/utils/utils.h b/src/utils/utils.h
index f24b9f94bd6..436b8628f6d 100644
--- a/src/utils/utils.h
+++ b/src/utils/utils.h
@@ -231,7 +231,7 @@ inline T RoundingAverageUnsigned(T a, T b) {
 
 // Compare two offsets with static cast
 #define STATIC_ASSERT_FIELD_OFFSETS_EQUAL(Offset1, Offset2) \
-  STATIC_ASSERT(static_cast<int>(Offset1) == Offset2)
+  static_assert(static_cast<int>(Offset1) == Offset2)
 // ----------------------------------------------------------------------------
 // Hash function.
 
@@ -324,8 +324,8 @@ class SetOncePointer {
 template <typename lchar, typename rchar>
 inline bool CompareCharsEqualUnsigned(const lchar* lhs, const rchar* rhs,
                                       size_t chars) {
-  STATIC_ASSERT(std::is_unsigned<lchar>::value);
-  STATIC_ASSERT(std::is_unsigned<rchar>::value);
+  static_assert(std::is_unsigned<lchar>::value);
+  static_assert(std::is_unsigned<rchar>::value);
   if (sizeof(*lhs) == sizeof(*rhs)) {
     // memcmp compares byte-by-byte, but for equality it doesn't matter whether
     // two-byte char comparison is little- or big-endian.
@@ -350,8 +350,8 @@ inline bool CompareCharsEqual(const lchar* lhs, const rchar* rhs,
 template <typename lchar, typename rchar>
 inline int CompareCharsUnsigned(const lchar* lhs, const rchar* rhs,
                                 size_t chars) {
-  STATIC_ASSERT(std::is_unsigned<lchar>::value);
-  STATIC_ASSERT(std::is_unsigned<rchar>::value);
+  static_assert(std::is_unsigned<lchar>::value);
+  static_assert(std::is_unsigned<rchar>::value);
   if (sizeof(*lhs) == sizeof(char) && sizeof(*rhs) == sizeof(char)) {
     // memcmp compares byte-by-byte, yielding wrong results for two-byte
     // strings on little-endian systems.
diff --git a/src/wasm/baseline/arm/liftoff-assembler-arm.h b/src/wasm/baseline/arm/liftoff-assembler-arm.h
index 7fd71793411..eb4293426d4 100644
--- a/src/wasm/baseline/arm/liftoff-assembler-arm.h
+++ b/src/wasm/baseline/arm/liftoff-assembler-arm.h
@@ -626,7 +626,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      Register instance,
                                                      int offset) {
-  STATIC_ASSERT(kTaggedSize == kSystemPointerSize);
+  static_assert(kTaggedSize == kSystemPointerSize);
   ldr(dst, MemOperand{instance, offset});
 }
 
@@ -738,7 +738,7 @@ void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
                                          int32_t offset_imm,
                                          LiftoffRegList pinned) {
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   liftoff::LoadInternal(this, LiftoffRegister(dst), src_addr, offset_reg,
                         offset_imm, LoadType::kI32Load, pinned);
 }
@@ -757,7 +757,7 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
                                           LiftoffRegister src,
                                           LiftoffRegList pinned,
                                           SkipWriteBarrier skip_write_barrier) {
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   Register actual_offset_reg = offset_reg;
   if (offset_reg != no_reg && offset_imm != 0) {
     if (cache_state()->is_used(LiftoffRegister(offset_reg))) {
diff --git a/src/wasm/baseline/arm64/liftoff-assembler-arm64.h b/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
index b3d9936851b..26db2e1e3e3 100644
--- a/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
+++ b/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
@@ -991,7 +991,7 @@ void LiftoffAssembler::FillStackSlotsWithZero(int start, int size) {
       IsImmLSUnscaled(-start - 12)) {
     // Special straight-line code for up to 12 slots. Generates one
     // instruction per two slots (<= 7 instructions total).
-    STATIC_ASSERT(kStackSlotSize == kSystemPointerSize);
+    static_assert(kStackSlotSize == kSystemPointerSize);
     uint32_t remainder = size;
     for (; remainder >= 2 * kStackSlotSize; remainder -= 2 * kStackSlotSize) {
       stp(xzr, xzr, liftoff::GetStackSlot(start + remainder));
diff --git a/src/wasm/baseline/ia32/liftoff-assembler-ia32.h b/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
index 8c25fb65694..5c3c24777dc 100644
--- a/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
+++ b/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
@@ -373,7 +373,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      Register instance,
                                                      int offset) {
-  STATIC_ASSERT(kTaggedSize == kSystemPointerSize);
+  static_assert(kTaggedSize == kSystemPointerSize);
   mov(dst, Operand{instance, offset});
 }
 
@@ -388,7 +388,7 @@ void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          int32_t offset_imm,
                                          LiftoffRegList pinned) {
   DCHECK_GE(offset_imm, 0);
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   Load(LiftoffRegister(dst), src_addr, offset_reg,
        static_cast<uint32_t>(offset_imm), LoadType::kI32Load, pinned);
 }
@@ -406,7 +406,7 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
                                           SkipWriteBarrier skip_write_barrier) {
   DCHECK_GE(offset_imm, 0);
   DCHECK_LE(offset_imm, std::numeric_limits<int32_t>::max());
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   Operand dst_op = offset_reg == no_reg
                        ? Operand(dst_addr, offset_imm)
                        : Operand(dst_addr, offset_reg, times_1, offset_imm);
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index 15ebced3e77..e543d750b78 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -545,7 +545,7 @@ class LiftoffCompiler {
 
   int GetFeedbackVectorSlots() const {
     // The number of instructions is capped by max function size.
-    STATIC_ASSERT(kV8MaxWasmFunctionSize < std::numeric_limits<int>::max());
+    static_assert(kV8MaxWasmFunctionSize < std::numeric_limits<int>::max());
     return static_cast<int>(num_call_instructions_) * 2;
   }
 
@@ -5570,7 +5570,7 @@ class LiftoffCompiler {
     LiftoffRegister src = __ PopToRegister();
     LiftoffRegister dst = __ GetUnusedRegister(kGpReg, {src}, {});
     if (SmiValuesAre31Bits()) {
-      STATIC_ASSERT(kSmiTag == 0);
+      static_assert(kSmiTag == 0);
       __ emit_i32_shli(dst.gp(), src.gp(), kSmiTagSize);
     } else {
       DCHECK(SmiValuesAre32Bits());
@@ -6122,7 +6122,7 @@ class LiftoffCompiler {
               kPointerLoadType, pinned);
     }
     // Shift {index} by 2 (multiply by 4) to represent kInt32Size items.
-    STATIC_ASSERT((1 << 2) == kInt32Size);
+    static_assert((1 << 2) == kInt32Size);
     __ emit_i32_shli(index, index, 2);
     __ Load(LiftoffRegister(scratch), table, index, 0, LoadType::kI32Load,
             pinned);
diff --git a/src/wasm/baseline/liftoff-register.h b/src/wasm/baseline/liftoff-register.h
index e9e6fd5d707..3604c0043fd 100644
--- a/src/wasm/baseline/liftoff-register.h
+++ b/src/wasm/baseline/liftoff-register.h
@@ -282,7 +282,7 @@ class LiftoffRegister {
   }
 
   constexpr int liftoff_code() const {
-    STATIC_ASSERT(sizeof(int) >= sizeof(storage_t));
+    static_assert(sizeof(int) >= sizeof(storage_t));
     return static_cast<int>(code_);
   }
 
diff --git a/src/wasm/baseline/loong64/liftoff-assembler-loong64.h b/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
index 6a02ec4b69e..eb065dd80c9 100644
--- a/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
+++ b/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
@@ -354,7 +354,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      Register instance,
                                                      int32_t offset) {
-  STATIC_ASSERT(kTaggedSize == kSystemPointerSize);
+  static_assert(kTaggedSize == kSystemPointerSize);
   Ld_d(dst, MemOperand(instance, offset));
 }
 
@@ -368,7 +368,7 @@ void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
                                          int32_t offset_imm,
                                          LiftoffRegList pinned) {
-  STATIC_ASSERT(kTaggedSize == kInt64Size);
+  static_assert(kTaggedSize == kInt64Size);
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   Ld_d(dst, src_op);
 }
diff --git a/src/wasm/baseline/mips/liftoff-assembler-mips.h b/src/wasm/baseline/mips/liftoff-assembler-mips.h
index b1f60b289a5..94fc3b0206f 100644
--- a/src/wasm/baseline/mips/liftoff-assembler-mips.h
+++ b/src/wasm/baseline/mips/liftoff-assembler-mips.h
@@ -485,7 +485,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      Register instance,
                                                      int32_t offset) {
-  STATIC_ASSERT(kTaggedSize == kSystemPointerSize);
+  static_assert(kTaggedSize == kSystemPointerSize);
   lw(dst, MemOperand(instance, offset));
 }
 
@@ -499,7 +499,7 @@ void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
                                          int32_t offset_imm,
                                          LiftoffRegList pinned) {
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   Load(LiftoffRegister(dst), src_addr, offset_reg,
        static_cast<uint32_t>(offset_imm), LoadType::kI32Load, pinned);
 }
@@ -516,7 +516,7 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
                                           LiftoffRegister src,
                                           LiftoffRegList pinned,
                                           SkipWriteBarrier skip_write_barrier) {
-  STATIC_ASSERT(kTaggedSize == kInt32Size);
+  static_assert(kTaggedSize == kInt32Size);
   Register dst = no_reg;
   if (offset_reg != no_reg) {
     dst = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
diff --git a/src/wasm/baseline/mips64/liftoff-assembler-mips64.h b/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
index f98840b218d..8492bb16a03 100644
--- a/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
+++ b/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
@@ -470,7 +470,7 @@ void LiftoffAssembler::LoadFromInstance(Register dst, Register instance,
 void LiftoffAssembler::LoadTaggedPointerFromInstance(Register dst,
                                                      Register instance,
                                                      int32_t offset) {
-  STATIC_ASSERT(kTaggedSize == kSystemPointerSize);
+  static_assert(kTaggedSize == kSystemPointerSize);
   Ld(dst, MemOperand(instance, offset));
 }
 
@@ -484,7 +484,7 @@ void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
                                          int32_t offset_imm,
                                          LiftoffRegList pinned) {
-  STATIC_ASSERT(kTaggedSize == kInt64Size);
+  static_assert(kTaggedSize == kInt64Size);
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   Ld(dst, src_op);
 }
@@ -501,7 +501,7 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
                                           LiftoffRegister src,
                                           LiftoffRegList pinned,
                                           SkipWriteBarrier skip_write_barrier) {
-  STATIC_ASSERT(kTaggedSize == kInt64Size);
+  static_assert(kTaggedSize == kInt64Size);
   Register scratch = pinned.set(GetUnusedRegister(kGpReg, pinned)).gp();
   MemOperand dst_op = liftoff::GetMemOp(this, dst_addr, offset_reg, offset_imm);
   Sd(src.gp(), dst_op);
diff --git a/src/wasm/function-body-decoder-impl.h b/src/wasm/function-body-decoder-impl.h
index 39c2d2f4c92..2d622d93373 100644
--- a/src/wasm/function-body-decoder-impl.h
+++ b/src/wasm/function-body-decoder-impl.h
@@ -136,7 +136,7 @@ void DecodeError(Decoder* decoder, const byte* pc, const char* str,
                  Args&&... args) {
   CHECK(validate == Decoder::kFullValidation ||
         validate == Decoder::kBooleanValidation);
-  STATIC_ASSERT(sizeof...(Args) > 0);
+  static_assert(sizeof...(Args) > 0);
   if (validate == Decoder::kBooleanValidation) {
     decoder->MarkError();
   } else {
@@ -161,7 +161,7 @@ template <Decoder::ValidateFlag validate, typename... Args>
 void DecodeError(Decoder* decoder, const char* str, Args&&... args) {
   CHECK(validate == Decoder::kFullValidation ||
         validate == Decoder::kBooleanValidation);
-  STATIC_ASSERT(sizeof...(Args) > 0);
+  static_assert(sizeof...(Args) > 0);
   if (validate == Decoder::kBooleanValidation) {
     decoder->MarkError();
   } else {
@@ -3261,7 +3261,7 @@ class WasmFullDecoder : public WasmDecoder<validate, decoding_mode> {
         LoadType::kI32Load16S, LoadType::kI32Load16U, LoadType::kI64Load8S,
         LoadType::kI64Load8U,  LoadType::kI64Load16S, LoadType::kI64Load16U,
         LoadType::kI64Load32S, LoadType::kI64Load32U};
-    STATIC_ASSERT(arraysize(kLoadTypes) == kMaxOpcode - kMinOpcode + 1);
+    static_assert(arraysize(kLoadTypes) == kMaxOpcode - kMinOpcode + 1);
     DCHECK_LE(kMinOpcode, opcode);
     DCHECK_GE(kMaxOpcode, opcode);
     return DecodeLoadMem(kLoadTypes[opcode - kMinOpcode]);
@@ -3277,7 +3277,7 @@ class WasmFullDecoder : public WasmDecoder<validate, decoding_mode> {
         StoreType::kI32Store,  StoreType::kI64Store,   StoreType::kF32Store,
         StoreType::kF64Store,  StoreType::kI32Store8,  StoreType::kI32Store16,
         StoreType::kI64Store8, StoreType::kI64Store16, StoreType::kI64Store32};
-    STATIC_ASSERT(arraysize(kStoreTypes) == kMaxOpcode - kMinOpcode + 1);
+    static_assert(arraysize(kStoreTypes) == kMaxOpcode - kMinOpcode + 1);
     DCHECK_LE(kMinOpcode, opcode);
     DCHECK_GE(kMaxOpcode, opcode);
     return DecodeStoreMem(kStoreTypes[opcode - kMinOpcode]);
diff --git a/src/wasm/function-compiler.h b/src/wasm/function-compiler.h
index dea854c41c7..b0bdf2881e7 100644
--- a/src/wasm/function-compiler.h
+++ b/src/wasm/function-compiler.h
@@ -98,7 +98,7 @@ class V8_EXPORT_PRIVATE WasmCompilationUnit final {
 // {WasmCompilationUnit} should be trivially copyable and small enough so we can
 // efficiently pass it by value.
 ASSERT_TRIVIALLY_COPYABLE(WasmCompilationUnit);
-STATIC_ASSERT(sizeof(WasmCompilationUnit) <= 2 * kSystemPointerSize);
+static_assert(sizeof(WasmCompilationUnit) <= 2 * kSystemPointerSize);
 
 class V8_EXPORT_PRIVATE JSToWasmWrapperCompilationUnit final {
  public:
diff --git a/src/wasm/jump-table-assembler.cc b/src/wasm/jump-table-assembler.cc
index ef79c55f1cc..b101d095402 100644
--- a/src/wasm/jump-table-assembler.cc
+++ b/src/wasm/jump-table-assembler.cc
@@ -113,8 +113,8 @@ void JumpTableAssembler::EmitFarJumpSlot(Address target) {
   // after the currently executing one.
   ldr_pcrel(pc, -kInstrSize);  // 1 instruction
   dd(target);                  // 4 bytes (== 1 instruction)
-  STATIC_ASSERT(kInstrSize == kInt32Size);
-  STATIC_ASSERT(kFarJumpTableSlotSize == 2 * kInstrSize);
+  static_assert(kInstrSize == kInt32Size);
+  static_assert(kFarJumpTableSlotSize == 2 * kInstrSize);
 }
 
 // static
@@ -171,9 +171,9 @@ void JumpTableAssembler::EmitFarJumpSlot(Address target) {
   nop();       // To keep the target below aligned to kSystemPointerSize.
 #endif
   dq(target);  // 8 bytes (== 2 instructions)
-  STATIC_ASSERT(2 * kInstrSize == kSystemPointerSize);
+  static_assert(2 * kInstrSize == kSystemPointerSize);
   const int kSlotCount = ENABLE_CONTROL_FLOW_INTEGRITY_BOOL ? 6 : 4;
-  STATIC_ASSERT(kFarJumpTableSlotSize == kSlotCount * kInstrSize);
+  static_assert(kFarJumpTableSlotSize == kSlotCount * kInstrSize);
 }
 
 // static
diff --git a/src/wasm/jump-table-assembler.h b/src/wasm/jump-table-assembler.h
index f2accac5530..9d8ff588039 100644
--- a/src/wasm/jump-table-assembler.h
+++ b/src/wasm/jump-table-assembler.h
@@ -235,7 +235,7 @@ class V8_EXPORT_PRIVATE JumpTableAssembler : public MacroAssembler {
 
   static constexpr int kJumpTableSlotsPerLine =
       kJumpTableLineSize / kJumpTableSlotSize;
-  STATIC_ASSERT(kJumpTableSlotsPerLine >= 1);
+  static_assert(kJumpTableSlotsPerLine >= 1);
 
   // {JumpTableAssembler} is never used during snapshot generation, and its code
   // must be independent of the code range of any isolate anyway. Just ensure
diff --git a/src/wasm/memory-protection-key.cc b/src/wasm/memory-protection-key.cc
index 6a367cc2bbd..f230760e6e3 100644
--- a/src/wasm/memory-protection-key.cc
+++ b/src/wasm/memory-protection-key.cc
@@ -128,7 +128,7 @@ int AllocateMemoryProtectionKey() {
   // errno, e.g., EINVAL vs ENOSPC vs ENOSYS. See manpages and glibc manual
   // (the latter is the authorative source):
   // https://www.gnu.org/software/libc/manual/html_mono/libc.html#Memory-Protection-Keys
-  STATIC_ASSERT(kNoMemoryProtectionKey == -1);
+  static_assert(kNoMemoryProtectionKey == -1);
   return pkey_alloc(/* flags, unused */ 0, kDisableAccess);
 }
 
diff --git a/src/wasm/memory-protection-key.h b/src/wasm/memory-protection-key.h
index 3fffee51f08..1e2eea48a8c 100644
--- a/src/wasm/memory-protection-key.h
+++ b/src/wasm/memory-protection-key.h
@@ -10,7 +10,7 @@
 #define V8_WASM_MEMORY_PROTECTION_KEY_H_
 
 #if defined(V8_OS_LINUX) && defined(V8_HOST_ARCH_X64)
-#include <sys/mman.h>  // For STATIC_ASSERT of permission values.
+#include <sys/mman.h>  // For static_assert of permission values.
 #undef MAP_TYPE  // Conflicts with MAP_TYPE in Torque-generated instance-types.h
 #endif
 
@@ -44,8 +44,8 @@ enum MemoryProtectionKeyPermission {
 // If sys/mman.h has PKEY support (on newer Linux distributions), ensure that
 // our definitions of the permissions is consistent with the ones in glibc.
 #if defined(PKEY_DISABLE_ACCESS)
-STATIC_ASSERT(kDisableAccess == PKEY_DISABLE_ACCESS);
-STATIC_ASSERT(kDisableWrite == PKEY_DISABLE_WRITE);
+static_assert(kDisableAccess == PKEY_DISABLE_ACCESS);
+static_assert(kDisableWrite == PKEY_DISABLE_WRITE);
 #endif
 
 // Call exactly once per process to determine if PKU is supported on this
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index cfcc042f3e9..725a140f526 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1574,7 +1574,7 @@ CompilationExecutionResult ExecuteCompilationUnits(
   std::shared_ptr<const WasmModule> module;
   // Task 0 is any main thread (there might be multiple from multiple isolates),
   // worker threads start at 1 (thus the "+ 1").
-  STATIC_ASSERT(kMainTaskId == 0);
+  static_assert(kMainTaskId == 0);
   int task_id = delegate ? (int{delegate->GetTaskId()} + 1) : kMainTaskId;
   DCHECK_LE(0, task_id);
   CompilationUnitQueues::Queue* queue;
diff --git a/src/wasm/module-decoder.cc b/src/wasm/module-decoder.cc
index 0d378675109..c3e6779bdec 100644
--- a/src/wasm/module-decoder.cc
+++ b/src/wasm/module-decoder.cc
@@ -1756,7 +1756,7 @@ class ModuleDecoderImpl : public Decoder {
 
   uint8_t validate_table_flags(const char* name) {
     uint8_t flags = consume_u8("table limits flags");
-    STATIC_ASSERT(kNoMaximum < kWithMaximum);
+    static_assert(kNoMaximum < kWithMaximum);
     if (V8_UNLIKELY(flags > kWithMaximum)) {
       errorf(pc() - 1, "invalid %s limits flags", name);
     }
diff --git a/src/wasm/wasm-code-manager.cc b/src/wasm/wasm-code-manager.cc
index 655458684bd..df5cefc4a15 100644
--- a/src/wasm/wasm-code-manager.cc
+++ b/src/wasm/wasm-code-manager.cc
@@ -296,10 +296,10 @@ void WasmCode::Validate() const {
   // The packing strategy for {tagged_parameter_slots} only works if both the
   // max number of parameters and their max combined stack slot usage fits into
   // their respective half of the result value.
-  STATIC_ASSERT(wasm::kV8MaxWasmFunctionParams <
+  static_assert(wasm::kV8MaxWasmFunctionParams <
                 std::numeric_limits<uint16_t>::max());
   static constexpr int kMaxSlotsPerParam = 4;  // S128 on 32-bit platforms.
-  STATIC_ASSERT(wasm::kV8MaxWasmFunctionParams * kMaxSlotsPerParam <
+  static_assert(wasm::kV8MaxWasmFunctionParams * kMaxSlotsPerParam <
                 std::numeric_limits<uint16_t>::max());
 
 #ifdef DEBUG
@@ -1077,7 +1077,7 @@ WasmCode* NativeModule::AddCodeForTesting(Handle<Code> code) {
                                source_pos_table->length());
   }
   CHECK(!code->is_off_heap_trampoline());
-  STATIC_ASSERT(Code::kOnHeapBodyIsContiguous);
+  static_assert(Code::kOnHeapBodyIsContiguous);
   base::Vector<const byte> instructions(
       reinterpret_cast<byte*>(code->raw_body_start()),
       static_cast<size_t>(code->raw_body_size()));
@@ -1633,7 +1633,7 @@ void NativeModule::AddCodeSpaceLocked(base::AddressRegion region) {
         WASM_RUNTIME_STUB_LIST(RUNTIME_STUB, RUNTIME_STUB_TRAP)};
 #undef RUNTIME_STUB
 #undef RUNTIME_STUB_TRAP
-    STATIC_ASSERT(Builtins::kAllBuiltinsAreIsolateIndependent);
+    static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
     Address builtin_addresses[WasmCode::kRuntimeStubCount];
     for (int i = 0; i < WasmCode::kRuntimeStubCount; ++i) {
       Builtin builtin = stub_names[i];
@@ -2589,7 +2589,7 @@ Builtin RuntimeStubIdToBuiltinName(WasmCode::RuntimeStubId stub_id) {
       WASM_RUNTIME_STUB_LIST(RUNTIME_STUB_NAME, RUNTIME_STUB_NAME_TRAP)};
 #undef RUNTIME_STUB_NAME
 #undef RUNTIME_STUB_NAME_TRAP
-  STATIC_ASSERT(arraysize(builtin_names) == WasmCode::kRuntimeStubCount);
+  static_assert(arraysize(builtin_names) == WasmCode::kRuntimeStubCount);
 
   DCHECK_GT(arraysize(builtin_names), stub_id);
   return builtin_names[stub_id];
@@ -2602,7 +2602,7 @@ const char* GetRuntimeStubName(WasmCode::RuntimeStubId stub_id) {
       RUNTIME_STUB_NAME, RUNTIME_STUB_NAME_TRAP) "<unknown>"};
 #undef RUNTIME_STUB_NAME
 #undef RUNTIME_STUB_NAME_TRAP
-  STATIC_ASSERT(arraysize(runtime_stub_names) ==
+  static_assert(arraysize(runtime_stub_names) ==
                 WasmCode::kRuntimeStubCount + 1);
 
   DCHECK_GT(arraysize(runtime_stub_names), stub_id);
diff --git a/src/wasm/wasm-code-manager.h b/src/wasm/wasm-code-manager.h
index 1fd65c2d0d2..a864ded30a1 100644
--- a/src/wasm/wasm-code-manager.h
+++ b/src/wasm/wasm-code-manager.h
@@ -496,7 +496,7 @@ class V8_EXPORT_PRIVATE WasmCode final {
 // Increase the limit if needed, but first check if the size increase is
 // justified.
 #ifndef V8_GC_MOLE
-STATIC_ASSERT(sizeof(WasmCode) <= 88);
+static_assert(sizeof(WasmCode) <= 88);
 #endif
 
 WasmCode::Kind GetCodeKind(const WasmCompilationResult& result);
diff --git a/src/wasm/wasm-engine.cc b/src/wasm/wasm-engine.cc
index a0cbbccacb9..cca041dd6e9 100644
--- a/src/wasm/wasm-engine.cc
+++ b/src/wasm/wasm-engine.cc
@@ -1683,7 +1683,7 @@ uint32_t max_mem_pages() {
   static_assert(
       kV8MaxWasmMemoryPages * kWasmPageSize <= JSArrayBuffer::kMaxByteLength,
       "Wasm memories must not be bigger than JSArrayBuffers");
-  STATIC_ASSERT(kV8MaxWasmMemoryPages <= kMaxUInt32);
+  static_assert(kV8MaxWasmMemoryPages <= kMaxUInt32);
   return std::min(uint32_t{kV8MaxWasmMemoryPages}, FLAG_wasm_max_mem_pages);
 }
 
diff --git a/src/wasm/wasm-module-builder.cc b/src/wasm/wasm-module-builder.cc
index d8723f2f1f7..c83d6cf5e57 100644
--- a/src/wasm/wasm-module-builder.cc
+++ b/src/wasm/wasm-module-builder.cc
@@ -529,10 +529,10 @@ void WriteInitializerExpressionWithEnd(ZoneBuffer* buffer,
     case WasmInitExpr::kStructNewWithRtt:
     case WasmInitExpr::kStructNewDefault:
     case WasmInitExpr::kStructNewDefaultWithRtt:
-      STATIC_ASSERT((kExprStructNew >> 8) == kGCPrefix);
-      STATIC_ASSERT((kExprStructNewWithRtt >> 8) == kGCPrefix);
-      STATIC_ASSERT((kExprStructNewDefault >> 8) == kGCPrefix);
-      STATIC_ASSERT((kExprStructNewDefaultWithRtt >> 8) == kGCPrefix);
+      static_assert((kExprStructNew >> 8) == kGCPrefix);
+      static_assert((kExprStructNewWithRtt >> 8) == kGCPrefix);
+      static_assert((kExprStructNewDefault >> 8) == kGCPrefix);
+      static_assert((kExprStructNewDefaultWithRtt >> 8) == kGCPrefix);
       for (const WasmInitExpr& operand : *init.operands()) {
         WriteInitializerExpressionWithEnd(buffer, operand, kWasmBottom);
       }
@@ -559,8 +559,8 @@ void WriteInitializerExpressionWithEnd(ZoneBuffer* buffer,
       break;
     case WasmInitExpr::kArrayInit:
     case WasmInitExpr::kArrayInitStatic:
-      STATIC_ASSERT((kExprArrayInit >> 8) == kGCPrefix);
-      STATIC_ASSERT((kExprArrayInitStatic >> 8) == kGCPrefix);
+      static_assert((kExprArrayInit >> 8) == kGCPrefix);
+      static_assert((kExprArrayInitStatic >> 8) == kGCPrefix);
       for (const WasmInitExpr& operand : *init.operands()) {
         WriteInitializerExpressionWithEnd(buffer, operand, kWasmBottom);
       }
@@ -572,7 +572,7 @@ void WriteInitializerExpressionWithEnd(ZoneBuffer* buffer,
       buffer->write_u32v(static_cast<uint32_t>(init.operands()->size() - 1));
       break;
     case WasmInitExpr::kRttCanon:
-      STATIC_ASSERT((kExprRttCanon >> 8) == kGCPrefix);
+      static_assert((kExprRttCanon >> 8) == kGCPrefix);
       buffer->write_u8(kGCPrefix);
       buffer->write_u8(static_cast<uint8_t>(kExprRttCanon));
       buffer->write_i32v(static_cast<int32_t>(init.immediate().index));
diff --git a/src/wasm/wasm-module.h b/src/wasm/wasm-module.h
index 613dbc3c700..c0b188f9857 100644
--- a/src/wasm/wasm-module.h
+++ b/src/wasm/wasm-module.h
@@ -152,10 +152,10 @@ class ConstantExpression {
   using KindField = LengthField::Next<Kind, kKindBits>;
 
   // Make sure we reserve enough bits for a {WireBytesRef}'s length and offset.
-  STATIC_ASSERT(kV8MaxWasmModuleSize <= LengthField::kMax + 1);
-  STATIC_ASSERT(kV8MaxWasmModuleSize <= OffsetField::kMax + 1);
+  static_assert(kV8MaxWasmModuleSize <= LengthField::kMax + 1);
+  static_assert(kV8MaxWasmModuleSize <= OffsetField::kMax + 1);
   // Make sure kind fits in kKindBits.
-  STATIC_ASSERT(kLastKind <= KindField::kMax + 1);
+  static_assert(kLastKind <= KindField::kMax + 1);
 
   explicit ConstantExpression(uint64_t bit_field) : bit_field_(bit_field) {}
 
@@ -164,7 +164,7 @@ class ConstantExpression {
 
 // We want to keep {ConstantExpression} small to reduce memory usage during
 // compilation/instantiation.
-STATIC_ASSERT(sizeof(ConstantExpression) <= 8);
+static_assert(sizeof(ConstantExpression) <= 8);
 
 // Static representation of a wasm global variable.
 struct WasmGlobal {
diff --git a/src/wasm/wasm-objects-inl.h b/src/wasm/wasm-objects-inl.h
index 3d5164beb37..480f9b38f1a 100644
--- a/src/wasm/wasm-objects-inl.h
+++ b/src/wasm/wasm-objects-inl.h
@@ -507,7 +507,7 @@ wasm::StructType* WasmStruct::GcSafeType(Map map) {
 int WasmStruct::Size(const wasm::StructType* type) {
   // Object size must fit into a Smi (because of filler objects), and its
   // computation must not overflow.
-  STATIC_ASSERT(Smi::kMaxValue <= kMaxInt);
+  static_assert(Smi::kMaxValue <= kMaxInt);
   DCHECK_LE(type->total_fields_size(), Smi::kMaxValue - kHeaderSize);
   return std::max(kHeaderSize + static_cast<int>(type->total_fields_size()),
                   Heap::kMinObjectSizeInTaggedWords * kTaggedSize);
@@ -520,7 +520,7 @@ void WasmStruct::EncodeInstanceSizeInMap(int instance_size, Map map) {
   // map so that the GC can read it without relying on any other objects
   // still being around. To solve this problem, we store the instance size
   // in two other fields that are otherwise unused for WasmStructs.
-  STATIC_ASSERT(0xFFFF - kHeaderSize >
+  static_assert(0xFFFF - kHeaderSize >
                 wasm::kMaxValueTypeSize * wasm::kV8MaxWasmStructFields);
   map.SetWasmByte1(instance_size & 0xFF);
   map.SetWasmByte2(instance_size >> 8);
diff --git a/src/wasm/wasm-objects.cc b/src/wasm/wasm-objects.cc
index 5dd491d0819..21098a21053 100644
--- a/src/wasm/wasm-objects.cc
+++ b/src/wasm/wasm-objects.cc
@@ -271,7 +271,7 @@ int WasmTableObject::Grow(Isolate* isolate, Handle<WasmTableObject> table,
 
   uint32_t new_size = old_size + count;
   // Even with 2x over-allocation, there should not be an integer overflow.
-  STATIC_ASSERT(wasm::kV8MaxWasmTableSize <= kMaxInt / 2);
+  static_assert(wasm::kV8MaxWasmTableSize <= kMaxInt / 2);
   DCHECK_GE(kMaxInt, new_size);
   int old_capacity = table->entries().length();
   if (new_size > static_cast<uint32_t>(old_capacity)) {
diff --git a/src/wasm/wasm-objects.h b/src/wasm/wasm-objects.h
index 7cfd82f323b..4d0abec3eea 100644
--- a/src/wasm/wasm-objects.h
+++ b/src/wasm/wasm-objects.h
@@ -418,13 +418,13 @@ class V8_EXPORT_PRIVATE WasmInstanceObject : public JSObject {
 
   DEFINE_FIELD_OFFSET_CONSTANTS(JSObject::kHeaderSize,
                                 WASM_INSTANCE_OBJECT_FIELDS)
-  STATIC_ASSERT(IsAligned(kHeaderSize, kTaggedSize));
+  static_assert(IsAligned(kHeaderSize, kTaggedSize));
   // TODO(ishell, v8:8875): When pointer compression is enabled 8-byte size
   // fields (external pointers, doubles and BigInt data) are only kTaggedSize
   // aligned so checking for alignments of fields bigger than kTaggedSize
   // doesn't make sense until v8:8875 is fixed.
 #define ASSERT_FIELD_ALIGNED(offset, size)                                 \
-  STATIC_ASSERT(size == 0 || IsAligned(offset, size) ||                    \
+  static_assert(size == 0 || IsAligned(offset, size) ||                    \
                 (COMPRESS_POINTERS_BOOL && (size == kSystemPointerSize) && \
                  IsAligned(offset, kTaggedSize)));
   WASM_INSTANCE_OBJECT_FIELDS(ASSERT_FIELD_ALIGNED)
@@ -687,7 +687,7 @@ class WasmIndirectFunctionTable
 
   DECL_PRINTER(WasmIndirectFunctionTable)
 
-  STATIC_ASSERT(kStartOfStrongFieldsOffset == kManagedNativeAllocationsOffset);
+  static_assert(kStartOfStrongFieldsOffset == kManagedNativeAllocationsOffset);
   using BodyDescriptor = FlexibleBodyDescriptor<kStartOfStrongFieldsOffset>;
 
   TQ_OBJECT_CONSTRUCTORS(WasmIndirectFunctionTable)
diff --git a/src/wasm/wasm-value.h b/src/wasm/wasm-value.h
index a27905b6cda..b049ce392f1 100644
--- a/src/wasm/wasm-value.h
+++ b/src/wasm/wasm-value.h
@@ -144,8 +144,8 @@ class WasmValue {
   }
 
   void CopyTo(byte* to) const {
-    STATIC_ASSERT(sizeof(float) == sizeof(Float32));
-    STATIC_ASSERT(sizeof(double) == sizeof(Float64));
+    static_assert(sizeof(float) == sizeof(Float32));
+    static_assert(sizeof(double) == sizeof(Float64));
     DCHECK(type_.is_numeric());
     memcpy(to, bit_pattern_, type_.value_kind_size());
   }
diff --git a/src/web-snapshot/web-snapshot.cc b/src/web-snapshot/web-snapshot.cc
index cc39897535a..8260289a30d 100644
--- a/src/web-snapshot/web-snapshot.cc
+++ b/src/web-snapshot/web-snapshot.cc
@@ -59,7 +59,7 @@ void WebSnapshotSerializerDeserializer::IterateBuiltinObjects(
   func(*factory()->NewStringFromAsciiChecked("Object.prototype"),
        isolate_->context().initial_object_prototype());
 
-  STATIC_ASSERT(kBuiltinObjectCount == 4);
+  static_assert(kBuiltinObjectCount == 4);
 }
 
 uint32_t WebSnapshotSerializerDeserializer::FunctionKindToFunctionFlags(
@@ -1870,7 +1870,7 @@ void WebSnapshotDeserializer::DeserializeStrings() {
     Throw("Malformed string table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   strings_handle_ = factory()->NewFixedArray(string_count_);
   strings_ = *strings_handle_;
   for (uint32_t i = 0; i < string_count_; ++i) {
@@ -1934,7 +1934,7 @@ void WebSnapshotDeserializer::DeserializeSymbols() {
     Throw("Malformed symbol table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   symbols_handle_ = factory()->NewFixedArray(symbol_count_);
   symbols_ = *symbols_handle_;
   for (uint32_t i = 0; i < symbol_count_; ++i) {
@@ -1971,7 +1971,7 @@ void WebSnapshotDeserializer::DeserializeMaps() {
     Throw("Malformed shape table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   maps_handle_ = factory()->NewFixedArray(map_count_);
   maps_ = *maps_handle_;
   for (uint32_t i = 0; i < map_count_; ++i) {
@@ -2045,7 +2045,7 @@ void WebSnapshotDeserializer::DeserializeBuiltinObjects() {
     Throw("Malformed builtin object table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   builtin_objects_handle_ = factory()->NewFixedArray(builtin_object_count_);
   builtin_objects_ = *builtin_objects_handle_;
   for (uint32_t i = 0; i < builtin_object_count_; ++i) {
@@ -2062,7 +2062,7 @@ void WebSnapshotDeserializer::DeserializeContexts() {
     Throw("Malformed context table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   contexts_handle_ = factory()->NewFixedArray(context_count_);
   contexts_ = *contexts_handle_;
   for (uint32_t i = 0; i < context_count_; ++i) {
@@ -2298,7 +2298,7 @@ void WebSnapshotDeserializer::DeserializeFunctions() {
     Throw("Malformed function table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount + 1 <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount + 1 <= FixedArray::kMaxLength);
   functions_handle_ = factory()->NewFixedArray(function_count_);
   functions_ = *functions_handle_;
 
@@ -2366,7 +2366,7 @@ void WebSnapshotDeserializer::DeserializeClasses() {
     Throw("Malformed class table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount + 1 <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount + 1 <= FixedArray::kMaxLength);
   classes_handle_ = factory()->NewFixedArray(class_count_);
   classes_ = *classes_handle_;
 
@@ -2520,7 +2520,7 @@ void WebSnapshotDeserializer::DeserializeObjects() {
     Throw("Malformed objects table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   objects_handle_ = factory()->NewFixedArray(object_count_);
   objects_ = *objects_handle_;
   for (; current_object_count_ < object_count_; ++current_object_count_) {
@@ -2730,7 +2730,7 @@ void WebSnapshotDeserializer::DeserializeArrays() {
     Throw("Malformed array table");
     return;
   }
-  STATIC_ASSERT(kMaxItemCount <= FixedArray::kMaxLength);
+  static_assert(kMaxItemCount <= FixedArray::kMaxLength);
   arrays_handle_ = factory()->NewFixedArray(array_count_);
   arrays_ = *arrays_handle_;
   for (; current_array_count_ < array_count_; ++current_array_count_) {
diff --git a/src/web-snapshot/web-snapshot.h b/src/web-snapshot/web-snapshot.h
index 1c7e12e25af..0f9e418e7f6 100644
--- a/src/web-snapshot/web-snapshot.h
+++ b/src/web-snapshot/web-snapshot.h
@@ -89,7 +89,7 @@ class WebSnapshotSerializerDeserializer {
       static_cast<uint32_t>(FixedArray::kMaxLength - 1);
   // This ensures indices and lengths can be converted between uint32_t and int
   // without problems:
-  STATIC_ASSERT(kMaxItemCount < std::numeric_limits<int32_t>::max());
+  static_assert(kMaxItemCount < std::numeric_limits<int32_t>::max());
 
  protected:
   explicit WebSnapshotSerializerDeserializer(Isolate* isolate)
diff --git a/src/zone/zone-chunk-list.h b/src/zone/zone-chunk-list.h
index 486e143d7ae..0aeaf09eb55 100644
--- a/src/zone/zone-chunk-list.h
+++ b/src/zone/zone-chunk-list.h
@@ -110,7 +110,7 @@ class ZoneChunkList : public ZoneObject {
 
   static constexpr uint32_t kMaxChunkCapacity = 256u;
 
-  STATIC_ASSERT(kMaxChunkCapacity == static_cast<uint32_t>(StartMode::kBig));
+  static_assert(kMaxChunkCapacity == static_cast<uint32_t>(StartMode::kBig));
 
   struct Chunk {
     uint32_t capacity_ = 0;
diff --git a/src/zone/zone-handle-set.h b/src/zone/zone-handle-set.h
index f8929037cd0..60865108f65 100644
--- a/src/zone/zone-handle-set.h
+++ b/src/zone/zone-handle-set.h
@@ -176,7 +176,7 @@ class ZoneHandleSet final {
     kTagMask = 3
   };
 
-  STATIC_ASSERT(kTagMask < kPointerAlignment);
+  static_assert(kTagMask < kPointerAlignment);
 
   Address data_;
 };
diff --git a/test/cctest/compiler/test-atomic-load-store-codegen.cc b/test/cctest/compiler/test-atomic-load-store-codegen.cc
index f3f710f0c9e..9352e855bd2 100644
--- a/test/cctest/compiler/test-atomic-load-store-codegen.cc
+++ b/test/cctest/compiler/test-atomic-load-store-codegen.cc
@@ -254,7 +254,7 @@ void AtomicStoreTagged(MachineType type, AtomicMemoryOrder order) {
   uintptr_t zap_data[] = {kZapValue, kZapValue};
   TaggedT zap_value;
 
-  STATIC_ASSERT(sizeof(TaggedT) <= sizeof(zap_data));
+  static_assert(sizeof(TaggedT) <= sizeof(zap_data));
   MemCopy(&zap_value, &zap_data, sizeof(TaggedT));
   InitBuffer(in_buffer, kNumElems, type);
 
diff --git a/test/cctest/compiler/test-codegen.h b/test/cctest/compiler/test-codegen.h
index b939c753e67..cf9c0a2c57f 100644
--- a/test/cctest/compiler/test-codegen.h
+++ b/test/cctest/compiler/test-codegen.h
@@ -153,7 +153,7 @@ class BufferedRawMachineAssemblerTester
   ReturnType Call(Params... p) {
     uintptr_t zap_data[] = {kZapValue, kZapValue};
     ReturnType return_value;
-    STATIC_ASSERT(sizeof(return_value) <= sizeof(zap_data));
+    static_assert(sizeof(return_value) <= sizeof(zap_data));
     MemCopy(&return_value, &zap_data, sizeof(return_value));
     CSignature::VerifyParams<Params...>(test_graph_signature_);
     CallHelper<int32_t>::Call(reinterpret_cast<void*>(&p)...,
diff --git a/test/cctest/compiler/test-run-load-store.cc b/test/cctest/compiler/test-run-load-store.cc
index 1fd163080d6..49b7f24774f 100644
--- a/test/cctest/compiler/test-run-load-store.cc
+++ b/test/cctest/compiler/test-run-load-store.cc
@@ -298,7 +298,7 @@ void RunLoadStore(MachineType type, TestAlignment t) {
   uintptr_t zap_data[] = {kZapValue, kZapValue};
   CType zap_value;
 
-  STATIC_ASSERT(sizeof(CType) <= sizeof(zap_data));
+  static_assert(sizeof(CType) <= sizeof(zap_data));
   MemCopy(&zap_value, &zap_data, sizeof(CType));
   InitBuffer(in_buffer, kNumElems, type);
 
diff --git a/test/cctest/interpreter/test-bytecode-generator.cc b/test/cctest/interpreter/test-bytecode-generator.cc
index d8c13d57fd7..aad52a81e5c 100644
--- a/test/cctest/interpreter/test-bytecode-generator.cc
+++ b/test/cctest/interpreter/test-bytecode-generator.cc
@@ -1530,7 +1530,7 @@ TEST(ContextVariables) {
   // The wide check below relies on MIN_CONTEXT_SLOTS + 3 + 250 == 256, if this
   // ever changes, the REPEAT_XXX should be changed to output the correct number
   // of unique variables to trigger the wide slot load / store.
-  STATIC_ASSERT(Context::MIN_CONTEXT_EXTENDED_SLOTS + 3 + 250 == 256);
+  static_assert(Context::MIN_CONTEXT_EXTENDED_SLOTS + 3 + 250 == 256);
 
   // For historical reasons, this test expects the first unique identifier
   // to be 896.
diff --git a/test/cctest/scope-test-helper.h b/test/cctest/scope-test-helper.h
index c733f9dbf53..b59ad296d1c 100644
--- a/test/cctest/scope-test-helper.h
+++ b/test/cctest/scope-test-helper.h
@@ -75,7 +75,7 @@ class ScopeTestHelper {
     if (precise_maybe_assigned) {
       CHECK_EQ(scope_local->maybe_assigned(), baseline_local->maybe_assigned());
     } else {
-      STATIC_ASSERT(kMaybeAssigned > kNotAssigned);
+      static_assert(kMaybeAssigned > kNotAssigned);
       CHECK_GE(scope_local->maybe_assigned(), baseline_local->maybe_assigned());
     }
   }
diff --git a/test/cctest/test-api.cc b/test/cctest/test-api.cc
index a65f4254527..c7d875a11c0 100644
--- a/test/cctest/test-api.cc
+++ b/test/cctest/test-api.cc
@@ -21146,8 +21146,8 @@ TEST(AccessCheckThrows) {
   CheckCorrectThrow("%GetProperty(other, 'x')");
   CheckCorrectThrow("%SetKeyedProperty(other, 'x', 'foo')");
   CheckCorrectThrow("%SetNamedProperty(other, 'y', 'foo')");
-  STATIC_ASSERT(static_cast<int>(i::LanguageMode::kSloppy) == 0);
-  STATIC_ASSERT(static_cast<int>(i::LanguageMode::kStrict) == 1);
+  static_assert(static_cast<int>(i::LanguageMode::kSloppy) == 0);
+  static_assert(static_cast<int>(i::LanguageMode::kStrict) == 1);
   CheckCorrectThrow("%DeleteProperty(other, 'x', 0)");  // 0 == SLOPPY
   CheckCorrectThrow("%DeleteProperty(other, 'x', 1)");  // 1 == STRICT
   CheckCorrectThrow("%DeleteProperty(other, '1', 0)");
@@ -21176,7 +21176,7 @@ const uint16_t kTwoByteSubjectString[] = {
     'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '\0'};
 
 const int kSubjectStringLength = arraysize(kOneByteSubjectString) - 1;
-STATIC_ASSERT(arraysize(kOneByteSubjectString) ==
+static_assert(arraysize(kOneByteSubjectString) ==
               arraysize(kTwoByteSubjectString));
 
 OneByteVectorResource one_byte_string_resource(v8::base::Vector<const char>(
diff --git a/test/cctest/test-assembler-arm.cc b/test/cctest/test-assembler-arm.cc
index bdad7948337..9dc2bad84fb 100644
--- a/test/cctest/test-assembler-arm.cc
+++ b/test/cctest/test-assembler-arm.cc
@@ -3369,7 +3369,7 @@ TEST(ARMv8_vsel) {
 #endif
     auto f = GeneratedCode<F_ippii>::FromCode(*code);
 
-    STATIC_ASSERT(kResultPass == -kResultFail);
+    static_assert(kResultPass == -kResultFail);
 #define CHECK_VSEL(n, z, c, v, vseleq, vselge, vselgt, vselvs)     \
   do {                                                             \
     ResultsF32 results_f32;                                        \
diff --git a/test/cctest/test-assembler-arm64.cc b/test/cctest/test-assembler-arm64.cc
index 2311c5e10db..71698569606 100644
--- a/test/cctest/test-assembler-arm64.cc
+++ b/test/cctest/test-assembler-arm64.cc
@@ -14985,7 +14985,7 @@ TEST(jump_tables_forward) {
   Label done;
 
   const Register& index = x0;
-  STATIC_ASSERT(sizeof(results[0]) == 4);
+  static_assert(sizeof(results[0]) == 4);
   const Register& value = w1;
   const Register& target = x2;
 
@@ -15046,7 +15046,7 @@ TEST(jump_tables_backward) {
   Label done;
 
   const Register& index = x0;
-  STATIC_ASSERT(sizeof(results[0]) == 4);
+  static_assert(sizeof(results[0]) == 4);
   const Register& value = w1;
   const Register& target = x2;
 
diff --git a/test/cctest/test-assembler-ia32.cc b/test/cctest/test-assembler-ia32.cc
index 131a52a396c..cea8e682e8f 100644
--- a/test/cctest/test-assembler-ia32.cc
+++ b/test/cctest/test-assembler-ia32.cc
@@ -1528,7 +1528,7 @@ TEST(DeoptExitSizeIsFixed) {
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       ExternalAssemblerBuffer(buffer, sizeof(buffer)));
 
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-code-stub-assembler.cc b/test/cctest/test-code-stub-assembler.cc
index 801c9d64765..61d18fb2b76 100644
--- a/test/cctest/test-code-stub-assembler.cc
+++ b/test/cctest/test-code-stub-assembler.cc
@@ -237,7 +237,7 @@ TEST(ToUint32) {
   };
   // clang-format on
 
-  STATIC_ASSERT(arraysize(inputs) == arraysize(expectations));
+  static_assert(arraysize(inputs) == arraysize(expectations));
 
   const int test_count = arraysize(inputs);
   for (int i = 0; i < test_count; i++) {
@@ -1061,7 +1061,7 @@ TEST(TransitionLookup) {
   Handle<Object> expect_not_found(Smi::FromInt(kNotFound), isolate);
 
   const int ATTRS_COUNT = (READ_ONLY | DONT_ENUM | DONT_DELETE) + 1;
-  STATIC_ASSERT(ATTRS_COUNT == 8);
+  static_assert(ATTRS_COUNT == 8);
 
   const int kKeysCount = 300;
   Handle<Map> root_map = Map::Create(isolate, 0);
@@ -1457,7 +1457,7 @@ TEST(TryGetOwnProperty) {
           factory->NewFunctionForTesting(factory->empty_string())),
       factory->NewPrivateSymbol(),
   };
-  STATIC_ASSERT(arraysize(values) < arraysize(names));
+  static_assert(arraysize(values) < arraysize(names));
 
   base::RandomNumberGenerator rand_gen(FLAG_random_seed);
 
diff --git a/test/cctest/test-descriptor-array.cc b/test/cctest/test-descriptor-array.cc
index 80d7e7a9427..47daff78eff 100644
--- a/test/cctest/test-descriptor-array.cc
+++ b/test/cctest/test-descriptor-array.cc
@@ -187,8 +187,8 @@ Handle<JSFunction> CreateCsaTransitionArrayLookup(Isolate* isolate) {
 
     m.BIND(&if_found);
     {
-      STATIC_ASSERT(static_cast<int>(PropertyKind::kData) == 0);
-      STATIC_ASSERT(NONE == 0);
+      static_assert(static_cast<int>(PropertyKind::kData) == 0);
+      static_assert(NONE == 0);
       const int kKeyToTargetOffset = (TransitionArray::kEntryTargetIndex -
                                       TransitionArray::kEntryKeyIndex) *
                                      kTaggedSize;
diff --git a/test/cctest/test-factory.cc b/test/cctest/test-factory.cc
index 97a4b5b959d..1e0f3b758d1 100644
--- a/test/cctest/test-factory.cc
+++ b/test/cctest/test-factory.cc
@@ -20,7 +20,7 @@ namespace {
 // This needs to be large enough to create a new nosnap Isolate, but smaller
 // than kMaximalCodeRangeSize so we can recover from the OOM.
 constexpr int kInstructionSize = 100 * MB;
-STATIC_ASSERT(kInstructionSize < kMaximalCodeRangeSize ||
+static_assert(kInstructionSize < kMaximalCodeRangeSize ||
               !kPlatformRequiresCodeRange);
 
 size_t NearHeapLimitCallback(void* raw_bool, size_t current_heap_limit,
diff --git a/test/cctest/test-field-type-tracking.cc b/test/cctest/test-field-type-tracking.cc
index 6efee779b76..d20eaa7ceb9 100644
--- a/test/cctest/test-field-type-tracking.cc
+++ b/test/cctest/test-field-type-tracking.cc
@@ -778,7 +778,7 @@ void TestGeneralizeField(const CRFTData& from, const CRFTData& to,
                          ChangeAlertMechanism expected_alert) {
   // Check the cases when the map being reconfigured is a part of the
   // transition tree.
-  STATIC_ASSERT(kPropCount > 4);
+  static_assert(kPropCount > 4);
   int indices[] = {0, 2, kPropCount - 1};
   for (int i = 0; i < static_cast<int>(arraysize(indices)); i++) {
     TestGeneralizeField(-1, indices[i], from, to, expected, expected_alert);
@@ -3011,7 +3011,7 @@ TEST(NormalizeToMigrationTarget) {
 }
 
 TEST(RepresentationPredicatesAreInSync) {
-  STATIC_ASSERT(Representation::kNumRepresentations == 6);
+  static_assert(Representation::kNumRepresentations == 6);
   static Representation reps[] = {
       Representation::None(),   Representation::Smi(),
       Representation::Double(), Representation::HeapObject(),
diff --git a/test/cctest/test-macro-assembler-arm.cc b/test/cctest/test-macro-assembler-arm.cc
index 23b5b2d4e85..26875793784 100644
--- a/test/cctest/test-macro-assembler-arm.cc
+++ b/test/cctest/test-macro-assembler-arm.cc
@@ -317,7 +317,7 @@ TEST(DeoptExitSizeIsFixed) {
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
 
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-arm64.cc b/test/cctest/test-macro-assembler-arm64.cc
index 4d37a6ac2a7..f43fad69965 100644
--- a/test/cctest/test-macro-assembler-arm64.cc
+++ b/test/cctest/test-macro-assembler-arm64.cc
@@ -104,7 +104,7 @@ TEST(DeoptExitSizeIsFixed) {
 
   AssemblerBufferWriteScope rw_scope(*buffer);
 
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-loong64.cc b/test/cctest/test-macro-assembler-loong64.cc
index 8c2005a0792..fb2295f496a 100644
--- a/test/cctest/test-macro-assembler-loong64.cc
+++ b/test/cctest/test-macro-assembler-loong64.cc
@@ -2896,7 +2896,7 @@ TEST(DeoptExitSizeIsFixed) {
   auto buffer = AllocateAssemblerBuffer();
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-mips.cc b/test/cctest/test-macro-assembler-mips.cc
index b74f40f8f52..11df7c410ba 100644
--- a/test/cctest/test-macro-assembler-mips.cc
+++ b/test/cctest/test-macro-assembler-mips.cc
@@ -1354,7 +1354,7 @@ TEST(DeoptExitSizeIsFixed) {
   auto buffer = AllocateAssemblerBuffer();
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-mips64.cc b/test/cctest/test-macro-assembler-mips64.cc
index f24f07af08c..56ecb3354da 100644
--- a/test/cctest/test-macro-assembler-mips64.cc
+++ b/test/cctest/test-macro-assembler-mips64.cc
@@ -1707,7 +1707,7 @@ TEST(DeoptExitSizeIsFixed) {
   auto buffer = AllocateAssemblerBuffer();
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-riscv64.cc b/test/cctest/test-macro-assembler-riscv64.cc
index 7148ac344c7..2226da7ecdc 100644
--- a/test/cctest/test-macro-assembler-riscv64.cc
+++ b/test/cctest/test-macro-assembler-riscv64.cc
@@ -1535,7 +1535,7 @@ TEST(DeoptExitSizeIsFixed) {
   auto buffer = AllocateAssemblerBuffer();
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-macro-assembler-x64.cc b/test/cctest/test-macro-assembler-x64.cc
index 1bc04263e8d..c60bc10982f 100644
--- a/test/cctest/test-macro-assembler-x64.cc
+++ b/test/cctest/test-macro-assembler-x64.cc
@@ -1056,7 +1056,7 @@ TEST(DeoptExitSizeIsFixed) {
   MacroAssembler masm(isolate, v8::internal::CodeObjectRequired::kYes,
                       buffer->CreateView());
 
-  STATIC_ASSERT(static_cast<int>(kFirstDeoptimizeKind) == 0);
+  static_assert(static_cast<int>(kFirstDeoptimizeKind) == 0);
   for (int i = 0; i < kDeoptimizeKindCount; i++) {
     DeoptimizeKind kind = static_cast<DeoptimizeKind>(i);
     Label before_exit;
diff --git a/test/cctest/test-parsing.cc b/test/cctest/test-parsing.cc
index 504012044fc..513cbebf926 100644
--- a/test/cctest/test-parsing.cc
+++ b/test/cctest/test-parsing.cc
@@ -3727,7 +3727,7 @@ static void TestMaybeAssigned(Input input, const char* variable, bool module,
 
   CHECK_NOT_NULL(var);
   CHECK_IMPLIES(input.assigned, var->is_used());
-  STATIC_ASSERT(true == i::kMaybeAssigned);
+  static_assert(true == i::kMaybeAssigned);
   CHECK_EQ(input.assigned, var->maybe_assigned() == i::kMaybeAssigned);
 }
 
diff --git a/test/cctest/test-strings.cc b/test/cctest/test-strings.cc
index ca27b79071a..b29a8a06071 100644
--- a/test/cctest/test-strings.cc
+++ b/test/cctest/test-strings.cc
@@ -1657,7 +1657,7 @@ TEST(InvalidExternalString) {
     CcTest::InitializeVM();                                              \
     LocalContext context;                                                \
     Isolate* isolate = CcTest::i_isolate();                              \
-    STATIC_ASSERT(String::kMaxLength < kMaxInt);                         \
+    static_assert(String::kMaxLength < kMaxInt);                         \
     static const int invalid = String::kMaxLength + 1;                   \
     HandleScope scope(isolate);                                          \
     v8::base::Vector<TYPE> dummy = v8::base::Vector<TYPE>::New(invalid); \
diff --git a/test/cctest/test-swiss-name-dictionary-csa.cc b/test/cctest/test-swiss-name-dictionary-csa.cc
index abd3580a973..5a9e00d9743 100644
--- a/test/cctest/test-swiss-name-dictionary-csa.cc
+++ b/test/cctest/test-swiss-name-dictionary-csa.cc
@@ -269,7 +269,7 @@ Handle<Code> CSATestRunner::create_find_entry(Isolate* isolate) {
     return FromCodeT(isolate->builtins()->code_handle(Builtin::kIllegal),
                      isolate);
   }
-  STATIC_ASSERT(kFindEntryParams == 2);  // (table, key)
+  static_assert(kFindEntryParams == 2);  // (table, key)
   compiler::CodeAssemblerTester asm_tester(isolate, kFindEntryParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -292,7 +292,7 @@ Handle<Code> CSATestRunner::create_find_entry(Isolate* isolate) {
 }
 
 Handle<Code> CSATestRunner::create_get_data(Isolate* isolate) {
-  STATIC_ASSERT(kGetDataParams == 2);  // (table, entry)
+  static_assert(kGetDataParams == 2);  // (table, entry)
   compiler::CodeAssemblerTester asm_tester(isolate, kGetDataParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -315,7 +315,7 @@ Handle<Code> CSATestRunner::create_get_data(Isolate* isolate) {
 }
 
 Handle<Code> CSATestRunner::create_put(Isolate* isolate) {
-  STATIC_ASSERT(kPutParams == 4);  // (table, entry, value, details)
+  static_assert(kPutParams == 4);  // (table, entry, value, details)
   compiler::CodeAssemblerTester asm_tester(isolate, kPutParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -342,7 +342,7 @@ Handle<Code> CSATestRunner::create_delete(Isolate* isolate) {
     return FromCodeT(isolate->builtins()->code_handle(Builtin::kIllegal),
                      isolate);
   }
-  STATIC_ASSERT(kDeleteParams == 2);  // (table, entry)
+  static_assert(kDeleteParams == 2);  // (table, entry)
   compiler::CodeAssemblerTester asm_tester(isolate, kDeleteParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -368,7 +368,7 @@ Handle<Code> CSATestRunner::create_add(Isolate* isolate) {
     return FromCodeT(isolate->builtins()->code_handle(Builtin::kIllegal),
                      isolate);
   }
-  STATIC_ASSERT(kAddParams == 4);  // (table, key, value, details)
+  static_assert(kAddParams == 4);  // (table, key, value, details)
   compiler::CodeAssemblerTester asm_tester(isolate, kAddParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -392,7 +392,7 @@ Handle<Code> CSATestRunner::create_add(Isolate* isolate) {
 }
 
 Handle<Code> CSATestRunner::create_allocate(Isolate* isolate) {
-  STATIC_ASSERT(kAllocateParams == 1);  // (capacity)
+  static_assert(kAllocateParams == 1);  // (capacity)
   compiler::CodeAssemblerTester asm_tester(isolate, kAllocateParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -407,7 +407,7 @@ Handle<Code> CSATestRunner::create_allocate(Isolate* isolate) {
 }
 
 Handle<Code> CSATestRunner::create_get_counts(Isolate* isolate) {
-  STATIC_ASSERT(kGetCountsParams == 1);  // (table)
+  static_assert(kGetCountsParams == 1);  // (table)
   compiler::CodeAssemblerTester asm_tester(isolate, kGetCountsParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
@@ -440,7 +440,7 @@ Handle<Code> CSATestRunner::create_get_counts(Isolate* isolate) {
 }
 
 Handle<Code> CSATestRunner::create_copy(Isolate* isolate) {
-  STATIC_ASSERT(kCopyParams == 1);  // (table)
+  static_assert(kCopyParams == 1);  // (table)
   compiler::CodeAssemblerTester asm_tester(isolate, kCopyParams + 1);
   CodeStubAssembler m(asm_tester.state());
   {
diff --git a/test/cctest/test-swiss-name-dictionary-shared-tests.h b/test/cctest/test-swiss-name-dictionary-shared-tests.h
index c36c65aa49b..9d8affca272 100644
--- a/test/cctest/test-swiss-name-dictionary-shared-tests.h
+++ b/test/cctest/test-swiss-name-dictionary-shared-tests.h
@@ -37,7 +37,7 @@ extern const char kCSATestFileName[];
 // they were directly written within bar.cc.
 template <typename TestRunner, char const* kTestFileName>
 struct SharedSwissTableTests {
-  STATIC_ASSERT((std::is_same<TestRunner, RuntimeTestRunner>::value) ||
+  static_assert((std::is_same<TestRunner, RuntimeTestRunner>::value) ||
                 (std::is_same<TestRunner, CSATestRunner>::value));
 
   SharedSwissTableTests() {
@@ -57,7 +57,7 @@ struct SharedSwissTableTests {
   // effects. Note that using just this value itself as an H1 value means that a
   // key will (try to) occupy bucket 0.
   static const int kBigModulus = (1 << 22);
-  STATIC_ASSERT(SwissNameDictionary::IsValidCapacity(kBigModulus));
+  static_assert(SwissNameDictionary::IsValidCapacity(kBigModulus));
 
   // Returns elements from TS::distinct_property_details in a determinstic
   // order. Subsequent calls with increasing |index| (and the same |offset|)
diff --git a/test/cctest/test-transitions.cc b/test/cctest/test-transitions.cc
index d9f1fc9fef6..c575b5d8e91 100644
--- a/test/cctest/test-transitions.cc
+++ b/test/cctest/test-transitions.cc
@@ -205,7 +205,7 @@ TEST(TransitionArray_SameFieldNamesDifferentAttributesSimple) {
   CHECK(map0->raw_transitions()->IsSmi());
 
   const int ATTRS_COUNT = (READ_ONLY | DONT_ENUM | DONT_DELETE) + 1;
-  STATIC_ASSERT(ATTRS_COUNT == 8);
+  static_assert(ATTRS_COUNT == 8);
   Handle<Map> attr_maps[ATTRS_COUNT];
   Handle<String> name = factory->InternalizeUtf8String("foo");
 
@@ -268,7 +268,7 @@ TEST(TransitionArray_SameFieldNamesDifferentAttributes) {
   }
 
   const int ATTRS_COUNT = (READ_ONLY | DONT_ENUM | DONT_DELETE) + 1;
-  STATIC_ASSERT(ATTRS_COUNT == 8);
+  static_assert(ATTRS_COUNT == 8);
   Handle<Map> attr_maps[ATTRS_COUNT];
   Handle<String> name = factory->InternalizeUtf8String("foo");
 
diff --git a/test/cctest/test-utils.cc b/test/cctest/test-utils.cc
index d204b19744d..b06669a9944 100644
--- a/test/cctest/test-utils.cc
+++ b/test/cctest/test-utils.cc
@@ -181,7 +181,7 @@ TEST(MemMove) {
   static const int kMinOffset = 32;
   static const int kMaxOffset = 64;
   static const int kMaxLength = 128;
-  STATIC_ASSERT(kMaxOffset + kMaxLength < kAreaSize);
+  static_assert(kMaxOffset + kMaxLength < kAreaSize);
 
   for (int src_offset = kMinOffset; src_offset <= kMaxOffset; src_offset++) {
     for (int dst_offset = kMinOffset; dst_offset <= kMaxOffset; dst_offset++) {
diff --git a/test/cctest/wasm/test-jump-table-assembler.cc b/test/cctest/wasm/test-jump-table-assembler.cc
index bb3bf15714c..9294c13474b 100644
--- a/test/cctest/wasm/test-jump-table-assembler.cc
+++ b/test/cctest/wasm/test-jump-table-assembler.cc
@@ -247,7 +247,7 @@ TEST(JumpTablePatchingStress) {
   constexpr int kNumberOfRunnerThreads = 5;
   constexpr int kNumberOfPatcherThreads = 3;
 
-  STATIC_ASSERT(kAssemblerBufferSize >= kJumpTableSize);
+  static_assert(kAssemblerBufferSize >= kJumpTableSize);
   auto buffer = AllocateAssemblerBuffer(kAssemblerBufferSize, nullptr,
                                         JitPermission::kMapAsJittable);
   byte* thunk_slot_buffer = buffer->start() + kBufferSlotStartOffset;
diff --git a/test/fuzzer/regexp-builtins.cc b/test/fuzzer/regexp-builtins.cc
index 5989f641e9a..f7722fee931 100644
--- a/test/fuzzer/regexp-builtins.cc
+++ b/test/fuzzer/regexp-builtins.cc
@@ -186,7 +186,7 @@ std::string PickRandomPresetPattern(FuzzerArgs* args) {
       "\\p{Changes_When_NFKC_Casefolded}",
   };
   static constexpr int preset_pattern_count = arraysize(preset_patterns);
-  STATIC_ASSERT(preset_pattern_count < 0xFF);
+  static_assert(preset_pattern_count < 0xFF);
 
   return std::string(preset_patterns[RandomByte(args) % preset_pattern_count]);
 }
@@ -250,7 +250,7 @@ std::string GenerateRandomFlags(FuzzerArgs* args) {
   CHECK_EQ(JSRegExp::kHasIndices, 1 << (kFlagCount - 1));
   CHECK_EQ(JSRegExp::kLinear, 1 << (kFlagCount - 2));
   CHECK_EQ(JSRegExp::kDotAll, 1 << (kFlagCount - 3));
-  STATIC_ASSERT((1 << kFlagCount) - 1 <= 0xFF);
+  static_assert((1 << kFlagCount) - 1 <= 0xFF);
 
   const size_t flags = RandomByte(args) & ((1 << kFlagCount) - 1);
 
diff --git a/test/fuzzer/wasm-compile.cc b/test/fuzzer/wasm-compile.cc
index dcc6430d075..56bcaa5b68e 100644
--- a/test/fuzzer/wasm-compile.cc
+++ b/test/fuzzer/wasm-compile.cc
@@ -76,7 +76,7 @@ class DataRange {
   T get() {
     // Bool needs special handling (see template specialization below).
     static_assert(!std::is_same<T, bool>::value, "bool needs special handling");
-    STATIC_ASSERT(max_bytes <= sizeof(T));
+    static_assert(max_bytes <= sizeof(T));
     // We want to support the case where we have less than sizeof(T) bytes
     // remaining in the slice. For example, if we emit an i32 constant, it's
     // okay if we don't have a full four bytes available, we'll just use what
@@ -712,7 +712,7 @@ class WasmGenerator {
 
   template <ValueKind wanted_kind>
   void local_op(DataRange* data, WasmOpcode opcode) {
-    STATIC_ASSERT(wanted_kind == kVoid || is_convertible_kind(wanted_kind));
+    static_assert(wanted_kind == kVoid || is_convertible_kind(wanted_kind));
     Var local = GetRandomLocal(data);
     // If there are no locals and no parameters, just generate any value (if a
     // value is needed), or do nothing.
@@ -766,7 +766,7 @@ class WasmGenerator {
 
   template <ValueKind wanted_kind>
   void global_op(DataRange* data) {
-    STATIC_ASSERT(wanted_kind == kVoid || is_convertible_kind(wanted_kind));
+    static_assert(wanted_kind == kVoid || is_convertible_kind(wanted_kind));
     constexpr bool is_set = wanted_kind == kVoid;
     Var global = GetRandomGlobal(data, is_set);
     // If there are no globals, just generate any value (if a value is needed),
diff --git a/test/unittests/base/vector-unittest.cc b/test/unittests/base/vector-unittest.cc
index 8984e678bdb..302cd5aeb5a 100644
--- a/test/unittests/base/vector-unittest.cc
+++ b/test/unittests/base/vector-unittest.cc
@@ -81,16 +81,16 @@ TEST(OwnedVectorConstruction, Equals) {
 TEST(VectorTest, ConstexprFactories) {
   static constexpr int kInit1[] = {4, 11, 3};
   static constexpr auto kVec1 = base::ArrayVector(kInit1);
-  STATIC_ASSERT(kVec1.size() == 3);
+  static_assert(kVec1.size() == 3);
   EXPECT_THAT(kVec1, testing::ElementsAreArray(kInit1));
 
   static constexpr auto kVec2 = base::VectorOf(kInit1, 2);
-  STATIC_ASSERT(kVec2.size() == 2);
+  static_assert(kVec2.size() == 2);
   EXPECT_THAT(kVec2, testing::ElementsAre(4, 11));
 
   static constexpr const char kInit3[] = "foobar";
   static constexpr auto kVec3 = base::StaticCharVector(kInit3);
-  STATIC_ASSERT(kVec3.size() == 6);
+  static_assert(kVec3.size() == 6);
   EXPECT_THAT(kVec3, testing::ElementsAreArray(kInit3, kInit3 + 6));
 }
 
diff --git a/test/unittests/heap/cppgc/garbage-collected-unittest.cc b/test/unittests/heap/cppgc/garbage-collected-unittest.cc
index c80baa48ead..02f18cfafdc 100644
--- a/test/unittests/heap/cppgc/garbage-collected-unittest.cc
+++ b/test/unittests/heap/cppgc/garbage-collected-unittest.cc
@@ -45,43 +45,43 @@ class GarbageCollectedTestWithHeap
 }  // namespace
 
 TEST(GarbageCollectedTest, GarbageCollectedTrait) {
-  STATIC_ASSERT(!IsGarbageCollectedTypeV<int>);
-  STATIC_ASSERT(!IsGarbageCollectedTypeV<NotGCed>);
-  STATIC_ASSERT(IsGarbageCollectedTypeV<GCed>);
-  STATIC_ASSERT(!IsGarbageCollectedTypeV<Mixin>);
-  STATIC_ASSERT(IsGarbageCollectedTypeV<GCedWithMixin>);
-  STATIC_ASSERT(!IsGarbageCollectedTypeV<MergedMixins>);
-  STATIC_ASSERT(IsGarbageCollectedTypeV<GCWithMergedMixins>);
+  static_assert(!IsGarbageCollectedTypeV<int>);
+  static_assert(!IsGarbageCollectedTypeV<NotGCed>);
+  static_assert(IsGarbageCollectedTypeV<GCed>);
+  static_assert(!IsGarbageCollectedTypeV<Mixin>);
+  static_assert(IsGarbageCollectedTypeV<GCedWithMixin>);
+  static_assert(!IsGarbageCollectedTypeV<MergedMixins>);
+  static_assert(IsGarbageCollectedTypeV<GCWithMergedMixins>);
 }
 
 TEST(GarbageCollectedTest, GarbageCollectedMixinTrait) {
-  STATIC_ASSERT(!IsGarbageCollectedMixinTypeV<int>);
-  STATIC_ASSERT(!IsGarbageCollectedMixinTypeV<GCed>);
-  STATIC_ASSERT(!IsGarbageCollectedMixinTypeV<NotGCed>);
-  STATIC_ASSERT(IsGarbageCollectedMixinTypeV<Mixin>);
-  STATIC_ASSERT(!IsGarbageCollectedMixinTypeV<GCedWithMixin>);
-  STATIC_ASSERT(IsGarbageCollectedMixinTypeV<MergedMixins>);
-  STATIC_ASSERT(!IsGarbageCollectedMixinTypeV<GCWithMergedMixins>);
+  static_assert(!IsGarbageCollectedMixinTypeV<int>);
+  static_assert(!IsGarbageCollectedMixinTypeV<GCed>);
+  static_assert(!IsGarbageCollectedMixinTypeV<NotGCed>);
+  static_assert(IsGarbageCollectedMixinTypeV<Mixin>);
+  static_assert(!IsGarbageCollectedMixinTypeV<GCedWithMixin>);
+  static_assert(IsGarbageCollectedMixinTypeV<MergedMixins>);
+  static_assert(!IsGarbageCollectedMixinTypeV<GCWithMergedMixins>);
 }
 
 TEST(GarbageCollectedTest, GarbageCollectedOrMixinTrait) {
-  STATIC_ASSERT(!IsGarbageCollectedOrMixinTypeV<int>);
-  STATIC_ASSERT(IsGarbageCollectedOrMixinTypeV<GCed>);
-  STATIC_ASSERT(!IsGarbageCollectedOrMixinTypeV<NotGCed>);
-  STATIC_ASSERT(IsGarbageCollectedOrMixinTypeV<Mixin>);
-  STATIC_ASSERT(IsGarbageCollectedOrMixinTypeV<GCedWithMixin>);
-  STATIC_ASSERT(IsGarbageCollectedOrMixinTypeV<MergedMixins>);
-  STATIC_ASSERT(IsGarbageCollectedOrMixinTypeV<GCWithMergedMixins>);
+  static_assert(!IsGarbageCollectedOrMixinTypeV<int>);
+  static_assert(IsGarbageCollectedOrMixinTypeV<GCed>);
+  static_assert(!IsGarbageCollectedOrMixinTypeV<NotGCed>);
+  static_assert(IsGarbageCollectedOrMixinTypeV<Mixin>);
+  static_assert(IsGarbageCollectedOrMixinTypeV<GCedWithMixin>);
+  static_assert(IsGarbageCollectedOrMixinTypeV<MergedMixins>);
+  static_assert(IsGarbageCollectedOrMixinTypeV<GCWithMergedMixins>);
 }
 
 TEST(GarbageCollectedTest, GarbageCollectedWithMixinTrait) {
-  STATIC_ASSERT(!IsGarbageCollectedWithMixinTypeV<int>);
-  STATIC_ASSERT(!IsGarbageCollectedWithMixinTypeV<GCed>);
-  STATIC_ASSERT(!IsGarbageCollectedWithMixinTypeV<NotGCed>);
-  STATIC_ASSERT(!IsGarbageCollectedWithMixinTypeV<Mixin>);
-  STATIC_ASSERT(IsGarbageCollectedWithMixinTypeV<GCedWithMixin>);
-  STATIC_ASSERT(!IsGarbageCollectedWithMixinTypeV<MergedMixins>);
-  STATIC_ASSERT(IsGarbageCollectedWithMixinTypeV<GCWithMergedMixins>);
+  static_assert(!IsGarbageCollectedWithMixinTypeV<int>);
+  static_assert(!IsGarbageCollectedWithMixinTypeV<GCed>);
+  static_assert(!IsGarbageCollectedWithMixinTypeV<NotGCed>);
+  static_assert(!IsGarbageCollectedWithMixinTypeV<Mixin>);
+  static_assert(IsGarbageCollectedWithMixinTypeV<GCedWithMixin>);
+  static_assert(!IsGarbageCollectedWithMixinTypeV<MergedMixins>);
+  static_assert(IsGarbageCollectedWithMixinTypeV<GCWithMergedMixins>);
 }
 
 namespace {
@@ -91,8 +91,8 @@ class ForwardDeclaredType;
 }  // namespace
 
 TEST(GarbageCollectedTest, CompleteTypeTrait) {
-  STATIC_ASSERT(IsCompleteV<GCed>);
-  STATIC_ASSERT(!IsCompleteV<ForwardDeclaredType>);
+  static_assert(IsCompleteV<GCed>);
+  static_assert(!IsCompleteV<ForwardDeclaredType>);
 }
 
 TEST_F(GarbageCollectedTestWithHeap, GetObjectStartReturnsCurrentAddress) {
diff --git a/test/unittests/objects/object-unittest.cc b/test/unittests/objects/object-unittest.cc
index fff9c84ea48..84a7013a66b 100644
--- a/test/unittests/objects/object-unittest.cc
+++ b/test/unittests/objects/object-unittest.cc
@@ -23,7 +23,7 @@ namespace {
 bool IsInStringInstanceTypeList(InstanceType instance_type) {
   switch (instance_type) {
 #define ASSERT_INSTANCE_TYPE(type, ...) \
-  STATIC_ASSERT(InstanceType::type < InstanceType::FIRST_NONSTRING_TYPE);
+  static_assert(InstanceType::type < InstanceType::FIRST_NONSTRING_TYPE);
     STRING_TYPE_LIST(ASSERT_INSTANCE_TYPE)
 #undef ASSERT_INSTANCE_TYPE
 #define TEST_INSTANCE_TYPE(type, ...) case InstanceType::type:
@@ -108,7 +108,7 @@ TEST_F(ObjectWithIsolate, DictionaryGrowth) {
 
   // This test documents the expected growth behavior of a dictionary getting
   // elements added to it one by one.
-  STATIC_ASSERT(HashTableBase::kMinCapacity == 4);
+  static_assert(HashTableBase::kMinCapacity == 4);
   uint32_t i = 1;
   // 3 elements fit into the initial capacity.
   for (; i <= 3; i++) {
diff --git a/test/unittests/wasm/liftoff-register-unittests.cc b/test/unittests/wasm/liftoff-register-unittests.cc
index 8ab3d500c14..e2ebb50dc12 100644
--- a/test/unittests/wasm/liftoff-register-unittests.cc
+++ b/test/unittests/wasm/liftoff-register-unittests.cc
@@ -33,10 +33,10 @@ namespace wasm {
 
 // The registers used by Liftoff and the registers spilled by the
 // WasmDebugBreak builtin should match.
-STATIC_ASSERT(kLiftoffAssemblerGpCacheRegs ==
+static_assert(kLiftoffAssemblerGpCacheRegs ==
               WasmDebugBreakFrameConstants::kPushedGpRegs);
 
-STATIC_ASSERT(kLiftoffAssemblerFpCacheRegs ==
+static_assert(kLiftoffAssemblerFpCacheRegs ==
               WasmDebugBreakFrameConstants::kPushedFpRegs);
 }  // namespace wasm
 }  // namespace internal
diff --git a/test/unittests/wasm/module-decoder-unittest.cc b/test/unittests/wasm/module-decoder-unittest.cc
index ef6cc8bcd8e..a866b5bad4c 100644
--- a/test/unittests/wasm/module-decoder-unittest.cc
+++ b/test/unittests/wasm/module-decoder-unittest.cc
@@ -152,7 +152,7 @@ namespace module_decoder_unittest {
 
 #define EXPECT_OFF_END_FAILURE(data, min)                              \
   do {                                                                 \
-    STATIC_ASSERT(min < arraysize(data));                              \
+    static_assert(min < arraysize(data));                              \
     for (size_t _length = min; _length < arraysize(data); _length++) { \
       EXPECT_FAILURE_LEN(data, _length);                               \
     }                                                                  \
@@ -1354,7 +1354,7 @@ TEST_F(WasmModuleVerifyTest, TagSectionBeforeMemory) {
 }
 
 TEST_F(WasmModuleVerifyTest, TagSectionAfterTableBeforeMemory) {
-  STATIC_ASSERT(kMemorySectionCode + 1 == kGlobalSectionCode);
+  static_assert(kMemorySectionCode + 1 == kGlobalSectionCode);
   static const byte data[] = {SECTION(Table, ENTRY_COUNT(0)),
                               SECTION(Tag, ENTRY_COUNT(0)),
                               SECTION(Memory, ENTRY_COUNT(0))};
@@ -3373,7 +3373,7 @@ TEST_F(WasmModuleVerifyTest, DataCountSectionBeforeElement) {
 }
 
 TEST_F(WasmModuleVerifyTest, DataCountSectionAfterStartBeforeElement) {
-  STATIC_ASSERT(kStartSectionCode + 1 == kElementSectionCode);
+  static_assert(kStartSectionCode + 1 == kElementSectionCode);
   static const byte data[] = {
       // We need the start section for this test, but the start section must
       // reference a valid function, which requires the type and function
diff --git a/tools/debug_helper/get-object-properties.cc b/tools/debug_helper/get-object-properties.cc
index 43a67941ac6..2c37251dc4a 100644
--- a/tools/debug_helper/get-object-properties.cc
+++ b/tools/debug_helper/get-object-properties.cc
@@ -257,7 +257,7 @@ class ReadStringVisitor : public TqObjectVisitor {
   void ReadStringCharacters(const TqString* object, uintptr_t data_address) {
     int32_t length = GetOrFinish(object->GetLengthValue(accessor_));
     for (; index_ < length && index_ < limit_ && !done_; ++index_) {
-      STATIC_ASSERT(sizeof(TChar) <= sizeof(char16_t));
+      static_assert(sizeof(TChar) <= sizeof(char16_t));
       char16_t c = static_cast<char16_t>(
           GetOrFinish(ReadCharacter<TChar>(data_address, index_)));
       if (!done_) AddCharacter(c);
-- 
2.35.1

