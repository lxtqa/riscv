From 3c2714d57370a05ea5ce99d79d92bf4980bb5d39 Mon Sep 17 00:00:00 2001
From: Nico Hartmann <nicohartmann@chromium.org>
Date: Tue, 20 Jun 2023 11:19:51 +0200
Subject: [PATCH] [compiler] Generalize InstructionSelectorT for Turboshaft
 (part 1)

Bug: v8:12783
Change-Id: Iefdef25326a855987eb0ef1a454c0157cd48e9c0
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4580211
Commit-Queue: Nico Hartmann <nicohartmann@chromium.org>
Reviewed-by: Darius Mercadier <dmercadier@chromium.org>
Cr-Commit-Position: refs/heads/main@{#88391}
---
 BUILD.bazel                                   |    3 +
 BUILD.gn                                      |    4 +
 .../backend/arm/instruction-selector-arm.cc   |  793 +--
 .../arm64/instruction-selector-arm64.cc       | 1120 ++--
 src/compiler/backend/frame-elider.cc          |   10 +-
 .../backend/ia32/instruction-selector-ia32.cc |  733 +--
 .../backend/instruction-selector-adapter.h    |  468 ++
 .../backend/instruction-selector-impl.h       |  276 +-
 src/compiler/backend/instruction-selector.cc  | 4547 ++++++++++-------
 src/compiler/backend/instruction-selector.h   |  559 +-
 src/compiler/backend/instruction.cc           |   90 +-
 src/compiler/backend/instruction.h            |   10 +
 .../loong64/instruction-selector-loong64.cc   |    6 +-
 .../mips64/instruction-selector-mips64.cc     |    6 +-
 .../backend/ppc/instruction-selector-ppc.cc   |    9 +-
 .../riscv/instruction-selector-riscv.h        |    3 +-
 .../riscv/instruction-selector-riscv32.cc     |    3 +-
 .../riscv/instruction-selector-riscv64.cc     |    3 +-
 .../backend/s390/instruction-selector-s390.cc |    8 +-
 .../backend/x64/instruction-selector-x64.cc   |  996 ++--
 src/compiler/node.h                           |   11 +
 src/compiler/pipeline.cc                      |  234 +-
 src/compiler/pipeline.h                       |    7 +
 src/compiler/turboshaft/graph.h               |    3 +-
 .../turboshaft/instruction-selection-phase.cc |   55 +
 .../turboshaft/instruction-selection-phase.h  |   26 +
 src/compiler/turboshaft/operations.h          |    4 +
 src/compiler/turboshaft/phase.h               |   51 +-
 src/logging/runtime-call-stats.h              |    1 +
 .../compiler/test-instruction-scheduler.cc    |    3 +
 .../backend/instruction-selector-unittest.cc  |    2 +-
 31 files changed, 6093 insertions(+), 3951 deletions(-)
 create mode 100644 src/compiler/backend/instruction-selector-adapter.h
 create mode 100644 src/compiler/turboshaft/instruction-selection-phase.cc
 create mode 100644 src/compiler/turboshaft/instruction-selection-phase.h

diff --git a/BUILD.bazel b/BUILD.bazel
index a0d4733c4a0..0cc51c88458 100644
--- a/BUILD.bazel
+++ b/BUILD.bazel
@@ -2724,6 +2724,7 @@ filegroup(
         "src/compiler/backend/instruction-scheduler.h",
         "src/compiler/backend/instruction-selector.cc",
         "src/compiler/backend/instruction-selector.h",
+        "src/compiler/backend/instruction-selector-adapter.h",
         "src/compiler/backend/instruction-selector-impl.h",
         "src/compiler/backend/jump-threading.cc",
         "src/compiler/backend/jump-threading.h",
@@ -2955,6 +2956,8 @@ filegroup(
         "src/compiler/turboshaft/graph-visualizer.cc",
         "src/compiler/turboshaft/graph-visualizer.h",
         "src/compiler/turboshaft/index.h",
+        "src/compiler/turboshaft/instruction-selection-phase.cc",
+        "src/compiler/turboshaft/instruction-selection-phase.h",
         "src/compiler/turboshaft/late-escape-analysis-reducer.cc",
         "src/compiler/turboshaft/late-escape-analysis-reducer.h",
         "src/compiler/turboshaft/layered-hash-map.h",
diff --git a/BUILD.gn b/BUILD.gn
index 4be669c3458..80916e19de4 100644
--- a/BUILD.gn
+++ b/BUILD.gn
@@ -1444,6 +1444,7 @@ config("toolchain") {
 
   if (is_win) {
     cflags += [
+      "/wd4172",  # Returning address of local variable.
       "/wd4245",  # Conversion with signed/unsigned mismatch.
       "/wd4267",  # Conversion with possible loss of data.
       "/wd4324",  # Padding structure due to alignment.
@@ -3099,6 +3100,7 @@ v8_header_set("v8_internal_headers") {
     "src/compiler/backend/gap-resolver.h",
     "src/compiler/backend/instruction-codes.h",
     "src/compiler/backend/instruction-scheduler.h",
+    "src/compiler/backend/instruction-selector-adapter.h",
     "src/compiler/backend/instruction-selector-impl.h",
     "src/compiler/backend/instruction-selector.h",
     "src/compiler/backend/instruction.h",
@@ -3228,6 +3230,7 @@ v8_header_set("v8_internal_headers") {
     "src/compiler/turboshaft/graph-visualizer.h",
     "src/compiler/turboshaft/graph.h",
     "src/compiler/turboshaft/index.h",
+    "src/compiler/turboshaft/instruction-selection-phase.h",
     "src/compiler/turboshaft/late-escape-analysis-reducer.h",
     "src/compiler/turboshaft/layered-hash-map.h",
     "src/compiler/turboshaft/machine-lowering-phase.h",
@@ -4710,6 +4713,7 @@ v8_source_set("v8_turboshaft") {
     "src/compiler/turboshaft/graph-builder.cc",
     "src/compiler/turboshaft/graph-visualizer.cc",
     "src/compiler/turboshaft/graph.cc",
+    "src/compiler/turboshaft/instruction-selection-phase.cc",
     "src/compiler/turboshaft/late-escape-analysis-reducer.cc",
     "src/compiler/turboshaft/machine-lowering-phase.cc",
     "src/compiler/turboshaft/memory-optimization-reducer.cc",
diff --git a/src/compiler/backend/arm/instruction-selector-arm.cc b/src/compiler/backend/arm/instruction-selector-arm.cc
index 7fd1224818a..7799fbfeb45 100644
--- a/src/compiler/backend/arm/instruction-selector-arm.cc
+++ b/src/compiler/backend/arm/instruction-selector-arm.cc
@@ -5,10 +5,13 @@
 #include "src/base/bits.h"
 #include "src/base/enum-set.h"
 #include "src/base/iterator.h"
+#include "src/base/logging.h"
 #include "src/codegen/machine-type.h"
+#include "src/compiler/backend/instruction-selector-adapter.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
+#include "src/compiler/turboshaft/operations.h"
 
 namespace v8 {
 namespace internal {
@@ -285,7 +288,7 @@ bool TryMatchImmediateOrShift(InstructionSelectorT<Adapter>* selector,
 template <typename Adapter>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, InstructionCode reverse_opcode,
-                FlagsContinuation* cont) {
+                FlagsContinuationT<Adapter>* cont) {
   ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand inputs[3];
@@ -336,7 +339,7 @@ void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, InstructionCode reverse_opcode) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitBinop(selector, node, opcode, reverse_opcode, &cont);
 }
 
@@ -414,10 +417,16 @@ void EmitAddBeforeS128LoadStore(InstructionSelectorT<Adapter>* selector,
   inputs[0] = addr;
 }
 
-template <typename Adapter>
-void EmitLoad(InstructionSelectorT<Adapter>* selector, InstructionCode opcode,
-              InstructionOperand* output, Node* base, Node* index) {
-  ArmOperandGeneratorT<Adapter> g(selector);
+void EmitLoad(InstructionSelectorT<TurboshaftAdapter>* selector,
+              InstructionCode opcode, InstructionOperand* output,
+              turboshaft::OpIndex base, turboshaft::OpIndex index) {
+  UNIMPLEMENTED();
+}
+
+void EmitLoad(InstructionSelectorT<TurbofanAdapter>* selector,
+              InstructionCode opcode, InstructionOperand* output, Node* base,
+              Node* index) {
+  ArmOperandGeneratorT<TurbofanAdapter> g(selector);
   InstructionOperand inputs[3];
   size_t input_count = 2;
 
@@ -641,11 +650,12 @@ void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
-  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
+  typename Adapter::LoadView load = this->load_view(node);
+  LoadRepresentation load_rep = load.loaded_rep();
   ArmOperandGeneratorT<Adapter> g(this);
-  Node* base = node->InputAt(0);
-  Node* index = node->InputAt(1);
+  node_t base = load.base();
+  node_t index = load.index();
 
   InstructionCode opcode = kArchNop;
   switch (load_rep.representation()) {
@@ -854,67 +864,71 @@ void InstructionSelectorT<Adapter>::VisitProtectedStore(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
-  MachineRepresentation load_rep =
-      LoadRepresentationOf(node->op()).representation();
-  ArmOperandGeneratorT<Adapter> g(this);
-  Node* base = node->InputAt(0);
-  Node* index = node->InputAt(1);
-
-  InstructionCode opcode = kArmLdr;
-  // Only floating point loads need to be specially handled; integer loads
-  // support unaligned access. We support unaligned FP loads by loading to
-  // integer registers first, then moving to the destination FP register. If
-  // NEON is supported, we use the vld1.8 instruction.
-  switch (load_rep) {
-    case MachineRepresentation::kFloat32: {
-      InstructionOperand temp = g.TempRegister();
-      EmitLoad(this, opcode, &temp, base, index);
-      Emit(kArmVmovF32U32, g.DefineAsRegister(node), temp);
-      return;
-    }
-    case MachineRepresentation::kFloat64: {
-      // Compute the address of the least-significant byte of the FP value.
-      // We assume that the base node is unlikely to be an encodable immediate
-      // or the result of a shift operation, so only consider the addressing
-      // mode that should be used for the index node.
-      InstructionCode add_opcode = kArmAdd;
-      InstructionOperand inputs[3];
-      inputs[0] = g.UseRegister(base);
-
-      size_t input_count;
-      if (TryMatchImmediateOrShift(this, &add_opcode, index, &input_count,
-                                   &inputs[1])) {
-        // input_count has been set by TryMatchImmediateOrShift(), so
-        // increment it to account for the base register in inputs[0].
-        input_count++;
-      } else {
-        add_opcode |= AddressingModeField::encode(kMode_Operand2_R);
-        inputs[1] = g.UseRegister(index);
-        input_count = 2;  // Base register and index.
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    MachineRepresentation load_rep =
+        LoadRepresentationOf(node->op()).representation();
+    ArmOperandGeneratorT<Adapter> g(this);
+    Node* base = node->InputAt(0);
+    Node* index = node->InputAt(1);
+
+    InstructionCode opcode = kArmLdr;
+    // Only floating point loads need to be specially handled; integer loads
+    // support unaligned access. We support unaligned FP loads by loading to
+    // integer registers first, then moving to the destination FP register. If
+    // NEON is supported, we use the vld1.8 instruction.
+    switch (load_rep) {
+      case MachineRepresentation::kFloat32: {
+        InstructionOperand temp = g.TempRegister();
+        EmitLoad(this, opcode, &temp, base, index);
+        Emit(kArmVmovF32U32, g.DefineAsRegister(node), temp);
+        return;
       }
+      case MachineRepresentation::kFloat64: {
+        // Compute the address of the least-significant byte of the FP value.
+        // We assume that the base node is unlikely to be an encodable immediate
+        // or the result of a shift operation, so only consider the addressing
+        // mode that should be used for the index node.
+        InstructionCode add_opcode = kArmAdd;
+        InstructionOperand inputs[3];
+        inputs[0] = g.UseRegister(base);
+
+        size_t input_count;
+        if (TryMatchImmediateOrShift(this, &add_opcode, index, &input_count,
+                                     &inputs[1])) {
+          // input_count has been set by TryMatchImmediateOrShift(), so
+          // increment it to account for the base register in inputs[0].
+          input_count++;
+        } else {
+          add_opcode |= AddressingModeField::encode(kMode_Operand2_R);
+          inputs[1] = g.UseRegister(index);
+          input_count = 2;  // Base register and index.
+        }
 
-      InstructionOperand addr = g.TempRegister();
-      Emit(add_opcode, 1, &addr, input_count, inputs);
-
-      if (CpuFeatures::IsSupported(NEON)) {
-        // With NEON we can load directly from the calculated address.
-        InstructionCode op = kArmVld1F64;
-        op |= AddressingModeField::encode(kMode_Operand2_R);
-        Emit(op, g.DefineAsRegister(node), addr);
-      } else {
-        // Load both halves and move to an FP register.
-        InstructionOperand fp_lo = g.TempRegister();
-        InstructionOperand fp_hi = g.TempRegister();
-        opcode |= AddressingModeField::encode(kMode_Offset_RI);
-        Emit(opcode, fp_lo, addr, g.TempImmediate(0));
-        Emit(opcode, fp_hi, addr, g.TempImmediate(4));
-        Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), fp_lo, fp_hi);
+        InstructionOperand addr = g.TempRegister();
+        Emit(add_opcode, 1, &addr, input_count, inputs);
+
+        if (CpuFeatures::IsSupported(NEON)) {
+          // With NEON we can load directly from the calculated address.
+          InstructionCode op = kArmVld1F64;
+          op |= AddressingModeField::encode(kMode_Operand2_R);
+          Emit(op, g.DefineAsRegister(node), addr);
+        } else {
+          // Load both halves and move to an FP register.
+          InstructionOperand fp_lo = g.TempRegister();
+          InstructionOperand fp_hi = g.TempRegister();
+          opcode |= AddressingModeField::encode(kMode_Offset_RI);
+          Emit(opcode, fp_lo, addr, g.TempImmediate(0));
+          Emit(opcode, fp_hi, addr, g.TempImmediate(4));
+          Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), fp_lo, fp_hi);
+        }
+        return;
       }
-      return;
+      default:
+        // All other cases should support unaligned accesses.
+        UNREACHABLE();
     }
-    default:
-      // All other cases should support unaligned accesses.
-      UNREACHABLE();
   }
 }
 
@@ -1162,8 +1176,20 @@ void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
-    Node* node, FlagsContinuation* cont) {
-  StackCheckKind kind = StackCheckKindOf(node->op());
+    node_t node, FlagsContinuation* cont) {
+  StackCheckKind kind;
+  node_t value;
+  if constexpr (Adapter::IsTurboshaft) {
+    const auto& op =
+        this->turboshaft_graph()
+            ->Get(node)
+            .template Cast<turboshaft::StackPointerGreaterThanOp>();
+    kind = op.kind;
+    value = op.stack_limit();
+  } else {
+    kind = StackCheckKindOf(node->op());
+    value = node->InputAt(0);
+  }
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
@@ -1182,7 +1208,6 @@ void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
                                  ? OperandGenerator::kUniqueRegister
                                  : OperandGenerator::kRegister;
 
-  Node* const value = node->InputAt(0);
   InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};
   static constexpr int input_count = arraysize(inputs);
 
@@ -1194,7 +1219,8 @@ namespace {
 
 template <typename Adapter, typename TryMatchShift>
 void VisitShift(InstructionSelectorT<Adapter>* selector, Node* node,
-                TryMatchShift try_match_shift, FlagsContinuation* cont) {
+                TryMatchShift try_match_shift,
+                FlagsContinuationT<Adapter>* cont) {
   ArmOperandGeneratorT<Adapter> g(selector);
   InstructionCode opcode = kArmMov;
   InstructionOperand inputs[2];
@@ -1219,7 +1245,7 @@ void VisitShift(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter, typename TryMatchShift>
 void VisitShift(InstructionSelectorT<Adapter>* selector, Node* node,
                 TryMatchShift try_match_shift) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitShift(selector, node, try_match_shift, &cont);
 }
 
@@ -1586,7 +1612,7 @@ namespace {
 
 template <typename Adapter>
 void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
-                              Node* node, FlagsContinuation* cont) {
+                              Node* node, FlagsContinuationT<Adapter>* cont) {
   ArmOperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand result_operand = g.DefineAsRegister(node);
@@ -1838,7 +1864,8 @@ void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(node_t node, int index) {
+}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
@@ -1847,34 +1874,38 @@ void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
-    Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
-
-  // Prepare for C function call.
-  if (call_descriptor->IsCFunctionCall()) {
-    Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
-                                         call_descriptor->ParameterCount())),
-         0, nullptr, 0, nullptr);
-
-    // Poke any stack arguments.
-    for (size_t n = 0; n < arguments->size(); ++n) {
-      PushParameter input = (*arguments)[n];
-      if (input.node) {
-        int slot = static_cast<int>(n);
-        Emit(kArmPoke | MiscField::encode(slot), g.NoOutput(),
-             g.UseRegister(input.node));
-      }
-    }
+    node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    // Push any stack arguments.
-    int stack_decrement = 0;
-    for (PushParameter input : base::Reversed(*arguments)) {
-      stack_decrement += kSystemPointerSize;
-      // Skip any alignment holes in pushed nodes.
-      if (input.node == nullptr) continue;
-      InstructionOperand decrement = g.UseImmediate(stack_decrement);
-      stack_decrement = 0;
-      Emit(kArmPush, g.NoOutput(), decrement, g.UseRegister(input.node));
+    ArmOperandGeneratorT<Adapter> g(this);
+
+    // Prepare for C function call.
+    if (call_descriptor->IsCFunctionCall()) {
+      Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
+                                           call_descriptor->ParameterCount())),
+           0, nullptr, 0, nullptr);
+
+      // Poke any stack arguments.
+      for (size_t n = 0; n < arguments->size(); ++n) {
+        PushParameter input = (*arguments)[n];
+        if (input.node) {
+          int slot = static_cast<int>(n);
+          Emit(kArmPoke | MiscField::encode(slot), g.NoOutput(),
+               g.UseRegister(input.node));
+        }
+      }
+    } else {
+      // Push any stack arguments.
+      int stack_decrement = 0;
+      for (PushParameter input : base::Reversed(*arguments)) {
+        stack_decrement += kSystemPointerSize;
+        // Skip any alignment holes in pushed nodes.
+        if (input.node == nullptr) continue;
+        InstructionOperand decrement = g.UseImmediate(stack_decrement);
+        stack_decrement = 0;
+        Emit(kArmPush, g.NoOutput(), decrement, g.UseRegister(input.node));
+      }
     }
   }
 }
@@ -1882,25 +1913,29 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
-
-  for (PushParameter output : *results) {
-    if (!output.location.IsCallerFrameSlot()) continue;
-    // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
-      DCHECK(!call_descriptor->IsCFunctionCall());
-      if (output.location.GetType() == MachineType::Float32()) {
-        MarkAsFloat32(output.node);
-      } else if (output.location.GetType() == MachineType::Float64()) {
-        MarkAsFloat64(output.node);
-      } else if (output.location.GetType() == MachineType::Simd128()) {
-        MarkAsSimd128(output.node);
+    node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    ArmOperandGeneratorT<Adapter> g(this);
+
+    for (PushParameter output : *results) {
+      if (!output.location.IsCallerFrameSlot()) continue;
+      // Skip any alignment holes in nodes.
+      if (output.node != nullptr) {
+        DCHECK(!call_descriptor->IsCFunctionCall());
+        if (output.location.GetType() == MachineType::Float32()) {
+          MarkAsFloat32(output.node);
+        } else if (output.location.GetType() == MachineType::Float64()) {
+          MarkAsFloat64(output.node);
+        } else if (output.location.GetType() == MachineType::Simd128()) {
+          MarkAsSimd128(output.node);
+        }
+        int offset = call_descriptor->GetOffsetToReturns();
+        int reverse_slot = -output.location.GetLocation() - offset;
+        Emit(kArmPeek, g.DefineAsRegister(output.node),
+             g.UseImmediate(reverse_slot));
       }
-      int offset = call_descriptor->GetOffsetToReturns();
-      int reverse_slot = -output.location.GetLocation() - offset;
-      Emit(kArmPeek, g.DefineAsRegister(output.node),
-           g.UseImmediate(reverse_slot));
     }
   }
 }
@@ -1916,45 +1951,55 @@ namespace {
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
                   InstructionCode opcode, InstructionOperand left,
-                  InstructionOperand right, FlagsContinuation* cont) {
+                  InstructionOperand right, FlagsContinuationT<Adapter>* cont) {
   selector->EmitWithContinuation(opcode, left, right, cont);
 }
 
 // Shared routine for multiple float32 compare operations.
 template <typename Adapter>
-void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  ArmOperandGeneratorT<Adapter> g(selector);
-  Float32BinopMatcher m(node);
-  if (m.right().Is(0.0f)) {
-    VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.left().node()),
-                 g.UseImmediate(m.right().node()), cont);
-  } else if (m.left().Is(0.0f)) {
-    cont->Commute();
-    VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.right().node()),
-                 g.UseImmediate(m.left().node()), cont);
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.left().node()),
-                 g.UseRegister(m.right().node()), cont);
+    ArmOperandGeneratorT<Adapter> g(selector);
+    Float32BinopMatcher m(node);
+    if (m.right().Is(0.0f)) {
+      VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.left().node()),
+                   g.UseImmediate(m.right().node()), cont);
+    } else if (m.left().Is(0.0f)) {
+      cont->Commute();
+      VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.right().node()),
+                   g.UseImmediate(m.left().node()), cont);
+    } else {
+      VisitCompare(selector, kArmVcmpF32, g.UseRegister(m.left().node()),
+                   g.UseRegister(m.right().node()), cont);
+    }
   }
 }
 
 // Shared routine for multiple float64 compare operations.
 template <typename Adapter>
-void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  ArmOperandGeneratorT<Adapter> g(selector);
-  Float64BinopMatcher m(node);
-  if (m.right().Is(0.0)) {
-    VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.left().node()),
-                 g.UseImmediate(m.right().node()), cont);
-  } else if (m.left().Is(0.0)) {
-    cont->Commute();
-    VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.right().node()),
-                 g.UseImmediate(m.left().node()), cont);
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.left().node()),
-                 g.UseRegister(m.right().node()), cont);
+    ArmOperandGeneratorT<Adapter> g(selector);
+    Float64BinopMatcher m(node);
+    if (m.right().Is(0.0)) {
+      VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.left().node()),
+                   g.UseImmediate(m.right().node()), cont);
+    } else if (m.left().Is(0.0)) {
+      cont->Commute();
+      VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.right().node()),
+                   g.UseImmediate(m.left().node()), cont);
+    } else {
+      VisitCompare(selector, kArmVcmpF64, g.UseRegister(m.left().node()),
+                   g.UseRegister(m.right().node()), cont);
+    }
   }
 }
 
@@ -2013,7 +2058,8 @@ FlagsCondition MapForFlagSettingBinop(FlagsCondition cond) {
 template <typename Adapter>
 void MaybeReplaceCmpZeroWithFlagSettingBinop(
     InstructionSelectorT<Adapter>* selector, Node** node, Node* binop,
-    InstructionCode* opcode, FlagsCondition cond, FlagsContinuation* cont) {
+    InstructionCode* opcode, FlagsCondition cond,
+    FlagsContinuationT<Adapter>* cont) {
   InstructionCode binop_opcode;
   InstructionCode no_output_opcode;
   switch (binop->opcode()) {
@@ -2053,80 +2099,91 @@ void MaybeReplaceCmpZeroWithFlagSettingBinop(
 
 // Shared routine for multiple word compare operations.
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont) {
-  ArmOperandGeneratorT<Adapter> g(selector);
-  Int32BinopMatcher m(node);
-  InstructionOperand inputs[3];
-  size_t input_count = 0;
-  InstructionOperand outputs[2];
-  size_t output_count = 0;
-  bool has_result = (opcode != kArmCmp) && (opcode != kArmCmn) &&
-                    (opcode != kArmTst) && (opcode != kArmTeq);
-
-  if (TryMatchImmediateOrShift(selector, &opcode, m.right().node(),
-                               &input_count, &inputs[1])) {
-    inputs[0] = g.UseRegister(m.left().node());
-    input_count++;
-  } else if (TryMatchImmediateOrShift(selector, &opcode, m.left().node(),
-                                      &input_count, &inputs[1])) {
-    if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
-    inputs[0] = g.UseRegister(m.right().node());
-    input_count++;
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node, InstructionCode opcode,
+                      FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    opcode |= AddressingModeField::encode(kMode_Operand2_R);
-    inputs[input_count++] = g.UseRegister(m.left().node());
-    inputs[input_count++] = g.UseRegister(m.right().node());
-  }
-
-  if (has_result) {
-    if (cont->IsDeoptimize()) {
-      // If we can deoptimize as a result of the binop, we need to make sure
-      // that the deopt inputs are not overwritten by the binop result. One way
-      // to achieve that is to declare the output register as same-as-first.
-      outputs[output_count++] = g.DefineSameAsFirst(node);
+    ArmOperandGeneratorT<Adapter> g(selector);
+    Int32BinopMatcher m(node);
+    InstructionOperand inputs[3];
+    size_t input_count = 0;
+    InstructionOperand outputs[2];
+    size_t output_count = 0;
+    bool has_result = (opcode != kArmCmp) && (opcode != kArmCmn) &&
+                      (opcode != kArmTst) && (opcode != kArmTeq);
+
+    if (TryMatchImmediateOrShift(selector, &opcode, m.right().node(),
+                                 &input_count, &inputs[1])) {
+      inputs[0] = g.UseRegister(m.left().node());
+      input_count++;
+    } else if (TryMatchImmediateOrShift(selector, &opcode, m.left().node(),
+                                        &input_count, &inputs[1])) {
+      if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
+      inputs[0] = g.UseRegister(m.right().node());
+      input_count++;
     } else {
-      outputs[output_count++] = g.DefineAsRegister(node);
+      opcode |= AddressingModeField::encode(kMode_Operand2_R);
+      inputs[input_count++] = g.UseRegister(m.left().node());
+      inputs[input_count++] = g.UseRegister(m.right().node());
     }
-  }
 
-  DCHECK_NE(0u, input_count);
-  DCHECK_GE(arraysize(inputs), input_count);
-  DCHECK_GE(arraysize(outputs), output_count);
+    if (has_result) {
+      if (cont->IsDeoptimize()) {
+        // If we can deoptimize as a result of the binop, we need to make sure
+        // that the deopt inputs are not overwritten by the binop result. One
+        // way to achieve that is to declare the output register as
+        // same-as-first.
+        outputs[output_count++] = g.DefineSameAsFirst(node);
+      } else {
+        outputs[output_count++] = g.DefineAsRegister(node);
+      }
+    }
 
-  selector->EmitWithContinuation(opcode, output_count, outputs, input_count,
-                                 inputs, cont);
+    DCHECK_NE(0u, input_count);
+    DCHECK_GE(arraysize(inputs), input_count);
+    DCHECK_GE(arraysize(outputs), output_count);
+
+    selector->EmitWithContinuation(opcode, output_count, outputs, input_count,
+                                   inputs, cont);
+  }
 }
 
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      FlagsContinuation* cont) {
-  InstructionCode opcode = kArmCmp;
-  Int32BinopMatcher m(node);
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node,
+                      FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    InstructionCode opcode = kArmCmp;
+    Int32BinopMatcher m(node);
 
-  FlagsCondition cond = cont->condition();
-  if (m.right().Is(0) && (m.left().IsInt32Add() || m.left().IsWord32Or() ||
-                          m.left().IsWord32And() || m.left().IsWord32Xor())) {
-    // Emit flag setting instructions for comparisons against zero.
-    if (CanUseFlagSettingBinop(cond)) {
-      Node* binop = m.left().node();
-      MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
-                                              cond, cont);
-    }
-  } else if (m.left().Is(0) &&
-             (m.right().IsInt32Add() || m.right().IsWord32Or() ||
-              m.right().IsWord32And() || m.right().IsWord32Xor())) {
-    // Same as above, but we need to commute the condition before we
-    // continue with the rest of the checks.
-    cond = CommuteFlagsCondition(cond);
-    if (CanUseFlagSettingBinop(cond)) {
-      Node* binop = m.right().node();
-      MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
-                                              cond, cont);
+    FlagsCondition cond = cont->condition();
+    if (m.right().Is(0) && (m.left().IsInt32Add() || m.left().IsWord32Or() ||
+                            m.left().IsWord32And() || m.left().IsWord32Xor())) {
+      // Emit flag setting instructions for comparisons against zero.
+      if (CanUseFlagSettingBinop(cond)) {
+        Node* binop = m.left().node();
+        MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
+                                                cond, cont);
+      }
+    } else if (m.left().Is(0) &&
+               (m.right().IsInt32Add() || m.right().IsWord32Or() ||
+                m.right().IsWord32And() || m.right().IsWord32Xor())) {
+      // Same as above, but we need to commute the condition before we
+      // continue with the rest of the checks.
+      cond = CommuteFlagsCondition(cond);
+      if (CanUseFlagSettingBinop(cond)) {
+        Node* binop = m.right().node();
+        MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
+                                                cond, cont);
+      }
     }
-  }
 
-  VisitWordCompare(selector, node, opcode, cont);
+    VisitWordCompare(selector, node, opcode, cont);
+  }
 }
 
 }  // namespace
@@ -2134,120 +2191,125 @@ void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
 // Shared routine for word comparisons against zero.
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWordCompareZero(
-    Node* user, Node* value, FlagsContinuation* cont) {
-  // Try to combine with comparisons against 0 by simply inverting the branch.
-  while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
-    Int32BinopMatcher m(value);
-    if (!m.right().Is(0)) break;
-
-    user = value;
-    value = m.left().node();
-    cont->Negate();
-  }
-
-  if (CanCover(user, value)) {
-    switch (value->opcode()) {
-      case IrOpcode::kWord32Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kInt32LessThan:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kInt32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kUint32LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kUint32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kFloat32Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThan:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThan);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat64Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThan:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThan);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kProjection:
-        // Check if this is the overflow output projection of an
-        // <Operation>WithOverflow node.
-        if (ProjectionIndexOf(value->op()) == 1u) {
-          // We cannot combine the <Operation>WithOverflow with this branch
-          // unless the 0th projection (the use of the actual value of the
-          // <Operation> is either nullptr, which means there's no use of the
-          // actual value, or was already defined, which means it is scheduled
-          // *AFTER* this branch).
-          Node* const node = value->InputAt(0);
-          Node* const result = NodeProperties::FindProjection(node, 0);
-          if (!result || IsDefined(result)) {
-            switch (node->opcode()) {
-              case IrOpcode::kInt32AddWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop(this, node, kArmAdd, kArmAdd, cont);
-              case IrOpcode::kInt32SubWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop(this, node, kArmSub, kArmRsb, cont);
-              case IrOpcode::kInt32MulWithOverflow:
-                // ARM doesn't set the overflow flag for multiplication, so we
-                // need to test on kNotEqual. Here is the code sequence used:
-                //   smull resultlow, resulthigh, left, right
-                //   cmp resulthigh, Operand(resultlow, ASR, 31)
-                cont->OverwriteAndNegateIfEqual(kNotEqual);
-                return EmitInt32MulWithOverflow(this, node, cont);
-              default:
-                break;
+    node_t user, node_t value, FlagsContinuation* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    // Try to combine with comparisons against 0 by simply inverting the branch.
+    while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
+      Int32BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+
+      user = value;
+      value = m.left().node();
+      cont->Negate();
+    }
+
+    if (CanCover(user, value)) {
+      switch (value->opcode()) {
+        case IrOpcode::kWord32Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kInt32LessThan:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kInt32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kUint32LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kUint32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kFloat32Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThan:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThan);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat64Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThan:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThan);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kProjection:
+          // Check if this is the overflow output projection of an
+          // <Operation>WithOverflow node.
+          if (ProjectionIndexOf(value->op()) == 1u) {
+            // We cannot combine the <Operation>WithOverflow with this branch
+            // unless the 0th projection (the use of the actual value of the
+            // <Operation> is either nullptr, which means there's no use of the
+            // actual value, or was already defined, which means it is scheduled
+            // *AFTER* this branch).
+            Node* const node = value->InputAt(0);
+            Node* const result = NodeProperties::FindProjection(node, 0);
+            if (!result || IsDefined(result)) {
+              switch (node->opcode()) {
+                case IrOpcode::kInt32AddWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop(this, node, kArmAdd, kArmAdd, cont);
+                case IrOpcode::kInt32SubWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop(this, node, kArmSub, kArmRsb, cont);
+                case IrOpcode::kInt32MulWithOverflow:
+                  // ARM doesn't set the overflow flag for multiplication, so we
+                  // need to test on kNotEqual. Here is the code sequence used:
+                  //   smull resultlow, resulthigh, left, right
+                  //   cmp resulthigh, Operand(resultlow, ASR, 31)
+                  cont->OverwriteAndNegateIfEqual(kNotEqual);
+                  return EmitInt32MulWithOverflow(this, node, cont);
+                default:
+                  break;
+              }
             }
           }
-        }
-        break;
-      case IrOpcode::kInt32Add:
-        return VisitWordCompare(this, value, kArmCmn, cont);
-      case IrOpcode::kInt32Sub:
-        return VisitWordCompare(this, value, kArmCmp, cont);
-      case IrOpcode::kWord32And:
-        return VisitWordCompare(this, value, kArmTst, cont);
-      case IrOpcode::kWord32Or:
-        return VisitBinop(this, value, kArmOrr, kArmOrr, cont);
-      case IrOpcode::kWord32Xor:
-        return VisitWordCompare(this, value, kArmTeq, cont);
-      case IrOpcode::kWord32Sar:
-        return VisitShift(this, value, TryMatchASR<Adapter>, cont);
-      case IrOpcode::kWord32Shl:
-        return VisitShift(this, value, TryMatchLSL<Adapter>, cont);
-      case IrOpcode::kWord32Shr:
-        return VisitShift(this, value, TryMatchLSR<Adapter>, cont);
-      case IrOpcode::kWord32Ror:
-        return VisitShift(this, value, TryMatchROR<Adapter>, cont);
-      case IrOpcode::kStackPointerGreaterThan:
-        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
-        return VisitStackPointerGreaterThan(value, cont);
-      default:
-        break;
+          break;
+        case IrOpcode::kInt32Add:
+          return VisitWordCompare(this, value, kArmCmn, cont);
+        case IrOpcode::kInt32Sub:
+          return VisitWordCompare(this, value, kArmCmp, cont);
+        case IrOpcode::kWord32And:
+          return VisitWordCompare(this, value, kArmTst, cont);
+        case IrOpcode::kWord32Or:
+          return VisitBinop(this, value, kArmOrr, kArmOrr, cont);
+        case IrOpcode::kWord32Xor:
+          return VisitWordCompare(this, value, kArmTeq, cont);
+        case IrOpcode::kWord32Sar:
+          return VisitShift(this, value, TryMatchASR<Adapter>, cont);
+        case IrOpcode::kWord32Shl:
+          return VisitShift(this, value, TryMatchLSL<Adapter>, cont);
+        case IrOpcode::kWord32Shr:
+          return VisitShift(this, value, TryMatchLSR<Adapter>, cont);
+        case IrOpcode::kWord32Ror:
+          return VisitShift(this, value, TryMatchROR<Adapter>, cont);
+        case IrOpcode::kStackPointerGreaterThan:
+          cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
+          return VisitStackPointerGreaterThan(value, cont);
+        default:
+          break;
+      }
     }
-  }
 
-  if (user->opcode() == IrOpcode::kWord32Equal) {
-    return VisitWordCompare(this, user, cont);
-  }
+    if (user->opcode() == IrOpcode::kWord32Equal) {
+      return VisitWordCompare(this, user, cont);
+    }
 
-  // Continuation could not be combined with a compare, emit compare against 0.
-  ArmOperandGeneratorT<Adapter> g(this);
-  InstructionCode const opcode =
-      kArmTst | AddressingModeField::encode(kMode_Operand2_R);
-  InstructionOperand const value_operand = g.UseRegister(value);
-  EmitWithContinuation(opcode, value_operand, value_operand, cont);
+    // Continuation could not be combined with a compare, emit compare against
+    // 0.
+    ArmOperandGeneratorT<Adapter> g(this);
+    InstructionCode const opcode =
+        kArmTst | AddressingModeField::encode(kMode_Operand2_R);
+    InstructionOperand const value_operand = g.UseRegister(value);
+    EmitWithContinuation(opcode, value_operand, value_operand, cont);
+  }
 }
 
 template <typename Adapter>
@@ -2257,7 +2319,8 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+  if (enable_switch_jump_table_ ==
+      InstructionSelector::kEnableSwitchJumpTable) {
     static const size_t kMaxTableSwitchValueRange = 2 << 16;
     size_t table_space_cost = 4 + sw.value_range();
     size_t table_time_cost = 3;
@@ -2284,36 +2347,40 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
+void InstructionSelectorT<Adapter>::VisitWord32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int32BinopMatcher m(node);
-  if (m.right().Is(0)) {
-    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Int32BinopMatcher m(node);
+    if (m.right().Is(0)) {
+      return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+    }
   }
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
@@ -2321,71 +2388,83 @@ void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kArmAdd, kArmAdd, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kArmSub, kArmRsb, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kArmSub, kArmRsb, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kArmSub, kArmRsb, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kArmSub, kArmRsb, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    // ARM doesn't set the overflow flag for multiplication, so we need to test
-    // on kNotEqual. Here is the code sequence used:
-    //   smull resultlow, resulthigh, left, right
-    //   cmp resulthigh, Operand(resultlow, ASR, 31)
-    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
-    return EmitInt32MulWithOverflow(this, node, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      // ARM doesn't set the overflow flag for multiplication, so we need to
+      // test on kNotEqual. Here is the code sequence used:
+      //   smull resultlow, resulthigh, left, right
+      //   cmp resulthigh, Operand(resultlow, ASR, 31)
+      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
+      return EmitInt32MulWithOverflow(this, node, &cont);
+    }
+    FlagsContinuation cont;
+    EmitInt32MulWithOverflow(this, node, &cont);
   }
-  FlagsContinuation cont;
-  EmitInt32MulWithOverflow(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
@@ -3533,14 +3612,13 @@ void InstructionSelectorT<Adapter>::VisitI32x4RelaxedTruncF64x2UZero(
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
-    OperandGenerator* g, int first_input_index, Node* node) {
+    OperandGenerator* g, int first_input_index, node_t node) {
   UNREACHABLE();
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   if (CpuFeatures::IsSupported(SUDIV)) {
     // The sdiv and udiv instructions correctly return 0 if the divisor is 0,
@@ -3567,9 +3645,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   base::EnumSet<MachineRepresentation> req_aligned;
   req_aligned.Add(MachineRepresentation::kFloat32);
   req_aligned.Add(MachineRepresentation::kFloat64);
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index cc5ccdd1402..c4d03fcc50c 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -12,6 +12,7 @@
 #include "src/compiler/machine-operator.h"
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
+#include "src/compiler/turboshaft/operations.h"
 
 namespace v8 {
 namespace internal {
@@ -515,7 +516,7 @@ uint8_t GetBinopProperties(InstructionCode opcode) {
 template <typename Adapter, typename Matcher>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode, ImmediateMode operand_mode,
-                FlagsContinuation* cont) {
+                FlagsContinuationT<Adapter>* cont) {
   Arm64OperandGeneratorT<Adapter> g(selector);
   InstructionOperand inputs[5];
   size_t input_count = 0;
@@ -591,7 +592,7 @@ void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter, typename Matcher>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 ArchOpcode opcode, ImmediateMode operand_mode) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitBinop<Adapter, Matcher>(selector, node, opcode, operand_mode, &cont);
 }
 
@@ -850,43 +851,42 @@ void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
-  InstructionCode opcode = kArchNop;
-  ImmediateMode immediate_mode = kNoImmediate;
-  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
-  MachineRepresentation rep = load_rep.representation();
-  switch (rep) {
-    case MachineRepresentation::kFloat32:
-      opcode = kArm64LdrS;
-      immediate_mode = kLoadStoreImm32;
-      break;
-    case MachineRepresentation::kFloat64:
-      opcode = kArm64LdrD;
-      immediate_mode = kLoadStoreImm64;
-      break;
-    case MachineRepresentation::kBit:  // Fall through.
-    case MachineRepresentation::kWord8:
-      opcode = load_rep.IsUnsigned()
-                   ? kArm64Ldrb
-                   : load_rep.semantic() == MachineSemantic::kInt32
-                         ? kArm64LdrsbW
-                         : kArm64Ldrsb;
-      immediate_mode = kLoadStoreImm8;
-      break;
-    case MachineRepresentation::kWord16:
-      opcode = load_rep.IsUnsigned()
-                   ? kArm64Ldrh
-                   : load_rep.semantic() == MachineSemantic::kInt32
-                         ? kArm64LdrshW
-                         : kArm64Ldrsh;
-      immediate_mode = kLoadStoreImm16;
-      break;
-    case MachineRepresentation::kWord32:
-      opcode = kArm64LdrW;
-      immediate_mode = kLoadStoreImm32;
-      break;
-    case MachineRepresentation::kCompressedPointer:  // Fall through.
-    case MachineRepresentation::kCompressed:
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    InstructionCode opcode = kArchNop;
+    ImmediateMode immediate_mode = kNoImmediate;
+    LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+    MachineRepresentation rep = load_rep.representation();
+    switch (rep) {
+      case MachineRepresentation::kFloat32:
+        opcode = kArm64LdrS;
+        immediate_mode = kLoadStoreImm32;
+        break;
+      case MachineRepresentation::kFloat64:
+        opcode = kArm64LdrD;
+        immediate_mode = kLoadStoreImm64;
+        break;
+      case MachineRepresentation::kBit:  // Fall through.
+      case MachineRepresentation::kWord8:
+        opcode = load_rep.IsUnsigned()                            ? kArm64Ldrb
+                 : load_rep.semantic() == MachineSemantic::kInt32 ? kArm64LdrsbW
+                                                                  : kArm64Ldrsb;
+        immediate_mode = kLoadStoreImm8;
+        break;
+      case MachineRepresentation::kWord16:
+        opcode = load_rep.IsUnsigned()                            ? kArm64Ldrh
+                 : load_rep.semantic() == MachineSemantic::kInt32 ? kArm64LdrshW
+                                                                  : kArm64Ldrsh;
+        immediate_mode = kLoadStoreImm16;
+        break;
+      case MachineRepresentation::kWord32:
+        opcode = kArm64LdrW;
+        immediate_mode = kLoadStoreImm32;
+        break;
+      case MachineRepresentation::kCompressedPointer:  // Fall through.
+      case MachineRepresentation::kCompressed:
 #ifdef V8_COMPRESS_POINTERS
       opcode = kArm64LdrW;
       immediate_mode = kLoadStoreImm32;
@@ -925,7 +925,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
     case MachineRepresentation::kMapWord:  // Fall through.
     case MachineRepresentation::kNone:
       UNREACHABLE();
-  }
+    }
   if (node->opcode() == IrOpcode::kProtectedLoad) {
     opcode |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
   } else if (node->opcode() == IrOpcode::kLoadTrapOnNull) {
@@ -933,6 +933,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
   }
 
   EmitLoad(this, node, opcode, immediate_mode, rep);
+  }
 }
 
 template <typename Adapter>
@@ -1439,8 +1440,20 @@ void InstructionSelectorT<Adapter>::VisitWord64Shl(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
-    Node* node, FlagsContinuation* cont) {
-  StackCheckKind kind = StackCheckKindOf(node->op());
+    node_t node, FlagsContinuationT<Adapter>* cont) {
+  StackCheckKind kind;
+  node_t value;
+  if constexpr (Adapter::IsTurboshaft) {
+    const auto& op =
+        this->turboshaft_graph()
+            ->Get(node)
+            .template Cast<turboshaft::StackPointerGreaterThanOp>();
+    kind = op.kind;
+    value = op.stack_limit();
+  } else {
+    kind = StackCheckKindOf(node->op());
+    value = node->InputAt(0);
+  }
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
@@ -1459,7 +1472,6 @@ void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
                                  ? OperandGenerator::kUniqueRegister
                                  : OperandGenerator::kRegister;
 
-  Node* const value = node->InputAt(0);
   InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};
   static constexpr int input_count = arraysize(inputs);
 
@@ -1883,7 +1895,7 @@ namespace {
 
 template <typename Adapter>
 void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
-                              Node* node, FlagsContinuation* cont) {
+                              Node* node, FlagsContinuationT<Adapter>* cont) {
   Arm64OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
@@ -1907,7 +1919,7 @@ void EmitInt32MulWithOverflow(InstructionSelectorT<Adapter>* selector,
 
 template <typename Adapter>
 void EmitInt64MulWithOverflow(InstructionSelectorT<Adapter>* selector,
-                              Node* node, FlagsContinuation* cont) {
+                              Node* node, FlagsContinuationT<Adapter>* cont) {
   Arm64OperandGeneratorT<Adapter> g(selector);
   Int64BinopMatcher m(node);
   InstructionOperand result = g.DefineAsRegister(node);
@@ -2434,7 +2446,8 @@ void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(node_t node, int index) {
+}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
@@ -2443,49 +2456,54 @@ void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
-    Node* node) {
-  Arm64OperandGeneratorT<Adapter> g(this);
+    node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Arm64OperandGeneratorT<Adapter> g(this);
 
-  // `arguments` includes alignment "holes". This means that slots bigger than
-  // kSystemPointerSize, e.g. Simd128, will span across multiple arguments.
-  int claim_count = static_cast<int>(arguments->size());
-  bool needs_padding = claim_count % 2 != 0;
-  int slot = claim_count - 1;
-  claim_count = RoundUp(claim_count, 2);
-  // Bump the stack pointer.
-  if (claim_count > 0) {
-    // TODO(titzer): claim and poke probably take small immediates.
-    // TODO(titzer): it would be better to bump the sp here only
-    //               and emit paired stores with increment for non c frames.
-    Emit(kArm64Claim, g.NoOutput(), g.TempImmediate(claim_count));
-
-    if (needs_padding) {
-      Emit(kArm64Poke, g.NoOutput(), g.UseImmediate(0),
-           g.TempImmediate(claim_count - 1));
+    // `arguments` includes alignment "holes". This means that slots bigger than
+    // kSystemPointerSize, e.g. Simd128, will span across multiple arguments.
+    int claim_count = static_cast<int>(arguments->size());
+    bool needs_padding = claim_count % 2 != 0;
+    int slot = claim_count - 1;
+    claim_count = RoundUp(claim_count, 2);
+    // Bump the stack pointer.
+    if (claim_count > 0) {
+      // TODO(titzer): claim and poke probably take small immediates.
+      // TODO(titzer): it would be better to bump the sp here only
+      //               and emit paired stores with increment for non c frames.
+      Emit(kArm64Claim, g.NoOutput(), g.TempImmediate(claim_count));
+
+      if (needs_padding) {
+        Emit(kArm64Poke, g.NoOutput(), g.UseImmediate(0),
+             g.TempImmediate(claim_count - 1));
+      }
     }
-  }
 
-  // Poke the arguments into the stack.
-  while (slot >= 0) {
-    PushParameter input0 = (*arguments)[slot];
-    // Skip holes in the param array. These represent both extra slots for
-    // multi-slot values and padding slots for alignment.
-    if (input0.node == nullptr) {
-      slot--;
-      continue;
-    }
-    PushParameter input1 = slot > 0 ? (*arguments)[slot - 1] : PushParameter();
-    // Emit a poke-pair if consecutive parameters have the same type.
-    // TODO(arm): Support consecutive Simd128 parameters.
-    if (input1.node != nullptr &&
-        input0.location.GetType() == input1.location.GetType()) {
-      Emit(kArm64PokePair, g.NoOutput(), g.UseRegister(input0.node),
-           g.UseRegister(input1.node), g.TempImmediate(slot));
-      slot -= 2;
-    } else {
-      Emit(kArm64Poke, g.NoOutput(), g.UseRegister(input0.node),
-           g.TempImmediate(slot));
-      slot--;
+    // Poke the arguments into the stack.
+    while (slot >= 0) {
+      PushParameter input0 = (*arguments)[slot];
+      // Skip holes in the param array. These represent both extra slots for
+      // multi-slot values and padding slots for alignment.
+      if (input0.node == nullptr) {
+        slot--;
+        continue;
+      }
+      PushParameter input1 =
+          slot > 0 ? (*arguments)[slot - 1] : PushParameter();
+      // Emit a poke-pair if consecutive parameters have the same type.
+      // TODO(arm): Support consecutive Simd128 parameters.
+      if (input1.node != nullptr &&
+          input0.location.GetType() == input1.location.GetType()) {
+        Emit(kArm64PokePair, g.NoOutput(), g.UseRegister(input0.node),
+             g.UseRegister(input1.node), g.TempImmediate(slot));
+        slot -= 2;
+      } else {
+        Emit(kArm64Poke, g.NoOutput(), g.UseRegister(input0.node),
+             g.TempImmediate(slot));
+        slot--;
+      }
     }
   }
 }
@@ -2493,13 +2511,13 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
+    node_t node) {
   Arm64OperandGeneratorT<Adapter> g(this);
 
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
     // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
+    if (this->valid(output.node)) {
       DCHECK(!call_descriptor->IsCFunctionCall());
 
       if (output.location.GetType() == MachineType::Float32()) {
@@ -2529,7 +2547,7 @@ namespace {
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
                   InstructionCode opcode, InstructionOperand left,
-                  InstructionOperand right, FlagsContinuation* cont) {
+                  InstructionOperand right, FlagsContinuationT<Adapter>* cont) {
   if (cont->IsSelect()) {
     Arm64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[] = {left, right,
@@ -2596,7 +2614,7 @@ FlagsCondition MapForFlagSettingBinop(FlagsCondition cond) {
 template <typename Adapter>
 void MaybeReplaceCmpZeroWithFlagSettingBinop(
     InstructionSelectorT<Adapter>* selector, Node** node, Node* binop,
-    ArchOpcode* opcode, FlagsCondition cond, FlagsContinuation* cont,
+    ArchOpcode* opcode, FlagsCondition cond, FlagsContinuationT<Adapter>* cont,
     ImmediateMode* immediate_mode) {
   ArchOpcode binop_opcode;
   ArchOpcode no_output_opcode;
@@ -2667,7 +2685,7 @@ FlagsCondition MapForCbz(FlagsCondition cond) {
 template <typename Adapter>
 void EmitBranchOrDeoptimize(InstructionSelectorT<Adapter>* selector,
                             InstructionCode opcode, InstructionOperand value,
-                            FlagsContinuation* cont) {
+                            FlagsContinuationT<Adapter>* cont) {
   DCHECK(cont->IsBranch() || cont->IsDeoptimize());
   selector->EmitWithContinuation(opcode, value, cont);
 }
@@ -2701,7 +2719,8 @@ struct CbzOrTbzMatchTrait<64> {
 template <typename Adapter, int N>
 bool TryEmitCbzOrTbz(InstructionSelectorT<Adapter>* selector, Node* node,
                      typename CbzOrTbzMatchTrait<N>::IntegralType value,
-                     Node* user, FlagsCondition cond, FlagsContinuation* cont) {
+                     Node* user, FlagsCondition cond,
+                     FlagsContinuationT<Adapter>* cont) {
   // Only handle branches and deoptimisations.
   if (!cont->IsBranch() && !cont->IsDeoptimize()) return false;
 
@@ -2776,7 +2795,7 @@ bool TryEmitCbzOrTbz(InstructionSelectorT<Adapter>* selector, Node* node,
 // Shared routine for multiple word compare operations.
 template <typename Adapter>
 void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont,
+                      InstructionCode opcode, FlagsContinuationT<Adapter>* cont,
                       ImmediateMode immediate_mode) {
   Arm64OperandGeneratorT<Adapter> g(selector);
 
@@ -2805,73 +2824,79 @@ void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
 }
 
 template <typename Adapter>
-void VisitWord32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                        FlagsContinuation* cont) {
-  Int32BinopMatcher m(node);
-  FlagsCondition cond = cont->condition();
-  if (m.right().HasResolvedValue()) {
-    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.left().node(),
-                                     m.right().ResolvedValue(), node, cond,
-                                     cont)) {
-      return;
-    }
-  } else if (m.left().HasResolvedValue()) {
-    FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
-    if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.right().node(),
-                                     m.left().ResolvedValue(), node,
-                                     commuted_cond, cont)) {
-      return;
-    }
-  }
-  ArchOpcode opcode = kArm64Cmp32;
-  ImmediateMode immediate_mode = kArithmeticImm;
-  if (m.right().Is(0) && (m.left().IsInt32Add() || m.left().IsWord32And())) {
-    // Emit flag setting add/and instructions for comparisons against zero.
-    if (CanUseFlagSettingBinop(cond)) {
-      Node* binop = m.left().node();
-      MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
-                                              cond, cont, &immediate_mode);
-    }
-  } else if (m.left().Is(0) &&
-             (m.right().IsInt32Add() || m.right().IsWord32And())) {
-    // Same as above, but we need to commute the condition before we
-    // continue with the rest of the checks.
-    FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
-    if (CanUseFlagSettingBinop(commuted_cond)) {
-      Node* binop = m.right().node();
-      MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
-                                              commuted_cond, cont,
-                                              &immediate_mode);
+void VisitWord32Compare(InstructionSelectorT<Adapter>* selector,
+                        typename Adapter::node_t node,
+                        FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Int32BinopMatcher m(node);
+    FlagsCondition cond = cont->condition();
+    if (m.right().HasResolvedValue()) {
+      if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.left().node(),
+                                       m.right().ResolvedValue(), node, cond,
+                                       cont)) {
+        return;
+      }
+    } else if (m.left().HasResolvedValue()) {
+      FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
+      if (TryEmitCbzOrTbz<Adapter, 32>(selector, m.right().node(),
+                                       m.left().ResolvedValue(), node,
+                                       commuted_cond, cont)) {
+        return;
+      }
     }
-  } else if (m.right().IsInt32Sub() && (cond == kEqual || cond == kNotEqual)) {
-    // Select negated compare for comparisons with negated right input.
-    // Only do this for kEqual and kNotEqual, which do not depend on the
-    // C and V flags, as those flags will be different with CMN when the
-    // right-hand side of the original subtraction is INT_MIN.
-    Node* sub = m.right().node();
-    Int32BinopMatcher msub(sub);
-    if (msub.left().Is(0)) {
-      bool can_cover = selector->CanCover(node, sub);
-      node->ReplaceInput(1, msub.right().node());
-      // Even if the comparison node covers the subtraction, after the input
-      // replacement above, the node still won't cover the input to the
-      // subtraction; the subtraction still uses it.
-      // In order to get shifted operations to work, we must remove the rhs
-      // input to the subtraction, as TryMatchAnyShift requires this node to
-      // cover the input shift. We do this by setting it to the lhs input,
-      // as we know it's zero, and the result of the subtraction isn't used by
-      // any other node.
-      if (can_cover) sub->ReplaceInput(1, msub.left().node());
-      opcode = kArm64Cmn32;
+    ArchOpcode opcode = kArm64Cmp32;
+    ImmediateMode immediate_mode = kArithmeticImm;
+    if (m.right().Is(0) && (m.left().IsInt32Add() || m.left().IsWord32And())) {
+      // Emit flag setting add/and instructions for comparisons against zero.
+      if (CanUseFlagSettingBinop(cond)) {
+        Node* binop = m.left().node();
+        MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
+                                                cond, cont, &immediate_mode);
+      }
+    } else if (m.left().Is(0) &&
+               (m.right().IsInt32Add() || m.right().IsWord32And())) {
+      // Same as above, but we need to commute the condition before we
+      // continue with the rest of the checks.
+      FlagsCondition commuted_cond = CommuteFlagsCondition(cond);
+      if (CanUseFlagSettingBinop(commuted_cond)) {
+        Node* binop = m.right().node();
+        MaybeReplaceCmpZeroWithFlagSettingBinop(selector, &node, binop, &opcode,
+                                                commuted_cond, cont,
+                                                &immediate_mode);
+      }
+    } else if (m.right().IsInt32Sub() &&
+               (cond == kEqual || cond == kNotEqual)) {
+      // Select negated compare for comparisons with negated right input.
+      // Only do this for kEqual and kNotEqual, which do not depend on the
+      // C and V flags, as those flags will be different with CMN when the
+      // right-hand side of the original subtraction is INT_MIN.
+      Node* sub = m.right().node();
+      Int32BinopMatcher msub(sub);
+      if (msub.left().Is(0)) {
+        bool can_cover = selector->CanCover(node, sub);
+        node->ReplaceInput(1, msub.right().node());
+        // Even if the comparison node covers the subtraction, after the input
+        // replacement above, the node still won't cover the input to the
+        // subtraction; the subtraction still uses it.
+        // In order to get shifted operations to work, we must remove the rhs
+        // input to the subtraction, as TryMatchAnyShift requires this node to
+        // cover the input shift. We do this by setting it to the lhs input,
+        // as we know it's zero, and the result of the subtraction isn't used by
+        // any other node.
+        if (can_cover) sub->ReplaceInput(1, msub.left().node());
+        opcode = kArm64Cmn32;
+      }
     }
+    VisitBinop<Adapter, Int32BinopMatcher>(selector, node, opcode,
+                                           immediate_mode, cont);
   }
-  VisitBinop<Adapter, Int32BinopMatcher>(selector, node, opcode, immediate_mode,
-                                         cont);
 }
 
 template <typename Adapter>
 void VisitWordTest(InstructionSelectorT<Adapter>* selector, Node* node,
-                   InstructionCode opcode, FlagsContinuation* cont) {
+                   InstructionCode opcode, FlagsContinuationT<Adapter>* cont) {
   Arm64OperandGeneratorT<Adapter> g(selector);
   VisitCompare(selector, opcode, g.UseRegister(node), g.UseRegister(node),
                cont);
@@ -2879,19 +2904,19 @@ void VisitWordTest(InstructionSelectorT<Adapter>* selector, Node* node,
 
 template <typename Adapter>
 void VisitWord32Test(InstructionSelectorT<Adapter>* selector, Node* node,
-                     FlagsContinuation* cont) {
+                     FlagsContinuationT<Adapter>* cont) {
   VisitWordTest(selector, node, kArm64Tst32, cont);
 }
 
 template <typename Adapter>
 void VisitWord64Test(InstructionSelectorT<Adapter>* selector, Node* node,
-                     FlagsContinuation* cont) {
+                     FlagsContinuationT<Adapter>* cont) {
   VisitWordTest(selector, node, kArm64Tst, cont);
 }
 
-template <typename Matcher>
+template <typename Adapter, typename Matcher>
 struct TestAndBranchMatcher {
-  TestAndBranchMatcher(Node* node, FlagsContinuation* cont)
+  TestAndBranchMatcher(Node* node, FlagsContinuationT<Adapter>* cont)
       : matches_(false), cont_(cont), matcher_(node) {
     Initialize();
   }
@@ -2909,7 +2934,7 @@ struct TestAndBranchMatcher {
 
  private:
   bool matches_;
-  FlagsContinuation* cont_;
+  FlagsContinuationT<Adapter>* cont_;
   Matcher matcher_;
 
   void Initialize() {
@@ -2927,39 +2952,49 @@ struct TestAndBranchMatcher {
 
 // Shared routine for multiple float32 compare operations.
 template <typename Adapter>
-void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Arm64OperandGeneratorT<Adapter> g(selector);
-  Float32BinopMatcher m(node);
-  if (m.right().Is(0.0f)) {
-    VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
-                 g.UseImmediate(m.right().node()), cont);
-  } else if (m.left().Is(0.0f)) {
-    cont->Commute();
-    VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.right().node()),
-                 g.UseImmediate(m.left().node()), cont);
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
-                 g.UseRegister(m.right().node()), cont);
+    Arm64OperandGeneratorT<Adapter> g(selector);
+    Float32BinopMatcher m(node);
+    if (m.right().Is(0.0f)) {
+      VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
+                   g.UseImmediate(m.right().node()), cont);
+    } else if (m.left().Is(0.0f)) {
+      cont->Commute();
+      VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.right().node()),
+                   g.UseImmediate(m.left().node()), cont);
+    } else {
+      VisitCompare(selector, kArm64Float32Cmp, g.UseRegister(m.left().node()),
+                   g.UseRegister(m.right().node()), cont);
+    }
   }
 }
 
 // Shared routine for multiple float64 compare operations.
 template <typename Adapter>
-void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Arm64OperandGeneratorT<Adapter> g(selector);
-  Float64BinopMatcher m(node);
-  if (m.right().Is(0.0)) {
-    VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
-                 g.UseImmediate(m.right().node()), cont);
-  } else if (m.left().Is(0.0)) {
-    cont->Commute();
-    VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.right().node()),
-                 g.UseImmediate(m.left().node()), cont);
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
   } else {
-    VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
-                 g.UseRegister(m.right().node()), cont);
+    Arm64OperandGeneratorT<Adapter> g(selector);
+    Float64BinopMatcher m(node);
+    if (m.right().Is(0.0)) {
+      VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
+                   g.UseImmediate(m.right().node()), cont);
+    } else if (m.left().Is(0.0)) {
+      cont->Commute();
+      VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.right().node()),
+                   g.UseImmediate(m.left().node()), cont);
+    } else {
+      VisitCompare(selector, kArm64Float64Cmp, g.UseRegister(m.left().node()),
+                   g.UseRegister(m.right().node()), cont);
+    }
   }
 }
 
@@ -3203,202 +3238,213 @@ void VisitAtomicBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWordCompareZero(
-    Node* user, Node* value, FlagsContinuation* cont) {
-  Arm64OperandGeneratorT<Adapter> g(this);
-  // Try to combine with comparisons against 0 by simply inverting the branch.
-  while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
-    Int32BinopMatcher m(value);
-    if (!m.right().Is(0)) break;
-
-    user = value;
-    value = m.left().node();
-    cont->Negate();
-  }
-
-  // Try to match bit checks to create TBZ/TBNZ instructions.
-  // Unlike the switch below, CanCover check is not needed here.
-  // If there are several uses of the given operation, we will generate a TBZ
-  // instruction for each. This is useful even if there are other uses of the
-  // arithmetic result, because it moves dependencies further back.
-  switch (value->opcode()) {
-    case IrOpcode::kWord64Equal: {
-      Int64BinopMatcher m(value);
-      if (m.right().Is(0)) {
-        Node* const left = m.left().node();
-        if (left->opcode() == IrOpcode::kWord64And) {
-          // Attempt to merge the Word64Equal(Word64And(x, y), 0) comparison
-          // into a tbz/tbnz instruction.
-          TestAndBranchMatcher<Uint64BinopMatcher> tbm(left, cont);
-          if (tbm.Matches()) {
-            Arm64OperandGeneratorT<Adapter> gen(this);
-            cont->OverwriteAndNegateIfEqual(kEqual);
-            this->EmitWithContinuation(kArm64TestAndBranch,
-                                       gen.UseRegister(tbm.input()),
-                                       gen.TempImmediate(tbm.bit()), cont);
-            return;
-          }
-        }
-      }
-      break;
-    }
-    case IrOpcode::kWord32And: {
-      TestAndBranchMatcher<Uint32BinopMatcher> tbm(value, cont);
-      if (tbm.Matches()) {
-        Arm64OperandGeneratorT<Adapter> gen(this);
-        this->EmitWithContinuation(kArm64TestAndBranch32,
-                                   gen.UseRegister(tbm.input()),
-                                   gen.TempImmediate(tbm.bit()), cont);
-        return;
-      }
-      break;
-    }
-    case IrOpcode::kWord64And: {
-      TestAndBranchMatcher<Uint64BinopMatcher> tbm(value, cont);
-      if (tbm.Matches()) {
-        Arm64OperandGeneratorT<Adapter> gen(this);
-        this->EmitWithContinuation(kArm64TestAndBranch,
-                                   gen.UseRegister(tbm.input()),
-                                   gen.TempImmediate(tbm.bit()), cont);
-        return;
-      }
-      break;
+    node_t user, node_t value, FlagsContinuation* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Arm64OperandGeneratorT<Adapter> g(this);
+    // Try to combine with comparisons against 0 by simply inverting the branch.
+    while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
+      Int32BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+
+      user = value;
+      value = m.left().node();
+      cont->Negate();
     }
-    default:
-      break;
-  }
 
-  if (CanCover(user, value)) {
+    // Try to match bit checks to create TBZ/TBNZ instructions.
+    // Unlike the switch below, CanCover check is not needed here.
+    // If there are several uses of the given operation, we will generate a TBZ
+    // instruction for each. This is useful even if there are other uses of the
+    // arithmetic result, because it moves dependencies further back.
     switch (value->opcode()) {
-      case IrOpcode::kWord32Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitWord32Compare(this, value, cont);
-      case IrOpcode::kInt32LessThan:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
-        return VisitWord32Compare(this, value, cont);
-      case IrOpcode::kInt32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
-        return VisitWord32Compare(this, value, cont);
-      case IrOpcode::kUint32LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
-        return VisitWord32Compare(this, value, cont);
-      case IrOpcode::kUint32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
-        return VisitWord32Compare(this, value, cont);
       case IrOpcode::kWord64Equal: {
-        cont->OverwriteAndNegateIfEqual(kEqual);
         Int64BinopMatcher m(value);
         if (m.right().Is(0)) {
           Node* const left = m.left().node();
-          if (CanCover(value, left) && left->opcode() == IrOpcode::kWord64And) {
-            return VisitWordCompare(this, left, kArm64Tst, cont, kLogical64Imm);
+          if (left->opcode() == IrOpcode::kWord64And) {
+            // Attempt to merge the Word64Equal(Word64And(x, y), 0) comparison
+            // into a tbz/tbnz instruction.
+            TestAndBranchMatcher<Adapter, Uint64BinopMatcher> tbm(left, cont);
+            if (tbm.Matches()) {
+              Arm64OperandGeneratorT<Adapter> gen(this);
+              cont->OverwriteAndNegateIfEqual(kEqual);
+              this->EmitWithContinuation(kArm64TestAndBranch,
+                                         gen.UseRegister(tbm.input()),
+                                         gen.TempImmediate(tbm.bit()), cont);
+              return;
+            }
           }
         }
-        return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        break;
       }
-      case IrOpcode::kInt64LessThan:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
-        return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
-      case IrOpcode::kInt64LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
-        return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
-      case IrOpcode::kUint64LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
-        return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
-      case IrOpcode::kUint64LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
-        return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
-      case IrOpcode::kFloat32Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThan:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThan);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat64Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThan:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThan);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kProjection:
-        // Check if this is the overflow output projection of an
-        // <Operation>WithOverflow node.
-        if (ProjectionIndexOf(value->op()) == 1u) {
-          // We cannot combine the <Operation>WithOverflow with this branch
-          // unless the 0th projection (the use of the actual value of the
-          // <Operation> is either nullptr, which means there's no use of the
-          // actual value, or was already defined, which means it is scheduled
-          // *AFTER* this branch).
-          Node* const node = value->InputAt(0);
-          Node* const result = NodeProperties::FindProjection(node, 0);
-          if (result == nullptr || IsDefined(result)) {
-            switch (node->opcode()) {
-              case IrOpcode::kInt32AddWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Adapter, Int32BinopMatcher>(
-                    this, node, kArm64Add32, kArithmeticImm, cont);
-              case IrOpcode::kInt32SubWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Adapter, Int32BinopMatcher>(
-                    this, node, kArm64Sub32, kArithmeticImm, cont);
-              case IrOpcode::kInt32MulWithOverflow:
-                // ARM64 doesn't set the overflow flag for multiplication, so we
-                // need to test on kNotEqual. Here is the code sequence used:
-                //   smull result, left, right
-                //   cmp result.X(), Operand(result, SXTW)
-                cont->OverwriteAndNegateIfEqual(kNotEqual);
-                return EmitInt32MulWithOverflow(this, node, cont);
-              case IrOpcode::kInt64AddWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Adapter, Int64BinopMatcher>(
-                    this, node, kArm64Add, kArithmeticImm, cont);
-              case IrOpcode::kInt64SubWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop<Adapter, Int64BinopMatcher>(
-                    this, node, kArm64Sub, kArithmeticImm, cont);
-              case IrOpcode::kInt64MulWithOverflow:
-                // ARM64 doesn't set the overflow flag for multiplication, so we
-                // need to test on kNotEqual. Here is the code sequence used:
-                //   mul result, left, right
-                //   smulh high, left, right
-                //   cmp high, result, asr 63
-                cont->OverwriteAndNegateIfEqual(kNotEqual);
-                return EmitInt64MulWithOverflow(this, node, cont);
-              default:
-                break;
-            }
-          }
+      case IrOpcode::kWord32And: {
+        TestAndBranchMatcher<Adapter, Uint32BinopMatcher> tbm(value, cont);
+        if (tbm.Matches()) {
+          Arm64OperandGeneratorT<Adapter> gen(this);
+          this->EmitWithContinuation(kArm64TestAndBranch32,
+                                     gen.UseRegister(tbm.input()),
+                                     gen.TempImmediate(tbm.bit()), cont);
+          return;
         }
         break;
-      case IrOpcode::kInt32Add:
-        return VisitWordCompare(this, value, kArm64Cmn32, cont, kArithmeticImm);
-      case IrOpcode::kInt32Sub:
-        return VisitWord32Compare(this, value, cont);
-      case IrOpcode::kWord32And:
-        return VisitWordCompare(this, value, kArm64Tst32, cont, kLogical32Imm);
-      case IrOpcode::kWord64And:
-        return VisitWordCompare(this, value, kArm64Tst, cont, kLogical64Imm);
-      case IrOpcode::kStackPointerGreaterThan:
-        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
-        return VisitStackPointerGreaterThan(value, cont);
+      }
+      case IrOpcode::kWord64And: {
+        TestAndBranchMatcher<Adapter, Uint64BinopMatcher> tbm(value, cont);
+        if (tbm.Matches()) {
+          Arm64OperandGeneratorT<Adapter> gen(this);
+          this->EmitWithContinuation(kArm64TestAndBranch,
+                                     gen.UseRegister(tbm.input()),
+                                     gen.TempImmediate(tbm.bit()), cont);
+          return;
+        }
+        break;
+      }
       default:
         break;
     }
-  }
 
-  // Branch could not be combined with a compare, compare against 0 and branch.
-  if (cont->IsBranch()) {
-    Emit(cont->Encode(kArm64CompareAndBranch32), g.NoOutput(),
-         g.UseRegister(value), g.Label(cont->true_block()),
-         g.Label(cont->false_block()));
-  } else {
-    VisitCompare(this, cont->Encode(kArm64Tst32), g.UseRegister(value),
-                 g.UseRegister(value), cont);
+    if (CanCover(user, value)) {
+      switch (value->opcode()) {
+        case IrOpcode::kWord32Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kInt32LessThan:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kInt32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kUint32LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kUint32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kWord64Equal: {
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          Int64BinopMatcher m(value);
+          if (m.right().Is(0)) {
+            Node* const left = m.left().node();
+            if (CanCover(value, left) &&
+                left->opcode() == IrOpcode::kWord64And) {
+              return VisitWordCompare(this, left, kArm64Tst, cont,
+                                      kLogical64Imm);
+            }
+          }
+          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        }
+        case IrOpcode::kInt64LessThan:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        case IrOpcode::kInt64LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        case IrOpcode::kUint64LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        case IrOpcode::kUint64LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+          return VisitWordCompare(this, value, kArm64Cmp, cont, kArithmeticImm);
+        case IrOpcode::kFloat32Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThan:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThan);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat64Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThan:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThan);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kFloatLessThanOrEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kProjection:
+          // Check if this is the overflow output projection of an
+          // <Operation>WithOverflow node.
+          if (ProjectionIndexOf(value->op()) == 1u) {
+            // We cannot combine the <Operation>WithOverflow with this branch
+            // unless the 0th projection (the use of the actual value of the
+            // <Operation> is either nullptr, which means there's no use of the
+            // actual value, or was already defined, which means it is scheduled
+            // *AFTER* this branch).
+            Node* const node = value->InputAt(0);
+            Node* const result = NodeProperties::FindProjection(node, 0);
+            if (result == nullptr || IsDefined(result)) {
+              switch (node->opcode()) {
+                case IrOpcode::kInt32AddWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop<Adapter, Int32BinopMatcher>(
+                      this, node, kArm64Add32, kArithmeticImm, cont);
+                case IrOpcode::kInt32SubWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop<Adapter, Int32BinopMatcher>(
+                      this, node, kArm64Sub32, kArithmeticImm, cont);
+                case IrOpcode::kInt32MulWithOverflow:
+                  // ARM64 doesn't set the overflow flag for multiplication, so
+                  // we need to test on kNotEqual. Here is the code sequence
+                  // used:
+                  //   smull result, left, right
+                  //   cmp result.X(), Operand(result, SXTW)
+                  cont->OverwriteAndNegateIfEqual(kNotEqual);
+                  return EmitInt32MulWithOverflow(this, node, cont);
+                case IrOpcode::kInt64AddWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop<Adapter, Int64BinopMatcher>(
+                      this, node, kArm64Add, kArithmeticImm, cont);
+                case IrOpcode::kInt64SubWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop<Adapter, Int64BinopMatcher>(
+                      this, node, kArm64Sub, kArithmeticImm, cont);
+                case IrOpcode::kInt64MulWithOverflow:
+                  // ARM64 doesn't set the overflow flag for multiplication, so
+                  // we need to test on kNotEqual. Here is the code sequence
+                  // used:
+                  //   mul result, left, right
+                  //   smulh high, left, right
+                  //   cmp high, result, asr 63
+                  cont->OverwriteAndNegateIfEqual(kNotEqual);
+                  return EmitInt64MulWithOverflow(this, node, cont);
+                default:
+                  break;
+              }
+            }
+          }
+          break;
+        case IrOpcode::kInt32Add:
+          return VisitWordCompare(this, value, kArm64Cmn32, cont,
+                                  kArithmeticImm);
+        case IrOpcode::kInt32Sub:
+          return VisitWord32Compare(this, value, cont);
+        case IrOpcode::kWord32And:
+          return VisitWordCompare(this, value, kArm64Tst32, cont,
+                                  kLogical32Imm);
+        case IrOpcode::kWord64And:
+          return VisitWordCompare(this, value, kArm64Tst, cont, kLogical64Imm);
+        case IrOpcode::kStackPointerGreaterThan:
+          cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
+          return VisitStackPointerGreaterThan(value, cont);
+        default:
+          break;
+      }
+    }
+
+    // Branch could not be combined with a compare, compare against 0 and
+    // branch.
+    if (cont->IsBranch()) {
+      Emit(cont->Encode(kArm64CompareAndBranch32), g.NoOutput(),
+           g.UseRegister(value), g.Label(cont->true_block()),
+           g.Label(cont->false_block()));
+    } else {
+      VisitCompare(this, cont->Encode(kArm64Tst32), g.UseRegister(value),
+                   g.UseRegister(value), cont);
+    }
   }
 }
 
@@ -3409,7 +3455,8 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+  if (enable_switch_jump_table_ ==
+      InstructionSelector::kEnableSwitchJumpTable) {
     static const size_t kMaxTableSwitchValueRange = 2 << 16;
     size_t table_space_cost = 4 + sw.value_range();
     size_t table_time_cost = 3;
@@ -3436,102 +3483,106 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
-  Node* const user = node;
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int32BinopMatcher m(user);
-  if (m.right().Is(0)) {
-    Node* const value = m.left().node();
-    if (CanCover(user, value)) {
-      switch (value->opcode()) {
-        case IrOpcode::kInt32Add:
-        case IrOpcode::kWord32And:
-          return VisitWord32Compare(this, node, &cont);
-        case IrOpcode::kInt32Sub:
-          return VisitWordCompare(this, value, kArm64Cmp32, &cont,
-                                  kArithmeticImm);
-        case IrOpcode::kWord32Equal: {
-          // Word32Equal(Word32Equal(x, y), 0) => Word32Compare(x, y, ne).
-          Int32BinopMatcher mequal(value);
-          node->ReplaceInput(0, mequal.left().node());
-          node->ReplaceInput(1, mequal.right().node());
-          cont.Negate();
-          // {node} still does not cover its new operands, because {mequal} is
-          // still using them.
-          // Since we won't generate any more code for {mequal}, set its
-          // operands to zero to make sure {node} can cover them.
-          // This improves pattern matching in VisitWord32Compare.
-          mequal.node()->ReplaceInput(0, m.right().node());
-          mequal.node()->ReplaceInput(1, m.right().node());
-          return VisitWord32Compare(this, node, &cont);
+void InstructionSelectorT<Adapter>::VisitWord32Equal(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Node* const user = node;
+    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+    Int32BinopMatcher m(user);
+    if (m.right().Is(0)) {
+      Node* const value = m.left().node();
+      if (CanCover(user, value)) {
+        switch (value->opcode()) {
+          case IrOpcode::kInt32Add:
+          case IrOpcode::kWord32And:
+            return VisitWord32Compare(this, node, &cont);
+          case IrOpcode::kInt32Sub:
+            return VisitWordCompare(this, value, kArm64Cmp32, &cont,
+                                    kArithmeticImm);
+          case IrOpcode::kWord32Equal: {
+            // Word32Equal(Word32Equal(x, y), 0) => Word32Compare(x, y, ne).
+            Int32BinopMatcher mequal(value);
+            node->ReplaceInput(0, mequal.left().node());
+            node->ReplaceInput(1, mequal.right().node());
+            cont.Negate();
+            // {node} still does not cover its new operands, because {mequal} is
+            // still using them.
+            // Since we won't generate any more code for {mequal}, set its
+            // operands to zero to make sure {node} can cover them.
+            // This improves pattern matching in VisitWord32Compare.
+            mequal.node()->ReplaceInput(0, m.right().node());
+            mequal.node()->ReplaceInput(1, m.right().node());
+            return VisitWord32Compare(this, node, &cont);
+          }
+          default:
+            break;
         }
-        default:
-          break;
+        return VisitWord32Test(this, value, &cont);
       }
-      return VisitWord32Test(this, value, &cont);
     }
-  }
 
-  if (isolate() && (V8_STATIC_ROOTS_BOOL ||
-                    (COMPRESS_POINTERS_BOOL && !isolate()->bootstrapper()))) {
-    Arm64OperandGeneratorT<Adapter> g(this);
-    const RootsTable& roots_table = isolate()->roots_table();
-    RootIndex root_index;
-    Node* left = nullptr;
-    Handle<HeapObject> right;
-    // HeapConstants and CompressedHeapConstants can be treated the same when
-    // using them as an input to a 32-bit comparison. Check whether either is
-    // present.
-    {
-      CompressedHeapObjectBinopMatcher m(node);
-      if (m.right().HasResolvedValue()) {
-        left = m.left().node();
-        right = m.right().ResolvedValue();
-      } else {
-        HeapObjectBinopMatcher m2(node);
-        if (m2.right().HasResolvedValue()) {
-          left = m2.left().node();
-          right = m2.right().ResolvedValue();
+    if (isolate() && (V8_STATIC_ROOTS_BOOL ||
+                      (COMPRESS_POINTERS_BOOL && !isolate()->bootstrapper()))) {
+      Arm64OperandGeneratorT<Adapter> g(this);
+      const RootsTable& roots_table = isolate()->roots_table();
+      RootIndex root_index;
+      Node* left = nullptr;
+      Handle<HeapObject> right;
+      // HeapConstants and CompressedHeapConstants can be treated the same when
+      // using them as an input to a 32-bit comparison. Check whether either is
+      // present.
+      {
+        CompressedHeapObjectBinopMatcher m(node);
+        if (m.right().HasResolvedValue()) {
+          left = m.left().node();
+          right = m.right().ResolvedValue();
+        } else {
+          HeapObjectBinopMatcher m2(node);
+          if (m2.right().HasResolvedValue()) {
+            left = m2.left().node();
+            right = m2.right().ResolvedValue();
+          }
         }
       }
-    }
-    if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
-      DCHECK_NE(left, nullptr);
-      if (RootsTable::IsReadOnly(root_index)) {
-        Tagged_t ptr =
-            MacroAssemblerBase::ReadOnlyRootPtr(root_index, isolate());
-        if (g.CanBeImmediate(ptr, ImmediateMode::kArithmeticImm)) {
-          return VisitCompare(this, kArm64Cmp32, g.UseRegister(left),
-                              g.TempImmediate(ptr), &cont);
+      if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
+        DCHECK_NE(left, nullptr);
+        if (RootsTable::IsReadOnly(root_index)) {
+          Tagged_t ptr =
+              MacroAssemblerBase::ReadOnlyRootPtr(root_index, isolate());
+          if (g.CanBeImmediate(ptr, ImmediateMode::kArithmeticImm)) {
+            return VisitCompare(this, kArm64Cmp32, g.UseRegister(left),
+                                g.TempImmediate(ptr), &cont);
+          }
         }
       }
     }
-  }
 
-  VisitWord32Compare(this, node, &cont);
+    VisitWord32Compare(this, node, &cont);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWord32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWord32Compare(this, node, &cont);
@@ -3539,125 +3590,170 @@ void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* const node) {
-  Node* const user = node;
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int64BinopMatcher m(user);
-  if (m.right().Is(0)) {
-    Node* const value = m.left().node();
-    if (CanCover(user, value)) {
-      switch (value->opcode()) {
-        case IrOpcode::kWord64And:
-          return VisitWordCompare(this, value, kArm64Tst, &cont, kLogical64Imm);
-        default:
-          break;
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Node* const user = node;
+    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+    Int64BinopMatcher m(user);
+    if (m.right().Is(0)) {
+      Node* const value = m.left().node();
+      if (CanCover(user, value)) {
+        switch (value->opcode()) {
+          case IrOpcode::kWord64And:
+            return VisitWordCompare(this, value, kArm64Tst, &cont,
+                                    kLogical64Imm);
+          default:
+            break;
+        }
+        return VisitWord64Test(this, value, &cont);
       }
-      return VisitWord64Test(this, value, &cont);
     }
+    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
   }
-  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
-                                                  kArithmeticImm, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                                    kArithmeticImm, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
+                                           kArithmeticImm, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Add32,
-                                         kArithmeticImm, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
-                                                  kArithmeticImm, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                                    kArithmeticImm, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
+                                           kArithmeticImm, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop<Adapter, Int32BinopMatcher>(this, node, kArm64Sub32,
-                                         kArithmeticImm, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    // ARM64 doesn't set the overflow flag for multiplication, so we need to
-    // test on kNotEqual. Here is the code sequence used:
-    //   smull result, left, right
-    //   cmp result.X(), Operand(result, SXTW)
-    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
-    return EmitInt32MulWithOverflow(this, node, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      // ARM64 doesn't set the overflow flag for multiplication, so we need to
+      // test on kNotEqual. Here is the code sequence used:
+      //   smull result, left, right
+      //   cmp result.X(), Operand(result, SXTW)
+      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
+      return EmitInt32MulWithOverflow(this, node, &cont);
+    }
+    FlagsContinuation cont;
+    EmitInt32MulWithOverflow(this, node, &cont);
   }
-  FlagsContinuation cont;
-  EmitInt32MulWithOverflow(this, node, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,
-                                                  kArithmeticImm, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,
+                                                    kArithmeticImm, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add,
+                                           kArithmeticImm, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Add, kArithmeticImm,
-                                         &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,
-                                                  kArithmeticImm, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,
+                                                    kArithmeticImm, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub,
+                                           kArithmeticImm, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop<Adapter, Int64BinopMatcher>(this, node, kArm64Sub, kArithmeticImm,
-                                         &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    // ARM64 doesn't set the overflow flag for multiplication, so we need to
-    // test on kNotEqual. Here is the code sequence used:
-    //   mul result, left, right
-    //   smulh high, left, right
-    //   cmp high, result, asr 63
-    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
-    return EmitInt64MulWithOverflow(this, node, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      // ARM64 doesn't set the overflow flag for multiplication, so we need to
+      // test on kNotEqual. Here is the code sequence used:
+      //   mul result, left, right
+      //   smulh high, left, right
+      //   cmp high, result, asr 63
+      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, ovf);
+      return EmitInt64MulWithOverflow(this, node, &cont);
+    }
+    FlagsContinuation cont;
+    EmitInt64MulWithOverflow(this, node, &cont);
   }
-  FlagsContinuation cont;
-  EmitInt64MulWithOverflow(this, node, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
-  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
-  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kArm64Cmp, &cont, kArithmeticImm);
+  }
 }
 
 template <typename Adapter>
@@ -3721,38 +3817,38 @@ void InstructionSelectorT<Adapter>::VisitFloat64Abs(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kFloatLessThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kFloatLessThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
@@ -5023,14 +5119,13 @@ void InstructionSelectorT<Adapter>::VisitI8x16Popcnt(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
-    OperandGenerator* g, int first_input_index, Node* node) {
+    OperandGenerator* g, int first_input_index, node_t node) {
   continuation_outputs_.push_back(g->DefineAsRegister(node));
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   return MachineOperatorBuilder::kFloat32RoundDown |
          MachineOperatorBuilder::kFloat64RoundDown |
          MachineOperatorBuilder::kFloat32RoundUp |
@@ -5054,9 +5149,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/backend/frame-elider.cc b/src/compiler/backend/frame-elider.cc
index 5454996b381..4c9de78628e 100644
--- a/src/compiler/backend/frame-elider.cc
+++ b/src/compiler/backend/frame-elider.cc
@@ -107,9 +107,13 @@ bool FrameElider::PropagateIntoBlock(InstructionBlock* block) {
   // Already marked, nothing to do...
   if (block->needs_frame()) return false;
 
-  // Never mark the dummy end node, otherwise we might incorrectly decide to
-  // put frame deconstruction code there later,
-  if (block->successors().empty()) return false;
+  // Turbofan does have an empty dummy end block, which we need to ignore here.
+  // However, Turboshaft does not have such a block.
+  if (!v8_flags.turboshaft_instruction_selection) {
+    // Never mark the dummy end node, otherwise we might incorrectly decide to
+    // put frame deconstruction code there later,
+    if (block->successors().empty()) return false;
+  }
 
   // Propagate towards the end ("downwards") if there is a predecessor needing
   // a frame, but don't "bleed" from deferred code to non-deferred code.
diff --git a/src/compiler/backend/ia32/instruction-selector-ia32.cc b/src/compiler/backend/ia32/instruction-selector-ia32.cc
index 9ac7f2ee31d..a9c970fe173 100644
--- a/src/compiler/backend/ia32/instruction-selector-ia32.cc
+++ b/src/compiler/backend/ia32/instruction-selector-ia32.cc
@@ -21,6 +21,7 @@
 #include "src/codegen/macro-assembler-base.h"
 #include "src/common/globals.h"
 #include "src/compiler/backend/instruction-codes.h"
+#include "src/compiler/backend/instruction-selector-adapter.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
 #include "src/compiler/backend/instruction-selector.h"
 #include "src/compiler/backend/instruction.h"
@@ -203,44 +204,51 @@ class IA32OperandGeneratorT final : public OperandGeneratorT<Adapter> {
                                        register_mode);
   }
 
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(AddressingMode,
+                                          GetEffectiveAddressMemoryOperand)
   AddressingMode GetEffectiveAddressMemoryOperand(
-      Node* node, InstructionOperand inputs[], size_t* input_count,
+      node_t node, InstructionOperand inputs[], size_t* input_count,
       RegisterMode register_mode = RegisterMode::kRegister) {
-    {
-      LoadMatcher<ExternalReferenceMatcher> m(node);
-      if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
-          selector()->CanAddressRelativeToRootsRegister(
-              m.object().ResolvedValue())) {
-        ptrdiff_t const delta =
-            m.index().ResolvedValue() +
-            MacroAssemblerBase::RootRegisterOffsetForExternalReference(
-                selector()->isolate(), m.object().ResolvedValue());
-        if (is_int32(delta)) {
-          inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
-          return kMode_Root;
+    if constexpr (Adapter::IsTurboshaft) {
+      UNIMPLEMENTED();
+    } else {
+      {
+        LoadMatcher<ExternalReferenceMatcher> m(node);
+        if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
+            selector()->CanAddressRelativeToRootsRegister(
+                m.object().ResolvedValue())) {
+          ptrdiff_t const delta =
+              m.index().ResolvedValue() +
+              MacroAssemblerBase::RootRegisterOffsetForExternalReference(
+                  selector()->isolate(), m.object().ResolvedValue());
+          if (is_int32(delta)) {
+            inputs[(*input_count)++] =
+                TempImmediate(static_cast<int32_t>(delta));
+            return kMode_Root;
+          }
         }
       }
-    }
 
-    BaseWithIndexAndDisplacement32Matcher m(node, AddressOption::kAllowAll);
-    DCHECK(m.matches());
-    if (m.base() != nullptr &&
-        m.base()->opcode() == IrOpcode::kLoadRootRegister) {
-      DCHECK_EQ(m.index(), nullptr);
-      DCHECK_EQ(m.scale(), 0);
-      inputs[(*input_count)++] = UseImmediate(m.displacement());
-      return kMode_Root;
-    } else if ((m.displacement() == nullptr ||
-                CanBeImmediate(m.displacement()))) {
-      return GenerateMemoryOperandInputs(
-          m.index(), m.scale(), m.base(), m.displacement(),
-          m.displacement_mode(), inputs, input_count, register_mode);
-    } else {
-      inputs[(*input_count)++] =
-          UseRegisterWithMode(node->InputAt(0), register_mode);
-      inputs[(*input_count)++] =
-          UseRegisterWithMode(node->InputAt(1), register_mode);
-      return kMode_MR1;
+      BaseWithIndexAndDisplacement32Matcher m(node, AddressOption::kAllowAll);
+      DCHECK(m.matches());
+      if (m.base() != nullptr &&
+          m.base()->opcode() == IrOpcode::kLoadRootRegister) {
+        DCHECK_EQ(m.index(), nullptr);
+        DCHECK_EQ(m.scale(), 0);
+        inputs[(*input_count)++] = UseImmediate(m.displacement());
+        return kMode_Root;
+      } else if ((m.displacement() == nullptr ||
+                  CanBeImmediate(m.displacement()))) {
+        return GenerateMemoryOperandInputs(
+            m.index(), m.scale(), m.base(), m.displacement(),
+            m.displacement_mode(), inputs, input_count, register_mode);
+      } else {
+        inputs[(*input_count)++] =
+            UseRegisterWithMode(node->InputAt(0), register_mode);
+        inputs[(*input_count)++] =
+            UseRegisterWithMode(node->InputAt(1), register_mode);
+        return kMode_MR1;
+      }
     }
   }
 
@@ -255,7 +263,8 @@ class IA32OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     }
   }
 
-  bool CanBeBetterLeftOperand(Node* node) const {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanBeBetterLeftOperand)
+  bool CanBeBetterLeftOperand(node_t node) const {
     return !selector()->IsLive(node);
   }
 };
@@ -623,7 +632,7 @@ void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node, node_t value,
                                               InstructionCode opcode) {
   IA32OperandGeneratorT<Adapter> g(this);
   InstructionOperand outputs[1];
@@ -637,8 +646,8 @@ void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
-  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
+  LoadRepresentation load_rep = this->load_view(node).loaded_rep();
   DCHECK(!load_rep.IsMapWord());
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
@@ -856,7 +865,7 @@ namespace {
 // Shared routine for multiple binary operations.
 template <typename Adapter>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
-                InstructionCode opcode, FlagsContinuation* cont) {
+                InstructionCode opcode, FlagsContinuationT<Adapter>* cont) {
   IA32OperandGeneratorT<Adapter> g(selector);
   Int32BinopMatcher m(node);
   Node* left = m.left().node();
@@ -915,7 +924,7 @@ void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter>
 void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                 InstructionCode opcode) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitBinop(selector, node, opcode, &cont);
 }
 
@@ -944,50 +953,55 @@ void InstructionSelectorT<Adapter>::VisitWord32Xor(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
-    Node* node, FlagsContinuation* cont) {
-  StackCheckKind kind = StackCheckKindOf(node->op());
-  InstructionCode opcode =
-      kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
-
-  int effect_level = GetEffectLevel(node, cont);
-
-  IA32OperandGeneratorT<Adapter> g(this);
+    node_t node, FlagsContinuation* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    StackCheckKind kind = StackCheckKindOf(node->op());
+    InstructionCode opcode = kArchStackPointerGreaterThan |
+                             MiscField::encode(static_cast<int>(kind));
 
-  // No outputs.
-  InstructionOperand* const outputs = nullptr;
-  const int output_count = 0;
+    int effect_level = GetEffectLevel(node, cont);
 
-  // Applying an offset to this stack check requires a temp register. Offsets
-  // are only applied to the first stack check. If applying an offset, we must
-  // ensure the input and temp registers do not alias, thus kUniqueRegister.
-  InstructionOperand temps[] = {g.TempRegister()};
-  const int temp_count = (kind == StackCheckKind::kJSFunctionEntry) ? 1 : 0;
-  const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)
-                                 ? OperandGenerator::kUniqueRegister
-                                 : OperandGenerator::kRegister;
+    IA32OperandGeneratorT<Adapter> g(this);
 
-  Node* const value = node->InputAt(0);
-  if (g.CanBeMemoryOperand(kIA32Cmp, node, value, effect_level)) {
-    DCHECK(value->opcode() == IrOpcode::kLoad ||
-           value->opcode() == IrOpcode::kLoadImmutable);
+    // No outputs.
+    InstructionOperand* const outputs = nullptr;
+    const int output_count = 0;
 
-    // GetEffectiveAddressMemoryOperand can create at most 3 inputs.
-    static constexpr int kMaxInputCount = 3;
+    // Applying an offset to this stack check requires a temp register. Offsets
+    // are only applied to the first stack check. If applying an offset, we must
+    // ensure the input and temp registers do not alias, thus kUniqueRegister.
+    InstructionOperand temps[] = {g.TempRegister()};
+    const int temp_count = (kind == StackCheckKind::kJSFunctionEntry) ? 1 : 0;
+    const auto register_mode = (kind == StackCheckKind::kJSFunctionEntry)
+                                   ? OperandGenerator::kUniqueRegister
+                                   : OperandGenerator::kRegister;
+
+    Node* const value = node->InputAt(0);
+    if (g.CanBeMemoryOperand(kIA32Cmp, node, value, effect_level)) {
+      DCHECK(value->opcode() == IrOpcode::kLoad ||
+             value->opcode() == IrOpcode::kLoadImmutable);
+
+      // GetEffectiveAddressMemoryOperand can create at most 3 inputs.
+      static constexpr int kMaxInputCount = 3;
+
+      size_t input_count = 0;
+      InstructionOperand inputs[kMaxInputCount];
+      AddressingMode addressing_mode = g.GetEffectiveAddressMemoryOperand(
+          value, inputs, &input_count, register_mode);
+      opcode |= AddressingModeField::encode(addressing_mode);
+      DCHECK_LE(input_count, kMaxInputCount);
 
-    size_t input_count = 0;
-    InstructionOperand inputs[kMaxInputCount];
-    AddressingMode addressing_mode = g.GetEffectiveAddressMemoryOperand(
-        value, inputs, &input_count, register_mode);
-    opcode |= AddressingModeField::encode(addressing_mode);
-    DCHECK_LE(input_count, kMaxInputCount);
-
-    EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
-                         temp_count, temps, cont);
-  } else {
-    InstructionOperand inputs[] = {g.UseRegisterWithMode(value, register_mode)};
-    static constexpr int input_count = arraysize(inputs);
-    EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
-                         temp_count, temps, cont);
+      EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
+                           temp_count, temps, cont);
+    } else {
+      InstructionOperand inputs[] = {
+          g.UseRegisterWithMode(value, register_mode)};
+      static constexpr int input_count = arraysize(inputs);
+      EmitWithContinuation(opcode, output_count, outputs, input_count, inputs,
+                           temp_count, temps, cont);
+    }
   }
 }
 
@@ -1547,7 +1561,8 @@ void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(node_t node, int index) {
+}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
@@ -1556,58 +1571,63 @@ void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
-    Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
+    node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
 
-  // Prepare for C function call.
-  if (call_descriptor->IsCFunctionCall()) {
-    InstructionOperand temps[] = {g.TempRegister()};
-    size_t const temp_count = arraysize(temps);
-    Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
-                                         call_descriptor->ParameterCount())),
-         0, nullptr, 0, nullptr, temp_count, temps);
-
-    // Poke any stack arguments.
-    for (size_t n = 0; n < arguments->size(); ++n) {
-      PushParameter input = (*arguments)[n];
-      if (input.node) {
-        int const slot = static_cast<int>(n);
-        InstructionOperand value = g.CanBeImmediate(node)
-                                       ? g.UseImmediate(input.node)
-                                       : g.UseRegister(input.node);
-        Emit(kIA32Poke | MiscField::encode(slot), g.NoOutput(), value);
+    // Prepare for C function call.
+    if (call_descriptor->IsCFunctionCall()) {
+      InstructionOperand temps[] = {g.TempRegister()};
+      size_t const temp_count = arraysize(temps);
+      Emit(kArchPrepareCallCFunction | MiscField::encode(static_cast<int>(
+                                           call_descriptor->ParameterCount())),
+           0, nullptr, 0, nullptr, temp_count, temps);
+
+      // Poke any stack arguments.
+      for (size_t n = 0; n < arguments->size(); ++n) {
+        PushParameter input = (*arguments)[n];
+        if (input.node) {
+          int const slot = static_cast<int>(n);
+          InstructionOperand value = g.CanBeImmediate(node)
+                                         ? g.UseImmediate(input.node)
+                                         : g.UseRegister(input.node);
+          Emit(kIA32Poke | MiscField::encode(slot), g.NoOutput(), value);
+        }
       }
-    }
-  } else {
-    // Push any stack arguments.
-    int effect_level = GetEffectLevel(node);
-    int stack_decrement = 0;
-    for (PushParameter input : base::Reversed(*arguments)) {
-      stack_decrement += kSystemPointerSize;
-      // Skip holes in the param array. These represent both extra slots for
-      // multi-slot values and padding slots for alignment.
-      if (input.node == nullptr) continue;
-      InstructionOperand decrement = g.UseImmediate(stack_decrement);
-      stack_decrement = 0;
-      if (g.CanBeImmediate(input.node)) {
-        Emit(kIA32Push, g.NoOutput(), decrement, g.UseImmediate(input.node));
-      } else if (IsSupported(INTEL_ATOM) ||
-                 sequence()->IsFP(GetVirtualRegister(input.node))) {
-        // TODO(bbudge): IA32Push cannot handle stack->stack double moves
-        // because there is no way to encode fixed double slots.
-        Emit(kIA32Push, g.NoOutput(), decrement, g.UseRegister(input.node));
-      } else if (g.CanBeMemoryOperand(kIA32Push, node, input.node,
-                                      effect_level)) {
-        InstructionOperand outputs[1];
-        InstructionOperand inputs[5];
-        size_t input_count = 0;
-        inputs[input_count++] = decrement;
-        AddressingMode mode = g.GetEffectiveAddressMemoryOperand(
-            input.node, inputs, &input_count);
-        InstructionCode opcode = kIA32Push | AddressingModeField::encode(mode);
-        Emit(opcode, 0, outputs, input_count, inputs);
-      } else {
-        Emit(kIA32Push, g.NoOutput(), decrement, g.UseAny(input.node));
+    } else {
+      // Push any stack arguments.
+      int effect_level = GetEffectLevel(node);
+      int stack_decrement = 0;
+      for (PushParameter input : base::Reversed(*arguments)) {
+        stack_decrement += kSystemPointerSize;
+        // Skip holes in the param array. These represent both extra slots for
+        // multi-slot values and padding slots for alignment.
+        if (input.node == nullptr) continue;
+        InstructionOperand decrement = g.UseImmediate(stack_decrement);
+        stack_decrement = 0;
+        if (g.CanBeImmediate(input.node)) {
+          Emit(kIA32Push, g.NoOutput(), decrement, g.UseImmediate(input.node));
+        } else if (IsSupported(INTEL_ATOM) ||
+                   sequence()->IsFP(GetVirtualRegister(input.node))) {
+          // TODO(bbudge): IA32Push cannot handle stack->stack double moves
+          // because there is no way to encode fixed double slots.
+          Emit(kIA32Push, g.NoOutput(), decrement, g.UseRegister(input.node));
+        } else if (g.CanBeMemoryOperand(kIA32Push, node, input.node,
+                                        effect_level)) {
+          InstructionOperand outputs[1];
+          InstructionOperand inputs[5];
+          size_t input_count = 0;
+          inputs[input_count++] = decrement;
+          AddressingMode mode = g.GetEffectiveAddressMemoryOperand(
+              input.node, inputs, &input_count);
+          InstructionCode opcode =
+              kIA32Push | AddressingModeField::encode(mode);
+          Emit(opcode, 0, outputs, input_count, inputs);
+        } else {
+          Emit(kIA32Push, g.NoOutput(), decrement, g.UseAny(input.node));
+        }
       }
     }
   }
@@ -1616,25 +1636,29 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
+    node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
 
-  for (PushParameter output : *results) {
-    if (!output.location.IsCallerFrameSlot()) continue;
-    // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
-      DCHECK(!call_descriptor->IsCFunctionCall());
-      if (output.location.GetType() == MachineType::Float32()) {
-        MarkAsFloat32(output.node);
-      } else if (output.location.GetType() == MachineType::Float64()) {
-        MarkAsFloat64(output.node);
-      } else if (output.location.GetType() == MachineType::Simd128()) {
-        MarkAsSimd128(output.node);
+    for (PushParameter output : *results) {
+      if (!output.location.IsCallerFrameSlot()) continue;
+      // Skip any alignment holes in nodes.
+      if (output.node != nullptr) {
+        DCHECK(!call_descriptor->IsCFunctionCall());
+        if (output.location.GetType() == MachineType::Float32()) {
+          MarkAsFloat32(output.node);
+        } else if (output.location.GetType() == MachineType::Float64()) {
+          MarkAsFloat64(output.node);
+        } else if (output.location.GetType() == MachineType::Simd128()) {
+          MarkAsSimd128(output.node);
+        }
+        int offset = call_descriptor->GetOffsetToReturns();
+        int reverse_slot = -output.location.GetLocation() - offset;
+        Emit(kIA32Peek, g.DefineAsRegister(output.node),
+             g.UseImmediate(reverse_slot));
       }
-      int offset = call_descriptor->GetOffsetToReturns();
-      int reverse_slot = -output.location.GetLocation() - offset;
-      Emit(kIA32Peek, g.DefineAsRegister(output.node),
-           g.UseImmediate(reverse_slot));
     }
   }
 }
@@ -1650,7 +1674,7 @@ template <typename Adapter>
 void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
                                    InstructionCode opcode, Node* left,
                                    InstructionOperand right,
-                                   FlagsContinuation* cont) {
+                                   FlagsContinuationT<Adapter>* cont) {
   DCHECK(left->opcode() == IrOpcode::kLoad ||
          left->opcode() == IrOpcode::kLoadImmutable);
   IA32OperandGeneratorT<Adapter> g(selector);
@@ -1668,15 +1692,16 @@ void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
                   InstructionCode opcode, InstructionOperand left,
-                  InstructionOperand right, FlagsContinuation* cont) {
+                  InstructionOperand right, FlagsContinuationT<Adapter>* cont) {
   selector->EmitWithContinuation(opcode, left, right, cont);
 }
 
 // Shared routine for multiple compare operations.
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
-                  InstructionCode opcode, Node* left, Node* right,
-                  FlagsContinuation* cont, bool commutative) {
+                  InstructionCode opcode, typename Adapter::node_t left,
+                  typename Adapter::node_t right,
+                  FlagsContinuationT<Adapter>* cont, bool commutative) {
   IA32OperandGeneratorT<Adapter> g(selector);
   if (commutative && g.CanBeBetterLeftOperand(right)) {
     std::swap(left, right);
@@ -1728,8 +1753,10 @@ MachineType MachineTypeForNarrow(Node* node, Node* hint_node) {
 
 // Tries to match the size of the given opcode to that of the operands, if
 // possible.
+template <typename Adapter>
 InstructionCode TryNarrowOpcodeSize(InstructionCode opcode, Node* left,
-                                    Node* right, FlagsContinuation* cont) {
+                                    Node* right,
+                                    FlagsContinuationT<Adapter>* cont) {
   // TODO(epertoso): we can probably get some size information out of phi nodes.
   // If the load representations don't match, both operands will be
   // zero/sign-extended to 32bit.
@@ -1770,71 +1797,79 @@ InstructionCode TryNarrowOpcodeSize(InstructionCode opcode, Node* left,
 
 // Shared routine for multiple float32 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   VisitCompare(selector, kIA32Float32Cmp, right, left, cont, false);
 }
 
 // Shared routine for multiple float64 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   VisitCompare(selector, kIA32Float64Cmp, right, left, cont, false);
 }
 
 // Shared routine for multiple word compare operations.
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont) {
-  IA32OperandGeneratorT<Adapter> g(selector);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node, InstructionCode opcode,
+                      FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<TurbofanAdapter> g(selector);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
 
-  InstructionCode narrowed_opcode =
-      TryNarrowOpcodeSize(opcode, left, right, cont);
+    InstructionCode narrowed_opcode =
+        TryNarrowOpcodeSize(opcode, left, right, cont);
 
-  int effect_level = selector->GetEffectLevel(node, cont);
+    int effect_level = selector->GetEffectLevel(node, cont);
 
-  // If one of the two inputs is an immediate, make sure it's on the right, or
-  // if one of the two inputs is a memory operand, make sure it's on the left.
-  if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
-      (g.CanBeMemoryOperand(narrowed_opcode, node, right, effect_level) &&
-       !g.CanBeMemoryOperand(narrowed_opcode, node, left, effect_level))) {
-    if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
-    std::swap(left, right);
-  }
+    // If one of the two inputs is an immediate, make sure it's on the right, or
+    // if one of the two inputs is a memory operand, make sure it's on the left.
+    if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
+        (g.CanBeMemoryOperand(narrowed_opcode, node, right, effect_level) &&
+         !g.CanBeMemoryOperand(narrowed_opcode, node, left, effect_level))) {
+      if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
+      std::swap(left, right);
+    }
 
-  // Match immediates on right side of comparison.
-  if (g.CanBeImmediate(right)) {
+    // Match immediates on right side of comparison.
+    if (g.CanBeImmediate(right)) {
+      if (g.CanBeMemoryOperand(narrowed_opcode, node, left, effect_level)) {
+        return VisitCompareWithMemoryOperand(selector, narrowed_opcode, left,
+                                             g.UseImmediate(right), cont);
+      }
+      return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
+                          cont);
+    }
+
+    // Match memory operands on left side of comparison.
     if (g.CanBeMemoryOperand(narrowed_opcode, node, left, effect_level)) {
-      return VisitCompareWithMemoryOperand(selector, narrowed_opcode, left,
-                                           g.UseImmediate(right), cont);
+      bool needs_byte_register =
+          narrowed_opcode == kIA32Test8 || narrowed_opcode == kIA32Cmp8;
+      return VisitCompareWithMemoryOperand(
+          selector, narrowed_opcode, left,
+          needs_byte_register ? g.UseByteRegister(right) : g.UseRegister(right),
+          cont);
     }
-    return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
-                        cont);
-  }
 
-  // Match memory operands on left side of comparison.
-  if (g.CanBeMemoryOperand(narrowed_opcode, node, left, effect_level)) {
-    bool needs_byte_register =
-        narrowed_opcode == kIA32Test8 || narrowed_opcode == kIA32Cmp8;
-    return VisitCompareWithMemoryOperand(
-        selector, narrowed_opcode, left,
-        needs_byte_register ? g.UseByteRegister(right) : g.UseRegister(right),
-        cont);
+    return VisitCompare(selector, opcode, left, right, cont,
+                        node->op()->HasProperty(Operator::kCommutative));
   }
-
-  return VisitCompare(selector, opcode, left, right, cont,
-                      node->op()->HasProperty(Operator::kCommutative));
 }
 
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      FlagsContinuation* cont) {
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node,
+                      FlagsContinuationT<Adapter>* cont) {
   VisitWordCompare(selector, node, kIA32Cmp, cont);
 }
 
@@ -1902,95 +1937,100 @@ void VisitPairAtomicBinOp(InstructionSelectorT<Adapter>* selector, Node* node,
 // Shared routine for word comparison with zero.
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWordCompareZero(
-    Node* user, Node* value, FlagsContinuation* cont) {
-  // Try to combine with comparisons against 0 by simply inverting the branch.
-  while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
-    Int32BinopMatcher m(value);
-    if (!m.right().Is(0)) break;
-
-    user = value;
-    value = m.left().node();
-    cont->Negate();
-  }
-
-  if (CanCover(user, value)) {
-    switch (value->opcode()) {
-      case IrOpcode::kWord32Equal:
-        cont->OverwriteAndNegateIfEqual(kEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kInt32LessThan:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThan);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kInt32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kUint32LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kUint32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kFloat32Equal:
-        cont->OverwriteAndNegateIfEqual(kUnorderedEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThan);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat32LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThanOrEqual);
-        return VisitFloat32Compare(this, value, cont);
-      case IrOpcode::kFloat64Equal:
-        cont->OverwriteAndNegateIfEqual(kUnorderedEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThan:
-        cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThan);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kFloat64LessThanOrEqual:
-        cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThanOrEqual);
-        return VisitFloat64Compare(this, value, cont);
-      case IrOpcode::kProjection:
-        // Check if this is the overflow output projection of an
-        // <Operation>WithOverflow node.
-        if (ProjectionIndexOf(value->op()) == 1u) {
-          // We cannot combine the <Operation>WithOverflow with this branch
-          // unless the 0th projection (the use of the actual value of the
-          // <Operation> is either nullptr, which means there's no use of the
-          // actual value, or was already defined, which means it is scheduled
-          // *AFTER* this branch).
-          Node* const node = value->InputAt(0);
-          Node* const result = NodeProperties::FindProjection(node, 0);
-          if (result == nullptr || IsDefined(result)) {
-            switch (node->opcode()) {
-              case IrOpcode::kInt32AddWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop(this, node, kIA32Add, cont);
-              case IrOpcode::kInt32SubWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop(this, node, kIA32Sub, cont);
-              case IrOpcode::kInt32MulWithOverflow:
-                cont->OverwriteAndNegateIfEqual(kOverflow);
-                return VisitBinop(this, node, kIA32Imul, cont);
-              default:
-                break;
+    node_t user, node_t value, FlagsContinuation* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    // Try to combine with comparisons against 0 by simply inverting the branch.
+    while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
+      Int32BinopMatcher m(value);
+      if (!m.right().Is(0)) break;
+
+      user = value;
+      value = m.left().node();
+      cont->Negate();
+    }
+
+    if (CanCover(user, value)) {
+      switch (value->opcode()) {
+        case IrOpcode::kWord32Equal:
+          cont->OverwriteAndNegateIfEqual(kEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kInt32LessThan:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThan);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kInt32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kSignedLessThanOrEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kUint32LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThan);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kUint32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedLessThanOrEqual);
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kFloat32Equal:
+          cont->OverwriteAndNegateIfEqual(kUnorderedEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThan);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat32LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThanOrEqual);
+          return VisitFloat32Compare(this, value, cont);
+        case IrOpcode::kFloat64Equal:
+          cont->OverwriteAndNegateIfEqual(kUnorderedEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThan:
+          cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThan);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kFloat64LessThanOrEqual:
+          cont->OverwriteAndNegateIfEqual(kUnsignedGreaterThanOrEqual);
+          return VisitFloat64Compare(this, value, cont);
+        case IrOpcode::kProjection:
+          // Check if this is the overflow output projection of an
+          // <Operation>WithOverflow node.
+          if (ProjectionIndexOf(value->op()) == 1u) {
+            // We cannot combine the <Operation>WithOverflow with this branch
+            // unless the 0th projection (the use of the actual value of the
+            // <Operation> is either nullptr, which means there's no use of the
+            // actual value, or was already defined, which means it is scheduled
+            // *AFTER* this branch).
+            Node* const node = value->InputAt(0);
+            Node* const result = NodeProperties::FindProjection(node, 0);
+            if (result == nullptr || IsDefined(result)) {
+              switch (node->opcode()) {
+                case IrOpcode::kInt32AddWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop(this, node, kIA32Add, cont);
+                case IrOpcode::kInt32SubWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop(this, node, kIA32Sub, cont);
+                case IrOpcode::kInt32MulWithOverflow:
+                  cont->OverwriteAndNegateIfEqual(kOverflow);
+                  return VisitBinop(this, node, kIA32Imul, cont);
+                default:
+                  break;
+              }
             }
           }
-        }
-        break;
-      case IrOpcode::kInt32Sub:
-        return VisitWordCompare(this, value, cont);
-      case IrOpcode::kWord32And:
-        return VisitWordCompare(this, value, kIA32Test, cont);
-      case IrOpcode::kStackPointerGreaterThan:
-        cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
-        return VisitStackPointerGreaterThan(value, cont);
-      default:
-        break;
+          break;
+        case IrOpcode::kInt32Sub:
+          return VisitWordCompare(this, value, cont);
+        case IrOpcode::kWord32And:
+          return VisitWordCompare(this, value, kIA32Test, cont);
+        case IrOpcode::kStackPointerGreaterThan:
+          cont->OverwriteAndNegateIfEqual(kStackPointerGreaterThanCondition);
+          return VisitStackPointerGreaterThan(value, cont);
+        default:
+          break;
+      }
     }
-  }
 
-  // Continuation could not be combined with a compare, emit compare against 0.
-  IA32OperandGeneratorT<Adapter> g(this);
-  VisitCompare(this, kIA32Cmp, g.Use(value), g.TempImmediate(0), cont);
+    // Continuation could not be combined with a compare, emit compare against
+    // 0.
+    IA32OperandGeneratorT<Adapter> g(this);
+    VisitCompare(this, kIA32Cmp, g.Use(value), g.TempImmediate(0), cont);
+  }
 }
 
 template <typename Adapter>
@@ -2000,7 +2040,8 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+  if (enable_switch_jump_table_ ==
+      InstructionSelector::kEnableSwitchJumpTable) {
     static const size_t kMaxTableSwitchValueRange = 2 << 16;
     size_t table_space_cost = 4 + sw.value_range();
     size_t table_time_cost = 3;
@@ -2027,36 +2068,40 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
+void InstructionSelectorT<Adapter>::VisitWord32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int32BinopMatcher m(node);
-  if (m.right().Is(0)) {
-    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Int32BinopMatcher m(node);
+    if (m.right().Is(0)) {
+      return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+    }
   }
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, &cont);
@@ -2064,69 +2109,81 @@ void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kIA32Add, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kIA32Add, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kIA32Add, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kIA32Add, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kIA32Sub, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kIA32Sub, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kIA32Sub, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kIA32Sub, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kIA32Imul, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kIA32Imul, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kIA32Imul, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kIA32Imul, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
@@ -2134,33 +2191,45 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  Float64Matcher mleft(left);
-  if (mleft.HasResolvedValue() &&
-      (base::bit_cast<uint64_t>(mleft.ResolvedValue()) >> 32) == 0u) {
-    Emit(kIA32Float64LoadLowWord32, g.DefineAsRegister(node), g.Use(right));
-    return;
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    Float64Matcher mleft(left);
+    if (mleft.HasResolvedValue() &&
+        (base::bit_cast<uint64_t>(mleft.ResolvedValue()) >> 32) == 0u) {
+      Emit(kIA32Float64LoadLowWord32, g.DefineAsRegister(node), g.Use(right));
+      return;
+    }
+    Emit(kIA32Float64InsertLowWord32, g.DefineSameAsFirst(node),
+         g.UseRegister(left), g.Use(right));
   }
-  Emit(kIA32Float64InsertLowWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.Use(right));
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  Emit(kIA32Float64InsertHighWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.Use(right));
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    Emit(kIA32Float64InsertHighWord32, g.DefineSameAsFirst(node),
+         g.UseRegister(left), g.Use(right));
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
-  Emit(kIA32Float64SilenceNaN, g.DefineSameAsFirst(node),
-       g.UseRegister(node->InputAt(0)));
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
+    Emit(kIA32Float64SilenceNaN, g.DefineSameAsFirst(node),
+         g.UseRegister(node->InputAt(0)));
+  }
 }
 
 template <typename Adapter>
@@ -3575,14 +3644,13 @@ void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
-    OperandGenerator* g, int first_input_index, Node* node) {
+    OperandGeneratorT<Adapter>* g, int first_input_index, node_t node) {
   UNREACHABLE();
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags =
       MachineOperatorBuilder::kWord32ShiftIsSafe |
       MachineOperatorBuilder::kWord32Ctz | MachineOperatorBuilder::kWord32Rol;
@@ -3603,9 +3671,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/backend/instruction-selector-adapter.h b/src/compiler/backend/instruction-selector-adapter.h
new file mode 100644
index 00000000000..3868bd0b479
--- /dev/null
+++ b/src/compiler/backend/instruction-selector-adapter.h
@@ -0,0 +1,468 @@
+// Copyright 2023 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_COMPILER_BACKEND_INSTRUCTION_SELECTOR_ADAPTER_H_
+#define V8_COMPILER_BACKEND_INSTRUCTION_SELECTOR_ADAPTER_H_
+
+#include "src/compiler/backend/instruction.h"
+#include "src/compiler/common-operator.h"
+#include "src/compiler/node-matchers.h"
+#include "src/compiler/schedule.h"
+#include "src/compiler/turboshaft/graph.h"
+#include "src/compiler/turboshaft/operations.h"
+
+// TODO(nicohartmann@):
+// During the transition period to a generic instruction selector, some
+// instantiations with TurboshaftAdapter will still call functions with
+// Node* arguments. Use `DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK` to define
+// a temporary fallback for these functions such that compilation is possible
+// while transitioning the instruction selector incrementally. Once all uses
+// of Node*, BasicBlock*, ... have been replaced, remove those fallbacks.
+#define DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(ret, name)             \
+  template <typename... Args>                                          \
+  std::enable_if_t<Adapter::IsTurboshaft &&                            \
+                       detail::AnyTurbofanNodeOrBlock<Args...>::value, \
+                   ret>                                                \
+  name(Args...) {                                                      \
+    UNREACHABLE();                                                     \
+  }
+
+namespace v8::internal::compiler {
+namespace detail {
+template <typename...>
+struct AnyTurbofanNodeOrBlock;
+template <typename Head, typename... Tail>
+struct AnyTurbofanNodeOrBlock<Head, Tail...> {
+  static constexpr bool value = std::is_same_v<Head, Node*> ||
+                                std::is_same_v<Head, BasicBlock*> ||
+                                AnyTurbofanNodeOrBlock<Tail...>::value;
+};
+template <>
+struct AnyTurbofanNodeOrBlock<> {
+  static constexpr bool value = false;
+};
+}  // namespace detail
+
+struct TurbofanAdapter {
+  static constexpr bool IsTurbofan = true;
+  static constexpr bool IsTurboshaft = false;
+  using schedule_t = Schedule*;
+  using block_t = BasicBlock*;
+  using block_range_t = ZoneVector<block_t>;
+  using node_t = Node*;
+  using inputs_t = Node::Inputs;
+  using opcode_t = IrOpcode::Value;
+  using id_t = uint32_t;
+
+  class CallView {
+   public:
+    explicit CallView(node_t node) : node_(node) {
+      DCHECK(node_->opcode() == IrOpcode::kCall ||
+             node_->opcode() == IrOpcode::kTailCall);
+    }
+
+    int return_count() const { return node_->op()->ValueOutputCount(); }
+    node_t callee() const { return node_->InputAt(0); }
+    node_t frame_state() const {
+      return node_->InputAt(static_cast<int>(call_descriptor()->InputCount()));
+    }
+    base::Vector<node_t> arguments() const {
+      base::Vector<node_t> inputs = node_->inputs_vector();
+      return inputs.SubVector(1, inputs.size());
+    }
+    const CallDescriptor* call_descriptor() const {
+      return CallDescriptorOf(node_->op());
+    }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+  };
+
+  class BranchView {
+   public:
+    explicit BranchView(node_t node) : node_(node) {
+      DCHECK_EQ(node_->opcode(), IrOpcode::kBranch);
+    }
+
+    node_t condition() const { return node_->InputAt(0); }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+  };
+
+  class WordBinopView {
+   public:
+    explicit WordBinopView(node_t node) : node_(node), m_(node) {}
+
+    void EnsureConstantIsRightIfCommutative() {
+      // Nothing to do. Matcher already ensures that.
+    }
+
+    node_t left() const { return m_.left().node(); }
+    node_t right() const { return m_.right().node(); }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+    Int32BinopMatcher m_;
+  };
+
+  class LoadView {
+   public:
+    explicit LoadView(node_t node) : node_(node) {
+      DCHECK(node_->opcode() == IrOpcode::kLoad ||
+             node_->opcode() == IrOpcode::kLoadImmutable ||
+             node_->opcode() == IrOpcode::kProtectedLoad ||
+             node_->opcode() == IrOpcode::kLoadTrapOnNull);
+    }
+
+    LoadRepresentation loaded_rep() const {
+      return LoadRepresentationOf(node_->op());
+    }
+
+    node_t base() const { return node_->InputAt(0); }
+    node_t index() const { return node_->InputAt(1); }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+  };
+
+  CallView call_view(node_t node) { return CallView{node}; }
+  BranchView branch_view(node_t node) { return BranchView(node); }
+  WordBinopView word_binop_view(node_t node) { return WordBinopView(node); }
+  LoadView load_view(node_t node) { return LoadView(node); }
+
+  void InitializeAdapter(schedule_t) {}
+
+  block_t block(schedule_t schedule, node_t node) const {
+    return schedule->block(node);
+  }
+
+  RpoNumber rpo_number(block_t block) const {
+    return RpoNumber::FromInt(block->rpo_number());
+  }
+
+  const block_range_t& rpo_order(schedule_t schedule) const {
+    return *schedule->rpo_order();
+  }
+
+  bool IsLoopHeader(block_t block) const { return block->IsLoopHeader(); }
+
+  size_t PredecessorCount(block_t block) const {
+    return block->PredecessorCount();
+  }
+  block_t PredecessorAt(block_t block, size_t index) const {
+    return block->PredecessorAt(index);
+  }
+
+  base::iterator_range<NodeVector::iterator> nodes(block_t block) {
+    return {block->begin(), block->end()};
+  }
+
+  bool IsPhi(node_t node) const { return node->opcode() == IrOpcode::kPhi; }
+  bool IsRetain(node_t node) const {
+    return node->opcode() == IrOpcode::kRetain;
+  }
+  bool IsHeapConstant(node_t node) const {
+    return node->opcode() == IrOpcode::kHeapConstant;
+  }
+  bool IsExternalConstant(node_t node) const {
+    return node->opcode() == IrOpcode::kExternalConstant;
+  }
+  bool IsRelocatableWasmConstant(node_t node) const {
+    return node->opcode() == IrOpcode::kRelocatableInt32Constant ||
+           node->opcode() == IrOpcode::kRelocatableInt64Constant;
+  }
+  bool IsLoadOrLoadImmutable(node_t node) const {
+    return node->opcode() == IrOpcode::kLoad ||
+           node->opcode() == IrOpcode::kLoadImmutable;
+  }
+
+  int value_input_count(node_t node) const {
+    return node->op()->ValueInputCount();
+  }
+  node_t input_at(node_t node, size_t index) const {
+    return node->InputAt(static_cast<int>(index));
+  }
+  inputs_t inputs(node_t node) const { return node->inputs(); }
+  opcode_t opcode(node_t node) const { return node->opcode(); }
+  bool is_exclusive_user_of(node_t user, node_t value) const {
+    for (Edge const edge : value->use_edges()) {
+      if (edge.from() != user && NodeProperties::IsValueEdge(edge)) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  id_t id(node_t node) const { return node->id(); }
+  static bool valid(node_t node) { return node != nullptr; }
+
+  node_t block_terminator(block_t block) const {
+    return block->control_input();
+  }
+  node_t parent_frame_state(node_t node) const {
+    DCHECK_EQ(node->opcode(), IrOpcode::kFrameState);
+    DCHECK_EQ(FrameState{node}.outer_frame_state(),
+              NodeProperties::GetFrameStateInput(node));
+    return NodeProperties::GetFrameStateInput(node);
+  }
+  int parameter_index_of(node_t node) const {
+    DCHECK(node->opcode() == IrOpcode::kParameter);
+    return ParameterIndexOf(node->op());
+  }
+
+  bool IsRequiredWhenUnused(node_t node) const {
+    return !node->op()->HasProperty(Operator::kEliminatable);
+  }
+  bool IsCommutative(node_t node) const {
+    return node->op()->HasProperty(Operator::kCommutative);
+  }
+};
+
+struct TurboshaftAdapter {
+  static constexpr bool IsTurbofan = false;
+  static constexpr bool IsTurboshaft = true;
+  // TODO(nicohartmann@): Rename schedule_t once Turbofan is gone.
+  using schedule_t = turboshaft::Graph*;
+  using block_t = turboshaft::Block*;
+  using block_range_t = ZoneVector<block_t>;
+  using node_t = turboshaft::OpIndex;
+  using inputs_t = base::Vector<const node_t>;
+  using opcode_t = turboshaft::Opcode;
+  using id_t = uint32_t;
+
+  class CallView {
+   public:
+    explicit CallView(turboshaft::Graph* graph, node_t node) : node_(node) {
+      if (graph->Get(node_).Is<turboshaft::TailCallOp>()) {
+        UNIMPLEMENTED();
+      }
+      op_ = &graph->Get(node_).Cast<turboshaft::CallOp>();
+    }
+
+    int return_count() const {
+      return static_cast<int>(op_->outputs_rep().size());
+    }
+    node_t callee() const { return op_->callee(); }
+    node_t frame_state() const { return op_->frame_state(); }
+    base::Vector<const node_t> arguments() const { return op_->arguments(); }
+    const CallDescriptor* call_descriptor() const {
+      return op_->descriptor->descriptor;
+    }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+    const turboshaft::CallOp* op_;
+  };
+
+  class BranchView {
+   public:
+    explicit BranchView(turboshaft::Graph* graph, node_t node) : node_(node) {
+      op_ = &graph->Get(node_).Cast<turboshaft::BranchOp>();
+    }
+
+    node_t condition() const { return op_->condition(); }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+    const turboshaft::BranchOp* op_;
+  };
+
+  class WordBinopView {
+   public:
+    explicit WordBinopView(turboshaft::Graph* graph, node_t node)
+        : node_(node) {
+      op_ = &graph->Get(node_).Cast<turboshaft::WordBinopOp>();
+      left_ = op_->left();
+      right_ = op_->right();
+      can_put_constant_right_ =
+          op_->IsCommutative(op_->kind) &&
+          graph->Get(left_).Is<turboshaft::ConstantOp>() &&
+          !graph->Get(right_).Is<turboshaft::ConstantOp>();
+    }
+
+    void EnsureConstantIsRightIfCommutative() {
+      if (can_put_constant_right_) {
+        std::swap(left_, right_);
+        can_put_constant_right_ = false;
+      }
+    }
+
+    node_t left() const { return left_; }
+    node_t right() const { return right_; }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+    const turboshaft::WordBinopOp* op_;
+    node_t left_;
+    node_t right_;
+    bool can_put_constant_right_;
+  };
+
+  class LoadView {
+   public:
+    explicit LoadView(turboshaft::Graph* graph, node_t node) : node_(node) {
+      op_ = &graph->Get(node_).Cast<turboshaft::LoadOp>();
+    }
+
+    LoadRepresentation loaded_rep() const {
+      return op_->loaded_rep.ToMachineType();
+    }
+
+    node_t base() const { return op_->base(); }
+    node_t index() const { return op_->index(); }
+
+    operator node_t() const { return node_; }
+
+   private:
+    node_t node_;
+    const turboshaft::LoadOp* op_;
+  };
+
+  CallView call_view(node_t node) { return CallView{graph_, node}; }
+  BranchView branch_view(node_t node) { return BranchView(graph_, node); }
+  WordBinopView word_binop_view(node_t node) {
+    return WordBinopView(graph_, node);
+  }
+  LoadView load_view(node_t node) { return LoadView(graph_, node); }
+
+  void InitializeAdapter(schedule_t schedule) { graph_ = schedule; }
+  turboshaft::Graph* turboshaft_graph() const { return graph_; }
+
+  block_t block(schedule_t schedule, node_t node) const {
+    // TODO(nicohartmann@): This might be too slow and we should consider
+    // precomputing.
+    return &schedule->Get(schedule->BlockOf(node));
+  }
+
+  RpoNumber rpo_number(block_t block) const {
+    return RpoNumber::FromInt(block->index().id());
+  }
+
+  const block_range_t& rpo_order(schedule_t schedule) {
+    return schedule->blocks_vector();
+  }
+
+  bool IsLoopHeader(block_t block) const { return block->IsLoop(); }
+
+  size_t PredecessorCount(block_t block) const {
+    return block->PredecessorCount();
+  }
+  block_t PredecessorAt(block_t block, size_t index) const {
+    return block->Predecessors()[index];
+  }
+
+  base::iterator_range<turboshaft::Graph::OpIndexIterator> nodes(
+      block_t block) {
+    return graph_->OperationIndices(*block);
+  }
+
+  bool IsPhi(node_t node) const {
+    return graph_->Get(node).Is<turboshaft::PhiOp>();
+  }
+  bool IsRetain(node_t node) const {
+    return graph_->Get(node).Is<turboshaft::RetainOp>();
+  }
+  bool IsHeapConstant(node_t node) const {
+    turboshaft::ConstantOp* constant =
+        graph_->Get(node).TryCast<turboshaft::ConstantOp>();
+    if (constant == nullptr) return false;
+    return constant->kind == turboshaft::ConstantOp::Kind::kHeapObject;
+  }
+  bool IsExternalConstant(node_t node) const {
+    turboshaft::ConstantOp* constant =
+        graph_->Get(node).TryCast<turboshaft::ConstantOp>();
+    if (constant == nullptr) return false;
+    return constant->kind == turboshaft::ConstantOp::Kind::kExternal;
+  }
+  bool IsRelocatableWasmConstant(node_t node) const {
+    turboshaft::ConstantOp* constant =
+        graph_->Get(node).TryCast<turboshaft::ConstantOp>();
+    if (constant == nullptr) return false;
+    return constant->kind ==
+           turboshaft::any_of(
+               turboshaft::ConstantOp::Kind::kRelocatableWasmCall,
+               turboshaft::ConstantOp::Kind::kRelocatableWasmStubCall);
+  }
+  bool IsLoadOrLoadImmutable(node_t node) const {
+    return graph_->Get(node).opcode == turboshaft::Opcode::kLoad;
+  }
+
+  int value_input_count(node_t node) const {
+    return graph_->Get(node).input_count;
+  }
+  node_t input_at(node_t node, size_t index) const {
+    return graph_->Get(node).input(index);
+  }
+  inputs_t inputs(node_t node) const { return graph_->Get(node).inputs(); }
+  opcode_t opcode(node_t node) const { return graph_->Get(node).opcode; }
+  bool is_exclusive_user_of(node_t user, node_t value) const {
+    DCHECK(valid(user));
+    DCHECK(valid(value));
+    const size_t use_count = base::count_if(
+        graph_->Get(user).inputs(),
+        [value](turboshaft::OpIndex input) { return input == value; });
+    DCHECK_LT(0, use_count);
+    DCHECK_LE(use_count, graph_->Get(value).saturated_use_count.Get());
+    const turboshaft::Operation& value_op = graph_->Get(value);
+    return (value_op.saturated_use_count.Get() == use_count) &&
+           !value_op.saturated_use_count.IsSaturated();
+  }
+
+  id_t id(node_t node) const { return node.id(); }
+  static bool valid(node_t node) { return node.valid(); }
+
+  node_t block_terminator(block_t block) const {
+    return graph_->PreviousIndex(block->end());
+  }
+  node_t parent_frame_state(node_t node) const {
+    const turboshaft::FrameStateOp& frame_state =
+        graph_->Get(node).Cast<turboshaft::FrameStateOp>();
+    return frame_state.parent_frame_state();
+  }
+  int parameter_index_of(node_t node) const {
+    const turboshaft::ParameterOp& parameter =
+        graph_->Get(node).Cast<turboshaft::ParameterOp>();
+    return parameter.parameter_index;
+  }
+
+  bool IsRequiredWhenUnused(node_t node) const {
+    return graph_->Get(node).IsRequiredWhenUnused();
+  }
+  bool IsCommutative(node_t node) const {
+    const turboshaft::Operation& op = graph_->Get(node);
+    if (const auto binop = op.TryCast<turboshaft::WordBinopOp>()) {
+      return turboshaft::WordBinopOp::IsCommutative(binop->kind);
+    } else if (const auto binop =
+                   op.TryCast<turboshaft::OverflowCheckedBinopOp>()) {
+      return turboshaft::OverflowCheckedBinopOp::IsCommutative(binop->kind);
+    } else if (const auto binop = op.TryCast<turboshaft::FloatBinopOp>()) {
+      return turboshaft::FloatBinopOp::IsCommutative(binop->kind);
+    } else if (op.Is<turboshaft::EqualOp>()) {
+      return turboshaft::EqualOp::IsCommutative();
+    }
+    return false;
+  }
+
+ private:
+  turboshaft::Graph* graph_;
+};
+
+}  // namespace v8::internal::compiler
+
+#endif  // V8_COMPILER_BACKEND_INSTRUCTION_SELECTOR_ADAPTER_H_
diff --git a/src/compiler/backend/instruction-selector-impl.h b/src/compiler/backend/instruction-selector-impl.h
index 3741ead2b2f..174d6d30a90 100644
--- a/src/compiler/backend/instruction-selector-impl.h
+++ b/src/compiler/backend/instruction-selector-impl.h
@@ -70,6 +70,7 @@ class SwitchInfo {
 
 #define OPERAND_GENERATOR_T_BOILERPLATE(adapter)           \
   using super = OperandGeneratorT<adapter>;                \
+  using node_t = typename adapter::node_t;                 \
   using RegisterMode = typename super::RegisterMode;       \
   using RegisterUseKind = typename super::RegisterUseKind; \
   using super::selector;                                   \
@@ -85,53 +86,66 @@ class SwitchInfo {
 // A helper class for the instruction selector that simplifies construction of
 // Operands. This class implements a base for architecture-specific helpers.
 template <typename Adapter>
-class OperandGeneratorT {
+class OperandGeneratorT : public Adapter {
  public:
+  using block_t = typename Adapter::block_t;
+  using node_t = typename Adapter::node_t;
+
   explicit OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
-      : selector_(selector) {}
+      : selector_(selector) {
+    Adapter::InitializeAdapter(selector->schedule());
+  }
 
   InstructionOperand NoOutput() {
     return InstructionOperand();  // Generates an invalid operand.
   }
 
-  InstructionOperand DefineAsRegister(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineAsRegister)
+  InstructionOperand DefineAsRegister(node_t node) {
     return Define(node,
                   UnallocatedOperand(UnallocatedOperand::MUST_HAVE_REGISTER,
                                      GetVReg(node)));
   }
 
-  InstructionOperand DefineSameAsInput(Node* node, int input_index) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineSameAsInput)
+  InstructionOperand DefineSameAsInput(node_t node, int input_index) {
     return Define(node, UnallocatedOperand(GetVReg(node), input_index));
   }
 
-  InstructionOperand DefineSameAsFirst(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineSameAsFirst)
+  InstructionOperand DefineSameAsFirst(node_t node) {
     return DefineSameAsInput(node, 0);
   }
 
-  InstructionOperand DefineAsFixed(Node* node, Register reg) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineAsFixed)
+  InstructionOperand DefineAsFixed(node_t node, Register reg) {
     return Define(node, UnallocatedOperand(UnallocatedOperand::FIXED_REGISTER,
                                            reg.code(), GetVReg(node)));
   }
 
   template <typename FPRegType>
-  InstructionOperand DefineAsFixed(Node* node, FPRegType reg) {
+  InstructionOperand DefineAsFixed(node_t node, FPRegType reg) {
     return Define(node,
                   UnallocatedOperand(UnallocatedOperand::FIXED_FP_REGISTER,
                                      reg.code(), GetVReg(node)));
   }
 
-  InstructionOperand DefineAsConstant(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineAsConstant)
+  InstructionOperand DefineAsConstant(node_t node) {
     selector()->MarkAsDefined(node);
     int virtual_register = GetVReg(node);
     sequence()->AddConstant(virtual_register, ToConstant(node));
     return ConstantOperand(virtual_register);
   }
 
-  InstructionOperand DefineAsLocation(Node* node, LinkageLocation location) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, DefineAsLocation)
+  InstructionOperand DefineAsLocation(node_t node, LinkageLocation location) {
     return Define(node, ToUnallocatedOperand(location, GetVReg(node)));
   }
 
-  InstructionOperand DefineAsDualLocation(Node* node,
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand,
+                                          DefineAsDualLocation)
+  InstructionOperand DefineAsDualLocation(node_t node,
                                           LinkageLocation primary_location,
                                           LinkageLocation secondary_location) {
     return Define(node,
@@ -139,63 +153,74 @@ class OperandGeneratorT {
                       primary_location, secondary_location, GetVReg(node)));
   }
 
-  InstructionOperand Use(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, Use)
+  InstructionOperand Use(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::NONE,
                                         UnallocatedOperand::USED_AT_START,
                                         GetVReg(node)));
   }
 
-  InstructionOperand UseAnyAtEnd(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseAnyAtEnd)
+  InstructionOperand UseAnyAtEnd(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::REGISTER_OR_SLOT,
                                         UnallocatedOperand::USED_AT_END,
                                         GetVReg(node)));
   }
 
-  InstructionOperand UseAny(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseAny)
+  InstructionOperand UseAny(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::REGISTER_OR_SLOT,
                                         UnallocatedOperand::USED_AT_START,
                                         GetVReg(node)));
   }
 
-  InstructionOperand UseRegisterOrSlotOrConstant(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand,
+                                          UseRegisterOrSlotOrConstant)
+  InstructionOperand UseRegisterOrSlotOrConstant(node_t node) {
     return Use(node, UnallocatedOperand(
                          UnallocatedOperand::REGISTER_OR_SLOT_OR_CONSTANT,
                          UnallocatedOperand::USED_AT_START, GetVReg(node)));
   }
 
-  InstructionOperand UseUniqueRegisterOrSlotOrConstant(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand,
+                                          UseUniqueRegisterOrSlotOrConstant)
+  InstructionOperand UseUniqueRegisterOrSlotOrConstant(node_t node) {
     return Use(node, UnallocatedOperand(
                          UnallocatedOperand::REGISTER_OR_SLOT_OR_CONSTANT,
                          GetVReg(node)));
   }
 
-  InstructionOperand UseRegister(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseRegister)
+  InstructionOperand UseRegister(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::MUST_HAVE_REGISTER,
                                         UnallocatedOperand::USED_AT_START,
                                         GetVReg(node)));
   }
 
-  InstructionOperand UseUniqueSlot(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseUniqueSlot)
+  InstructionOperand UseUniqueSlot(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::MUST_HAVE_SLOT,
                                         GetVReg(node)));
   }
 
   // Use register or operand for the node. If a register is chosen, it won't
   // alias any temporary or output registers.
-  InstructionOperand UseUnique(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseUnique)
+  InstructionOperand UseUnique(node_t node) {
     return Use(node,
                UnallocatedOperand(UnallocatedOperand::NONE, GetVReg(node)));
   }
 
   // Use a unique register for the node that does not alias any temporary or
   // output registers.
-  InstructionOperand UseUniqueRegister(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseUniqueRegister)
+  InstructionOperand UseUniqueRegister(node_t node) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::MUST_HAVE_REGISTER,
                                         GetVReg(node)));
   }
 
   enum class RegisterUseKind { kUseRegister, kUseUniqueRegister };
-  InstructionOperand UseRegister(Node* node, RegisterUseKind unique_reg) {
+  InstructionOperand UseRegister(node_t node, RegisterUseKind unique_reg) {
     if (V8_LIKELY(unique_reg == RegisterUseKind::kUseRegister)) {
       return UseRegister(node);
     } else {
@@ -204,13 +229,14 @@ class OperandGeneratorT {
     }
   }
 
-  InstructionOperand UseFixed(Node* node, Register reg) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseFixed)
+  InstructionOperand UseFixed(node_t node, Register reg) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::FIXED_REGISTER,
                                         reg.code(), GetVReg(node)));
   }
 
   template <typename FPRegType>
-  InstructionOperand UseFixed(Node* node, FPRegType reg) {
+  InstructionOperand UseFixed(node_t node, FPRegType reg) {
     return Use(node, UnallocatedOperand(UnallocatedOperand::FIXED_FP_REGISTER,
                                         reg.code(), GetVReg(node)));
   }
@@ -223,15 +249,19 @@ class OperandGeneratorT {
     return sequence()->AddImmediate(Constant(immediate));
   }
 
-  InstructionOperand UseImmediate(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseImmediate)
+  InstructionOperand UseImmediate(node_t node) {
     return sequence()->AddImmediate(ToConstant(node));
   }
 
-  InstructionOperand UseNegatedImmediate(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand,
+                                          UseNegatedImmediate)
+  InstructionOperand UseNegatedImmediate(node_t node) {
     return sequence()->AddImmediate(ToNegatedConstant(node));
   }
 
-  InstructionOperand UseLocation(Node* node, LinkageLocation location) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, UseLocation)
+  InstructionOperand UseLocation(node_t node, LinkageLocation location) {
     return Use(node, ToUnallocatedOperand(location, GetVReg(node)));
   }
 
@@ -274,7 +304,9 @@ class OperandGeneratorT {
     kUniqueRegister,
   };
 
-  InstructionOperand UseRegisterWithMode(Node* node,
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand,
+                                          UseRegisterWithMode)
+  InstructionOperand UseRegisterWithMode(node_t node,
                                          RegisterMode register_mode) {
     return register_mode == kRegister ? UseRegister(node)
                                       : UseUniqueRegister(node);
@@ -335,9 +367,9 @@ class OperandGeneratorT {
     return ToUnallocatedOperand(location, sequence()->NextVirtualRegister());
   }
 
-  InstructionOperand Label(BasicBlock* block) {
-    return sequence()->AddImmediate(
-        Constant(RpoNumber::FromInt(block->rpo_number())));
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(InstructionOperand, Label)
+  InstructionOperand Label(block_t block) {
+    return sequence()->AddImmediate(Constant(this->rpo_number(block)));
   }
 
  protected:
@@ -346,97 +378,135 @@ class OperandGeneratorT {
   Zone* zone() const { return selector()->instruction_zone(); }
 
  private:
-  int GetVReg(Node* node) const { return selector_->GetVirtualRegister(node); }
-
-  static Constant ToConstant(const Node* node) {
-    switch (node->opcode()) {
-      case IrOpcode::kInt32Constant:
-        return Constant(OpParameter<int32_t>(node->op()));
-      case IrOpcode::kInt64Constant:
-        return Constant(OpParameter<int64_t>(node->op()));
-      case IrOpcode::kTaggedIndexConstant: {
-        // Unencoded index value.
-        intptr_t value =
-            static_cast<intptr_t>(OpParameter<int32_t>(node->op()));
-        DCHECK(TaggedIndex::IsValid(value));
-        // Generate it as 32/64-bit constant in a tagged form.
-        Address tagged_index = TaggedIndex::FromIntptr(value).ptr();
-        if (kSystemPointerSize == kInt32Size) {
-          return Constant(static_cast<int32_t>(tagged_index));
-        } else {
-          return Constant(static_cast<int64_t>(tagged_index));
-        }
-      }
-      case IrOpcode::kFloat32Constant:
-        return Constant(OpParameter<float>(node->op()));
-      case IrOpcode::kRelocatableInt32Constant:
-      case IrOpcode::kRelocatableInt64Constant:
-        return Constant(OpParameter<RelocatablePtrConstantInfo>(node->op()));
-      case IrOpcode::kFloat64Constant:
-      case IrOpcode::kNumberConstant:
-        return Constant(OpParameter<double>(node->op()));
-      case IrOpcode::kExternalConstant:
-        return Constant(OpParameter<ExternalReference>(node->op()));
-      case IrOpcode::kComment: {
-        // We cannot use {intptr_t} here, since the Constant constructor would
-        // be ambiguous on some architectures.
-        using ptrsize_int_t =
-            std::conditional<kSystemPointerSize == 8, int64_t, int32_t>::type;
-        return Constant(reinterpret_cast<ptrsize_int_t>(
-            OpParameter<const char*>(node->op())));
-      }
-      case IrOpcode::kHeapConstant:
-        return Constant(HeapConstantOf(node->op()));
-      case IrOpcode::kCompressedHeapConstant:
-        return Constant(HeapConstantOf(node->op()), true);
-      case IrOpcode::kDeadValue: {
-        switch (DeadValueRepresentationOf(node->op())) {
-          case MachineRepresentation::kBit:
-          case MachineRepresentation::kWord32:
-          case MachineRepresentation::kTagged:
-          case MachineRepresentation::kTaggedSigned:
-          case MachineRepresentation::kTaggedPointer:
-          case MachineRepresentation::kCompressed:
-          case MachineRepresentation::kCompressedPointer:
-            return Constant(static_cast<int32_t>(0));
-          case MachineRepresentation::kWord64:
-            return Constant(static_cast<int64_t>(0));
-          case MachineRepresentation::kFloat64:
-            return Constant(static_cast<double>(0));
-          case MachineRepresentation::kFloat32:
-            return Constant(static_cast<float>(0));
+  int GetVReg(node_t node) const { return selector_->GetVirtualRegister(node); }
+
+  Constant ToConstant(node_t node) {
+    if constexpr (Adapter::IsTurboshaft) {
+      using Kind = turboshaft::ConstantOp::Kind;
+      if (const turboshaft::ConstantOp* constant =
+              this->turboshaft_graph()
+                  ->Get(node)
+                  .template TryCast<turboshaft::ConstantOp>()) {
+        switch (constant->kind) {
+          case Kind::kWord32:
+            return Constant(static_cast<int32_t>(constant->word32()));
+          case Kind::kWord64:
+            return Constant(static_cast<int64_t>(constant->word64()));
+          case Kind::kHeapObject:
+          case Kind::kCompressedHeapObject:
+            return Constant(constant->handle(),
+                            constant->kind == Kind::kCompressedHeapObject);
+          case Kind::kExternal:
+            return Constant(constant->external_reference());
           default:
             UNREACHABLE();
         }
-        break;
       }
-      default:
-        break;
+      UNREACHABLE();
+    } else {
+      switch (node->opcode()) {
+        case IrOpcode::kInt32Constant:
+          return Constant(OpParameter<int32_t>(node->op()));
+        case IrOpcode::kInt64Constant:
+          return Constant(OpParameter<int64_t>(node->op()));
+        case IrOpcode::kTaggedIndexConstant: {
+          // Unencoded index value.
+          intptr_t value =
+              static_cast<intptr_t>(OpParameter<int32_t>(node->op()));
+          DCHECK(TaggedIndex::IsValid(value));
+          // Generate it as 32/64-bit constant in a tagged form.
+          Address tagged_index = TaggedIndex::FromIntptr(value).ptr();
+          if (kSystemPointerSize == kInt32Size) {
+            return Constant(static_cast<int32_t>(tagged_index));
+          } else {
+            return Constant(static_cast<int64_t>(tagged_index));
+          }
+        }
+        case IrOpcode::kFloat32Constant:
+          return Constant(OpParameter<float>(node->op()));
+        case IrOpcode::kRelocatableInt32Constant:
+        case IrOpcode::kRelocatableInt64Constant:
+          return Constant(OpParameter<RelocatablePtrConstantInfo>(node->op()));
+        case IrOpcode::kFloat64Constant:
+        case IrOpcode::kNumberConstant:
+          return Constant(OpParameter<double>(node->op()));
+        case IrOpcode::kExternalConstant:
+          return Constant(OpParameter<ExternalReference>(node->op()));
+        case IrOpcode::kComment: {
+          // We cannot use {intptr_t} here, since the Constant constructor would
+          // be ambiguous on some architectures.
+          using ptrsize_int_t =
+              std::conditional<kSystemPointerSize == 8, int64_t, int32_t>::type;
+          return Constant(reinterpret_cast<ptrsize_int_t>(
+              OpParameter<const char*>(node->op())));
+        }
+        case IrOpcode::kHeapConstant:
+          return Constant(HeapConstantOf(node->op()));
+        case IrOpcode::kCompressedHeapConstant:
+          return Constant(HeapConstantOf(node->op()), true);
+        case IrOpcode::kDeadValue: {
+          switch (DeadValueRepresentationOf(node->op())) {
+            case MachineRepresentation::kBit:
+            case MachineRepresentation::kWord32:
+            case MachineRepresentation::kTagged:
+            case MachineRepresentation::kTaggedSigned:
+            case MachineRepresentation::kTaggedPointer:
+            case MachineRepresentation::kCompressed:
+            case MachineRepresentation::kCompressedPointer:
+              return Constant(static_cast<int32_t>(0));
+            case MachineRepresentation::kWord64:
+              return Constant(static_cast<int64_t>(0));
+            case MachineRepresentation::kFloat64:
+              return Constant(static_cast<double>(0));
+            case MachineRepresentation::kFloat32:
+              return Constant(static_cast<float>(0));
+            default:
+              UNREACHABLE();
+          }
+          break;
+        }
+        default:
+          break;
+      }
     }
     UNREACHABLE();
   }
 
-  static Constant ToNegatedConstant(const Node* node) {
-    switch (node->opcode()) {
-      case IrOpcode::kInt32Constant:
-        return Constant(-OpParameter<int32_t>(node->op()));
-      case IrOpcode::kInt64Constant:
-        return Constant(-OpParameter<int64_t>(node->op()));
-      default:
-        break;
+  Constant ToNegatedConstant(node_t node) {
+    if constexpr (Adapter::IsTurboshaft) {
+      if (const turboshaft::ConstantOp* constant =
+              this->schedule()
+                  ->Get(node)
+                  .template TryCast<turboshaft::ConstantOp>();
+          constant != nullptr) {
+        if (constant->kind == turboshaft::ConstantOp::Kind::kWord32) {
+          return Constant(-static_cast<int32_t>(constant->word32()));
+        } else if (constant->kind == turboshaft::ConstantOp::Kind::kWord64) {
+          return Constant(-static_cast<int64_t>(constant->word64()));
+        }
+      }
+    } else {
+      switch (node->opcode()) {
+        case IrOpcode::kInt32Constant:
+          return Constant(-OpParameter<int32_t>(node->op()));
+        case IrOpcode::kInt64Constant:
+          return Constant(-OpParameter<int64_t>(node->op()));
+        default:
+          break;
+      }
     }
     UNREACHABLE();
   }
 
-  UnallocatedOperand Define(Node* node, UnallocatedOperand operand) {
-    DCHECK_NOT_NULL(node);
+  UnallocatedOperand Define(node_t node, UnallocatedOperand operand) {
+    DCHECK(this->valid(node));
     DCHECK_EQ(operand.virtual_register(), GetVReg(node));
     selector()->MarkAsDefined(node);
     return operand;
   }
 
-  UnallocatedOperand Use(Node* node, UnallocatedOperand operand) {
-    DCHECK_NOT_NULL(node);
+  UnallocatedOperand Use(node_t node, UnallocatedOperand operand) {
+    DCHECK(this->valid(node));
     DCHECK_EQ(operand.virtual_register(), GetVReg(node));
     selector()->MarkAsUsed(node);
     return operand;
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index b0b76cbe482..17c21a9635f 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -9,15 +9,18 @@
 #include "src/base/iterator.h"
 #include "src/codegen/assembler-inl.h"
 #include "src/codegen/interface-descriptors-inl.h"
+#include "src/codegen/machine-type.h"
 #include "src/codegen/tick-counter.h"
 #include "src/common/globals.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/instruction.h"
 #include "src/compiler/common-operator.h"
 #include "src/compiler/compiler-source-position-table.h"
 #include "src/compiler/js-heap-broker.h"
 #include "src/compiler/node-properties.h"
 #include "src/compiler/schedule.h"
 #include "src/compiler/state-values-utils.h"
+#include "src/compiler/turboshaft/operations.h"
 
 #if V8_ENABLE_WEBASSEMBLY
 #include "src/wasm/simd-shuffle.h"
@@ -38,14 +41,16 @@ Smi NumberConstantToSmi(Node* node) {
 template <typename Adapter>
 InstructionSelectorT<Adapter>::InstructionSelectorT(
     Zone* zone, size_t node_count, Linkage* linkage,
-    InstructionSequence* sequence, Schedule* schedule,
+    InstructionSequence* sequence, schedule_t schedule,
     SourcePositionTable* source_positions, Frame* frame,
-    EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
-    JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
-    size_t* max_pushed_argument_count, SourcePositionMode source_position_mode,
-    Features features, EnableScheduling enable_scheduling,
-    EnableRootsRelativeAddressing enable_roots_relative_addressing,
-    EnableTraceTurboJson trace_turbo)
+    InstructionSelector::EnableSwitchJumpTable enable_switch_jump_table,
+    TickCounter* tick_counter, JSHeapBroker* broker,
+    size_t* max_unoptimized_frame_height, size_t* max_pushed_argument_count,
+    InstructionSelector::SourcePositionMode source_position_mode,
+    Features features, InstructionSelector::EnableScheduling enable_scheduling,
+    InstructionSelector::EnableRootsRelativeAddressing
+        enable_roots_relative_addressing,
+    InstructionSelector::EnableTraceTurboJson trace_turbo)
     : zone_(zone),
       linkage_(linkage),
       sequence_(sequence),
@@ -88,7 +93,7 @@ InstructionSelectorT<Adapter>::InstructionSelectorT(
   continuation_inputs_.reserve(5);
   continuation_outputs_.reserve(2);
 
-  if (trace_turbo_ == kEnableTraceTurboJson) {
+  if (trace_turbo_ == InstructionSelector::kEnableTraceTurboJson) {
     instr_origins_.assign(node_count, {-1, 0});
   }
 }
@@ -96,23 +101,25 @@ InstructionSelectorT<Adapter>::InstructionSelectorT(
 template <typename Adapter>
 base::Optional<BailoutReason>
 InstructionSelectorT<Adapter>::SelectInstructions() {
+  Adapter::InitializeAdapter(schedule_);
+
   // Mark the inputs of all phis in loop headers as used.
-  BasicBlockVector* blocks = schedule()->rpo_order();
-  for (auto const block : *blocks) {
-    if (!block->IsLoopHeader()) continue;
-    DCHECK_LE(2u, block->PredecessorCount());
-    for (Node* const phi : *block) {
-      if (phi->opcode() != IrOpcode::kPhi) continue;
+  block_range_t blocks = this->rpo_order(schedule());
+  for (const block_t block : blocks) {
+    if (!this->IsLoopHeader(block)) continue;
+    DCHECK_LE(2u, this->PredecessorCount(block));
+    for (node_t node : this->nodes(block)) {
+      if (!this->IsPhi(node)) continue;
 
       // Mark all inputs as used.
-      for (Node* const input : phi->inputs()) {
+      for (node_t input : this->inputs(node)) {
         MarkAsUsed(input);
       }
     }
   }
 
   // Visit each basic block in post order.
-  for (auto i = blocks->rbegin(); i != blocks->rend(); ++i) {
+  for (auto i = blocks.rbegin(); i != blocks.rend(); ++i) {
     VisitBlock(*i);
     if (instruction_selection_failed())
       return BailoutReason::kCodeGenerationFailed;
@@ -123,16 +130,16 @@ InstructionSelectorT<Adapter>::SelectInstructions() {
     scheduler_ = zone()->template New<InstructionScheduler>(zone(), sequence());
   }
 
-  for (auto const block : *blocks) {
+  for (const block_t block : blocks) {
     InstructionBlock* instruction_block =
-        sequence()->InstructionBlockAt(RpoNumber::FromInt(block->rpo_number()));
+        sequence()->InstructionBlockAt(this->rpo_number(block));
     for (size_t i = 0; i < instruction_block->phis().size(); i++) {
       UpdateRenamesInPhi(instruction_block->PhiAt(i));
     }
     size_t end = instruction_block->code_end();
     size_t start = instruction_block->code_start();
     DCHECK_LE(end, start);
-    StartBlock(RpoNumber::FromInt(block->rpo_number()));
+    StartBlock(this->rpo_number(block));
     if (end != start) {
       while (start-- > end + 1) {
         UpdateRenames(instructions_[start]);
@@ -141,7 +148,7 @@ InstructionSelectorT<Adapter>::SelectInstructions() {
       UpdateRenames(instructions_[end]);
       AddTerminator(instructions_[end]);
     }
-    EndBlock(RpoNumber::FromInt(block->rpo_number()));
+    EndBlock(this->rpo_number(block));
   }
 #if DEBUG
   sequence()->ValidateSSA();
@@ -293,38 +300,51 @@ Instruction* InstructionSelectorT<Adapter>::Emit(Instruction* instr) {
 }
 
 template <typename Adapter>
-bool InstructionSelectorT<Adapter>::CanCover(Node* user, Node* node) const {
+bool InstructionSelectorT<Adapter>::CanCover(node_t user, node_t node) const {
   // 1. Both {user} and {node} must be in the same basic block.
-  if (schedule()->block(node) != current_block_) {
+  if (this->block(schedule(), node) != current_block_) {
     return false;
   }
   // 2. Pure {node}s must be owned by the {user}.
-  if (node->op()->HasProperty(Operator::kPure)) {
-    return node->OwnedBy(user);
+  if constexpr (Adapter::IsTurbofan) {
+    // For Turboshaft, this should be subsumed by check 4.
+    if (node->op()->HasProperty(Operator::kPure)) {
+      return node->OwnedBy(user);
+    }
   }
   // 3. Impure {node}s must match the effect level of {user}.
   if (GetEffectLevel(node) != current_effect_level_) {
+    // TODO(nicohartmann@): We should revisit CanCover for Turboshaft.
     return false;
   }
   // 4. Only {node} must have value edges pointing to {user}.
-  for (Edge const edge : node->use_edges()) {
-    if (edge.from() != user && NodeProperties::IsValueEdge(edge)) {
-      return false;
-    }
-  }
-  return true;
+  return this->is_exclusive_user_of(user, node);
 }
 
 template <typename Adapter>
 bool InstructionSelectorT<Adapter>::IsOnlyUserOfNodeInSameBlock(
-    Node* user, Node* node) const {
-  BasicBlock* bb_user = schedule()->block(user);
-  BasicBlock* bb_node = schedule()->block(node);
+    node_t user, node_t node) const {
+  block_t bb_user = this->block(schedule(), user);
+  block_t bb_node = this->block(schedule(), node);
   if (bb_user != bb_node) return false;
-  for (Edge const edge : node->use_edges()) {
-    Node* from = edge.from();
-    if ((from != user) && (schedule()->block(from) == bb_user)) {
-      return false;
+
+  if constexpr (Adapter::IsTurboshaft) {
+    const turboshaft::Operation& user_op = this->turboshaft_graph()->Get(user);
+    if (user_op.saturated_use_count.Get() == 1) return true;
+    // TODO(nicohartmann@): We should find a better way to do this.
+    for (const auto& op : this->turboshaft_graph()->operations(*bb_user)) {
+      if (&op == &user_op) continue;
+      for (turboshaft::OpIndex input : op.inputs()) {
+        if (input == node) return false;
+      }
+    }
+    return true;
+  } else {
+    for (Edge const edge : node->use_edges()) {
+      Node* from = edge.from();
+      if ((from != user) && (this->block(schedule(), from) == bb_user)) {
+        return false;
+      }
     }
   }
   return true;
@@ -374,8 +394,7 @@ void InstructionSelectorT<Adapter>::TryRename(InstructionOperand* op) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::SetRename(const Node* node,
-                                              const Node* rename) {
+void InstructionSelectorT<Adapter>::SetRename(node_t node, node_t rename) {
   int vreg = GetVirtualRegister(node);
   if (static_cast<size_t>(vreg) >= virtual_register_rename_.size()) {
     int invalid = InstructionOperand::kInvalidVirtualRegister;
@@ -385,9 +404,9 @@ void InstructionSelectorT<Adapter>::SetRename(const Node* node,
 }
 
 template <typename Adapter>
-int InstructionSelectorT<Adapter>::GetVirtualRegister(const Node* node) {
-  DCHECK_NOT_NULL(node);
-  size_t const id = node->id();
+int InstructionSelectorT<Adapter>::GetVirtualRegister(node_t node) {
+  DCHECK(this->valid(node));
+  size_t const id = this->id(node);
   DCHECK_LT(id, virtual_registers_.size());
   int virtual_register = virtual_registers_[id];
   if (virtual_register == InstructionOperand::kInvalidVirtualRegister) {
@@ -411,55 +430,56 @@ InstructionSelectorT<Adapter>::GetVirtualRegistersForTesting() const {
 }
 
 template <typename Adapter>
-bool InstructionSelectorT<Adapter>::IsDefined(Node* node) const {
-  DCHECK_NOT_NULL(node);
-  return defined_.Contains(node->id());
+bool InstructionSelectorT<Adapter>::IsDefined(node_t node) const {
+  DCHECK(this->valid(node));
+  return defined_.Contains(this->id(node));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::MarkAsDefined(Node* node) {
-  DCHECK_NOT_NULL(node);
-  defined_.Add(node->id());
+void InstructionSelectorT<Adapter>::MarkAsDefined(node_t node) {
+  DCHECK(this->valid(node));
+  defined_.Add(this->id(node));
 }
 
 template <typename Adapter>
-bool InstructionSelectorT<Adapter>::IsUsed(Node* node) const {
-  DCHECK_NOT_NULL(node);
-  // TODO(bmeurer): This is a terrible monster hack, but we have to make sure
-  // that the Retain is actually emitted, otherwise the GC will mess up.
-  if (node->opcode() == IrOpcode::kRetain) return true;
-  if (!node->op()->HasProperty(Operator::kEliminatable)) return true;
-  return used_.Contains(node->id());
+bool InstructionSelectorT<Adapter>::IsUsed(node_t node) const {
+  DCHECK(this->valid(node));
+  if constexpr (Adapter::IsTurbofan) {
+    // TODO(bmeurer): This is a terrible monster hack, but we have to make sure
+    // that the Retain is actually emitted, otherwise the GC will mess up.
+    if (this->IsRetain(node)) return true;
+  }
+  if (this->IsRequiredWhenUnused(node)) return true;
+  return used_.Contains(this->id(node));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::MarkAsUsed(Node* node) {
-  DCHECK_NOT_NULL(node);
-  used_.Add(node->id());
+void InstructionSelectorT<Adapter>::MarkAsUsed(node_t node) {
+  DCHECK(this->valid(node));
+  used_.Add(this->id(node));
 }
 
 template <typename Adapter>
-int InstructionSelectorT<Adapter>::GetEffectLevel(Node* node) const {
-  DCHECK_NOT_NULL(node);
-  size_t const id = node->id();
+int InstructionSelectorT<Adapter>::GetEffectLevel(node_t node) const {
+  DCHECK(this->valid(node));
+  size_t const id = this->id(node);
   DCHECK_LT(id, effect_level_.size());
   return effect_level_[id];
 }
 
 template <typename Adapter>
 int InstructionSelectorT<Adapter>::GetEffectLevel(
-    Node* node, FlagsContinuation* cont) const {
-  return cont->IsBranch()
-             ? GetEffectLevel(
-                   cont->true_block()->PredecessorAt(0)->control_input())
-             : GetEffectLevel(node);
+    node_t node, FlagsContinuation* cont) const {
+  return cont->IsBranch() ? GetEffectLevel(this->block_terminator(
+                                this->PredecessorAt(cont->true_block(), 0)))
+                          : GetEffectLevel(node);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::SetEffectLevel(Node* node,
+void InstructionSelectorT<Adapter>::SetEffectLevel(node_t node,
                                                    int effect_level) {
-  DCHECK_NOT_NULL(node);
-  size_t const id = node->id();
+  DCHECK(this->valid(node));
+  size_t const id = this->id(node);
   DCHECK_LT(id, effect_level_.size());
   effect_level_[id] = effect_level;
 }
@@ -477,7 +497,8 @@ bool InstructionSelectorT<Adapter>::CanAddressRelativeToRootsRegister(
   //    addresses guaranteed not to change between code generation and
   //    execution?
   const bool all_root_relative_offsets_are_constant =
-      (enable_roots_relative_addressing_ == kEnableRootsRelativeAddressing);
+      (enable_roots_relative_addressing_ ==
+       InstructionSelector::kEnableRootsRelativeAddressing);
   if (all_root_relative_offsets_are_constant) return true;
 
   // 3. IsAddressableThroughRootRegister: Is the target address guaranteed to
@@ -503,21 +524,72 @@ void InstructionSelectorT<Adapter>::MarkAsRepresentation(
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::MarkAsRepresentation(
-    MachineRepresentation rep, Node* node) {
+    MachineRepresentation rep, node_t node) {
   sequence()->MarkAsRepresentation(rep, GetVirtualRegister(node));
 }
 
 namespace {
 
-template <typename Adapter>
 InstructionOperand OperandForDeopt(Isolate* isolate,
-                                   OperandGeneratorT<Adapter>* g, Node* input,
+                                   OperandGeneratorT<TurboshaftAdapter>* g,
+                                   turboshaft::OpIndex input,
                                    FrameStateInputKind kind,
                                    MachineRepresentation rep) {
   if (rep == MachineRepresentation::kNone) {
     return g->TempImmediate(FrameStateDescriptor::kImpossibleValue);
   }
 
+  const turboshaft::Operation& op = g->turboshaft_graph()->Get(input);
+  if (const turboshaft::ConstantOp* constant =
+          op.TryCast<turboshaft::ConstantOp>()) {
+    switch (constant->kind) {
+      case turboshaft::ConstantOp::Kind::kHeapObject:
+      case turboshaft::ConstantOp::Kind::kCompressedHeapObject: {
+        if (!CanBeTaggedOrCompressedPointer(rep)) {
+          // If we have inconsistent static and dynamic types, e.g. if we
+          // smi-check a string, we can get here with a heap object that
+          // says it is a smi. In that case, we return an invalid instruction
+          // operand, which will be interpreted as an optimized-out value.
+
+          // TODO(jarin) Ideally, we should turn the current instruction
+          // into an abort (we should never execute it).
+          return InstructionOperand();
+        }
+
+        Handle<HeapObject> object = constant->handle();
+        RootIndex root_index;
+        if (isolate->roots_table().IsRootHandle(object, &root_index) &&
+            root_index == RootIndex::kOptimizedOut) {
+          // For an optimized-out object we return an invalid instruction
+          // operand, so that we take the fast path for optimized-out values.
+          return InstructionOperand();
+        }
+
+        return g->UseImmediate(input);
+      }
+      default:
+        UNIMPLEMENTED();
+    }
+  } else {
+    switch (kind) {
+      case FrameStateInputKind::kStackSlot:
+        return g->UseUniqueSlot(input);
+      case FrameStateInputKind::kAny:
+        // Currently deopts "wrap" other operations, so the deopt's inputs
+        // are potentially needed until the end of the deoptimising code.
+        return g->UseAnyAtEnd(input);
+    }
+  }
+}
+
+InstructionOperand OperandForDeopt(Isolate* isolate,
+                                   OperandGeneratorT<TurbofanAdapter>* g,
+                                   Node* input, FrameStateInputKind kind,
+                                   MachineRepresentation rep) {
+  if (rep == MachineRepresentation::kNone) {
+    return g->TempImmediate(FrameStateDescriptor::kImpossibleValue);
+  }
+
   switch (input->opcode()) {
     case IrOpcode::kInt32Constant:
     case IrOpcode::kInt64Constant:
@@ -619,12 +691,39 @@ class StateObjectDeduplicator {
   ZoneVector<Node*> objects_;
 };
 
-// Returns the number of instruction operands added to inputs.
-template <typename Adapter>
-size_t InstructionSelectorT<Adapter>::AddOperandToStateValueDescriptor(
+template <>
+size_t
+InstructionSelectorT<TurboshaftAdapter>::AddOperandToStateValueDescriptor(
     StateValueList* values, InstructionOperandVector* inputs,
-    OperandGenerator* g, StateObjectDeduplicator* deduplicator, Node* input,
+    OperandGeneratorT<TurboshaftAdapter>* g,
+    StateObjectDeduplicator* deduplicator, turboshaft::OpIndex input,
     MachineType type, FrameStateInputKind kind, Zone* zone) {
+  if (!input.valid()) {
+    // We use an invalid input to mark optimized out values.
+    values->PushOptimizedOut();
+    DCHECK(type.IsNone());
+    return 0;
+  }
+  InstructionOperand op =
+      OperandForDeopt(isolate(), g, input, kind, type.representation());
+  if (op.kind() == InstructionOperand::INVALID) {
+    // Invalid operand means the value is impossible or optimized-out.
+    values->PushOptimizedOut();
+    return 0;
+  } else {
+    inputs->push_back(op);
+    values->PushPlain(type);
+    return 1;
+  }
+}
+
+// Returns the number of instruction operands added to inputs.
+template <>
+size_t InstructionSelectorT<TurbofanAdapter>::AddOperandToStateValueDescriptor(
+    StateValueList* values, InstructionOperandVector* inputs,
+    OperandGeneratorT<TurbofanAdapter>* g,
+    StateObjectDeduplicator* deduplicator, Node* input, MachineType type,
+    FrameStateInputKind kind, Zone* zone) {
   DCHECK_NOT_NULL(input);
   switch (input->opcode()) {
     case IrOpcode::kArgumentsElementsState: {
@@ -736,14 +835,15 @@ class InstructionSelectorT<Adapter>::CachedStateValuesBuilder {
   size_t deduplicator_start_;
 };
 
-template <typename Adapter>
-size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
+template <>
+size_t InstructionSelectorT<TurbofanAdapter>::AddInputsToFrameStateDescriptor(
     StateValueList* values, InstructionOperandVector* inputs,
-    OperandGenerator* g, StateObjectDeduplicator* deduplicator, Node* node,
+    OperandGeneratorT<TurbofanAdapter>* g,
+    StateObjectDeduplicator* deduplicator, node_t node,
     FrameStateInputKind kind, Zone* zone) {
-  // StateValues are often shared across different nodes, and processing them is
-  // expensive, so cache the result of processing a StateValue so that we can
-  // quickly copy the result if we see it again.
+  // StateValues are often shared across different nodes, and processing them
+  // is expensive, so cache the result of processing a StateValue so that we
+  // can quickly copy the result if we see it again.
   FrameStateInput key(node, kind);
   auto cache_entry = state_values_cache_.find(key);
   if (cache_entry != state_values_cache_.end()) {
@@ -767,8 +867,8 @@ size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
       ++it;
     }
     if (cache_builder.CanCache()) {
-      // Use this->zone() to build the cache entry in the instruction selector's
-      // zone rather than the more long-lived instruction zone.
+      // Use this->zone() to build the cache entry in the instruction
+      // selector's zone rather than the more long-lived instruction zone.
       state_values_cache_.emplace(key, cache_builder.Build(this->zone()));
     }
     return entries;
@@ -776,11 +876,125 @@ size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
 }
 
 // Returns the number of instruction operands added to inputs.
-template <typename Adapter>
-size_t InstructionSelectorT<Adapter>::AddInputsToFrameStateDescriptor(
-    FrameStateDescriptor* descriptor, FrameState state, OperandGenerator* g,
+template <>
+size_t InstructionSelectorT<TurboshaftAdapter>::AddInputsToFrameStateDescriptor(
+    FrameStateDescriptor* descriptor, node_t state_node, OperandGenerator* g,
+    StateObjectDeduplicator* deduplicator, InstructionOperandVector* inputs,
+    FrameStateInputKind kind, Zone* zone) {
+  turboshaft::FrameStateOp& state =
+      schedule()->Get(state_node).template Cast<turboshaft::FrameStateOp>();
+  const FrameStateInfo& info = state.data->frame_state_info;
+  turboshaft::FrameStateData::Iterator it =
+      state.data->iterator(state.state_values());
+
+  struct StateValue {
+    turboshaft::OpIndex input;
+    MachineType type;
+
+    static StateValue OptimizedOut() {
+      return {turboshaft::OpIndex::Invalid(), MachineType::None()};
+    }
+  };
+
+  auto BuildDeoptInput =
+      [&](turboshaft::FrameStateData::Iterator* it) -> StateValue {
+    switch (it->current_instr()) {
+      using Instr = turboshaft::FrameStateData::Instr;
+      case Instr::kInput: {
+        StateValue result;
+        it->ConsumeInput(&result.type, &result.input);
+        const turboshaft::Operation& op = schedule()->Get(result.input);
+        if (op.outputs_rep()[0] ==
+                turboshaft::RegisterRepresentation::Word64() &&
+            result.type.representation() == MachineRepresentation::kWord32) {
+          // 64 to 32-bit conversion is implicit in turboshaft, but explicit
+          // in turbofan, so we insert this conversion.
+          UNIMPLEMENTED();
+        }
+        return result;
+      }
+      default:
+        UNIMPLEMENTED();
+    }
+  };
+
+  auto BuildStateValues = [&](turboshaft::FrameStateData::Iterator* it,
+                              int32_t count) {
+    base::SmallVector<StateValue, 16> inputs;
+    for (int32_t i = 0; i < count; ++i) {
+      if (it->current_instr() ==
+          turboshaft::FrameStateData::Instr::kUnusedRegister) {
+        it->ConsumeUnusedRegister();
+        inputs.push_back(StateValue::OptimizedOut());
+      } else {
+        inputs.push_back(BuildDeoptInput(it));
+      }
+    }
+    return inputs;
+  };
+
+  size_t entries = 0;
+  size_t initial_size = inputs->size();
+  USE(initial_size);  // initial_size is only used for debug.
+  if (descriptor->outer_state()) {
+    // TODO(nicohartmann@): We might need to support this.
+    UNIMPLEMENTED();
+  }
+
+  auto params = BuildStateValues(&it, info.parameter_count());
+  auto locals = BuildStateValues(&it, info.local_count());
+  auto stacks = BuildStateValues(&it, info.stack_count());
+  auto context = BuildDeoptInput(&it);
+  auto function = BuildDeoptInput(&it);
+
+  DCHECK_EQ(descriptor->parameters_count(), info.parameter_count());
+  DCHECK_EQ(descriptor->locals_count(), info.local_count());
+  DCHECK_EQ(descriptor->stack_count(), info.stack_count());
+
+  StateValueList* values_descriptor = descriptor->GetStateValueDescriptors();
+
+  DCHECK_EQ(values_descriptor->size(), 0u);
+  values_descriptor->ReserveSize(descriptor->GetSize());
+
+  DCHECK(this->valid(function.input));
+  DCHECK_EQ(function.type, MachineType::AnyTagged());
+  entries += AddOperandToStateValueDescriptor(
+      values_descriptor, inputs, g, deduplicator, function.input, function.type,
+      FrameStateInputKind::kStackSlot, zone);
+
+  for (auto [input, type] : params) {
+    entries += AddOperandToStateValueDescriptor(
+        values_descriptor, inputs, g, deduplicator, input, type, kind, zone);
+  }
+
+  if (descriptor->HasContext()) {
+    DCHECK(this->valid(context.input));
+    DCHECK_EQ(context.type, MachineType::AnyTagged());
+    entries += AddOperandToStateValueDescriptor(
+        values_descriptor, inputs, g, deduplicator, context.input, context.type,
+        FrameStateInputKind::kStackSlot, zone);
+  }
+
+  for (auto [index, type] : locals) {
+    entries += AddOperandToStateValueDescriptor(
+        values_descriptor, inputs, g, deduplicator, index, type, kind, zone);
+  }
+
+  for (auto [index, type] : stacks) {
+    entries += AddOperandToStateValueDescriptor(
+        values_descriptor, inputs, g, deduplicator, index, type, kind, zone);
+  }
+
+  DCHECK_EQ(initial_size + entries, inputs->size());
+  return entries;
+}
+
+template <>
+size_t InstructionSelectorT<TurbofanAdapter>::AddInputsToFrameStateDescriptor(
+    FrameStateDescriptor* descriptor, node_t state_node, OperandGenerator* g,
     StateObjectDeduplicator* deduplicator, InstructionOperandVector* inputs,
     FrameStateInputKind kind, Zone* zone) {
+  FrameState state{state_node};
   size_t entries = 0;
   size_t initial_size = inputs->size();
   USE(initial_size);  // initial_size is only used for debug.
@@ -895,7 +1109,7 @@ Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
               DeoptFrameStateOffsetField::encode(static_cast<int>(input_count));
     AppendDeoptimizeArguments(&continuation_inputs_, cont->reason(),
                               cont->node_id(), cont->feedback(),
-                              FrameState{cont->frame_state()});
+                              cont->frame_state());
   } else if (cont->IsSet()) {
     continuation_outputs_.push_back(g.DefineAsRegister(cont->result()));
   } else if (cont->IsSelect()) {
@@ -927,26 +1141,33 @@ Instruction* InstructionSelectorT<Adapter>::EmitWithContinuation(
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AppendDeoptimizeArguments(
-    InstructionOperandVector* args, DeoptimizeReason reason, NodeId node_id,
-    FeedbackSource const& feedback, FrameState frame_state,
-    DeoptimizeKind kind) {
-  OperandGenerator g(this);
-  FrameStateDescriptor* const descriptor = GetFrameStateDescriptor(frame_state);
-  int const state_id = sequence()->AddDeoptimizationEntry(
-      descriptor, kind, reason, node_id, feedback);
-  args->push_back(g.TempImmediate(state_id));
-  StateObjectDeduplicator deduplicator(instruction_zone());
-  AddInputsToFrameStateDescriptor(descriptor, frame_state, &g, &deduplicator,
-                                  args, FrameStateInputKind::kAny,
-                                  instruction_zone());
+    InstructionOperandVector* args, DeoptimizeReason reason, id_t node_id,
+    FeedbackSource const& feedback, node_t frame_state, DeoptimizeKind kind) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this for turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    OperandGenerator g(this);
+    FrameStateDescriptor* const descriptor =
+        GetFrameStateDescriptor(static_cast<Node*>(frame_state));
+    int const state_id = sequence()->AddDeoptimizationEntry(
+        descriptor, kind, reason, node_id, feedback);
+    args->push_back(g.TempImmediate(state_id));
+    StateObjectDeduplicator deduplicator(instruction_zone());
+    AddInputsToFrameStateDescriptor(descriptor, frame_state, &g, &deduplicator,
+                                    args, FrameStateInputKind::kAny,
+                                    instruction_zone());
+  }
 }
 
 // An internal helper class for generating the operands to calls.
 // TODO(bmeurer): Get rid of the CallBuffer business and make
 // InstructionSelector::VisitCall platform independent instead.
-struct CallBuffer {
-  CallBuffer(Zone* zone, const CallDescriptor* call_descriptor,
-             FrameStateDescriptor* frame_state)
+template <typename Adapter>
+struct CallBufferT {
+  using PushParameter = PushParameterT<Adapter>;
+  CallBufferT(Zone* zone, const CallDescriptor* call_descriptor,
+              FrameStateDescriptor* frame_state)
       : descriptor(call_descriptor),
         frame_state_descriptor(frame_state),
         output_nodes(zone),
@@ -982,15 +1203,13 @@ struct CallBuffer {
 // InstructionSelector::VisitCall platform independent instead.
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::InitializeCallBuffer(
-    Node* call, CallBuffer* buffer, CallBufferFlags flags,
+    node_t node, CallBuffer* buffer, CallBufferFlags flags,
     int stack_param_delta) {
   OperandGenerator g(this);
   size_t ret_count = buffer->descriptor->ReturnCount();
   bool is_tail_call = (flags & kCallTail) != 0;
-  DCHECK_LE(call->op()->ValueOutputCount(), ret_count);
-  DCHECK_EQ(
-      call->op()->ValueInputCount(),
-      static_cast<int>(buffer->input_count() + buffer->frame_state_count()));
+  auto call = this->call_view(node);
+  DCHECK_LE(call.return_count(), ret_count);
 
   if (ret_count > 0) {
     // Collect the projections that represent multiple outputs from this call.
@@ -998,24 +1217,29 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
       PushParameter result = {call, buffer->descriptor->GetReturnLocation(0)};
       buffer->output_nodes.push_back(result);
     } else {
-      buffer->output_nodes.resize(ret_count);
-      for (size_t i = 0; i < ret_count; ++i) {
-        LinkageLocation location = buffer->descriptor->GetReturnLocation(i);
-        buffer->output_nodes[i] = PushParameter(nullptr, location);
-      }
-      for (Edge const edge : call->use_edges()) {
-        if (!NodeProperties::IsValueEdge(edge)) continue;
-        Node* node = edge.from();
-        DCHECK_EQ(IrOpcode::kProjection, node->opcode());
-        size_t const index = ProjectionIndexOf(node->op());
-
-        DCHECK_LT(index, buffer->output_nodes.size());
-        DCHECK(!buffer->output_nodes[index].node);
-        buffer->output_nodes[index].node = node;
-      }
+      if constexpr (Adapter::IsTurboshaft) {
+        // TODO(nicohartmann@): Support this.
+        UNIMPLEMENTED();
+      } else {
+        buffer->output_nodes.resize(ret_count);
+        for (size_t i = 0; i < ret_count; ++i) {
+          LinkageLocation location = buffer->descriptor->GetReturnLocation(i);
+          buffer->output_nodes[i] = PushParameter(nullptr, location);
+        }
+        for (Edge const edge : ((node_t)call)->use_edges()) {
+          if (!NodeProperties::IsValueEdge(edge)) continue;
+          Node* node = edge.from();
+          DCHECK_EQ(IrOpcode::kProjection, node->opcode());
+          size_t const index = ProjectionIndexOf(node->op());
+
+          DCHECK_LT(index, buffer->output_nodes.size());
+          DCHECK(!buffer->output_nodes[index].node);
+          buffer->output_nodes[index].node = node;
+        }
 
-      frame_->EnsureReturnSlots(
-          static_cast<int>(buffer->descriptor->ReturnSlotCount()));
+        frame_->EnsureReturnSlots(
+            static_cast<int>(buffer->descriptor->ReturnSlotCount()));
+      }
     }
 
     // Filter out the outputs that aren't live because no projection uses them.
@@ -1025,61 +1249,58 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
             : buffer->frame_state_descriptor->state_combine()
                   .ConsumedOutputCount();
     for (size_t i = 0; i < buffer->output_nodes.size(); i++) {
-      bool output_is_live = buffer->output_nodes[i].node != nullptr ||
+      bool output_is_live = this->valid(buffer->output_nodes[i].node) ||
                             i < outputs_needed_by_framestate;
       if (output_is_live) {
         LinkageLocation location = buffer->output_nodes[i].location;
         MachineRepresentation rep = location.GetType().representation();
 
-        Node* output = buffer->output_nodes[i].node;
-        InstructionOperand op = output == nullptr
+        node_t output = buffer->output_nodes[i].node;
+        InstructionOperand op = !this->valid(output)
                                     ? g.TempLocation(location)
                                     : g.DefineAsLocation(output, location);
         MarkAsRepresentation(rep, op);
 
         if (!UnallocatedOperand::cast(op).HasFixedSlotPolicy()) {
           buffer->outputs.push_back(op);
-          buffer->output_nodes[i].node = nullptr;
+          buffer->output_nodes[i].node = {};
         }
       }
     }
   }
 
   // The first argument is always the callee code.
-  Node* callee = call->InputAt(0);
+  node_t callee = call.callee();
   bool call_code_immediate = (flags & kCallCodeImmediate) != 0;
   bool call_address_immediate = (flags & kCallAddressImmediate) != 0;
   bool call_use_fixed_target_reg = (flags & kCallFixedTargetRegister) != 0;
   switch (buffer->descriptor->kind()) {
     case CallDescriptor::kCallCodeObject:
       buffer->instruction_args.push_back(
-          (call_code_immediate && callee->opcode() == IrOpcode::kHeapConstant)
+          (call_code_immediate && this->IsHeapConstant(callee))
               ? g.UseImmediate(callee)
-              : call_use_fixed_target_reg
-                    ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
-                    : g.UseRegister(callee));
+          : call_use_fixed_target_reg
+              ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
+              : g.UseRegister(callee));
       break;
     case CallDescriptor::kCallAddress:
       buffer->instruction_args.push_back(
-          (call_address_immediate &&
-           callee->opcode() == IrOpcode::kExternalConstant)
+          (call_address_immediate && this->IsExternalConstant(callee))
               ? g.UseImmediate(callee)
-              : call_use_fixed_target_reg
-                    ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
-                    : g.UseRegister(callee));
+          : call_use_fixed_target_reg
+              ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
+              : g.UseRegister(callee));
       break;
 #if V8_ENABLE_WEBASSEMBLY
     case CallDescriptor::kCallWasmCapiFunction:
     case CallDescriptor::kCallWasmFunction:
     case CallDescriptor::kCallWasmImportWrapper:
       buffer->instruction_args.push_back(
-          (call_address_immediate &&
-           (callee->opcode() == IrOpcode::kRelocatableInt64Constant ||
-            callee->opcode() == IrOpcode::kRelocatableInt32Constant))
+          (call_address_immediate && this->IsRelocatableWasmConstant(callee))
               ? g.UseImmediate(callee)
-              : call_use_fixed_target_reg
-                    ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
-                    : g.UseRegister(callee));
+          : call_use_fixed_target_reg
+              ? g.UseFixed(callee, kJavaScriptCallCodeStartRegister)
+              : g.UseRegister(callee));
       break;
 #endif  // V8_ENABLE_WEBASSEMBLY
     case CallDescriptor::kCallBuiltinPointer: {
@@ -1116,21 +1337,19 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
   size_t frame_state_entries = 0;
   USE(frame_state_entries);  // frame_state_entries is only used for debug.
   if (buffer->frame_state_descriptor != nullptr) {
-    FrameState frame_state{
-        call->InputAt(static_cast<int>(buffer->descriptor->InputCount()))};
+    node_t frame_state = call.frame_state();
 
     // If it was a syntactic tail call we need to drop the current frame and
     // all the frames on top of it that are either inlined extra arguments
     // or a tail caller frame.
     if (is_tail_call) {
-      frame_state = FrameState{NodeProperties::GetFrameStateInput(frame_state)};
+      frame_state = this->parent_frame_state(frame_state);
       buffer->frame_state_descriptor =
           buffer->frame_state_descriptor->outer_state();
       while (buffer->frame_state_descriptor != nullptr &&
              buffer->frame_state_descriptor->type() ==
                  FrameStateType::kInlinedExtraArguments) {
-        frame_state =
-            FrameState{NodeProperties::GetFrameStateInput(frame_state)};
+        frame_state = this->parent_frame_state(frame_state);
         buffer->frame_state_descriptor =
             buffer->frame_state_descriptor->outer_state();
       }
@@ -1138,7 +1357,7 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
 
     int const state_id = sequence()->AddDeoptimizationEntry(
         buffer->frame_state_descriptor, DeoptimizeKind::kLazy,
-        DeoptimizeReason::kUnknown, call->id(), FeedbackSource());
+        DeoptimizeReason::kUnknown, this->id(call), FeedbackSource());
     buffer->instruction_args.push_back(g.TempImmediate(state_id));
 
     StateObjectDeduplicator deduplicator(instruction_zone());
@@ -1158,12 +1377,12 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
   // arguments require an explicit push instruction before the call and do
   // not appear as arguments to the call. Everything else ends up
   // as an InstructionOperand argument to the call.
-  auto iter(call->inputs().begin());
+  auto arguments = call.arguments();
+  auto iter(arguments.begin());
+  // call->inputs().begin());
   size_t pushed_count = 0;
-  for (size_t index = 0; index < input_count; ++iter, ++index) {
-    DCHECK(iter != call->inputs().end());
-    DCHECK_NE(IrOpcode::kFrameState, (*iter)->op()->opcode());
-    if (index == 0) continue;  // The first argument (callee) is already done.
+  for (size_t index = 1; index < input_count; ++iter, ++index) {
+    DCHECK_NE(iter, arguments.end());
 
     LinkageLocation location = buffer->descriptor->GetInputLocation(index);
     if (is_tail_call) {
@@ -1211,7 +1430,7 @@ void InstructionSelectorT<Adapter>::InitializeCallBuffer(
 
 template <typename Adapter>
 bool InstructionSelectorT<Adapter>::IsSourcePositionUsed(Node* node) {
-  return (source_position_mode_ == kAllSourcePositions ||
+  return (source_position_mode_ == InstructionSelector::kAllSourcePositions ||
           node->opcode() == IrOpcode::kCall ||
           node->opcode() == IrOpcode::kTrapIf ||
           node->opcode() == IrOpcode::kTrapUnless ||
@@ -1221,8 +1440,25 @@ bool InstructionSelectorT<Adapter>::IsSourcePositionUsed(Node* node) {
           node->opcode() == IrOpcode::kStoreTrapOnNull);
 }
 
+namespace {
+bool increment_effect_level_for_opcode(IrOpcode::Value opcode) {
+  return opcode == IrOpcode::kStore || opcode == IrOpcode::kUnalignedStore ||
+         opcode == IrOpcode::kCall || opcode == IrOpcode::kProtectedStore ||
+         opcode == IrOpcode::kStoreTrapOnNull ||
+#define ADD_EFFECT_FOR_ATOMIC_OP(Opcode) opcode == IrOpcode::k##Opcode ||
+         MACHINE_ATOMIC_OP_LIST(ADD_EFFECT_FOR_ATOMIC_OP)
+#undef ADD_EFFECT_FOR_ATOMIC_OP
+                 opcode == IrOpcode::kMemoryBarrier;
+}
+
+bool increment_effect_level_for_opcode(turboshaft::Opcode opcode) {
+  // TODO(nicohartmann@): Implement this.
+  return true;
+}
+}  // namespace
+
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
+void InstructionSelectorT<Adapter>::VisitBlock(block_t block) {
   DCHECK(!current_block_);
   current_block_ = block;
   auto current_num_instructions = [&] {
@@ -1232,40 +1468,37 @@ void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
   int current_block_end = current_num_instructions();
 
   int effect_level = 0;
-  for (Node* const node : *block) {
+  for (node_t node : this->nodes(block)) {
     SetEffectLevel(node, effect_level);
     current_effect_level_ = effect_level;
-    if (node->opcode() == IrOpcode::kStore ||
-        node->opcode() == IrOpcode::kUnalignedStore ||
-        node->opcode() == IrOpcode::kCall ||
-        node->opcode() == IrOpcode::kProtectedStore ||
-        node->opcode() == IrOpcode::kStoreTrapOnNull ||
-#define ADD_EFFECT_FOR_ATOMIC_OP(Opcode) \
-  node->opcode() == IrOpcode::k##Opcode ||
-        MACHINE_ATOMIC_OP_LIST(ADD_EFFECT_FOR_ATOMIC_OP)
-#undef ADD_EFFECT_FOR_ATOMIC_OP
-                node->opcode() == IrOpcode::kMemoryBarrier) {
+    if (increment_effect_level_for_opcode(this->opcode(node))) {
       ++effect_level;
     }
   }
 
   // We visit the control first, then the nodes in the block, so the block's
   // control input should be on the same effect level as the last node.
-  if (block->control_input() != nullptr) {
-    SetEffectLevel(block->control_input(), effect_level);
+  if (node_t terminator = this->block_terminator(block);
+      this->valid(terminator)) {
+    SetEffectLevel(terminator, effect_level);
     current_effect_level_ = effect_level;
   }
 
-  auto FinishEmittedInstructions = [&](Node* node, int instruction_start) {
+  auto FinishEmittedInstructions = [&](node_t node, int instruction_start) {
     if (instruction_selection_failed()) return false;
     if (current_num_instructions() == instruction_start) return true;
     std::reverse(instructions_.begin() + instruction_start,
                  instructions_.end());
-    if (!node) return true;
+    if (!this->valid(node)) return true;
     if (!source_positions_) return true;
-    SourcePosition source_position = source_positions_->GetSourcePosition(node);
-    if (source_position.IsKnown() && IsSourcePositionUsed(node)) {
-      sequence()->SetSourcePosition(instructions_.back(), source_position);
+    if constexpr (std::is_same_v<Adapter, TurbofanAdapter>) {
+      SourcePosition source_position =
+          source_positions_->GetSourcePosition(node);
+      if (source_position.IsKnown() && IsSourcePositionUsed(node)) {
+        sequence()->SetSourcePosition(instructions_.back(), source_position);
+      }
+    } else {
+      // TODO(nicohartmann@): Reconsider this.
     }
     return true;
   };
@@ -1273,13 +1506,14 @@ void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
   // Generate code for the block control "top down", but schedule the code
   // "bottom up".
   VisitControl(block);
-  if (!FinishEmittedInstructions(block->control_input(), current_block_end)) {
+  if (!FinishEmittedInstructions(this->block_terminator(block),
+                                 current_block_end)) {
     return;
   }
 
   // Visit code in reverse control flow order, because architecture-specific
   // matching may cover more than one node at a time.
-  for (auto node : base::Reversed(*block)) {
+  for (node_t node : base::Reversed(this->nodes(block))) {
     int current_node_end = current_num_instructions();
     // Skip nodes that are unused or already defined.
     if (IsUsed(node) && !IsDefined(node)) {
@@ -1288,15 +1522,15 @@ void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
       VisitNode(node);
       if (!FinishEmittedInstructions(node, current_node_end)) return;
     }
-    if (trace_turbo_ == kEnableTraceTurboJson) {
-      instr_origins_[node->id()] = {current_num_instructions(),
-                                    current_node_end};
+    if (trace_turbo_ == InstructionSelector::kEnableTraceTurboJson) {
+      instr_origins_[this->id(node)] = {current_num_instructions(),
+                                        current_node_end};
     }
   }
 
   // We're done with the block.
   InstructionBlock* instruction_block =
-      sequence()->InstructionBlockAt(RpoNumber::FromInt(block->rpo_number()));
+      sequence()->InstructionBlockAt(this->rpo_number(block));
   if (current_num_instructions() == current_block_end) {
     // Avoid empty block: insert a {kArchNop} instruction.
     Emit(Instruction::New(sequence()->zone(), kArchNop));
@@ -1307,1431 +1541,126 @@ void InstructionSelectorT<Adapter>::VisitBlock(BasicBlock* block) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitControl(BasicBlock* block) {
-#ifdef DEBUG
-  // SSA deconstruction requires targets of branches not to have phis.
-  // Edge split form guarantees this property, but is more strict.
-  if (block->SuccessorCount() > 1) {
-    for (BasicBlock* const successor : block->successors()) {
-      for (Node* const node : *successor) {
-        if (IrOpcode::IsPhiOpcode(node->opcode())) {
-          std::ostringstream str;
-          str << "You might have specified merged variables for a label with "
-              << "only one predecessor." << std::endl
-              << "# Current Block: " << *successor << std::endl
-              << "#          Node: " << *node;
-          FATAL("%s", str.str().c_str());
-        }
-      }
-    }
+void InstructionSelectorT<Adapter>::MarkPairProjectionsAsWord32(Node* node) {
+  Node* projection0 = NodeProperties::FindProjection(node, 0);
+  if (projection0) {
+    MarkAsWord32(projection0);
   }
-#endif
+  Node* projection1 = NodeProperties::FindProjection(node, 1);
+  if (projection1) {
+    MarkAsWord32(projection1);
+  }
+}
 
-  Node* input = block->control_input();
-  int instruction_end = static_cast<int>(instructions_.size());
-  switch (block->control()) {
-    case BasicBlock::kGoto:
-      VisitGoto(block->SuccessorAt(0));
-      break;
-    case BasicBlock::kCall: {
-      DCHECK_EQ(IrOpcode::kCall, input->opcode());
-      BasicBlock* success = block->SuccessorAt(0);
-      BasicBlock* exception = block->SuccessorAt(1);
-      VisitCall(input, exception);
-      VisitGoto(success);
-      break;
-    }
-    case BasicBlock::kTailCall: {
-      DCHECK_EQ(IrOpcode::kTailCall, input->opcode());
-      VisitTailCall(input);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitNode(
+    turboshaft::OpIndex node) {
+  tick_counter_->TickAndMaybeEnterSafepoint();
+  const turboshaft::Operation& op = schedule()->Get(node);
+  using Opcode = turboshaft::Opcode;
+  switch (op.opcode) {
+    case Opcode::kBranch:
+    case Opcode::kGoto:
+    case Opcode::kReturn:
+      // Those are already handled in VisitControl.
       break;
+    case Opcode::kParameter: {
+      // Parameters should always be scheduled to the first block.
+      DCHECK_EQ(this->rpo_number(this->block(schedule(), node)).ToInt(), 0);
+      MachineType type = linkage()->GetParameterType(
+          op.Cast<turboshaft::ParameterOp>().parameter_index);
+      MarkAsRepresentation(type.representation(), node);
+      return VisitParameter(node);
     }
-    case BasicBlock::kBranch: {
-      DCHECK_EQ(IrOpcode::kBranch, input->opcode());
-      // TODO(nicohartmann@): Once all branches have explicitly specified
-      // semantics, we should allow only BranchSemantics::kMachine here.
-      DCHECK_NE(BranchSemantics::kJS,
-                BranchParametersOf(input->op()).semantics());
-      BasicBlock* tbranch = block->SuccessorAt(0);
-      BasicBlock* fbranch = block->SuccessorAt(1);
-      if (tbranch == fbranch) {
-        VisitGoto(tbranch);
-      } else {
-        VisitBranch(input, tbranch, fbranch);
+    case Opcode::kConstant: {
+      const turboshaft::ConstantOp& constant =
+          op.Cast<turboshaft::ConstantOp>();
+      using Kind = turboshaft::ConstantOp::Kind;
+      switch (constant.kind) {
+        case Kind::kWord32:
+        case Kind::kWord64:
+        case Kind::kTaggedIndex:
+        case Kind::kExternal:
+          break;
+        case Kind::kFloat32:
+          MarkAsFloat32(node);
+          break;
+        case Kind::kFloat64:
+          MarkAsFloat64(node);
+          break;
+        case Kind::kHeapObject:
+          MarkAsTagged(node);
+          break;
+        case Kind::kCompressedHeapObject:
+          MarkAsCompressed(node);
+          break;
+        case Kind::kNumber:
+          if (!IsSmiDouble(constant.number())) MarkAsTagged(node);
+          break;
+        case Kind::kRelocatableWasmCall:
+        case Kind::kRelocatableWasmStubCall:
+          UNIMPLEMENTED();
       }
+      VisitConstant(node);
       break;
     }
-    case BasicBlock::kSwitch: {
-      DCHECK_EQ(IrOpcode::kSwitch, input->opcode());
-      // Last successor must be {IfDefault}.
-      BasicBlock* default_branch = block->successors().back();
-      DCHECK_EQ(IrOpcode::kIfDefault, default_branch->front()->opcode());
-      // All other successors must be {IfValue}s.
-      int32_t min_value = std::numeric_limits<int32_t>::max();
-      int32_t max_value = std::numeric_limits<int32_t>::min();
-      size_t case_count = block->SuccessorCount() - 1;
-      ZoneVector<CaseInfo> cases(case_count, zone());
-      for (size_t i = 0; i < case_count; ++i) {
-        BasicBlock* branch = block->SuccessorAt(i);
-        const IfValueParameters& p = IfValueParametersOf(branch->front()->op());
-        cases[i] = CaseInfo{p.value(), p.comparison_order(), branch};
-        if (min_value > p.value()) min_value = p.value();
-        if (max_value < p.value()) max_value = p.value();
-      }
-      SwitchInfo sw(cases, min_value, max_value, default_branch);
-      VisitSwitch(input, sw);
+    case Opcode::kCall:
+      VisitCall(node);
       break;
-    }
-    case BasicBlock::kReturn: {
-      DCHECK_EQ(IrOpcode::kReturn, input->opcode());
-      VisitReturn(input);
+    case Opcode::kFrameConstant: {
+      const turboshaft::FrameConstantOp& constant =
+          op.Cast<turboshaft::FrameConstantOp>();
+      using Kind = turboshaft::FrameConstantOp::Kind;
+      OperandGenerator g(this);
+      switch (constant.kind) {
+        case Kind::kStackCheckOffset:
+          Emit(kArchStackCheckOffset, g.DefineAsRegister(node));
+          break;
+        case Kind::kFramePointer:
+          Emit(kArchFramePointer, g.DefineAsRegister(node));
+          break;
+        case Kind::kParentFramePointer:
+          Emit(kArchParentFramePointer, g.DefineAsRegister(node));
+          break;
+      }
       break;
     }
-    case BasicBlock::kDeoptimize: {
-      DeoptimizeParameters p = DeoptimizeParametersOf(input->op());
-      FrameState value{input->InputAt(0)};
-      VisitDeoptimize(p.reason(), input->id(), p.feedback(), value);
+    case Opcode::kStackPointerGreaterThan:
+      VisitStackPointerGreaterThan(node);
       break;
+    case Opcode::kLoad: {
+      MachineRepresentation rep = op.Cast<turboshaft::LoadOp>()
+                                      .loaded_rep.ToMachineType()
+                                      .representation();
+      MarkAsRepresentation(rep, node);
+      return VisitLoad(node);
     }
-    case BasicBlock::kThrow:
-      DCHECK_EQ(IrOpcode::kThrow, input->opcode());
-      VisitThrow(input);
-      break;
-    case BasicBlock::kNone: {
-      // Exit block doesn't have control.
-      DCHECK_NULL(input);
-      break;
+    default: {
+      const std::string op_string = op.ToString();
+      PrintF("\033[31mNo ISEL support for: %s\033[m\n", op_string.c_str());
+      FATAL("Unexpected operation #%d:%s", node.id(), op_string.c_str());
     }
-    default:
-      UNREACHABLE();
-  }
-  if (trace_turbo_ == kEnableTraceTurboJson && input) {
-    int instruction_start = static_cast<int>(instructions_.size());
-    instr_origins_[input->id()] = {instruction_start, instruction_end};
   }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::MarkPairProjectionsAsWord32(Node* node) {
-  Node* projection0 = NodeProperties::FindProjection(node, 0);
-  if (projection0) {
-    MarkAsWord32(projection0);
-  }
-  Node* projection1 = NodeProperties::FindProjection(node, 1);
-  if (projection1) {
-    MarkAsWord32(projection1);
-  }
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitNode(Node* node) {
-  tick_counter_->TickAndMaybeEnterSafepoint();
-  DCHECK_NOT_NULL(schedule()->block(node));  // should only use scheduled nodes.
-  switch (node->opcode()) {
-    case IrOpcode::kTraceInstruction:
-#if V8_TARGET_ARCH_X64
-      return VisitTraceInstruction(node);
-#else
-      return;
-#endif
-    case IrOpcode::kStart:
-    case IrOpcode::kLoop:
-    case IrOpcode::kEnd:
-    case IrOpcode::kBranch:
-    case IrOpcode::kIfTrue:
-    case IrOpcode::kIfFalse:
-    case IrOpcode::kIfSuccess:
-    case IrOpcode::kSwitch:
-    case IrOpcode::kIfValue:
-    case IrOpcode::kIfDefault:
-    case IrOpcode::kEffectPhi:
-    case IrOpcode::kMerge:
-    case IrOpcode::kTerminate:
-    case IrOpcode::kBeginRegion:
-      // No code needed for these graph artifacts.
-      return;
-    case IrOpcode::kIfException:
-      return MarkAsTagged(node), VisitIfException(node);
-    case IrOpcode::kFinishRegion:
-      return MarkAsTagged(node), VisitFinishRegion(node);
-    case IrOpcode::kParameter: {
-      // Parameters should always be scheduled to the first block.
-      DCHECK_EQ(schedule()->block(node)->rpo_number(), 0);
-      MachineType type =
-          linkage()->GetParameterType(ParameterIndexOf(node->op()));
-      MarkAsRepresentation(type.representation(), node);
-      return VisitParameter(node);
-    }
-    case IrOpcode::kOsrValue:
-      return MarkAsTagged(node), VisitOsrValue(node);
-    case IrOpcode::kPhi: {
-      MachineRepresentation rep = PhiRepresentationOf(node->op());
-      if (rep == MachineRepresentation::kNone) return;
-      MarkAsRepresentation(rep, node);
-      return VisitPhi(node);
-    }
-    case IrOpcode::kProjection:
-      return VisitProjection(node);
-    case IrOpcode::kInt32Constant:
-    case IrOpcode::kInt64Constant:
-    case IrOpcode::kTaggedIndexConstant:
-    case IrOpcode::kExternalConstant:
-    case IrOpcode::kRelocatableInt32Constant:
-    case IrOpcode::kRelocatableInt64Constant:
-      return VisitConstant(node);
-    case IrOpcode::kFloat32Constant:
-      return MarkAsFloat32(node), VisitConstant(node);
-    case IrOpcode::kFloat64Constant:
-      return MarkAsFloat64(node), VisitConstant(node);
-    case IrOpcode::kHeapConstant:
-      return MarkAsTagged(node), VisitConstant(node);
-    case IrOpcode::kCompressedHeapConstant:
-      return MarkAsCompressed(node), VisitConstant(node);
-    case IrOpcode::kNumberConstant: {
-      double value = OpParameter<double>(node->op());
-      if (!IsSmiDouble(value)) MarkAsTagged(node);
-      return VisitConstant(node);
-    }
-    case IrOpcode::kCall:
-      return VisitCall(node);
-    case IrOpcode::kDeoptimizeIf:
-      return VisitDeoptimizeIf(node);
-    case IrOpcode::kDeoptimizeUnless:
-      return VisitDeoptimizeUnless(node);
-    case IrOpcode::kTrapIf:
-      return VisitTrapIf(node, TrapIdOf(node->op()));
-    case IrOpcode::kTrapUnless:
-      return VisitTrapUnless(node, TrapIdOf(node->op()));
-    case IrOpcode::kFrameState:
-    case IrOpcode::kStateValues:
-    case IrOpcode::kObjectState:
-      return;
-    case IrOpcode::kAbortCSADcheck:
-      VisitAbortCSADcheck(node);
-      return;
-    case IrOpcode::kDebugBreak:
-      VisitDebugBreak(node);
-      return;
-    case IrOpcode::kUnreachable:
-      VisitUnreachable(node);
-      return;
-    case IrOpcode::kStaticAssert:
-      VisitStaticAssert(node);
-      return;
-    case IrOpcode::kDeadValue:
-      VisitDeadValue(node);
-      return;
-    case IrOpcode::kComment:
-      VisitComment(node);
-      return;
-    case IrOpcode::kRetain:
-      VisitRetain(node);
-      return;
-    case IrOpcode::kLoad:
-    case IrOpcode::kLoadImmutable: {
-      LoadRepresentation type = LoadRepresentationOf(node->op());
-      MarkAsRepresentation(type.representation(), node);
-      return VisitLoad(node);
-    }
-    case IrOpcode::kLoadTransform: {
-      LoadTransformParameters params = LoadTransformParametersOf(node->op());
-      if (params.transformation == LoadTransformation::kS256Load32Splat ||
-          params.transformation == LoadTransformation::kS256Load64Splat) {
-        MarkAsRepresentation(MachineRepresentation::kSimd256, node);
-      } else {
-        MarkAsRepresentation(MachineRepresentation::kSimd128, node);
-      }
-      return VisitLoadTransform(node);
-    }
-    case IrOpcode::kLoadLane: {
-      MarkAsRepresentation(MachineRepresentation::kSimd128, node);
-      return VisitLoadLane(node);
-    }
-    case IrOpcode::kStore:
-      return VisitStore(node);
-    case IrOpcode::kStorePair:
-      return VisitStorePair(node);
-    case IrOpcode::kProtectedStore:
-    case IrOpcode::kStoreTrapOnNull:
-      return VisitProtectedStore(node);
-    case IrOpcode::kStoreLane: {
-      MarkAsRepresentation(MachineRepresentation::kSimd128, node);
-      return VisitStoreLane(node);
-    }
-    case IrOpcode::kWord32And:
-      return MarkAsWord32(node), VisitWord32And(node);
-    case IrOpcode::kWord32Or:
-      return MarkAsWord32(node), VisitWord32Or(node);
-    case IrOpcode::kWord32Xor:
-      return MarkAsWord32(node), VisitWord32Xor(node);
-    case IrOpcode::kWord32Shl:
-      return MarkAsWord32(node), VisitWord32Shl(node);
-    case IrOpcode::kWord32Shr:
-      return MarkAsWord32(node), VisitWord32Shr(node);
-    case IrOpcode::kWord32Sar:
-      return MarkAsWord32(node), VisitWord32Sar(node);
-    case IrOpcode::kWord32Rol:
-      return MarkAsWord32(node), VisitWord32Rol(node);
-    case IrOpcode::kWord32Ror:
-      return MarkAsWord32(node), VisitWord32Ror(node);
-    case IrOpcode::kWord32Equal:
-      return VisitWord32Equal(node);
-    case IrOpcode::kWord32Clz:
-      return MarkAsWord32(node), VisitWord32Clz(node);
-    case IrOpcode::kWord32Ctz:
-      return MarkAsWord32(node), VisitWord32Ctz(node);
-    case IrOpcode::kWord32ReverseBits:
-      return MarkAsWord32(node), VisitWord32ReverseBits(node);
-    case IrOpcode::kWord32ReverseBytes:
-      return MarkAsWord32(node), VisitWord32ReverseBytes(node);
-    case IrOpcode::kInt32AbsWithOverflow:
-      return MarkAsWord32(node), VisitInt32AbsWithOverflow(node);
-    case IrOpcode::kWord32Popcnt:
-      return MarkAsWord32(node), VisitWord32Popcnt(node);
-    case IrOpcode::kWord64Popcnt:
-      return MarkAsWord32(node), VisitWord64Popcnt(node);
-    case IrOpcode::kWord32Select:
-      return MarkAsWord32(node), VisitSelect(node);
-    case IrOpcode::kWord64And:
-      return MarkAsWord64(node), VisitWord64And(node);
-    case IrOpcode::kWord64Or:
-      return MarkAsWord64(node), VisitWord64Or(node);
-    case IrOpcode::kWord64Xor:
-      return MarkAsWord64(node), VisitWord64Xor(node);
-    case IrOpcode::kWord64Shl:
-      return MarkAsWord64(node), VisitWord64Shl(node);
-    case IrOpcode::kWord64Shr:
-      return MarkAsWord64(node), VisitWord64Shr(node);
-    case IrOpcode::kWord64Sar:
-      return MarkAsWord64(node), VisitWord64Sar(node);
-    case IrOpcode::kWord64Rol:
-      return MarkAsWord64(node), VisitWord64Rol(node);
-    case IrOpcode::kWord64Ror:
-      return MarkAsWord64(node), VisitWord64Ror(node);
-    case IrOpcode::kWord64Clz:
-      return MarkAsWord64(node), VisitWord64Clz(node);
-    case IrOpcode::kWord64Ctz:
-      return MarkAsWord64(node), VisitWord64Ctz(node);
-    case IrOpcode::kWord64ReverseBits:
-      return MarkAsWord64(node), VisitWord64ReverseBits(node);
-    case IrOpcode::kWord64ReverseBytes:
-      return MarkAsWord64(node), VisitWord64ReverseBytes(node);
-    case IrOpcode::kSimd128ReverseBytes:
-      return MarkAsSimd128(node), VisitSimd128ReverseBytes(node);
-    case IrOpcode::kInt64AbsWithOverflow:
-      return MarkAsWord64(node), VisitInt64AbsWithOverflow(node);
-    case IrOpcode::kWord64Equal:
-      return VisitWord64Equal(node);
-    case IrOpcode::kWord64Select:
-      return MarkAsWord64(node), VisitSelect(node);
-    case IrOpcode::kInt32Add:
-      return MarkAsWord32(node), VisitInt32Add(node);
-    case IrOpcode::kInt32AddWithOverflow:
-      return MarkAsWord32(node), VisitInt32AddWithOverflow(node);
-    case IrOpcode::kInt32Sub:
-      return MarkAsWord32(node), VisitInt32Sub(node);
-    case IrOpcode::kInt32SubWithOverflow:
-      return VisitInt32SubWithOverflow(node);
-    case IrOpcode::kInt32Mul:
-      return MarkAsWord32(node), VisitInt32Mul(node);
-    case IrOpcode::kInt32MulWithOverflow:
-      return MarkAsWord32(node), VisitInt32MulWithOverflow(node);
-    case IrOpcode::kInt32MulHigh:
-      return VisitInt32MulHigh(node);
-    case IrOpcode::kInt64MulHigh:
-      return VisitInt64MulHigh(node);
-    case IrOpcode::kInt32Div:
-      return MarkAsWord32(node), VisitInt32Div(node);
-    case IrOpcode::kInt32Mod:
-      return MarkAsWord32(node), VisitInt32Mod(node);
-    case IrOpcode::kInt32LessThan:
-      return VisitInt32LessThan(node);
-    case IrOpcode::kInt32LessThanOrEqual:
-      return VisitInt32LessThanOrEqual(node);
-    case IrOpcode::kUint32Div:
-      return MarkAsWord32(node), VisitUint32Div(node);
-    case IrOpcode::kUint32LessThan:
-      return VisitUint32LessThan(node);
-    case IrOpcode::kUint32LessThanOrEqual:
-      return VisitUint32LessThanOrEqual(node);
-    case IrOpcode::kUint32Mod:
-      return MarkAsWord32(node), VisitUint32Mod(node);
-    case IrOpcode::kUint32MulHigh:
-      return VisitUint32MulHigh(node);
-    case IrOpcode::kUint64MulHigh:
-      return VisitUint64MulHigh(node);
-    case IrOpcode::kInt64Add:
-      return MarkAsWord64(node), VisitInt64Add(node);
-    case IrOpcode::kInt64AddWithOverflow:
-      return MarkAsWord64(node), VisitInt64AddWithOverflow(node);
-    case IrOpcode::kInt64Sub:
-      return MarkAsWord64(node), VisitInt64Sub(node);
-    case IrOpcode::kInt64SubWithOverflow:
-      return MarkAsWord64(node), VisitInt64SubWithOverflow(node);
-    case IrOpcode::kInt64Mul:
-      return MarkAsWord64(node), VisitInt64Mul(node);
-    case IrOpcode::kInt64MulWithOverflow:
-      return MarkAsWord64(node), VisitInt64MulWithOverflow(node);
-    case IrOpcode::kInt64Div:
-      return MarkAsWord64(node), VisitInt64Div(node);
-    case IrOpcode::kInt64Mod:
-      return MarkAsWord64(node), VisitInt64Mod(node);
-    case IrOpcode::kInt64LessThan:
-      return VisitInt64LessThan(node);
-    case IrOpcode::kInt64LessThanOrEqual:
-      return VisitInt64LessThanOrEqual(node);
-    case IrOpcode::kUint64Div:
-      return MarkAsWord64(node), VisitUint64Div(node);
-    case IrOpcode::kUint64LessThan:
-      return VisitUint64LessThan(node);
-    case IrOpcode::kUint64LessThanOrEqual:
-      return VisitUint64LessThanOrEqual(node);
-    case IrOpcode::kUint64Mod:
-      return MarkAsWord64(node), VisitUint64Mod(node);
-    case IrOpcode::kBitcastTaggedToWord:
-    case IrOpcode::kBitcastTaggedToWordForTagAndSmiBits:
-      return MarkAsRepresentation(MachineType::PointerRepresentation(), node),
-             VisitBitcastTaggedToWord(node);
-    case IrOpcode::kBitcastWordToTagged:
-      return MarkAsTagged(node), VisitBitcastWordToTagged(node);
-    case IrOpcode::kBitcastWordToTaggedSigned:
-      return MarkAsRepresentation(MachineRepresentation::kTaggedSigned, node),
-             EmitIdentity(node);
-    case IrOpcode::kChangeFloat32ToFloat64:
-      return MarkAsFloat64(node), VisitChangeFloat32ToFloat64(node);
-    case IrOpcode::kChangeInt32ToFloat64:
-      return MarkAsFloat64(node), VisitChangeInt32ToFloat64(node);
-    case IrOpcode::kChangeInt64ToFloat64:
-      return MarkAsFloat64(node), VisitChangeInt64ToFloat64(node);
-    case IrOpcode::kChangeUint32ToFloat64:
-      return MarkAsFloat64(node), VisitChangeUint32ToFloat64(node);
-    case IrOpcode::kChangeFloat64ToInt32:
-      return MarkAsWord32(node), VisitChangeFloat64ToInt32(node);
-    case IrOpcode::kChangeFloat64ToInt64:
-      return MarkAsWord64(node), VisitChangeFloat64ToInt64(node);
-    case IrOpcode::kChangeFloat64ToUint32:
-      return MarkAsWord32(node), VisitChangeFloat64ToUint32(node);
-    case IrOpcode::kChangeFloat64ToUint64:
-      return MarkAsWord64(node), VisitChangeFloat64ToUint64(node);
-    case IrOpcode::kFloat64SilenceNaN:
-      MarkAsFloat64(node);
-      if (CanProduceSignalingNaN(node->InputAt(0))) {
-        return VisitFloat64SilenceNaN(node);
-      } else {
-        return EmitIdentity(node);
-      }
-    case IrOpcode::kTruncateFloat64ToInt64:
-      return MarkAsWord64(node), VisitTruncateFloat64ToInt64(node);
-    case IrOpcode::kTruncateFloat64ToUint32:
-      return MarkAsWord32(node), VisitTruncateFloat64ToUint32(node);
-    case IrOpcode::kTruncateFloat32ToInt32:
-      return MarkAsWord32(node), VisitTruncateFloat32ToInt32(node);
-    case IrOpcode::kTruncateFloat32ToUint32:
-      return MarkAsWord32(node), VisitTruncateFloat32ToUint32(node);
-    case IrOpcode::kTryTruncateFloat32ToInt64:
-      return MarkAsWord64(node), VisitTryTruncateFloat32ToInt64(node);
-    case IrOpcode::kTryTruncateFloat64ToInt64:
-      return MarkAsWord64(node), VisitTryTruncateFloat64ToInt64(node);
-    case IrOpcode::kTryTruncateFloat32ToUint64:
-      return MarkAsWord64(node), VisitTryTruncateFloat32ToUint64(node);
-    case IrOpcode::kTryTruncateFloat64ToUint64:
-      return MarkAsWord64(node), VisitTryTruncateFloat64ToUint64(node);
-    case IrOpcode::kTryTruncateFloat64ToInt32:
-      return MarkAsWord32(node), VisitTryTruncateFloat64ToInt32(node);
-    case IrOpcode::kTryTruncateFloat64ToUint32:
-      return MarkAsWord32(node), VisitTryTruncateFloat64ToUint32(node);
-    case IrOpcode::kBitcastWord32ToWord64:
-      return MarkAsWord64(node), VisitBitcastWord32ToWord64(node);
-    case IrOpcode::kChangeInt32ToInt64:
-      return MarkAsWord64(node), VisitChangeInt32ToInt64(node);
-    case IrOpcode::kChangeUint32ToUint64:
-      return MarkAsWord64(node), VisitChangeUint32ToUint64(node);
-    case IrOpcode::kTruncateFloat64ToFloat32:
-      return MarkAsFloat32(node), VisitTruncateFloat64ToFloat32(node);
-    case IrOpcode::kTruncateFloat64ToWord32:
-      return MarkAsWord32(node), VisitTruncateFloat64ToWord32(node);
-    case IrOpcode::kTruncateInt64ToInt32:
-      return MarkAsWord32(node), VisitTruncateInt64ToInt32(node);
-    case IrOpcode::kRoundFloat64ToInt32:
-      return MarkAsWord32(node), VisitRoundFloat64ToInt32(node);
-    case IrOpcode::kRoundInt64ToFloat32:
-      return MarkAsFloat32(node), VisitRoundInt64ToFloat32(node);
-    case IrOpcode::kRoundInt32ToFloat32:
-      return MarkAsFloat32(node), VisitRoundInt32ToFloat32(node);
-    case IrOpcode::kRoundInt64ToFloat64:
-      return MarkAsFloat64(node), VisitRoundInt64ToFloat64(node);
-    case IrOpcode::kBitcastFloat32ToInt32:
-      return MarkAsWord32(node), VisitBitcastFloat32ToInt32(node);
-    case IrOpcode::kRoundUint32ToFloat32:
-      return MarkAsFloat32(node), VisitRoundUint32ToFloat32(node);
-    case IrOpcode::kRoundUint64ToFloat32:
-      return MarkAsFloat64(node), VisitRoundUint64ToFloat32(node);
-    case IrOpcode::kRoundUint64ToFloat64:
-      return MarkAsFloat64(node), VisitRoundUint64ToFloat64(node);
-    case IrOpcode::kBitcastFloat64ToInt64:
-      return MarkAsWord64(node), VisitBitcastFloat64ToInt64(node);
-    case IrOpcode::kBitcastInt32ToFloat32:
-      return MarkAsFloat32(node), VisitBitcastInt32ToFloat32(node);
-    case IrOpcode::kBitcastInt64ToFloat64:
-      return MarkAsFloat64(node), VisitBitcastInt64ToFloat64(node);
-    case IrOpcode::kFloat32Add:
-      return MarkAsFloat32(node), VisitFloat32Add(node);
-    case IrOpcode::kFloat32Sub:
-      return MarkAsFloat32(node), VisitFloat32Sub(node);
-    case IrOpcode::kFloat32Neg:
-      return MarkAsFloat32(node), VisitFloat32Neg(node);
-    case IrOpcode::kFloat32Mul:
-      return MarkAsFloat32(node), VisitFloat32Mul(node);
-    case IrOpcode::kFloat32Div:
-      return MarkAsFloat32(node), VisitFloat32Div(node);
-    case IrOpcode::kFloat32Abs:
-      return MarkAsFloat32(node), VisitFloat32Abs(node);
-    case IrOpcode::kFloat32Sqrt:
-      return MarkAsFloat32(node), VisitFloat32Sqrt(node);
-    case IrOpcode::kFloat32Equal:
-      return VisitFloat32Equal(node);
-    case IrOpcode::kFloat32LessThan:
-      return VisitFloat32LessThan(node);
-    case IrOpcode::kFloat32LessThanOrEqual:
-      return VisitFloat32LessThanOrEqual(node);
-    case IrOpcode::kFloat32Max:
-      return MarkAsFloat32(node), VisitFloat32Max(node);
-    case IrOpcode::kFloat32Min:
-      return MarkAsFloat32(node), VisitFloat32Min(node);
-    case IrOpcode::kFloat32Select:
-      return MarkAsFloat32(node), VisitSelect(node);
-    case IrOpcode::kFloat64Add:
-      return MarkAsFloat64(node), VisitFloat64Add(node);
-    case IrOpcode::kFloat64Sub:
-      return MarkAsFloat64(node), VisitFloat64Sub(node);
-    case IrOpcode::kFloat64Neg:
-      return MarkAsFloat64(node), VisitFloat64Neg(node);
-    case IrOpcode::kFloat64Mul:
-      return MarkAsFloat64(node), VisitFloat64Mul(node);
-    case IrOpcode::kFloat64Div:
-      return MarkAsFloat64(node), VisitFloat64Div(node);
-    case IrOpcode::kFloat64Mod:
-      return MarkAsFloat64(node), VisitFloat64Mod(node);
-    case IrOpcode::kFloat64Min:
-      return MarkAsFloat64(node), VisitFloat64Min(node);
-    case IrOpcode::kFloat64Max:
-      return MarkAsFloat64(node), VisitFloat64Max(node);
-    case IrOpcode::kFloat64Abs:
-      return MarkAsFloat64(node), VisitFloat64Abs(node);
-    case IrOpcode::kFloat64Acos:
-      return MarkAsFloat64(node), VisitFloat64Acos(node);
-    case IrOpcode::kFloat64Acosh:
-      return MarkAsFloat64(node), VisitFloat64Acosh(node);
-    case IrOpcode::kFloat64Asin:
-      return MarkAsFloat64(node), VisitFloat64Asin(node);
-    case IrOpcode::kFloat64Asinh:
-      return MarkAsFloat64(node), VisitFloat64Asinh(node);
-    case IrOpcode::kFloat64Atan:
-      return MarkAsFloat64(node), VisitFloat64Atan(node);
-    case IrOpcode::kFloat64Atanh:
-      return MarkAsFloat64(node), VisitFloat64Atanh(node);
-    case IrOpcode::kFloat64Atan2:
-      return MarkAsFloat64(node), VisitFloat64Atan2(node);
-    case IrOpcode::kFloat64Cbrt:
-      return MarkAsFloat64(node), VisitFloat64Cbrt(node);
-    case IrOpcode::kFloat64Cos:
-      return MarkAsFloat64(node), VisitFloat64Cos(node);
-    case IrOpcode::kFloat64Cosh:
-      return MarkAsFloat64(node), VisitFloat64Cosh(node);
-    case IrOpcode::kFloat64Exp:
-      return MarkAsFloat64(node), VisitFloat64Exp(node);
-    case IrOpcode::kFloat64Expm1:
-      return MarkAsFloat64(node), VisitFloat64Expm1(node);
-    case IrOpcode::kFloat64Log:
-      return MarkAsFloat64(node), VisitFloat64Log(node);
-    case IrOpcode::kFloat64Log1p:
-      return MarkAsFloat64(node), VisitFloat64Log1p(node);
-    case IrOpcode::kFloat64Log10:
-      return MarkAsFloat64(node), VisitFloat64Log10(node);
-    case IrOpcode::kFloat64Log2:
-      return MarkAsFloat64(node), VisitFloat64Log2(node);
-    case IrOpcode::kFloat64Pow:
-      return MarkAsFloat64(node), VisitFloat64Pow(node);
-    case IrOpcode::kFloat64Sin:
-      return MarkAsFloat64(node), VisitFloat64Sin(node);
-    case IrOpcode::kFloat64Sinh:
-      return MarkAsFloat64(node), VisitFloat64Sinh(node);
-    case IrOpcode::kFloat64Sqrt:
-      return MarkAsFloat64(node), VisitFloat64Sqrt(node);
-    case IrOpcode::kFloat64Tan:
-      return MarkAsFloat64(node), VisitFloat64Tan(node);
-    case IrOpcode::kFloat64Tanh:
-      return MarkAsFloat64(node), VisitFloat64Tanh(node);
-    case IrOpcode::kFloat64Equal:
-      return VisitFloat64Equal(node);
-    case IrOpcode::kFloat64LessThan:
-      return VisitFloat64LessThan(node);
-    case IrOpcode::kFloat64LessThanOrEqual:
-      return VisitFloat64LessThanOrEqual(node);
-    case IrOpcode::kFloat64Select:
-      return MarkAsFloat64(node), VisitSelect(node);
-    case IrOpcode::kFloat32RoundDown:
-      return MarkAsFloat32(node), VisitFloat32RoundDown(node);
-    case IrOpcode::kFloat64RoundDown:
-      return MarkAsFloat64(node), VisitFloat64RoundDown(node);
-    case IrOpcode::kFloat32RoundUp:
-      return MarkAsFloat32(node), VisitFloat32RoundUp(node);
-    case IrOpcode::kFloat64RoundUp:
-      return MarkAsFloat64(node), VisitFloat64RoundUp(node);
-    case IrOpcode::kFloat32RoundTruncate:
-      return MarkAsFloat32(node), VisitFloat32RoundTruncate(node);
-    case IrOpcode::kFloat64RoundTruncate:
-      return MarkAsFloat64(node), VisitFloat64RoundTruncate(node);
-    case IrOpcode::kFloat64RoundTiesAway:
-      return MarkAsFloat64(node), VisitFloat64RoundTiesAway(node);
-    case IrOpcode::kFloat32RoundTiesEven:
-      return MarkAsFloat32(node), VisitFloat32RoundTiesEven(node);
-    case IrOpcode::kFloat64RoundTiesEven:
-      return MarkAsFloat64(node), VisitFloat64RoundTiesEven(node);
-    case IrOpcode::kFloat64ExtractLowWord32:
-      return MarkAsWord32(node), VisitFloat64ExtractLowWord32(node);
-    case IrOpcode::kFloat64ExtractHighWord32:
-      return MarkAsWord32(node), VisitFloat64ExtractHighWord32(node);
-    case IrOpcode::kFloat64InsertLowWord32:
-      return MarkAsFloat64(node), VisitFloat64InsertLowWord32(node);
-    case IrOpcode::kFloat64InsertHighWord32:
-      return MarkAsFloat64(node), VisitFloat64InsertHighWord32(node);
-    case IrOpcode::kStackSlot:
-      return VisitStackSlot(node);
-    case IrOpcode::kStackPointerGreaterThan:
-      return VisitStackPointerGreaterThan(node);
-    case IrOpcode::kLoadStackCheckOffset:
-      return VisitLoadStackCheckOffset(node);
-    case IrOpcode::kLoadFramePointer:
-      return VisitLoadFramePointer(node);
-    case IrOpcode::kLoadParentFramePointer:
-      return VisitLoadParentFramePointer(node);
-    case IrOpcode::kLoadRootRegister:
-      return VisitLoadRootRegister(node);
-    case IrOpcode::kUnalignedLoad: {
-      LoadRepresentation type = LoadRepresentationOf(node->op());
-      MarkAsRepresentation(type.representation(), node);
-      return VisitUnalignedLoad(node);
-    }
-    case IrOpcode::kUnalignedStore:
-      return VisitUnalignedStore(node);
-    case IrOpcode::kInt32PairAdd:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitInt32PairAdd(node);
-    case IrOpcode::kInt32PairSub:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitInt32PairSub(node);
-    case IrOpcode::kInt32PairMul:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitInt32PairMul(node);
-    case IrOpcode::kWord32PairShl:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitWord32PairShl(node);
-    case IrOpcode::kWord32PairShr:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitWord32PairShr(node);
-    case IrOpcode::kWord32PairSar:
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitWord32PairSar(node);
-    case IrOpcode::kMemoryBarrier:
-      return VisitMemoryBarrier(node);
-    case IrOpcode::kWord32AtomicLoad: {
-      AtomicLoadParameters params = AtomicLoadParametersOf(node->op());
-      LoadRepresentation type = params.representation();
-      MarkAsRepresentation(type.representation(), node);
-      return VisitWord32AtomicLoad(node);
-    }
-    case IrOpcode::kWord64AtomicLoad: {
-      AtomicLoadParameters params = AtomicLoadParametersOf(node->op());
-      LoadRepresentation type = params.representation();
-      MarkAsRepresentation(type.representation(), node);
-      return VisitWord64AtomicLoad(node);
-    }
-    case IrOpcode::kWord32AtomicStore:
-      return VisitWord32AtomicStore(node);
-    case IrOpcode::kWord64AtomicStore:
-      return VisitWord64AtomicStore(node);
-    case IrOpcode::kWord32AtomicPairStore:
-      return VisitWord32AtomicPairStore(node);
-    case IrOpcode::kWord32AtomicPairLoad: {
-      MarkAsWord32(node);
-      MarkPairProjectionsAsWord32(node);
-      return VisitWord32AtomicPairLoad(node);
-    }
-#define ATOMIC_CASE(name, rep)                         \
-  case IrOpcode::k##rep##Atomic##name: {               \
-    MachineType type = AtomicOpType(node->op());       \
-    MarkAsRepresentation(type.representation(), node); \
-    return Visit##rep##Atomic##name(node);             \
-  }
-      ATOMIC_CASE(Add, Word32)
-      ATOMIC_CASE(Add, Word64)
-      ATOMIC_CASE(Sub, Word32)
-      ATOMIC_CASE(Sub, Word64)
-      ATOMIC_CASE(And, Word32)
-      ATOMIC_CASE(And, Word64)
-      ATOMIC_CASE(Or, Word32)
-      ATOMIC_CASE(Or, Word64)
-      ATOMIC_CASE(Xor, Word32)
-      ATOMIC_CASE(Xor, Word64)
-      ATOMIC_CASE(Exchange, Word32)
-      ATOMIC_CASE(Exchange, Word64)
-      ATOMIC_CASE(CompareExchange, Word32)
-      ATOMIC_CASE(CompareExchange, Word64)
-#undef ATOMIC_CASE
-#define ATOMIC_CASE(name)                     \
-  case IrOpcode::kWord32AtomicPair##name: {   \
-    MarkAsWord32(node);                       \
-    MarkPairProjectionsAsWord32(node);        \
-    return VisitWord32AtomicPair##name(node); \
-  }
-      ATOMIC_CASE(Add)
-      ATOMIC_CASE(Sub)
-      ATOMIC_CASE(And)
-      ATOMIC_CASE(Or)
-      ATOMIC_CASE(Xor)
-      ATOMIC_CASE(Exchange)
-      ATOMIC_CASE(CompareExchange)
-#undef ATOMIC_CASE
-    case IrOpcode::kProtectedLoad:
-    case IrOpcode::kLoadTrapOnNull: {
-      LoadRepresentation type = LoadRepresentationOf(node->op());
-      MarkAsRepresentation(type.representation(), node);
-      return VisitProtectedLoad(node);
-    }
-    case IrOpcode::kSignExtendWord8ToInt32:
-      return MarkAsWord32(node), VisitSignExtendWord8ToInt32(node);
-    case IrOpcode::kSignExtendWord16ToInt32:
-      return MarkAsWord32(node), VisitSignExtendWord16ToInt32(node);
-    case IrOpcode::kSignExtendWord8ToInt64:
-      return MarkAsWord64(node), VisitSignExtendWord8ToInt64(node);
-    case IrOpcode::kSignExtendWord16ToInt64:
-      return MarkAsWord64(node), VisitSignExtendWord16ToInt64(node);
-    case IrOpcode::kSignExtendWord32ToInt64:
-      return MarkAsWord64(node), VisitSignExtendWord32ToInt64(node);
-    case IrOpcode::kF64x2Splat:
-      return MarkAsSimd128(node), VisitF64x2Splat(node);
-    case IrOpcode::kF64x2ExtractLane:
-      return MarkAsFloat64(node), VisitF64x2ExtractLane(node);
-    case IrOpcode::kF64x2ReplaceLane:
-      return MarkAsSimd128(node), VisitF64x2ReplaceLane(node);
-    case IrOpcode::kF64x2Abs:
-      return MarkAsSimd128(node), VisitF64x2Abs(node);
-    case IrOpcode::kF64x2Neg:
-      return MarkAsSimd128(node), VisitF64x2Neg(node);
-    case IrOpcode::kF64x2Sqrt:
-      return MarkAsSimd128(node), VisitF64x2Sqrt(node);
-    case IrOpcode::kF64x2Add:
-      return MarkAsSimd128(node), VisitF64x2Add(node);
-    case IrOpcode::kF64x2Sub:
-      return MarkAsSimd128(node), VisitF64x2Sub(node);
-    case IrOpcode::kF64x2Mul:
-      return MarkAsSimd128(node), VisitF64x2Mul(node);
-    case IrOpcode::kF64x2Div:
-      return MarkAsSimd128(node), VisitF64x2Div(node);
-    case IrOpcode::kF64x2Min:
-      return MarkAsSimd128(node), VisitF64x2Min(node);
-    case IrOpcode::kF64x2Max:
-      return MarkAsSimd128(node), VisitF64x2Max(node);
-    case IrOpcode::kF64x2Eq:
-      return MarkAsSimd128(node), VisitF64x2Eq(node);
-    case IrOpcode::kF64x2Ne:
-      return MarkAsSimd128(node), VisitF64x2Ne(node);
-    case IrOpcode::kF64x2Lt:
-      return MarkAsSimd128(node), VisitF64x2Lt(node);
-    case IrOpcode::kF64x2Le:
-      return MarkAsSimd128(node), VisitF64x2Le(node);
-    case IrOpcode::kF64x2Qfma:
-      return MarkAsSimd128(node), VisitF64x2Qfma(node);
-    case IrOpcode::kF64x2Qfms:
-      return MarkAsSimd128(node), VisitF64x2Qfms(node);
-    case IrOpcode::kF64x2Pmin:
-      return MarkAsSimd128(node), VisitF64x2Pmin(node);
-    case IrOpcode::kF64x2Pmax:
-      return MarkAsSimd128(node), VisitF64x2Pmax(node);
-    case IrOpcode::kF64x2Ceil:
-      return MarkAsSimd128(node), VisitF64x2Ceil(node);
-    case IrOpcode::kF64x2Floor:
-      return MarkAsSimd128(node), VisitF64x2Floor(node);
-    case IrOpcode::kF64x2Trunc:
-      return MarkAsSimd128(node), VisitF64x2Trunc(node);
-    case IrOpcode::kF64x2NearestInt:
-      return MarkAsSimd128(node), VisitF64x2NearestInt(node);
-    case IrOpcode::kF64x2ConvertLowI32x4S:
-      return MarkAsSimd128(node), VisitF64x2ConvertLowI32x4S(node);
-    case IrOpcode::kF64x2ConvertLowI32x4U:
-      return MarkAsSimd128(node), VisitF64x2ConvertLowI32x4U(node);
-    case IrOpcode::kF64x2PromoteLowF32x4:
-      return MarkAsSimd128(node), VisitF64x2PromoteLowF32x4(node);
-    case IrOpcode::kF32x4Splat:
-      return MarkAsSimd128(node), VisitF32x4Splat(node);
-    case IrOpcode::kF32x4ExtractLane:
-      return MarkAsFloat32(node), VisitF32x4ExtractLane(node);
-    case IrOpcode::kF32x4ReplaceLane:
-      return MarkAsSimd128(node), VisitF32x4ReplaceLane(node);
-    case IrOpcode::kF32x4SConvertI32x4:
-      return MarkAsSimd128(node), VisitF32x4SConvertI32x4(node);
-    case IrOpcode::kF32x4UConvertI32x4:
-      return MarkAsSimd128(node), VisitF32x4UConvertI32x4(node);
-    case IrOpcode::kF32x4Abs:
-      return MarkAsSimd128(node), VisitF32x4Abs(node);
-    case IrOpcode::kF32x4Neg:
-      return MarkAsSimd128(node), VisitF32x4Neg(node);
-    case IrOpcode::kF32x4Sqrt:
-      return MarkAsSimd128(node), VisitF32x4Sqrt(node);
-    case IrOpcode::kF32x4Add:
-      return MarkAsSimd128(node), VisitF32x4Add(node);
-    case IrOpcode::kF32x4Sub:
-      return MarkAsSimd128(node), VisitF32x4Sub(node);
-    case IrOpcode::kF32x4Mul:
-      return MarkAsSimd128(node), VisitF32x4Mul(node);
-    case IrOpcode::kF32x4Div:
-      return MarkAsSimd128(node), VisitF32x4Div(node);
-    case IrOpcode::kF32x4Min:
-      return MarkAsSimd128(node), VisitF32x4Min(node);
-    case IrOpcode::kF32x4Max:
-      return MarkAsSimd128(node), VisitF32x4Max(node);
-    case IrOpcode::kF32x4Eq:
-      return MarkAsSimd128(node), VisitF32x4Eq(node);
-    case IrOpcode::kF32x4Ne:
-      return MarkAsSimd128(node), VisitF32x4Ne(node);
-    case IrOpcode::kF32x4Lt:
-      return MarkAsSimd128(node), VisitF32x4Lt(node);
-    case IrOpcode::kF32x4Le:
-      return MarkAsSimd128(node), VisitF32x4Le(node);
-    case IrOpcode::kF32x4Qfma:
-      return MarkAsSimd128(node), VisitF32x4Qfma(node);
-    case IrOpcode::kF32x4Qfms:
-      return MarkAsSimd128(node), VisitF32x4Qfms(node);
-    case IrOpcode::kF32x4Pmin:
-      return MarkAsSimd128(node), VisitF32x4Pmin(node);
-    case IrOpcode::kF32x4Pmax:
-      return MarkAsSimd128(node), VisitF32x4Pmax(node);
-    case IrOpcode::kF32x4Ceil:
-      return MarkAsSimd128(node), VisitF32x4Ceil(node);
-    case IrOpcode::kF32x4Floor:
-      return MarkAsSimd128(node), VisitF32x4Floor(node);
-    case IrOpcode::kF32x4Trunc:
-      return MarkAsSimd128(node), VisitF32x4Trunc(node);
-    case IrOpcode::kF32x4NearestInt:
-      return MarkAsSimd128(node), VisitF32x4NearestInt(node);
-    case IrOpcode::kF32x4DemoteF64x2Zero:
-      return MarkAsSimd128(node), VisitF32x4DemoteF64x2Zero(node);
-    case IrOpcode::kI64x2Splat:
-      return MarkAsSimd128(node), VisitI64x2Splat(node);
-    case IrOpcode::kI64x2SplatI32Pair:
-      return MarkAsSimd128(node), VisitI64x2SplatI32Pair(node);
-    case IrOpcode::kI64x2ExtractLane:
-      return MarkAsWord64(node), VisitI64x2ExtractLane(node);
-    case IrOpcode::kI64x2ReplaceLane:
-      return MarkAsSimd128(node), VisitI64x2ReplaceLane(node);
-    case IrOpcode::kI64x2ReplaceLaneI32Pair:
-      return MarkAsSimd128(node), VisitI64x2ReplaceLaneI32Pair(node);
-    case IrOpcode::kI64x2Abs:
-      return MarkAsSimd128(node), VisitI64x2Abs(node);
-    case IrOpcode::kI64x2Neg:
-      return MarkAsSimd128(node), VisitI64x2Neg(node);
-    case IrOpcode::kI64x2SConvertI32x4Low:
-      return MarkAsSimd128(node), VisitI64x2SConvertI32x4Low(node);
-    case IrOpcode::kI64x2SConvertI32x4High:
-      return MarkAsSimd128(node), VisitI64x2SConvertI32x4High(node);
-    case IrOpcode::kI64x2UConvertI32x4Low:
-      return MarkAsSimd128(node), VisitI64x2UConvertI32x4Low(node);
-    case IrOpcode::kI64x2UConvertI32x4High:
-      return MarkAsSimd128(node), VisitI64x2UConvertI32x4High(node);
-    case IrOpcode::kI64x2BitMask:
-      return MarkAsWord32(node), VisitI64x2BitMask(node);
-    case IrOpcode::kI64x2Shl:
-      return MarkAsSimd128(node), VisitI64x2Shl(node);
-    case IrOpcode::kI64x2ShrS:
-      return MarkAsSimd128(node), VisitI64x2ShrS(node);
-    case IrOpcode::kI64x2Add:
-      return MarkAsSimd128(node), VisitI64x2Add(node);
-    case IrOpcode::kI64x2Sub:
-      return MarkAsSimd128(node), VisitI64x2Sub(node);
-    case IrOpcode::kI64x2Mul:
-      return MarkAsSimd128(node), VisitI64x2Mul(node);
-    case IrOpcode::kI64x2Eq:
-      return MarkAsSimd128(node), VisitI64x2Eq(node);
-    case IrOpcode::kI64x2Ne:
-      return MarkAsSimd128(node), VisitI64x2Ne(node);
-    case IrOpcode::kI64x2GtS:
-      return MarkAsSimd128(node), VisitI64x2GtS(node);
-    case IrOpcode::kI64x2GeS:
-      return MarkAsSimd128(node), VisitI64x2GeS(node);
-    case IrOpcode::kI64x2ShrU:
-      return MarkAsSimd128(node), VisitI64x2ShrU(node);
-    case IrOpcode::kI64x2ExtMulLowI32x4S:
-      return MarkAsSimd128(node), VisitI64x2ExtMulLowI32x4S(node);
-    case IrOpcode::kI64x2ExtMulHighI32x4S:
-      return MarkAsSimd128(node), VisitI64x2ExtMulHighI32x4S(node);
-    case IrOpcode::kI64x2ExtMulLowI32x4U:
-      return MarkAsSimd128(node), VisitI64x2ExtMulLowI32x4U(node);
-    case IrOpcode::kI64x2ExtMulHighI32x4U:
-      return MarkAsSimd128(node), VisitI64x2ExtMulHighI32x4U(node);
-    case IrOpcode::kI32x4Splat:
-      return MarkAsSimd128(node), VisitI32x4Splat(node);
-    case IrOpcode::kI32x4ExtractLane:
-      return MarkAsWord32(node), VisitI32x4ExtractLane(node);
-    case IrOpcode::kI32x4ReplaceLane:
-      return MarkAsSimd128(node), VisitI32x4ReplaceLane(node);
-    case IrOpcode::kI32x4SConvertF32x4:
-      return MarkAsSimd128(node), VisitI32x4SConvertF32x4(node);
-    case IrOpcode::kI32x4SConvertI16x8Low:
-      return MarkAsSimd128(node), VisitI32x4SConvertI16x8Low(node);
-    case IrOpcode::kI32x4SConvertI16x8High:
-      return MarkAsSimd128(node), VisitI32x4SConvertI16x8High(node);
-    case IrOpcode::kI32x4Neg:
-      return MarkAsSimd128(node), VisitI32x4Neg(node);
-    case IrOpcode::kI32x4Shl:
-      return MarkAsSimd128(node), VisitI32x4Shl(node);
-    case IrOpcode::kI32x4ShrS:
-      return MarkAsSimd128(node), VisitI32x4ShrS(node);
-    case IrOpcode::kI32x4Add:
-      return MarkAsSimd128(node), VisitI32x4Add(node);
-    case IrOpcode::kI32x4Sub:
-      return MarkAsSimd128(node), VisitI32x4Sub(node);
-    case IrOpcode::kI32x4Mul:
-      return MarkAsSimd128(node), VisitI32x4Mul(node);
-    case IrOpcode::kI32x4MinS:
-      return MarkAsSimd128(node), VisitI32x4MinS(node);
-    case IrOpcode::kI32x4MaxS:
-      return MarkAsSimd128(node), VisitI32x4MaxS(node);
-    case IrOpcode::kI32x4Eq:
-      return MarkAsSimd128(node), VisitI32x4Eq(node);
-    case IrOpcode::kI32x4Ne:
-      return MarkAsSimd128(node), VisitI32x4Ne(node);
-    case IrOpcode::kI32x4GtS:
-      return MarkAsSimd128(node), VisitI32x4GtS(node);
-    case IrOpcode::kI32x4GeS:
-      return MarkAsSimd128(node), VisitI32x4GeS(node);
-    case IrOpcode::kI32x4UConvertF32x4:
-      return MarkAsSimd128(node), VisitI32x4UConvertF32x4(node);
-    case IrOpcode::kI32x4UConvertI16x8Low:
-      return MarkAsSimd128(node), VisitI32x4UConvertI16x8Low(node);
-    case IrOpcode::kI32x4UConvertI16x8High:
-      return MarkAsSimd128(node), VisitI32x4UConvertI16x8High(node);
-    case IrOpcode::kI32x4ShrU:
-      return MarkAsSimd128(node), VisitI32x4ShrU(node);
-    case IrOpcode::kI32x4MinU:
-      return MarkAsSimd128(node), VisitI32x4MinU(node);
-    case IrOpcode::kI32x4MaxU:
-      return MarkAsSimd128(node), VisitI32x4MaxU(node);
-    case IrOpcode::kI32x4GtU:
-      return MarkAsSimd128(node), VisitI32x4GtU(node);
-    case IrOpcode::kI32x4GeU:
-      return MarkAsSimd128(node), VisitI32x4GeU(node);
-    case IrOpcode::kI32x4Abs:
-      return MarkAsSimd128(node), VisitI32x4Abs(node);
-    case IrOpcode::kI32x4BitMask:
-      return MarkAsWord32(node), VisitI32x4BitMask(node);
-    case IrOpcode::kI32x4DotI16x8S:
-      return MarkAsSimd128(node), VisitI32x4DotI16x8S(node);
-    case IrOpcode::kI32x4ExtMulLowI16x8S:
-      return MarkAsSimd128(node), VisitI32x4ExtMulLowI16x8S(node);
-    case IrOpcode::kI32x4ExtMulHighI16x8S:
-      return MarkAsSimd128(node), VisitI32x4ExtMulHighI16x8S(node);
-    case IrOpcode::kI32x4ExtMulLowI16x8U:
-      return MarkAsSimd128(node), VisitI32x4ExtMulLowI16x8U(node);
-    case IrOpcode::kI32x4ExtMulHighI16x8U:
-      return MarkAsSimd128(node), VisitI32x4ExtMulHighI16x8U(node);
-    case IrOpcode::kI32x4ExtAddPairwiseI16x8S:
-      return MarkAsSimd128(node), VisitI32x4ExtAddPairwiseI16x8S(node);
-    case IrOpcode::kI32x4ExtAddPairwiseI16x8U:
-      return MarkAsSimd128(node), VisitI32x4ExtAddPairwiseI16x8U(node);
-    case IrOpcode::kI32x4TruncSatF64x2SZero:
-      return MarkAsSimd128(node), VisitI32x4TruncSatF64x2SZero(node);
-    case IrOpcode::kI32x4TruncSatF64x2UZero:
-      return MarkAsSimd128(node), VisitI32x4TruncSatF64x2UZero(node);
-    case IrOpcode::kI16x8Splat:
-      return MarkAsSimd128(node), VisitI16x8Splat(node);
-    case IrOpcode::kI16x8ExtractLaneU:
-      return MarkAsWord32(node), VisitI16x8ExtractLaneU(node);
-    case IrOpcode::kI16x8ExtractLaneS:
-      return MarkAsWord32(node), VisitI16x8ExtractLaneS(node);
-    case IrOpcode::kI16x8ReplaceLane:
-      return MarkAsSimd128(node), VisitI16x8ReplaceLane(node);
-    case IrOpcode::kI16x8SConvertI8x16Low:
-      return MarkAsSimd128(node), VisitI16x8SConvertI8x16Low(node);
-    case IrOpcode::kI16x8SConvertI8x16High:
-      return MarkAsSimd128(node), VisitI16x8SConvertI8x16High(node);
-    case IrOpcode::kI16x8Neg:
-      return MarkAsSimd128(node), VisitI16x8Neg(node);
-    case IrOpcode::kI16x8Shl:
-      return MarkAsSimd128(node), VisitI16x8Shl(node);
-    case IrOpcode::kI16x8ShrS:
-      return MarkAsSimd128(node), VisitI16x8ShrS(node);
-    case IrOpcode::kI16x8SConvertI32x4:
-      return MarkAsSimd128(node), VisitI16x8SConvertI32x4(node);
-    case IrOpcode::kI16x8Add:
-      return MarkAsSimd128(node), VisitI16x8Add(node);
-    case IrOpcode::kI16x8AddSatS:
-      return MarkAsSimd128(node), VisitI16x8AddSatS(node);
-    case IrOpcode::kI16x8Sub:
-      return MarkAsSimd128(node), VisitI16x8Sub(node);
-    case IrOpcode::kI16x8SubSatS:
-      return MarkAsSimd128(node), VisitI16x8SubSatS(node);
-    case IrOpcode::kI16x8Mul:
-      return MarkAsSimd128(node), VisitI16x8Mul(node);
-    case IrOpcode::kI16x8MinS:
-      return MarkAsSimd128(node), VisitI16x8MinS(node);
-    case IrOpcode::kI16x8MaxS:
-      return MarkAsSimd128(node), VisitI16x8MaxS(node);
-    case IrOpcode::kI16x8Eq:
-      return MarkAsSimd128(node), VisitI16x8Eq(node);
-    case IrOpcode::kI16x8Ne:
-      return MarkAsSimd128(node), VisitI16x8Ne(node);
-    case IrOpcode::kI16x8GtS:
-      return MarkAsSimd128(node), VisitI16x8GtS(node);
-    case IrOpcode::kI16x8GeS:
-      return MarkAsSimd128(node), VisitI16x8GeS(node);
-    case IrOpcode::kI16x8UConvertI8x16Low:
-      return MarkAsSimd128(node), VisitI16x8UConvertI8x16Low(node);
-    case IrOpcode::kI16x8UConvertI8x16High:
-      return MarkAsSimd128(node), VisitI16x8UConvertI8x16High(node);
-    case IrOpcode::kI16x8ShrU:
-      return MarkAsSimd128(node), VisitI16x8ShrU(node);
-    case IrOpcode::kI16x8UConvertI32x4:
-      return MarkAsSimd128(node), VisitI16x8UConvertI32x4(node);
-    case IrOpcode::kI16x8AddSatU:
-      return MarkAsSimd128(node), VisitI16x8AddSatU(node);
-    case IrOpcode::kI16x8SubSatU:
-      return MarkAsSimd128(node), VisitI16x8SubSatU(node);
-    case IrOpcode::kI16x8MinU:
-      return MarkAsSimd128(node), VisitI16x8MinU(node);
-    case IrOpcode::kI16x8MaxU:
-      return MarkAsSimd128(node), VisitI16x8MaxU(node);
-    case IrOpcode::kI16x8GtU:
-      return MarkAsSimd128(node), VisitI16x8GtU(node);
-    case IrOpcode::kI16x8GeU:
-      return MarkAsSimd128(node), VisitI16x8GeU(node);
-    case IrOpcode::kI16x8RoundingAverageU:
-      return MarkAsSimd128(node), VisitI16x8RoundingAverageU(node);
-    case IrOpcode::kI16x8Q15MulRSatS:
-      return MarkAsSimd128(node), VisitI16x8Q15MulRSatS(node);
-    case IrOpcode::kI16x8Abs:
-      return MarkAsSimd128(node), VisitI16x8Abs(node);
-    case IrOpcode::kI16x8BitMask:
-      return MarkAsWord32(node), VisitI16x8BitMask(node);
-    case IrOpcode::kI16x8ExtMulLowI8x16S:
-      return MarkAsSimd128(node), VisitI16x8ExtMulLowI8x16S(node);
-    case IrOpcode::kI16x8ExtMulHighI8x16S:
-      return MarkAsSimd128(node), VisitI16x8ExtMulHighI8x16S(node);
-    case IrOpcode::kI16x8ExtMulLowI8x16U:
-      return MarkAsSimd128(node), VisitI16x8ExtMulLowI8x16U(node);
-    case IrOpcode::kI16x8ExtMulHighI8x16U:
-      return MarkAsSimd128(node), VisitI16x8ExtMulHighI8x16U(node);
-    case IrOpcode::kI16x8ExtAddPairwiseI8x16S:
-      return MarkAsSimd128(node), VisitI16x8ExtAddPairwiseI8x16S(node);
-    case IrOpcode::kI16x8ExtAddPairwiseI8x16U:
-      return MarkAsSimd128(node), VisitI16x8ExtAddPairwiseI8x16U(node);
-    case IrOpcode::kI8x16Splat:
-      return MarkAsSimd128(node), VisitI8x16Splat(node);
-    case IrOpcode::kI8x16ExtractLaneU:
-      return MarkAsWord32(node), VisitI8x16ExtractLaneU(node);
-    case IrOpcode::kI8x16ExtractLaneS:
-      return MarkAsWord32(node), VisitI8x16ExtractLaneS(node);
-    case IrOpcode::kI8x16ReplaceLane:
-      return MarkAsSimd128(node), VisitI8x16ReplaceLane(node);
-    case IrOpcode::kI8x16Neg:
-      return MarkAsSimd128(node), VisitI8x16Neg(node);
-    case IrOpcode::kI8x16Shl:
-      return MarkAsSimd128(node), VisitI8x16Shl(node);
-    case IrOpcode::kI8x16ShrS:
-      return MarkAsSimd128(node), VisitI8x16ShrS(node);
-    case IrOpcode::kI8x16SConvertI16x8:
-      return MarkAsSimd128(node), VisitI8x16SConvertI16x8(node);
-    case IrOpcode::kI8x16Add:
-      return MarkAsSimd128(node), VisitI8x16Add(node);
-    case IrOpcode::kI8x16AddSatS:
-      return MarkAsSimd128(node), VisitI8x16AddSatS(node);
-    case IrOpcode::kI8x16Sub:
-      return MarkAsSimd128(node), VisitI8x16Sub(node);
-    case IrOpcode::kI8x16SubSatS:
-      return MarkAsSimd128(node), VisitI8x16SubSatS(node);
-    case IrOpcode::kI8x16MinS:
-      return MarkAsSimd128(node), VisitI8x16MinS(node);
-    case IrOpcode::kI8x16MaxS:
-      return MarkAsSimd128(node), VisitI8x16MaxS(node);
-    case IrOpcode::kI8x16Eq:
-      return MarkAsSimd128(node), VisitI8x16Eq(node);
-    case IrOpcode::kI8x16Ne:
-      return MarkAsSimd128(node), VisitI8x16Ne(node);
-    case IrOpcode::kI8x16GtS:
-      return MarkAsSimd128(node), VisitI8x16GtS(node);
-    case IrOpcode::kI8x16GeS:
-      return MarkAsSimd128(node), VisitI8x16GeS(node);
-    case IrOpcode::kI8x16ShrU:
-      return MarkAsSimd128(node), VisitI8x16ShrU(node);
-    case IrOpcode::kI8x16UConvertI16x8:
-      return MarkAsSimd128(node), VisitI8x16UConvertI16x8(node);
-    case IrOpcode::kI8x16AddSatU:
-      return MarkAsSimd128(node), VisitI8x16AddSatU(node);
-    case IrOpcode::kI8x16SubSatU:
-      return MarkAsSimd128(node), VisitI8x16SubSatU(node);
-    case IrOpcode::kI8x16MinU:
-      return MarkAsSimd128(node), VisitI8x16MinU(node);
-    case IrOpcode::kI8x16MaxU:
-      return MarkAsSimd128(node), VisitI8x16MaxU(node);
-    case IrOpcode::kI8x16GtU:
-      return MarkAsSimd128(node), VisitI8x16GtU(node);
-    case IrOpcode::kI8x16GeU:
-      return MarkAsSimd128(node), VisitI8x16GeU(node);
-    case IrOpcode::kI8x16RoundingAverageU:
-      return MarkAsSimd128(node), VisitI8x16RoundingAverageU(node);
-    case IrOpcode::kI8x16Popcnt:
-      return MarkAsSimd128(node), VisitI8x16Popcnt(node);
-    case IrOpcode::kI8x16Abs:
-      return MarkAsSimd128(node), VisitI8x16Abs(node);
-    case IrOpcode::kI8x16BitMask:
-      return MarkAsWord32(node), VisitI8x16BitMask(node);
-    case IrOpcode::kS128Const:
-      return MarkAsSimd128(node), VisitS128Const(node);
-    case IrOpcode::kS128Zero:
-      return MarkAsSimd128(node), VisitS128Zero(node);
-    case IrOpcode::kS128And:
-      return MarkAsSimd128(node), VisitS128And(node);
-    case IrOpcode::kS128Or:
-      return MarkAsSimd128(node), VisitS128Or(node);
-    case IrOpcode::kS128Xor:
-      return MarkAsSimd128(node), VisitS128Xor(node);
-    case IrOpcode::kS128Not:
-      return MarkAsSimd128(node), VisitS128Not(node);
-    case IrOpcode::kS128Select:
-      return MarkAsSimd128(node), VisitS128Select(node);
-    case IrOpcode::kS128AndNot:
-      return MarkAsSimd128(node), VisitS128AndNot(node);
-    case IrOpcode::kI8x16Swizzle:
-      return MarkAsSimd128(node), VisitI8x16Swizzle(node);
-    case IrOpcode::kI8x16Shuffle:
-      return MarkAsSimd128(node), VisitI8x16Shuffle(node);
-    case IrOpcode::kV128AnyTrue:
-      return MarkAsWord32(node), VisitV128AnyTrue(node);
-    case IrOpcode::kI64x2AllTrue:
-      return MarkAsWord32(node), VisitI64x2AllTrue(node);
-    case IrOpcode::kI32x4AllTrue:
-      return MarkAsWord32(node), VisitI32x4AllTrue(node);
-    case IrOpcode::kI16x8AllTrue:
-      return MarkAsWord32(node), VisitI16x8AllTrue(node);
-    case IrOpcode::kI8x16AllTrue:
-      return MarkAsWord32(node), VisitI8x16AllTrue(node);
-    case IrOpcode::kI8x16RelaxedLaneSelect:
-      return MarkAsSimd128(node), VisitI8x16RelaxedLaneSelect(node);
-    case IrOpcode::kI16x8RelaxedLaneSelect:
-      return MarkAsSimd128(node), VisitI16x8RelaxedLaneSelect(node);
-    case IrOpcode::kI32x4RelaxedLaneSelect:
-      return MarkAsSimd128(node), VisitI32x4RelaxedLaneSelect(node);
-    case IrOpcode::kI64x2RelaxedLaneSelect:
-      return MarkAsSimd128(node), VisitI64x2RelaxedLaneSelect(node);
-    case IrOpcode::kF32x4RelaxedMin:
-      return MarkAsSimd128(node), VisitF32x4RelaxedMin(node);
-    case IrOpcode::kF32x4RelaxedMax:
-      return MarkAsSimd128(node), VisitF32x4RelaxedMax(node);
-    case IrOpcode::kF64x2RelaxedMin:
-      return MarkAsSimd128(node), VisitF64x2RelaxedMin(node);
-    case IrOpcode::kF64x2RelaxedMax:
-      return MarkAsSimd128(node), VisitF64x2RelaxedMax(node);
-    case IrOpcode::kI32x4RelaxedTruncF64x2SZero:
-      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF64x2SZero(node);
-    case IrOpcode::kI32x4RelaxedTruncF64x2UZero:
-      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF64x2UZero(node);
-    case IrOpcode::kI32x4RelaxedTruncF32x4S:
-      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF32x4S(node);
-    case IrOpcode::kI32x4RelaxedTruncF32x4U:
-      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF32x4U(node);
-    case IrOpcode::kI16x8RelaxedQ15MulRS:
-      return MarkAsSimd128(node), VisitI16x8RelaxedQ15MulRS(node);
-    case IrOpcode::kI16x8DotI8x16I7x16S:
-      return MarkAsSimd128(node), VisitI16x8DotI8x16I7x16S(node);
-    case IrOpcode::kI32x4DotI8x16I7x16AddS:
-      return MarkAsSimd128(node), VisitI32x4DotI8x16I7x16AddS(node);
-
-      // SIMD256
-#if V8_TARGET_ARCH_X64
-    case IrOpcode::kF64x4Min:
-      return MarkAsSimd256(node), VisitF64x4Min(node);
-    case IrOpcode::kF64x4Max:
-      return MarkAsSimd256(node), VisitF64x4Max(node);
-    case IrOpcode::kF64x4Add:
-      return MarkAsSimd256(node), VisitF64x4Add(node);
-    case IrOpcode::kF32x8Add:
-      return MarkAsSimd256(node), VisitF32x8Add(node);
-    case IrOpcode::kI64x4Add:
-      return MarkAsSimd256(node), VisitI64x4Add(node);
-    case IrOpcode::kI32x8Add:
-      return MarkAsSimd256(node), VisitI32x8Add(node);
-    case IrOpcode::kI16x16Add:
-      return MarkAsSimd256(node), VisitI16x16Add(node);
-    case IrOpcode::kI8x32Add:
-      return MarkAsSimd256(node), VisitI8x32Add(node);
-    case IrOpcode::kF64x4Sub:
-      return MarkAsSimd256(node), VisitF64x4Sub(node);
-    case IrOpcode::kF32x8Sub:
-      return MarkAsSimd256(node), VisitF32x8Sub(node);
-    case IrOpcode::kF32x8Min:
-      return MarkAsSimd256(node), VisitF32x8Min(node);
-    case IrOpcode::kF32x8Max:
-      return MarkAsSimd256(node), VisitF32x8Max(node);
-    case IrOpcode::kI64x4Ne:
-      return MarkAsSimd256(node), VisitI64x4Ne(node);
-    case IrOpcode::kI32x8Ne:
-      return MarkAsSimd256(node), VisitI32x8Ne(node);
-    case IrOpcode::kI32x8GtU:
-      return MarkAsSimd256(node), VisitI32x8GtU(node);
-    case IrOpcode::kI32x8GeS:
-      return MarkAsSimd256(node), VisitI32x8GeS(node);
-    case IrOpcode::kI32x8GeU:
-      return MarkAsSimd256(node), VisitI32x8GeU(node);
-    case IrOpcode::kI16x16Ne:
-      return MarkAsSimd256(node), VisitI16x16Ne(node);
-    case IrOpcode::kI16x16GtU:
-      return MarkAsSimd256(node), VisitI16x16GtU(node);
-    case IrOpcode::kI16x16GeS:
-      return MarkAsSimd256(node), VisitI16x16GeS(node);
-    case IrOpcode::kI16x16GeU:
-      return MarkAsSimd256(node), VisitI16x16GeU(node);
-    case IrOpcode::kI8x32Ne:
-      return MarkAsSimd256(node), VisitI8x32Ne(node);
-    case IrOpcode::kI8x32GtU:
-      return MarkAsSimd256(node), VisitI8x32GtU(node);
-    case IrOpcode::kI8x32GeS:
-      return MarkAsSimd256(node), VisitI8x32GeS(node);
-    case IrOpcode::kI8x32GeU:
-      return MarkAsSimd256(node), VisitI8x32GeU(node);
-    case IrOpcode::kI64x4Sub:
-      return MarkAsSimd256(node), VisitI64x4Sub(node);
-    case IrOpcode::kI32x8Sub:
-      return MarkAsSimd256(node), VisitI32x8Sub(node);
-    case IrOpcode::kI16x16Sub:
-      return MarkAsSimd256(node), VisitI16x16Sub(node);
-    case IrOpcode::kI8x32Sub:
-      return MarkAsSimd256(node), VisitI8x32Sub(node);
-    case IrOpcode::kF64x4Mul:
-      return MarkAsSimd256(node), VisitF64x4Mul(node);
-    case IrOpcode::kF32x8Mul:
-      return MarkAsSimd256(node), VisitF32x8Mul(node);
-    case IrOpcode::kI64x4Mul:
-      return MarkAsSimd256(node), VisitI64x4Mul(node);
-    case IrOpcode::kI32x8Mul:
-      return MarkAsSimd256(node), VisitI32x8Mul(node);
-    case IrOpcode::kI16x16Mul:
-      return MarkAsSimd256(node), VisitI16x16Mul(node);
-    case IrOpcode::kF32x8Div:
-      return MarkAsSimd256(node), VisitF32x8Div(node);
-    case IrOpcode::kF64x4Div:
-      return MarkAsSimd256(node), VisitF64x4Div(node);
-    case IrOpcode::kI16x16AddSatS:
-      return MarkAsSimd256(node), VisitI16x16AddSatS(node);
-    case IrOpcode::kI8x32AddSatS:
-      return MarkAsSimd256(node), VisitI8x32AddSatS(node);
-    case IrOpcode::kI16x16AddSatU:
-      return MarkAsSimd256(node), VisitI16x16AddSatU(node);
-    case IrOpcode::kI8x32AddSatU:
-      return MarkAsSimd256(node), VisitI8x32AddSatU(node);
-    case IrOpcode::kI16x16SubSatS:
-      return MarkAsSimd256(node), VisitI16x16SubSatS(node);
-    case IrOpcode::kI8x32SubSatS:
-      return MarkAsSimd256(node), VisitI8x32SubSatS(node);
-    case IrOpcode::kI16x16SubSatU:
-      return MarkAsSimd256(node), VisitI16x16SubSatU(node);
-    case IrOpcode::kI8x32SubSatU:
-      return MarkAsSimd256(node), VisitI8x32SubSatU(node);
-    case IrOpcode::kF64x4ConvertI32x4S:
-      return MarkAsSimd256(node), VisitF64x4ConvertI32x4S(node);
-    case IrOpcode::kF32x8SConvertI32x8:
-      return MarkAsSimd256(node), VisitF32x8SConvertI32x8(node);
-    case IrOpcode::kF32x4DemoteF64x4:
-      return MarkAsSimd256(node), VisitF32x4DemoteF64x4(node);
-    case IrOpcode::kI64x4SConvertI32x4:
-      return MarkAsSimd256(node), VisitI64x4SConvertI32x4(node);
-    case IrOpcode::kI64x4UConvertI32x4:
-      return MarkAsSimd256(node), VisitI64x4UConvertI32x4(node);
-    case IrOpcode::kI32x8SConvertI16x8:
-      return MarkAsSimd256(node), VisitI32x8SConvertI16x8(node);
-    case IrOpcode::kI32x8UConvertI16x8:
-      return MarkAsSimd256(node), VisitI32x8UConvertI16x8(node);
-    case IrOpcode::kI16x16SConvertI8x16:
-      return MarkAsSimd256(node), VisitI16x16SConvertI8x16(node);
-    case IrOpcode::kI16x16UConvertI8x16:
-      return MarkAsSimd256(node), VisitI16x16UConvertI8x16(node);
-    case IrOpcode::kI16x16SConvertI32x8:
-      return MarkAsSimd256(node), VisitI16x16SConvertI32x8(node);
-    case IrOpcode::kI16x16UConvertI32x8:
-      return MarkAsSimd256(node), VisitI16x16UConvertI32x8(node);
-    case IrOpcode::kI8x32SConvertI16x16:
-      return MarkAsSimd256(node), VisitI8x32SConvertI16x16(node);
-    case IrOpcode::kI8x32UConvertI16x16:
-      return MarkAsSimd256(node), VisitI8x32UConvertI16x16(node);
-    case IrOpcode::kF32x8Abs:
-      return MarkAsSimd256(node), VisitF32x8Abs(node);
-    case IrOpcode::kF32x8Neg:
-      return MarkAsSimd256(node), VisitF32x8Neg(node);
-    case IrOpcode::kF32x8Sqrt:
-      return MarkAsSimd256(node), VisitF32x8Sqrt(node);
-    case IrOpcode::kF64x4Sqrt:
-      return MarkAsSimd256(node), VisitF64x4Sqrt(node);
-    case IrOpcode::kI32x8Abs:
-      return MarkAsSimd256(node), VisitI32x8Abs(node);
-    case IrOpcode::kI32x8Neg:
-      return MarkAsSimd256(node), VisitI32x8Neg(node);
-    case IrOpcode::kI16x16Abs:
-      return MarkAsSimd256(node), VisitI16x16Abs(node);
-    case IrOpcode::kI16x16Neg:
-      return MarkAsSimd256(node), VisitI16x16Neg(node);
-    case IrOpcode::kI8x32Abs:
-      return MarkAsSimd256(node), VisitI8x32Abs(node);
-    case IrOpcode::kI8x32Neg:
-      return MarkAsSimd256(node), VisitI8x32Neg(node);
-    case IrOpcode::kI64x4Shl:
-      return MarkAsSimd256(node), VisitI64x4Shl(node);
-    case IrOpcode::kI64x4ShrU:
-      return MarkAsSimd256(node), VisitI64x4ShrU(node);
-    case IrOpcode::kI32x8Shl:
-      return MarkAsSimd256(node), VisitI32x8Shl(node);
-    case IrOpcode::kI32x8ShrS:
-      return MarkAsSimd256(node), VisitI32x8ShrS(node);
-    case IrOpcode::kI32x8ShrU:
-      return MarkAsSimd256(node), VisitI32x8ShrU(node);
-    case IrOpcode::kI16x16Shl:
-      return MarkAsSimd256(node), VisitI16x16Shl(node);
-    case IrOpcode::kI16x16ShrS:
-      return MarkAsSimd256(node), VisitI16x16ShrS(node);
-    case IrOpcode::kI16x16ShrU:
-      return MarkAsSimd256(node), VisitI16x16ShrU(node);
-    case IrOpcode::kI32x8DotI16x16S:
-      return MarkAsSimd256(node), VisitI32x8DotI16x16S(node);
-    case IrOpcode::kI16x16RoundingAverageU:
-      return MarkAsSimd256(node), VisitI16x16RoundingAverageU(node);
-    case IrOpcode::kI8x32RoundingAverageU:
-      return MarkAsSimd256(node), VisitI8x32RoundingAverageU(node);
-    case IrOpcode::kS256Zero:
-      return MarkAsSimd256(node), VisitS256Zero(node);
-    case IrOpcode::kS256And:
-      return MarkAsSimd256(node), VisitS256And(node);
-    case IrOpcode::kS256Or:
-      return MarkAsSimd256(node), VisitS256Or(node);
-    case IrOpcode::kS256Xor:
-      return MarkAsSimd256(node), VisitS256Xor(node);
-    case IrOpcode::kS256Not:
-      return MarkAsSimd256(node), VisitS256Not(node);
-    case IrOpcode::kS256Select:
-      return MarkAsSimd256(node), VisitS256Select(node);
-    case IrOpcode::kS256AndNot:
-      return MarkAsSimd256(node), VisitS256AndNot(node);
-    case IrOpcode::kF32x8Eq:
-      return MarkAsSimd256(node), VisitF32x8Eq(node);
-    case IrOpcode::kF64x4Eq:
-      return MarkAsSimd256(node), VisitF64x4Eq(node);
-    case IrOpcode::kI64x4Eq:
-      return MarkAsSimd256(node), VisitI64x4Eq(node);
-    case IrOpcode::kI32x8Eq:
-      return MarkAsSimd256(node), VisitI32x8Eq(node);
-    case IrOpcode::kI16x16Eq:
-      return MarkAsSimd256(node), VisitI16x16Eq(node);
-    case IrOpcode::kI8x32Eq:
-      return MarkAsSimd256(node), VisitI8x32Eq(node);
-    case IrOpcode::kF32x8Ne:
-      return MarkAsSimd256(node), VisitF32x8Ne(node);
-    case IrOpcode::kF64x4Ne:
-      return MarkAsSimd256(node), VisitF64x4Ne(node);
-    case IrOpcode::kI64x4GtS:
-      return MarkAsSimd256(node), VisitI64x4GtS(node);
-    case IrOpcode::kI32x8GtS:
-      return MarkAsSimd256(node), VisitI32x8GtS(node);
-    case IrOpcode::kI16x16GtS:
-      return MarkAsSimd256(node), VisitI16x16GtS(node);
-    case IrOpcode::kI8x32GtS:
-      return MarkAsSimd256(node), VisitI8x32GtS(node);
-    case IrOpcode::kF64x4Lt:
-      return MarkAsSimd256(node), VisitF64x4Lt(node);
-    case IrOpcode::kF32x8Lt:
-      return MarkAsSimd256(node), VisitF32x8Lt(node);
-    case IrOpcode::kF64x4Le:
-      return MarkAsSimd256(node), VisitF64x4Le(node);
-    case IrOpcode::kF32x8Le:
-      return MarkAsSimd256(node), VisitF32x8Le(node);
-    case IrOpcode::kI32x8MinS:
-      return MarkAsSimd256(node), VisitI32x8MinS(node);
-    case IrOpcode::kI16x16MinS:
-      return MarkAsSimd256(node), VisitI16x16MinS(node);
-    case IrOpcode::kI8x32MinS:
-      return MarkAsSimd256(node), VisitI8x32MinS(node);
-    case IrOpcode::kI32x8MinU:
-      return MarkAsSimd256(node), VisitI32x8MinU(node);
-    case IrOpcode::kI16x16MinU:
-      return MarkAsSimd256(node), VisitI16x16MinU(node);
-    case IrOpcode::kI8x32MinU:
-      return MarkAsSimd256(node), VisitI8x32MinU(node);
-    case IrOpcode::kI32x8MaxS:
-      return MarkAsSimd256(node), VisitI32x8MaxS(node);
-    case IrOpcode::kI16x16MaxS:
-      return MarkAsSimd256(node), VisitI16x16MaxS(node);
-    case IrOpcode::kI8x32MaxS:
-      return MarkAsSimd256(node), VisitI8x32MaxS(node);
-    case IrOpcode::kI32x8MaxU:
-      return MarkAsSimd256(node), VisitI32x8MaxU(node);
-    case IrOpcode::kI16x16MaxU:
-      return MarkAsSimd256(node), VisitI16x16MaxU(node);
-    case IrOpcode::kI8x32MaxU:
-      return MarkAsSimd256(node), VisitI8x32MaxU(node);
-    case IrOpcode::kI64x4Splat:
-      return MarkAsSimd256(node), VisitI64x4Splat(node);
-    case IrOpcode::kI32x8Splat:
-      return MarkAsSimd256(node), VisitI32x8Splat(node);
-    case IrOpcode::kI16x16Splat:
-      return MarkAsSimd256(node), VisitI16x16Splat(node);
-    case IrOpcode::kI8x32Splat:
-      return MarkAsSimd256(node), VisitI8x32Splat(node);
-    case IrOpcode::kI64x4ExtMulI32x4S:
-      return MarkAsSimd256(node), VisitI64x4ExtMulI32x4S(node);
-    case IrOpcode::kI64x4ExtMulI32x4U:
-      return MarkAsSimd256(node), VisitI64x4ExtMulI32x4U(node);
-    case IrOpcode::kI32x8ExtMulI16x8S:
-      return MarkAsSimd256(node), VisitI32x8ExtMulI16x8S(node);
-    case IrOpcode::kI32x8ExtMulI16x8U:
-      return MarkAsSimd256(node), VisitI32x8ExtMulI16x8U(node);
-    case IrOpcode::kI16x16ExtMulI8x16S:
-      return MarkAsSimd256(node), VisitI16x16ExtMulI8x16S(node);
-    case IrOpcode::kI16x16ExtMulI8x16U:
-      return MarkAsSimd256(node), VisitI16x16ExtMulI8x16U(node);
-    case IrOpcode::kI32x8ExtAddPairwiseI16x16S:
-      return MarkAsSimd256(node), VisitI32x8ExtAddPairwiseI16x16S(node);
-    case IrOpcode::kI32x8ExtAddPairwiseI16x16U:
-      return MarkAsSimd256(node), VisitI32x8ExtAddPairwiseI16x16U(node);
-    case IrOpcode::kI16x16ExtAddPairwiseI8x32S:
-      return MarkAsSimd256(node), VisitI16x16ExtAddPairwiseI8x32S(node);
-    case IrOpcode::kI16x16ExtAddPairwiseI8x32U:
-      return MarkAsSimd256(node), VisitI16x16ExtAddPairwiseI8x32U(node);
-#endif  //  V8_TARGET_ARCH_X64
-    default:
-      FATAL("Unexpected operator #%d:%s @ node #%d", node->opcode(),
-            node->op()->mnemonic(), node->id());
-  }
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kStackPointerGreaterThanCondition, node);
-  VisitStackPointerGreaterThan(node, &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoadStackCheckOffset(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchStackCheckOffset, g.DefineAsRegister(node));
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoadFramePointer(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchFramePointer, g.DefineAsRegister(node));
+void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(node_t node) {
+  FlagsContinuation cont =
+      FlagsContinuation::ForSet(kStackPointerGreaterThanCondition, node);
+  VisitStackPointerGreaterThan(node, &cont);
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitLoadStackCheckOffset(
+    Node* node) {
+  OperandGenerator g(this);
+  Emit(kArchStackCheckOffset, g.DefineAsRegister(node));
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitLoadFramePointer(Node* node) {
+  OperandGenerator g(this);
+  Emit(kArchFramePointer, g.DefineAsRegister(node));
 }
 
 template <typename Adapter>
@@ -3340,9 +2269,9 @@ void InstructionSelectorT<Adapter>::VisitFinishRegion(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitParameter(Node* node) {
+void InstructionSelectorT<Adapter>::VisitParameter(node_t node) {
   OperandGenerator g(this);
-  int index = ParameterIndexOf(node->op());
+  int index = this->parameter_index_of(node);
 
   if (linkage()->GetParameterLocation(index).IsNullRegister()) {
     EmitMoveParamToFPR(node, index);
@@ -3375,8 +2304,8 @@ constexpr InstructionCode EncodeCallDescriptorFlags(
 
 }  // namespace
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitIfException(Node* node) {
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitIfException(Node* node) {
   OperandGenerator g(this);
   DCHECK_EQ(IrOpcode::kCall, node->InputAt(1)->opcode());
   Emit(kArchNop, g.DefineAsLocation(node, ExceptionLocation()));
@@ -3397,9 +2326,7 @@ void InstructionSelectorT<Adapter>::VisitPhi(Node* node) {
   PhiInstruction* phi = instruction_zone()->template New<PhiInstruction>(
       instruction_zone(), GetVirtualRegister(node),
       static_cast<size_t>(input_count));
-  sequence()
-      ->InstructionBlockAt(RpoNumber::FromInt(current_block_->rpo_number()))
-      ->AddPhi(phi);
+  sequence()->InstructionBlockAt(this->rpo_number(current_block_))->AddPhi(phi);
   for (int i = 0; i < input_count; ++i) {
     Node* const input = node->InputAt(i);
     MarkAsUsed(input);
@@ -3445,432 +2372,1949 @@ void InstructionSelectorT<Adapter>::VisitProjection(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitConstant(Node* node) {
-  // We must emit a NOP here because every live range needs a defining
-  // instruction in the register allocator.
-  OperandGenerator g(this);
-  Emit(kArchNop, g.DefineAsConstant(node));
+void InstructionSelectorT<Adapter>::VisitConstant(node_t node) {
+  // We must emit a NOP here because every live range needs a defining
+  // instruction in the register allocator.
+  OperandGenerator g(this);
+  Emit(kArchNop, g.DefineAsConstant(node));
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::UpdateMaxPushedArgumentCount(size_t count) {
+  *max_pushed_argument_count_ = std::max(count, *max_pushed_argument_count_);
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitCall(node_t node, block_t handler) {
+  OperandGenerator g(this);
+  auto call = this->call_view(node);
+  const CallDescriptor* call_descriptor = call.call_descriptor();
+  SaveFPRegsMode mode = call_descriptor->NeedsCallerSavedFPRegisters()
+                            ? SaveFPRegsMode::kSave
+                            : SaveFPRegsMode::kIgnore;
+
+  if (call_descriptor->NeedsCallerSavedRegisters()) {
+    Emit(kArchSaveCallerRegisters | MiscField::encode(static_cast<int>(mode)),
+         g.NoOutput());
+  }
+
+  FrameStateDescriptor* frame_state_descriptor = nullptr;
+  if (call_descriptor->NeedsFrameState()) {
+    frame_state_descriptor = GetFrameStateDescriptor(call.frame_state());
+  }
+
+  CallBuffer buffer(zone(), call_descriptor, frame_state_descriptor);
+  CallDescriptor::Flags flags = call_descriptor->flags();
+
+  // Compute InstructionOperands for inputs and outputs.
+  // TODO(turbofan): on some architectures it's probably better to use
+  // the code object in a register if there are multiple uses of it.
+  // Improve constant pool and the heuristics in the register allocator
+  // for where to emit constants.
+  CallBufferFlags call_buffer_flags(kCallCodeImmediate | kCallAddressImmediate);
+  if (flags & CallDescriptor::kFixedTargetRegister) {
+    call_buffer_flags |= kCallFixedTargetRegister;
+  }
+  InitializeCallBuffer(node, &buffer, call_buffer_flags);
+
+  EmitPrepareArguments(&buffer.pushed_nodes, call_descriptor, node);
+  UpdateMaxPushedArgumentCount(buffer.pushed_nodes.size());
+
+  // Pass label of exception handler block.
+  if (handler) {
+    if constexpr (Adapter::IsTurbofan) {
+      DCHECK_EQ(IrOpcode::kIfException, handler->front()->opcode());
+    }
+    flags |= CallDescriptor::kHasExceptionHandler;
+    buffer.instruction_args.push_back(g.Label(handler));
+  }
+
+  // Select the appropriate opcode based on the call type.
+  InstructionCode opcode;
+  switch (call_descriptor->kind()) {
+    case CallDescriptor::kCallAddress: {
+      int gp_param_count =
+          static_cast<int>(call_descriptor->GPParameterCount());
+      int fp_param_count =
+          static_cast<int>(call_descriptor->FPParameterCount());
+#if ABI_USES_FUNCTION_DESCRIPTORS
+      // Highest fp_param_count bit is used on AIX to indicate if a CFunction
+      // call has function descriptor or not.
+      static_assert(FPParamField::kSize == kHasFunctionDescriptorBitShift + 1);
+      if (!call_descriptor->NoFunctionDescriptor()) {
+        fp_param_count |= 1 << kHasFunctionDescriptorBitShift;
+      }
+#endif
+      opcode = kArchCallCFunction | ParamField::encode(gp_param_count) |
+               FPParamField::encode(fp_param_count);
+      break;
+    }
+    case CallDescriptor::kCallCodeObject:
+      opcode = EncodeCallDescriptorFlags(kArchCallCodeObject, flags);
+      break;
+    case CallDescriptor::kCallJSFunction:
+      opcode = EncodeCallDescriptorFlags(kArchCallJSFunction, flags);
+      break;
+#if V8_ENABLE_WEBASSEMBLY
+    case CallDescriptor::kCallWasmCapiFunction:
+    case CallDescriptor::kCallWasmFunction:
+    case CallDescriptor::kCallWasmImportWrapper:
+      opcode = EncodeCallDescriptorFlags(kArchCallWasmFunction, flags);
+      break;
+#endif  // V8_ENABLE_WEBASSEMBLY
+    case CallDescriptor::kCallBuiltinPointer:
+      opcode = EncodeCallDescriptorFlags(kArchCallBuiltinPointer, flags);
+      break;
+  }
+
+  // Emit the call instruction.
+  size_t const output_count = buffer.outputs.size();
+  auto* outputs = output_count ? &buffer.outputs.front() : nullptr;
+  Instruction* call_instr =
+      Emit(opcode, output_count, outputs, buffer.instruction_args.size(),
+           &buffer.instruction_args.front());
+  if (instruction_selection_failed()) return;
+  call_instr->MarkAsCall();
+
+  EmitPrepareResults(&(buffer.output_nodes), call_descriptor, node);
+
+  if (call_descriptor->NeedsCallerSavedRegisters()) {
+    Emit(
+        kArchRestoreCallerRegisters | MiscField::encode(static_cast<int>(mode)),
+        g.NoOutput());
+  }
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    OperandGenerator g(this);
+
+    auto caller = linkage()->GetIncomingDescriptor();
+    auto callee = CallDescriptorOf(node->op());
+    DCHECK(caller->CanTailCall(callee));
+    const int stack_param_delta = callee->GetStackParameterDelta(caller);
+    CallBuffer buffer(zone(), callee, nullptr);
+
+    // Compute InstructionOperands for inputs and outputs.
+    CallBufferFlags flags(kCallCodeImmediate | kCallTail);
+    if (IsTailCallAddressImmediate()) {
+      flags |= kCallAddressImmediate;
+    }
+    if (callee->flags() & CallDescriptor::kFixedTargetRegister) {
+      flags |= kCallFixedTargetRegister;
+    }
+    InitializeCallBuffer(node, &buffer, flags, stack_param_delta);
+    UpdateMaxPushedArgumentCount(stack_param_delta);
+
+    // Select the appropriate opcode based on the call type.
+    InstructionCode opcode;
+    InstructionOperandVector temps(zone());
+    switch (callee->kind()) {
+      case CallDescriptor::kCallCodeObject:
+        opcode = kArchTailCallCodeObject;
+        break;
+      case CallDescriptor::kCallAddress:
+        DCHECK(!caller->IsJSFunctionCall());
+        opcode = kArchTailCallAddress;
+        break;
+#if V8_ENABLE_WEBASSEMBLY
+      case CallDescriptor::kCallWasmFunction:
+        DCHECK(!caller->IsJSFunctionCall());
+        opcode = kArchTailCallWasm;
+        break;
+#endif  // V8_ENABLE_WEBASSEMBLY
+      default:
+        UNREACHABLE();
+    }
+    opcode = EncodeCallDescriptorFlags(opcode, callee->flags());
+
+    Emit(kArchPrepareTailCall, g.NoOutput());
+
+    // Add an immediate operand that represents the offset to the first slot
+    // that is unused with respect to the stack pointer that has been updated
+    // for the tail call instruction. Backends that pad arguments can write the
+    // padding value at this offset from the stack.
+    const int optional_padding_offset =
+        callee->GetOffsetToFirstUnusedStackSlot() - 1;
+    buffer.instruction_args.push_back(g.TempImmediate(optional_padding_offset));
+
+    const int first_unused_slot_offset =
+        kReturnAddressStackSlotCount + stack_param_delta;
+    buffer.instruction_args.push_back(
+        g.TempImmediate(first_unused_slot_offset));
+
+    // Emit the tailcall instruction.
+    Emit(opcode, 0, nullptr, buffer.instruction_args.size(),
+         &buffer.instruction_args.front(), temps.size(),
+         temps.empty() ? nullptr : &temps.front());
+  }
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitGoto(block_t target) {
+  // jump to the next block.
+  OperandGenerator g(this);
+  Emit(kArchJmp, g.NoOutput(), g.Label(target));
+}
+
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitReturn(node_t node) {
+  const turboshaft::ReturnOp& ret =
+      schedule()->Get(node).Cast<turboshaft::ReturnOp>();
+
+  OperandGenerator g(this);
+  const int input_count =
+      linkage()->GetIncomingDescriptor()->ReturnCount() == 0
+          ? 1
+          : (1 + static_cast<int>(ret.return_values().size()));
+  DCHECK_GE(input_count, 1);
+
+  auto value_locations =
+      zone()->template NewArray<InstructionOperand>(input_count);
+  const turboshaft::Operation& pop_count = schedule()->Get(ret.pop_count());
+  if (const turboshaft::ConstantOp* constant =
+          pop_count.TryCast<turboshaft::ConstantOp>();
+      constant != nullptr &&
+      (constant->kind == turboshaft::ConstantOp::Kind::kWord32 ||
+       constant->kind == turboshaft::ConstantOp::Kind::kWord64)) {
+    value_locations[0] = g.UseImmediate(ret.pop_count());
+  } else {
+    value_locations[0] = g.UseRegister(ret.pop_count());
+  }
+  for (int i = 0; i < input_count - 1; ++i) {
+    value_locations[i + 1] =
+        g.UseLocation(ret.return_values()[i], linkage()->GetReturnLocation(i));
+  }
+  Emit(kArchRet, 0, nullptr, input_count, value_locations);
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitReturn(node_t ret) {
+  OperandGenerator g(this);
+  const int input_count = linkage()->GetIncomingDescriptor()->ReturnCount() == 0
+                              ? 1
+                              : ret->op()->ValueInputCount();
+  DCHECK_GE(input_count, 1);
+  auto value_locations =
+      zone()->template NewArray<InstructionOperand>(input_count);
+  Node* pop_count = ret->InputAt(0);
+  value_locations[0] = (pop_count->opcode() == IrOpcode::kInt32Constant ||
+                        pop_count->opcode() == IrOpcode::kInt64Constant)
+                           ? g.UseImmediate(pop_count)
+                           : g.UseRegister(pop_count);
+  for (int i = 1; i < input_count; ++i) {
+    value_locations[i] =
+        g.UseLocation(ret->InputAt(i), linkage()->GetReturnLocation(i - 1));
+  }
+  Emit(kArchRet, 0, nullptr, input_count, value_locations);
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBranch(node_t branch_node,
+                                                block_t tbranch,
+                                                block_t fbranch) {
+  auto branch = this->branch_view(branch_node);
+  TryPrepareScheduleFirstProjection(branch.condition());
+
+  FlagsContinuation cont =
+      FlagsContinuation::ForBranch(kNotEqual, tbranch, fbranch);
+  VisitWordCompareZero(branch, branch.condition(), &cont);
+}
+
+// When a DeoptimizeIf/DeoptimizeUnless/Branch depends on a BinopOverflow, the
+// InstructionSelector can sometimes generate a fuse instruction covering both
+// the BinopOverflow and the DeoptIf/Branch, and the final emitted code will
+// look like:
+//
+//     r = BinopOverflow
+//     jo branch_target/deopt_target
+//
+// When this fusing fails, the final code looks like:
+//
+//     r = BinopOverflow
+//     o = sete  // sets overflow bit
+//     cmp o, 0
+//     jnz branch_target/deopt_target
+//
+// To be able to fuse tue BinopOverflow and the DeoptIf/Branch, the 1st
+// projection (Projection[0], which contains the actual result) must already be
+// scheduled (and a few other conditions must be satisfied, see
+// InstructionSelectorXXX::VisitWordCompareZero).
+// TryPrepareScheduleFirstProjection is thus called from
+// VisitDeoptimizeIf/VisitDeoptimizeUnless/VisitBranch and detects if the 1st
+// projection could be scheduled now, and, if so, defines it.
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::TryPrepareScheduleFirstProjection(
+    node_t maybe_projection) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+  } else {
+    if (maybe_projection->opcode() != IrOpcode::kProjection) {
+      // The DeoptimizeIf/DeoptimizeUnless/Branch condition is not a projection.
+      return;
+    }
+
+    if (ProjectionIndexOf(maybe_projection->op()) != 1u) {
+      // The DeoptimizeIf/DeoptimizeUnless/Branch isn't on the Projection[1]
+      // (ie, not on the overflow bit of a BinopOverflow).
+      return;
+    }
+
+    Node* const node = maybe_projection->InputAt(0);
+    if (this->block(schedule_, node) != current_block_) {
+      // The projection input is not in the current block, so it shouldn't be
+      // emitted now, so we don't need to eagerly schedule its Projection[0].
+      return;
+    }
+
+    switch (node->opcode()) {
+      case IrOpcode::kInt32AddWithOverflow:
+      case IrOpcode::kInt32SubWithOverflow:
+      case IrOpcode::kInt32MulWithOverflow:
+      case IrOpcode::kInt64AddWithOverflow:
+      case IrOpcode::kInt64SubWithOverflow:
+      case IrOpcode::kInt64MulWithOverflow: {
+        Node* result = NodeProperties::FindProjection(node, 0);
+        if (result == nullptr || IsDefined(result)) {
+          // No Projection(0), or it's already defined.
+          return;
+        }
+
+        if (this->block(schedule_, result) != current_block_) {
+          // {result} wasn't planned to be scheduled in {current_block_}. To
+          // avoid adding checks to see if it can still be scheduled now, we
+          // just bail out.
+          return;
+        }
+
+        // Checking if all uses of {result} that are in the current block have
+        // already been Defined.
+        // We also ignore Phi uses: if {result} is used in a Phi in the block in
+        // which it is defined, this means that this block is a loop header, and
+        // {result} back into it through the back edge. In this case, it's
+        // normal to schedule {result} before the Phi that uses it.
+        for (Node* use : result->uses()) {
+          if (!IsDefined(use) &&
+              this->block(schedule_, use) == current_block_ &&
+              use->opcode() != IrOpcode::kPhi) {
+            // {use} is in the current block but is not defined yet. It's
+            // possible that it's not actually used, but the IsUsed(x) predicate
+            // is not valid until we have visited `x`, so we overaproximate and
+            // assume that {use} is itself used.
+            return;
+          }
+        }
+
+        // Visiting the projection now. Note that this relies on the fact that
+        // VisitProjection doesn't Emit something: if it did, then we could be
+        // Emitting something after a Branch, which is invalid (Branch can only
+        // be at the end of a block, and the end of a block must always be a
+        // block terminator). (remember that we emit operation in reverse order,
+        // so because we are doing TryPrepareScheduleFirstProjection before
+        // actually emitting the Branch, it would be after in the final
+        // instruction sequence, not before)
+        VisitProjection(result);
+        return;
+      }
+
+      default:
+        return;
+    }
+  }
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitDeoptimizeIf(Node* node) {
+  TryPrepareScheduleFirstProjection(node->InputAt(0));
+
+  DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
+  FlagsContinuation cont = FlagsContinuation::ForDeoptimize(
+      kNotEqual, p.reason(), node->id(), p.feedback(),
+      FrameState{node->InputAt(1)});
+  VisitWordCompareZero(node, node->InputAt(0), &cont);
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeoptimizeUnless(Node* node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    TryPrepareScheduleFirstProjection(node->InputAt(0));
+
+    DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
+    FlagsContinuation cont = FlagsContinuation::ForDeoptimize(
+        kEqual, p.reason(), node->id(), p.feedback(),
+        FrameState{node->InputAt(1)});
+    VisitWordCompareZero(node, node->InputAt(0), &cont);
+  }
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitSelect(Node* node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSelect(
+        kNotEqual, node, node->InputAt(1), node->InputAt(2));
+    VisitWordCompareZero(node, node->InputAt(0), &cont);
+  }
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitTrapIf(node_t node, TrapId trap_id) {
+  // FrameStates are only used for wasm traps inlined in JS. In that case the
+  // trap node will be lowered (replaced) before instruction selection.
+  // Therefore any TrapIf node has only one input.
+  DCHECK_EQ(this->value_input_count(node), 1);
+  FlagsContinuation cont = FlagsContinuation::ForTrap(kNotEqual, trap_id);
+  VisitWordCompareZero(node, this->input_at(node, 0), &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::UpdateMaxPushedArgumentCount(size_t count) {
-  *max_pushed_argument_count_ = std::max(count, *max_pushed_argument_count_);
+void InstructionSelectorT<Adapter>::VisitTrapUnless(node_t node,
+                                                    TrapId trap_id) {
+  // FrameStates are only used for wasm traps inlined in JS. In that case the
+  // trap node will be lowered (replaced) before instruction selection.
+  // Therefore any TrapUnless node has only one input.
+  DCHECK_EQ(this->value_input_count(node), 1);
+  FlagsContinuation cont = FlagsContinuation::ForTrap(kEqual, trap_id);
+  VisitWordCompareZero(node, this->input_at(node, 0), &cont);
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::EmitIdentity(Node* node) {
+  MarkAsUsed(node->InputAt(0));
+  MarkAsDefined(node);
+  SetRename(node, node->InputAt(0));
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeoptimize(
+    DeoptimizeReason reason, id_t node_id, FeedbackSource const& feedback,
+    node_t frame_state) {
+  InstructionOperandVector args(instruction_zone());
+  AppendDeoptimizeArguments(&args, reason, node_id, feedback, frame_state);
+  Emit(kArchDeoptimize, 0, nullptr, args.size(), &args.front(), 0, nullptr);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitCall(Node* node, BasicBlock* handler) {
+void InstructionSelectorT<Adapter>::VisitThrow(Node* node) {
   OperandGenerator g(this);
-  auto call_descriptor = CallDescriptorOf(node->op());
-  SaveFPRegsMode mode = call_descriptor->NeedsCallerSavedFPRegisters()
-                            ? SaveFPRegsMode::kSave
-                            : SaveFPRegsMode::kIgnore;
+  Emit(kArchThrowTerminator, g.NoOutput());
+}
 
-  if (call_descriptor->NeedsCallerSavedRegisters()) {
-    Emit(kArchSaveCallerRegisters | MiscField::encode(static_cast<int>(mode)),
-         g.NoOutput());
-  }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDebugBreak(Node* node) {
+  OperandGenerator g(this);
+  Emit(kArchDebugBreak, g.NoOutput());
+}
 
-  FrameStateDescriptor* frame_state_descriptor = nullptr;
-  if (call_descriptor->NeedsFrameState()) {
-    frame_state_descriptor = GetFrameStateDescriptor(FrameState{
-        node->InputAt(static_cast<int>(call_descriptor->InputCount()))});
-  }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitUnreachable(Node* node) {
+  OperandGenerator g(this);
+  Emit(kArchDebugBreak, g.NoOutput());
+}
 
-  CallBuffer buffer(zone(), call_descriptor, frame_state_descriptor);
-  CallDescriptor::Flags flags = call_descriptor->flags();
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitStaticAssert(Node* node) {
+  Node* asserted = node->InputAt(0);
+  UnparkedScopeIfNeeded scope(broker_);
+  AllowHandleDereference allow_handle_dereference;
+  asserted->Print(4);
+  FATAL(
+      "Expected Turbofan static assert to hold, but got non-true input:\n  %s",
+      StaticAssertSourceOf(node->op()));
+}
 
-  // Compute InstructionOperands for inputs and outputs.
-  // TODO(turbofan): on some architectures it's probably better to use
-  // the code object in a register if there are multiple uses of it.
-  // Improve constant pool and the heuristics in the register allocator
-  // for where to emit constants.
-  CallBufferFlags call_buffer_flags(kCallCodeImmediate | kCallAddressImmediate);
-  if (flags & CallDescriptor::kFixedTargetRegister) {
-    call_buffer_flags |= kCallFixedTargetRegister;
-  }
-  InitializeCallBuffer(node, &buffer, call_buffer_flags);
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitDeadValue(Node* node) {
+  OperandGenerator g(this);
+  MarkAsRepresentation(DeadValueRepresentationOf(node->op()), node);
+  Emit(kArchDebugBreak, g.DefineAsConstant(node));
+}
 
-  EmitPrepareArguments(&buffer.pushed_nodes, call_descriptor, node);
-  UpdateMaxPushedArgumentCount(buffer.pushed_nodes.size());
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitComment(Node* node) {
+  OperandGenerator g(this);
+  InstructionOperand operand(g.UseImmediate(node));
+  Emit(kArchComment, 0, nullptr, 1, &operand);
+}
 
-  // Pass label of exception handler block.
-  if (handler) {
-    DCHECK_EQ(IrOpcode::kIfException, handler->front()->opcode());
-    flags |= CallDescriptor::kHasExceptionHandler;
-    buffer.instruction_args.push_back(g.Label(handler));
-  }
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitRetain(Node* node) {
+  OperandGenerator g(this);
+  Emit(kArchNop, g.NoOutput(), g.UseAny(node->InputAt(0)));
+}
 
-  // Select the appropriate opcode based on the call type.
-  InstructionCode opcode;
-  switch (call_descriptor->kind()) {
-    case CallDescriptor::kCallAddress: {
-      int gp_param_count =
-          static_cast<int>(call_descriptor->GPParameterCount());
-      int fp_param_count =
-          static_cast<int>(call_descriptor->FPParameterCount());
-#if ABI_USES_FUNCTION_DESCRIPTORS
-      // Highest fp_param_count bit is used on AIX to indicate if a CFunction
-      // call has function descriptor or not.
-      static_assert(FPParamField::kSize == kHasFunctionDescriptorBitShift + 1);
-      if (!call_descriptor->NoFunctionDescriptor()) {
-        fp_param_count |= 1 << kHasFunctionDescriptorBitShift;
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitControl(
+    turboshaft::Block* block) {
+#ifdef DEBUG
+  // SSA deconstruction requires targets of branches not to have phis.
+  // Edge split form guarantees this property, but is more strict.
+  if (auto successors = turboshaft::SuccessorBlocks(
+          block->LastOperation(*turboshaft_graph()));
+      successors.size() > 1) {
+    for (turboshaft::Block* successor : successors) {
+      if (successor->HasPhis(*turboshaft_graph())) {
+        std::ostringstream str;
+        str << "You might have specified merged variables for a label with "
+            << "only one predecessor." << std::endl
+            << "# Current Block: " << successor->index() << std::endl;
+        FATAL("%s", str.str().c_str());
       }
-#endif
-      opcode = kArchCallCFunction | ParamField::encode(gp_param_count) |
-               FPParamField::encode(fp_param_count);
-      break;
     }
-    case CallDescriptor::kCallCodeObject:
-      opcode = EncodeCallDescriptorFlags(kArchCallCodeObject, flags);
+  }
+#endif  // DEBUG
+  const turboshaft::Operation& op = block->LastOperation(*schedule());
+  turboshaft::OpIndex index = schedule()->Index(op);
+  int instruction_end = static_cast<int>(instructions_.size());
+  using Opcode = turboshaft::Opcode;
+  switch (op.opcode) {
+    case Opcode::kGoto:
+      VisitGoto(op.Cast<turboshaft::GotoOp>().destination);
       break;
-    case CallDescriptor::kCallJSFunction:
-      opcode = EncodeCallDescriptorFlags(kArchCallJSFunction, flags);
+    case Opcode::kReturn:
+      VisitReturn(index);
       break;
-#if V8_ENABLE_WEBASSEMBLY
-    case CallDescriptor::kCallWasmCapiFunction:
-    case CallDescriptor::kCallWasmFunction:
-    case CallDescriptor::kCallWasmImportWrapper:
-      opcode = EncodeCallDescriptorFlags(kArchCallWasmFunction, flags);
+    case Opcode::kDeoptimize: {
+      const turboshaft::DeoptimizeOp& deoptimize =
+          op.Cast<turboshaft::DeoptimizeOp>();
+      VisitDeoptimize(deoptimize.parameters->reason(), index.id(),
+                      deoptimize.parameters->feedback(),
+                      deoptimize.frame_state());
       break;
-#endif  // V8_ENABLE_WEBASSEMBLY
-    case CallDescriptor::kCallBuiltinPointer:
-      opcode = EncodeCallDescriptorFlags(kArchCallBuiltinPointer, flags);
+    }
+    case Opcode::kBranch: {
+      const turboshaft::BranchOp& branch = op.Cast<turboshaft::BranchOp>();
+      block_t tbranch = branch.if_true;
+      block_t fbranch = branch.if_false;
+      if (tbranch == fbranch) {
+        VisitGoto(tbranch);
+      } else {
+        VisitBranch(index, tbranch, fbranch);
+      }
       break;
-  }
-
-  // Emit the call instruction.
-  size_t const output_count = buffer.outputs.size();
-  auto* outputs = output_count ? &buffer.outputs.front() : nullptr;
-  Instruction* call_instr =
-      Emit(opcode, output_count, outputs, buffer.instruction_args.size(),
-           &buffer.instruction_args.front());
-  if (instruction_selection_failed()) return;
-  call_instr->MarkAsCall();
+    }
 
-  EmitPrepareResults(&(buffer.output_nodes), call_descriptor, node);
+    default: {
+      const std::string op_string = op.ToString();
+      PrintF("\033[31mNo ISEL support for: %s\033[m\n", op_string.c_str());
+      FATAL("Unexpected operation #%d:%s", index.id(), op_string.c_str());
+    }
+  }
 
-  if (call_descriptor->NeedsCallerSavedRegisters()) {
-    Emit(
-        kArchRestoreCallerRegisters | MiscField::encode(static_cast<int>(mode)),
-        g.NoOutput());
+  if (trace_turbo_ == InstructionSelector::kEnableTraceTurboJson) {
+    DCHECK(index.valid());
+    int instruction_start = static_cast<int>(instructions_.size());
+    instr_origins_[this->id(index)] = {instruction_start, instruction_end};
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
-  OperandGenerator g(this);
-
-  auto caller = linkage()->GetIncomingDescriptor();
-  auto callee = CallDescriptorOf(node->op());
-  DCHECK(caller->CanTailCall(callee));
-  const int stack_param_delta = callee->GetStackParameterDelta(caller);
-  CallBuffer buffer(zone(), callee, nullptr);
-
-  // Compute InstructionOperands for inputs and outputs.
-  CallBufferFlags flags(kCallCodeImmediate | kCallTail);
-  if (IsTailCallAddressImmediate()) {
-    flags |= kCallAddressImmediate;
-  }
-  if (callee->flags() & CallDescriptor::kFixedTargetRegister) {
-    flags |= kCallFixedTargetRegister;
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitControl(BasicBlock* block) {
+#ifdef DEBUG
+  // SSA deconstruction requires targets of branches not to have phis.
+  // Edge split form guarantees this property, but is more strict.
+  if (block->SuccessorCount() > 1) {
+    for (BasicBlock* const successor : block->successors()) {
+      for (Node* const node : *successor) {
+        if (IrOpcode::IsPhiOpcode(node->opcode())) {
+          std::ostringstream str;
+          str << "You might have specified merged variables for a label with "
+              << "only one predecessor." << std::endl
+              << "# Current Block: " << *successor << std::endl
+              << "#          Node: " << *node;
+          FATAL("%s", str.str().c_str());
+        }
+      }
+    }
   }
-  InitializeCallBuffer(node, &buffer, flags, stack_param_delta);
-  UpdateMaxPushedArgumentCount(stack_param_delta);
+#endif
 
-  // Select the appropriate opcode based on the call type.
-  InstructionCode opcode;
-  InstructionOperandVector temps(zone());
-  switch (callee->kind()) {
-    case CallDescriptor::kCallCodeObject:
-      opcode = kArchTailCallCodeObject;
+  Node* input = block->control_input();
+  int instruction_end = static_cast<int>(instructions_.size());
+  switch (block->control()) {
+    case BasicBlock::kGoto:
+      VisitGoto(block->SuccessorAt(0));
       break;
-    case CallDescriptor::kCallAddress:
-      DCHECK(!caller->IsJSFunctionCall());
-      opcode = kArchTailCallAddress;
+    case BasicBlock::kCall: {
+      DCHECK_EQ(IrOpcode::kCall, input->opcode());
+      BasicBlock* success = block->SuccessorAt(0);
+      BasicBlock* exception = block->SuccessorAt(1);
+      VisitCall(input, exception);
+      VisitGoto(success);
       break;
-#if V8_ENABLE_WEBASSEMBLY
-    case CallDescriptor::kCallWasmFunction:
-      DCHECK(!caller->IsJSFunctionCall());
-      opcode = kArchTailCallWasm;
+    }
+    case BasicBlock::kTailCall: {
+      DCHECK_EQ(IrOpcode::kTailCall, input->opcode());
+      VisitTailCall(input);
+      break;
+    }
+    case BasicBlock::kBranch: {
+      DCHECK_EQ(IrOpcode::kBranch, input->opcode());
+      // TODO(nicohartmann@): Once all branches have explicitly specified
+      // semantics, we should allow only BranchSemantics::kMachine here.
+      DCHECK_NE(BranchSemantics::kJS,
+                BranchParametersOf(input->op()).semantics());
+      BasicBlock* tbranch = block->SuccessorAt(0);
+      BasicBlock* fbranch = block->SuccessorAt(1);
+      if (tbranch == fbranch) {
+        VisitGoto(tbranch);
+      } else {
+        VisitBranch(input, tbranch, fbranch);
+      }
+      break;
+    }
+    case BasicBlock::kSwitch: {
+      DCHECK_EQ(IrOpcode::kSwitch, input->opcode());
+      // Last successor must be {IfDefault}.
+      BasicBlock* default_branch = block->successors().back();
+      DCHECK_EQ(IrOpcode::kIfDefault, default_branch->front()->opcode());
+      // All other successors must be {IfValue}s.
+      int32_t min_value = std::numeric_limits<int32_t>::max();
+      int32_t max_value = std::numeric_limits<int32_t>::min();
+      size_t case_count = block->SuccessorCount() - 1;
+      ZoneVector<CaseInfo> cases(case_count, zone());
+      for (size_t i = 0; i < case_count; ++i) {
+        BasicBlock* branch = block->SuccessorAt(i);
+        const IfValueParameters& p = IfValueParametersOf(branch->front()->op());
+        cases[i] = CaseInfo{p.value(), p.comparison_order(), branch};
+        if (min_value > p.value()) min_value = p.value();
+        if (max_value < p.value()) max_value = p.value();
+      }
+      SwitchInfo sw(cases, min_value, max_value, default_branch);
+      VisitSwitch(input, sw);
+      break;
+    }
+    case BasicBlock::kReturn: {
+      DCHECK_EQ(IrOpcode::kReturn, input->opcode());
+      VisitReturn(input);
+      break;
+    }
+    case BasicBlock::kDeoptimize: {
+      DeoptimizeParameters p = DeoptimizeParametersOf(input->op());
+      FrameState value{input->InputAt(0)};
+      VisitDeoptimize(p.reason(), input->id(), p.feedback(), value);
+      break;
+    }
+    case BasicBlock::kThrow:
+      DCHECK_EQ(IrOpcode::kThrow, input->opcode());
+      VisitThrow(input);
+      break;
+    case BasicBlock::kNone: {
+      // Exit block doesn't have control.
+      DCHECK_NULL(input);
       break;
-#endif  // V8_ENABLE_WEBASSEMBLY
+    }
     default:
       UNREACHABLE();
   }
-  opcode = EncodeCallDescriptorFlags(opcode, callee->flags());
-
-  Emit(kArchPrepareTailCall, g.NoOutput());
-
-  // Add an immediate operand that represents the offset to the first slot that
-  // is unused with respect to the stack pointer that has been updated for the
-  // tail call instruction. Backends that pad arguments can write the padding
-  // value at this offset from the stack.
-  const int optional_padding_offset =
-      callee->GetOffsetToFirstUnusedStackSlot() - 1;
-  buffer.instruction_args.push_back(g.TempImmediate(optional_padding_offset));
-
-  const int first_unused_slot_offset =
-      kReturnAddressStackSlotCount + stack_param_delta;
-  buffer.instruction_args.push_back(g.TempImmediate(first_unused_slot_offset));
-
-  // Emit the tailcall instruction.
-  Emit(opcode, 0, nullptr, buffer.instruction_args.size(),
-       &buffer.instruction_args.front(), temps.size(),
-       temps.empty() ? nullptr : &temps.front());
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitGoto(BasicBlock* target) {
-  // jump to the next block.
-  OperandGenerator g(this);
-  Emit(kArchJmp, g.NoOutput(), g.Label(target));
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitReturn(Node* ret) {
-  OperandGenerator g(this);
-  const int input_count = linkage()->GetIncomingDescriptor()->ReturnCount() == 0
-                              ? 1
-                              : ret->op()->ValueInputCount();
-  DCHECK_GE(input_count, 1);
-  auto value_locations =
-      zone()->template NewArray<InstructionOperand>(input_count);
-  Node* pop_count = ret->InputAt(0);
-  value_locations[0] = (pop_count->opcode() == IrOpcode::kInt32Constant ||
-                        pop_count->opcode() == IrOpcode::kInt64Constant)
-                           ? g.UseImmediate(pop_count)
-                           : g.UseRegister(pop_count);
-  for (int i = 1; i < input_count; ++i) {
-    value_locations[i] =
-        g.UseLocation(ret->InputAt(i), linkage()->GetReturnLocation(i - 1));
+  if (trace_turbo_ == InstructionSelector::kEnableTraceTurboJson && input) {
+    int instruction_start = static_cast<int>(instructions_.size());
+    instr_origins_[input->id()] = {instruction_start, instruction_end};
   }
-  Emit(kArchRet, 0, nullptr, input_count, value_locations);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitBranch(Node* branch,
-                                                BasicBlock* tbranch,
-                                                BasicBlock* fbranch) {
-  TryPrepareScheduleFirstProjection(branch->InputAt(0));
-
-  FlagsContinuation cont =
-      FlagsContinuation::ForBranch(kNotEqual, tbranch, fbranch);
-  VisitWordCompareZero(branch, branch->InputAt(0), &cont);
 }
 
-// When a DeoptimizeIf/DeoptimizeUnless/Branch depends on a BinopOverflow, the
-// InstructionSelector can sometimes generate a fuse instruction covering both
-// the BinopOverflow and the DeoptIf/Branch, and the final emitted code will
-// look like:
-//
-//     r = BinopOverflow
-//     jo branch_target/deopt_target
-//
-// When this fusing fails, the final code looks like:
-//
-//     r = BinopOverflow
-//     o = sete  // sets overflow bit
-//     cmp o, 0
-//     jnz branch_target/deopt_target
-//
-// To be able to fuse tue BinopOverflow and the DeoptIf/Branch, the 1st
-// projection (Projection[0], which contains the actual result) must already be
-// scheduled (and a few other conditions must be satisfied, see
-// InstructionSelectorXXX::VisitWordCompareZero).
-// TryPrepareScheduleFirstProjection is thus called from
-// VisitDeoptimizeIf/VisitDeoptimizeUnless/VisitBranch and detects if the 1st
-// projection could be scheduled now, and, if so, defines it.
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::TryPrepareScheduleFirstProjection(
-    Node* const maybe_projection) {
-  if (maybe_projection->opcode() != IrOpcode::kProjection) {
-    // The DeoptimizeIf/DeoptimizeUnless/Branch condition is not a projection.
-    return;
-  }
-
-  if (ProjectionIndexOf(maybe_projection->op()) != 1u) {
-    // The DeoptimizeIf/DeoptimizeUnless/Branch isn't on the Projection[1] (ie,
-    // not on the overflow bit of a BinopOverflow).
-    return;
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitNode(Node* node) {
+  tick_counter_->TickAndMaybeEnterSafepoint();
+  DCHECK_NOT_NULL(
+      this->block(schedule(), node));  // should only use scheduled nodes.
+  switch (node->opcode()) {
+    case IrOpcode::kTraceInstruction:
+#if V8_TARGET_ARCH_X64
+      return VisitTraceInstruction(node);
+#else
+      return;
+#endif
+    case IrOpcode::kStart:
+    case IrOpcode::kLoop:
+    case IrOpcode::kEnd:
+    case IrOpcode::kBranch:
+    case IrOpcode::kIfTrue:
+    case IrOpcode::kIfFalse:
+    case IrOpcode::kIfSuccess:
+    case IrOpcode::kSwitch:
+    case IrOpcode::kIfValue:
+    case IrOpcode::kIfDefault:
+    case IrOpcode::kEffectPhi:
+    case IrOpcode::kMerge:
+    case IrOpcode::kTerminate:
+    case IrOpcode::kBeginRegion:
+      // No code needed for these graph artifacts.
+      return;
+    case IrOpcode::kIfException:
+      return MarkAsTagged(node), VisitIfException(node);
+    case IrOpcode::kFinishRegion:
+      return MarkAsTagged(node), VisitFinishRegion(node);
+    case IrOpcode::kParameter: {
+      // Parameters should always be scheduled to the first block.
+      DCHECK_EQ(this->rpo_number(this->block(schedule(), node)).ToInt(), 0);
+      MachineType type =
+          linkage()->GetParameterType(ParameterIndexOf(node->op()));
+      MarkAsRepresentation(type.representation(), node);
+      return VisitParameter(node);
+    }
+    case IrOpcode::kOsrValue:
+      return MarkAsTagged(node), VisitOsrValue(node);
+    case IrOpcode::kPhi: {
+      MachineRepresentation rep = PhiRepresentationOf(node->op());
+      if (rep == MachineRepresentation::kNone) return;
+      MarkAsRepresentation(rep, node);
+      return VisitPhi(node);
+    }
+    case IrOpcode::kProjection:
+      return VisitProjection(node);
+    case IrOpcode::kInt32Constant:
+    case IrOpcode::kInt64Constant:
+    case IrOpcode::kTaggedIndexConstant:
+    case IrOpcode::kExternalConstant:
+    case IrOpcode::kRelocatableInt32Constant:
+    case IrOpcode::kRelocatableInt64Constant:
+      return VisitConstant(node);
+    case IrOpcode::kFloat32Constant:
+      return MarkAsFloat32(node), VisitConstant(node);
+    case IrOpcode::kFloat64Constant:
+      return MarkAsFloat64(node), VisitConstant(node);
+    case IrOpcode::kHeapConstant:
+      return MarkAsTagged(node), VisitConstant(node);
+    case IrOpcode::kCompressedHeapConstant:
+      return MarkAsCompressed(node), VisitConstant(node);
+    case IrOpcode::kNumberConstant: {
+      double value = OpParameter<double>(node->op());
+      if (!IsSmiDouble(value)) MarkAsTagged(node);
+      return VisitConstant(node);
+    }
+    case IrOpcode::kCall:
+      return VisitCall(node);
+    case IrOpcode::kDeoptimizeIf:
+      return VisitDeoptimizeIf(node);
+    case IrOpcode::kDeoptimizeUnless:
+      return VisitDeoptimizeUnless(node);
+    case IrOpcode::kTrapIf:
+      return VisitTrapIf(node, TrapIdOf(node->op()));
+    case IrOpcode::kTrapUnless:
+      return VisitTrapUnless(node, TrapIdOf(node->op()));
+    case IrOpcode::kFrameState:
+    case IrOpcode::kStateValues:
+    case IrOpcode::kObjectState:
+      return;
+    case IrOpcode::kAbortCSADcheck:
+      VisitAbortCSADcheck(node);
+      return;
+    case IrOpcode::kDebugBreak:
+      VisitDebugBreak(node);
+      return;
+    case IrOpcode::kUnreachable:
+      VisitUnreachable(node);
+      return;
+    case IrOpcode::kStaticAssert:
+      VisitStaticAssert(node);
+      return;
+    case IrOpcode::kDeadValue:
+      VisitDeadValue(node);
+      return;
+    case IrOpcode::kComment:
+      VisitComment(node);
+      return;
+    case IrOpcode::kRetain:
+      VisitRetain(node);
+      return;
+    case IrOpcode::kLoad:
+    case IrOpcode::kLoadImmutable: {
+      LoadRepresentation type = LoadRepresentationOf(node->op());
+      MarkAsRepresentation(type.representation(), node);
+      return VisitLoad(node);
+    }
+    case IrOpcode::kLoadTransform: {
+      LoadTransformParameters params = LoadTransformParametersOf(node->op());
+      if (params.transformation == LoadTransformation::kS256Load32Splat ||
+          params.transformation == LoadTransformation::kS256Load64Splat) {
+        MarkAsRepresentation(MachineRepresentation::kSimd256, node);
+      } else {
+        MarkAsRepresentation(MachineRepresentation::kSimd128, node);
+      }
+      return VisitLoadTransform(node);
+    }
+    case IrOpcode::kLoadLane: {
+      MarkAsRepresentation(MachineRepresentation::kSimd128, node);
+      return VisitLoadLane(node);
+    }
+    case IrOpcode::kStore:
+      return VisitStore(node);
+    case IrOpcode::kStorePair:
+      return VisitStorePair(node);
+    case IrOpcode::kProtectedStore:
+    case IrOpcode::kStoreTrapOnNull:
+      return VisitProtectedStore(node);
+    case IrOpcode::kStoreLane: {
+      MarkAsRepresentation(MachineRepresentation::kSimd128, node);
+      return VisitStoreLane(node);
+    }
+    case IrOpcode::kWord32And:
+      return MarkAsWord32(node), VisitWord32And(node);
+    case IrOpcode::kWord32Or:
+      return MarkAsWord32(node), VisitWord32Or(node);
+    case IrOpcode::kWord32Xor:
+      return MarkAsWord32(node), VisitWord32Xor(node);
+    case IrOpcode::kWord32Shl:
+      return MarkAsWord32(node), VisitWord32Shl(node);
+    case IrOpcode::kWord32Shr:
+      return MarkAsWord32(node), VisitWord32Shr(node);
+    case IrOpcode::kWord32Sar:
+      return MarkAsWord32(node), VisitWord32Sar(node);
+    case IrOpcode::kWord32Rol:
+      return MarkAsWord32(node), VisitWord32Rol(node);
+    case IrOpcode::kWord32Ror:
+      return MarkAsWord32(node), VisitWord32Ror(node);
+    case IrOpcode::kWord32Equal:
+      return VisitWord32Equal(node);
+    case IrOpcode::kWord32Clz:
+      return MarkAsWord32(node), VisitWord32Clz(node);
+    case IrOpcode::kWord32Ctz:
+      return MarkAsWord32(node), VisitWord32Ctz(node);
+    case IrOpcode::kWord32ReverseBits:
+      return MarkAsWord32(node), VisitWord32ReverseBits(node);
+    case IrOpcode::kWord32ReverseBytes:
+      return MarkAsWord32(node), VisitWord32ReverseBytes(node);
+    case IrOpcode::kInt32AbsWithOverflow:
+      return MarkAsWord32(node), VisitInt32AbsWithOverflow(node);
+    case IrOpcode::kWord32Popcnt:
+      return MarkAsWord32(node), VisitWord32Popcnt(node);
+    case IrOpcode::kWord64Popcnt:
+      return MarkAsWord32(node), VisitWord64Popcnt(node);
+    case IrOpcode::kWord32Select:
+      return MarkAsWord32(node), VisitSelect(node);
+    case IrOpcode::kWord64And:
+      return MarkAsWord64(node), VisitWord64And(node);
+    case IrOpcode::kWord64Or:
+      return MarkAsWord64(node), VisitWord64Or(node);
+    case IrOpcode::kWord64Xor:
+      return MarkAsWord64(node), VisitWord64Xor(node);
+    case IrOpcode::kWord64Shl:
+      return MarkAsWord64(node), VisitWord64Shl(node);
+    case IrOpcode::kWord64Shr:
+      return MarkAsWord64(node), VisitWord64Shr(node);
+    case IrOpcode::kWord64Sar:
+      return MarkAsWord64(node), VisitWord64Sar(node);
+    case IrOpcode::kWord64Rol:
+      return MarkAsWord64(node), VisitWord64Rol(node);
+    case IrOpcode::kWord64Ror:
+      return MarkAsWord64(node), VisitWord64Ror(node);
+    case IrOpcode::kWord64Clz:
+      return MarkAsWord64(node), VisitWord64Clz(node);
+    case IrOpcode::kWord64Ctz:
+      return MarkAsWord64(node), VisitWord64Ctz(node);
+    case IrOpcode::kWord64ReverseBits:
+      return MarkAsWord64(node), VisitWord64ReverseBits(node);
+    case IrOpcode::kWord64ReverseBytes:
+      return MarkAsWord64(node), VisitWord64ReverseBytes(node);
+    case IrOpcode::kSimd128ReverseBytes:
+      return MarkAsSimd128(node), VisitSimd128ReverseBytes(node);
+    case IrOpcode::kInt64AbsWithOverflow:
+      return MarkAsWord64(node), VisitInt64AbsWithOverflow(node);
+    case IrOpcode::kWord64Equal:
+      return VisitWord64Equal(node);
+    case IrOpcode::kWord64Select:
+      return MarkAsWord64(node), VisitSelect(node);
+    case IrOpcode::kInt32Add:
+      return MarkAsWord32(node), VisitInt32Add(node);
+    case IrOpcode::kInt32AddWithOverflow:
+      return MarkAsWord32(node), VisitInt32AddWithOverflow(node);
+    case IrOpcode::kInt32Sub:
+      return MarkAsWord32(node), VisitInt32Sub(node);
+    case IrOpcode::kInt32SubWithOverflow:
+      return VisitInt32SubWithOverflow(node);
+    case IrOpcode::kInt32Mul:
+      return MarkAsWord32(node), VisitInt32Mul(node);
+    case IrOpcode::kInt32MulWithOverflow:
+      return MarkAsWord32(node), VisitInt32MulWithOverflow(node);
+    case IrOpcode::kInt32MulHigh:
+      return VisitInt32MulHigh(node);
+    case IrOpcode::kInt64MulHigh:
+      return VisitInt64MulHigh(node);
+    case IrOpcode::kInt32Div:
+      return MarkAsWord32(node), VisitInt32Div(node);
+    case IrOpcode::kInt32Mod:
+      return MarkAsWord32(node), VisitInt32Mod(node);
+    case IrOpcode::kInt32LessThan:
+      return VisitInt32LessThan(node);
+    case IrOpcode::kInt32LessThanOrEqual:
+      return VisitInt32LessThanOrEqual(node);
+    case IrOpcode::kUint32Div:
+      return MarkAsWord32(node), VisitUint32Div(node);
+    case IrOpcode::kUint32LessThan:
+      return VisitUint32LessThan(node);
+    case IrOpcode::kUint32LessThanOrEqual:
+      return VisitUint32LessThanOrEqual(node);
+    case IrOpcode::kUint32Mod:
+      return MarkAsWord32(node), VisitUint32Mod(node);
+    case IrOpcode::kUint32MulHigh:
+      return VisitUint32MulHigh(node);
+    case IrOpcode::kUint64MulHigh:
+      return VisitUint64MulHigh(node);
+    case IrOpcode::kInt64Add:
+      return MarkAsWord64(node), VisitInt64Add(node);
+    case IrOpcode::kInt64AddWithOverflow:
+      return MarkAsWord64(node), VisitInt64AddWithOverflow(node);
+    case IrOpcode::kInt64Sub:
+      return MarkAsWord64(node), VisitInt64Sub(node);
+    case IrOpcode::kInt64SubWithOverflow:
+      return MarkAsWord64(node), VisitInt64SubWithOverflow(node);
+    case IrOpcode::kInt64Mul:
+      return MarkAsWord64(node), VisitInt64Mul(node);
+    case IrOpcode::kInt64MulWithOverflow:
+      return MarkAsWord64(node), VisitInt64MulWithOverflow(node);
+    case IrOpcode::kInt64Div:
+      return MarkAsWord64(node), VisitInt64Div(node);
+    case IrOpcode::kInt64Mod:
+      return MarkAsWord64(node), VisitInt64Mod(node);
+    case IrOpcode::kInt64LessThan:
+      return VisitInt64LessThan(node);
+    case IrOpcode::kInt64LessThanOrEqual:
+      return VisitInt64LessThanOrEqual(node);
+    case IrOpcode::kUint64Div:
+      return MarkAsWord64(node), VisitUint64Div(node);
+    case IrOpcode::kUint64LessThan:
+      return VisitUint64LessThan(node);
+    case IrOpcode::kUint64LessThanOrEqual:
+      return VisitUint64LessThanOrEqual(node);
+    case IrOpcode::kUint64Mod:
+      return MarkAsWord64(node), VisitUint64Mod(node);
+    case IrOpcode::kBitcastTaggedToWord:
+    case IrOpcode::kBitcastTaggedToWordForTagAndSmiBits:
+      return MarkAsRepresentation(MachineType::PointerRepresentation(), node),
+             VisitBitcastTaggedToWord(node);
+    case IrOpcode::kBitcastWordToTagged:
+      return MarkAsTagged(node), VisitBitcastWordToTagged(node);
+    case IrOpcode::kBitcastWordToTaggedSigned:
+      return MarkAsRepresentation(MachineRepresentation::kTaggedSigned, node),
+             EmitIdentity(node);
+    case IrOpcode::kChangeFloat32ToFloat64:
+      return MarkAsFloat64(node), VisitChangeFloat32ToFloat64(node);
+    case IrOpcode::kChangeInt32ToFloat64:
+      return MarkAsFloat64(node), VisitChangeInt32ToFloat64(node);
+    case IrOpcode::kChangeInt64ToFloat64:
+      return MarkAsFloat64(node), VisitChangeInt64ToFloat64(node);
+    case IrOpcode::kChangeUint32ToFloat64:
+      return MarkAsFloat64(node), VisitChangeUint32ToFloat64(node);
+    case IrOpcode::kChangeFloat64ToInt32:
+      return MarkAsWord32(node), VisitChangeFloat64ToInt32(node);
+    case IrOpcode::kChangeFloat64ToInt64:
+      return MarkAsWord64(node), VisitChangeFloat64ToInt64(node);
+    case IrOpcode::kChangeFloat64ToUint32:
+      return MarkAsWord32(node), VisitChangeFloat64ToUint32(node);
+    case IrOpcode::kChangeFloat64ToUint64:
+      return MarkAsWord64(node), VisitChangeFloat64ToUint64(node);
+    case IrOpcode::kFloat64SilenceNaN:
+      MarkAsFloat64(node);
+      if (CanProduceSignalingNaN(node->InputAt(0))) {
+        return VisitFloat64SilenceNaN(node);
+      } else {
+        return EmitIdentity(node);
+      }
+    case IrOpcode::kTruncateFloat64ToInt64:
+      return MarkAsWord64(node), VisitTruncateFloat64ToInt64(node);
+    case IrOpcode::kTruncateFloat64ToUint32:
+      return MarkAsWord32(node), VisitTruncateFloat64ToUint32(node);
+    case IrOpcode::kTruncateFloat32ToInt32:
+      return MarkAsWord32(node), VisitTruncateFloat32ToInt32(node);
+    case IrOpcode::kTruncateFloat32ToUint32:
+      return MarkAsWord32(node), VisitTruncateFloat32ToUint32(node);
+    case IrOpcode::kTryTruncateFloat32ToInt64:
+      return MarkAsWord64(node), VisitTryTruncateFloat32ToInt64(node);
+    case IrOpcode::kTryTruncateFloat64ToInt64:
+      return MarkAsWord64(node), VisitTryTruncateFloat64ToInt64(node);
+    case IrOpcode::kTryTruncateFloat32ToUint64:
+      return MarkAsWord64(node), VisitTryTruncateFloat32ToUint64(node);
+    case IrOpcode::kTryTruncateFloat64ToUint64:
+      return MarkAsWord64(node), VisitTryTruncateFloat64ToUint64(node);
+    case IrOpcode::kTryTruncateFloat64ToInt32:
+      return MarkAsWord32(node), VisitTryTruncateFloat64ToInt32(node);
+    case IrOpcode::kTryTruncateFloat64ToUint32:
+      return MarkAsWord32(node), VisitTryTruncateFloat64ToUint32(node);
+    case IrOpcode::kBitcastWord32ToWord64:
+      return MarkAsWord64(node), VisitBitcastWord32ToWord64(node);
+    case IrOpcode::kChangeInt32ToInt64:
+      return MarkAsWord64(node), VisitChangeInt32ToInt64(node);
+    case IrOpcode::kChangeUint32ToUint64:
+      return MarkAsWord64(node), VisitChangeUint32ToUint64(node);
+    case IrOpcode::kTruncateFloat64ToFloat32:
+      return MarkAsFloat32(node), VisitTruncateFloat64ToFloat32(node);
+    case IrOpcode::kTruncateFloat64ToWord32:
+      return MarkAsWord32(node), VisitTruncateFloat64ToWord32(node);
+    case IrOpcode::kTruncateInt64ToInt32:
+      return MarkAsWord32(node), VisitTruncateInt64ToInt32(node);
+    case IrOpcode::kRoundFloat64ToInt32:
+      return MarkAsWord32(node), VisitRoundFloat64ToInt32(node);
+    case IrOpcode::kRoundInt64ToFloat32:
+      return MarkAsFloat32(node), VisitRoundInt64ToFloat32(node);
+    case IrOpcode::kRoundInt32ToFloat32:
+      return MarkAsFloat32(node), VisitRoundInt32ToFloat32(node);
+    case IrOpcode::kRoundInt64ToFloat64:
+      return MarkAsFloat64(node), VisitRoundInt64ToFloat64(node);
+    case IrOpcode::kBitcastFloat32ToInt32:
+      return MarkAsWord32(node), VisitBitcastFloat32ToInt32(node);
+    case IrOpcode::kRoundUint32ToFloat32:
+      return MarkAsFloat32(node), VisitRoundUint32ToFloat32(node);
+    case IrOpcode::kRoundUint64ToFloat32:
+      return MarkAsFloat64(node), VisitRoundUint64ToFloat32(node);
+    case IrOpcode::kRoundUint64ToFloat64:
+      return MarkAsFloat64(node), VisitRoundUint64ToFloat64(node);
+    case IrOpcode::kBitcastFloat64ToInt64:
+      return MarkAsWord64(node), VisitBitcastFloat64ToInt64(node);
+    case IrOpcode::kBitcastInt32ToFloat32:
+      return MarkAsFloat32(node), VisitBitcastInt32ToFloat32(node);
+    case IrOpcode::kBitcastInt64ToFloat64:
+      return MarkAsFloat64(node), VisitBitcastInt64ToFloat64(node);
+    case IrOpcode::kFloat32Add:
+      return MarkAsFloat32(node), VisitFloat32Add(node);
+    case IrOpcode::kFloat32Sub:
+      return MarkAsFloat32(node), VisitFloat32Sub(node);
+    case IrOpcode::kFloat32Neg:
+      return MarkAsFloat32(node), VisitFloat32Neg(node);
+    case IrOpcode::kFloat32Mul:
+      return MarkAsFloat32(node), VisitFloat32Mul(node);
+    case IrOpcode::kFloat32Div:
+      return MarkAsFloat32(node), VisitFloat32Div(node);
+    case IrOpcode::kFloat32Abs:
+      return MarkAsFloat32(node), VisitFloat32Abs(node);
+    case IrOpcode::kFloat32Sqrt:
+      return MarkAsFloat32(node), VisitFloat32Sqrt(node);
+    case IrOpcode::kFloat32Equal:
+      return VisitFloat32Equal(node);
+    case IrOpcode::kFloat32LessThan:
+      return VisitFloat32LessThan(node);
+    case IrOpcode::kFloat32LessThanOrEqual:
+      return VisitFloat32LessThanOrEqual(node);
+    case IrOpcode::kFloat32Max:
+      return MarkAsFloat32(node), VisitFloat32Max(node);
+    case IrOpcode::kFloat32Min:
+      return MarkAsFloat32(node), VisitFloat32Min(node);
+    case IrOpcode::kFloat32Select:
+      return MarkAsFloat32(node), VisitSelect(node);
+    case IrOpcode::kFloat64Add:
+      return MarkAsFloat64(node), VisitFloat64Add(node);
+    case IrOpcode::kFloat64Sub:
+      return MarkAsFloat64(node), VisitFloat64Sub(node);
+    case IrOpcode::kFloat64Neg:
+      return MarkAsFloat64(node), VisitFloat64Neg(node);
+    case IrOpcode::kFloat64Mul:
+      return MarkAsFloat64(node), VisitFloat64Mul(node);
+    case IrOpcode::kFloat64Div:
+      return MarkAsFloat64(node), VisitFloat64Div(node);
+    case IrOpcode::kFloat64Mod:
+      return MarkAsFloat64(node), VisitFloat64Mod(node);
+    case IrOpcode::kFloat64Min:
+      return MarkAsFloat64(node), VisitFloat64Min(node);
+    case IrOpcode::kFloat64Max:
+      return MarkAsFloat64(node), VisitFloat64Max(node);
+    case IrOpcode::kFloat64Abs:
+      return MarkAsFloat64(node), VisitFloat64Abs(node);
+    case IrOpcode::kFloat64Acos:
+      return MarkAsFloat64(node), VisitFloat64Acos(node);
+    case IrOpcode::kFloat64Acosh:
+      return MarkAsFloat64(node), VisitFloat64Acosh(node);
+    case IrOpcode::kFloat64Asin:
+      return MarkAsFloat64(node), VisitFloat64Asin(node);
+    case IrOpcode::kFloat64Asinh:
+      return MarkAsFloat64(node), VisitFloat64Asinh(node);
+    case IrOpcode::kFloat64Atan:
+      return MarkAsFloat64(node), VisitFloat64Atan(node);
+    case IrOpcode::kFloat64Atanh:
+      return MarkAsFloat64(node), VisitFloat64Atanh(node);
+    case IrOpcode::kFloat64Atan2:
+      return MarkAsFloat64(node), VisitFloat64Atan2(node);
+    case IrOpcode::kFloat64Cbrt:
+      return MarkAsFloat64(node), VisitFloat64Cbrt(node);
+    case IrOpcode::kFloat64Cos:
+      return MarkAsFloat64(node), VisitFloat64Cos(node);
+    case IrOpcode::kFloat64Cosh:
+      return MarkAsFloat64(node), VisitFloat64Cosh(node);
+    case IrOpcode::kFloat64Exp:
+      return MarkAsFloat64(node), VisitFloat64Exp(node);
+    case IrOpcode::kFloat64Expm1:
+      return MarkAsFloat64(node), VisitFloat64Expm1(node);
+    case IrOpcode::kFloat64Log:
+      return MarkAsFloat64(node), VisitFloat64Log(node);
+    case IrOpcode::kFloat64Log1p:
+      return MarkAsFloat64(node), VisitFloat64Log1p(node);
+    case IrOpcode::kFloat64Log10:
+      return MarkAsFloat64(node), VisitFloat64Log10(node);
+    case IrOpcode::kFloat64Log2:
+      return MarkAsFloat64(node), VisitFloat64Log2(node);
+    case IrOpcode::kFloat64Pow:
+      return MarkAsFloat64(node), VisitFloat64Pow(node);
+    case IrOpcode::kFloat64Sin:
+      return MarkAsFloat64(node), VisitFloat64Sin(node);
+    case IrOpcode::kFloat64Sinh:
+      return MarkAsFloat64(node), VisitFloat64Sinh(node);
+    case IrOpcode::kFloat64Sqrt:
+      return MarkAsFloat64(node), VisitFloat64Sqrt(node);
+    case IrOpcode::kFloat64Tan:
+      return MarkAsFloat64(node), VisitFloat64Tan(node);
+    case IrOpcode::kFloat64Tanh:
+      return MarkAsFloat64(node), VisitFloat64Tanh(node);
+    case IrOpcode::kFloat64Equal:
+      return VisitFloat64Equal(node);
+    case IrOpcode::kFloat64LessThan:
+      return VisitFloat64LessThan(node);
+    case IrOpcode::kFloat64LessThanOrEqual:
+      return VisitFloat64LessThanOrEqual(node);
+    case IrOpcode::kFloat64Select:
+      return MarkAsFloat64(node), VisitSelect(node);
+    case IrOpcode::kFloat32RoundDown:
+      return MarkAsFloat32(node), VisitFloat32RoundDown(node);
+    case IrOpcode::kFloat64RoundDown:
+      return MarkAsFloat64(node), VisitFloat64RoundDown(node);
+    case IrOpcode::kFloat32RoundUp:
+      return MarkAsFloat32(node), VisitFloat32RoundUp(node);
+    case IrOpcode::kFloat64RoundUp:
+      return MarkAsFloat64(node), VisitFloat64RoundUp(node);
+    case IrOpcode::kFloat32RoundTruncate:
+      return MarkAsFloat32(node), VisitFloat32RoundTruncate(node);
+    case IrOpcode::kFloat64RoundTruncate:
+      return MarkAsFloat64(node), VisitFloat64RoundTruncate(node);
+    case IrOpcode::kFloat64RoundTiesAway:
+      return MarkAsFloat64(node), VisitFloat64RoundTiesAway(node);
+    case IrOpcode::kFloat32RoundTiesEven:
+      return MarkAsFloat32(node), VisitFloat32RoundTiesEven(node);
+    case IrOpcode::kFloat64RoundTiesEven:
+      return MarkAsFloat64(node), VisitFloat64RoundTiesEven(node);
+    case IrOpcode::kFloat64ExtractLowWord32:
+      return MarkAsWord32(node), VisitFloat64ExtractLowWord32(node);
+    case IrOpcode::kFloat64ExtractHighWord32:
+      return MarkAsWord32(node), VisitFloat64ExtractHighWord32(node);
+    case IrOpcode::kFloat64InsertLowWord32:
+      return MarkAsFloat64(node), VisitFloat64InsertLowWord32(node);
+    case IrOpcode::kFloat64InsertHighWord32:
+      return MarkAsFloat64(node), VisitFloat64InsertHighWord32(node);
+    case IrOpcode::kStackSlot:
+      return VisitStackSlot(node);
+    case IrOpcode::kStackPointerGreaterThan:
+      return VisitStackPointerGreaterThan(node);
+    case IrOpcode::kLoadStackCheckOffset:
+      return VisitLoadStackCheckOffset(node);
+    case IrOpcode::kLoadFramePointer:
+      return VisitLoadFramePointer(node);
+    case IrOpcode::kLoadParentFramePointer:
+      return VisitLoadParentFramePointer(node);
+    case IrOpcode::kLoadRootRegister:
+      return VisitLoadRootRegister(node);
+    case IrOpcode::kUnalignedLoad: {
+      LoadRepresentation type = LoadRepresentationOf(node->op());
+      MarkAsRepresentation(type.representation(), node);
+      return VisitUnalignedLoad(node);
+    }
+    case IrOpcode::kUnalignedStore:
+      return VisitUnalignedStore(node);
+    case IrOpcode::kInt32PairAdd:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitInt32PairAdd(node);
+    case IrOpcode::kInt32PairSub:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitInt32PairSub(node);
+    case IrOpcode::kInt32PairMul:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitInt32PairMul(node);
+    case IrOpcode::kWord32PairShl:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitWord32PairShl(node);
+    case IrOpcode::kWord32PairShr:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitWord32PairShr(node);
+    case IrOpcode::kWord32PairSar:
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitWord32PairSar(node);
+    case IrOpcode::kMemoryBarrier:
+      return VisitMemoryBarrier(node);
+    case IrOpcode::kWord32AtomicLoad: {
+      AtomicLoadParameters params = AtomicLoadParametersOf(node->op());
+      LoadRepresentation type = params.representation();
+      MarkAsRepresentation(type.representation(), node);
+      return VisitWord32AtomicLoad(node);
+    }
+    case IrOpcode::kWord64AtomicLoad: {
+      AtomicLoadParameters params = AtomicLoadParametersOf(node->op());
+      LoadRepresentation type = params.representation();
+      MarkAsRepresentation(type.representation(), node);
+      return VisitWord64AtomicLoad(node);
+    }
+    case IrOpcode::kWord32AtomicStore:
+      return VisitWord32AtomicStore(node);
+    case IrOpcode::kWord64AtomicStore:
+      return VisitWord64AtomicStore(node);
+    case IrOpcode::kWord32AtomicPairStore:
+      return VisitWord32AtomicPairStore(node);
+    case IrOpcode::kWord32AtomicPairLoad: {
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      return VisitWord32AtomicPairLoad(node);
+    }
+#define ATOMIC_CASE(name, rep)                         \
+  case IrOpcode::k##rep##Atomic##name: {               \
+    MachineType type = AtomicOpType(node->op());       \
+    MarkAsRepresentation(type.representation(), node); \
+    return Visit##rep##Atomic##name(node);             \
   }
-
-  Node* const node = maybe_projection->InputAt(0);
-  if (schedule_->block(node) != current_block_) {
-    // The projection input is not in the current block, so it shouldn't be
-    // emitted now, so we don't need to eagerly schedule its Projection[0].
-    return;
+      ATOMIC_CASE(Add, Word32)
+      ATOMIC_CASE(Add, Word64)
+      ATOMIC_CASE(Sub, Word32)
+      ATOMIC_CASE(Sub, Word64)
+      ATOMIC_CASE(And, Word32)
+      ATOMIC_CASE(And, Word64)
+      ATOMIC_CASE(Or, Word32)
+      ATOMIC_CASE(Or, Word64)
+      ATOMIC_CASE(Xor, Word32)
+      ATOMIC_CASE(Xor, Word64)
+      ATOMIC_CASE(Exchange, Word32)
+      ATOMIC_CASE(Exchange, Word64)
+      ATOMIC_CASE(CompareExchange, Word32)
+      ATOMIC_CASE(CompareExchange, Word64)
+#undef ATOMIC_CASE
+#define ATOMIC_CASE(name)                     \
+  case IrOpcode::kWord32AtomicPair##name: {   \
+    MarkAsWord32(node);                       \
+    MarkPairProjectionsAsWord32(node);        \
+    return VisitWord32AtomicPair##name(node); \
   }
-
-  switch (node->opcode()) {
-    case IrOpcode::kInt32AddWithOverflow:
-    case IrOpcode::kInt32SubWithOverflow:
-    case IrOpcode::kInt32MulWithOverflow:
-    case IrOpcode::kInt64AddWithOverflow:
-    case IrOpcode::kInt64SubWithOverflow:
-    case IrOpcode::kInt64MulWithOverflow: {
-      Node* result = NodeProperties::FindProjection(node, 0);
-      if (result == nullptr || IsDefined(result)) {
-        // No Projection(0), or it's already defined.
-        return;
-      }
-
-      if (schedule_->block(result) != current_block_) {
-        // {result} wasn't planned to be scheduled in {current_block_}. To avoid
-        // adding checks to see if it can still be scheduled now, we just bail
-        // out.
-        return;
-      }
-
-      // Checking if all uses of {result} that are in the current block have
-      // already been Defined.
-      // We also ignore Phi uses: if {result} is used in a Phi in the block in
-      // which it is defined, this means that this block is a loop header, and
-      // {result} back into it through the back edge. In this case, it's normal
-      // to schedule {result} before the Phi that uses it.
-      for (Node* use : result->uses()) {
-        if (!IsDefined(use) && schedule_->block(use) == current_block_ &&
-            use->opcode() != IrOpcode::kPhi) {
-          // {use} is in the current block but is not defined yet. It's possible
-          // that it's not actually used, but the IsUsed(x) predicate is not
-          // valid until we have visited `x`, so we overaproximate and assume
-          // that {use} is itself used.
-          return;
-        }
-      }
-
-      // Visiting the projection now. Note that this relies on the fact that
-      // VisitProjection doesn't Emit something: if it did, then we could be
-      // Emitting something after a Branch, which is invalid (Branch can only be
-      // at the end of a block, and the end of a block must always be a block
-      // terminator). (remember that we emit operation in reverse order, so
-      // because we are doing TryPrepareScheduleFirstProjection before actually
-      // emitting the Branch, it would be after in the final instruction
-      // sequence, not before)
-      VisitProjection(result);
-      return;
+      ATOMIC_CASE(Add)
+      ATOMIC_CASE(Sub)
+      ATOMIC_CASE(And)
+      ATOMIC_CASE(Or)
+      ATOMIC_CASE(Xor)
+      ATOMIC_CASE(Exchange)
+      ATOMIC_CASE(CompareExchange)
+#undef ATOMIC_CASE
+    case IrOpcode::kProtectedLoad:
+    case IrOpcode::kLoadTrapOnNull: {
+      LoadRepresentation type = LoadRepresentationOf(node->op());
+      MarkAsRepresentation(type.representation(), node);
+      return VisitProtectedLoad(node);
     }
+    case IrOpcode::kSignExtendWord8ToInt32:
+      return MarkAsWord32(node), VisitSignExtendWord8ToInt32(node);
+    case IrOpcode::kSignExtendWord16ToInt32:
+      return MarkAsWord32(node), VisitSignExtendWord16ToInt32(node);
+    case IrOpcode::kSignExtendWord8ToInt64:
+      return MarkAsWord64(node), VisitSignExtendWord8ToInt64(node);
+    case IrOpcode::kSignExtendWord16ToInt64:
+      return MarkAsWord64(node), VisitSignExtendWord16ToInt64(node);
+    case IrOpcode::kSignExtendWord32ToInt64:
+      return MarkAsWord64(node), VisitSignExtendWord32ToInt64(node);
+    case IrOpcode::kF64x2Splat:
+      return MarkAsSimd128(node), VisitF64x2Splat(node);
+    case IrOpcode::kF64x2ExtractLane:
+      return MarkAsFloat64(node), VisitF64x2ExtractLane(node);
+    case IrOpcode::kF64x2ReplaceLane:
+      return MarkAsSimd128(node), VisitF64x2ReplaceLane(node);
+    case IrOpcode::kF64x2Abs:
+      return MarkAsSimd128(node), VisitF64x2Abs(node);
+    case IrOpcode::kF64x2Neg:
+      return MarkAsSimd128(node), VisitF64x2Neg(node);
+    case IrOpcode::kF64x2Sqrt:
+      return MarkAsSimd128(node), VisitF64x2Sqrt(node);
+    case IrOpcode::kF64x2Add:
+      return MarkAsSimd128(node), VisitF64x2Add(node);
+    case IrOpcode::kF64x2Sub:
+      return MarkAsSimd128(node), VisitF64x2Sub(node);
+    case IrOpcode::kF64x2Mul:
+      return MarkAsSimd128(node), VisitF64x2Mul(node);
+    case IrOpcode::kF64x2Div:
+      return MarkAsSimd128(node), VisitF64x2Div(node);
+    case IrOpcode::kF64x2Min:
+      return MarkAsSimd128(node), VisitF64x2Min(node);
+    case IrOpcode::kF64x2Max:
+      return MarkAsSimd128(node), VisitF64x2Max(node);
+    case IrOpcode::kF64x2Eq:
+      return MarkAsSimd128(node), VisitF64x2Eq(node);
+    case IrOpcode::kF64x2Ne:
+      return MarkAsSimd128(node), VisitF64x2Ne(node);
+    case IrOpcode::kF64x2Lt:
+      return MarkAsSimd128(node), VisitF64x2Lt(node);
+    case IrOpcode::kF64x2Le:
+      return MarkAsSimd128(node), VisitF64x2Le(node);
+    case IrOpcode::kF64x2Qfma:
+      return MarkAsSimd128(node), VisitF64x2Qfma(node);
+    case IrOpcode::kF64x2Qfms:
+      return MarkAsSimd128(node), VisitF64x2Qfms(node);
+    case IrOpcode::kF64x2Pmin:
+      return MarkAsSimd128(node), VisitF64x2Pmin(node);
+    case IrOpcode::kF64x2Pmax:
+      return MarkAsSimd128(node), VisitF64x2Pmax(node);
+    case IrOpcode::kF64x2Ceil:
+      return MarkAsSimd128(node), VisitF64x2Ceil(node);
+    case IrOpcode::kF64x2Floor:
+      return MarkAsSimd128(node), VisitF64x2Floor(node);
+    case IrOpcode::kF64x2Trunc:
+      return MarkAsSimd128(node), VisitF64x2Trunc(node);
+    case IrOpcode::kF64x2NearestInt:
+      return MarkAsSimd128(node), VisitF64x2NearestInt(node);
+    case IrOpcode::kF64x2ConvertLowI32x4S:
+      return MarkAsSimd128(node), VisitF64x2ConvertLowI32x4S(node);
+    case IrOpcode::kF64x2ConvertLowI32x4U:
+      return MarkAsSimd128(node), VisitF64x2ConvertLowI32x4U(node);
+    case IrOpcode::kF64x2PromoteLowF32x4:
+      return MarkAsSimd128(node), VisitF64x2PromoteLowF32x4(node);
+    case IrOpcode::kF32x4Splat:
+      return MarkAsSimd128(node), VisitF32x4Splat(node);
+    case IrOpcode::kF32x4ExtractLane:
+      return MarkAsFloat32(node), VisitF32x4ExtractLane(node);
+    case IrOpcode::kF32x4ReplaceLane:
+      return MarkAsSimd128(node), VisitF32x4ReplaceLane(node);
+    case IrOpcode::kF32x4SConvertI32x4:
+      return MarkAsSimd128(node), VisitF32x4SConvertI32x4(node);
+    case IrOpcode::kF32x4UConvertI32x4:
+      return MarkAsSimd128(node), VisitF32x4UConvertI32x4(node);
+    case IrOpcode::kF32x4Abs:
+      return MarkAsSimd128(node), VisitF32x4Abs(node);
+    case IrOpcode::kF32x4Neg:
+      return MarkAsSimd128(node), VisitF32x4Neg(node);
+    case IrOpcode::kF32x4Sqrt:
+      return MarkAsSimd128(node), VisitF32x4Sqrt(node);
+    case IrOpcode::kF32x4Add:
+      return MarkAsSimd128(node), VisitF32x4Add(node);
+    case IrOpcode::kF32x4Sub:
+      return MarkAsSimd128(node), VisitF32x4Sub(node);
+    case IrOpcode::kF32x4Mul:
+      return MarkAsSimd128(node), VisitF32x4Mul(node);
+    case IrOpcode::kF32x4Div:
+      return MarkAsSimd128(node), VisitF32x4Div(node);
+    case IrOpcode::kF32x4Min:
+      return MarkAsSimd128(node), VisitF32x4Min(node);
+    case IrOpcode::kF32x4Max:
+      return MarkAsSimd128(node), VisitF32x4Max(node);
+    case IrOpcode::kF32x4Eq:
+      return MarkAsSimd128(node), VisitF32x4Eq(node);
+    case IrOpcode::kF32x4Ne:
+      return MarkAsSimd128(node), VisitF32x4Ne(node);
+    case IrOpcode::kF32x4Lt:
+      return MarkAsSimd128(node), VisitF32x4Lt(node);
+    case IrOpcode::kF32x4Le:
+      return MarkAsSimd128(node), VisitF32x4Le(node);
+    case IrOpcode::kF32x4Qfma:
+      return MarkAsSimd128(node), VisitF32x4Qfma(node);
+    case IrOpcode::kF32x4Qfms:
+      return MarkAsSimd128(node), VisitF32x4Qfms(node);
+    case IrOpcode::kF32x4Pmin:
+      return MarkAsSimd128(node), VisitF32x4Pmin(node);
+    case IrOpcode::kF32x4Pmax:
+      return MarkAsSimd128(node), VisitF32x4Pmax(node);
+    case IrOpcode::kF32x4Ceil:
+      return MarkAsSimd128(node), VisitF32x4Ceil(node);
+    case IrOpcode::kF32x4Floor:
+      return MarkAsSimd128(node), VisitF32x4Floor(node);
+    case IrOpcode::kF32x4Trunc:
+      return MarkAsSimd128(node), VisitF32x4Trunc(node);
+    case IrOpcode::kF32x4NearestInt:
+      return MarkAsSimd128(node), VisitF32x4NearestInt(node);
+    case IrOpcode::kF32x4DemoteF64x2Zero:
+      return MarkAsSimd128(node), VisitF32x4DemoteF64x2Zero(node);
+    case IrOpcode::kI64x2Splat:
+      return MarkAsSimd128(node), VisitI64x2Splat(node);
+    case IrOpcode::kI64x2SplatI32Pair:
+      return MarkAsSimd128(node), VisitI64x2SplatI32Pair(node);
+    case IrOpcode::kI64x2ExtractLane:
+      return MarkAsWord64(node), VisitI64x2ExtractLane(node);
+    case IrOpcode::kI64x2ReplaceLane:
+      return MarkAsSimd128(node), VisitI64x2ReplaceLane(node);
+    case IrOpcode::kI64x2ReplaceLaneI32Pair:
+      return MarkAsSimd128(node), VisitI64x2ReplaceLaneI32Pair(node);
+    case IrOpcode::kI64x2Abs:
+      return MarkAsSimd128(node), VisitI64x2Abs(node);
+    case IrOpcode::kI64x2Neg:
+      return MarkAsSimd128(node), VisitI64x2Neg(node);
+    case IrOpcode::kI64x2SConvertI32x4Low:
+      return MarkAsSimd128(node), VisitI64x2SConvertI32x4Low(node);
+    case IrOpcode::kI64x2SConvertI32x4High:
+      return MarkAsSimd128(node), VisitI64x2SConvertI32x4High(node);
+    case IrOpcode::kI64x2UConvertI32x4Low:
+      return MarkAsSimd128(node), VisitI64x2UConvertI32x4Low(node);
+    case IrOpcode::kI64x2UConvertI32x4High:
+      return MarkAsSimd128(node), VisitI64x2UConvertI32x4High(node);
+    case IrOpcode::kI64x2BitMask:
+      return MarkAsWord32(node), VisitI64x2BitMask(node);
+    case IrOpcode::kI64x2Shl:
+      return MarkAsSimd128(node), VisitI64x2Shl(node);
+    case IrOpcode::kI64x2ShrS:
+      return MarkAsSimd128(node), VisitI64x2ShrS(node);
+    case IrOpcode::kI64x2Add:
+      return MarkAsSimd128(node), VisitI64x2Add(node);
+    case IrOpcode::kI64x2Sub:
+      return MarkAsSimd128(node), VisitI64x2Sub(node);
+    case IrOpcode::kI64x2Mul:
+      return MarkAsSimd128(node), VisitI64x2Mul(node);
+    case IrOpcode::kI64x2Eq:
+      return MarkAsSimd128(node), VisitI64x2Eq(node);
+    case IrOpcode::kI64x2Ne:
+      return MarkAsSimd128(node), VisitI64x2Ne(node);
+    case IrOpcode::kI64x2GtS:
+      return MarkAsSimd128(node), VisitI64x2GtS(node);
+    case IrOpcode::kI64x2GeS:
+      return MarkAsSimd128(node), VisitI64x2GeS(node);
+    case IrOpcode::kI64x2ShrU:
+      return MarkAsSimd128(node), VisitI64x2ShrU(node);
+    case IrOpcode::kI64x2ExtMulLowI32x4S:
+      return MarkAsSimd128(node), VisitI64x2ExtMulLowI32x4S(node);
+    case IrOpcode::kI64x2ExtMulHighI32x4S:
+      return MarkAsSimd128(node), VisitI64x2ExtMulHighI32x4S(node);
+    case IrOpcode::kI64x2ExtMulLowI32x4U:
+      return MarkAsSimd128(node), VisitI64x2ExtMulLowI32x4U(node);
+    case IrOpcode::kI64x2ExtMulHighI32x4U:
+      return MarkAsSimd128(node), VisitI64x2ExtMulHighI32x4U(node);
+    case IrOpcode::kI32x4Splat:
+      return MarkAsSimd128(node), VisitI32x4Splat(node);
+    case IrOpcode::kI32x4ExtractLane:
+      return MarkAsWord32(node), VisitI32x4ExtractLane(node);
+    case IrOpcode::kI32x4ReplaceLane:
+      return MarkAsSimd128(node), VisitI32x4ReplaceLane(node);
+    case IrOpcode::kI32x4SConvertF32x4:
+      return MarkAsSimd128(node), VisitI32x4SConvertF32x4(node);
+    case IrOpcode::kI32x4SConvertI16x8Low:
+      return MarkAsSimd128(node), VisitI32x4SConvertI16x8Low(node);
+    case IrOpcode::kI32x4SConvertI16x8High:
+      return MarkAsSimd128(node), VisitI32x4SConvertI16x8High(node);
+    case IrOpcode::kI32x4Neg:
+      return MarkAsSimd128(node), VisitI32x4Neg(node);
+    case IrOpcode::kI32x4Shl:
+      return MarkAsSimd128(node), VisitI32x4Shl(node);
+    case IrOpcode::kI32x4ShrS:
+      return MarkAsSimd128(node), VisitI32x4ShrS(node);
+    case IrOpcode::kI32x4Add:
+      return MarkAsSimd128(node), VisitI32x4Add(node);
+    case IrOpcode::kI32x4Sub:
+      return MarkAsSimd128(node), VisitI32x4Sub(node);
+    case IrOpcode::kI32x4Mul:
+      return MarkAsSimd128(node), VisitI32x4Mul(node);
+    case IrOpcode::kI32x4MinS:
+      return MarkAsSimd128(node), VisitI32x4MinS(node);
+    case IrOpcode::kI32x4MaxS:
+      return MarkAsSimd128(node), VisitI32x4MaxS(node);
+    case IrOpcode::kI32x4Eq:
+      return MarkAsSimd128(node), VisitI32x4Eq(node);
+    case IrOpcode::kI32x4Ne:
+      return MarkAsSimd128(node), VisitI32x4Ne(node);
+    case IrOpcode::kI32x4GtS:
+      return MarkAsSimd128(node), VisitI32x4GtS(node);
+    case IrOpcode::kI32x4GeS:
+      return MarkAsSimd128(node), VisitI32x4GeS(node);
+    case IrOpcode::kI32x4UConvertF32x4:
+      return MarkAsSimd128(node), VisitI32x4UConvertF32x4(node);
+    case IrOpcode::kI32x4UConvertI16x8Low:
+      return MarkAsSimd128(node), VisitI32x4UConvertI16x8Low(node);
+    case IrOpcode::kI32x4UConvertI16x8High:
+      return MarkAsSimd128(node), VisitI32x4UConvertI16x8High(node);
+    case IrOpcode::kI32x4ShrU:
+      return MarkAsSimd128(node), VisitI32x4ShrU(node);
+    case IrOpcode::kI32x4MinU:
+      return MarkAsSimd128(node), VisitI32x4MinU(node);
+    case IrOpcode::kI32x4MaxU:
+      return MarkAsSimd128(node), VisitI32x4MaxU(node);
+    case IrOpcode::kI32x4GtU:
+      return MarkAsSimd128(node), VisitI32x4GtU(node);
+    case IrOpcode::kI32x4GeU:
+      return MarkAsSimd128(node), VisitI32x4GeU(node);
+    case IrOpcode::kI32x4Abs:
+      return MarkAsSimd128(node), VisitI32x4Abs(node);
+    case IrOpcode::kI32x4BitMask:
+      return MarkAsWord32(node), VisitI32x4BitMask(node);
+    case IrOpcode::kI32x4DotI16x8S:
+      return MarkAsSimd128(node), VisitI32x4DotI16x8S(node);
+    case IrOpcode::kI32x4ExtMulLowI16x8S:
+      return MarkAsSimd128(node), VisitI32x4ExtMulLowI16x8S(node);
+    case IrOpcode::kI32x4ExtMulHighI16x8S:
+      return MarkAsSimd128(node), VisitI32x4ExtMulHighI16x8S(node);
+    case IrOpcode::kI32x4ExtMulLowI16x8U:
+      return MarkAsSimd128(node), VisitI32x4ExtMulLowI16x8U(node);
+    case IrOpcode::kI32x4ExtMulHighI16x8U:
+      return MarkAsSimd128(node), VisitI32x4ExtMulHighI16x8U(node);
+    case IrOpcode::kI32x4ExtAddPairwiseI16x8S:
+      return MarkAsSimd128(node), VisitI32x4ExtAddPairwiseI16x8S(node);
+    case IrOpcode::kI32x4ExtAddPairwiseI16x8U:
+      return MarkAsSimd128(node), VisitI32x4ExtAddPairwiseI16x8U(node);
+    case IrOpcode::kI32x4TruncSatF64x2SZero:
+      return MarkAsSimd128(node), VisitI32x4TruncSatF64x2SZero(node);
+    case IrOpcode::kI32x4TruncSatF64x2UZero:
+      return MarkAsSimd128(node), VisitI32x4TruncSatF64x2UZero(node);
+    case IrOpcode::kI16x8Splat:
+      return MarkAsSimd128(node), VisitI16x8Splat(node);
+    case IrOpcode::kI16x8ExtractLaneU:
+      return MarkAsWord32(node), VisitI16x8ExtractLaneU(node);
+    case IrOpcode::kI16x8ExtractLaneS:
+      return MarkAsWord32(node), VisitI16x8ExtractLaneS(node);
+    case IrOpcode::kI16x8ReplaceLane:
+      return MarkAsSimd128(node), VisitI16x8ReplaceLane(node);
+    case IrOpcode::kI16x8SConvertI8x16Low:
+      return MarkAsSimd128(node), VisitI16x8SConvertI8x16Low(node);
+    case IrOpcode::kI16x8SConvertI8x16High:
+      return MarkAsSimd128(node), VisitI16x8SConvertI8x16High(node);
+    case IrOpcode::kI16x8Neg:
+      return MarkAsSimd128(node), VisitI16x8Neg(node);
+    case IrOpcode::kI16x8Shl:
+      return MarkAsSimd128(node), VisitI16x8Shl(node);
+    case IrOpcode::kI16x8ShrS:
+      return MarkAsSimd128(node), VisitI16x8ShrS(node);
+    case IrOpcode::kI16x8SConvertI32x4:
+      return MarkAsSimd128(node), VisitI16x8SConvertI32x4(node);
+    case IrOpcode::kI16x8Add:
+      return MarkAsSimd128(node), VisitI16x8Add(node);
+    case IrOpcode::kI16x8AddSatS:
+      return MarkAsSimd128(node), VisitI16x8AddSatS(node);
+    case IrOpcode::kI16x8Sub:
+      return MarkAsSimd128(node), VisitI16x8Sub(node);
+    case IrOpcode::kI16x8SubSatS:
+      return MarkAsSimd128(node), VisitI16x8SubSatS(node);
+    case IrOpcode::kI16x8Mul:
+      return MarkAsSimd128(node), VisitI16x8Mul(node);
+    case IrOpcode::kI16x8MinS:
+      return MarkAsSimd128(node), VisitI16x8MinS(node);
+    case IrOpcode::kI16x8MaxS:
+      return MarkAsSimd128(node), VisitI16x8MaxS(node);
+    case IrOpcode::kI16x8Eq:
+      return MarkAsSimd128(node), VisitI16x8Eq(node);
+    case IrOpcode::kI16x8Ne:
+      return MarkAsSimd128(node), VisitI16x8Ne(node);
+    case IrOpcode::kI16x8GtS:
+      return MarkAsSimd128(node), VisitI16x8GtS(node);
+    case IrOpcode::kI16x8GeS:
+      return MarkAsSimd128(node), VisitI16x8GeS(node);
+    case IrOpcode::kI16x8UConvertI8x16Low:
+      return MarkAsSimd128(node), VisitI16x8UConvertI8x16Low(node);
+    case IrOpcode::kI16x8UConvertI8x16High:
+      return MarkAsSimd128(node), VisitI16x8UConvertI8x16High(node);
+    case IrOpcode::kI16x8ShrU:
+      return MarkAsSimd128(node), VisitI16x8ShrU(node);
+    case IrOpcode::kI16x8UConvertI32x4:
+      return MarkAsSimd128(node), VisitI16x8UConvertI32x4(node);
+    case IrOpcode::kI16x8AddSatU:
+      return MarkAsSimd128(node), VisitI16x8AddSatU(node);
+    case IrOpcode::kI16x8SubSatU:
+      return MarkAsSimd128(node), VisitI16x8SubSatU(node);
+    case IrOpcode::kI16x8MinU:
+      return MarkAsSimd128(node), VisitI16x8MinU(node);
+    case IrOpcode::kI16x8MaxU:
+      return MarkAsSimd128(node), VisitI16x8MaxU(node);
+    case IrOpcode::kI16x8GtU:
+      return MarkAsSimd128(node), VisitI16x8GtU(node);
+    case IrOpcode::kI16x8GeU:
+      return MarkAsSimd128(node), VisitI16x8GeU(node);
+    case IrOpcode::kI16x8RoundingAverageU:
+      return MarkAsSimd128(node), VisitI16x8RoundingAverageU(node);
+    case IrOpcode::kI16x8Q15MulRSatS:
+      return MarkAsSimd128(node), VisitI16x8Q15MulRSatS(node);
+    case IrOpcode::kI16x8Abs:
+      return MarkAsSimd128(node), VisitI16x8Abs(node);
+    case IrOpcode::kI16x8BitMask:
+      return MarkAsWord32(node), VisitI16x8BitMask(node);
+    case IrOpcode::kI16x8ExtMulLowI8x16S:
+      return MarkAsSimd128(node), VisitI16x8ExtMulLowI8x16S(node);
+    case IrOpcode::kI16x8ExtMulHighI8x16S:
+      return MarkAsSimd128(node), VisitI16x8ExtMulHighI8x16S(node);
+    case IrOpcode::kI16x8ExtMulLowI8x16U:
+      return MarkAsSimd128(node), VisitI16x8ExtMulLowI8x16U(node);
+    case IrOpcode::kI16x8ExtMulHighI8x16U:
+      return MarkAsSimd128(node), VisitI16x8ExtMulHighI8x16U(node);
+    case IrOpcode::kI16x8ExtAddPairwiseI8x16S:
+      return MarkAsSimd128(node), VisitI16x8ExtAddPairwiseI8x16S(node);
+    case IrOpcode::kI16x8ExtAddPairwiseI8x16U:
+      return MarkAsSimd128(node), VisitI16x8ExtAddPairwiseI8x16U(node);
+    case IrOpcode::kI8x16Splat:
+      return MarkAsSimd128(node), VisitI8x16Splat(node);
+    case IrOpcode::kI8x16ExtractLaneU:
+      return MarkAsWord32(node), VisitI8x16ExtractLaneU(node);
+    case IrOpcode::kI8x16ExtractLaneS:
+      return MarkAsWord32(node), VisitI8x16ExtractLaneS(node);
+    case IrOpcode::kI8x16ReplaceLane:
+      return MarkAsSimd128(node), VisitI8x16ReplaceLane(node);
+    case IrOpcode::kI8x16Neg:
+      return MarkAsSimd128(node), VisitI8x16Neg(node);
+    case IrOpcode::kI8x16Shl:
+      return MarkAsSimd128(node), VisitI8x16Shl(node);
+    case IrOpcode::kI8x16ShrS:
+      return MarkAsSimd128(node), VisitI8x16ShrS(node);
+    case IrOpcode::kI8x16SConvertI16x8:
+      return MarkAsSimd128(node), VisitI8x16SConvertI16x8(node);
+    case IrOpcode::kI8x16Add:
+      return MarkAsSimd128(node), VisitI8x16Add(node);
+    case IrOpcode::kI8x16AddSatS:
+      return MarkAsSimd128(node), VisitI8x16AddSatS(node);
+    case IrOpcode::kI8x16Sub:
+      return MarkAsSimd128(node), VisitI8x16Sub(node);
+    case IrOpcode::kI8x16SubSatS:
+      return MarkAsSimd128(node), VisitI8x16SubSatS(node);
+    case IrOpcode::kI8x16MinS:
+      return MarkAsSimd128(node), VisitI8x16MinS(node);
+    case IrOpcode::kI8x16MaxS:
+      return MarkAsSimd128(node), VisitI8x16MaxS(node);
+    case IrOpcode::kI8x16Eq:
+      return MarkAsSimd128(node), VisitI8x16Eq(node);
+    case IrOpcode::kI8x16Ne:
+      return MarkAsSimd128(node), VisitI8x16Ne(node);
+    case IrOpcode::kI8x16GtS:
+      return MarkAsSimd128(node), VisitI8x16GtS(node);
+    case IrOpcode::kI8x16GeS:
+      return MarkAsSimd128(node), VisitI8x16GeS(node);
+    case IrOpcode::kI8x16ShrU:
+      return MarkAsSimd128(node), VisitI8x16ShrU(node);
+    case IrOpcode::kI8x16UConvertI16x8:
+      return MarkAsSimd128(node), VisitI8x16UConvertI16x8(node);
+    case IrOpcode::kI8x16AddSatU:
+      return MarkAsSimd128(node), VisitI8x16AddSatU(node);
+    case IrOpcode::kI8x16SubSatU:
+      return MarkAsSimd128(node), VisitI8x16SubSatU(node);
+    case IrOpcode::kI8x16MinU:
+      return MarkAsSimd128(node), VisitI8x16MinU(node);
+    case IrOpcode::kI8x16MaxU:
+      return MarkAsSimd128(node), VisitI8x16MaxU(node);
+    case IrOpcode::kI8x16GtU:
+      return MarkAsSimd128(node), VisitI8x16GtU(node);
+    case IrOpcode::kI8x16GeU:
+      return MarkAsSimd128(node), VisitI8x16GeU(node);
+    case IrOpcode::kI8x16RoundingAverageU:
+      return MarkAsSimd128(node), VisitI8x16RoundingAverageU(node);
+    case IrOpcode::kI8x16Popcnt:
+      return MarkAsSimd128(node), VisitI8x16Popcnt(node);
+    case IrOpcode::kI8x16Abs:
+      return MarkAsSimd128(node), VisitI8x16Abs(node);
+    case IrOpcode::kI8x16BitMask:
+      return MarkAsWord32(node), VisitI8x16BitMask(node);
+    case IrOpcode::kS128Const:
+      return MarkAsSimd128(node), VisitS128Const(node);
+    case IrOpcode::kS128Zero:
+      return MarkAsSimd128(node), VisitS128Zero(node);
+    case IrOpcode::kS128And:
+      return MarkAsSimd128(node), VisitS128And(node);
+    case IrOpcode::kS128Or:
+      return MarkAsSimd128(node), VisitS128Or(node);
+    case IrOpcode::kS128Xor:
+      return MarkAsSimd128(node), VisitS128Xor(node);
+    case IrOpcode::kS128Not:
+      return MarkAsSimd128(node), VisitS128Not(node);
+    case IrOpcode::kS128Select:
+      return MarkAsSimd128(node), VisitS128Select(node);
+    case IrOpcode::kS128AndNot:
+      return MarkAsSimd128(node), VisitS128AndNot(node);
+    case IrOpcode::kI8x16Swizzle:
+      return MarkAsSimd128(node), VisitI8x16Swizzle(node);
+    case IrOpcode::kI8x16Shuffle:
+      return MarkAsSimd128(node), VisitI8x16Shuffle(node);
+    case IrOpcode::kV128AnyTrue:
+      return MarkAsWord32(node), VisitV128AnyTrue(node);
+    case IrOpcode::kI64x2AllTrue:
+      return MarkAsWord32(node), VisitI64x2AllTrue(node);
+    case IrOpcode::kI32x4AllTrue:
+      return MarkAsWord32(node), VisitI32x4AllTrue(node);
+    case IrOpcode::kI16x8AllTrue:
+      return MarkAsWord32(node), VisitI16x8AllTrue(node);
+    case IrOpcode::kI8x16AllTrue:
+      return MarkAsWord32(node), VisitI8x16AllTrue(node);
+    case IrOpcode::kI8x16RelaxedLaneSelect:
+      return MarkAsSimd128(node), VisitI8x16RelaxedLaneSelect(node);
+    case IrOpcode::kI16x8RelaxedLaneSelect:
+      return MarkAsSimd128(node), VisitI16x8RelaxedLaneSelect(node);
+    case IrOpcode::kI32x4RelaxedLaneSelect:
+      return MarkAsSimd128(node), VisitI32x4RelaxedLaneSelect(node);
+    case IrOpcode::kI64x2RelaxedLaneSelect:
+      return MarkAsSimd128(node), VisitI64x2RelaxedLaneSelect(node);
+    case IrOpcode::kF32x4RelaxedMin:
+      return MarkAsSimd128(node), VisitF32x4RelaxedMin(node);
+    case IrOpcode::kF32x4RelaxedMax:
+      return MarkAsSimd128(node), VisitF32x4RelaxedMax(node);
+    case IrOpcode::kF64x2RelaxedMin:
+      return MarkAsSimd128(node), VisitF64x2RelaxedMin(node);
+    case IrOpcode::kF64x2RelaxedMax:
+      return MarkAsSimd128(node), VisitF64x2RelaxedMax(node);
+    case IrOpcode::kI32x4RelaxedTruncF64x2SZero:
+      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF64x2SZero(node);
+    case IrOpcode::kI32x4RelaxedTruncF64x2UZero:
+      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF64x2UZero(node);
+    case IrOpcode::kI32x4RelaxedTruncF32x4S:
+      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF32x4S(node);
+    case IrOpcode::kI32x4RelaxedTruncF32x4U:
+      return MarkAsSimd128(node), VisitI32x4RelaxedTruncF32x4U(node);
+    case IrOpcode::kI16x8RelaxedQ15MulRS:
+      return MarkAsSimd128(node), VisitI16x8RelaxedQ15MulRS(node);
+    case IrOpcode::kI16x8DotI8x16I7x16S:
+      return MarkAsSimd128(node), VisitI16x8DotI8x16I7x16S(node);
+    case IrOpcode::kI32x4DotI8x16I7x16AddS:
+      return MarkAsSimd128(node), VisitI32x4DotI8x16I7x16AddS(node);
 
+      // SIMD256
+#if V8_TARGET_ARCH_X64
+    case IrOpcode::kF64x4Min:
+      return MarkAsSimd256(node), VisitF64x4Min(node);
+    case IrOpcode::kF64x4Max:
+      return MarkAsSimd256(node), VisitF64x4Max(node);
+    case IrOpcode::kF64x4Add:
+      return MarkAsSimd256(node), VisitF64x4Add(node);
+    case IrOpcode::kF32x8Add:
+      return MarkAsSimd256(node), VisitF32x8Add(node);
+    case IrOpcode::kI64x4Add:
+      return MarkAsSimd256(node), VisitI64x4Add(node);
+    case IrOpcode::kI32x8Add:
+      return MarkAsSimd256(node), VisitI32x8Add(node);
+    case IrOpcode::kI16x16Add:
+      return MarkAsSimd256(node), VisitI16x16Add(node);
+    case IrOpcode::kI8x32Add:
+      return MarkAsSimd256(node), VisitI8x32Add(node);
+    case IrOpcode::kF64x4Sub:
+      return MarkAsSimd256(node), VisitF64x4Sub(node);
+    case IrOpcode::kF32x8Sub:
+      return MarkAsSimd256(node), VisitF32x8Sub(node);
+    case IrOpcode::kF32x8Min:
+      return MarkAsSimd256(node), VisitF32x8Min(node);
+    case IrOpcode::kF32x8Max:
+      return MarkAsSimd256(node), VisitF32x8Max(node);
+    case IrOpcode::kI64x4Ne:
+      return MarkAsSimd256(node), VisitI64x4Ne(node);
+    case IrOpcode::kI32x8Ne:
+      return MarkAsSimd256(node), VisitI32x8Ne(node);
+    case IrOpcode::kI32x8GtU:
+      return MarkAsSimd256(node), VisitI32x8GtU(node);
+    case IrOpcode::kI32x8GeS:
+      return MarkAsSimd256(node), VisitI32x8GeS(node);
+    case IrOpcode::kI32x8GeU:
+      return MarkAsSimd256(node), VisitI32x8GeU(node);
+    case IrOpcode::kI16x16Ne:
+      return MarkAsSimd256(node), VisitI16x16Ne(node);
+    case IrOpcode::kI16x16GtU:
+      return MarkAsSimd256(node), VisitI16x16GtU(node);
+    case IrOpcode::kI16x16GeS:
+      return MarkAsSimd256(node), VisitI16x16GeS(node);
+    case IrOpcode::kI16x16GeU:
+      return MarkAsSimd256(node), VisitI16x16GeU(node);
+    case IrOpcode::kI8x32Ne:
+      return MarkAsSimd256(node), VisitI8x32Ne(node);
+    case IrOpcode::kI8x32GtU:
+      return MarkAsSimd256(node), VisitI8x32GtU(node);
+    case IrOpcode::kI8x32GeS:
+      return MarkAsSimd256(node), VisitI8x32GeS(node);
+    case IrOpcode::kI8x32GeU:
+      return MarkAsSimd256(node), VisitI8x32GeU(node);
+    case IrOpcode::kI64x4Sub:
+      return MarkAsSimd256(node), VisitI64x4Sub(node);
+    case IrOpcode::kI32x8Sub:
+      return MarkAsSimd256(node), VisitI32x8Sub(node);
+    case IrOpcode::kI16x16Sub:
+      return MarkAsSimd256(node), VisitI16x16Sub(node);
+    case IrOpcode::kI8x32Sub:
+      return MarkAsSimd256(node), VisitI8x32Sub(node);
+    case IrOpcode::kF64x4Mul:
+      return MarkAsSimd256(node), VisitF64x4Mul(node);
+    case IrOpcode::kF32x8Mul:
+      return MarkAsSimd256(node), VisitF32x8Mul(node);
+    case IrOpcode::kI64x4Mul:
+      return MarkAsSimd256(node), VisitI64x4Mul(node);
+    case IrOpcode::kI32x8Mul:
+      return MarkAsSimd256(node), VisitI32x8Mul(node);
+    case IrOpcode::kI16x16Mul:
+      return MarkAsSimd256(node), VisitI16x16Mul(node);
+    case IrOpcode::kF32x8Div:
+      return MarkAsSimd256(node), VisitF32x8Div(node);
+    case IrOpcode::kF64x4Div:
+      return MarkAsSimd256(node), VisitF64x4Div(node);
+    case IrOpcode::kI16x16AddSatS:
+      return MarkAsSimd256(node), VisitI16x16AddSatS(node);
+    case IrOpcode::kI8x32AddSatS:
+      return MarkAsSimd256(node), VisitI8x32AddSatS(node);
+    case IrOpcode::kI16x16AddSatU:
+      return MarkAsSimd256(node), VisitI16x16AddSatU(node);
+    case IrOpcode::kI8x32AddSatU:
+      return MarkAsSimd256(node), VisitI8x32AddSatU(node);
+    case IrOpcode::kI16x16SubSatS:
+      return MarkAsSimd256(node), VisitI16x16SubSatS(node);
+    case IrOpcode::kI8x32SubSatS:
+      return MarkAsSimd256(node), VisitI8x32SubSatS(node);
+    case IrOpcode::kI16x16SubSatU:
+      return MarkAsSimd256(node), VisitI16x16SubSatU(node);
+    case IrOpcode::kI8x32SubSatU:
+      return MarkAsSimd256(node), VisitI8x32SubSatU(node);
+    case IrOpcode::kF64x4ConvertI32x4S:
+      return MarkAsSimd256(node), VisitF64x4ConvertI32x4S(node);
+    case IrOpcode::kF32x8SConvertI32x8:
+      return MarkAsSimd256(node), VisitF32x8SConvertI32x8(node);
+    case IrOpcode::kF32x4DemoteF64x4:
+      return MarkAsSimd256(node), VisitF32x4DemoteF64x4(node);
+    case IrOpcode::kI64x4SConvertI32x4:
+      return MarkAsSimd256(node), VisitI64x4SConvertI32x4(node);
+    case IrOpcode::kI64x4UConvertI32x4:
+      return MarkAsSimd256(node), VisitI64x4UConvertI32x4(node);
+    case IrOpcode::kI32x8SConvertI16x8:
+      return MarkAsSimd256(node), VisitI32x8SConvertI16x8(node);
+    case IrOpcode::kI32x8UConvertI16x8:
+      return MarkAsSimd256(node), VisitI32x8UConvertI16x8(node);
+    case IrOpcode::kI16x16SConvertI8x16:
+      return MarkAsSimd256(node), VisitI16x16SConvertI8x16(node);
+    case IrOpcode::kI16x16UConvertI8x16:
+      return MarkAsSimd256(node), VisitI16x16UConvertI8x16(node);
+    case IrOpcode::kI16x16SConvertI32x8:
+      return MarkAsSimd256(node), VisitI16x16SConvertI32x8(node);
+    case IrOpcode::kI16x16UConvertI32x8:
+      return MarkAsSimd256(node), VisitI16x16UConvertI32x8(node);
+    case IrOpcode::kI8x32SConvertI16x16:
+      return MarkAsSimd256(node), VisitI8x32SConvertI16x16(node);
+    case IrOpcode::kI8x32UConvertI16x16:
+      return MarkAsSimd256(node), VisitI8x32UConvertI16x16(node);
+    case IrOpcode::kF32x8Abs:
+      return MarkAsSimd256(node), VisitF32x8Abs(node);
+    case IrOpcode::kF32x8Neg:
+      return MarkAsSimd256(node), VisitF32x8Neg(node);
+    case IrOpcode::kF32x8Sqrt:
+      return MarkAsSimd256(node), VisitF32x8Sqrt(node);
+    case IrOpcode::kF64x4Sqrt:
+      return MarkAsSimd256(node), VisitF64x4Sqrt(node);
+    case IrOpcode::kI32x8Abs:
+      return MarkAsSimd256(node), VisitI32x8Abs(node);
+    case IrOpcode::kI32x8Neg:
+      return MarkAsSimd256(node), VisitI32x8Neg(node);
+    case IrOpcode::kI16x16Abs:
+      return MarkAsSimd256(node), VisitI16x16Abs(node);
+    case IrOpcode::kI16x16Neg:
+      return MarkAsSimd256(node), VisitI16x16Neg(node);
+    case IrOpcode::kI8x32Abs:
+      return MarkAsSimd256(node), VisitI8x32Abs(node);
+    case IrOpcode::kI8x32Neg:
+      return MarkAsSimd256(node), VisitI8x32Neg(node);
+    case IrOpcode::kI64x4Shl:
+      return MarkAsSimd256(node), VisitI64x4Shl(node);
+    case IrOpcode::kI64x4ShrU:
+      return MarkAsSimd256(node), VisitI64x4ShrU(node);
+    case IrOpcode::kI32x8Shl:
+      return MarkAsSimd256(node), VisitI32x8Shl(node);
+    case IrOpcode::kI32x8ShrS:
+      return MarkAsSimd256(node), VisitI32x8ShrS(node);
+    case IrOpcode::kI32x8ShrU:
+      return MarkAsSimd256(node), VisitI32x8ShrU(node);
+    case IrOpcode::kI16x16Shl:
+      return MarkAsSimd256(node), VisitI16x16Shl(node);
+    case IrOpcode::kI16x16ShrS:
+      return MarkAsSimd256(node), VisitI16x16ShrS(node);
+    case IrOpcode::kI16x16ShrU:
+      return MarkAsSimd256(node), VisitI16x16ShrU(node);
+    case IrOpcode::kI32x8DotI16x16S:
+      return MarkAsSimd256(node), VisitI32x8DotI16x16S(node);
+    case IrOpcode::kI16x16RoundingAverageU:
+      return MarkAsSimd256(node), VisitI16x16RoundingAverageU(node);
+    case IrOpcode::kI8x32RoundingAverageU:
+      return MarkAsSimd256(node), VisitI8x32RoundingAverageU(node);
+    case IrOpcode::kS256Zero:
+      return MarkAsSimd256(node), VisitS256Zero(node);
+    case IrOpcode::kS256And:
+      return MarkAsSimd256(node), VisitS256And(node);
+    case IrOpcode::kS256Or:
+      return MarkAsSimd256(node), VisitS256Or(node);
+    case IrOpcode::kS256Xor:
+      return MarkAsSimd256(node), VisitS256Xor(node);
+    case IrOpcode::kS256Not:
+      return MarkAsSimd256(node), VisitS256Not(node);
+    case IrOpcode::kS256Select:
+      return MarkAsSimd256(node), VisitS256Select(node);
+    case IrOpcode::kS256AndNot:
+      return MarkAsSimd256(node), VisitS256AndNot(node);
+    case IrOpcode::kF32x8Eq:
+      return MarkAsSimd256(node), VisitF32x8Eq(node);
+    case IrOpcode::kF64x4Eq:
+      return MarkAsSimd256(node), VisitF64x4Eq(node);
+    case IrOpcode::kI64x4Eq:
+      return MarkAsSimd256(node), VisitI64x4Eq(node);
+    case IrOpcode::kI32x8Eq:
+      return MarkAsSimd256(node), VisitI32x8Eq(node);
+    case IrOpcode::kI16x16Eq:
+      return MarkAsSimd256(node), VisitI16x16Eq(node);
+    case IrOpcode::kI8x32Eq:
+      return MarkAsSimd256(node), VisitI8x32Eq(node);
+    case IrOpcode::kF32x8Ne:
+      return MarkAsSimd256(node), VisitF32x8Ne(node);
+    case IrOpcode::kF64x4Ne:
+      return MarkAsSimd256(node), VisitF64x4Ne(node);
+    case IrOpcode::kI64x4GtS:
+      return MarkAsSimd256(node), VisitI64x4GtS(node);
+    case IrOpcode::kI32x8GtS:
+      return MarkAsSimd256(node), VisitI32x8GtS(node);
+    case IrOpcode::kI16x16GtS:
+      return MarkAsSimd256(node), VisitI16x16GtS(node);
+    case IrOpcode::kI8x32GtS:
+      return MarkAsSimd256(node), VisitI8x32GtS(node);
+    case IrOpcode::kF64x4Lt:
+      return MarkAsSimd256(node), VisitF64x4Lt(node);
+    case IrOpcode::kF32x8Lt:
+      return MarkAsSimd256(node), VisitF32x8Lt(node);
+    case IrOpcode::kF64x4Le:
+      return MarkAsSimd256(node), VisitF64x4Le(node);
+    case IrOpcode::kF32x8Le:
+      return MarkAsSimd256(node), VisitF32x8Le(node);
+    case IrOpcode::kI32x8MinS:
+      return MarkAsSimd256(node), VisitI32x8MinS(node);
+    case IrOpcode::kI16x16MinS:
+      return MarkAsSimd256(node), VisitI16x16MinS(node);
+    case IrOpcode::kI8x32MinS:
+      return MarkAsSimd256(node), VisitI8x32MinS(node);
+    case IrOpcode::kI32x8MinU:
+      return MarkAsSimd256(node), VisitI32x8MinU(node);
+    case IrOpcode::kI16x16MinU:
+      return MarkAsSimd256(node), VisitI16x16MinU(node);
+    case IrOpcode::kI8x32MinU:
+      return MarkAsSimd256(node), VisitI8x32MinU(node);
+    case IrOpcode::kI32x8MaxS:
+      return MarkAsSimd256(node), VisitI32x8MaxS(node);
+    case IrOpcode::kI16x16MaxS:
+      return MarkAsSimd256(node), VisitI16x16MaxS(node);
+    case IrOpcode::kI8x32MaxS:
+      return MarkAsSimd256(node), VisitI8x32MaxS(node);
+    case IrOpcode::kI32x8MaxU:
+      return MarkAsSimd256(node), VisitI32x8MaxU(node);
+    case IrOpcode::kI16x16MaxU:
+      return MarkAsSimd256(node), VisitI16x16MaxU(node);
+    case IrOpcode::kI8x32MaxU:
+      return MarkAsSimd256(node), VisitI8x32MaxU(node);
+    case IrOpcode::kI64x4Splat:
+      return MarkAsSimd256(node), VisitI64x4Splat(node);
+    case IrOpcode::kI32x8Splat:
+      return MarkAsSimd256(node), VisitI32x8Splat(node);
+    case IrOpcode::kI16x16Splat:
+      return MarkAsSimd256(node), VisitI16x16Splat(node);
+    case IrOpcode::kI8x32Splat:
+      return MarkAsSimd256(node), VisitI8x32Splat(node);
+    case IrOpcode::kI64x4ExtMulI32x4S:
+      return MarkAsSimd256(node), VisitI64x4ExtMulI32x4S(node);
+    case IrOpcode::kI64x4ExtMulI32x4U:
+      return MarkAsSimd256(node), VisitI64x4ExtMulI32x4U(node);
+    case IrOpcode::kI32x8ExtMulI16x8S:
+      return MarkAsSimd256(node), VisitI32x8ExtMulI16x8S(node);
+    case IrOpcode::kI32x8ExtMulI16x8U:
+      return MarkAsSimd256(node), VisitI32x8ExtMulI16x8U(node);
+    case IrOpcode::kI16x16ExtMulI8x16S:
+      return MarkAsSimd256(node), VisitI16x16ExtMulI8x16S(node);
+    case IrOpcode::kI16x16ExtMulI8x16U:
+      return MarkAsSimd256(node), VisitI16x16ExtMulI8x16U(node);
+    case IrOpcode::kI32x8ExtAddPairwiseI16x16S:
+      return MarkAsSimd256(node), VisitI32x8ExtAddPairwiseI16x16S(node);
+    case IrOpcode::kI32x8ExtAddPairwiseI16x16U:
+      return MarkAsSimd256(node), VisitI32x8ExtAddPairwiseI16x16U(node);
+    case IrOpcode::kI16x16ExtAddPairwiseI8x32S:
+      return MarkAsSimd256(node), VisitI16x16ExtAddPairwiseI8x32S(node);
+    case IrOpcode::kI16x16ExtAddPairwiseI8x32U:
+      return MarkAsSimd256(node), VisitI16x16ExtAddPairwiseI8x32U(node);
+#endif  //  V8_TARGET_ARCH_X64
     default:
-      return;
+      FATAL("Unexpected operator #%d:%s @ node #%d", node->opcode(),
+            node->op()->mnemonic(), node->id());
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitDeoptimizeIf(Node* node) {
-  TryPrepareScheduleFirstProjection(node->InputAt(0));
-
-  DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
-  FlagsContinuation cont = FlagsContinuation::ForDeoptimize(
-      kNotEqual, p.reason(), node->id(), p.feedback(),
-      FrameState{node->InputAt(1)});
-  VisitWordCompareZero(node, node->InputAt(0), &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitDeoptimizeUnless(Node* node) {
-  TryPrepareScheduleFirstProjection(node->InputAt(0));
-
-  DeoptimizeParameters p = DeoptimizeParametersOf(node->op());
-  FlagsContinuation cont = FlagsContinuation::ForDeoptimize(
-      kEqual, p.reason(), node->id(), p.feedback(),
-      FrameState{node->InputAt(1)});
-  VisitWordCompareZero(node, node->InputAt(0), &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitSelect(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSelect(kNotEqual, node,
-                                   node->InputAt(1), node->InputAt(2));
-  VisitWordCompareZero(node, node->InputAt(0), &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTrapIf(Node* node, TrapId trap_id) {
-  // FrameStates are only used for wasm traps inlined in JS. In that case the
-  // trap node will be lowered (replaced) before instruction selection.
-  // Therefore any TrapIf node has only one input.
-  DCHECK_EQ(node->op()->ValueInputCount(), 1);
-  FlagsContinuation cont = FlagsContinuation::ForTrap(kNotEqual, trap_id);
-  VisitWordCompareZero(node, node->InputAt(0), &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTrapUnless(Node* node,
-                                                    TrapId trap_id) {
-  // FrameStates are only used for wasm traps inlined in JS. In that case the
-  // trap node will be lowered (replaced) before instruction selection.
-  // Therefore any TrapUnless node has only one input.
-  DCHECK_EQ(node->op()->ValueInputCount(), 1);
-  FlagsContinuation cont = FlagsContinuation::ForTrap(kEqual, trap_id);
-  VisitWordCompareZero(node, node->InputAt(0), &cont);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitIdentity(Node* node) {
-  MarkAsUsed(node->InputAt(0));
-  MarkAsDefined(node);
-  SetRename(node, node->InputAt(0));
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitDeoptimize(
-    DeoptimizeReason reason, NodeId node_id, FeedbackSource const& feedback,
-    FrameState frame_state) {
-  InstructionOperandVector args(instruction_zone());
-  AppendDeoptimizeArguments(&args, reason, node_id, feedback, frame_state);
-  Emit(kArchDeoptimize, 0, nullptr, args.size(), &args.front(), 0, nullptr);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitThrow(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchThrowTerminator, g.NoOutput());
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitDebugBreak(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchDebugBreak, g.NoOutput());
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnreachable(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchDebugBreak, g.NoOutput());
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStaticAssert(Node* node) {
-  Node* asserted = node->InputAt(0);
-  UnparkedScopeIfNeeded scope(broker_);
-  AllowHandleDereference allow_handle_dereference;
-  asserted->Print(4);
-  FATAL(
-      "Expected Turbofan static assert to hold, but got non-true input:\n  %s",
-      StaticAssertSourceOf(node->op()));
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitDeadValue(Node* node) {
-  OperandGenerator g(this);
-  MarkAsRepresentation(DeadValueRepresentationOf(node->op()), node);
-  Emit(kArchDebugBreak, g.DefineAsConstant(node));
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitComment(Node* node) {
-  OperandGenerator g(this);
-  InstructionOperand operand(g.UseImmediate(node));
-  Emit(kArchComment, 0, nullptr, 1, &operand);
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitRetain(Node* node) {
-  OperandGenerator g(this);
-  Emit(kArchNop, g.NoOutput(), g.UseAny(node->InputAt(0)));
-}
-
 template <typename Adapter>
 bool InstructionSelectorT<Adapter>::CanProduceSignalingNaN(Node* node) {
   // TODO(jarin) Improve the heuristic here.
@@ -3933,6 +4377,39 @@ bool InstructionSelectorT<Adapter>::ZeroExtendsWord32ToWord64(
 
 namespace {
 
+FrameStateDescriptor* GetFrameStateDescriptorInternal(
+    Zone* zone, turboshaft::Graph* graph,
+    const turboshaft::FrameStateOp& state) {
+  const FrameStateInfo& state_info = state.data->frame_state_info;
+  int parameters = state_info.parameter_count();
+  int locals = state_info.local_count();
+  int stack = state_info.stack_count();
+
+  FrameStateDescriptor* outer_state = nullptr;
+  if (state.inlined) {
+    outer_state = GetFrameStateDescriptorInternal(
+        zone, graph,
+        graph->Get(state.parent_frame_state())
+            .template Cast<turboshaft::FrameStateOp>());
+  }
+
+#if V8_ENABLE_WEBASSEMBLY
+  if (state_info.type() == FrameStateType::kJSToWasmBuiltinContinuation) {
+    auto function_info = static_cast<const JSToWasmFrameStateFunctionInfo*>(
+        state_info.function_info());
+    return zone->New<JSToWasmFrameStateDescriptor>(
+        zone, state_info.type(), state_info.bailout_id(),
+        state_info.state_combine(), parameters, locals, stack,
+        state_info.shared_info(), outer_state, function_info->signature());
+  }
+#endif  // V8_ENABLE_WEBASSEMBLY
+
+  return zone->New<FrameStateDescriptor>(
+      zone, state_info.type(), state_info.bailout_id(),
+      state_info.state_combine(), parameters, locals, stack,
+      state_info.shared_info(), outer_state);
+}
+
 FrameStateDescriptor* GetFrameStateDescriptorInternal(Zone* zone,
                                                       FrameState state) {
   DCHECK_EQ(IrOpcode::kFrameState, state->opcode());
@@ -3967,9 +4444,25 @@ FrameStateDescriptor* GetFrameStateDescriptorInternal(Zone* zone,
 
 }  // namespace
 
-template <typename Adapter>
-FrameStateDescriptor* InstructionSelectorT<Adapter>::GetFrameStateDescriptor(
-    FrameState state) {
+template <>
+FrameStateDescriptor*
+InstructionSelectorT<TurboshaftAdapter>::GetFrameStateDescriptor(node_t node) {
+  const turboshaft::FrameStateOp& state =
+      this->turboshaft_graph()
+          ->Get(node)
+          .template Cast<turboshaft::FrameStateOp>();
+  auto* desc = GetFrameStateDescriptorInternal(instruction_zone(),
+                                               this->turboshaft_graph(), state);
+  *max_unoptimized_frame_height_ =
+      std::max(*max_unoptimized_frame_height_,
+               desc->total_conservative_frame_size_in_bytes());
+  return desc;
+}
+
+template <>
+FrameStateDescriptor*
+InstructionSelectorT<TurbofanAdapter>::GetFrameStateDescriptor(node_t node) {
+  FrameState state{node};
   auto* desc = GetFrameStateDescriptorInternal(instruction_zone(), state);
   *max_unoptimized_frame_height_ =
       std::max(*max_unoptimized_frame_height_,
@@ -4009,10 +4502,90 @@ void InstructionSelectorT<Adapter>::SwapShuffleInputs(Node* node) {
 }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
-    InstructionSelectorT<TurbofanAdapter>;
-template class EXPORT_TEMPLATE_DEFINE(V8_EXPORT_PRIVATE)
-    InstructionSelectorT<TurboshaftAdapter>;
+template class InstructionSelectorT<TurbofanAdapter>;
+template class InstructionSelectorT<TurboshaftAdapter>;
+
+// static
+InstructionSelector InstructionSelector::ForTurbofan(
+    Zone* zone, size_t node_count, Linkage* linkage,
+    InstructionSequence* sequence, Schedule* schedule,
+    SourcePositionTable* source_positions, Frame* frame,
+    EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
+    JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
+    size_t* max_pushed_argument_count, SourcePositionMode source_position_mode,
+    Features features, EnableScheduling enable_scheduling,
+    EnableRootsRelativeAddressing enable_roots_relative_addressing,
+    EnableTraceTurboJson trace_turbo) {
+  return InstructionSelector(
+      new InstructionSelectorT<TurbofanAdapter>(
+          zone, node_count, linkage, sequence, schedule, source_positions,
+          frame, enable_switch_jump_table, tick_counter, broker,
+          max_unoptimized_frame_height, max_pushed_argument_count,
+          source_position_mode, features, enable_scheduling,
+          enable_roots_relative_addressing, trace_turbo),
+      nullptr);
+}
+
+InstructionSelector InstructionSelector::ForTurboshaft(
+    Zone* zone, size_t node_count, Linkage* linkage,
+    InstructionSequence* sequence, turboshaft::Graph* schedule,
+    SourcePositionTable* source_positions, Frame* frame,
+    EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
+    JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
+    size_t* max_pushed_argument_count, SourcePositionMode source_position_mode,
+    Features features, EnableScheduling enable_scheduling,
+    EnableRootsRelativeAddressing enable_roots_relative_addressing,
+    EnableTraceTurboJson trace_turbo) {
+  return InstructionSelector(
+      nullptr,
+      new InstructionSelectorT<TurboshaftAdapter>(
+          zone, node_count, linkage, sequence, schedule, source_positions,
+          frame, enable_switch_jump_table, tick_counter, broker,
+          max_unoptimized_frame_height, max_pushed_argument_count,
+          source_position_mode, features, enable_scheduling,
+          enable_roots_relative_addressing, trace_turbo));
+}
+
+InstructionSelector::InstructionSelector(
+    InstructionSelectorT<TurbofanAdapter>* turbofan_impl,
+    InstructionSelectorT<TurboshaftAdapter>* turboshaft_impl)
+    : turbofan_impl_(turbofan_impl), turboshaft_impl_(turboshaft_impl) {
+  DCHECK_NE(!turbofan_impl_, !turboshaft_impl_);
+}
+
+InstructionSelector::~InstructionSelector() {
+  DCHECK_NE(!turbofan_impl_, !turboshaft_impl_);
+  delete turbofan_impl_;
+  delete turboshaft_impl_;
+}
+
+#define DISPATCH_TO_IMPL(...)                    \
+  DCHECK_NE(!turbofan_impl_, !turboshaft_impl_); \
+  if (turbofan_impl_) {                          \
+    return turbofan_impl_->__VA_ARGS__;          \
+  } else {                                       \
+    return turboshaft_impl_->__VA_ARGS__;        \
+  }
+
+base::Optional<BailoutReason> InstructionSelector::SelectInstructions() {
+  DISPATCH_TO_IMPL(SelectInstructions())
+}
+
+bool InstructionSelector::IsSupported(CpuFeature feature) const {
+  DISPATCH_TO_IMPL(IsSupported(feature))
+}
+
+const ZoneVector<std::pair<int, int>>& InstructionSelector::instr_origins()
+    const {
+  DISPATCH_TO_IMPL(instr_origins())
+}
+
+const std::map<NodeId, int> InstructionSelector::GetVirtualRegistersForTesting()
+    const {
+  DISPATCH_TO_IMPL(GetVirtualRegistersForTesting());
+}
+
+#undef DISPATCH_TO_IMPL
 
 }  // namespace compiler
 }  // namespace internal
diff --git a/src/compiler/backend/instruction-selector.h b/src/compiler/backend/instruction-selector.h
index 11ba995dccd..da66637189c 100644
--- a/src/compiler/backend/instruction-selector.h
+++ b/src/compiler/backend/instruction-selector.h
@@ -8,13 +8,18 @@
 #include <map>
 
 #include "src/codegen/cpu-features.h"
+#include "src/codegen/machine-type.h"
 #include "src/compiler/backend/instruction-scheduler.h"
+#include "src/compiler/backend/instruction-selector-adapter.h"
 #include "src/compiler/backend/instruction.h"
 #include "src/compiler/common-operator.h"
 #include "src/compiler/feedback-source.h"
 #include "src/compiler/linkage.h"
 #include "src/compiler/machine-operator.h"
+#include "src/compiler/node-matchers.h"
 #include "src/compiler/node.h"
+#include "src/compiler/turboshaft/operations.h"
+#include "src/compiler/turboshaft/utils.h"
 #include "src/utils/bit-vector.h"
 #include "src/zone/zone-containers.h"
 
@@ -31,65 +36,154 @@ namespace compiler {
 
 // Forward declarations.
 class BasicBlock;
-struct CallBuffer;  // TODO(bmeurer): Remove this.
+template <typename Adapter>
+struct CallBufferT;  // TODO(bmeurer): Remove this.
+template <typename Adapter>
+class InstructionSelectorT;
 class Linkage;
 template <typename Adapter>
 class OperandGeneratorT;
 class SwitchInfo;
 class StateObjectDeduplicator;
 
-struct TurbofanAdapter {};
+class V8_EXPORT_PRIVATE InstructionSelector final {
+ public:
+  enum SourcePositionMode { kCallSourcePositions, kAllSourcePositions };
+  enum EnableScheduling { kDisableScheduling, kEnableScheduling };
+  enum EnableRootsRelativeAddressing {
+    kDisableRootsRelativeAddressing,
+    kEnableRootsRelativeAddressing
+  };
+  enum EnableSwitchJumpTable {
+    kDisableSwitchJumpTable,
+    kEnableSwitchJumpTable
+  };
+  enum EnableTraceTurboJson { kDisableTraceTurboJson, kEnableTraceTurboJson };
+
+  class Features final {
+   public:
+    Features() : bits_(0) {}
+    explicit Features(unsigned bits) : bits_(bits) {}
+    explicit Features(CpuFeature f) : bits_(1u << f) {}
+    Features(CpuFeature f1, CpuFeature f2) : bits_((1u << f1) | (1u << f2)) {}
+
+    bool Contains(CpuFeature f) const { return (bits_ & (1u << f)); }
+
+   private:
+    unsigned bits_;
+  };
+
+  static InstructionSelector ForTurbofan(
+      Zone* zone, size_t node_count, Linkage* linkage,
+      InstructionSequence* sequence, Schedule* schedule,
+      SourcePositionTable* source_positions, Frame* frame,
+      EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
+      JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
+      size_t* max_pushed_argument_count,
+      SourcePositionMode source_position_mode = kCallSourcePositions,
+      Features features = SupportedFeatures(),
+      EnableScheduling enable_scheduling = v8_flags.turbo_instruction_scheduling
+                                               ? kEnableScheduling
+                                               : kDisableScheduling,
+      EnableRootsRelativeAddressing enable_roots_relative_addressing =
+          kDisableRootsRelativeAddressing,
+      EnableTraceTurboJson trace_turbo = kDisableTraceTurboJson);
+
+  static InstructionSelector ForTurboshaft(
+      Zone* zone, size_t node_count, Linkage* linkage,
+      InstructionSequence* sequence, turboshaft::Graph* schedule,
+      SourcePositionTable* source_positions, Frame* frame,
+      EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
+      JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
+      size_t* max_pushed_argument_count,
+      SourcePositionMode source_position_mode = kCallSourcePositions,
+      Features features = SupportedFeatures(),
+      EnableScheduling enable_scheduling = v8_flags.turbo_instruction_scheduling
+                                               ? kEnableScheduling
+                                               : kDisableScheduling,
+      EnableRootsRelativeAddressing enable_roots_relative_addressing =
+          kDisableRootsRelativeAddressing,
+      EnableTraceTurboJson trace_turbo = kDisableTraceTurboJson);
+
+  ~InstructionSelector();
 
-struct TurboshaftAdapter {};
+  base::Optional<BailoutReason> SelectInstructions();
+
+  bool IsSupported(CpuFeature feature) const;
+
+  // Returns the features supported on the target platform.
+  static Features SupportedFeatures() {
+    return Features(CpuFeatures::SupportedFeatures());
+  }
+
+  const ZoneVector<std::pair<int, int>>& instr_origins() const;
+  const std::map<NodeId, int> GetVirtualRegistersForTesting() const;
+
+  static MachineOperatorBuilder::Flags SupportedMachineOperatorFlags();
+  static MachineOperatorBuilder::AlignmentRequirements AlignmentRequirements();
+
+ private:
+  InstructionSelector(InstructionSelectorT<TurbofanAdapter>* turbofan_impl,
+                      InstructionSelectorT<TurboshaftAdapter>* turboshaft_impl);
+  InstructionSelector(const InstructionSelector&) = delete;
+  InstructionSelector& operator=(const InstructionSelector&) = delete;
+
+  InstructionSelectorT<TurbofanAdapter>* turbofan_impl_;
+  InstructionSelectorT<TurboshaftAdapter>* turboshaft_impl_;
+};
 
 // The flags continuation is a way to combine a branch or a materialization
 // of a boolean value with an instruction that sets the flags register.
 // The whole instruction is treated as a unit by the register allocator, and
 // thus no spills or moves can be introduced between the flags-setting
 // instruction and the branch or set it should be combined with.
-class FlagsContinuation final {
+template <typename Adapter>
+class FlagsContinuationT final {
  public:
-  FlagsContinuation() : mode_(kFlags_none) {}
+  using block_t = typename Adapter::block_t;
+  using node_t = typename Adapter::node_t;
+  using id_t = typename Adapter::id_t;
+
+  FlagsContinuationT() : mode_(kFlags_none) {}
 
   // Creates a new flags continuation from the given condition and true/false
   // blocks.
-  static FlagsContinuation ForBranch(FlagsCondition condition,
-                                     BasicBlock* true_block,
-                                     BasicBlock* false_block) {
-    return FlagsContinuation(kFlags_branch, condition, true_block, false_block);
+  static FlagsContinuationT ForBranch(FlagsCondition condition,
+                                      block_t true_block, block_t false_block) {
+    return FlagsContinuationT(kFlags_branch, condition, true_block,
+                              false_block);
   }
 
   // Creates a new flags continuation for an eager deoptimization exit.
-  static FlagsContinuation ForDeoptimize(FlagsCondition condition,
-                                         DeoptimizeReason reason,
-                                         NodeId node_id,
-                                         FeedbackSource const& feedback,
-                                         FrameState frame_state) {
-    return FlagsContinuation(kFlags_deoptimize, condition, reason, node_id,
-                             feedback, frame_state);
-  }
-  static FlagsContinuation ForDeoptimizeForTesting(
-      FlagsCondition condition, DeoptimizeReason reason, NodeId node_id,
-      FeedbackSource const& feedback, Node* frame_state) {
+  static FlagsContinuationT ForDeoptimize(FlagsCondition condition,
+                                          DeoptimizeReason reason, id_t node_id,
+                                          FeedbackSource const& feedback,
+                                          node_t frame_state) {
+    return FlagsContinuationT(kFlags_deoptimize, condition, reason, node_id,
+                              feedback, frame_state);
+  }
+  static FlagsContinuationT ForDeoptimizeForTesting(
+      FlagsCondition condition, DeoptimizeReason reason, id_t node_id,
+      FeedbackSource const& feedback, node_t frame_state) {
     // test-instruction-scheduler.cc passes a dummy Node* as frame_state.
     // Contents don't matter as long as it's not nullptr.
-    return FlagsContinuation(kFlags_deoptimize, condition, reason, node_id,
-                             feedback, frame_state);
+    return FlagsContinuationT(kFlags_deoptimize, condition, reason, node_id,
+                              feedback, frame_state);
   }
 
   // Creates a new flags continuation for a boolean value.
-  static FlagsContinuation ForSet(FlagsCondition condition, Node* result) {
-    return FlagsContinuation(condition, result);
+  static FlagsContinuationT ForSet(FlagsCondition condition, node_t result) {
+    return FlagsContinuationT(condition, result);
   }
 
   // Creates a new flags continuation for a wasm trap.
-  static FlagsContinuation ForTrap(FlagsCondition condition, TrapId trap_id) {
-    return FlagsContinuation(condition, trap_id);
+  static FlagsContinuationT ForTrap(FlagsCondition condition, TrapId trap_id) {
+    return FlagsContinuationT(condition, trap_id);
   }
 
-  static FlagsContinuation ForSelect(FlagsCondition condition, Node* result,
-                                     Node* true_value, Node* false_value) {
-    return FlagsContinuation(condition, result, true_value, false_value);
+  static FlagsContinuationT ForSelect(FlagsCondition condition, node_t result,
+                                      node_t true_value, node_t false_value) {
+    return FlagsContinuationT(condition, result, true_value, false_value);
   }
 
   bool IsNone() const { return mode_ == kFlags_none; }
@@ -106,7 +200,7 @@ class FlagsContinuation final {
     DCHECK(IsDeoptimize());
     return reason_;
   }
-  NodeId node_id() const {
+  id_t node_id() const {
     DCHECK(IsDeoptimize());
     return node_id_;
   }
@@ -114,11 +208,11 @@ class FlagsContinuation final {
     DCHECK(IsDeoptimize());
     return feedback_;
   }
-  Node* frame_state() const {
+  node_t frame_state() const {
     DCHECK(IsDeoptimize());
     return frame_state_or_result_;
   }
-  Node* result() const {
+  node_t result() const {
     DCHECK(IsSet() || IsSelect());
     return frame_state_or_result_;
   }
@@ -126,19 +220,19 @@ class FlagsContinuation final {
     DCHECK(IsTrap());
     return trap_id_;
   }
-  BasicBlock* true_block() const {
+  block_t true_block() const {
     DCHECK(IsBranch());
     return true_block_;
   }
-  BasicBlock* false_block() const {
+  block_t false_block() const {
     DCHECK(IsBranch());
     return false_block_;
   }
-  Node* true_value() const {
+  node_t true_value() const {
     DCHECK(IsSelect());
     return true_value_;
   }
-  Node* false_value() const {
+  node_t false_value() const {
     DCHECK(IsSelect());
     return false_value_;
   }
@@ -191,8 +285,8 @@ class FlagsContinuation final {
   }
 
  private:
-  FlagsContinuation(FlagsMode mode, FlagsCondition condition,
-                    BasicBlock* true_block, BasicBlock* false_block)
+  FlagsContinuationT(FlagsMode mode, FlagsCondition condition,
+                     block_t true_block, block_t false_block)
       : mode_(mode),
         condition_(condition),
         true_block_(true_block),
@@ -202,9 +296,9 @@ class FlagsContinuation final {
     DCHECK_NOT_NULL(false_block);
   }
 
-  FlagsContinuation(FlagsMode mode, FlagsCondition condition,
-                    DeoptimizeReason reason, NodeId node_id,
-                    FeedbackSource const& feedback, Node* frame_state)
+  FlagsContinuationT(FlagsMode mode, FlagsCondition condition,
+                     DeoptimizeReason reason, id_t node_id,
+                     FeedbackSource const& feedback, node_t frame_state)
       : mode_(mode),
         condition_(condition),
         reason_(reason),
@@ -212,53 +306,55 @@ class FlagsContinuation final {
         feedback_(feedback),
         frame_state_or_result_(frame_state) {
     DCHECK(mode == kFlags_deoptimize);
-    DCHECK_NOT_NULL(frame_state);
+    DCHECK(Adapter::valid(frame_state));
   }
 
-  FlagsContinuation(FlagsCondition condition, Node* result)
+  FlagsContinuationT(FlagsCondition condition, node_t result)
       : mode_(kFlags_set),
         condition_(condition),
         frame_state_or_result_(result) {
-    DCHECK_NOT_NULL(result);
+    DCHECK(Adapter::valid(result));
   }
 
-  FlagsContinuation(FlagsCondition condition, TrapId trap_id)
+  FlagsContinuationT(FlagsCondition condition, TrapId trap_id)
       : mode_(kFlags_trap), condition_(condition), trap_id_(trap_id) {}
 
-  FlagsContinuation(FlagsCondition condition, Node* result, Node* true_value,
-                    Node* false_value)
+  FlagsContinuationT(FlagsCondition condition, node_t result, node_t true_value,
+                     node_t false_value)
       : mode_(kFlags_select),
         condition_(condition),
         frame_state_or_result_(result),
         true_value_(true_value),
         false_value_(false_value) {
-    DCHECK_NOT_NULL(result);
-    DCHECK_NOT_NULL(true_value);
-    DCHECK_NOT_NULL(false_value);
+    DCHECK(Adapter::valid(result));
+    DCHECK(Adapter::valid(true_value));
+    DCHECK(Adapter::valid(false_value));
   }
 
   FlagsMode const mode_;
   FlagsCondition condition_;
   DeoptimizeReason reason_;         // Only valid if mode_ == kFlags_deoptimize*
-  NodeId node_id_;                  // Only valid if mode_ == kFlags_deoptimize*
+  id_t node_id_;                    // Only valid if mode_ == kFlags_deoptimize*
   FeedbackSource feedback_;         // Only valid if mode_ == kFlags_deoptimize*
-  Node* frame_state_or_result_;     // Only valid if mode_ == kFlags_deoptimize*
+  node_t frame_state_or_result_;    // Only valid if mode_ == kFlags_deoptimize*
                                     // or mode_ == kFlags_set.
-  BasicBlock* true_block_;          // Only valid if mode_ == kFlags_branch*.
-  BasicBlock* false_block_;         // Only valid if mode_ == kFlags_branch*.
+  block_t true_block_;              // Only valid if mode_ == kFlags_branch*.
+  block_t false_block_;             // Only valid if mode_ == kFlags_branch*.
   TrapId trap_id_;                  // Only valid if mode_ == kFlags_trap.
-  Node* true_value_;                // Only valid if mode_ == kFlags_select.
-  Node* false_value_;               // Only valid if mode_ == kFlags_select.
+  node_t true_value_;               // Only valid if mode_ == kFlags_select.
+  node_t false_value_;              // Only valid if mode_ == kFlags_select.
 };
 
 // This struct connects nodes of parameters which are going to be pushed on the
 // call stack with their parameter index in the call descriptor of the callee.
-struct PushParameter {
-  PushParameter(Node* n = nullptr,
-                LinkageLocation l = LinkageLocation::ForAnyRegister())
+template <typename Adapter>
+struct PushParameterT {
+  using node_t = typename Adapter::node_t;
+  PushParameterT(node_t n = {},
+                 LinkageLocation l = LinkageLocation::ForAnyRegister())
       : node(n), location(l) {}
 
-  Node* node;
+  node_t node;
   LinkageLocation location;
 };
 
@@ -266,39 +362,40 @@ enum class FrameStateInputKind { kAny, kStackSlot };
 
 // Instruction selection generates an InstructionSequence for a given Schedule.
 template <typename Adapter>
-class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
+class InstructionSelectorT final : public Adapter {
  public:
   using OperandGenerator = OperandGeneratorT<Adapter>;
-  // Forward declarations.
-  class Features;
+  using PushParameter = PushParameterT<Adapter>;
+  using CallBuffer = CallBufferT<Adapter>;
+  using FlagsContinuation = FlagsContinuationT<Adapter>;
 
-  enum SourcePositionMode { kCallSourcePositions, kAllSourcePositions };
-  enum EnableScheduling { kDisableScheduling, kEnableScheduling };
-  enum EnableRootsRelativeAddressing {
-    kDisableRootsRelativeAddressing,
-    kEnableRootsRelativeAddressing
-  };
-  enum EnableSwitchJumpTable {
-    kDisableSwitchJumpTable,
-    kEnableSwitchJumpTable
-  };
-  enum EnableTraceTurboJson { kDisableTraceTurboJson, kEnableTraceTurboJson };
+  using schedule_t = typename Adapter::schedule_t;
+  using block_t = typename Adapter::block_t;
+  using block_range_t = typename Adapter::block_range_t;
+  using node_t = typename Adapter::node_t;
+  using id_t = typename Adapter::id_t;
+
+  using Features = InstructionSelector::Features;
 
   InstructionSelectorT(
       Zone* zone, size_t node_count, Linkage* linkage,
-      InstructionSequence* sequence, Schedule* schedule,
+      InstructionSequence* sequence, schedule_t schedule,
       SourcePositionTable* source_positions, Frame* frame,
-      EnableSwitchJumpTable enable_switch_jump_table, TickCounter* tick_counter,
-      JSHeapBroker* broker, size_t* max_unoptimized_frame_height,
-      size_t* max_pushed_argument_count,
-      SourcePositionMode source_position_mode = kCallSourcePositions,
+      InstructionSelector::EnableSwitchJumpTable enable_switch_jump_table,
+      TickCounter* tick_counter, JSHeapBroker* broker,
+      size_t* max_unoptimized_frame_height, size_t* max_pushed_argument_count,
+      InstructionSelector::SourcePositionMode source_position_mode =
+          InstructionSelector::kCallSourcePositions,
       Features features = SupportedFeatures(),
-      EnableScheduling enable_scheduling = v8_flags.turbo_instruction_scheduling
-                                               ? kEnableScheduling
-                                               : kDisableScheduling,
-      EnableRootsRelativeAddressing enable_roots_relative_addressing =
-          kDisableRootsRelativeAddressing,
-      EnableTraceTurboJson trace_turbo = kDisableTraceTurboJson);
+      InstructionSelector::EnableScheduling enable_scheduling =
+          v8_flags.turbo_instruction_scheduling
+              ? InstructionSelector::kEnableScheduling
+              : InstructionSelector::kDisableScheduling,
+      InstructionSelector::EnableRootsRelativeAddressing
+          enable_roots_relative_addressing =
+              InstructionSelector::kDisableRootsRelativeAddressing,
+      InstructionSelector::EnableTraceTurboJson trace_turbo =
+          InstructionSelector::kDisableTraceTurboJson);
 
   // Visit code for the entire graph with the included schedule.
   base::Optional<BailoutReason> SelectInstructions();
@@ -374,19 +471,6 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // ============== Architecture-independent CPU feature methods. ==============
   // ===========================================================================
 
-  class Features final {
-   public:
-    Features() : bits_(0) {}
-    explicit Features(unsigned bits) : bits_(bits) {}
-    explicit Features(CpuFeature f) : bits_(1u << f) {}
-    Features(CpuFeature f1, CpuFeature f2) : bits_((1u << f1) | (1u << f2)) {}
-
-    bool Contains(CpuFeature f) const { return (bits_ & (1u << f)); }
-
-   private:
-    unsigned bits_;
-  };
-
   bool IsSupported(CpuFeature feature) const {
     return features_.Contains(feature);
   }
@@ -396,11 +480,6 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
     return Features(CpuFeatures::SupportedFeatures());
   }
 
-  // TODO(sigurds) This should take a CpuFeatures argument.
-  static MachineOperatorBuilder::Flags SupportedMachineOperatorFlags();
-
-  static MachineOperatorBuilder::AlignmentRequirements AlignmentRequirements();
-
   // ===========================================================================
   // ============ Architecture-independent graph covering methods. =============
   // ===========================================================================
@@ -413,7 +492,8 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // For pure nodes, CanCover(a,b) is checked to avoid duplicated execution:
   // If this is not the case, code for b must still be generated for other
   // users, and fusing is unlikely to improve performance.
-  bool CanCover(Node* user, Node* node) const;
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanCover)
+  bool CanCover(node_t user, node_t node) const;
 
   // Used in pattern matching during code generation.
   // This function checks that {node} and {user} are in the same basic block,
@@ -438,27 +518,32 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // visiting Word32Compare, we would then have to select an instruction for
   // OtherOp *afterwards*, which means we would attempt to use the result of
   // the add before we have defined it.
-  bool IsOnlyUserOfNodeInSameBlock(Node* user, Node* node) const;
+  bool IsOnlyUserOfNodeInSameBlock(node_t user, node_t node) const;
 
   // Checks if {node} was already defined, and therefore code was already
   // generated for it.
-  bool IsDefined(Node* node) const;
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, IsDefined)
+  bool IsDefined(node_t node) const;
 
   // Checks if {node} has any uses, and therefore code has to be generated for
   // it.
-  bool IsUsed(Node* node) const;
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, IsUsed)
+  bool IsUsed(node_t node) const;
 
   // Checks if {node} is currently live.
-  bool IsLive(Node* node) const { return !IsDefined(node) && IsUsed(node); }
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, IsLive)
+  bool IsLive(node_t node) const { return !IsDefined(node) && IsUsed(node); }
 
   // Gets the effect level of {node}.
-  int GetEffectLevel(Node* node) const;
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(int, GetEffectLevel)
+  int GetEffectLevel(node_t node) const;
 
   // Gets the effect level of {node}, appropriately adjusted based on
   // continuation flags if the node is a branch.
-  int GetEffectLevel(Node* node, FlagsContinuation* cont) const;
+  int GetEffectLevel(node_t node, FlagsContinuation* cont) const;
 
-  int GetVirtualRegister(const Node* node);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(int, GetVirtualRegister)
+  int GetVirtualRegister(node_t node);
   const std::map<NodeId, int> GetVirtualRegistersForTesting() const;
 
   // Check if we can generate loads and stores of ExternalConstants relative
@@ -478,14 +563,14 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   friend class OperandGeneratorT<Adapter>;
 
   bool UseInstructionScheduling() const {
-    return (enable_scheduling_ == kEnableScheduling) &&
+    return (enable_scheduling_ == InstructionSelector::kEnableScheduling) &&
            InstructionScheduler::SchedulerSupported();
   }
 
   void AppendDeoptimizeArguments(InstructionOperandVector* args,
-                                 DeoptimizeReason reason, NodeId node_id,
+                                 DeoptimizeReason reason, id_t node_id,
                                  FeedbackSource const& feedback,
-                                 FrameState frame_state,
+                                 node_t frame_state,
                                  DeoptimizeKind kind = DeoptimizeKind::kEager);
 
   void EmitTableSwitch(const SwitchInfo& sw,
@@ -495,45 +580,58 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
 
   void TryRename(InstructionOperand* op);
   int GetRename(int virtual_register);
-  void SetRename(const Node* node, const Node* rename);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, SetRename)
+  void SetRename(node_t node, node_t rename);
   void UpdateRenames(Instruction* instruction);
   void UpdateRenamesInPhi(PhiInstruction* phi);
 
   // Inform the instruction selection that {node} was just defined.
-  void MarkAsDefined(Node* node);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsDefined)
+  void MarkAsDefined(node_t node);
 
   // Inform the instruction selection that {node} has at least one use and we
   // will need to generate code for it.
-  void MarkAsUsed(Node* node);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsUsed)
+  void MarkAsUsed(node_t node);
 
   // Sets the effect level of {node}.
-  void SetEffectLevel(Node* node, int effect_level);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, SetEffectLevel)
+  void SetEffectLevel(node_t node, int effect_level);
 
   // Inform the register allocation of the representation of the value produced
   // by {node}.
-  void MarkAsRepresentation(MachineRepresentation rep, Node* node);
-  void MarkAsWord32(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsRepresentation)
+  void MarkAsRepresentation(MachineRepresentation rep, node_t node);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsWord32)
+  void MarkAsWord32(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kWord32, node);
   }
-  void MarkAsWord64(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsWord64)
+  void MarkAsWord64(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kWord64, node);
   }
-  void MarkAsFloat32(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsFloat32)
+  void MarkAsFloat32(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kFloat32, node);
   }
-  void MarkAsFloat64(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsFloat64)
+  void MarkAsFloat64(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kFloat64, node);
   }
-  void MarkAsSimd128(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsSimd128)
+  void MarkAsSimd128(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kSimd128, node);
   }
-  void MarkAsSimd256(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsSimd256)
+  void MarkAsSimd256(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kSimd256, node);
   }
-  void MarkAsTagged(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsTagged)
+  void MarkAsTagged(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kTagged, node);
   }
-  void MarkAsCompressed(Node* node) {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, MarkAsCompressed)
+  void MarkAsCompressed(node_t node) {
     MarkAsRepresentation(MachineRepresentation::kCompressed, node);
   }
 
@@ -555,15 +653,15 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // to the inputs and outputs of the call.
   // {call_code_immediate} to generate immediate operands to calls of code.
   // {call_address_immediate} to generate immediate operands to address calls.
-  void InitializeCallBuffer(Node* call, CallBuffer* buffer,
+  void InitializeCallBuffer(node_t call, CallBuffer* buffer,
                             CallBufferFlags flags, int stack_slot_delta = 0);
   bool IsTailCallAddressImmediate();
 
   void UpdateMaxPushedArgumentCount(size_t count);
 
-  FrameStateDescriptor* GetFrameStateDescriptor(FrameState node);
+  FrameStateDescriptor* GetFrameStateDescriptor(node_t node);
   size_t AddInputsToFrameStateDescriptor(FrameStateDescriptor* descriptor,
-                                         FrameState state, OperandGenerator* g,
+                                         node_t state, OperandGenerator* g,
                                          StateObjectDeduplicator* deduplicator,
                                          InstructionOperandVector* inputs,
                                          FrameStateInputKind kind, Zone* zone);
@@ -571,13 +669,13 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
                                          InstructionOperandVector* inputs,
                                          OperandGenerator* g,
                                          StateObjectDeduplicator* deduplicator,
-                                         Node* node, FrameStateInputKind kind,
+                                         node_t node, FrameStateInputKind kind,
                                          Zone* zone);
   size_t AddOperandToStateValueDescriptor(StateValueList* values,
                                           InstructionOperandVector* inputs,
                                           OperandGenerator* g,
                                           StateObjectDeduplicator* deduplicator,
-                                          Node* input, MachineType type,
+                                          node_t input, MachineType type,
                                           FrameStateInputKind kind, Zone* zone);
 
   // ===========================================================================
@@ -585,65 +683,186 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // ===========================================================================
 
   // Visit nodes in the given block and generate code.
-  void VisitBlock(BasicBlock* block);
+  void VisitBlock(block_t block);
 
   // Visit the node for the control flow at the end of the block, generating
   // code if necessary.
-  void VisitControl(BasicBlock* block);
+  void VisitControl(block_t block);
 
   // Visit the node and generate code, if any.
-  void VisitNode(Node* node);
+  void VisitNode(node_t node);
 
   // Visit the node and generate code for IEEE 754 functions.
   void VisitFloat64Ieee754Binop(Node*, InstructionCode code);
   void VisitFloat64Ieee754Unop(Node*, InstructionCode code);
 
+#define DECLARE_GENERATOR_T(x) void Visit##x(node_t node);
+  DECLARE_GENERATOR_T(Word32Equal)
+  DECLARE_GENERATOR_T(Int32LessThan)
+  DECLARE_GENERATOR_T(Int32LessThanOrEqual)
+  DECLARE_GENERATOR_T(Uint32LessThan)
+  DECLARE_GENERATOR_T(Uint32LessThanOrEqual)
+  DECLARE_GENERATOR_T(Float32Equal)
+  DECLARE_GENERATOR_T(Float32LessThan)
+  DECLARE_GENERATOR_T(Float32LessThanOrEqual)
+  DECLARE_GENERATOR_T(Float64Equal)
+  DECLARE_GENERATOR_T(Float64LessThan)
+  DECLARE_GENERATOR_T(Float64LessThanOrEqual)
+  DECLARE_GENERATOR_T(Load)
+  DECLARE_GENERATOR_T(StackPointerGreaterThan)
+#undef DECLARE_GENERATOR_T
+
 #define DECLARE_GENERATOR(x) void Visit##x(Node* node);
-  MACHINE_OP_LIST(DECLARE_GENERATOR)
+  // MACHINE_OP_LIST
+  MACHINE_UNOP_32_LIST(DECLARE_GENERATOR)
+  MACHINE_BINOP_32_LIST(DECLARE_GENERATOR)
+  MACHINE_BINOP_64_LIST(DECLARE_GENERATOR)
+  // MACHINE_COMPARE_BINOP_LIST
+  DECLARE_GENERATOR(Word64Equal)
+  DECLARE_GENERATOR(Int64LessThan)
+  DECLARE_GENERATOR(Int64LessThanOrEqual)
+  DECLARE_GENERATOR(Uint64LessThan)
+  DECLARE_GENERATOR(Uint64LessThanOrEqual)
+  // END MACHINE_COMPARE_BINOP_LIST
+  MACHINE_FLOAT32_BINOP_LIST(DECLARE_GENERATOR)
+  MACHINE_FLOAT32_UNOP_LIST(DECLARE_GENERATOR)
+  MACHINE_FLOAT64_BINOP_LIST(DECLARE_GENERATOR)
+  MACHINE_FLOAT64_UNOP_LIST(DECLARE_GENERATOR)
+  MACHINE_ATOMIC_OP_LIST(DECLARE_GENERATOR)
+  DECLARE_GENERATOR(AbortCSADcheck)
+  DECLARE_GENERATOR(DebugBreak)
+  DECLARE_GENERATOR(Comment)
+  DECLARE_GENERATOR(LoadImmutable)
+  DECLARE_GENERATOR(Store)
+  DECLARE_GENERATOR(StorePair)
+  DECLARE_GENERATOR(StackSlot)
+  DECLARE_GENERATOR(Word32Popcnt)
+  DECLARE_GENERATOR(Word64Popcnt)
+  DECLARE_GENERATOR(Word64Clz)
+  DECLARE_GENERATOR(Word64Ctz)
+  DECLARE_GENERATOR(Word64ClzLowerable)
+  DECLARE_GENERATOR(Word64CtzLowerable)
+  DECLARE_GENERATOR(Word64ReverseBits)
+  DECLARE_GENERATOR(Word64ReverseBytes)
+  DECLARE_GENERATOR(Simd128ReverseBytes)
+  DECLARE_GENERATOR(Int64AbsWithOverflow)
+  DECLARE_GENERATOR(BitcastTaggedToWord)
+  DECLARE_GENERATOR(BitcastTaggedToWordForTagAndSmiBits)
+  DECLARE_GENERATOR(BitcastWordToTagged)
+  DECLARE_GENERATOR(BitcastWordToTaggedSigned)
+  DECLARE_GENERATOR(TruncateFloat64ToWord32)
+  DECLARE_GENERATOR(ChangeFloat32ToFloat64)
+  DECLARE_GENERATOR(ChangeFloat64ToInt32)
+  DECLARE_GENERATOR(ChangeFloat64ToInt64)
+  DECLARE_GENERATOR(ChangeFloat64ToUint32)
+  DECLARE_GENERATOR(ChangeFloat64ToUint64)
+  DECLARE_GENERATOR(Float64SilenceNaN)
+  DECLARE_GENERATOR(TruncateFloat64ToInt64)
+  DECLARE_GENERATOR(TruncateFloat64ToUint32)
+  DECLARE_GENERATOR(TruncateFloat32ToInt32)
+  DECLARE_GENERATOR(TruncateFloat32ToUint32)
+  DECLARE_GENERATOR(TryTruncateFloat32ToInt64)
+  DECLARE_GENERATOR(TryTruncateFloat64ToInt64)
+  DECLARE_GENERATOR(TryTruncateFloat32ToUint64)
+  DECLARE_GENERATOR(TryTruncateFloat64ToUint64)
+  DECLARE_GENERATOR(TryTruncateFloat64ToInt32)
+  DECLARE_GENERATOR(TryTruncateFloat64ToUint32)
+  DECLARE_GENERATOR(ChangeInt32ToFloat64)
+  DECLARE_GENERATOR(BitcastWord32ToWord64)
+  DECLARE_GENERATOR(ChangeInt32ToInt64)
+  DECLARE_GENERATOR(ChangeInt64ToFloat64)
+  DECLARE_GENERATOR(ChangeUint32ToFloat64)
+  DECLARE_GENERATOR(ChangeUint32ToUint64)
+  DECLARE_GENERATOR(TruncateFloat64ToFloat32)
+  DECLARE_GENERATOR(TruncateInt64ToInt32)
+  DECLARE_GENERATOR(RoundFloat64ToInt32)
+  DECLARE_GENERATOR(RoundInt32ToFloat32)
+  DECLARE_GENERATOR(RoundInt64ToFloat32)
+  DECLARE_GENERATOR(RoundInt64ToFloat64)
+  DECLARE_GENERATOR(RoundUint32ToFloat32)
+  DECLARE_GENERATOR(RoundUint64ToFloat32)
+  DECLARE_GENERATOR(RoundUint64ToFloat64)
+  DECLARE_GENERATOR(BitcastFloat32ToInt32)
+  DECLARE_GENERATOR(BitcastFloat64ToInt64)
+  DECLARE_GENERATOR(BitcastInt32ToFloat32)
+  DECLARE_GENERATOR(BitcastInt64ToFloat64)
+  DECLARE_GENERATOR(Float64ExtractLowWord32)
+  DECLARE_GENERATOR(Float64ExtractHighWord32)
+  DECLARE_GENERATOR(Float64InsertLowWord32)
+  DECLARE_GENERATOR(Float64InsertHighWord32)
+  DECLARE_GENERATOR(Word32Select)
+  DECLARE_GENERATOR(Word64Select)
+  DECLARE_GENERATOR(Float32Select)
+  DECLARE_GENERATOR(Float64Select)
+  DECLARE_GENERATOR(LoadStackCheckOffset)
+  DECLARE_GENERATOR(LoadFramePointer)
+  DECLARE_GENERATOR(LoadParentFramePointer)
+  DECLARE_GENERATOR(LoadRootRegister)
+  DECLARE_GENERATOR(UnalignedLoad)
+  DECLARE_GENERATOR(UnalignedStore)
+  DECLARE_GENERATOR(Int32PairAdd)
+  DECLARE_GENERATOR(Int32PairSub)
+  DECLARE_GENERATOR(Int32PairMul)
+  DECLARE_GENERATOR(Word32PairShl)
+  DECLARE_GENERATOR(Word32PairShr)
+  DECLARE_GENERATOR(Word32PairSar)
+  DECLARE_GENERATOR(ProtectedLoad)
+  DECLARE_GENERATOR(ProtectedStore)
+  DECLARE_GENERATOR(LoadTrapOnNull)
+  DECLARE_GENERATOR(StoreTrapOnNull)
+  DECLARE_GENERATOR(MemoryBarrier)
+  DECLARE_GENERATOR(SignExtendWord8ToInt32)
+  DECLARE_GENERATOR(SignExtendWord16ToInt32)
+  DECLARE_GENERATOR(SignExtendWord8ToInt64)
+  DECLARE_GENERATOR(SignExtendWord16ToInt64)
+  DECLARE_GENERATOR(SignExtendWord32ToInt64)
+  DECLARE_GENERATOR(TraceInstruction)
+  // END MACHINE_OP_LIST
   MACHINE_SIMD128_OP_LIST(DECLARE_GENERATOR)
   MACHINE_SIMD256_OP_LIST(DECLARE_GENERATOR)
 #undef DECLARE_GENERATOR
 
   // Visit the load node with a value and opcode to replace with.
-  void VisitLoad(Node* node, Node* value, InstructionCode opcode);
+  void VisitLoad(node_t node, node_t value, InstructionCode opcode);
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void, VisitLoad)
   void VisitLoadTransform(Node* node, Node* value, InstructionCode opcode);
   void VisitFinishRegion(Node* node);
-  void VisitParameter(Node* node);
+  void VisitParameter(node_t node);
   void VisitIfException(Node* node);
   void VisitOsrValue(Node* node);
   void VisitPhi(Node* node);
   void VisitProjection(Node* node);
-  void VisitConstant(Node* node);
-  void VisitCall(Node* call, BasicBlock* handler = nullptr);
+  void VisitConstant(node_t node);
+  void VisitCall(node_t call, block_t handler = {});
   void VisitDeoptimizeIf(Node* node);
   void VisitDeoptimizeUnless(Node* node);
   void VisitDynamicCheckMapsWithDeoptUnless(Node* node);
-  void VisitTrapIf(Node* node, TrapId trap_id);
-  void VisitTrapUnless(Node* node, TrapId trap_id);
+  void VisitTrapIf(node_t node, TrapId trap_id);
+  void VisitTrapUnless(node_t node, TrapId trap_id);
   void VisitTailCall(Node* call);
-  void VisitGoto(BasicBlock* target);
-  void VisitBranch(Node* input, BasicBlock* tbranch, BasicBlock* fbranch);
+  void VisitGoto(block_t target);
+  void VisitBranch(node_t input, block_t tbranch, block_t fbranch);
   void VisitSwitch(Node* node, const SwitchInfo& sw);
-  void VisitDeoptimize(DeoptimizeReason reason, NodeId node_id,
-                       FeedbackSource const& feedback, FrameState frame_state);
+  void VisitDeoptimize(DeoptimizeReason reason, id_t node_id,
+                       FeedbackSource const& feedback, node_t frame_state);
   void VisitSelect(Node* node);
-  void VisitReturn(Node* ret);
+  void VisitReturn(node_t node);
   void VisitThrow(Node* node);
   void VisitRetain(Node* node);
   void VisitUnreachable(Node* node);
   void VisitStaticAssert(Node* node);
   void VisitDeadValue(Node* node);
 
-  void TryPrepareScheduleFirstProjection(Node* maybe_projection);
+  void TryPrepareScheduleFirstProjection(node_t maybe_projection);
 
-  void VisitStackPointerGreaterThan(Node* node, FlagsContinuation* cont);
+  void VisitStackPointerGreaterThan(node_t node, FlagsContinuation* cont);
 
-  void VisitWordCompareZero(Node* user, Node* value, FlagsContinuation* cont);
+  void VisitWordCompareZero(node_t user, node_t value, FlagsContinuation* cont);
 
-  void EmitPrepareArguments(ZoneVector<compiler::PushParameter>* arguments,
-                            const CallDescriptor* call_descriptor, Node* node);
-  void EmitPrepareResults(ZoneVector<compiler::PushParameter>* results,
-                          const CallDescriptor* call_descriptor, Node* node);
+  void EmitPrepareArguments(ZoneVector<PushParameter>* arguments,
+                            const CallDescriptor* call_descriptor, node_t node);
+  void EmitPrepareResults(ZoneVector<PushParameter>* results,
+                          const CallDescriptor* call_descriptor, node_t node);
 
   // In LOONG64, calling convention uses free GP param register to pass
   // floating-point arguments when no FP param register is available. But
@@ -653,12 +872,12 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   // Moving floating-point param from GP param register to FPR to participate in
   // subsequent operations, whether CallCFunction or normal floating-point
   // operations.
-  void EmitMoveParamToFPR(Node* node, int index);
+  void EmitMoveParamToFPR(node_t node, int index);
 
   bool CanProduceSignalingNaN(Node* node);
 
   void AddOutputToSelectContinuation(OperandGenerator* g, int first_input_index,
-                                     Node* node);
+                                     node_t node);
 
   // ===========================================================================
   // ============= Vector instruction (SIMD) helper fns. =======================
@@ -676,7 +895,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
 
   // ===========================================================================
 
-  Schedule* schedule() const { return schedule_; }
+  schedule_t schedule() const { return schedule_; }
   Linkage* linkage() const { return linkage_; }
   InstructionSequence* sequence() const { return sequence_; }
   Zone* instruction_zone() const { return sequence()->zone(); }
@@ -713,10 +932,10 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
 #endif  // V8_TARGET_ARCH_64_BIT
 
   struct FrameStateInput {
-    FrameStateInput(Node* node_, FrameStateInputKind kind_)
+    FrameStateInput(node_t node_, FrameStateInputKind kind_)
         : node(node_), kind(kind_) {}
 
-    Node* node;
+    node_t node;
     FrameStateInputKind kind;
 
     struct Hash {
@@ -743,10 +962,10 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   Linkage* const linkage_;
   InstructionSequence* const sequence_;
   SourcePositionTable* const source_positions_;
-  SourcePositionMode const source_position_mode_;
+  InstructionSelector::SourcePositionMode const source_position_mode_;
   Features features_;
-  Schedule* const schedule_;
-  BasicBlock* current_block_;
+  schedule_t const schedule_;
+  block_t current_block_;
   ZoneVector<Instruction*> instructions_;
   InstructionOperandVector continuation_inputs_;
   InstructionOperandVector continuation_outputs_;
@@ -758,9 +977,10 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   IntVector virtual_registers_;
   IntVector virtual_register_rename_;
   InstructionScheduler* scheduler_;
-  EnableScheduling enable_scheduling_;
-  EnableRootsRelativeAddressing enable_roots_relative_addressing_;
-  EnableSwitchJumpTable enable_switch_jump_table_;
+  InstructionSelector::EnableScheduling enable_scheduling_;
+  InstructionSelector::EnableRootsRelativeAddressing
+      enable_roots_relative_addressing_;
+  InstructionSelector::EnableSwitchJumpTable enable_switch_jump_table_;
   ZoneUnorderedMap<FrameStateInput, CachedStateValues*,
                    typename FrameStateInput::Hash,
                    typename FrameStateInput::Equal>
@@ -769,7 +989,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
   Frame* frame_;
   bool instruction_selection_failed_;
   ZoneVector<std::pair<int, int>> instr_origins_;
-  EnableTraceTurboJson trace_turbo_;
+  InstructionSelector::EnableTraceTurboJson trace_turbo_;
   TickCounter* const tick_counter_;
   // The broker is only used for unparking the LocalHeap for diagnostic printing
   // for failed StaticAsserts.
@@ -788,13 +1008,6 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) InstructionSelectorT final {
 #endif
 };
 
-using InstructionSelector = InstructionSelectorT<TurbofanAdapter>;
-
-extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-    InstructionSelectorT<TurbofanAdapter>;
-extern template class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE)
-    InstructionSelectorT<TurboshaftAdapter>;
-
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/instruction.cc b/src/compiler/backend/instruction.cc
index 7eb8a2d9be6..a131a9c9439 100644
--- a/src/compiler/backend/instruction.cc
+++ b/src/compiler/backend/instruction.cc
@@ -7,6 +7,7 @@
 #include <cstddef>
 #include <iomanip>
 
+#include "src/base/iterator.h"
 #include "src/codegen/aligned-slot-allocator.h"
 #include "src/codegen/interface-descriptors.h"
 #include "src/codegen/machine-type.h"
@@ -16,6 +17,8 @@
 #include "src/compiler/graph.h"
 #include "src/compiler/node.h"
 #include "src/compiler/schedule.h"
+#include "src/compiler/turboshaft/graph.h"
+#include "src/compiler/turboshaft/operations.h"
 #include "src/deoptimizer/deoptimizer.h"
 #include "src/execution/frames.h"
 #include "src/execution/isolate-utils-inl.h"
@@ -667,11 +670,21 @@ static RpoNumber GetRpo(const BasicBlock* block) {
   return RpoNumber::FromInt(block->rpo_number());
 }
 
+static RpoNumber GetRpo(const turboshaft::Block* block) {
+  if (block == nullptr) return RpoNumber::Invalid();
+  return RpoNumber::FromInt(block->index().id());
+}
+
 static RpoNumber GetLoopEndRpo(const BasicBlock* block) {
   if (!block->IsLoopHeader()) return RpoNumber::Invalid();
   return RpoNumber::FromInt(block->loop_end()->rpo_number());
 }
 
+static RpoNumber GetLoopEndRpo(const turboshaft::Block* block) {
+  if (!block->IsLoop()) return RpoNumber::Invalid();
+  return GetRpo(block->LastPredecessor());
+}
+
 static InstructionBlock* InstructionBlockFor(Zone* zone,
                                              const BasicBlock* block) {
   bool is_handler =
@@ -695,6 +708,40 @@ static InstructionBlock* InstructionBlockFor(Zone* zone,
   return instr_block;
 }
 
+static InstructionBlock* InstructionBlockFor(Zone* zone,
+                                             const turboshaft::Graph& graph,
+                                             const turboshaft::Block* block) {
+  bool is_handler = block->IsHandler();
+  // TODO(nicohartmann@): Properly get the loop_header.
+  turboshaft::Block* loop_header = nullptr;  // block->loop_header()
+  // TODO(nicohartmann@): Properly get the deferred.
+  bool deferred = false;
+  InstructionBlock* instr_block = zone->New<InstructionBlock>(
+      zone, GetRpo(block), GetRpo(loop_header), GetLoopEndRpo(block),
+      GetRpo(block->GetDominator()), deferred, is_handler);
+  // Map successors and predecessors.
+  base::SmallVector<turboshaft::Block*, 4> succs =
+      turboshaft::SuccessorBlocks(block->LastOperation(graph));
+  instr_block->successors().reserve(succs.size());
+  for (const turboshaft::Block* successor : succs) {
+    instr_block->successors().push_back(GetRpo(successor));
+  }
+  instr_block->predecessors().reserve(block->PredecessorCount());
+  for (const turboshaft::Block* predecessor = block->LastPredecessor();
+       predecessor; predecessor = predecessor->NeighboringPredecessor()) {
+    instr_block->predecessors().push_back(GetRpo(predecessor));
+  }
+  std::reverse(instr_block->predecessors().begin(),
+               instr_block->predecessors().end());
+  if (block->HasExactlyNPredecessors(1)) {
+    const turboshaft::Block* predecessor = block->LastPredecessor();
+    if (predecessor->LastOperation(graph).Is<turboshaft::SwitchOp>()) {
+      instr_block->set_switch_target(true);
+    }
+  }
+  return instr_block;
+}
+
 std::ostream& operator<<(std::ostream& os,
                          const PrintableInstructionBlock& printable_block) {
   const InstructionBlock* block = printable_block.block_;
@@ -754,12 +801,27 @@ InstructionBlocks* InstructionSequence::InstructionBlocksFor(
   for (BasicBlockVector::const_iterator it = schedule->rpo_order()->begin();
        it != schedule->rpo_order()->end(); ++it, ++rpo_number) {
     DCHECK(!(*blocks)[rpo_number]);
-    DCHECK(GetRpo(*it).ToSize() == rpo_number);
+    DCHECK_EQ(GetRpo(*it).ToSize(), rpo_number);
     (*blocks)[rpo_number] = InstructionBlockFor(zone, *it);
   }
   return blocks;
 }
 
+InstructionBlocks* InstructionSequence::InstructionBlocksFor(
+    Zone* zone, const turboshaft::Graph& graph) {
+  InstructionBlocks* blocks = zone->NewArray<InstructionBlocks>(1);
+  new (blocks)
+      InstructionBlocks(static_cast<int>(graph.block_count()), nullptr, zone);
+  size_t rpo_number = 0;
+  for (const turboshaft::Block& block : graph.blocks()) {
+    DCHECK(!(*blocks)[rpo_number]);
+    DCHECK_EQ(RpoNumber::FromInt(block.index().id()).ToSize(), rpo_number);
+    (*blocks)[rpo_number] = InstructionBlockFor(zone, graph, &block);
+    ++rpo_number;
+  }
+  return blocks;
+}
+
 void InstructionSequence::ValidateEdgeSplitForm() const {
   // Validate blocks are in edge-split form: no block with multiple successors
   // has an edge to a block (== a successor) with more than one predecessors.
@@ -1231,6 +1293,32 @@ std::ostream& operator<<(std::ostream& os, const InstructionSequence& code) {
   return os;
 }
 
+std::ostream& operator<<(std::ostream& os, StateValueKind kind) {
+  switch (kind) {
+    case StateValueKind::kArgumentsElements:
+      return os << "ArgumentsElements";
+    case StateValueKind::kArgumentsLength:
+      return os << "ArgumentsLength";
+    case StateValueKind::kPlain:
+      return os << "Plain";
+    case StateValueKind::kOptimizedOut:
+      return os << "OptimizedOut";
+    case StateValueKind::kNested:
+      return os << "Nested";
+    case StateValueKind::kDuplicate:
+      return os << "Duplicate";
+  }
+}
+
+void StateValueDescriptor::Print(std::ostream& os) const {
+  os << "kind=" << kind_ << ", type=" << type_;
+  if (kind_ == StateValueKind::kDuplicate || kind_ == StateValueKind::kNested) {
+    os << ", id=" << id_;
+  } else if (kind_ == StateValueKind::kArgumentsElements) {
+    os << ", args_type=" << args_type_;
+  }
+}
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/backend/instruction.h b/src/compiler/backend/instruction.h
index 7e19aaf28a6..605e546730e 100644
--- a/src/compiler/backend/instruction.h
+++ b/src/compiler/backend/instruction.h
@@ -31,6 +31,10 @@ namespace compiler {
 class Schedule;
 class SourcePositionTable;
 
+namespace turboshaft {
+class Graph;
+}
+
 #if defined(V8_CC_MSVC) && defined(V8_TARGET_ARCH_IA32)
 // MSVC on x86 has issues with ALIGNAS(8) on InstructionOperand, but does
 // align the object to 8 bytes anyway (covered by a static assert below).
@@ -1254,6 +1258,8 @@ enum class StateValueKind : uint8_t {
   kDuplicate
 };
 
+std::ostream& operator<<(std::ostream& os, StateValueKind kind);
+
 class StateValueDescriptor {
  public:
   StateValueDescriptor()
@@ -1310,6 +1316,8 @@ class StateValueDescriptor {
     return args_type_;
   }
 
+  void Print(std::ostream& os) const;
+
  private:
   StateValueDescriptor(StateValueKind kind, MachineType type)
       : kind_(kind), type_(type) {}
@@ -1711,6 +1719,8 @@ class V8_EXPORT_PRIVATE InstructionSequence final
  public:
   static InstructionBlocks* InstructionBlocksFor(Zone* zone,
                                                  const Schedule* schedule);
+  static InstructionBlocks* InstructionBlocksFor(
+      Zone* zone, const turboshaft::Graph& graph);
   InstructionSequence(Isolate* isolate, Zone* zone,
                       InstructionBlocks* instruction_blocks);
   InstructionSequence(const InstructionSequence&) = delete;
diff --git a/src/compiler/backend/loong64/instruction-selector-loong64.cc b/src/compiler/backend/loong64/instruction-selector-loong64.cc
index 6aafa980704..efb4905742b 100644
--- a/src/compiler/backend/loong64/instruction-selector-loong64.cc
+++ b/src/compiler/backend/loong64/instruction-selector-loong64.cc
@@ -3615,9 +3615,8 @@ void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   return flags | MachineOperatorBuilder::kWord32ShiftIsSafe |
          MachineOperatorBuilder::kInt32DivIsSafe |
@@ -3633,9 +3632,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/backend/mips64/instruction-selector-mips64.cc b/src/compiler/backend/mips64/instruction-selector-mips64.cc
index 8cb02072775..ec53f71d7ea 100644
--- a/src/compiler/backend/mips64/instruction-selector-mips64.cc
+++ b/src/compiler/backend/mips64/instruction-selector-mips64.cc
@@ -3711,9 +3711,8 @@ void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   return flags | MachineOperatorBuilder::kWord32Ctz |
          MachineOperatorBuilder::kWord64Ctz |
@@ -3733,9 +3732,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   if (kArchVariant == kMips64r6) {
     return MachineOperatorBuilder::AlignmentRequirements::
         FullUnalignedAccessSupport();
diff --git a/src/compiler/backend/ppc/instruction-selector-ppc.cc b/src/compiler/backend/ppc/instruction-selector-ppc.cc
index 862ec20afcf..bf8df29ef74 100644
--- a/src/compiler/backend/ppc/instruction-selector-ppc.cc
+++ b/src/compiler/backend/ppc/instruction-selector-ppc.cc
@@ -3047,10 +3047,8 @@ void InstructionSelectorT<Adapter>::VisitF32x4NearestInt(Node* node) {
   UNREACHABLE();
 }
 
-// static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   return MachineOperatorBuilder::kFloat32RoundDown |
          MachineOperatorBuilder::kFloat64RoundDown |
          MachineOperatorBuilder::kFloat32RoundUp |
@@ -3063,10 +3061,7 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
   // We omit kWord32ShiftIsSafe as s[rl]w use 0x3F as a mask rather than 0x1F.
 }
 
-// static
-template <typename Adapter>
-MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+MachineOperatorBuilder::AlignmentRequirements AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv.h b/src/compiler/backend/riscv/instruction-selector-riscv.h
index 910c579a078..abda9123d2a 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv.h
+++ b/src/compiler/backend/riscv/instruction-selector-riscv.h
@@ -1417,9 +1417,8 @@ void InstructionSelectorT<Adapter>::VisitF64x2Pmax(Node* node) {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
 #ifdef RISCV_HAS_NO_UNALIGNED
   return MachineOperatorBuilder::AlignmentRequirements::
       NoUnalignedAccessSupport();
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv32.cc b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
index 48eff28d980..e10f653f097 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv32.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv32.cc
@@ -1460,9 +1460,8 @@ void InstructionSelectorT<Adapter>::VisitWord32AtomicPairCompareExchange(
        temps);
 }
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   return flags | MachineOperatorBuilder::kWord32Ctz |
          MachineOperatorBuilder::kWord32Ctz |
diff --git a/src/compiler/backend/riscv/instruction-selector-riscv64.cc b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
index c564e6d40cc..4a305da302e 100644
--- a/src/compiler/backend/riscv/instruction-selector-riscv64.cc
+++ b/src/compiler/backend/riscv/instruction-selector-riscv64.cc
@@ -2362,9 +2362,8 @@ void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags = MachineOperatorBuilder::kNoFlags;
   return flags | MachineOperatorBuilder::kWord32Ctz |
          MachineOperatorBuilder::kWord64Ctz |
diff --git a/src/compiler/backend/s390/instruction-selector-s390.cc b/src/compiler/backend/s390/instruction-selector-s390.cc
index d64861b66f3..efb7bba2c18 100644
--- a/src/compiler/backend/s390/instruction-selector-s390.cc
+++ b/src/compiler/backend/s390/instruction-selector-s390.cc
@@ -3188,10 +3188,8 @@ void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
   UNREACHABLE();
 }
 
-// static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   return MachineOperatorBuilder::kFloat32RoundDown |
          MachineOperatorBuilder::kFloat64RoundDown |
          MachineOperatorBuilder::kFloat32RoundUp |
@@ -3207,10 +3205,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
          MachineOperatorBuilder::kWord64Popcnt;
 }
 
-// static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index f501996cf7d..07aaae3c3d1 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -11,11 +11,14 @@
 #include "src/codegen/machine-type.h"
 #include "src/compiler/backend/instruction-codes.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/instruction-selector.h"
 #include "src/compiler/backend/instruction.h"
 #include "src/compiler/machine-operator.h"
 #include "src/compiler/node-matchers.h"
 #include "src/compiler/node-properties.h"
 #include "src/compiler/opcodes.h"
+#include "src/compiler/turboshaft/operations.h"
+#include "src/compiler/turboshaft/representations.h"
 #include "src/roots/roots-inl.h"
 
 #if V8_ENABLE_WEBASSEMBLY
@@ -56,41 +59,51 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
   explicit X64OperandGeneratorT(InstructionSelectorT<Adapter>* selector)
       : super(selector) {}
 
-  bool CanBeImmediate(Node* node) {
-    switch (node->opcode()) {
-      case IrOpcode::kCompressedHeapConstant: {
-        if (!COMPRESS_POINTERS_BOOL) return false;
-        // For builtin code we need static roots
-        if (selector()->isolate()->bootstrapper() && !V8_STATIC_ROOTS_BOOL) {
+  template <typename T>
+  bool CanBeImmediate(T*) {
+    UNREACHABLE(/*REMOVE*/);
+  }
+
+  bool CanBeImmediate(node_t node) {
+    if constexpr (Adapter::IsTurboshaft) {
+      // TODO(nicohartmann@): Implement this for Turobshaft.
+      return false;
+    } else {
+      switch (node->opcode()) {
+        case IrOpcode::kCompressedHeapConstant: {
+          if (!COMPRESS_POINTERS_BOOL) return false;
+          // For builtin code we need static roots
+          if (selector()->isolate()->bootstrapper() && !V8_STATIC_ROOTS_BOOL) {
+            return false;
+          }
+          const RootsTable& roots_table = selector()->isolate()->roots_table();
+          RootIndex root_index;
+          CompressedHeapObjectMatcher m(node);
+          if (m.HasResolvedValue() &&
+              roots_table.IsRootHandle(m.ResolvedValue(), &root_index)) {
+            return RootsTable::IsReadOnly(root_index);
+          }
           return false;
         }
-        const RootsTable& roots_table = selector()->isolate()->roots_table();
-        RootIndex root_index;
-        CompressedHeapObjectMatcher m(node);
-        if (m.HasResolvedValue() &&
-            roots_table.IsRootHandle(m.ResolvedValue(), &root_index)) {
-          return RootsTable::IsReadOnly(root_index);
+        case IrOpcode::kInt32Constant:
+        case IrOpcode::kRelocatableInt32Constant: {
+          const int32_t value = OpParameter<int32_t>(node->op());
+          // int32_t min will overflow if displacement mode is
+          // kNegativeDisplacement.
+          return value != std::numeric_limits<int32_t>::min();
         }
-        return false;
-      }
-      case IrOpcode::kInt32Constant:
-      case IrOpcode::kRelocatableInt32Constant: {
-        const int32_t value = OpParameter<int32_t>(node->op());
-        // int32_t min will overflow if displacement mode is
-        // kNegativeDisplacement.
-        return value != std::numeric_limits<int32_t>::min();
-      }
-      case IrOpcode::kInt64Constant: {
-        const int64_t value = OpParameter<int64_t>(node->op());
-        return std::numeric_limits<int32_t>::min() < value &&
-               value <= std::numeric_limits<int32_t>::max();
-      }
-      case IrOpcode::kNumberConstant: {
-        const double value = OpParameter<double>(node->op());
-        return base::bit_cast<int64_t>(value) == 0;
+        case IrOpcode::kInt64Constant: {
+          const int64_t value = OpParameter<int64_t>(node->op());
+          return std::numeric_limits<int32_t>::min() < value &&
+                 value <= std::numeric_limits<int32_t>::max();
+        }
+        case IrOpcode::kNumberConstant: {
+          const double value = OpParameter<double>(node->op());
+          return base::bit_cast<int64_t>(value) == 0;
+        }
+        default:
+          return false;
       }
-      default:
-        return false;
     }
   }
 
@@ -103,63 +116,69 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     return static_cast<int32_t>(OpParameter<int64_t>(node->op()));
   }
 
-  bool CanBeMemoryOperand(InstructionCode opcode, Node* node, Node* input,
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanBeMemoryOperand)
+  bool CanBeMemoryOperand(InstructionCode opcode, node_t node, node_t input,
                           int effect_level) {
-    if ((input->opcode() != IrOpcode::kLoad &&
-         input->opcode() != IrOpcode::kLoadImmutable) ||
-        !selector()->CanCover(node, input)) {
+    if constexpr (Adapter::IsTurboshaft) {
+      // TODO(nicohartmann@): Implement this for Turboshaft.
       return false;
-    }
-    if (effect_level != selector()->GetEffectLevel(input)) {
+    } else {
+      if ((input->opcode() != IrOpcode::kLoad &&
+           input->opcode() != IrOpcode::kLoadImmutable) ||
+          !selector()->CanCover(node, input)) {
+        return false;
+      }
+      if (effect_level != selector()->GetEffectLevel(input)) {
+        return false;
+      }
+      MachineRepresentation rep =
+          LoadRepresentationOf(input->op()).representation();
+      switch (opcode) {
+        case kX64And:
+        case kX64Or:
+        case kX64Xor:
+        case kX64Add:
+        case kX64Sub:
+        case kX64Push:
+        case kX64Cmp:
+        case kX64Test:
+          // When pointer compression is enabled 64-bit memory operands can't be
+          // used for tagged values.
+          return rep == MachineRepresentation::kWord64 ||
+                 (!COMPRESS_POINTERS_BOOL && IsAnyTagged(rep));
+        case kX64And32:
+        case kX64Or32:
+        case kX64Xor32:
+        case kX64Add32:
+        case kX64Sub32:
+        case kX64Cmp32:
+        case kX64Test32:
+          // When pointer compression is enabled 32-bit memory operands can be
+          // used for tagged values.
+          return rep == MachineRepresentation::kWord32 ||
+                 (COMPRESS_POINTERS_BOOL &&
+                  (IsAnyTagged(rep) || IsAnyCompressed(rep)));
+        case kAVXFloat64Add:
+        case kAVXFloat64Sub:
+        case kAVXFloat64Mul:
+          DCHECK_EQ(MachineRepresentation::kFloat64, rep);
+          return true;
+        case kAVXFloat32Add:
+        case kAVXFloat32Sub:
+        case kAVXFloat32Mul:
+          DCHECK_EQ(MachineRepresentation::kFloat32, rep);
+          return true;
+        case kX64Cmp16:
+        case kX64Test16:
+          return rep == MachineRepresentation::kWord16;
+        case kX64Cmp8:
+        case kX64Test8:
+          return rep == MachineRepresentation::kWord8;
+        default:
+          break;
+      }
       return false;
     }
-    MachineRepresentation rep =
-        LoadRepresentationOf(input->op()).representation();
-    switch (opcode) {
-      case kX64And:
-      case kX64Or:
-      case kX64Xor:
-      case kX64Add:
-      case kX64Sub:
-      case kX64Push:
-      case kX64Cmp:
-      case kX64Test:
-        // When pointer compression is enabled 64-bit memory operands can't be
-        // used for tagged values.
-        return rep == MachineRepresentation::kWord64 ||
-               (!COMPRESS_POINTERS_BOOL && IsAnyTagged(rep));
-      case kX64And32:
-      case kX64Or32:
-      case kX64Xor32:
-      case kX64Add32:
-      case kX64Sub32:
-      case kX64Cmp32:
-      case kX64Test32:
-        // When pointer compression is enabled 32-bit memory operands can be
-        // used for tagged values.
-        return rep == MachineRepresentation::kWord32 ||
-               (COMPRESS_POINTERS_BOOL &&
-                (IsAnyTagged(rep) || IsAnyCompressed(rep)));
-      case kAVXFloat64Add:
-      case kAVXFloat64Sub:
-      case kAVXFloat64Mul:
-        DCHECK_EQ(MachineRepresentation::kFloat64, rep);
-        return true;
-      case kAVXFloat32Add:
-      case kAVXFloat32Sub:
-      case kAVXFloat32Mul:
-        DCHECK_EQ(MachineRepresentation::kFloat32, rep);
-        return true;
-      case kX64Cmp16:
-      case kX64Test16:
-        return rep == MachineRepresentation::kWord16;
-      case kX64Cmp8:
-      case kX64Test8:
-        return rep == MachineRepresentation::kWord8;
-      default:
-        break;
-    }
-    return false;
   }
 
   AddressingMode GenerateMemoryOperandInputs(
@@ -266,66 +285,12 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     return mode;
   }
 
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(AddressingMode,
+                                          GetEffectiveAddressMemoryOperand)
+
   AddressingMode GetEffectiveAddressMemoryOperand(
-      Node* operand, InstructionOperand inputs[], size_t* input_count,
-      RegisterUseKind reg_kind = RegisterUseKind::kUseRegister) {
-    {
-      LoadMatcher<ExternalReferenceMatcher> m(operand);
-      if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
-          selector()->CanAddressRelativeToRootsRegister(
-              m.object().ResolvedValue())) {
-        ptrdiff_t const delta =
-            m.index().ResolvedValue() +
-            MacroAssemblerBase::RootRegisterOffsetForExternalReference(
-                selector()->isolate(), m.object().ResolvedValue());
-        if (is_int32(delta)) {
-          inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
-          return kMode_Root;
-        }
-      }
-    }
-    BaseWithIndexAndDisplacement64Matcher m(operand, AddressOption::kAllowAll);
-    DCHECK(m.matches());
-    // Decompress pointer by complex addressing mode.
-    if (IsCompressed(m.base())) {
-      DCHECK(m.index() == nullptr);
-      DCHECK(m.displacement() == nullptr || CanBeImmediate(m.displacement()));
-      AddressingMode mode = kMode_MCR;
-      inputs[(*input_count)++] = UseRegister(m.base(), reg_kind);
-      if (m.displacement() != nullptr) {
-        inputs[(*input_count)++] =
-            m.displacement_mode() == kNegativeDisplacement
-                ? UseNegatedImmediate(m.displacement())
-                : UseImmediate(m.displacement());
-        mode = kMode_MCRI;
-      }
-      return mode;
-    }
-    if (m.base() != nullptr &&
-        m.base()->opcode() == IrOpcode::kLoadRootRegister) {
-      DCHECK_EQ(m.index(), nullptr);
-      DCHECK_EQ(m.scale(), 0);
-      inputs[(*input_count)++] = UseImmediate(m.displacement());
-      return kMode_Root;
-    } else if (m.displacement() == nullptr ||
-               CanBeImmediate(m.displacement())) {
-      return GenerateMemoryOperandInputs(
-          m.index(), m.scale(), m.base(), m.displacement(),
-          m.displacement_mode(), inputs, input_count, reg_kind);
-    } else if (m.base() == nullptr &&
-               m.displacement_mode() == kPositiveDisplacement) {
-      // The displacement cannot be an immediate, but we can use the
-      // displacement as base instead and still benefit from addressing
-      // modes for the scale.
-      return GenerateMemoryOperandInputs(m.index(), m.scale(), m.displacement(),
-                                         nullptr, m.displacement_mode(), inputs,
-                                         input_count, reg_kind);
-    } else {
-      inputs[(*input_count)++] = UseRegister(operand->InputAt(0), reg_kind);
-      inputs[(*input_count)++] = UseRegister(operand->InputAt(1), reg_kind);
-      return kMode_MR1;
-    }
-  }
+      node_t operand, InstructionOperand inputs[], size_t* input_count,
+      RegisterUseKind reg_kind = RegisterUseKind::kUseRegister);
 
   InstructionOperand GetEffectiveIndexOperand(Node* index,
                                               AddressingMode* mode) {
@@ -338,11 +303,107 @@ class X64OperandGeneratorT final : public OperandGeneratorT<Adapter> {
     }
   }
 
-  bool CanBeBetterLeftOperand(Node* node) const {
+  DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(bool, CanBeBetterLeftOperand)
+  bool CanBeBetterLeftOperand(node_t node) const {
     return !selector()->IsLive(node);
   }
 };
 
+template <>
+AddressingMode
+X64OperandGeneratorT<TurboshaftAdapter>::GetEffectiveAddressMemoryOperand(
+    node_t operand, InstructionOperand inputs[], size_t* input_count,
+    RegisterUseKind reg_kind) {
+  const turboshaft::Operation& op = this->turboshaft_graph()->Get(operand);
+  const turboshaft::LoadOp& load = op.Cast<turboshaft::LoadOp>();
+  const turboshaft::Operation& base =
+      this->turboshaft_graph()->Get(load.base());
+
+  if (const auto constant = base.TryCast<turboshaft::ConstantOp>();
+      constant != nullptr &&
+      constant->kind == turboshaft::ConstantOp::Kind::kExternal) {
+    ExternalReference reference = constant->external_reference();
+    if (selector()->CanAddressRelativeToRootsRegister(reference)) {
+      const ptrdiff_t delta =
+          load.offset +
+          MacroAssemblerBase::RootRegisterOffsetForExternalReference(
+              selector()->isolate(), reference);
+      if (is_int32(delta)) {
+        inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
+        return kMode_Root;
+      }
+    }
+  }
+
+  DCHECK_GE(op.input_count, 2);
+
+  // TODO(nicohartmann@): Implement this properly for Turboshaft.
+  inputs[(*input_count)++] = UseRegister(op.input(0), reg_kind);
+  inputs[(*input_count)++] = UseRegister(op.input(1), reg_kind);
+  return kMode_MR1;
+}
+
+template <>
+AddressingMode
+X64OperandGeneratorT<TurbofanAdapter>::GetEffectiveAddressMemoryOperand(
+    node_t operand, InstructionOperand inputs[], size_t* input_count,
+    RegisterUseKind reg_kind) {
+  {
+    LoadMatcher<ExternalReferenceMatcher> m(operand);
+    if (m.index().HasResolvedValue() && m.object().HasResolvedValue() &&
+        selector()->CanAddressRelativeToRootsRegister(
+            m.object().ResolvedValue())) {
+      ptrdiff_t const delta =
+          m.index().ResolvedValue() +
+          MacroAssemblerBase::RootRegisterOffsetForExternalReference(
+              selector()->isolate(), m.object().ResolvedValue());
+      if (is_int32(delta)) {
+        inputs[(*input_count)++] = TempImmediate(static_cast<int32_t>(delta));
+        return kMode_Root;
+      }
+    }
+  }
+  BaseWithIndexAndDisplacement64Matcher m(operand, AddressOption::kAllowAll);
+  DCHECK(m.matches());
+  // Decompress pointer by complex addressing mode.
+  if (IsCompressed(m.base())) {
+    DCHECK(m.index() == nullptr);
+    DCHECK(m.displacement() == nullptr || CanBeImmediate(m.displacement()));
+    AddressingMode mode = kMode_MCR;
+    inputs[(*input_count)++] = UseRegister(m.base(), reg_kind);
+    if (m.displacement() != nullptr) {
+      inputs[(*input_count)++] = m.displacement_mode() == kNegativeDisplacement
+                                     ? UseNegatedImmediate(m.displacement())
+                                     : UseImmediate(m.displacement());
+      mode = kMode_MCRI;
+    }
+    return mode;
+  }
+  if (m.base() != nullptr &&
+      m.base()->opcode() == IrOpcode::kLoadRootRegister) {
+    DCHECK_EQ(m.index(), nullptr);
+    DCHECK_EQ(m.scale(), 0);
+    inputs[(*input_count)++] = UseImmediate(m.displacement());
+    return kMode_Root;
+  } else if (m.displacement() == nullptr || CanBeImmediate(m.displacement())) {
+    return GenerateMemoryOperandInputs(m.index(), m.scale(), m.base(),
+                                       m.displacement(), m.displacement_mode(),
+                                       inputs, input_count, reg_kind);
+  } else if (m.base() == nullptr &&
+             m.displacement_mode() == kPositiveDisplacement) {
+    // The displacement cannot be an immediate, but we can use the
+    // displacement as base instead and still benefit from addressing
+    // modes for the scale.
+    return GenerateMemoryOperandInputs(m.index(), m.scale(), m.displacement(),
+                                       nullptr, m.displacement_mode(), inputs,
+                                       input_count, reg_kind);
+  } else {
+    inputs[(*input_count)++] = UseRegister(operand->InputAt(0), reg_kind);
+    inputs[(*input_count)++] = UseRegister(operand->InputAt(1), reg_kind);
+    return kMode_MR1;
+  }
+}
+
 namespace {
 
 ArchOpcode GetLoadOpcode(LoadRepresentation load_rep) {
@@ -594,7 +655,7 @@ void InstructionSelectorT<Adapter>::VisitLoadTransform(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node, node_t value,
                                               InstructionCode opcode) {
   X64OperandGeneratorT<Adapter> g(this);
 #ifdef V8_IS_TSAN
@@ -615,21 +676,25 @@ void InstructionSelectorT<Adapter>::VisitLoad(Node* node, Node* value,
   AddressingMode mode =
       g.GetEffectiveAddressMemoryOperand(value, inputs, &input_count, reg_kind);
   InstructionCode code = opcode | AddressingModeField::encode(mode);
-  if (node->opcode() == IrOpcode::kProtectedLoad ||
-      ((node->opcode() == IrOpcode::kWord32AtomicLoad ||
-        node->opcode() == IrOpcode::kWord64AtomicLoad) &&
-       (AtomicLoadParametersOf(node->op()).kind() ==
-        MemoryAccessKind::kProtected))) {
-    code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
-  } else if (node->opcode() == IrOpcode::kLoadTrapOnNull) {
-    code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+  } else {
+    if (node->opcode() == IrOpcode::kProtectedLoad ||
+        ((node->opcode() == IrOpcode::kWord32AtomicLoad ||
+          node->opcode() == IrOpcode::kWord64AtomicLoad) &&
+         (AtomicLoadParametersOf(node->op()).kind() ==
+          MemoryAccessKind::kProtected))) {
+      code |= AccessModeField::encode(kMemoryAccessProtectedMemOutOfBounds);
+    } else if (node->opcode() == IrOpcode::kLoadTrapOnNull) {
+      code |= AccessModeField::encode(kMemoryAccessProtectedNullDereference);
+    }
   }
   Emit(code, 1, outputs, input_count, inputs, temp_count, temps);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoad(Node* node) {
-  LoadRepresentation load_rep = LoadRepresentationOf(node->op());
+void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
+  LoadRepresentation load_rep = this->load_view(node).loaded_rep();
   DCHECK(!load_rep.IsMapWord());
   VisitLoad(node, node, GetLoadOpcode(load_rep));
 }
@@ -831,14 +896,22 @@ void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
   Emit(opcode, 0, nullptr, input_count, inputs);
 }
 
+static void VisitBinop(InstructionSelectorT<TurboshaftAdapter>*, Node* node,
+                       InstructionCode opcode,
+                       FlagsContinuationT<TurboshaftAdapter>* cont) {
+  UNIMPLEMENTED();
+}
+
 // Shared routine for multiple binary operations.
 template <typename Adapter>
-static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
-                       InstructionCode opcode, FlagsContinuation* cont) {
+static void VisitBinop(InstructionSelectorT<Adapter>* selector,
+                       typename Adapter::node_t node, InstructionCode opcode,
+                       FlagsContinuationT<Adapter>* cont) {
   X64OperandGeneratorT<Adapter> g(selector);
-  Int32BinopMatcher m(node);
-  Node* left = m.left().node();
-  Node* right = m.right().node();
+  auto binop = selector->word_binop_view(node);
+  binop.EnsureConstantIsRightIfCommutative();
+  typename Adapter::node_t left = binop.left();
+  typename Adapter::node_t right = binop.right();
   InstructionOperand inputs[8];
   size_t input_count = 0;
   InstructionOperand outputs[1];
@@ -861,8 +934,7 @@ static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
     inputs[input_count++] = g.UseImmediate(right);
   } else {
     int effect_level = selector->GetEffectLevel(node, cont);
-    if (node->op()->HasProperty(Operator::kCommutative) &&
-        g.CanBeBetterLeftOperand(right) &&
+    if (selector->IsCommutative(node) && g.CanBeBetterLeftOperand(right) &&
         (!g.CanBeBetterLeftOperand(left) ||
          !g.CanBeMemoryOperand(opcode, node, right, effect_level))) {
       std::swap(left, right);
@@ -898,7 +970,7 @@ static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
 template <typename Adapter>
 static void VisitBinop(InstructionSelectorT<Adapter>* selector, Node* node,
                        InstructionCode opcode) {
-  FlagsContinuation cont;
+  FlagsContinuationT<Adapter> cont;
   VisitBinop(selector, node, opcode, &cont);
 }
 
@@ -968,18 +1040,25 @@ void InstructionSelectorT<Adapter>::VisitWord64Xor(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(
-    Node* node, FlagsContinuation* cont) {
-  StackCheckKind kind = StackCheckKindOf(node->op());
+    node_t node, FlagsContinuation* cont) {
+  StackCheckKind kind;
+  if constexpr (Adapter::IsTurboshaft) {
+    kind = this->turboshaft_graph()
+               ->Get(node)
+               .template Cast<turboshaft::StackPointerGreaterThanOp>()
+               .kind;
+  } else {
+    kind = StackCheckKindOf(node->op());
+  }
   InstructionCode opcode =
       kArchStackPointerGreaterThan | MiscField::encode(static_cast<int>(kind));
 
   int effect_level = GetEffectLevel(node, cont);
 
   X64OperandGeneratorT<Adapter> g(this);
-  Node* const value = node->InputAt(0);
+  node_t value = this->input_at(node, 0);
   if (g.CanBeMemoryOperand(kX64Cmp, node, value, effect_level)) {
-    DCHECK(IrOpcode::kLoad == value->opcode() ||
-           IrOpcode::kLoadImmutable == value->opcode());
+    DCHECK(this->IsLoadOrLoadImmutable(value));
 
     // GetEffectiveAddressMemoryOperand can create at most 3 inputs.
     static constexpr int kMaxInputCount = 3;
@@ -1104,7 +1183,7 @@ void VisitWord64Shift(InstructionSelectorT<Adapter>* selector, Node* node,
 // Shared routine for multiple shift operations with continuation.
 template <typename Adapter, typename BinopMatcher, int Bits>
 bool TryVisitWordShift(InstructionSelectorT<Adapter>* selector, Node* node,
-                       ArchOpcode opcode, FlagsContinuation* cont) {
+                       ArchOpcode opcode, FlagsContinuationT<Adapter>* cont) {
   X64OperandGeneratorT<Adapter> g(selector);
   BinopMatcher m(node);
   Node* left = m.left().node();
@@ -1416,12 +1495,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Add(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Add, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Add, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Add, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Add, &cont);
 }
 
 template <typename Adapter>
@@ -1488,12 +1572,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Sub(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Sub, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Sub, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Sub, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Sub, &cont);
 }
 
 namespace {
@@ -1570,13 +1659,18 @@ void InstructionSelectorT<Adapter>::VisitInt32Mul(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32MulWithOverflow(Node* node) {
-  // TODO(mvstanton): Use Int32ScaleMatcher somehow.
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Imul32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    // TODO(mvstanton): Use Int32ScaleMatcher somehow.
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Imul32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Imul32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Imul32, &cont);
 }
 
 template <typename Adapter>
@@ -1594,12 +1688,17 @@ void InstructionSelectorT<Adapter>::VisitInt64Mul(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64MulWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Imul, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Imul, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Imul, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Imul, &cont);
 }
 
 template <typename Adapter>
@@ -2262,7 +2361,8 @@ void InstructionSelectorT<Adapter>::VisitFloat64Ieee754Unop(
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(Node* node, int index) {}
+void InstructionSelectorT<Adapter>::EmitMoveParamToFPR(node_t node, int index) {
+}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
@@ -2271,7 +2371,7 @@ void InstructionSelectorT<Adapter>::EmitMoveFPRToParam(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     ZoneVector<PushParameter>* arguments, const CallDescriptor* call_descriptor,
-    Node* node) {
+    node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
 
   // Prepare for C function call.
@@ -2283,7 +2383,7 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
     // Poke any stack arguments.
     for (size_t n = 0; n < arguments->size(); ++n) {
       PushParameter input = (*arguments)[n];
-      if (input.node) {
+      if (this->valid(input.node)) {
         int slot = static_cast<int>(n);
         InstructionOperand value = g.CanBeImmediate(input.node)
                                        ? g.UseImmediate(input.node)
@@ -2299,7 +2399,7 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
       stack_decrement += kSystemPointerSize;
       // Skip holes in the param array. These represent both extra slots for
       // multi-slot values and padding slots for alignment.
-      if (input.node == nullptr) continue;
+      if (!this->valid(input.node)) continue;
       InstructionOperand decrement = g.UseImmediate(stack_decrement);
       stack_decrement = 0;
       if (g.CanBeImmediate(input.node)) {
@@ -2329,12 +2429,12 @@ void InstructionSelectorT<Adapter>::EmitPrepareArguments(
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::EmitPrepareResults(
     ZoneVector<PushParameter>* results, const CallDescriptor* call_descriptor,
-    Node* node) {
+    node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
   for (PushParameter output : *results) {
     if (!output.location.IsCallerFrameSlot()) continue;
     // Skip any alignment holes in nodes.
-    if (output.node != nullptr) {
+    if (this->valid(output.node)) {
       DCHECK(!call_descriptor->IsCFunctionCall());
       if (output.location.GetType() == MachineType::Float32()) {
         MarkAsFloat32(output.node);
@@ -2361,11 +2461,11 @@ namespace {
 
 template <typename Adapter>
 void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
-                                   InstructionCode opcode, Node* left,
+                                   InstructionCode opcode,
+                                   typename Adapter::node_t left,
                                    InstructionOperand right,
-                                   FlagsContinuation* cont) {
-  DCHECK(IrOpcode::kLoad == left->opcode() ||
-         IrOpcode::kLoadImmutable == left->opcode());
+                                   FlagsContinuationT<Adapter>* cont) {
+  DCHECK(selector->IsLoadOrLoadImmutable(left));
   X64OperandGeneratorT<Adapter> g(selector);
   size_t input_count = 0;
   InstructionOperand inputs[6];
@@ -2391,7 +2491,7 @@ void VisitCompareWithMemoryOperand(InstructionSelectorT<Adapter>* selector,
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
                   InstructionCode opcode, InstructionOperand left,
-                  InstructionOperand right, FlagsContinuation* cont) {
+                  InstructionOperand right, FlagsContinuationT<Adapter>* cont) {
   if (cont->IsSelect()) {
     X64OperandGeneratorT<Adapter> g(selector);
     InstructionOperand inputs[4] = {left, right};
@@ -2412,8 +2512,9 @@ void VisitCompare(InstructionSelectorT<Adapter>* selector,
 // Shared routine for multiple compare operations.
 template <typename Adapter>
 void VisitCompare(InstructionSelectorT<Adapter>* selector,
-                  InstructionCode opcode, Node* left, Node* right,
-                  FlagsContinuation* cont, bool commutative) {
+                  InstructionCode opcode, typename Adapter::node_t left,
+                  typename Adapter::node_t right,
+                  FlagsContinuationT<Adapter>* cont, bool commutative) {
   X64OperandGeneratorT<Adapter> g(selector);
   if (commutative && g.CanBeBetterLeftOperand(right)) {
     std::swap(left, right);
@@ -2520,8 +2621,10 @@ MachineType MachineTypeForNarrowWordAnd(Node* and_node, Node* constant_node) {
 
 // Tries to match the size of the given opcode to that of the operands, if
 // possible.
+template <typename Adapter>
 InstructionCode TryNarrowOpcodeSize(InstructionCode opcode, Node* left,
-                                    Node* right, FlagsContinuation* cont) {
+                                    Node* right,
+                                    FlagsContinuationT<Adapter>* cont) {
   MachineType left_type = MachineType::None();
   MachineType right_type = MachineType::None();
   if (IsWordAnd(left) && IsIntConstant(right)) {
@@ -2641,64 +2744,69 @@ void RemoveUnnecessaryWordAnd(InstructionCode opcode, Node** and_node) {
 
 // Shared routine for multiple word compare operations.
 template <typename Adapter>
-void VisitWordCompare(InstructionSelectorT<Adapter>* selector, Node* node,
-                      InstructionCode opcode, FlagsContinuation* cont) {
-  X64OperandGeneratorT<Adapter> g(selector);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
+void VisitWordCompare(InstructionSelectorT<Adapter>* selector,
+                      typename Adapter::node_t node, InstructionCode opcode,
+                      FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    X64OperandGeneratorT<Adapter> g(selector);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+
+    // The 32-bit comparisons automatically truncate Word64
+    // values to Word32 range, no need to do that explicitly.
+    if (opcode == kX64Cmp32 || opcode == kX64Test32) {
+      if (left->opcode() == IrOpcode::kTruncateInt64ToInt32) {
+        left = left->InputAt(0);
+      }
 
-  // The 32-bit comparisons automatically truncate Word64
-  // values to Word32 range, no need to do that explicitly.
-  if (opcode == kX64Cmp32 || opcode == kX64Test32) {
-    if (left->opcode() == IrOpcode::kTruncateInt64ToInt32) {
-      left = left->InputAt(0);
+      if (right->opcode() == IrOpcode::kTruncateInt64ToInt32) {
+        right = right->InputAt(0);
+      }
     }
 
-    if (right->opcode() == IrOpcode::kTruncateInt64ToInt32) {
-      right = right->InputAt(0);
-    }
-  }
+    opcode = TryNarrowOpcodeSize(opcode, left, right, cont);
 
-  opcode = TryNarrowOpcodeSize(opcode, left, right, cont);
+    // If one of the two inputs is an immediate, make sure it's on the right, or
+    // if one of the two inputs is a memory operand, make sure it's on the left.
+    int effect_level = selector->GetEffectLevel(node, cont);
 
-  // If one of the two inputs is an immediate, make sure it's on the right, or
-  // if one of the two inputs is a memory operand, make sure it's on the left.
-  int effect_level = selector->GetEffectLevel(node, cont);
+    if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
+        (g.CanBeMemoryOperand(opcode, node, right, effect_level) &&
+         !g.CanBeMemoryOperand(opcode, node, left, effect_level))) {
+      if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
+      std::swap(left, right);
+    }
 
-  if ((!g.CanBeImmediate(right) && g.CanBeImmediate(left)) ||
-      (g.CanBeMemoryOperand(opcode, node, right, effect_level) &&
-       !g.CanBeMemoryOperand(opcode, node, left, effect_level))) {
-    if (!node->op()->HasProperty(Operator::kCommutative)) cont->Commute();
-    std::swap(left, right);
-  }
+    if (IsWordAnd(left)) {
+      RemoveUnnecessaryWordAnd(opcode, &left);
+    }
 
-  if (IsWordAnd(left)) {
-    RemoveUnnecessaryWordAnd(opcode, &left);
-  }
+    // Match immediates on right side of comparison.
+    if (g.CanBeImmediate(right)) {
+      if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
+        return VisitCompareWithMemoryOperand(selector, opcode, left,
+                                             g.UseImmediate(right), cont);
+      }
+      return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
+                          cont);
+    }
 
-  // Match immediates on right side of comparison.
-  if (g.CanBeImmediate(right)) {
+    // Match memory operands on left side of comparison.
     if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
       return VisitCompareWithMemoryOperand(selector, opcode, left,
-                                           g.UseImmediate(right), cont);
+                                           g.UseRegister(right), cont);
     }
-    return VisitCompare(selector, opcode, g.Use(left), g.UseImmediate(right),
-                        cont);
-  }
 
-  // Match memory operands on left side of comparison.
-  if (g.CanBeMemoryOperand(opcode, node, left, effect_level)) {
-    return VisitCompareWithMemoryOperand(selector, opcode, left,
-                                         g.UseRegister(right), cont);
+    return VisitCompare(selector, opcode, left, right, cont,
+                        node->op()->HasProperty(Operator::kCommutative));
   }
-
-  return VisitCompare(selector, opcode, left, right, cont,
-                      node->op()->HasProperty(Operator::kCommutative));
 }
 
 template <typename Adapter>
 void VisitWord64EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
-                          FlagsContinuation* cont) {
+                          FlagsContinuationT<Adapter>* cont) {
   if (selector->CanUseRootsRegister()) {
     X64OperandGeneratorT<Adapter> g(selector);
     const RootsTable& roots_table = selector->isolate()->roots_table();
@@ -2719,59 +2827,129 @@ void VisitWord64EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
 }
 
 template <typename Adapter>
-void VisitWord32EqualImpl(InstructionSelectorT<Adapter>* selector, Node* node,
-                          FlagsContinuation* cont) {
-  if (COMPRESS_POINTERS_BOOL && selector->isolate()) {
-    X64OperandGeneratorT<Adapter> g(selector);
-    const RootsTable& roots_table = selector->isolate()->roots_table();
-    RootIndex root_index;
-    Node* left = nullptr;
-    Handle<HeapObject> right;
-    // HeapConstants and CompressedHeapConstants can be treated the same when
-    // using them as an input to a 32-bit comparison. Check whether either is
-    // present.
-    {
-      CompressedHeapObjectBinopMatcher m(node);
-      if (m.right().HasResolvedValue()) {
-        left = m.left().node();
-        right = m.right().ResolvedValue();
-      } else {
-        HeapObjectBinopMatcher m2(node);
-        if (m2.right().HasResolvedValue()) {
-          left = m2.left().node();
-          right = m2.right().ResolvedValue();
+void VisitWord32EqualImpl(InstructionSelectorT<Adapter>* selector,
+                          typename Adapter::node_t node,
+                          FlagsContinuationT<Adapter>* cont) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    if (COMPRESS_POINTERS_BOOL && selector->isolate()) {
+      X64OperandGeneratorT<Adapter> g(selector);
+      const RootsTable& roots_table = selector->isolate()->roots_table();
+      RootIndex root_index;
+      Node* left = nullptr;
+      Handle<HeapObject> right;
+      // HeapConstants and CompressedHeapConstants can be treated the same when
+      // using them as an input to a 32-bit comparison. Check whether either is
+      // present.
+      {
+        CompressedHeapObjectBinopMatcher m(node);
+        if (m.right().HasResolvedValue()) {
+          left = m.left().node();
+          right = m.right().ResolvedValue();
+        } else {
+          HeapObjectBinopMatcher m2(node);
+          if (m2.right().HasResolvedValue()) {
+            left = m2.left().node();
+            right = m2.right().ResolvedValue();
+          }
+        }
+      }
+      if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
+        DCHECK_NE(left, nullptr);
+        if (RootsTable::IsReadOnly(root_index) &&
+            (V8_STATIC_ROOTS_BOOL || !selector->isolate()->bootstrapper())) {
+          return VisitCompare(
+              selector, kX64Cmp32, g.UseRegister(left),
+              g.TempImmediate(MacroAssemblerBase::ReadOnlyRootPtr(
+                  root_index, selector->isolate())),
+              cont);
+        }
+        if (selector->CanUseRootsRegister()) {
+          InstructionCode opcode =
+              kX64Cmp32 | AddressingModeField::encode(kMode_Root);
+          return VisitCompare(
+              selector, opcode,
+              g.TempImmediate(
+                  MacroAssemblerBase::RootRegisterOffsetForRootIndex(
+                      root_index)),
+              g.UseRegister(left), cont);
         }
       }
     }
-    if (!right.is_null() && roots_table.IsRootHandle(right, &root_index)) {
-      DCHECK_NE(left, nullptr);
-      if (RootsTable::IsReadOnly(root_index) &&
-          (V8_STATIC_ROOTS_BOOL || !selector->isolate()->bootstrapper())) {
-        return VisitCompare(selector, kX64Cmp32, g.UseRegister(left),
-                            g.TempImmediate(MacroAssemblerBase::ReadOnlyRootPtr(
-                                root_index, selector->isolate())),
-                            cont);
+    VisitWordCompare(selector, node, kX64Cmp32, cont);
+  }
+}
+
+void VisitCompareZero(InstructionSelectorT<TurboshaftAdapter>* selector,
+                      turboshaft::OpIndex user, turboshaft::OpIndex node,
+                      InstructionCode opcode,
+                      FlagsContinuationT<TurboshaftAdapter>* cont) {
+  X64OperandGeneratorT<TurboshaftAdapter> g(selector);
+  const turboshaft::Operation& op = selector->turboshaft_graph()->Get(node);
+  if (cont->IsBranch() &&
+      (cont->condition() == kNotEqual || cont->condition() == kEqual)) {
+    if (const auto binop = op.TryCast<turboshaft::WordBinopOp>()) {
+      using Kind = turboshaft::WordBinopOp::Kind;
+      if (selector->IsOnlyUserOfNodeInSameBlock(user, node)) {
+        const bool is64 =
+            binop->rep == turboshaft::WordRepresentation::Word64();
+        switch (binop->kind) {
+          case Kind::kAdd:
+            return VisitBinop(selector, node, is64 ? kX64Add : kX64Add32, cont);
+          case Kind::kSub:
+            return VisitBinop(selector, node, is64 ? kX64Sub : kX64Sub32, cont);
+          case Kind::kBitwiseAnd:
+            return VisitBinop(selector, node, is64 ? kX64And : kX64And32, cont);
+          case Kind::kBitwiseOr:
+            return VisitBinop(selector, node, is64 ? kX64Or : kX64Or32, cont);
+          default:
+            break;
+        }
       }
-      if (selector->CanUseRootsRegister()) {
-        InstructionCode opcode =
-            kX64Cmp32 | AddressingModeField::encode(kMode_Root);
-        return VisitCompare(
-            selector, opcode,
-            g.TempImmediate(
-                MacroAssemblerBase::RootRegisterOffsetForRootIndex(root_index)),
-            g.UseRegister(left), cont);
+    } else if (const auto shift = op.TryCast<turboshaft::ShiftOp>()) {
+      using Kind = turboshaft::ShiftOp::Kind;
+      if (selector->IsOnlyUserOfNodeInSameBlock(user, node)) {
+        switch (shift->kind) {
+          case Kind::kShiftLeft:
+          case Kind::kShiftRightLogical:
+            UNIMPLEMENTED();
+          default:
+            break;
+        }
       }
     }
   }
-  VisitWordCompare(selector, node, kX64Cmp32, cont);
+
+  int effect_level = selector->GetEffectLevel(node, cont);
+  if (const auto load = op.TryCast<turboshaft::LoadOp>()) {
+    if (load->loaded_rep == turboshaft::MemoryRepresentation::Int8()) {
+      if (opcode == kX64Cmp32) {
+        opcode = kX64Cmp8;
+      } else if (opcode == kX64Test32) {
+        opcode = kX64Test8;
+      }
+    } else if (load->loaded_rep == turboshaft::MemoryRepresentation::Int16()) {
+      if (opcode == kX64Cmp32) {
+        opcode = kX64Cmp16;
+      } else if (opcode == kX64Test32) {
+        opcode = kX64Test16;
+      }
+    }
+  }
+  if (g.CanBeMemoryOperand(opcode, user, node, effect_level)) {
+    VisitCompareWithMemoryOperand(selector, opcode, node, g.TempImmediate(0),
+                                  cont);
+  } else {
+    VisitCompare(selector, opcode, g.Use(node), g.TempImmediate(0), cont);
+  }
 }
 
 // Shared routine for comparison with zero.
-template <typename Adapter>
-void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
-                      Node* node, InstructionCode opcode,
-                      FlagsContinuation* cont) {
-  X64OperandGeneratorT<Adapter> g(selector);
+void VisitCompareZero(InstructionSelectorT<TurbofanAdapter>* selector,
+                      Node* user, Node* node, InstructionCode opcode,
+                      FlagsContinuationT<TurbofanAdapter>* cont) {
+  X64OperandGeneratorT<TurbofanAdapter> g(selector);
   if (cont->IsBranch() &&
       (cont->condition() == kNotEqual || cont->condition() == kEqual)) {
     switch (node->opcode()) {
@@ -2794,8 +2972,10 @@ void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
 #undef FLAGS_SET_BINOP_LIST
 #undef FLAGS_SET_BINOP
 
-#define TRY_VISIT_WORD32_SHIFT TryVisitWordShift<Adapter, Int32BinopMatcher, 32>
-#define TRY_VISIT_WORD64_SHIFT TryVisitWordShift<Adapter, Int64BinopMatcher, 64>
+#define TRY_VISIT_WORD32_SHIFT \
+  TryVisitWordShift<TurbofanAdapter, Int32BinopMatcher, 32>
+#define TRY_VISIT_WORD64_SHIFT \
+  TryVisitWordShift<TurbofanAdapter, Int64BinopMatcher, 64>
 // Skip Word64Sar/Word32Sar since no instruction reduction in most cases.
 #define FLAGS_SET_SHIFT_LIST(V)                    \
   V(kWord32Shl, TRY_VISIT_WORD32_SHIFT, kX64Shl32) \
@@ -2849,10 +3029,11 @@ void VisitCompareZero(InstructionSelectorT<Adapter>* selector, Node* user,
 
 // Shared routine for multiple float32 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   InstructionCode const opcode =
       selector->IsSupported(AVX) ? kAVXFloat32Cmp : kSSEFloat32Cmp;
   VisitCompare(selector, opcode, right, left, cont, false);
@@ -2860,10 +3041,11 @@ void VisitFloat32Compare(InstructionSelectorT<Adapter>* selector, Node* node,
 
 // Shared routine for multiple float64 compare operations (inputs commuted).
 template <typename Adapter>
-void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector, Node* node,
-                         FlagsContinuation* cont) {
-  Node* const left = node->InputAt(0);
-  Node* const right = node->InputAt(1);
+void VisitFloat64Compare(InstructionSelectorT<Adapter>* selector,
+                         typename Adapter::node_t node,
+                         FlagsContinuationT<Adapter>* cont) {
+  auto left = selector->input_at(node, 0);
+  auto right = selector->input_at(node, 1);
   InstructionCode const opcode =
       selector->IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
   VisitCompare(selector, opcode, right, left, cont, false);
@@ -2921,8 +3103,35 @@ void VisitAtomicCompareExchange(InstructionSelectorT<Adapter>* selector,
 }  // namespace
 
 // Shared routine for word comparison against zero.
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWordCompareZero(
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitWordCompareZero(
+    node_t user, node_t value, FlagsContinuation* cont) {
+  for (const turboshaft::EqualOp* equal =
+           this->turboshaft_graph()->Get(value).TryCast<turboshaft::EqualOp>();
+       equal != nullptr && CanCover(user, value);) {
+    const turboshaft::ConstantOp* right_constant =
+        this->turboshaft_graph()
+            ->Get(equal->right())
+            .TryCast<turboshaft::ConstantOp>();
+    if (right_constant == nullptr) break;
+    if (right_constant->kind != turboshaft::ConstantOp::Kind::kWord32) break;
+    if (right_constant->word32() != 0) break;
+
+    user = value;
+    value = equal->left();
+    cont->Negate();
+  }
+
+  if (CanCover(user, value)) {
+    // TODO(nicohartmann@): Implement for Turboshaft.
+  }
+
+  // Branch could not be combined with a compare, emit compare against 0.
+  VisitCompareZero(this, user, value, kX64Cmp32, cont);
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitWordCompareZero(
     Node* user, Node* value, FlagsContinuation* cont) {
   // Try to combine with comparisons against 0 by simply inverting the branch.
   while (value->opcode() == IrOpcode::kWord32Equal && CanCover(user, value)) {
@@ -3079,7 +3288,8 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
   InstructionOperand value_operand = g.UseRegister(node->InputAt(0));
 
   // Emit either ArchTableSwitch or ArchBinarySearchSwitch.
-  if (enable_switch_jump_table_ == kEnableSwitchJumpTable) {
+  if (enable_switch_jump_table_ ==
+      InstructionSelector::kEnableSwitchJumpTable) {
     static const size_t kMaxTableSwitchValueRange = 2 << 16;
     size_t table_space_cost = 4 + sw.value_range();
     size_t table_time_cost = 3;
@@ -3115,37 +3325,43 @@ void InstructionSelectorT<Adapter>::VisitSwitch(Node* node,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32Equal(Node* const node) {
-  Node* user = node;
+void InstructionSelectorT<Adapter>::VisitWord32Equal(node_t node) {
+  node_t user = node;
   FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int32BinopMatcher m(user);
-  if (m.right().Is(0)) {
-    return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    USE(user);
+    UNIMPLEMENTED();
+  } else {
+    Int32BinopMatcher m(user);
+    if (m.right().Is(0)) {
+      return VisitWordCompareZero(m.node(), m.left().node(), &cont);
+    }
   }
   VisitWord32EqualImpl(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThan(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
   VisitWordCompare(this, node, kX64Cmp32, &cont);
@@ -3153,115 +3369,155 @@ void InstructionSelectorT<Adapter>::VisitUint32LessThanOrEqual(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitWord64Equal(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
-  Int64BinopMatcher m(node);
-  if (m.right().Is(0)) {
-    // Try to combine the equality check with a comparison.
-    Node* const user = m.node();
-    Node* const value = m.left().node();
-    if (CanCover(user, value)) {
-      switch (value->opcode()) {
-        case IrOpcode::kInt64Sub:
-          return VisitWordCompare(this, value, kX64Cmp, &cont);
-        case IrOpcode::kWord64And:
-          return VisitWordCompare(this, value, kX64Test, &cont);
-        default:
-          break;
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kEqual, node);
+    Int64BinopMatcher m(node);
+    if (m.right().Is(0)) {
+      // Try to combine the equality check with a comparison.
+      Node* const user = m.node();
+      Node* const value = m.left().node();
+      if (CanCover(user, value)) {
+        switch (value->opcode()) {
+          case IrOpcode::kInt64Sub:
+            return VisitWordCompare(this, value, kX64Cmp, &cont);
+          case IrOpcode::kWord64And:
+            return VisitWordCompare(this, value, kX64Test, &cont);
+          default:
+            break;
+        }
       }
     }
+    VisitWord64EqualImpl(this, node, &cont);
   }
-  VisitWord64EqualImpl(this, node, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32AddWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Add32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Add32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Add32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Add32, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt32SubWithOverflow(Node* node) {
-  if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
-    FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
-    return VisitBinop(this, node, kX64Sub32, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    if (Node* ovf = NodeProperties::FindProjection(node, 1)) {
+      FlagsContinuation cont = FlagsContinuation::ForSet(kOverflow, ovf);
+      return VisitBinop(this, node, kX64Sub32, &cont);
+    }
+    FlagsContinuation cont;
+    VisitBinop(this, node, kX64Sub32, &cont);
   }
-  FlagsContinuation cont;
-  VisitBinop(this, node, kX64Sub32, &cont);
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kSignedLessThan, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitInt64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kSignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThan(Node* node) {
-  FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont = FlagsContinuation::ForSet(kUnsignedLessThan, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitUint64LessThanOrEqual(Node* node) {
-  FlagsContinuation cont =
-      FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
-  VisitWordCompare(this, node, kX64Cmp, &cont);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    FlagsContinuation cont =
+        FlagsContinuation::ForSet(kUnsignedLessThanOrEqual, node);
+    VisitWordCompare(this, node, kX64Cmp, &cont);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThan(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThan(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat32LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat32Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64Equal(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64Equal(node_t node) {
   FlagsContinuation cont = FlagsContinuation::ForSet(kUnorderedEqual, node);
   VisitFloat64Compare(this, node, &cont);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
-  Float64BinopMatcher m(node);
-  if (m.left().Is(0.0) && m.right().IsFloat64Abs()) {
-    // This matches the pattern
-    //
-    //   Float64LessThan(#0.0, Float64Abs(x))
-    //
-    // which TurboFan generates for NumberToBoolean in the general case,
-    // and which evaluates to false if x is 0, -0 or NaN. We can compile
-    // this to a simple (v)ucomisd using not_equal flags condition, which
-    // avoids the costly Float64Abs.
-    FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, node);
-    InstructionCode const opcode =
-        IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
-    return VisitCompare(this, opcode, m.left().node(), m.right().InputAt(0),
-                        &cont, false);
+void InstructionSelectorT<Adapter>::VisitFloat64LessThan(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Implement this.
+    UNIMPLEMENTED();
+  } else {
+    Float64BinopMatcher m(node);
+    if (m.left().Is(0.0) && m.right().IsFloat64Abs()) {
+      // This matches the pattern
+      //
+      //   Float64LessThan(#0.0, Float64Abs(x))
+      //
+      // which TurboFan generates for NumberToBoolean in the general case,
+      // and which evaluates to false if x is 0, -0 or NaN. We can compile
+      // this to a simple (v)ucomisd using not_equal flags condition, which
+      // avoids the costly Float64Abs.
+      FlagsContinuation cont = FlagsContinuation::ForSet(kNotEqual, node);
+      InstructionCode const opcode =
+          IsSupported(AVX) ? kAVXFloat64Cmp : kSSEFloat64Cmp;
+      return VisitCompare(this, opcode, m.left().node(), m.right().InputAt(0),
+                          &cont, false);
+    }
   }
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThan, node);
@@ -3269,7 +3525,7 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThan(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
   FlagsContinuation cont =
       FlagsContinuation::ForSet(kUnsignedGreaterThanOrEqual, node);
   VisitFloat64Compare(this, node, &cont);
@@ -4968,15 +5224,14 @@ void InstructionSelectorT<Adapter>::VisitI32x4DotI8x16I7x16AddS(Node* node) {
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::AddOutputToSelectContinuation(
-    OperandGenerator* g, int first_input_index, Node* node) {
+    OperandGenerator* g, int first_input_index, node_t node) {
   continuation_outputs_.push_back(
       g->DefineSameAsInput(node, first_input_index));
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::Flags
-InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
+InstructionSelector::SupportedMachineOperatorFlags() {
   MachineOperatorBuilder::Flags flags =
       MachineOperatorBuilder::kWord32ShiftIsSafe |
       MachineOperatorBuilder::kWord32Ctz | MachineOperatorBuilder::kWord64Ctz |
@@ -5001,9 +5256,8 @@ InstructionSelectorT<Adapter>::SupportedMachineOperatorFlags() {
 }
 
 // static
-template <typename Adapter>
 MachineOperatorBuilder::AlignmentRequirements
-InstructionSelectorT<Adapter>::AlignmentRequirements() {
+InstructionSelector::AlignmentRequirements() {
   return MachineOperatorBuilder::AlignmentRequirements::
       FullUnalignedAccessSupport();
 }
diff --git a/src/compiler/node.h b/src/compiler/node.h
index b7776e78e77..4b225c1a25b 100644
--- a/src/compiler/node.h
+++ b/src/compiler/node.h
@@ -106,6 +106,7 @@ class V8_EXPORT_PRIVATE Node final {
 
   class Inputs;
   inline Inputs inputs() const;
+  inline base::Vector<Node*> inputs_vector() const;
 
   class UseEdges final {
    public:
@@ -484,6 +485,16 @@ Node::Inputs Node::inputs() const {
   }
 }
 
+base::Vector<Node*> Node::inputs_vector() const {
+  int inline_count = InlineCountField::decode(bit_field_);
+  if (inline_count != kOutlineMarker) {
+    return base::VectorOf<Node*>(inline_inputs(), inline_count);
+  } else {
+    return base::VectorOf<Node*>(outline_inputs()->inputs(),
+                                 outline_inputs()->count_);
+  }
+}
+
 // A forward iterator to visit the edges for the input dependencies of a node.
 class Node::InputEdges::iterator final {
  public:
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index 1c766427753..b5149852d70 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -88,6 +88,7 @@
 #include "src/compiler/turboshaft/graph-visualizer.h"
 #include "src/compiler/turboshaft/graph.h"
 #include "src/compiler/turboshaft/index.h"
+#include "src/compiler/turboshaft/instruction-selection-phase.h"
 #include "src/compiler/turboshaft/machine-lowering-phase.h"
 #include "src/compiler/turboshaft/optimization-phase.h"
 #include "src/compiler/turboshaft/optimize-phase.h"
@@ -375,9 +376,19 @@ class PipelineData {
   Graph* graph() const { return graph_; }
   void set_graph(Graph* graph) { graph_ = graph; }
   turboshaft::PipelineData CreateTurboshaftPipeline() {
-    return turboshaft::PipelineData{info_,        schedule_, graph_zone_,
-                                    broker_,      isolate_,  source_positions_,
-                                    node_origins_};
+    return turboshaft::PipelineData{info_,
+                                    schedule_,
+                                    graph_zone_,
+                                    broker_,
+                                    isolate_,
+                                    source_positions_,
+                                    node_origins_,
+                                    sequence_,
+                                    frame_,
+                                    assembler_options_,
+                                    &max_unoptimized_frame_height_,
+                                    &max_pushed_argument_count_,
+                                    instruction_zone_};
   }
   SourcePositionTable* source_positions() const { return source_positions_; }
   NodeOriginTable* node_origins() const { return node_origins_; }
@@ -751,9 +762,17 @@ class PipelineImpl final {
   void Revectorize();
 #endif  // V8_ENABLE_WASM_SIMD256_REVEC
 
-  // Substep B.2. Select instructions from a scheduled graph.
+  // Substep B.2.turbofan Select instructions from a scheduled graph.
   bool SelectInstructions(Linkage* linkage);
 
+  // Substep B.2.turboshaft Select instructions from a turboshaft graph.
+  bool SelectInstructionsTurboshaft(
+      Linkage* linkage,
+      base::Optional<turboshaft::PipelineData::Scope>& turboshaft_scope);
+
+  // Substep B.3. Run register allocation on the instruction sequence.
+  bool AllocateRegisters(CallDescriptor* call_descriptor);
+
   // Step C. Run the code assembly pass.
   void AssembleCode(Linkage* linkage);
 
@@ -1359,6 +1378,7 @@ auto PipelineImpl::Run(Args&&... args) {
     using result_t =
         decltype(phase.Run(scope.zone(), std::forward<Args>(args)...));
     CodeTracer* code_tracer = nullptr;
+    USE(code_tracer);
     if (turboshaft::PipelineData::Get().info()->trace_turbo_graph()) {
       // NOTE: We must not call `GetCodeTracer` if tracing is not enabled,
       // because it may not yet be initialized then and doing so from the
@@ -1367,13 +1387,17 @@ auto PipelineImpl::Run(Args&&... args) {
     }
     if constexpr (std::is_same_v<result_t, void>) {
       phase.Run(scope.zone(), std::forward<Args>(args)...);
-      turboshaft::PrintTurboshaftGraph(scope.zone(), code_tracer,
-                                       Phase::phase_name());
+      if constexpr (turboshaft::produces_printable_graph<Phase>::value) {
+        turboshaft::PrintTurboshaftGraph(scope.zone(), code_tracer,
+                                         Phase::phase_name());
+      }
       return;
     } else {
       auto result = phase.Run(scope.zone(), std::forward<Args>(args)...);
-      turboshaft::PrintTurboshaftGraph(scope.zone(), code_tracer,
-                                       Phase::phase_name());
+      if constexpr (turboshaft::produces_printable_graph<Phase>::value) {
+        turboshaft::PrintTurboshaftGraph(scope.zone(), code_tracer,
+                                         Phase::phase_name());
+      }
       return result;
     }
   }
@@ -2397,82 +2421,45 @@ struct RevectorizePhase {
 };
 #endif  // V8_ENABLE_WASM_SIMD256_REVEC
 
-struct InstructionRangesAsJSON {
-  const InstructionSequence* sequence;
-  const ZoneVector<std::pair<int, int>>* instr_origins;
-};
-
-std::ostream& operator<<(std::ostream& out, const InstructionRangesAsJSON& s) {
-  const int max = static_cast<int>(s.sequence->LastInstructionIndex());
-
-  out << ", \"nodeIdToInstructionRange\": {";
-  bool need_comma = false;
-  for (size_t i = 0; i < s.instr_origins->size(); ++i) {
-    std::pair<int, int> offset = (*s.instr_origins)[i];
-    if (offset.first == -1) continue;
-    const int first = max - offset.first + 1;
-    const int second = max - offset.second + 1;
-    if (need_comma) out << ", ";
-    out << "\"" << i << "\": [" << first << ", " << second << "]";
-    need_comma = true;
-  }
-  out << "}";
-  out << ", \"blockIdToInstructionRange\": {";
-  need_comma = false;
-  for (auto block : s.sequence->instruction_blocks()) {
-    if (need_comma) out << ", ";
-    out << "\"" << block->rpo_number() << "\": [" << block->code_start() << ", "
-        << block->code_end() << "]";
-    need_comma = true;
-  }
-  out << "}";
-  return out;
-}
-
 struct InstructionSelectionPhase {
   DECL_PIPELINE_PHASE_CONSTANTS(SelectInstructions)
 
   base::Optional<BailoutReason> Run(PipelineData* data, Zone* temp_zone,
                                     Linkage* linkage) {
-    if (v8_flags.turboshaft && v8_flags.turboshaft_instruction_selection) {
-      UNIMPLEMENTED();
-    } else {
-      InstructionSelector selector(
-          temp_zone, data->graph()->NodeCount(), linkage, data->sequence(),
-          data->schedule(), data->source_positions(), data->frame(),
-          data->info()->switch_jump_table()
-              ? InstructionSelector::kEnableSwitchJumpTable
-              : InstructionSelector::kDisableSwitchJumpTable,
-          &data->info()->tick_counter(), data->broker(),
-          data->address_of_max_unoptimized_frame_height(),
-          data->address_of_max_pushed_argument_count(),
-          data->info()->source_positions()
-              ? InstructionSelector::kAllSourcePositions
-              : InstructionSelector::kCallSourcePositions,
-          InstructionSelector::SupportedFeatures(),
-          v8_flags.turbo_instruction_scheduling
-              ? InstructionSelector::kEnableScheduling
-              : InstructionSelector::kDisableScheduling,
-          data->assembler_options().enable_root_relative_access
-              ? InstructionSelector::kEnableRootsRelativeAddressing
-              : InstructionSelector::kDisableRootsRelativeAddressing,
-          data->info()->trace_turbo_json()
-              ? InstructionSelector::kEnableTraceTurboJson
-              : InstructionSelector::kDisableTraceTurboJson);
-      if (base::Optional<BailoutReason> bailout =
-              selector.SelectInstructions()) {
-        return bailout;
-      }
-      if (data->info()->trace_turbo_json()) {
-        TurboJsonFile json_of(data->info(), std::ios_base::app);
-        json_of << "{\"name\":\"" << phase_name()
-                << "\",\"type\":\"instructions\""
-                << InstructionRangesAsJSON{data->sequence(),
-                                           &selector.instr_origins()}
-                << "},\n";
-      }
-      return base::nullopt;
+    InstructionSelector selector = InstructionSelector::ForTurbofan(
+        temp_zone, data->graph()->NodeCount(), linkage, data->sequence(),
+        data->schedule(), data->source_positions(), data->frame(),
+        data->info()->switch_jump_table()
+            ? InstructionSelector::kEnableSwitchJumpTable
+            : InstructionSelector::kDisableSwitchJumpTable,
+        &data->info()->tick_counter(), data->broker(),
+        data->address_of_max_unoptimized_frame_height(),
+        data->address_of_max_pushed_argument_count(),
+        data->info()->source_positions()
+            ? InstructionSelector::kAllSourcePositions
+            : InstructionSelector::kCallSourcePositions,
+        InstructionSelector::SupportedFeatures(),
+        v8_flags.turbo_instruction_scheduling
+            ? InstructionSelector::kEnableScheduling
+            : InstructionSelector::kDisableScheduling,
+        data->assembler_options().enable_root_relative_access
+            ? InstructionSelector::kEnableRootsRelativeAddressing
+            : InstructionSelector::kDisableRootsRelativeAddressing,
+        data->info()->trace_turbo_json()
+            ? InstructionSelector::kEnableTraceTurboJson
+            : InstructionSelector::kDisableTraceTurboJson);
+    if (base::Optional<BailoutReason> bailout = selector.SelectInstructions()) {
+      return bailout;
+    }
+    if (data->info()->trace_turbo_json()) {
+      TurboJsonFile json_of(data->info(), std::ios_base::app);
+      json_of << "{\"name\":\"" << phase_name()
+              << "\",\"type\":\"instructions\""
+              << InstructionRangesAsJSON{data->sequence(),
+                                         &selector.instr_origins()}
+              << "},\n";
     }
+    return base::nullopt;
   }
 };
 
@@ -3090,7 +3077,7 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
     UnparkedScopeIfNeeded scope(data->broker(),
                                 v8_flags.turboshaft_trace_reduction);
 
-    turboshaft::PipelineData::Scope turboshaft_pipeline(
+    base::Optional<turboshaft::PipelineData::Scope> turboshaft_pipeline(
         data->CreateTurboshaftPipeline());
     turboshaft::Tracing::Scope tracing_scope(data->info());
 
@@ -3120,6 +3107,13 @@ bool PipelineImpl::OptimizeGraph(Linkage* linkage) {
     Run<turboshaft::DeadCodeEliminationPhase>();
     Run<turboshaft::DecompressionOptimizationPhase>();
 
+    if (v8_flags.turboshaft_instruction_selection) {
+      // Run Turboshaft instruction selection.
+      return SelectInstructionsTurboshaft(linkage, turboshaft_pipeline);
+    }
+
+    // Otherwise, translate back to Turbofan and run instruction selection on
+    // the sea of nodes graph.
     auto [new_graph, new_schedule] =
         Run<turboshaft::RecreateSchedulePhase>(linkage);
     data->set_graph(new_graph);
@@ -3948,6 +3942,61 @@ bool PipelineImpl::SelectInstructions(Linkage* linkage) {
 
   data->DeleteGraphZone();
 
+  return AllocateRegisters(call_descriptor);
+}
+
+bool PipelineImpl::SelectInstructionsTurboshaft(
+    Linkage* linkage,
+    base::Optional<turboshaft::PipelineData::Scope>& turboshaft_scope) {
+  auto call_descriptor = linkage->GetIncomingDescriptor();
+  PipelineData* turbofan_data = this->data_;
+
+  turboshaft_scope->Value().InitializeInstructionSequence(call_descriptor);
+
+  // Depending on which code path led us to this function, the frame may or
+  // may not have been initialized. If it hasn't yet, initialize it now.
+  if (!turbofan_data->frame()) {
+    turbofan_data->InitializeFrameData(call_descriptor);
+  }
+  // Select and schedule instructions covering the scheduled graph.
+  if (base::Optional<BailoutReason> bailout =
+          Run<turboshaft::InstructionSelectionPhase>(linkage)) {
+    info()->AbortOptimization(*bailout);
+    turbofan_data->EndPhaseKind();
+    return false;
+  }
+
+  // TODO(nicohartmann@): We might need to provide this.
+  // if (info()->trace_turbo_json()) {
+  //   UnparkedScopeIfNeeded scope(turbofan_data->broker());
+  //   AllowHandleDereference allow_deref;
+  //   TurboCfgFile tcf(isolate());
+  //   tcf << AsC1V("CodeGen", turbofan_data->schedule(),
+  //                turbofan_data->source_positions(),
+  //                turbofan_data->sequence());
+
+  //   std::ostringstream source_position_output;
+  //   // Output source position information before the graph is deleted.
+  //   if (data_->source_positions() != nullptr) {
+  //     data_->source_positions()->PrintJson(source_position_output);
+  //   } else {
+  //     source_position_output << "{}";
+  //   }
+  //   source_position_output << ",\n\"nodeOrigins\" : ";
+  //   data_->node_origins()->PrintJson(source_position_output);
+  //   data_->set_source_position_output(source_position_output.str());
+  // }
+
+  turboshaft_scope.reset();
+  turbofan_data->DeleteGraphZone();
+
+  return AllocateRegisters(call_descriptor);
+}
+
+bool PipelineImpl::AllocateRegisters(CallDescriptor* call_descriptor) {
+  PipelineData* data = this->data_;
+  DCHECK_NOT_NULL(data->sequence());
+
   data->BeginPhaseKind("V8.TFRegisterAllocation");
 
   bool run_verifier = v8_flags.turbo_verify_allocation;
@@ -4362,6 +4411,33 @@ ObserveNodeManager* PipelineImpl::observe_node_manager() const {
   return data_->observe_node_manager();
 }
 
+std::ostream& operator<<(std::ostream& out, const InstructionRangesAsJSON& s) {
+  const int max = static_cast<int>(s.sequence->LastInstructionIndex());
+
+  out << ", \"nodeIdToInstructionRange\": {";
+  bool need_comma = false;
+  for (size_t i = 0; i < s.instr_origins->size(); ++i) {
+    std::pair<int, int> offset = (*s.instr_origins)[i];
+    if (offset.first == -1) continue;
+    const int first = max - offset.first + 1;
+    const int second = max - offset.second + 1;
+    if (need_comma) out << ", ";
+    out << "\"" << i << "\": [" << first << ", " << second << "]";
+    need_comma = true;
+  }
+  out << "}";
+  out << ", \"blockIdToInstructionRange\": {";
+  need_comma = false;
+  for (auto block : s.sequence->instruction_blocks()) {
+    if (need_comma) out << ", ";
+    out << "\"" << block->rpo_number() << "\": [" << block->code_start() << ", "
+        << block->code_end() << "]";
+    need_comma = true;
+  }
+  out << "}";
+  return out;
+}
+
 }  // namespace compiler
 }  // namespace internal
 }  // namespace v8
diff --git a/src/compiler/pipeline.h b/src/compiler/pipeline.h
index ba59b59b182..f0ca767fba7 100644
--- a/src/compiler/pipeline.h
+++ b/src/compiler/pipeline.h
@@ -40,6 +40,13 @@ class Schedule;
 class SourcePositionTable;
 struct WasmCompilationData;
 
+struct InstructionRangesAsJSON {
+  const InstructionSequence* sequence;
+  const ZoneVector<std::pair<int, int>>* instr_origins;
+};
+
+std::ostream& operator<<(std::ostream& out, const InstructionRangesAsJSON& s);
+
 class Pipeline : public AllStatic {
  public:
   // Returns a new compilation job for the given JavaScript function.
diff --git a/src/compiler/turboshaft/graph.h b/src/compiler/turboshaft/graph.h
index 0cd6858e138..f1aced9b51d 100644
--- a/src/compiler/turboshaft/graph.h
+++ b/src/compiler/turboshaft/graph.h
@@ -237,7 +237,7 @@ class RandomAccessStackDominatorNode
  public:
   void SetDominator(Derived* dominator);
   void SetAsDominatorRoot();
-  Derived* GetDominator() { return nxt_; }
+  Derived* GetDominator() const { return nxt_; }
 
   // Returns the lowest common dominator of {this} and {other}.
   Derived* GetCommonDominator(
@@ -779,6 +779,7 @@ class Graph {
             base::DerefPtrIterator<const Block>(bound_blocks_.data() +
                                                 bound_blocks_.size())};
   }
+  const ZoneVector<Block*>& blocks_vector() const { return bound_blocks_; }
 
   bool IsLoopBackedge(const GotoOp& op) const {
     DCHECK(op.destination->IsBound());
diff --git a/src/compiler/turboshaft/instruction-selection-phase.cc b/src/compiler/turboshaft/instruction-selection-phase.cc
new file mode 100644
index 00000000000..0480c955a8f
--- /dev/null
+++ b/src/compiler/turboshaft/instruction-selection-phase.cc
@@ -0,0 +1,55 @@
+// Copyright 2023 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#include "src/compiler/turboshaft/instruction-selection-phase.h"
+
+#include "src/codegen/optimized-compilation-info.h"
+#include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/instruction-selector.h"
+#include "src/compiler/graph-visualizer.h"
+#include "src/compiler/pipeline.h"
+
+namespace v8::internal::compiler::turboshaft {
+
+base::Optional<BailoutReason> InstructionSelectionPhase::Run(Zone* temp_zone,
+                                                             Linkage* linkage) {
+  turboshaft::PipelineData* data = &turboshaft::PipelineData::Get();
+  turboshaft::Graph& graph = turboshaft::PipelineData::Get().graph();
+
+  InstructionSelector selector = InstructionSelector::ForTurboshaft(
+      temp_zone, graph.op_id_count(), linkage, data->sequence(), &graph,
+      data->source_positions(), data->frame(),
+      data->info()->switch_jump_table()
+          ? InstructionSelector::kEnableSwitchJumpTable
+          : InstructionSelector::kDisableSwitchJumpTable,
+      &data->info()->tick_counter(), data->broker(),
+      data->address_of_max_unoptimized_frame_height(),
+      data->address_of_max_pushed_argument_count(),
+      data->info()->source_positions()
+          ? InstructionSelector::kAllSourcePositions
+          : InstructionSelector::kCallSourcePositions,
+      InstructionSelector::SupportedFeatures(),
+      v8_flags.turbo_instruction_scheduling
+          ? InstructionSelector::kEnableScheduling
+          : InstructionSelector::kDisableScheduling,
+      data->assembler_options().enable_root_relative_access
+          ? InstructionSelector::kEnableRootsRelativeAddressing
+          : InstructionSelector::kDisableRootsRelativeAddressing,
+      data->info()->trace_turbo_json()
+          ? InstructionSelector::kEnableTraceTurboJson
+          : InstructionSelector::kDisableTraceTurboJson);
+  if (base::Optional<BailoutReason> bailout = selector.SelectInstructions()) {
+    return bailout;
+  }
+  if (data->info()->trace_turbo_json()) {
+    TurboJsonFile json_of(data->info(), std::ios_base::app);
+    json_of << "{\"name\":\"" << phase_name() << "\",\"type\":\"instructions\""
+            << InstructionRangesAsJSON{data->sequence(),
+                                       &selector.instr_origins()}
+            << "},\n";
+  }
+  return base::nullopt;
+}
+
+}  // namespace v8::internal::compiler::turboshaft
diff --git a/src/compiler/turboshaft/instruction-selection-phase.h b/src/compiler/turboshaft/instruction-selection-phase.h
new file mode 100644
index 00000000000..e559f9564f6
--- /dev/null
+++ b/src/compiler/turboshaft/instruction-selection-phase.h
@@ -0,0 +1,26 @@
+// Copyright 2023 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+#ifndef V8_COMPILER_TURBOSHAFT_INSTRUCTION_SELECTION_PHASE_H_
+#define V8_COMPILER_TURBOSHAFT_INSTRUCTION_SELECTION_PHASE_H_
+
+#include "src/compiler/turboshaft/phase.h"
+
+namespace v8::internal::compiler::turboshaft {
+
+struct InstructionSelectionPhase {
+  DECL_TURBOSHAFT_PHASE_CONSTANTS(InstructionSelection)
+
+  base::Optional<BailoutReason> Run(Zone* temp_zone, Linkage* linkage);
+};
+
+// Disable printing a default turboshaft graph as this phase produces an
+// instruction seqeuence rather than a new graph.
+template <>
+struct produces_printable_graph<InstructionSelectionPhase>
+    : public std::false_type {};
+
+}  // namespace v8::internal::compiler::turboshaft
+
+#endif  // V8_COMPILER_TURBOSHAFT_INSTRUCTION_SELECTION_PHASE_H_
diff --git a/src/compiler/turboshaft/operations.h b/src/compiler/turboshaft/operations.h
index d62994b2d0f..d042e3bbab4 100644
--- a/src/compiler/turboshaft/operations.h
+++ b/src/compiler/turboshaft/operations.h
@@ -546,6 +546,8 @@ class SaturatedUint8 {
   void SetToOne() { val = 1; }
 
   bool IsZero() const { return val == 0; }
+  bool IsOne() const { return val == 1; }
+  bool IsSaturated() const { return val == kMax; }
   uint8_t Get() const { return val; }
 
  private:
@@ -1259,6 +1261,8 @@ struct EqualOp : FixedArityOperationT<2, EqualOp> {
   OpIndex left() const { return input(0); }
   OpIndex right() const { return input(1); }
 
+  static bool IsCommutative() { return true; }
+
   bool ValidInputRep(
       base::Vector<const RegisterRepresentation> input_reps) const;
 
diff --git a/src/compiler/turboshaft/phase.h b/src/compiler/turboshaft/phase.h
index 8d27d04bd1b..3f454693ab6 100644
--- a/src/compiler/turboshaft/phase.h
+++ b/src/compiler/turboshaft/phase.h
@@ -5,7 +5,11 @@
 #ifndef V8_COMPILER_TURBOSHAFT_PHASE_H_
 #define V8_COMPILER_TURBOSHAFT_PHASE_H_
 
+#include <type_traits>
+
 #include "src/base/contextual.h"
+#include "src/codegen/assembler.h"
+#include "src/compiler/backend/instruction.h"
 #include "src/compiler/compiler-source-position-table.h"
 #include "src/compiler/node-origin-table.h"
 #include "src/compiler/phase.h"
@@ -22,13 +26,21 @@ class Schedule;
 
 namespace v8::internal::compiler::turboshaft {
 
+template <typename P>
+struct produces_printable_graph : public std::true_type {};
+
 class PipelineData : public base::ContextualClass<PipelineData> {
  public:
   explicit PipelineData(OptimizedCompilationInfo* const& info,
                         Schedule*& schedule, Zone*& graph_zone,
                         JSHeapBroker*& broker, Isolate* const& isolate,
                         SourcePositionTable*& source_positions,
-                        NodeOriginTable*& node_origins)
+                        NodeOriginTable*& node_origins,
+                        InstructionSequence*& sequence, Frame*& frame,
+                        AssemblerOptions& assembler_options,
+                        size_t* address_of_max_unoptimized_frame_height,
+                        size_t* address_of_max_pushed_argument_count,
+                        Zone*& instruction_zone)
       : info_(info),
         schedule_(schedule),
         graph_zone_(graph_zone),
@@ -36,6 +48,14 @@ class PipelineData : public base::ContextualClass<PipelineData> {
         isolate_(isolate),
         source_positions_(source_positions),
         node_origins_(node_origins),
+        sequence_(sequence),
+        frame_(frame),
+        assembler_options_(assembler_options),
+        address_of_max_unoptimized_frame_height_(
+            address_of_max_unoptimized_frame_height),
+        address_of_max_pushed_argument_count_(
+            address_of_max_pushed_argument_count),
+        instruction_zone_(instruction_zone),
         graph_(std::make_unique<turboshaft::Graph>(graph_zone_)) {}
 
   bool has_graph() const { return graph_ != nullptr; }
@@ -48,9 +68,32 @@ class PipelineData : public base::ContextualClass<PipelineData> {
   Isolate* isolate() const { return isolate_; }
   SourcePositionTable* source_positions() const { return source_positions_; }
   NodeOriginTable* node_origins() const { return node_origins_; }
+  InstructionSequence* sequence() const { return sequence_; }
+  Frame* frame() const { return frame_; }
+  AssemblerOptions& assembler_options() const { return assembler_options_; }
+  size_t* address_of_max_unoptimized_frame_height() const {
+    return address_of_max_unoptimized_frame_height_;
+  }
+  size_t* address_of_max_pushed_argument_count() const {
+    return address_of_max_pushed_argument_count_;
+  }
+  Zone* instruction_zone() const { return instruction_zone_; }
 
   void reset_schedule() { schedule_ = nullptr; }
 
+  void InitializeInstructionSequence(const CallDescriptor* call_descriptor) {
+    DCHECK_NULL(sequence_);
+    InstructionBlocks* instruction_blocks =
+        InstructionSequence::InstructionBlocksFor(instruction_zone(), *graph_);
+    sequence_ = instruction_zone()->New<InstructionSequence>(
+        isolate(), instruction_zone(), instruction_blocks);
+    if (call_descriptor && call_descriptor->RequiresFrameAsIncoming()) {
+      sequence_->instruction_blocks()[0]->mark_needs_frame();
+    } else {
+      DCHECK(call_descriptor->CalleeSavedFPRegisters().is_empty());
+    }
+  }
+
  private:
   // Turbofan's PipelineData owns most of these objects. We only hold references
   // to them.
@@ -63,6 +106,12 @@ class PipelineData : public base::ContextualClass<PipelineData> {
   Isolate* const& isolate_;
   SourcePositionTable*& source_positions_;
   NodeOriginTable*& node_origins_;
+  InstructionSequence*& sequence_;
+  Frame*& frame_;
+  AssemblerOptions& assembler_options_;
+  size_t* address_of_max_unoptimized_frame_height_;
+  size_t* address_of_max_pushed_argument_count_;
+  Zone*& instruction_zone_;
 
   std::unique_ptr<turboshaft::Graph> graph_;
 };
diff --git a/src/logging/runtime-call-stats.h b/src/logging/runtime-call-stats.h
index 8d9031571ad..7ec67b20d15 100644
--- a/src/logging/runtime-call-stats.h
+++ b/src/logging/runtime-call-stats.h
@@ -378,6 +378,7 @@ class RuntimeCallTimer final {
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftBuildGraph)            \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftDeadCodeElimination)   \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftDecompressionOpt)      \
+  ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftInstructionSelection)  \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftLateOptimization)      \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftMachineLowering)       \
   ADD_THREAD_SPECIFIC_COUNTER(V, Optimize, TurboshaftOptimize)              \
diff --git a/test/cctest/compiler/test-instruction-scheduler.cc b/test/cctest/compiler/test-instruction-scheduler.cc
index ab65b5d485d..39b0c704f39 100644
--- a/test/cctest/compiler/test-instruction-scheduler.cc
+++ b/test/cctest/compiler/test-instruction-scheduler.cc
@@ -4,6 +4,7 @@
 
 #include "src/compiler/backend/instruction-scheduler.h"
 #include "src/compiler/backend/instruction-selector-impl.h"
+#include "src/compiler/backend/instruction-selector.h"
 #include "src/compiler/backend/instruction.h"
 #include "test/cctest/cctest.h"
 
@@ -11,6 +12,8 @@ namespace v8 {
 namespace internal {
 namespace compiler {
 
+using FlagsContinuation = FlagsContinuationT<TurbofanAdapter>;
+
 // Create InstructionBlocks with a single block.
 InstructionBlocks* CreateSingleBlock(Zone* zone) {
   InstructionBlock* block = zone->New<InstructionBlock>(
diff --git a/test/unittests/compiler/backend/instruction-selector-unittest.cc b/test/unittests/compiler/backend/instruction-selector-unittest.cc
index 30928fe94a8..77d6fde45c8 100644
--- a/test/unittests/compiler/backend/instruction-selector-unittest.cc
+++ b/test/unittests/compiler/backend/instruction-selector-unittest.cc
@@ -44,7 +44,7 @@ InstructionSelectorTest::Stream InstructionSelectorTest::StreamBuilder::Build(
   TickCounter tick_counter;
   size_t max_unoptimized_frame_height = 0;
   size_t max_pushed_argument_count = 0;
-  InstructionSelector selector(
+  InstructionSelector selector = InstructionSelector::ForTurbofan(
       test_->zone(), node_count, &linkage, &sequence, schedule,
       &source_position_table, nullptr,
       InstructionSelector::kEnableSwitchJumpTable, &tick_counter, nullptr,
-- 
2.35.1

