From b9c4a8495560cd3ee14f32aa9f6cf5acee18811a Mon Sep 17 00:00:00 2001
From: Jakob Kummerow <jkummerow@chromium.org>
Date: Tue, 28 Jun 2022 12:14:27 +0200
Subject: [PATCH] [wasm] Fix and harden all conditional tier-up checks

This remodels the tier-up checks on loop back edges to avoid
modifying the cache state by taking temp registers passed in
from the caller, and not causing the instance to get cached.

Additionally, this introduces FreezeCacheState scopes, which
allow us to enforce that certain ranges don't cause any cache
modifications. Conditional jumps require such a scope to be
around, which should help ensure that we don't forget to add
them to any future code we write.

Drive-by cleanup: drop {pinned} lists from a few Load helper
functions. They don't allocate registers (and shouldn't), so
they don't need to know about pinned registers.

Fixed: chromium:1339321
Change-Id: I1c7660418a85259e96c5e0dcfeaf12dab2114e8c
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3724787
Reviewed-by: Manos Koukoutos <manoskouk@chromium.org>
Commit-Queue: Jakob Kummerow <jkummerow@chromium.org>
Cr-Commit-Position: refs/heads/main@{#81411}
---
 src/wasm/baseline/arm/liftoff-assembler-arm.h |  61 +-
 .../baseline/arm64/liftoff-assembler-arm64.h  |  23 +-
 .../baseline/ia32/liftoff-assembler-ia32.h    |  28 +-
 src/wasm/baseline/liftoff-assembler.cc        |   8 +
 src/wasm/baseline/liftoff-assembler.h         |  71 +-
 src/wasm/baseline/liftoff-compiler.cc         | 885 ++++++++++--------
 .../loong64/liftoff-assembler-loong64.h       |  17 +-
 .../baseline/mips/liftoff-assembler-mips.h    |   6 +-
 .../mips64/liftoff-assembler-mips64.h         |  23 +-
 src/wasm/baseline/ppc/liftoff-assembler-ppc.h |  25 +-
 .../riscv64/liftoff-assembler-riscv64.h       |  23 +-
 .../baseline/s390/liftoff-assembler-s390.h    |  25 +-
 src/wasm/baseline/x64/liftoff-assembler-x64.h |  26 +-
 .../regress/wasm/regress-crbug-1339321.js     |  66 ++
 14 files changed, 786 insertions(+), 501 deletions(-)
 create mode 100644 test/mjsunit/regress/wasm/regress-crbug-1339321.js

diff --git a/src/wasm/baseline/arm/liftoff-assembler-arm.h b/src/wasm/baseline/arm/liftoff-assembler-arm.h
index 79f8fb6d369..18efbb0c68a 100644
--- a/src/wasm/baseline/arm/liftoff-assembler-arm.h
+++ b/src/wasm/baseline/arm/liftoff-assembler-arm.h
@@ -643,7 +643,6 @@ namespace liftoff {
 inline void LoadInternal(LiftoffAssembler* lasm, LiftoffRegister dst,
                          Register src_addr, Register offset_reg,
                          int32_t offset_imm, LoadType type,
-                         LiftoffRegList pinned,
                          uint32_t* protected_load_pc = nullptr,
                          bool is_load_mem = false) {
   DCHECK_IMPLIES(type.value_type() == kWasmI64, dst.is_gp_pair());
@@ -738,11 +737,10 @@ inline void LoadInternal(LiftoffAssembler* lasm, LiftoffRegister dst,
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   static_assert(kTaggedSize == kInt32Size);
   liftoff::LoadInternal(this, LiftoffRegister(dst), src_addr, offset_reg,
-                        offset_imm, LoadType::kI32Load, pinned);
+                        offset_imm, LoadType::kI32Load);
 }
 
 void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,
@@ -794,13 +792,12 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uint32_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   // Offsets >=2GB are statically OOB on 32-bit systems.
   DCHECK_LE(offset_imm, std::numeric_limits<int32_t>::max());
   liftoff::LoadInternal(this, dst, src_addr, offset_reg,
-                        static_cast<int32_t>(offset_imm), type, pinned,
+                        static_cast<int32_t>(offset_imm), type,
                         protected_load_pc, is_load_mem);
 }
 
@@ -1079,7 +1076,7 @@ void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
                                   Register offset_reg, uint32_t offset_imm,
                                   LoadType type, LiftoffRegList pinned) {
   if (type.value() != LoadType::kI64Load) {
-    Load(dst, src_addr, offset_reg, offset_imm, type, pinned, nullptr, true);
+    Load(dst, src_addr, offset_reg, offset_imm, type, nullptr, true);
     dmb(ISH);
     return;
   }
@@ -1385,9 +1382,34 @@ void LiftoffAssembler::LoadReturnStackSlot(LiftoffRegister dst, int offset,
 void LiftoffAssembler::MoveStackValue(uint32_t dst_offset, uint32_t src_offset,
                                       ValueKind kind) {
   DCHECK_NE(dst_offset, src_offset);
-  LiftoffRegister reg = GetUnusedRegister(reg_class_for(kind), {});
-  Fill(reg, src_offset, kind);
-  Spill(dst_offset, reg, kind);
+  bool must_pop = false;
+  Register scratch = no_reg;
+  if (cache_state()->has_unused_register(kGpReg)) {
+    scratch = cache_state()->unused_register(kGpReg).gp();
+  } else {
+    scratch = r0;
+    Push(scratch);
+    must_pop = true;
+  }
+  const int kRegSize = 4;
+  DCHECK_EQ(0, SlotSizeForType(kind) % kRegSize);
+  int words = SlotSizeForType(kind) / kRegSize;
+  if (src_offset < dst_offset) {
+    do {
+      ldr(scratch, liftoff::GetStackSlot(src_offset));
+      str(scratch, liftoff::GetStackSlot(dst_offset));
+      dst_offset -= kSystemPointerSize;
+      src_offset -= kSystemPointerSize;
+    } while (--words);
+  } else {
+    while (words--) {
+      ldr(scratch, liftoff::GetStackSlot(src_offset - words * kRegSize));
+      str(scratch, liftoff::GetStackSlot(dst_offset - words * kRegSize));
+    }
+  }
+  if (must_pop) {
+    Pop(scratch);
+  }
 }
 
 void LiftoffAssembler::Move(Register dst, Register src, ValueKind kind) {
@@ -2209,7 +2231,8 @@ void LiftoffAssembler::emit_jump(Register target) { bx(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
 
   if (rhs == no_reg) {
@@ -2225,15 +2248,16 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   cmp(lhs, Operand(imm));
   b(label, cond);
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   sub(value, value, Operand(subtrahend), SetCC);
   b(result_negative, mi);
 }
@@ -2337,7 +2361,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   tst(obj, Operand(kSmiTagMask));
   Condition condition = mode == kJumpOnSmi ? eq : ne;
   b(condition, target);
diff --git a/src/wasm/baseline/arm64/liftoff-assembler-arm64.h b/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
index 4f049f1d2a7..8f3143eca17 100644
--- a/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
+++ b/src/wasm/baseline/arm64/liftoff-assembler-arm64.h
@@ -469,8 +469,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   UseScratchRegisterScope temps(this);
   MemOperand src_op =
       liftoff::GetMemOp(this, &temps, src_addr, offset_reg, offset_imm);
@@ -526,9 +525,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   UseScratchRegisterScope temps(this);
   MemOperand src_op = liftoff::GetMemOp(this, &temps, src_addr, offset_reg,
                                         offset_imm, i64_offset);
@@ -1555,7 +1553,8 @@ void LiftoffAssembler::emit_jump(Register target) { Br(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   switch (kind) {
     case kI32:
@@ -1586,15 +1585,16 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   Cmp(lhs.W(), Operand(imm));
   B(label, cond);
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   Subs(value.W(), value.W(), Immediate(subtrahend));
   B(result_negative, mi);
 }
@@ -1657,7 +1657,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   Label* smi_label = mode == kJumpOnSmi ? target : nullptr;
   Label* not_smi_label = mode == kJumpOnNotSmi ? target : nullptr;
   JumpIfSmi(obj, smi_label, not_smi_label);
diff --git a/src/wasm/baseline/ia32/liftoff-assembler-ia32.h b/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
index ec48b69a19c..4e67fc8f2ab 100644
--- a/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
+++ b/src/wasm/baseline/ia32/liftoff-assembler-ia32.h
@@ -384,12 +384,11 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   DCHECK_GE(offset_imm, 0);
   static_assert(kTaggedSize == kInt32Size);
   Load(LiftoffRegister(dst), src_addr, offset_reg,
-       static_cast<uint32_t>(offset_imm), LoadType::kI32Load, pinned);
+       static_cast<uint32_t>(offset_imm), LoadType::kI32Load);
 }
 
 void LiftoffAssembler::LoadFullPointer(Register dst, Register src_addr,
@@ -433,9 +432,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uint32_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   // Offsets >=2GB are statically OOB on 32-bit systems.
   DCHECK_LE(offset_imm, std::numeric_limits<int32_t>::max());
   DCHECK_EQ(type.value_type() == kWasmI64, dst.is_gp_pair());
@@ -580,7 +578,7 @@ void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
                                   Register offset_reg, uint32_t offset_imm,
                                   LoadType type, LiftoffRegList pinned) {
   if (type.value() != LoadType::kI64Load) {
-    Load(dst, src_addr, offset_reg, offset_imm, type, pinned, nullptr, true);
+    Load(dst, src_addr, offset_reg, offset_imm, type, nullptr, true);
     return;
   }
 
@@ -2460,7 +2458,8 @@ void LiftoffAssembler::emit_jump(Register target) { jmp(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (rhs != no_reg) {
     switch (kind) {
@@ -2484,16 +2483,16 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 }
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
-                                           Label* label, Register lhs,
-                                           int imm) {
+                                           Label* label, Register lhs, int imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   cmp(lhs, Immediate(imm));
   j(cond, label);
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   sub(value, Immediate(subtrahend));
   j(negative, result_negative);
 }
@@ -2640,7 +2639,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   test_b(obj, Immediate(kSmiTagMask));
   Condition condition = mode == kJumpOnSmi ? zero : not_zero;
   j(condition, target);
diff --git a/src/wasm/baseline/liftoff-assembler.cc b/src/wasm/baseline/liftoff-assembler.cc
index 93caeb2be14..6cf7df8044e 100644
--- a/src/wasm/baseline/liftoff-assembler.cc
+++ b/src/wasm/baseline/liftoff-assembler.cc
@@ -1189,6 +1189,10 @@ void LiftoffAssembler::MoveToReturnLocations(
   // original state in case it is needed after the current instruction
   // (conditional branch).
   CacheState saved_state;
+#if DEBUG
+  uint32_t saved_state_frozenness = cache_state_.frozen;
+  cache_state_.frozen = 0;
+#endif
   saved_state.Split(*cache_state());
   int call_desc_return_idx = 0;
   DCHECK_LE(sig->return_count(), cache_state_.stack_height());
@@ -1241,6 +1245,9 @@ void LiftoffAssembler::MoveToReturnLocations(
     }
   }
   cache_state()->Steal(saved_state);
+#if DEBUG
+  cache_state_.frozen = saved_state_frozenness;
+#endif
 }
 
 #ifdef ENABLE_SLOW_DCHECKS
@@ -1337,6 +1344,7 @@ LiftoffRegister LiftoffAssembler::SpillAdjacentFpRegisters(
 }
 
 void LiftoffAssembler::SpillRegister(LiftoffRegister reg) {
+  DCHECK(!cache_state_.frozen);
   int remaining_uses = cache_state_.get_use_count(reg);
   DCHECK_LT(0, remaining_uses);
   for (uint32_t idx = cache_state_.stack_height() - 1;; --idx) {
diff --git a/src/wasm/baseline/liftoff-assembler.h b/src/wasm/baseline/liftoff-assembler.h
index 24945f1a345..623ea778a54 100644
--- a/src/wasm/baseline/liftoff-assembler.h
+++ b/src/wasm/baseline/liftoff-assembler.h
@@ -95,6 +95,20 @@ inline constexpr LiftoffCondition Flip(LiftoffCondition cond) {
   }
 }
 
+class LiftoffAssembler;
+class FreezeCacheState {
+ public:
+#if DEBUG
+  explicit FreezeCacheState(LiftoffAssembler& assm);
+  ~FreezeCacheState();
+
+ private:
+  LiftoffAssembler& assm_;
+#else
+  explicit FreezeCacheState(LiftoffAssembler& assm) {}
+#endif
+};
+
 class LiftoffAssembler : public TurboAssembler {
  public:
   // Each slot in our stack frame currently has exactly 8 bytes.
@@ -224,12 +238,17 @@ class LiftoffAssembler : public TurboAssembler {
     void DefineSafepointWithCalleeSavedRegisters(
         SafepointTableBuilder::Safepoint& safepoint);
 
+    // TODO(jkummerow): Wrap all accesses to {stack_state} in accessors that
+    // check {frozen}.
     base::SmallVector<VarState, 8> stack_state;
     LiftoffRegList used_registers;
     uint32_t register_use_count[kAfterMaxLiftoffRegCode] = {0};
     LiftoffRegList last_spilled_regs;
     Register cached_instance = no_reg;
     Register cached_mem_start = no_reg;
+#if DEBUG
+    uint32_t frozen = 0;
+#endif
 
     bool has_unused_register(RegClass rc, LiftoffRegList pinned = {}) const {
       if (kNeedI64RegPair && rc == kGpRegPair) {
@@ -286,6 +305,7 @@ class LiftoffAssembler : public TurboAssembler {
     }
 
     LiftoffRegister take_volatile_register(LiftoffRegList candidates) {
+      DCHECK(!frozen);
       DCHECK(has_volatile_register(candidates));
       Register reg = no_reg;
       if (cached_instance != no_reg && candidates.has(cached_instance)) {
@@ -305,6 +325,7 @@ class LiftoffAssembler : public TurboAssembler {
     }
 
     void SetCacheRegister(Register* cache, Register reg) {
+      DCHECK(!frozen);
       DCHECK_EQ(no_reg, *cache);
       *cache = reg;
       int liftoff_code = LiftoffRegister{reg}.liftoff_code();
@@ -337,6 +358,7 @@ class LiftoffAssembler : public TurboAssembler {
     }
 
     void ClearCacheRegister(Register* cache) {
+      DCHECK(!frozen);
       DCHECK(cache == &cached_instance || cache == &cached_mem_start);
       if (*cache == no_reg) return;
       int liftoff_code = LiftoffRegister{*cache}.liftoff_code();
@@ -358,6 +380,7 @@ class LiftoffAssembler : public TurboAssembler {
     }
 
     void inc_used(LiftoffRegister reg) {
+      DCHECK(!frozen);
       if (reg.is_pair()) {
         inc_used(reg.low());
         inc_used(reg.high());
@@ -370,6 +393,7 @@ class LiftoffAssembler : public TurboAssembler {
 
     // Returns whether this was the last use.
     void dec_used(LiftoffRegister reg) {
+      DCHECK(!frozen);
       DCHECK(is_used(reg));
       if (reg.is_pair()) {
         dec_used(reg.low());
@@ -399,6 +423,7 @@ class LiftoffAssembler : public TurboAssembler {
     }
 
     void clear_used(LiftoffRegister reg) {
+      DCHECK(!frozen);
       if (reg.is_pair()) {
         clear_used(reg.low());
         clear_used(reg.high());
@@ -411,11 +436,13 @@ class LiftoffAssembler : public TurboAssembler {
     bool is_free(LiftoffRegister reg) const { return !is_used(reg); }
 
     void reset_used_registers() {
+      DCHECK(!frozen);
       used_registers = {};
       memset(register_use_count, 0, sizeof(register_use_count));
     }
 
     LiftoffRegister GetNextSpillReg(LiftoffRegList candidates) {
+      DCHECK(!frozen);
       DCHECK(!candidates.is_empty());
       // This method should only be called if none of the candidates is free.
       DCHECK(candidates.MaskOut(used_registers).is_empty());
@@ -566,6 +593,7 @@ class LiftoffAssembler : public TurboAssembler {
   LiftoffRegister GetUnusedRegister(
       RegClass rc, std::initializer_list<LiftoffRegister> try_first,
       LiftoffRegList pinned) {
+    DCHECK(!cache_state_.frozen);
     for (LiftoffRegister reg : try_first) {
       DCHECK_EQ(reg.reg_class(), rc);
       if (cache_state_.is_free(reg)) return reg;
@@ -575,6 +603,7 @@ class LiftoffAssembler : public TurboAssembler {
 
   // Get an unused register for class {rc}, potentially spilling to free one.
   LiftoffRegister GetUnusedRegister(RegClass rc, LiftoffRegList pinned) {
+    DCHECK(!cache_state_.frozen);
     if (kNeedI64RegPair && rc == kGpRegPair) {
       LiftoffRegList candidates = kGpCacheRegList.MaskOut(pinned);
       Register low = candidates.clear(GetUnusedRegister(candidates)).gp();
@@ -596,6 +625,7 @@ class LiftoffAssembler : public TurboAssembler {
 
   // Get an unused register of {candidates}, potentially spilling to free one.
   LiftoffRegister GetUnusedRegister(LiftoffRegList candidates) {
+    DCHECK(!cache_state_.frozen);
     DCHECK(!candidates.is_empty());
     if (cache_state_.has_unused_register(candidates)) {
       return cache_state_.unused_register(candidates);
@@ -687,7 +717,13 @@ class LiftoffAssembler : public TurboAssembler {
 
   void MoveToReturnLocations(const FunctionSig*,
                              compiler::CallDescriptor* descriptor);
-
+#if DEBUG
+  void SetCacheStateFrozen() { cache_state_.frozen++; }
+  void UnfreezeCacheState() {
+    DCHECK_GT(cache_state_.frozen, 0);
+    cache_state_.frozen--;
+  }
+#endif
 #ifdef ENABLE_SLOW_DCHECKS
   // Validate that the register use counts reflect the state of the cache.
   bool ValidateCacheState() const;
@@ -725,8 +761,7 @@ class LiftoffAssembler : public TurboAssembler {
   inline void SpillInstance(Register instance);
   inline void ResetOSRTarget();
   inline void LoadTaggedPointer(Register dst, Register src_addr,
-                                Register offset_reg, int32_t offset_imm,
-                                LiftoffRegList pinned);
+                                Register offset_reg, int32_t offset_imm);
   inline void LoadFullPointer(Register dst, Register src_addr,
                               int32_t offset_imm);
   enum SkipWriteBarrier : bool {
@@ -740,25 +775,24 @@ class LiftoffAssembler : public TurboAssembler {
   void LoadFixedArrayLengthAsInt32(LiftoffRegister dst, Register array,
                                    LiftoffRegList pinned) {
     int offset = FixedArray::kLengthOffset - kHeapObjectTag;
-    LoadSmiAsInt32(dst, array, offset, pinned);
+    LoadSmiAsInt32(dst, array, offset);
   }
-  void LoadSmiAsInt32(LiftoffRegister dst, Register src_addr, int32_t offset,
-                      LiftoffRegList pinned) {
+  void LoadSmiAsInt32(LiftoffRegister dst, Register src_addr, int32_t offset) {
     if (SmiValuesAre32Bits()) {
 #if V8_TARGET_LITTLE_ENDIAN
       DCHECK_EQ(kSmiShiftSize + kSmiTagSize, 4 * kBitsPerByte);
       offset += 4;
 #endif
-      Load(dst, src_addr, no_reg, offset, LoadType::kI32Load, pinned);
+      Load(dst, src_addr, no_reg, offset, LoadType::kI32Load);
     } else {
       DCHECK(SmiValuesAre31Bits());
-      Load(dst, src_addr, no_reg, offset, LoadType::kI32Load, pinned);
+      Load(dst, src_addr, no_reg, offset, LoadType::kI32Load);
       emit_i32_sari(dst.gp(), dst.gp(), kSmiTagSize);
     }
   }
   inline void IncrementSmi(LiftoffRegister dst, int offset);
   inline void Load(LiftoffRegister dst, Register src_addr, Register offset_reg,
-                   uintptr_t offset_imm, LoadType type, LiftoffRegList pinned,
+                   uintptr_t offset_imm, LoadType type,
                    uint32_t* protected_load_pc = nullptr,
                    bool is_load_mem = false, bool i64_offset = false);
   inline void Store(Register dst_addr, Register offset_reg,
@@ -1028,11 +1062,13 @@ class LiftoffAssembler : public TurboAssembler {
   inline void emit_jump(Register);
 
   inline void emit_cond_jump(LiftoffCondition, Label*, ValueKind value,
-                             Register lhs, Register rhs = no_reg);
+                             Register lhs, Register rhs,
+                             const FreezeCacheState& frozen);
   inline void emit_i32_cond_jumpi(LiftoffCondition, Label*, Register lhs,
-                                  int imm);
+                                  int imm, const FreezeCacheState& frozen);
   inline void emit_i32_subi_jump_negative(Register value, int subtrahend,
-                                          Label* result_negative);
+                                          Label* result_negative,
+                                          const FreezeCacheState& frozen);
   // Set {dst} to 1 if condition holds, 0 otherwise.
   inline void emit_i32_eqz(Register dst, Register src);
   inline void emit_i32_set_cond(LiftoffCondition, Register dst, Register lhs,
@@ -1052,7 +1088,8 @@ class LiftoffAssembler : public TurboAssembler {
                           LiftoffRegister false_value, ValueKind kind);
 
   enum SmiCheckMode { kJumpOnSmi, kJumpOnNotSmi };
-  inline void emit_smi_check(Register obj, Label* target, SmiCheckMode mode);
+  inline void emit_smi_check(Register obj, Label* target, SmiCheckMode mode,
+                             const FreezeCacheState& frozen);
 
   inline void LoadTransform(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
@@ -1625,6 +1662,14 @@ class LiftoffAssembler : public TurboAssembler {
 
 std::ostream& operator<<(std::ostream& os, LiftoffAssembler::VarState);
 
+#if DEBUG
+inline FreezeCacheState::FreezeCacheState(LiftoffAssembler& assm)
+    : assm_(assm) {
+  assm.SetCacheStateFrozen();
+}
+inline FreezeCacheState::~FreezeCacheState() { assm_.UnfreezeCacheState(); }
+#endif
+
 // =======================================================================
 // Partially platform-independent implementations of the platform-dependent
 // part.
diff --git a/src/wasm/baseline/liftoff-compiler.cc b/src/wasm/baseline/liftoff-compiler.cc
index 4b19b9e9b9a..eefaab5c095 100644
--- a/src/wasm/baseline/liftoff-compiler.cc
+++ b/src/wasm/baseline/liftoff-compiler.cc
@@ -47,6 +47,15 @@ namespace {
 
 #define __ asm_.
 
+// It's important that we don't modify the LiftoffAssembler's cache state
+// in conditionally-executed code paths. Creating these witnesses helps
+// enforce that (using DCHECKs in the cache state).
+// Conditional jump instructions require a witness to have been created (to
+// make sure we don't forget); the witness should stay alive until the label
+// is bound where regular control flow resumes. This implies that when we're
+// jumping to a trap, the live range of the witness isn't important.
+#define FREEZE_STATE(witness_name) FreezeCacheState witness_name(asm_)
+
 #define TRACE(...)                                            \
   do {                                                        \
     if (FLAG_trace_liftoff) PrintF("[liftoff] " __VA_ARGS__); \
@@ -731,7 +740,7 @@ class LiftoffCompiler {
   }
 
   void TierupCheck(FullDecoder* decoder, WasmCodePosition position,
-                   int budget_used) {
+                   int budget_used, Register tmp1, Register tmp2) {
     // We should always decrement the budget, and we don't expect integer
     // overflows in the budget calculation.
     DCHECK_LE(1, budget_used);
@@ -742,17 +751,30 @@ class LiftoffCompiler {
     const int kMax = FLAG_wasm_tiering_budget / 4;
     if (budget_used > kMax) budget_used = kMax;
 
-    LiftoffRegList pinned;
-    LiftoffRegister budget_reg =
-        pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    LiftoffRegister array_reg =
-        pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    LOAD_INSTANCE_FIELD(array_reg.gp(), TieringBudgetArray, kSystemPointerSize,
-                        pinned);
+    LiftoffRegister budget_reg(tmp2);
+    // Be careful not to cause caching of the instance.
+    Register instance = __ cache_state()->cached_instance;
+    if (instance == no_reg) {
+      instance = tmp1;
+      __ LoadInstanceFromFrame(instance);
+    }
+    constexpr int kArraySize = kSystemPointerSize;
+    constexpr int kArrayOffset =
+        WASM_INSTANCE_OBJECT_FIELD_OFFSET(TieringBudgetArray);
+    static_assert(WASM_INSTANCE_OBJECT_FIELD_SIZE(TieringBudgetArray) ==
+                  kArraySize);
+    Register array_reg = tmp1;  // Overwriting {instance}.
+    __ LoadFromInstance(array_reg, instance, kArrayOffset, kArraySize);
     uint32_t offset =
         kInt32Size * declared_function_index(env_->module, func_index_);
-    __ Load(budget_reg, array_reg.gp(), no_reg, offset, LoadType::kI32Load,
-            pinned);
+#if V8_TARGET_ARCH_ARM || V8_TARGET_ARCH_ARM64
+    // Platforms where both this load and the later store would have to
+    // explicitly add the offset can save code size by performing the addition
+    // only once.
+    __ emit_ptrsize_addi(array_reg, array_reg, offset);
+    offset = 0;
+#endif
+    __ Load(budget_reg, array_reg, no_reg, offset, LoadType::kI32Load, {});
     LiftoffRegList regs_to_save = __ cache_state()->used_registers;
     // The cached instance will be reloaded separately.
     if (__ cache_state()->cached_instance != no_reg) {
@@ -770,10 +792,10 @@ class LiftoffCompiler {
         position, regs_to_save, __ cache_state()->cached_instance, spilled_regs,
         safepoint_info, RegisterOOLDebugSideTableEntry(decoder)));
     OutOfLineCode& ool = out_of_line_code_.back();
+    FREEZE_STATE(trapping);
     __ emit_i32_subi_jump_negative(budget_reg.gp(), budget_used,
-                                   ool.label.get());
-    __ Store(array_reg.gp(), no_reg, offset, budget_reg, StoreType::kI32Store,
-             pinned);
+                                   ool.label.get(), trapping);
+    __ Store(array_reg, no_reg, offset, budget_reg, StoreType::kI32Store, {});
     __ bind(ool.continuation.get());
   }
 
@@ -854,8 +876,7 @@ class LiftoffCompiler {
           WASM_INSTANCE_OBJECT_FIELD_OFFSET(FeedbackVectors));
       __ LoadTaggedPointer(tmp.gp(), tmp.gp(), no_reg,
                            wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(
-                               declared_func_index),
-                           pinned);
+                               declared_func_index));
       __ Spill(liftoff::kFeedbackVectorOffset, tmp, kPointerKind);
     }
     if (for_debugging_) __ ResetOSRTarget();
@@ -1083,12 +1104,13 @@ class LiftoffCompiler {
       // Check the "hook on function call" flag. If set, trigger a break.
       LOAD_INSTANCE_FIELD(flag, HookOnFunctionCallAddress, kSystemPointerSize,
                           {});
+      FREEZE_STATE(frozen);
       __ Load(LiftoffRegister{flag}, flag, no_reg, 0, LoadType::kI32Load8U, {});
-      __ emit_cond_jump(kNotEqualZero, &do_break, kI32, flag);
+      __ emit_cond_jump(kNotEqualZero, &do_break, kI32, flag, no_reg, frozen);
 
       // Check if we should stop on "script entry".
       LOAD_INSTANCE_FIELD(flag, BreakOnEntry, kUInt8Size, {});
-      __ emit_cond_jump(kEqualZero, &no_break, kI32, flag);
+      __ emit_cond_jump(kEqualZero, &no_break, kI32, flag, no_reg, frozen);
 
       __ bind(&do_break);
       EmitBreakpoint(decoder);
@@ -1113,16 +1135,18 @@ class LiftoffCompiler {
       pinned.set(max_steps);
       LiftoffRegister max_steps_addr = __ GetUnusedRegister(kGpReg, pinned);
       pinned.set(max_steps_addr);
-      __ LoadConstant(
-          max_steps_addr,
-          WasmValue::ForUintPtr(reinterpret_cast<uintptr_t>(max_steps_)));
-      __ Load(max_steps, max_steps_addr.gp(), no_reg, 0, LoadType::kI32Load,
-              pinned);
-      Label cont;
-      __ emit_i32_cond_jumpi(kUnequal, &cont, max_steps.gp(), 0);
-      // Abort.
-      Trap(decoder, kTrapUnreachable);
-      __ bind(&cont);
+      {
+        FREEZE_STATE(frozen);
+        __ LoadConstant(
+            max_steps_addr,
+            WasmValue::ForUintPtr(reinterpret_cast<uintptr_t>(max_steps_)));
+        __ Load(max_steps, max_steps_addr.gp(), no_reg, 0, LoadType::kI32Load);
+        Label cont;
+        __ emit_i32_cond_jumpi(kUnequal, &cont, max_steps.gp(), 0, frozen);
+        // Abort.
+        Trap(decoder, kTrapUnreachable);
+        __ bind(&cont);
+      }
       __ emit_i32_subi(max_steps.gp(), max_steps.gp(), 1);
       __ Store(max_steps_addr.gp(), no_reg, 0, max_steps, StoreType::kI32Store,
                pinned);
@@ -1251,17 +1275,20 @@ class LiftoffCompiler {
     LOAD_TAGGED_PTR_INSTANCE_FIELD(imm_tag, TagsTable, pinned);
     __ LoadTaggedPointer(
         imm_tag, imm_tag, no_reg,
-        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index), {});
+        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index));
 
     CODE_COMMENT("compare tags");
-    Label caught;
-    __ emit_cond_jump(kEqual, &caught, kI32, imm_tag, caught_tag.gp());
-    // The tags don't match, merge the current state into the catch state and
-    // jump to the next handler.
-    __ MergeFullStackWith(block->try_info->catch_state, *__ cache_state());
-    __ emit_jump(&block->try_info->catch_label);
-
-    __ bind(&caught);
+    {
+      FREEZE_STATE(frozen);
+      Label caught;
+      __ emit_cond_jump(kEqual, &caught, kI32, imm_tag, caught_tag.gp(),
+                        frozen);
+      // The tags don't match, merge the current state into the catch state and
+      // jump to the next handler.
+      __ MergeFullStackWith(block->try_info->catch_state, *__ cache_state());
+      __ emit_jump(&block->try_info->catch_label);
+      __ bind(&caught);
+    }
     if (!block->try_info->in_handler) {
       block->try_info->in_handler = true;
       num_exceptions_++;
@@ -1331,14 +1358,17 @@ class LiftoffCompiler {
     }
   }
 
-  void JumpIfFalse(FullDecoder* decoder, Label* false_dst) {
+  void JumpIfFalse(FullDecoder* decoder, Label* false_dst,
+                   std::unique_ptr<FreezeCacheState>& will_freeze) {
     LiftoffCondition cond =
         test_and_reset_outstanding_op(kExprI32Eqz) ? kNotEqualZero : kEqualZero;
 
     if (!has_outstanding_op()) {
       // Unary comparison.
       Register value = __ PopToRegister().gp();
-      __ emit_cond_jump(cond, false_dst, kI32, value);
+      will_freeze.reset(new FreezeCacheState(asm_));
+      __ emit_cond_jump(cond, false_dst, kI32, value, no_reg,
+                        *will_freeze.get());
       return;
     }
 
@@ -1351,7 +1381,8 @@ class LiftoffCompiler {
       int32_t rhs_imm = rhs_slot.i32_const();
       __ cache_state()->stack_state.pop_back();
       Register lhs = __ PopToRegister().gp();
-      __ emit_i32_cond_jumpi(cond, false_dst, lhs, rhs_imm);
+      will_freeze.reset(new FreezeCacheState(asm_));
+      __ emit_i32_cond_jumpi(cond, false_dst, lhs, rhs_imm, *will_freeze.get());
       return;
     }
 
@@ -1362,13 +1393,16 @@ class LiftoffCompiler {
       int32_t lhs_imm = lhs_slot.i32_const();
       __ cache_state()->stack_state.pop_back();
       // Flip the condition, because {lhs} and {rhs} are swapped.
-      __ emit_i32_cond_jumpi(Flip(cond), false_dst, rhs, lhs_imm);
+      will_freeze.reset(new FreezeCacheState(asm_));
+      __ emit_i32_cond_jumpi(Flip(cond), false_dst, rhs, lhs_imm,
+                             *will_freeze.get());
       return;
     }
 
     // Compare two arbitrary values.
     Register lhs = __ PopToRegister(LiftoffRegList{rhs}).gp();
-    __ emit_cond_jump(cond, false_dst, kI32, lhs, rhs);
+    will_freeze.reset(new FreezeCacheState(asm_));
+    __ emit_cond_jump(cond, false_dst, kI32, lhs, rhs, *will_freeze.get());
   }
 
   void If(FullDecoder* decoder, const Value& cond, Control* if_block) {
@@ -1379,7 +1413,9 @@ class LiftoffCompiler {
     if_block->else_state = std::make_unique<ElseState>();
 
     // Test the condition on the value stack, jump to else if zero.
-    JumpIfFalse(decoder, if_block->else_state->label.get());
+    std::unique_ptr<FreezeCacheState> frozen;
+    JumpIfFalse(decoder, if_block->else_state->label.get(), frozen);
+    frozen.reset();
 
     // Store the state (after popping the value) for executing the else branch.
     if_block->else_state->state.Split(*__ cache_state());
@@ -1606,7 +1642,9 @@ class LiftoffCompiler {
             __ GetUnusedRegister(kGpReg, LiftoffRegList{dst});
         LiftoffRegister dst_regs[] = {ret_reg, dst};
         GenerateCCall(dst_regs, &sig, dst_kind, &src, ext_ref);
-        __ emit_cond_jump(kEqual, trap, kI32, ret_reg.gp());
+        // It's okay that this is short-lived: we're trapping anyway.
+        FREEZE_STATE(trapping);
+        __ emit_cond_jump(kEqual, trap, kI32, ret_reg.gp(), no_reg, trapping);
       } else {
         ValueKind sig_kinds[] = {src_kind};
         ValueKindSig sig(0, 1, sig_kinds);
@@ -1836,11 +1874,13 @@ class LiftoffCompiler {
     LiftoffRegister result_regs[] = {ret, dst};
     auto sig = MakeSig::Returns(kI32).Params(kI64, kI64);
     GenerateCCall(result_regs, &sig, kI64, arg_regs, ext_ref);
+    FREEZE_STATE(trapping);
     __ LoadConstant(tmp, WasmValue(int32_t{0}));
-    __ emit_cond_jump(kEqual, trap_by_zero, kI32, ret.gp(), tmp.gp());
+    __ emit_cond_jump(kEqual, trap_by_zero, kI32, ret.gp(), tmp.gp(), trapping);
     if (trap_unrepresentable) {
       __ LoadConstant(tmp, WasmValue(int32_t{-1}));
-      __ emit_cond_jump(kEqual, trap_unrepresentable, kI32, ret.gp(), tmp.gp());
+      __ emit_cond_jump(kEqual, trap_unrepresentable, kI32, ret.gp(), tmp.gp(),
+                        trapping);
     }
   }
 
@@ -2237,14 +2277,30 @@ class LiftoffCompiler {
     __ DeallocateStackSlot(sizeof(int64_t));
   }
 
-  void TierupCheckOnExit(FullDecoder* decoder) {
+  void TierupCheckOnTailCall(FullDecoder* decoder) {
     if (!dynamic_tiering()) return;
-    TierupCheck(decoder, decoder->position(), __ pc_offset());
+    LiftoffRegList pinned;
+    Register tmp1 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    TierupCheck(decoder, decoder->position(), __ pc_offset(), tmp1, tmp2);
   }
 
-  void DoReturn(FullDecoder* decoder, uint32_t /* drop_values */) {
+  void DoReturn(FullDecoder* decoder, uint32_t /* drop values */) {
+    Register tmp1 = no_reg;
+    Register tmp2 = no_reg;
+    if (dynamic_tiering()) {
+      LiftoffRegList pinned;
+      tmp1 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+      tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    }
+    ReturnImpl(decoder, tmp1, tmp2);
+  }
+
+  void ReturnImpl(FullDecoder* decoder, Register tmp1, Register tmp2) {
     if (FLAG_trace_wasm) TraceFunctionExit(decoder);
-    TierupCheckOnExit(decoder);
+    if (dynamic_tiering()) {
+      TierupCheck(decoder, decoder->position(), __ pc_offset(), tmp1, tmp2);
+    }
     size_t num_returns = decoder->sig_->return_count();
     if (num_returns > 0) __ MoveToReturnLocations(decoder->sig_, descriptor_);
     __ LeaveFrame(StackFrame::WASM);
@@ -2343,7 +2399,7 @@ class LiftoffCompiler {
       LOAD_INSTANCE_FIELD(addr, ImportedMutableGlobals, kSystemPointerSize,
                           *pinned);
       __ Load(LiftoffRegister(addr), addr, no_reg,
-              global->index * sizeof(Address), kPointerLoadType, *pinned);
+              global->index * sizeof(Address), kPointerLoadType);
       *offset = 0;
     } else {
       LOAD_INSTANCE_FIELD(addr, GlobalsStart, kSystemPointerSize, *pinned);
@@ -2362,8 +2418,7 @@ class LiftoffCompiler {
     *base = globals_buffer;
     __ LoadTaggedPointer(
         *base, globals_buffer, no_reg,
-        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(global->offset),
-        *pinned);
+        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(global->offset));
 
     // For the offset we need the index of the global in the buffer, and
     // then calculate the actual offset from the index. Load the index from
@@ -2376,8 +2431,7 @@ class LiftoffCompiler {
     *offset = imported_mutable_globals;
     __ Load(LiftoffRegister(*offset), imported_mutable_globals, no_reg,
             global->index * sizeof(Address),
-            kSystemPointerSize == 4 ? LoadType::kI32Load : LoadType::kI64Load,
-            *pinned);
+            kSystemPointerSize == 4 ? LoadType::kI32Load : LoadType::kI64Load);
     __ emit_i32_shli(*offset, *offset, kTaggedSizeLog2);
     __ emit_i32_addi(*offset, *offset,
                      wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(0));
@@ -2398,7 +2452,7 @@ class LiftoffCompiler {
         Register offset = no_reg;
         GetBaseAndOffsetForImportedMutableExternRefGlobal(global, &pinned,
                                                           &base, &offset);
-        __ LoadTaggedPointer(base, base, offset, 0, pinned);
+        __ LoadTaggedPointer(base, base, offset, 0);
         __ PushRegister(kind, LiftoffRegister(base));
         return;
       }
@@ -2411,8 +2465,7 @@ class LiftoffCompiler {
       Register value = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
       __ LoadTaggedPointer(value, globals_buffer, no_reg,
                            wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(
-                               imm.global->offset),
-                           pinned);
+                               imm.global->offset));
       __ PushRegister(kind, LiftoffRegister(value));
       return;
     }
@@ -2422,7 +2475,7 @@ class LiftoffCompiler {
     LiftoffRegister value =
         pinned.set(__ GetUnusedRegister(reg_class_for(kind), pinned));
     LoadType type = LoadType::ForValueKind(kind);
-    __ Load(value, addr, no_reg, offset, type, pinned, nullptr, false);
+    __ Load(value, addr, no_reg, offset, type, nullptr, false);
     __ PushRegister(kind, value);
   }
 
@@ -2546,7 +2599,11 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
     LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
     LoadNullValue(null.gp(), pinned);
-    __ emit_cond_jump(kUnequal, trap_label, kOptRef, obj.gp(), null.gp());
+    {
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kUnequal, trap_label, kOptRef, obj.gp(), null.gp(),
+                        trapping);
+    }
     __ PushRegister(kOptRef, obj);
   }
 
@@ -2566,10 +2623,11 @@ class LiftoffCompiler {
     LiftoffRegister dst = __ GetUnusedRegister(true_value.reg_class(),
                                                {true_value, false_value}, {});
     if (!__ emit_select(dst, condition, true_value, false_value, kind)) {
+      FREEZE_STATE(frozen);
       // Emit generic code (using branches) instead.
       Label cont;
       Label case_false;
-      __ emit_cond_jump(kEqual, &case_false, kI32, condition);
+      __ emit_cond_jump(kEqual, &case_false, kI32, condition, no_reg, frozen);
       if (dst != true_value) __ Move(dst, true_value, kind);
       __ emit_jump(&cont);
 
@@ -2580,16 +2638,21 @@ class LiftoffCompiler {
     __ PushRegister(kind, dst);
   }
 
-  void BrImpl(FullDecoder* decoder, Control* target) {
+  // {tmp1} and {tmp2} may be {no_reg} if it is guaranteed that {target}
+  // isn't a loop.
+  void BrImpl(FullDecoder* decoder, Control* target, Register tmp1,
+              Register tmp2) {
     if (dynamic_tiering()) {
       if (target->is_loop()) {
         DCHECK(target->label.get()->is_bound());
+        DCHECK_NE(tmp1, no_reg);
+        DCHECK_NE(tmp2, no_reg);
         int jump_distance = __ pc_offset() - target->label.get()->pos();
         // For now we just add one as the cost for the tier up check. We might
         // want to revisit this when tuning tiering budgets later.
         const int kTierUpCheckCost = 1;
         TierupCheck(decoder, decoder->position(),
-                    jump_distance + kTierUpCheckCost);
+                    jump_distance + kTierUpCheckCost, tmp1, tmp2);
       } else {
         // To estimate time spent in this function more accurately, we could
         // increment the tiering budget on forward jumps. However, we don't
@@ -2608,16 +2671,35 @@ class LiftoffCompiler {
     __ jmp(target->label.get());
   }
 
+  bool NeedsTierupCheck(FullDecoder* decoder, uint32_t br_depth) {
+    if (!dynamic_tiering()) return false;
+    return br_depth == decoder->control_depth() - 1 ||
+           decoder->control_at(br_depth)->is_loop();
+  }
+
+  struct TierupTempRegisters {
+    Register tmp1 = no_reg;
+    Register tmp2 = no_reg;
+  };
+  void AllocateTempRegisters(TierupTempRegisters& temps) {
+    LiftoffRegList pinned;
+    temps.tmp1 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    temps.tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+  }
+
   void BrOrRet(FullDecoder* decoder, uint32_t depth,
                uint32_t /* drop_values */) {
-    BrOrRetImpl(decoder, depth);
+    TierupTempRegisters temps;
+    if (NeedsTierupCheck(decoder, depth)) AllocateTempRegisters(temps);
+    BrOrRetImpl(decoder, depth, temps.tmp1, temps.tmp2);
   }
 
-  void BrOrRetImpl(FullDecoder* decoder, uint32_t depth) {
+  void BrOrRetImpl(FullDecoder* decoder, uint32_t depth, Register tmp1,
+                   Register tmp2) {
     if (depth == decoder->control_depth() - 1) {
-      DoReturn(decoder, 0);
+      ReturnImpl(decoder, tmp1, tmp2);
     } else {
-      BrImpl(decoder, decoder->control_at(depth));
+      BrImpl(decoder, decoder->control_at(depth), tmp1, tmp2);
     }
   }
 
@@ -2631,24 +2713,14 @@ class LiftoffCompiler {
     }
 
     Label cont_false;
+    TierupTempRegisters temps;
+    if (NeedsTierupCheck(decoder, depth)) AllocateTempRegisters(temps);
 
     // Test the condition on the value stack, jump to {cont_false} if zero.
-    JumpIfFalse(decoder, &cont_false);
-
-    // As a quickfix for https://crbug.com/1314184 we store the cache state
-    // before calling {BrOrRetImpl} under dynamic tiering, because the tier up
-    // check modifies the cache state (GetUnusedRegister,
-    // LoadInstanceIntoRegister).
-    // TODO(wasm): This causes significant overhead during compilation; try to
-    // avoid this, maybe by passing in scratch registers.
-    if (dynamic_tiering()) {
-      LiftoffAssembler::CacheState old_cache_state;
-      old_cache_state.Split(*__ cache_state());
-      BrOrRetImpl(decoder, depth);
-      __ cache_state()->Steal(old_cache_state);
-    } else {
-      BrOrRetImpl(decoder, depth);
-    }
+    std::unique_ptr<FreezeCacheState> frozen;
+    JumpIfFalse(decoder, &cont_false, frozen);
+
+    BrOrRetImpl(decoder, depth, temps.tmp1, temps.tmp2);
 
     __ bind(&cont_false);
   }
@@ -2656,24 +2728,14 @@ class LiftoffCompiler {
   // Generate a branch table case, potentially reusing previously generated
   // stack transfer code.
   void GenerateBrCase(FullDecoder* decoder, uint32_t br_depth,
-                      std::map<uint32_t, MovableLabel>* br_targets) {
+                      std::map<uint32_t, MovableLabel>* br_targets,
+                      Register tmp1, Register tmp2) {
     MovableLabel& label = (*br_targets)[br_depth];
     if (label.get()->is_bound()) {
       __ jmp(label.get());
     } else {
       __ bind(label.get());
-      if (dynamic_tiering()) {
-        // {BrOrRet} caches the instance, which is only valid for the branch
-        // taken. On fallthrough, we must forget this state change.
-        // (See comment in {BrIf} above, this is a variant of
-        // crbug.com/1314184.)
-        LiftoffAssembler::CacheState old_cache_state;
-        old_cache_state.Split(*__ cache_state());
-        BrOrRet(decoder, br_depth, 0);
-        __ cache_state()->Steal(old_cache_state);
-      } else {
-        BrOrRet(decoder, br_depth, 0);
-      }
+      BrOrRetImpl(decoder, br_depth, tmp1, tmp2);
     }
   }
 
@@ -2682,12 +2744,14 @@ class LiftoffCompiler {
   void GenerateBrTable(FullDecoder* decoder, LiftoffRegister tmp,
                        LiftoffRegister value, uint32_t min, uint32_t max,
                        BranchTableIterator<validate>* table_iterator,
-                       std::map<uint32_t, MovableLabel>* br_targets) {
+                       std::map<uint32_t, MovableLabel>* br_targets,
+                       Register tmp1, Register tmp2,
+                       const FreezeCacheState& frozen) {
     DCHECK_LT(min, max);
     // Check base case.
     if (max == min + 1) {
       DCHECK_EQ(min, table_iterator->cur_index());
-      GenerateBrCase(decoder, table_iterator->next(), br_targets);
+      GenerateBrCase(decoder, table_iterator->next(), br_targets, tmp1, tmp2);
       return;
     }
 
@@ -2695,34 +2759,58 @@ class LiftoffCompiler {
     Label upper_half;
     __ LoadConstant(tmp, WasmValue(split));
     __ emit_cond_jump(kUnsignedGreaterEqual, &upper_half, kI32, value.gp(),
-                      tmp.gp());
+                      tmp.gp(), frozen);
     // Emit br table for lower half:
-    GenerateBrTable(decoder, tmp, value, min, split, table_iterator,
-                    br_targets);
+    GenerateBrTable(decoder, tmp, value, min, split, table_iterator, br_targets,
+                    tmp1, tmp2, frozen);
     __ bind(&upper_half);
     // table_iterator will trigger a DCHECK if we don't stop decoding now.
     if (did_bailout()) return;
     // Emit br table for upper half:
-    GenerateBrTable(decoder, tmp, value, split, max, table_iterator,
-                    br_targets);
+    GenerateBrTable(decoder, tmp, value, split, max, table_iterator, br_targets,
+                    tmp1, tmp2, frozen);
   }
 
   void BrTable(FullDecoder* decoder, const BranchTableImmediate<validate>& imm,
                const Value& key) {
     LiftoffRegList pinned;
     LiftoffRegister value = pinned.set(__ PopToRegister());
+
+    // Reserve temp registers if any of the table entries will do a tierup
+    // check (function exit, or loop back edge).
+    Register tmp1 = no_reg;
+    Register tmp2 = no_reg;
+    if (dynamic_tiering()) {
+      bool need_temps = false;
+      BranchTableIterator<validate> table_iterator(decoder, imm);
+      while (table_iterator.has_next()) {
+        uint32_t depth = table_iterator.next();
+        if (depth == decoder->control_depth() - 1 ||
+            decoder->control_at(depth)->is_loop()) {
+          need_temps = true;
+          break;
+        }
+      }
+      if (need_temps) {
+        LiftoffRegList pinned;
+        tmp1 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+        tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+      }
+    }
+
     BranchTableIterator<validate> table_iterator(decoder, imm);
     std::map<uint32_t, MovableLabel> br_targets;
 
     if (imm.table_count > 0) {
       LiftoffRegister tmp = __ GetUnusedRegister(kGpReg, pinned);
       __ LoadConstant(tmp, WasmValue(uint32_t{imm.table_count}));
+      FREEZE_STATE(frozen);
       Label case_default;
       __ emit_cond_jump(kUnsignedGreaterEqual, &case_default, kI32, value.gp(),
-                        tmp.gp());
+                        tmp.gp(), frozen);
 
       GenerateBrTable(decoder, tmp, value, 0, imm.table_count, &table_iterator,
-                      &br_targets);
+                      &br_targets, tmp1, tmp2, frozen);
 
       __ bind(&case_default);
       // table_iterator will trigger a DCHECK if we don't stop decoding now.
@@ -2730,7 +2818,7 @@ class LiftoffCompiler {
     }
 
     // Generate the default case.
-    GenerateBrCase(decoder, table_iterator.next(), &br_targets);
+    GenerateBrCase(decoder, table_iterator.next(), &br_targets, tmp1, tmp2);
     DCHECK(!table_iterator.has_next());
   }
 
@@ -2842,7 +2930,9 @@ class LiftoffCompiler {
       __ emit_u32_to_uintptr(index_ptrsize, index_ptrsize);
     } else if (kSystemPointerSize == kInt32Size) {
       DCHECK_GE(kMaxUInt32, env_->max_memory_size);
-      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, index.high_gp());
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, index.high_gp(),
+                        no_reg, trapping);
     }
 
     uintptr_t end_offset = offset + access_size - 1u;
@@ -2855,12 +2945,13 @@ class LiftoffCompiler {
 
     __ LoadConstant(end_offset_reg, WasmValue::ForUintPtr(end_offset));
 
+    FREEZE_STATE(trapping);
     // If the end offset is larger than the smallest memory, dynamically check
     // the end offset against the actual memory size, which is not known at
     // compile time. Otherwise, only one check is required (see below).
     if (end_offset > env_->min_memory_size) {
       __ emit_cond_jump(kUnsignedGreaterEqual, trap_label, kPointerKind,
-                        end_offset_reg.gp(), mem_size.gp());
+                        end_offset_reg.gp(), mem_size.gp(), trapping);
     }
 
     // Just reuse the end_offset register for computing the effective size
@@ -2870,7 +2961,7 @@ class LiftoffCompiler {
                         end_offset_reg.gp());
 
     __ emit_cond_jump(kUnsignedGreaterEqual, trap_label, kPointerKind,
-                      index_ptrsize, effective_size_reg.gp());
+                      index_ptrsize, effective_size_reg.gp(), trapping);
     return index_ptrsize;
   }
 
@@ -2882,6 +2973,7 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapUnalignedAccess, 0);
     Register address = __ GetUnusedRegister(kGpReg, pinned).gp();
 
+    FREEZE_STATE(trapping);
     const uint32_t align_mask = access_size - 1;
     if ((offset & align_mask) == 0) {
       // If {offset} is aligned, we can produce faster code.
@@ -2891,12 +2983,12 @@ class LiftoffCompiler {
       // {emit_cond_jump} to use the "test" instruction without the "and" here.
       // Then we can also avoid using the temp register here.
       __ emit_i32_andi(address, index, align_mask);
-      __ emit_cond_jump(kUnequal, trap_label, kI32, address);
+      __ emit_cond_jump(kUnequal, trap_label, kI32, address, no_reg, trapping);
     } else {
       // For alignment checks we only look at the lower 32-bits in {offset}.
       __ emit_i32_addi(address, index, static_cast<uint32_t>(offset));
       __ emit_i32_andi(address, address, align_mask);
-      __ emit_cond_jump(kUnequal, trap_label, kI32, address);
+      __ emit_cond_jump(kUnequal, trap_label, kI32, address, no_reg, trapping);
     }
   }
 
@@ -3012,8 +3104,7 @@ class LiftoffCompiler {
       LiftoffRegList pinned;
       Register mem = pinned.set(GetMemoryStart(pinned));
       LiftoffRegister value = pinned.set(__ GetUnusedRegister(rc, pinned));
-      __ Load(value, mem, no_reg, offset, type, pinned, nullptr, true,
-              i64_offset);
+      __ Load(value, mem, no_reg, offset, type, nullptr, true, i64_offset);
       __ PushRegister(kind, value);
     } else {
       LiftoffRegister full_index = __ PopToRegister();
@@ -3030,7 +3121,7 @@ class LiftoffCompiler {
       LiftoffRegister value = pinned.set(__ GetUnusedRegister(rc, pinned));
 
       uint32_t protected_load_pc = 0;
-      __ Load(value, mem, index, offset, type, pinned, &protected_load_pc, true,
+      __ Load(value, mem, index, offset, type, &protected_load_pc, true,
               i64_offset);
       if (env_->bounds_checks == kTrapHandler) {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapMemOutOfBounds,
@@ -3229,12 +3320,16 @@ class LiftoffCompiler {
       // >=256TB). The int32_t value will be sign-extended below.
       __ LoadConstant(result, WasmValue(int32_t{-1}));
       if (kNeedI64RegPair) {
-        __ emit_cond_jump(kUnequal /* neq */, &done, kI32, input.high_gp());
+        FREEZE_STATE(all_spilled_anyway);
+        __ emit_cond_jump(kUnequal, &done, kI32, input.high_gp(), no_reg,
+                          all_spilled_anyway);
         input = input.low();
       } else {
         LiftoffRegister high_word = __ GetUnusedRegister(kGpReg, pinned);
         __ emit_i64_shri(high_word, input, 32);
-        __ emit_cond_jump(kUnequal /* neq */, &done, kI32, high_word.gp());
+        FREEZE_STATE(all_spilled_anyway);
+        __ emit_cond_jump(kUnequal, &done, kI32, high_word.gp(), no_reg,
+                          all_spilled_anyway);
       }
     }
 
@@ -3372,21 +3467,21 @@ class LiftoffCompiler {
   void ReturnCall(FullDecoder* decoder,
                   const CallFunctionImmediate<validate>& imm,
                   const Value args[]) {
-    TierupCheckOnExit(decoder);
+    TierupCheckOnTailCall(decoder);
     CallDirect(decoder, imm, args, nullptr, kTailCall);
   }
 
   void ReturnCallIndirect(FullDecoder* decoder, const Value& index_val,
                           const CallIndirectImmediate<validate>& imm,
                           const Value args[]) {
-    TierupCheckOnExit(decoder);
+    TierupCheckOnTailCall(decoder);
     CallIndirect(decoder, index_val, imm, kTailCall);
   }
 
   void ReturnCallRef(FullDecoder* decoder, const Value& func_ref,
                      const FunctionSig* sig, uint32_t sig_index,
                      const Value args[]) {
-    TierupCheckOnExit(decoder);
+    TierupCheckOnTailCall(decoder);
     CallRef(decoder, func_ref.type, sig, kTailCall);
   }
 
@@ -3402,15 +3497,25 @@ class LiftoffCompiler {
 
     Label cont_false;
     LiftoffRegList pinned;
-    LiftoffRegister ref = pinned.set(__ PopToRegister(pinned));
-    LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
-    LoadNullValue(null.gp(), pinned);
-    __ emit_cond_jump(kUnequal, &cont_false, ref_object.type.kind(), ref.gp(),
-                      null.gp());
-    if (pass_null_along_branch) __ PushRegister(kOptRef, null);
-    BrOrRet(decoder, depth, 0);
+    LiftoffRegister ref =
+        pinned.set(pass_null_along_branch ? __ PeekToRegister(0, pinned)
+                                          : __ PopToRegister(pinned));
+    Register null = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register tmp = NeedsTierupCheck(decoder, depth)
+                       ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
+                       : no_reg;
+    LoadNullValue(null, pinned);
+    {
+      FREEZE_STATE(frozen);
+      __ emit_cond_jump(kUnequal, &cont_false, ref_object.type.kind(), ref.gp(),
+                        null, frozen);
+      BrOrRetImpl(decoder, depth, null, tmp);
+    }
     __ bind(&cont_false);
-    __ PushRegister(kRef, ref);
+    if (!pass_null_along_branch) {
+      // We popped the value earlier, must push it back now.
+      __ PushRegister(kRef, ref);
+    }
   }
 
   void BrOnNonNull(FullDecoder* decoder, const Value& ref_object,
@@ -3425,16 +3530,20 @@ class LiftoffCompiler {
 
     Label cont_false;
     LiftoffRegList pinned;
-    LiftoffRegister ref = pinned.set(__ PopToRegister(pinned));
-    // Put the reference back onto the stack for the branch.
-    __ PushRegister(kRef, ref);
+    LiftoffRegister ref = pinned.set(__ PeekToRegister(0, pinned));
 
-    Register null = __ GetUnusedRegister(kGpReg, pinned).gp();
+    Register null = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register tmp = NeedsTierupCheck(decoder, depth)
+                       ? pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp()
+                       : no_reg;
     LoadNullValue(null, pinned);
-    __ emit_cond_jump(kEqual, &cont_false, ref_object.type.kind(), ref.gp(),
-                      null);
+    {
+      FREEZE_STATE(frozen);
+      __ emit_cond_jump(kEqual, &cont_false, ref_object.type.kind(), ref.gp(),
+                        null, frozen);
 
-    BrOrRet(decoder, depth, 0);
+      BrOrRetImpl(decoder, depth, null, tmp);
+    }
     // Drop the reference if we are not branching.
     if (drop_null_on_fallthrough) __ DropValues(1);
     __ bind(&cont_false);
@@ -4309,7 +4418,7 @@ class LiftoffCompiler {
                                LiftoffRegList pinned) {
     __ LoadSmiAsInt32(
         dst, values_array.gp(),
-        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(*index), pinned);
+        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(*index));
     (*index)++;
   }
 
@@ -4444,8 +4553,7 @@ class LiftoffCompiler {
       case wasm::kRtt: {
         __ LoadTaggedPointer(
             value.gp(), values_array.gp(), no_reg,
-            wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(*index),
-            pinned);
+            wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(*index));
         (*index)++;
         break;
       }
@@ -4548,7 +4656,7 @@ class LiftoffCompiler {
     LOAD_TAGGED_PTR_INSTANCE_FIELD(exception_tag.gp(), TagsTable, pinned);
     __ LoadTaggedPointer(
         exception_tag.gp(), exception_tag.gp(), no_reg,
-        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index), {});
+        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index));
 
     // Finally, call WasmThrow.
     CallRuntimeStub(WasmCode::kWasmThrow,
@@ -5015,7 +5123,9 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapMemOutOfBounds);
     if (mem_offsets_high_word != no_reg) {
       // If any high word has bits set, jump to the OOB trap.
-      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word);
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word,
+                        no_reg, trapping);
       pinned.clear(mem_offsets_high_word);
     }
 
@@ -5032,7 +5142,8 @@ class LiftoffCompiler {
     LiftoffRegister result(instance);
     GenerateCCall(&result, &sig, kVoid, args,
                   ExternalReference::wasm_memory_init());
-    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp());
+    FREEZE_STATE(trapping);
+    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp(), no_reg, trapping);
   }
 
   void DataDrop(FullDecoder* decoder, const IndexImmediate<validate>& imm) {
@@ -5080,7 +5191,9 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapMemOutOfBounds);
     if (mem_offsets_high_word != no_reg) {
       // If any high word has bits set, jump to the OOB trap.
-      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word);
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word,
+                        no_reg, trapping);
     }
 
     auto sig = MakeSig::Returns(kI32).Params(kPointerKind, kPointerKind,
@@ -5091,7 +5204,8 @@ class LiftoffCompiler {
     LiftoffRegister result(instance);
     GenerateCCall(&result, &sig, kVoid, args,
                   ExternalReference::wasm_memory_copy());
-    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp());
+    FREEZE_STATE(trapping);
+    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp(), no_reg, trapping);
   }
 
   void MemoryFill(FullDecoder* decoder,
@@ -5117,7 +5231,9 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapMemOutOfBounds);
     if (mem_offsets_high_word != no_reg) {
       // If any high word has bits set, jump to the OOB trap.
-      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word);
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kNotEqualZero, trap_label, kI32, mem_offsets_high_word,
+                        no_reg, trapping);
     }
 
     auto sig = MakeSig::Returns(kI32).Params(kPointerKind, kPointerKind, kI32,
@@ -5128,7 +5244,8 @@ class LiftoffCompiler {
     LiftoffRegister result(instance);
     GenerateCCall(&result, &sig, kVoid, args,
                   ExternalReference::wasm_memory_fill());
-    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp());
+    FREEZE_STATE(trapping);
+    __ emit_cond_jump(kEqual, trap_label, kI32, result.gp(), no_reg, trapping);
   }
 
   void LoadSmi(LiftoffRegister reg, int value) {
@@ -5254,7 +5371,7 @@ class LiftoffCompiler {
     Register table = tables;
     __ LoadTaggedPointer(
         table, tables, no_reg,
-        ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index), pinned);
+        ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index));
 
     int length_field_size = WasmTableObject::kCurrentLengthOffsetEnd -
                             WasmTableObject::kCurrentLengthOffset + 1;
@@ -5262,8 +5379,7 @@ class LiftoffCompiler {
     Register result = table;
     __ Load(LiftoffRegister(result), table, no_reg,
             wasm::ObjectAccess::ToTagged(WasmTableObject::kCurrentLengthOffset),
-            length_field_size == 4 ? LoadType::kI32Load : LoadType::kI64Load,
-            pinned);
+            length_field_size == 4 ? LoadType::kI32Load : LoadType::kI64Load);
 
     __ SmiUntag(result);
     __ PushRegister(kI32, LiftoffRegister(result));
@@ -5387,8 +5503,9 @@ class LiftoffCompiler {
           __ LoadToRegister(__ cache_state()->stack_state.end()[-2], {});
       Label* trap_label =
           AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapArrayTooLarge);
+      FREEZE_STATE(trapping);
       __ emit_i32_cond_jumpi(kUnsignedGreaterThan, trap_label, length.gp(),
-                             WasmArray::MaxLength(imm.array_type));
+                             WasmArray::MaxLength(imm.array_type), trapping);
     }
     ValueKind elem_kind = imm.array_type->element_type().kind();
     int elem_size = value_kind_size(elem_kind);
@@ -5433,8 +5550,19 @@ class LiftoffCompiler {
     __ emit_i32_add(end_offset.gp(), end_offset.gp(), offset.gp());
     Label loop, done;
     __ bind(&loop);
-    __ emit_cond_jump(kUnsignedGreaterEqual, &done, kI32, offset.gp(),
-                      end_offset.gp());
+    {
+      // This is subtle: {StoreObjectField} can request a temp register, which
+      // is precisely what {FREEZE_STATE} (with non-trivial live range) is
+      // supposed to guard against. In this case it's fine though, because we've
+      // just done a call, so there are plenty of recently-spilled unused
+      // registers, so requesting a temp register won't actually cause any state
+      // changes.
+      // TODO(jkummerow): See if we can make this more elegant, e.g. by passing
+      // a temp register to {StoreObjectField}.
+      FREEZE_STATE(in_this_case_its_fine);
+      __ emit_cond_jump(kUnsignedGreaterEqual, &done, kI32, offset.gp(),
+                        end_offset.gp(), in_this_case_its_fine);
+    }
     StoreObjectField(obj.gp(), offset.gp(), 0, value, pinned, elem_kind);
     __ emit_i32_addi(offset.gp(), offset.gp(), elem_size);
     __ emit_jump(&loop);
@@ -5652,11 +5780,12 @@ class LiftoffCompiler {
   }
 
   void RttCanon(FullDecoder* decoder, uint32_t type_index, Value* result) {
-    LiftoffRegister rtt = __ GetUnusedRegister(kGpReg, {});
-    LOAD_TAGGED_PTR_INSTANCE_FIELD(rtt.gp(), ManagedObjectMaps, {});
+    LiftoffRegList pinned;
+    LiftoffRegister rtt = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
+    LOAD_TAGGED_PTR_INSTANCE_FIELD(rtt.gp(), ManagedObjectMaps, pinned);
     __ LoadTaggedPointer(
         rtt.gp(), rtt.gp(), no_reg,
-        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(type_index), {});
+        wasm::ObjectAccess::ElementOffsetInTaggedFixedArray(type_index));
     __ PushRegister(kRtt, rtt);
   }
 
@@ -5667,82 +5796,79 @@ class LiftoffCompiler {
 
   // Falls through on match (=successful type check).
   // Returns the register containing the object.
-  LiftoffRegister SubtypeCheck(FullDecoder* decoder, const Value& obj,
-                               const Value& rtt, Label* no_match,
-                               NullSucceeds null_succeeds,
-                               LiftoffRegList pinned = {},
-                               Register opt_scratch = no_reg) {
+  void SubtypeCheck(const WasmModule* module, Register obj_reg,
+                    ValueType obj_type, Register rtt_reg, ValueType rtt_type,
+                    Register scratch_null, Register scratch2, Label* no_match,
+                    NullSucceeds null_succeeds,
+                    const FreezeCacheState& frozen) {
     Label match;
-    LiftoffRegister rtt_reg = pinned.set(__ PopToRegister(pinned));
-    LiftoffRegister obj_reg = pinned.set(__ PopToRegister(pinned));
 
-    // Reserve all temporary registers up front, so that the cache state
-    // tracking doesn't get confused by the following conditional jumps.
-    LiftoffRegister tmp1 =
-        opt_scratch != no_reg
-            ? LiftoffRegister(opt_scratch)
-            : pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    LiftoffRegister tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    if (obj.type.is_nullable()) {
-      LoadNullValue(tmp1.gp(), pinned);
+    if (obj_type.is_nullable()) {
       __ emit_cond_jump(kEqual, null_succeeds ? &match : no_match,
-                        obj.type.kind(), obj_reg.gp(), tmp1.gp());
+                        obj_type.kind(), obj_reg, scratch_null, frozen);
     }
+    Register tmp1 = scratch_null;  // Done with null checks.
 
-    __ LoadMap(tmp1.gp(), obj_reg.gp());
+    __ LoadMap(tmp1, obj_reg);
     // {tmp1} now holds the object's map.
 
     // Check for rtt equality, and if not, check if the rtt is a struct/array
     // rtt.
-    __ emit_cond_jump(kEqual, &match, rtt.type.kind(), tmp1.gp(), rtt_reg.gp());
+    __ emit_cond_jump(kEqual, &match, rtt_type.kind(), tmp1, rtt_reg, frozen);
 
     // Constant-time subtyping check: load exactly one candidate RTT from the
     // supertypes list.
     // Step 1: load the WasmTypeInfo into {tmp1}.
     constexpr int kTypeInfoOffset = wasm::ObjectAccess::ToTagged(
         Map::kConstructorOrBackPointerOrNativeContextOffset);
-    __ LoadTaggedPointer(tmp1.gp(), tmp1.gp(), no_reg, kTypeInfoOffset, pinned);
+    __ LoadTaggedPointer(tmp1, tmp1, no_reg, kTypeInfoOffset);
     // Step 2: check the list's length if needed.
-    uint32_t rtt_depth =
-        GetSubtypingDepth(decoder->module_, rtt.type.ref_index());
+    uint32_t rtt_depth = GetSubtypingDepth(module, rtt_type.ref_index());
     if (rtt_depth >= kMinimumSupertypeArraySize) {
-      LiftoffRegister list_length = tmp2;
+      LiftoffRegister list_length(scratch2);
       int offset =
           ObjectAccess::ToTagged(WasmTypeInfo::kSupertypesLengthOffset);
-      __ LoadSmiAsInt32(list_length, tmp1.gp(), offset, pinned);
+      __ LoadSmiAsInt32(list_length, tmp1, offset);
       __ emit_i32_cond_jumpi(kUnsignedLessEqual, no_match, list_length.gp(),
-                             rtt_depth);
+                             rtt_depth, frozen);
     }
     // Step 3: load the candidate list slot into {tmp1}, and compare it.
     __ LoadTaggedPointer(
-        tmp1.gp(), tmp1.gp(), no_reg,
+        tmp1, tmp1, no_reg,
         ObjectAccess::ToTagged(WasmTypeInfo::kSupertypesOffset +
-                               rtt_depth * kTaggedSize),
-        pinned);
-    __ emit_cond_jump(kUnequal, no_match, rtt.type.kind(), tmp1.gp(),
-                      rtt_reg.gp());
+                               rtt_depth * kTaggedSize));
+    __ emit_cond_jump(kUnequal, no_match, rtt_type.kind(), tmp1, rtt_reg,
+                      frozen);
 
     // Fall through to {match}.
     __ bind(&match);
-    return obj_reg;
   }
 
   void RefTest(FullDecoder* decoder, const Value& obj, const Value& rtt,
                Value* /* result_val */) {
     Label return_false, done;
     LiftoffRegList pinned;
-    LiftoffRegister result = pinned.set(__ GetUnusedRegister(kGpReg, {}));
+    LiftoffRegister rtt_reg = pinned.set(__ PopToRegister(pinned));
+    LiftoffRegister obj_reg = pinned.set(__ PopToRegister(pinned));
+    Register scratch_null =
+        pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    LiftoffRegister result = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
+    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
 
-    SubtypeCheck(decoder, obj, rtt, &return_false, kNullFails, pinned,
-                 result.gp());
+    {
+      FREEZE_STATE(frozen);
+      SubtypeCheck(decoder->module_, obj_reg.gp(), obj.type, rtt_reg.gp(),
+                   rtt.type, scratch_null, result.gp(), &return_false,
+                   kNullFails, frozen);
 
-    __ LoadConstant(result, WasmValue(1));
-    // TODO(jkummerow): Emit near jumps on platforms where it's more efficient.
-    __ emit_jump(&done);
+      __ LoadConstant(result, WasmValue(1));
+      // TODO(jkummerow): Emit near jumps on platforms that have them.
+      __ emit_jump(&done);
 
-    __ bind(&return_false);
-    __ LoadConstant(result, WasmValue(0));
-    __ bind(&done);
+      __ bind(&return_false);
+      __ LoadConstant(result, WasmValue(0));
+      __ bind(&done);
+    }
     __ PushRegister(kI32, result);
   }
 
@@ -5751,13 +5877,25 @@ class LiftoffCompiler {
     if (FLAG_experimental_wasm_assume_ref_cast_succeeds) {
       // Just drop the rtt.
       __ DropValues(1);
-    } else {
-      Label* trap_label =
-          AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
-      LiftoffRegister obj_reg =
-          SubtypeCheck(decoder, obj, rtt, trap_label, kNullSucceeds);
-      __ PushRegister(obj.type.kind(), obj_reg);
+      return;
+    }
+    Label* trap_label =
+        AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
+    LiftoffRegList pinned;
+    LiftoffRegister rtt_reg = pinned.set(__ PopToRegister(pinned));
+    LiftoffRegister obj_reg = pinned.set(__ PopToRegister(pinned));
+    Register scratch_null =
+        pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register scratch2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
+
+    {
+      FREEZE_STATE(frozen);
+      SubtypeCheck(decoder->module_, obj_reg.gp(), obj.type, rtt_reg.gp(),
+                   rtt.type, scratch_null, scratch2, trap_label, kNullSucceeds,
+                   frozen);
     }
+    __ PushRegister(obj.type.kind(), obj_reg);
   }
 
   void BrOnCast(FullDecoder* decoder, const Value& obj, const Value& rtt,
@@ -5770,16 +5908,22 @@ class LiftoffCompiler {
     }
 
     Label cont_false;
-    LiftoffRegister obj_reg =
-        SubtypeCheck(decoder, obj, rtt, &cont_false, kNullFails);
+    LiftoffRegList pinned;
+    LiftoffRegister rtt_reg = pinned.set(__ PopToRegister(pinned));
+    LiftoffRegister obj_reg = pinned.set(__ PeekToRegister(0, pinned));
+    Register scratch_null =
+        pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register scratch2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
+    FREEZE_STATE(frozen);
+
+    SubtypeCheck(decoder->module_, obj_reg.gp(), obj.type, rtt_reg.gp(),
+                 rtt.type, scratch_null, scratch2, &cont_false, kNullFails,
+                 frozen);
 
-    __ PushRegister(rtt.type.is_bottom() ? kBottom : obj.type.kind(), obj_reg);
-    BrOrRet(decoder, depth, 0);
+    BrOrRetImpl(decoder, depth, scratch_null, scratch2);
 
     __ bind(&cont_false);
-    // Drop the branch's value, restore original value.
-    Drop(decoder);
-    __ PushRegister(obj.type.kind(), obj_reg);
   }
 
   void BrOnCastFail(FullDecoder* decoder, const Value& obj, const Value& rtt,
@@ -5792,102 +5936,142 @@ class LiftoffCompiler {
     }
 
     Label cont_branch, fallthrough;
-    LiftoffRegister obj_reg =
-        SubtypeCheck(decoder, obj, rtt, &cont_branch, kNullFails);
-    __ PushRegister(obj.type.kind(), obj_reg);
+    LiftoffRegList pinned;
+    LiftoffRegister rtt_reg = pinned.set(__ PopToRegister(pinned));
+    LiftoffRegister obj_reg = pinned.set(__ PeekToRegister(0, pinned));
+    Register scratch_null =
+        pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    Register scratch2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    if (obj.type.is_nullable()) LoadNullValue(scratch_null, pinned);
+    FREEZE_STATE(frozen);
+
+    SubtypeCheck(decoder->module_, obj_reg.gp(), obj.type, rtt_reg.gp(),
+                 rtt.type, scratch_null, scratch2, &cont_branch, kNullFails,
+                 frozen);
     __ emit_jump(&fallthrough);
 
     __ bind(&cont_branch);
-    BrOrRet(decoder, depth, 0);
+    BrOrRetImpl(decoder, depth, scratch_null, scratch2);
 
     __ bind(&fallthrough);
   }
 
-  // Abstract type checkers. They all return the object register and fall
-  // through to match.
-  LiftoffRegister DataCheck(const Value& obj, Label* no_match,
-                            LiftoffRegList pinned, Register opt_scratch) {
-    TypeCheckRegisters registers =
-        TypeCheckPrelude(obj, no_match, pinned, opt_scratch);
-    EmitDataRefCheck(registers.map_reg.gp(), no_match, registers.tmp_reg,
-                     pinned);
-    return registers.obj_reg;
-  }
-
-  LiftoffRegister ArrayCheck(const Value& obj, Label* no_match,
-                             LiftoffRegList pinned, Register opt_scratch) {
-    TypeCheckRegisters registers =
-        TypeCheckPrelude(obj, no_match, pinned, opt_scratch);
-    __ Load(registers.map_reg, registers.map_reg.gp(), no_reg,
-            wasm::ObjectAccess::ToTagged(Map::kInstanceTypeOffset),
-            LoadType::kI32Load16U, pinned);
-    __ emit_i32_cond_jumpi(kUnequal, no_match, registers.map_reg.gp(),
-                           WASM_ARRAY_TYPE);
-    return registers.obj_reg;
-  }
-
-  LiftoffRegister FuncCheck(const Value& obj, Label* no_match,
-                            LiftoffRegList pinned, Register opt_scratch) {
-    TypeCheckRegisters registers =
-        TypeCheckPrelude(obj, no_match, pinned, opt_scratch);
-    __ Load(registers.map_reg, registers.map_reg.gp(), no_reg,
-            wasm::ObjectAccess::ToTagged(Map::kInstanceTypeOffset),
-            LoadType::kI32Load16U, pinned);
-    __ emit_i32_cond_jumpi(kUnequal, no_match, registers.map_reg.gp(),
-                           WASM_INTERNAL_FUNCTION_TYPE);
-    return registers.obj_reg;
-  }
-
-  LiftoffRegister I31Check(const Value& object, Label* no_match,
-                           LiftoffRegList pinned, Register opt_scratch) {
-    LiftoffRegister obj_reg = pinned.set(__ PopToRegister(pinned));
+  struct TypeCheck {
+    Register obj_reg = no_reg;
+    ValueType obj_type;
+    Register tmp1 = no_reg;
+    Register tmp2 = no_reg;
+    Label* no_match;
+
+    TypeCheck(ValueType obj_type, Label* no_match)
+        : obj_type(obj_type), no_match(no_match) {}
+
+    Register null_reg() { return tmp1; }       // After {Initialize}.
+    Register instance_type() { return tmp1; }  // After {LoadInstanceType}.
+  };
+
+  enum PopOrPeek { kPop, kPeek };
+
+  void Initialize(TypeCheck& check, PopOrPeek pop_or_peek) {
+    LiftoffRegList pinned;
+    if (pop_or_peek == kPop) {
+      check.obj_reg = pinned.set(__ PopToRegister(pinned)).gp();
+    } else {
+      check.obj_reg = pinned.set(__ PeekToRegister(0, pinned)).gp();
+    }
+    check.tmp1 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    check.tmp2 = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
+    if (check.obj_type.is_nullable()) {
+      LoadNullValue(check.null_reg(), pinned);
+    }
+  }
+  void LoadInstanceType(TypeCheck& check, const FreezeCacheState& frozen) {
+    if (check.obj_type.is_nullable()) {
+      __ emit_cond_jump(kEqual, check.no_match, kOptRef, check.obj_reg,
+                        check.null_reg(), frozen);
+    }
+    __ emit_smi_check(check.obj_reg, check.no_match,
+                      LiftoffAssembler::kJumpOnSmi, frozen);
+    __ LoadMap(check.instance_type(), check.obj_reg);
+    __ Load(LiftoffRegister(check.instance_type()), check.instance_type(),
+            no_reg, wasm::ObjectAccess::ToTagged(Map::kInstanceTypeOffset),
+            LoadType::kI32Load16U);
+  }
 
-    __ emit_smi_check(obj_reg.gp(), no_match, LiftoffAssembler::kJumpOnNotSmi);
+  // Abstract type checkers. They all fall through on match.
+  void DataCheck(TypeCheck& check, const FreezeCacheState& frozen) {
+    LoadInstanceType(check, frozen);
+    // We're going to test a range of WasmObject instance types with a single
+    // unsigned comparison.
+    Register tmp = check.instance_type();
+    __ emit_i32_subi(tmp, tmp, FIRST_WASM_OBJECT_TYPE);
+    __ emit_i32_cond_jumpi(kUnsignedGreaterThan, check.no_match, tmp,
+                           LAST_WASM_OBJECT_TYPE - FIRST_WASM_OBJECT_TYPE,
+                           frozen);
+  }
 
-    return obj_reg;
+  void ArrayCheck(TypeCheck& check, const FreezeCacheState& frozen) {
+    LoadInstanceType(check, frozen);
+    LiftoffRegister instance_type(check.instance_type());
+    __ emit_i32_cond_jumpi(kUnequal, check.no_match, check.instance_type(),
+                           WASM_ARRAY_TYPE, frozen);
   }
 
-  using TypeChecker = LiftoffRegister (LiftoffCompiler::*)(
-      const Value& obj, Label* no_match, LiftoffRegList pinned,
-      Register opt_scratch);
+  void FuncCheck(TypeCheck& check, const FreezeCacheState& frozen) {
+    LoadInstanceType(check, frozen);
+    __ emit_i32_cond_jumpi(kUnequal, check.no_match, check.instance_type(),
+                           WASM_INTERNAL_FUNCTION_TYPE, frozen);
+  }
+
+  void I31Check(TypeCheck& check, const FreezeCacheState& frozen) {
+    __ emit_smi_check(check.obj_reg, check.no_match,
+                      LiftoffAssembler::kJumpOnNotSmi, frozen);
+  }
+
+  using TypeChecker = void (LiftoffCompiler::*)(TypeCheck& check,
+                                                const FreezeCacheState& frozen);
 
   template <TypeChecker type_checker>
   void AbstractTypeCheck(const Value& object) {
     Label match, no_match, done;
-    LiftoffRegList pinned;
-    LiftoffRegister result = pinned.set(__ GetUnusedRegister(kGpReg, {}));
+    TypeCheck check(object.type, &no_match);
+    Initialize(check, kPop);
+    LiftoffRegister result(check.tmp1);
+    {
+      FREEZE_STATE(frozen);
 
-    (this->*type_checker)(object, &no_match, pinned, result.gp());
+      (this->*type_checker)(check, frozen);
 
-    __ bind(&match);
-    __ LoadConstant(result, WasmValue(1));
-    // TODO(jkummerow): Emit near jumps on platforms where it's more efficient.
-    __ emit_jump(&done);
+      __ bind(&match);
+      __ LoadConstant(result, WasmValue(1));
+      // TODO(jkummerow): Emit near jumps on platforms that have them.
+      __ emit_jump(&done);
 
-    __ bind(&no_match);
-    __ LoadConstant(result, WasmValue(0));
-    __ bind(&done);
+      __ bind(&no_match);
+      __ LoadConstant(result, WasmValue(0));
+      __ bind(&done);
+    }
     __ PushRegister(kI32, result);
   }
 
   void RefIsData(FullDecoder* /* decoder */, const Value& object,
                  Value* /* result_val */) {
-    return AbstractTypeCheck<&LiftoffCompiler::DataCheck>(object);
+    AbstractTypeCheck<&LiftoffCompiler::DataCheck>(object);
   }
 
   void RefIsFunc(FullDecoder* /* decoder */, const Value& object,
                  Value* /* result_val */) {
-    return AbstractTypeCheck<&LiftoffCompiler::FuncCheck>(object);
+    AbstractTypeCheck<&LiftoffCompiler::FuncCheck>(object);
   }
 
   void RefIsArray(FullDecoder* /* decoder */, const Value& object,
                   Value* /* result_val */) {
-    return AbstractTypeCheck<&LiftoffCompiler::ArrayCheck>(object);
+    AbstractTypeCheck<&LiftoffCompiler::ArrayCheck>(object);
   }
 
   void RefIsI31(FullDecoder* decoder, const Value& object,
                 Value* /* result */) {
-    return AbstractTypeCheck<&LiftoffCompiler::I31Check>(object);
+    AbstractTypeCheck<&LiftoffCompiler::I31Check>(object);
   }
 
   template <TypeChecker type_checker>
@@ -5895,30 +6079,28 @@ class LiftoffCompiler {
                         ValueKind result_kind) {
     Label* trap_label =
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapIllegalCast);
-    Label match;
-    LiftoffRegister obj_reg =
-        (this->*type_checker)(object, trap_label, {}, no_reg);
-    __ bind(&match);
-    __ PushRegister(result_kind, obj_reg);
+    TypeCheck check(object.type, trap_label);
+    Initialize(check, kPeek);
+    FREEZE_STATE(frozen);
+    (this->*type_checker)(check, frozen);
   }
 
   void RefAsData(FullDecoder* decoder, const Value& object,
                  Value* /* result */) {
-    return AbstractTypeCast<&LiftoffCompiler::DataCheck>(object, decoder, kRef);
+    AbstractTypeCast<&LiftoffCompiler::DataCheck>(object, decoder, kRef);
   }
 
   void RefAsFunc(FullDecoder* decoder, const Value& object,
                  Value* /* result */) {
-    return AbstractTypeCast<&LiftoffCompiler::FuncCheck>(object, decoder, kRef);
+    AbstractTypeCast<&LiftoffCompiler::FuncCheck>(object, decoder, kRef);
   }
 
   void RefAsI31(FullDecoder* decoder, const Value& object, Value* result) {
-    return AbstractTypeCast<&LiftoffCompiler::I31Check>(object, decoder, kRef);
+    AbstractTypeCast<&LiftoffCompiler::I31Check>(object, decoder, kRef);
   }
 
   void RefAsArray(FullDecoder* decoder, const Value& object, Value* result) {
-    return AbstractTypeCast<&LiftoffCompiler::ArrayCheck>(object, decoder,
-                                                          kRef);
+    AbstractTypeCast<&LiftoffCompiler::ArrayCheck>(object, decoder, kRef);
   }
 
   template <TypeChecker type_checker>
@@ -5932,11 +6114,12 @@ class LiftoffCompiler {
     }
 
     Label no_match;
-    LiftoffRegister obj_reg =
-        (this->*type_checker)(object, &no_match, {}, no_reg);
+    TypeCheck check(object.type, &no_match);
+    Initialize(check, kPeek);
+    FREEZE_STATE(frozen);
 
-    __ PushRegister(kRef, obj_reg);
-    BrOrRet(decoder, br_depth, 0);
+    (this->*type_checker)(check, frozen);
+    BrOrRetImpl(decoder, br_depth, check.tmp1, check.tmp2);
 
     __ bind(&no_match);
   }
@@ -5952,63 +6135,58 @@ class LiftoffCompiler {
     }
 
     Label no_match, end;
-    LiftoffRegister obj_reg =
-        (this->*type_checker)(object, &no_match, {}, no_reg);
-    __ PushRegister(kRef, obj_reg);
+    TypeCheck check(object.type, &no_match);
+    Initialize(check, kPeek);
+    FREEZE_STATE(frozen);
+
+    (this->*type_checker)(check, frozen);
     __ emit_jump(&end);
 
     __ bind(&no_match);
-    BrOrRet(decoder, br_depth, 0);
+    BrOrRetImpl(decoder, br_depth, check.tmp1, check.tmp2);
 
     __ bind(&end);
   }
 
   void BrOnData(FullDecoder* decoder, const Value& object,
                 Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnAbstractType<&LiftoffCompiler::DataCheck>(object, decoder,
-                                                         br_depth);
+    BrOnAbstractType<&LiftoffCompiler::DataCheck>(object, decoder, br_depth);
   }
 
   void BrOnFunc(FullDecoder* decoder, const Value& object,
                 Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnAbstractType<&LiftoffCompiler::FuncCheck>(object, decoder,
-                                                         br_depth);
+    BrOnAbstractType<&LiftoffCompiler::FuncCheck>(object, decoder, br_depth);
   }
 
   void BrOnI31(FullDecoder* decoder, const Value& object,
                Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnAbstractType<&LiftoffCompiler::I31Check>(object, decoder,
-                                                        br_depth);
+    BrOnAbstractType<&LiftoffCompiler::I31Check>(object, decoder, br_depth);
   }
 
   void BrOnArray(FullDecoder* decoder, const Value& object,
                  Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnAbstractType<&LiftoffCompiler::ArrayCheck>(object, decoder,
-                                                          br_depth);
+    BrOnAbstractType<&LiftoffCompiler::ArrayCheck>(object, decoder, br_depth);
   }
 
   void BrOnNonData(FullDecoder* decoder, const Value& object,
                    Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnNonAbstractType<&LiftoffCompiler::DataCheck>(object, decoder,
-                                                            br_depth);
+    BrOnNonAbstractType<&LiftoffCompiler::DataCheck>(object, decoder, br_depth);
   }
 
   void BrOnNonFunc(FullDecoder* decoder, const Value& object,
                    Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnNonAbstractType<&LiftoffCompiler::FuncCheck>(object, decoder,
-                                                            br_depth);
+    BrOnNonAbstractType<&LiftoffCompiler::FuncCheck>(object, decoder, br_depth);
   }
 
   void BrOnNonI31(FullDecoder* decoder, const Value& object,
                   Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnNonAbstractType<&LiftoffCompiler::I31Check>(object, decoder,
-                                                           br_depth);
+    BrOnNonAbstractType<&LiftoffCompiler::I31Check>(object, decoder, br_depth);
   }
 
   void BrOnNonArray(FullDecoder* decoder, const Value& object,
                     Value* /* value_on_branch */, uint32_t br_depth) {
-    return BrOnNonAbstractType<&LiftoffCompiler::ArrayCheck>(object, decoder,
-                                                             br_depth);
+    BrOnNonAbstractType<&LiftoffCompiler::ArrayCheck>(object, decoder,
+                                                      br_depth);
   }
 
   void StringNewWtf8(FullDecoder* decoder,
@@ -6435,7 +6613,7 @@ class LiftoffCompiler {
       LOAD_INSTANCE_FIELD(imported_targets, ImportedFunctionTargets,
                           kSystemPointerSize, pinned);
       __ Load(LiftoffRegister(target), imported_targets, no_reg,
-              imm.index * sizeof(Address), kPointerLoadType, pinned);
+              imm.index * sizeof(Address), kPointerLoadType);
 
       Register imported_function_refs = tmp;
       LOAD_TAGGED_PTR_INSTANCE_FIELD(imported_function_refs,
@@ -6443,7 +6621,7 @@ class LiftoffCompiler {
       Register imported_function_ref = tmp;
       __ LoadTaggedPointer(
           imported_function_ref, imported_function_refs, no_reg,
-          ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index), pinned);
+          ObjectAccess::ElementOffsetInTaggedFixedArray(imm.index));
 
       Register* explicit_instance = &imported_function_ref;
       __ PrepareCall(&sig, call_descriptor, &target, explicit_instance);
@@ -6501,7 +6679,7 @@ class LiftoffCompiler {
     Register index = __ PopToModifiableRegister().gp();
 
     LiftoffRegList pinned{index};
-    // Get three temporary registers.
+    // Get all temporary registers unconditionally up front.
     Register table = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
     Register tmp_const = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
     Register scratch = pinned.set(__ GetUnusedRegister(kGpReg, pinned)).gp();
@@ -6515,8 +6693,7 @@ class LiftoffCompiler {
       indirect_function_table = indirect_function_tables;
       __ LoadTaggedPointer(
           indirect_function_table, indirect_function_tables, no_reg,
-          ObjectAccess::ElementOffsetInTaggedFixedArray(imm.table_imm.index),
-          pinned);
+          ObjectAccess::ElementOffsetInTaggedFixedArray(imm.table_imm.index));
     }
 
     // Bounds check against the table size.
@@ -6537,10 +6714,13 @@ class LiftoffCompiler {
       __ Load(
           LiftoffRegister(tmp_const), indirect_function_table, no_reg,
           wasm::ObjectAccess::ToTagged(WasmIndirectFunctionTable::kSizeOffset),
-          LoadType::kI32Load, pinned);
+          LoadType::kI32Load);
+    }
+    {
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kUnsignedGreaterEqual, invalid_func_label, kI32, index,
+                        tmp_const, trapping);
     }
-    __ emit_cond_jump(kUnsignedGreaterEqual, invalid_func_label, kI32, index,
-                      tmp_const);
 
     CODE_COMMENT("Check indirect call signature");
     // Load the signature from {instance->ift_sig_ids[key]}
@@ -6551,21 +6731,23 @@ class LiftoffCompiler {
       __ Load(LiftoffRegister(table), indirect_function_table, no_reg,
               wasm::ObjectAccess::ToTagged(
                   WasmIndirectFunctionTable::kSigIdsOffset),
-              kPointerLoadType, pinned);
+              kPointerLoadType);
     }
     // Shift {index} by 2 (multiply by 4) to represent kInt32Size items.
     static_assert((1 << 2) == kInt32Size);
     __ emit_i32_shli(index, index, 2);
-    __ Load(LiftoffRegister(scratch), table, index, 0, LoadType::kI32Load,
-            pinned);
+    __ Load(LiftoffRegister(scratch), table, index, 0, LoadType::kI32Load);
 
     // Compare against expected signature.
     __ LoadConstant(LiftoffRegister(tmp_const), WasmValue(canonical_sig_num));
 
     Label* sig_mismatch_label =
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapFuncSigMismatch);
-    __ emit_cond_jump(kUnequal, sig_mismatch_label, kPointerKind, scratch,
-                      tmp_const);
+    {
+      FREEZE_STATE(trapping);
+      __ emit_cond_jump(kUnequal, sig_mismatch_label, kPointerKind, scratch,
+                        tmp_const, trapping);
+    }
 
     // At this point {index} has already been multiplied by 4.
     CODE_COMMENT("Execute indirect call");
@@ -6582,12 +6764,10 @@ class LiftoffCompiler {
     } else {
       __ LoadTaggedPointer(
           table, indirect_function_table, no_reg,
-          wasm::ObjectAccess::ToTagged(WasmIndirectFunctionTable::kRefsOffset),
-          pinned);
+          wasm::ObjectAccess::ToTagged(WasmIndirectFunctionTable::kRefsOffset));
     }
     __ LoadTaggedPointer(tmp_const, table, index,
-                         ObjectAccess::ElementOffsetInTaggedFixedArray(0),
-                         pinned);
+                         ObjectAccess::ElementOffsetInTaggedFixedArray(0));
 
     if (kTaggedSize != kSystemPointerSize) {
       DCHECK_EQ(kSystemPointerSize, kTaggedSize * 2);
@@ -6606,10 +6786,9 @@ class LiftoffCompiler {
       __ Load(LiftoffRegister(table), indirect_function_table, no_reg,
               wasm::ObjectAccess::ToTagged(
                   WasmIndirectFunctionTable::kTargetsOffset),
-              kPointerLoadType, pinned);
+              kPointerLoadType);
     }
-    __ Load(LiftoffRegister(scratch), table, index, 0, kPointerLoadType,
-            pinned);
+    __ Load(LiftoffRegister(scratch), table, index, 0, kPointerLoadType);
 
     auto call_descriptor =
         compiler::GetWasmCallDescriptor(compilation_zone_, imm.sig);
@@ -6701,8 +6880,7 @@ class LiftoffCompiler {
       // Load "ref" (instance or WasmApiFunctionRef) and target.
       __ LoadTaggedPointer(
           instance.gp(), func_ref.gp(), no_reg,
-          wasm::ObjectAccess::ToTagged(WasmInternalFunction::kRefOffset),
-          pinned);
+          wasm::ObjectAccess::ToTagged(WasmInternalFunction::kRefOffset));
 
 #ifdef V8_SANDBOXED_EXTERNAL_POINTERS
       LOAD_INSTANCE_FIELD(temp.gp(), IsolateRoot, kSystemPointerSize, pinned);
@@ -6713,20 +6891,20 @@ class LiftoffCompiler {
       __ Load(target, func_ref.gp(), no_reg,
               wasm::ObjectAccess::ToTagged(
                   WasmInternalFunction::kForeignAddressOffset),
-              kPointerLoadType, pinned);
+              kPointerLoadType);
 #endif
 
+      FREEZE_STATE(frozen);
       Label perform_call;
 
       LiftoffRegister null_address = temp;
       __ LoadConstant(null_address, WasmValue::ForUintPtr(0));
       __ emit_cond_jump(kUnequal, &perform_call, kRef, target.gp(),
-                        null_address.gp());
+                        null_address.gp(), frozen);
       // The cached target can only be null for WasmJSFunctions.
       __ LoadTaggedPointer(
           target.gp(), func_ref.gp(), no_reg,
-          wasm::ObjectAccess::ToTagged(WasmInternalFunction::kCodeOffset),
-          pinned);
+          wasm::ObjectAccess::ToTagged(WasmInternalFunction::kCodeOffset));
 #ifdef V8_EXTERNAL_CODE_SPACE
       __ LoadCodeDataContainerEntry(target.gp(), target.gp());
 #else
@@ -6778,8 +6956,9 @@ class LiftoffCompiler {
         AddOutOfLineTrap(decoder, WasmCode::kThrowWasmTrapNullDereference);
     LiftoffRegister null = __ GetUnusedRegister(kGpReg, pinned);
     LoadNullValue(null.gp(), pinned);
+    FREEZE_STATE(trapping);
     __ emit_cond_jump(LiftoffCondition::kEqual, trap_label, kOptRef, object,
-                      null.gp());
+                      null.gp(), trapping);
   }
 
   void BoundsCheckArray(FullDecoder* decoder, LiftoffRegister array,
@@ -6790,10 +6969,10 @@ class LiftoffCompiler {
     LiftoffRegister length = __ GetUnusedRegister(kGpReg, pinned);
     constexpr int kLengthOffset =
         wasm::ObjectAccess::ToTagged(WasmArray::kLengthOffset);
-    __ Load(length, array.gp(), no_reg, kLengthOffset, LoadType::kI32Load,
-            pinned);
+    __ Load(length, array.gp(), no_reg, kLengthOffset, LoadType::kI32Load);
+    FREEZE_STATE(trapping);
     __ emit_cond_jump(LiftoffCondition::kUnsignedGreaterEqual, trap_label, kI32,
-                      index.gp(), length.gp());
+                      index.gp(), length.gp(), trapping);
   }
 
   int StructFieldOffset(const StructType* struct_type, int field_index) {
@@ -6805,11 +6984,11 @@ class LiftoffCompiler {
                        int offset, ValueKind kind, bool is_signed,
                        LiftoffRegList pinned) {
     if (is_reference(kind)) {
-      __ LoadTaggedPointer(dst.gp(), src, offset_reg, offset, pinned);
+      __ LoadTaggedPointer(dst.gp(), src, offset_reg, offset);
     } else {
       // Primitive kind.
       LoadType load_type = LoadType::ForValueKind(kind, is_signed);
-      __ Load(dst, src, offset_reg, offset, load_type, pinned);
+      __ Load(dst, src, offset_reg, offset, load_type);
     }
   }
 
@@ -6852,48 +7031,6 @@ class LiftoffCompiler {
     }
   }
 
-  struct TypeCheckRegisters {
-    LiftoffRegister obj_reg, map_reg, tmp_reg;
-  };
-
-  TypeCheckRegisters TypeCheckPrelude(const Value& obj, Label* no_match,
-                                      LiftoffRegList pinned,
-                                      Register opt_scratch) {
-    LiftoffRegister obj_reg = pinned.set(__ PopToRegister(pinned));
-
-    // Reserve all temporary registers up front, so that the cache state
-    // tracking doesn't get confused by the following conditional jumps.
-    LiftoffRegister map_reg =
-        opt_scratch != no_reg
-            ? LiftoffRegister(opt_scratch)
-            : pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-    LiftoffRegister tmp_reg = pinned.set(__ GetUnusedRegister(kGpReg, pinned));
-
-    if (obj.type.is_nullable()) {
-      LoadNullValue(map_reg.gp(), pinned);
-      __ emit_cond_jump(kEqual, no_match, kOptRef, obj_reg.gp(), map_reg.gp());
-    }
-
-    __ emit_smi_check(obj_reg.gp(), no_match, LiftoffAssembler::kJumpOnSmi);
-
-    __ LoadMap(map_reg.gp(), obj_reg.gp());
-
-    return {obj_reg, map_reg, tmp_reg};
-  }
-
-  void EmitDataRefCheck(Register map, Label* not_data_ref, LiftoffRegister tmp,
-                        LiftoffRegList pinned) {
-    constexpr int kInstanceTypeOffset =
-        wasm::ObjectAccess::ToTagged(Map::kInstanceTypeOffset);
-    __ Load(tmp, map, no_reg, kInstanceTypeOffset, LoadType::kI32Load16U,
-            pinned);
-    // We're going to test a range of WasmObject instance types with a single
-    // unsigned comparison.
-    __ emit_i32_subi(tmp.gp(), tmp.gp(), FIRST_WASM_OBJECT_TYPE);
-    __ emit_i32_cond_jumpi(kUnsignedGreaterThan, not_data_ref, tmp.gp(),
-                           LAST_WASM_OBJECT_TYPE - FIRST_WASM_OBJECT_TYPE);
-  }
-
   void MaybeOSR() {
     if (V8_UNLIKELY(for_debugging_)) {
       __ MaybeOSR();
diff --git a/src/wasm/baseline/loong64/liftoff-assembler-loong64.h b/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
index a74f21a9270..d8188fde58f 100644
--- a/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
+++ b/src/wasm/baseline/loong64/liftoff-assembler-loong64.h
@@ -365,8 +365,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   static_assert(kTaggedSize == kInt64Size);
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   Ld_d(dst, src_op);
@@ -419,9 +418,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
 
   if (protected_load_pc) *protected_load_pc = pc_offset();
@@ -1608,14 +1606,15 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   TurboAssembler::Branch(label, cond, lhs, Operand(imm));
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   TurboAssembler::Sub_d(value, value, Operand(subtrahend));
   TurboAssembler::Branch(result_negative, less, value, Operand(zero_reg));
 }
diff --git a/src/wasm/baseline/mips/liftoff-assembler-mips.h b/src/wasm/baseline/mips/liftoff-assembler-mips.h
index 739c4801760..d1a5cdc6903 100644
--- a/src/wasm/baseline/mips/liftoff-assembler-mips.h
+++ b/src/wasm/baseline/mips/liftoff-assembler-mips.h
@@ -1597,9 +1597,9 @@ void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
   TurboAssembler::Branch(label, cond, lhs, Operand(imm));
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   TurboAssembler::Subu(value, value, Operand(subtrahend));
   TurboAssembler::Branch(result_negative, less, value, Operand(zero_reg));
 }
diff --git a/src/wasm/baseline/mips64/liftoff-assembler-mips64.h b/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
index 4c5e79f83bd..4d3cadac752 100644
--- a/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
+++ b/src/wasm/baseline/mips64/liftoff-assembler-mips64.h
@@ -481,8 +481,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   static_assert(kTaggedSize == kInt64Size);
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   Ld(dst, src_op);
@@ -526,9 +525,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
 
   if (protected_load_pc) *protected_load_pc = pc_offset();
@@ -1806,7 +1804,8 @@ void LiftoffAssembler::emit_jump(Register target) {
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (rhs == no_reg) {
     DCHECK(kind == kI32 || kind == kI64);
@@ -1821,14 +1820,15 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   TurboAssembler::Branch(label, cond, lhs, Operand(imm));
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   TurboAssembler::Dsubu(value, value, Operand(subtrahend));
   TurboAssembler::Branch(result_negative, less, value, Operand(zero_reg));
 }
@@ -2002,7 +2002,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   And(scratch, obj, Operand(kSmiTagMask));
diff --git a/src/wasm/baseline/ppc/liftoff-assembler-ppc.h b/src/wasm/baseline/ppc/liftoff-assembler-ppc.h
index 398e8df2f09..6f8d2269bf6 100644
--- a/src/wasm/baseline/ppc/liftoff-assembler-ppc.h
+++ b/src/wasm/baseline/ppc/liftoff-assembler-ppc.h
@@ -300,8 +300,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   LoadTaggedPointerField(dst, MemOperand(src_addr, offset_reg, offset_imm), r0);
 }
 
@@ -345,9 +344,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   if (!i64_offset && offset_reg != no_reg) {
     ZeroExtWord32(ip, offset_reg);
     offset_reg = ip;
@@ -504,7 +502,7 @@ void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
 void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
                                   Register offset_reg, uintptr_t offset_imm,
                                   LoadType type, LiftoffRegList pinned) {
-  Load(dst, src_addr, offset_reg, offset_imm, type, pinned, nullptr, true);
+  Load(dst, src_addr, offset_reg, offset_imm, type, nullptr, true);
   lwsync();
 }
 
@@ -1592,7 +1590,8 @@ void LiftoffAssembler::emit_jump(Register target) { Jump(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   bool use_signed = liftoff::UseSignedOp(liftoff_cond);
 
@@ -1631,7 +1630,8 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   bool use_signed = liftoff::UseSignedOp(liftoff_cond);
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (use_signed) {
@@ -1642,9 +1642,9 @@ void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
   b(cond, label);
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   SubS64(value, value, Operand(subtrahend), r0, LeaveOE, SetRC);
   blt(result_negative, cr0);
 }
@@ -1740,7 +1740,8 @@ void LiftoffAssembler::LoadTransform(LiftoffRegister dst, Register src_addr,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   TestIfSmi(obj, r0);
   Condition condition = mode == kJumpOnSmi ? eq : ne;
   b(condition, target, cr0);  // branch if SMI
diff --git a/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h b/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h
index 8a41a4ab2ad..29e4fcfcf32 100644
--- a/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h
+++ b/src/wasm/baseline/riscv64/liftoff-assembler-riscv64.h
@@ -474,8 +474,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
   LoadTaggedPointerField(dst, src_op);
 }
@@ -516,9 +515,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   MemOperand src_op = liftoff::GetMemOp(this, src_addr, offset_reg, offset_imm);
 
   if (protected_load_pc) *protected_load_pc = pc_offset();
@@ -1604,7 +1602,8 @@ void LiftoffAssembler::emit_jump(Register target) {
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (rhs == no_reg) {
     DCHECK(kind == kI32 || kind == kI64);
@@ -1619,14 +1618,15 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   TurboAssembler::Branch(label, cond, lhs, Operand(imm));
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   Sub64(value, value, Operand(subtrahend));
   TurboAssembler::Branch(result_negative, lt, value, Operand(zero_reg));
 }
@@ -1695,7 +1695,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   And(scratch, obj, Operand(kSmiTagMask));
diff --git a/src/wasm/baseline/s390/liftoff-assembler-s390.h b/src/wasm/baseline/s390/liftoff-assembler-s390.h
index d66df4033d7..15ad5437530 100644
--- a/src/wasm/baseline/s390/liftoff-assembler-s390.h
+++ b/src/wasm/baseline/s390/liftoff-assembler-s390.h
@@ -278,8 +278,7 @@ void LiftoffAssembler::ResetOSRTarget() {}
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   CHECK(is_int20(offset_imm));
   LoadTaggedPointerField(
       dst,
@@ -324,9 +323,8 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   UseScratchRegisterScope temps(this);
   if (!is_int20(offset_imm)) {
     mov(ip, Operand(offset_imm));
@@ -490,7 +488,7 @@ void LiftoffAssembler::Store(Register dst_addr, Register offset_reg,
 void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
                                   Register offset_reg, uintptr_t offset_imm,
                                   LoadType type, LiftoffRegList pinned) {
-  Load(dst, src_addr, offset_reg, offset_imm, type, pinned, nullptr, true);
+  Load(dst, src_addr, offset_reg, offset_imm, type, nullptr, true);
 }
 
 void LiftoffAssembler::AtomicStore(Register dst_addr, Register offset_reg,
@@ -2123,7 +2121,8 @@ void LiftoffAssembler::emit_jump(Register target) { Jump(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   bool use_signed = liftoff::UseSignedOp(liftoff_cond);
 
@@ -2162,7 +2161,8 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
                                            Label* label, Register lhs,
-                                           int32_t imm) {
+                                           int32_t imm,
+                                           const FreezeCacheState& frozen) {
   bool use_signed = liftoff::UseSignedOp(liftoff_cond);
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (use_signed) {
@@ -2183,9 +2183,9 @@ void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
     bind(&done);            \
   }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   SubS64(value, value, Operand(subtrahend));
   blt(result_negative);
 }
@@ -2255,7 +2255,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   TestIfSmi(obj);
   Condition condition = mode == kJumpOnSmi ? eq : ne;
   b(condition, target);  // branch if SMI
diff --git a/src/wasm/baseline/x64/liftoff-assembler-x64.h b/src/wasm/baseline/x64/liftoff-assembler-x64.h
index 3b7fbb6e06e..3f72136c953 100644
--- a/src/wasm/baseline/x64/liftoff-assembler-x64.h
+++ b/src/wasm/baseline/x64/liftoff-assembler-x64.h
@@ -378,8 +378,7 @@ void LiftoffAssembler::ResetOSRTarget() {
 
 void LiftoffAssembler::LoadTaggedPointer(Register dst, Register src_addr,
                                          Register offset_reg,
-                                         int32_t offset_imm,
-                                         LiftoffRegList pinned) {
+                                         int32_t offset_imm) {
   DCHECK_GE(offset_imm, 0);
   if (FLAG_debug_code && offset_reg != no_reg) {
     AssertZeroExtended(offset_reg);
@@ -434,14 +433,13 @@ void LiftoffAssembler::StoreTaggedPointer(Register dst_addr,
 void LiftoffAssembler::AtomicLoad(LiftoffRegister dst, Register src_addr,
                                   Register offset_reg, uintptr_t offset_imm,
                                   LoadType type, LiftoffRegList pinned) {
-  Load(dst, src_addr, offset_reg, offset_imm, type, pinned, nullptr, true);
+  Load(dst, src_addr, offset_reg, offset_imm, type, nullptr, true);
 }
 
 void LiftoffAssembler::Load(LiftoffRegister dst, Register src_addr,
                             Register offset_reg, uintptr_t offset_imm,
-                            LoadType type, LiftoffRegList pinned,
-                            uint32_t* protected_load_pc, bool is_load_mem,
-                            bool i64_offset) {
+                            LoadType type, uint32_t* protected_load_pc,
+                            bool is_load_mem, bool i64_offset) {
   if (offset_reg != no_reg && !i64_offset) {
     AssertZeroExtended(offset_reg);
   }
@@ -2135,7 +2133,8 @@ void LiftoffAssembler::emit_jump(Register target) { jmp(target); }
 
 void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
                                       Label* label, ValueKind kind,
-                                      Register lhs, Register rhs) {
+                                      Register lhs, Register rhs,
+                                      const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   if (rhs != no_reg) {
     switch (kind) {
@@ -2162,16 +2161,16 @@ void LiftoffAssembler::emit_cond_jump(LiftoffCondition liftoff_cond,
 }
 
 void LiftoffAssembler::emit_i32_cond_jumpi(LiftoffCondition liftoff_cond,
-                                           Label* label, Register lhs,
-                                           int imm) {
+                                           Label* label, Register lhs, int imm,
+                                           const FreezeCacheState& frozen) {
   Condition cond = liftoff::ToCondition(liftoff_cond);
   cmpl(lhs, Immediate(imm));
   j(cond, label);
 }
 
-void LiftoffAssembler::emit_i32_subi_jump_negative(Register value,
-                                                   int subtrahend,
-                                                   Label* result_negative) {
+void LiftoffAssembler::emit_i32_subi_jump_negative(
+    Register value, int subtrahend, Label* result_negative,
+    const FreezeCacheState& frozen) {
   subl(value, Immediate(subtrahend));
   j(negative, result_negative);
 }
@@ -2275,7 +2274,8 @@ bool LiftoffAssembler::emit_select(LiftoffRegister dst, Register condition,
 }
 
 void LiftoffAssembler::emit_smi_check(Register obj, Label* target,
-                                      SmiCheckMode mode) {
+                                      SmiCheckMode mode,
+                                      const FreezeCacheState& frozen) {
   testb(obj, Immediate(kSmiTagMask));
   Condition condition = mode == kJumpOnSmi ? zero : not_zero;
   j(condition, target);
diff --git a/test/mjsunit/regress/wasm/regress-crbug-1339321.js b/test/mjsunit/regress/wasm/regress-crbug-1339321.js
new file mode 100644
index 00000000000..38739c94c0c
--- /dev/null
+++ b/test/mjsunit/regress/wasm/regress-crbug-1339321.js
@@ -0,0 +1,66 @@
+// Copyright 2022 the V8 project authors. All rights reserved.
+// Use of this source code is governed by a BSD-style license that can be
+// found in the LICENSE file.
+
+// Flags: --experimental-wasm-gc
+
+d8.file.execute("test/mjsunit/wasm/wasm-module-builder.js");
+
+let builder = new WasmModuleBuilder();
+builder.addFunction("leak", kSig_l_v)
+  .exportFunc()
+  .addLocals(kWasmI64, 10)
+  .addBody([
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprBlock, kWasmVoid,
+      kExprLoop, kWasmVoid,
+        kExprRefNull, kAnyRefCode,
+        kExprBrOnNonNull, 0,
+        kExprBr, 1,
+      kExprEnd,  // loop
+    kExprEnd,  // block
+    kExprDrop,
+    kExprDrop,
+    kExprDrop,
+    kExprDrop,
+  ]);
+
+let loop_type = builder.addType(makeSig([kWasmFuncRef], []));
+builder.addFunction("crash", kSig_v_v).exportFunc().addBody([
+  kExprRefFunc, 0,
+  kExprLoop, loop_type,
+    kExprRefFunc, 0,
+    kGCPrefix, kExprBrOnNonFunc, 0,
+    kExprDrop,
+    kExprDrop,
+  kExprEnd,  // loop
+]);
+
+let array_type = builder.addArray(wasmOptRefType(kSig_i_i), true);
+builder.addFunction("array", kSig_l_v).exportFunc()
+  .addLocals(kWasmI64, 10)
+  .addBody([
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprLocalGet, 0,
+    kExprI32Const, 0,
+    kGCPrefix, kExprArrayNewDefault, array_type,
+    kExprDrop,
+    kExprDrop,
+    kExprDrop,
+    kExprDrop,
+    kExprDrop,
+])
+
+let instance = builder.instantiate();
+let result = instance.exports.leak();
+assertEquals(0n, result);
+result = instance.exports.array();
+assertEquals(0n, result);
+instance.exports.crash();
-- 
2.35.1

