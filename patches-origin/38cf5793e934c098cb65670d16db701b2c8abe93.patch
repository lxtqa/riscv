From 38cf5793e934c098cb65670d16db701b2c8abe93 Mon Sep 17 00:00:00 2001
From: Clemens Backes <clemensb@chromium.org>
Date: Tue, 6 Sep 2022 14:52:46 +0200
Subject: [PATCH] [codegen] Use v8_flags for accessing flag values

Avoid the deprecated FLAG_* syntax, access flag values via the
{v8_flags} struct instead.

R=ishell@chromium.org

Bug: v8:12887
Change-Id: I457fd781f13c37ffdaa19e29c8f998ee3eaa55a5
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/3875085
Reviewed-by: Igor Sheludko <ishell@chromium.org>
Commit-Queue: Clemens Backes <clemensb@chromium.org>
Cr-Commit-Position: refs/heads/main@{#82996}
---
 src/codegen/arm/assembler-arm-inl.h           |   2 +-
 src/codegen/arm/assembler-arm.cc              |  14 +-
 src/codegen/arm/macro-assembler-arm.cc        |  44 +++---
 src/codegen/arm/macro-assembler-arm.h         |   4 +-
 src/codegen/arm64/assembler-arm64-inl.h       |   2 +-
 src/codegen/arm64/assembler-arm64.cc          |   6 +-
 src/codegen/arm64/macro-assembler-arm64-inl.h |   8 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  56 +++----
 src/codegen/arm64/macro-assembler-arm64.h     |   4 +-
 src/codegen/assembler.cc                      |   4 +-
 src/codegen/assembler.h                       |  19 +--
 src/codegen/code-stub-assembler.cc            |  34 ++---
 src/codegen/code-stub-assembler.h             |   6 +-
 src/codegen/compilation-cache.cc              |   2 +-
 src/codegen/compilation-cache.h               |   6 +-
 src/codegen/compiler.cc                       | 110 ++++++-------
 src/codegen/external-reference-encoder.cc     |   2 +-
 src/codegen/external-reference.cc             |  14 +-
 src/codegen/external-reference.h              |   8 +-
 src/codegen/flush-instruction-cache.cc        |   2 +-
 src/codegen/ia32/assembler-ia32-inl.h         |   2 +-
 src/codegen/ia32/assembler-ia32.cc            |  28 ++--
 src/codegen/ia32/macro-assembler-ia32.cc      |  44 +++---
 src/codegen/ia32/macro-assembler-ia32.h       |   4 +-
 src/codegen/loong64/assembler-loong64-inl.h   |   2 +-
 src/codegen/loong64/assembler-loong64.cc      |   4 +-
 .../loong64/macro-assembler-loong64.cc        |  44 +++---
 src/codegen/loong64/macro-assembler-loong64.h |   6 +-
 src/codegen/macro-assembler.h                 |  12 +-
 src/codegen/mips/assembler-mips-inl.h         |   2 +-
 src/codegen/mips/assembler-mips.cc            |   4 +-
 src/codegen/mips/macro-assembler-mips.cc      |  44 +++---
 src/codegen/mips/macro-assembler-mips.h       |   4 +-
 src/codegen/mips64/assembler-mips64-inl.h     |   2 +-
 src/codegen/mips64/assembler-mips64.cc        |   4 +-
 src/codegen/mips64/macro-assembler-mips64.cc  |  48 +++---
 src/codegen/mips64/macro-assembler-mips64.h   |   6 +-
 src/codegen/optimized-compilation-info.cc     |  24 +--
 src/codegen/pending-optimization-table.cc     |   8 +-
 src/codegen/ppc/assembler-ppc-inl.h           |  10 +-
 src/codegen/ppc/assembler-ppc.cc              |   8 +-
 src/codegen/ppc/assembler-ppc.h               |   2 +-
 src/codegen/ppc/macro-assembler-ppc.cc        |  63 ++++----
 src/codegen/ppc/macro-assembler-ppc.h         |   6 +-
 src/codegen/reloc-info.cc                     |   2 +-
 src/codegen/riscv/assembler-riscv-inl.h       |   2 +-
 src/codegen/riscv/assembler-riscv.cc          |  14 +-
 src/codegen/riscv/assembler-riscv.h           |   6 +-
 src/codegen/riscv/base-assembler-riscv.h      |   6 +-
 src/codegen/riscv/base-constants-riscv.cc     |   2 +-
 src/codegen/riscv/base-constants-riscv.h      |   4 +-
 src/codegen/riscv/macro-assembler-riscv.cc    | 144 +++++++++---------
 src/codegen/riscv/macro-assembler-riscv.h     |   4 +-
 src/codegen/s390/assembler-s390-inl.h         |   2 +-
 src/codegen/s390/macro-assembler-s390.cc      |  50 +++---
 src/codegen/s390/macro-assembler-s390.h       |   6 +-
 src/codegen/safepoint-table.h                 |   2 +-
 src/codegen/turbo-assembler.h                 |   4 +-
 src/codegen/x64/assembler-x64-inl.h           |   2 +-
 src/codegen/x64/assembler-x64.cc              |  40 ++---
 src/codegen/x64/macro-assembler-x64.cc        |  52 +++----
 src/codegen/x64/macro-assembler-x64.h         |   4 +-
 test/unittests/codegen/code-pages-unittest.cc |   6 +-
 test/unittests/codegen/factory-unittest.cc    |   2 +-
 .../codegen/pointer-auth-arm64-unittest.cc    |   2 +-
 65 files changed, 545 insertions(+), 539 deletions(-)

diff --git a/src/codegen/arm/assembler-arm-inl.h b/src/codegen/arm/assembler-arm-inl.h
index 7873a1a465a..cb0e5f6b753 100644
--- a/src/codegen/arm/assembler-arm-inl.h
+++ b/src/codegen/arm/assembler-arm-inl.h
@@ -124,7 +124,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/arm/assembler-arm.cc b/src/codegen/arm/assembler-arm.cc
index 54f13398b51..31d76c61984 100644
--- a/src/codegen/arm/assembler-arm.cc
+++ b/src/codegen/arm/assembler-arm.cc
@@ -58,7 +58,7 @@ static const unsigned kArmv8 = kArmv7WithSudiv | (1u << ARMv8);
 
 static unsigned CpuFeaturesFromCommandLine() {
   unsigned result;
-  const char* arm_arch = FLAG_arm_arch;
+  const char* arm_arch = v8_flags.arm_arch;
   if (strcmp(arm_arch, "armv8") == 0) {
     result = kArmv8;
   } else if (strcmp(arm_arch, "armv7+sudiv") == 0) {
@@ -81,12 +81,12 @@ static unsigned CpuFeaturesFromCommandLine() {
   // If any of the old (deprecated) flags are specified, print a warning, but
   // otherwise try to respect them for now.
   // TODO(jbramley): When all the old bots have been updated, remove this.
-  base::Optional<bool> maybe_enable_armv7 = FLAG_enable_armv7;
-  base::Optional<bool> maybe_enable_vfp3 = FLAG_enable_vfp3;
-  base::Optional<bool> maybe_enable_32dregs = FLAG_enable_32dregs;
-  base::Optional<bool> maybe_enable_neon = FLAG_enable_neon;
-  base::Optional<bool> maybe_enable_sudiv = FLAG_enable_sudiv;
-  base::Optional<bool> maybe_enable_armv8 = FLAG_enable_armv8;
+  base::Optional<bool> maybe_enable_armv7 = v8_flags.enable_armv7;
+  base::Optional<bool> maybe_enable_vfp3 = v8_flags.enable_vfp3;
+  base::Optional<bool> maybe_enable_32dregs = v8_flags.enable_32dregs;
+  base::Optional<bool> maybe_enable_neon = v8_flags.enable_neon;
+  base::Optional<bool> maybe_enable_sudiv = v8_flags.enable_sudiv;
+  base::Optional<bool> maybe_enable_armv8 = v8_flags.enable_armv8;
   if (maybe_enable_armv7.has_value() || maybe_enable_vfp3.has_value() ||
       maybe_enable_32dregs.has_value() || maybe_enable_neon.has_value() ||
       maybe_enable_sudiv.has_value() || maybe_enable_armv8.has_value()) {
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index d6f4bcc63c7..ef27543742c 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -696,7 +696,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   // of the object, so so offset must be a multiple of kPointerSize.
   DCHECK(IsAligned(offset, kPointerSize));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     UseScratchRegisterScope temps(this);
@@ -821,7 +821,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
                                  Register value, LinkRegisterStatus lr_status,
                                  SaveFPRegsMode fp_mode, SmiCheck smi_check) {
   DCHECK(!AreAliased(object, value));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
@@ -832,7 +832,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
     Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -864,7 +864,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
     pop(lr);
   }
 
-  if (FLAG_debug_code) Move(slot_address, Operand(kZapValue));
+  if (v8_flags.debug_code) Move(slot_address, Operand(kZapValue));
 
   bind(&done);
 }
@@ -1483,7 +1483,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   PushCommonFrame(scratch);
   // Reserve room for saved entry sp.
   sub(sp, fp, Operand(ExitFrameConstants::kFixedFrameSizeFromFp));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(scratch, Operand::Zero());
     str(scratch, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
@@ -1532,7 +1532,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_ARM
 }
 
@@ -2011,7 +2011,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     CompareObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);
@@ -2162,7 +2162,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Move(scratch2, ExternalReference::Create(counter));
     ldr(scratch1, MemOperand(scratch2));
@@ -2175,7 +2175,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Move(scratch2, ExternalReference::Create(counter));
     ldr(scratch1, MemOperand(scratch2));
@@ -2186,15 +2186,15 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void TurboAssembler::Assert(Condition cond, AbortReason reason) {
-  if (FLAG_debug_code) Check(cond, reason);
+  if (v8_flags.debug_code) Check(cond, reason);
 }
 
 void TurboAssembler::AssertUnreachable(AbortReason reason) {
-  if (FLAG_debug_code) Abort(reason);
+  if (v8_flags.debug_code) Abort(reason);
 }
 
 void MacroAssembler::AssertNotSmi(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2202,7 +2202,7 @@ void MacroAssembler::AssertNotSmi(Register object) {
 }
 
 void MacroAssembler::AssertSmi(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2210,7 +2210,7 @@ void MacroAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2224,7 +2224,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2238,7 +2238,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2252,7 +2252,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   tst(object, Operand(kSmiTagMask));
@@ -2264,7 +2264,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
@@ -2285,7 +2285,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Label done_checking;
   AssertNotSmi(object);
@@ -2310,7 +2310,7 @@ void TurboAssembler::Abort(AbortReason reason) {
   ASM_CODE_COMMENT(this);
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -2344,7 +2344,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, ip);
       Call(ip);
     } else {
@@ -2695,7 +2695,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
   // running in the simulator. The simulator has its own alignment check which
   // provides more information.
 #if V8_HOST_ARCH_ARM
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kPointerSize) {
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 90354cd4151..30d1e6d1140 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -821,14 +821,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index ee15bd74f4c..5df2d876457 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -709,7 +709,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/arm64/assembler-arm64.cc b/src/codegen/arm64/assembler-arm64.cc
index 8e96f630f76..754c79815ab 100644
--- a/src/codegen/arm64/assembler-arm64.cc
+++ b/src/codegen/arm64/assembler-arm64.cc
@@ -45,16 +45,16 @@ namespace {
 
 #ifdef USE_SIMULATOR
 unsigned SimulatorFeaturesFromCommandLine() {
-  if (strcmp(FLAG_sim_arm64_optional_features, "none") == 0) {
+  if (strcmp(v8_flags.sim_arm64_optional_features, "none") == 0) {
     return 0;
   }
-  if (strcmp(FLAG_sim_arm64_optional_features, "all") == 0) {
+  if (strcmp(v8_flags.sim_arm64_optional_features, "all") == 0) {
     return (1u << NUMBER_OF_CPU_FEATURES) - 1;
   }
   fprintf(
       stderr,
       "Error: unrecognised value for --sim-arm64-optional-features ('%s').\n",
-      FLAG_sim_arm64_optional_features.value());
+      v8_flags.sim_arm64_optional_features.value());
   fprintf(stderr,
           "Supported values are:  none\n"
           "                       all\n");
diff --git a/src/codegen/arm64/macro-assembler-arm64-inl.h b/src/codegen/arm64/macro-assembler-arm64-inl.h
index caf9884ec80..bf347084910 100644
--- a/src/codegen/arm64/macro-assembler-arm64-inl.h
+++ b/src/codegen/arm64/macro-assembler-arm64-inl.h
@@ -1060,7 +1060,7 @@ void TurboAssembler::SmiTag(Register smi) { SmiTag(smi, smi); }
 
 void TurboAssembler::SmiUntag(Register dst, Register src) {
   DCHECK(dst.Is64Bits() && src.Is64Bits());
-  if (FLAG_enable_slow_asserts) {
+  if (v8_flags.enable_slow_asserts) {
     AssertSmi(src);
   }
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -1101,7 +1101,7 @@ void TurboAssembler::SmiUntag(Register smi) { SmiUntag(smi, smi); }
 
 void TurboAssembler::SmiToInt32(Register smi) {
   DCHECK(smi.Is64Bits());
-  if (FLAG_enable_slow_asserts) {
+  if (v8_flags.enable_slow_asserts) {
     AssertSmi(smi);
   }
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -1221,7 +1221,7 @@ void TurboAssembler::Poke(const CPURegister& src, const Operand& offset) {
 
   if (offset.IsImmediate()) {
     DCHECK_GE(offset.ImmediateValue(), 0);
-  } else if (FLAG_debug_code) {
+  } else if (v8_flags.debug_code) {
     Cmp(xzr, offset);
     Check(le, AbortReason::kStackAccessBelowStackPointer);
   }
@@ -1233,7 +1233,7 @@ template <TurboAssembler::LoadLRMode lr_mode>
 void TurboAssembler::Peek(const CPURegister& dst, const Operand& offset) {
   if (offset.IsImmediate()) {
     DCHECK_GE(offset.ImmediateValue(), 0);
-  } else if (FLAG_debug_code) {
+  } else if (v8_flags.debug_code) {
     Cmp(xzr, offset);
     Check(le, AbortReason::kStackAccessBelowStackPointer);
   }
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index e273661a24c..bab3bb22ff1 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -1372,7 +1372,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CompareObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);
     Assert(eq, AbortReason::kExpectedFeedbackVector);
   }
@@ -1464,7 +1464,7 @@ void MacroAssembler::MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void TurboAssembler::AssertSpAligned() {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   HardAbortScope hard_abort(this);  // Avoid calls to Abort.
   // Arm64 requires the stack pointer to be 16-byte aligned prior to address
@@ -1477,7 +1477,7 @@ void TurboAssembler::AssertSpAligned() {
 }
 
 void TurboAssembler::AssertFPCRState(Register fpcr) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Label unexpected_mode, done;
   UseScratchRegisterScope temps(this);
@@ -1501,7 +1501,7 @@ void TurboAssembler::AssertFPCRState(Register fpcr) {
 }
 
 void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   Tst(object, kSmiTagMask);
@@ -1509,7 +1509,7 @@ void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
 }
 
 void MacroAssembler::AssertNotSmi(Register object, AbortReason reason) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   Tst(object, kSmiTagMask);
@@ -1517,7 +1517,7 @@ void MacroAssembler::AssertNotSmi(Register object, AbortReason reason) {
 }
 
 void MacroAssembler::AssertCodeT(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsNotACodeT);
 
@@ -1529,7 +1529,7 @@ void MacroAssembler::AssertCodeT(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAConstructor);
 
@@ -1544,7 +1544,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
 
@@ -1557,7 +1557,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
 
@@ -1570,7 +1570,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotABoundFunction);
 
@@ -1582,7 +1582,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
 
@@ -1599,7 +1599,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 }
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.AcquireX();
@@ -1613,7 +1613,7 @@ void MacroAssembler::AssertUndefinedOrAllocationSite(Register object) {
 }
 
 void TurboAssembler::AssertPositiveOrZero(Register value) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Label done;
   int sign_bit = value.Is64Bits() ? kXSignBit : kWSignBit;
@@ -1623,13 +1623,13 @@ void TurboAssembler::AssertPositiveOrZero(Register value) {
 }
 
 void TurboAssembler::Assert(Condition cond, AbortReason reason) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Check(cond, reason);
   }
 }
 
 void TurboAssembler::AssertUnreachable(AbortReason reason) {
-  if (FLAG_debug_code) Abort(reason);
+  if (v8_flags.debug_code) Abort(reason);
 }
 #endif  // V8_ENABLE_DEBUG_CODE
 
@@ -1655,7 +1655,7 @@ void TurboAssembler::CopyDoubleWords(Register dst, Register src, Register count,
   ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(dst, src, count));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Register pointer1 = dst;
     Register pointer2 = src;
     if (mode == kSrcLessThanDst) {
@@ -1866,7 +1866,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_ARM64
 }
 
@@ -2420,7 +2420,7 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
 #endif
   Poke(x17, 0);
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify fp[kSPOffset]-8");
     // Verify that the slot below fp[kSPOffset]-8 points to the signed return
     // location.
@@ -2979,7 +2979,7 @@ void MacroAssembler::LeaveExitFrame(bool restore_doubles,
       ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   Ldr(cp, MemOperand(scratch));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Also emit debug code to clear the cp in the top frame.
     Mov(scratch2, Operand(Context::kInvalidContext));
     Mov(scratch, ExternalReference::Create(IsolateAddressId::kContextAddress,
@@ -3018,7 +3018,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch2) {
   ASM_CODE_COMMENT(this);
   DCHECK_NE(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
     // field.
@@ -3173,7 +3173,7 @@ void TurboAssembler::DecompressTaggedSigned(const Register& destination,
                                             const MemOperand& field_operand) {
   ASM_CODE_COMMENT(this);
   Ldr(destination.W(), field_operand);
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Corrupt the top 32 bits. Made up of 16 fixed bits and 16 pc offset bits.
     Add(destination, destination,
         ((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32);
@@ -3207,7 +3207,7 @@ void TurboAssembler::AtomicDecompressTaggedSigned(const Register& destination,
   ASM_CODE_COMMENT(this);
   Add(temp, base, index);
   Ldar(destination.W(), temp);
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Corrupt the top 32 bits. Made up of 16 fixed bits and 16 pc offset bits.
     Add(destination, destination,
         ((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32);
@@ -3269,7 +3269,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   // of the object, so offset must be a multiple of kTaggedSize.
   DCHECK(IsAligned(offset, kTaggedSize));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     UseScratchRegisterScope temps(this);
@@ -3477,7 +3477,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
   ASM_LOCATION_IN_ASSEMBLER("MacroAssembler::RecordWrite");
   DCHECK(!AreAliased(object, value));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     UseScratchRegisterScope temps(this);
     Register temp = temps.AcquireX();
@@ -3488,7 +3488,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
     Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -3520,7 +3520,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
   if (lr_status == kLRHasNotBeenSaved) {
     Pop<TurboAssembler::kAuthLR>(lr, padreg);
   }
-  if (FLAG_debug_code) Mov(slot_address, Operand(kZapValue));
+  if (v8_flags.debug_code) Mov(slot_address, Operand(kZapValue));
 
   Bind(&done);
 }
@@ -3538,7 +3538,7 @@ void TurboAssembler::DebugBreak() { Debug("DebugBreak", 0, BREAK); }
 
 void TurboAssembler::Abort(AbortReason reason) {
   ASM_CODE_COMMENT(this);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     RecordComment("Abort message: ");
     RecordComment(GetAbortReason(reason));
   }
@@ -3575,7 +3575,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       UseScratchRegisterScope temps(this);
       Register scratch = temps.AcquireX();
       LoadEntryFromBuiltin(Builtin::kAbort, scratch);
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index 0f77e03655d..08c02d22ed0 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -2063,14 +2063,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, -value, scratch1, scratch2);
   }
 
diff --git a/src/codegen/assembler.cc b/src/codegen/assembler.cc
index 3efa20f2075..961eb74e4d8 100644
--- a/src/codegen/assembler.cc
+++ b/src/codegen/assembler.cc
@@ -67,7 +67,7 @@ AssemblerOptions AssemblerOptions::Default(Isolate* isolate) {
 
   // So here we enable simulator specific code if not generating the snapshot or
   // if we are but we are targetting the simulator *only*.
-  options.enable_simulator_code = !serializer || FLAG_target_is_simulator;
+  options.enable_simulator_code = !serializer || v8_flags.target_is_simulator;
 #endif
 
 #if V8_TARGET_ARCH_X64 || V8_TARGET_ARCH_ARM64
@@ -304,7 +304,7 @@ Handle<HeapObject> AssemblerBase::GetEmbeddedObject(
 
 
 int Assembler::WriteCodeComments() {
-  if (!FLAG_code_comments) return 0;
+  if (!v8_flags.code_comments) return 0;
   CHECK_IMPLIES(code_comments_writer_.entry_count() > 0,
                 options().emit_code_comments);
   if (code_comments_writer_.entry_count() == 0) return 0;
diff --git a/src/codegen/assembler.h b/src/codegen/assembler.h
index aa3b886d2a0..49e3d3270e8 100644
--- a/src/codegen/assembler.h
+++ b/src/codegen/assembler.h
@@ -190,7 +190,7 @@ struct V8_EXPORT_PRIVATE AssemblerOptions {
   // on a function prologue/epilogue.
   bool collect_win64_unwind_info = false;
   // Whether to emit code comments.
-  bool emit_code_comments = FLAG_code_comments;
+  bool emit_code_comments = v8_flags.code_comments;
 
   static AssemblerOptions Default(Isolate* isolate);
   static AssemblerOptions DefaultForOffHeapTrampoline(Isolate* isolate);
@@ -245,7 +245,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
   }
 
   bool is_constant_pool_available() const {
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       // We need to disable constant pool here for embeded builtins
       // because the metadata section is not adjacent to instructions
       return constant_pool_available_ && !options().isolate_independent_code;
@@ -306,7 +306,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
   V8_INLINE void RecordComment(const char* comment) {
     // Set explicit dependency on --code-comments for dead-code elimination in
     // release builds.
-    if (!FLAG_code_comments) return;
+    if (!v8_flags.code_comments) return;
     if (options().emit_code_comments) {
       code_comments_writer_.Add(pc_offset(), std::string(comment));
     }
@@ -315,7 +315,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
   V8_INLINE void RecordComment(std::string comment) {
     // Set explicit dependency on --code-comments for dead-code elimination in
     // release builds.
-    if (!FLAG_code_comments) return;
+    if (!v8_flags.code_comments) return;
     if (options().emit_code_comments) {
       code_comments_writer_.Add(pc_offset(), std::move(comment));
     }
@@ -326,10 +326,10 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
    public:
     explicit CodeComment(Assembler* assembler, const std::string& comment)
         : assembler_(assembler) {
-      if (FLAG_code_comments) Open(comment);
+      if (v8_flags.code_comments) Open(comment);
     }
     ~CodeComment() {
-      if (FLAG_code_comments) Close();
+      if (v8_flags.code_comments) Close();
     }
     static const int kIndentWidth = 2;
 
@@ -375,7 +375,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
   byte* pc_;
 
   void set_constant_pool_available(bool available) {
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       constant_pool_available_ = available;
     } else {
       // Embedded constant pool not supported on this architecture.
@@ -394,7 +394,8 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
     DCHECK(!RelocInfo::IsNoInfo(rmode));
     if (options().disable_reloc_info_for_patching) return false;
     if (RelocInfo::IsOnlyForSerializer(rmode) &&
-        !options().record_reloc_info_for_serialization && !FLAG_debug_code) {
+        !options().record_reloc_info_for_serialization &&
+        !v8_flags.debug_code) {
       return false;
     }
 #ifndef ENABLE_DISASSEMBLER
@@ -488,7 +489,7 @@ class V8_EXPORT_PRIVATE V8_NODISCARD CpuFeatureScope {
 #define NOOP_UNLESS_DEBUG_CODE ;
 #else
 #define NOOP_UNLESS_DEBUG_CODE \
-  { static_assert(FLAG_debug_code.value() == false); }
+  { static_assert(v8_flags.debug_code.value() == false); }
 #endif
 
 }  // namespace internal
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 688eb4540e3..550025ec72c 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -38,27 +38,27 @@ namespace internal {
 CodeStubAssembler::CodeStubAssembler(compiler::CodeAssemblerState* state)
     : compiler::CodeAssembler(state),
       TorqueGeneratedExportedMacrosAssembler(state) {
-  if (DEBUG_BOOL && FLAG_csa_trap_on_node != nullptr) {
+  if (DEBUG_BOOL && v8_flags.csa_trap_on_node != nullptr) {
     HandleBreakOnNode();
   }
 }
 
 void CodeStubAssembler::HandleBreakOnNode() {
-  // FLAG_csa_trap_on_node should be in a form "STUB,NODE" where STUB is a
+  // v8_flags.csa_trap_on_node should be in a form "STUB,NODE" where STUB is a
   // string specifying the name of a stub and NODE is number specifying node id.
   const char* name = state()->name();
   size_t name_length = strlen(name);
-  if (strncmp(FLAG_csa_trap_on_node, name, name_length) != 0) {
+  if (strncmp(v8_flags.csa_trap_on_node, name, name_length) != 0) {
     // Different name.
     return;
   }
-  size_t option_length = strlen(FLAG_csa_trap_on_node);
+  size_t option_length = strlen(v8_flags.csa_trap_on_node);
   if (option_length < name_length + 2 ||
-      FLAG_csa_trap_on_node[name_length] != ',') {
+      v8_flags.csa_trap_on_node[name_length] != ',') {
     // Option is too short.
     return;
   }
-  const char* start = &FLAG_csa_trap_on_node[name_length + 1];
+  const char* start = &v8_flags.csa_trap_on_node[name_length + 1];
   char* end;
   int node_id = static_cast<int>(strtol(start, &end, 10));
   if (start == end) {
@@ -72,7 +72,7 @@ void CodeStubAssembler::Dcheck(const BranchGenerator& branch,
                                const char* message, const char* file, int line,
                                std::initializer_list<ExtraNode> extra_nodes) {
 #if defined(DEBUG)
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Check(branch, message, file, line, extra_nodes);
   }
 #endif
@@ -82,7 +82,7 @@ void CodeStubAssembler::Dcheck(const NodeGenerator<BoolT>& condition_body,
                                const char* message, const char* file, int line,
                                std::initializer_list<ExtraNode> extra_nodes) {
 #if defined(DEBUG)
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Check(condition_body, message, file, line, extra_nodes);
   }
 #endif
@@ -92,7 +92,7 @@ void CodeStubAssembler::Dcheck(TNode<Word32T> condition_node,
                                const char* message, const char* file, int line,
                                std::initializer_list<ExtraNode> extra_nodes) {
 #if defined(DEBUG)
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Check(condition_node, message, file, line, extra_nodes);
   }
 #endif
@@ -1367,7 +1367,7 @@ TNode<HeapObject> CodeStubAssembler::AllocateInNewSpace(
 TNode<HeapObject> CodeStubAssembler::Allocate(TNode<IntPtrT> size_in_bytes,
                                               AllocationFlags flags) {
   Comment("Allocate");
-  if (FLAG_single_generation) flags |= AllocationFlag::kPretenured;
+  if (v8_flags.single_generation) flags |= AllocationFlag::kPretenured;
   bool const new_space = !(flags & AllocationFlag::kPretenured);
   bool const allow_large_objects =
       flags & AllocationFlag::kAllowLargeObjectAllocation;
@@ -2241,7 +2241,7 @@ CodeStubAssembler::LoadFixedArrayElement<IntPtrT>(TNode<FixedArray>,
 void CodeStubAssembler::FixedArrayBoundsCheck(TNode<FixedArrayBase> array,
                                               TNode<Smi> index,
                                               int additional_offset) {
-  if (!FLAG_fixed_array_bounds_checks) return;
+  if (!v8_flags.fixed_array_bounds_checks) return;
   DCHECK(IsAligned(additional_offset, kTaggedSize));
   TNode<Smi> effective_index;
   Smi constant_index;
@@ -2259,7 +2259,7 @@ void CodeStubAssembler::FixedArrayBoundsCheck(TNode<FixedArrayBase> array,
 void CodeStubAssembler::FixedArrayBoundsCheck(TNode<FixedArrayBase> array,
                                               TNode<IntPtrT> index,
                                               int additional_offset) {
-  if (!FLAG_fixed_array_bounds_checks) return;
+  if (!v8_flags.fixed_array_bounds_checks) return;
   DCHECK(IsAligned(additional_offset, kTaggedSize));
   // IntPtrAdd does constant-folding automatically.
   TNode<IntPtrT> effective_index =
@@ -5379,7 +5379,7 @@ void CodeStubAssembler::InitializeAllocationMemento(
   StoreMapNoWriteBarrier(memento, RootIndex::kAllocationMementoMap);
   StoreObjectFieldNoWriteBarrier(
       memento, AllocationMemento::kAllocationSiteOffset, allocation_site);
-  if (FLAG_allocation_site_pretenuring) {
+  if (v8_flags.allocation_site_pretenuring) {
     TNode<Int32T> count = LoadObjectField<Int32T>(
         allocation_site, AllocationSite::kPretenureCreateCountOffset);
 
@@ -7178,7 +7178,7 @@ TNode<String> ToDirectStringAssembler::TryToDirect(Label* if_bailout) {
   // Sliced string. Fetch parent and correct start index by offset.
   BIND(&if_issliced);
   {
-    if (!FLAG_string_slices || (flags_ & kDontUnpackSlicedStrings)) {
+    if (!v8_flags.string_slices || (flags_ & kDontUnpackSlicedStrings)) {
       Goto(if_bailout);
     } else {
       const TNode<String> string = var_string_.value();
@@ -7980,7 +7980,7 @@ TNode<WordT> CodeStubAssembler::UpdateWord(TNode<WordT> word,
 }
 
 void CodeStubAssembler::SetCounter(StatsCounter* counter, int value) {
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     TNode<ExternalReference> counter_address =
         ExternalConstant(ExternalReference::Create(counter));
     StoreNoWriteBarrier(MachineRepresentation::kWord32, counter_address,
@@ -7990,7 +7990,7 @@ void CodeStubAssembler::SetCounter(StatsCounter* counter, int value) {
 
 void CodeStubAssembler::IncrementCounter(StatsCounter* counter, int delta) {
   DCHECK_GT(delta, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     TNode<ExternalReference> counter_address =
         ExternalConstant(ExternalReference::Create(counter));
     // This operation has to be exactly 32-bit wide in case the external
@@ -8004,7 +8004,7 @@ void CodeStubAssembler::IncrementCounter(StatsCounter* counter, int delta) {
 
 void CodeStubAssembler::DecrementCounter(StatsCounter* counter, int delta) {
   DCHECK_GT(delta, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     TNode<ExternalReference> counter_address =
         ExternalConstant(ExternalReference::Create(counter));
     // This operation has to be exactly 32-bit wide in case the external
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index a955be6d597..324e6a43a77 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -310,9 +310,9 @@ enum class PrimitiveType { kBoolean, kNumber, kString, kSymbol };
   EXPAND(TYPED_VARIABLE_CONSTRUCTOR(__VA_ARGS__, this))
 
 #ifdef ENABLE_SLOW_DCHECKS
-#define CSA_SLOW_DCHECK(csa, ...) \
-  if (FLAG_enable_slow_asserts) { \
-    CSA_DCHECK(csa, __VA_ARGS__); \
+#define CSA_SLOW_DCHECK(csa, ...)     \
+  if (v8_flags.enable_slow_asserts) { \
+    CSA_DCHECK(csa, __VA_ARGS__);     \
   }
 #else
 #define CSA_SLOW_DCHECK(csa, ...) ((void)0)
diff --git a/src/codegen/compilation-cache.cc b/src/codegen/compilation-cache.cc
index ea68330d638..d5d371e26c3 100644
--- a/src/codegen/compilation-cache.cc
+++ b/src/codegen/compilation-cache.cc
@@ -63,7 +63,7 @@ void CompilationCacheRegExp::Age() {
 
 void CompilationCacheScript::Age() {
   DisallowGarbageCollection no_gc;
-  if (!FLAG_isolate_script_cache_ageing) return;
+  if (!v8_flags.isolate_script_cache_ageing) return;
   if (table_.IsUndefined(isolate())) return;
   CompilationCacheTable table = CompilationCacheTable::cast(table_);
 
diff --git a/src/codegen/compilation-cache.h b/src/codegen/compilation-cache.h
index 3d78de632d1..536413322c3 100644
--- a/src/codegen/compilation-cache.h
+++ b/src/codegen/compilation-cache.h
@@ -212,11 +212,11 @@ class V8_EXPORT_PRIVATE CompilationCache {
   base::HashMap* EagerOptimizingSet();
 
   bool IsEnabledScriptAndEval() const {
-    return FLAG_compilation_cache && enabled_script_and_eval_;
+    return v8_flags.compilation_cache && enabled_script_and_eval_;
   }
   bool IsEnabledScript(LanguageMode language_mode) {
-    // Tests can change FLAG_use_strict at runtime. The compilation cache only
-    // contains scripts which were compiled with the default language mode.
+    // Tests can change v8_flags.use_strict at runtime. The compilation cache
+    // only contains scripts which were compiled with the default language mode.
     return IsEnabledScriptAndEval() && language_mode == LanguageMode::kSloppy;
   }
 
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index a0f85198ef1..6e603e215b5 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -104,7 +104,7 @@ class CompilerTracer : public AllStatic {
  public:
   static void TraceStartBaselineCompile(Isolate* isolate,
                                         Handle<SharedFunctionInfo> shared) {
-    if (!FLAG_trace_baseline) return;
+    if (!v8_flags.trace_baseline) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "compiling method", shared, CodeKind::BASELINE);
     PrintTraceSuffix(scope);
@@ -113,7 +113,7 @@ class CompilerTracer : public AllStatic {
   static void TraceStartMaglevCompile(Isolate* isolate,
                                       Handle<JSFunction> function,
                                       ConcurrencyMode mode) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "compiling method", function, CodeKind::MAGLEV);
     PrintF(scope.file(), ", mode: %s", ToString(mode));
@@ -122,7 +122,7 @@ class CompilerTracer : public AllStatic {
 
   static void TracePrepareJob(Isolate* isolate, OptimizedCompilationInfo* info,
                               ConcurrencyMode mode) {
-    if (!FLAG_trace_opt || !info->IsOptimizing()) return;
+    if (!v8_flags.trace_opt || !info->IsOptimizing()) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "compiling method", info);
     if (info->is_osr()) PrintF(scope.file(), " OSR");
@@ -134,7 +134,7 @@ class CompilerTracer : public AllStatic {
                                       Handle<JSFunction> function,
                                       BytecodeOffset osr_offset,
                                       ConcurrencyMode mode) {
-    if (!FLAG_trace_osr) return;
+    if (!v8_flags.trace_osr) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(
         scope.file(),
@@ -145,7 +145,7 @@ class CompilerTracer : public AllStatic {
   static void TraceOptimizeOSRFinished(Isolate* isolate,
                                        Handle<JSFunction> function,
                                        BytecodeOffset osr_offset) {
-    if (!FLAG_trace_osr) return;
+    if (!v8_flags.trace_osr) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(scope.file(),
            "[OSR - compilation finished. function: %s, osr offset: %d]\n",
@@ -156,7 +156,7 @@ class CompilerTracer : public AllStatic {
                                         Handle<JSFunction> function,
                                         BytecodeOffset osr_offset,
                                         ConcurrencyMode mode) {
-    if (!FLAG_trace_osr) return;
+    if (!v8_flags.trace_osr) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(scope.file(),
            "[OSR - available (compilation completed or cache hit). function: "
@@ -168,7 +168,7 @@ class CompilerTracer : public AllStatic {
                                           Handle<JSFunction> function,
                                           BytecodeOffset osr_offset,
                                           ConcurrencyMode mode) {
-    if (!FLAG_trace_osr) return;
+    if (!v8_flags.trace_osr) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(scope.file(),
            "[OSR - unavailable (failed or in progress). function: %s, osr "
@@ -180,7 +180,7 @@ class CompilerTracer : public AllStatic {
                                     OptimizedCompilationInfo* info,
                                     double ms_creategraph, double ms_optimize,
                                     double ms_codegen) {
-    if (!FLAG_trace_opt || !info->IsOptimizing()) return;
+    if (!v8_flags.trace_opt || !info->IsOptimizing()) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "optimizing", info);
     if (info->is_osr()) PrintF(scope.file(), " OSR");
@@ -192,7 +192,7 @@ class CompilerTracer : public AllStatic {
   static void TraceFinishBaselineCompile(Isolate* isolate,
                                          Handle<SharedFunctionInfo> shared,
                                          double ms_timetaken) {
-    if (!FLAG_trace_baseline) return;
+    if (!v8_flags.trace_baseline) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "compiling", shared, CodeKind::BASELINE);
     PrintF(scope.file(), " - took %0.3f ms", ms_timetaken);
@@ -203,7 +203,7 @@ class CompilerTracer : public AllStatic {
                                        Handle<JSFunction> function,
                                        double ms_prepare, double ms_optimize,
                                        double ms_codegen) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "completed compiling", function, CodeKind::MAGLEV);
     PrintF(scope.file(), " - took %0.3f, %0.3f, %0.3f ms", ms_prepare,
@@ -214,7 +214,7 @@ class CompilerTracer : public AllStatic {
   static void TraceAbortedMaglevCompile(Isolate* isolate,
                                         Handle<JSFunction> function,
                                         BailoutReason bailout_reason) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "aborted compiling", function, CodeKind::MAGLEV);
     PrintF(scope.file(), " because: %s", GetBailoutReason(bailout_reason));
@@ -223,7 +223,7 @@ class CompilerTracer : public AllStatic {
 
   static void TraceCompletedJob(Isolate* isolate,
                                 OptimizedCompilationInfo* info) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "completed optimizing", info);
     if (info->is_osr()) PrintF(scope.file(), " OSR");
@@ -232,7 +232,7 @@ class CompilerTracer : public AllStatic {
 
   static void TraceAbortedJob(Isolate* isolate,
                               OptimizedCompilationInfo* info) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "aborted optimizing", info);
     if (info->is_osr()) PrintF(scope.file(), " OSR");
@@ -245,7 +245,7 @@ class CompilerTracer : public AllStatic {
                                          Handle<JSFunction> function,
                                          BytecodeOffset osr_offset,
                                          CodeKind code_kind) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "found optimized code for", function, code_kind);
     if (IsOSR(osr_offset)) {
@@ -257,7 +257,7 @@ class CompilerTracer : public AllStatic {
   static void TraceOptimizeForAlwaysOpt(Isolate* isolate,
                                         Handle<JSFunction> function,
                                         CodeKind code_kind) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintTracePrefix(scope, "optimizing", function, code_kind);
     PrintF(scope.file(), " because --always-turbofan");
@@ -266,7 +266,7 @@ class CompilerTracer : public AllStatic {
 
   static void TraceMarkForAlwaysOpt(Isolate* isolate,
                                     Handle<JSFunction> function) {
-    if (!FLAG_trace_opt) return;
+    if (!v8_flags.trace_opt) return;
     CodeTracer::Scope scope(isolate->GetCodeTracer());
     PrintF(scope.file(), "[marking ");
     function->ShortPrint(scope.file());
@@ -321,7 +321,7 @@ void Compiler::LogFunctionCompilation(Isolate* isolate,
   // script name and line number. Check explicitly whether logging is
   // enabled as finding the line number is not free.
   if (!isolate->v8_file_logger()->is_listening_to_code_events() &&
-      !isolate->is_profiling() && !FLAG_log_function_events &&
+      !isolate->is_profiling() && !v8_flags.log_function_events &&
       !isolate->logger()->is_listening_to_code_events()) {
     return;
   }
@@ -339,7 +339,7 @@ void Compiler::LogFunctionCompilation(Isolate* isolate,
   if (!vector.is_null()) {
     LOG(isolate, FeedbackVectorEvent(*vector, *abstract_code));
   }
-  if (!FLAG_log_function_events) return;
+  if (!v8_flags.log_function_events) return;
 
   std::string name;
   switch (kind) {
@@ -526,7 +526,7 @@ void TurbofanCompilationJob::RecordCompilationStats(ConcurrencyMode mode,
   double ms_codegen = time_taken_to_finalize_.InMillisecondsF();
   CompilerTracer::TraceCompilationStats(
       isolate, compilation_info(), ms_creategraph, ms_optimize, ms_codegen);
-  if (FLAG_trace_opt_stats) {
+  if (v8_flags.trace_opt_stats) {
     static double compilation_time = 0.0;
     static int compiled_functions = 0;
     static int code_size = 0;
@@ -614,14 +614,14 @@ namespace {
 #if V8_ENABLE_WEBASSEMBLY
 bool UseAsmWasm(FunctionLiteral* literal, bool asm_wasm_broken) {
   // Check whether asm.js validation is enabled.
-  if (!FLAG_validate_asm) return false;
+  if (!v8_flags.validate_asm) return false;
 
   // Modules that have validated successfully, but were subsequently broken by
   // invalid module instantiation attempts are off limit forever.
   if (asm_wasm_broken) return false;
 
   // In stress mode we want to run the validator on everything.
-  if (FLAG_stress_validate_asm) return true;
+  if (v8_flags.stress_validate_asm) return true;
 
   // In general, we respect the "use asm" directive.
   return literal->scope()->IsAsmModule();
@@ -631,7 +631,7 @@ bool UseAsmWasm(FunctionLiteral* literal, bool asm_wasm_broken) {
 void InstallInterpreterTrampolineCopy(Isolate* isolate,
                                       Handle<SharedFunctionInfo> shared_info,
                                       LogEventListener::CodeTag log_tag) {
-  DCHECK(FLAG_interpreted_frames_native_stack);
+  DCHECK(v8_flags.interpreted_frames_native_stack);
   if (!shared_info->function_data(kAcquireLoad).IsBytecodeArray()) {
     DCHECK(!shared_info->HasBytecodeArray());
     return;
@@ -1064,7 +1064,7 @@ bool CompileTurbofan_Concurrent(Isolate* isolate,
   Handle<JSFunction> function = compilation_info->closure();
 
   if (!isolate->optimizing_compile_dispatcher()->IsQueueAvailable()) {
-    if (FLAG_trace_concurrent_recompilation) {
+    if (v8_flags.trace_concurrent_recompilation) {
       PrintF("  ** Compilation queue full, will retry optimizing ");
       function->ShortPrint();
       PrintF(" later.\n");
@@ -1073,7 +1073,7 @@ bool CompileTurbofan_Concurrent(Isolate* isolate,
   }
 
   if (isolate->heap()->HighMemoryPressure()) {
-    if (FLAG_trace_concurrent_recompilation) {
+    if (v8_flags.trace_concurrent_recompilation) {
       PrintF("  ** High memory pressure, will retry optimizing ");
       function->ShortPrint();
       PrintF(" later.\n");
@@ -1094,7 +1094,7 @@ bool CompileTurbofan_Concurrent(Isolate* isolate,
   // The background recompile will own this job.
   isolate->optimizing_compile_dispatcher()->QueueForOptimization(job.release());
 
-  if (FLAG_trace_concurrent_recompilation) {
+  if (v8_flags.trace_concurrent_recompilation) {
     PrintF("  ** Queued ");
     function->ShortPrint();
     PrintF(" for concurrent optimization.\n");
@@ -1118,9 +1118,9 @@ bool ShouldOptimize(CodeKind code_kind, Handle<SharedFunctionInfo> shared) {
   DCHECK(CodeKindIsOptimizedJSFunction(code_kind));
   switch (code_kind) {
     case CodeKind::TURBOFAN:
-      return FLAG_turbofan && shared->PassesFilter(FLAG_turbo_filter);
+      return v8_flags.turbofan && shared->PassesFilter(v8_flags.turbo_filter);
     case CodeKind::MAGLEV:
-      return FLAG_maglev && shared->PassesFilter(FLAG_maglev_filter);
+      return v8_flags.maglev && shared->PassesFilter(v8_flags.maglev_filter);
     default:
       UNREACHABLE();
   }
@@ -1198,7 +1198,7 @@ MaybeHandle<CodeT> CompileMaglev(Isolate* isolate, Handle<JSFunction> function,
                                  BytecodeOffset osr_offset,
                                  CompileResultBehavior result_behavior) {
 #ifdef V8_ENABLE_MAGLEV
-  DCHECK(FLAG_maglev);
+  DCHECK(v8_flags.maglev);
   // TODO(v8:7700): Add missing support.
   CHECK(!IsOSR(osr_offset));
   CHECK(result_behavior == CompileResultBehavior::kDefault);
@@ -1289,7 +1289,7 @@ MaybeHandle<CodeT> GetOrCompileOptimized(
 
   // If code was pending optimization for testing, remove the entry from the
   // table that was preventing the bytecode from being flushed.
-  if (V8_UNLIKELY(FLAG_testing_d8_test_runner)) {
+  if (V8_UNLIKELY(v8_flags.testing_d8_test_runner)) {
     PendingOptimizationTable::FunctionWasOptimized(isolate, function);
   }
 
@@ -1323,11 +1323,11 @@ void SpawnDuplicateConcurrentJobForStressTesting(Isolate* isolate,
   // TODO(v8:7700): Support Maglev.
   if (code_kind == CodeKind::MAGLEV) return;
 
-  DCHECK(FLAG_stress_concurrent_inlining &&
+  DCHECK(v8_flags.stress_concurrent_inlining &&
          isolate->concurrent_recompilation_enabled() && IsSynchronous(mode) &&
          isolate->node_observer() == nullptr);
   CompileResultBehavior result_behavior =
-      FLAG_stress_concurrent_inlining_attach_code
+      v8_flags.stress_concurrent_inlining_attach_code
           ? CompileResultBehavior::kDefault
           : CompileResultBehavior::kDiscardForTesting;
   USE(GetOrCompileOptimized(isolate, function, ConcurrencyMode::kConcurrent,
@@ -1385,7 +1385,7 @@ void FinalizeUnoptimizedCompilation(
     compile_state->pending_error_handler()->ReportWarnings(isolate, script);
   }
 
-  bool need_source_positions = FLAG_stress_lazy_source_positions ||
+  bool need_source_positions = v8_flags.stress_lazy_source_positions ||
                                (!flags.collect_source_positions() &&
                                 isolate->NeedsSourcePositionsForProfiling());
 
@@ -1408,7 +1408,7 @@ void FinalizeUnoptimizedCompilation(
       log_tag = LogEventListener::CodeTag::kFunction;
     }
     log_tag = V8FileLogger::ToNativeByScript(log_tag, *script);
-    if (FLAG_interpreted_frames_native_stack) {
+    if (v8_flags.interpreted_frames_native_stack) {
       InstallInterpreterTrampolineCopy(isolate, shared_info, log_tag);
     }
     Handle<CoverageInfo> coverage_info;
@@ -1535,7 +1535,7 @@ MaybeHandle<SharedFunctionInfo> CompileToplevel(
       isolate, script, parse_info->flags(), parse_info->state(),
       finalize_unoptimized_compilation_data_list);
 
-  if (FLAG_always_sparkplug) {
+  if (v8_flags.always_sparkplug) {
     CompileAllWithBaseline(isolate, finalize_unoptimized_compilation_data_list);
   }
 
@@ -1582,13 +1582,13 @@ BackgroundCompileTask::BackgroundCompileTask(
     ScriptCompiler::CompileOptions options)
     : isolate_for_local_isolate_(isolate),
       flags_(UnoptimizedCompileFlags::ForToplevelCompile(
-          isolate, true, construct_language_mode(FLAG_use_strict),
+          isolate, true, construct_language_mode(v8_flags.use_strict),
           REPLMode::kNo, type,
           options != ScriptCompiler::CompileOptions::kEagerCompile &&
-              FLAG_lazy_streaming)),
+              v8_flags.lazy_streaming)),
       character_stream_(ScannerStream::For(streamed_data->source_stream.get(),
                                            streamed_data->encoding)),
-      stack_size_(i::FLAG_stack_size),
+      stack_size_(v8_flags.stack_size),
       worker_thread_runtime_call_stats_(
           isolate->counters()->worker_thread_runtime_call_stats()),
       timer_(isolate->counters()->compile_script_on_background()),
@@ -1888,7 +1888,7 @@ void BackgroundCompileTask::Run(
 
   if (maybe_result.is_null()) {
     PreparePendingException(isolate, &info);
-  } else if (FLAG_enable_slow_asserts) {
+  } else if (v8_flags.enable_slow_asserts) {
 #ifdef ENABLE_SLOW_DCHECKS
     MergeAssumptionChecker checker(isolate);
     checker.IterateObjects(*maybe_result.ToHandleChecked());
@@ -2275,7 +2275,7 @@ void BackgroundDeserializeTask::Run() {
   Handle<SharedFunctionInfo> inner_result;
   off_thread_data_ =
       CodeSerializer::StartDeserializeOffThread(&isolate, &cached_data_);
-  if (FLAG_enable_slow_asserts && off_thread_data_.HasResult()) {
+  if (v8_flags.enable_slow_asserts && off_thread_data_.HasResult()) {
 #ifdef ENABLE_SLOW_DCHECKS
     MergeAssumptionChecker checker(&isolate);
     checker.IterateObjects(*off_thread_data_.GetOnlyScript(isolate.heap()));
@@ -2287,7 +2287,7 @@ void BackgroundDeserializeTask::SourceTextAvailable(
     Isolate* isolate, Handle<String> source_text,
     const ScriptDetails& script_details) {
   DCHECK_EQ(isolate, isolate_for_local_isolate_);
-  LanguageMode language_mode = construct_language_mode(FLAG_use_strict);
+  LanguageMode language_mode = construct_language_mode(v8_flags.use_strict);
   background_merge_task_.SetUpOnMainThread(isolate, source_text, script_details,
                                            language_mode);
 }
@@ -2307,13 +2307,13 @@ void BackgroundCompileTask::SourceTextAvailable(
 }
 
 bool BackgroundDeserializeTask::ShouldMergeWithExistingScript() const {
-  DCHECK(FLAG_merge_background_deserialized_script_with_compilation_cache);
+  DCHECK(v8_flags.merge_background_deserialized_script_with_compilation_cache);
   return background_merge_task_.HasCachedScript() &&
          off_thread_data_.HasResult();
 }
 
 bool BackgroundCompileTask::ShouldMergeWithExistingScript() const {
-  DCHECK(FLAG_stress_background_compile);
+  DCHECK(v8_flags.stress_background_compile);
   DCHECK(!script_.is_null());
   return background_merge_task_.HasCachedScript() &&
          jobs_to_retry_finalization_on_main_thread_.empty();
@@ -2532,7 +2532,7 @@ bool Compiler::Compile(Isolate* isolate, Handle<SharedFunctionInfo> shared_info,
   FinalizeUnoptimizedCompilation(isolate, script, flags, &compile_state,
                                  finalize_unoptimized_compilation_data_list);
 
-  if (FLAG_always_sparkplug) {
+  if (v8_flags.always_sparkplug) {
     CompileAllWithBaseline(isolate, finalize_unoptimized_compilation_data_list);
   }
 
@@ -2577,9 +2577,9 @@ bool Compiler::Compile(Isolate* isolate, Handle<JSFunction> function,
 
   // Optimize now if --always-turbofan is enabled.
 #if V8_ENABLE_WEBASSEMBLY
-  if (FLAG_always_turbofan && !function->shared().HasAsmWasmData()) {
+  if (v8_flags.always_turbofan && !function->shared().HasAsmWasmData()) {
 #else
-  if (FLAG_always_turbofan) {
+  if (v8_flags.always_turbofan) {
 #endif  // V8_ENABLE_WEBASSEMBLY
     CompilerTracer::TraceOptimizeForAlwaysOpt(isolate, function,
                                               CodeKindForTopTier());
@@ -2587,7 +2587,7 @@ bool Compiler::Compile(Isolate* isolate, Handle<JSFunction> function,
     const CodeKind code_kind = CodeKindForTopTier();
     const ConcurrencyMode concurrency_mode = ConcurrencyMode::kSynchronous;
 
-    if (FLAG_stress_concurrent_inlining &&
+    if (v8_flags.stress_concurrent_inlining &&
         isolate->concurrent_recompilation_enabled() &&
         isolate->node_observer() == nullptr) {
       SpawnDuplicateConcurrentJobForStressTesting(isolate, function,
@@ -2740,7 +2740,7 @@ void Compiler::CompileOptimized(Isolate* isolate, Handle<JSFunction> function,
   DCHECK(CodeKindIsOptimizedJSFunction(code_kind));
   DCHECK(AllowCompilation::IsAllowed(isolate));
 
-  if (FLAG_stress_concurrent_inlining &&
+  if (v8_flags.stress_concurrent_inlining &&
       isolate->concurrent_recompilation_enabled() && IsSynchronous(mode) &&
       isolate->node_observer() == nullptr) {
     SpawnDuplicateConcurrentJobForStressTesting(isolate, function, mode,
@@ -2816,7 +2816,7 @@ MaybeHandle<JSFunction> Compiler::GetFunctionFromEval(
   } else {
     UnoptimizedCompileFlags flags = UnoptimizedCompileFlags::ForToplevelCompile(
         isolate, true, language_mode, REPLMode::kNo, ScriptType::kClassic,
-        FLAG_lazy_eval);
+        v8_flags.lazy_eval);
     flags.set_is_eval(true);
     flags.set_parsing_while_debugging(parsing_while_debugging);
     DCHECK(!flags.is_module());
@@ -3470,7 +3470,7 @@ MaybeHandle<SharedFunctionInfo> GetSharedFunctionInfoForScriptImpl(
   }
 
   if (V8_UNLIKELY(
-          i::FLAG_experimental_web_snapshots &&
+          v8_flags.experimental_web_snapshots &&
           (source->IsExternalOneByteString() || source->IsSeqOneByteString() ||
            source->IsExternalTwoByteString() || source->IsSeqTwoByteString()) &&
           source->length() > 4)) {
@@ -3492,7 +3492,7 @@ MaybeHandle<SharedFunctionInfo> GetSharedFunctionInfoForScriptImpl(
     }
   }
 
-  LanguageMode language_mode = construct_language_mode(FLAG_use_strict);
+  LanguageMode language_mode = construct_language_mode(v8_flags.use_strict);
   CompilationCache* compilation_cache = isolate->compilation_cache();
 
   // For extensions or REPL mode scripts neither do a compilation cache lookup,
@@ -3573,7 +3573,7 @@ MaybeHandle<SharedFunctionInfo> GetSharedFunctionInfoForScriptImpl(
 
   if (maybe_result.is_null()) {
     // No cache entry found compile the script.
-    if (FLAG_stress_background_compile &&
+    if (v8_flags.stress_background_compile &&
         CanBackgroundCompile(script_details, extension, compile_options,
                              natives)) {
       // If the --stress-background-compile flag is set, do the actual
@@ -3587,7 +3587,7 @@ MaybeHandle<SharedFunctionInfo> GetSharedFunctionInfoForScriptImpl(
               script_details.repl_mode,
               script_details.origin_options.IsModule() ? ScriptType::kModule
                                                        : ScriptType::kClassic,
-              FLAG_lazy);
+              v8_flags.lazy);
 
       flags.set_is_eager(compile_options == ScriptCompiler::kEagerCompile);
 
@@ -3676,7 +3676,7 @@ MaybeHandle<JSFunction> Compiler::GetWrappedFunction(
     DCHECK(cached_data);
   }
 
-  LanguageMode language_mode = construct_language_mode(FLAG_use_strict);
+  LanguageMode language_mode = construct_language_mode(v8_flags.use_strict);
 
   MaybeHandle<SharedFunctionInfo> maybe_result;
   bool can_consume_code_cache =
@@ -3702,7 +3702,7 @@ MaybeHandle<JSFunction> Compiler::GetWrappedFunction(
   if (!maybe_result.ToHandle(&wrapped)) {
     UnoptimizedCompileFlags flags = UnoptimizedCompileFlags::ForToplevelCompile(
         isolate, true, language_mode, script_details.repl_mode,
-        ScriptType::kClassic, FLAG_lazy);
+        ScriptType::kClassic, v8_flags.lazy);
     flags.set_is_eval(true);  // Use an eval scope as declaration scope.
     flags.set_function_syntax_kind(FunctionSyntaxKind::kWrapped);
     // TODO(delphick): Remove this and instead make the wrapped and wrapper
@@ -4081,7 +4081,7 @@ void Compiler::PostInstantiation(Handle<JSFunction> function) {
       }
     }
 
-    if (FLAG_always_turbofan && shared->allows_lazy_compilation() &&
+    if (v8_flags.always_turbofan && shared->allows_lazy_compilation() &&
         !shared->optimization_disabled() &&
         !function->HasAvailableOptimizedCode()) {
       CompilerTracer::TraceMarkForAlwaysOpt(isolate, function);
diff --git a/src/codegen/external-reference-encoder.cc b/src/codegen/external-reference-encoder.cc
index 0dfe3e976a4..67a69075454 100644
--- a/src/codegen/external-reference-encoder.cc
+++ b/src/codegen/external-reference-encoder.cc
@@ -44,7 +44,7 @@ ExternalReferenceEncoder::ExternalReferenceEncoder(Isolate* isolate) {
 
 #ifdef DEBUG
 ExternalReferenceEncoder::~ExternalReferenceEncoder() {
-  if (!i::FLAG_external_reference_stats) return;
+  if (!v8_flags.external_reference_stats) return;
   if (api_references_ == nullptr) return;
   for (uint32_t i = 0; api_references_[i] != 0; ++i) {
     Address addr = static_cast<Address>(api_references_[i]);
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 3730210b52c..9c0be0159de 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -572,17 +572,17 @@ ExternalReference ExternalReference::address_of_min_int() {
 
 ExternalReference
 ExternalReference::address_of_mock_arraybuffer_allocator_flag() {
-  return ExternalReference(&FLAG_mock_arraybuffer_allocator);
+  return ExternalReference(&v8_flags.mock_arraybuffer_allocator);
 }
 
-// TODO(jgruber): Update the other extrefs pointing at FLAG_ addresses to be
+// TODO(jgruber): Update the other extrefs pointing at v8_flags. addresses to be
 // called address_of_FLAG_foo (easier grep-ability).
 ExternalReference ExternalReference::address_of_FLAG_trace_osr() {
-  return ExternalReference(&FLAG_trace_osr);
+  return ExternalReference(&v8_flags.trace_osr);
 }
 
 ExternalReference ExternalReference::address_of_builtin_subclassing_flag() {
-  return ExternalReference(&FLAG_builtin_subclassing);
+  return ExternalReference(&v8_flags.builtin_subclassing);
 }
 
 ExternalReference ExternalReference::address_of_runtime_stats_flag() {
@@ -590,7 +590,7 @@ ExternalReference ExternalReference::address_of_runtime_stats_flag() {
 }
 
 ExternalReference ExternalReference::address_of_shared_string_table_flag() {
-  return ExternalReference(&FLAG_shared_string_table);
+  return ExternalReference(&v8_flags.shared_string_table);
 }
 
 ExternalReference ExternalReference::address_of_load_from_stack_count(
@@ -702,7 +702,7 @@ ExternalReference ExternalReference::supports_cetss_address() {
 
 ExternalReference
 ExternalReference::address_of_enable_experimental_regexp_engine() {
-  return ExternalReference(&FLAG_enable_experimental_regexp_engine);
+  return ExternalReference(&v8_flags.enable_experimental_regexp_engine);
 }
 
 namespace {
@@ -1439,7 +1439,7 @@ bool operator!=(ExternalReference lhs, ExternalReference rhs) {
 }
 
 size_t hash_value(ExternalReference reference) {
-  if (FLAG_predictable) {
+  if (v8_flags.predictable) {
     // Avoid ASLR non-determinism in predictable mode. For this, just take the
     // lowest 12 bit corresponding to a 4K page size.
     return base::hash<Address>()(reference.address() & 0xfff);
diff --git a/src/codegen/external-reference.h b/src/codegen/external-reference.h
index 31fe8ae1347..ab065ecc5b7 100644
--- a/src/codegen/external-reference.h
+++ b/src/codegen/external-reference.h
@@ -96,8 +96,8 @@ class StatsCounter;
 
 #define EXTERNAL_REFERENCE_LIST(V)                                             \
   V(abort_with_reason, "abort_with_reason")                                    \
-  V(address_of_FLAG_trace_osr, "FLAG_trace_osr")                               \
-  V(address_of_builtin_subclassing_flag, "FLAG_builtin_subclassing")           \
+  V(address_of_FLAG_trace_osr, "v8_flags.trace_osr")                           \
+  V(address_of_builtin_subclassing_flag, "v8_flags.builtin_subclassing")       \
   V(address_of_double_abs_constant, "double_absolute_constant")                \
   V(address_of_double_neg_constant, "double_negate_constant")                  \
   V(address_of_enable_experimental_regexp_engine,                              \
@@ -106,10 +106,10 @@ class StatsCounter;
   V(address_of_float_neg_constant, "float_negate_constant")                    \
   V(address_of_min_int, "LDoubleConstant::min_int")                            \
   V(address_of_mock_arraybuffer_allocator_flag,                                \
-    "FLAG_mock_arraybuffer_allocator")                                         \
+    "v8_flags.mock_arraybuffer_allocator")                                     \
   V(address_of_one_half, "LDoubleConstant::one_half")                          \
   V(address_of_runtime_stats_flag, "TracingFlags::runtime_stats")              \
-  V(address_of_shared_string_table_flag, "FLAG_shared_string_table")           \
+  V(address_of_shared_string_table_flag, "v8_flags.shared_string_table")       \
   V(address_of_the_hole_nan, "the_hole_nan")                                   \
   V(address_of_uint32_bias, "uint32_bias")                                     \
   V(baseline_pc_for_bytecode_offset, "BaselinePCForBytecodeOffset")            \
diff --git a/src/codegen/flush-instruction-cache.cc b/src/codegen/flush-instruction-cache.cc
index cb4088af135..ea02e177717 100644
--- a/src/codegen/flush-instruction-cache.cc
+++ b/src/codegen/flush-instruction-cache.cc
@@ -13,7 +13,7 @@ namespace internal {
 
 void FlushInstructionCache(void* start, size_t size) {
   if (size == 0) return;
-  if (FLAG_jitless) return;
+  if (v8_flags.jitless) return;
 
 #if defined(USE_SIMULATOR)
   base::MutexGuard lock_guard(Simulator::i_cache_mutex());
diff --git a/src/codegen/ia32/assembler-ia32-inl.h b/src/codegen/ia32/assembler-ia32-inl.h
index 57a835fe248..e1d001ed89f 100644
--- a/src/codegen/ia32/assembler-ia32-inl.h
+++ b/src/codegen/ia32/assembler-ia32-inl.h
@@ -101,7 +101,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/ia32/assembler-ia32.cc b/src/codegen/ia32/assembler-ia32.cc
index 5756d364e5e..b55a57d45cf 100644
--- a/src/codegen/ia32/assembler-ia32.cc
+++ b/src/codegen/ia32/assembler-ia32.cc
@@ -117,7 +117,7 @@ bool OSHasAVXSupport() {
 bool CpuFeatures::SupportsWasmSimd128() {
 #if V8_ENABLE_WEBASSEMBLY
   if (IsSupported(SSE4_1)) return true;
-  if (FLAG_wasm_simd_ssse3_codegen && IsSupported(SSSE3)) return true;
+  if (v8_flags.wasm_simd_ssse3_codegen && IsSupported(SSSE3)) return true;
 #endif  // V8_ENABLE_WEBASSEMBLY
   return false;
 }
@@ -140,13 +140,13 @@ void CpuFeatures::ProbeImpl(bool cross_compile) {
     if (cpu.has_fma3()) SetSupported(FMA3);
   }
 
-  if (cpu.has_bmi1() && FLAG_enable_bmi1) SetSupported(BMI1);
-  if (cpu.has_bmi2() && FLAG_enable_bmi2) SetSupported(BMI2);
-  if (cpu.has_lzcnt() && FLAG_enable_lzcnt) SetSupported(LZCNT);
-  if (cpu.has_popcnt() && FLAG_enable_popcnt) SetSupported(POPCNT);
-  if (strcmp(FLAG_mcpu, "auto") == 0) {
+  if (cpu.has_bmi1() && v8_flags.enable_bmi1) SetSupported(BMI1);
+  if (cpu.has_bmi2() && v8_flags.enable_bmi2) SetSupported(BMI2);
+  if (cpu.has_lzcnt() && v8_flags.enable_lzcnt) SetSupported(LZCNT);
+  if (cpu.has_popcnt() && v8_flags.enable_popcnt) SetSupported(POPCNT);
+  if (strcmp(v8_flags.mcpu, "auto") == 0) {
     if (cpu.is_atom()) SetSupported(INTEL_ATOM);
-  } else if (strcmp(FLAG_mcpu, "atom") == 0) {
+  } else if (strcmp(v8_flags.mcpu, "atom") == 0) {
     SetSupported(INTEL_ATOM);
   }
 
@@ -154,13 +154,13 @@ void CpuFeatures::ProbeImpl(bool cross_compile) {
   // AVX but not SSE4_2, if we have --enable-avx and --no-enable-sse4-2, the
   // code above would set AVX to supported, and SSE4_2 to unsupported, then the
   // checks below will set AVX to unsupported.
-  if (!FLAG_enable_sse3) SetUnsupported(SSE3);
-  if (!FLAG_enable_ssse3 || !IsSupported(SSE3)) SetUnsupported(SSSE3);
-  if (!FLAG_enable_sse4_1 || !IsSupported(SSSE3)) SetUnsupported(SSE4_1);
-  if (!FLAG_enable_sse4_2 || !IsSupported(SSE4_1)) SetUnsupported(SSE4_2);
-  if (!FLAG_enable_avx || !IsSupported(SSE4_2)) SetUnsupported(AVX);
-  if (!FLAG_enable_avx2 || !IsSupported(AVX)) SetUnsupported(AVX2);
-  if (!FLAG_enable_fma3 || !IsSupported(AVX)) SetUnsupported(FMA3);
+  if (!v8_flags.enable_sse3) SetUnsupported(SSE3);
+  if (!v8_flags.enable_ssse3 || !IsSupported(SSE3)) SetUnsupported(SSSE3);
+  if (!v8_flags.enable_sse4_1 || !IsSupported(SSSE3)) SetUnsupported(SSE4_1);
+  if (!v8_flags.enable_sse4_2 || !IsSupported(SSE4_1)) SetUnsupported(SSE4_2);
+  if (!v8_flags.enable_avx || !IsSupported(SSE4_2)) SetUnsupported(AVX);
+  if (!v8_flags.enable_avx2 || !IsSupported(AVX)) SetUnsupported(AVX2);
+  if (!v8_flags.enable_fma3 || !IsSupported(AVX)) SetUnsupported(FMA3);
 
   // Set a static value on whether Simd is supported.
   // This variable is only used for certain archs to query SupportWasmSimd128()
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index fdf063d0d3e..be0a5c8f92a 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -392,7 +392,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kTaggedSize));
 
   lea(slot_address, FieldOperand(object, offset));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label ok;
     test_b(slot_address, Immediate(kTaggedSize - 1));
     j(zero, &ok, Label::kNear);
@@ -406,7 +406,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(value, Immediate(base::bit_cast<int32_t>(kZapValue)));
     mov(slot_address, Immediate(base::bit_cast<int32_t>(kZapValue)));
   }
@@ -502,11 +502,11 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
   DCHECK(!AreAliased(object, value, slot_address));
   AssertNotSmi(object);
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     cmp(value, Operand(slot_address, 0));
@@ -540,7 +540,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Clobber slot_address and value");
     mov(slot_address, Immediate(base::bit_cast<int32_t>(kZapValue)));
     mov(value, Immediate(base::bit_cast<int32_t>(kZapValue)));
@@ -770,7 +770,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CmpObjectType(object, FEEDBACK_VECTOR_TYPE, scratch);
     Assert(equal, AbortReason::kExpectedFeedbackVector);
   }
@@ -871,7 +871,7 @@ void MacroAssembler::MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(equal, AbortReason::kOperandIsNotASmi);
@@ -879,7 +879,7 @@ void MacroAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotAConstructor);
@@ -893,7 +893,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotAFunction);
@@ -907,7 +907,7 @@ void MacroAssembler::AssertFunction(Register object, Register scratch) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotAFunction);
@@ -922,7 +922,7 @@ void MacroAssembler::AssertCallableFunction(Register object, Register scratch) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotABoundFunction);
@@ -934,7 +934,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
 
   test(object, Immediate(kSmiTagMask));
@@ -957,7 +957,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
@@ -971,7 +971,7 @@ void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
 }
 
 void MacroAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmi);
@@ -979,11 +979,11 @@ void MacroAssembler::AssertNotSmi(Register object) {
 }
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason) {
-  if (FLAG_debug_code) Check(cc, reason);
+  if (v8_flags.debug_code) Check(cc, reason);
 }
 
 void TurboAssembler::AssertUnreachable(AbortReason reason) {
-  if (FLAG_debug_code) Abort(reason);
+  if (v8_flags.debug_code) Abort(reason);
 }
 #endif  // V8_ENABLE_DEBUG_CODE
 
@@ -1081,7 +1081,7 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
 
 void TurboAssembler::LeaveFrame(StackFrame::Type type) {
   ASM_CODE_COMMENT(this);
-  if (FLAG_debug_code && !StackFrame::IsJavaScript(type)) {
+  if (v8_flags.debug_code && !StackFrame::IsJavaScript(type)) {
     cmp(Operand(ebp, CommonFrameConstants::kContextOrFrameTypeOffset),
         Immediate(StackFrame::TypeToMarker(type)));
     Check(equal, AbortReason::kStackFrameTypesMustMatch);
@@ -1817,7 +1817,7 @@ void MacroAssembler::LoadWeakValue(Register in_out, Label* target_if_cleared) {
 void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Operand operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter), scratch);
@@ -1832,7 +1832,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
 void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Operand operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter), scratch);
@@ -1868,7 +1868,7 @@ void TurboAssembler::CheckStackAlignment() {
 }
 
 void TurboAssembler::Abort(AbortReason reason) {
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -1899,7 +1899,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       Call(EntryFromBuiltinAsOperand(Builtin::kAbort));
     } else {
       Call(BUILTIN_CODE(isolate(), Abort), RelocInfo::CODE_TARGET);
@@ -1938,7 +1938,7 @@ void TurboAssembler::CallCFunction(Register function, int num_arguments) {
   DCHECK_LE(num_arguments, kMaxCParameters);
   DCHECK(has_frame());
   // Check stack alignment.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CheckStackAlignment();
   }
 
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 027f60f8301..15c65aefe50 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -651,12 +651,12 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // StatsCounter support
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch);
diff --git a/src/codegen/loong64/assembler-loong64-inl.h b/src/codegen/loong64/assembler-loong64-inl.h
index ec472275b15..6a77068d0b3 100644
--- a/src/codegen/loong64/assembler-loong64-inl.h
+++ b/src/codegen/loong64/assembler-loong64-inl.h
@@ -120,7 +120,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/loong64/assembler-loong64.cc b/src/codegen/loong64/assembler-loong64.cc
index 1395cf65472..da8e626f3da 100644
--- a/src/codegen/loong64/assembler-loong64.cc
+++ b/src/codegen/loong64/assembler-loong64.cc
@@ -160,13 +160,13 @@ Assembler::Assembler(const AssemblerOptions& options,
   trampoline_pool_blocked_nesting_ = 0;
   // We leave space (16 * kTrampolineSlotsSize)
   // for BlockTrampolinePoolScope buffer.
-  next_buffer_check_ = FLAG_force_long_branches
+  next_buffer_check_ = v8_flags.force_long_branches
                            ? kMaxInt
                            : kMax16BranchOffset - kTrampolineSlotsSize * 16;
   internal_trampoline_exception_ = false;
   last_bound_pos_ = 0;
 
-  trampoline_emitted_ = FLAG_force_long_branches;
+  trampoline_emitted_ = v8_flags.force_long_branches;
   unbound_labels_count_ = 0;
   block_buffer_growth_ = false;
 }
diff --git a/src/codegen/loong64/macro-assembler-loong64.cc b/src/codegen/loong64/macro-assembler-loong64.cc
index 6299a66333c..a3672cde566 100644
--- a/src/codegen/loong64/macro-assembler-loong64.cc
+++ b/src/codegen/loong64/macro-assembler-loong64.cc
@@ -147,7 +147,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   // of the object, so offset must be a multiple of kPointerSize.
   DCHECK(IsAligned(offset, kPointerSize));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label ok;
     BlockTrampolinePoolScope block_trampoline_pool(this);
     UseScratchRegisterScope temps(this);
@@ -276,7 +276,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
                                  SaveFPRegsMode fp_mode, SmiCheck smi_check) {
   DCHECK(!AreAliased(object, value));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     Add_d(scratch, object, offset);
@@ -285,7 +285,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
            Operand(value));
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -3346,7 +3346,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
@@ -3362,7 +3362,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
@@ -3392,7 +3392,7 @@ void TurboAssembler::Check(Condition cc, AbortReason reason, Register rj,
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -3423,7 +3423,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, t7);
       Call(t7);
     } else {
@@ -3521,7 +3521,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Set up new frame pointer.
   addi_d(fp, sp, ExitFrameConstants::kFixedFrameSizeFromFp);
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     St_d(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
 
@@ -3595,7 +3595,7 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
      ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   Ld_d(cp, MemOperand(t8, 0));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temp(this);
     Register scratch = temp.Acquire();
     li(scratch, Operand(Context::kInvalidContext));
@@ -3633,7 +3633,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_LOONG64
 }
 
@@ -3667,11 +3667,11 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason, Register rs,
                             Operand rk) {
-  if (FLAG_debug_code) Check(cc, reason, rs, rk);
+  if (v8_flags.debug_code) Check(cc, reason, rs, rk);
 }
 
 void TurboAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -3682,7 +3682,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 }
 
 void TurboAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -3693,7 +3693,7 @@ void TurboAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertStackIsAligned() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     const int frame_alignment = ActivationFrameAlignment();
     const int frame_alignment_mask = frame_alignment - 1;
@@ -3715,7 +3715,7 @@ void MacroAssembler::AssertStackIsAligned() {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -3731,7 +3731,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -3748,7 +3748,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -3766,7 +3766,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -3780,7 +3780,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
   static_assert(kSmiTag == 0);
@@ -3796,7 +3796,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
@@ -3976,7 +3976,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
   // provides more information.
 
 #if V8_HOST_ARCH_LOONG64
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kPointerSize) {
@@ -4204,7 +4204,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     GetObjectType(object, scratch, scratch);
     Assert(eq, AbortReason::kExpectedFeedbackVector, scratch,
            Operand(FEEDBACK_VECTOR_TYPE));
diff --git a/src/codegen/loong64/macro-assembler-loong64.h b/src/codegen/loong64/macro-assembler-loong64.h
index 2ae943774d4..8d8019049d6 100644
--- a/src/codegen/loong64/macro-assembler-loong64.h
+++ b/src/codegen/loong64/macro-assembler-loong64.h
@@ -453,7 +453,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   // On LoongArch64, we should sign-extend 32-bit values.
   void SmiToInt32(Register smi) {
-    if (FLAG_enable_slow_asserts) {
+    if (v8_flags.enable_slow_asserts) {
       AssertSmi(smi);
     }
     DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -989,14 +989,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/macro-assembler.h b/src/codegen/macro-assembler.h
index 08f47638fd8..3fb7b0ca58a 100644
--- a/src/codegen/macro-assembler.h
+++ b/src/codegen/macro-assembler.h
@@ -139,10 +139,10 @@ class V8_NODISCARD FrameAndConstantPoolScope {
       : masm_(masm),
         type_(type),
         old_has_frame_(masm->has_frame()),
-        old_constant_pool_available_(FLAG_enable_embedded_constant_pool &&
+        old_constant_pool_available_(v8_flags.enable_embedded_constant_pool &&
                                      masm->is_constant_pool_available()) {
     masm->set_has_frame(true);
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       masm->set_constant_pool_available(true);
     }
     if (type_ != StackFrame::MANUAL && type_ != StackFrame::NO_FRAME_TYPE) {
@@ -153,7 +153,7 @@ class V8_NODISCARD FrameAndConstantPoolScope {
   ~FrameAndConstantPoolScope() {
     masm_->LeaveFrame(type_);
     masm_->set_has_frame(old_has_frame_);
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       masm_->set_constant_pool_available(old_constant_pool_available_);
     }
   }
@@ -172,14 +172,14 @@ class V8_NODISCARD ConstantPoolUnavailableScope {
  public:
   explicit ConstantPoolUnavailableScope(Assembler* assembler)
       : assembler_(assembler),
-        old_constant_pool_available_(FLAG_enable_embedded_constant_pool &&
+        old_constant_pool_available_(v8_flags.enable_embedded_constant_pool &&
                                      assembler->is_constant_pool_available()) {
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       assembler->set_constant_pool_available(false);
     }
   }
   ~ConstantPoolUnavailableScope() {
-    if (FLAG_enable_embedded_constant_pool) {
+    if (v8_flags.enable_embedded_constant_pool) {
       assembler_->set_constant_pool_available(old_constant_pool_available_);
     }
   }
diff --git a/src/codegen/mips/assembler-mips-inl.h b/src/codegen/mips/assembler-mips-inl.h
index 6bda8bd7437..56e1bf2cfc1 100644
--- a/src/codegen/mips/assembler-mips-inl.h
+++ b/src/codegen/mips/assembler-mips-inl.h
@@ -190,7 +190,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
                                      icache_flush_mode);
   }
   if (write_barrier_mode == UPDATE_WRITE_BARRIER && !host().is_null() &&
-      !FLAG_disable_write_barriers) {
+      !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target);
   }
 }
diff --git a/src/codegen/mips/assembler-mips.cc b/src/codegen/mips/assembler-mips.cc
index 63f0373b686..8bcc6a2e08b 100644
--- a/src/codegen/mips/assembler-mips.cc
+++ b/src/codegen/mips/assembler-mips.cc
@@ -299,13 +299,13 @@ Assembler::Assembler(const AssemblerOptions& options,
   trampoline_pool_blocked_nesting_ = 0;
   // We leave space (16 * kTrampolineSlotsSize)
   // for BlockTrampolinePoolScope buffer.
-  next_buffer_check_ = FLAG_force_long_branches
+  next_buffer_check_ = v8_flags.force_long_branches
                            ? kMaxInt
                            : kMaxBranchOffset - kTrampolineSlotsSize * 16;
   internal_trampoline_exception_ = false;
   last_bound_pos_ = 0;
 
-  trampoline_emitted_ = FLAG_force_long_branches;
+  trampoline_emitted_ = v8_flags.force_long_branches;
   unbound_labels_count_ = 0;
   block_buffer_growth_ = false;
 }
diff --git a/src/codegen/mips/macro-assembler-mips.cc b/src/codegen/mips/macro-assembler-mips.cc
index a94591daf41..6eba156d2d4 100644
--- a/src/codegen/mips/macro-assembler-mips.cc
+++ b/src/codegen/mips/macro-assembler-mips.cc
@@ -158,7 +158,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kPointerSize));
 
   Addu(dst, object, Operand(offset - kHeapObjectTag));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     BlockTrampolinePoolScope block_trampoline_pool(this);
     Label ok;
     And(t8, dst, Operand(kPointerSize - 1));
@@ -174,7 +174,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     li(value, Operand(base::bit_cast<int32_t>(kZapValue + 4)));
     li(dst, Operand(base::bit_cast<int32_t>(kZapValue + 8)));
   }
@@ -281,7 +281,7 @@ void MacroAssembler::RecordWrite(Register object, Register address,
   DCHECK(!AreAliased(object, address, value, t8));
   DCHECK(!AreAliased(object, address, value, t9));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     DCHECK(!AreAliased(object, value, scratch));
@@ -291,8 +291,8 @@ void MacroAssembler::RecordWrite(Register object, Register address,
   }
 
   if ((remembered_set_action == RememberedSetAction::kOmit &&
-       !FLAG_incremental_marking) ||
-      FLAG_disable_write_barriers) {
+       !v8_flags.incremental_marking) ||
+      v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -330,7 +330,7 @@ void MacroAssembler::RecordWrite(Register object, Register address,
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     li(address, Operand(base::bit_cast<int32_t>(kZapValue + 12)));
     li(value, Operand(base::bit_cast<int32_t>(kZapValue + 16)));
     li(slot_address, Operand(base::bit_cast<int32_t>(kZapValue + 20)));
@@ -4711,7 +4711,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     li(scratch2, ExternalReference::Create(counter));
     lw(scratch1, MemOperand(scratch2));
@@ -4724,7 +4724,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     li(scratch2, ExternalReference::Create(counter));
     lw(scratch1, MemOperand(scratch2));
@@ -4741,7 +4741,7 @@ void TurboAssembler::DebugBreak() { stop(); }
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason, Register rs,
                             Operand rt) {
-  if (FLAG_debug_code) Check(cc, reason, rs, rt);
+  if (v8_flags.debug_code) Check(cc, reason, rs, rt);
 }
 
 void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
@@ -4756,7 +4756,7 @@ void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -4879,7 +4879,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Set up new frame pointer.
   addiu(fp, sp, ExitFrameConstants::kFixedFrameSizeFromFp);
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     sw(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
 
@@ -4989,12 +4989,12 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_MIPS
 }
 
 void MacroAssembler::AssertStackIsAligned() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     const int frame_alignment = ActivationFrameAlignment();
     const int frame_alignment_mask = frame_alignment - 1;
@@ -5032,7 +5032,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label,
 }
 
 void MacroAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -5043,7 +5043,7 @@ void MacroAssembler::AssertNotSmi(Register object) {
 }
 
 void MacroAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -5054,7 +5054,7 @@ void MacroAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5070,7 +5070,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5087,7 +5087,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5105,7 +5105,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5119,7 +5119,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
   static_assert(kSmiTag == 0);
@@ -5147,7 +5147,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
@@ -5448,7 +5448,7 @@ void TurboAssembler::CallCFunctionHelper(Register function_base,
   // PrepareCallCFunction. The C function must be called via t9, for mips ABI.
 
 #if V8_HOST_ARCH_MIPS
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kPointerSize) {
diff --git a/src/codegen/mips/macro-assembler-mips.h b/src/codegen/mips/macro-assembler-mips.h
index 024ab0ffddc..df6fc491971 100644
--- a/src/codegen/mips/macro-assembler-mips.h
+++ b/src/codegen/mips/macro-assembler-mips.h
@@ -1094,14 +1094,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/mips64/assembler-mips64-inl.h b/src/codegen/mips64/assembler-mips64-inl.h
index 2ef960d7803..7ae17d08476 100644
--- a/src/codegen/mips64/assembler-mips64-inl.h
+++ b/src/codegen/mips64/assembler-mips64-inl.h
@@ -167,7 +167,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/mips64/assembler-mips64.cc b/src/codegen/mips64/assembler-mips64.cc
index 4a4bf415aa2..3ddb972a4c7 100644
--- a/src/codegen/mips64/assembler-mips64.cc
+++ b/src/codegen/mips64/assembler-mips64.cc
@@ -260,13 +260,13 @@ Assembler::Assembler(const AssemblerOptions& options,
   trampoline_pool_blocked_nesting_ = 0;
   // We leave space (16 * kTrampolineSlotsSize)
   // for BlockTrampolinePoolScope buffer.
-  next_buffer_check_ = FLAG_force_long_branches
+  next_buffer_check_ = v8_flags.force_long_branches
                            ? kMaxInt
                            : kMaxBranchOffset - kTrampolineSlotsSize * 16;
   internal_trampoline_exception_ = false;
   last_bound_pos_ = 0;
 
-  trampoline_emitted_ = FLAG_force_long_branches;
+  trampoline_emitted_ = v8_flags.force_long_branches;
   unbound_labels_count_ = 0;
   block_buffer_growth_ = false;
 }
diff --git a/src/codegen/mips64/macro-assembler-mips64.cc b/src/codegen/mips64/macro-assembler-mips64.cc
index 3d22a3daf59..d95566a8a7b 100644
--- a/src/codegen/mips64/macro-assembler-mips64.cc
+++ b/src/codegen/mips64/macro-assembler-mips64.cc
@@ -155,7 +155,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kPointerSize));
 
   Daddu(dst, object, Operand(offset - kHeapObjectTag));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     BlockTrampolinePoolScope block_trampoline_pool(this);
     Label ok;
     And(t8, dst, Operand(kPointerSize - 1));
@@ -170,7 +170,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     li(value, Operand(base::bit_cast<int64_t>(kZapValue + 4)));
     li(dst, Operand(base::bit_cast<int64_t>(kZapValue + 8)));
   }
@@ -262,7 +262,7 @@ void MacroAssembler::RecordWrite(Register object, Register address,
   DCHECK(!AreAliased(object, address, value, t8));
   DCHECK(!AreAliased(object, address, value, t9));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     DCHECK(!AreAliased(object, value, scratch));
@@ -271,7 +271,7 @@ void MacroAssembler::RecordWrite(Register object, Register address,
            Operand(value));
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -310,7 +310,7 @@ void MacroAssembler::RecordWrite(Register object, Register address,
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     li(address, Operand(base::bit_cast<int64_t>(kZapValue + 12)));
     li(value, Operand(base::bit_cast<int64_t>(kZapValue + 16)));
     li(slot_address, Operand(base::bit_cast<int64_t>(kZapValue + 20)));
@@ -5248,7 +5248,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
@@ -5264,7 +5264,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
@@ -5294,7 +5294,7 @@ void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -5325,7 +5325,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, t9);
       Call(t9);
     } else {
@@ -5422,7 +5422,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Set up new frame pointer.
   daddiu(fp, sp, ExitFrameConstants::kFixedFrameSizeFromFp);
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Sd(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
 
@@ -5496,7 +5496,7 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
      ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   Ld(cp, MemOperand(t8));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temp(this);
     Register scratch = temp.Acquire();
     li(scratch, Operand(Context::kInvalidContext));
@@ -5535,7 +5535,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_MIPS
 }
 
@@ -5571,11 +5571,11 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label,
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason, Register rs,
                             Operand rt) {
-  if (FLAG_debug_code) Check(cc, reason, rs, rt);
+  if (v8_flags.debug_code) Check(cc, reason, rs, rt);
 }
 
 void TurboAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -5586,7 +5586,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 }
 
 void TurboAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     UseScratchRegisterScope temps(this);
@@ -5597,7 +5597,7 @@ void TurboAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertStackIsAligned() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     const int frame_alignment = ActivationFrameAlignment();
     const int frame_alignment_mask = frame_alignment - 1;
@@ -5619,7 +5619,7 @@ void MacroAssembler::AssertStackIsAligned() {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5635,7 +5635,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5652,7 +5652,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5670,7 +5670,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5684,7 +5684,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
   static_assert(kSmiTag == 0);
@@ -5700,7 +5700,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
@@ -6001,7 +6001,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
   // PrepareCallCFunction. The C function must be called via t9, for mips ABI.
 
 #if V8_HOST_ARCH_MIPS || V8_HOST_ARCH_MIPS64
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kPointerSize) {
@@ -6248,7 +6248,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     GetObjectType(object, scratch, scratch);
     Assert(eq, AbortReason::kExpectedFeedbackVector, scratch,
            Operand(FEEDBACK_VECTOR_TYPE));
diff --git a/src/codegen/mips64/macro-assembler-mips64.h b/src/codegen/mips64/macro-assembler-mips64.h
index bc631aa26ce..5eb890b84f0 100644
--- a/src/codegen/mips64/macro-assembler-mips64.h
+++ b/src/codegen/mips64/macro-assembler-mips64.h
@@ -518,7 +518,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   // On MIPS64, we should sign-extend 32-bit values.
   void SmiToInt32(Register smi) {
-    if (FLAG_enable_slow_asserts) {
+    if (v8_flags.enable_slow_asserts) {
       AssertSmi(smi);
     }
     DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -1176,14 +1176,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/optimized-compilation-info.cc b/src/codegen/optimized-compilation-info.cc
index 1a3d7c7492c..603de092d2a 100644
--- a/src/codegen/optimized-compilation-info.cc
+++ b/src/codegen/optimized-compilation-info.cc
@@ -42,7 +42,7 @@ OptimizedCompilationInfo::OptimizedCompilationInfo(
     set_source_positions();
   }
 
-  SetTracingFlags(shared->PassesFilter(FLAG_trace_turbo_filter));
+  SetTracingFlags(shared->PassesFilter(v8_flags.trace_turbo_filter));
   ConfigureFlags();
 
   if (isolate->node_observer()) {
@@ -57,29 +57,29 @@ OptimizedCompilationInfo::OptimizedCompilationInfo(
       optimization_id_(kNoOptimizationId),
       debug_name_(debug_name) {
   SetTracingFlags(
-      PassesFilter(debug_name, base::CStrVector(FLAG_trace_turbo_filter)));
+      PassesFilter(debug_name, base::CStrVector(v8_flags.trace_turbo_filter)));
   ConfigureFlags();
 }
 
 void OptimizedCompilationInfo::ConfigureFlags() {
-  if (FLAG_turbo_inline_js_wasm_calls) set_inline_js_wasm_calls();
+  if (v8_flags.turbo_inline_js_wasm_calls) set_inline_js_wasm_calls();
 
   switch (code_kind_) {
     case CodeKind::TURBOFAN:
       set_called_with_code_start_register();
       set_switch_jump_table();
-      if (FLAG_analyze_environment_liveness) {
+      if (v8_flags.analyze_environment_liveness) {
         set_analyze_environment_liveness();
       }
-      if (FLAG_turbo_splitting) set_splitting();
+      if (v8_flags.turbo_splitting) set_splitting();
       break;
     case CodeKind::BYTECODE_HANDLER:
       set_called_with_code_start_register();
-      if (FLAG_turbo_splitting) set_splitting();
+      if (v8_flags.turbo_splitting) set_splitting();
       break;
     case CodeKind::BUILTIN:
     case CodeKind::FOR_TESTING:
-      if (FLAG_turbo_splitting) set_splitting();
+      if (v8_flags.turbo_splitting) set_splitting();
 #if ENABLE_GDB_JIT_INTERFACE && DEBUG
       set_source_positions();
 #endif  // ENABLE_GDB_JIT_INTERFACE && DEBUG
@@ -224,11 +224,11 @@ int OptimizedCompilationInfo::AddInlinedFunction(
 
 void OptimizedCompilationInfo::SetTracingFlags(bool passes_filter) {
   if (!passes_filter) return;
-  if (FLAG_trace_turbo) set_trace_turbo_json();
-  if (FLAG_trace_turbo_graph) set_trace_turbo_graph();
-  if (FLAG_trace_turbo_scheduled) set_trace_turbo_scheduled();
-  if (FLAG_trace_turbo_alloc) set_trace_turbo_allocation();
-  if (FLAG_trace_heap_broker) set_trace_heap_broker();
+  if (v8_flags.trace_turbo) set_trace_turbo_json();
+  if (v8_flags.trace_turbo_graph) set_trace_turbo_graph();
+  if (v8_flags.trace_turbo_scheduled) set_trace_turbo_scheduled();
+  if (v8_flags.trace_turbo_alloc) set_trace_turbo_allocation();
+  if (v8_flags.trace_heap_broker) set_trace_heap_broker();
 }
 
 OptimizedCompilationInfo::InlinedFunctionHolder::InlinedFunctionHolder(
diff --git a/src/codegen/pending-optimization-table.cc b/src/codegen/pending-optimization-table.cc
index 5e88b1e4567..3fa70d104d4 100644
--- a/src/codegen/pending-optimization-table.cc
+++ b/src/codegen/pending-optimization-table.cc
@@ -24,7 +24,7 @@ using FunctionStatusFlags = base::Flags<FunctionStatus>;
 void PendingOptimizationTable::PreparedForOptimization(
     Isolate* isolate, Handle<JSFunction> function,
     bool allow_heuristic_optimization) {
-  DCHECK(FLAG_testing_d8_test_runner);
+  DCHECK(v8_flags.testing_d8_test_runner);
 
   FunctionStatusFlags status = FunctionStatus::kPrepareForOptimize;
   if (allow_heuristic_optimization) {
@@ -52,7 +52,7 @@ void PendingOptimizationTable::PreparedForOptimization(
 
 bool PendingOptimizationTable::IsHeuristicOptimizationAllowed(
     Isolate* isolate, JSFunction function) {
-  DCHECK(FLAG_testing_d8_test_runner);
+  DCHECK(v8_flags.testing_d8_test_runner);
 
   Handle<Object> table =
       handle(isolate->heap()->pending_optimize_for_test_bytecode(), isolate);
@@ -73,7 +73,7 @@ bool PendingOptimizationTable::IsHeuristicOptimizationAllowed(
 
 void PendingOptimizationTable::MarkedForOptimization(
     Isolate* isolate, Handle<JSFunction> function) {
-  DCHECK(FLAG_testing_d8_test_runner);
+  DCHECK(v8_flags.testing_d8_test_runner);
 
   Handle<Object> table =
       handle(isolate->heap()->pending_optimize_for_test_bytecode(), isolate);
@@ -106,7 +106,7 @@ void PendingOptimizationTable::MarkedForOptimization(
 
 void PendingOptimizationTable::FunctionWasOptimized(
     Isolate* isolate, Handle<JSFunction> function) {
-  DCHECK(FLAG_testing_d8_test_runner);
+  DCHECK(v8_flags.testing_d8_test_runner);
 
   if (isolate->heap()->pending_optimize_for_test_bytecode().IsUndefined()) {
     return;
diff --git a/src/codegen/ppc/assembler-ppc-inl.h b/src/codegen/ppc/assembler-ppc-inl.h
index bde1b23ce8a..8997bab36ed 100644
--- a/src/codegen/ppc/assembler-ppc-inl.h
+++ b/src/codegen/ppc/assembler-ppc-inl.h
@@ -87,7 +87,7 @@ Address RelocInfo::target_address() {
 Address RelocInfo::target_address_address() {
   DCHECK(HasTargetAddressAddress());
 
-  if (FLAG_enable_embedded_constant_pool &&
+  if (v8_flags.enable_embedded_constant_pool &&
       Assembler::IsConstantPoolLoadStart(pc_)) {
     // We return the PC for embedded constant pool since this function is used
     // by the serializer and expects the address to reside within the code
@@ -108,7 +108,7 @@ Address RelocInfo::target_address_address() {
 }
 
 Address RelocInfo::constant_pool_entry_address() {
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     DCHECK(constant_pool_);
     ConstantPoolEntry::Access access;
     if (Assembler::IsConstantPoolLoadStart(pc_, &access))
@@ -195,7 +195,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
@@ -269,7 +269,7 @@ void Assembler::UntrackBranch() {
 
 // Fetch the 32bit value from the FIXED_SEQUENCE lis/ori
 Address Assembler::target_address_at(Address pc, Address constant_pool) {
-  if (FLAG_enable_embedded_constant_pool && constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool && constant_pool) {
     ConstantPoolEntry::Access access;
     if (IsConstantPoolLoadStart(pc, &access))
       return Memory<Address>(target_constant_pool_address_at(
@@ -444,7 +444,7 @@ void Assembler::deserialization_set_target_internal_reference_at(
 void Assembler::set_target_address_at(Address pc, Address constant_pool,
                                       Address target,
                                       ICacheFlushMode icache_flush_mode) {
-  if (FLAG_enable_embedded_constant_pool && constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool && constant_pool) {
     ConstantPoolEntry::Access access;
     if (IsConstantPoolLoadStart(pc, &access)) {
       Memory<Address>(target_constant_pool_address_at(
diff --git a/src/codegen/ppc/assembler-ppc.cc b/src/codegen/ppc/assembler-ppc.cc
index 2e8fb9f7e03..796bd09d554 100644
--- a/src/codegen/ppc/assembler-ppc.cc
+++ b/src/codegen/ppc/assembler-ppc.cc
@@ -167,7 +167,8 @@ bool RelocInfo::IsCodedSpecially() {
 }
 
 bool RelocInfo::IsInConstantPool() {
-  if (FLAG_enable_embedded_constant_pool && constant_pool_ != kNullAddress) {
+  if (v8_flags.enable_embedded_constant_pool &&
+      constant_pool_ != kNullAddress) {
     return Assembler::IsConstantPoolLoadStart(pc_);
   }
   return false;
@@ -237,7 +238,7 @@ Assembler::Assembler(const AssemblerOptions& options,
   internal_trampoline_exception_ = false;
   last_bound_pos_ = 0;
   optimizable_cmpi_pos_ = -1;
-  trampoline_emitted_ = FLAG_force_long_branches;
+  trampoline_emitted_ = v8_flags.force_long_branches;
   tracked_branch_count_ = 0;
   relocations_.reserve(128);
 }
@@ -1309,7 +1310,8 @@ int Assembler::instructions_required_for_mov(Register dst,
 
 bool Assembler::use_constant_pool_for_mov(Register dst, const Operand& src,
                                           bool canOptimize) const {
-  if (!FLAG_enable_embedded_constant_pool || !is_constant_pool_available()) {
+  if (!v8_flags.enable_embedded_constant_pool ||
+      !is_constant_pool_available()) {
     // If there is no constant pool available, we must use a mov
     // immediate sequence.
     return false;
diff --git a/src/codegen/ppc/assembler-ppc.h b/src/codegen/ppc/assembler-ppc.h
index 837a415997a..6563287d0a8 100644
--- a/src/codegen/ppc/assembler-ppc.h
+++ b/src/codegen/ppc/assembler-ppc.h
@@ -310,7 +310,7 @@ class Assembler : public AssemblerBase {
   static constexpr int kTaggedLoadInstructions = 1;
 #endif
   static constexpr int kMovInstructions =
-      FLAG_enable_embedded_constant_pool.value()
+      v8_flags.enable_embedded_constant_pool.value()
           ? kMovInstructionsConstantPool
           : kMovInstructionsNoConstantPool;
 
diff --git a/src/codegen/ppc/macro-assembler-ppc.cc b/src/codegen/ppc/macro-assembler-ppc.cc
index e432ba3d666..2e9682b22c4 100644
--- a/src/codegen/ppc/macro-assembler-ppc.cc
+++ b/src/codegen/ppc/macro-assembler-ppc.cc
@@ -742,7 +742,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kTaggedSize));
 
   AddS64(slot_address, object, Operand(offset - kHeapObjectTag), r0);
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label ok;
     andi(r0, slot_address, Operand(kTaggedSize - 1));
     beq(&ok, cr0);
@@ -756,7 +756,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 4)));
     mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 8)));
   }
@@ -846,13 +846,13 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
                                  Register value, LinkRegisterStatus lr_status,
                                  SaveFPRegsMode fp_mode, SmiCheck smi_check) {
   DCHECK(!AreAliased(object, value, slot_address));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     LoadTaggedPointerField(r0, MemOperand(slot_address));
     CmpS64(r0, value);
     Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -883,13 +883,13 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
     mtlr(r0);
   }
 
-  if (FLAG_debug_code) mov(slot_address, Operand(kZapValue));
+  if (v8_flags.debug_code) mov(slot_address, Operand(kZapValue));
 
   bind(&done);
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 12)));
     mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 16)));
   }
@@ -898,7 +898,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
 void TurboAssembler::PushCommonFrame(Register marker_reg) {
   int fp_delta = 0;
   mflr(r0);
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     if (marker_reg.is_valid()) {
       Push(r0, fp, kConstantPoolRegister, marker_reg);
       fp_delta = 2;
@@ -921,7 +921,7 @@ void TurboAssembler::PushCommonFrame(Register marker_reg) {
 void TurboAssembler::PushStandardFrame(Register function_reg) {
   int fp_delta = 0;
   mflr(r0);
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     if (function_reg.is_valid()) {
       Push(r0, fp, kConstantPoolRegister, cp, function_reg);
       fp_delta = 3;
@@ -943,7 +943,7 @@ void TurboAssembler::PushStandardFrame(Register function_reg) {
 }
 
 void TurboAssembler::RestoreFrameStateForTailCall() {
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     LoadU64(kConstantPoolRegister,
             MemOperand(fp, StandardFrameConstants::kConstantPoolOffset));
     set_constant_pool_available(false);
@@ -1230,7 +1230,7 @@ void TurboAssembler::StubPrologue(StackFrame::Type type) {
     mov(r11, Operand(StackFrame::TypeToMarker(type)));
     PushCommonFrame(r11);
   }
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     LoadConstantPoolPointerRegister();
     set_constant_pool_available(true);
   }
@@ -1238,7 +1238,7 @@ void TurboAssembler::StubPrologue(StackFrame::Type type) {
 
 void TurboAssembler::Prologue() {
   PushStandardFrame(r4);
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     // base contains prologue address
     LoadConstantPoolPointerRegister();
     set_constant_pool_available(true);
@@ -1288,7 +1288,8 @@ void TurboAssembler::DropArgumentsAndPushNewReceiver(Register argc,
 
 void TurboAssembler::EnterFrame(StackFrame::Type type,
                                 bool load_constant_pool_pointer_reg) {
-  if (FLAG_enable_embedded_constant_pool && load_constant_pool_pointer_reg) {
+  if (v8_flags.enable_embedded_constant_pool &&
+      load_constant_pool_pointer_reg) {
     // Push type explicitly so we can leverage the constant pool.
     // This path cannot rely on ip containing code entry.
     PushCommonFrame();
@@ -1321,7 +1322,7 @@ int TurboAssembler::LeaveFrame(StackFrame::Type type, int stack_adjustment) {
   int frame_ends;
   LoadU64(r0, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
   LoadU64(ip, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     LoadU64(kConstantPoolRegister,
             MemOperand(fp, StandardFrameConstants::kConstantPoolOffset));
   }
@@ -1368,11 +1369,11 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Reserve room for saved entry sp.
   subi(sp, fp, Operand(ExitFrameConstants::kFixedFrameSizeFromFp));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     li(r8, Operand::Zero());
     StoreU64(r8, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
-  if (FLAG_enable_embedded_constant_pool) {
+  if (v8_flags.enable_embedded_constant_pool) {
     StoreU64(kConstantPoolRegister,
              MemOperand(fp, ExitFrameConstants::kConstantPoolOffset));
   }
@@ -1427,7 +1428,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif
 }
 
@@ -2075,7 +2076,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CompareObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);
     Assert(eq, AbortReason::kExpectedFeedbackVector);
   }
@@ -2234,7 +2235,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
     // field.
@@ -2249,7 +2250,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t dummy_stats_counter_
     // field.
@@ -2271,7 +2272,7 @@ void TurboAssembler::Check(Condition cond, AbortReason reason, CRegister cr) {
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -2302,7 +2303,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, ip);
       Call(ip);
     } else {
@@ -2328,11 +2329,11 @@ void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void TurboAssembler::Assert(Condition cond, AbortReason reason, CRegister cr) {
-  if (FLAG_debug_code) Check(cond, reason, cr);
+  if (v8_flags.debug_code) Check(cond, reason, cr);
 }
 
 void TurboAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmi, cr0);
@@ -2340,7 +2341,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 }
 
 void TurboAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(eq, AbortReason::kOperandIsNotASmi, cr0);
@@ -2348,7 +2349,7 @@ void TurboAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor, cr0);
@@ -2362,7 +2363,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
@@ -2376,7 +2377,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   TestIfSmi(object, r0);
@@ -2390,7 +2391,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object, r0);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, cr0);
@@ -2402,7 +2403,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   TestIfSmi(object, r0);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, cr0);
 
@@ -2422,7 +2423,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label done_checking;
     AssertNotSmi(object);
     CompareRoot(object, RootIndex::kUndefinedValue);
@@ -2652,7 +2653,7 @@ void TurboAssembler::LoadSmiLiteral(Register dst, Smi smi) {
 
 void TurboAssembler::LoadDoubleLiteral(DoubleRegister result,
                                        base::Double value, Register scratch) {
-  if (FLAG_enable_embedded_constant_pool && is_constant_pool_available() &&
+  if (v8_flags.enable_embedded_constant_pool && is_constant_pool_available() &&
       !(scratch == r0 && ConstantPoolAccessIsInOverflow())) {
     ConstantPoolEntry::Access access = ConstantPoolAddEntry(value);
     if (access == ConstantPoolEntry::OVERFLOWED) {
diff --git a/src/codegen/ppc/macro-assembler-ppc.h b/src/codegen/ppc/macro-assembler-ppc.h
index cbf7f696fe0..08c44ed459e 100644
--- a/src/codegen/ppc/macro-assembler-ppc.h
+++ b/src/codegen/ppc/macro-assembler-ppc.h
@@ -804,7 +804,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
     }
   }
   void SmiToInt32(Register smi) {
-    if (FLAG_enable_slow_asserts) {
+    if (v8_flags.enable_slow_asserts) {
       AssertSmi(smi);
     }
     DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -1365,14 +1365,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/reloc-info.cc b/src/codegen/reloc-info.cc
index 8625a7ef928..70b85ef0775 100644
--- a/src/codegen/reloc-info.cc
+++ b/src/codegen/reloc-info.cc
@@ -350,7 +350,7 @@ void RelocInfo::set_target_address(Address target,
   Assembler::set_target_address_at(pc_, constant_pool_, target,
                                    icache_flush_mode);
   if (!host().is_null() && IsCodeTargetMode(rmode_) &&
-      !FLAG_disable_write_barriers) {
+      !v8_flags.disable_write_barriers) {
     Code target_code = Code::GetCodeFromTargetAddress(target);
     WriteBarrierForCode(host(), this, target_code, write_barrier_mode);
   }
diff --git a/src/codegen/riscv/assembler-riscv-inl.h b/src/codegen/riscv/assembler-riscv-inl.h
index a81d2566895..ba5e3459069 100644
--- a/src/codegen/riscv/assembler-riscv-inl.h
+++ b/src/codegen/riscv/assembler-riscv-inl.h
@@ -206,7 +206,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
                                      icache_flush_mode);
   }
   if (write_barrier_mode == UPDATE_WRITE_BARRIER && !host().is_null() &&
-      !FLAG_disable_write_barriers) {
+      !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target);
   }
 }
diff --git a/src/codegen/riscv/assembler-riscv.cc b/src/codegen/riscv/assembler-riscv.cc
index bd729bf7766..1b56afe7b97 100644
--- a/src/codegen/riscv/assembler-riscv.cc
+++ b/src/codegen/riscv/assembler-riscv.cc
@@ -205,13 +205,13 @@ Assembler::Assembler(const AssemblerOptions& options,
   trampoline_pool_blocked_nesting_ = 0;
   // We leave space (16 * kTrampolineSlotsSize)
   // for BlockTrampolinePoolScope buffer.
-  next_buffer_check_ = FLAG_force_long_branches
+  next_buffer_check_ = v8_flags.force_long_branches
                            ? kMaxInt
                            : kMaxBranchOffset - kTrampolineSlotsSize * 16;
   internal_trampoline_exception_ = false;
   last_bound_pos_ = 0;
 
-  trampoline_emitted_ = FLAG_force_long_branches;
+  trampoline_emitted_ = v8_flags.force_long_branches;
   unbound_labels_count_ = 0;
   block_buffer_growth_ = false;
 }
@@ -483,7 +483,7 @@ bool Assembler::MustUseReg(RelocInfo::Mode rmode) {
 }
 
 void Assembler::disassembleInstr(Instr instr) {
-  if (!FLAG_riscv_debug) return;
+  if (!v8_flags.riscv_debug) return;
   disasm::NameConverter converter;
   disasm::Disassembler disasm(converter);
   base::EmbeddedVector<char, 128> disasm_buffer;
@@ -758,7 +758,7 @@ uintptr_t Assembler::jump_address(Label* L) {
     }
   }
   uintptr_t imm = reinterpret_cast<uintptr_t>(buffer_start_) + target_pos;
-  if (FLAG_riscv_c_extension)
+  if (v8_flags.riscv_c_extension)
     DCHECK_EQ(imm & 1, 0);
   else
     DCHECK_EQ(imm & 3, 0);
@@ -789,7 +789,7 @@ int32_t Assembler::branch_long_offset(Label* L) {
     }
   }
   intptr_t offset = target_pos - pc_offset();
-  if (FLAG_riscv_c_extension)
+  if (v8_flags.riscv_c_extension)
     DCHECK_EQ(offset & 1, 0);
   else
     DCHECK_EQ(offset & 3, 0);
@@ -863,14 +863,14 @@ void Assembler::label_at_put(Label* L, int at_offset) {
 // Definitions for using compressed vs non compressed
 
 void Assembler::NOP() {
-  if (FLAG_riscv_c_extension)
+  if (v8_flags.riscv_c_extension)
     c_nop();
   else
     nop();
 }
 
 void Assembler::EBREAK() {
-  if (FLAG_riscv_c_extension)
+  if (v8_flags.riscv_c_extension)
     c_ebreak();
   else
     ebreak();
diff --git a/src/codegen/riscv/assembler-riscv.h b/src/codegen/riscv/assembler-riscv.h
index 452c0b16eaf..f383cbf9212 100644
--- a/src/codegen/riscv/assembler-riscv.h
+++ b/src/codegen/riscv/assembler-riscv.h
@@ -63,9 +63,9 @@
 namespace v8 {
 namespace internal {
 
-#define DEBUG_PRINTF(...) \
-  if (FLAG_riscv_debug) { \
-    printf(__VA_ARGS__);  \
+#define DEBUG_PRINTF(...)     \
+  if (v8_flags.riscv_debug) { \
+    printf(__VA_ARGS__);      \
   }
 
 class SafepointTableBuilder;
diff --git a/src/codegen/riscv/base-assembler-riscv.h b/src/codegen/riscv/base-assembler-riscv.h
index 460a86c0afb..8bdfd4ecd1a 100644
--- a/src/codegen/riscv/base-assembler-riscv.h
+++ b/src/codegen/riscv/base-assembler-riscv.h
@@ -52,9 +52,9 @@
 namespace v8 {
 namespace internal {
 
-#define DEBUG_PRINTF(...) \
-  if (FLAG_riscv_debug) { \
-    printf(__VA_ARGS__);  \
+#define DEBUG_PRINTF(...)     \
+  if (v8_flags.riscv_debug) { \
+    printf(__VA_ARGS__);      \
   }
 
 class SafepointTableBuilder;
diff --git a/src/codegen/riscv/base-constants-riscv.cc b/src/codegen/riscv/base-constants-riscv.cc
index 90d2c561584..76ff68efca5 100644
--- a/src/codegen/riscv/base-constants-riscv.cc
+++ b/src/codegen/riscv/base-constants-riscv.cc
@@ -238,7 +238,7 @@ InstructionBase::Type InstructionBase::InstructionType() const {
     return kUnsupported;
   }
   // RV64C Instruction
-  if (FLAG_riscv_c_extension && IsShortInstruction()) {
+  if (v8_flags.riscv_c_extension && IsShortInstruction()) {
     switch (InstructionBits() & kRvcOpcodeMask) {
       case RO_C_ADDI4SPN:
         return kCIWType;
diff --git a/src/codegen/riscv/base-constants-riscv.h b/src/codegen/riscv/base-constants-riscv.h
index e64f36416e9..077ae9110c3 100644
--- a/src/codegen/riscv/base-constants-riscv.h
+++ b/src/codegen/riscv/base-constants-riscv.h
@@ -657,14 +657,14 @@ class InstructionBase {
   bool IsShortInstruction() const;
 
   inline uint8_t InstructionSize() const {
-    return (FLAG_riscv_c_extension && this->IsShortInstruction())
+    return (v8_flags.riscv_c_extension && this->IsShortInstruction())
                ? kShortInstrSize
                : kInstrSize;
   }
 
   // Get the raw instruction bits.
   inline Instr InstructionBits() const {
-    if (FLAG_riscv_c_extension && this->IsShortInstruction()) {
+    if (v8_flags.riscv_c_extension && this->IsShortInstruction()) {
       return 0x0000FFFF & (*reinterpret_cast<const ShortInstr*>(this));
     }
     return *reinterpret_cast<const Instr*>(this);
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 7976b05f8d2..9c1b758ecc5 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -137,7 +137,7 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 }  // namespace
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     GetObjectType(object, scratch, scratch);
     Assert(eq, AbortReason::kExpectedFeedbackVector, scratch,
            Operand(FEEDBACK_VECTOR_TYPE));
@@ -299,7 +299,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   // of the object, so offset must be a multiple of kTaggedSize.
   DCHECK(IsAligned(offset, kTaggedSize));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label ok;
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
@@ -397,7 +397,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
                                  SaveFPRegsMode fp_mode, SmiCheck smi_check) {
   DCHECK(!AreAliased(object, value));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(this);
     Register temp = temps.Acquire();
     DCHECK(!AreAliased(object, value, temp));
@@ -407,7 +407,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
            Operand(value));
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
@@ -450,7 +450,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
   if (ra_status == kRAHasNotBeenSaved) {
     pop(ra);
   }
-  if (FLAG_debug_code) li(slot_address, Operand(kZapValue));
+  if (v8_flags.debug_code) li(slot_address, Operand(kZapValue));
 
   bind(&done);
 }
@@ -460,7 +460,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
 #if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_addw(rd, rt.rm());
@@ -468,7 +468,7 @@ void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
       addw(rd, rs, rt.rm());
     }
   } else {
-    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+    if (v8_flags.riscv_c_extension && is_int6(rt.immediate()) &&
         (rd.code() == rs.code()) && (rd != zero_reg) &&
         !MustUseReg(rt.rmode())) {
       c_addiw(rd, static_cast<int8_t>(rt.immediate()));
@@ -490,7 +490,7 @@ void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_subw(rd, rt.rm());
@@ -499,7 +499,7 @@ void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
     }
   } else {
     DCHECK(is_int32(rt.immediate()));
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         (rd != zero_reg) && is_int6(-rt.immediate()) &&
         !MustUseReg(rt.rmode())) {
       c_addiw(
@@ -540,21 +540,21 @@ void TurboAssembler::SubWord(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Sub64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_sub(rd, rt.rm());
     } else {
       sub(rd, rs, rt.rm());
     }
-  } else if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+  } else if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
              (rd != zero_reg) && is_int6(-rt.immediate()) &&
              (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
     c_addi(rd,
            static_cast<int8_t>(
                -rt.immediate()));  // No c_subi instr, use c_addi(x, y, -imm).
 
-  } else if (FLAG_riscv_c_extension && is_int10(-rt.immediate()) &&
+  } else if (v8_flags.riscv_c_extension && is_int10(-rt.immediate()) &&
              (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
              (rd.code() == rs.code()) && (rd == sp) &&
              !MustUseReg(rt.rmode())) {
@@ -589,25 +589,26 @@ void TurboAssembler::Sub64(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Add64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         (rt.rm() != zero_reg) && (rs != zero_reg)) {
       c_add(rd, rt.rm());
     } else {
       add(rd, rs, rt.rm());
     }
   } else {
-    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+    if (v8_flags.riscv_c_extension && is_int6(rt.immediate()) &&
         (rd.code() == rs.code()) && (rd != zero_reg) && (rt.immediate() != 0) &&
         !MustUseReg(rt.rmode())) {
       c_addi(rd, static_cast<int8_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && is_int10(rt.immediate()) &&
+    } else if (v8_flags.riscv_c_extension && is_int10(rt.immediate()) &&
                (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
                (rd.code() == rs.code()) && (rd == sp) &&
                !MustUseReg(rt.rmode())) {
       c_addi16sp(static_cast<int16_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && ((rd.code() & 0b11000) == 0b01000) &&
-               (rs == sp) && is_uint10(rt.immediate()) &&
-               (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
+    } else if (v8_flags.riscv_c_extension &&
+               ((rd.code() & 0b11000) == 0b01000) && (rs == sp) &&
+               is_uint10(rt.immediate()) && (rt.immediate() != 0) &&
+               !MustUseReg(rt.rmode())) {
       c_addi4spn(rd, static_cast<uint16_t>(rt.immediate()));
     } else if (is_int12(rt.immediate()) && !MustUseReg(rt.rmode())) {
       addi(rd, rs, static_cast<int32_t>(rt.immediate()));
@@ -789,25 +790,26 @@ void TurboAssembler::AddWord(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Add32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         (rt.rm() != zero_reg) && (rs != zero_reg)) {
       c_add(rd, rt.rm());
     } else {
       add(rd, rs, rt.rm());
     }
   } else {
-    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+    if (v8_flags.riscv_c_extension && is_int6(rt.immediate()) &&
         (rd.code() == rs.code()) && (rd != zero_reg) && (rt.immediate() != 0) &&
         !MustUseReg(rt.rmode())) {
       c_addi(rd, static_cast<int8_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && is_int10(rt.immediate()) &&
+    } else if (v8_flags.riscv_c_extension && is_int10(rt.immediate()) &&
                (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
                (rd.code() == rs.code()) && (rd == sp) &&
                !MustUseReg(rt.rmode())) {
       c_addi16sp(static_cast<int16_t>(rt.immediate()));
-    } else if (FLAG_riscv_c_extension && ((rd.code() & 0b11000) == 0b01000) &&
-               (rs == sp) && is_uint10(rt.immediate()) &&
-               (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
+    } else if (v8_flags.riscv_c_extension &&
+               ((rd.code() & 0b11000) == 0b01000) && (rs == sp) &&
+               is_uint10(rt.immediate()) && (rt.immediate() != 0) &&
+               !MustUseReg(rt.rmode())) {
       c_addi4spn(rd, static_cast<uint16_t>(rt.immediate()));
     } else if (is_int12(rt.immediate()) && !MustUseReg(rt.rmode())) {
       addi(rd, rs, static_cast<int32_t>(rt.immediate()));
@@ -832,21 +834,21 @@ void TurboAssembler::SubWord(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Sub32(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_sub(rd, rt.rm());
     } else {
       sub(rd, rs, rt.rm());
     }
-  } else if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+  } else if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
              (rd != zero_reg) && is_int6(-rt.immediate()) &&
              (rt.immediate() != 0) && !MustUseReg(rt.rmode())) {
     c_addi(rd,
            static_cast<int8_t>(
                -rt.immediate()));  // No c_subi instr, use c_addi(x, y, -imm).
 
-  } else if (FLAG_riscv_c_extension && is_int10(-rt.immediate()) &&
+  } else if (v8_flags.riscv_c_extension && is_int10(-rt.immediate()) &&
              (rt.immediate() != 0) && ((rt.immediate() & 0xf) == 0) &&
              (rd.code() == rs.code()) && (rd == sp) &&
              !MustUseReg(rt.rmode())) {
@@ -973,7 +975,7 @@ void TurboAssembler::Divu(Register res, Register rs, const Operand& rt) {
 
 void TurboAssembler::And(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_and(rd, rt.rm());
@@ -981,7 +983,7 @@ void TurboAssembler::And(Register rd, Register rs, const Operand& rt) {
       and_(rd, rs, rt.rm());
     }
   } else {
-    if (FLAG_riscv_c_extension && is_int6(rt.immediate()) &&
+    if (v8_flags.riscv_c_extension && is_int6(rt.immediate()) &&
         !MustUseReg(rt.rmode()) && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000)) {
       c_andi(rd, static_cast<int8_t>(rt.immediate()));
@@ -999,7 +1001,7 @@ void TurboAssembler::And(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Or(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_or(rd, rt.rm());
@@ -1021,7 +1023,7 @@ void TurboAssembler::Or(Register rd, Register rs, const Operand& rt) {
 
 void TurboAssembler::Xor(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         ((rd.code() & 0b11000) == 0b01000) &&
         ((rt.rm().code() & 0b11000) == 0b01000)) {
       c_xor(rd, rt.rm());
@@ -1227,7 +1229,7 @@ void TurboAssembler::SraWord(Register rd, Register rs, const Operand& rt) {
 void TurboAssembler::Sra64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     sra(rd, rs, rt.rm());
-  } else if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+  } else if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
              ((rd.code() & 0b11000) == 0b01000) && is_int6(rt.immediate())) {
     uint8_t shamt = static_cast<uint8_t>(rt.immediate());
     c_srai(rd, shamt);
@@ -1244,7 +1246,7 @@ void TurboAssembler::SrlWord(Register rd, Register rs, const Operand& rt) {
 void TurboAssembler::Srl64(Register rd, Register rs, const Operand& rt) {
   if (rt.is_reg()) {
     srl(rd, rs, rt.rm());
-  } else if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+  } else if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
              ((rd.code() & 0b11000) == 0b01000) && is_int6(rt.immediate())) {
     uint8_t shamt = static_cast<uint8_t>(rt.immediate());
     c_srli(rd, shamt);
@@ -1263,7 +1265,7 @@ void TurboAssembler::Sll64(Register rd, Register rs, const Operand& rt) {
     sll(rd, rs, rt.rm());
   } else {
     uint8_t shamt = static_cast<uint8_t>(rt.immediate());
-    if (FLAG_riscv_c_extension && (rd.code() == rs.code()) &&
+    if (v8_flags.riscv_c_extension && (rd.code() == rs.code()) &&
         (rd != zero_reg) && (shamt != 0) && is_uint6(shamt)) {
       c_slli(rd, shamt);
     } else {
@@ -1384,7 +1386,7 @@ void TurboAssembler::Ror(Register rd, Register rs, const Operand& rt) {
 #endif
 
 void TurboAssembler::Li(Register rd, intptr_t imm) {
-  if (FLAG_riscv_c_extension && (rd != zero_reg) && is_int6(imm)) {
+  if (v8_flags.riscv_c_extension && (rd != zero_reg) && is_int6(imm)) {
     c_li(rd, imm);
   } else {
     RV_li(rd, imm);
@@ -1392,7 +1394,7 @@ void TurboAssembler::Li(Register rd, intptr_t imm) {
 }
 
 void TurboAssembler::Mv(Register rd, const Operand& rt) {
-  if (FLAG_riscv_c_extension && (rd != zero_reg) && (rt.rm() != zero_reg)) {
+  if (v8_flags.riscv_c_extension && (rd != zero_reg) && (rt.rm() != zero_reg)) {
     c_mv(rd, rt.rm());
   } else {
     mv(rd, rt.rm());
@@ -1893,11 +1895,11 @@ void TurboAssembler::Sh(Register rd, const MemOperand& rs) {
 
 void TurboAssembler::Lw(Register rd, const MemOperand& rs) {
   auto fn = [this](Register target, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint7(source.offset()) && ((source.offset() & 0x3) == 0)) {
       this->c_lw(target, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (target != zero_reg) &&
+    } else if (v8_flags.riscv_c_extension && (target != zero_reg) &&
                is_uint8(source.offset()) && (source.rm() == sp) &&
                ((source.offset() & 0x3) == 0)) {
       this->c_lwsp(target, source.offset());
@@ -1918,11 +1920,11 @@ void TurboAssembler::Lwu(Register rd, const MemOperand& rs) {
 #endif
 void TurboAssembler::Sw(Register rd, const MemOperand& rs) {
   auto fn = [this](Register value, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint7(source.offset()) && ((source.offset() & 0x3) == 0)) {
       this->c_sw(value, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (source.rm() == sp) &&
+    } else if (v8_flags.riscv_c_extension && (source.rm() == sp) &&
                is_uint8(source.offset()) && (((source.offset() & 0x3) == 0))) {
       this->c_swsp(value, source.offset());
     } else {
@@ -1935,11 +1937,11 @@ void TurboAssembler::Sw(Register rd, const MemOperand& rs) {
 #if V8_TARGET_ARCH_RISCV64
 void TurboAssembler::Ld(Register rd, const MemOperand& rs) {
   auto fn = [this](Register target, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint8(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_ld(target, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (target != zero_reg) &&
+    } else if (v8_flags.riscv_c_extension && (target != zero_reg) &&
                is_uint9(source.offset()) && (source.rm() == sp) &&
                ((source.offset() & 0x7) == 0)) {
       this->c_ldsp(target, source.offset());
@@ -1952,11 +1954,11 @@ void TurboAssembler::Ld(Register rd, const MemOperand& rs) {
 
 void TurboAssembler::Sd(Register rd, const MemOperand& rs) {
   auto fn = [this](Register value, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint8(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_sd(value, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (source.rm() == sp) &&
+    } else if (v8_flags.riscv_c_extension && (source.rm() == sp) &&
                is_uint9(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_sdsp(value, source.offset());
     } else {
@@ -1982,11 +1984,11 @@ void TurboAssembler::StoreFloat(FPURegister fs, const MemOperand& src) {
 
 void TurboAssembler::LoadDouble(FPURegister fd, const MemOperand& src) {
   auto fn = [this](FPURegister target, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((target.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint8(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_fld(target, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (source.rm() == sp) &&
+    } else if (v8_flags.riscv_c_extension && (source.rm() == sp) &&
                is_uint9(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_fldsp(target, source.offset());
     } else {
@@ -1998,11 +2000,11 @@ void TurboAssembler::LoadDouble(FPURegister fd, const MemOperand& src) {
 
 void TurboAssembler::StoreDouble(FPURegister fs, const MemOperand& src) {
   auto fn = [this](FPURegister value, const MemOperand& source) {
-    if (FLAG_riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
+    if (v8_flags.riscv_c_extension && ((value.code() & 0b11000) == 0b01000) &&
         ((source.rm().code() & 0b11000) == 0b01000) &&
         is_uint8(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_fsd(value, source.rm(), source.offset());
-    } else if (FLAG_riscv_c_extension && (source.rm() == sp) &&
+    } else if (v8_flags.riscv_c_extension && (source.rm() == sp) &&
                is_uint9(source.offset()) && ((source.offset() & 0x7) == 0)) {
       this->c_fsdsp(value, source.offset());
     } else {
@@ -2123,7 +2125,7 @@ void TurboAssembler::li(Register rd, Operand j, LiFlags mode) {
     UseScratchRegisterScope temps(this);
     int count = li_estimate(j.immediate(), temps.hasAvailable());
     int reverse_count = li_estimate(~j.immediate(), temps.hasAvailable());
-    if (FLAG_riscv_constant_pool && count >= 4 && reverse_count >= 4) {
+    if (v8_flags.riscv_constant_pool && count >= 4 && reverse_count >= 4) {
       // Ld/Lw a Address from a constant pool.
       RecordEntry((uintptr_t)j.immediate(), j.rmode());
       auipc(rd, 0);
@@ -4478,7 +4480,7 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
   //
   Assembler::BlockTrampolinePoolScope block_trampoline_pool(this);
   int kNumInstructionsToJump = 5;
-  if (FLAG_riscv_c_extension) kNumInstructionsToJump = 4;
+  if (v8_flags.riscv_c_extension) kNumInstructionsToJump = 4;
   Label find_ra;
   // Adjust the value in ra to point to the correct return location, one
   // instruction past the real call into C code (the jalr(t6)), and push it.
@@ -5318,7 +5320,7 @@ void MacroAssembler::JumpToOffHeapInstructionStream(Address entry) {
   // Ld a Address from a constant pool.
   // Record a value into constant pool.
   ASM_CODE_COMMENT(this);
-  if (!FLAG_riscv_constant_pool) {
+  if (!v8_flags.riscv_constant_pool) {
     li(kOffHeapTrampolineRegister, Operand(entry, RelocInfo::OFF_HEAP_TARGET));
   } else {
     RecordEntry(entry, RelocInfo::OFF_HEAP_TARGET);
@@ -5341,7 +5343,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t
@@ -5357,7 +5359,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     // This operation has to be exactly 32-bit wide in case the external
     // reference table redirects the counter to a uint32_t
@@ -5377,7 +5379,7 @@ void TurboAssembler::DebugBreak() { stop(); }
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason, Register rs,
                             Operand rt) {
-  if (FLAG_debug_code) Check(cc, reason, rs, rt);
+  if (v8_flags.debug_code) Check(cc, reason, rs, rt);
 }
 
 void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
@@ -5392,7 +5394,7 @@ void TurboAssembler::Check(Condition cc, AbortReason reason, Register rs,
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -5423,7 +5425,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, t6);
       Call(t6);
     } else {
@@ -5529,7 +5531,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Set up new frame pointer.
   addi(fp, sp, ExitFrameConstants::kFixedFrameSizeFromFp);
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     StoreWord(zero_reg, MemOperand(fp, ExitFrameConstants::kSPOffset));
   }
 
@@ -5612,7 +5614,7 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
      ExternalReference::Create(IsolateAddressId::kContextAddress, isolate()));
   LoadWord(cp, MemOperand(scratch));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     UseScratchRegisterScope temp(this);
     Register scratch2 = temp.Acquire();
     li(scratch2, Operand(Context::kInvalidContext));
@@ -5651,12 +5653,12 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif  // V8_HOST_ARCH_RISCV64
 }
 
 void MacroAssembler::AssertStackIsAligned() {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     const int frame_alignment = ActivationFrameAlignment();
     const int frame_alignment_mask = frame_alignment - 1;
@@ -5694,7 +5696,7 @@ void TurboAssembler::SmiUntag(Register dst, const MemOperand& src) {
 
 void TurboAssembler::SmiToInt32(Register smi) {
   ASM_CODE_COMMENT(this);
-  if (FLAG_enable_slow_asserts) {
+  if (v8_flags.enable_slow_asserts) {
     AssertSmi(smi);
   }
   DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -5735,7 +5737,7 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 }
 
 void TurboAssembler::AssertNotSmi(Register object, AbortReason reason) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
@@ -5745,7 +5747,7 @@ void TurboAssembler::AssertNotSmi(Register object, AbortReason reason) {
 }
 
 void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     static_assert(kSmiTag == 0);
     DCHECK(object != kScratchReg);
@@ -5755,7 +5757,7 @@ void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     DCHECK(object != kScratchReg);
     BlockTrampolinePoolScope block_trampoline_pool(this);
@@ -5773,7 +5775,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5793,7 +5795,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
@@ -5809,7 +5811,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     BlockTrampolinePoolScope block_trampoline_pool(this);
     static_assert(kSmiTag == 0);
@@ -5824,7 +5826,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   BlockTrampolinePoolScope block_trampoline_pool(this);
   static_assert(kSmiTag == 0);
@@ -5843,7 +5845,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
@@ -6019,7 +6021,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
   // PrepareCallCFunction.
 
 #if V8_HOST_ARCH_RISCV32 || V8_HOST_ARCH_RISCV64
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kSystemPointerSize) {
@@ -6250,7 +6252,7 @@ void TurboAssembler::DecompressTaggedSigned(const Register& destination,
                                             const MemOperand& field_operand) {
   ASM_CODE_COMMENT(this);
   Lwu(destination, field_operand);
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     // Corrupt the top 32 bits. Made up of 16 fixed bits and 16 pc offset bits.
     AddWord(destination, destination,
             Operand(((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32));
diff --git a/src/codegen/riscv/macro-assembler-riscv.h b/src/codegen/riscv/macro-assembler-riscv.h
index 70adc8aa6bf..9c8cb963bfd 100644
--- a/src/codegen/riscv/macro-assembler-riscv.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -1394,14 +1394,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/s390/assembler-s390-inl.h b/src/codegen/s390/assembler-s390-inl.h
index bd54b6eff2f..3a63c5abbc7 100644
--- a/src/codegen/s390/assembler-s390-inl.h
+++ b/src/codegen/s390/assembler-s390-inl.h
@@ -189,7 +189,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
     Assembler::set_target_address_at(pc_, constant_pool_, target.ptr(),
                                      icache_flush_mode);
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/s390/macro-assembler-s390.cc b/src/codegen/s390/macro-assembler-s390.cc
index 205052406ff..90c5f758365 100644
--- a/src/codegen/s390/macro-assembler-s390.cc
+++ b/src/codegen/s390/macro-assembler-s390.cc
@@ -979,7 +979,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kTaggedSize));
 
   lay(slot_address, MemOperand(object, offset - kHeapObjectTag));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label ok;
     AndP(r0, slot_address, Operand(kTaggedSize - 1));
     beq(&ok, Label::kNear);
@@ -993,7 +993,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 4)));
     mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 8)));
   }
@@ -1082,13 +1082,13 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
                                  Register value, LinkRegisterStatus lr_status,
                                  SaveFPRegsMode fp_mode, SmiCheck smi_check) {
   DCHECK(!AreAliased(object, slot_address, value));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     LoadTaggedPointerField(r0, MemOperand(slot_address));
     CmpS64(value, r0);
     Check(eq, AbortReason::kWrongAddressOrValuePassedToRecordWrite);
   }
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
   // First, check if a write barrier is even needed. The tests below
@@ -1116,13 +1116,13 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
     pop(r14);
   }
 
-  if (FLAG_debug_code) mov(slot_address, Operand(kZapValue));
+  if (v8_flags.debug_code) mov(slot_address, Operand(kZapValue));
 
   bind(&done);
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     mov(slot_address, Operand(base::bit_cast<intptr_t>(kZapValue + 12)));
     mov(value, Operand(base::bit_cast<intptr_t>(kZapValue + 16)));
   }
@@ -1165,12 +1165,12 @@ void TurboAssembler::PushStandardFrame(Register function_reg) {
 }
 
 void TurboAssembler::RestoreFrameStateForTailCall() {
-  // if (FLAG_enable_embedded_constant_pool) {
+  // if (v8_flags.enable_embedded_constant_pool) {
   //   LoadU64(kConstantPoolRegister,
   //         MemOperand(fp, StandardFrameConstants::kConstantPoolOffset));
   //   set_constant_pool_available(false);
   // }
-  DCHECK(!FLAG_enable_embedded_constant_pool);
+  DCHECK(!v8_flags.enable_embedded_constant_pool);
   LoadU64(r14, MemOperand(fp, StandardFrameConstants::kCallerPCOffset));
   LoadU64(fp, MemOperand(fp, StandardFrameConstants::kCallerFPOffset));
 }
@@ -1561,7 +1561,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
   // Reserve room for saved entry sp.
   lay(sp, MemOperand(fp, -ExitFrameConstants::kFixedFrameSizeFromFp));
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     StoreU64(MemOperand(fp, ExitFrameConstants::kSPOffset), Operand::Zero(),
              r1);
   }
@@ -1613,7 +1613,7 @@ int TurboAssembler::ActivationFrameAlignment() {
   // alignment. As the simulator is used to generate snapshots we do not know
   // if the target platform will need alignment, so this is controlled from a
   // flag.
-  return FLAG_sim_stack_alignment;
+  return v8_flags.sim_stack_alignment;
 #endif
 }
 
@@ -2070,7 +2070,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CompareObjectType(object, scratch, scratch, FEEDBACK_VECTOR_TYPE);
     Assert(eq, AbortReason::kExpectedFeedbackVector);
   }
@@ -2228,7 +2228,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK(value > 0 && is_int8(value));
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     Move(scratch2, ExternalReference::Create(counter));
     // @TODO(john.yan): can be optimized by asi()
     LoadS32(scratch1, MemOperand(scratch2));
@@ -2241,7 +2241,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
   DCHECK(value > 0 && is_int8(value));
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     Move(scratch2, ExternalReference::Create(counter));
     // @TODO(john.yan): can be optimized by asi()
     LoadS32(scratch1, MemOperand(scratch2));
@@ -2261,7 +2261,7 @@ void TurboAssembler::Check(Condition cond, AbortReason reason, CRegister cr) {
 void TurboAssembler::Abort(AbortReason reason) {
   Label abort_start;
   bind(&abort_start);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -2295,7 +2295,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       LoadEntryFromBuiltin(Builtin::kAbort, ip);
       Call(ip);
     } else {
@@ -2320,15 +2320,15 @@ void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void TurboAssembler::Assert(Condition cond, AbortReason reason, CRegister cr) {
-  if (FLAG_debug_code) Check(cond, reason, cr);
+  if (v8_flags.debug_code) Check(cond, reason, cr);
 }
 
 void TurboAssembler::AssertUnreachable(AbortReason reason) {
-  if (FLAG_debug_code) Abort(reason);
+  if (v8_flags.debug_code) Abort(reason);
 }
 
 void TurboAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmi, cr0);
@@ -2336,7 +2336,7 @@ void TurboAssembler::AssertNotSmi(Register object) {
 }
 
 void TurboAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(eq, AbortReason::kOperandIsNotASmi, cr0);
@@ -2344,7 +2344,7 @@ void TurboAssembler::AssertSmi(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object, Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor);
@@ -2356,7 +2356,7 @@ void MacroAssembler::AssertConstructor(Register object, Register scratch) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotAFunction, cr0);
@@ -2370,7 +2370,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   static_assert(kSmiTag == 0);
   TestIfSmi(object);
@@ -2384,7 +2384,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     static_assert(kSmiTag == 0);
     TestIfSmi(object);
     Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction, cr0);
@@ -2396,7 +2396,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   TestIfSmi(object);
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject, cr0);
 
@@ -2416,7 +2416,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     Label done_checking;
     AssertNotSmi(object);
     CompareRoot(object, RootIndex::kUndefinedValue);
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index c3728a3e033..b337b7633a3 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -1061,7 +1061,7 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
     lgfr(dst, dst);
   }
   void SmiToInt32(Register smi) {
-    if (FLAG_enable_slow_asserts) {
+    if (v8_flags.enable_slow_asserts) {
       AssertSmi(smi);
     }
     DCHECK(SmiValuesAre32Bits() || SmiValuesAre31Bits());
@@ -1619,14 +1619,14 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   void IncrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value, Register scratch1,
                             Register scratch2);
   void DecrementCounter(StatsCounter* counter, int value, Register scratch1,
                         Register scratch2) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value, scratch1, scratch2);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value, Register scratch1,
diff --git a/src/codegen/safepoint-table.h b/src/codegen/safepoint-table.h
index 70b5b0e6ce5..963729dd79c 100644
--- a/src/codegen/safepoint-table.h
+++ b/src/codegen/safepoint-table.h
@@ -130,7 +130,7 @@ class SafepointTable {
   using DeoptIndexSizeField = PcSizeField::Next<int, 3>;
   // In 22 bits, we can encode up to 4M bytes, corresponding to 32M frame slots,
   // which is 128MB on 32-bit and 256MB on 64-bit systems. The stack size is
-  // limited to a bit below 1MB anyway (see FLAG_stack_size).
+  // limited to a bit below 1MB anyway (see v8_flags.stack_size).
   using TaggedSlotsBytesField = DeoptIndexSizeField::Next<int, 22>;
 
   SafepointTable(Address instruction_start, Address safepoint_table_address);
diff --git a/src/codegen/turbo-assembler.h b/src/codegen/turbo-assembler.h
index bc0b696ff64..88ec873425c 100644
--- a/src/codegen/turbo-assembler.h
+++ b/src/codegen/turbo-assembler.h
@@ -103,7 +103,7 @@ class V8_EXPORT_PRIVATE TurboAssemblerBase : public Assembler {
 
   V8_INLINE std::string CommentForOffHeapTrampoline(const char* prefix,
                                                     Builtin builtin) {
-    if (!FLAG_code_comments) return "";
+    if (!v8_flags.code_comments) return "";
     std::ostringstream str;
     str << "Inlined  Trampoline for " << prefix << " to "
         << Builtins::name(builtin);
@@ -122,7 +122,7 @@ class V8_EXPORT_PRIVATE TurboAssemblerBase : public Assembler {
   bool root_array_available_ = true;
 
   // Immediately trap instead of calling {Abort} when debug code fails.
-  bool trap_on_abort_ = FLAG_trap_on_abort;
+  bool trap_on_abort_ = v8_flags.trap_on_abort;
 
   // Emit a C call to abort instead of a runtime call.
   bool hard_abort_ = false;
diff --git a/src/codegen/x64/assembler-x64-inl.h b/src/codegen/x64/assembler-x64-inl.h
index 675698a3372..a00c63ee00c 100644
--- a/src/codegen/x64/assembler-x64-inl.h
+++ b/src/codegen/x64/assembler-x64-inl.h
@@ -389,7 +389,7 @@ void RelocInfo::set_target_object(Heap* heap, HeapObject target,
   if (icache_flush_mode != SKIP_ICACHE_FLUSH) {
     FlushInstructionCache(pc_, sizeof(Address));
   }
-  if (!host().is_null() && !FLAG_disable_write_barriers) {
+  if (!host().is_null() && !v8_flags.disable_write_barriers) {
     WriteBarrierForCode(host(), this, target, write_barrier_mode);
   }
 }
diff --git a/src/codegen/x64/assembler-x64.cc b/src/codegen/x64/assembler-x64.cc
index 20a9c5fdbbe..796a74ce1c9 100644
--- a/src/codegen/x64/assembler-x64.cc
+++ b/src/codegen/x64/assembler-x64.cc
@@ -78,7 +78,7 @@ bool OSHasAVXSupport() {
 bool CpuFeatures::SupportsWasmSimd128() {
 #if V8_ENABLE_WEBASSEMBLY
   if (IsSupported(SSE4_1)) return true;
-  if (FLAG_wasm_simd_ssse3_codegen && IsSupported(SSSE3)) return true;
+  if (v8_flags.wasm_simd_ssse3_codegen && IsSupported(SSSE3)) return true;
 #endif  // V8_ENABLE_WEBASSEMBLY
   return false;
 }
@@ -103,14 +103,14 @@ void CpuFeatures::ProbeImpl(bool cross_compile) {
   }
 
   // SAHF is not generally available in long mode.
-  if (cpu.has_sahf() && FLAG_enable_sahf) SetSupported(SAHF);
-  if (cpu.has_bmi1() && FLAG_enable_bmi1) SetSupported(BMI1);
-  if (cpu.has_bmi2() && FLAG_enable_bmi2) SetSupported(BMI2);
-  if (cpu.has_lzcnt() && FLAG_enable_lzcnt) SetSupported(LZCNT);
-  if (cpu.has_popcnt() && FLAG_enable_popcnt) SetSupported(POPCNT);
-  if (strcmp(FLAG_mcpu, "auto") == 0) {
+  if (cpu.has_sahf() && v8_flags.enable_sahf) SetSupported(SAHF);
+  if (cpu.has_bmi1() && v8_flags.enable_bmi1) SetSupported(BMI1);
+  if (cpu.has_bmi2() && v8_flags.enable_bmi2) SetSupported(BMI2);
+  if (cpu.has_lzcnt() && v8_flags.enable_lzcnt) SetSupported(LZCNT);
+  if (cpu.has_popcnt() && v8_flags.enable_popcnt) SetSupported(POPCNT);
+  if (strcmp(v8_flags.mcpu, "auto") == 0) {
     if (cpu.is_atom()) SetSupported(INTEL_ATOM);
-  } else if (strcmp(FLAG_mcpu, "atom") == 0) {
+  } else if (strcmp(v8_flags.mcpu, "atom") == 0) {
     SetSupported(INTEL_ATOM);
   }
 
@@ -118,13 +118,13 @@ void CpuFeatures::ProbeImpl(bool cross_compile) {
   // AVX but not SSE4_2, if we have --enable-avx and --no-enable-sse4-2, the
   // code above would set AVX to supported, and SSE4_2 to unsupported, then the
   // checks below will set AVX to unsupported.
-  if (!FLAG_enable_sse3) SetUnsupported(SSE3);
-  if (!FLAG_enable_ssse3 || !IsSupported(SSE3)) SetUnsupported(SSSE3);
-  if (!FLAG_enable_sse4_1 || !IsSupported(SSSE3)) SetUnsupported(SSE4_1);
-  if (!FLAG_enable_sse4_2 || !IsSupported(SSE4_1)) SetUnsupported(SSE4_2);
-  if (!FLAG_enable_avx || !IsSupported(SSE4_2)) SetUnsupported(AVX);
-  if (!FLAG_enable_avx2 || !IsSupported(AVX)) SetUnsupported(AVX2);
-  if (!FLAG_enable_fma3 || !IsSupported(AVX)) SetUnsupported(FMA3);
+  if (!v8_flags.enable_sse3) SetUnsupported(SSE3);
+  if (!v8_flags.enable_ssse3 || !IsSupported(SSE3)) SetUnsupported(SSSE3);
+  if (!v8_flags.enable_sse4_1 || !IsSupported(SSSE3)) SetUnsupported(SSE4_1);
+  if (!v8_flags.enable_sse4_2 || !IsSupported(SSE4_1)) SetUnsupported(SSE4_2);
+  if (!v8_flags.enable_avx || !IsSupported(SSE4_2)) SetUnsupported(AVX);
+  if (!v8_flags.enable_avx2 || !IsSupported(AVX)) SetUnsupported(AVX2);
+  if (!v8_flags.enable_fma3 || !IsSupported(AVX)) SetUnsupported(FMA3);
 
   // Set a static value on whether Simd is supported.
   // This variable is only used for certain archs to query SupportWasmSimd128()
@@ -267,9 +267,9 @@ bool ConstPool::AddSharedEntry(uint64_t data, int offset) {
 }
 
 bool ConstPool::TryRecordEntry(intptr_t data, RelocInfo::Mode mode) {
-  if (!FLAG_partial_constant_pool) return false;
+  if (!v8_flags.partial_constant_pool) return false;
   DCHECK_WITH_MSG(
-      FLAG_text_is_readable,
+      v8_flags.text_is_readable,
       "The partial constant pool requires a readable .text section");
   if (!RelocInfo::IsShareableRelocMode(mode)) return false;
 
@@ -328,7 +328,7 @@ void Assembler::PatchConstPool() {
 }
 
 bool Assembler::UseConstPoolFor(RelocInfo::Mode rmode) {
-  if (!FLAG_partial_constant_pool) return false;
+  if (!v8_flags.partial_constant_pool) return false;
   return (rmode == RelocInfo::NO_INFO ||
           rmode == RelocInfo::EXTERNAL_REFERENCE ||
           rmode == RelocInfo::OFF_HEAP_TARGET);
@@ -2006,8 +2006,8 @@ void Assembler::Nop(int n) {
 
 void Assembler::emit_trace_instruction(Immediate markid) {
   EnsureSpace ensure_space(this);
-  if (FLAG_wasm_trace_native != nullptr &&
-      !strcmp(FLAG_wasm_trace_native, "cpuid")) {
+  if (v8_flags.wasm_trace_native != nullptr &&
+      !strcmp(v8_flags.wasm_trace_native, "cpuid")) {
     // This is the optionally selected cpuid sequence which computes a magic
     // number based upon the markid. The low 16 bits of the magic number are
     // 0x4711 and the high 16 bits are the low 16 bits of the markid. This
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 62fda6336d0..b6eba5f1277 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -377,7 +377,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kTaggedSize));
 
   leaq(slot_address, FieldOperand(object, offset));
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Debug check slot_address");
     Label ok;
     testb(slot_address, Immediate(kTaggedSize - 1));
@@ -392,7 +392,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
   // Clobber clobbered input registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Zap scratch registers");
     Move(value, kZapValue, RelocInfo::NO_INFO);
     Move(slot_address, kZapValue, RelocInfo::NO_INFO);
@@ -623,11 +623,11 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
   DCHECK(!AreAliased(object, slot_address, value));
   AssertNotSmi(object);
 
-  if (FLAG_disable_write_barriers) {
+  if (v8_flags.disable_write_barriers) {
     return;
   }
 
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Debug check slot_address");
     Label ok;
     cmp_tagged(value, Operand(slot_address, 0));
@@ -661,7 +661,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
 
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     ASM_CODE_COMMENT_STRING(this, "Zap scratch registers");
     Move(slot_address, kZapValue, RelocInfo::NO_INFO);
     Move(value, kZapValue, RelocInfo::NO_INFO);
@@ -693,7 +693,7 @@ void TurboAssembler::CheckStackAlignment() {
 
 void TurboAssembler::Abort(AbortReason reason) {
   ASM_CODE_COMMENT(this);
-  if (FLAG_code_comments) {
+  if (v8_flags.code_comments) {
     const char* msg = GetAbortReason(reason);
     RecordComment("Abort message: ");
     RecordComment(msg);
@@ -725,7 +725,7 @@ void TurboAssembler::Abort(AbortReason reason) {
       // Generate an indirect call via builtins entry table here in order to
       // ensure that the interpreter_entry_return_pc_offset is the same for
       // InterpreterEntryTrampoline and InterpreterEntryTrampolineForProfiling
-      // when FLAG_debug_code is enabled.
+      // when v8_flags.debug_code is enabled.
       Call(EntryFromBuiltinAsOperand(Builtin::kAbort));
     } else {
       Call(BUILTIN_CODE(isolate(), Abort), RelocInfo::CODE_TARGET);
@@ -833,7 +833,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void MacroAssembler::AssertFeedbackVector(Register object) {
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CmpObjectType(object, FEEDBACK_VECTOR_TYPE, kScratchRegister);
     Assert(equal, AbortReason::kExpectedFeedbackVector);
   }
@@ -2645,28 +2645,28 @@ Immediate MacroAssembler::ClearedValue() const {
 
 #ifdef V8_ENABLE_DEBUG_CODE
 void TurboAssembler::AssertNotSmi(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Condition is_smi = CheckSmi(object);
   Check(NegateCondition(is_smi), AbortReason::kOperandIsASmi);
 }
 
 void TurboAssembler::AssertSmi(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Condition is_smi = CheckSmi(object);
   Check(is_smi, AbortReason::kOperandIsNotASmi);
 }
 
 void TurboAssembler::AssertSmi(Operand object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Condition is_smi = CheckSmi(object);
   Check(is_smi, AbortReason::kOperandIsNotASmi);
 }
 
 void TurboAssembler::AssertZeroExtended(Register int32_register) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   DCHECK_NE(int32_register, kScratchRegister);
   movq(kScratchRegister, int64_t{0x0000000100000000});
@@ -2675,7 +2675,7 @@ void TurboAssembler::AssertZeroExtended(Register int32_register) {
 }
 
 void TurboAssembler::AssertSignedBitOfSmiIsZero(Register smi_register) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   DCHECK(COMPRESS_POINTERS_BOOL);
   testl(smi_register, Immediate(int32_t{0x10000000}));
@@ -2683,7 +2683,7 @@ void TurboAssembler::AssertSignedBitOfSmiIsZero(Register smi_register) {
 }
 
 void MacroAssembler::AssertCodeT(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsNotACodeT);
@@ -2695,7 +2695,7 @@ void MacroAssembler::AssertCodeT(Register object) {
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotAConstructor);
@@ -2708,7 +2708,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotAFunction);
@@ -2721,7 +2721,7 @@ void MacroAssembler::AssertFunction(Register object) {
 }
 
 void MacroAssembler::AssertCallableFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotAFunction);
@@ -2734,7 +2734,7 @@ void MacroAssembler::AssertCallableFunction(Register object) {
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotABoundFunction);
@@ -2745,7 +2745,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
@@ -2764,7 +2764,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 }
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object) {
-  if (!FLAG_debug_code) return;
+  if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   Label done_checking;
   AssertNotSmi(object);
@@ -2780,11 +2780,11 @@ void MacroAssembler::AssertUndefinedOrAllocationSite(Register object) {
 }
 
 void TurboAssembler::Assert(Condition cc, AbortReason reason) {
-  if (FLAG_debug_code) Check(cc, reason);
+  if (v8_flags.debug_code) Check(cc, reason);
 }
 
 void TurboAssembler::AssertUnreachable(AbortReason reason) {
-  if (FLAG_debug_code) Abort(reason);
+  if (v8_flags.debug_code) Abort(reason);
 }
 #endif  // V8_ENABLE_DEBUG_CODE
 
@@ -2797,7 +2797,7 @@ void MacroAssembler::LoadWeakValue(Register in_out, Label* target_if_cleared) {
 
 void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Operand counter_operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter));
@@ -2814,7 +2814,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value) {
 
 void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value) {
   DCHECK_GT(value, 0);
-  if (FLAG_native_code_counters && counter->Enabled()) {
+  if (v8_flags.native_code_counters && counter->Enabled()) {
     ASM_CODE_COMMENT(this);
     Operand counter_operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter));
@@ -3082,7 +3082,7 @@ void TurboAssembler::LeaveFrame(StackFrame::Type type) {
   // TODO(v8:11429): Consider passing BASELINE instead, and checking for
   // IsJSFrame or similar. Could then unify with manual frame leaves in the
   // interpreter too.
-  if (FLAG_debug_code && !StackFrame::IsJavaScript(type)) {
+  if (v8_flags.debug_code && !StackFrame::IsJavaScript(type)) {
     cmpq(Operand(rbp, CommonFrameConstants::kContextOrFrameTypeOffset),
          Immediate(StackFrame::TypeToMarker(type)));
     Check(equal, AbortReason::kStackFrameTypesMustMatch);
@@ -3338,7 +3338,7 @@ void TurboAssembler::CallCFunction(Register function, int num_arguments) {
   DCHECK_LE(num_arguments, kMaxCParameters);
   DCHECK(has_frame());
   // Check stack alignment.
-  if (FLAG_debug_code) {
+  if (v8_flags.debug_code) {
     CheckStackAlignment();
   }
 
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index 8217aa003f2..7c97692f64b 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -919,12 +919,12 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // ---------------------------------------------------------------------------
   // StatsCounter support
   void IncrementCounter(StatsCounter* counter, int value) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitIncrementCounter(counter, value);
   }
   void EmitIncrementCounter(StatsCounter* counter, int value);
   void DecrementCounter(StatsCounter* counter, int value) {
-    if (!FLAG_native_code_counters) return;
+    if (!v8_flags.native_code_counters) return;
     EmitDecrementCounter(counter, value);
   }
   void EmitDecrementCounter(StatsCounter* counter, int value);
diff --git a/test/unittests/codegen/code-pages-unittest.cc b/test/unittests/codegen/code-pages-unittest.cc
index 7183d529f88..6fc67e4ed04 100644
--- a/test/unittests/codegen/code-pages-unittest.cc
+++ b/test/unittests/codegen/code-pages-unittest.cc
@@ -137,7 +137,7 @@ TEST_F(CodePagesTest, CodePagesCorrectContents) {
 }
 
 TEST_F(CodePagesTest, OptimizedCodeWithCodeRange) {
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
   if (!i_isolate()->RequiresCodeRange()) return;
 
   HandleScope scope(i_isolate());
@@ -166,7 +166,7 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodePages) {
   // We don't want incremental marking to start which could cause the code to
   // not be collected on the CollectGarbage() call.
   ManualGCScope manual_gc_scope(i_isolate());
-  FLAG_allow_natives_syntax = true;
+  v8_flags.allow_natives_syntax = true;
 
   const void* created_page = nullptr;
   int num_foos_created = 0;
@@ -196,7 +196,7 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodePages) {
       // --always-sparkplug (if this check fails, we'll have to re-think this
       // test).
       if (foo->shared().HasBaselineCode()) {
-        EXPECT_TRUE(FLAG_always_sparkplug);
+        EXPECT_TRUE(v8_flags.always_sparkplug);
         return;
       }
       CodeT codet = foo->code();
diff --git a/test/unittests/codegen/factory-unittest.cc b/test/unittests/codegen/factory-unittest.cc
index f67301e3d04..3fb9140d1be 100644
--- a/test/unittests/codegen/factory-unittest.cc
+++ b/test/unittests/codegen/factory-unittest.cc
@@ -57,7 +57,7 @@ size_t NearHeapLimitCallback(void* raw_bool, size_t current_heap_limit,
 class FactoryCodeBuilderOOMTest : public TestWithIsolate {
  public:
   static void SetUpTestSuite() {
-    FLAG_max_old_space_size = kInstructionSize / MB / 2;  // In MB.
+    v8_flags.max_old_space_size = kInstructionSize / MB / 2;  // In MB.
   }
 
   void SetUp() override {
diff --git a/test/unittests/codegen/pointer-auth-arm64-unittest.cc b/test/unittests/codegen/pointer-auth-arm64-unittest.cc
index ae9c7b43f46..5f34f8313e5 100644
--- a/test/unittests/codegen/pointer-auth-arm64-unittest.cc
+++ b/test/unittests/codegen/pointer-auth-arm64-unittest.cc
@@ -32,7 +32,7 @@ TEST_F(PointerAuthArm64Test, compute_pac) {
 }
 
 TEST_F(PointerAuthArm64Test, add_and_auth_pac) {
-  i::FLAG_sim_abort_on_bad_auth = false;
+  i::v8_flags.sim_abort_on_bad_auth = false;
   Decoder<DispatchingDecoderVisitor>* decoder =
       new Decoder<DispatchingDecoderVisitor>();
   Simulator simulator(decoder);
-- 
2.35.1

