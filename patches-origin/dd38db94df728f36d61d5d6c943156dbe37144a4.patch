From dd38db94df728f36d61d5d6c943156dbe37144a4 Mon Sep 17 00:00:00 2001
From: Jakob Linke <jgruber@chromium.org>
Date: Mon, 16 Jan 2023 15:39:08 +0100
Subject: [PATCH] [codet] Remove the CodeT type alias

.. now that it unconditionally refers to CodeDataContainer. All
previous references to 'CodeT' (the type and as part of names) are
now updated to 'CodeDataContainer', including 'codet', 'CODET', etc.

Bug: v8:13654
Change-Id: I7abbba040091eddf3ef09028a891aed460363929
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4165619
Commit-Queue: Jakob Linke <jgruber@chromium.org>
Reviewed-by: Michael Lippautz <mlippautz@chromium.org>
Cr-Commit-Position: refs/heads/main@{#85325}
---
 src/api/api-natives.cc                        |   6 +-
 src/api/api.cc                                |   5 +-
 src/baseline/arm/baseline-assembler-arm-inl.h |   3 +-
 .../arm64/baseline-assembler-arm64-inl.h      |   2 +-
 src/baseline/baseline-batch-compiler.cc       |   3 +-
 .../ia32/baseline-assembler-ia32-inl.h        |   2 +-
 .../loong64/baseline-assembler-loong64-inl.h  |   4 +-
 .../mips64/baseline-assembler-mips64-inl.h    |   4 +-
 src/baseline/ppc/baseline-assembler-ppc-inl.h |   3 +-
 .../riscv/baseline-assembler-riscv-inl.h      |   2 +-
 .../s390/baseline-assembler-s390-inl.h        |   3 +-
 src/baseline/x64/baseline-assembler-x64-inl.h |   2 +-
 src/builtins/arm/builtins-arm.cc              |  33 +++---
 src/builtins/arm64/builtins-arm64.cc          |  41 +++----
 src/builtins/builtins-array-gen.cc            |   2 +-
 src/builtins/builtins-async-gen.cc            |   2 +-
 src/builtins/builtins-constructor-gen.cc      |   2 +-
 src/builtins/builtins-internal-gen.cc         |   6 +-
 src/builtins/builtins-lazy-gen.cc             |  21 ++--
 src/builtins/builtins-lazy-gen.h              |   3 +-
 src/builtins/builtins-regexp-gen.cc           |  10 +-
 src/builtins/builtins-regexp-gen.h            |   2 -
 src/builtins/builtins.cc                      |  30 ++---
 src/builtins/builtins.h                       |  28 ++---
 src/builtins/ia32/builtins-ia32.cc            |  38 ++++---
 src/builtins/loong64/builtins-loong64.cc      |  37 +++---
 src/builtins/mips64/builtins-mips64.cc        |  37 +++---
 src/builtins/ppc/builtins-ppc.cc              |  33 +++---
 src/builtins/riscv/builtins-riscv.cc          |   7 +-
 src/builtins/s390/builtins-s390.cc            |  33 +++---
 src/builtins/setup-builtins-internal.cc       |  12 +-
 src/builtins/x64/builtins-x64.cc              |  43 +++----
 src/codegen/arm/assembler-arm-inl.h           |   2 +-
 src/codegen/arm/assembler-arm.h               |   2 +-
 src/codegen/arm/macro-assembler-arm.cc        |  27 +++--
 src/codegen/arm/macro-assembler-arm.h         |  10 +-
 src/codegen/arm64/assembler-arm64-inl.h       |   6 +-
 src/codegen/arm64/assembler-arm64.cc          |   2 +-
 src/codegen/arm64/assembler-arm64.h           |   2 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |  42 ++++---
 src/codegen/arm64/macro-assembler-arm64.h     |  16 ++-
 src/codegen/assembler.cc                      |   5 +-
 src/codegen/assembler.h                       |   6 +-
 src/codegen/bailout-reason.h                  |   2 +-
 src/codegen/callable.h                        |   6 +-
 src/codegen/code-factory.cc                   |  10 +-
 src/codegen/code-factory.h                    |   9 +-
 src/codegen/code-reference.cc                 |  40 +++----
 src/codegen/code-reference.h                  |  25 ++--
 src/codegen/code-stub-assembler.cc            |  35 +++---
 src/codegen/code-stub-assembler.h             |  27 ++---
 src/codegen/compiler.cc                       |  66 ++++++-----
 src/codegen/compiler.h                        |   6 +-
 src/codegen/ia32/assembler-ia32-inl.h         |   2 +-
 src/codegen/ia32/assembler-ia32.cc            |   7 +-
 src/codegen/ia32/assembler-ia32.h             |   8 +-
 src/codegen/ia32/macro-assembler-ia32.cc      |  25 ++--
 src/codegen/ia32/macro-assembler-ia32.h       |   7 +-
 src/codegen/loong64/assembler-loong64-inl.h   |   2 +-
 src/codegen/loong64/assembler-loong64.h       |   2 +-
 .../loong64/macro-assembler-loong64.cc        |  30 ++---
 src/codegen/loong64/macro-assembler-loong64.h |  12 +-
 src/codegen/mips64/macro-assembler-mips64.cc  |  30 ++---
 src/codegen/mips64/macro-assembler-mips64.h   |  12 +-
 src/codegen/ppc/macro-assembler-ppc.cc        |  29 ++---
 src/codegen/ppc/macro-assembler-ppc.h         |  11 +-
 src/codegen/riscv/macro-assembler-riscv.cc    |  15 ++-
 src/codegen/riscv/macro-assembler-riscv.h     |   5 +-
 src/codegen/s390/assembler-s390.cc            |   4 +-
 src/codegen/s390/assembler-s390.h             |   5 +-
 src/codegen/s390/macro-assembler-s390.cc      |  27 +++--
 src/codegen/s390/macro-assembler-s390.h       |   9 +-
 src/codegen/x64/assembler-x64-inl.h           |   2 +-
 src/codegen/x64/assembler-x64.cc              |   9 +-
 src/codegen/x64/assembler-x64.h               |   8 +-
 src/codegen/x64/macro-assembler-x64.cc        |  50 ++++----
 src/codegen/x64/macro-assembler-x64.h         |  19 ++--
 src/common/globals.h                          |   5 -
 src/compiler/backend/code-generator-impl.h    |   4 +-
 .../backend/ia32/code-generator-ia32.cc       |   4 +-
 src/compiler/backend/instruction.cc           |   6 +-
 src/compiler/backend/instruction.h            |   2 +-
 .../backend/x64/code-generator-x64.cc         |   4 +-
 src/compiler/bytecode-graph-builder.cc        |   2 +-
 src/compiler/code-assembler.cc                |  10 +-
 src/compiler/code-assembler.h                 |  18 +--
 src/compiler/heap-refs.cc                     |  18 ++-
 src/compiler/heap-refs.h                      |  13 +--
 src/compiler/js-call-reducer.cc               |   4 +-
 src/compiler/js-operator.cc                   |   2 +-
 src/compiler/js-operator.h                    |  12 +-
 src/compiler/turboshaft/assembler.cc          |   2 +-
 src/compiler/turboshaft/assembler.h           |   2 +-
 src/compiler/wasm-compiler.cc                 |   7 +-
 src/compiler/wasm-compiler.h                  |   2 +-
 src/debug/debug-evaluate.cc                   |   2 +-
 src/debug/debug.cc                            |   3 +-
 src/deoptimizer/deoptimizer.cc                |  31 +++--
 src/deoptimizer/deoptimizer.h                 |   3 +-
 src/deoptimizer/translated-state.cc           |   5 +-
 src/diagnostics/disassembler.cc               |   4 +-
 src/diagnostics/objects-debug.cc              |  12 +-
 src/diagnostics/objects-printer.cc            |   3 +-
 src/execution/execution.cc                    |  10 +-
 src/execution/execution.h                     |   2 +-
 src/execution/frames.cc                       |  62 +++++-----
 src/execution/frames.h                        |  12 +-
 src/execution/isolate-data.h                  |   2 +-
 src/execution/isolate.cc                      |  56 +++++----
 src/execution/simulator.h                     |   4 +-
 src/heap/factory.cc                           |  30 ++---
 src/heap/factory.h                            |  22 ++--
 src/heap/heap.cc                              |   4 -
 src/heap/heap.h                               |   6 -
 src/heap/mark-compact.cc                      |  29 ++---
 src/heap/marking-visitor-inl.h                |  12 +-
 src/ic/accessor-assembler.cc                  |  21 ++--
 src/ic/accessor-assembler.h                   |   5 +-
 src/ic/handler-configuration-inl.h            |   8 +-
 src/ic/handler-configuration.cc               |   9 +-
 src/ic/handler-configuration.h                |   6 +-
 src/ic/ic-inl.h                               |   2 +-
 src/interpreter/interpreter-assembler.cc      |  13 ++-
 src/interpreter/interpreter-generator.cc      |   5 +-
 src/interpreter/interpreter.cc                |  13 ++-
 src/interpreter/interpreter.h                 |   6 +-
 src/logging/log.cc                            |  17 ++-
 src/maglev/maglev-compiler.cc                 |   4 +-
 src/maglev/maglev-compiler.h                  |   2 +-
 src/maglev/maglev-concurrent-dispatcher.cc    |   8 +-
 src/maglev/maglev.cc                          |   4 +-
 src/maglev/maglev.h                           |   4 +-
 src/objects/call-site-info-inl.h              |   5 +-
 src/objects/code-inl.h                        | 107 ++++++++++--------
 src/objects/code.cc                           |  29 ++---
 src/objects/code.h                            |  45 ++++----
 src/objects/data-handler.tq                   |   8 +-
 src/objects/feedback-vector-inl.h             |  17 +--
 src/objects/feedback-vector.cc                |  13 ++-
 src/objects/feedback-vector.h                 |  10 +-
 src/objects/heap-object.h                     |   1 -
 src/objects/instance-type-inl.h               |   4 -
 src/objects/instance-type.h                   |   4 -
 src/objects/js-function-inl.h                 |  20 ++--
 src/objects/js-function.cc                    |   2 +-
 src/objects/js-function.h                     |  10 +-
 src/objects/js-regexp-inl.h                   |   4 +-
 src/objects/js-regexp.cc                      |   6 +-
 src/objects/objects-inl.h                     |   3 -
 src/objects/objects.h                         |   1 -
 src/objects/shared-function-info-inl.h        |  30 ++---
 src/objects/shared-function-info.cc           |  10 +-
 src/objects/shared-function-info.h            |   6 +-
 src/profiler/cpu-profiler.cc                  |   2 +-
 src/profiler/heap-snapshot-generator.cc       |  24 ++--
 src/profiler/heap-snapshot-generator.h        |   2 +-
 src/regexp/regexp-macro-assembler.cc          |   2 +-
 src/regexp/regexp.cc                          |  16 +--
 src/runtime/runtime-compiler.cc               |  27 +++--
 src/runtime/runtime-test-wasm.cc              |   2 +-
 src/runtime/runtime-test.cc                   |  17 +--
 src/runtime/runtime-wasm.cc                   |   5 +-
 src/snapshot/code-serializer.cc               |   4 +-
 src/snapshot/embedded/embedded-data.cc        |  14 +--
 src/snapshot/embedded/embedded-file-writer.cc |   2 +-
 src/snapshot/serializer-inl.h                 |   2 +-
 src/snapshot/serializer.cc                    |   4 +-
 src/wasm/c-api.cc                             |   5 +-
 src/wasm/function-compiler.cc                 |  10 +-
 src/wasm/function-compiler.h                  |  12 +-
 src/wasm/module-compiler.cc                   |  10 +-
 src/wasm/module-instantiate.cc                |   2 +-
 src/wasm/wasm-js.cc                           |   2 +-
 src/wasm/wasm-objects.cc                      |  24 ++--
 src/wasm/wasm-objects.h                       |   4 +-
 test/cctest/cctest.cc                         |   2 +-
 test/cctest/compiler/codegen-tester.h         |   4 +-
 test/cctest/compiler/function-tester.cc       |   6 +-
 test/cctest/compiler/function-tester.h        |   2 +-
 test/cctest/compiler/test-code-generator.cc   |  10 +-
 .../test-concurrent-shared-function-info.cc   |  10 +-
 test/cctest/compiler/test-run-machops.cc      |   4 +-
 test/cctest/compiler/test-run-native-calls.cc |  37 +++---
 test/cctest/heap/test-heap.cc                 |  11 +-
 test/cctest/heap/test-weak-references.cc      |   4 +-
 test/cctest/test-accessor-assembler.cc        |   2 +-
 test/cctest/test-accessors.cc                 |   3 +-
 test/cctest/test-cpu-profiler.cc              |   2 +-
 test/cctest/test-debug.cc                     |   2 +-
 test/cctest/test-disasm-regex-helper.cc       |   2 +-
 test/cctest/test-heap-profiler.cc             |   5 +-
 test/cctest/test-swiss-name-dictionary-csa.cc |  48 ++++----
 test/cctest/test-unwinder-code-pages.cc       |  14 +--
 test/cctest/wasm/test-c-wasm-entry.cc         |   2 +-
 test/cctest/wasm/test-gc.cc                   |   2 +-
 test/cctest/wasm/test-run-wasm-wrappers.cc    |   8 +-
 test/common/call-tester.h                     |   6 +-
 test/common/code-assembler-tester.h           |   5 +-
 .../assembler/disasm-x64-unittest.cc          |   6 +-
 test/unittests/codegen/code-pages-unittest.cc |  12 +-
 test/unittests/compiler/codegen-tester.h      |   4 +-
 test/unittests/compiler/function-tester.cc    |   4 +-
 .../compiler/run-tail-calls-unittest.cc       |   6 +-
 .../interpreter/interpreter-unittest.cc       |  20 ++--
 test/unittests/logging/log-unittest.cc        |   3 +-
 test/unittests/regexp/regexp-unittest.cc      |   8 +-
 206 files changed, 1307 insertions(+), 1167 deletions(-)

diff --git a/src/api/api-natives.cc b/src/api/api-natives.cc
index d0b29872342..061cd1f307b 100644
--- a/src/api/api-natives.cc
+++ b/src/api/api-natives.cc
@@ -83,7 +83,8 @@ MaybeHandle<Object> DefineAccessorProperty(Isolate* isolate,
         InstantiateFunction(isolate,
                             Handle<FunctionTemplateInfo>::cast(getter)),
         Object);
-    Handle<CodeT> trampoline = BUILTIN_CODE(isolate, DebugBreakTrampoline);
+    Handle<CodeDataContainer> trampoline =
+        BUILTIN_CODE(isolate, DebugBreakTrampoline);
     Handle<JSFunction>::cast(getter)->set_code(*trampoline);
   }
   if (setter->IsFunctionTemplateInfo() &&
@@ -93,7 +94,8 @@ MaybeHandle<Object> DefineAccessorProperty(Isolate* isolate,
         InstantiateFunction(isolate,
                             Handle<FunctionTemplateInfo>::cast(setter)),
         Object);
-    Handle<CodeT> trampoline = BUILTIN_CODE(isolate, DebugBreakTrampoline);
+    Handle<CodeDataContainer> trampoline =
+        BUILTIN_CODE(isolate, DebugBreakTrampoline);
     Handle<JSFunction>::cast(setter)->set_code(*trampoline);
   }
   RETURN_ON_EXCEPTION(
diff --git a/src/api/api.cc b/src/api/api.cc
index b7b919d10c9..18766a15c5a 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -6642,7 +6642,8 @@ Local<Context> NewContext(
   // TODO(jkummerow): This is for crbug.com/713699. Remove it if it doesn't
   // fail.
   // Sanity-check that the isolate is initialized and usable.
-  CHECK(i_isolate->builtins()->code(i::Builtin::kIllegal).IsCodeT());
+  CHECK(
+      i_isolate->builtins()->code(i::Builtin::kIllegal).IsCodeDataContainer());
 
   TRACE_EVENT_CALL_STATS_SCOPED(i_isolate, "v8", "V8.NewContext");
   API_RCS_SCOPE(i_isolate, Context, New);
@@ -9707,7 +9708,7 @@ JSEntryStubs Isolate::GetJSEntryStubs() {
        {i::Builtin::kJSRunMicrotasksEntry,
         &entry_stubs.js_run_microtasks_entry_stub}}};
   for (auto& pair : stubs) {
-    i::CodeT js_entry = i_isolate->builtins()->code(pair.first);
+    i::CodeDataContainer js_entry = i_isolate->builtins()->code(pair.first);
     pair.second->code.start =
         reinterpret_cast<const void*>(js_entry.InstructionStart());
     pair.second->code.length_in_bytes = js_entry.InstructionSize();
diff --git a/src/baseline/arm/baseline-assembler-arm-inl.h b/src/baseline/arm/baseline-assembler-arm-inl.h
index 5cb855e416e..22aee5d00dd 100644
--- a/src/baseline/arm/baseline-assembler-arm-inl.h
+++ b/src/baseline/arm/baseline-assembler-arm-inl.h
@@ -409,7 +409,8 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   {
     ScratchRegisterScope temps(this);
     Register scratch = temps.AcquireScratch();
-    __ TestCodeTIsMarkedForDeoptimization(scratch_and_result, scratch);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(scratch_and_result,
+                                                      scratch);
     __ b(eq, on_result);
     __ mov(scratch, __ ClearedValue());
     StoreTaggedFieldNoWriteBarrier(
diff --git a/src/baseline/arm64/baseline-assembler-arm64-inl.h b/src/baseline/arm64/baseline-assembler-arm64-inl.h
index 20344a081ec..f4bca7a5021 100644
--- a/src/baseline/arm64/baseline-assembler-arm64-inl.h
+++ b/src/baseline/arm64/baseline-assembler-arm64-inl.h
@@ -464,7 +464,7 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   // Is it marked_for_deoptimization? If yes, clear the slot.
   {
     ScratchRegisterScope temps(this);
-    __ JumpIfCodeTIsMarkedForDeoptimization(
+    __ JumpIfCodeDataContainerIsMarkedForDeoptimization(
         scratch_and_result, temps.AcquireScratch(), &clear_slot);
     __ B(on_result);
   }
diff --git a/src/baseline/baseline-batch-compiler.cc b/src/baseline/baseline-batch-compiler.cc
index 9193551b847..b84df2f3f59 100644
--- a/src/baseline/baseline-batch-compiler.cc
+++ b/src/baseline/baseline-batch-compiler.cc
@@ -74,7 +74,8 @@ class BaselineCompilerTask {
       return;
     }
 
-    shared_function_info_->set_baseline_code(ToCodeT(*code), kReleaseStore);
+    shared_function_info_->set_baseline_code(ToCodeDataContainer(*code),
+                                             kReleaseStore);
     if (v8_flags.trace_baseline_concurrent_compilation) {
       CodeTracer::Scope scope(isolate->GetCodeTracer());
       std::stringstream ss;
diff --git a/src/baseline/ia32/baseline-assembler-ia32-inl.h b/src/baseline/ia32/baseline-assembler-ia32-inl.h
index 3fcc1da7d6e..26b198c6ce9 100644
--- a/src/baseline/ia32/baseline-assembler-ia32-inl.h
+++ b/src/baseline/ia32/baseline-assembler-ia32-inl.h
@@ -387,7 +387,7 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   // Is it marked_for_deoptimization? If yes, clear the slot.
   {
     ScratchRegisterScope temps(this);
-    __ TestCodeTIsMarkedForDeoptimization(scratch_and_result);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(scratch_and_result);
     __ j(equal, on_result, distance);
     __ mov(FieldOperand(feedback_vector,
                         FeedbackVector::OffsetOfElementAt(slot.ToInt())),
diff --git a/src/baseline/loong64/baseline-assembler-loong64-inl.h b/src/baseline/loong64/baseline-assembler-loong64-inl.h
index cc5694554a9..5d84367b7a7 100644
--- a/src/baseline/loong64/baseline-assembler-loong64-inl.h
+++ b/src/baseline/loong64/baseline-assembler-loong64-inl.h
@@ -383,8 +383,8 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   {
     ScratchRegisterScope temps(this);
     Register scratch = temps.AcquireScratch();
-    __ TestCodeTIsMarkedForDeoptimizationAndJump(scratch_and_result, scratch,
-                                                 eq, on_result);
+    __ TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+        scratch_and_result, scratch, eq, on_result);
     __ li(scratch, __ ClearedValue());
     StoreTaggedFieldNoWriteBarrier(
         feedback_vector, FeedbackVector::OffsetOfElementAt(slot.ToInt()),
diff --git a/src/baseline/mips64/baseline-assembler-mips64-inl.h b/src/baseline/mips64/baseline-assembler-mips64-inl.h
index 17bd834d5d5..1b38d97d238 100644
--- a/src/baseline/mips64/baseline-assembler-mips64-inl.h
+++ b/src/baseline/mips64/baseline-assembler-mips64-inl.h
@@ -393,8 +393,8 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   {
     ScratchRegisterScope temps(this);
     Register scratch = temps.AcquireScratch();
-    __ TestCodeTIsMarkedForDeoptimizationAndJump(scratch_and_result, scratch,
-                                                 eq, on_result);
+    __ TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+        scratch_and_result, scratch, eq, on_result);
     __ li(scratch, __ ClearedValue());
     StoreTaggedFieldNoWriteBarrier(
         feedback_vector, FeedbackVector::OffsetOfElementAt(slot.ToInt()),
diff --git a/src/baseline/ppc/baseline-assembler-ppc-inl.h b/src/baseline/ppc/baseline-assembler-ppc-inl.h
index 654f98e9a9a..8c65c188d12 100644
--- a/src/baseline/ppc/baseline-assembler-ppc-inl.h
+++ b/src/baseline/ppc/baseline-assembler-ppc-inl.h
@@ -537,7 +537,8 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   {
     ScratchRegisterScope temps(this);
     Register scratch = temps.AcquireScratch();
-    __ TestCodeTIsMarkedForDeoptimization(scratch_and_result, scratch, r0);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(scratch_and_result,
+                                                      scratch, r0);
     __ beq(on_result, cr0);
     __ mov(scratch, __ ClearedValue());
     StoreTaggedFieldNoWriteBarrier(
diff --git a/src/baseline/riscv/baseline-assembler-riscv-inl.h b/src/baseline/riscv/baseline-assembler-riscv-inl.h
index 59cffa9e292..d3cf5fffee9 100644
--- a/src/baseline/riscv/baseline-assembler-riscv-inl.h
+++ b/src/baseline/riscv/baseline-assembler-riscv-inl.h
@@ -383,7 +383,7 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   // Is it marked_for_deoptimization? If yes, clear the slot.
   {
     ScratchRegisterScope temps(this);
-    __ JumpIfCodeTIsMarkedForDeoptimization(
+    __ JumpIfCodeDataContainerIsMarkedForDeoptimization(
         scratch_and_result, temps.AcquireScratch(), &clear_slot);
     Jump(on_result);
   }
diff --git a/src/baseline/s390/baseline-assembler-s390-inl.h b/src/baseline/s390/baseline-assembler-s390-inl.h
index c04e7734aef..415aae0bb3d 100644
--- a/src/baseline/s390/baseline-assembler-s390-inl.h
+++ b/src/baseline/s390/baseline-assembler-s390-inl.h
@@ -550,7 +550,8 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
   {
     ScratchRegisterScope temps(this);
     Register scratch = temps.AcquireScratch();
-    __ TestCodeTIsMarkedForDeoptimization(scratch_and_result, scratch);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(scratch_and_result,
+                                                      scratch);
     __ beq(on_result);
     __ mov(scratch, __ ClearedValue());
     StoreTaggedFieldNoWriteBarrier(
diff --git a/src/baseline/x64/baseline-assembler-x64-inl.h b/src/baseline/x64/baseline-assembler-x64-inl.h
index fff84a20569..919c8fabd48 100644
--- a/src/baseline/x64/baseline-assembler-x64-inl.h
+++ b/src/baseline/x64/baseline-assembler-x64-inl.h
@@ -410,7 +410,7 @@ void BaselineAssembler::TryLoadOptimizedOsrCode(Register scratch_and_result,
 
   // Is it marked_for_deoptimization? If yes, clear the slot.
   {
-    __ TestCodeTIsMarkedForDeoptimization(scratch_and_result);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(scratch_and_result);
     __ j(equal, on_result, distance);
     __ StoreTaggedField(
         FieldOperand(feedback_vector,
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 034d6f6fe24..d6680be5e29 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -311,8 +311,8 @@ void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {
   Generate_JSBuiltinsConstructStubHelper(masm);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
   __ ldr(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
@@ -327,11 +327,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Label* is_baseline) {
   ASM_CODE_COMMENT(masm);
   Label done;
-  __ CompareObjectType(sfi_data, scratch1, scratch1, CODET_TYPE);
+  __ CompareObjectType(sfi_data, scratch1, scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ b(ne, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ b(eq, is_baseline);
     __ bind(&not_baseline);
   } else {
@@ -631,7 +631,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   //
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   DCHECK_EQ(kPushedStackSpace, pushed_stack_space);
   USE(pushed_stack_space);
@@ -769,9 +769,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     }
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the JS frame and remove the parameters (except function), and
@@ -1458,7 +1458,8 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
     // Tail call to the array construct stub (still in the caller
     // context at this point).
-    Handle<CodeT> code = BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
+    Handle<CodeDataContainer> code =
+        BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
     __ Jump(code, RelocInfo::CODE_TARGET);
   } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // Call the constructor with r0, r1, and r3 unmodified.
@@ -2001,7 +2002,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r1 : target
   //  -- r0 : number of parameters on the stack
@@ -2067,9 +2068,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r0 : the number of arguments
   //  -- r3 : the new.target (for [[Construct]] calls)
@@ -3567,7 +3568,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CompareObjectType(code_obj, r3, r3, CODET_TYPE);
+    __ CompareObjectType(code_obj, r3, r3, CODE_DATA_CONTAINER_TYPE);
     __ b(eq, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -3580,12 +3581,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // Start with baseline code.
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CompareObjectType(code_obj, r3, r3, CODET_TYPE);
+    __ CompareObjectType(code_obj, r3, r3, CODE_DATA_CONTAINER_TYPE);
     __ Assert(eq, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, r3);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, r3);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index 7cfc0359a4f..8abe1e749cf 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -388,19 +388,20 @@ void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) {
   __ Unreachable();
 }
 
-static void AssertCodeTIsBaselineAllowClobber(MacroAssembler* masm,
-                                              Register code, Register scratch) {
+static void AssertCodeDataContainerIsBaselineAllowClobber(MacroAssembler* masm,
+                                                          Register code,
+                                                          Register scratch) {
   // Verify that the code kind is baseline code via the CodeKind.
-  __ Ldr(scratch, FieldMemOperand(code, CodeT::kFlagsOffset));
-  __ DecodeField<CodeT::KindField>(scratch);
+  __ Ldr(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
+  __ DecodeField<CodeDataContainer::KindField>(scratch);
   __ Cmp(scratch, Operand(static_cast<int>(CodeKind::BASELINE)));
   __ Assert(eq, AbortReason::kExpectedBaselineData);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
-  return AssertCodeTIsBaselineAllowClobber(masm, code, scratch);
+  return AssertCodeDataContainerIsBaselineAllowClobber(masm, code, scratch);
 }
 
 // TODO(v8:11429): Add a path for "not_compiled" and unify the two uses under
@@ -411,11 +412,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Label* is_baseline) {
   ASM_CODE_COMMENT(masm);
   Label done;
-  __ CompareObjectType(sfi_data, scratch1, scratch1, CODET_TYPE);
+  __ CompareObjectType(sfi_data, scratch1, scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ B(ne, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ B(eq, is_baseline);
     __ Bind(&not_baseline);
   } else {
@@ -758,7 +759,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   //
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -932,9 +933,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // x28 : pointer cage base register (kPtrComprCageBaseRegister).
     // x29 : frame pointer (fp).
 
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the JS internal frame and remove the parameters (except function),
@@ -2334,7 +2335,7 @@ void Generate_PrepareForCopyingVarargs(MacroAssembler* masm, Register argc,
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- x1 : target
   //  -- x0 : number of parameters on the stack
@@ -2407,9 +2408,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- x0 : the number of arguments
   //  -- x3 : the new.target (for [[Construct]] calls)
@@ -5705,7 +5706,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CompareObjectType(code_obj, x3, x3, CODET_TYPE);
+    __ CompareObjectType(code_obj, x3, x3, CODE_DATA_CONTAINER_TYPE);
     __ B(eq, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -5718,12 +5719,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // Start with baseline code.
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CompareObjectType(code_obj, x3, x3, CODET_TYPE);
+    __ CompareObjectType(code_obj, x3, x3, CODE_DATA_CONTAINER_TYPE);
     __ Assert(eq, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, x3);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, x3);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
diff --git a/src/builtins/builtins-array-gen.cc b/src/builtins/builtins-array-gen.cc
index f72d3793370..9c83d0b2f33 100644
--- a/src/builtins/builtins-array-gen.cc
+++ b/src/builtins/builtins-array-gen.cc
@@ -1808,7 +1808,7 @@ TF_BUILTIN(ArrayConstructor, ArrayBuiltinsAssembler) {
 void ArrayBuiltinsAssembler::TailCallArrayConstructorStub(
     const Callable& callable, TNode<Context> context, TNode<JSFunction> target,
     TNode<HeapObject> allocation_site_or_undefined, TNode<Int32T> argc) {
-  TNode<CodeT> code = HeapConstant(callable.code());
+  TNode<CodeDataContainer> code = HeapConstant(callable.code());
 
   // We are going to call here ArrayNoArgumentsConstructor or
   // ArraySingleArgumentsConstructor which in addition to the register arguments
diff --git a/src/builtins/builtins-async-gen.cc b/src/builtins/builtins-async-gen.cc
index 6336654391d..4cef2403d2f 100644
--- a/src/builtins/builtins-async-gen.cc
+++ b/src/builtins/builtins-async-gen.cc
@@ -180,7 +180,7 @@ void AsyncBuiltinsAssembler::InitializeNativeClosure(
   // which almost doubles the size of `await` builtins (unnecessarily).
   TNode<Smi> builtin_id = LoadObjectField<Smi>(
       shared_info, SharedFunctionInfo::kFunctionDataOffset);
-  TNode<CodeT> code = LoadBuiltin(builtin_id);
+  TNode<CodeDataContainer> code = LoadBuiltin(builtin_id);
   StoreObjectFieldNoWriteBarrier(function, JSFunction::kCodeOffset, code);
 }
 
diff --git a/src/builtins/builtins-constructor-gen.cc b/src/builtins/builtins-constructor-gen.cc
index c90bbddc9ea..e09e4841d98 100644
--- a/src/builtins/builtins-constructor-gen.cc
+++ b/src/builtins/builtins-constructor-gen.cc
@@ -252,7 +252,7 @@ TF_BUILTIN(FastNewClosure, ConstructorBuiltinsAssembler) {
   StoreObjectFieldNoWriteBarrier(result, JSFunction::kSharedFunctionInfoOffset,
                                  shared_function_info);
   StoreObjectFieldNoWriteBarrier(result, JSFunction::kContextOffset, context);
-  TNode<CodeT> lazy_builtin =
+  TNode<CodeDataContainer> lazy_builtin =
       HeapConstant(BUILTIN_CODE(isolate(), CompileLazy));
   StoreObjectFieldNoWriteBarrier(result, JSFunction::kCodeOffset, lazy_builtin);
   Return(result);
diff --git a/src/builtins/builtins-internal-gen.cc b/src/builtins/builtins-internal-gen.cc
index 81090d7ba10..7f883ff09c0 100644
--- a/src/builtins/builtins-internal-gen.cc
+++ b/src/builtins/builtins-internal-gen.cc
@@ -104,7 +104,7 @@ TF_BUILTIN(DebugBreakTrampoline, CodeStubAssembler) {
 
   BIND(&tailcall_to_shared);
   // Tail call into code object on the SharedFunctionInfo.
-  TNode<CodeT> code = GetSharedFunctionInfoCode(shared);
+  TNode<CodeDataContainer> code = GetSharedFunctionInfoCode(shared);
   TailCallJSCode(code, context, function, new_target, arg_count);
 }
 
@@ -1230,7 +1230,7 @@ TF_BUILTIN(AdaptorWithBuiltinExitFrame, CodeStubAssembler) {
       Int32Constant(BuiltinExitFrameConstants::kNumExtraArgsWithoutReceiver));
 
   const bool builtin_exit_frame = true;
-  TNode<CodeT> code = HeapConstant(
+  TNode<CodeDataContainer> code = HeapConstant(
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame));
 
   // Unconditionally push argc, target and new target as extra stack arguments.
@@ -1568,7 +1568,7 @@ TF_BUILTIN(InstantiateAsmJs, CodeStubAssembler) {
   // On failure, tail call back to regular JavaScript by re-calling the given
   // function which has been reset to the compile lazy builtin.
 
-  TNode<CodeT> code = LoadJSFunctionCode(function);
+  TNode<CodeDataContainer> code = LoadJSFunctionCode(function);
   TailCallJSCode(code, context, function, new_target, arg_count);
 }
 
diff --git a/src/builtins/builtins-lazy-gen.cc b/src/builtins/builtins-lazy-gen.cc
index 1c283a353ef..400dbde9e74 100644
--- a/src/builtins/builtins-lazy-gen.cc
+++ b/src/builtins/builtins-lazy-gen.cc
@@ -15,7 +15,7 @@ namespace v8 {
 namespace internal {
 
 void LazyBuiltinsAssembler::GenerateTailCallToJSCode(
-    TNode<CodeT> code, TNode<JSFunction> function) {
+    TNode<CodeDataContainer> code, TNode<JSFunction> function) {
   auto argc = UncheckedParameter<Int32T>(Descriptor::kActualArgumentsCount);
   auto context = Parameter<Context>(Descriptor::kContext);
   auto new_target = Parameter<Object>(Descriptor::kNewTarget);
@@ -25,7 +25,8 @@ void LazyBuiltinsAssembler::GenerateTailCallToJSCode(
 void LazyBuiltinsAssembler::GenerateTailCallToReturnedCode(
     Runtime::FunctionId function_id, TNode<JSFunction> function) {
   auto context = Parameter<Context>(Descriptor::kContext);
-  TNode<CodeT> code = CAST(CallRuntime(function_id, context, function));
+  TNode<CodeDataContainer> code =
+      CAST(CallRuntime(function_id, context, function));
   GenerateTailCallToJSCode(code, function);
 }
 
@@ -63,8 +64,8 @@ void LazyBuiltinsAssembler::MaybeTailCallOptimizedCodeSlot(
     TNode<MaybeObject> maybe_optimized_code_entry = LoadMaybeWeakObjectField(
         feedback_vector, FeedbackVector::kMaybeOptimizedCodeOffset);
 
-    // Optimized code slot is a weak reference to CodeT object.
-    TNode<CodeT> optimized_code = CAST(GetHeapObjectAssumeWeak(
+    // Optimized code slot is a weak reference to CodeDataContainer object.
+    TNode<CodeDataContainer> optimized_code = CAST(GetHeapObjectAssumeWeak(
         maybe_optimized_code_entry, &heal_optimized_code_slot));
 
     // Check if the optimized code is marked for deopt. If it is, call the
@@ -100,7 +101,7 @@ void LazyBuiltinsAssembler::CompileLazy(TNode<JSFunction> function) {
   TNode<SharedFunctionInfo> shared =
       CAST(LoadObjectField(function, JSFunction::kSharedFunctionInfoOffset));
   TVARIABLE(Uint16T, sfi_data_type);
-  TNode<CodeT> sfi_code =
+  TNode<CodeDataContainer> sfi_code =
       GetSharedFunctionInfoCode(shared, &sfi_data_type, &compile_function);
 
   TNode<HeapObject> feedback_cell_value = LoadFeedbackCellValue(function);
@@ -129,17 +130,18 @@ void LazyBuiltinsAssembler::CompileLazy(TNode<JSFunction> function) {
   // be the InterpreterEntryTrampoline to start executing existing bytecode.
   BIND(&maybe_use_sfi_code);
   Label tailcall_code(this), baseline(this);
-  TVARIABLE(CodeT, code);
+  TVARIABLE(CodeDataContainer, code);
 
   // Check if we have baseline code.
-  GotoIf(InstanceTypeEqual(sfi_data_type.value(), CODET_TYPE), &baseline);
+  GotoIf(InstanceTypeEqual(sfi_data_type.value(), CODE_DATA_CONTAINER_TYPE),
+         &baseline);
 
   code = sfi_code;
   Goto(&tailcall_code);
 
   BIND(&baseline);
   // Ensure we have a feedback vector.
-  code = Select<CodeT>(
+  code = Select<CodeDataContainer>(
       IsFeedbackVector(feedback_cell_value), [=]() { return sfi_code; },
       [=]() {
         return CAST(CallRuntime(Runtime::kInstallBaselineCode,
@@ -164,7 +166,8 @@ TF_BUILTIN(CompileLazy, LazyBuiltinsAssembler) {
 TF_BUILTIN(CompileLazyDeoptimizedCode, LazyBuiltinsAssembler) {
   auto function = Parameter<JSFunction>(Descriptor::kTarget);
 
-  TNode<CodeT> code = HeapConstant(BUILTIN_CODE(isolate(), CompileLazy));
+  TNode<CodeDataContainer> code =
+      HeapConstant(BUILTIN_CODE(isolate(), CompileLazy));
   // Set the code slot inside the JSFunction to CompileLazy.
   StoreObjectField(function, JSFunction::kCodeOffset, code);
   GenerateTailCallToJSCode(code, function);
diff --git a/src/builtins/builtins-lazy-gen.h b/src/builtins/builtins-lazy-gen.h
index b7dcbb71d98..1beac6dcfb1 100644
--- a/src/builtins/builtins-lazy-gen.h
+++ b/src/builtins/builtins-lazy-gen.h
@@ -17,7 +17,8 @@ class LazyBuiltinsAssembler : public CodeStubAssembler {
   explicit LazyBuiltinsAssembler(compiler::CodeAssemblerState* state)
       : CodeStubAssembler(state) {}
 
-  void GenerateTailCallToJSCode(TNode<CodeT> code, TNode<JSFunction> function);
+  void GenerateTailCallToJSCode(TNode<CodeDataContainer> code,
+                                TNode<JSFunction> function);
 
   void GenerateTailCallToReturnedCode(Runtime::FunctionId function_id,
                                       TNode<JSFunction> function);
diff --git a/src/builtins/builtins-regexp-gen.cc b/src/builtins/builtins-regexp-gen.cc
index ac09d862f94..b7af7d08656 100644
--- a/src/builtins/builtins-regexp-gen.cc
+++ b/src/builtins/builtins-regexp-gen.cc
@@ -45,12 +45,6 @@ TNode<IntPtrT> RegExpBuiltinsAssembler::IntPtrZero() {
   return IntPtrConstant(0);
 }
 
-// If code is a builtin, return the address to the (possibly embedded) builtin
-// code entry, otherwise return the entry of the code object itself.
-TNode<RawPtrT> RegExpBuiltinsAssembler::LoadCodeObjectEntry(TNode<CodeT> code) {
-  return GetCodeEntry(code);
-}
-
 // -----------------------------------------------------------------------------
 // ES6 section 21.2 RegExp Objects
 
@@ -522,7 +516,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
 #endif
 
   GotoIf(TaggedIsSmi(var_code.value()), &runtime);
-  TNode<CodeT> code = CAST(var_code.value());
+  TNode<CodeDataContainer> code = CAST(var_code.value());
 
   Label if_success(this), if_exception(this, Label::kDeferred);
   {
@@ -586,7 +580,7 @@ TNode<HeapObject> RegExpBuiltinsAssembler::RegExpExecInternal(
     MachineType arg8_type = type_tagged;
     TNode<JSRegExp> arg8 = regexp;
 
-    TNode<RawPtrT> code_entry = LoadCodeObjectEntry(code);
+    TNode<RawPtrT> code_entry = GetCodeEntry(code);
 
     // AIX uses function descriptors on CFunction calls. code_entry in this case
     // may also point to a Regex interpreter entry trampoline which does not
diff --git a/src/builtins/builtins-regexp-gen.h b/src/builtins/builtins-regexp-gen.h
index f8944935611..96497a0a99d 100644
--- a/src/builtins/builtins-regexp-gen.h
+++ b/src/builtins/builtins-regexp-gen.h
@@ -21,8 +21,6 @@ class RegExpBuiltinsAssembler : public CodeStubAssembler {
   TNode<Smi> SmiZero();
   TNode<IntPtrT> IntPtrZero();
 
-  TNode<RawPtrT> LoadCodeObjectEntry(TNode<CodeT> code);
-
   // Allocate either a JSRegExpResult or a JSRegExpResultWithIndices (depending
   // on has_indices) with the given length (the number of captures, including
   // the match itself), index (the index where the match starts), and input
diff --git a/src/builtins/builtins.cc b/src/builtins/builtins.cc
index 6818115fd4f..e34044d38d4 100644
--- a/src/builtins/builtins.cc
+++ b/src/builtins/builtins.cc
@@ -121,7 +121,7 @@ const char* Builtins::Lookup(Address pc) {
   return nullptr;
 }
 
-Handle<CodeT> Builtins::CallFunction(ConvertReceiverMode mode) {
+Handle<CodeDataContainer> Builtins::CallFunction(ConvertReceiverMode mode) {
   switch (mode) {
     case ConvertReceiverMode::kNullOrUndefined:
       return code_handle(Builtin::kCallFunction_ReceiverIsNullOrUndefined);
@@ -133,7 +133,7 @@ Handle<CodeT> Builtins::CallFunction(ConvertReceiverMode mode) {
   UNREACHABLE();
 }
 
-Handle<CodeT> Builtins::Call(ConvertReceiverMode mode) {
+Handle<CodeDataContainer> Builtins::Call(ConvertReceiverMode mode) {
   switch (mode) {
     case ConvertReceiverMode::kNullOrUndefined:
       return code_handle(Builtin::kCall_ReceiverIsNullOrUndefined);
@@ -145,7 +145,8 @@ Handle<CodeT> Builtins::Call(ConvertReceiverMode mode) {
   UNREACHABLE();
 }
 
-Handle<CodeT> Builtins::NonPrimitiveToPrimitive(ToPrimitiveHint hint) {
+Handle<CodeDataContainer> Builtins::NonPrimitiveToPrimitive(
+    ToPrimitiveHint hint) {
   switch (hint) {
     case ToPrimitiveHint::kDefault:
       return code_handle(Builtin::kNonPrimitiveToPrimitive_Default);
@@ -157,7 +158,8 @@ Handle<CodeT> Builtins::NonPrimitiveToPrimitive(ToPrimitiveHint hint) {
   UNREACHABLE();
 }
 
-Handle<CodeT> Builtins::OrdinaryToPrimitive(OrdinaryToPrimitiveHint hint) {
+Handle<CodeDataContainer> Builtins::OrdinaryToPrimitive(
+    OrdinaryToPrimitiveHint hint) {
   switch (hint) {
     case OrdinaryToPrimitiveHint::kNumber:
       return code_handle(Builtin::kOrdinaryToPrimitive_Number);
@@ -179,21 +181,21 @@ FullObjectSlot Builtins::builtin_tier0_slot(Builtin builtin) {
   return FullObjectSlot(location);
 }
 
-void Builtins::set_code(Builtin builtin, CodeT code) {
+void Builtins::set_code(Builtin builtin, CodeDataContainer code) {
   DCHECK_EQ(builtin, code.builtin_id());
   DCHECK(Internals::HasHeapObjectTag(code.ptr()));
   // The given builtin may be uninitialized thus we cannot check its type here.
   isolate_->builtin_table()[Builtins::ToInt(builtin)] = code.ptr();
 }
 
-CodeT Builtins::code(Builtin builtin) {
+CodeDataContainer Builtins::code(Builtin builtin) {
   Address ptr = isolate_->builtin_table()[Builtins::ToInt(builtin)];
-  return CodeT::cast(Object(ptr));
+  return CodeDataContainer::cast(Object(ptr));
 }
 
-Handle<CodeT> Builtins::code_handle(Builtin builtin) {
+Handle<CodeDataContainer> Builtins::code_handle(Builtin builtin) {
   Address* location = &isolate_->builtin_table()[Builtins::ToInt(builtin)];
-  return Handle<CodeT>(location);
+  return Handle<CodeDataContainer>(location);
 }
 
 // static
@@ -229,7 +231,7 @@ CallInterfaceDescriptor Builtins::CallInterfaceDescriptorFor(Builtin builtin) {
 
 // static
 Callable Builtins::CallableFor(Isolate* isolate, Builtin builtin) {
-  Handle<CodeT> code = isolate->builtins()->code_handle(builtin);
+  Handle<CodeDataContainer> code = isolate->builtins()->code_handle(builtin);
   return Callable{code, CallInterfaceDescriptorFor(builtin)};
 }
 
@@ -256,7 +258,7 @@ void Builtins::PrintBuiltinCode() {
                      base::CStrVector(v8_flags.print_builtin_code_filter))) {
       CodeTracer::Scope trace_scope(isolate_->GetCodeTracer());
       OFStream os(trace_scope.file());
-      CodeT builtin_code = code(builtin);
+      CodeDataContainer builtin_code = code(builtin);
       builtin_code.Disassemble(builtin_name, os, isolate_);
       os << "\n";
     }
@@ -270,7 +272,7 @@ void Builtins::PrintBuiltinSize() {
        ++builtin) {
     const char* builtin_name = name(builtin);
     const char* kind = KindNameOf(builtin);
-    CodeT code = Builtins::code(builtin);
+    CodeDataContainer code = Builtins::code(builtin);
     PrintF(stdout, "%s Builtin, %s, %d\n", kind, builtin_name,
            code.InstructionSize());
   }
@@ -331,7 +333,7 @@ void Builtins::EmitCodeCreateEvents(Isolate* isolate) {
   int i = 0;
   HandleScope scope(isolate);
   for (; i < ToInt(Builtin::kFirstBytecodeHandler); i++) {
-    Handle<CodeT> builtin_code(&builtins[i]);
+    Handle<CodeDataContainer> builtin_code(&builtins[i]);
     Handle<AbstractCode> code = ToAbstractCode(builtin_code, isolate);
     PROFILE(isolate, CodeCreateEvent(LogEventListener::CodeTag::kBuiltin, code,
                                      Builtins::name(FromInt(i))));
@@ -339,7 +341,7 @@ void Builtins::EmitCodeCreateEvents(Isolate* isolate) {
 
   static_assert(kLastBytecodeHandlerPlusOne == kBuiltinCount);
   for (; i < kBuiltinCount; i++) {
-    Handle<CodeT> builtin_code(&builtins[i]);
+    Handle<CodeDataContainer> builtin_code(&builtins[i]);
     Handle<AbstractCode> code = ToAbstractCode(builtin_code, isolate);
     interpreter::Bytecode bytecode =
         builtin_metadata[i].data.bytecode_and_scale.bytecode;
diff --git a/src/builtins/builtins.h b/src/builtins/builtins.h
index 8098a0e64cf..ba8e393a670 100644
--- a/src/builtins/builtins.h
+++ b/src/builtins/builtins.h
@@ -138,17 +138,19 @@ class Builtins {
   }
 
   // Convenience wrappers.
-  Handle<CodeT> CallFunction(ConvertReceiverMode = ConvertReceiverMode::kAny);
-  Handle<CodeT> Call(ConvertReceiverMode = ConvertReceiverMode::kAny);
-  Handle<CodeT> NonPrimitiveToPrimitive(
+  Handle<CodeDataContainer> CallFunction(
+      ConvertReceiverMode = ConvertReceiverMode::kAny);
+  Handle<CodeDataContainer> Call(
+      ConvertReceiverMode = ConvertReceiverMode::kAny);
+  Handle<CodeDataContainer> NonPrimitiveToPrimitive(
       ToPrimitiveHint hint = ToPrimitiveHint::kDefault);
-  Handle<CodeT> OrdinaryToPrimitive(OrdinaryToPrimitiveHint hint);
+  Handle<CodeDataContainer> OrdinaryToPrimitive(OrdinaryToPrimitiveHint hint);
 
   // Used by CreateOffHeapTrampolines in isolate.cc.
-  void set_code(Builtin builtin, CodeT code);
+  void set_code(Builtin builtin, CodeDataContainer code);
 
-  V8_EXPORT_PRIVATE CodeT code(Builtin builtin);
-  V8_EXPORT_PRIVATE Handle<CodeT> code_handle(Builtin builtin);
+  V8_EXPORT_PRIVATE CodeDataContainer code(Builtin builtin);
+  V8_EXPORT_PRIVATE Handle<CodeDataContainer> code_handle(Builtin builtin);
 
   static CallInterfaceDescriptor CallInterfaceDescriptorFor(Builtin builtin);
   V8_EXPORT_PRIVATE static Callable CallableFor(Isolate* isolate,
@@ -192,8 +194,8 @@ class Builtins {
   }
 
   // True, iff the given code object is a builtin with off-heap embedded code.
-  template <typename CodeOrCodeT>
-  static bool IsIsolateIndependentBuiltin(CodeOrCodeT code) {
+  template <typename CodeOrCodeDataContainer>
+  static bool IsIsolateIndependentBuiltin(CodeOrCodeDataContainer code) {
     Builtin builtin = code.builtin_id();
     return Builtins::IsBuiltinId(builtin) &&
            Builtins::IsIsolateIndependent(builtin);
@@ -287,10 +289,10 @@ class Builtins {
 
   enum class CallOrConstructMode { kCall, kConstruct };
   static void Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                              Handle<CodeT> code);
-  static void Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                     CallOrConstructMode mode,
-                                                     Handle<CodeT> code);
+                                              Handle<CodeDataContainer> code);
+  static void Generate_CallOrConstructForwardVarargs(
+      MacroAssembler* masm, CallOrConstructMode mode,
+      Handle<CodeDataContainer> code);
 
   enum class InterpreterEntryTrampolineMode {
     // The version of InterpreterEntryTrampoline used by default.
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index 0ec23fdafe4..75b21fd37f5 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -414,7 +414,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -513,9 +513,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ mov(edi, Operand(scratch2, EntryFrameConstants::kFunctionArgOffset));
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the internal frame. Notice that this also removes the empty.
@@ -555,12 +555,12 @@ static void GetSharedFunctionInfoBytecode(MacroAssembler* masm,
   __ bind(&done);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
-  __ mov(scratch, FieldOperand(code, CodeT::kFlagsOffset));
-  __ DecodeField<CodeT::KindField>(scratch);
+  __ mov(scratch, FieldOperand(code, CodeDataContainer::kFlagsOffset));
+  __ DecodeField<CodeDataContainer::KindField>(scratch);
   __ cmp(scratch, Immediate(static_cast<int>(CodeKind::BASELINE)));
   __ Assert(equal, AbortReason::kExpectedBaselineData);
 }
@@ -573,11 +573,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   Label done;
   __ LoadMap(scratch1, sfi_data);
 
-  __ CmpInstanceType(scratch1, CODET_TYPE);
+  __ CmpInstanceType(scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ j(not_equal, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ j(equal, is_baseline);
     __ bind(&not_baseline);
   } else {
@@ -689,7 +689,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
 
     __ bind(&is_baseline);
     __ Pop(eax);
-    __ CmpObjectType(ecx, CODET_TYPE, ecx);
+    __ CmpObjectType(ecx, CODE_DATA_CONTAINER_TYPE, ecx);
     __ Assert(equal, AbortReason::kMissingBytecodeArray);
 
     __ bind(&ok);
@@ -2054,7 +2054,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- edi    : target
   //  -- esi    : context for the Call / Construct builtin
@@ -2147,9 +2147,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- eax : the number of arguments
   //  -- edi : the target to call (can be any Object)
@@ -4208,7 +4208,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CmpObjectType(code_obj, CODET_TYPE, kInterpreterBytecodeOffsetRegister);
+    __ CmpObjectType(code_obj, CODE_DATA_CONTAINER_TYPE,
+                     kInterpreterBytecodeOffsetRegister);
     __ j(equal, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -4221,12 +4222,13 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
 
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CmpObjectType(code_obj, CODET_TYPE, kInterpreterBytecodeOffsetRegister);
+    __ CmpObjectType(code_obj, CODE_DATA_CONTAINER_TYPE,
+                     kInterpreterBytecodeOffsetRegister);
     __ Assert(equal, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, ecx);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, ecx);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
diff --git a/src/builtins/loong64/builtins-loong64.cc b/src/builtins/loong64/builtins-loong64.cc
index edc87d8014f..65e85fbc0d3 100644
--- a/src/builtins/loong64/builtins-loong64.cc
+++ b/src/builtins/loong64/builtins-loong64.cc
@@ -299,12 +299,12 @@ void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {
   Generate_JSBuiltinsConstructStubHelper(masm);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
-  __ Ld_d(scratch, FieldMemOperand(code, CodeT::kFlagsOffset));
-  __ DecodeField<CodeT::KindField>(scratch);
+  __ Ld_d(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
+  __ DecodeField<CodeDataContainer::KindField>(scratch);
   __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,
             Operand(static_cast<int>(CodeKind::BASELINE)));
 }
@@ -320,12 +320,12 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   __ GetObjectType(sfi_data, scratch1, scratch1);
   if (v8_flags.debug_code) {
     Label not_baseline;
-    __ Branch(&not_baseline, ne, scratch1, Operand(CODET_TYPE));
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    __ Branch(&not_baseline, ne, scratch1, Operand(CODE_DATA_CONTAINER_TYPE));
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ Branch(is_baseline);
     __ bind(&not_baseline);
   } else {
-    __ Branch(is_baseline, eq, scratch1, Operand(CODET_TYPE));
+    __ Branch(is_baseline, eq, scratch1, Operand(CODE_DATA_CONTAINER_TYPE));
   }
   __ Branch(&done, ne, scratch1, Operand(INTERPRETER_DATA_TYPE));
   __ Ld_d(sfi_data,
@@ -648,7 +648,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
 
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -754,9 +754,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // s7 is cp. Do not init.
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Leave internal frame.
@@ -2012,7 +2012,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 
 // static
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- a1 : target
   //  -- a0 : number of parameters on the stack
@@ -2081,9 +2081,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- a0 : the number of arguments
   //  -- a3 : the new.target (for [[Construct]] calls)
@@ -3586,7 +3586,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (!is_osr) {
     Label start_with_baseline;
     __ GetObjectType(code_obj, t2, t2);
-    __ Branch(&start_with_baseline, eq, t2, Operand(CODET_TYPE));
+    __ Branch(&start_with_baseline, eq, t2, Operand(CODE_DATA_CONTAINER_TYPE));
 
     // Start with bytecode as there is no baseline code.
     Builtin builtin_id = next_bytecode
@@ -3599,11 +3599,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
     __ GetObjectType(code_obj, t2, t2);
-    __ Assert(eq, AbortReason::kExpectedBaselineData, t2, Operand(CODET_TYPE));
+    __ Assert(eq, AbortReason::kExpectedBaselineData, t2,
+              Operand(CODE_DATA_CONTAINER_TYPE));
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, t2);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, t2);
   }
 
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index 890b06b46b8..09ff9eb59f9 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -299,12 +299,12 @@ void Builtins::Generate_JSBuiltinsConstructStub(MacroAssembler* masm) {
   Generate_JSBuiltinsConstructStubHelper(masm);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
-  __ Ld(scratch, FieldMemOperand(code, CodeT::kFlagsOffset));
-  __ DecodeField<CodeT::KindField>(scratch);
+  __ Ld(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
+  __ DecodeField<CodeDataContainer::KindField>(scratch);
   __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,
             Operand(static_cast<int>(CodeKind::BASELINE)));
 }
@@ -320,12 +320,12 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   __ GetObjectType(sfi_data, scratch1, scratch1);
   if (v8_flags.debug_code) {
     Label not_baseline;
-    __ Branch(&not_baseline, ne, scratch1, Operand(CODET_TYPE));
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    __ Branch(&not_baseline, ne, scratch1, Operand(CODE_DATA_CONTAINER_TYPE));
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ Branch(is_baseline);
     __ bind(&not_baseline);
   } else {
-    __ Branch(is_baseline, eq, scratch1, Operand(CODET_TYPE));
+    __ Branch(is_baseline, eq, scratch1, Operand(CODE_DATA_CONTAINER_TYPE));
   }
   __ Branch(&done, ne, scratch1, Operand(INTERPRETER_DATA_TYPE));
   __ Ld(sfi_data,
@@ -649,7 +649,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
 
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -755,9 +755,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     // s7 is cp. Do not init.
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Leave internal frame.
@@ -2005,7 +2005,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 
 // static
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- a1 : target
   //  -- a0 : number of parameters on the stack
@@ -2074,9 +2074,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- a0 : the number of arguments
   //  -- a3 : the new.target (for [[Construct]] calls)
@@ -3610,7 +3610,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   if (!is_osr) {
     Label start_with_baseline;
     __ GetObjectType(code_obj, t2, t2);
-    __ Branch(&start_with_baseline, eq, t2, Operand(CODET_TYPE));
+    __ Branch(&start_with_baseline, eq, t2, Operand(CODE_DATA_CONTAINER_TYPE));
 
     // Start with bytecode as there is no baseline code.
     Builtin builtin_id = next_bytecode
@@ -3623,11 +3623,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
     __ GetObjectType(code_obj, t2, t2);
-    __ Assert(eq, AbortReason::kExpectedBaselineData, t2, Operand(CODET_TYPE));
+    __ Assert(eq, AbortReason::kExpectedBaselineData, t2,
+              Operand(CODE_DATA_CONTAINER_TYPE));
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, t2);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, t2);
   }
 
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index cb8a1c6a304..f8d00eb74d5 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -35,8 +35,8 @@ namespace internal {
 #define __ ACCESS_MASM(masm)
 namespace {
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
   __ LoadU32(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
@@ -52,11 +52,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   USE(GetSharedFunctionInfoBytecodeOrBaseline);
   ASM_CODE_COMMENT(masm);
   Label done;
-  __ CompareObjectType(sfi_data, scratch1, scratch1, CODET_TYPE);
+  __ CompareObjectType(sfi_data, scratch1, scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ b(ne, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ beq(is_baseline);
     __ bind(&not_baseline);
   } else {
@@ -131,7 +131,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CompareObjectType(code_obj, r6, r6, CODET_TYPE);
+    __ CompareObjectType(code_obj, r6, r6, CODE_DATA_CONTAINER_TYPE);
     __ b(eq, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -144,12 +144,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // Start with baseline code.
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CompareObjectType(code_obj, r6, r6, CODET_TYPE);
+    __ CompareObjectType(code_obj, r6, r6, CODE_DATA_CONTAINER_TYPE);
     __ Assert(eq, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, r6);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, r6);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
@@ -940,7 +940,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -1056,9 +1056,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ mr(r17, r7);
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the JS frame and remove the parameters (except function), and
@@ -1734,7 +1734,8 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
     // Tail call to the array construct stub (still in the caller
     // context at this point).
-    Handle<CodeT> code = BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
+    Handle<CodeDataContainer> code =
+        BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
     __ Jump(code, RelocInfo::CODE_TARGET);
   } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // Call the constructor with r3, r4, and r6 unmodified.
@@ -2218,7 +2219,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r4 : target
   //  -- r3 : number of parameters on the stack
@@ -2289,9 +2290,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r3 : the number of arguments
   //  -- r6 : the new.target (for [[Construct]] calls)
diff --git a/src/builtins/riscv/builtins-riscv.cc b/src/builtins/riscv/builtins-riscv.cc
index 95deaed31f0..b5bae021611 100644
--- a/src/builtins/riscv/builtins-riscv.cc
+++ b/src/builtins/riscv/builtins-riscv.cc
@@ -341,7 +341,7 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   Label done;
 
   __ GetObjectType(sfi_data, scratch1, scratch1);
-  __ Branch(is_baseline, eq, scratch1, Operand(CODET_TYPE));
+  __ Branch(is_baseline, eq, scratch1, Operand(CODE_DATA_CONTAINER_TYPE));
 
   __ Branch(&done, ne, scratch1, Operand(INTERPRETER_DATA_TYPE),
             Label::Distance::kNear);
@@ -3664,7 +3664,8 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     UseScratchRegisterScope temps(masm);
     Register scratch = temps.Acquire();
     __ GetObjectType(code_obj, scratch, scratch);
-    __ Branch(&start_with_baseline, eq, scratch, Operand(CODET_TYPE));
+    __ Branch(&start_with_baseline, eq, scratch,
+              Operand(CODE_DATA_CONTAINER_TYPE));
 
     // Start with bytecode as there is no baseline code.
     Builtin builtin_id = next_bytecode
@@ -3680,7 +3681,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     Register scratch = temps.Acquire();
     __ GetObjectType(code_obj, scratch, scratch);
     __ Assert(eq, AbortReason::kExpectedBaselineData, scratch,
-              Operand(CODET_TYPE));
+              Operand(CODE_DATA_CONTAINER_TYPE));
   }
   if (v8_flags.debug_code) {
     UseScratchRegisterScope temps(masm);
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index 8bda6e3e3ff..6fab790f932 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -36,8 +36,8 @@ namespace internal {
 
 namespace {
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
   // Verify that the code kind is baseline code via the CodeKind.
   __ LoadU32(scratch, FieldMemOperand(code, CodeDataContainer::kFlagsOffset));
@@ -53,11 +53,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   USE(GetSharedFunctionInfoBytecodeOrBaseline);
   ASM_CODE_COMMENT(masm);
   Label done;
-  __ CompareObjectType(sfi_data, scratch1, scratch1, CODET_TYPE);
+  __ CompareObjectType(sfi_data, scratch1, scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ b(ne, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ beq(is_baseline);
     __ bind(&not_baseline);
   } else {
@@ -131,7 +131,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CompareObjectType(code_obj, r5, r5, CODET_TYPE);
+    __ CompareObjectType(code_obj, r5, r5, CODE_DATA_CONTAINER_TYPE);
     __ b(eq, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -144,12 +144,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // Start with baseline code.
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CompareObjectType(code_obj, r5, r5, CODET_TYPE);
+    __ CompareObjectType(code_obj, r5, r5, CODE_DATA_CONTAINER_TYPE);
     __ Assert(eq, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, r5);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, r5);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
@@ -937,7 +937,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   USE(pushed_stack_space);
   DCHECK_EQ(kPushedStackSpace, pushed_stack_space);
@@ -1087,9 +1087,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ mov(r9, r6);
 
     // Invoke the code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the JS frame and remove the parameters (except function), and
@@ -1756,7 +1756,8 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
 
     // Tail call to the array construct stub (still in the caller
     // context at this point).
-    Handle<CodeT> code = BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
+    Handle<CodeDataContainer> code =
+        BUILTIN_CODE(masm->isolate(), ArrayConstructorImpl);
     __ Jump(code, RelocInfo::CODE_TARGET);
   } else if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     // Call the constructor with r2, r3, and r5 unmodified.
@@ -2221,7 +2222,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r3 : target
   //  -- r2 : number of parameters on the stack
@@ -2293,9 +2294,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- r2 : the number of arguments
   //  -- r5 : the new.target (for [[Construct]] calls)
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index a54c2c466cb..08a71f0d398 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -211,7 +211,7 @@ Code BuildWithCodeStubAssemblerCS(Isolate* isolate, Builtin builtin,
 void SetupIsolateDelegate::AddBuiltin(Builtins* builtins, Builtin builtin,
                                       Code code) {
   DCHECK_EQ(builtin, code.builtin_id());
-  builtins->set_code(builtin, ToCodeT(code));
+  builtins->set_code(builtin, ToCodeDataContainer(code));
 }
 
 // static
@@ -242,7 +242,7 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
   PtrComprCageBase cage_base(isolate);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = FromCodeT(builtins->code(builtin));
+    Code code = FromCodeDataContainer(builtins->code(builtin));
     isolate->heap()->UnprotectAndRegisterMemoryChunk(
         code, UnprotectMemoryOrigin::kMainThread);
     bool flush_icache = false;
@@ -253,16 +253,16 @@ void SetupIsolateDelegate::ReplacePlaceholders(Isolate* isolate) {
         DCHECK_IMPLIES(RelocInfo::IsRelativeCodeTarget(rinfo->rmode()),
                        Builtins::IsIsolateIndependent(target.builtin_id()));
         if (!target.is_builtin()) continue;
-        CodeT new_target = builtins->code(target.builtin_id());
+        CodeDataContainer new_target = builtins->code(target.builtin_id());
         rinfo->set_target_address(new_target.raw_instruction_start(),
                                   UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       } else {
         DCHECK(RelocInfo::IsEmbeddedObjectMode(rinfo->rmode()));
         Object object = rinfo->target_object(cage_base);
-        if (!object.IsCodeT(cage_base)) continue;
-        CodeT target = CodeT::cast(object);
+        if (!object.IsCodeDataContainer(cage_base)) continue;
+        CodeDataContainer target = CodeDataContainer::cast(object);
         if (!target.is_builtin()) continue;
-        CodeT new_target = builtins->code(target.builtin_id());
+        CodeDataContainer new_target = builtins->code(target.builtin_id());
         rinfo->set_target_object(isolate->heap(), new_target,
                                  UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
       }
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index e18754dfb0a..9cf3eeba2a2 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -464,7 +464,7 @@ void Generate_JSEntryVariant(MacroAssembler* masm, StackFrame::Type type,
 
   // Invoke the function by calling through JS entry trampoline builtin and
   // pop the faked function when we return.
-  Handle<CodeT> trampoline_code =
+  Handle<CodeDataContainer> trampoline_code =
       masm->isolate()->builtins()->code_handle(entry_trampoline);
   __ Call(trampoline_code, RelocInfo::CODE_TARGET);
 
@@ -637,9 +637,9 @@ static void Generate_JSEntryTrampolineHelper(MacroAssembler* masm,
     __ Push(r9);
 
     // Invoke the builtin code.
-    Handle<CodeT> builtin = is_construct
-                                ? BUILTIN_CODE(masm->isolate(), Construct)
-                                : masm->isolate()->builtins()->Call();
+    Handle<CodeDataContainer> builtin =
+        is_construct ? BUILTIN_CODE(masm->isolate(), Construct)
+                     : masm->isolate()->builtins()->Call();
     __ Call(builtin, RelocInfo::CODE_TARGET);
 
     // Exit the internal frame. Notice that this also removes the empty
@@ -664,19 +664,20 @@ void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {
   __ Jump(BUILTIN_CODE(masm->isolate(), RunMicrotasks), RelocInfo::CODE_TARGET);
 }
 
-static void AssertCodeTIsBaselineAllowClobber(MacroAssembler* masm,
-                                              Register code, Register scratch) {
+static void AssertCodeDataContainerIsBaselineAllowClobber(MacroAssembler* masm,
+                                                          Register code,
+                                                          Register scratch) {
   // Verify that the code kind is baseline code via the CodeKind.
-  __ movl(scratch, FieldOperand(code, CodeT::kFlagsOffset));
-  __ DecodeField<CodeT::KindField>(scratch);
+  __ movl(scratch, FieldOperand(code, CodeDataContainer::kFlagsOffset));
+  __ DecodeField<CodeDataContainer::KindField>(scratch);
   __ cmpl(scratch, Immediate(static_cast<int>(CodeKind::BASELINE)));
   __ Assert(equal, AbortReason::kExpectedBaselineData);
 }
 
-static void AssertCodeTIsBaseline(MacroAssembler* masm, Register code,
-                                  Register scratch) {
+static void AssertCodeDataContainerIsBaseline(MacroAssembler* masm,
+                                              Register code, Register scratch) {
   DCHECK(!AreAliased(code, scratch));
-  return AssertCodeTIsBaselineAllowClobber(masm, code, scratch);
+  return AssertCodeDataContainerIsBaselineAllowClobber(masm, code, scratch);
 }
 
 static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
@@ -687,11 +688,11 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
   Label done;
   __ LoadMap(scratch1, sfi_data);
 
-  __ CmpInstanceType(scratch1, CODET_TYPE);
+  __ CmpInstanceType(scratch1, CODE_DATA_CONTAINER_TYPE);
   if (v8_flags.debug_code) {
     Label not_baseline;
     __ j(not_equal, &not_baseline);
-    AssertCodeTIsBaseline(masm, sfi_data, scratch1);
+    AssertCodeDataContainerIsBaseline(masm, sfi_data, scratch1);
     __ j(equal, is_baseline);
     __ bind(&not_baseline);
   } else {
@@ -807,7 +808,7 @@ void Builtins::Generate_ResumeGeneratorTrampoline(MacroAssembler* masm) {
     __ jmp(&ok);
 
     __ bind(&is_baseline);
-    __ CmpObjectType(rcx, CODET_TYPE, rcx);
+    __ CmpObjectType(rcx, CODE_DATA_CONTAINER_TYPE, rcx);
     __ Assert(equal, AbortReason::kMissingBytecodeArray);
 
     __ bind(&ok);
@@ -2047,7 +2048,7 @@ void Generate_AllocateSpaceAndShiftExistingArguments(
 // static
 // TODO(v8:11615): Observe Code::kMaxArguments in CallOrConstructVarargs
 void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
-                                               Handle<CodeT> code) {
+                                               Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- rdi    : target
   //  -- rax    : number of parameters on the stack
@@ -2116,9 +2117,9 @@ void Builtins::Generate_CallOrConstructVarargs(MacroAssembler* masm,
 }
 
 // static
-void Builtins::Generate_CallOrConstructForwardVarargs(MacroAssembler* masm,
-                                                      CallOrConstructMode mode,
-                                                      Handle<CodeT> code) {
+void Builtins::Generate_CallOrConstructForwardVarargs(
+    MacroAssembler* masm, CallOrConstructMode mode,
+    Handle<CodeDataContainer> code) {
   // ----------- S t a t e -------------
   //  -- rax : the number of arguments
   //  -- rdx : the new target (for [[Construct]] calls)
@@ -5343,7 +5344,7 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
   // always have baseline code.
   if (!is_osr) {
     Label start_with_baseline;
-    __ CmpObjectType(code_obj, CODET_TYPE, kScratchRegister);
+    __ CmpObjectType(code_obj, CODE_DATA_CONTAINER_TYPE, kScratchRegister);
     __ j(equal, &start_with_baseline);
 
     // Start with bytecode as there is no baseline code.
@@ -5356,12 +5357,12 @@ void Generate_BaselineOrInterpreterEntry(MacroAssembler* masm,
     // Start with baseline code.
     __ bind(&start_with_baseline);
   } else if (v8_flags.debug_code) {
-    __ CmpObjectType(code_obj, CODET_TYPE, kScratchRegister);
+    __ CmpObjectType(code_obj, CODE_DATA_CONTAINER_TYPE, kScratchRegister);
     __ Assert(equal, AbortReason::kExpectedBaselineData);
   }
 
   if (v8_flags.debug_code) {
-    AssertCodeTIsBaseline(masm, code_obj, r11);
+    AssertCodeDataContainerIsBaseline(masm, code_obj, r11);
   }
   __ LoadCodeDataContainerCodeNonBuiltin(code_obj, code_obj);
 
diff --git a/src/codegen/arm/assembler-arm-inl.h b/src/codegen/arm/assembler-arm-inl.h
index 3f2a1ce380a..c92eb184660 100644
--- a/src/codegen/arm/assembler-arm-inl.h
+++ b/src/codegen/arm/assembler-arm-inl.h
@@ -156,7 +156,7 @@ void RelocInfo::WipeOut() {
   }
 }
 
-Handle<CodeT> Assembler::relative_code_target_object_handle_at(
+Handle<CodeDataContainer> Assembler::relative_code_target_object_handle_at(
     Address pc) const {
   Instruction* branch = Instruction::At(pc);
   int code_target_index = branch->GetBranchOffset() / kInstrSize;
diff --git a/src/codegen/arm/assembler-arm.h b/src/codegen/arm/assembler-arm.h
index 580660cf298..2c45c48a1cb 100644
--- a/src/codegen/arm/assembler-arm.h
+++ b/src/codegen/arm/assembler-arm.h
@@ -1190,7 +1190,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void Move32BitImmediate(Register rd, const Operand& x, Condition cond = al);
 
   // Get the code target object for a pc-relative call or jump.
-  V8_INLINE Handle<CodeT> relative_code_target_object_handle_at(
+  V8_INLINE Handle<CodeDataContainer> relative_code_target_object_handle_at(
       Address pc_) const;
 
  protected:
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index ca2ce6559ce..79738e006c4 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -161,7 +161,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode,
   Jump(static_cast<intptr_t>(target), rmode, cond);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   DCHECK_IMPLIES(options().isolate_independent_code,
@@ -225,7 +225,7 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode, Condition cond,
   }
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, TargetAddressStorageMode mode,
                           bool check_constant_pool) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
@@ -294,7 +294,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin, Condition cond) {
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         bl(code_target_index * kInstrSize, cond,
            RelocInfo::RELATIVE_CODE_TARGET);
@@ -326,7 +327,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cond) {
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         b(code_target_index * kInstrSize, cond,
           RelocInfo::RELATIVE_CODE_TARGET);
@@ -402,10 +404,10 @@ void TurboAssembler::Drop(Register count, Condition cond) {
   add(sp, sp, Operand(count, LSL, kPointerSizeLog2), LeaveCC, cond);
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimization(Register codet,
-                                                        Register scratch) {
-  ldr(scratch,
-      FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset));
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container, Register scratch) {
+  ldr(scratch, FieldMemOperand(code_data_container,
+                               CodeDataContainer::kKindSpecificFlagsOffset));
   tst(scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
 }
 
@@ -1930,8 +1932,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   // runtime to clear it.
   {
     UseScratchRegisterScope temps(masm);
-    __ TestCodeTIsMarkedForDeoptimization(optimized_code_entry,
-                                          temps.Acquire());
+    __ TestCodeDataContainerIsMarkedForDeoptimization(optimized_code_entry,
+                                                      temps.Acquire());
     __ b(ne, &heal_optimized_code_slot);
   }
 
@@ -2059,7 +2061,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   // smarter.
   mov(r0, Operand(num_arguments));
   Move(r1, ExternalReference::Create(f));
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -2084,7 +2087,7 @@ void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
   DCHECK_EQ(builtin.address() & 1, 1);
 #endif
   Move(r1, builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
diff --git a/src/codegen/arm/macro-assembler-arm.h b/src/codegen/arm/macro-assembler-arm.h
index 9fd04590d49..dce5db1a5ab 100644
--- a/src/codegen/arm/macro-assembler-arm.h
+++ b/src/codegen/arm/macro-assembler-arm.h
@@ -308,8 +308,8 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void Call(Address target, RelocInfo::Mode rmode, Condition cond = al,
             TargetAddressStorageMode mode = CAN_INLINE_TARGET_ADDRESS,
             bool check_constant_pool = true);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
-            Condition cond = al,
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET, Condition cond = al,
             TargetAddressStorageMode mode = CAN_INLINE_TARGET_ADDRESS,
             bool check_constant_pool = true);
   void Call(Label* target);
@@ -440,7 +440,8 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
                      Register exclusion3 = no_reg);
   void Jump(Register target, Condition cond = al);
   void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, Condition cond = al);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
+            Condition cond = al);
   void Jump(const ExternalReference& reference);
 
   // Perform a floating-point min or max operation with the
@@ -891,7 +892,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     DecodeField<Field>(reg, reg);
   }
 
-  void TestCodeTIsMarkedForDeoptimization(Register codet, Register scratch);
+  void TestCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container, Register scratch);
   Operand ClearedValue() const;
 
  private:
diff --git a/src/codegen/arm64/assembler-arm64-inl.h b/src/codegen/arm64/assembler-arm64-inl.h
index 37158040ead..deacdbeb23a 100644
--- a/src/codegen/arm64/assembler-arm64-inl.h
+++ b/src/codegen/arm64/assembler-arm64-inl.h
@@ -485,15 +485,15 @@ Tagged_t Assembler::target_compressed_address_at(Address pc,
   return Memory<Tagged_t>(target_pointer_address_at(pc));
 }
 
-Handle<CodeT> Assembler::code_target_object_handle_at(Address pc) {
+Handle<CodeDataContainer> Assembler::code_target_object_handle_at(Address pc) {
   Instruction* instr = reinterpret_cast<Instruction*>(pc);
   if (instr->IsLdrLiteralX()) {
-    return Handle<CodeT>(reinterpret_cast<Address*>(
+    return Handle<CodeDataContainer>(reinterpret_cast<Address*>(
         Assembler::target_address_at(pc, 0 /* unused */)));
   } else {
     DCHECK(instr->IsBranchAndLink() || instr->IsUnconditionalBranch());
     DCHECK_EQ(instr->ImmPCOffset() % kInstrSize, 0);
-    return Handle<CodeT>::cast(
+    return Handle<CodeDataContainer>::cast(
         GetEmbeddedObject(instr->ImmPCOffset() >> kInstrSizeLog2));
   }
 }
diff --git a/src/codegen/arm64/assembler-arm64.cc b/src/codegen/arm64/assembler-arm64.cc
index 396955b28e4..e73c8be5a88 100644
--- a/src/codegen/arm64/assembler-arm64.cc
+++ b/src/codegen/arm64/assembler-arm64.cc
@@ -4383,7 +4383,7 @@ void Assembler::near_call(int offset, RelocInfo::Mode rmode) {
 void Assembler::near_call(HeapNumberRequest request) {
   BlockPoolsScope no_pool_before_bl_instr(this);
   RequestHeapNumber(request);
-  EmbeddedObjectIndex index = AddEmbeddedObject(Handle<CodeT>());
+  EmbeddedObjectIndex index = AddEmbeddedObject(Handle<CodeDataContainer>());
   RecordRelocInfo(RelocInfo::CODE_TARGET, index, NO_POOL_ENTRY);
   DCHECK(is_int32(index));
   bl(static_cast<int>(index));
diff --git a/src/codegen/arm64/assembler-arm64.h b/src/codegen/arm64/assembler-arm64.h
index dd5c1905b7e..35cf8f8b137 100644
--- a/src/codegen/arm64/assembler-arm64.h
+++ b/src/codegen/arm64/assembler-arm64.h
@@ -262,7 +262,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
 
   // Returns the handle for the code object called at 'pc'.
   // This might need to be temporarily encoded as an offset into code_targets_.
-  inline Handle<CodeT> code_target_object_handle_at(Address pc);
+  inline Handle<CodeDataContainer> code_target_object_handle_at(Address pc);
   inline EmbeddedObjectIndex embedded_object_index_referenced_from(Address pc);
   inline void set_embedded_object_index_referenced_from(
       Address p, EmbeddedObjectIndex index);
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index 08dd3848701..fc4234868bd 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -1413,9 +1413,9 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
-  __ AssertCodeT(optimized_code_entry);
-  __ JumpIfCodeTIsMarkedForDeoptimization(optimized_code_entry, scratch,
-                                          &heal_optimized_code_slot);
+  __ AssertCodeDataContainer(optimized_code_entry);
+  __ JumpIfCodeDataContainerIsMarkedForDeoptimization(
+      optimized_code_entry, scratch, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure into
   // the optimized functions list, then tail call the optimized code.
@@ -1447,7 +1447,7 @@ void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(
   ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(optimized_code, closure));
   // Store code entry in the closure.
-  AssertCodeT(optimized_code);
+  AssertCodeDataContainer(optimized_code);
   StoreTaggedField(optimized_code,
                    FieldMemOperand(closure, JSFunction::kCodeOffset));
   RecordWriteField(closure, JSFunction::kCodeOffset, optimized_code,
@@ -1596,16 +1596,16 @@ void TurboAssembler::AssertZeroExtended(Register int32_register) {
   Check(ls, AbortReason::k32BitValueInRegisterIsNotZeroExtended);
 }
 
-void MacroAssembler::AssertCodeT(Register object) {
+void MacroAssembler::AssertCodeDataContainer(Register object) {
   if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
-  AssertNotSmi(object, AbortReason::kOperandIsNotACodeT);
+  AssertNotSmi(object, AbortReason::kOperandIsNotACodeDataContainer);
 
   UseScratchRegisterScope temps(this);
   Register temp = temps.AcquireX();
 
-  CompareObjectType(object, temp, temp, CODET_TYPE);
-  Check(eq, AbortReason::kOperandIsNotACodeT);
+  CompareObjectType(object, temp, temp, CODE_DATA_CONTAINER_TYPE);
+  Check(eq, AbortReason::kOperandIsNotACodeDataContainer);
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
@@ -1913,7 +1913,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   Mov(x0, num_arguments);
   Mov(x1, ExternalReference::Create(f));
 
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -1921,7 +1922,7 @@ void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              bool builtin_exit_frame) {
   ASM_CODE_COMMENT(this);
   Mov(x1, builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
@@ -2146,7 +2147,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode,
   JumpHelper(offset, rmode, cond);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   DCHECK_IMPLIES(options().isolate_independent_code,
@@ -2190,7 +2191,8 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode) {
   }
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode) {
+void TurboAssembler::Call(Handle<CodeDataContainer> code,
+                          RelocInfo::Mode rmode) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code));
   BlockPoolsScope scope(this);
@@ -2201,7 +2203,7 @@ void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode) {
     return;
   }
 
-  DCHECK(FromCodeT(*code).IsExecutable());
+  DCHECK(FromCodeDataContainer(*code).IsExecutable());
   DCHECK(RelocInfo::IsCodeTarget(rmode));
 
   if (CanUseNearCallOrJump(rmode)) {
@@ -2283,7 +2285,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         EmbeddedObjectIndex index = AddEmbeddedObject(code);
         DCHECK(is_int32(index));
         near_call(static_cast<int32_t>(index), RelocInfo::CODE_TARGET);
@@ -2336,7 +2339,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cond) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         EmbeddedObjectIndex index = AddEmbeddedObject(code);
         DCHECK(is_int32(index));
         JumpHelper(static_cast<int64_t>(index), RelocInfo::CODE_TARGET, cond);
@@ -2681,10 +2685,12 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
   Bind(&done);
 }
 
-void MacroAssembler::JumpIfCodeTIsMarkedForDeoptimization(
-    Register codet, Register scratch, Label* if_marked_for_deoptimization) {
+void MacroAssembler::JumpIfCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container, Register scratch,
+    Label* if_marked_for_deoptimization) {
   Ldr(scratch.W(),
-      FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset));
+      FieldMemOperand(code_data_container,
+                      CodeDataContainer::kKindSpecificFlagsOffset));
   Tbnz(scratch.W(), Code::kMarkedForDeoptimizationBit,
        if_marked_for_deoptimization);
 }
diff --git a/src/codegen/arm64/macro-assembler-arm64.h b/src/codegen/arm64/macro-assembler-arm64.h
index e978c047c6e..091e61d2384 100644
--- a/src/codegen/arm64/macro-assembler-arm64.h
+++ b/src/codegen/arm64/macro-assembler-arm64.h
@@ -974,12 +974,14 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   void Jump(Register target, Condition cond = al);
   void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, Condition cond = al);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
+            Condition cond = al);
   void Jump(const ExternalReference& reference);
 
   void Call(Register target);
   void Call(Address target, RelocInfo::Mode rmode);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);
   void Call(ExternalReference target);
 
   // Generate an indirect call (for when a direct call's range is not adequate).
@@ -1897,8 +1899,9 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     DecodeField<Field>(reg, reg);
   }
 
-  void JumpIfCodeTIsMarkedForDeoptimization(
-      Register codet, Register scratch, Label* if_marked_for_deoptimization);
+  void JumpIfCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container, Register scratch,
+      Label* if_marked_for_deoptimization);
   Operand ClearedValue() const;
 
   Operand ReceiverOperand(const Register arg_count);
@@ -1907,8 +1910,9 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
 
   inline void JumpIfNotSmi(Register value, Label* not_smi_label);
 
-  // Abort execution if argument is not a CodeT, enabled via --debug-code.
-  void AssertCodeT(Register object) NOOP_UNLESS_DEBUG_CODE
+  // Abort execution if argument is not a CodeDataContainer, enabled via
+  // --debug-code.
+  void AssertCodeDataContainer(Register object) NOOP_UNLESS_DEBUG_CODE
 
       // Abort execution if argument is not a Constructor, enabled via
       // --debug-code.
diff --git a/src/codegen/assembler.cc b/src/codegen/assembler.cc
index 05165f1bfd4..45e572505cd 100644
--- a/src/codegen/assembler.cc
+++ b/src/codegen/assembler.cc
@@ -264,7 +264,7 @@ void AssemblerBase::RequestHeapNumber(HeapNumberRequest request) {
   heap_number_requests_.push_front(request);
 }
 
-int AssemblerBase::AddCodeTarget(Handle<CodeT> target) {
+int AssemblerBase::AddCodeTarget(Handle<CodeDataContainer> target) {
   int current = static_cast<int>(code_targets_.size());
   if (current > 0 && !target.is_null() &&
       code_targets_.back().address() == target.address()) {
@@ -276,7 +276,8 @@ int AssemblerBase::AddCodeTarget(Handle<CodeT> target) {
   }
 }
 
-Handle<CodeT> AssemblerBase::GetCodeTarget(intptr_t code_target_index) const {
+Handle<CodeDataContainer> AssemblerBase::GetCodeTarget(
+    intptr_t code_target_index) const {
   DCHECK_LT(static_cast<size_t>(code_target_index), code_targets_.size());
   return code_targets_[code_target_index];
 }
diff --git a/src/codegen/assembler.h b/src/codegen/assembler.h
index 4e098dcd200..47066fa4081 100644
--- a/src/codegen/assembler.h
+++ b/src/codegen/assembler.h
@@ -355,8 +355,8 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
  protected:
   // Add 'target' to the {code_targets_} vector, if necessary, and return the
   // offset at which it is stored.
-  int AddCodeTarget(Handle<CodeT> target);
-  Handle<CodeT> GetCodeTarget(intptr_t code_target_index) const;
+  int AddCodeTarget(Handle<CodeDataContainer> target);
+  Handle<CodeDataContainer> GetCodeTarget(intptr_t code_target_index) const;
 
   // Add 'object' to the {embedded_objects_} vector and return the index at
   // which it is stored.
@@ -412,7 +412,7 @@ class V8_EXPORT_PRIVATE AssemblerBase : public Malloced {
   // guaranteed to fit in the instruction's offset field. We keep track of the
   // code handles we encounter in calls in this vector, and encode the index of
   // the code handle in the vector instead.
-  std::vector<Handle<CodeT>> code_targets_;
+  std::vector<Handle<CodeDataContainer>> code_targets_;
 
   // If an assembler needs a small number to refer to a heap object handle
   // (for example, because there are only 32bit available on a 64bit arch), the
diff --git a/src/codegen/bailout-reason.h b/src/codegen/bailout-reason.h
index cdd9e123185..8c064578f3d 100644
--- a/src/codegen/bailout-reason.h
+++ b/src/codegen/bailout-reason.h
@@ -54,7 +54,7 @@ namespace internal {
   V(kOperandIsNotAFunction, "Operand is not a function")                       \
   V(kOperandIsNotACallableFunction, "Operand is not a callable function")      \
   V(kOperandIsNotAGeneratorObject, "Operand is not a generator object")        \
-  V(kOperandIsNotACodeT, "Operand is not a CodeT")                             \
+  V(kOperandIsNotACodeDataContainer, "Operand is not a CodeDataContainer")     \
   V(kOperandIsNotASmi, "Operand is not a smi")                                 \
   V(kPromiseAlreadySettled, "Promise already settled")                         \
   V(kReceivedInvalidReturnAddress, "Received invalid return address")          \
diff --git a/src/codegen/callable.h b/src/codegen/callable.h
index 79a70514af1..dee7029d375 100644
--- a/src/codegen/callable.h
+++ b/src/codegen/callable.h
@@ -16,14 +16,14 @@ class Code;
 // Associates a body of code with an interface descriptor.
 class Callable final {
  public:
-  Callable(Handle<CodeT> code, CallInterfaceDescriptor descriptor)
+  Callable(Handle<CodeDataContainer> code, CallInterfaceDescriptor descriptor)
       : code_(code), descriptor_(descriptor) {}
 
-  Handle<CodeT> code() const { return code_; }
+  Handle<CodeDataContainer> code() const { return code_; }
   CallInterfaceDescriptor descriptor() const { return descriptor_; }
 
  private:
-  const Handle<CodeT> code_;
+  const Handle<CodeDataContainer> code_;
   const CallInterfaceDescriptor descriptor_;
 };
 
diff --git a/src/codegen/code-factory.cc b/src/codegen/code-factory.cc
index 01be4f31508..31349b92194 100644
--- a/src/codegen/code-factory.cc
+++ b/src/codegen/code-factory.cc
@@ -14,13 +14,15 @@ namespace v8 {
 namespace internal {
 
 // static
-Handle<CodeT> CodeFactory::RuntimeCEntry(Isolate* isolate, int result_size) {
+Handle<CodeDataContainer> CodeFactory::RuntimeCEntry(Isolate* isolate,
+                                                     int result_size) {
   return CodeFactory::CEntry(isolate, result_size);
 }
 
 // static
-Handle<CodeT> CodeFactory::CEntry(Isolate* isolate, int result_size,
-                                  ArgvMode argv_mode, bool builtin_exit_frame) {
+Handle<CodeDataContainer> CodeFactory::CEntry(Isolate* isolate, int result_size,
+                                              ArgvMode argv_mode,
+                                              bool builtin_exit_frame) {
   // Aliases for readability below.
   const int rs = result_size;
   const ArgvMode am = argv_mode;
@@ -254,7 +256,7 @@ Callable CodeFactory::InterpreterPushArgsThenConstruct(
 
 // static
 Callable CodeFactory::InterpreterCEntry(Isolate* isolate, int result_size) {
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate, result_size, ArgvMode::kRegister);
   if (result_size == 1) {
     return Callable(code, InterpreterCEntry1Descriptor{});
diff --git a/src/codegen/code-factory.h b/src/codegen/code-factory.h
index 73629f5f049..de64bed4dbd 100644
--- a/src/codegen/code-factory.h
+++ b/src/codegen/code-factory.h
@@ -26,11 +26,12 @@ class V8_EXPORT_PRIVATE CodeFactory final {
   // stack and the arguments count is passed via register) which currently
   // can't be expressed in CallInterfaceDescriptor. Therefore only the code
   // is exported here.
-  static Handle<CodeT> RuntimeCEntry(Isolate* isolate, int result_size = 1);
+  static Handle<CodeDataContainer> RuntimeCEntry(Isolate* isolate,
+                                                 int result_size = 1);
 
-  static Handle<CodeT> CEntry(Isolate* isolate, int result_size = 1,
-                              ArgvMode argv_mode = ArgvMode::kStack,
-                              bool builtin_exit_frame = false);
+  static Handle<CodeDataContainer> CEntry(Isolate* isolate, int result_size = 1,
+                                          ArgvMode argv_mode = ArgvMode::kStack,
+                                          bool builtin_exit_frame = false);
 
   // Initial states for ICs.
   static Callable LoadGlobalIC(Isolate* isolate, TypeofMode typeof_mode);
diff --git a/src/codegen/code-reference.cc b/src/codegen/code-reference.cc
index 3055147ebb1..93fc96431ea 100644
--- a/src/codegen/code-reference.cc
+++ b/src/codegen/code-reference.cc
@@ -18,9 +18,9 @@ namespace internal {
 
 namespace {
 
-template <typename CodeOrCodeT>
-struct CodeOrCodeTOps {
-  Handle<CodeOrCodeT> code;
+template <typename CodeOrCodeDataContainer>
+struct CodeOrCodeDataContainerOps {
+  Handle<CodeOrCodeDataContainer> code;
 
   Address constant_pool() const { return code->constant_pool(); }
   Address instruction_start() const { return code->InstructionStart(); }
@@ -33,8 +33,8 @@ struct CodeOrCodeTOps {
   int code_comments_size() const { return code->code_comments_size(); }
 };
 
-using CodeOps = CodeOrCodeTOps<Code>;
-using CodeTOps = CodeOrCodeTOps<CodeT>;
+using CodeOps = CodeOrCodeDataContainerOps<Code>;
+using CodeDataContainerOps = CodeOrCodeDataContainerOps<CodeDataContainer>;
 
 #if V8_ENABLE_WEBASSEMBLY
 struct WasmCodeOps {
@@ -92,21 +92,21 @@ struct CodeDescOps {
 #define HANDLE_WASM(...) UNREACHABLE()
 #endif
 
-#define DISPATCH(ret, method)                                 \
-  ret CodeReference::method() const {                         \
-    DCHECK(!is_null());                                       \
-    switch (kind_) {                                          \
-      case Kind::CODE:                                        \
-        return CodeOps{code_}.method();                       \
-      case Kind::CODET:                                       \
-        return CodeTOps{codet_}.method();                     \
-      case Kind::WASM_CODE:                                   \
-        HANDLE_WASM(return WasmCodeOps{wasm_code_}.method()); \
-      case Kind::CODE_DESC:                                   \
-        return CodeDescOps{code_desc_}.method();              \
-      default:                                                \
-        UNREACHABLE();                                        \
-    }                                                         \
+#define DISPATCH(ret, method)                                       \
+  ret CodeReference::method() const {                               \
+    DCHECK(!is_null());                                             \
+    switch (kind_) {                                                \
+      case Kind::CODE:                                              \
+        return CodeOps{code_}.method();                             \
+      case Kind::CODE_DATA_CONTAINER:                               \
+        return CodeDataContainerOps{code_data_container_}.method(); \
+      case Kind::WASM_CODE:                                         \
+        HANDLE_WASM(return WasmCodeOps{wasm_code_}.method());       \
+      case Kind::CODE_DESC:                                         \
+        return CodeDescOps{code_desc_}.method();                    \
+      default:                                                      \
+        UNREACHABLE();                                              \
+    }                                                               \
   }
 
 DISPATCH(Address, constant_pool)
diff --git a/src/codegen/code-reference.h b/src/codegen/code-reference.h
index 9f027fbfab4..ce3251e6003 100644
--- a/src/codegen/code-reference.h
+++ b/src/codegen/code-reference.h
@@ -28,8 +28,9 @@ class CodeReference {
   explicit CodeReference(const CodeDesc* code_desc)
       : kind_(Kind::CODE_DESC), code_desc_(code_desc) {}
   explicit CodeReference(Handle<Code> code) : kind_(Kind::CODE), code_(code) {}
-  explicit CodeReference(Handle<CodeT> codet)
-      : kind_(Kind::CODET), codet_(codet) {}
+  explicit CodeReference(Handle<CodeDataContainer> code_data_container)
+      : kind_(Kind::CODE_DATA_CONTAINER),
+        code_data_container_(code_data_container) {}
 
   Address constant_pool() const;
   Address instruction_start() const;
@@ -43,7 +44,9 @@ class CodeReference {
 
   bool is_null() const { return kind_ == Kind::NONE; }
   bool is_code() const { return kind_ == Kind::CODE; }
-  bool is_codet() const { return kind_ == Kind::CODET; }
+  bool is_code_data_container() const {
+    return kind_ == Kind::CODE_DATA_CONTAINER;
+  }
   bool is_wasm_code() const { return kind_ == Kind::WASM_CODE; }
 
   Handle<Code> as_code() const {
@@ -51,9 +54,9 @@ class CodeReference {
     return code_;
   }
 
-  Handle<CodeT> as_codet() const {
-    DCHECK_EQ(Kind::CODET, kind_);
-    return codet_;
+  Handle<CodeDataContainer> as_code_data_container() const {
+    DCHECK_EQ(Kind::CODE_DATA_CONTAINER, kind_);
+    return code_data_container_;
   }
 
   const wasm::WasmCode* as_wasm_code() const {
@@ -62,13 +65,19 @@ class CodeReference {
   }
 
  private:
-  enum class Kind { NONE, CODE, CODET, WASM_CODE, CODE_DESC } kind_;
+  enum class Kind {
+    NONE,
+    CODE,
+    CODE_DATA_CONTAINER,
+    WASM_CODE,
+    CODE_DESC
+  } kind_;
   union {
     std::nullptr_t null_;
     const wasm::WasmCode* wasm_code_;
     const CodeDesc* code_desc_;
     Handle<Code> code_;
-    Handle<CodeT> codet_;
+    Handle<CodeDataContainer> code_data_container_;
   };
 
   DISALLOW_NEW_AND_DELETE()
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 9533d94776b..36816971889 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -3145,19 +3145,19 @@ TNode<BytecodeArray> CodeStubAssembler::LoadSharedFunctionInfoBytecodeArray(
   Label check_for_interpreter_data(this, &var_result);
   Label done(this, &var_result);
 
-  GotoIfNot(HasInstanceType(var_result.value(), CODET_TYPE),
+  GotoIfNot(HasInstanceType(var_result.value(), CODE_DATA_CONTAINER_TYPE),
             &check_for_interpreter_data);
   {
-    TNode<CodeT> code = CAST(var_result.value());
+    TNode<CodeDataContainer> code = CAST(var_result.value());
 #ifdef DEBUG
     TNode<Int32T> code_flags =
-        LoadObjectField<Int32T>(code, CodeT::kFlagsOffset);
-    CSA_DCHECK(
-        this, Word32Equal(DecodeWord32<CodeT::KindField>(code_flags),
-                          Int32Constant(static_cast<int>(CodeKind::BASELINE))));
+        LoadObjectField<Int32T>(code, CodeDataContainer::kFlagsOffset);
+    CSA_DCHECK(this, Word32Equal(
+                         DecodeWord32<CodeDataContainer::KindField>(code_flags),
+                         Int32Constant(static_cast<int>(CodeKind::BASELINE))));
 #endif  // DEBUG
     TNode<HeapObject> baseline_data = LoadObjectField<HeapObject>(
-        FromCodeTNonBuiltin(code),
+        FromCodeDataContainerNonBuiltin(code),
         Code::kDeoptimizationDataOrInterpreterDataOffset);
     var_result = baseline_data;
   }
@@ -15463,7 +15463,7 @@ TNode<BoolT> CodeStubAssembler::NeedsAnyPromiseHooks(TNode<Uint32T> flags) {
   return Word32NotEqual(flags, Int32Constant(0));
 }
 
-TNode<CodeT> CodeStubAssembler::LoadBuiltin(TNode<Smi> builtin_id) {
+TNode<CodeDataContainer> CodeStubAssembler::LoadBuiltin(TNode<Smi> builtin_id) {
   CSA_DCHECK(this, SmiBelow(builtin_id, SmiConstant(Builtins::kBuiltinCount)));
 
   TNode<IntPtrT> offset =
@@ -15475,13 +15475,13 @@ TNode<CodeT> CodeStubAssembler::LoadBuiltin(TNode<Smi> builtin_id) {
   return CAST(BitcastWordToTagged(Load<RawPtrT>(table, offset)));
 }
 
-TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
+TNode<CodeDataContainer> CodeStubAssembler::GetSharedFunctionInfoCode(
     TNode<SharedFunctionInfo> shared_info, TVariable<Uint16T>* data_type_out,
     Label* if_compile_lazy) {
   TNode<Object> sfi_data =
       LoadObjectField(shared_info, SharedFunctionInfo::kFunctionDataOffset);
 
-  TVARIABLE(CodeT, sfi_code);
+  TVARIABLE(CodeDataContainer, sfi_code);
 
   Label done(this);
   Label check_instance_type(this);
@@ -15507,7 +15507,7 @@ TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
 
   int32_t case_values[] = {
     BYTECODE_ARRAY_TYPE,
-    CODET_TYPE,
+    CODE_DATA_CONTAINER_TYPE,
     UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_TYPE,
     UNCOMPILED_DATA_WITH_PREPARSE_DATA_TYPE,
     UNCOMPILED_DATA_WITHOUT_PREPARSE_DATA_WITH_JOB_TYPE,
@@ -15557,7 +15557,7 @@ TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
   // IsBaselineData: Execute baseline code
   BIND(&check_is_baseline_data);
   {
-    TNode<CodeT> baseline_code = CAST(sfi_data);
+    TNode<CodeDataContainer> baseline_code = CAST(sfi_data);
     sfi_code = baseline_code;
     Goto(&done);
   }
@@ -15579,7 +15579,7 @@ TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
   CSA_DCHECK(this,
              Word32Equal(data_type, Int32Constant(INTERPRETER_DATA_TYPE)));
   {
-    TNode<CodeT> trampoline =
+    TNode<CodeDataContainer> trampoline =
         LoadInterpreterDataInterpreterTrampoline(CAST(sfi_data));
     sfi_code = trampoline;
   }
@@ -15607,21 +15607,22 @@ TNode<CodeT> CodeStubAssembler::GetSharedFunctionInfoCode(
   return sfi_code.value();
 }
 
-TNode<RawPtrT> CodeStubAssembler::GetCodeEntry(TNode<CodeT> code) {
+TNode<RawPtrT> CodeStubAssembler::GetCodeEntry(TNode<CodeDataContainer> code) {
   return LoadObjectField<RawPtrT>(
       code, IntPtrConstant(CodeDataContainer::kCodeEntryPointOffset));
 }
 
-TNode<BoolT> CodeStubAssembler::IsMarkedForDeoptimization(TNode<CodeT> codet) {
+TNode<BoolT> CodeStubAssembler::IsMarkedForDeoptimization(
+    TNode<CodeDataContainer> code_data_container) {
   return IsSetWord32<Code::MarkedForDeoptimizationField>(
-      LoadObjectField<Int32T>(codet,
+      LoadObjectField<Int32T>(code_data_container,
                               CodeDataContainer::kKindSpecificFlagsOffset));
 }
 
 TNode<JSFunction> CodeStubAssembler::AllocateFunctionWithMapAndContext(
     TNode<Map> map, TNode<SharedFunctionInfo> shared_info,
     TNode<Context> context) {
-  const TNode<CodeT> code = GetSharedFunctionInfoCode(shared_info);
+  const TNode<CodeDataContainer> code = GetSharedFunctionInfoCode(shared_info);
 
   // TODO(ishell): All the callers of this function pass map loaded from
   // Context::STRICT_FUNCTION_WITHOUT_PROTOTYPE_MAP_INDEX. So we can remove
diff --git a/src/codegen/code-stub-assembler.h b/src/codegen/code-stub-assembler.h
index bdf86ca1945..42e17d98627 100644
--- a/src/codegen/code-stub-assembler.h
+++ b/src/codegen/code-stub-assembler.h
@@ -834,16 +834,9 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
 
   void FastCheck(TNode<BoolT> condition);
 
-  TNode<BoolT> IsCodeTMap(TNode<Map> map) {
-    return IsCodeDataContainerMap(map);
-  }
-  TNode<BoolT> IsCodeT(TNode<HeapObject> object) {
-    return IsCodeTMap(LoadMap(object));
-  }
-
   // TODO(v8:11880): remove once Code::bytecode_or_interpreter_data field
-  // is cached in or moved to CodeT.
-  TNode<Code> FromCodeTNonBuiltin(TNode<CodeT> code) {
+  // is cached in or moved to CodeDataContainer.
+  TNode<Code> FromCodeDataContainerNonBuiltin(TNode<CodeDataContainer> code) {
     // Compute the Code object pointer from the code entry point.
     TNode<RawPtrT> code_entry = Load<RawPtrT>(
         code, IntPtrConstant(CodeDataContainer::kCodeEntryPointOffset -
@@ -853,18 +846,14 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
     return CAST(o);
   }
 
-  TNode<CodeT> ToCodeT(TNode<Code> code) {
+  TNode<CodeDataContainer> ToCodeDataContainer(TNode<Code> code) {
     return LoadObjectField<CodeDataContainer>(code,
                                               Code::kCodeDataContainerOffset);
   }
 
-  TNode<CodeT> ToCodeT(TNode<Code> code,
-                       TNode<CodeDataContainer> code_data_container) {
-    return code_data_container;
-  }
-
-  TNode<RawPtrT> GetCodeEntry(TNode<CodeT> code);
-  TNode<BoolT> IsMarkedForDeoptimization(TNode<CodeT> codet);
+  TNode<RawPtrT> GetCodeEntry(TNode<CodeDataContainer> code);
+  TNode<BoolT> IsMarkedForDeoptimization(
+      TNode<CodeDataContainer> code_data_container);
 
   // The following Call wrappers call an object according to the semantics that
   // one finds in the EcmaScript spec, operating on an Callable (e.g. a
@@ -3862,7 +3851,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
                                 ElementsKind kind = HOLEY_ELEMENTS);
 
   // Load a builtin's code from the builtin array in the isolate.
-  TNode<CodeT> LoadBuiltin(TNode<Smi> builtin_id);
+  TNode<CodeDataContainer> LoadBuiltin(TNode<Smi> builtin_id);
 
   // Figure out the SFI's code object using its data field.
   // If |data_type_out| is provided, the instance type of the function data will
@@ -3870,7 +3859,7 @@ class V8_EXPORT_PRIVATE CodeStubAssembler
   // data_type_out will be set to 0.
   // If |if_compile_lazy| is provided then the execution will go to the given
   // label in case of an CompileLazy code object.
-  TNode<CodeT> GetSharedFunctionInfoCode(
+  TNode<CodeDataContainer> GetSharedFunctionInfoCode(
       TNode<SharedFunctionInfo> shared_info,
       TVariable<Uint16T>* data_type_out = nullptr,
       Label* if_compile_lazy = nullptr);
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index 773b79a5bb1..f3cda767251 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -652,7 +652,7 @@ void InstallInterpreterTrampolineCopy(Isolate* isolate,
           INTERPRETER_DATA_TYPE, AllocationType::kOld));
 
   interpreter_data->set_bytecode_array(*bytecode_array);
-  interpreter_data->set_interpreter_trampoline(ToCodeT(*code));
+  interpreter_data->set_interpreter_trampoline(ToCodeDataContainer(*code));
 
   shared_info->set_interpreter_data(*interpreter_data);
 
@@ -922,7 +922,7 @@ bool FinalizeDeferredUnoptimizedCompilationJobs(
 // A wrapper to access the optimized code cache slots on the feedback vector.
 class OptimizedCodeCache : public AllStatic {
  public:
-  static V8_WARN_UNUSED_RESULT MaybeHandle<CodeT> Get(
+  static V8_WARN_UNUSED_RESULT MaybeHandle<CodeDataContainer> Get(
       Isolate* isolate, Handle<JSFunction> function, BytecodeOffset osr_offset,
       CodeKind code_kind) {
     if (!CodeKindIsStoredInOptimizedCodeCache(code_kind)) return {};
@@ -932,13 +932,13 @@ class OptimizedCodeCache : public AllStatic {
     SharedFunctionInfo shared = function->shared();
     RCS_SCOPE(isolate, RuntimeCallCounterId::kCompileGetFromOptimizedCodeMap);
 
-    CodeT code;
+    CodeDataContainer code;
     FeedbackVector feedback_vector = function->feedback_vector();
     if (IsOSR(osr_offset)) {
       Handle<BytecodeArray> bytecode(shared.GetBytecodeArray(isolate), isolate);
       interpreter::BytecodeArrayIterator it(bytecode, osr_offset.ToInt());
       DCHECK_EQ(it.current_bytecode(), interpreter::Bytecode::kJumpLoop);
-      base::Optional<CodeT> maybe_code =
+      base::Optional<CodeDataContainer> maybe_code =
           feedback_vector.GetOptimizedOsrCode(isolate, it.GetSlotOperand(2));
       if (maybe_code.has_value()) code = maybe_code.value();
     } else {
@@ -961,7 +961,7 @@ class OptimizedCodeCache : public AllStatic {
   }
 
   static void Insert(Isolate* isolate, JSFunction function,
-                     BytecodeOffset osr_offset, CodeT code,
+                     BytecodeOffset osr_offset, CodeDataContainer code,
                      bool is_function_context_specializing) {
     const CodeKind kind = code.kind();
     if (!CodeKindIsStoredInOptimizedCodeCache(kind)) return;
@@ -1052,7 +1052,7 @@ bool CompileTurbofan_NotConcurrent(Isolate* isolate,
   DCHECK(!isolate->has_pending_exception());
   OptimizedCodeCache::Insert(isolate, *compilation_info->closure(),
                              compilation_info->osr_offset(),
-                             ToCodeT(*compilation_info->code()),
+                             ToCodeDataContainer(*compilation_info->code()),
                              compilation_info->function_context_specializing());
   job->RecordFunctionCompilation(LogEventListener::CodeTag::kFunction, isolate);
   return true;
@@ -1128,12 +1128,10 @@ bool ShouldOptimize(CodeKind code_kind, Handle<SharedFunctionInfo> shared) {
   }
 }
 
-MaybeHandle<CodeT> CompileTurbofan(Isolate* isolate,
-                                   Handle<JSFunction> function,
-                                   Handle<SharedFunctionInfo> shared,
-                                   ConcurrencyMode mode,
-                                   BytecodeOffset osr_offset,
-                                   CompileResultBehavior result_behavior) {
+MaybeHandle<CodeDataContainer> CompileTurbofan(
+    Isolate* isolate, Handle<JSFunction> function,
+    Handle<SharedFunctionInfo> shared, ConcurrencyMode mode,
+    BytecodeOffset osr_offset, CompileResultBehavior result_behavior) {
   VMState<COMPILER> state(isolate);
   TimerEventScope<TimerEventOptimizeCode> optimize_code_timer(isolate);
   RCS_SCOPE(isolate, RuntimeCallCounterId::kOptimizeCode);
@@ -1165,7 +1163,7 @@ MaybeHandle<CodeT> CompileTurbofan(Isolate* isolate,
   } else {
     DCHECK(IsSynchronous(mode));
     if (CompileTurbofan_NotConcurrent(isolate, job.get())) {
-      return ToCodeT(job->compilation_info()->code(), isolate);
+      return ToCodeDataContainer(job->compilation_info()->code(), isolate);
     }
   }
 
@@ -1178,10 +1176,11 @@ MaybeHandle<CodeT> CompileTurbofan(Isolate* isolate,
 void RecordMaglevFunctionCompilation(Isolate* isolate,
                                      Handle<JSFunction> function) {
   PtrComprCageBase cage_base(isolate);
-  // TODO(v8:13261): We should be able to pass a CodeT AbstractCode in here, but
-  // LinuxPerfJitLogger only supports Code AbstractCode.
+  // TODO(v8:13261): We should be able to pass a CodeDataContainer AbstractCode
+  // in here, but LinuxPerfJitLogger only supports Code AbstractCode.
   Handle<AbstractCode> abstract_code(
-      AbstractCode::cast(FromCodeT(function->code(cage_base))), isolate);
+      AbstractCode::cast(FromCodeDataContainer(function->code(cage_base))),
+      isolate);
   Handle<SharedFunctionInfo> shared(function->shared(cage_base), isolate);
   Handle<Script> script(Script::cast(shared->script(cage_base)), isolate);
   Handle<FeedbackVector> feedback_vector(function->feedback_vector(cage_base),
@@ -1197,10 +1196,9 @@ void RecordMaglevFunctionCompilation(Isolate* isolate,
 }
 #endif  // V8_ENABLE_MAGLEV
 
-MaybeHandle<CodeT> CompileMaglev(Isolate* isolate, Handle<JSFunction> function,
-                                 ConcurrencyMode mode,
-                                 BytecodeOffset osr_offset,
-                                 CompileResultBehavior result_behavior) {
+MaybeHandle<CodeDataContainer> CompileMaglev(
+    Isolate* isolate, Handle<JSFunction> function, ConcurrencyMode mode,
+    BytecodeOffset osr_offset, CompileResultBehavior result_behavior) {
 #ifdef V8_ENABLE_MAGLEV
   DCHECK(v8_flags.maglev);
   // TODO(v8:7700): Add missing support.
@@ -1268,7 +1266,7 @@ MaybeHandle<CodeT> CompileMaglev(Isolate* isolate, Handle<JSFunction> function,
 #endif  // V8_ENABLE_MAGLEV
 }
 
-MaybeHandle<CodeT> GetOrCompileOptimized(
+MaybeHandle<CodeDataContainer> GetOrCompileOptimized(
     Isolate* isolate, Handle<JSFunction> function, ConcurrencyMode mode,
     CodeKind code_kind, BytecodeOffset osr_offset = BytecodeOffset::None(),
     CompileResultBehavior result_behavior = CompileResultBehavior::kDefault) {
@@ -1298,7 +1296,7 @@ MaybeHandle<CodeT> GetOrCompileOptimized(
   // turbo_filter.
   if (!ShouldOptimize(code_kind, shared)) return {};
 
-  Handle<CodeT> cached_code;
+  Handle<CodeDataContainer> cached_code;
   if (OptimizedCodeCache::Get(isolate, function, osr_offset, code_kind)
           .ToHandle(&cached_code)) {
     return cached_code;
@@ -2566,7 +2564,7 @@ bool Compiler::Compile(Isolate* isolate, Handle<JSFunction> function,
   }
 
   DCHECK(is_compiled_scope->is_compiled());
-  Handle<CodeT> code = handle(shared_info->GetCode(), isolate);
+  Handle<CodeDataContainer> code = handle(shared_info->GetCode(), isolate);
 
   // Initialize the feedback cell for this JSFunction and reset the interrupt
   // budget for feedback vector allocation even if there is a closure feedback
@@ -2595,7 +2593,7 @@ bool Compiler::Compile(Isolate* isolate, Handle<JSFunction> function,
                                                   concurrency_mode, code_kind);
     }
 
-    Handle<CodeT> maybe_code;
+    Handle<CodeDataContainer> maybe_code;
     if (GetOrCompileOptimized(isolate, function, concurrency_mode, code_kind)
             .ToHandle(&maybe_code)) {
       code = maybe_code;
@@ -2648,7 +2646,7 @@ bool Compiler::CompileSharedWithBaseline(Isolate* isolate,
       // report these somehow, or silently ignore them?
       return false;
     }
-    shared->set_baseline_code(ToCodeT(*code), kReleaseStore);
+    shared->set_baseline_code(ToCodeDataContainer(*code), kReleaseStore);
   }
   double time_taken_ms = time_taken.InMillisecondsF();
 
@@ -2676,7 +2674,7 @@ bool Compiler::CompileBaseline(Isolate* isolate, Handle<JSFunction> function,
   // Baseline code needs a feedback vector.
   JSFunction::EnsureFeedbackVector(isolate, function, is_compiled_scope);
 
-  CodeT baseline_code = shared->baseline_code(kAcquireLoad);
+  CodeDataContainer baseline_code = shared->baseline_code(kAcquireLoad);
   DCHECK_EQ(baseline_code.kind(), CodeKind::BASELINE);
   function->set_code(baseline_code);
   return true;
@@ -2720,7 +2718,7 @@ void Compiler::CompileOptimized(Isolate* isolate, Handle<JSFunction> function,
                                                 code_kind);
   }
 
-  Handle<CodeT> code;
+  Handle<CodeDataContainer> code;
   if (GetOrCompileOptimized(isolate, function, mode, code_kind)
           .ToHandle(&code)) {
     function->set_code(*code, kReleaseStore);
@@ -3854,10 +3852,9 @@ template Handle<SharedFunctionInfo> Compiler::GetSharedFunctionInfo(
     FunctionLiteral* literal, Handle<Script> script, LocalIsolate* isolate);
 
 // static
-MaybeHandle<CodeT> Compiler::CompileOptimizedOSR(Isolate* isolate,
-                                                 Handle<JSFunction> function,
-                                                 BytecodeOffset osr_offset,
-                                                 ConcurrencyMode mode) {
+MaybeHandle<CodeDataContainer> Compiler::CompileOptimizedOSR(
+    Isolate* isolate, Handle<JSFunction> function, BytecodeOffset osr_offset,
+    ConcurrencyMode mode) {
   DCHECK(IsOSR(osr_offset));
 
   if (V8_UNLIKELY(isolate->serializer_enabled())) return {};
@@ -3880,7 +3877,7 @@ MaybeHandle<CodeT> Compiler::CompileOptimizedOSR(Isolate* isolate,
   function->feedback_vector().reset_osr_urgency();
 
   CompilerTracer::TraceOptimizeOSRStarted(isolate, function, osr_offset, mode);
-  MaybeHandle<CodeT> result = GetOrCompileOptimized(
+  MaybeHandle<CodeDataContainer> result = GetOrCompileOptimized(
       isolate, function, mode, CodeKind::TURBOFAN, osr_offset);
 
   if (result.is_null()) {
@@ -3944,7 +3941,8 @@ void Compiler::FinalizeTurbofanCompilationJob(TurbofanCompilationJob* job,
         ResetTieringState(*function, osr_offset);
         OptimizedCodeCache::Insert(
             isolate, *compilation_info->closure(),
-            compilation_info->osr_offset(), ToCodeT(*compilation_info->code()),
+            compilation_info->osr_offset(),
+            ToCodeDataContainer(*compilation_info->code()),
             compilation_info->function_context_specializing());
         CompilerTracer::TraceCompletedJob(isolate, compilation_info);
         if (IsOSR(osr_offset)) {
@@ -4034,7 +4032,7 @@ void Compiler::PostInstantiation(Handle<JSFunction> function) {
       // deoptimized code just before installing it on the funciton.
       function->feedback_vector().EvictOptimizedCodeMarkedForDeoptimization(
           *shared, "new function from shared function info");
-      CodeT code = function->feedback_vector().optimized_code();
+      CodeDataContainer code = function->feedback_vector().optimized_code();
       if (!code.is_null()) {
         // Caching of optimized code enabled and optimized code found.
         DCHECK(!code.marked_for_deoptimization());
diff --git a/src/codegen/compiler.h b/src/codegen/compiler.h
index 21ee79145a9..bd4151489d5 100644
--- a/src/codegen/compiler.h
+++ b/src/codegen/compiler.h
@@ -95,9 +95,9 @@ class V8_EXPORT_PRIVATE Compiler : public AllStatic {
   // Generate and return optimized code for OSR. The empty handle is returned
   // either on failure, or after spawning a concurrent OSR task (in which case
   // a future OSR request will pick up the resulting code object).
-  V8_WARN_UNUSED_RESULT static MaybeHandle<CodeT> CompileOptimizedOSR(
-      Isolate* isolate, Handle<JSFunction> function, BytecodeOffset osr_offset,
-      ConcurrencyMode mode);
+  V8_WARN_UNUSED_RESULT static MaybeHandle<CodeDataContainer>
+  CompileOptimizedOSR(Isolate* isolate, Handle<JSFunction> function,
+                      BytecodeOffset osr_offset, ConcurrencyMode mode);
 
   V8_WARN_UNUSED_RESULT static MaybeHandle<SharedFunctionInfo>
   CompileForLiveEdit(ParseInfo* parse_info, Handle<Script> script,
diff --git a/src/codegen/ia32/assembler-ia32-inl.h b/src/codegen/ia32/assembler-ia32-inl.h
index d09b33858d0..353010c00ac 100644
--- a/src/codegen/ia32/assembler-ia32-inl.h
+++ b/src/codegen/ia32/assembler-ia32-inl.h
@@ -166,7 +166,7 @@ void Assembler::emit(uint32_t x, RelocInfo::Mode rmode) {
   emit(x);
 }
 
-void Assembler::emit(Handle<CodeT> code, RelocInfo::Mode rmode) {
+void Assembler::emit(Handle<CodeDataContainer> code, RelocInfo::Mode rmode) {
   emit(code.address(), rmode);
 }
 
diff --git a/src/codegen/ia32/assembler-ia32.cc b/src/codegen/ia32/assembler-ia32.cc
index a2960bb7d01..ddbe5b82c02 100644
--- a/src/codegen/ia32/assembler-ia32.cc
+++ b/src/codegen/ia32/assembler-ia32.cc
@@ -1639,7 +1639,7 @@ void Assembler::call(Operand adr) {
   emit_operand(edx, adr);
 }
 
-void Assembler::call(Handle<CodeT> code, RelocInfo::Mode rmode) {
+void Assembler::call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode) {
   EnsureSpace ensure_space(this);
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   EMIT(0xE8);
@@ -1709,7 +1709,7 @@ void Assembler::jmp(Operand adr) {
   emit_operand(esp, adr);
 }
 
-void Assembler::jmp(Handle<CodeT> code, RelocInfo::Mode rmode) {
+void Assembler::jmp(Handle<CodeDataContainer> code, RelocInfo::Mode rmode) {
   EnsureSpace ensure_space(this);
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   EMIT(0xE9);
@@ -1769,7 +1769,8 @@ void Assembler::j(Condition cc, byte* entry, RelocInfo::Mode rmode) {
   emit(entry - (pc_ + sizeof(int32_t)), rmode);
 }
 
-void Assembler::j(Condition cc, Handle<CodeT> code, RelocInfo::Mode rmode) {
+void Assembler::j(Condition cc, Handle<CodeDataContainer> code,
+                  RelocInfo::Mode rmode) {
   EnsureSpace ensure_space(this);
   // 0000 1111 1000 tttn #32-bit disp
   EMIT(0x0F);
diff --git a/src/codegen/ia32/assembler-ia32.h b/src/codegen/ia32/assembler-ia32.h
index da9c112461f..782c30627af 100644
--- a/src/codegen/ia32/assembler-ia32.h
+++ b/src/codegen/ia32/assembler-ia32.h
@@ -746,7 +746,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void call(Address entry, RelocInfo::Mode rmode);
   void call(Register reg) { call(Operand(reg)); }
   void call(Operand adr);
-  void call(Handle<CodeT> code, RelocInfo::Mode rmode);
+  void call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode);
   void wasm_call(Address address, RelocInfo::Mode rmode);
 
   // Jumps
@@ -755,7 +755,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void jmp(Address entry, RelocInfo::Mode rmode);
   void jmp(Register reg) { jmp(Operand(reg)); }
   void jmp(Operand adr);
-  void jmp(Handle<CodeT> code, RelocInfo::Mode rmode);
+  void jmp(Handle<CodeDataContainer> code, RelocInfo::Mode rmode);
   // Unconditional jump relative to the current address. Low-level routine,
   // use with caution!
   void jmp_rel(int offset);
@@ -763,7 +763,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // Conditional jumps
   void j(Condition cc, Label* L, Label::Distance distance = Label::kFar);
   void j(Condition cc, byte* entry, RelocInfo::Mode rmode);
-  void j(Condition cc, Handle<CodeT> code,
+  void j(Condition cc, Handle<CodeDataContainer> code,
          RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);
 
   // Floating-point operations
@@ -1688,7 +1688,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   inline void emit(uint32_t x);
   inline void emit(Handle<HeapObject> handle);
   inline void emit(uint32_t x, RelocInfo::Mode rmode);
-  inline void emit(Handle<CodeT> code, RelocInfo::Mode rmode);
+  inline void emit(Handle<CodeDataContainer> code, RelocInfo::Mode rmode);
   inline void emit(const Immediate& x);
   inline void emit_b(Immediate x);
   inline void emit_w(const Immediate& x);
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index c6bbfcc9d9c..f3507c00f12 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -706,8 +706,10 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
   CompareRange(instance_type_out, lower_limit, higher_limit, scratch);
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimization(Register codet) {
-  test(FieldOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset),
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container) {
+  test(FieldOperand(code_data_container,
+                    CodeDataContainer::kKindSpecificFlagsOffset),
        Immediate(1 << Code::kMarkedForDeoptimizationBit));
 }
 
@@ -740,7 +742,7 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, bailout to a
   // given label.
-  __ TestCodeTIsMarkedForDeoptimization(optimized_code_entry);
+  __ TestCodeDataContainerIsMarkedForDeoptimization(optimized_code_entry);
   __ j(not_zero, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure
@@ -1282,7 +1284,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   // smarter.
   Move(kRuntimeCallArgCountRegister, Immediate(num_arguments));
   Move(kRuntimeCallFunctionRegister, Immediate(ExternalReference::Create(f)));
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -1314,7 +1317,7 @@ void MacroAssembler::JumpToExternalReference(const ExternalReference& ext,
   ASM_CODE_COMMENT(this);
   // Set the entry point and jump to the C entry runtime stub.
   Move(kRuntimeCallFunctionRegister, Immediate(ext));
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
@@ -1964,7 +1967,8 @@ void TurboAssembler::PushPC() {
   bind(&get_pc);
 }
 
-void TurboAssembler::Call(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
+void TurboAssembler::Call(Handle<CodeDataContainer> code_object,
+                          RelocInfo::Mode rmode) {
   ASM_CODE_COMMENT(this);
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
@@ -2012,7 +2016,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
       call(EntryFromBuiltinAsOperand(builtin));
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       call(code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -2033,7 +2038,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
       jmp(EntryFromBuiltinAsOperand(builtin));
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       jmp(code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -2089,7 +2095,8 @@ void TurboAssembler::Jump(const ExternalReference& reference) {
                                  isolate(), reference)));
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
+void TurboAssembler::Jump(Handle<CodeDataContainer> code_object,
+                          RelocInfo::Mode rmode) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   Builtin builtin = Builtin::kNoBuiltinId;
diff --git a/src/codegen/ia32/macro-assembler-ia32.h b/src/codegen/ia32/macro-assembler-ia32.h
index 8ddbe378a04..4e50c7784dc 100644
--- a/src/codegen/ia32/macro-assembler-ia32.h
+++ b/src/codegen/ia32/macro-assembler-ia32.h
@@ -149,7 +149,7 @@ class V8_EXPORT_PRIVATE TurboAssembler
   void Call(Register reg) { call(reg); }
   void Call(Operand op) { call(op); }
   void Call(Label* target) { call(target); }
-  void Call(Handle<CodeT> code_object, RelocInfo::Mode rmode);
+  void Call(Handle<CodeDataContainer> code_object, RelocInfo::Mode rmode);
 
   // Load the builtin given by the Smi in |builtin_index| into the same
   // register.
@@ -172,7 +172,7 @@ class V8_EXPORT_PRIVATE TurboAssembler
                                    JumpMode jump_mode = JumpMode::kJump);
 
   void Jump(const ExternalReference& reference);
-  void Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode);
+  void Jump(Handle<CodeDataContainer> code_object, RelocInfo::Mode rmode);
 
   void LoadMap(Register destination, Register object);
 
@@ -559,7 +559,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     and_(reg, Immediate(mask));
   }
 
-  void TestCodeTIsMarkedForDeoptimization(Register codet);
+  void TestCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container);
   Immediate ClearedValue() const;
 
   // Tiering support.
diff --git a/src/codegen/loong64/assembler-loong64-inl.h b/src/codegen/loong64/assembler-loong64-inl.h
index 910f7e86f40..0475506baa0 100644
--- a/src/codegen/loong64/assembler-loong64-inl.h
+++ b/src/codegen/loong64/assembler-loong64-inl.h
@@ -137,7 +137,7 @@ Address RelocInfo::target_internal_reference_address() {
   return pc_;
 }
 
-Handle<CodeT> Assembler::relative_code_target_object_handle_at(
+Handle<CodeDataContainer> Assembler::relative_code_target_object_handle_at(
     Address pc) const {
   Instr instr = Assembler::instr_at(pc);
   int32_t code_target_index = instr & kImm26Mask;
diff --git a/src/codegen/loong64/assembler-loong64.h b/src/codegen/loong64/assembler-loong64.h
index b5a16b4554f..d78aca52a91 100644
--- a/src/codegen/loong64/assembler-loong64.h
+++ b/src/codegen/loong64/assembler-loong64.h
@@ -824,7 +824,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void CheckTrampolinePool();
 
   // Get the code target object for a pc-relative call or jump.
-  V8_INLINE Handle<CodeT> relative_code_target_object_handle_at(
+  V8_INLINE Handle<CodeDataContainer> relative_code_target_object_handle_at(
       Address pc_) const;
 
   inline int UnboundLabelsCount() { return unbound_labels_count_; }
diff --git a/src/codegen/loong64/macro-assembler-loong64.cc b/src/codegen/loong64/macro-assembler-loong64.cc
index 304fea56a67..0b31867a215 100644
--- a/src/codegen/loong64/macro-assembler-loong64.cc
+++ b/src/codegen/loong64/macro-assembler-loong64.cc
@@ -2584,7 +2584,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode, Condition cond,
   Jump(static_cast<intptr_t>(target), rmode, cond, rj, rk);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, Register rj, const Operand& rk) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   BlockTrampolinePoolScope block_trampoline_pool(this);
@@ -2659,7 +2659,7 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode, Condition cond,
   bind(&skip);
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, Register rj, const Operand& rk) {
   BlockTrampolinePoolScope block_trampoline_pool(this);
   Builtin builtin = Builtin::kNoBuiltinId;
@@ -2718,7 +2718,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         RecordRelocInfo(RelocInfo::RELATIVE_CODE_TARGET);
         bl(code_target_index);
@@ -2754,7 +2755,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         RecordRelocInfo(RelocInfo::RELATIVE_CODE_TARGET);
         b(code_target_index);
@@ -3003,12 +3005,11 @@ void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch1,
   Branch(stack_overflow, le, scratch1, Operand(scratch2));
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimizationAndJump(Register codet,
-                                                               Register scratch,
-                                                               Condition cond,
-                                                               Label* target) {
-  Ld_wu(scratch,
-        FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset));
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+    Register code_data_container, Register scratch, Condition cond,
+    Label* target) {
+  Ld_wu(scratch, FieldMemOperand(code_data_container,
+                                 CodeDataContainer::kKindSpecificFlagsOffset));
   And(scratch, scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
   Branch(target, cond, scratch, Operand(zero_reg));
 }
@@ -3373,7 +3374,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   // smarter.
   PrepareCEntryArgs(num_arguments);
   PrepareCEntryFunction(ExternalReference::Create(f));
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -3390,7 +3392,7 @@ void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
 void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              bool builtin_exit_frame) {
   PrepareCEntryFunction(builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET, al, zero_reg, Operand(zero_reg));
 }
@@ -4195,8 +4197,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
-  __ TestCodeTIsMarkedForDeoptimizationAndJump(optimized_code_entry, a6, ne,
-                                               &heal_optimized_code_slot);
+  __ TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+      optimized_code_entry, a6, ne, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure into
   // the optimized functions list, then tail call the optimized code.
diff --git a/src/codegen/loong64/macro-assembler-loong64.h b/src/codegen/loong64/macro-assembler-loong64.h
index 9402f03decf..0a26c740113 100644
--- a/src/codegen/loong64/macro-assembler-loong64.h
+++ b/src/codegen/loong64/macro-assembler-loong64.h
@@ -188,12 +188,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // it to register use ld_d, it can be used in wasm jump table for concurrent
   // patching.
   void PatchAndJump(Address target);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, COND_ARGS);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode, COND_ARGS);
   void Jump(const ExternalReference& reference);
   void Call(Register target, COND_ARGS);
   void Call(Address target, RelocInfo::Mode rmode, COND_ARGS);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
-            COND_ARGS);
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET, COND_ARGS);
   void Call(Label* target);
 
   // Load the builtin given by the Smi in |builtin_index| into the same
@@ -832,9 +832,9 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // less efficient form using xor instead of mov is emitted.
   void Swap(Register reg1, Register reg2, Register scratch = no_reg);
 
-  void TestCodeTIsMarkedForDeoptimizationAndJump(Register codet,
-                                                 Register scratch,
-                                                 Condition cond, Label* target);
+  void TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+      Register code_data_container, Register scratch, Condition cond,
+      Label* target);
   Operand ClearedValue() const;
 
   void PushRoot(RootIndex index) {
diff --git a/src/codegen/mips64/macro-assembler-mips64.cc b/src/codegen/mips64/macro-assembler-mips64.cc
index 18f59514a64..f998aa23610 100644
--- a/src/codegen/mips64/macro-assembler-mips64.cc
+++ b/src/codegen/mips64/macro-assembler-mips64.cc
@@ -4326,7 +4326,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode, Condition cond,
   Jump(static_cast<intptr_t>(target), rmode, cond, rs, rt, bd);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, Register rs, const Operand& rt,
                           BranchDelaySlot bd) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
@@ -4399,7 +4399,7 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode, Condition cond,
   Call(t9, cond, rs, rt, bd);
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, Register rs, const Operand& rt,
                           BranchDelaySlot bd) {
   BlockTrampolinePoolScope block_trampoline_pool(this);
@@ -4454,7 +4454,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
       break;
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       IndirectLoadConstant(temp, code);
       CallCodeDataContainerObject(temp);
       break;
@@ -4482,7 +4483,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
       break;
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       IndirectLoadConstant(temp, code);
       JumpCodeDataContainerObject(temp);
       break;
@@ -4919,12 +4921,11 @@ void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch1,
   Branch(stack_overflow, le, scratch1, Operand(scratch2));
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimizationAndJump(Register codet,
-                                                               Register scratch,
-                                                               Condition cond,
-                                                               Label* target) {
-  Lwu(scratch,
-      FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset));
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+    Register code_data_container, Register scratch, Condition cond,
+    Label* target) {
+  Lwu(scratch, FieldMemOperand(code_data_container,
+                               CodeDataContainer::kKindSpecificFlagsOffset));
   And(scratch, scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
   Branch(target, cond, scratch, Operand(zero_reg));
 }
@@ -5277,7 +5278,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   // smarter.
   PrepareCEntryArgs(num_arguments);
   PrepareCEntryFunction(ExternalReference::Create(f));
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -5295,7 +5297,7 @@ void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              BranchDelaySlot bd,
                                              bool builtin_exit_frame) {
   PrepareCEntryFunction(builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET, al, zero_reg, Operand(zero_reg), bd);
 }
@@ -6242,8 +6244,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
-  __ TestCodeTIsMarkedForDeoptimizationAndJump(optimized_code_entry, scratch1,
-                                               ne, &heal_optimized_code_slot);
+  __ TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+      optimized_code_entry, scratch1, ne, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure into
   // the optimized functions list, then tail call the optimized code.
diff --git a/src/codegen/mips64/macro-assembler-mips64.h b/src/codegen/mips64/macro-assembler-mips64.h
index 68402cf3875..bff64ec7850 100644
--- a/src/codegen/mips64/macro-assembler-mips64.h
+++ b/src/codegen/mips64/macro-assembler-mips64.h
@@ -244,12 +244,12 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // it to register use ld, it can be used in wasm jump table for concurrent
   // patching.
   void PatchAndJump(Address target);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, COND_ARGS);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode, COND_ARGS);
   void Jump(const ExternalReference& reference);
   void Call(Register target, COND_ARGS);
   void Call(Address target, RelocInfo::Mode rmode, COND_ARGS);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
-            COND_ARGS);
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET, COND_ARGS);
   void Call(Label* target);
   void LoadAddress(Register dst, Label* target);
 
@@ -1005,9 +1005,9 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
   // less efficient form using xor instead of mov is emitted.
   void Swap(Register reg1, Register reg2, Register scratch = no_reg);
 
-  void TestCodeTIsMarkedForDeoptimizationAndJump(Register codet,
-                                                 Register scratch,
-                                                 Condition cond, Label* target);
+  void TestCodeDataContainerIsMarkedForDeoptimizationAndJump(
+      Register code_data_container, Register scratch, Condition cond,
+      Label* target);
   Operand ClearedValue() const;
 
   void PushRoot(RootIndex index) {
diff --git a/src/codegen/ppc/macro-assembler-ppc.cc b/src/codegen/ppc/macro-assembler-ppc.cc
index e87dfdd29bb..fbd9f79088b 100644
--- a/src/codegen/ppc/macro-assembler-ppc.cc
+++ b/src/codegen/ppc/macro-assembler-ppc.cc
@@ -189,7 +189,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode, Condition cond,
   Jump(static_cast<intptr_t>(target), rmode, cond, cr);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond, CRegister cr) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   DCHECK_IMPLIES(options().isolate_independent_code,
@@ -252,7 +252,7 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode,
   bctrl();
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond) {
   BlockTrampolinePoolScope block_trampoline_pool(this);
   DCHECK(RelocInfo::IsCodeTarget(rmode));
@@ -293,7 +293,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin, Condition cond) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         Call(static_cast<Address>(code_target_index), RelocInfo::CODE_TARGET,
              cond);
@@ -336,7 +337,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cond,
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         int32_t code_target_index = AddCodeTarget(code);
         Jump(static_cast<intptr_t>(code_target_index), RelocInfo::CODE_TARGET,
              cond, cr);
@@ -363,11 +365,11 @@ void TurboAssembler::Drop(Register count, Register scratch) {
   add(sp, sp, scratch);
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimization(Register codet,
-                                                        Register scratch1,
-                                                        Register scratch2) {
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container, Register scratch1, Register scratch2) {
   LoadS32(scratch1,
-          FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset),
+          FieldMemOperand(code_data_container,
+                          CodeDataContainer::kKindSpecificFlagsOffset),
           scratch2);
   TestBit(scratch1, Code::kMarkedForDeoptimizationBit, scratch2);
 }
@@ -2044,8 +2046,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   // runtime to clear it.
   {
     UseScratchRegisterScope temps(masm);
-    __ TestCodeTIsMarkedForDeoptimization(optimized_code_entry, temps.Acquire(),
-                                          scratch);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(optimized_code_entry,
+                                                      temps.Acquire(), scratch);
     __ bne(&heal_optimized_code_slot, cr0);
   }
 
@@ -2185,9 +2187,10 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   mov(r3, Operand(num_arguments));
   Move(r4, ExternalReference::Create(f));
 #if V8_TARGET_ARCH_PPC64
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
 #else
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), 1);
+  Handle<CodeDataContainer> code = CodeFactory::CEntry(isolate(), 1);
 #endif
   Call(code, RelocInfo::CODE_TARGET);
 }
@@ -2204,7 +2207,7 @@ void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
 void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              bool builtin_exit_frame) {
   Move(r4, builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
diff --git a/src/codegen/ppc/macro-assembler-ppc.h b/src/codegen/ppc/macro-assembler-ppc.h
index a07bd67538d..2f3ec3ac00f 100644
--- a/src/codegen/ppc/macro-assembler-ppc.h
+++ b/src/codegen/ppc/macro-assembler-ppc.h
@@ -716,14 +716,15 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   void Jump(Register target);
   void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al,
             CRegister cr = cr7);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, Condition cond = al,
-            CRegister cr = cr7);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
+            Condition cond = al, CRegister cr = cr7);
   void Jump(const ExternalReference& reference);
   void Jump(intptr_t target, RelocInfo::Mode rmode, Condition cond = al,
             CRegister cr = cr7);
   void Call(Register target);
   void Call(Address target, RelocInfo::Mode rmode, Condition cond = al);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
             Condition cond = al);
   void Call(Label* target);
 
@@ -1723,8 +1724,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     DecodeField<Field>(reg, reg, rc);
   }
 
-  void TestCodeTIsMarkedForDeoptimization(Register codet, Register scratch1,
-                                          Register scratch2);
+  void TestCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container, Register scratch1, Register scratch2);
   Operand ClearedValue() const;
 
  private:
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 2c8caca0c36..9741cb7446e 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -114,8 +114,8 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
-  __ JumpIfCodeTIsMarkedForDeoptimization(optimized_code_entry, scratch1,
-                                          &heal_optimized_code_slot);
+  __ JumpIfCodeDataContainerIsMarkedForDeoptimization(
+      optimized_code_entry, scratch1, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure into
   // the optimized functions list, then tail call the optimized code.
@@ -4448,7 +4448,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
     }
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         EmbeddedObjectIndex index = AddEmbeddedObject(code);
         DCHECK(is_int32(index));
         RecordRelocInfo(RelocInfo::RELATIVE_CODE_TARGET,
@@ -5733,10 +5734,12 @@ void TurboAssembler::JumpIfSmi(Register value, Label* smi_label) {
   Branch(smi_label, eq, scratch, Operand(zero_reg));
 }
 
-void MacroAssembler::JumpIfCodeTIsMarkedForDeoptimization(
-    Register codet, Register scratch, Label* if_marked_for_deoptimization) {
+void MacroAssembler::JumpIfCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container, Register scratch,
+    Label* if_marked_for_deoptimization) {
   LoadTaggedPointerField(
-      scratch, FieldMemOperand(codet, Code::kCodeDataContainerOffset));
+      scratch,
+      FieldMemOperand(code_data_container, Code::kCodeDataContainerOffset));
   Lw(scratch,
      FieldMemOperand(scratch, CodeDataContainer::kKindSpecificFlagsOffset));
   And(scratch, scratch, Operand(1 << Code::kMarkedForDeoptimizationBit));
diff --git a/src/codegen/riscv/macro-assembler-riscv.h b/src/codegen/riscv/macro-assembler-riscv.h
index cbe9c9dd6fe..fef42badf3e 100644
--- a/src/codegen/riscv/macro-assembler-riscv.h
+++ b/src/codegen/riscv/macro-assembler-riscv.h
@@ -1475,8 +1475,9 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
                                        ArgumentsCountType type,
                                        ArgumentsCountMode mode,
                                        Register scratch = no_reg);
-  void JumpIfCodeTIsMarkedForDeoptimization(
-      Register codet, Register scratch, Label* if_marked_for_deoptimization);
+  void JumpIfCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container, Register scratch,
+      Label* if_marked_for_deoptimization);
   Operand ClearedValue() const;
 
   // Jump if the register contains a non-smi.
diff --git a/src/codegen/s390/assembler-s390.cc b/src/codegen/s390/assembler-s390.cc
index 635fa41675c..c2608b04926 100644
--- a/src/codegen/s390/assembler-s390.cc
+++ b/src/codegen/s390/assembler-s390.cc
@@ -689,7 +689,7 @@ void Assembler::EnsureSpaceFor(int space_needed) {
   }
 }
 
-void Assembler::call(Handle<CodeT> target, RelocInfo::Mode rmode) {
+void Assembler::call(Handle<CodeDataContainer> target, RelocInfo::Mode rmode) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   EnsureSpace ensure_space(this);
 
@@ -698,7 +698,7 @@ void Assembler::call(Handle<CodeT> target, RelocInfo::Mode rmode) {
   brasl(r14, Operand(target_index));
 }
 
-void Assembler::jump(Handle<CodeT> target, RelocInfo::Mode rmode,
+void Assembler::jump(Handle<CodeDataContainer> target, RelocInfo::Mode rmode,
                      Condition cond) {
   DCHECK(RelocInfo::IsRelativeCodeTarget(rmode));
   EnsureSpace ensure_space(this);
diff --git a/src/codegen/s390/assembler-s390.h b/src/codegen/s390/assembler-s390.h
index 324d7c1ae96..cdde38d8ec4 100644
--- a/src/codegen/s390/assembler-s390.h
+++ b/src/codegen/s390/assembler-s390.h
@@ -1073,8 +1073,9 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
     basr(r14, r1);
   }
 
-  void call(Handle<CodeT> target, RelocInfo::Mode rmode);
-  void jump(Handle<CodeT> target, RelocInfo::Mode rmode, Condition cond);
+  void call(Handle<CodeDataContainer> target, RelocInfo::Mode rmode);
+  void jump(Handle<CodeDataContainer> target, RelocInfo::Mode rmode,
+            Condition cond);
 
 // S390 instruction generation
 #define DECLARE_VRR_A_INSTRUCTIONS(name, opcode_name, opcode_value)           \
diff --git a/src/codegen/s390/macro-assembler-s390.cc b/src/codegen/s390/macro-assembler-s390.cc
index 5479a5ae991..78e4c7c8e36 100644
--- a/src/codegen/s390/macro-assembler-s390.cc
+++ b/src/codegen/s390/macro-assembler-s390.cc
@@ -416,7 +416,7 @@ void TurboAssembler::Jump(Address target, RelocInfo::Mode rmode,
   Jump(static_cast<intptr_t>(target), rmode, cond);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   DCHECK_IMPLIES(options().isolate_independent_code,
@@ -469,7 +469,7 @@ void TurboAssembler::Call(Address target, RelocInfo::Mode rmode,
   basr(r14, ip);
 }
 
-void TurboAssembler::Call(Handle<CodeT> code, RelocInfo::Mode rmode,
+void TurboAssembler::Call(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
                           Condition cond) {
   DCHECK(RelocInfo::IsCodeTarget(rmode) && cond == al);
 
@@ -502,7 +502,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin, Condition cond) {
       Call(ip);
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       call(code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -528,7 +529,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cond) {
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
       if (options().use_pc_relative_calls_and_jumps_for_mksnapshot) {
-        Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+        Handle<CodeDataContainer> code =
+            isolate()->builtins()->code_handle(builtin);
         jump(code, RelocInfo::RELATIVE_CODE_TARGET, cond);
       } else {
         LoadU64(ip, EntryFromBuiltinAsOperand(builtin));
@@ -557,10 +559,11 @@ void TurboAssembler::Drop(Register count, Register scratch) {
   AddS64(sp, sp, scratch);
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimization(Register codet,
-                                                        Register scratch) {
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container, Register scratch) {
   LoadS32(scratch,
-          FieldMemOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset));
+          FieldMemOperand(code_data_container,
+                          CodeDataContainer::kKindSpecificFlagsOffset));
   TestBit(scratch, Code::kMarkedForDeoptimizationBit, scratch);
 }
 
@@ -2043,7 +2046,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
   {
-    __ TestCodeTIsMarkedForDeoptimization(optimized_code_entry, scratch);
+    __ TestCodeDataContainerIsMarkedForDeoptimization(optimized_code_entry,
+                                                      scratch);
     __ bne(&heal_optimized_code_slot);
   }
 
@@ -2182,9 +2186,10 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   mov(r2, Operand(num_arguments));
   Move(r3, ExternalReference::Create(f));
 #if V8_TARGET_ARCH_S390X
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
 #else
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), 1);
+  Handle<CodeDataContainer> code = CodeFactory::CEntry(isolate(), 1);
 #endif
 
   Call(code, RelocInfo::CODE_TARGET);
@@ -2202,7 +2207,7 @@ void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
 void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              bool builtin_exit_frame) {
   Move(r3, builtin);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
diff --git a/src/codegen/s390/macro-assembler-s390.h b/src/codegen/s390/macro-assembler-s390.h
index 9cee10c88b8..b0d4583c469 100644
--- a/src/codegen/s390/macro-assembler-s390.h
+++ b/src/codegen/s390/macro-assembler-s390.h
@@ -96,7 +96,8 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
   // Jump, Call, and Ret pseudo instructions implementing inter-working.
   void Jump(Register target, Condition cond = al);
   void Jump(Address target, RelocInfo::Mode rmode, Condition cond = al);
-  void Jump(Handle<CodeT> code, RelocInfo::Mode rmode, Condition cond = al);
+  void Jump(Handle<CodeDataContainer> code, RelocInfo::Mode rmode,
+            Condition cond = al);
   void Jump(const ExternalReference& reference);
   // Jump the register contains a smi.
   inline void JumpIfSmi(Register value, Label* smi_label) {
@@ -110,7 +111,8 @@ class V8_EXPORT_PRIVATE TurboAssembler : public TurboAssemblerBase {
 
   void Call(Register target);
   void Call(Address target, RelocInfo::Mode rmode, Condition cond = al);
-  void Call(Handle<CodeT> code, RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
+  void Call(Handle<CodeDataContainer> code,
+            RelocInfo::Mode rmode = RelocInfo::CODE_TARGET,
             Condition cond = al);
   void Ret() { b(r14); }
   void Ret(Condition cond) { b(cond, r14); }
@@ -1799,7 +1801,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
                    LinkRegisterStatus lr_status, SaveFPRegsMode save_fp,
                    SmiCheck smi_check = SmiCheck::kInline);
 
-  void TestCodeTIsMarkedForDeoptimization(Register codet, Register scratch);
+  void TestCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container, Register scratch);
   Operand ClearedValue() const;
 
  private:
diff --git a/src/codegen/x64/assembler-x64-inl.h b/src/codegen/x64/assembler-x64-inl.h
index 3aa9beb2340..2660c5815bf 100644
--- a/src/codegen/x64/assembler-x64-inl.h
+++ b/src/codegen/x64/assembler-x64-inl.h
@@ -226,7 +226,7 @@ int Assembler::deserialization_special_target_size(
   return kSpecialTargetSize;
 }
 
-Handle<CodeT> Assembler::code_target_object_handle_at(Address pc) {
+Handle<CodeDataContainer> Assembler::code_target_object_handle_at(Address pc) {
   return GetCodeTarget(ReadUnalignedValue<int32_t>(pc));
 }
 
diff --git a/src/codegen/x64/assembler-x64.cc b/src/codegen/x64/assembler-x64.cc
index 311850db2eb..4881006a703 100644
--- a/src/codegen/x64/assembler-x64.cc
+++ b/src/codegen/x64/assembler-x64.cc
@@ -1024,9 +1024,9 @@ void Assembler::call(Label* L) {
   }
 }
 
-void Assembler::call(Handle<CodeT> target, RelocInfo::Mode rmode) {
+void Assembler::call(Handle<CodeDataContainer> target, RelocInfo::Mode rmode) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
-  DCHECK(FromCodeT(*target).IsExecutable());
+  DCHECK(FromCodeDataContainer(*target).IsExecutable());
   EnsureSpace ensure_space(this);
   // 1110 1000 #32-bit disp.
   emit(0xE8);
@@ -1437,7 +1437,8 @@ void Assembler::j(Condition cc, Address entry, RelocInfo::Mode rmode) {
   emitl(static_cast<int32_t>(entry));
 }
 
-void Assembler::j(Condition cc, Handle<CodeT> target, RelocInfo::Mode rmode) {
+void Assembler::j(Condition cc, Handle<CodeDataContainer> target,
+                  RelocInfo::Mode rmode) {
   EnsureSpace ensure_space(this);
   DCHECK(is_uint4(cc));
   // 0000 1111 1000 tttn #32-bit disp.
@@ -1516,7 +1517,7 @@ void Assembler::jmp(Label* L, Label::Distance distance) {
   }
 }
 
-void Assembler::jmp(Handle<CodeT> target, RelocInfo::Mode rmode) {
+void Assembler::jmp(Handle<CodeDataContainer> target, RelocInfo::Mode rmode) {
   DCHECK(RelocInfo::IsCodeTarget(rmode));
   EnsureSpace ensure_space(this);
   // 1110 1001 #32-bit disp.
diff --git a/src/codegen/x64/assembler-x64.h b/src/codegen/x64/assembler-x64.h
index 1b34ae073af..edc5198b21a 100644
--- a/src/codegen/x64/assembler-x64.h
+++ b/src/codegen/x64/assembler-x64.h
@@ -478,7 +478,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
       Address pc, Address target,
       RelocInfo::Mode mode = RelocInfo::INTERNAL_REFERENCE);
 
-  inline Handle<CodeT> code_target_object_handle_at(Address pc);
+  inline Handle<CodeDataContainer> code_target_object_handle_at(Address pc);
   inline Handle<HeapObject> compressed_embedded_object_handle_at(Address pc);
 
   // Number of bytes taken up by the branch target in the code.
@@ -827,7 +827,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   void near_jmp(intptr_t disp, RelocInfo::Mode rmode);
   void near_j(Condition cc, intptr_t disp, RelocInfo::Mode rmode);
 
-  void call(Handle<CodeT> target,
+  void call(Handle<CodeDataContainer> target,
             RelocInfo::Mode rmode = RelocInfo::CODE_TARGET);
 
   // Call near absolute indirect, address in register
@@ -838,7 +838,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // Use a 32-bit signed displacement.
   // Unconditional jump to L
   void jmp(Label* L, Label::Distance distance = Label::kFar);
-  void jmp(Handle<CodeT> target, RelocInfo::Mode rmode);
+  void jmp(Handle<CodeDataContainer> target, RelocInfo::Mode rmode);
 
   // Jump near absolute indirect (r64)
   void jmp(Register adr);
@@ -851,7 +851,7 @@ class V8_EXPORT_PRIVATE Assembler : public AssemblerBase {
   // Conditional jumps
   void j(Condition cc, Label* L, Label::Distance distance = Label::kFar);
   void j(Condition cc, Address entry, RelocInfo::Mode rmode);
-  void j(Condition cc, Handle<CodeT> target, RelocInfo::Mode rmode);
+  void j(Condition cc, Handle<CodeDataContainer> target, RelocInfo::Mode rmode);
 
   // Floating-point operations
   void fld(int i);
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index 19ad424ecd9..483747cb173 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -575,7 +575,8 @@ void TurboAssembler::CallTSANStoreStub(Register address, Register value,
 
   if (isolate()) {
     Builtin builtin = CodeFactory::GetTSANStoreStub(fp_mode, size, order);
-    Handle<CodeT> code_target = isolate()->builtins()->code_handle(builtin);
+    Handle<CodeDataContainer> code_target =
+        isolate()->builtins()->code_handle(builtin);
     Call(code_target, RelocInfo::CODE_TARGET);
   }
 #if V8_ENABLE_WEBASSEMBLY
@@ -616,7 +617,8 @@ void TurboAssembler::CallTSANRelaxedLoadStub(Register address,
 
   if (isolate()) {
     Builtin builtin = CodeFactory::GetTSANRelaxedLoadStub(fp_mode, size);
-    Handle<CodeT> code_target = isolate()->builtins()->code_handle(builtin);
+    Handle<CodeDataContainer> code_target =
+        isolate()->builtins()->code_handle(builtin);
     Call(code_target, RelocInfo::CODE_TARGET);
   }
 #if V8_ENABLE_WEBASSEMBLY
@@ -776,7 +778,8 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f,
   // smarter.
   Move(rax, num_arguments);
   LoadAddress(rbx, ExternalReference::Create(f));
-  Handle<CodeT> code = CodeFactory::CEntry(isolate(), f->result_size);
+  Handle<CodeDataContainer> code =
+      CodeFactory::CEntry(isolate(), f->result_size);
   Call(code, RelocInfo::CODE_TARGET);
 }
 
@@ -804,7 +807,7 @@ void MacroAssembler::JumpToExternalReference(const ExternalReference& ext,
   ASM_CODE_COMMENT(this);
   // Set the entry point and jump to the C entry runtime stub.
   LoadAddress(rbx, ext);
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       CodeFactory::CEntry(isolate(), 1, ArgvMode::kStack, builtin_exit_frame);
   Jump(code, RelocInfo::CODE_TARGET);
 }
@@ -834,8 +837,8 @@ void TailCallOptimizedCodeSlot(MacroAssembler* masm,
 
   // Check if the optimized code is marked for deopt. If it is, call the
   // runtime to clear it.
-  __ AssertCodeT(optimized_code_entry);
-  __ TestCodeTIsMarkedForDeoptimization(optimized_code_entry);
+  __ AssertCodeDataContainer(optimized_code_entry);
+  __ TestCodeDataContainerIsMarkedForDeoptimization(optimized_code_entry);
   __ j(not_zero, &heal_optimized_code_slot);
 
   // Optimized code is good, get it into the closure and link the closure into
@@ -903,7 +906,7 @@ void MacroAssembler::ReplaceClosureCodeWithOptimizedCode(
   DCHECK(!AreAliased(optimized_code, closure, scratch1, slot_address));
   DCHECK_EQ(closure, kJSFunctionRegister);
   // Store the optimized code in the closure.
-  AssertCodeT(optimized_code);
+  AssertCodeDataContainer(optimized_code);
   StoreTaggedField(FieldOperand(closure, JSFunction::kCodeOffset),
                    optimized_code);
   // Write barrier clobbers scratch1 below.
@@ -2144,7 +2147,8 @@ void TurboAssembler::Jump(Address destination, RelocInfo::Mode rmode,
   bind(&skip);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
+void TurboAssembler::Jump(Handle<CodeDataContainer> code_object,
+                          RelocInfo::Mode rmode) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   Builtin builtin = Builtin::kNoBuiltinId;
@@ -2156,8 +2160,8 @@ void TurboAssembler::Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
   jmp(code_object, rmode);
 }
 
-void TurboAssembler::Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode,
-                          Condition cc) {
+void TurboAssembler::Jump(Handle<CodeDataContainer> code_object,
+                          RelocInfo::Mode rmode, Condition cc) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   Builtin builtin = Builtin::kNoBuiltinId;
@@ -2193,7 +2197,8 @@ void TurboAssembler::Call(Address destination, RelocInfo::Mode rmode) {
   call(kScratchRegister);
 }
 
-void TurboAssembler::Call(Handle<CodeT> code_object, RelocInfo::Mode rmode) {
+void TurboAssembler::Call(Handle<CodeDataContainer> code_object,
+                          RelocInfo::Mode rmode) {
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   Builtin builtin = Builtin::kNoBuiltinId;
@@ -2244,7 +2249,8 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
       Call(EntryFromBuiltinAsOperand(builtin));
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       call(code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -2265,7 +2271,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
       Jump(EntryFromBuiltinAsOperand(builtin));
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       jmp(code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -2286,7 +2293,8 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin, Condition cc) {
       Jump(EntryFromBuiltinAsOperand(builtin), cc);
       break;
     case BuiltinCallJumpMode::kForMksnapshot: {
-      Handle<CodeT> code = isolate()->builtins()->code_handle(builtin);
+      Handle<CodeDataContainer> code =
+          isolate()->builtins()->code_handle(builtin);
       j(cc, code, RelocInfo::CODE_TARGET);
       break;
     }
@@ -2598,8 +2606,10 @@ void MacroAssembler::CmpInstanceTypeRange(Register map,
   CompareRange(instance_type_out, lower_limit, higher_limit);
 }
 
-void MacroAssembler::TestCodeTIsMarkedForDeoptimization(Register codet) {
-  testl(FieldOperand(codet, CodeDataContainer::kKindSpecificFlagsOffset),
+void MacroAssembler::TestCodeDataContainerIsMarkedForDeoptimization(
+    Register code_data_container) {
+  testl(FieldOperand(code_data_container,
+                     CodeDataContainer::kKindSpecificFlagsOffset),
         Immediate(1 << Code::kMarkedForDeoptimizationBit));
 }
 
@@ -2647,16 +2657,16 @@ void TurboAssembler::AssertSignedBitOfSmiIsZero(Register smi_register) {
   Check(zero, AbortReason::kSignedBitOfSmiIsNotZero);
 }
 
-void TurboAssembler::AssertCodeT(Register object) {
+void TurboAssembler::AssertCodeDataContainer(Register object) {
   if (!v8_flags.debug_code) return;
   ASM_CODE_COMMENT(this);
   testb(object, Immediate(kSmiTagMask));
-  Check(not_equal, AbortReason::kOperandIsNotACodeT);
+  Check(not_equal, AbortReason::kOperandIsNotACodeDataContainer);
   Push(object);
   LoadMap(object, object);
-  CmpInstanceType(object, CODET_TYPE);
+  CmpInstanceType(object, CODE_DATA_CONTAINER_TYPE);
   popq(object);
-  Check(equal, AbortReason::kOperandIsNotACodeT);
+  Check(equal, AbortReason::kOperandIsNotACodeDataContainer);
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
diff --git a/src/codegen/x64/macro-assembler-x64.h b/src/codegen/x64/macro-assembler-x64.h
index a38f214ba91..c06ed63819b 100644
--- a/src/codegen/x64/macro-assembler-x64.h
+++ b/src/codegen/x64/macro-assembler-x64.h
@@ -388,7 +388,7 @@ class V8_EXPORT_PRIVATE TurboAssembler
 
   void Call(Register reg) { call(reg); }
   void Call(Operand op);
-  void Call(Handle<CodeT> code_object, RelocInfo::Mode rmode);
+  void Call(Handle<CodeDataContainer> code_object, RelocInfo::Mode rmode);
   void Call(Address destination, RelocInfo::Mode rmode);
   void Call(ExternalReference ext);
   void Call(Label* target) { call(target); }
@@ -418,8 +418,9 @@ class V8_EXPORT_PRIVATE TurboAssembler
   void Jump(const ExternalReference& reference);
   void Jump(Operand op);
   void Jump(Operand op, Condition cc);
-  void Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode);
-  void Jump(Handle<CodeT> code_object, RelocInfo::Mode rmode, Condition cc);
+  void Jump(Handle<CodeDataContainer> code_object, RelocInfo::Mode rmode);
+  void Jump(Handle<CodeDataContainer> code_object, RelocInfo::Mode rmode,
+            Condition cc);
 
   void BailoutIfDeoptimized(Register scratch);
   void CallForDeoptimization(Builtin target, int deopt_id, Label* exit,
@@ -473,11 +474,12 @@ class V8_EXPORT_PRIVATE TurboAssembler
   // Always use unsigned comparisons: above and below, not less and greater.
   void CmpInstanceType(Register map, InstanceType type);
 
-  // Abort execution if argument is not a CodeT, enabled via --debug-code.
-  void AssertCodeT(Register object) NOOP_UNLESS_DEBUG_CODE
+  // Abort execution if argument is not a CodeDataContainer, enabled via
+  // --debug-code.
+  void AssertCodeDataContainer(Register object) NOOP_UNLESS_DEBUG_CODE
 
-  // Print a message to stdout and abort execution.
-  void Abort(AbortReason msg);
+      // Print a message to stdout and abort execution.
+      void Abort(AbortReason msg);
 
   // Check that the stack is aligned.
   void CheckStackAlignment();
@@ -828,7 +830,8 @@ class V8_EXPORT_PRIVATE MacroAssembler : public TurboAssembler {
     andq(reg, Immediate(mask));
   }
 
-  void TestCodeTIsMarkedForDeoptimization(Register codet);
+  void TestCodeDataContainerIsMarkedForDeoptimization(
+      Register code_data_container);
   Immediate ClearedValue() const;
 
   // Tiering support.
diff --git a/src/common/globals.h b/src/common/globals.h
index b47e65131ce..17fabafbcd5 100644
--- a/src/common/globals.h
+++ b/src/common/globals.h
@@ -235,11 +235,6 @@ const size_t kShortBuiltinCallsOldSpaceSizeThreshold = size_t{2} * GB;
 #define V8_EXTERNAL_CODE_SPACE_BOOL false
 #endif
 
-// TODO(jgruber): Remove the CodeT alias, rename CodeDataContainer to Code and
-// Code to CodeInstructions (or similar).
-class CodeDataContainer;
-using CodeT = CodeDataContainer;
-
 // V8_HEAP_USE_PTHREAD_JIT_WRITE_PROTECT controls how V8 sets permissions for
 // executable pages.
 // In particular,
diff --git a/src/compiler/backend/code-generator-impl.h b/src/compiler/backend/code-generator-impl.h
index df463396e54..d18be05378d 100644
--- a/src/compiler/backend/code-generator-impl.h
+++ b/src/compiler/backend/code-generator-impl.h
@@ -90,7 +90,7 @@ class InstructionOperandConverter {
     return ToExternalReference(instr_->InputAt(index));
   }
 
-  Handle<CodeT> InputCode(size_t index) {
+  Handle<CodeDataContainer> InputCode(size_t index) {
     return ToCode(instr_->InputAt(index));
   }
 
@@ -172,7 +172,7 @@ class InstructionOperandConverter {
     return ToConstant(op).ToExternalReference();
   }
 
-  Handle<CodeT> ToCode(InstructionOperand* op) {
+  Handle<CodeDataContainer> ToCode(InstructionOperand* op) {
     return ToConstant(op).ToCode();
   }
 
diff --git a/src/compiler/backend/ia32/code-generator-ia32.cc b/src/compiler/backend/ia32/code-generator-ia32.cc
index 5c9519a808c..58ad77f6bb8 100644
--- a/src/compiler/backend/ia32/code-generator-ia32.cc
+++ b/src/compiler/backend/ia32/code-generator-ia32.cc
@@ -686,7 +686,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     case kArchCallCodeObject: {
       InstructionOperand* op = instr->InputAt(0);
       if (op->IsImmediate()) {
-        Handle<CodeT> code = i.InputCode(0);
+        Handle<CodeDataContainer> code = i.InputCode(0);
         __ Call(code, RelocInfo::CODE_TARGET);
       } else {
         Register reg = i.InputRegister(0);
@@ -739,7 +739,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
 #endif  // V8_ENABLE_WEBASSEMBLY
     case kArchTailCallCodeObject: {
       if (HasImmediateInput(instr, 0)) {
-        Handle<CodeT> code = i.InputCode(0);
+        Handle<CodeDataContainer> code = i.InputCode(0);
         __ Jump(code, RelocInfo::CODE_TARGET);
       } else {
         Register reg = i.InputRegister(0);
diff --git a/src/compiler/backend/instruction.cc b/src/compiler/backend/instruction.cc
index 98def0c78b6..c9eda634c56 100644
--- a/src/compiler/backend/instruction.cc
+++ b/src/compiler/backend/instruction.cc
@@ -581,11 +581,11 @@ Handle<HeapObject> Constant::ToHeapObject() const {
   return value;
 }
 
-Handle<CodeT> Constant::ToCode() const {
+Handle<CodeDataContainer> Constant::ToCode() const {
   DCHECK_EQ(kHeapObject, type());
-  Handle<CodeT> value(
+  Handle<CodeDataContainer> value(
       reinterpret_cast<Address*>(static_cast<intptr_t>(value_)));
-  DCHECK(value->IsCodeT(GetPtrComprCageBaseSlow(*value)));
+  DCHECK(value->IsCodeDataContainer(GetPtrComprCageBaseSlow(*value)));
   return value;
 }
 
diff --git a/src/compiler/backend/instruction.h b/src/compiler/backend/instruction.h
index e9f0f9514bd..0d2d161fbcd 100644
--- a/src/compiler/backend/instruction.h
+++ b/src/compiler/backend/instruction.h
@@ -1192,7 +1192,7 @@ class V8_EXPORT_PRIVATE Constant final {
   }
 
   Handle<HeapObject> ToHeapObject() const;
-  Handle<CodeT> ToCode() const;
+  Handle<CodeDataContainer> ToCode() const;
 
  private:
   Type type_;
diff --git a/src/compiler/backend/x64/code-generator-x64.cc b/src/compiler/backend/x64/code-generator-x64.cc
index 0e49be77445..59d6dac02c8 100644
--- a/src/compiler/backend/x64/code-generator-x64.cc
+++ b/src/compiler/backend/x64/code-generator-x64.cc
@@ -1263,7 +1263,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
     }
     case kArchCallCodeObject: {
       if (HasImmediateInput(instr, 0)) {
-        Handle<CodeT> code = i.InputCode(0);
+        Handle<CodeDataContainer> code = i.InputCode(0);
         __ Call(code, RelocInfo::CODE_TARGET);
       } else {
         Register reg = i.InputRegister(0);
@@ -1323,7 +1323,7 @@ CodeGenerator::CodeGenResult CodeGenerator::AssembleArchInstruction(
 #endif  // V8_ENABLE_WEBASSEMBLY
     case kArchTailCallCodeObject: {
       if (HasImmediateInput(instr, 0)) {
-        Handle<CodeT> code = i.InputCode(0);
+        Handle<CodeDataContainer> code = i.InputCode(0);
         __ Jump(code, RelocInfo::CODE_TARGET);
       } else {
         Register reg = i.InputRegister(0);
diff --git a/src/compiler/bytecode-graph-builder.cc b/src/compiler/bytecode-graph-builder.cc
index adf8b1d7a5e..08f19a590ae 100644
--- a/src/compiler/bytecode-graph-builder.cc
+++ b/src/compiler/bytecode-graph-builder.cc
@@ -2202,7 +2202,7 @@ void BytecodeGraphBuilder::VisitCreateClosure() {
           bytecode_iterator().GetFlag8Operand(2))
           ? AllocationType::kOld
           : AllocationType::kYoung;
-  CodeTRef compile_lazy =
+  CodeDataContainerRef compile_lazy =
       MakeRef(broker(), *BUILTIN_CODE(jsgraph()->isolate(), CompileLazy));
   const Operator* op =
       javascript()->CreateClosure(shared_info, compile_lazy, allocation);
diff --git a/src/compiler/code-assembler.cc b/src/compiler/code-assembler.cc
index 5269b54b4d6..ccfefd98a6f 100644
--- a/src/compiler/code-assembler.cc
+++ b/src/compiler/code-assembler.cc
@@ -1000,7 +1000,7 @@ Node* CodeAssembler::CallRuntimeImpl(
     Runtime::FunctionId function, TNode<Object> context,
     std::initializer_list<TNode<Object>> args) {
   int result_size = Runtime::FunctionForId(function)->result_size;
-  TNode<CodeT> centry =
+  TNode<CodeDataContainer> centry =
       HeapConstant(CodeFactory::RuntimeCEntry(isolate(), result_size));
   constexpr size_t kMaxNumArgs = 6;
   DCHECK_GE(kMaxNumArgs, args.size());
@@ -1033,7 +1033,7 @@ void CodeAssembler::TailCallRuntimeImpl(
     Runtime::FunctionId function, TNode<Int32T> arity, TNode<Object> context,
     std::initializer_list<TNode<Object>> args) {
   int result_size = Runtime::FunctionForId(function)->result_size;
-  TNode<CodeT> centry =
+  TNode<CodeDataContainer> centry =
       HeapConstant(CodeFactory::RuntimeCEntry(isolate(), result_size));
   constexpr size_t kMaxNumArgs = 6;
   DCHECK_GE(kMaxNumArgs, args.size());
@@ -1089,7 +1089,8 @@ Node* CodeAssembler::CallStubN(StubCallMode call_mode,
 }
 
 void CodeAssembler::TailCallStubImpl(const CallInterfaceDescriptor& descriptor,
-                                     TNode<CodeT> target, TNode<Object> context,
+                                     TNode<CodeDataContainer> target,
+                                     TNode<Object> context,
                                      std::initializer_list<Node*> args) {
   constexpr size_t kMaxNumArgs = 11;
   DCHECK_GE(kMaxNumArgs, args.size());
@@ -1194,7 +1195,8 @@ template V8_EXPORT_PRIVATE void CodeAssembler::TailCallBytecodeDispatch(
     TNode<Object>, TNode<IntPtrT>, TNode<BytecodeArray>,
     TNode<ExternalReference>);
 
-void CodeAssembler::TailCallJSCode(TNode<CodeT> code, TNode<Context> context,
+void CodeAssembler::TailCallJSCode(TNode<CodeDataContainer> code,
+                                   TNode<Context> context,
                                    TNode<JSFunction> function,
                                    TNode<Object> new_target,
                                    TNode<Int32T> arg_count) {
diff --git a/src/compiler/code-assembler.h b/src/compiler/code-assembler.h
index 0bd840f8191..8d9e7047298 100644
--- a/src/compiler/code-assembler.h
+++ b/src/compiler/code-assembler.h
@@ -1170,13 +1170,14 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   template <class T = Object, class... TArgs>
   TNode<T> CallStub(Callable const& callable, TNode<Object> context,
                     TArgs... args) {
-    TNode<CodeT> target = HeapConstant(callable.code());
+    TNode<CodeDataContainer> target = HeapConstant(callable.code());
     return CallStub<T>(callable.descriptor(), target, context, args...);
   }
 
   template <class T = Object, class... TArgs>
   TNode<T> CallStub(const CallInterfaceDescriptor& descriptor,
-                    TNode<CodeT> target, TNode<Object> context, TArgs... args) {
+                    TNode<CodeDataContainer> target, TNode<Object> context,
+                    TArgs... args) {
     return UncheckedCast<T>(CallStubR(StubCallMode::kCallCodeObject, descriptor,
                                       target, context, args...));
   }
@@ -1192,13 +1193,14 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   template <class... TArgs>
   void TailCallStub(Callable const& callable, TNode<Object> context,
                     TArgs... args) {
-    TNode<CodeT> target = HeapConstant(callable.code());
+    TNode<CodeDataContainer> target = HeapConstant(callable.code());
     TailCallStub(callable.descriptor(), target, context, args...);
   }
 
   template <class... TArgs>
   void TailCallStub(const CallInterfaceDescriptor& descriptor,
-                    TNode<CodeT> target, TNode<Object> context, TArgs... args) {
+                    TNode<CodeDataContainer> target, TNode<Object> context,
+                    TArgs... args) {
     TailCallStubImpl(descriptor, target, context, {args...});
   }
 
@@ -1221,7 +1223,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
   // Note that no arguments adaption is going on here - all the JavaScript
   // arguments are left on the stack unmodified. Therefore, this tail call can
   // only be used after arguments adaptation has been performed already.
-  void TailCallJSCode(TNode<CodeT> code, TNode<Context> context,
+  void TailCallJSCode(TNode<CodeDataContainer> code, TNode<Context> context,
                       TNode<JSFunction> function, TNode<Object> new_target,
                       TNode<Int32T> arg_count);
 
@@ -1230,7 +1232,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
                        Node* receiver, TArgs... args) {
     int argc = JSParameterCount(static_cast<int>(sizeof...(args)));
     TNode<Int32T> arity = Int32Constant(argc);
-    TNode<CodeT> target = HeapConstant(callable.code());
+    TNode<CodeDataContainer> target = HeapConstant(callable.code());
     return CAST(CallJSStubImpl(callable.descriptor(), target, CAST(context),
                                CAST(function), {}, arity, {receiver, args...}));
   }
@@ -1241,7 +1243,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
     int argc = JSParameterCount(static_cast<int>(sizeof...(args)));
     TNode<Int32T> arity = Int32Constant(argc);
     TNode<Object> receiver = LoadRoot(RootIndex::kUndefinedValue);
-    TNode<CodeT> target = HeapConstant(callable.code());
+    TNode<CodeDataContainer> target = HeapConstant(callable.code());
     return CallJSStubImpl(callable.descriptor(), target, CAST(context),
                           CAST(function), CAST(new_target), arity,
                           {receiver, args...});
@@ -1340,7 +1342,7 @@ class V8_EXPORT_PRIVATE CodeAssembler {
                            std::initializer_list<TNode<Object>> args);
 
   void TailCallStubImpl(const CallInterfaceDescriptor& descriptor,
-                        TNode<CodeT> target, TNode<Object> context,
+                        TNode<CodeDataContainer> target, TNode<Object> context,
                         std::initializer_list<Node*> args);
 
   void TailCallStubThenBytecodeDispatchImpl(
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index bd02cd16399..40ae942bf0a 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -1048,10 +1048,6 @@ ObjectData* JSHeapBroker::TryGetOrCreateData(Handle<Object> object,
 HEAP_BROKER_OBJECT_LIST(DEFINE_IS_AND_AS)
 #undef DEFINE_IS_AND_AS
 
-bool ObjectRef::IsCodeT() const { return IsCodeDataContainer(); }
-
-CodeTRef ObjectRef::AsCodeT() const { return AsCodeDataContainer(); }
-
 bool ObjectRef::IsSmi() const { return data()->is_smi(); }
 
 int ObjectRef::AsSmi() const {
@@ -2204,8 +2200,8 @@ BIMODAL_ACCESSOR(JSFunction, SharedFunctionInfo, shared)
 #undef JSFUNCTION_BIMODAL_ACCESSOR_WITH_DEP
 #undef JSFUNCTION_BIMODAL_ACCESSOR_WITH_DEP_C
 
-CodeTRef JSFunctionRef::code() const {
-  CodeT code = object()->code(kAcquireLoad);
+CodeDataContainerRef JSFunctionRef::code() const {
+  CodeDataContainer code = object()->code(kAcquireLoad);
   return MakeRefAssumeMemoryFence(broker(), code);
 }
 
@@ -2307,14 +2303,14 @@ unsigned CodeRef::GetInlinedBytecodeSize() const {
 }
 
 unsigned CodeDataContainerRef::GetInlinedBytecodeSize() const {
-  CodeDataContainer codet = *object();
-  if (codet.is_off_heap_trampoline()) {
+  CodeDataContainer code_data_container = *object();
+  if (code_data_container.is_off_heap_trampoline()) {
     return 0;
   }
 
-  // Safe to do a relaxed conversion to Code here since CodeT::code field is
-  // modified only by GC and the CodeT was acquire-loaded.
-  Code code = codet.code(kRelaxedLoad);
+  // Safe to do a relaxed conversion to Code here since CodeDataContainer::code
+  // field is modified only by GC and the CodeDataContainer was acquire-loaded.
+  Code code = code_data_container.code(kRelaxedLoad);
   return GetInlinedBytecodeSizeImpl(code);
 }
 
diff --git a/src/compiler/heap-refs.h b/src/compiler/heap-refs.h
index 22159eef1d6..4ced0cdbe09 100644
--- a/src/compiler/heap-refs.h
+++ b/src/compiler/heap-refs.h
@@ -208,9 +208,6 @@ class TinyRef {
 HEAP_BROKER_OBJECT_LIST(V)
 #undef V
 
-using CodeTRef = CodeDataContainerRef;
-using CodeTTinyRef = CodeDataContainerTinyRef;
-
 class V8_EXPORT_PRIVATE ObjectRef {
  public:
   ObjectRef(JSHeapBroker* broker, ObjectData* data, bool check_type = true)
@@ -233,14 +230,6 @@ class V8_EXPORT_PRIVATE ObjectRef {
   HEAP_BROKER_OBJECT_LIST(HEAP_AS_METHOD_DECL)
 #undef HEAP_AS_METHOD_DECL
 
-  // CodeT is defined as an alias to either CodeDataContainer or Code, depending
-  // on the architecture. We can't put it in HEAP_BROKER_OBJECT_LIST, because
-  // this list already contains CodeDataContainer and Code. Still, defining
-  // IsCodeT and AsCodeT is useful to write code that is independent of
-  // V8_EXTERNAL_CODE_SPACE.
-  bool IsCodeT() const;
-  CodeTRef AsCodeT() const;
-
   bool IsNull() const;
   bool IsNullOrUndefined() const;
   bool IsTheHole() const;
@@ -470,7 +459,7 @@ class V8_EXPORT_PRIVATE JSFunctionRef : public JSObjectRef {
   ContextRef context() const;
   NativeContextRef native_context() const;
   SharedFunctionInfoRef shared() const;
-  CodeTRef code() const;
+  CodeDataContainerRef code() const;
 
   bool has_initial_map(CompilationDependencies* dependencies) const;
   bool PrototypeRequiresRuntimeLookup(
diff --git a/src/compiler/js-call-reducer.cc b/src/compiler/js-call-reducer.cc
index 3a757c6fdb7..6c2e82fa970 100644
--- a/src/compiler/js-call-reducer.cc
+++ b/src/compiler/js-call-reducer.cc
@@ -586,7 +586,7 @@ class PromiseBuiltinReducerAssembler : public JSCallReducerAssembler {
         isolate()->factory()->many_closures_cell();
     Callable const callable =
         Builtins::CallableFor(isolate(), shared.builtin_id());
-    CodeTRef code = MakeRef(broker_, *callable.code());
+    CodeDataContainerRef code = MakeRef(broker_, *callable.code());
     return AddNode<JSFunction>(graph()->NewNode(
         javascript()->CreateClosure(shared, code), HeapConstant(feedback_cell),
         context, effect(), control()));
@@ -6909,7 +6909,7 @@ Node* JSCallReducer::CreateClosureFromBuiltinSharedFunctionInfo(
       isolate()->factory()->many_closures_cell();
   Callable const callable =
       Builtins::CallableFor(isolate(), shared.builtin_id());
-  CodeTRef code = MakeRef(broker(), *callable.code());
+  CodeDataContainerRef code = MakeRef(broker(), *callable.code());
   return graph()->NewNode(javascript()->CreateClosure(shared, code),
                           jsgraph()->HeapConstant(feedback_cell), context,
                           effect, control);
diff --git a/src/compiler/js-operator.cc b/src/compiler/js-operator.cc
index b72fe0a6669..06ab9cd543c 100644
--- a/src/compiler/js-operator.cc
+++ b/src/compiler/js-operator.cc
@@ -1310,7 +1310,7 @@ const Operator* JSOperatorBuilder::CreateBoundFunction(size_t arity,
 }
 
 const Operator* JSOperatorBuilder::CreateClosure(
-    const SharedFunctionInfoRef& shared_info, const CodeTRef& code,
+    const SharedFunctionInfoRef& shared_info, const CodeDataContainerRef& code,
     AllocationType allocation) {
   static constexpr int kFeedbackCell = 1;
   static constexpr int kArity = kFeedbackCell;
diff --git a/src/compiler/js-operator.h b/src/compiler/js-operator.h
index 53cf1474794..32cfcb254d3 100644
--- a/src/compiler/js-operator.h
+++ b/src/compiler/js-operator.h
@@ -676,18 +676,21 @@ const CreateBoundFunctionParameters& CreateBoundFunctionParametersOf(
 class CreateClosureParameters final {
  public:
   CreateClosureParameters(const SharedFunctionInfoRef& shared_info,
-                          const CodeTRef& code, AllocationType allocation)
+                          const CodeDataContainerRef& code,
+                          AllocationType allocation)
       : shared_info_(shared_info), code_(code), allocation_(allocation) {}
 
   SharedFunctionInfoRef shared_info(JSHeapBroker* broker) const {
     return shared_info_.AsRef(broker);
   }
-  CodeTRef code(JSHeapBroker* broker) const { return code_.AsRef(broker); }
+  CodeDataContainerRef code(JSHeapBroker* broker) const {
+    return code_.AsRef(broker);
+  }
   AllocationType allocation() const { return allocation_; }
 
  private:
   const SharedFunctionInfoTinyRef shared_info_;
-  const CodeTTinyRef code_;
+  const CodeDataContainerTinyRef code_;
   AllocationType const allocation_;
 
   friend bool operator==(CreateClosureParameters const&,
@@ -950,7 +953,8 @@ class V8_EXPORT_PRIVATE JSOperatorBuilder final
   const Operator* CreateCollectionIterator(CollectionKind, IterationKind);
   const Operator* CreateBoundFunction(size_t arity, const MapRef& map);
   const Operator* CreateClosure(
-      const SharedFunctionInfoRef& shared_info, const CodeTRef& code,
+      const SharedFunctionInfoRef& shared_info,
+      const CodeDataContainerRef& code,
       AllocationType allocation = AllocationType::kYoung);
   const Operator* CreateIterResultObject();
   const Operator* CreateStringIterator();
diff --git a/src/compiler/turboshaft/assembler.cc b/src/compiler/turboshaft/assembler.cc
index b7d9fd7e544..4ffd234f5f1 100644
--- a/src/compiler/turboshaft/assembler.cc
+++ b/src/compiler/turboshaft/assembler.cc
@@ -9,7 +9,7 @@
 
 namespace v8::internal::compiler::turboshaft {
 
-Handle<CodeT> BuiltinCodeHandle(Builtin builtin, Isolate* isolate) {
+Handle<CodeDataContainer> BuiltinCodeHandle(Builtin builtin, Isolate* isolate) {
   return isolate->builtins()->code_handle(builtin);
 }
 
diff --git a/src/compiler/turboshaft/assembler.h b/src/compiler/turboshaft/assembler.h
index aa671d5af4c..ffa6c99738e 100644
--- a/src/compiler/turboshaft/assembler.h
+++ b/src/compiler/turboshaft/assembler.h
@@ -33,7 +33,7 @@ enum class Builtin : int32_t;
 
 namespace v8::internal::compiler::turboshaft {
 
-Handle<CodeT> BuiltinCodeHandle(Builtin builtin, Isolate* isolate);
+Handle<CodeDataContainer> BuiltinCodeHandle(Builtin builtin, Isolate* isolate);
 
 // Forward declarations
 template <class Assembler>
diff --git a/src/compiler/wasm-compiler.cc b/src/compiler/wasm-compiler.cc
index a1e4caf0e61..9f1b6421895 100644
--- a/src/compiler/wasm-compiler.cc
+++ b/src/compiler/wasm-compiler.cc
@@ -8414,8 +8414,9 @@ MaybeHandle<Code> CompileJSToJSWrapper(Isolate* isolate,
   return code;
 }
 
-Handle<CodeT> CompileCWasmEntry(Isolate* isolate, const wasm::FunctionSig* sig,
-                                const wasm::WasmModule* module) {
+Handle<CodeDataContainer> CompileCWasmEntry(Isolate* isolate,
+                                            const wasm::FunctionSig* sig,
+                                            const wasm::WasmModule* module) {
   std::unique_ptr<Zone> zone = std::make_unique<Zone>(
       isolate->allocator(), ZONE_NAME, kCompressGraphZone);
   Graph* graph = zone->New<Graph>(zone.get());
@@ -8464,7 +8465,7 @@ Handle<CodeT> CompileCWasmEntry(Isolate* isolate, const wasm::FunctionSig* sig,
            CompilationJob::FAILED);
   CHECK_NE(job->FinalizeJob(isolate), CompilationJob::FAILED);
 
-  return ToCodeT(job->compilation_info()->code(), isolate);
+  return ToCodeDataContainer(job->compilation_info()->code(), isolate);
 }
 
 namespace {
diff --git a/src/compiler/wasm-compiler.h b/src/compiler/wasm-compiler.h
index 6e2ba0d9515..089129fbffb 100644
--- a/src/compiler/wasm-compiler.h
+++ b/src/compiler/wasm-compiler.h
@@ -165,7 +165,7 @@ enum CWasmEntryParameters {
 
 // Compiles a stub with C++ linkage, to be called from Execution::CallWasm,
 // which knows how to feed it its parameters.
-V8_EXPORT_PRIVATE Handle<CodeT> CompileCWasmEntry(
+V8_EXPORT_PRIVATE Handle<CodeDataContainer> CompileCWasmEntry(
     Isolate*, const wasm::FunctionSig*, const wasm::WasmModule* module);
 
 // Values from the instance object are cached between Wasm-level function calls.
diff --git a/src/debug/debug-evaluate.cc b/src/debug/debug-evaluate.cc
index 04629bba319..a1ebe998c82 100644
--- a/src/debug/debug-evaluate.cc
+++ b/src/debug/debug-evaluate.cc
@@ -1231,7 +1231,7 @@ void DebugEvaluate::VerifyTransitiveBuiltins(Isolate* isolate) {
   for (Builtin caller = Builtins::kFirst; caller <= Builtins::kLast; ++caller) {
     DebugInfo::SideEffectState state = BuiltinGetSideEffectState(caller);
     if (state != DebugInfo::kHasNoSideEffect) continue;
-    Code code = FromCodeT(isolate->builtins()->code(caller));
+    Code code = FromCodeDataContainer(isolate->builtins()->code(caller));
     int mode = RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
                RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
 
diff --git a/src/debug/debug.cc b/src/debug/debug.cc
index fc9a3881f46..c6748b19c4b 100644
--- a/src/debug/debug.cc
+++ b/src/debug/debug.cc
@@ -1636,7 +1636,8 @@ void Debug::InstallDebugBreakTrampoline() {
 
   if (!needs_to_use_trampoline) return;
 
-  Handle<CodeT> trampoline = BUILTIN_CODE(isolate_, DebugBreakTrampoline);
+  Handle<CodeDataContainer> trampoline =
+      BUILTIN_CODE(isolate_, DebugBreakTrampoline);
   std::vector<Handle<JSFunction>> needs_compile;
   using AccessorPairWithContext =
       std::pair<Handle<AccessorPair>, Handle<NativeContext>>;
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index db5d3f058e9..06b8cab915c 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -229,7 +229,7 @@ DeoptimizedFrameInfo* Deoptimizer::DebuggerInspectableFrame(
 namespace {
 class ActivationsFinder : public ThreadVisitor {
  public:
-  ActivationsFinder(CodeT topmost_optimized_code,
+  ActivationsFinder(CodeDataContainer topmost_optimized_code,
                     bool safe_to_deopt_topmost_optimized_code) {
 #ifdef DEBUG
     topmost_ = topmost_optimized_code;
@@ -243,7 +243,8 @@ class ActivationsFinder : public ThreadVisitor {
   void VisitThread(Isolate* isolate, ThreadLocalTop* top) override {
     for (StackFrameIterator it(isolate, top); !it.done(); it.Advance()) {
       if (it.frame()->is_optimized()) {
-        CodeT code = it.frame()->LookupCodeT().ToCodeT();
+        CodeDataContainer code =
+            it.frame()->LookupCodeDataContainer().ToCodeDataContainer();
         if (CodeKindCanDeoptimize(code.kind()) &&
             code.marked_for_deoptimization()) {
           // Obtain the trampoline to the deoptimizer call.
@@ -272,7 +273,7 @@ class ActivationsFinder : public ThreadVisitor {
 
  private:
 #ifdef DEBUG
-  CodeT topmost_;
+  CodeDataContainer topmost_;
   bool safe_to_deopt_;
 #endif
 };
@@ -283,7 +284,7 @@ class ActivationsFinder : public ThreadVisitor {
 void Deoptimizer::DeoptimizeMarkedCode(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
 
-  CodeT topmost_optimized_code;
+  CodeDataContainer topmost_optimized_code;
   bool safe_to_deopt_topmost_optimized_code = false;
 #ifdef DEBUG
   // Make sure all activations of optimized code can deopt at their current PC.
@@ -292,7 +293,8 @@ void Deoptimizer::DeoptimizeMarkedCode(Isolate* isolate) {
   for (StackFrameIterator it(isolate, isolate->thread_local_top()); !it.done();
        it.Advance()) {
     if (it.frame()->is_optimized()) {
-      CodeT code = it.frame()->LookupCodeT().ToCodeT();
+      CodeDataContainer code =
+          it.frame()->LookupCodeDataContainer().ToCodeDataContainer();
       JSFunction function =
           static_cast<OptimizedFrame*>(it.frame())->function();
       TraceFoundActivation(isolate, function);
@@ -347,7 +349,8 @@ void Deoptimizer::DeoptimizeAll(Isolate* isolate) {
   DeoptimizeMarkedCode(isolate);
 }
 
-void Deoptimizer::DeoptimizeFunction(JSFunction function, CodeT code) {
+void Deoptimizer::DeoptimizeFunction(JSFunction function,
+                                     CodeDataContainer code) {
   Isolate* isolate = function.GetIsolate();
   RCS_SCOPE(isolate, RuntimeCallCounterId::kDeoptimizeCode);
   TimerEventScope<TimerEventDeoptimizeCode> timer(isolate);
@@ -934,7 +937,7 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
   const bool deopt_to_baseline =
       shared.HasBaselineCode() && v8_flags.deopt_to_baseline;
   const bool restart_frame = goto_catch_handler && is_restart_frame();
-  CodeT dispatch_builtin = builtins->code(
+  CodeDataContainer dispatch_builtin = builtins->code(
       DispatchBuiltinFor(deopt_to_baseline, advance_bc, restart_frame));
 
   if (verbose_tracing_enabled()) {
@@ -1175,7 +1178,8 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
     Register context_reg = JavaScriptFrame::context_register();
     output_frame->SetRegister(context_reg.code(), context_value);
     // Set the continuation for the topmost frame.
-    CodeT continuation = builtins->code(Builtin::kNotifyDeoptimized);
+    CodeDataContainer continuation =
+        builtins->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
         static_cast<intptr_t>(continuation.InstructionStart()));
   }
@@ -1255,7 +1259,8 @@ void Deoptimizer::DoComputeConstructStubFrame(TranslatedFrame* translated_frame,
   CHECK(!is_topmost || deopt_kind_ == DeoptimizeKind::kLazy);
 
   Builtins* builtins = isolate_->builtins();
-  CodeT construct_stub = builtins->code(Builtin::kJSConstructStubGeneric);
+  CodeDataContainer construct_stub =
+      builtins->code(Builtin::kJSConstructStubGeneric);
   BytecodeOffset bytecode_offset = translated_frame->bytecode_offset();
 
   const int parameters_count = translated_frame->height();
@@ -1409,7 +1414,8 @@ void Deoptimizer::DoComputeConstructStubFrame(TranslatedFrame* translated_frame,
   // Set the continuation for the topmost frame.
   if (is_topmost) {
     DCHECK_EQ(DeoptimizeKind::kLazy, deopt_kind_);
-    CodeT continuation = builtins->code(Builtin::kNotifyDeoptimized);
+    CodeDataContainer continuation =
+        builtins->code(Builtin::kNotifyDeoptimized);
     output_frame->SetContinuation(
         static_cast<intptr_t>(continuation.InstructionStart()));
   }
@@ -1833,7 +1839,7 @@ void Deoptimizer::DoComputeBuiltinContinuation(
   // For JSToWasmBuiltinContinuations use ContinueToCodeStubBuiltin, and not
   // ContinueToCodeStubBuiltinWithResult because we don't want to overwrite the
   // return value that we have already set.
-  CodeT continue_to_builtin =
+  CodeDataContainer continue_to_builtin =
       isolate()->builtins()->code(TrampolineForBuiltinContinuation(
           mode, frame_info.frame_has_result_stack_slot() &&
                     !is_js_to_wasm_builtin_continuation));
@@ -1850,7 +1856,8 @@ void Deoptimizer::DoComputeBuiltinContinuation(
         static_cast<intptr_t>(continue_to_builtin.InstructionStart()));
   }
 
-  CodeT continuation = isolate()->builtins()->code(Builtin::kNotifyDeoptimized);
+  CodeDataContainer continuation =
+      isolate()->builtins()->code(Builtin::kNotifyDeoptimized);
   output_frame->SetContinuation(
       static_cast<intptr_t>(continuation.InstructionStart()));
 }
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index ad05d874582..3617fdd61a6 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -79,7 +79,8 @@ class Deoptimizer : public Malloced {
   // again and any activations of the optimized code will get deoptimized when
   // execution returns. If {code} is specified then the given code is targeted
   // instead of the function code (e.g. OSR code not installed on function).
-  static void DeoptimizeFunction(JSFunction function, CodeT code = {});
+  static void DeoptimizeFunction(JSFunction function,
+                                 CodeDataContainer code = {});
 
   // Deoptimize all code in the given isolate.
   V8_EXPORT_PRIVATE static void DeoptimizeAll(Isolate* isolate);
diff --git a/src/deoptimizer/translated-state.cc b/src/deoptimizer/translated-state.cc
index 42f2a2bba0f..f48294676d9 100644
--- a/src/deoptimizer/translated-state.cc
+++ b/src/deoptimizer/translated-state.cc
@@ -2216,8 +2216,9 @@ void TranslatedState::StoreMaterializedValuesAndDeopt(JavaScriptFrame* frame) {
                             previously_materialized_objects);
     CHECK_EQ(frames_[0].kind(), TranslatedFrame::kUnoptimizedFunction);
     CHECK_EQ(frame->function(), frames_[0].front().GetRawValue());
-    Deoptimizer::DeoptimizeFunction(frame->function(),
-                                    frame->LookupCodeT().ToCodeT());
+    Deoptimizer::DeoptimizeFunction(
+        frame->function(),
+        frame->LookupCodeDataContainer().ToCodeDataContainer());
   }
 }
 
diff --git a/src/diagnostics/disassembler.cc b/src/diagnostics/disassembler.cc
index dad22ba046e..bd42ce3b68b 100644
--- a/src/diagnostics/disassembler.cc
+++ b/src/diagnostics/disassembler.cc
@@ -257,10 +257,10 @@ static void PrintRelocInfo(std::ostringstream& out, Isolate* isolate,
     out << "    ;; external reference (" << reference_name << ")";
   } else if (RelocInfo::IsCodeTargetMode(rmode)) {
     out << "    ;; code:";
-    CodeT code =
+    CodeDataContainer code =
         isolate->heap()
             ->GcSafeFindCodeForInnerPointer(relocinfo->target_address())
-            .ToCodeT();
+            .ToCodeDataContainer();
     CodeKind kind = code.kind();
     if (code.is_builtin()) {
       out << " Builtin::" << Builtins::name(code.builtin_id());
diff --git a/src/diagnostics/objects-debug.cc b/src/diagnostics/objects-debug.cc
index e7a4b608db5..51beddff1d3 100644
--- a/src/diagnostics/objects-debug.cc
+++ b/src/diagnostics/objects-debug.cc
@@ -921,7 +921,7 @@ void JSFunction::JSFunctionVerify(Isolate* isolate) {
   VerifyPointer(isolate, raw_feedback_cell(isolate));
   CHECK(raw_feedback_cell(isolate).IsFeedbackCell());
   VerifyPointer(isolate, code(isolate));
-  CHECK(code(isolate).IsCodeT());
+  CHECK(code(isolate).IsCodeDataContainer());
   CHECK(map(isolate).is_callable());
   Handle<JSFunction> function(*this, isolate);
   LookupIterator it(isolate, function, isolate->factory()->prototype_string(),
@@ -1546,9 +1546,9 @@ void JSRegExp::JSRegExpVerify(Isolate* isolate) {
       Object latin1_bytecode = arr.get(JSRegExp::kIrregexpLatin1BytecodeIndex);
       Object uc16_bytecode = arr.get(JSRegExp::kIrregexpUC16BytecodeIndex);
 
-      bool is_compiled = latin1_code.IsCodeT();
+      bool is_compiled = latin1_code.IsCodeDataContainer();
       if (is_compiled) {
-        CHECK_EQ(CodeT::cast(latin1_code).builtin_id(),
+        CHECK_EQ(CodeDataContainer::cast(latin1_code).builtin_id(),
                  Builtin::kRegExpExperimentalTrampoline);
         CHECK_EQ(uc16_code, latin1_code);
 
@@ -1580,11 +1580,11 @@ void JSRegExp::JSRegExpVerify(Isolate* isolate) {
       // Code: Compiled irregexp code or trampoline to the interpreter.
       CHECK((one_byte_data.IsSmi() &&
              Smi::ToInt(one_byte_data) == JSRegExp::kUninitializedValue) ||
-            one_byte_data.IsCodeT());
+            one_byte_data.IsCodeDataContainer());
       Object uc16_data = arr.get(JSRegExp::kIrregexpUC16CodeIndex);
       CHECK((uc16_data.IsSmi() &&
              Smi::ToInt(uc16_data) == JSRegExp::kUninitializedValue) ||
-            uc16_data.IsCodeT());
+            uc16_data.IsCodeDataContainer());
 
       Object one_byte_bytecode =
           arr.get(JSRegExp::kIrregexpLatin1BytecodeIndex);
@@ -1855,7 +1855,7 @@ void DataHandler::DataHandlerVerify(Isolate* isolate) {
   CHECK(IsDataHandler());
   VerifyPointer(isolate, smi_handler(isolate));
   CHECK_IMPLIES(!smi_handler().IsSmi(),
-                IsStoreHandler() && smi_handler().IsCodeT());
+                IsStoreHandler() && smi_handler().IsCodeDataContainer());
   VerifyPointer(isolate, validity_cell(isolate));
   CHECK(validity_cell().IsSmi() || validity_cell().IsCell());
   int data_count = data_field_count();
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index d38c19d35d2..bbcfdca356a 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -3048,7 +3048,8 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_Code(void* object) {
 #ifdef ENABLE_DISASSEMBLER
   i::StdoutStream os;
   if (lookup_result.IsCodeDataContainer()) {
-    i::CodeT code = i::CodeT::cast(lookup_result.code_data_container());
+    i::CodeDataContainer code =
+        i::CodeDataContainer::cast(lookup_result.code_data_container());
     code.Disassemble(nullptr, os, isolate, address);
   } else {
     lookup_result.code().Disassemble(nullptr, os, isolate, address);
diff --git a/src/execution/execution.cc b/src/execution/execution.cc
index b76a601b8fe..28d29715a57 100644
--- a/src/execution/execution.cc
+++ b/src/execution/execution.cc
@@ -168,8 +168,9 @@ InvokeParams InvokeParams::SetUpForRunMicrotasks(
   return params;
 }
 
-Handle<CodeT> JSEntry(Isolate* isolate, Execution::Target execution_target,
-                      bool is_construct) {
+Handle<CodeDataContainer> JSEntry(Isolate* isolate,
+                                  Execution::Target execution_target,
+                                  bool is_construct) {
   if (is_construct) {
     DCHECK_EQ(Execution::Target::kCallable, execution_target);
     return BUILTIN_CODE(isolate, JSConstructEntry);
@@ -397,7 +398,7 @@ V8_WARN_UNUSED_RESULT MaybeHandle<Object> Invoke(Isolate* isolate,
 
   // Placeholder for return value.
   Object value;
-  Handle<CodeT> code =
+  Handle<CodeDataContainer> code =
       JSEntry(isolate, params.execution_target, params.is_construct);
   {
     // Save and restore context around invocation and block the
@@ -611,7 +612,8 @@ static_assert(offsetof(StackHandlerMarker, padding) ==
 static_assert(sizeof(StackHandlerMarker) == StackHandlerConstants::kSize);
 
 #if V8_ENABLE_WEBASSEMBLY
-void Execution::CallWasm(Isolate* isolate, Handle<CodeT> wrapper_code,
+void Execution::CallWasm(Isolate* isolate,
+                         Handle<CodeDataContainer> wrapper_code,
                          Address wasm_call_target, Handle<Object> object_ref,
                          Address packed_args) {
   using WasmEntryStub = GeneratedCode<Address(
diff --git a/src/execution/execution.h b/src/execution/execution.h
index 7fb97a77450..04077bebe80 100644
--- a/src/execution/execution.h
+++ b/src/execution/execution.h
@@ -77,7 +77,7 @@ class Execution final : public AllStatic {
   // Upon return, either isolate->has_pending_exception() is true, or
   // the function's return values are in {packed_args}.
   V8_EXPORT_PRIVATE static void CallWasm(Isolate* isolate,
-                                         Handle<CodeT> wrapper_code,
+                                         Handle<CodeDataContainer> wrapper_code,
                                          Address wasm_call_target,
                                          Handle<Object> object_ref,
                                          Address packed_args);
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index 214ce54adb8..a5decf1df17 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -205,7 +205,8 @@ StackFrame* StackFrameIteratorBase::SingletonFor(StackFrame::Type type) {
 
 void TypedFrameWithJSLinkage::Iterate(RootVisitor* v) const {
   IterateExpressions(v);
-  IteratePc(v, pc_address(), constant_pool_address(), LookupCodeT());
+  IteratePc(v, pc_address(), constant_pool_address(),
+            LookupCodeDataContainer());
 }
 
 // -------------------------------------------------------------------------
@@ -566,7 +567,7 @@ CodeLookupResult GetContainingCode(Isolate* isolate, Address pc) {
 }
 }  // namespace
 
-CodeLookupResult StackFrame::LookupCodeT() const {
+CodeLookupResult StackFrame::LookupCodeDataContainer() const {
   CodeLookupResult result = GetContainingCode(isolate(), pc());
   if (DEBUG_BOOL) {
     CHECK(result.IsFound());
@@ -588,7 +589,7 @@ void StackFrame::IteratePc(RootVisitor* v, Address* pc_address,
                            CodeLookupResult lookup_result) const {
   if (lookup_result.IsCodeDataContainer()) {
     // The embedded builtins are immovable, so there's no need to update PCs on
-    // the stack, just visit the CodeT object.
+    // the stack, just visit the CodeDataContainer object.
     Object code = lookup_result.code_data_container();
     v->VisitRunningCode(FullObjectSlot(&code));
     return;
@@ -618,8 +619,8 @@ void StackFrame::SetReturnAddressLocationResolver(
 
 namespace {
 
-template <typename CodeOrCodeT>
-inline StackFrame::Type ComputeBuiltinFrameType(CodeOrCodeT code) {
+template <typename CodeOrCodeDataContainer>
+inline StackFrame::Type ComputeBuiltinFrameType(CodeOrCodeDataContainer code) {
   if (code.is_interpreter_trampoline_builtin() ||
       // Frames for baseline entry trampolines on the stack are still
       // interpreted frames.
@@ -702,11 +703,11 @@ StackFrame::Type StackFrame::ComputeType(const StackFrameIteratorBase* iterator,
       switch (lookup_result.kind()) {
         case CodeKind::BUILTIN: {
           if (StackFrame::IsTypeMarker(marker)) break;
-          // We can't use lookup_result.ToCodeT() because we might in the
-          // middle of GC.
+          // We can't use lookup_result.ToCodeDataContainer() because we might
+          // in the middle of GC.
           if (lookup_result.IsCodeDataContainer()) {
             return ComputeBuiltinFrameType(
-                CodeT::cast(lookup_result.code_data_container()));
+                CodeDataContainer::cast(lookup_result.code_data_container()));
           }
           return ComputeBuiltinFrameType(lookup_result.code());
         }
@@ -849,7 +850,8 @@ void ExitFrame::ComputeCallerState(State* state) const {
 void ExitFrame::Iterate(RootVisitor* v) const {
   // The arguments are traversed as part of the expression stack of
   // the calling frame.
-  IteratePc(v, pc_address(), constant_pool_address(), LookupCodeT());
+  IteratePc(v, pc_address(), constant_pool_address(),
+            LookupCodeDataContainer());
 }
 
 StackFrame::Type ExitFrame::GetStateForFramePointer(Address fp, State* state) {
@@ -925,7 +927,7 @@ void BuiltinExitFrame::Summarize(std::vector<FrameSummary>* frames) const {
   DCHECK(frames->empty());
   Handle<FixedArray> parameters = GetParameters();
   DisallowGarbageCollection no_gc;
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   int code_offset = code.GetOffsetFromInstructionStart(isolate(), pc());
   FrameSummary::JavaScriptFrameSummary summary(
       isolate(), receiver(), function(), code.ToAbstractCode(), code_offset,
@@ -1037,7 +1039,7 @@ Object CommonFrame::context() const {
 }
 
 int CommonFrame::position() const {
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   int code_offset = code.GetOffsetFromInstructionStart(isolate(), pc());
   return code.ToAbstractCode().SourcePosition(isolate(), code_offset);
 }
@@ -1668,10 +1670,10 @@ HeapObject StubFrame::unchecked_code() const {
 }
 
 int StubFrame::LookupExceptionHandlerInTable() {
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   DCHECK(code.is_turbofanned());
   DCHECK_EQ(code.kind(), CodeKind::BUILTIN);
-  HandlerTable table(code.codet());
+  HandlerTable table(code.code_data_container());
   int pc_offset = code.GetOffsetFromInstructionStart(isolate(), pc());
   return table.LookupReturn(pc_offset);
 }
@@ -1695,7 +1697,7 @@ HeapObject CommonFrameWithJSLinkage::unchecked_code() const {
 }
 
 int TurbofanFrame::ComputeParametersCount() const {
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   if (code.kind() == CodeKind::BUILTIN) {
     return static_cast<int>(
                Memory<intptr_t>(fp() + StandardFrameConstants::kArgCOffset)) -
@@ -1733,7 +1735,7 @@ bool CommonFrameWithJSLinkage::IsConstructor() const {
 void CommonFrameWithJSLinkage::Summarize(
     std::vector<FrameSummary>* functions) const {
   DCHECK(functions->empty());
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   int offset = code.GetOffsetFromInstructionStart(isolate(), pc());
   Handle<AbstractCode> abstract_code(code.ToAbstractCode(), isolate());
   Handle<FixedArray> params = GetParameters();
@@ -1772,7 +1774,7 @@ Script JavaScriptFrame::script() const {
 int CommonFrameWithJSLinkage::LookupExceptionHandlerInTable(
     int* stack_depth, HandlerTable::CatchPrediction* prediction) {
   if (DEBUG_BOOL) {
-    CodeLookupResult code_lookup_result = LookupCodeT();
+    CodeLookupResult code_lookup_result = LookupCodeDataContainer();
     CHECK(!code_lookup_result.has_handler_table());
     CHECK(!code_lookup_result.is_optimized_code() ||
           code_lookup_result.kind() == CodeKind::BASELINE);
@@ -1831,7 +1833,7 @@ void JavaScriptFrame::PrintTop(Isolate* isolate, FILE* file, bool print_args,
         code_offset = baseline_frame->GetBytecodeOffset();
         abstract_code = AbstractCode::cast(baseline_frame->GetBytecodeArray());
       } else {
-        CodeLookupResult code = frame->LookupCodeT();
+        CodeLookupResult code = frame->LookupCodeDataContainer();
         code_offset = code.GetOffsetFromInstructionStart(isolate, frame->pc());
       }
       PrintFunctionAndOffset(function, abstract_code, code_offset, file,
@@ -2156,7 +2158,7 @@ void OptimizedFrame::Summarize(std::vector<FrameSummary>* frames) const {
 
   // Delegate to JS frame in absence of deoptimization info.
   // TODO(turbofan): Revisit once we support deoptimization across the board.
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   if (code.kind() == CodeKind::BUILTIN) {
     return JavaScriptFrame::Summarize(frames);
   }
@@ -2258,7 +2260,7 @@ int OptimizedFrame::LookupExceptionHandlerInTable(
   // to use FrameSummary to find the corresponding code offset in unoptimized
   // code to perform prediction there.
   DCHECK_NULL(prediction);
-  CodeT code = LookupCodeT().ToCodeT();
+  CodeDataContainer code = LookupCodeDataContainer().ToCodeDataContainer();
 
   HandlerTable table(code);
   if (table.NumberOfReturnEntries() == 0) return -1;
@@ -2275,7 +2277,7 @@ int OptimizedFrame::LookupExceptionHandlerInTable(
   return table.LookupReturn(pc_offset);
 }
 
-int MaglevFrame::FindReturnPCForTrampoline(CodeT code,
+int MaglevFrame::FindReturnPCForTrampoline(CodeDataContainer code,
                                            int trampoline_pc) const {
   DCHECK_EQ(code.kind(), CodeKind::MAGLEV);
   DCHECK(code.marked_for_deoptimization());
@@ -2283,7 +2285,7 @@ int MaglevFrame::FindReturnPCForTrampoline(CodeT code,
   return safepoints.find_return_pc(trampoline_pc);
 }
 
-int TurbofanFrame::FindReturnPCForTrampoline(CodeT code,
+int TurbofanFrame::FindReturnPCForTrampoline(CodeDataContainer code,
                                              int trampoline_pc) const {
   DCHECK_EQ(code.kind(), CodeKind::TURBOFAN);
   DCHECK(code.marked_for_deoptimization());
@@ -2296,7 +2298,7 @@ DeoptimizationData OptimizedFrame::GetDeoptimizationData(
   DCHECK(is_optimized());
 
   JSFunction opt_function = function();
-  CodeT code = opt_function.code();
+  CodeDataContainer code = opt_function.code();
 
   // The code object may have been replaced by lazy deoptimization. Fall
   // back to a slow search in this case to find the original optimized
@@ -2305,7 +2307,7 @@ DeoptimizationData OptimizedFrame::GetDeoptimizationData(
     CodeLookupResult lookup_result =
         isolate()->heap()->GcSafeFindCodeForInnerPointer(pc());
     CHECK(lookup_result.IsFound());
-    code = lookup_result.ToCodeT();
+    code = lookup_result.ToCodeDataContainer();
   }
   DCHECK(!code.is_null());
   DCHECK(CodeKindCanDeoptimize(code.kind()));
@@ -2335,7 +2337,7 @@ void OptimizedFrame::GetFunctions(
 
   // Delegate to JS frame in absence of turbofan deoptimization.
   // TODO(turbofan): Revisit once we support deoptimization across the board.
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   if (code.kind() == CodeKind::BUILTIN) {
     return JavaScriptFrame::GetFunctions(functions);
   }
@@ -2463,12 +2465,12 @@ void InterpretedFrame::PatchBytecodeArray(BytecodeArray bytecode_array) {
 }
 
 int BaselineFrame::GetBytecodeOffset() const {
-  Code code = LookupCodeT().code();
+  Code code = LookupCodeDataContainer().code();
   return code.GetBytecodeOffsetForBaselinePC(this->pc(), GetBytecodeArray());
 }
 
 intptr_t BaselineFrame::GetPCForBytecodeOffset(int bytecode_offset) const {
-  Code code = LookupCodeT().code();
+  Code code = LookupCodeDataContainer().code();
   return code.GetBaselineStartPCForBytecodeOffset(bytecode_offset,
                                                   GetBytecodeArray());
 }
@@ -2918,7 +2920,8 @@ void JavaScriptFrame::Print(StringStream* accumulator, PrintMode mode,
 }
 
 void EntryFrame::Iterate(RootVisitor* v) const {
-  IteratePc(v, pc_address(), constant_pool_address(), LookupCodeT());
+  IteratePc(v, pc_address(), constant_pool_address(),
+            LookupCodeDataContainer());
 }
 
 void CommonFrame::IterateExpressions(RootVisitor* v) const {
@@ -2941,11 +2944,12 @@ void CommonFrame::IterateExpressions(RootVisitor* v) const {
 
 void JavaScriptFrame::Iterate(RootVisitor* v) const {
   IterateExpressions(v);
-  IteratePc(v, pc_address(), constant_pool_address(), LookupCodeT());
+  IteratePc(v, pc_address(), constant_pool_address(),
+            LookupCodeDataContainer());
 }
 
 void InternalFrame::Iterate(RootVisitor* v) const {
-  CodeLookupResult code = LookupCodeT();
+  CodeLookupResult code = LookupCodeDataContainer();
   IteratePc(v, pc_address(), constant_pool_address(), code);
   // Internal frames typically do not receive any arguments, hence their stack
   // only contains tagged pointers.
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 4bed950bb55..c1990fbc854 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -298,7 +298,7 @@ class StackFrame {
   virtual Type type() const = 0;
 
   // Get the code associated with this frame. The result might be a Code object,
-  // a CodeT object or an empty value.
+  // a CodeDataContainer object or an empty value.
   // This method is used by Isolate::PushStackTraceAndDie() for collecting a
   // stack trace on fatal error and thus it might be called in the middle of GC
   // and should be as safe as possible.
@@ -306,7 +306,7 @@ class StackFrame {
 
   // Search for the code associated with this frame.
   // TODO(v8:11880): rename to LookupCode()
-  V8_EXPORT_PRIVATE CodeLookupResult LookupCodeT() const;
+  V8_EXPORT_PRIVATE CodeLookupResult LookupCodeDataContainer() const;
 
   virtual void Iterate(RootVisitor* v) const = 0;
   void IteratePc(RootVisitor* v, Address* pc_address,
@@ -857,7 +857,7 @@ class OptimizedFrame : public JavaScriptFrame {
   int LookupExceptionHandlerInTable(
       int* data, HandlerTable::CatchPrediction* prediction) override;
 
-  virtual int FindReturnPCForTrampoline(CodeT code,
+  virtual int FindReturnPCForTrampoline(CodeDataContainer code,
                                         int trampoline_pc) const = 0;
 
  protected:
@@ -969,7 +969,8 @@ class MaglevFrame : public OptimizedFrame {
 
   void Iterate(RootVisitor* v) const override;
 
-  int FindReturnPCForTrampoline(CodeT code, int trampoline_pc) const override;
+  int FindReturnPCForTrampoline(CodeDataContainer code,
+                                int trampoline_pc) const override;
 
   BytecodeOffset GetBytecodeOffsetForOSR() const;
 
@@ -988,7 +989,8 @@ class TurbofanFrame : public OptimizedFrame {
 
   void Iterate(RootVisitor* v) const override;
 
-  int FindReturnPCForTrampoline(CodeT code, int trampoline_pc) const override;
+  int FindReturnPCForTrampoline(CodeDataContainer code,
+                                int trampoline_pc) const override;
 
  protected:
   inline explicit TurbofanFrame(StackFrameIteratorBase* iterator);
diff --git a/src/execution/isolate-data.h b/src/execution/isolate-data.h
index 1e97f34a567..8cd206b9ae1 100644
--- a/src/execution/isolate-data.h
+++ b/src/execution/isolate-data.h
@@ -247,7 +247,7 @@ class IsolateData final {
   // The entry table is in IsolateData for easy access through kRootRegister.
   Address builtin_entry_table_[Builtins::kBuiltinCount] = {};
 
-  // The entries in this array are tagged pointers to CodeT objects.
+  // The entries in this array are tagged pointers to CodeDataContainer objects.
   Address builtin_table_[Builtins::kBuiltinCount] = {};
 
   LinearAllocationArea new_allocation_info_;
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 251032ca105..e528ff3107a 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -440,10 +440,11 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
   // Hash data sections of builtin code objects.
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    CodeT codet = builtins()->code(builtin);
+    CodeDataContainer code_data_container = builtins()->code(builtin);
 
-    DCHECK(Internals::HasHeapObjectTag(codet.ptr()));
-    uint8_t* const code_ptr = reinterpret_cast<uint8_t*>(codet.address());
+    DCHECK(Internals::HasHeapObjectTag(code_data_container.ptr()));
+    uint8_t* const code_ptr =
+        reinterpret_cast<uint8_t*>(code_data_container.address());
 
     // These static asserts ensure we don't miss relevant fields. We don't
     // hash code cage base and code entry point. Other data fields must
@@ -466,13 +467,15 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
     // builtin trampolines. The rest of the data fields should be the same.
     // So we temporarily set |is_off_heap_trampoline| to true during hash
     // computation.
-    bool is_off_heap_trampoline_sav = codet.is_off_heap_trampoline();
-    codet.set_is_off_heap_trampoline_for_hash(true);
+    bool is_off_heap_trampoline_sav =
+        code_data_container.is_off_heap_trampoline();
+    code_data_container.set_is_off_heap_trampoline_for_hash(true);
 
     for (int j = kStartOffset; j < CodeDataContainer::kUnalignedSize; j++) {
       hash = base::hash_combine(hash, size_t{code_ptr[j]});
     }
-    codet.set_is_off_heap_trampoline_for_hash(is_off_heap_trampoline_sav);
+    code_data_container.set_is_off_heap_trampoline_for_hash(
+        is_off_heap_trampoline_sav);
   }
 
   // The builtins constants table is also tightly tied to embedded builtins.
@@ -771,7 +774,7 @@ class CallSiteBuilder {
 
     Handle<Object> receiver(combinator->native_context().promise_function(),
                             isolate_);
-    Handle<CodeT> code(combinator->code(), isolate_);
+    Handle<CodeDataContainer> code(combinator->code(), isolate_);
 
     // TODO(mmarchini) save Promises list from the Promise combinator
     Handle<FixedArray> parameters = isolate_->factory()->empty_fixed_array();
@@ -1997,7 +2000,7 @@ Object Isolate::UnwindAndFindHandler() {
       CHECK(frame->is_java_script());
 
       if (frame->is_turbofan()) {
-        Code code = frame->LookupCodeT().code();
+        Code code = frame->LookupCodeDataContainer().code();
         // The debugger triggers lazy deopt for the "to-be-restarted" frame
         // immediately when the CDP event arrives while paused.
         CHECK(code.marked_for_deoptimization());
@@ -2020,7 +2023,7 @@ Object Isolate::UnwindAndFindHandler() {
       DCHECK(!frame->is_maglev());
 
       debug()->clear_restart_frame();
-      CodeT code = *BUILTIN_CODE(this, RestartFrameTrampoline);
+      CodeDataContainer code = *BUILTIN_CODE(this, RestartFrameTrampoline);
       return FoundHandler(Context(), code.InstructionStart(), 0,
                           code.constant_pool(), kNullAddress, frame->fp(),
                           visited_frames);
@@ -2036,7 +2039,8 @@ Object Isolate::UnwindAndFindHandler() {
         thread_local_top()->handler_ = handler->next_address();
 
         // Gather information from the handler.
-        CodeT code = frame->LookupCodeT().codet();
+        CodeDataContainer code =
+            frame->LookupCodeDataContainer().code_data_container();
         HandlerTable table(code);
         return FoundHandler(Context(), code.InstructionStart(this, frame->pc()),
                             table.LookupReturn(0), code.constant_pool(),
@@ -2048,7 +2052,7 @@ Object Isolate::UnwindAndFindHandler() {
       case StackFrame::C_WASM_ENTRY: {
         StackHandler* handler = frame->top_handler();
         thread_local_top()->handler_ = handler->next_address();
-        Code code = frame->LookupCodeT().code();
+        Code code = frame->LookupCodeDataContainer().code();
         HandlerTable table(code);
         Address instruction_start = code.InstructionStart(this, frame->pc());
         int return_offset = static_cast<int>(frame->pc() - instruction_start);
@@ -2108,7 +2112,8 @@ Object Isolate::UnwindAndFindHandler() {
         int offset = opt_frame->LookupExceptionHandlerInTable(nullptr, nullptr);
         if (offset < 0) break;
         // The code might be an optimized code or a turbofanned builtin.
-        CodeT code = frame->LookupCodeT().ToCodeT();
+        CodeDataContainer code =
+            frame->LookupCodeDataContainer().ToCodeDataContainer();
         // Compute the stack pointer from the frame pointer. This ensures
         // that argument slots on the stack are dropped as returning would.
         Address return_sp = frame->fp() +
@@ -2142,7 +2147,8 @@ Object Isolate::UnwindAndFindHandler() {
 
         // The code might be a dynamically generated stub or a turbofanned
         // embedded builtin.
-        CodeT code = stub_frame->LookupCodeT().ToCodeT();
+        CodeDataContainer code =
+            stub_frame->LookupCodeDataContainer().ToCodeDataContainer();
         if (code.kind() != CodeKind::BUILTIN || !code.is_turbofanned() ||
             !code.has_handler_table()) {
           break;
@@ -2192,7 +2198,7 @@ Object Isolate::UnwindAndFindHandler() {
 
         if (frame->is_baseline()) {
           BaselineFrame* sp_frame = BaselineFrame::cast(js_frame);
-          Code code = sp_frame->LookupCodeT().code();
+          Code code = sp_frame->LookupCodeDataContainer().code();
           DCHECK(!code.is_off_heap_trampoline());
           intptr_t pc_offset = sp_frame->GetPCForBytecodeOffset(offset);
           // Patch the context register directly on the frame, so that we don't
@@ -2205,7 +2211,8 @@ Object Isolate::UnwindAndFindHandler() {
           InterpretedFrame::cast(js_frame)->PatchBytecodeOffset(
               static_cast<int>(offset));
 
-          CodeT code = *BUILTIN_CODE(this, InterpreterEnterAtBytecode);
+          CodeDataContainer code =
+              *BUILTIN_CODE(this, InterpreterEnterAtBytecode);
           // We subtract a frame from visited_frames because otherwise the
           // shadow stack will drop the underlying interpreter entry trampoline
           // in which the handler runs.
@@ -2237,7 +2244,8 @@ Object Isolate::UnwindAndFindHandler() {
 
         // Reconstruct the stack pointer from the frame pointer.
         Address return_sp = js_frame->fp() - js_frame->GetSPToFPDelta();
-        CodeT code = js_frame->LookupCodeT().codet();
+        CodeDataContainer code =
+            js_frame->LookupCodeDataContainer().code_data_container();
         return FoundHandler(Context(), code.InstructionStart(), 0,
                             code.constant_pool(), return_sp, frame->fp(),
                             visited_frames);
@@ -2254,8 +2262,9 @@ Object Isolate::UnwindAndFindHandler() {
       USE(removed);
       // If there were any materialized objects, the code should be
       // marked for deopt.
-      DCHECK_IMPLIES(
-          removed, frame->LookupCodeT().ToCodeT().marked_for_deoptimization());
+      DCHECK_IMPLIES(removed, frame->LookupCodeDataContainer()
+                                  .ToCodeDataContainer()
+                                  .marked_for_deoptimization());
     }
   }
 
@@ -2354,7 +2363,7 @@ Isolate::CatchType Isolate::PredictExceptionCatcher() {
       }
 
       case StackFrame::STUB: {
-        CodeLookupResult code = frame->LookupCodeT();
+        CodeLookupResult code = frame->LookupCodeDataContainer();
         if (code.kind() != CodeKind::BUILTIN || !code.has_handler_table() ||
             !code.is_turbofanned()) {
           break;
@@ -2366,7 +2375,7 @@ Isolate::CatchType Isolate::PredictExceptionCatcher() {
       }
 
       case StackFrame::JAVA_SCRIPT_BUILTIN_CONTINUATION_WITH_CATCH: {
-        CodeLookupResult code = frame->LookupCodeT();
+        CodeLookupResult code = frame->LookupCodeDataContainer();
         CatchType prediction = ToCatchType(code.GetBuiltinCatchPrediction());
         if (prediction != NOT_CAUGHT) return prediction;
         break;
@@ -2833,7 +2842,7 @@ Handle<Object> Isolate::GetPromiseOnStackOnThrow() {
     if (frame->is_java_script()) {
       catch_prediction = PredictException(JavaScriptFrame::cast(frame));
     } else if (frame->type() == StackFrame::STUB) {
-      CodeLookupResult code = frame->LookupCodeT();
+      CodeLookupResult code = frame->LookupCodeDataContainer();
       if (code.kind() != CodeKind::BUILTIN || !code.has_handler_table() ||
           !code.is_turbofanned()) {
         continue;
@@ -3908,8 +3917,9 @@ void CreateOffHeapTrampolines(Isolate* isolate) {
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     Address instruction_start = d.InstructionStartOfBuiltin(builtin);
-    Handle<CodeT> trampoline = isolate->factory()->NewOffHeapTrampolineFor(
-        builtins->code_handle(builtin), instruction_start);
+    Handle<CodeDataContainer> trampoline =
+        isolate->factory()->NewOffHeapTrampolineFor(
+            builtins->code_handle(builtin), instruction_start);
 
     // From this point onwards, the old builtin code object is unreachable and
     // will be collected by the next GC.
diff --git a/src/execution/simulator.h b/src/execution/simulator.h
index 11887e6d9af..872d5369c11 100644
--- a/src/execution/simulator.h
+++ b/src/execution/simulator.h
@@ -110,8 +110,8 @@ class GeneratedCode {
     return GeneratedCode(isolate, reinterpret_cast<Signature*>(buffer));
   }
 
-  template <typename CodeOrCodeT>
-  static GeneratedCode FromCode(CodeOrCodeT code) {
+  template <typename CodeOrCodeDataContainer>
+  static GeneratedCode FromCode(CodeOrCodeDataContainer code) {
     return FromAddress(code.GetIsolate(), code.entry());
   }
 
diff --git a/src/heap/factory.cc b/src/heap/factory.cc
index d098e459ea9..a10f45024bf 100644
--- a/src/heap/factory.cc
+++ b/src/heap/factory.cc
@@ -1667,8 +1667,8 @@ Handle<WasmInternalFunction> Factory::NewWasmInternalFunction(
 Handle<WasmJSFunctionData> Factory::NewWasmJSFunctionData(
     Address opt_call_target, Handle<JSReceiver> callable, int return_count,
     int parameter_count, Handle<PodArray<wasm::ValueType>> serialized_sig,
-    Handle<CodeT> wrapper_code, Handle<Map> rtt, wasm::Suspend suspend,
-    wasm::Promise promise) {
+    Handle<CodeDataContainer> wrapper_code, Handle<Map> rtt,
+    wasm::Suspend suspend, wasm::Promise promise) {
   Handle<WasmApiFunctionRef> ref =
       NewWasmApiFunctionRef(callable, suspend, Handle<WasmInstanceObject>());
   Handle<WasmInternalFunction> internal =
@@ -1700,10 +1700,11 @@ Handle<WasmResumeData> Factory::NewWasmResumeData(
 }
 
 Handle<WasmExportedFunctionData> Factory::NewWasmExportedFunctionData(
-    Handle<CodeT> export_wrapper, Handle<WasmInstanceObject> instance,
-    Address call_target, Handle<Object> ref, int func_index,
-    const wasm::FunctionSig* sig, uint32_t canonical_type_index,
-    int wrapper_budget, Handle<Map> rtt, wasm::Promise promise) {
+    Handle<CodeDataContainer> export_wrapper,
+    Handle<WasmInstanceObject> instance, Address call_target,
+    Handle<Object> ref, int func_index, const wasm::FunctionSig* sig,
+    uint32_t canonical_type_index, int wrapper_budget, Handle<Map> rtt,
+    wasm::Promise promise) {
   Handle<WasmInternalFunction> internal =
       NewWasmInternalFunction(call_target, Handle<HeapObject>::cast(ref), rtt);
   Map map = *wasm_exported_function_data_map();
@@ -1719,8 +1720,8 @@ Handle<WasmExportedFunctionData> Factory::NewWasmExportedFunctionData(
   result.init_sig(isolate(), sig);
   result.set_canonical_type_index(canonical_type_index);
   result.set_wrapper_budget(wrapper_budget);
-  // We can't skip the write barrier because the CodeT (CodeDataContainer)
-  // objects are not immovable.
+  // We can't skip the write barrier because the CodeDataContainer
+  // (CodeDataContainer) objects are not immovable.
   result.set_c_wrapper_code(*BUILTIN_CODE(isolate(), Illegal),
                             UPDATE_WRITE_BARRIER);
   result.set_packed_args_size(0);
@@ -1732,7 +1733,7 @@ Handle<WasmExportedFunctionData> Factory::NewWasmExportedFunctionData(
 
 Handle<WasmCapiFunctionData> Factory::NewWasmCapiFunctionData(
     Address call_target, Handle<Foreign> embedder_data,
-    Handle<CodeT> wrapper_code, Handle<Map> rtt,
+    Handle<CodeDataContainer> wrapper_code, Handle<Map> rtt,
     Handle<PodArray<wasm::ValueType>> serialized_sig) {
   Handle<WasmApiFunctionRef> ref = NewWasmApiFunctionRef(
       Handle<JSReceiver>(), wasm::kNoSuspend, Handle<WasmInstanceObject>());
@@ -2480,8 +2481,8 @@ Handle<DeoptimizationLiteralArray> Factory::NewDeoptimizationLiteralArray(
       NewWeakFixedArray(length, AllocationType::kOld));
 }
 
-Handle<CodeT> Factory::NewOffHeapTrampolineFor(Handle<CodeT> code,
-                                               Address off_heap_entry) {
+Handle<CodeDataContainer> Factory::NewOffHeapTrampolineFor(
+    Handle<CodeDataContainer> code, Address off_heap_entry) {
   CHECK_NOT_NULL(isolate()->embedded_blob_code());
   CHECK_NE(0, isolate()->embedded_blob_code_size());
   CHECK(Builtins::IsIsolateIndependentBuiltin(*code));
@@ -2498,7 +2499,7 @@ Handle<CodeT> Factory::NewOffHeapTrampolineFor(Handle<CodeT> code,
       code->kind_specific_flags(kRelaxedLoad), kRelaxedStore);
   code_data_container->set_code_entry_point(isolate(),
                                             code->code_entry_point());
-  return Handle<CodeT>::cast(code_data_container);
+  return Handle<CodeDataContainer>::cast(code_data_container);
 }
 
 Handle<BytecodeArray> Factory::CopyBytecodeArray(Handle<BytecodeArray> source) {
@@ -4038,7 +4039,7 @@ Handle<JSFunction> Factory::JSFunctionBuilder::Build() {
   PrepareMap();
   PrepareFeedbackCell();
 
-  Handle<CodeT> code = handle(sfi_->GetCode(), isolate_);
+  Handle<CodeDataContainer> code = handle(sfi_->GetCode(), isolate_);
   Handle<JSFunction> result = BuildRaw(code);
 
   if (code->kind() == CodeKind::BASELINE) {
@@ -4050,7 +4051,8 @@ Handle<JSFunction> Factory::JSFunctionBuilder::Build() {
   return result;
 }
 
-Handle<JSFunction> Factory::JSFunctionBuilder::BuildRaw(Handle<CodeT> code) {
+Handle<JSFunction> Factory::JSFunctionBuilder::BuildRaw(
+    Handle<CodeDataContainer> code) {
   Isolate* isolate = isolate_;
   Factory* factory = isolate_->factory();
 
diff --git a/src/heap/factory.h b/src/heap/factory.h
index 7014a1fa159..253fe504312 100644
--- a/src/heap/factory.h
+++ b/src/heap/factory.h
@@ -636,13 +636,14 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
                                                        Handle<Map> rtt);
   Handle<WasmCapiFunctionData> NewWasmCapiFunctionData(
       Address call_target, Handle<Foreign> embedder_data,
-      Handle<CodeT> wrapper_code, Handle<Map> rtt,
+      Handle<CodeDataContainer> wrapper_code, Handle<Map> rtt,
       Handle<PodArray<wasm::ValueType>> serialized_sig);
   Handle<WasmExportedFunctionData> NewWasmExportedFunctionData(
-      Handle<CodeT> export_wrapper, Handle<WasmInstanceObject> instance,
-      Address call_target, Handle<Object> ref, int func_index,
-      const wasm::FunctionSig* sig, uint32_t canonical_type_index,
-      int wrapper_budget, Handle<Map> rtt, wasm::Promise promise);
+      Handle<CodeDataContainer> export_wrapper,
+      Handle<WasmInstanceObject> instance, Address call_target,
+      Handle<Object> ref, int func_index, const wasm::FunctionSig* sig,
+      uint32_t canonical_type_index, int wrapper_budget, Handle<Map> rtt,
+      wasm::Promise promise);
   Handle<WasmApiFunctionRef> NewWasmApiFunctionRef(
       Handle<JSReceiver> callable, wasm::Suspend suspend,
       Handle<WasmInstanceObject> instance);
@@ -651,8 +652,8 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
   Handle<WasmJSFunctionData> NewWasmJSFunctionData(
       Address opt_call_target, Handle<JSReceiver> callable, int return_count,
       int parameter_count, Handle<PodArray<wasm::ValueType>> serialized_sig,
-      Handle<CodeT> wrapper_code, Handle<Map> rtt, wasm::Suspend suspend,
-      wasm::Promise promise);
+      Handle<CodeDataContainer> wrapper_code, Handle<Map> rtt,
+      wasm::Suspend suspend, wasm::Promise promise);
   Handle<WasmResumeData> NewWasmResumeData(
       Handle<WasmSuspenderObject> suspender, wasm::OnResume on_resume);
   Handle<WasmStruct> NewWasmStruct(const wasm::StructType* type,
@@ -754,8 +755,8 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
 
   // Allocates a new code object and initializes it as the trampoline to the
   // given off-heap entry point.
-  Handle<CodeT> NewOffHeapTrampolineFor(Handle<CodeT> code,
-                                        Address off_heap_entry);
+  Handle<CodeDataContainer> NewOffHeapTrampolineFor(
+      Handle<CodeDataContainer> code, Address off_heap_entry);
 
   Handle<BytecodeArray> CopyBytecodeArray(Handle<BytecodeArray>);
 
@@ -918,7 +919,8 @@ class V8_EXPORT_PRIVATE Factory : public FactoryBase<Factory> {
     void PrepareMap();
     void PrepareFeedbackCell();
 
-    V8_WARN_UNUSED_RESULT Handle<JSFunction> BuildRaw(Handle<CodeT> code);
+    V8_WARN_UNUSED_RESULT Handle<JSFunction> BuildRaw(
+        Handle<CodeDataContainer> code);
 
     Isolate* const isolate_;
     Handle<SharedFunctionInfo> sfi_;
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index c162ef02e35..3a86d20faf9 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -7050,10 +7050,6 @@ void Heap::WriteBarrierForRangeImpl(MemoryChunk* source_page, HeapObject object,
 
   MarkCompactCollector* collector = this->mark_compact_collector();
 
-  CodeTPageHeaderModificationScope rwx_write_scope(
-      "Marking CodeT objects might require write access to the CodeT page "
-      "header");
-
   for (TSlot slot = start_slot; slot < end_slot; ++slot) {
     typename TSlot::TObject value = *slot;
     HeapObject value_heap_object;
diff --git a/src/heap/heap.h b/src/heap/heap.h
index 1572bbf79e2..f77b845c826 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -2609,12 +2609,6 @@ using CodePageHeaderModificationScope = RwxMemoryWriteScope;
 using CodePageHeaderModificationScope = NopRwxMemoryWriteScope;
 #endif  // V8_HEAP_USE_PTHREAD_JIT_WRITE_PROTECT
 
-// The CodeTPageHeaderModificationScope enables write access to CodeT objects
-// page headers.
-// This scope is no-op because CodeT objects are data objects and thus the page
-// header is always in writable state.
-using CodeTPageHeaderModificationScope = NopRwxMemoryWriteScope;
-
 // The CodePageMemoryModificationScope does not check if transitions to
 // writeable and back to executable are actually allowed, i.e. the MemoryChunk
 // was registered to be executable. It can be used by concurrent threads.
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index 052a1a88206..dd1d25b07ab 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -1093,9 +1093,9 @@ class MarkCompactCollector::RootMarkingVisitor final : public RootVisitor {
     // that heap snapshots accurately describe the roots.
     HeapObject value = HeapObject::cast(*p);
     if (!IsCodeSpaceObject(value)) {
-      // The slot might contain a CodeT object representing an embedded
-      // builtin, which doesn't require additional processing.
-      DCHECK(CodeT::cast(value).is_off_heap_trampoline());
+      // The slot might contain a CodeDataContainer object representing an
+      // embedded builtin, which doesn't require additional processing.
+      DCHECK(CodeDataContainer::cast(value).is_off_heap_trampoline());
     } else {
       Code code = Code::cast(value);
       if (code.kind() != CodeKind::BASELINE) {
@@ -2676,7 +2676,7 @@ void MarkCompactCollector::ProcessTopOptimizedFrame(ObjectVisitor* visitor,
        it.Advance()) {
     if (it.frame()->is_unoptimized()) return;
     if (it.frame()->is_optimized()) {
-      CodeLookupResult lookup_result = it.frame()->LookupCodeT();
+      CodeLookupResult lookup_result = it.frame()->LookupCodeDataContainer();
       // Embedded builtins can't deoptimize.
       if (lookup_result.IsCodeDataContainer()) return;
       Code code = lookup_result.code();
@@ -3307,14 +3307,16 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
   SharedFunctionInfo flushing_candidate;
   while (local_weak_objects()->code_flushing_candidates_local.Pop(
       &flushing_candidate)) {
-    CodeT baseline_codet;
+    CodeDataContainer baseline_code_data_container;
     Code baseline_code;
     HeapObject baseline_bytecode_or_interpreter_data;
     if (v8_flags.flush_baseline_code && flushing_candidate.HasBaselineCode()) {
-      baseline_codet =
-          CodeT::cast(flushing_candidate.function_data(kAcquireLoad));
-      // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-      baseline_code = FromCodeT(baseline_codet, isolate(), kRelaxedLoad);
+      baseline_code_data_container = CodeDataContainer::cast(
+          flushing_candidate.function_data(kAcquireLoad));
+      // Safe to do a relaxed load here since the CodeDataContainer was
+      // acquire-loaded.
+      baseline_code = FromCodeDataContainer(baseline_code_data_container,
+                                            isolate(), kRelaxedLoad);
       baseline_bytecode_or_interpreter_data =
           baseline_code.bytecode_or_interpreter_data(isolate());
     }
@@ -3341,10 +3343,11 @@ void MarkCompactCollector::ProcessOldCodeCandidates() {
         // to bailout if there is no bytecode.
         DCHECK(is_bytecode_live);
 
-        // Regardless of whether the CodeT is a CodeDataContainer or the Code
-        // itself, if the Code is live then the CodeT has to be live and will
-        // have been marked via the owning JSFunction.
-        DCHECK(non_atomic_marking_state()->IsBlackOrGrey(baseline_codet));
+        // Regardless of whether the CodeDataContainer is a CodeDataContainer or
+        // the Code itself, if the Code is live then the CodeDataContainer has
+        // to be live and will have been marked via the owning JSFunction.
+        DCHECK(non_atomic_marking_state()->IsBlackOrGrey(
+            baseline_code_data_container));
       } else if (is_bytecode_live || bytecode_already_decompiled) {
         // Reset the function_data field to the BytecodeArray, InterpreterData,
         // or UncompiledData found on the baseline code. We can skip this step
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index e8c0f3a4d85..13824fdca0a 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -209,11 +209,13 @@ int MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitSharedFunctionInfo(
     // If bytecode flushing is disabled but baseline code flushing is enabled
     // then we have to visit the bytecode but not the baseline code.
     DCHECK(IsBaselineCodeFlushingEnabled(code_flush_mode_));
-    CodeT baseline_codet = CodeT::cast(shared_info.function_data(kAcquireLoad));
-    // Safe to do a relaxed load here since the CodeT was acquire-loaded.
-    Code baseline_code =
-        FromCodeT(baseline_codet, ObjectVisitorWithCageBases::code_cage_base(),
-                  kRelaxedLoad);
+    CodeDataContainer baseline_code_data_container =
+        CodeDataContainer::cast(shared_info.function_data(kAcquireLoad));
+    // Safe to do a relaxed load here since the CodeDataContainer was
+    // acquire-loaded.
+    Code baseline_code = FromCodeDataContainer(
+        baseline_code_data_container,
+        ObjectVisitorWithCageBases::code_cage_base(), kRelaxedLoad);
     // Visit the bytecode hanging off baseline code.
     VisitPointer(baseline_code,
                  baseline_code.RawField(
diff --git a/src/ic/accessor-assembler.cc b/src/ic/accessor-assembler.cc
index 4c7770c6951..325ddfb63fb 100644
--- a/src/ic/accessor-assembler.cc
+++ b/src/ic/accessor-assembler.cc
@@ -209,7 +209,7 @@ void AccessorAssembler::HandleLoadICHandlerCase(
   BIND(&try_proto_handler);
   {
     GotoIf(IsWeakOrCleared(handler), &call_getter);
-    GotoIf(IsCodeT(CAST(handler)), &call_code_handler);
+    GotoIf(IsCodeDataContainer(CAST(handler)), &call_code_handler);
     HandleLoadICProtoHandler(p, CAST(handler), &var_holder, &var_smi_handler,
                              &if_smi_handler, miss, exit_point, ic_mode,
                              access_mode);
@@ -237,7 +237,7 @@ void AccessorAssembler::HandleLoadICHandlerCase(
 
   BIND(&call_code_handler);
   {
-    TNode<CodeT> code_handler = CAST(handler);
+    TNode<CodeDataContainer> code_handler = CAST(handler);
     exit_point->ReturnCallStub(LoadWithVectorDescriptor{}, code_handler,
                                p->context(), p->lookup_start_object(),
                                p->name(), p->slot(), p->vector());
@@ -1015,7 +1015,7 @@ TNode<Object> AccessorAssembler::HandleProtoHandler(
     if (on_code_handler) {
       Label if_smi_handler(this);
       GotoIf(TaggedIsSmi(smi_or_code_handler), &if_smi_handler);
-      TNode<CodeT> code = CAST(smi_or_code_handler);
+      TNode<CodeDataContainer> code = CAST(smi_or_code_handler);
       on_code_handler(code);
 
       BIND(&if_smi_handler);
@@ -1376,7 +1376,8 @@ void AccessorAssembler::HandleStoreICHandlerCase(
     GotoIf(IsWeakOrCleared(ref_handler), &store_transition_or_global);
     TNode<HeapObject> strong_handler = CAST(handler);
     TNode<Map> handler_map = LoadMap(strong_handler);
-    Branch(IsCodeTMap(handler_map), &call_handler, &if_proto_handler);
+    Branch(IsCodeDataContainerMap(handler_map), &call_handler,
+           &if_proto_handler);
 
     BIND(&if_proto_handler);
     {
@@ -1387,7 +1388,7 @@ void AccessorAssembler::HandleStoreICHandlerCase(
     // |handler| is a heap object. Must be code, call it.
     BIND(&call_handler);
     {
-      TNode<CodeT> code_handler = CAST(strong_handler);
+      TNode<CodeDataContainer> code_handler = CAST(strong_handler);
       TailCallStub(StoreWithVectorDescriptor{}, code_handler, p->context(),
                    p->receiver(), p->name(), p->value(), p->slot(),
                    p->vector());
@@ -1784,7 +1785,7 @@ void AccessorAssembler::HandleStoreICProtoHandler(
   OnCodeHandler on_code_handler;
   if (support_elements == kSupportElements) {
     // Code sub-handlers are expected only in KeyedStoreICs.
-    on_code_handler = [=](TNode<CodeT> code_handler) {
+    on_code_handler = [=](TNode<CodeDataContainer> code_handler) {
       // This is either element store or transitioning element store.
       Label if_element_store(this), if_transitioning_element_store(this);
       Branch(IsStoreHandler0Map(LoadMap(handler)), &if_element_store,
@@ -3049,7 +3050,7 @@ void AccessorAssembler::LoadIC_BytecodeHandler(const LazyLoadICParameters* p,
 
     // Call into the stub that implements the non-inlined parts of LoadIC.
     Callable ic = Builtins::CallableFor(isolate(), Builtin::kLoadIC_Noninlined);
-    TNode<CodeT> code_target = HeapConstant(ic.code());
+    TNode<CodeDataContainer> code_target = HeapConstant(ic.code());
     exit_point->ReturnCallStub(ic.descriptor(), code_target, p->context(),
                                p->receiver_and_lookup_start_object(), p->name(),
                                p->slot(), p->vector());
@@ -4079,11 +4080,11 @@ void AccessorAssembler::StoreInArrayLiteralIC(const StoreICParameters* p) {
       GotoIf(TaggedIsSmi(var_handler.value()), &if_smi_handler);
 
       TNode<HeapObject> handler = CAST(var_handler.value());
-      GotoIfNot(IsCodeT(handler), &if_transitioning_element_store);
+      GotoIfNot(IsCodeDataContainer(handler), &if_transitioning_element_store);
 
       {
         // Call the handler.
-        TNode<CodeT> code_handler = CAST(handler);
+        TNode<CodeDataContainer> code_handler = CAST(handler);
         TailCallStub(StoreWithVectorDescriptor{}, code_handler, p->context(),
                      p->receiver(), p->name(), p->value(), p->slot(),
                      p->vector());
@@ -4096,7 +4097,7 @@ void AccessorAssembler::StoreInArrayLiteralIC(const StoreICParameters* p) {
         TNode<Map> transition_map =
             CAST(GetHeapObjectAssumeWeak(maybe_transition_map, &miss));
         GotoIf(IsDeprecatedMap(transition_map), &miss);
-        TNode<CodeT> code =
+        TNode<CodeDataContainer> code =
             CAST(LoadObjectField(handler, StoreHandler::kSmiHandlerOffset));
         TailCallStub(StoreTransitionDescriptor{}, code, p->context(),
                      p->receiver(), p->name(), transition_map, p->value(),
diff --git a/src/ic/accessor-assembler.h b/src/ic/accessor-assembler.h
index 6750befdc40..d68adf957a0 100644
--- a/src/ic/accessor-assembler.h
+++ b/src/ic/accessor-assembler.h
@@ -499,7 +499,8 @@ class V8_EXPORT_PRIVATE AccessorAssembler : public CodeStubAssembler {
 
   // Low-level helpers.
 
-  using OnCodeHandler = std::function<void(TNode<CodeT> code_handler)>;
+  using OnCodeHandler =
+      std::function<void(TNode<CodeDataContainer> code_handler)>;
   using OnFoundOnLookupStartObject = std::function<void(
       TNode<PropertyDictionary> properties, TNode<IntPtrT> name_index)>;
 
@@ -606,7 +607,7 @@ class ExitPoint {
 
   template <class... TArgs>
   void ReturnCallStub(const CallInterfaceDescriptor& descriptor,
-                      TNode<CodeT> target, TNode<Context> context,
+                      TNode<CodeDataContainer> target, TNode<Context> context,
                       TArgs... args) {
     if (IsDirect()) {
       asm_->TailCallStub(descriptor, target, context, args...);
diff --git a/src/ic/handler-configuration-inl.h b/src/ic/handler-configuration-inl.h
index 3e01b612263..1499cbaf575 100644
--- a/src/ic/handler-configuration-inl.h
+++ b/src/ic/handler-configuration-inl.h
@@ -154,7 +154,7 @@ Handle<Smi> StoreHandler::StoreInterceptor(Isolate* isolate) {
   return handle(Smi::FromInt(config), isolate);
 }
 
-Handle<CodeT> StoreHandler::StoreSloppyArgumentsBuiltin(
+Handle<CodeDataContainer> StoreHandler::StoreSloppyArgumentsBuiltin(
     Isolate* isolate, KeyedAccessStoreMode mode) {
   switch (mode) {
     case STANDARD_STORE:
@@ -173,8 +173,8 @@ Handle<CodeT> StoreHandler::StoreSloppyArgumentsBuiltin(
   }
 }
 
-Handle<CodeT> StoreHandler::StoreFastElementBuiltin(Isolate* isolate,
-                                                    KeyedAccessStoreMode mode) {
+Handle<CodeDataContainer> StoreHandler::StoreFastElementBuiltin(
+    Isolate* isolate, KeyedAccessStoreMode mode) {
   switch (mode) {
     case STANDARD_STORE:
       return BUILTIN_CODE(isolate, StoreFastElementIC_Standard);
@@ -190,7 +190,7 @@ Handle<CodeT> StoreHandler::StoreFastElementBuiltin(Isolate* isolate,
   }
 }
 
-Handle<CodeT> StoreHandler::ElementsTransitionAndStoreBuiltin(
+Handle<CodeDataContainer> StoreHandler::ElementsTransitionAndStoreBuiltin(
     Isolate* isolate, KeyedAccessStoreMode mode) {
   switch (mode) {
     case STANDARD_STORE:
diff --git a/src/ic/handler-configuration.cc b/src/ic/handler-configuration.cc
index ece15ff24b9..9f6b2836188 100644
--- a/src/ic/handler-configuration.cc
+++ b/src/ic/handler-configuration.cc
@@ -523,9 +523,9 @@ void LoadHandler::PrintHandler(Object handler, std::ostream& os) {
     os << "LoadHandler(Smi)(";
     PrintSmiLoadHandler(raw_handler, os);
     os << ")";
-  } else if (handler.IsCodeT()) {
+  } else if (handler.IsCodeDataContainer()) {
     os << "LoadHandler(Code)("
-       << Builtins::name(CodeT::cast(handler).builtin_id()) << ")";
+       << Builtins::name(CodeDataContainer::cast(handler).builtin_id()) << ")";
   } else if (handler.IsSymbol()) {
     os << "LoadHandler(Symbol)(" << Brief(Symbol::cast(handler)) << ")";
   } else if (handler.IsLoadHandler()) {
@@ -566,8 +566,9 @@ void StoreHandler::PrintHandler(Object handler, std::ostream& os) {
   } else if (handler.IsStoreHandler()) {
     os << "StoreHandler(";
     StoreHandler store_handler = StoreHandler::cast(handler);
-    if (store_handler.smi_handler().IsCodeT()) {
-      CodeT code = CodeT::cast(store_handler.smi_handler());
+    if (store_handler.smi_handler().IsCodeDataContainer()) {
+      CodeDataContainer code =
+          CodeDataContainer::cast(store_handler.smi_handler());
       os << "builtin = ";
       code.ShortPrint(os);
     } else {
diff --git a/src/ic/handler-configuration.h b/src/ic/handler-configuration.h
index 8e3d7e9de45..f3886e21e0b 100644
--- a/src/ic/handler-configuration.h
+++ b/src/ic/handler-configuration.h
@@ -358,11 +358,11 @@ class StoreHandler final : public DataHandler {
   // Creates a Smi-handler for storing a property to an interceptor.
   static inline Handle<Smi> StoreInterceptor(Isolate* isolate);
 
-  static inline Handle<CodeT> StoreSloppyArgumentsBuiltin(
+  static inline Handle<CodeDataContainer> StoreSloppyArgumentsBuiltin(
       Isolate* isolate, KeyedAccessStoreMode mode);
-  static inline Handle<CodeT> StoreFastElementBuiltin(
+  static inline Handle<CodeDataContainer> StoreFastElementBuiltin(
       Isolate* isolate, KeyedAccessStoreMode mode);
-  static inline Handle<CodeT> ElementsTransitionAndStoreBuiltin(
+  static inline Handle<CodeDataContainer> ElementsTransitionAndStoreBuiltin(
       Isolate* isolate, KeyedAccessStoreMode mode);
 
   // Creates a Smi-handler for storing a property.
diff --git a/src/ic/ic-inl.h b/src/ic/ic-inl.h
index cc998c72b26..5df94c839b1 100644
--- a/src/ic/ic-inl.h
+++ b/src/ic/ic-inl.h
@@ -32,7 +32,7 @@ bool IC::IsHandler(MaybeObject object) {
           (heap_object.IsMap() || heap_object.IsPropertyCell() ||
            heap_object.IsAccessorPair())) ||
          (object->GetHeapObjectIfStrong(&heap_object) &&
-          (heap_object.IsDataHandler() || heap_object.IsCodeT()));
+          (heap_object.IsDataHandler() || heap_object.IsCodeDataContainer()));
 }
 
 bool IC::vector_needs_update() {
diff --git a/src/interpreter/interpreter-assembler.cc b/src/interpreter/interpreter-assembler.cc
index deca6314168..374dee756d6 100644
--- a/src/interpreter/interpreter-assembler.cc
+++ b/src/interpreter/interpreter-assembler.cc
@@ -730,7 +730,7 @@ void InterpreterAssembler::CallJSAndDispatch(
 
   Callable callable = CodeFactory::InterpreterPushArgsThenCall(
       isolate(), receiver_mode, InterpreterPushArgsMode::kOther);
-  TNode<CodeT> code_target = HeapConstant(callable.code());
+  TNode<CodeDataContainer> code_target = HeapConstant(callable.code());
 
   TailCallStubThenBytecodeDispatch(callable.descriptor(), code_target, context,
                                    args_count, args.base_reg_location(),
@@ -751,7 +751,7 @@ void InterpreterAssembler::CallJSAndDispatch(TNode<Object> function,
          bytecode_ == Bytecode::kInvokeIntrinsic);
   DCHECK_EQ(Bytecodes::GetReceiverMode(bytecode_), receiver_mode);
   Callable callable = CodeFactory::Call(isolate());
-  TNode<CodeT> code_target = HeapConstant(callable.code());
+  TNode<CodeDataContainer> code_target = HeapConstant(callable.code());
 
   arg_count = JSParameterCount(arg_count);
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
@@ -796,7 +796,7 @@ void InterpreterAssembler::CallJSWithSpreadAndDispatch(
   Callable callable = CodeFactory::InterpreterPushArgsThenCall(
       isolate(), ConvertReceiverMode::kAny,
       InterpreterPushArgsMode::kWithFinalSpread);
-  TNode<CodeT> code_target = HeapConstant(callable.code());
+  TNode<CodeDataContainer> code_target = HeapConstant(callable.code());
 
   TNode<Word32T> args_count = args.reg_count();
   TailCallStubThenBytecodeDispatch(callable.descriptor(), code_target, context,
@@ -981,7 +981,7 @@ TNode<T> InterpreterAssembler::CallRuntimeN(TNode<Uint32T> function_id,
   DCHECK(Bytecodes::MakesCallAlongCriticalPath(bytecode_));
   DCHECK(Bytecodes::IsCallRuntime(bytecode_));
   Callable callable = CodeFactory::InterpreterCEntry(isolate(), return_count);
-  TNode<CodeT> code_target = HeapConstant(callable.code());
+  TNode<CodeDataContainer> code_target = HeapConstant(callable.code());
 
   // Get the function entry from the function id.
   TNode<RawPtrT> function_table = ReinterpretCast<RawPtrT>(ExternalConstant(
@@ -1391,8 +1391,9 @@ void InterpreterAssembler::OnStackReplacement(
         LoadFunctionClosure(), JSFunction::kSharedFunctionInfoOffset);
     TNode<HeapObject> sfi_data = LoadObjectField<HeapObject>(
         sfi, SharedFunctionInfo::kFunctionDataOffset);
-    GotoIf(InstanceTypeEqual(LoadInstanceType(sfi_data), CODET_TYPE),
-           &osr_to_sparkplug);
+    GotoIf(
+        InstanceTypeEqual(LoadInstanceType(sfi_data), CODE_DATA_CONTAINER_TYPE),
+        &osr_to_sparkplug);
 
     // Case 3).
     {
diff --git a/src/interpreter/interpreter-generator.cc b/src/interpreter/interpreter-generator.cc
index 82f053300c3..a7909732116 100644
--- a/src/interpreter/interpreter-generator.cc
+++ b/src/interpreter/interpreter-generator.cc
@@ -2175,8 +2175,9 @@ IGNITION_HANDLER(JumpLoop, InterpreterAssembler) {
       LoadFunctionClosure(), JSFunction::kSharedFunctionInfoOffset);
   TNode<HeapObject> sfi_data =
       LoadObjectField<HeapObject>(sfi, SharedFunctionInfo::kFunctionDataOffset);
-  Branch(InstanceTypeEqual(LoadInstanceType(sfi_data), CODET_TYPE),
-         &maybe_osr_because_baseline, &ok);
+  Branch(
+      InstanceTypeEqual(LoadInstanceType(sfi_data), CODE_DATA_CONTAINER_TYPE),
+      &maybe_osr_because_baseline, &ok);
 
   BIND(&ok);
   // The backward jump can trigger a budget interrupt, which can handle stack
diff --git a/src/interpreter/interpreter.cc b/src/interpreter/interpreter.cc
index 11c83fa9b58..4aa48b0c70c 100644
--- a/src/interpreter/interpreter.cc
+++ b/src/interpreter/interpreter.cc
@@ -111,15 +111,15 @@ Builtin BuiltinIndexFromBytecode(Bytecode bytecode,
 
 }  // namespace
 
-CodeT Interpreter::GetBytecodeHandler(Bytecode bytecode,
-                                      OperandScale operand_scale) {
+CodeDataContainer Interpreter::GetBytecodeHandler(Bytecode bytecode,
+                                                  OperandScale operand_scale) {
   Builtin builtin = BuiltinIndexFromBytecode(bytecode, operand_scale);
   return isolate_->builtins()->code(builtin);
 }
 
 void Interpreter::SetBytecodeHandler(Bytecode bytecode,
                                      OperandScale operand_scale,
-                                     CodeT handler) {
+                                     CodeDataContainer handler) {
   DCHECK(handler.is_off_heap_trampoline());
   DCHECK(handler.kind() == CodeKind::BYTECODE_HANDLER);
   size_t index = GetDispatchTableIndex(bytecode, operand_scale);
@@ -341,16 +341,17 @@ void Interpreter::Initialize() {
 
   // Set the interpreter entry trampoline entry point now that builtins are
   // initialized.
-  Handle<CodeT> code = BUILTIN_CODE(isolate_, InterpreterEntryTrampoline);
+  Handle<CodeDataContainer> code =
+      BUILTIN_CODE(isolate_, InterpreterEntryTrampoline);
   DCHECK(builtins->is_initialized());
   DCHECK(code->is_off_heap_trampoline() ||
-         isolate_->heap()->IsImmovable(FromCodeT(*code)));
+         isolate_->heap()->IsImmovable(FromCodeDataContainer(*code)));
   interpreter_entry_trampoline_instruction_start_ = code->InstructionStart();
 
   // Initialize the dispatch table.
   ForEachBytecode([=](Bytecode bytecode, OperandScale operand_scale) {
     Builtin builtin = BuiltinIndexFromBytecode(bytecode, operand_scale);
-    CodeT handler = builtins->code(builtin);
+    CodeDataContainer handler = builtins->code(builtin);
     if (Bytecodes::BytecodeHasHandler(bytecode, operand_scale)) {
 #ifdef DEBUG
       std::string builtin_name(Builtins::name(builtin));
diff --git a/src/interpreter/interpreter.h b/src/interpreter/interpreter.h
index 16dc5ef8aa3..759b18a6eb9 100644
--- a/src/interpreter/interpreter.h
+++ b/src/interpreter/interpreter.h
@@ -62,12 +62,12 @@ class Interpreter {
 
   // If the bytecode handler for |bytecode| and |operand_scale| has not yet
   // been loaded, deserialize it. Then return the handler.
-  V8_EXPORT_PRIVATE CodeT GetBytecodeHandler(Bytecode bytecode,
-                                             OperandScale operand_scale);
+  V8_EXPORT_PRIVATE CodeDataContainer
+  GetBytecodeHandler(Bytecode bytecode, OperandScale operand_scale);
 
   // Set the bytecode handler for |bytecode| and |operand_scale|.
   void SetBytecodeHandler(Bytecode bytecode, OperandScale operand_scale,
-                          CodeT handler);
+                          CodeDataContainer handler);
 
   V8_EXPORT_PRIVATE Handle<JSObject> GetDispatchCountersObject();
 
diff --git a/src/logging/log.cc b/src/logging/log.cc
index ae6a85af488..5de5572ba32 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -1374,7 +1374,7 @@ void V8FileLogger::LogCodeDisassemble(Handle<AbstractCode> code) {
 #endif
     } else if (code->IsCodeDataContainer(cage_base)) {
 #ifdef ENABLE_DISASSEMBLER
-      CodeT::cast(*code).Disassemble(nullptr, stream, isolate_);
+      CodeDataContainer::cast(*code).Disassemble(nullptr, stream, isolate_);
 #endif
     } else {
       BytecodeArray::cast(*code).Disassemble(stream);
@@ -1956,7 +1956,7 @@ EnumerateCompiledFunctions(Heap* heap) {
           Script::cast(function.shared().script()).HasValidSource()) {
         // TODO(v8:13261): use ToAbstractCode() here.
         record(function.shared(),
-               AbstractCode::cast(FromCodeT(function.code())));
+               AbstractCode::cast(FromCodeDataContainer(function.code())));
       }
     }
   }
@@ -2337,7 +2337,7 @@ void ExistingCodeLogger::LogCodeObjects() {
     // AbstactCode is Code|CodeDataContainer|BytecodeArray but we want to log
     // code objects only once, thus we ignore Code objects which will be logged
     // via corresponding CodeDataContainer.
-    if (InstanceTypeChecker::IsCodeT(instance_type) ||
+    if (InstanceTypeChecker::IsCodeDataContainer(instance_type) ||
         InstanceTypeChecker::IsBytecodeArray(instance_type)) {
       LogCodeObject(AbstractCode::cast(obj));
     }
@@ -2347,7 +2347,7 @@ void ExistingCodeLogger::LogCodeObjects() {
 void ExistingCodeLogger::LogBuiltins() {
   DCHECK(isolate_->builtins()->is_initialized());
   // The main "copy" of used builtins are logged by LogCodeObjects() while
-  // iterating CodeT objects.
+  // iterating CodeDataContainer objects.
   // TODO(v8:11880): Log other copies of remapped builtins once we
   // decide to remap them multiple times into the code range (for example
   // for arm64).
@@ -2370,15 +2370,14 @@ void ExistingCodeLogger::LogCompiledFunctions(
     if (shared->HasInterpreterData()) {
       // TODO(v8:13261): use ToAbstractCode() here.
       LogExistingFunction(
-          shared,
-          Handle<AbstractCode>(
-              AbstractCode::cast(FromCodeT(shared->InterpreterTrampoline())),
-              isolate_));
+          shared, Handle<AbstractCode>(AbstractCode::cast(FromCodeDataContainer(
+                                           shared->InterpreterTrampoline())),
+                                       isolate_));
     }
     if (shared->HasBaselineCode()) {
       // TODO(v8:13261): use ToAbstractCode() here.
       LogExistingFunction(shared, Handle<AbstractCode>(
-                                      AbstractCode::cast(FromCodeT(
+                                      AbstractCode::cast(FromCodeDataContainer(
                                           shared->baseline_code(kAcquireLoad))),
                                       isolate_));
     }
diff --git a/src/maglev/maglev-compiler.cc b/src/maglev/maglev-compiler.cc
index 86f4dc45a0a..51ae1879947 100644
--- a/src/maglev/maglev-compiler.cc
+++ b/src/maglev/maglev-compiler.cc
@@ -427,7 +427,7 @@ bool MaglevCompiler::Compile(LocalIsolate* local_isolate,
 }
 
 // static
-MaybeHandle<CodeT> MaglevCompiler::GenerateCode(
+MaybeHandle<CodeDataContainer> MaglevCompiler::GenerateCode(
     Isolate* isolate, MaglevCompilationInfo* compilation_info) {
   MaglevCodeGenerator* const code_generator =
       compilation_info->code_generator();
@@ -453,7 +453,7 @@ MaybeHandle<CodeT> MaglevCompiler::GenerateCode(
     code->Print();
   }
 
-  return ToCodeT(code, isolate);
+  return ToCodeDataContainer(code, isolate);
 }
 
 }  // namespace maglev
diff --git a/src/maglev/maglev-compiler.h b/src/maglev/maglev-compiler.h
index 12520e515f3..549ab246a54 100644
--- a/src/maglev/maglev-compiler.h
+++ b/src/maglev/maglev-compiler.h
@@ -29,7 +29,7 @@ class MaglevCompiler : public AllStatic {
 
   // Called on the main thread after Compile has completed.
   // TODO(v8:7700): Move this to a different class?
-  static MaybeHandle<CodeT> GenerateCode(
+  static MaybeHandle<CodeDataContainer> GenerateCode(
       Isolate* isolate, MaglevCompilationInfo* compilation_info);
 };
 
diff --git a/src/maglev/maglev-concurrent-dispatcher.cc b/src/maglev/maglev-concurrent-dispatcher.cc
index a098505e4dd..73c2d3cf378 100644
--- a/src/maglev/maglev-concurrent-dispatcher.cc
+++ b/src/maglev/maglev-concurrent-dispatcher.cc
@@ -114,11 +114,13 @@ CompilationJob::Status MaglevCompilationJob::ExecuteJobImpl(
 }
 
 CompilationJob::Status MaglevCompilationJob::FinalizeJobImpl(Isolate* isolate) {
-  Handle<CodeT> codet;
-  if (!maglev::MaglevCompiler::GenerateCode(isolate, info()).ToHandle(&codet)) {
+  Handle<CodeDataContainer> code_data_container;
+  if (!maglev::MaglevCompiler::GenerateCode(isolate, info())
+           .ToHandle(&code_data_container)) {
     return CompilationJob::FAILED;
   }
-  info()->toplevel_compilation_unit()->function().object()->set_code(*codet);
+  info()->toplevel_compilation_unit()->function().object()->set_code(
+      *code_data_container);
   return CompilationJob::SUCCEEDED;
 }
 
diff --git a/src/maglev/maglev.cc b/src/maglev/maglev.cc
index bfc22f12fb7..44209d31087 100644
--- a/src/maglev/maglev.cc
+++ b/src/maglev/maglev.cc
@@ -11,8 +11,8 @@
 namespace v8 {
 namespace internal {
 
-MaybeHandle<CodeT> Maglev::Compile(Isolate* isolate,
-                                   Handle<JSFunction> function) {
+MaybeHandle<CodeDataContainer> Maglev::Compile(Isolate* isolate,
+                                               Handle<JSFunction> function) {
   DCHECK(v8_flags.maglev);
   std::unique_ptr<maglev::MaglevCompilationInfo> info =
       maglev::MaglevCompilationInfo::New(isolate, function);
diff --git a/src/maglev/maglev.h b/src/maglev/maglev.h
index 7207fdec5ea..f7b7ba11779 100644
--- a/src/maglev/maglev.h
+++ b/src/maglev/maglev.h
@@ -19,8 +19,8 @@ class Maglev : public AllStatic {
  public:
   // TODO(v8:7700): This entry point is only used for testing. Consider
   // removing it once BenchMaglev runtime functions are no longer useful.
-  static MaybeHandle<CodeT> Compile(Isolate* isolate,
-                                    Handle<JSFunction> function);
+  static MaybeHandle<CodeDataContainer> Compile(Isolate* isolate,
+                                                Handle<JSFunction> function);
 };
 
 }  // namespace internal
diff --git a/src/objects/call-site-info-inl.h b/src/objects/call-site-info-inl.h
index bbcc88f4c4a..eebe993bbc1 100644
--- a/src/objects/call-site-info-inl.h
+++ b/src/objects/call-site-info-inl.h
@@ -37,9 +37,10 @@ DEF_GETTER(CallSiteInfo, code_object, HeapObject) {
 
 void CallSiteInfo::set_code_object(HeapObject code, WriteBarrierMode mode) {
   // The |code_object| field can contain many types of objects, but only Code
-  // values have to be converted to CodeT.
+  // values have to be converted to CodeDataContainer.
   if (IsCodeSpaceObject(code)) {
-    TorqueGeneratedClass::set_code_object(ToCodeT(Code::cast(code)), mode);
+    TorqueGeneratedClass::set_code_object(ToCodeDataContainer(Code::cast(code)),
+                                          mode);
   } else {
     TorqueGeneratedClass::set_code_object(code, mode);
   }
diff --git a/src/objects/code-inl.h b/src/objects/code-inl.h
index 00a7b321eea..6bd25347cef 100644
--- a/src/objects/code-inl.h
+++ b/src/objects/code-inl.h
@@ -52,7 +52,7 @@ int AbstractCode::InstructionSize(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionSize();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().InstructionSize();
+    return GetCodeDataContainer().InstructionSize();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return GetBytecodeArray().length();
@@ -66,11 +66,11 @@ ByteArray AbstractCode::SourcePositionTableInternal(
     DCHECK_NE(GetCode().kind(), CodeKind::BASELINE);
     return GetCode().source_position_table(cage_base);
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    CodeT codet = GetCodeT();
-    if (codet.is_off_heap_trampoline()) {
+    CodeDataContainer code_data_container = GetCodeDataContainer();
+    if (code_data_container.is_off_heap_trampoline()) {
       return GetReadOnlyRoots().empty_byte_array();
     }
-    return codet.source_position_table(cage_base);
+    return code_data_container.source_position_table(cage_base);
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return GetBytecodeArray().SourcePositionTable(cage_base);
@@ -83,11 +83,12 @@ ByteArray AbstractCode::SourcePositionTable(PtrComprCageBase cage_base,
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().SourcePositionTable(cage_base, sfi);
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    CodeT codet = GetCodeT();
-    if (codet.is_off_heap_trampoline()) {
+    CodeDataContainer code_data_container = GetCodeDataContainer();
+    if (code_data_container.is_off_heap_trampoline()) {
       return GetReadOnlyRoots().empty_byte_array();
     }
-    return FromCodeT(codet).SourcePositionTable(cage_base, sfi);
+    return FromCodeDataContainer(code_data_container)
+        .SourcePositionTable(cage_base, sfi);
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return GetBytecodeArray().SourcePositionTable(cage_base);
@@ -99,10 +100,11 @@ int AbstractCode::SizeIncludingMetadata(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().SizeIncludingMetadata(cage_base);
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    CodeT codet = GetCodeT();
-    return codet.is_off_heap_trampoline()
+    CodeDataContainer code_data_container = GetCodeDataContainer();
+    return code_data_container.is_off_heap_trampoline()
                ? 0
-               : FromCodeT(codet).SizeIncludingMetadata(cage_base);
+               : FromCodeDataContainer(code_data_container)
+                     .SizeIncludingMetadata(cage_base);
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return GetBytecodeArray().SizeIncludingMetadata();
@@ -114,7 +116,7 @@ Address AbstractCode::InstructionStart(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionStart();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().InstructionStart();
+    return GetCodeDataContainer().InstructionStart();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return GetBytecodeArray().GetFirstBytecodeAddress();
@@ -126,7 +128,7 @@ Address AbstractCode::InstructionEnd(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().InstructionEnd();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().InstructionEnd();
+    return GetCodeDataContainer().InstructionEnd();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     BytecodeArray bytecode_array = GetBytecodeArray();
@@ -140,7 +142,7 @@ bool AbstractCode::contains(Isolate* isolate, Address inner_pointer) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().contains(isolate, inner_pointer);
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().contains(isolate, inner_pointer);
+    return GetCodeDataContainer().contains(isolate, inner_pointer);
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return (address() <= inner_pointer) &&
@@ -153,7 +155,7 @@ CodeKind AbstractCode::kind(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().kind();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().kind();
+    return GetCodeDataContainer().kind();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return CodeKind::INTERPRETED_FUNCTION;
@@ -165,7 +167,7 @@ Builtin AbstractCode::builtin_id(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().builtin_id();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().builtin_id();
+    return GetCodeDataContainer().builtin_id();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return Builtin::kNoBuiltinId;
@@ -177,7 +179,7 @@ bool AbstractCode::is_off_heap_trampoline(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().is_off_heap_trampoline();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().is_off_heap_trampoline();
+    return GetCodeDataContainer().is_off_heap_trampoline();
   } else {
     DCHECK(InstanceTypeChecker::IsBytecodeArray(instance_type));
     return false;
@@ -190,7 +192,7 @@ HandlerTable::CatchPrediction AbstractCode::GetBuiltinCatchPrediction(
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode().GetBuiltinCatchPrediction();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT().GetBuiltinCatchPrediction();
+    return GetCodeDataContainer().GetBuiltinCatchPrediction();
   } else {
     UNREACHABLE();
   }
@@ -200,8 +202,8 @@ bool AbstractCode::IsCode(PtrComprCageBase cage_base) const {
   return HeapObject::IsCode(cage_base);
 }
 
-bool AbstractCode::IsCodeT(PtrComprCageBase cage_base) const {
-  return HeapObject::IsCodeT(cage_base);
+bool AbstractCode::IsCodeDataContainer(PtrComprCageBase cage_base) const {
+  return HeapObject::IsCodeDataContainer(cage_base);
 }
 
 bool AbstractCode::IsBytecodeArray(PtrComprCageBase cage_base) const {
@@ -210,7 +212,9 @@ bool AbstractCode::IsBytecodeArray(PtrComprCageBase cage_base) const {
 
 Code AbstractCode::GetCode() { return Code::cast(*this); }
 
-CodeT AbstractCode::GetCodeT() { return CodeT::cast(*this); }
+CodeDataContainer AbstractCode::GetCodeDataContainer() {
+  return CodeDataContainer::cast(*this);
+}
 
 BytecodeArray AbstractCode::GetBytecodeArray() {
   return BytecodeArray::cast(*this);
@@ -221,20 +225,21 @@ Code AbstractCode::ToCode(PtrComprCageBase cage_base) {
   if (InstanceTypeChecker::IsCode(instance_type)) {
     return GetCode();
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    CodeT codet = GetCodeT();
-    DCHECK(!codet.is_off_heap_trampoline());
-    return FromCodeT(codet);
+    CodeDataContainer code_data_container = GetCodeDataContainer();
+    DCHECK(!code_data_container.is_off_heap_trampoline());
+    return FromCodeDataContainer(code_data_container);
   } else {
     UNREACHABLE();
   }
 }
 
-CodeT AbstractCode::ToCodeT(PtrComprCageBase cage_base) {
+CodeDataContainer AbstractCode::ToCodeDataContainer(
+    PtrComprCageBase cage_base) {
   InstanceType instance_type = map(cage_base).instance_type();
   if (InstanceTypeChecker::IsCode(instance_type)) {
-    return i::ToCodeT(GetCode());
+    return i::ToCodeDataContainer(GetCode());
   } else if (InstanceTypeChecker::IsCodeDataContainer(instance_type)) {
-    return GetCodeT();
+    return GetCodeDataContainer();
   } else {
     UNREACHABLE();
   }
@@ -365,30 +370,32 @@ CodeDataContainer Code::GCSafeCodeDataContainer(AcquireLoadTag) const {
 }
 
 // Helper functions for converting Code objects to CodeDataContainer and back.
-inline CodeT ToCodeT(Code code) {
+inline CodeDataContainer ToCodeDataContainer(Code code) {
   return code.code_data_container(kAcquireLoad);
 }
 
-inline Handle<CodeT> ToCodeT(Handle<Code> code, Isolate* isolate) {
-  return handle(ToCodeT(*code), isolate);
+inline Handle<CodeDataContainer> ToCodeDataContainer(Handle<Code> code,
+                                                     Isolate* isolate) {
+  return handle(ToCodeDataContainer(*code), isolate);
 }
 
-inline MaybeHandle<CodeT> ToCodeT(MaybeHandle<Code> maybe_code,
-                                  Isolate* isolate) {
+inline MaybeHandle<CodeDataContainer> ToCodeDataContainer(
+    MaybeHandle<Code> maybe_code, Isolate* isolate) {
   Handle<Code> code;
-  if (maybe_code.ToHandle(&code)) return ToCodeT(code, isolate);
+  if (maybe_code.ToHandle(&code)) return ToCodeDataContainer(code, isolate);
   return {};
 }
 
-inline Code FromCodeT(CodeT code) {
+inline Code FromCodeDataContainer(CodeDataContainer code) {
   DCHECK(!code.is_off_heap_trampoline());
   // Compute the Code object pointer from the code entry point.
   Address ptr = code.code_entry_point() - Code::kHeaderSize + kHeapObjectTag;
   return Code::cast(Object(ptr));
 }
 
-inline Code FromCodeT(CodeT code, PtrComprCageBase code_cage_base,
-                      RelaxedLoadTag tag) {
+inline Code FromCodeDataContainer(CodeDataContainer code,
+                                  PtrComprCageBase code_cage_base,
+                                  RelaxedLoadTag tag) {
   DCHECK(!code.is_off_heap_trampoline());
   // Since the code entry point field is not aligned we can't load it atomically
   // and use for Code object pointer calculation. So, we load and decompress
@@ -396,23 +403,26 @@ inline Code FromCodeT(CodeT code, PtrComprCageBase code_cage_base,
   return code.code(code_cage_base, tag);
 }
 
-inline Code FromCodeT(CodeT code, Isolate* isolate, RelaxedLoadTag tag) {
+inline Code FromCodeDataContainer(CodeDataContainer code, Isolate* isolate,
+                                  RelaxedLoadTag tag) {
 #ifdef V8_EXTERNAL_CODE_SPACE
-  return FromCodeT(code, PtrComprCageBase{isolate->code_cage_base()}, tag);
+  return FromCodeDataContainer(
+      code, PtrComprCageBase{isolate->code_cage_base()}, tag);
 #else
-  return FromCodeT(code, GetPtrComprCageBase(code), tag);
+  return FromCodeDataContainer(code, GetPtrComprCageBase(code), tag);
 #endif  // V8_EXTERNAL_CODE_SPACE
 }
 
-inline Handle<Code> FromCodeT(Handle<CodeT> code, Isolate* isolate) {
-  return handle(FromCodeT(*code), isolate);
+inline Handle<Code> FromCodeDataContainer(Handle<CodeDataContainer> code,
+                                          Isolate* isolate) {
+  return handle(FromCodeDataContainer(*code), isolate);
 }
 
-inline AbstractCode ToAbstractCode(CodeT code) {
+inline AbstractCode ToAbstractCode(CodeDataContainer code) {
   return AbstractCode::cast(code);
 }
 
-inline Handle<AbstractCode> ToAbstractCode(Handle<CodeT> code,
+inline Handle<AbstractCode> ToAbstractCode(Handle<CodeDataContainer> code,
                                            Isolate* isolate) {
   return Handle<AbstractCode>::cast(code);
 }
@@ -474,11 +484,12 @@ AbstractCode CodeLookupResult::ToAbstractCode() const {
 
 Code CodeLookupResult::ToCode() const {
   DCHECK(IsFound());
-  return IsCode() ? code() : FromCodeT(code_data_container());
+  return IsCode() ? code() : FromCodeDataContainer(code_data_container());
 }
 
-CodeT CodeLookupResult::ToCodeT() const {
-  return IsCodeDataContainer() ? code_data_container() : i::ToCodeT(code());
+CodeDataContainer CodeLookupResult::ToCodeDataContainer() const {
+  return IsCodeDataContainer() ? code_data_container()
+                               : i::ToCodeDataContainer(code());
 }
 
 void Code::WipeOutHeader() {
@@ -1445,14 +1456,16 @@ inline bool CodeDataContainer::is_baseline_leave_frame_builtin() const {
 //
 
 #define DEF_PRIMITIVE_FORWARDING_CDC_GETTER(name, type) \
-  type CodeDataContainer::name() const { return FromCodeT(*this).name(); }
+  type CodeDataContainer::name() const {                \
+    return FromCodeDataContainer(*this).name();         \
+  }
 
 #define DEF_FORWARDING_CDC_GETTER(name, type, result_if_off_heap) \
   DEF_GETTER(CodeDataContainer, name, type) {                     \
     if (is_off_heap_trampoline()) {                               \
       return GetReadOnlyRoots().result_if_off_heap();             \
     }                                                             \
-    return FromCodeT(*this).name(cage_base);                      \
+    return FromCodeDataContainer(*this).name(cage_base);          \
   }
 
 DEF_FORWARDING_CDC_GETTER(deoptimization_data, FixedArray, empty_fixed_array)
diff --git a/src/objects/code.cc b/src/objects/code.cc
index fcf776024e4..05e1ecc29ab 100644
--- a/src/objects/code.cc
+++ b/src/objects/code.cc
@@ -204,8 +204,8 @@ void Code::RelocateFromDesc(ByteArray reloc_info, Heap* heap,
       // Rewrite code handles to direct pointers to the first instruction in the
       // code object.
       Handle<HeapObject> p = it.rinfo()->target_object_handle(origin);
-      DCHECK(p->IsCodeT(GetPtrComprCageBaseSlow(*p)));
-      Code code = FromCodeT(CodeT::cast(*p));
+      DCHECK(p->IsCodeDataContainer(GetPtrComprCageBaseSlow(*p)));
+      Code code = FromCodeDataContainer(CodeDataContainer::cast(*p));
       it.rinfo()->set_target_address(code.raw_instruction_start(),
                                      UPDATE_WRITE_BARRIER, SKIP_ICACHE_FLUSH);
     } else if (RelocInfo::IsNearBuiltinEntry(mode)) {
@@ -531,10 +531,10 @@ void DeoptimizationData::DeoptimizationDataPrint(std::ostream& os) {
 
 namespace {
 
-template <typename CodeOrCodeT>
+template <typename CodeOrCodeDataContainer>
 inline void DisassembleCodeRange(Isolate* isolate, std::ostream& os,
-                                 CodeOrCodeT code, Address begin, size_t size,
-                                 Address current_pc) {
+                                 CodeOrCodeDataContainer code, Address begin,
+                                 size_t size, Address current_pc) {
   Address end = begin + size;
   AllowHandleAllocation allow_handles;
   DisallowGarbageCollection no_gc;
@@ -544,9 +544,9 @@ inline void DisassembleCodeRange(Isolate* isolate, std::ostream& os,
                        CodeReference(handle(code, isolate)), current_pc);
 }
 
-template <typename CodeOrCodeT>
+template <typename CodeOrCodeDataContainer>
 void Disassemble(const char* name, std::ostream& os, Isolate* isolate,
-                 CodeOrCodeT code, Address current_pc) {
+                 CodeOrCodeDataContainer code, Address current_pc) {
   CodeKind kind = code.kind();
   os << "kind = " << CodeKindToString(kind) << "\n";
   if (name == nullptr && code.is_builtin()) {
@@ -919,11 +919,12 @@ Handle<DependentCode> DependentCode::InsertWeakCode(
     Handle<Code> code) {
   if (entries->length() == entries->capacity()) {
     // We'd have to grow - try to compact first.
-    entries->IterateAndCompact([](CodeT, DependencyGroups) { return false; });
+    entries->IterateAndCompact(
+        [](CodeDataContainer, DependencyGroups) { return false; });
   }
 
-  MaybeObjectHandle code_slot(HeapObjectReference::Weak(ToCodeT(*code)),
-                              isolate);
+  MaybeObjectHandle code_slot(
+      HeapObjectReference::Weak(ToCodeDataContainer(*code)), isolate);
   MaybeObjectHandle group_slot(MaybeObject::FromSmi(Smi::FromInt(groups)),
                                isolate);
   entries = Handle<DependentCode>::cast(
@@ -936,7 +937,7 @@ Handle<DependentCode> DependentCode::New(Isolate* isolate,
                                          Handle<Code> code) {
   Handle<DependentCode> result = Handle<DependentCode>::cast(
       isolate->factory()->NewWeakArrayList(LengthFor(1), AllocationType::kOld));
-  result->Set(0, HeapObjectReference::Weak(ToCodeT(*code)));
+  result->Set(0, HeapObjectReference::Weak(ToCodeDataContainer(*code)));
   result->Set(1, Smi::FromInt(groups));
   return result;
 }
@@ -961,7 +962,7 @@ void DependentCode::IterateAndCompact(const IterateAndCompactFn& fn) {
       continue;
     }
 
-    if (fn(CodeT::cast(obj->GetHeapObjectAssumeWeak()),
+    if (fn(CodeDataContainer::cast(obj->GetHeapObjectAssumeWeak()),
            static_cast<DependencyGroups>(
                Get(i + kGroupsSlotOffset).ToSmi().value()))) {
       len = FillEntryFromBack(i, len);
@@ -978,7 +979,7 @@ bool DependentCode::MarkCodeForDeoptimization(
   DisallowGarbageCollection no_gc;
 
   bool marked_something = false;
-  IterateAndCompact([&](CodeT code, DependencyGroups groups) {
+  IterateAndCompact([&](CodeDataContainer code, DependencyGroups groups) {
     if ((groups & deopt_groups) == 0) return false;
 
     if (!code.marked_for_deoptimization()) {
@@ -1029,7 +1030,7 @@ void Code::SetMarkedForDeoptimization(const char* reason) {
 
 void CodeDataContainer::SetMarkedForDeoptimization(const char* reason) {
   set_marked_for_deoptimization(true);
-  Deoptimizer::TraceMarkForDeoptimization(FromCodeT(*this), reason);
+  Deoptimizer::TraceMarkForDeoptimization(FromCodeDataContainer(*this), reason);
 }
 
 const char* DependentCode::DependencyGroupName(DependencyGroup group) {
diff --git a/src/objects/code.h b/src/objects/code.h
index 94e88f4452d..7fe94ea0133 100644
--- a/src/objects/code.h
+++ b/src/objects/code.h
@@ -887,11 +887,6 @@ class CodeLookupResult {
     return code_data_container_;
   }
 
-  // Returns the CodeT object corresponding to the result in question.
-  // The method doesn't try to convert Code result to CodeT, one should use
-  // ToCodeT() instead if the conversion logic is required.
-  CodeT codet() const { return code_data_container(); }
-
   // Helper methods, in case of successful lookup return the result of
   // respective accessor of the Code/CodeDataContainer object found.
   // It's safe use them from GC.
@@ -923,10 +918,10 @@ class CodeLookupResult {
   // check.
   inline Code ToCode() const;
 
-  // Helper method, converts the successful lookup result to CodeT object.
-  // It's not safe to be used from GC because conversion might perform a map
-  // check.
-  inline CodeT ToCodeT() const;
+  // Helper method, converts the successful lookup result to CodeDataContainer
+  // object. It's not safe to be used from GC because conversion might perform a
+  // map check.
+  inline CodeDataContainer ToCodeDataContainer() const;
 
   bool operator==(const CodeLookupResult& other) const {
     return code_ == other.code_ &&
@@ -957,16 +952,19 @@ class Code::OptimizedCodeIterator {
   DISALLOW_GARBAGE_COLLECTION(no_gc)
 };
 
-// Helper functions for converting Code objects to CodeDataContainer and back
-// when V8_EXTERNAL_CODE_SPACE is enabled.
-inline CodeT ToCodeT(Code code);
-inline Handle<CodeT> ToCodeT(Handle<Code> code, Isolate* isolate);
-inline Code FromCodeT(CodeT code);
-inline Code FromCodeT(CodeT code, Isolate* isolate, RelaxedLoadTag);
-inline Code FromCodeT(CodeT code, PtrComprCageBase, RelaxedLoadTag);
-inline Handle<Code> FromCodeT(Handle<CodeT> code, Isolate* isolate);
-inline AbstractCode ToAbstractCode(CodeT code);
-inline Handle<AbstractCode> ToAbstractCode(Handle<CodeT> code,
+// Helper functions for converting Code objects to CodeDataContainer and back.
+inline CodeDataContainer ToCodeDataContainer(Code code);
+inline Handle<CodeDataContainer> ToCodeDataContainer(Handle<Code> code,
+                                                     Isolate* isolate);
+inline Code FromCodeDataContainer(CodeDataContainer code);
+inline Code FromCodeDataContainer(CodeDataContainer code, Isolate* isolate,
+                                  RelaxedLoadTag);
+inline Code FromCodeDataContainer(CodeDataContainer code, PtrComprCageBase,
+                                  RelaxedLoadTag);
+inline Handle<Code> FromCodeDataContainer(Handle<CodeDataContainer> code,
+                                          Isolate* isolate);
+inline AbstractCode ToAbstractCode(CodeDataContainer code);
+inline Handle<AbstractCode> ToAbstractCode(Handle<CodeDataContainer> code,
                                            Isolate* isolate);
 
 // AbstractCode is a helper wrapper around
@@ -1026,14 +1024,14 @@ class AbstractCode : public HeapObject {
   // should work for both regular V8 heap objects and external code space
   // objects.
   inline bool IsCode(PtrComprCageBase cage_base) const;
-  inline bool IsCodeT(PtrComprCageBase cage_base) const;
+  inline bool IsCodeDataContainer(PtrComprCageBase cage_base) const;
   inline bool IsBytecodeArray(PtrComprCageBase cage_base) const;
 
   inline Code ToCode(PtrComprCageBase cage_base);
-  inline CodeT ToCodeT(PtrComprCageBase cage_base);
+  inline CodeDataContainer ToCodeDataContainer(PtrComprCageBase cage_base);
 
   inline Code GetCode();
-  inline CodeT GetCodeT();
+  inline CodeDataContainer GetCodeDataContainer();
   inline BytecodeArray GetBytecodeArray();
 
   // AbstractCode might be represented by both Code and non-Code objects and
@@ -1148,7 +1146,8 @@ class DependentCode : public WeakArrayList {
 
   // The callback is called for all non-cleared entries, and should return true
   // iff the current entry should be cleared.
-  using IterateAndCompactFn = std::function<bool(CodeT, DependencyGroups)>;
+  using IterateAndCompactFn =
+      std::function<bool(CodeDataContainer, DependencyGroups)>;
   void IterateAndCompact(const IterateAndCompactFn& fn);
 
   // Fills the given entry with the last non-cleared entry in this list, and
diff --git a/src/objects/data-handler.tq b/src/objects/data-handler.tq
index 144dc382081..e3ccf3d9555 100644
--- a/src/objects/data-handler.tq
+++ b/src/objects/data-handler.tq
@@ -6,10 +6,10 @@
 // here, please also update DataHandlerVerify in objects-debug.cc.
 @abstract
 extern class DataHandler extends Struct {
-  // [smi_handler]: A Smi which encodes a handler or CodeT object (we still
-  // use code handlers for accessing lexical environment variables, but soon
-  // only smi handlers will remain). See LoadHandler and StoreHandler for
-  // details about encoding.
+  // [smi_handler]: A Smi which encodes a handler or CodeDataContainer object
+  // (we still use code handlers for accessing lexical environment variables,
+  // but soon only smi handlers will remain). See LoadHandler and StoreHandler
+  // for details about encoding.
   smi_handler: Smi|CodeDataContainer;
 
   // [validity_cell]: A validity Cell that guards prototype chain modifications.
diff --git a/src/objects/feedback-vector-inl.h b/src/objects/feedback-vector-inl.h
index dd8016c13e7..a0566b4693c 100644
--- a/src/objects/feedback-vector-inl.h
+++ b/src/objects/feedback-vector-inl.h
@@ -148,13 +148,13 @@ void FeedbackVector::set_maybe_has_optimized_osr_code(bool value) {
   set_osr_state(MaybeHasOptimizedOsrCodeBit::update(osr_state(), value));
 }
 
-CodeT FeedbackVector::optimized_code() const {
+CodeDataContainer FeedbackVector::optimized_code() const {
   MaybeObject slot = maybe_optimized_code(kAcquireLoad);
   DCHECK(slot->IsWeakOrCleared());
   HeapObject heap_object;
-  CodeT code;
+  CodeDataContainer code;
   if (slot->GetHeapObject(&heap_object)) {
-    code = CodeT::cast(heap_object);
+    code = CodeDataContainer::cast(heap_object);
   }
   // It is possible that the maybe_optimized_code slot is cleared but the flags
   // haven't been updated yet. We update them when we execute the function next
@@ -202,20 +202,21 @@ void FeedbackVector::set_log_next_execution(bool value) {
   set_flags(LogNextExecutionBit::update(flags(), value));
 }
 
-base::Optional<CodeT> FeedbackVector::GetOptimizedOsrCode(Isolate* isolate,
-                                                          FeedbackSlot slot) {
+base::Optional<CodeDataContainer> FeedbackVector::GetOptimizedOsrCode(
+    Isolate* isolate, FeedbackSlot slot) {
   MaybeObject maybe_code = Get(isolate, slot);
   if (maybe_code->IsCleared()) return {};
 
-  CodeT codet = CodeT::cast(maybe_code->GetHeapObject());
-  if (codet.marked_for_deoptimization()) {
+  CodeDataContainer code_data_container =
+      CodeDataContainer::cast(maybe_code->GetHeapObject());
+  if (code_data_container.marked_for_deoptimization()) {
     // Clear the cached Code object if deoptimized.
     // TODO(jgruber): Add tracing.
     Set(slot, HeapObjectReference::ClearedValue(isolate));
     return {};
   }
 
-  return codet;
+  return code_data_container;
 }
 
 // Conversion from an integer index to either a slot or an ic slot.
diff --git a/src/objects/feedback-vector.cc b/src/objects/feedback-vector.cc
index 4c19c07fc2f..33c8ca0eb66 100644
--- a/src/objects/feedback-vector.cc
+++ b/src/objects/feedback-vector.cc
@@ -352,7 +352,7 @@ void FeedbackVector::SaturatingIncrementProfilerTicks() {
   if (ticks < Smi::kMaxValue) set_profiler_ticks(ticks + 1);
 }
 
-void FeedbackVector::SetOptimizedCode(CodeT code) {
+void FeedbackVector::SetOptimizedCode(CodeDataContainer code) {
   DCHECK(CodeKindIsOptimizedJSFunction(code.kind()));
   // We should set optimized code only when there is no valid optimized code.
   DCHECK(!has_optimized_code() ||
@@ -390,7 +390,8 @@ void FeedbackVector::ClearOptimizedCode() {
   set_maybe_has_turbofan_code(false);
 }
 
-void FeedbackVector::SetOptimizedOsrCode(FeedbackSlot slot, CodeT code) {
+void FeedbackVector::SetOptimizedOsrCode(FeedbackSlot slot,
+                                         CodeDataContainer code) {
   DCHECK(CodeKindIsOptimizedJSFunction(code.kind()));
   DCHECK(!slot.IsInvalid());
   Set(slot, HeapObjectReference::Weak(code));
@@ -438,7 +439,7 @@ void FeedbackVector::EvictOptimizedCodeMarkedForDeoptimization(
     return;
   }
 
-  CodeT code = CodeT::cast(slot->GetHeapObject());
+  CodeDataContainer code = CodeDataContainer::cast(slot->GetHeapObject());
   if (code.marked_for_deoptimization()) {
     Deoptimizer::TraceEvictFromOptimizedCodeCache(shared, reason);
     ClearOptimizedCode();
@@ -1224,7 +1225,8 @@ KeyedAccessStoreMode FeedbackNexus::GetKeyedAccessStoreMode() const {
         if (mode != STANDARD_STORE) return mode;
         continue;
       } else {
-        CodeT code = CodeT::cast(data_handler->smi_handler());
+        CodeDataContainer code =
+            CodeDataContainer::cast(data_handler->smi_handler());
         builtin_handler = code.builtin_id();
       }
 
@@ -1243,7 +1245,8 @@ KeyedAccessStoreMode FeedbackNexus::GetKeyedAccessStoreMode() const {
       continue;
     } else {
       // Element store without prototype chain check.
-      CodeT code = CodeT::cast(*maybe_code_handler.object());
+      CodeDataContainer code =
+          CodeDataContainer::cast(*maybe_code_handler.object());
       builtin_handler = code.builtin_id();
     }
 
diff --git a/src/objects/feedback-vector.h b/src/objects/feedback-vector.h
index 8d096fa948b..c8d42485a37 100644
--- a/src/objects/feedback-vector.h
+++ b/src/objects/feedback-vector.h
@@ -255,7 +255,7 @@ class FeedbackVector
   // The `osr_state` contains the osr_urgency and maybe_has_optimized_osr_code.
   inline void reset_osr_state();
 
-  inline CodeT optimized_code() const;
+  inline CodeDataContainer optimized_code() const;
   // Whether maybe_optimized_code contains a cached Code object.
   inline bool has_optimized_code() const;
 
@@ -269,16 +269,16 @@ class FeedbackVector
   inline bool maybe_has_turbofan_code() const;
   inline void set_maybe_has_turbofan_code(bool value);
 
-  void SetOptimizedCode(CodeT code);
+  void SetOptimizedCode(CodeDataContainer code);
   void EvictOptimizedCodeMarkedForDeoptimization(SharedFunctionInfo shared,
                                                  const char* reason);
   void ClearOptimizedCode();
 
   // Optimized OSR'd code is cached in JumpLoop feedback vector slots. The
   // slots either contain a Code object or the ClearedValue.
-  inline base::Optional<CodeT> GetOptimizedOsrCode(Isolate* isolate,
-                                                   FeedbackSlot slot);
-  void SetOptimizedOsrCode(FeedbackSlot slot, CodeT code);
+  inline base::Optional<CodeDataContainer> GetOptimizedOsrCode(
+      Isolate* isolate, FeedbackSlot slot);
+  void SetOptimizedOsrCode(FeedbackSlot slot, CodeDataContainer code);
 
   inline TieringState tiering_state() const;
   void set_tiering_state(TieringState state);
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index 752e91b61e6..0c8d1377eee 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -101,7 +101,6 @@ class HeapObject : public Object {
   HEAP_OBJECT_TYPE_LIST(IS_TYPE_FUNCTION_DECL)
   IS_TYPE_FUNCTION_DECL(HashTableBase)
   IS_TYPE_FUNCTION_DECL(SmallOrderedHashTable)
-  IS_TYPE_FUNCTION_DECL(CodeT)
 #undef IS_TYPE_FUNCTION_DECL
 
 // Oddball checks are faster when they are raw pointer comparisons, so the
diff --git a/src/objects/instance-type-inl.h b/src/objects/instance-type-inl.h
index e5b34c4365e..0f3a22e0d7b 100644
--- a/src/objects/instance-type-inl.h
+++ b/src/objects/instance-type-inl.h
@@ -88,10 +88,6 @@ V8_INLINE constexpr bool IsFreeSpaceOrFiller(InstanceType instance_type) {
   return instance_type == FREE_SPACE_TYPE || instance_type == FILLER_TYPE;
 }
 
-V8_INLINE constexpr bool IsCodeT(InstanceType instance_type) {
-  return instance_type == CODET_TYPE;
-}
-
 }  // namespace InstanceTypeChecker
 
 // INSTANCE_TYPE_CHECKERS macro defines some "types" that do not have
diff --git a/src/objects/instance-type.h b/src/objects/instance-type.h
index a4f4cc15b99..791055c5b16 100644
--- a/src/objects/instance-type.h
+++ b/src/objects/instance-type.h
@@ -191,8 +191,6 @@ enum InstanceType : uint16_t {
   FIRST_TYPE = FIRST_HEAP_OBJECT_TYPE,
   LAST_TYPE = LAST_HEAP_OBJECT_TYPE,
   BIGINT_TYPE = BIG_INT_BASE_TYPE,
-
-  CODET_TYPE = CODE_DATA_CONTAINER_TYPE,
 };
 
 // This constant is defined outside of the InstanceType enum because the
@@ -292,8 +290,6 @@ namespace InstanceTypeChecker {
 
 INSTANCE_TYPE_CHECKERS(IS_TYPE_FUNCTION_DECL)
 
-IS_TYPE_FUNCTION_DECL(CodeT)
-
 #undef IS_TYPE_FUNCTION_DECL
 }  // namespace InstanceTypeChecker
 
diff --git a/src/objects/js-function-inl.h b/src/objects/js-function-inl.h
index 669a5301228..542f77ca689 100644
--- a/src/objects/js-function-inl.h
+++ b/src/objects/js-function-inl.h
@@ -69,10 +69,12 @@ AbstractCode JSFunction::abstract_code(IsolateT* isolate) {
 
 int JSFunction::length() { return shared().length(); }
 
-ACCESSORS_RELAXED(JSFunction, code, CodeT, kCodeOffset)
-RELEASE_ACQUIRE_GETTER_CHECKED(JSFunction, code, CodeT, kCodeOffset, true)
-void JSFunction::set_code(CodeT value, ReleaseStoreTag, WriteBarrierMode mode) {
-  TaggedField<CodeT, kCodeOffset>::Release_Store(*this, value);
+ACCESSORS_RELAXED(JSFunction, code, CodeDataContainer, kCodeOffset)
+RELEASE_ACQUIRE_GETTER_CHECKED(JSFunction, code, CodeDataContainer, kCodeOffset,
+                               true)
+void JSFunction::set_code(CodeDataContainer value, ReleaseStoreTag,
+                          WriteBarrierMode mode) {
+  TaggedField<CodeDataContainer, kCodeOffset>::Release_Store(*this, value);
   CONDITIONAL_WRITE_BARRIER(*this, kCodeOffset, value, mode);
   if (V8_UNLIKELY(v8_flags.log_function_events && has_feedback_vector())) {
     feedback_vector().set_log_next_execution(true);
@@ -81,7 +83,7 @@ void JSFunction::set_code(CodeT value, ReleaseStoreTag, WriteBarrierMode mode) {
 RELEASE_ACQUIRE_ACCESSORS(JSFunction, context, Context, kContextOffset)
 
 void JSFunction::set_code(Code code, ReleaseStoreTag, WriteBarrierMode mode) {
-  set_code(ToCodeT(code), kReleaseStore, mode);
+  set_code(ToCodeDataContainer(code), kReleaseStore, mode);
 }
 
 Address JSFunction::code_entry_point() const {
@@ -237,8 +239,8 @@ bool JSFunction::ShouldFlushBaselineCode(
   // code field. We don't use release stores when copying code pointers from
   // SFI / FV to JSFunction but it is safe in practice.
   Object maybe_code = ACQUIRE_READ_FIELD(*this, kCodeOffset);
-  if (!maybe_code.IsCodeT()) return false;
-  CodeT code = CodeT::cast(maybe_code);
+  if (!maybe_code.IsCodeDataContainer()) return false;
+  CodeDataContainer code = CodeDataContainer::cast(maybe_code);
   if (code.kind() != CodeKind::BASELINE) return false;
 
   SharedFunctionInfo shared = SharedFunctionInfo::cast(maybe_shared);
@@ -254,8 +256,8 @@ bool JSFunction::NeedsResetDueToFlushedBytecode() {
   if (!maybe_shared.IsSharedFunctionInfo()) return false;
 
   Object maybe_code = ACQUIRE_READ_FIELD(*this, kCodeOffset);
-  if (!maybe_code.IsCodeT()) return false;
-  CodeT code = CodeT::cast(maybe_code);
+  if (!maybe_code.IsCodeDataContainer()) return false;
+  CodeDataContainer code = CodeDataContainer::cast(maybe_code);
 
   SharedFunctionInfo shared = SharedFunctionInfo::cast(maybe_shared);
   return !shared.is_compiled() && code.builtin_id() != Builtin::kCompileLazy;
diff --git a/src/objects/js-function.cc b/src/objects/js-function.cc
index b1ccfa7259f..e6b8a8363ba 100644
--- a/src/objects/js-function.cc
+++ b/src/objects/js-function.cc
@@ -52,7 +52,7 @@ CodeKinds JSFunction::GetAvailableCodeKinds() const {
   // Check the optimized code cache.
   if (has_feedback_vector() && feedback_vector().has_optimized_code() &&
       !feedback_vector().optimized_code().marked_for_deoptimization()) {
-    CodeT code = feedback_vector().optimized_code();
+    CodeDataContainer code = feedback_vector().optimized_code();
     DCHECK(CodeKindIsOptimizedJSFunction(code.kind()));
     result |= CodeKindToCodeKindFlag(code.kind());
   }
diff --git a/src/objects/js-function.h b/src/objects/js-function.h
index c4bb5d06d21..76331de1890 100644
--- a/src/objects/js-function.h
+++ b/src/objects/js-function.h
@@ -118,10 +118,12 @@ class JSFunction : public TorqueGeneratedJSFunction<
   // optimized code object, or when reading from the background thread.
   // Storing a builtin doesn't require release semantics because these objects
   // are fully initialized.
-  DECL_ACCESSORS(code, CodeT)
-  DECL_RELEASE_ACQUIRE_ACCESSORS(code, CodeT)
-  // Convenient overloads to avoid unnecessary Code <-> CodeT conversions.
-  // TODO(v8:11880): remove once |code| accessors are migrated to CodeT.
+  DECL_ACCESSORS(code, CodeDataContainer)
+  DECL_RELEASE_ACQUIRE_ACCESSORS(code, CodeDataContainer)
+  // Convenient overloads to avoid unnecessary Code <-> CodeDataContainer
+  // conversions.
+  // TODO(v8:11880): remove once |code| accessors are migrated to
+  // CodeDataContainer.
   inline void set_code(Code code, ReleaseStoreTag,
                        WriteBarrierMode mode = UPDATE_WRITE_BARRIER);
 
diff --git a/src/objects/js-regexp-inl.h b/src/objects/js-regexp-inl.h
index bd3b15f8d58..61ca9470004 100644
--- a/src/objects/js-regexp-inl.h
+++ b/src/objects/js-regexp-inl.h
@@ -113,9 +113,9 @@ bool JSRegExp::HasCompiledCode() const {
   if (type_tag() != IRREGEXP) return false;
   Smi uninitialized = Smi::FromInt(kUninitializedValue);
 #ifdef DEBUG
-  DCHECK(DataAt(kIrregexpLatin1CodeIndex).IsCodeT() ||
+  DCHECK(DataAt(kIrregexpLatin1CodeIndex).IsCodeDataContainer() ||
          DataAt(kIrregexpLatin1CodeIndex) == uninitialized);
-  DCHECK(DataAt(kIrregexpUC16CodeIndex).IsCodeT() ||
+  DCHECK(DataAt(kIrregexpUC16CodeIndex).IsCodeDataContainer() ||
          DataAt(kIrregexpUC16CodeIndex) == uninitialized);
   DCHECK(DataAt(kIrregexpLatin1BytecodeIndex).IsByteArray() ||
          DataAt(kIrregexpLatin1BytecodeIndex) == uninitialized);
diff --git a/src/objects/js-regexp.cc b/src/objects/js-regexp.cc
index ac474916b8c..25260f7c1e3 100644
--- a/src/objects/js-regexp.cc
+++ b/src/objects/js-regexp.cc
@@ -153,12 +153,12 @@ MaybeHandle<JSRegExp> JSRegExp::New(Isolate* isolate, Handle<String> pattern,
 Object JSRegExp::code(bool is_latin1) const {
   DCHECK_EQ(type_tag(), JSRegExp::IRREGEXP);
   Object value = DataAt(code_index(is_latin1));
-  DCHECK(value.IsSmi() || value.IsCodeT());
+  DCHECK(value.IsSmi() || value.IsCodeDataContainer());
   return value;
 }
 
 void JSRegExp::set_code(bool is_latin1, Handle<Code> code) {
-  SetDataAt(code_index(is_latin1), ToCodeT(*code));
+  SetDataAt(code_index(is_latin1), ToCodeDataContainer(*code));
 }
 
 Object JSRegExp::bytecode(bool is_latin1) const {
@@ -172,7 +172,7 @@ void JSRegExp::set_bytecode_and_trampoline(Isolate* isolate,
   SetDataAt(kIrregexpLatin1BytecodeIndex, *bytecode);
   SetDataAt(kIrregexpUC16BytecodeIndex, *bytecode);
 
-  Handle<CodeT> trampoline =
+  Handle<CodeDataContainer> trampoline =
       BUILTIN_CODE(isolate, RegExpExperimentalTrampoline);
   SetDataAt(JSRegExp::kIrregexpLatin1CodeIndex, *trampoline);
   SetDataAt(JSRegExp::kIrregexpUC16CodeIndex, *trampoline);
diff --git a/src/objects/objects-inl.h b/src/objects/objects-inl.h
index 618ad5fad84..0b3269cd74f 100644
--- a/src/objects/objects-inl.h
+++ b/src/objects/objects-inl.h
@@ -99,7 +99,6 @@ bool Object::IsJSObjectThatCanBeTrackedAsPrototype() const {
 HEAP_OBJECT_TYPE_LIST(IS_TYPE_FUNCTION_DEF)
 IS_TYPE_FUNCTION_DEF(HashTableBase)
 IS_TYPE_FUNCTION_DEF(SmallOrderedHashTable)
-IS_TYPE_FUNCTION_DEF(CodeT)
 #undef IS_TYPE_FUNCTION_DEF
 
 #define IS_TYPE_FUNCTION_DEF(Type, Value)                        \
@@ -227,8 +226,6 @@ bool HeapObject::IsNullOrUndefined() const {
   return IsNullOrUndefined(GetReadOnlyRoots());
 }
 
-DEF_GETTER(HeapObject, IsCodeT, bool) { return IsCodeDataContainer(cage_base); }
-
 DEF_GETTER(HeapObject, IsUniqueName, bool) {
   return IsInternalizedString(cage_base) || IsSymbol(cage_base);
 }
diff --git a/src/objects/objects.h b/src/objects/objects.h
index 3e99ae6fec8..c674cf1430e 100644
--- a/src/objects/objects.h
+++ b/src/objects/objects.h
@@ -316,7 +316,6 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
   HEAP_OBJECT_TYPE_LIST(IS_TYPE_FUNCTION_DECL)
   IS_TYPE_FUNCTION_DECL(HashTableBase)
   IS_TYPE_FUNCTION_DECL(SmallOrderedHashTable)
-  IS_TYPE_FUNCTION_DECL(CodeT)
 #undef IS_TYPE_FUNCTION_DECL
   V8_INLINE bool IsNumber(ReadOnlyRoots roots) const;
 
diff --git a/src/objects/shared-function-info-inl.h b/src/objects/shared-function-info-inl.h
index 43ef0d3d173..3559b2eac38 100644
--- a/src/objects/shared-function-info-inl.h
+++ b/src/objects/shared-function-info-inl.h
@@ -559,7 +559,7 @@ DEF_GETTER(SharedFunctionInfo, HasBytecodeArray, bool) {
       HeapObject::cast(data).map(cage_base).instance_type();
   return InstanceTypeChecker::IsBytecodeArray(instance_type) ||
          InstanceTypeChecker::IsInterpreterData(instance_type) ||
-         InstanceTypeChecker::IsCodeT(instance_type);
+         InstanceTypeChecker::IsCodeDataContainer(instance_type);
 }
 
 template <typename IsolateT>
@@ -578,8 +578,8 @@ BytecodeArray SharedFunctionInfo::GetBytecodeArray(IsolateT* isolate) const {
 
 BytecodeArray SharedFunctionInfo::GetActiveBytecodeArray() const {
   Object data = function_data(kAcquireLoad);
-  if (data.IsCodeT()) {
-    CodeT baseline_code = CodeT::cast(data);
+  if (data.IsCodeDataContainer()) {
+    CodeDataContainer baseline_code = CodeDataContainer::cast(data);
     data = baseline_code.bytecode_or_interpreter_data();
   }
   if (data.IsBytecodeArray()) {
@@ -623,8 +623,8 @@ bool SharedFunctionInfo::ShouldFlushCode(
   // check if it is old. Note, this is done this way since this function can be
   // called by the concurrent marker.
   Object data = function_data(kAcquireLoad);
-  if (data.IsCodeT()) {
-    CodeT baseline_code = CodeT::cast(data);
+  if (data.IsCodeDataContainer()) {
+    CodeDataContainer baseline_code = CodeDataContainer::cast(data);
     DCHECK_EQ(baseline_code.kind(), CodeKind::BASELINE);
     // If baseline code flushing isn't enabled and we have baseline data on SFI
     // we cannot flush baseline / bytecode.
@@ -644,15 +644,15 @@ bool SharedFunctionInfo::ShouldFlushCode(
   return bytecode.IsOld();
 }
 
-DEF_GETTER(SharedFunctionInfo, InterpreterTrampoline, CodeT) {
+DEF_GETTER(SharedFunctionInfo, InterpreterTrampoline, CodeDataContainer) {
   DCHECK(HasInterpreterData(cage_base));
   return interpreter_data(cage_base).interpreter_trampoline(cage_base);
 }
 
 DEF_GETTER(SharedFunctionInfo, HasInterpreterData, bool) {
   Object data = function_data(cage_base, kAcquireLoad);
-  if (data.IsCodeT(cage_base)) {
-    CodeT baseline_code = CodeT::cast(data);
+  if (data.IsCodeDataContainer(cage_base)) {
+    CodeDataContainer baseline_code = CodeDataContainer::cast(data);
     DCHECK_EQ(baseline_code.kind(), CodeKind::BASELINE);
     data = baseline_code.bytecode_or_interpreter_data(cage_base);
   }
@@ -662,8 +662,8 @@ DEF_GETTER(SharedFunctionInfo, HasInterpreterData, bool) {
 DEF_GETTER(SharedFunctionInfo, interpreter_data, InterpreterData) {
   DCHECK(HasInterpreterData(cage_base));
   Object data = function_data(cage_base, kAcquireLoad);
-  if (data.IsCodeT(cage_base)) {
-    CodeT baseline_code = CodeT::cast(data);
+  if (data.IsCodeDataContainer(cage_base)) {
+    CodeDataContainer baseline_code = CodeDataContainer::cast(data);
     DCHECK_EQ(baseline_code.kind(), CodeKind::BASELINE);
     data = baseline_code.bytecode_or_interpreter_data(cage_base);
   }
@@ -679,19 +679,19 @@ void SharedFunctionInfo::set_interpreter_data(
 
 DEF_GETTER(SharedFunctionInfo, HasBaselineCode, bool) {
   Object data = function_data(cage_base, kAcquireLoad);
-  if (data.IsCodeT(cage_base)) {
-    DCHECK_EQ(CodeT::cast(data).kind(), CodeKind::BASELINE);
+  if (data.IsCodeDataContainer(cage_base)) {
+    DCHECK_EQ(CodeDataContainer::cast(data).kind(), CodeKind::BASELINE);
     return true;
   }
   return false;
 }
 
-DEF_ACQUIRE_GETTER(SharedFunctionInfo, baseline_code, CodeT) {
+DEF_ACQUIRE_GETTER(SharedFunctionInfo, baseline_code, CodeDataContainer) {
   DCHECK(HasBaselineCode(cage_base));
-  return CodeT::cast(function_data(cage_base, kAcquireLoad));
+  return CodeDataContainer::cast(function_data(cage_base, kAcquireLoad));
 }
 
-void SharedFunctionInfo::set_baseline_code(CodeT baseline_code,
+void SharedFunctionInfo::set_baseline_code(CodeDataContainer baseline_code,
                                            ReleaseStoreTag tag,
                                            WriteBarrierMode mode) {
   DCHECK_EQ(baseline_code.kind(), CodeKind::BASELINE);
diff --git a/src/objects/shared-function-info.cc b/src/objects/shared-function-info.cc
index f68f05e3200..c76a73eb648 100644
--- a/src/objects/shared-function-info.cc
+++ b/src/objects/shared-function-info.cc
@@ -68,7 +68,7 @@ void SharedFunctionInfo::Init(ReadOnlyRoots ro_roots, int unique_id) {
   clear_padding();
 }
 
-CodeT SharedFunctionInfo::GetCode() const {
+CodeDataContainer SharedFunctionInfo::GetCode() const {
   // ======
   // NOTE: This chain of checks MUST be kept in sync with the equivalent CSA
   // GetSharedFunctionInfoCode method in code-stub-assembler.cc.
@@ -86,10 +86,10 @@ CodeT SharedFunctionInfo::GetCode() const {
     DCHECK(HasBytecodeArray());
     return isolate->builtins()->code(Builtin::kInterpreterEntryTrampoline);
   }
-  if (data.IsCodeT()) {
+  if (data.IsCodeDataContainer()) {
     // Having baseline Code means we are a compiled, baseline function.
     DCHECK(HasBaselineCode());
-    return CodeT::cast(data);
+    return CodeDataContainer::cast(data);
   }
 #if V8_ENABLE_WEBASSEMBLY
   if (data.IsAsmWasmData()) {
@@ -128,8 +128,8 @@ CodeT SharedFunctionInfo::GetCode() const {
     return isolate->builtins()->code(Builtin::kHandleApiCall);
   }
   if (data.IsInterpreterData()) {
-    CodeT code = InterpreterTrampoline();
-    DCHECK(code.IsCodeT());
+    CodeDataContainer code = InterpreterTrampoline();
+    DCHECK(code.IsCodeDataContainer());
     DCHECK(code.is_interpreter_trampoline_builtin());
     return code;
   }
diff --git a/src/objects/shared-function-info.h b/src/objects/shared-function-info.h
index b0a73066559..86cd6622bd0 100644
--- a/src/objects/shared-function-info.h
+++ b/src/objects/shared-function-info.h
@@ -205,7 +205,7 @@ class SharedFunctionInfo
   inline void SetName(String name);
 
   // Get the code object which represents the execution of this function.
-  V8_EXPORT_PRIVATE CodeT GetCode() const;
+  V8_EXPORT_PRIVATE CodeDataContainer GetCode() const;
 
   // Get the abstract code associated with the function, which will either be
   // a Code object or a BytecodeArray.
@@ -332,12 +332,12 @@ class SharedFunctionInfo
   inline BytecodeArray GetBytecodeArray(IsolateT* isolate) const;
 
   inline void set_bytecode_array(BytecodeArray bytecode);
-  DECL_GETTER(InterpreterTrampoline, CodeT)
+  DECL_GETTER(InterpreterTrampoline, CodeDataContainer)
   DECL_GETTER(HasInterpreterData, bool)
   DECL_GETTER(interpreter_data, InterpreterData)
   inline void set_interpreter_data(InterpreterData interpreter_data);
   DECL_GETTER(HasBaselineCode, bool)
-  DECL_RELEASE_ACQUIRE_ACCESSORS(baseline_code, CodeT)
+  DECL_RELEASE_ACQUIRE_ACCESSORS(baseline_code, CodeDataContainer)
   inline void FlushBaselineCode();
   inline BytecodeArray GetActiveBytecodeArray() const;
   inline void SetActiveBytecodeArray(BytecodeArray bytecode);
diff --git a/src/profiler/cpu-profiler.cc b/src/profiler/cpu-profiler.cc
index f8641063a88..f03f7ef5fc9 100644
--- a/src/profiler/cpu-profiler.cc
+++ b/src/profiler/cpu-profiler.cc
@@ -415,7 +415,7 @@ void ProfilerCodeObserver::LogBuiltins() {
        ++builtin) {
     CodeEventsContainer evt_rec(CodeEventRecord::Type::kReportBuiltin);
     ReportBuiltinEventRecord* rec = &evt_rec.ReportBuiltinEventRecord_;
-    CodeT code = builtins->code(builtin);
+    CodeDataContainer code = builtins->code(builtin);
     rec->instruction_start = code.InstructionStart();
     rec->instruction_size = code.InstructionSize();
     rec->builtin = builtin;
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index 9d3dcf1297e..8545cd02eff 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -1468,10 +1468,11 @@ void V8HeapExplorer::ExtractMapReferences(HeapEntry* entry, Map map) {
 void V8HeapExplorer::ExtractSharedFunctionInfoReferences(
     HeapEntry* entry, SharedFunctionInfo shared) {
   std::unique_ptr<char[]> name = shared.DebugNameCStr();
-  CodeT code = shared.GetCode();
+  CodeDataContainer code = shared.GetCode();
   // Don't try to get the Code object from Code-less embedded builtin.
-  HeapObject maybe_code_obj =
-      code.is_off_heap_trampoline() ? HeapObject::cast(code) : FromCodeT(code);
+  HeapObject maybe_code_obj = code.is_off_heap_trampoline()
+                                  ? HeapObject::cast(code)
+                                  : FromCodeDataContainer(code);
   if (name[0] != '\0') {
     TagObject(maybe_code_obj,
               names_->GetFormatted("(code for %s)", name.get()));
@@ -1542,10 +1543,12 @@ void V8HeapExplorer::ExtractWeakCellReferences(HeapEntry* entry,
                    WeakCell::kUnregisterTokenOffset);
 }
 
-void V8HeapExplorer::TagBuiltinCodeObject(CodeT code, const char* name) {
+void V8HeapExplorer::TagBuiltinCodeObject(CodeDataContainer code,
+                                          const char* name) {
   TagObject(code, names_->GetFormatted("(%s builtin handle)", name));
   if (!code.is_off_heap_trampoline()) {
-    TagObject(FromCodeT(code), names_->GetFormatted("(%s builtin)", name));
+    TagObject(FromCodeDataContainer(code),
+              names_->GetFormatted("(%s builtin)", name));
   }
 }
 
@@ -1972,7 +1975,8 @@ class RootsReferencesExtractor : public RootVisitor {
   void VisitRootPointer(Root root, const char* description,
                         FullObjectSlot object) override {
     if (root == Root::kBuiltins) {
-      explorer_->TagBuiltinCodeObject(CodeT::cast(*object), description);
+      explorer_->TagBuiltinCodeObject(CodeDataContainer::cast(*object),
+                                      description);
     }
     explorer_->SetGcSubrootReference(root, description, visiting_weak_roots_,
                                      *object);
@@ -2003,10 +2007,10 @@ class RootsReferencesExtractor : public RootVisitor {
     // deoptimization literals in running code as stack roots.
     HeapObject value = HeapObject::cast(*p);
     if (!IsCodeSpaceObject(value)) {
-      // When external code space is enabled, the slot might contain a CodeT
-      // object representing an embedded builtin, which doesn't require
-      // additional processing.
-      DCHECK(CodeT::cast(value).is_off_heap_trampoline());
+      // When external code space is enabled, the slot might contain a
+      // CodeDataContainer object representing an embedded builtin, which
+      // doesn't require additional processing.
+      DCHECK(CodeDataContainer::cast(value).is_off_heap_trampoline());
     } else {
       Code code = Code::cast(value);
       if (code.kind() != CodeKind::BASELINE) {
diff --git a/src/profiler/heap-snapshot-generator.h b/src/profiler/heap-snapshot-generator.h
index 1ead386db21..7d70ebf70e6 100644
--- a/src/profiler/heap-snapshot-generator.h
+++ b/src/profiler/heap-snapshot-generator.h
@@ -391,7 +391,7 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   bool IterateAndExtractReferences(HeapSnapshotGenerator* generator);
   void CollectGlobalObjectsTags();
   void MakeGlobalObjectTagMap(const IsolateSafepointScope& safepoint_scope);
-  void TagBuiltinCodeObject(CodeT code, const char* name);
+  void TagBuiltinCodeObject(CodeDataContainer code, const char* name);
   HeapEntry* AddEntry(Address address,
                       HeapEntry::Type type,
                       const char* name,
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index 672b1785dfd..038834da780 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -431,7 +431,7 @@ int NativeRegExpMacroAssembler::Execute(
   RegExpStackScope stack_scope(isolate);
 
   bool is_one_byte = String::IsOneByteRepresentationUnderneath(input);
-  CodeT code = CodeT::cast(regexp.code(is_one_byte));
+  CodeDataContainer code = CodeDataContainer::cast(regexp.code(is_one_byte));
   RegExp::CallOrigin call_origin = RegExp::CallOrigin::kFromRuntime;
 
   using RegexpMatcherSig =
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index ce323df96cd..1c8759b2a6e 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -99,7 +99,7 @@ class RegExpImpl final : public AllStatic {
   static void SetIrregexpMaxRegisterCount(FixedArray re, int value);
   static int IrregexpNumberOfCaptures(FixedArray re);
   static ByteArray IrregexpByteCode(FixedArray re, bool is_one_byte);
-  static CodeT IrregexpNativeCode(FixedArray re, bool is_one_byte);
+  static CodeDataContainer IrregexpNativeCode(FixedArray re, bool is_one_byte);
 };
 
 // static
@@ -450,7 +450,7 @@ bool RegExpImpl::EnsureCompiledIrregexp(Isolate* isolate, Handle<JSRegExp> re,
   }
 
   if (!needs_initial_compilation && !needs_tier_up_compilation) {
-    DCHECK(compiled_code.IsCodeT());
+    DCHECK(compiled_code.IsCodeDataContainer());
     DCHECK_IMPLIES(v8_flags.regexp_interpret_all, bytecode.IsByteArray());
     return true;
   }
@@ -483,7 +483,8 @@ bool RegExpCodeIsValidForPreCompilation(Handle<JSRegExp> re, bool is_one_byte) {
     DCHECK_EQ(JSRegExp::kUninitializedValue, entry_value);
     DCHECK_EQ(JSRegExp::kUninitializedValue, bytecode_value);
   } else {
-    DCHECK(entry.IsSmi() || (entry.IsCodeT() && bytecode.IsByteArray()));
+    DCHECK(entry.IsSmi() ||
+           (entry.IsCodeDataContainer() && bytecode.IsByteArray()));
   }
 
   return true;
@@ -576,7 +577,7 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
       Handle<FixedArray>(FixedArray::cast(re->data()), isolate);
   if (compile_data.compilation_target == RegExpCompilationTarget::kNative) {
     Code code = Code::cast(*compile_data.code);
-    data->set(JSRegExp::code_index(is_one_byte), ToCodeT(code));
+    data->set(JSRegExp::code_index(is_one_byte), ToCodeDataContainer(code));
 
     // Reset bytecode to uninitialized. In case we use tier-up we know that
     // tier-up has happened this way.
@@ -588,7 +589,7 @@ bool RegExpImpl::CompileIrregexp(Isolate* isolate, Handle<JSRegExp> re,
     // Store code generated by compiler in bytecode and trampoline to
     // interpreter in code.
     data->set(JSRegExp::bytecode_index(is_one_byte), *compile_data.code);
-    Handle<CodeT> trampoline =
+    Handle<CodeDataContainer> trampoline =
         BUILTIN_CODE(isolate, RegExpInterpreterTrampoline);
     data->set(JSRegExp::code_index(is_one_byte), *trampoline);
   }
@@ -629,8 +630,9 @@ ByteArray RegExpImpl::IrregexpByteCode(FixedArray re, bool is_one_byte) {
   return ByteArray::cast(re.get(JSRegExp::bytecode_index(is_one_byte)));
 }
 
-CodeT RegExpImpl::IrregexpNativeCode(FixedArray re, bool is_one_byte) {
-  return CodeT::cast(re.get(JSRegExp::code_index(is_one_byte)));
+CodeDataContainer RegExpImpl::IrregexpNativeCode(FixedArray re,
+                                                 bool is_one_byte) {
+  return CodeDataContainer::cast(re.get(JSRegExp::code_index(is_one_byte)));
 }
 
 void RegExpImpl::IrregexpInitialize(Isolate* isolate, Handle<JSRegExp> re,
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index 98e7733995f..fa4454533ee 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -85,7 +85,7 @@ RUNTIME_FUNCTION(Runtime_InstallBaselineCode) {
                                             &is_compiled_scope);
   {
     DisallowGarbageCollection no_gc;
-    CodeT baseline_code = sfi->baseline_code(kAcquireLoad);
+    CodeDataContainer baseline_code = sfi->baseline_code(kAcquireLoad);
     function->set_code(baseline_code);
     if V8_LIKELY (!v8_flags.log_function_events) return baseline_code;
   }
@@ -250,8 +250,8 @@ bool DeoptExitIsInsideOsrLoop(Isolate* isolate, JSFunction function,
 
 bool TryGetOptimizedOsrCode(Isolate* isolate, FeedbackVector vector,
                             const interpreter::BytecodeArrayIterator& it,
-                            CodeT* code_out) {
-  base::Optional<CodeT> maybe_code =
+                            CodeDataContainer* code_out) {
+  base::Optional<CodeDataContainer> maybe_code =
       vector.GetOptimizedOsrCode(isolate, it.GetSlotOperand(2));
   if (maybe_code.has_value()) {
     *code_out = maybe_code.value();
@@ -289,8 +289,8 @@ void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate, JSFunction function,
                                         deopt_exit_offset.ToInt());
 
   FeedbackVector vector = function.feedback_vector();
-  CodeT code;
-  base::SmallVector<CodeT, 8> osr_codes;
+  CodeDataContainer code;
+  base::SmallVector<CodeDataContainer, 8> osr_codes;
   // Visit before the first loop-with-deopt is found
   for (; !it.done(); it.Advance()) {
     // We're only interested in loop ranges.
@@ -414,11 +414,13 @@ RUNTIME_FUNCTION(Runtime_NotifyDeoptimized) {
   // the loop should pay for the deoptimization costs.
   const BytecodeOffset osr_offset = optimized_code->osr_offset();
   if (osr_offset.IsNone()) {
-    Deoptimizer::DeoptimizeFunction(*function, ToCodeT(*optimized_code));
+    Deoptimizer::DeoptimizeFunction(*function,
+                                    ToCodeDataContainer(*optimized_code));
     DeoptAllOsrLoopsContainingDeoptExit(isolate, *function, deopt_exit_offset);
   } else if (DeoptExitIsInsideOsrLoop(isolate, *function, deopt_exit_offset,
                                       osr_offset)) {
-    Deoptimizer::DeoptimizeFunction(*function, ToCodeT(*optimized_code));
+    Deoptimizer::DeoptimizeFunction(*function,
+                                    ToCodeDataContainer(*optimized_code));
   }
 
   return ReadOnlyRoots(isolate).undefined_value();
@@ -459,10 +461,11 @@ void GetOsrOffsetAndFunctionForOSR(Isolate* isolate, BytecodeOffset* osr_offset,
   // Determine the frame that triggered the OSR request.
   JavaScriptFrameIterator it(isolate);
   UnoptimizedFrame* frame = UnoptimizedFrame::cast(it.frame());
-  DCHECK_IMPLIES(frame->is_interpreted(),
-                 frame->LookupCodeT().is_interpreter_trampoline_builtin());
+  DCHECK_IMPLIES(
+      frame->is_interpreted(),
+      frame->LookupCodeDataContainer().is_interpreter_trampoline_builtin());
   DCHECK_IMPLIES(frame->is_baseline(),
-                 frame->LookupCodeT().kind() == CodeKind::BASELINE);
+                 frame->LookupCodeDataContainer().kind() == CodeKind::BASELINE);
 
   *osr_offset = BytecodeOffset(frame->GetBytecodeOffset());
   *function = handle(frame->function(), isolate);
@@ -479,7 +482,7 @@ Object CompileOptimizedOSR(Isolate* isolate, Handle<JSFunction> function,
           ? ConcurrencyMode::kConcurrent
           : ConcurrencyMode::kSynchronous;
 
-  Handle<CodeT> result;
+  Handle<CodeDataContainer> result;
   if (!Compiler::CompileOptimizedOSR(isolate, function, osr_offset, mode)
            .ToHandle(&result)) {
     // An empty result can mean one of two things:
@@ -545,7 +548,7 @@ RUNTIME_FUNCTION(Runtime_CompileOptimizedOSRFromMaglev) {
 
   JavaScriptFrameIterator it(isolate);
   MaglevFrame* frame = MaglevFrame::cast(it.frame());
-  DCHECK_EQ(frame->LookupCodeT().kind(), CodeKind::MAGLEV);
+  DCHECK_EQ(frame->LookupCodeDataContainer().kind(), CodeKind::MAGLEV);
   Handle<JSFunction> function = handle(frame->function(), isolate);
 
   // This path is only relevant for tests (all production configurations enable
diff --git a/src/runtime/runtime-test-wasm.cc b/src/runtime/runtime-test-wasm.cc
index e6044fbf553..5a07f257eae 100644
--- a/src/runtime/runtime-test-wasm.cc
+++ b/src/runtime/runtime-test-wasm.cc
@@ -265,7 +265,7 @@ RUNTIME_FUNCTION(Runtime_IsWasmCode) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
   auto function = JSFunction::cast(args[0]);
-  CodeT code = function.code();
+  CodeDataContainer code = function.code();
   bool is_js_to_wasm = code.kind() == CodeKind::JS_TO_WASM_FUNCTION ||
                        (code.builtin_id() == Builtin::kGenericJSToWasmWrapper);
   return isolate->heap()->ToBoolean(is_js_to_wasm);
diff --git a/src/runtime/runtime-test.cc b/src/runtime/runtime-test.cc
index e33c7c38972..8c5d47d4a44 100644
--- a/src/runtime/runtime-test.cc
+++ b/src/runtime/runtime-test.cc
@@ -326,11 +326,12 @@ Object OptimizeFunctionOnNextCall(RuntimeArguments& args, Isolate* isolate,
   // function has.
   if (!function->is_compiled()) {
     DCHECK(function->shared().HasBytecodeArray());
-    CodeT codet = *BUILTIN_CODE(isolate, InterpreterEntryTrampoline);
+    CodeDataContainer code_data_container =
+        *BUILTIN_CODE(isolate, InterpreterEntryTrampoline);
     if (function->shared().HasBaselineCode()) {
-      codet = function->shared().baseline_code(kAcquireLoad);
+      code_data_container = function->shared().baseline_code(kAcquireLoad);
     }
-    function->set_code(codet);
+    function->set_code(code_data_container);
   }
 
   TraceManualRecompile(*function, target_kind, concurrency_mode);
@@ -404,10 +405,10 @@ RUNTIME_FUNCTION(Runtime_BenchMaglev) {
   Handle<JSFunction> function = args.at<JSFunction>(0);
   int count = args.smi_value_at(1);
 
-  Handle<CodeT> codet;
+  Handle<CodeDataContainer> code_data_container;
   base::ElapsedTimer timer;
   timer.Start();
-  codet = Maglev::Compile(isolate, function).ToHandleChecked();
+  code_data_container = Maglev::Compile(isolate, function).ToHandleChecked();
   for (int i = 1; i < count; ++i) {
     HandleScope handle_scope(isolate);
     Maglev::Compile(isolate, function);
@@ -415,7 +416,7 @@ RUNTIME_FUNCTION(Runtime_BenchMaglev) {
   PrintF("Maglev compile time: %g ms!\n",
          timer.Elapsed().InMillisecondsF() / count);
 
-  function->set_code(*codet);
+  function->set_code(*code_data_container);
 
   return ReadOnlyRoots(isolate).undefined_value();
 }
@@ -788,7 +789,7 @@ RUNTIME_FUNCTION(Runtime_GetOptimizationStatus) {
   }
 
   if (function->HasAttachedOptimizedCode()) {
-    CodeT code = function->code();
+    CodeDataContainer code = function->code();
     if (code.marked_for_deoptimization()) {
       status |= static_cast<int>(OptimizationStatus::kMarkedForDeoptimization);
     } else {
@@ -1436,7 +1437,7 @@ RUNTIME_FUNCTION(Runtime_RegexpHasNativeCode) {
   bool is_latin1 = Oddball::cast(args[1]).ToBool(isolate);
   bool result;
   if (regexp.type_tag() == JSRegExp::IRREGEXP) {
-    result = regexp.code(is_latin1).IsCodeT();
+    result = regexp.code(is_latin1).IsCodeDataContainer();
   } else {
     result = false;
   }
diff --git a/src/runtime/runtime-wasm.cc b/src/runtime/runtime-wasm.cc
index 66ea521b45f..449f180f003 100644
--- a/src/runtime/runtime-wasm.cc
+++ b/src/runtime/runtime-wasm.cc
@@ -288,7 +288,8 @@ RUNTIME_FUNCTION(Runtime_WasmAllocateFeedbackVector) {
 
 namespace {
 void ReplaceWrapper(Isolate* isolate, Handle<WasmInstanceObject> instance,
-                    int function_index, Handle<CodeT> wrapper_code) {
+                    int function_index,
+                    Handle<CodeDataContainer> wrapper_code) {
   Handle<WasmInternalFunction> internal =
       WasmInstanceObject::GetWasmInternalFunction(isolate, instance,
                                                   function_index)
@@ -330,7 +331,7 @@ RUNTIME_FUNCTION(Runtime_WasmCompileWrapper) {
     return ReadOnlyRoots(isolate).undefined_value();
   }
 
-  Handle<CodeT> wrapper_code =
+  Handle<CodeDataContainer> wrapper_code =
       wasm::JSToWasmWrapperCompilationUnit::CompileSpecificJSToWasmWrapper(
           isolate, sig, canonical_sig_index, module);
 
diff --git a/src/snapshot/code-serializer.cc b/src/snapshot/code-serializer.cc
index 05eb3595f7f..20f54e1147b 100644
--- a/src/snapshot/code-serializer.cc
+++ b/src/snapshot/code-serializer.cc
@@ -275,9 +275,9 @@ void CreateInterpreterDataForDeserializedCode(Isolate* isolate,
             INTERPRETER_DATA_TYPE, AllocationType::kOld));
 
     interpreter_data->set_bytecode_array(info->GetBytecodeArray(isolate));
-    interpreter_data->set_interpreter_trampoline(ToCodeT(*code));
+    interpreter_data->set_interpreter_trampoline(ToCodeDataContainer(*code));
     if (info->HasBaselineCode()) {
-      FromCodeT(info->baseline_code(kAcquireLoad))
+      FromCodeDataContainer(info->baseline_code(kAcquireLoad))
           .set_bytecode_or_interpreter_data(*interpreter_data);
     } else {
       info->set_interpreter_data(*interpreter_data);
diff --git a/src/snapshot/embedded/embedded-data.cc b/src/snapshot/embedded/embedded-data.cc
index f7c89443298..175c6ea7e7b 100644
--- a/src/snapshot/embedded/embedded-data.cc
+++ b/src/snapshot/embedded/embedded-data.cc
@@ -219,7 +219,7 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = FromCodeT(isolate->builtins()->code(builtin));
+    Code code = FromCodeDataContainer(isolate->builtins()->code(builtin));
     RelocIterator on_heap_it(code, kRelocMask);
     RelocIterator off_heap_it(blob, code, kRelocMask);
 
@@ -257,8 +257,8 @@ void FinalizeEmbeddedCodeTargets(Isolate* isolate, EmbeddedData* blob) {
   }
 }
 
-void EnsureRelocatable(CodeT codet) {
-  Code code = FromCodeT(codet);
+void EnsureRelocatable(CodeDataContainer code_data_container) {
+  Code code = FromCodeDataContainer(code_data_container);
   if (code.relocation_size() == 0) return;
 
   // On some architectures (arm) the builtin might have a non-empty reloc
@@ -286,7 +286,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = FromCodeT(builtins->code(builtin));
+    Code code = FromCodeDataContainer(builtins->code(builtin));
 
     // Sanity-check that the given builtin is isolate-independent and does not
     // use the trampoline register in its calling convention.
@@ -371,7 +371,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = FromCodeT(builtins->code(builtin));
+    Code code = FromCodeDataContainer(builtins->code(builtin));
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].metadata_offset;
     uint8_t* dst = raw_metadata_start + offset;
@@ -389,7 +389,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   static_assert(Builtins::kAllBuiltinsAreIsolateIndependent);
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = FromCodeT(builtins->code(builtin));
+    Code code = FromCodeDataContainer(builtins->code(builtin));
     uint32_t offset =
         layout_descriptions[static_cast<int>(builtin)].instruction_offset;
     uint8_t* dst = raw_code_start + offset;
@@ -425,7 +425,7 @@ EmbeddedData EmbeddedData::FromIsolate(Isolate* isolate) {
   if (DEBUG_BOOL) {
     for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
          ++builtin) {
-      Code code = FromCodeT(builtins->code(builtin));
+      Code code = FromCodeDataContainer(builtins->code(builtin));
 
       CHECK_EQ(d.InstructionSizeOfBuiltin(builtin), code.InstructionSize());
       CHECK_EQ(d.MetadataSizeOfBuiltin(builtin), code.MetadataSize());
diff --git a/src/snapshot/embedded/embedded-file-writer.cc b/src/snapshot/embedded/embedded-file-writer.cc
index 66cb66234d0..2d2a4922063 100644
--- a/src/snapshot/embedded/embedded-file-writer.cc
+++ b/src/snapshot/embedded/embedded-file-writer.cc
@@ -269,7 +269,7 @@ void EmbeddedFileWriter::PrepareBuiltinSourcePositionMap(Builtins* builtins) {
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
     // Retrieve the SourcePositionTable and copy it.
-    Code code = FromCodeT(builtins->code(builtin));
+    Code code = FromCodeDataContainer(builtins->code(builtin));
     // Verify that the code object is still the "real code" and not a
     // trampoline (which wouldn't have source positions).
     DCHECK(!code.is_off_heap_trampoline());
diff --git a/src/snapshot/serializer-inl.h b/src/snapshot/serializer-inl.h
index 681d90d60b2..29d78103bb4 100644
--- a/src/snapshot/serializer-inl.h
+++ b/src/snapshot/serializer-inl.h
@@ -20,7 +20,7 @@ bool Serializer::IsNotMappedSymbol(HeapObject obj) const {
     // Code references anywhere except the CodeDadaContainer objects.
     // In particular, the Code objects should not appear in serializer's
     // identity map. This should be possible once the IsolateData::builtins
-    // table is migrated to contain CodeT references.
+    // table is migrated to contain CodeDataContainer references.
     return obj.ptr() == not_mapped_symbol.ptr();
   }
   return obj == not_mapped_symbol;
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index 08c9e704cc5..a3448986a28 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -130,8 +130,8 @@ void Serializer::SerializeObject(Handle<HeapObject> obj) {
   // indirection and serialize the actual string directly.
   if (obj->IsThinString(isolate())) {
     obj = handle(ThinString::cast(*obj).actual(isolate()), isolate());
-  } else if (obj->IsCodeT(isolate())) {
-    CodeT code = CodeT::cast(*obj);
+  } else if (obj->IsCodeDataContainer(isolate())) {
+    CodeDataContainer code = CodeDataContainer::cast(*obj);
     if (code.kind() == CodeKind::BASELINE) {
       // For now just serialize the BytecodeArray instead of baseline code.
       // TODO(v8:11429,pthier): Handle Baseline code in cases we want to
diff --git a/src/wasm/c-api.cc b/src/wasm/c-api.cc
index 67f1f1d80ab..768a3f1d06e 100644
--- a/src/wasm/c-api.cc
+++ b/src/wasm/c-api.cc
@@ -1543,7 +1543,7 @@ void PrepareFunctionData(i::Isolate* isolate,
     return;
   }
   // Compile wrapper code.
-  i::Handle<i::CodeT> wrapper_code =
+  i::Handle<i::CodeDataContainer> wrapper_code =
       i::compiler::CompileCWasmEntry(isolate, sig, module);
   function_data->set_c_wrapper_code(*wrapper_code);
   // Compute packed args size.
@@ -1680,7 +1680,8 @@ auto Func::call(const Val args[], Val results[]) const -> own<Trap> {
   const i::wasm::FunctionSig* sig =
       instance->module()->functions[function_index].sig;
   PrepareFunctionData(isolate, function_data, sig, instance->module());
-  i::Handle<i::CodeT> wrapper_code(function_data->c_wrapper_code(), isolate);
+  i::Handle<i::CodeDataContainer> wrapper_code(function_data->c_wrapper_code(),
+                                               isolate);
   i::Address call_target = function_data->internal().call_target(isolate);
 
   i::wasm::CWasmArgumentsPacker packer(function_data->packed_args_size());
diff --git a/src/wasm/function-compiler.cc b/src/wasm/function-compiler.cc
index 42039d75e2d..9eec0228666 100644
--- a/src/wasm/function-compiler.cc
+++ b/src/wasm/function-compiler.cc
@@ -246,7 +246,7 @@ void JSToWasmWrapperCompilationUnit::Execute() {
   }
 }
 
-Handle<CodeT> JSToWasmWrapperCompilationUnit::Finalize() {
+Handle<CodeDataContainer> JSToWasmWrapperCompilationUnit::Finalize() {
   if (use_generic_wrapper_) {
     return isolate_->builtins()->code_handle(Builtin::kGenericJSToWasmWrapper);
   }
@@ -261,11 +261,12 @@ Handle<CodeT> JSToWasmWrapperCompilationUnit::Finalize() {
     PROFILE(isolate_, CodeCreateEvent(LogEventListener::CodeTag::kStub,
                                       Handle<AbstractCode>::cast(code), name));
   }
-  return ToCodeT(code, isolate_);
+  return ToCodeDataContainer(code, isolate_);
 }
 
 // static
-Handle<CodeT> JSToWasmWrapperCompilationUnit::CompileJSToWasmWrapper(
+Handle<CodeDataContainer>
+JSToWasmWrapperCompilationUnit::CompileJSToWasmWrapper(
     Isolate* isolate, const FunctionSig* sig, uint32_t canonical_sig_index,
     const WasmModule* module, bool is_import) {
   // Run the compilation unit synchronously.
@@ -278,7 +279,8 @@ Handle<CodeT> JSToWasmWrapperCompilationUnit::CompileJSToWasmWrapper(
 }
 
 // static
-Handle<CodeT> JSToWasmWrapperCompilationUnit::CompileSpecificJSToWasmWrapper(
+Handle<CodeDataContainer>
+JSToWasmWrapperCompilationUnit::CompileSpecificJSToWasmWrapper(
     Isolate* isolate, const FunctionSig* sig, uint32_t canonical_sig_index,
     const WasmModule* module) {
   // Run the compilation unit synchronously.
diff --git a/src/wasm/function-compiler.h b/src/wasm/function-compiler.h
index a10927ecf20..0a5d89e8b4a 100644
--- a/src/wasm/function-compiler.h
+++ b/src/wasm/function-compiler.h
@@ -115,22 +115,20 @@ class V8_EXPORT_PRIVATE JSToWasmWrapperCompilationUnit final {
   Isolate* isolate() const { return isolate_; }
 
   void Execute();
-  Handle<CodeT> Finalize();
+  Handle<CodeDataContainer> Finalize();
 
   bool is_import() const { return is_import_; }
   const FunctionSig* sig() const { return sig_; }
   uint32_t canonical_sig_index() const { return canonical_sig_index_; }
 
   // Run a compilation unit synchronously.
-  static Handle<CodeT> CompileJSToWasmWrapper(Isolate* isolate,
-                                              const FunctionSig* sig,
-                                              uint32_t canonical_sig_index,
-                                              const WasmModule* module,
-                                              bool is_import);
+  static Handle<CodeDataContainer> CompileJSToWasmWrapper(
+      Isolate* isolate, const FunctionSig* sig, uint32_t canonical_sig_index,
+      const WasmModule* module, bool is_import);
 
   // Run a compilation unit synchronously, but ask for the specific
   // wrapper.
-  static Handle<CodeT> CompileSpecificJSToWasmWrapper(
+  static Handle<CodeDataContainer> CompileSpecificJSToWasmWrapper(
       Isolate* isolate, const FunctionSig* sig, uint32_t canonical_sig_index,
       const WasmModule* module);
 
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index 1456d4955e3..b4c801310e9 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1421,9 +1421,9 @@ void TierUpNowForTesting(Isolate* isolate, WasmInstanceObject instance,
 
 namespace {
 
-void RecordStats(CodeT codet, Counters* counters) {
-  if (codet.is_off_heap_trampoline()) return;
-  Code code = FromCodeT(codet);
+void RecordStats(CodeDataContainer code_data_container, Counters* counters) {
+  if (code_data_container.is_off_heap_trampoline()) return;
+  Code code = FromCodeDataContainer(code_data_container);
   counters->wasm_generated_code_size()->Increment(code.raw_body_size());
   counters->wasm_reloc_size()->Increment(code.relocation_info().length());
 }
@@ -3338,7 +3338,7 @@ void CompilationStateImpl::FinalizeJSToWasmWrappers(Isolate* isolate,
   CodePageCollectionMemoryModificationScope modification_scope(isolate->heap());
   for (auto& unit : js_to_wasm_wrapper_units_) {
     DCHECK_EQ(isolate, unit->isolate());
-    Handle<CodeT> code = unit->Finalize();
+    Handle<CodeDataContainer> code = unit->Finalize();
     uint32_t index =
         GetExportWrapperIndex(unit->canonical_sig_index(), unit->is_import());
     isolate->heap()->js_to_wasm_wrappers().Set(index,
@@ -3800,7 +3800,7 @@ void CompileJsToWasmWrappers(Isolate* isolate, const WasmModule* module) {
     JSToWasmWrapperKey key = pair.first;
     JSToWasmWrapperCompilationUnit* unit = pair.second.get();
     DCHECK_EQ(isolate, unit->isolate());
-    Handle<CodeT> code = unit->Finalize();
+    Handle<CodeDataContainer> code = unit->Finalize();
     int wrapper_index = GetExportWrapperIndex(key.second, key.first);
     isolate->heap()->js_to_wasm_wrappers().Set(
         wrapper_index, HeapObjectReference::Strong(*code));
diff --git a/src/wasm/module-instantiate.cc b/src/wasm/module-instantiate.cc
index 832bc656ae9..205e1bbd205 100644
--- a/src/wasm/module-instantiate.cc
+++ b/src/wasm/module-instantiate.cc
@@ -814,7 +814,7 @@ MaybeHandle<WasmInstanceObject> InstanceBuilder::Build() {
     uint32_t canonical_sig_index =
         module_->isorecursive_canonical_type_ids[module_->functions[start_index]
                                                      .sig_index];
-    Handle<CodeT> wrapper_code =
+    Handle<CodeDataContainer> wrapper_code =
         JSToWasmWrapperCompilationUnit::CompileJSToWasmWrapper(
             isolate_, function.sig, canonical_sig_index, module_,
             function.imported);
diff --git a/src/wasm/wasm-js.cc b/src/wasm/wasm-js.cc
index e99b71b7046..7273b647f47 100644
--- a/src/wasm/wasm-js.cc
+++ b/src/wasm/wasm-js.cc
@@ -2102,7 +2102,7 @@ void WebAssemblyFunction(const v8::FunctionCallbackInfo<v8::Value>& args) {
     i::Handle<i::WasmInstanceObject> instance(
         i::WasmInstanceObject::cast(data.internal().ref()), i_isolate);
     int func_index = data.function_index();
-    i::Handle<i::CodeT> wrapper =
+    i::Handle<i::CodeDataContainer> wrapper =
         BUILTIN_CODE(i_isolate, WasmReturnPromiseOnSuspend);
     i::Handle<i::JSFunction> result = i::WasmExportedFunction::New(
         i_isolate, instance, func_index,
diff --git a/src/wasm/wasm-objects.cc b/src/wasm/wasm-objects.cc
index a0047876949..6c96bc97f41 100644
--- a/src/wasm/wasm-objects.cc
+++ b/src/wasm/wasm-objects.cc
@@ -1358,10 +1358,10 @@ WasmInstanceObject::GetOrCreateWasmInternalFunction(
 
   MaybeObject entry = isolate->heap()->js_to_wasm_wrappers().Get(wrapper_index);
 
-  Handle<CodeT> wrapper;
-  // {entry} can be cleared, {undefined}, or a ready {CodeT}.
-  if (entry.IsStrongOrWeak() && entry.GetHeapObject().IsCodeT()) {
-    wrapper = handle(CodeT::cast(entry.GetHeapObject()), isolate);
+  Handle<CodeDataContainer> wrapper;
+  // {entry} can be cleared, {undefined}, or a ready {CodeDataContainer}.
+  if (entry.IsStrongOrWeak() && entry.GetHeapObject().IsCodeDataContainer()) {
+    wrapper = handle(CodeDataContainer::cast(entry.GetHeapObject()), isolate);
   } else {
     // The wrapper may not exist yet if no function in the exports section has
     // this signature. We compile it and store the wrapper in the module for
@@ -1857,7 +1857,7 @@ uint32_t WasmExceptionPackage::GetEncodedSize(const wasm::WasmTagSig* sig) {
 bool WasmExportedFunction::IsWasmExportedFunction(Object object) {
   if (!object.IsJSFunction()) return false;
   JSFunction js_function = JSFunction::cast(object);
-  CodeT code = js_function.code();
+  CodeDataContainer code = js_function.code();
   if (CodeKind::JS_TO_WASM_FUNCTION != code.kind() &&
       code.builtin_id() != Builtin::kGenericJSToWasmWrapper &&
       code.builtin_id() != Builtin::kWasmReturnPromiseOnSuspend) {
@@ -1917,7 +1917,7 @@ int WasmExportedFunction::function_index() {
 
 Handle<WasmExportedFunction> WasmExportedFunction::New(
     Isolate* isolate, Handle<WasmInstanceObject> instance, int func_index,
-    int arity, Handle<CodeT> export_wrapper) {
+    int arity, Handle<CodeDataContainer> export_wrapper) {
   DCHECK(
       CodeKind::JS_TO_WASM_FUNCTION == export_wrapper->kind() ||
       (export_wrapper->is_builtin() &&
@@ -2053,7 +2053,7 @@ Handle<WasmJSFunction> WasmJSFunction::New(Isolate* isolate,
   }
   // TODO(wasm): Think about caching and sharing the JS-to-JS wrappers per
   // signature instead of compiling a new one for every instantiation.
-  Handle<CodeT> wrapper_code = ToCodeT(
+  Handle<CodeDataContainer> wrapper_code = ToCodeDataContainer(
       compiler::CompileJSToJSWrapper(isolate, sig, nullptr).ToHandleChecked(),
       isolate);
 
@@ -2087,11 +2087,11 @@ Handle<WasmJSFunction> WasmJSFunction::New(Isolate* isolate,
     }
     // TODO(wasm): Think about caching and sharing the wasm-to-JS wrappers per
     // signature instead of compiling a new one for every instantiation.
-    Handle<CodeT> wasm_to_js_wrapper_code =
-        ToCodeT(compiler::CompileWasmToJSWrapper(isolate, sig, kind,
-                                                 expected_arity, suspend)
-                    .ToHandleChecked(),
-                isolate);
+    Handle<CodeDataContainer> wasm_to_js_wrapper_code =
+        ToCodeDataContainer(compiler::CompileWasmToJSWrapper(
+                                isolate, sig, kind, expected_arity, suspend)
+                                .ToHandleChecked(),
+                            isolate);
     function_data->internal().set_code(*wasm_to_js_wrapper_code);
   }
 
diff --git a/src/wasm/wasm-objects.h b/src/wasm/wasm-objects.h
index 0a884de1225..da517789452 100644
--- a/src/wasm/wasm-objects.h
+++ b/src/wasm/wasm-objects.h
@@ -615,7 +615,7 @@ class WasmExportedFunction : public JSFunction {
 
   V8_EXPORT_PRIVATE static Handle<WasmExportedFunction> New(
       Isolate* isolate, Handle<WasmInstanceObject> instance, int func_index,
-      int arity, Handle<CodeT> export_wrapper);
+      int arity, Handle<CodeDataContainer> export_wrapper);
 
   Address GetWasmCallTarget();
 
@@ -780,7 +780,7 @@ class WasmJSFunctionData
     : public TorqueGeneratedWasmJSFunctionData<WasmJSFunctionData,
                                                WasmFunctionData> {
  public:
-  DECL_ACCESSORS(wasm_to_js_wrapper_code, CodeT)
+  DECL_ACCESSORS(wasm_to_js_wrapper_code, CodeDataContainer)
 
   // Dispatched behavior.
   DECL_PRINTER(WasmJSFunctionData)
diff --git a/test/cctest/cctest.cc b/test/cctest/cctest.cc
index 4a149849ac4..c6c99ae1dc3 100644
--- a/test/cctest/cctest.cc
+++ b/test/cctest/cctest.cc
@@ -336,7 +336,7 @@ i::Handle<i::JSFunction> Optimize(
   CHECK(info.shared_info()->HasBytecodeArray());
   i::JSFunction::EnsureFeedbackVector(isolate, function, &is_compiled_scope);
 
-  i::Handle<i::CodeT> code = i::ToCodeT(
+  i::Handle<i::CodeDataContainer> code = i::ToCodeDataContainer(
       i::compiler::Pipeline::GenerateCodeForTesting(&info, isolate, out_broker)
           .ToHandleChecked(),
       isolate);
diff --git a/test/cctest/compiler/codegen-tester.h b/test/cctest/compiler/codegen-tester.h
index 64637cfea2c..12972096a2c 100644
--- a/test/cctest/compiler/codegen-tester.h
+++ b/test/cctest/compiler/codegen-tester.h
@@ -77,7 +77,9 @@ class RawMachineAssemblerTester : public HandleAndZoneScope,
     return code_.ToHandleChecked();
   }
 
-  Handle<CodeT> GetCodeT() { return ToCodeT(GetCode(), main_isolate()); }
+  Handle<CodeDataContainer> GetCodeDataContainer() {
+    return ToCodeDataContainer(GetCode(), main_isolate());
+  }
 
  protected:
   Address Generate() override {
diff --git a/test/cctest/compiler/function-tester.cc b/test/cctest/compiler/function-tester.cc
index 05df38af4db..cdfb16be0ae 100644
--- a/test/cctest/compiler/function-tester.cc
+++ b/test/cctest/compiler/function-tester.cc
@@ -47,17 +47,17 @@ FunctionTester::FunctionTester(Handle<Code> code, int param_count)
   CHECK(!code.is_null());
   CHECK(code->IsCode());
   Compile(function);
-  function->set_code(ToCodeT(*code), kReleaseStore);
+  function->set_code(ToCodeDataContainer(*code), kReleaseStore);
 }
 
-FunctionTester::FunctionTester(Handle<CodeT> code, int param_count)
+FunctionTester::FunctionTester(Handle<CodeDataContainer> code, int param_count)
     : isolate(main_isolate()),
       canonical(isolate),
       function((v8_flags.allow_natives_syntax = true,
                 NewFunction(BuildFunction(param_count).c_str()))),
       flags_(0) {
   CHECK(!code.is_null());
-  CHECK(code->IsCodeT());
+  CHECK(code->IsCodeDataContainer());
   Compile(function);
   function->set_code(*code, kReleaseStore);
 }
diff --git a/test/cctest/compiler/function-tester.h b/test/cctest/compiler/function-tester.h
index c3c3c676b5b..1c4fd2507f8 100644
--- a/test/cctest/compiler/function-tester.h
+++ b/test/cctest/compiler/function-tester.h
@@ -26,7 +26,7 @@ class FunctionTester : public InitializedHandleScope {
   FunctionTester(Graph* graph, int param_count);
 
   FunctionTester(Handle<Code> code, int param_count);
-  FunctionTester(Handle<CodeT> code, int param_count);
+  FunctionTester(Handle<CodeDataContainer> code, int param_count);
 
   // Assumes VoidDescriptor call interface.
   explicit FunctionTester(Handle<Code> code);
diff --git a/test/cctest/compiler/test-code-generator.cc b/test/cctest/compiler/test-code-generator.cc
index 4d77fee0441..3a853e5999e 100644
--- a/test/cctest/compiler/test-code-generator.cc
+++ b/test/cctest/compiler/test-code-generator.cc
@@ -104,9 +104,9 @@ Handle<Code> BuildSetupFunction(Isolate* isolate,
   params.push_back(__ Parameter<Object>(1));
   // The parameters of the teardown function are the results of the test
   // function.
-  params.push_back(__ HeapConstant(
-      ToCodeT(BuildTeardownFunction(isolate, teardown_call_descriptor, results),
-              isolate)));
+  params.push_back(__ HeapConstant(ToCodeDataContainer(
+      BuildTeardownFunction(isolate, teardown_call_descriptor, results),
+      isolate)));
   // First allocate the FixedArray which will hold the final results. Here we
   // should take care of all allocations, meaning we allocate HeapNumbers and
   // FixedArrays representing Simd128 values.
@@ -746,8 +746,8 @@ class TestEnvironment : public HandleAndZoneScope {
       // return value will be freed along with it. Copy the result into
       // state_out.
       FunctionTester ft(setup, 2);
-      Handle<FixedArray> result =
-          ft.CallChecked<FixedArray>(ToCodeT(test, main_isolate()), state_in);
+      Handle<FixedArray> result = ft.CallChecked<FixedArray>(
+          ToCodeDataContainer(test, main_isolate()), state_in);
       CHECK_EQ(result->length(), state_in->length());
       result->CopyTo(0, *state_out, 0, result->length());
     }
diff --git a/test/cctest/compiler/test-concurrent-shared-function-info.cc b/test/cctest/compiler/test-concurrent-shared-function-info.cc
index d9aabbe0c96..5282ee44c8b 100644
--- a/test/cctest/compiler/test-concurrent-shared-function-info.cc
+++ b/test/cctest/compiler/test-concurrent-shared-function-info.cc
@@ -36,14 +36,16 @@ void ExpectSharedFunctionInfoState(SharedFunctionInfo sfi,
   switch (expectedState) {
     case SfiState::Compiled:
       CHECK(function_data.IsBytecodeArray() ||
-            (function_data.IsCodeT() &&
-             CodeT::cast(function_data).kind() == CodeKind::BASELINE));
+            (function_data.IsCodeDataContainer() &&
+             CodeDataContainer::cast(function_data).kind() ==
+                 CodeKind::BASELINE));
       CHECK(script_or_debug_info.IsScript());
       break;
     case SfiState::DebugInfo:
       CHECK(function_data.IsBytecodeArray() ||
-            (function_data.IsCodeT() &&
-             CodeT::cast(function_data).kind() == CodeKind::BASELINE));
+            (function_data.IsCodeDataContainer() &&
+             CodeDataContainer::cast(function_data).kind() ==
+                 CodeKind::BASELINE));
       CHECK(script_or_debug_info.IsDebugInfo());
       {
         DebugInfo debug_info = DebugInfo::cast(script_or_debug_info);
diff --git a/test/cctest/compiler/test-run-machops.cc b/test/cctest/compiler/test-run-machops.cc
index 503705ef748..641ecbc92d3 100644
--- a/test/cctest/compiler/test-run-machops.cc
+++ b/test/cctest/compiler/test-run-machops.cc
@@ -7535,10 +7535,10 @@ TEST(RunComputedCodeObject) {
   RawMachineLabel merge;
   r.Branch(r.Parameter(0), &tlabel, &flabel);
   r.Bind(&tlabel);
-  Node* fa = r.HeapConstant(a.GetCodeT());
+  Node* fa = r.HeapConstant(a.GetCodeDataContainer());
   r.Goto(&merge);
   r.Bind(&flabel);
-  Node* fb = r.HeapConstant(b.GetCodeT());
+  Node* fb = r.HeapConstant(b.GetCodeDataContainer());
   r.Goto(&merge);
   r.Bind(&merge);
   Node* phi = r.Phi(MachineRepresentation::kWord32, fa, fb);
diff --git a/test/cctest/compiler/test-run-native-calls.cc b/test/cctest/compiler/test-run-native-calls.cc
index 70d36cb863f..ba56488fe9d 100644
--- a/test/cctest/compiler/test-run-native-calls.cc
+++ b/test/cctest/compiler/test-run-native-calls.cc
@@ -241,8 +241,10 @@ class Int32Signature : public MachineSignature {
   }
 };
 
-Handle<CodeT> CompileGraph(const char* name, CallDescriptor* call_descriptor,
-                           Graph* graph, Schedule* schedule = nullptr) {
+Handle<CodeDataContainer> CompileGraph(const char* name,
+                                       CallDescriptor* call_descriptor,
+                                       Graph* graph,
+                                       Schedule* schedule = nullptr) {
   Isolate* isolate = CcTest::InitIsolateOnce();
   OptimizedCompilationInfo info(base::ArrayVector("testing"), graph->zone(),
                                 CodeKind::FOR_TESTING);
@@ -256,11 +258,11 @@ Handle<CodeT> CompileGraph(const char* name, CallDescriptor* call_descriptor,
     code->Disassemble(name, os, isolate);
   }
 #endif
-  return ToCodeT(code, isolate);
+  return ToCodeDataContainer(code, isolate);
 }
 
-Handle<CodeT> WrapWithCFunction(Handle<CodeT> inner,
-                                CallDescriptor* call_descriptor) {
+Handle<CodeDataContainer> WrapWithCFunction(Handle<CodeDataContainer> inner,
+                                            CallDescriptor* call_descriptor) {
   Zone zone(inner->GetIsolate()->allocator(), ZONE_NAME, kCompressGraphZone);
   int param_count = static_cast<int>(call_descriptor->ParameterCount());
   GraphAndBuilders caller(&zone);
@@ -424,7 +426,7 @@ class Computer {
     CHECK_LE(num_params, kMaxParamCount);
     Isolate* isolate = CcTest::InitIsolateOnce();
     HandleScope scope(isolate);
-    Handle<CodeT> inner;
+    Handle<CodeDataContainer> inner;
     {
       // Build the graph for the computation.
       Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -439,7 +441,7 @@ class Computer {
 
     {
       // constant mode.
-      Handle<CodeT> wrapper;
+      Handle<CodeDataContainer> wrapper;
       {
         // Wrap the above code with a callable function that passes constants.
         Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -473,7 +475,7 @@ class Computer {
 
     {
       // buffer mode.
-      Handle<CodeT> wrapper;
+      Handle<CodeDataContainer> wrapper;
       {
         // Wrap the above code with a callable function that loads from {input}.
         Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -533,8 +535,9 @@ static void TestInt32Sub(CallDescriptor* desc) {
     b.graph()->SetEnd(ret);
   }
 
-  Handle<CodeT> inner_code = CompileGraph("Int32Sub", desc, inner.graph());
-  Handle<CodeT> wrapper = WrapWithCFunction(inner_code, desc);
+  Handle<CodeDataContainer> inner_code =
+      CompileGraph("Int32Sub", desc, inner.graph());
+  Handle<CodeDataContainer> wrapper = WrapWithCFunction(inner_code, desc);
   MachineSignature* msig = desc->GetMachineSignature(&zone);
   CodeRunner<int32_t> runnable(isolate, wrapper,
                                CSignature::FromMachine(&zone, msig));
@@ -556,7 +559,7 @@ static void CopyTwentyInt32(CallDescriptor* desc) {
   int32_t output[kNumParams];
   Isolate* isolate = CcTest::InitIsolateOnce();
   HandleScope scope(isolate);
-  Handle<CodeT> inner;
+  Handle<CodeDataContainer> inner;
   {
     // Writes all parameters into the output buffer.
     Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -573,7 +576,7 @@ static void CopyTwentyInt32(CallDescriptor* desc) {
   }
 
   CSignatureOf<int32_t> csig;
-  Handle<CodeT> wrapper;
+  Handle<CodeDataContainer> wrapper;
   {
     // Loads parameters from the input buffer and calls the above code.
     Zone zone(isolate->allocator(), ZONE_NAME, kCompressGraphZone);
@@ -939,7 +942,7 @@ TEST(Float64Select_stack_params_return_reg) {
 template <typename CType, int which>
 static void Build_Select_With_Call(CallDescriptor* desc,
                                    RawMachineAssembler* raw) {
-  Handle<CodeT> inner;
+  Handle<CodeDataContainer> inner;
   int num_params = ParamCount(desc);
   CHECK_LE(num_params, kMaxParamCount);
   {
@@ -951,7 +954,7 @@ static void Build_Select_With_Call(CallDescriptor* desc,
     r.Return(r.Parameter(which));
     inner = CompileGraph("Select-indirection", desc, &graph, r.ExportForTest());
     CHECK(!inner.is_null());
-    CHECK(inner->IsCodeT());
+    CHECK(inner->IsCodeDataContainer());
   }
 
   {
@@ -1038,7 +1041,7 @@ void MixedParamTest(int start) {
     MachineSignature* sig = builder.Build();
     CallDescriptor* desc = config.Create(&zone, sig);
 
-    Handle<CodeT> select;
+    Handle<CodeDataContainer> select;
     {
       // build the select.
       Zone select_zone(&allocator, ZONE_NAME, kCompressGraphZone);
@@ -1050,7 +1053,7 @@ void MixedParamTest(int start) {
 
     {
       // call the select.
-      Handle<CodeT> wrapper;
+      Handle<CodeDataContainer> wrapper;
       int32_t expected_ret;
       char bytes[kDoubleSize];
       alignas(8) char output[kDoubleSize];
@@ -1157,7 +1160,7 @@ void TestStackSlot(MachineType slot_type, T expected) {
 
   // Create inner function g. g has lots of parameters so that they are passed
   // over the stack.
-  Handle<CodeT> inner;
+  Handle<CodeDataContainer> inner;
   Graph graph(&zone);
   RawMachineAssembler g(isolate, &graph, desc);
 
diff --git a/test/cctest/heap/test-heap.cc b/test/cctest/heap/test-heap.cc
index c189ca53626..c603e78a866 100644
--- a/test/cctest/heap/test-heap.cc
+++ b/test/cctest/heap/test-heap.cc
@@ -4187,7 +4187,8 @@ TEST(EnsureAllocationSiteDependentCodesProcessed) {
     CHECK_EQ(dependency.length(), DependentCode::kSlotsPerEntry);
     MaybeObject code = dependency.Get(0 + DependentCode::kCodeSlotOffset);
     CHECK(code->IsWeak());
-    CHECK_EQ(bar_handle->code(), CodeT::cast(code->GetHeapObjectAssumeWeak()));
+    CHECK_EQ(bar_handle->code(),
+             CodeDataContainer::cast(code->GetHeapObjectAssumeWeak()));
     Smi groups = dependency.Get(0 + DependentCode::kGroupsSlotOffset).ToSmi();
     CHECK_EQ(static_cast<DependentCode::DependencyGroups>(groups.value()),
              DependentCode::kAllocationSiteTransitionChangedGroup |
@@ -4340,7 +4341,7 @@ TEST(CellsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCodeT(bar->code()), isolate);
+    code = handle(FromCodeDataContainer(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4387,7 +4388,7 @@ TEST(ObjectsInOptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCodeT(bar->code()), isolate);
+    code = handle(FromCodeDataContainer(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4453,7 +4454,7 @@ TEST(NewSpaceObjectsInOptimizedCode) {
     HeapVerifier::VerifyHeap(CcTest::heap());
 #endif
     CHECK(!bar->code().marked_for_deoptimization());
-    code = handle(FromCodeT(bar->code()), isolate);
+    code = handle(FromCodeDataContainer(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
@@ -4500,7 +4501,7 @@ TEST(ObjectsInEagerlyDeoptimizedCodeAreWeak) {
         *v8::Local<v8::Function>::Cast(CcTest::global()
                                            ->Get(context.local(), v8_str("bar"))
                                            .ToLocalChecked())));
-    code = handle(FromCodeT(bar->code()), isolate);
+    code = handle(FromCodeDataContainer(bar->code()), isolate);
     code = scope.CloseAndEscape(code);
   }
 
diff --git a/test/cctest/heap/test-weak-references.cc b/test/cctest/heap/test-weak-references.cc
index 669ab7b208e..2fdeff08107 100644
--- a/test/cctest/heap/test-weak-references.cc
+++ b/test/cctest/heap/test-weak-references.cc
@@ -53,10 +53,10 @@ TEST(WeakReferencesBasic) {
     assm.nop();  // supported on all architectures
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Handle<CodeT> code = ToCodeT(
+    Handle<CodeDataContainer> code = ToCodeDataContainer(
         Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build(),
         isolate);
-    CHECK(code->IsCodeT());
+    CHECK(code->IsCodeDataContainer());
 
     lh->set_data1(HeapObjectReference::Weak(*code));
     HeapObject code_heap_object;
diff --git a/test/cctest/test-accessor-assembler.cc b/test/cctest/test-accessor-assembler.cc
index d4ade8a0836..2c9d9528a0f 100644
--- a/test/cctest/test-accessor-assembler.cc
+++ b/test/cctest/test-accessor-assembler.cc
@@ -216,7 +216,7 @@ TEST(TryProbeStubCache) {
     Handle<JSObject> receiver = receivers[index % receivers.size()];
     Handle<Code> handler = handlers[index % handlers.size()];
     stub_cache.Set(*name, receiver->map(),
-                   MaybeObject::FromObject(ToCodeT(*handler)));
+                   MaybeObject::FromObject(ToCodeDataContainer(*handler)));
   }
 
   // Perform some queries.
diff --git a/test/cctest/test-accessors.cc b/test/cctest/test-accessors.cc
index 08a44598176..3936df2459b 100644
--- a/test/cctest/test-accessors.cc
+++ b/test/cctest/test-accessors.cc
@@ -533,7 +533,8 @@ static void StackCheck(Local<String> name,
   for (int i = 0; !iter.done(); i++) {
     i::StackFrame* frame = iter.frame();
     CHECK(i != 0 || (frame->type() == i::StackFrame::EXIT));
-    i::CodeT code = frame->LookupCodeT().ToCodeT();
+    i::CodeDataContainer code =
+        frame->LookupCodeDataContainer().ToCodeDataContainer();
     CHECK(code.contains(isolate, frame->pc()));
     iter.Advance();
   }
diff --git a/test/cctest/test-cpu-profiler.cc b/test/cctest/test-cpu-profiler.cc
index 13b24a24e63..a5c771d667f 100644
--- a/test/cctest/test-cpu-profiler.cc
+++ b/test/cctest/test-cpu-profiler.cc
@@ -4260,7 +4260,7 @@ int GetSourcePositionEntryCount(i::Isolate* isolate, const char* source,
   i::Handle<i::JSFunction> function = i::Handle<i::JSFunction>::cast(
       v8::Utils::OpenHandle(*CompileRun(source)));
   if (function->ActiveTierIsIgnition()) return -1;
-  i::Handle<i::Code> code(i::FromCodeT(function->code()), isolate);
+  i::Handle<i::Code> code(i::FromCodeDataContainer(function->code()), isolate);
   i::SourcePositionTableIterator iterator(
       ByteArray::cast(code->source_position_table()));
 
diff --git a/test/cctest/test-debug.cc b/test/cctest/test-debug.cc
index 22098df4633..d5039e6761e 100644
--- a/test/cctest/test-debug.cc
+++ b/test/cctest/test-debug.cc
@@ -4574,7 +4574,7 @@ TEST(BuiltinsExceptionPrediction) {
   bool fail = false;
   for (i::Builtin builtin = i::Builtins::kFirst; builtin <= i::Builtins::kLast;
        ++builtin) {
-    i::CodeT code = builtins->code(builtin);
+    i::CodeDataContainer code = builtins->code(builtin);
     if (code.kind() != i::CodeKind::BUILTIN) continue;
     auto prediction = code.GetBuiltinCatchPrediction();
     USE(prediction);
diff --git a/test/cctest/test-disasm-regex-helper.cc b/test/cctest/test-disasm-regex-helper.cc
index e0ac994044b..d51e343224e 100644
--- a/test/cctest/test-disasm-regex-helper.cc
+++ b/test/cctest/test-disasm-regex-helper.cc
@@ -21,7 +21,7 @@ std::string DisassembleFunction(const char* function) {
           CcTest::global()->Get(context, v8_str(function)).ToLocalChecked())));
 
   Isolate* isolate = CcTest::i_isolate();
-  Handle<Code> code(FromCodeT(f->code()), isolate);
+  Handle<Code> code(FromCodeDataContainer(f->code()), isolate);
   Address begin = code->raw_instruction_start();
   Address end = code->raw_instruction_end();
   std::ostringstream os;
diff --git a/test/cctest/test-heap-profiler.cc b/test/cctest/test-heap-profiler.cc
index 8876bbbf6ff..516b2e55a0c 100644
--- a/test/cctest/test-heap-profiler.cc
+++ b/test/cctest/test-heap-profiler.cc
@@ -4098,8 +4098,9 @@ TEST(WeakReference) {
 
   // Manually inlined version of FeedbackVector::SetOptimizedCode (needed due
   // to the FOR_TESTING code kind).
-  fv->set_maybe_optimized_code(i::HeapObjectReference::Weak(ToCodeT(*code)),
-                               v8::kReleaseStore);
+  fv->set_maybe_optimized_code(
+      i::HeapObjectReference::Weak(ToCodeDataContainer(*code)),
+      v8::kReleaseStore);
   fv->set_flags(
       i::FeedbackVector::MaybeHasTurbofanCodeBit::encode(true) |
       i::FeedbackVector::TieringStateBits::encode(i::TieringState::kNone));
diff --git a/test/cctest/test-swiss-name-dictionary-csa.cc b/test/cctest/test-swiss-name-dictionary-csa.cc
index 29e26f45ee3..577f1ae8cf8 100644
--- a/test/cctest/test-swiss-name-dictionary-csa.cc
+++ b/test/cctest/test-swiss-name-dictionary-csa.cc
@@ -91,14 +91,14 @@ class CSATestRunner {
   compiler::FunctionTester copy_ft_;
 
   // Used to create the FunctionTesters above.
-  static Handle<CodeT> create_get_data(Isolate* isolate);
-  static Handle<CodeT> create_find_entry(Isolate* isolate);
-  static Handle<CodeT> create_put(Isolate* isolate);
-  static Handle<CodeT> create_delete(Isolate* isolate);
-  static Handle<CodeT> create_add(Isolate* isolate);
-  static Handle<CodeT> create_allocate(Isolate* isolate);
-  static Handle<CodeT> create_get_counts(Isolate* isolate);
-  static Handle<CodeT> create_copy(Isolate* isolate);
+  static Handle<CodeDataContainer> create_get_data(Isolate* isolate);
+  static Handle<CodeDataContainer> create_find_entry(Isolate* isolate);
+  static Handle<CodeDataContainer> create_put(Isolate* isolate);
+  static Handle<CodeDataContainer> create_delete(Isolate* isolate);
+  static Handle<CodeDataContainer> create_add(Isolate* isolate);
+  static Handle<CodeDataContainer> create_allocate(Isolate* isolate);
+  static Handle<CodeDataContainer> create_get_counts(Isolate* isolate);
+  static Handle<CodeDataContainer> create_copy(Isolate* isolate);
 
   // Number of parameters of each of the tester functions above.
   static constexpr int kFindEntryParams = 2;  // (table, key)
@@ -262,7 +262,7 @@ void CSATestRunner::PrintTable() {
 #endif
 }
 
-Handle<CodeT> CSATestRunner::create_find_entry(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_find_entry(Isolate* isolate) {
   // TODO(v8:11330): Remove once CSA implementation has a fallback for
   // non-SSSE3/AVX configurations.
   if (!IsEnabled()) {
@@ -288,10 +288,10 @@ Handle<CodeT> CSATestRunner::create_find_entry(Isolate* isolate) {
     m.Return(m.SmiFromIntPtr(entry_var.value()));
   }
 
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_get_data(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_get_data(Isolate* isolate) {
   static_assert(kGetDataParams == 2);  // (table, entry)
   compiler::CodeAssemblerTester asm_tester(isolate,
                                            JSParameterCount(kGetDataParams));
@@ -312,10 +312,10 @@ Handle<CodeT> CSATestRunner::create_get_data(Isolate* isolate) {
 
     m.Return(data);
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_put(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_put(Isolate* isolate) {
   static_assert(kPutParams == 4);  // (table, entry, value, details)
   compiler::CodeAssemblerTester asm_tester(isolate,
                                            JSParameterCount(kPutParams));
@@ -334,10 +334,10 @@ Handle<CodeT> CSATestRunner::create_put(Isolate* isolate) {
 
     m.Return(m.UndefinedConstant());
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_delete(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_delete(Isolate* isolate) {
   // TODO(v8:11330): Remove once CSA implementation has a fallback for
   // non-SSSE3/AVX configurations.
   if (!IsEnabled()) {
@@ -360,10 +360,10 @@ Handle<CodeT> CSATestRunner::create_delete(Isolate* isolate) {
     m.Bind(&done);
     m.Return(shrunk_table_var.value());
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_add(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_add(Isolate* isolate) {
   // TODO(v8:11330): Remove once CSA implementation has a fallback for
   // non-SSSE3/AVX configurations.
   if (!IsEnabled()) {
@@ -390,10 +390,10 @@ Handle<CodeT> CSATestRunner::create_add(Isolate* isolate) {
     m.Bind(&needs_resize);
     m.Return(m.FalseConstant());
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_allocate(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_allocate(Isolate* isolate) {
   static_assert(kAllocateParams == 1);  // (capacity)
   compiler::CodeAssemblerTester asm_tester(isolate,
                                            JSParameterCount(kAllocateParams));
@@ -406,10 +406,10 @@ Handle<CodeT> CSATestRunner::create_allocate(Isolate* isolate) {
 
     m.Return(table);
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_get_counts(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_get_counts(Isolate* isolate) {
   static_assert(kGetCountsParams == 1);  // (table)
   compiler::CodeAssemblerTester asm_tester(isolate,
                                            JSParameterCount(kGetCountsParams));
@@ -440,10 +440,10 @@ Handle<CodeT> CSATestRunner::create_get_counts(Isolate* isolate) {
 
     m.Return(results);
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
-Handle<CodeT> CSATestRunner::create_copy(Isolate* isolate) {
+Handle<CodeDataContainer> CSATestRunner::create_copy(Isolate* isolate) {
   static_assert(kCopyParams == 1);  // (table)
   compiler::CodeAssemblerTester asm_tester(isolate,
                                            JSParameterCount(kCopyParams));
@@ -453,7 +453,7 @@ Handle<CodeT> CSATestRunner::create_copy(Isolate* isolate) {
 
     m.Return(m.CopySwissNameDictionary(table));
   }
-  return asm_tester.GenerateCodeTCloseAndEscape();
+  return asm_tester.GenerateCodeDataContainerCloseAndEscape();
 }
 
 void CSATestRunner::CheckAgainstReference() {
diff --git a/test/cctest/test-unwinder-code-pages.cc b/test/cctest/test-unwinder-code-pages.cc
index 86479697a6d..6d6cc54386a 100644
--- a/test/cctest/test-unwinder-code-pages.cc
+++ b/test/cctest/test-unwinder-code-pages.cc
@@ -168,7 +168,7 @@ TEST(Unwind_BuiltinPCInMiddle_Success_CodePagesAPI) {
   register_state.fp = stack;
 
   // Put the current PC inside of a valid builtin.
-  CodeT builtin = *BUILTIN_CODE(i_isolate, StringEqual);
+  CodeDataContainer builtin = *BUILTIN_CODE(i_isolate, StringEqual);
   const uintptr_t offset = 40;
   CHECK_LT(offset, builtin.InstructionSize());
   register_state.pc =
@@ -225,7 +225,7 @@ TEST(Unwind_BuiltinPCAtStart_Success_CodePagesAPI) {
 
   // Put the current PC at the start of a valid builtin, so that we are setting
   // up the frame.
-  CodeT builtin = *BUILTIN_CODE(i_isolate, StringEqual);
+  CodeDataContainer builtin = *BUILTIN_CODE(i_isolate, StringEqual);
   register_state.pc = reinterpret_cast<void*>(builtin.InstructionStart());
 
   bool unwound = v8::Unwinder::TryUnwindV8Frames(
@@ -296,12 +296,12 @@ TEST(Unwind_CodeObjectPCInMiddle_Success_CodePagesAPI) {
       Handle<JSFunction>::cast(v8::Utils::OpenHandle(*local_foo));
 
   // Put the current PC inside of the created code object.
-  CodeT codet = foo->code();
+  CodeDataContainer code_data_container = foo->code();
   // We don't produce optimized code when run with --no-turbofan and
   // --no-maglev.
-  if (!codet.is_optimized_code()) return;
+  if (!code_data_container.is_optimized_code()) return;
 
-  Code code = FromCodeT(codet);
+  Code code = FromCodeDataContainer(code_data_container);
   // We don't want the offset too early or it could be the `push rbp`
   // instruction (which is not at the start of generated code, because the lazy
   // deopt check happens before frame setup).
@@ -456,7 +456,7 @@ TEST(Unwind_JSEntry_Fail_CodePagesAPI) {
   CHECK_LE(pages_length, arraysize(code_pages));
   RegisterState register_state;
 
-  CodeT js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
+  CodeDataContainer js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
   byte* start = reinterpret_cast<byte*>(js_entry.InstructionStart());
   register_state.pc = start + 10;
 
@@ -638,7 +638,7 @@ TEST(PCIsInV8_InJSEntryRange_CodePagesAPI) {
       isolate->CopyCodePages(arraysize(code_pages), code_pages);
   CHECK_LE(pages_length, arraysize(code_pages));
 
-  CodeT js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
+  CodeDataContainer js_entry = *BUILTIN_CODE(i_isolate, JSEntry);
   byte* start = reinterpret_cast<byte*>(js_entry.InstructionStart());
   size_t length = js_entry.InstructionSize();
 
diff --git a/test/cctest/wasm/test-c-wasm-entry.cc b/test/cctest/wasm/test-c-wasm-entry.cc
index 0b12ef3174d..711dbb9b9e7 100644
--- a/test/cctest/wasm/test-c-wasm-entry.cc
+++ b/test/cctest/wasm/test-c-wasm-entry.cc
@@ -80,7 +80,7 @@ class CWasmEntryArgTester {
   Isolate* isolate_;
   std::function<ReturnType(Args...)> expected_fn_;
   const FunctionSig* sig_;
-  Handle<CodeT> c_wasm_entry_;
+  Handle<CodeDataContainer> c_wasm_entry_;
   WasmCode* wasm_code_;
 };
 
diff --git a/test/cctest/wasm/test-gc.cc b/test/cctest/wasm/test-gc.cc
index 63ec4b88ebc..6acceac319c 100644
--- a/test/cctest/wasm/test-gc.cc
+++ b/test/cctest/wasm/test-gc.cc
@@ -242,7 +242,7 @@ class WasmGCTester {
     NativeModule* native_module = instance_->module_object().native_module();
     Address wasm_call_target = instance_->GetCallTarget(function_index);
     Handle<Object> object_ref = instance_;
-    Handle<CodeT> c_wasm_entry =
+    Handle<CodeDataContainer> c_wasm_entry =
         compiler::CompileCWasmEntry(isolate_, sig, native_module->module());
     Execution::CallWasm(isolate_, c_wasm_entry, wasm_call_target, object_ref,
                         packer->argv());
diff --git a/test/cctest/wasm/test-run-wasm-wrappers.cc b/test/cctest/wasm/test-run-wasm-wrappers.cc
index 4f9dae4333a..54c6b4aada4 100644
--- a/test/cctest/wasm/test-run-wasm-wrappers.cc
+++ b/test/cctest/wasm/test-run-wasm-wrappers.cc
@@ -32,12 +32,12 @@ Handle<WasmInstanceObject> CompileModule(Zone* zone, Isolate* isolate,
   return maybe_instance.ToHandleChecked();
 }
 
-bool IsGeneric(CodeT wrapper) {
+bool IsGeneric(CodeDataContainer wrapper) {
   return wrapper.is_builtin() &&
          wrapper.builtin_id() == Builtin::kGenericJSToWasmWrapper;
 }
 
-bool IsSpecific(CodeT wrapper) {
+bool IsSpecific(CodeDataContainer wrapper) {
   return wrapper.kind() == CodeKind::JS_TO_WASM_FUNCTION;
 }
 
@@ -161,7 +161,7 @@ TEST(WrapperReplacement) {
 
     // Call the exported Wasm function as many times as required to almost
     // exhaust the remaining budget for using the generic wrapper.
-    Handle<CodeT> wrapper_before_call;
+    Handle<CodeDataContainer> wrapper_before_call;
     for (int i = remaining_budget; i > 0; --i) {
       // Verify that the wrapper to be used is the generic one.
       wrapper_before_call = handle(main_function_data->wrapper_code(), isolate);
@@ -174,7 +174,7 @@ TEST(WrapperReplacement) {
     }
 
     // Get the wrapper-code object after the wrapper replacement.
-    CodeT wrapper_after_call = main_function_data->wrapper_code();
+    CodeDataContainer wrapper_after_call = main_function_data->wrapper_code();
 
     // Verify that the budget has been exhausted.
     CHECK_EQ(main_function_data->wrapper_budget(), 0);
diff --git a/test/common/call-tester.h b/test/common/call-tester.h
index b3ea9f17fc4..150193b933b 100644
--- a/test/common/call-tester.h
+++ b/test/common/call-tester.h
@@ -55,8 +55,10 @@ class CodeRunner : public CallHelper<T> {
  public:
   CodeRunner(Isolate* isolate, Handle<Code> code, MachineSignature* csig)
       : CallHelper<T>(isolate, csig), code_(code) {}
-  CodeRunner(Isolate* isolate, Handle<CodeT> code, MachineSignature* csig)
-      : CallHelper<T>(isolate, csig), code_(FromCodeT(*code), isolate) {}
+  CodeRunner(Isolate* isolate, Handle<CodeDataContainer> code,
+             MachineSignature* csig)
+      : CallHelper<T>(isolate, csig),
+        code_(FromCodeDataContainer(*code), isolate) {}
   ~CodeRunner() override = default;
 
   Address Generate() override { return code_->entry(); }
diff --git a/test/common/code-assembler-tester.h b/test/common/code-assembler-tester.h
index b26857ab252..eaf1109bc54 100644
--- a/test/common/code-assembler-tester.h
+++ b/test/common/code-assembler-tester.h
@@ -74,8 +74,9 @@ class CodeAssemblerTester {
     return scope_.CloseAndEscape(GenerateCode());
   }
 
-  Handle<CodeT> GenerateCodeTCloseAndEscape() {
-    return scope_.CloseAndEscape(ToCodeT(GenerateCode(), scope_.isolate()));
+  Handle<CodeDataContainer> GenerateCodeDataContainerCloseAndEscape() {
+    return scope_.CloseAndEscape(
+        ToCodeDataContainer(GenerateCode(), scope_.isolate()));
   }
 
  private:
diff --git a/test/unittests/assembler/disasm-x64-unittest.cc b/test/unittests/assembler/disasm-x64-unittest.cc
index 70ad27b345f..5b38c796d23 100644
--- a/test/unittests/assembler/disasm-x64-unittest.cc
+++ b/test/unittests/assembler/disasm-x64-unittest.cc
@@ -49,7 +49,7 @@ using DisasmX64Test = TestWithIsolate;
 
 namespace {
 
-Handle<CodeT> CreateDummyCode(Isolate* isolate) {
+Handle<CodeDataContainer> CreateDummyCode(Isolate* isolate) {
   i::byte buffer[128];
   Assembler assm(AssemblerOptions{},
                  ExternalAssemblerBuffer(buffer, sizeof(buffer)));
@@ -59,7 +59,7 @@ Handle<CodeT> CreateDummyCode(Isolate* isolate) {
   assm.GetCode(isolate, &desc);
   Handle<Code> code =
       Factory::CodeBuilder(isolate, desc, CodeKind::FOR_TESTING).Build();
-  return ToCodeT(code, isolate);
+  return ToCodeDataContainer(code, isolate);
 }
 
 }  // namespace
@@ -82,7 +82,7 @@ TEST_F(DisasmX64Test, DisasmX64) {
   __ bind(&L2);
   __ call(rcx);
   __ nop();
-  Handle<CodeT> ic = CreateDummyCode(isolate());
+  Handle<CodeDataContainer> ic = CreateDummyCode(isolate());
   __ call(ic, RelocInfo::CODE_TARGET);
   __ nop();
 
diff --git a/test/unittests/codegen/code-pages-unittest.cc b/test/unittests/codegen/code-pages-unittest.cc
index a7ac67eef59..e2328fc4499 100644
--- a/test/unittests/codegen/code-pages-unittest.cc
+++ b/test/unittests/codegen/code-pages-unittest.cc
@@ -149,11 +149,11 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodeRange) {
   Handle<JSFunction> foo =
       Handle<JSFunction>::cast(v8::Utils::OpenHandle(*local_foo));
 
-  CodeT codet = foo->code();
+  CodeDataContainer code_data_container = foo->code();
   // We don't produce optimized code when run with --no-turbofan and
   // --no-maglev.
-  if (!codet.is_optimized_code()) return;
-  Code foo_code = FromCodeT(codet);
+  if (!code_data_container.is_optimized_code()) return;
+  Code foo_code = FromCodeDataContainer(code_data_container);
 
   EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
@@ -199,11 +199,11 @@ TEST_F(CodePagesTest, OptimizedCodeWithCodePages) {
         EXPECT_TRUE(v8_flags.always_sparkplug);
         return;
       }
-      CodeT codet = foo->code();
+      CodeDataContainer code_data_container = foo->code();
       // We don't produce optimized code when run with --no-turbofan and
       // --no-maglev.
-      if (!codet.is_optimized_code()) return;
-      Code foo_code = FromCodeT(codet);
+      if (!code_data_container.is_optimized_code()) return;
+      Code foo_code = FromCodeDataContainer(code_data_container);
 
       EXPECT_TRUE(i_isolate()->heap()->InSpace(foo_code, CODE_SPACE));
 
diff --git a/test/unittests/compiler/codegen-tester.h b/test/unittests/compiler/codegen-tester.h
index 22ceedd382b..37082387de4 100644
--- a/test/unittests/compiler/codegen-tester.h
+++ b/test/unittests/compiler/codegen-tester.h
@@ -77,7 +77,9 @@ class RawMachineAssemblerTester : public CallHelper<ReturnType>,
     return code_.ToHandleChecked();
   }
 
-  Handle<CodeT> GetCodeT() { return ToCodeT(GetCode(), isolate_); }
+  Handle<CodeDataContainer> GetCodeDataContainer() {
+    return ToCodeDataContainer(GetCode(), isolate_);
+  }
 
  protected:
   Address Generate() override {
diff --git a/test/unittests/compiler/function-tester.cc b/test/unittests/compiler/function-tester.cc
index a3e1c1097f4..295e8c1323f 100644
--- a/test/unittests/compiler/function-tester.cc
+++ b/test/unittests/compiler/function-tester.cc
@@ -61,7 +61,7 @@ FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code,
       flags_(0) {
   CHECK(!code.is_null());
   Compile(function);
-  function->set_code(ToCodeT(*code), kReleaseStore);
+  function->set_code(ToCodeDataContainer(*code), kReleaseStore);
 }
 
 FunctionTester::FunctionTester(Isolate* isolate, Handle<Code> code)
@@ -192,7 +192,7 @@ Handle<JSFunction> FunctionTester::Optimize(
   CHECK(info.shared_info()->HasBytecodeArray());
   JSFunction::EnsureFeedbackVector(isolate, function, &is_compiled_scope);
 
-  Handle<CodeT> code = ToCodeT(
+  Handle<CodeDataContainer> code = ToCodeDataContainer(
       compiler::Pipeline::GenerateCodeForTesting(&info, isolate, out_broker)
           .ToHandleChecked(),
       isolate);
diff --git a/test/unittests/compiler/run-tail-calls-unittest.cc b/test/unittests/compiler/run-tail-calls-unittest.cc
index c8a5332034c..3104ff80e0d 100644
--- a/test/unittests/compiler/run-tail-calls-unittest.cc
+++ b/test/unittests/compiler/run-tail-calls-unittest.cc
@@ -43,8 +43,8 @@ Handle<Code> BuildCaller(Isolate* isolate, CallDescriptor* call_descriptor,
   CodeStubAssembler assembler(tester.state());
   std::vector<Node*> params;
   // The first parameter is always the callee.
-  Handle<CodeT> code =
-      ToCodeT(BuildCallee(isolate, callee_descriptor), isolate);
+  Handle<CodeDataContainer> code =
+      ToCodeDataContainer(BuildCallee(isolate, callee_descriptor), isolate);
   params.push_back(__ HeapConstant(code));
   int param_slots = static_cast<int>(callee_descriptor->ParameterSlotCount());
   for (int i = 0; i < param_slots; ++i) {
@@ -64,7 +64,7 @@ Handle<Code> BuildSetupFunction(Isolate* isolate,
   CodeStubAssembler assembler(tester.state());
   std::vector<Node*> params;
   // The first parameter is always the callee.
-  Handle<CodeT> code = ToCodeT(
+  Handle<CodeDataContainer> code = ToCodeDataContainer(
       BuildCaller(isolate, caller_descriptor, callee_descriptor), isolate);
   params.push_back(__ HeapConstant(code));
   // Set up arguments for "Caller".
diff --git a/test/unittests/interpreter/interpreter-unittest.cc b/test/unittests/interpreter/interpreter-unittest.cc
index 7322abb41c2..a027e9ac38e 100644
--- a/test/unittests/interpreter/interpreter-unittest.cc
+++ b/test/unittests/interpreter/interpreter-unittest.cc
@@ -4760,11 +4760,11 @@ TEST_F(InterpreterTest, InterpreterWithNativeStack) {
   i::Handle<i::JSFunction> f = i::Handle<i::JSFunction>::cast(o);
 
   CHECK(f->shared().HasBytecodeArray());
-  i::CodeT code = f->shared().GetCode();
-  i::Handle<i::CodeT> interpreter_entry_trampoline =
+  i::CodeDataContainer code = f->shared().GetCode();
+  i::Handle<i::CodeDataContainer> interpreter_entry_trampoline =
       BUILTIN_CODE(i_isolate(), InterpreterEntryTrampoline);
 
-  CHECK(code.IsCodeT());
+  CHECK(code.IsCodeDataContainer());
   CHECK(code.is_interpreter_trampoline_builtin());
   CHECK_NE(code.address(), interpreter_entry_trampoline->address());
 }
@@ -4774,24 +4774,24 @@ TEST_F(InterpreterTest, InterpreterGetBytecodeHandler) {
   Interpreter* interpreter = i_isolate()->interpreter();
 
   // Test that single-width bytecode handlers deserializer correctly.
-  CodeT wide_handler =
+  CodeDataContainer wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kWide, OperandScale::kSingle);
 
   CHECK_EQ(wide_handler.builtin_id(), Builtin::kWideHandler);
 
-  CodeT add_handler =
+  CodeDataContainer add_handler =
       interpreter->GetBytecodeHandler(Bytecode::kAdd, OperandScale::kSingle);
 
   CHECK_EQ(add_handler.builtin_id(), Builtin::kAddHandler);
 
   // Test that double-width bytecode handlers deserializer correctly, including
   // an illegal bytecode handler since there is no Wide.Wide handler.
-  CodeT wide_wide_handler =
+  CodeDataContainer wide_wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kWide, OperandScale::kDouble);
 
   CHECK_EQ(wide_wide_handler.builtin_id(), Builtin::kIllegalHandler);
 
-  CodeT add_wide_handler =
+  CodeDataContainer add_wide_handler =
       interpreter->GetBytecodeHandler(Bytecode::kAdd, OperandScale::kDouble);
 
   CHECK_EQ(add_wide_handler.builtin_id(), Builtin::kAddWideHandler);
@@ -4984,15 +4984,15 @@ TEST_F(InterpreterTest, InterpreterCollectSourcePositions_GenerateStackTrace) {
 
 TEST_F(InterpreterTest, InterpreterLookupNameOfBytecodeHandler) {
   Interpreter* interpreter = i_isolate()->interpreter();
-  CodeT ldaLookupSlot = interpreter->GetBytecodeHandler(
+  CodeDataContainer ldaLookupSlot = interpreter->GetBytecodeHandler(
       Bytecode::kLdaLookupSlot, OperandScale::kSingle);
   CheckStringEqual("LdaLookupSlotHandler",
                    Builtins::name(ldaLookupSlot.builtin_id()));
-  CodeT wideLdaLookupSlot = interpreter->GetBytecodeHandler(
+  CodeDataContainer wideLdaLookupSlot = interpreter->GetBytecodeHandler(
       Bytecode::kLdaLookupSlot, OperandScale::kDouble);
   CheckStringEqual("LdaLookupSlotWideHandler",
                    Builtins::name(wideLdaLookupSlot.builtin_id()));
-  CodeT extraWideLdaLookupSlot = interpreter->GetBytecodeHandler(
+  CodeDataContainer extraWideLdaLookupSlot = interpreter->GetBytecodeHandler(
       Bytecode::kLdaLookupSlot, OperandScale::kQuadruple);
   CheckStringEqual("LdaLookupSlotExtraWideHandler",
                    Builtins::name(extraWideLdaLookupSlot.builtin_id()));
diff --git a/test/unittests/logging/log-unittest.cc b/test/unittests/logging/log-unittest.cc
index 764064c40df..bdbcea55041 100644
--- a/test/unittests/logging/log-unittest.cc
+++ b/test/unittests/logging/log-unittest.cc
@@ -1214,7 +1214,8 @@ TEST_F(LogTest, BuiltinsNotLoggedAsLazyCompile) {
     logger.StopLogging();
 
     i::Isolate* i_isolate = logger.i_isolate();
-    i::Handle<i::CodeT> builtin = BUILTIN_CODE(i_isolate, BooleanConstructor);
+    i::Handle<i::CodeDataContainer> builtin =
+        BUILTIN_CODE(i_isolate, BooleanConstructor);
     v8::base::EmbeddedVector<char, 100> buffer;
 
     // Should only be logged as "Builtin" with a name, never as "Function".
diff --git a/test/unittests/regexp/regexp-unittest.cc b/test/unittests/regexp/regexp-unittest.cc
index 065eea336f4..9f07d2ab532 100644
--- a/test/unittests/regexp/regexp-unittest.cc
+++ b/test/unittests/regexp/regexp-unittest.cc
@@ -2314,10 +2314,12 @@ TEST_F(RegExpTestWithContext, UnicodePropertyEscapeCodeSize) {
   if (maybe_bytecode.IsByteArray()) {
     // On x64, excessive inlining produced >250KB.
     CHECK_LT(ByteArray::cast(maybe_bytecode).Size(), kMaxSize);
-  } else if (maybe_code.IsCodeT()) {
+  } else if (maybe_code.IsCodeDataContainer()) {
     // On x64, excessive inlining produced >360KB.
-    CHECK_LT(FromCodeT(CodeT::cast(maybe_code)).Size(), kMaxSize);
-    CHECK_EQ(FromCodeT(CodeT::cast(maybe_code)).kind(), CodeKind::REGEXP);
+    CHECK_LT(FromCodeDataContainer(CodeDataContainer::cast(maybe_code)).Size(),
+             kMaxSize);
+    CHECK_EQ(FromCodeDataContainer(CodeDataContainer::cast(maybe_code)).kind(),
+             CodeKind::REGEXP);
   } else {
     UNREACHABLE();
   }
-- 
2.35.1

