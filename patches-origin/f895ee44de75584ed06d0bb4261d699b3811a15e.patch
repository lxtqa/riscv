From f895ee44de75584ed06d0bb4261d699b3811a15e Mon Sep 17 00:00:00 2001
From: Leszek Swirski <leszeks@chromium.org>
Date: Tue, 5 Sep 2023 12:51:08 +0200
Subject: [PATCH] [tagged-ptr] Remove Tagged<Foo> -> Foo conversion

Remove the implicit conversion from Tagged<Foo> to Foo, forcing all
remaining Foo variables to be changed to Tagged<Foo> if there's a
Tagged<Foo> involved.

The conversion from Foo to Tagged<Foo> temporarily remains, for uses of
the `*this` pointer,

Bug: v8:12710
Change-Id: Id00c538d01670ad91d694d99e51e463db4850eee
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4838236
Reviewed-by: Michael Lippautz <mlippautz@chromium.org>
Commit-Queue: Michael Lippautz <mlippautz@chromium.org>
Auto-Submit: Leszek Swirski <leszeks@chromium.org>
Cr-Commit-Position: refs/heads/main@{#89796}
---
 src/api/api-natives.cc                        |   6 +-
 src/api/api.cc                                |   4 +-
 src/api/api.h                                 |   4 +-
 src/asmjs/asm-js.cc                           |   3 +-
 src/ast/scopes.cc                             |   8 +-
 src/baseline/baseline-compiler.cc             |   6 +-
 src/baseline/bytecode-offset-iterator.h       |   2 +-
 src/builtins/builtins-date.cc                 |   4 +-
 src/builtins/builtins-string.cc               |  19 +--
 src/builtins/builtins-utils.h                 |   8 +-
 src/builtins/ia32/builtins-ia32.cc            |   2 +-
 src/codegen/compilation-cache.h               |   4 +-
 src/codegen/compiler.cc                       |  64 ++++-----
 src/codegen/external-reference.cc             |  13 +-
 src/common/code-memory-access-inl.h           |   4 +-
 src/common/code-memory-access.h               |   4 +-
 src/compiler/access-info.cc                   |   4 +-
 src/compiler/heap-refs.cc                     |  29 ++--
 src/compiler/js-heap-broker.cc                |   2 +-
 src/compiler/pipeline.cc                      |   6 +-
 src/date/date.h                               |   2 +-
 src/debug/debug-coverage.cc                   |   7 +-
 src/debug/debug-evaluate.cc                   |   6 +-
 src/debug/debug-frames.h                      |   2 +-
 src/debug/debug-interface.cc                  |   2 +-
 src/debug/debug-stack-trace-iterator.cc       |   2 +-
 src/debug/debug.cc                            |  37 ++---
 src/debug/debug.h                             |  13 +-
 src/debug/liveedit.cc                         |   2 +-
 src/deoptimizer/deoptimizer.cc                |   9 +-
 src/deoptimizer/deoptimizer.h                 |   4 +-
 src/deoptimizer/translated-state.cc           |  34 ++---
 src/deoptimizer/translated-state.h            |   4 +-
 src/diagnostics/basic-block-profiler.cc       |  13 +-
 src/diagnostics/etw-jit-win.cc                |  24 ++--
 src/diagnostics/gdb-jit.cc                    |  23 +--
 src/diagnostics/objects-printer.cc            |  14 +-
 src/execution/arguments.h                     |   5 +-
 src/execution/frames-inl.h                    |   2 +-
 src/execution/frames.cc                       |  36 ++---
 src/execution/frames.h                        |   6 +-
 src/execution/isolate-inl.h                   |   2 +-
 src/execution/isolate.cc                      |  57 ++++----
 src/execution/isolate.h                       |  24 ++--
 src/execution/local-isolate.h                 |   2 +-
 src/execution/protectors-inl.h                |   2 +-
 src/execution/thread-local-top.h              |  10 +-
 src/handles/global-handles-inl.h              |   4 +-
 src/handles/global-handles.h                  |   6 +-
 src/handles/handles-inl.h                     |   2 +-
 src/handles/handles.cc                        |   8 +-
 src/handles/handles.h                         |   2 +-
 src/handles/local-handles-inl.h               |   2 +-
 src/handles/maybe-handles-inl.h               |  17 +--
 src/handles/maybe-handles.h                   |  17 +--
 src/heap/allocation-result.h                  |   4 +-
 src/heap/code-stats.cc                        |   8 +-
 src/heap/conservative-stack-visitor.cc        |  10 +-
 src/heap/factory-base.cc                      |  10 +-
 src/heap/free-list.h                          |   2 +-
 src/heap/heap-verifier.cc                     |  18 +--
 src/heap/heap-verifier.h                      |  14 +-
 src/heap/heap.cc                              |  31 ++--
 src/heap/heap.h                               |  19 +--
 src/heap/incremental-marking.cc               |   4 +-
 src/heap/mark-compact.cc                      |  10 +-
 src/heap/mark-sweep-utilities.cc              |   2 +-
 src/heap/marking-visitor.h                    |   2 +-
 src/heap/marking.h                            |   6 +-
 src/heap/object-lock.h                        |   2 +-
 src/heap/object-stats.cc                      |   4 +-
 src/heap/objects-visiting-inl.h               |   2 +-
 src/heap/objects-visiting.cc                  |   6 +-
 src/heap/objects-visiting.h                   |   5 +-
 src/heap/paged-spaces.cc                      |   2 +-
 src/heap/paged-spaces.h                       |   2 +-
 src/heap/pretenuring-handler-inl.h            |   3 +-
 src/heap/pretenuring-handler.h                |   2 +-
 src/heap/read-only-promotion.cc               |  28 ++--
 src/heap/reference-summarizer.cc              |   2 +-
 src/heap/reference-summarizer.h               |   3 +-
 src/heap/scavenger.h                          |   9 +-
 src/heap/setup-heap-internal.cc               |   8 +-
 .../young-generation-marking-visitor-inl.h    |   2 +-
 src/ic/call-optimization.cc                   |   5 +-
 src/ic/call-optimization.h                    |   2 +-
 src/ic/handler-configuration.cc               |   6 +-
 src/ic/ic.cc                                  |   2 +-
 src/init/bootstrapper.h                       |   2 +-
 src/interpreter/bytecode-array-iterator.h     |   2 +-
 src/interpreter/constant-array-builder.cc     |   4 +-
 src/interpreter/constant-array-builder.h      |   6 +-
 src/json/json-stringifier.cc                  |  40 +++---
 src/logging/log.cc                            |  18 +--
 src/maglev/maglev-graph-builder.cc            |   6 +-
 src/objects/bytecode-array.cc                 |   4 +-
 src/objects/call-site-info.cc                 |  22 +--
 src/objects/call-site-info.h                  |   2 +-
 src/objects/compilation-cache-table-inl.h     |   2 +-
 src/objects/compilation-cache-table.cc        |   2 +-
 src/objects/compilation-cache-table.h         |   6 +-
 src/objects/contexts-inl.h                    |   8 +-
 src/objects/contexts.h                        |   8 +-
 src/objects/deoptimization-data.cc            |  36 ++---
 src/objects/deoptimization-data.h             |   2 +-
 src/objects/dependent-code.cc                 |   5 +-
 src/objects/elements.cc                       | 127 +++++++++--------
 src/objects/feedback-vector-inl.h             |   4 +-
 src/objects/feedback-vector.cc                |   2 +-
 src/objects/feedback-vector.h                 |   6 +-
 src/objects/fixed-array-inl.h                 |  10 +-
 src/objects/fixed-array.h                     |   2 +-
 src/objects/hash-table.h                      |  12 +-
 src/objects/heap-object-inl.h                 |   2 +-
 src/objects/heap-object.h                     |   7 -
 src/objects/js-function.cc                    |   6 +-
 src/objects/js-objects.cc                     |   4 +-
 src/objects/js-objects.h                      |   3 +-
 src/objects/js-struct.cc                      |   2 +-
 src/objects/keys.cc                           |  12 +-
 src/objects/lookup-cache.h                    |   4 +-
 src/objects/lookup.cc                         |   4 +-
 src/objects/map-inl.h                         |   2 +-
 src/objects/map-updater.cc                    |   8 +-
 src/objects/map-updater.h                     |   2 +-
 src/objects/map.cc                            |  11 +-
 src/objects/map.h                             |  12 +-
 src/objects/module-inl.h                      |   2 +-
 src/objects/module.cc                         |   4 +-
 src/objects/object-macros.h                   |   6 +-
 src/objects/object-type.cc                    |   4 +-
 src/objects/objects-body-descriptors-inl.h    |   8 +-
 src/objects/objects-body-descriptors.h        |   3 +-
 src/objects/objects.cc                        |  40 +++---
 src/objects/objects.h                         |   6 -
 src/objects/ordered-hash-table-inl.h          |   4 +-
 src/objects/ordered-hash-table.cc             |   2 +-
 src/objects/ordered-hash-table.h              |   5 +-
 src/objects/prototype.h                       |   6 +-
 src/objects/scope-info.cc                     |   6 +-
 src/objects/scope-info.h                      |   2 +-
 src/objects/shared-function-info-inl.h        |  20 +--
 src/objects/shared-function-info.h            |   2 +-
 src/objects/smi.h                             |   5 -
 src/objects/string-inl.h                      |  18 +--
 src/objects/string-set.h                      |   2 +-
 src/objects/string-table.cc                   |  53 +++----
 src/objects/string.h                          |   8 +-
 src/objects/swiss-name-dictionary.cc          |   6 +-
 src/objects/tagged-impl.cc                    |   4 +-
 src/objects/tagged-impl.h                     |   4 +-
 src/objects/tagged-index.h                    |   5 -
 src/objects/tagged.h                          |  19 ---
 src/objects/templates.cc                      |   4 +-
 src/objects/transitions.cc                    |  24 ++--
 src/objects/transitions.h                     |   2 +-
 src/parsing/scanner-character-streams.cc      |   4 +-
 src/profiler/heap-snapshot-generator.cc       |  15 +-
 src/profiler/heap-snapshot-generator.h        |   4 +-
 src/profiler/profiler-listener.h              |   4 +-
 src/profiler/sampling-heap-profiler.cc        |   2 +-
 .../experimental/experimental-interpreter.cc  |   4 +-
 .../loong64/regexp-macro-assembler-loong64.cc |   2 +-
 .../mips64/regexp-macro-assembler-mips64.cc   |   2 +-
 src/regexp/regexp-interpreter.cc              |   2 +-
 src/regexp/regexp-macro-assembler.cc          |  17 +--
 src/regexp/regexp-utils.cc                    |  10 +-
 src/regexp/regexp.cc                          |   2 +-
 src/regexp/regexp.h                           |   2 +-
 src/runtime/runtime-atomics.cc                |  11 +-
 src/runtime/runtime-classes.cc                |  30 ++--
 src/runtime/runtime-compiler.cc               |   2 +-
 src/runtime/runtime-generator.cc              |   2 +-
 src/runtime/runtime-module.cc                 |   5 +-
 src/runtime/runtime-numbers.cc                |   2 +-
 src/runtime/runtime-object.cc                 |  30 ++--
 src/runtime/runtime-operators.cc              |  12 +-
 src/runtime/runtime-regexp.cc                 |  63 ++++----
 src/runtime/runtime-scopes.cc                 |  37 ++---
 src/runtime/runtime-strings.cc                |  10 +-
 src/runtime/runtime-wasm.cc                   |  81 ++++++-----
 src/snapshot/context-serializer.cc            |   4 +-
 src/snapshot/context-serializer.h             |   4 +-
 src/snapshot/serializer-deserializer.cc       |   2 +-
 src/snapshot/serializer-inl.h                 |   3 +-
 src/snapshot/serializer.cc                    |  17 +--
 src/snapshot/shared-heap-serializer.cc        |   4 +-
 src/snapshot/snapshot.cc                      |   6 +-
 src/snapshot/snapshot.h                       |   2 +-
 src/snapshot/startup-serializer.cc            |   8 +-
 src/snapshot/startup-serializer.h             |   5 +-
 src/strings/string-builder-inl.h              |   2 +-
 src/torque/cc-generator.cc                    |   4 +-
 src/torque/implementation-visitor.cc          |  26 ++--
 src/torque/types.cc                           |   4 +-
 src/utils/address-map.cc                      |   4 +-
 src/wasm/module-compiler.cc                   |   4 +-
 src/wasm/wasm-external-refs.cc                |  20 +--
 src/wasm/wasm-objects-inl.h                   |  40 +++---
 src/wasm/wasm-objects.h                       |  18 +--
 test/cctest/compiler/test-run-load-store.cc   |   2 +-
 .../cctest/heap/test-concurrent-allocation.cc |   8 +-
 test/cctest/heap/test-heap.cc                 | 134 +++++++++---------
 test/cctest/test-accessors.cc                 |   2 +-
 test/cctest/test-api.cc                       |  16 +--
 test/cctest/test-assembler-loong64.cc         |   2 +-
 test/cctest/test-assembler-mips64.cc          |   2 +-
 test/cctest/test-assembler-ppc.cc             |  16 +--
 test/cctest/test-assembler-riscv32.cc         |   2 +-
 test/cctest/test-assembler-riscv64.cc         |   2 +-
 test/cctest/test-cpu-profiler.cc              |   4 +-
 test/cctest/test-heap-profiler.cc             |   2 +-
 test/cctest/test-serialize.cc                 |  19 +--
 test/cctest/test-smi-lexicographic-compare.cc |   4 +-
 test/cctest/test-strings.cc                   |   2 +-
 test/common/c-signature.h                     |   5 +-
 test/mkgrokdump/mkgrokdump.cc                 |   7 +-
 .../execution/microtask-queue-unittest.cc     |   8 +-
 .../conservative-stack-visitor-unittest.cc    |   2 +-
 test/unittests/heap/heap-unittest.cc          |   4 +-
 .../heap/inner-pointer-resolution-unittest.cc |  12 +-
 .../heap/persistent-handles-unittest.cc       |   2 +-
 test/unittests/objects/roots-unittest.cc      |   2 +-
 test/unittests/parser/parsing-unittest.cc     |   2 +-
 test/unittests/test-helpers.cc                |   2 +-
 test/unittests/torque/torque-unittest.cc      |   2 +-
 tools/gcmole/gcmole-test.cc                   |  70 +++++----
 tools/gcmole/test-expectations.txt            | 124 ++++++++--------
 228 files changed, 1289 insertions(+), 1226 deletions(-)

diff --git a/src/api/api-natives.cc b/src/api/api-natives.cc
index 3015c2f80fb..16710ee00f6 100644
--- a/src/api/api-natives.cc
+++ b/src/api/api-natives.cc
@@ -195,13 +195,13 @@ MaybeHandle<JSObject> ConfigureInstance(Isolate* isolate, Handle<JSObject> obj,
 
   // Walk the inheritance chain and copy all accessors to current object.
   int max_number_of_properties = 0;
-  TemplateInfoT info = *data;
+  Tagged<TemplateInfoT> info = *data;
   while (!info.is_null()) {
-    Tagged<Object> props = info.property_accessors();
+    Tagged<Object> props = info->property_accessors();
     if (!IsUndefined(props, isolate)) {
       max_number_of_properties += TemplateList::cast(props)->length();
     }
-    info = info.GetParent(isolate);
+    info = info->GetParent(isolate);
   }
 
   if (max_number_of_properties > 0) {
diff --git a/src/api/api.cc b/src/api/api.cc
index f6505e6820b..31ed7601eb7 100644
--- a/src/api/api.cc
+++ b/src/api/api.cc
@@ -7024,7 +7024,7 @@ class ObjectVisitorDeepFreezer : i::ObjectVisitor {
 
   i::Isolate* isolate_;
   Context::DeepFreezeDelegate* delegate_;
-  std::unordered_set<i::Object, i::Object::Hasher> done_list_;
+  std::unordered_set<i::Tagged<i::Object>, i::Object::Hasher> done_list_;
   std::vector<i::Handle<i::JSReceiver>> objects_to_freeze_;
   std::vector<i::Handle<i::AccessorPair>> lazy_accessor_pairs_to_freeze_;
   base::Optional<ErrorInfo> error_;
@@ -10343,7 +10343,7 @@ String::Value::~Value() { i::DeleteArray(str_); }
     i::Isolate* i_isolate = i::Isolate::Current();                         \
     API_RCS_SCOPE(i_isolate, NAME, New);                                   \
     ENTER_V8_NO_SCRIPT_NO_EXCEPTION(i_isolate);                            \
-    i::Object error;                                                       \
+    i::Tagged<i::Object> error;                                            \
     {                                                                      \
       i::HandleScope scope(i_isolate);                                     \
       i::Handle<i::String> message = Utils::OpenHandle(*raw_message);      \
diff --git a/src/api/api.h b/src/api/api.h
index 8b67c3e045d..002c8293efc 100644
--- a/src/api/api.h
+++ b/src/api/api.h
@@ -408,11 +408,11 @@ class HandleScopeImplementer {
   // `is_microtask_context_[i]` is 1.
   // TODO(tzik): Remove |is_microtask_context_| after the deprecated
   // v8::Isolate::GetEnteredContext() is removed.
-  DetachableVector<NativeContext> entered_contexts_;
+  DetachableVector<Tagged<NativeContext>> entered_contexts_;
   DetachableVector<int8_t> is_microtask_context_;
 
   // Used as a stack to keep track of saved contexts.
-  DetachableVector<Context> saved_contexts_;
+  DetachableVector<Tagged<Context>> saved_contexts_;
   Address* spare_;
   Address* last_handle_before_deferred_block_;
   // This is only used for threading support.
diff --git a/src/asmjs/asm-js.cc b/src/asmjs/asm-js.cc
index c02e218a8ad..edd8888ce8d 100644
--- a/src/asmjs/asm-js.cc
+++ b/src/asmjs/asm-js.cc
@@ -72,7 +72,8 @@ bool AreStdlibMembersValid(Isolate* isolate, Handle<JSReceiver> stdlib,
         base::StaticCharVector(#fname)));                                  \
     Handle<Object> value = StdlibMathMember(isolate, stdlib, name);        \
     if (!IsJSFunction(*value)) return false;                               \
-    SharedFunctionInfo shared = Handle<JSFunction>::cast(value)->shared(); \
+    Tagged<SharedFunctionInfo> shared =                                    \
+        Handle<JSFunction>::cast(value)->shared();                         \
     if (!shared->HasBuiltinId() ||                                         \
         shared->builtin_id() != Builtin::kMath##FName) {                   \
       return false;                                                        \
diff --git a/src/ast/scopes.cc b/src/ast/scopes.cc
index 003bd0f2736..672440a2b4b 100644
--- a/src/ast/scopes.cc
+++ b/src/ast/scopes.cc
@@ -204,7 +204,7 @@ ClassScope::ClassScope(IsolateT* isolate, Zone* zone,
   // If the class variable is context-allocated and its index is
   // saved for deserialization, deserialize it.
   if (scope_info->HasSavedClassVariable()) {
-    String name;
+    Tagged<String> name;
     int index;
     std::tie(name, index) = scope_info->SavedClassVariable();
     DCHECK_EQ(scope_info->ContextLocalMode(index), VariableMode::kConst);
@@ -475,7 +475,7 @@ Scope* Scope::DeserializeScopeChain(IsolateT* isolate, Zone* zone,
       DCHECK_EQ(scope_info->ContextLocalMode(0), VariableMode::kVar);
       DCHECK_EQ(scope_info->ContextLocalInitFlag(0), kCreatedInitialized);
       DCHECK(scope_info->HasInlinedLocalNames());
-      String name = scope_info->ContextInlinedLocalName(0);
+      Tagged<String> name = scope_info->ContextInlinedLocalName(0);
       MaybeAssignedFlag maybe_assigned =
           scope_info->ContextLocalMaybeAssignedFlag(0);
       outer_scope =
@@ -976,8 +976,8 @@ Variable* Scope::LookupInScopeInfo(const AstRawString* name, Scope* cache) {
   DCHECK_NULL(cache->variables_.Lookup(name));
   DisallowGarbageCollection no_gc;
 
-  String name_handle = *name->string();
-  ScopeInfo scope_info = *scope_info_;
+  Tagged<String> name_handle = *name->string();
+  Tagged<ScopeInfo> scope_info = *scope_info_;
   // The Scope is backed up by ScopeInfo. This means it cannot operate in a
   // heap-independent mode, and all strings must be internalized immediately. So
   // it's ok to get the Handle<String> here.
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index 1252af976e0..978c092826e 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -78,7 +78,7 @@ namespace detail {
 #ifdef DEBUG
 bool Clobbers(Register target, Register reg) { return target == reg; }
 bool Clobbers(Register target, Handle<Object> handle) { return false; }
-bool Clobbers(Register target, Smi smi) { return false; }
+bool Clobbers(Register target, Tagged<Smi> smi) { return false; }
 bool Clobbers(Register target, Tagged<TaggedIndex> index) { return false; }
 bool Clobbers(Register target, int32_t imm) { return false; }
 bool Clobbers(Register target, RootIndex index) { return false; }
@@ -92,7 +92,7 @@ bool MachineTypeMatches(MachineType type, MemOperand reg) { return true; }
 bool MachineTypeMatches(MachineType type, Handle<HeapObject> handle) {
   return type.IsTagged() && !type.IsTaggedSigned();
 }
-bool MachineTypeMatches(MachineType type, Smi handle) {
+bool MachineTypeMatches(MachineType type, Tagged<Smi> handle) {
   return type.IsTagged() && !type.IsTaggedPointer();
 }
 bool MachineTypeMatches(MachineType type, Tagged<TaggedIndex> handle) {
@@ -712,7 +712,7 @@ void BaselineCompiler::VisitLdaZero() {
 }
 
 void BaselineCompiler::VisitLdaSmi() {
-  Smi constant = Smi::FromInt(iterator().GetImmediateOperand(0));
+  Tagged<Smi> constant = Smi::FromInt(iterator().GetImmediateOperand(0));
   __ Move(kInterpreterAccumulatorRegister, constant);
 }
 
diff --git a/src/baseline/bytecode-offset-iterator.h b/src/baseline/bytecode-offset-iterator.h
index 6f30cd9a72d..06d02207ebc 100644
--- a/src/baseline/bytecode-offset-iterator.h
+++ b/src/baseline/bytecode-offset-iterator.h
@@ -84,7 +84,7 @@ class V8_EXPORT_PRIVATE BytecodeOffsetIterator {
   Address current_pc_start_offset_;
   Address current_pc_end_offset_;
   int current_bytecode_offset_;
-  BytecodeArray bytecode_handle_storage_;
+  Tagged<BytecodeArray> bytecode_handle_storage_;
   interpreter::BytecodeArrayIterator bytecode_iterator_;
   LocalHeap* local_heap_;
   base::Optional<DisallowGarbageCollection> no_gc_;
diff --git a/src/builtins/builtins-date.cc b/src/builtins/builtins-date.cc
index f539f1f6071..be4694d958c 100644
--- a/src/builtins/builtins-date.cc
+++ b/src/builtins/builtins-date.cc
@@ -58,8 +58,8 @@ double ParseDateTimeString(Isolate* isolate, Handle<String> str) {
   return DateCache::TimeClip(date);
 }
 
-Object SetLocalDateValue(Isolate* isolate, Handle<JSDate> date,
-                         double time_val) {
+Tagged<Object> SetLocalDateValue(Isolate* isolate, Handle<JSDate> date,
+                                 double time_val) {
   if (time_val >= -DateCache::kMaxTimeBeforeUTCInMs &&
       time_val <= DateCache::kMaxTimeBeforeUTCInMs) {
     time_val = isolate->date_cache()->ToUTC(static_cast<int64_t>(time_val));
diff --git a/src/builtins/builtins-string.cc b/src/builtins/builtins-string.cc
index db193bdf8bd..18a00bbd7a5 100644
--- a/src/builtins/builtins-string.cc
+++ b/src/builtins/builtins-string.cc
@@ -244,9 +244,9 @@ inline bool ToUpperOverflows(base::uc32 character) {
 }
 
 template <class Converter>
-V8_WARN_UNUSED_RESULT static Object ConvertCaseHelper(
-    Isolate* isolate, String string, SeqString result, int result_length,
-    unibrow::Mapping<Converter, 128>* mapping) {
+V8_WARN_UNUSED_RESULT static Tagged<Object> ConvertCaseHelper(
+    Isolate* isolate, Tagged<String> string, Tagged<SeqString> result,
+    int result_length, unibrow::Mapping<Converter, 128>* mapping) {
   DisallowGarbageCollection no_gc;
   // We try this twice, once with the assumption that the result is no longer
   // than the input and, if that assumption breaks, again with the exact
@@ -272,16 +272,16 @@ V8_WARN_UNUSED_RESULT static Object ConvertCaseHelper(
     int char_length = mapping->get(current, next, chars);
     if (char_length == 0) {
       // The case conversion of this character is the character itself.
-      result.Set(i, current);
+      result->Set(i, current);
       i++;
     } else if (char_length == 1 &&
                (ignore_overflow || !ToUpperOverflows(current))) {
       // Common case: converting the letter resulted in one character.
       DCHECK(static_cast<base::uc32>(chars[0]) != current);
-      result.Set(i, chars[0]);
+      result->Set(i, chars[0]);
       has_changed_character = true;
       i++;
-    } else if (result_length == string.length()) {
+    } else if (result_length == string->length()) {
       bool overflows = ToUpperOverflows(current);
       // We've assumed that the result would be as long as the
       // input but here is a character that converts to several
@@ -322,7 +322,7 @@ V8_WARN_UNUSED_RESULT static Object ConvertCaseHelper(
                                              : Smi::FromInt(current_length);
     } else {
       for (int j = 0; j < char_length; j++) {
-        result.Set(i, chars[j]);
+        result->Set(i, chars[j]);
         i++;
       }
       has_changed_character = true;
@@ -341,7 +341,7 @@ V8_WARN_UNUSED_RESULT static Object ConvertCaseHelper(
 }
 
 template <class Converter>
-V8_WARN_UNUSED_RESULT static Object ConvertCase(
+V8_WARN_UNUSED_RESULT static Tagged<Object> ConvertCase(
     Handle<String> s, Isolate* isolate,
     unibrow::Mapping<Converter, 128>* mapping) {
   s = String::Flatten(isolate, s);
@@ -379,7 +379,8 @@ V8_WARN_UNUSED_RESULT static Object ConvertCase(
     result = isolate->factory()->NewRawTwoByteString(length).ToHandleChecked();
   }
 
-  Object answer = ConvertCaseHelper(isolate, *s, *result, length, mapping);
+  Tagged<Object> answer =
+      ConvertCaseHelper(isolate, *s, *result, length, mapping);
   if (IsException(answer, isolate) || IsString(answer)) return answer;
 
   DCHECK(IsSmi(answer));
diff --git a/src/builtins/builtins-utils.h b/src/builtins/builtins-utils.h
index 9d9a51f33ba..116a190f67a 100644
--- a/src/builtins/builtins-utils.h
+++ b/src/builtins/builtins-utils.h
@@ -104,7 +104,7 @@ static_assert(BuiltinArguments::kNumExtraArgsWithReceiver ==
 // TODO(cbruni): add global flag to check whether any tracing events have been
 // enabled.
 #define BUILTIN_RCS(name)                                                  \
-  V8_WARN_UNUSED_RESULT static Object Builtin_Impl_##name(                 \
+  V8_WARN_UNUSED_RESULT static Tagged<Object> Builtin_Impl_##name(         \
       BuiltinArguments args, Isolate* isolate);                            \
                                                                            \
   V8_NOINLINE static Address Builtin_Impl_Stats_##name(                    \
@@ -126,11 +126,11 @@ static_assert(BuiltinArguments::kNumExtraArgsWithReceiver ==
     return BUILTIN_CONVERT_RESULT(Builtin_Impl_##name(args, isolate));     \
   }                                                                        \
                                                                            \
-  V8_WARN_UNUSED_RESULT static Object Builtin_Impl_##name(                 \
+  V8_WARN_UNUSED_RESULT static Tagged<Object> Builtin_Impl_##name(         \
       BuiltinArguments args, Isolate* isolate)
 
 #define BUILTIN_NO_RCS(name)                                               \
-  V8_WARN_UNUSED_RESULT static Object Builtin_Impl_##name(                 \
+  V8_WARN_UNUSED_RESULT static Tagged<Object> Builtin_Impl_##name(         \
       BuiltinArguments args, Isolate* isolate);                            \
                                                                            \
   V8_WARN_UNUSED_RESULT Address Builtin_##name(                            \
@@ -140,7 +140,7 @@ static_assert(BuiltinArguments::kNumExtraArgsWithReceiver ==
     return BUILTIN_CONVERT_RESULT(Builtin_Impl_##name(args, isolate));     \
   }                                                                        \
                                                                            \
-  V8_WARN_UNUSED_RESULT static Object Builtin_Impl_##name(                 \
+  V8_WARN_UNUSED_RESULT static Tagged<Object> Builtin_Impl_##name(         \
       BuiltinArguments args, Isolate* isolate)
 
 #ifdef V8_RUNTIME_CALL_STATS
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index f603a1e611c..fc86e56aedc 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -1603,7 +1603,7 @@ static void Generate_InterpreterEnterBytecode(MacroAssembler* masm) {
   // Set the return address to the correct point in the interpreter entry
   // trampoline.
   Label builtin_trampoline, trampoline_loaded;
-  Smi interpreter_entry_return_pc_offset(
+  Tagged<Smi> interpreter_entry_return_pc_offset(
       masm->isolate()->heap()->interpreter_entry_return_pc_offset());
   DCHECK_NE(interpreter_entry_return_pc_offset, Smi::zero());
 
diff --git a/src/codegen/compilation-cache.h b/src/codegen/compilation-cache.h
index 6c93a7871ce..0b25c5b404c 100644
--- a/src/codegen/compilation-cache.h
+++ b/src/codegen/compilation-cache.h
@@ -40,7 +40,7 @@ class CompilationCacheEvalOrScript {
   Isolate* isolate() const { return isolate_; }
 
   Isolate* const isolate_;
-  Object table_;
+  Tagged<Object> table_;
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(CompilationCacheEvalOrScript);
 };
@@ -127,7 +127,7 @@ class CompilationCacheRegExp {
   Isolate* isolate() const { return isolate_; }
 
   Isolate* const isolate_;
-  Object tables_[kGenerations];  // One for each generation.
+  Tagged<Object> tables_[kGenerations];  // One for each generation.
 
   DISALLOW_IMPLICIT_CONSTRUCTORS(CompilationCacheRegExp);
 };
diff --git a/src/codegen/compiler.cc b/src/codegen/compiler.cc
index 972898e2a95..bc9d8702773 100644
--- a/src/codegen/compiler.cc
+++ b/src/codegen/compiler.cc
@@ -76,7 +76,7 @@ namespace {
 
 constexpr bool IsOSR(BytecodeOffset osr_offset) { return !osr_offset.IsNone(); }
 
-void SetTieringState(JSFunction function, BytecodeOffset osr_offset,
+void SetTieringState(Tagged<JSFunction> function, BytecodeOffset osr_offset,
                      TieringState value) {
   if (IsOSR(osr_offset)) {
     function->set_osr_tiering_state(value);
@@ -85,7 +85,7 @@ void SetTieringState(JSFunction function, BytecodeOffset osr_offset,
   }
 }
 
-void ResetTieringState(JSFunction function, BytecodeOffset osr_offset) {
+void ResetTieringState(Tagged<JSFunction> function, BytecodeOffset osr_offset) {
   if (function->has_feedback_vector()) {
     SetTieringState(function, osr_offset, TieringState::kNone);
   }
@@ -375,7 +375,7 @@ void Compiler::LogFunctionCompilation(Isolate* isolate,
 namespace {
 
 ScriptOriginOptions OriginOptionsForEval(
-    Object script, ParsingWhileDebugging parsing_while_debugging) {
+    Tagged<Object> script, ParsingWhileDebugging parsing_while_debugging) {
   bool is_shared_cross_origin =
       parsing_while_debugging == ParsingWhileDebugging::kYes;
   bool is_opaque = false;
@@ -733,8 +733,8 @@ void EnsureSharedFunctionInfosArrayOnScript(Handle<Script> script,
   script->set_shared_function_infos(*infos);
 }
 
-void UpdateSharedFunctionFlagsAfterCompilation(FunctionLiteral* literal,
-                                               SharedFunctionInfo shared_info) {
+void UpdateSharedFunctionFlagsAfterCompilation(
+    FunctionLiteral* literal, Tagged<SharedFunctionInfo> shared_info) {
   DCHECK_EQ(shared_info->language_mode(), literal->language_mode());
 
   // These fields are all initialised in ParseInfo from the SharedFunctionInfo,
@@ -960,17 +960,17 @@ class OptimizedCodeCache : public AllStatic {
     if (!function->has_feedback_vector()) return {};
 
     DisallowGarbageCollection no_gc;
-    SharedFunctionInfo shared = function->shared();
+    Tagged<SharedFunctionInfo> shared = function->shared();
     RCS_SCOPE(isolate, RuntimeCallCounterId::kCompileGetFromOptimizedCodeMap);
 
-    Code code;
-    FeedbackVector feedback_vector = function->feedback_vector();
+    Tagged<Code> code;
+    Tagged<FeedbackVector> feedback_vector = function->feedback_vector();
     if (IsOSR(osr_offset)) {
       Handle<BytecodeArray> bytecode(shared->GetBytecodeArray(isolate),
                                      isolate);
       interpreter::BytecodeArrayIterator it(bytecode, osr_offset.ToInt());
       DCHECK_EQ(it.current_bytecode(), interpreter::Bytecode::kJumpLoop);
-      base::Optional<Code> maybe_code =
+      base::Optional<Tagged<Code>> maybe_code =
           feedback_vector->GetOptimizedOsrCode(isolate, it.GetSlotOperand(2));
       if (maybe_code.has_value()) code = maybe_code.value();
     } else {
@@ -991,18 +991,18 @@ class OptimizedCodeCache : public AllStatic {
     return handle(code, isolate);
   }
 
-  static void Insert(Isolate* isolate, JSFunction function,
-                     BytecodeOffset osr_offset, Code code,
+  static void Insert(Isolate* isolate, Tagged<JSFunction> function,
+                     BytecodeOffset osr_offset, Tagged<Code> code,
                      bool is_function_context_specializing) {
     const CodeKind kind = code->kind();
     if (!CodeKindIsStoredInOptimizedCodeCache(kind)) return;
 
-    FeedbackVector feedback_vector = function->feedback_vector();
+    Tagged<FeedbackVector> feedback_vector = function->feedback_vector();
 
     if (IsOSR(osr_offset)) {
       DCHECK(CodeKindCanOSR(kind));
       DCHECK(!is_function_context_specializing);
-      SharedFunctionInfo shared = function->shared();
+      Tagged<SharedFunctionInfo> shared = function->shared();
       Handle<BytecodeArray> bytecode(shared->GetBytecodeArray(isolate),
                                      isolate);
       interpreter::BytecodeArrayIterator it(bytecode, osr_offset.ToInt());
@@ -1688,7 +1688,7 @@ BackgroundCompileTask::~BackgroundCompileTask() = default;
 
 namespace {
 
-void SetScriptFieldsFromDetails(Isolate* isolate, Script script,
+void SetScriptFieldsFromDetails(Isolate* isolate, Tagged<Script> script,
                                 ScriptDetails script_details,
                                 DisallowGarbageCollection* no_gc) {
   Handle<Object> script_name;
@@ -1729,7 +1729,7 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   void IterateObjects(Tagged<HeapObject> start) {
     QueueVisit(start, kNormalObject);
     while (to_visit_.size() > 0) {
-      std::pair<HeapObject, ObjectKind> pair = to_visit_.top();
+      std::pair<Tagged<HeapObject>, ObjectKind> pair = to_visit_.top();
       to_visit_.pop();
       Tagged<HeapObject> current = pair.first;
       // The Script's shared_function_infos list and the constant pools for all
@@ -1818,12 +1818,12 @@ class MergeAssumptionChecker final : public ObjectVisitor {
   DisallowGarbageCollection no_gc_;
 
   PtrComprCageBase cage_base_;
-  std::stack<std::pair<HeapObject, ObjectKind>> to_visit_;
+  std::stack<std::pair<Tagged<HeapObject>, ObjectKind>> to_visit_;
 
   // Objects that are either in to_visit_ or done being visited. It is safe to
   // use HeapObject directly here because GC is disallowed while running this
   // visitor.
-  std::unordered_set<HeapObject, Object::Hasher> visited_;
+  std::unordered_set<Tagged<HeapObject>, Object::Hasher> visited_;
 
   ObjectKind current_object_kind_ = kNormalObject;
 };
@@ -1979,12 +1979,12 @@ class ConstantPoolPointerForwarder {
                                         LocalHeap* local_heap)
       : cage_base_(cage_base), local_heap_(local_heap) {}
 
-  void AddBytecodeArray(BytecodeArray bytecode_array) {
+  void AddBytecodeArray(Tagged<BytecodeArray> bytecode_array) {
     CHECK(IsBytecodeArray(bytecode_array));
     bytecode_arrays_to_update_.push_back(handle(bytecode_array, local_heap_));
   }
 
-  void Forward(SharedFunctionInfo from, SharedFunctionInfo to) {
+  void Forward(Tagged<SharedFunctionInfo> from, Tagged<SharedFunctionInfo> to) {
     forwarding_table_[from->function_literal_id()] = handle(to, local_heap_);
   }
 
@@ -1994,7 +1994,7 @@ class ConstantPoolPointerForwarder {
     for (Handle<BytecodeArray> bytecode_array : bytecode_arrays_to_update_) {
       local_heap_->Safepoint();
       DisallowGarbageCollection no_gc;
-      FixedArray constant_pool = bytecode_array->constant_pool();
+      Tagged<FixedArray> constant_pool = bytecode_array->constant_pool();
       IterateConstantPool(constant_pool);
     }
   }
@@ -2002,9 +2002,9 @@ class ConstantPoolPointerForwarder {
   bool HasAnythingToForward() const { return !forwarding_table_.empty(); }
 
  private:
-  void IterateConstantPool(FixedArray constant_pool) {
+  void IterateConstantPool(Tagged<FixedArray> constant_pool) {
     for (int i = 0, length = constant_pool->length(); i < length; ++i) {
-      Object obj = constant_pool->get(i);
+      Tagged<Object> obj = constant_pool->get(i);
       if (IsSmi(obj)) continue;
       Tagged<HeapObject> heap_obj = HeapObject::cast(obj);
       if (IsFixedArray(heap_obj, cage_base_)) {
@@ -2087,7 +2087,7 @@ void BackgroundMergeTask::BeginMergeInBackground(LocalIsolate* isolate,
     MaybeObject maybe_old_toplevel_sfi =
         old_script->shared_function_infos()->Get(kFunctionLiteralIdTopLevel);
     if (maybe_old_toplevel_sfi.IsWeak()) {
-      SharedFunctionInfo old_toplevel_sfi = SharedFunctionInfo::cast(
+      Tagged<SharedFunctionInfo> old_toplevel_sfi = SharedFunctionInfo::cast(
           maybe_old_toplevel_sfi.GetHeapObjectAssumeWeak());
       toplevel_sfi_from_cached_script_ =
           local_heap->NewPersistentHandle(old_toplevel_sfi);
@@ -2102,13 +2102,13 @@ void BackgroundMergeTask::BeginMergeInBackground(LocalIsolate* isolate,
     DisallowGarbageCollection no_gc;
     MaybeObject maybe_new_sfi = new_script->shared_function_infos()->Get(i);
     if (maybe_new_sfi.IsWeak()) {
-      SharedFunctionInfo new_sfi =
+      Tagged<SharedFunctionInfo> new_sfi =
           SharedFunctionInfo::cast(maybe_new_sfi.GetHeapObjectAssumeWeak());
       MaybeObject maybe_old_sfi = old_script->shared_function_infos()->Get(i);
       if (maybe_old_sfi.IsWeak()) {
         // The old script and the new script both have SharedFunctionInfos for
         // this function literal.
-        SharedFunctionInfo old_sfi =
+        Tagged<SharedFunctionInfo> old_sfi =
             SharedFunctionInfo::cast(maybe_old_sfi.GetHeapObjectAssumeWeak());
         forwarder.Forward(new_sfi, old_sfi);
         if (new_sfi->HasBytecodeArray()) {
@@ -2182,7 +2182,7 @@ Handle<SharedFunctionInfo> BackgroundMergeTask::CompleteMergeInForeground(
       // The old script's SFI didn't exist during the background work, but
       // does now. This means a re-merge is necessary so that any pointers to
       // the new script's SFI are updated to point to the old script's SFI.
-      SharedFunctionInfo old_sfi =
+      Tagged<SharedFunctionInfo> old_sfi =
           SharedFunctionInfo::cast(maybe_old_sfi.GetHeapObjectAssumeWeak());
       forwarder.Forward(*new_sfi, old_sfi);
     } else {
@@ -2509,10 +2509,10 @@ bool Compiler::CollectSourcePositions(Isolate* isolate,
 
   // If debugging, make sure that instrumented bytecode has the source position
   // table set on it as well.
-  if (base::Optional<DebugInfo> debug_info =
+  if (base::Optional<Tagged<DebugInfo>> debug_info =
           shared_info->TryGetDebugInfo(isolate)) {
-    if (debug_info->HasInstrumentedBytecodeArray()) {
-      ByteArray source_position_table =
+    if (debug_info.value()->HasInstrumentedBytecodeArray()) {
+      Tagged<ByteArray> source_position_table =
           job->compilation_info()->bytecode_array()->SourcePositionTable();
       shared_info->GetActiveBytecodeArray()->set_source_position_table(
           source_position_table, kReleaseStore);
@@ -2759,7 +2759,7 @@ bool Compiler::CompileBaseline(Isolate* isolate, Handle<JSFunction> function,
   // Baseline code needs a feedback vector.
   JSFunction::EnsureFeedbackVector(isolate, function, is_compiled_scope);
 
-  Code baseline_code = shared->baseline_code(kAcquireLoad);
+  Tagged<Code> baseline_code = shared->baseline_code(kAcquireLoad);
   DCHECK_EQ(baseline_code->kind(), CodeKind::BASELINE);
   function->set_code(baseline_code);
   return true;
@@ -3786,7 +3786,7 @@ MaybeHandle<JSFunction> Compiler::GetWrappedFunction(
     ASSIGN_RETURN_ON_EXCEPTION(isolate, top_level, maybe_result, JSFunction);
 
     SharedFunctionInfo::ScriptIterator infos(isolate, *script);
-    for (SharedFunctionInfo info = infos.Next(); !info.is_null();
+    for (Tagged<SharedFunctionInfo> info = infos.Next(); !info.is_null();
          info = infos.Next()) {
       if (info->is_wrapped()) {
         wrapped = Handle<SharedFunctionInfo>(info, isolate);
@@ -4098,7 +4098,7 @@ void Compiler::PostInstantiation(Handle<JSFunction> function,
       // deoptimized code just before installing it on the funciton.
       function->feedback_vector()->EvictOptimizedCodeMarkedForDeoptimization(
           isolate, *shared, "new function from shared function info");
-      Code code = function->feedback_vector()->optimized_code();
+      Tagged<Code> code = function->feedback_vector()->optimized_code();
       if (!code.is_null()) {
         // Caching of optimized code enabled and optimized code found.
         DCHECK(!code->marked_for_deoptimization());
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 1163601c26f..c6fedbcfe05 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -374,9 +374,10 @@ intptr_t DebugBreakAtEntry(Isolate* isolate, Address raw_sfi) {
 Address DebugGetCoverageInfo(Isolate* isolate, Address raw_sfi) {
   DisallowGarbageCollection no_gc;
   Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(Object(raw_sfi));
-  base::Optional<DebugInfo> debug_info = isolate->debug()->TryGetDebugInfo(sfi);
-  if (debug_info.has_value() && debug_info->HasCoverageInfo()) {
-    return debug_info->coverage_info().ptr();
+  base::Optional<Tagged<DebugInfo>> debug_info =
+      isolate->debug()->TryGetDebugInfo(sfi);
+  if (debug_info.has_value() && debug_info.value()->HasCoverageInfo()) {
+    return debug_info.value()->coverage_info().ptr();
   }
   return Smi::zero().ptr();
 }
@@ -1182,12 +1183,12 @@ static size_t NameDictionaryLookupForwardedString(Isolate* isolate,
   // This function should only be used as the slow path for forwarded strings.
   DCHECK(Name::IsForwardingIndex(key->raw_hash_field()));
 
-  Dictionary dict = Dictionary::cast(Object(raw_dict));
+  Tagged<Dictionary> dict = Dictionary::cast(Object(raw_dict));
   ReadOnlyRoots roots(isolate);
   uint32_t hash = key->hash();
   InternalIndex entry = mode == kFindExisting
-                            ? dict.FindEntry(isolate, roots, key, hash)
-                            : dict.FindInsertionEntry(isolate, roots, hash);
+                            ? dict->FindEntry(isolate, roots, key, hash)
+                            : dict->FindInsertionEntry(isolate, roots, hash);
   return entry.raw_value();
 }
 
diff --git a/src/common/code-memory-access-inl.h b/src/common/code-memory-access-inl.h
index ec67efa1c0c..1c643515a71 100644
--- a/src/common/code-memory-access-inl.h
+++ b/src/common/code-memory-access-inl.h
@@ -71,7 +71,7 @@ void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(T value) {
 }
 
 template <typename T, size_t offset>
-void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(T value,
+void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(Tagged<T> value,
                                                              ReleaseStoreTag) {
   // These asserts are no strict requirements, they just guard against
   // non-implemented functionality.
@@ -83,7 +83,7 @@ void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(T value,
 }
 
 template <typename T, size_t offset>
-void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(T value,
+void ThreadIsolation::WritableJitAllocation::WriteHeaderSlot(Tagged<T> value,
                                                              RelaxedStoreTag) {
   // These asserts are no strict requirements, they just guard against
   // non-implemented functionality.
diff --git a/src/common/code-memory-access.h b/src/common/code-memory-access.h
index b22f6725085..5818305790f 100644
--- a/src/common/code-memory-access.h
+++ b/src/common/code-memory-access.h
@@ -316,9 +316,9 @@ class V8_EXPORT ThreadIsolation {
     template <typename T, size_t offset>
     V8_INLINE void WriteHeaderSlot(T value);
     template <typename T, size_t offset>
-    V8_INLINE void WriteHeaderSlot(T value, ReleaseStoreTag);
+    V8_INLINE void WriteHeaderSlot(Tagged<T> value, ReleaseStoreTag);
     template <typename T, size_t offset>
-    V8_INLINE void WriteHeaderSlot(T value, RelaxedStoreTag);
+    V8_INLINE void WriteHeaderSlot(Tagged<T> value, RelaxedStoreTag);
 
     // CopyCode and CopyData have the same implementation at the moment, but
     // they will diverge once we implement validation.
diff --git a/src/compiler/access-info.cc b/src/compiler/access-info.cc
index e0c3cb94712..6240b89a553 100644
--- a/src/compiler/access-info.cc
+++ b/src/compiler/access-info.cc
@@ -566,7 +566,7 @@ PropertyAccessInfo AccessorAccessInfoHelper(
       return PropertyAccessInfo::Invalid(zone);
     }
     if (DEBUG_BOOL && holder.has_value()) {
-      base::Optional<NativeContext> holder_creation_context =
+      base::Optional<Tagged<NativeContext>> holder_creation_context =
           holder->object()->GetCreationContextRaw();
       CHECK(holder_creation_context.has_value());
       CHECK_EQ(*broker->target_native_context().object(),
@@ -592,7 +592,7 @@ PropertyAccessInfo AccessorAccessInfoHelper(
     }
   }
   if (access_mode == AccessMode::kLoad) {
-    base::Optional<Name> cached_property_name =
+    base::Optional<Tagged<Name>> cached_property_name =
         FunctionTemplateInfo::TryGetCachedPropertyName(isolate, *accessor);
     if (cached_property_name.has_value()) {
       OptionalNameRef cached_property_name_ref =
diff --git a/src/compiler/heap-refs.cc b/src/compiler/heap-refs.cc
index 4a0c6078061..ef8b0ea7046 100644
--- a/src/compiler/heap-refs.cc
+++ b/src/compiler/heap-refs.cc
@@ -180,7 +180,8 @@ class PropertyCellData : public HeapObjectData {
 
 namespace {
 
-ZoneVector<Address> GetCFunctions(FixedArray function_overloads, Zone* zone) {
+ZoneVector<Address> GetCFunctions(Tagged<FixedArray> function_overloads,
+                                  Zone* zone) {
   const int len = function_overloads->length() /
                   FunctionTemplateInfo::kFunctionOverloadEntrySize;
   ZoneVector<Address> c_functions = ZoneVector<Address>(len, zone);
@@ -191,8 +192,8 @@ ZoneVector<Address> GetCFunctions(FixedArray function_overloads, Zone* zone) {
   return c_functions;
 }
 
-ZoneVector<const CFunctionInfo*> GetCSignatures(FixedArray function_overloads,
-                                                Zone* zone) {
+ZoneVector<const CFunctionInfo*> GetCSignatures(
+    Tagged<FixedArray> function_overloads, Zone* zone) {
   const int len = function_overloads->length() /
                   FunctionTemplateInfo::kFunctionOverloadEntrySize;
   ZoneVector<const CFunctionInfo*> c_signatures =
@@ -294,7 +295,7 @@ OptionalObjectRef GetOwnFastDataPropertyFromHeap(JSHeapBroker* broker,
     // happen if the Ref was created in a prior GC epoch, and the object
     // shrunk in size. It might end up at the edge of a heap boundary. If
     // we see that the map is the same in this GC epoch, we are safe.
-    Map map = holder.object()->map(cage_base, kAcquireLoad);
+    Tagged<Map> map = holder.object()->map(cage_base, kAcquireLoad);
     if (*holder.map(broker).object() != map) {
       TRACE_BROKER_MISSING(broker, "Map changed for " << holder);
       return {};
@@ -321,7 +322,8 @@ OptionalObjectRef GetOwnFastDataPropertyFromHeap(JSHeapBroker* broker,
             "Expected PropertyArray for backing store in " << holder << ".");
         return {};
       }
-      PropertyArray properties = PropertyArray::cast(raw_properties_or_hash);
+      Tagged<PropertyArray> properties =
+          PropertyArray::cast(raw_properties_or_hash);
       const int array_index = field_index.outobject_array_index();
       if (array_index < properties->length(kAcquireLoad)) {
         constant = properties->get(array_index);
@@ -803,7 +805,7 @@ namespace {
 
 bool IsReadOnlyLengthDescriptor(Isolate* isolate, Handle<Map> jsarray_map) {
   DCHECK(!jsarray_map->is_dictionary_map());
-  DescriptorArray descriptors =
+  Tagged<DescriptorArray> descriptors =
       jsarray_map->instance_descriptors(isolate, kRelaxedLoad);
   static_assert(
       JSArray::kLengthOffset == JSObject::kHeaderSize,
@@ -942,7 +944,7 @@ ContextRef ContextRef::previous(JSHeapBroker* broker, size_t* depth) const {
 
   if (*depth == 0) return *this;
 
-  Context current = *object();
+  Tagged<Context> current = *object();
   while (*depth != 0 && i::IsContext(current->unchecked_previous())) {
     current = Context::cast(current->unchecked_previous());
     (*depth)--;
@@ -1098,7 +1100,7 @@ OptionalMapRef MapRef::AsElementsKind(JSHeapBroker* broker,
   const ElementsKind current_kind = elements_kind();
   if (kind == current_kind) return *this;
 
-  base::Optional<Map> maybe_result = Map::TryAsElementsKind(
+  base::Optional<Tagged<Map>> maybe_result = Map::TryAsElementsKind(
       broker->isolate(), object(), kind, ConcurrencyMode::kConcurrent);
 
 #ifdef DEBUG
@@ -1230,7 +1232,7 @@ OptionalObjectRef JSObjectRef::RawInobjectPropertyAt(JSHeapBroker* broker,
   {
     DisallowGarbageCollection no_gc;
     PtrComprCageBase cage_base = broker->cage_base();
-    Map current_map = object()->map(cage_base, kAcquireLoad);
+    Tagged<Map> current_map = object()->map(cage_base, kAcquireLoad);
 
     // If the map changed in some prior GC epoch, our {index} could be
     // outside the valid bounds of the cached map.
@@ -1632,7 +1634,7 @@ HEAP_ACCESSOR_C(SharedFunctionInfo, Builtin, builtin_id)
 BytecodeArrayRef SharedFunctionInfoRef::GetBytecodeArray(
     JSHeapBroker* broker) const {
   CHECK(HasBytecodeArray());
-  BytecodeArray bytecode_array;
+  Tagged<BytecodeArray> bytecode_array;
   if (!broker->IsMainThread()) {
     bytecode_array = object()->GetBytecodeArray(broker->local_isolate());
   } else {
@@ -2057,7 +2059,7 @@ OptionalMapRef HeapObjectRef::map_direct_read(JSHeapBroker* broker) const {
 
 namespace {
 
-OddballType GetOddballType(Isolate* isolate, Map map) {
+OddballType GetOddballType(Isolate* isolate, Tagged<Map> map) {
   if (map->instance_type() != ODDBALL_TYPE) {
     return OddballType::kNone;
   }
@@ -2084,7 +2086,8 @@ OddballType GetOddballType(Isolate* isolate, Map map) {
 
 HeapObjectType HeapObjectRef::GetHeapObjectType(JSHeapBroker* broker) const {
   if (data_->should_access_heap()) {
-    Map map = Handle<HeapObject>::cast(object())->map(broker->cage_base());
+    Tagged<Map> map =
+        Handle<HeapObject>::cast(object())->map(broker->cage_base());
     HeapObjectType::Flags flags(0);
     if (map->is_undetectable()) flags |= HeapObjectType::kUndetectable;
     if (map->is_callable()) flags |= HeapObjectType::kCallable;
@@ -2340,7 +2343,7 @@ std::ostream& operator<<(std::ostream& os, ObjectRef ref) {
 }
 
 unsigned CodeRef::GetInlinedBytecodeSize() const {
-  Code code = *object();
+  Tagged<Code> code = *object();
   const unsigned value = code->inlined_bytecode_size();
   if (value != 0 && code->marked_for_deoptimization()) {
     // Don't report inlined bytecode size if the code object was already
diff --git a/src/compiler/js-heap-broker.cc b/src/compiler/js-heap-broker.cc
index 5106bb0b51f..2eca58dcc2f 100644
--- a/src/compiler/js-heap-broker.cc
+++ b/src/compiler/js-heap-broker.cc
@@ -490,7 +490,7 @@ ProcessedFeedback const& JSHeapBroker::ReadFeedbackForPropertyAccess(
       if (map.is_deprecated()) {
         // TODO(ishell): support fast map updating if we enable it.
         CHECK(!v8_flags.fast_map_update);
-        base::Optional<Map> maybe_map = MapUpdater::TryUpdateNoLock(
+        base::Optional<Tagged<Map>> maybe_map = MapUpdater::TryUpdateNoLock(
             isolate(), *map.object(), ConcurrencyMode::kConcurrent);
         if (maybe_map.has_value()) {
           map = MakeRefAssumeMemoryFence(this, maybe_map.value());
diff --git a/src/compiler/pipeline.cc b/src/compiler/pipeline.cc
index 507203b89da..d7f22bd8df4 100644
--- a/src/compiler/pipeline.cc
+++ b/src/compiler/pipeline.cc
@@ -920,7 +920,7 @@ void PrintFunctionSource(OptimizedCompilationInfo* info, Isolate* isolate,
 
     if (!IsUndefined(script->source(), isolate)) {
       CodeTracer::StreamScope tracing_scope(isolate->GetCodeTracer());
-      Object source_name = script->name();
+      Tagged<Object> source_name = script->name();
       auto& os = tracing_scope.stream();
       os << "--- FUNCTION SOURCE (";
       if (IsString(source_name)) {
@@ -1356,7 +1356,7 @@ void PipelineCompilationJob::RegisterWeakObjectsInOptimizedCode(
     int const mode_mask = RelocInfo::EmbeddedObjectModeMask();
     for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
       DCHECK(RelocInfo::IsEmbeddedObjectMode(it.rinfo()->rmode()));
-      HeapObject target_object = it.rinfo()->target_object(cage_base);
+      Tagged<HeapObject> target_object = it.rinfo()->target_object(cage_base);
       if (code->IsWeakObjectInOptimizedCode(target_object)) {
         if (IsMap(target_object, cage_base)) {
           maps.push_back(handle(Map::cast(target_object), isolate));
@@ -4389,7 +4389,7 @@ bool PipelineImpl::CheckNoDeprecatedMaps(Handle<Code> code) {
   int mode_mask = RelocInfo::EmbeddedObjectModeMask();
   for (RelocIterator it(*code, mode_mask); !it.done(); it.next()) {
     DCHECK(RelocInfo::IsEmbeddedObjectMode(it.rinfo()->rmode()));
-    HeapObject obj = it.rinfo()->target_object(data_->isolate());
+    Tagged<HeapObject> obj = it.rinfo()->target_object(data_->isolate());
     if (IsMap(obj) && Map::cast(obj)->is_deprecated()) {
       return false;
     }
diff --git a/src/date/date.h b/src/date/date.h
index b2facb39504..7ad1430a130 100644
--- a/src/date/date.h
+++ b/src/date/date.h
@@ -213,7 +213,7 @@ class V8_EXPORT_PRIVATE DateCache {
     return segment->start_sec > segment->end_sec;
   }
 
-  Smi stamp_;
+  Tagged<Smi> stamp_;
 
   // Daylight Saving Time cache.
   DST dst_[kDSTSize];
diff --git a/src/debug/debug-coverage.cc b/src/debug/debug-coverage.cc
index e28ae287cb0..d374633650c 100644
--- a/src/debug/debug-coverage.cc
+++ b/src/debug/debug-coverage.cc
@@ -18,11 +18,12 @@ namespace v8 {
 namespace internal {
 
 class SharedToCounterMap
-    : public base::TemplateHashMapImpl<SharedFunctionInfo, uint32_t,
-                                       base::KeyEqualityMatcher<Object>,
+    : public base::TemplateHashMapImpl<Tagged<SharedFunctionInfo>, uint32_t,
+                                       base::KeyEqualityMatcher<Tagged<Object>>,
                                        base::DefaultAllocationPolicy> {
  public:
-  using Entry = base::TemplateHashMapEntry<SharedFunctionInfo, uint32_t>;
+  using Entry =
+      base::TemplateHashMapEntry<Tagged<SharedFunctionInfo>, uint32_t>;
   inline void Add(Tagged<SharedFunctionInfo> key, uint32_t count) {
     Entry* entry = LookupOrInsert(key, Hash(key), []() { return 0; });
     uint32_t old_count = entry->value;
diff --git a/src/debug/debug-evaluate.cc b/src/debug/debug-evaluate.cc
index ab4843c7cc1..a8d001d636d 100644
--- a/src/debug/debug-evaluate.cc
+++ b/src/debug/debug-evaluate.cc
@@ -1072,7 +1072,7 @@ DebugInfo::SideEffectState DebugEvaluate::FunctionGetSideEffectState(
     return requires_runtime_checks ? DebugInfo::kRequiresRuntimeChecks
                                    : DebugInfo::kHasNoSideEffect;
   } else if (info->IsApiFunction()) {
-    Code code = info->GetCode(isolate);
+    Tagged<Code> code = info->GetCode(isolate);
     if (code->is_builtin()) {
       return code->builtin_id() == Builtin::kHandleApiCallOrConstruct
                  ? DebugInfo::kHasNoSideEffect
@@ -1242,14 +1242,14 @@ void DebugEvaluate::VerifyTransitiveBuiltins(Isolate* isolate) {
   for (Builtin caller = Builtins::kFirst; caller <= Builtins::kLast; ++caller) {
     DebugInfo::SideEffectState state = BuiltinGetSideEffectState(caller);
     if (state != DebugInfo::kHasNoSideEffect) continue;
-    Code code = isolate->builtins()->code(caller);
+    Tagged<Code> code = isolate->builtins()->code(caller);
     int mode = RelocInfo::ModeMask(RelocInfo::CODE_TARGET) |
                RelocInfo::ModeMask(RelocInfo::RELATIVE_CODE_TARGET);
 
     for (RelocIterator it(code, mode); !it.done(); it.next()) {
       RelocInfo* rinfo = it.rinfo();
       DCHECK(RelocInfo::IsCodeTargetMode(rinfo->rmode()));
-      Code lookup_result =
+      Tagged<Code> lookup_result =
           isolate->heap()->FindCodeForInnerPointer(rinfo->target_address());
       Builtin callee = lookup_result->builtin_id();
       if (BuiltinGetSideEffectState(callee) == DebugInfo::kHasNoSideEffect) {
diff --git a/src/debug/debug-frames.h b/src/debug/debug-frames.h
index b8e99e7fb44..19352c71625 100644
--- a/src/debug/debug-frames.h
+++ b/src/debug/debug-frames.h
@@ -76,7 +76,7 @@ class RedirectActiveFunctions : public ThreadVisitor {
   void VisitThread(Isolate* isolate, ThreadLocalTop* top) override;
 
  private:
-  SharedFunctionInfo shared_;
+  Tagged<SharedFunctionInfo> shared_;
   Mode mode_;
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
 };
diff --git a/src/debug/debug-interface.cc b/src/debug/debug-interface.cc
index fb3b6b69e4b..fcd8fbaab3f 100644
--- a/src/debug/debug-interface.cc
+++ b/src/debug/debug-interface.cc
@@ -1022,7 +1022,7 @@ void ResetBlackboxedStateCache(Isolate* v8_isolate, Local<Script> script) {
   for (i::Tagged<i::SharedFunctionInfo> info = iter.Next(); !info.is_null();
        info = iter.Next()) {
     if (auto debug_info = isolate->debug()->TryGetDebugInfo(info)) {
-      debug_info->set_computed_debug_is_blackboxed(false);
+      debug_info.value()->set_computed_debug_is_blackboxed(false);
     }
   }
 }
diff --git a/src/debug/debug-stack-trace-iterator.cc b/src/debug/debug-stack-trace-iterator.cc
index 7ef5f236e4f..fba279e4db4 100644
--- a/src/debug/debug-stack-trace-iterator.cc
+++ b/src/debug/debug-stack-trace-iterator.cc
@@ -70,7 +70,7 @@ int DebugStackTraceIterator::GetContextId() const {
   DCHECK(!Done());
   Handle<Object> context = frame_inspector_->GetContext();
   if (IsContext(*context)) {
-    Object value =
+    Tagged<Object> value =
         Context::cast(*context)->native_context()->debug_context_id();
     if (IsSmi(value)) return Smi::ToInt(value);
   }
diff --git a/src/debug/debug.cc b/src/debug/debug.cc
index 6ad81e60c91..600f6fc8167 100644
--- a/src/debug/debug.cc
+++ b/src/debug/debug.cc
@@ -482,7 +482,7 @@ bool DebugInfoCollection::Contains(Tagged<SharedFunctionInfo> sfi) const {
   return true;
 }
 
-base::Optional<DebugInfo> DebugInfoCollection::Find(
+base::Optional<Tagged<DebugInfo>> DebugInfoCollection::Find(
     Tagged<SharedFunctionInfo> sfi) const {
   auto it = map_.find(sfi->unique_id());
   if (it == map_.end()) return {};
@@ -763,7 +763,7 @@ bool Debug::IsMutedAtCurrentLocation(JavaScriptFrame* frame) {
 namespace {
 
 // Convenience helper for easier base::Optional translation.
-bool ToHandle(Isolate* isolate, base::Optional<DebugInfo> debug_info,
+bool ToHandle(Isolate* isolate, base::Optional<Tagged<DebugInfo>> debug_info,
               Handle<DebugInfo>* out) {
   if (!debug_info.has_value()) return false;
   *out = handle(debug_info.value(), isolate);
@@ -1235,7 +1235,7 @@ void Debug::PrepareStepOnThrow() {
   while (!it.done()) {
     JavaScriptFrame* frame = it.frame();
     if (frame->LookupExceptionHandlerInTable(nullptr, nullptr) > 0) break;
-    std::vector<SharedFunctionInfo> infos;
+    std::vector<Tagged<SharedFunctionInfo>> infos;
     frame->GetFunctions(&infos);
     current_frame_count -= infos.size();
     it.Advance();
@@ -1587,7 +1587,7 @@ class DiscardBaselineCodeVisitor : public ThreadVisitor {
   }
 
  private:
-  SharedFunctionInfo shared_;
+  Tagged<SharedFunctionInfo> shared_;
 };
 }  // namespace
 
@@ -1691,9 +1691,10 @@ namespace {
 bool IsJSFunctionAndNeedsTrampoline(Isolate* isolate,
                                     Tagged<Object> maybe_function) {
   if (!IsJSFunction(maybe_function)) return false;
-  base::Optional<DebugInfo> debug_info = isolate->debug()->TryGetDebugInfo(
-      JSFunction::cast(maybe_function)->shared());
-  return debug_info.has_value() && debug_info->CanBreakAtEntry();
+  base::Optional<Tagged<DebugInfo>> debug_info =
+      isolate->debug()->TryGetDebugInfo(
+          JSFunction::cast(maybe_function)->shared());
+  return debug_info.has_value() && debug_info.value()->CanBreakAtEntry();
 }
 
 }  // namespace
@@ -1730,7 +1731,7 @@ void Debug::InstallDebugBreakTrampoline() {
   std::vector<AccessorPairWithContext> needs_instantiate;
   {
     // Deduplicate {needs_instantiate} by recording all collected AccessorPairs.
-    std::set<AccessorPair> recorded;
+    std::set<Tagged<AccessorPair>> recorded;
     HeapObjectIterator iterator(isolate_->heap());
     DisallowGarbageCollection no_gc;
     for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
@@ -1922,8 +1923,8 @@ class SharedFunctionInfoFinder {
   Tagged<JSFunction> ResultClosure() { return current_candidate_closure_; }
 
  private:
-  SharedFunctionInfo current_candidate_;
-  JSFunction current_candidate_closure_;
+  Tagged<SharedFunctionInfo> current_candidate_;
+  Tagged<JSFunction> current_candidate_closure_;
   int current_start_position_;
   int target_position_;
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
@@ -2151,7 +2152,7 @@ Handle<DebugInfo> Debug::GetOrCreateDebugInfo(
     Handle<SharedFunctionInfo> shared) {
   RCS_SCOPE(isolate_, RuntimeCallCounterId::kDebugger);
 
-  if (base::Optional<DebugInfo> di = debug_infos_.Find(*shared)) {
+  if (base::Optional<Tagged<DebugInfo>> di = debug_infos_.Find(*shared)) {
     return handle(di.value(), isolate_);
   }
 
@@ -2246,7 +2247,7 @@ Handle<FixedArray> Debug::GetLoadedScripts() {
   return FixedArray::ShrinkOrEmpty(isolate_, results, length);
 }
 
-base::Optional<DebugInfo> Debug::TryGetDebugInfo(
+base::Optional<Tagged<DebugInfo>> Debug::TryGetDebugInfo(
     Tagged<SharedFunctionInfo> sfi) {
   return debug_infos_.Find(sfi);
 }
@@ -2256,22 +2257,22 @@ bool Debug::HasDebugInfo(Tagged<SharedFunctionInfo> sfi) {
 }
 
 bool Debug::HasCoverageInfo(Tagged<SharedFunctionInfo> sfi) {
-  if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
-    return debug_info->HasCoverageInfo();
+  if (base::Optional<Tagged<DebugInfo>> debug_info = TryGetDebugInfo(sfi)) {
+    return debug_info.value()->HasCoverageInfo();
   }
   return false;
 }
 
 bool Debug::HasBreakInfo(Tagged<SharedFunctionInfo> sfi) {
-  if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
-    return debug_info->HasBreakInfo();
+  if (base::Optional<Tagged<DebugInfo>> debug_info = TryGetDebugInfo(sfi)) {
+    return debug_info.value()->HasBreakInfo();
   }
   return false;
 }
 
 bool Debug::BreakAtEntry(Tagged<SharedFunctionInfo> sfi) {
-  if (base::Optional<DebugInfo> debug_info = TryGetDebugInfo(sfi)) {
-    return debug_info->BreakAtEntry();
+  if (base::Optional<Tagged<DebugInfo>> debug_info = TryGetDebugInfo(sfi)) {
+    return debug_info.value()->BreakAtEntry();
   }
   return false;
 }
diff --git a/src/debug/debug.h b/src/debug/debug.h
index a96f057c51e..71cc0acd8db 100644
--- a/src/debug/debug.h
+++ b/src/debug/debug.h
@@ -191,7 +191,7 @@ class DebugInfoCollection final {
   void Insert(Tagged<SharedFunctionInfo> sfi, Tagged<DebugInfo> debug_info);
 
   bool Contains(Tagged<SharedFunctionInfo> sfi) const;
-  base::Optional<DebugInfo> Find(Tagged<SharedFunctionInfo> sfi) const;
+  base::Optional<Tagged<DebugInfo>> Find(Tagged<SharedFunctionInfo> sfi) const;
 
   void DeleteSlow(Tagged<SharedFunctionInfo> sfi);
 
@@ -273,7 +273,8 @@ class V8_EXPORT_PRIVATE Debug {
   Handle<FixedArray> GetLoadedScripts();
 
   // DebugInfo accessors.
-  base::Optional<DebugInfo> TryGetDebugInfo(Tagged<SharedFunctionInfo> sfi);
+  base::Optional<Tagged<DebugInfo>> TryGetDebugInfo(
+      Tagged<SharedFunctionInfo> sfi);
   bool HasDebugInfo(Tagged<SharedFunctionInfo> sfi);
   bool HasCoverageInfo(Tagged<SharedFunctionInfo> sfi);
   bool HasBreakInfo(Tagged<SharedFunctionInfo> sfi);
@@ -608,7 +609,7 @@ class V8_EXPORT_PRIVATE Debug {
 
     // If set, next PrepareStepIn will ignore this function until stepped into
     // another function, at which point this will be cleared.
-    Object ignore_step_into_function_;
+    Tagged<Object> ignore_step_into_function_;
 
     // If set then we need to repeat StepOut action at return.
     bool fast_forward_to_return_;
@@ -626,10 +627,10 @@ class V8_EXPORT_PRIVATE Debug {
     int target_frame_count_;
 
     // Value of the accumulator at the point of entering the debugger.
-    Object return_value_;
+    Tagged<Object> return_value_;
 
     // The suspended generator object to track when stepping.
-    Object suspended_generator_;
+    Tagged<Object> suspended_generator_;
 
     // Last used inspector breakpoint id.
     int last_breakpoint_id_;
@@ -645,7 +646,7 @@ class V8_EXPORT_PRIVATE Debug {
 
     // Throwing an exception may cause a Promise rejection.  For this purpose
     // we keep track of a stack of nested promises.
-    Object promise_stack_;
+    Tagged<Object> promise_stack_;
 
     // Frame ID for the frame that needs to be restarted. StackFrameId::NO_ID
     // otherwise. The unwinder uses the id to restart execution in this frame
diff --git a/src/debug/liveedit.cc b/src/debug/liveedit.cc
index a40774e01e1..58c8103bf50 100644
--- a/src/debug/liveedit.cc
+++ b/src/debug/liveedit.cc
@@ -913,7 +913,7 @@ void LiveEdit::PatchScript(Isolate* isolate, Handle<Script> script,
 
     isolate->compilation_cache()->Remove(sfi);
     isolate->debug()->DeoptimizeFunction(sfi);
-    if (base::Optional<DebugInfo> di = sfi->TryGetDebugInfo(isolate)) {
+    if (base::Optional<Tagged<DebugInfo>> di = sfi->TryGetDebugInfo(isolate)) {
       Handle<DebugInfo> debug_info(di.value(), isolate);
       isolate->debug()->RemoveBreakInfoAndMaybeFree(debug_info);
     }
diff --git a/src/deoptimizer/deoptimizer.cc b/src/deoptimizer/deoptimizer.cc
index 7026d81fe75..0698a43bf76 100644
--- a/src/deoptimizer/deoptimizer.cc
+++ b/src/deoptimizer/deoptimizer.cc
@@ -340,7 +340,7 @@ class ActivationsFinder : public ThreadVisitor {
 
  private:
 #ifdef DEBUG
-  GcSafeCode topmost_;
+  Tagged<GcSafeCode> topmost_;
   bool safe_to_deopt_;
 #endif
 };
@@ -1012,9 +1012,10 @@ void Deoptimizer::DoComputeUnoptimizedFrame(TranslatedFrame* translated_frame,
   TranslatedFrame::iterator function_iterator = value_iterator++;
 
   Tagged<BytecodeArray> bytecode_array;
-  base::Optional<DebugInfo> debug_info = shared->TryGetDebugInfo(isolate());
-  if (debug_info.has_value() && debug_info->HasBreakInfo()) {
-    bytecode_array = debug_info->DebugBytecodeArray();
+  base::Optional<Tagged<DebugInfo>> debug_info =
+      shared->TryGetDebugInfo(isolate());
+  if (debug_info.has_value() && debug_info.value()->HasBreakInfo()) {
+    bytecode_array = debug_info.value()->DebugBytecodeArray();
   } else {
     bytecode_array = shared->GetBytecodeArray(isolate());
   }
diff --git a/src/deoptimizer/deoptimizer.h b/src/deoptimizer/deoptimizer.h
index f37506b7558..7e71e7ca22e 100644
--- a/src/deoptimizer/deoptimizer.h
+++ b/src/deoptimizer/deoptimizer.h
@@ -204,8 +204,8 @@ class Deoptimizer : public Malloced {
   bool is_restart_frame() const { return restart_frame_index_ >= 0; }
 
   Isolate* isolate_;
-  JSFunction function_;
-  Code compiled_code_;
+  Tagged<JSFunction> function_;
+  Tagged<Code> compiled_code_;
   unsigned deopt_exit_index_;
   BytecodeOffset bytecode_offset_in_outermost_frame_ = BytecodeOffset::None();
   DeoptimizeKind deopt_kind_;
diff --git a/src/deoptimizer/translated-state.cc b/src/deoptimizer/translated-state.cc
index fc7c1b94009..a89d5eeac5e 100644
--- a/src/deoptimizer/translated-state.cc
+++ b/src/deoptimizer/translated-state.cc
@@ -64,7 +64,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
       } else {
         DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 3);
       }
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       os << "{bytecode_offset=" << bytecode_offset << ", function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get()
          << ", height=" << height << ", retval=@" << return_value_offset << "(#"
@@ -77,7 +77,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 3);
       int bailout_id = iterator.NextOperand();
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       unsigned height = iterator.NextOperand();
       os << "{bailout_id=" << bailout_id << ", function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get()
@@ -89,7 +89,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
     case TranslationOpcode::CONSTRUCT_CREATE_STUB_FRAME: {
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 2);
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       unsigned height = iterator.NextOperand();
       os << "{construct create stub, function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get()
@@ -100,7 +100,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
     case TranslationOpcode::CONSTRUCT_INVOKE_STUB_FRAME: {
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 1);
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       os << "{construct invoke stub, function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get() << "}";
       break;
@@ -112,7 +112,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 3);
       int bailout_id = iterator.NextOperand();
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       unsigned height = iterator.NextOperand();
       os << "{bailout_id=" << bailout_id << ", function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get()
@@ -125,7 +125,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 4);
       int bailout_id = iterator.NextOperand();
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       unsigned height = iterator.NextOperand();
       int wasm_return_type = iterator.NextOperand();
       os << "{bailout_id=" << bailout_id << ", function="
@@ -139,7 +139,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
     case TranslationOpcode::INLINED_EXTRA_ARGUMENTS: {
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 2);
       int shared_info_id = iterator.NextOperand();
-      Object shared_info = literal_array->get(shared_info_id);
+      Tagged<Object> shared_info = literal_array->get(shared_info_id);
       unsigned height = iterator.NextOperand();
       os << "{function="
          << SharedFunctionInfo::cast(shared_info)->DebugNameCStr().get()
@@ -292,7 +292,7 @@ void DeoptimizationFrameTranslationPrintSingleOpcode(
     case TranslationOpcode::LITERAL: {
       DCHECK_EQ(TranslationOpcodeOperandCount(opcode), 1);
       int literal_index = iterator.NextOperand();
-      Object literal_value = literal_array->get(literal_index);
+      Tagged<Object> literal_value = literal_array->get(literal_index);
       os << "{literal_id=" << literal_index << " (" << Brief(literal_value)
          << ")}";
       break;
@@ -500,16 +500,16 @@ Tagged<Object> TranslatedValue::GetRawValue() const {
   // Otherwise, do a best effort to get the value without allocation.
   switch (kind()) {
     case kTagged: {
-      Object object = raw_literal();
+      Tagged<Object> object = raw_literal();
       if (IsSlicedString(object)) {
         // If {object} is a sliced string of length smaller than
         // SlicedString::kMinLength, then trim the underlying SeqString and
         // return it. This assumes that such sliced strings are only built by
         // the fast string builder optimization of Turbofan's
         // StringBuilderOptimizer/EffectControlLinearizer.
-        SlicedString string = SlicedString::cast(object);
+        Tagged<SlicedString> string = SlicedString::cast(object);
         if (string->length() < SlicedString::kMinLength) {
-          String backing_store = string->parent();
+          Tagged<String> backing_store = string->parent();
           CHECK(IsSeqString(backing_store));
 
           // Creating filler at the end of the backing store if needed.
@@ -1573,7 +1573,7 @@ int TranslatedState::CreateNextTranslatedValue(
 
     case TranslationOpcode::LITERAL: {
       int literal_index = iterator->NextOperand();
-      Object value = literal_array->get(literal_index);
+      Tagged<Object> value = literal_array->get(literal_index);
       if (trace_file != nullptr) {
         PrintF(trace_file, V8PRIxPTR_FMT " ; (literal %2d) ", value.ptr(),
                literal_index);
@@ -1613,7 +1613,7 @@ Address TranslatedState::DecompressIfNeeded(intptr_t value) {
 TranslatedState::TranslatedState(const JavaScriptFrame* frame)
     : purpose_(kFrameInspection) {
   int deopt_index = SafepointEntry::kNoDeoptIndex;
-  DeoptimizationData data =
+  Tagged<DeoptimizationData> data =
       static_cast<const OptimizedFrame*>(frame)->GetDeoptimizationData(
           &deopt_index);
   DCHECK(!data.is_null() && deopt_index != SafepointEntry::kNoDeoptIndex);
@@ -1841,7 +1841,7 @@ void TranslatedState::EnsureObjectAllocatedAt(TranslatedValue* slot) {
 }
 
 int TranslatedValue::GetSmiValue() const {
-  Object value = GetRawValue();
+  Tagged<Object> value = GetRawValue();
   CHECK(IsSmi(value));
   return Smi::cast(value).value();
 }
@@ -2096,7 +2096,7 @@ void TranslatedState::EnsurePropertiesAllocatedAndMarked(
   Tagged<ByteArray> raw_object_storage = *object_storage;
 
   // Set markers for out-of-object properties.
-  DescriptorArray descriptors = map->instance_descriptors(isolate());
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate());
   for (InternalIndex i : map->IterateOwnDescriptors()) {
     FieldIndex index = FieldIndex::ForDescriptor(raw_map, i);
     Representation representation = descriptors->GetDetails(i).representation();
@@ -2135,7 +2135,7 @@ void TranslatedState::EnsureJSObjectAllocated(TranslatedValue* slot,
   DisallowGarbageCollection no_gc;
   Tagged<Map> raw_map = *map;
   Tagged<ByteArray> raw_object_storage = *object_storage;
-  DescriptorArray descriptors = map->instance_descriptors(isolate());
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate());
 
   // Set markers for in-object properties.
   for (InternalIndex i : raw_map->IterateOwnDescriptors()) {
@@ -2231,7 +2231,7 @@ void TranslatedState::InitializeJSObjectAt(
       // value.
       Handle<HeapObject> field_value = slot->storage();
       CHECK(IsCode(*field_value));
-      Code value = Code::cast(*field_value);
+      Tagged<Code> value = Code::cast(*field_value);
       object_storage->RawIndirectPointerField(offset).Relaxed_Store(value);
       INDIRECT_POINTER_WRITE_BARRIER(*object_storage, offset, value);
     } else if (marker == kStoreHeapObject) {
diff --git a/src/deoptimizer/translated-state.h b/src/deoptimizer/translated-state.h
index 34ed3584535..6222aacd49f 100644
--- a/src/deoptimizer/translated-state.h
+++ b/src/deoptimizer/translated-state.h
@@ -334,7 +334,7 @@ class TranslatedFrame {
 
   Kind kind_;
   BytecodeOffset bytecode_offset_;
-  SharedFunctionInfo raw_shared_info_;
+  Tagged<SharedFunctionInfo> raw_shared_info_;
   Handle<SharedFunctionInfo> shared_info_;
   int height_;
   int return_value_offset_;
@@ -489,7 +489,7 @@ class TranslatedState {
   };
   std::deque<ObjectPosition> object_positions_;
   Handle<FeedbackVector> feedback_vector_handle_;
-  FeedbackVector feedback_vector_;
+  Tagged<FeedbackVector> feedback_vector_;
   FeedbackSlot feedback_slot_;
 };
 
diff --git a/src/diagnostics/basic-block-profiler.cc b/src/diagnostics/basic-block-profiler.cc
index ba1a6e15169..52a631cd3d2 100644
--- a/src/diagnostics/basic-block-profiler.cc
+++ b/src/diagnostics/basic-block-profiler.cc
@@ -86,13 +86,14 @@ void BasicBlockProfilerData::CopyFromJSHeap(
   function_name_ = js_heap_data->name()->ToCString().get();
   schedule_ = js_heap_data->schedule()->ToCString().get();
   code_ = js_heap_data->code()->ToCString().get();
-  FixedUInt32Array counts = FixedUInt32Array::cast(js_heap_data->counts());
-  for (int i = 0; i < counts.length() / kBlockCountSlotSize; ++i) {
-    counts_.push_back(counts.get(i));
+  Tagged<FixedUInt32Array> counts =
+      FixedUInt32Array::cast(js_heap_data->counts());
+  for (int i = 0; i < counts->length() / kBlockCountSlotSize; ++i) {
+    counts_.push_back(counts->get(i));
   }
-  FixedInt32Array block_ids(js_heap_data->block_ids());
-  for (int i = 0; i < block_ids.length() / kBlockIdSlotSize; ++i) {
-    block_ids_.push_back(block_ids.get(i));
+  Tagged<FixedInt32Array> block_ids(js_heap_data->block_ids());
+  for (int i = 0; i < block_ids->length() / kBlockIdSlotSize; ++i) {
+    block_ids_.push_back(block_ids->get(i));
   }
   Tagged<PodArray<std::pair<int32_t, int32_t>>> branches =
       js_heap_data->branches();
diff --git a/src/diagnostics/etw-jit-win.cc b/src/diagnostics/etw-jit-win.cc
index ed9a24bb12d..fff41fc2237 100644
--- a/src/diagnostics/etw-jit-win.cc
+++ b/src/diagnostics/etw-jit-win.cc
@@ -247,7 +247,7 @@ void MaybeSetHandlerNow(Isolate* isolate) {
 }
 
 // TODO(v8/11911): UnboundScript::GetLineNumber should be replaced
-SharedFunctionInfo GetSharedFunctionInfo(const JitCodeEvent* event) {
+Tagged<SharedFunctionInfo> GetSharedFunctionInfo(const JitCodeEvent* event) {
   return event->script.IsEmpty() ? Tagged<SharedFunctionInfo>()
                                  : *Utils::OpenHandle(*event->script);
 }
@@ -265,8 +265,8 @@ std::wstring GetScriptMethodNameFromEvent(const JitCodeEvent* event) {
 }
 
 std::wstring GetScriptMethodNameFromSharedFunctionInfo(
-    const SharedFunctionInfo& sfi) {
-  auto sfi_name = sfi.DebugNameCStr();
+    Tagged<SharedFunctionInfo> sfi) {
+  auto sfi_name = sfi->DebugNameCStr();
   int method_name_length = static_cast<int>(strlen(sfi_name.get()));
   std::wstring method_name(method_name_length, L'\0');
   MultiByteToWideChar(CP_UTF8, 0, sfi_name.get(), method_name_length,
@@ -374,22 +374,22 @@ void EventHandler(const JitCodeEvent* event) {
   uint32_t script_line = -1;
   uint32_t script_column = -1;
 
-  SharedFunctionInfo sfi = GetSharedFunctionInfo(event);
-  if (!sfi.is_null() && IsScript(sfi.script())) {
-    Script script = Script::cast(sfi.script());
+  Tagged<SharedFunctionInfo> sfi = GetSharedFunctionInfo(event);
+  if (!sfi.is_null() && IsScript(sfi->script())) {
+    Tagged<Script> script = Script::cast(sfi->script());
 
     // if the first time seeing this source file, log the SourceLoad event
-    script_id = script.id();
+    script_id = script->id();
     if (IsolateLoadScriptData::MaybeAddLoadedScript(isolate, script_id)) {
       std::wstring wstr_name(0, L'\0');
-      Object script_name = script.GetNameOrSourceURL();
+      Tagged<Object> script_name = script->GetNameOrSourceURL();
       if (IsString(script_name)) {
-        String v8str_name = String::cast(script_name);
-        wstr_name.resize(v8str_name.length());
+        Tagged<String> v8str_name = String::cast(script_name);
+        wstr_name.resize(v8str_name->length());
         // On Windows wchar_t == uint16_t. const_Cast needed for C++14.
         uint16_t* wstr_data = const_cast<uint16_t*>(
             reinterpret_cast<const uint16_t*>(wstr_name.data()));
-        String::WriteToFlat(v8str_name, wstr_data, 0, v8str_name.length());
+        String::WriteToFlat(v8str_name, wstr_data, 0, v8str_name->length());
       }
 
       constexpr static auto source_load_event_meta =
@@ -406,7 +406,7 @@ void EventHandler(const JitCodeEvent* event) {
     }
 
     Script::PositionInfo info;
-    script.GetPositionInfo(sfi.StartPosition(), &info);
+    script->GetPositionInfo(sfi->StartPosition(), &info);
     script_line = info.line + 1;
     script_column = info.column + 1;
   }
diff --git a/src/diagnostics/gdb-jit.cc b/src/diagnostics/gdb-jit.cc
index efeb95f81a8..29bb4d1222e 100644
--- a/src/diagnostics/gdb-jit.cc
+++ b/src/diagnostics/gdb-jit.cc
@@ -909,7 +909,7 @@ class CodeDescription {
 #endif
 
   CodeDescription(const char* name, base::AddressRegion region,
-                  SharedFunctionInfo shared, LineInfo* lineinfo,
+                  Tagged<SharedFunctionInfo> shared, LineInfo* lineinfo,
                   bool is_function)
       : name_(name),
         shared_info_(shared),
@@ -925,7 +925,7 @@ class CodeDescription {
 
   bool has_scope_info() const { return !shared_info_.is_null(); }
 
-  ScopeInfo scope_info() const {
+  Tagged<ScopeInfo> scope_info() const {
     DCHECK(has_scope_info());
     return shared_info_->scope_info();
   }
@@ -940,7 +940,7 @@ class CodeDescription {
     return !shared_info_.is_null() && IsScript(shared_info_->script());
   }
 
-  Script script() { return Script::cast(shared_info_->script()); }
+  Tagged<Script> script() { return Script::cast(shared_info_->script()); }
 
   bool IsLineInfoAvailable() { return lineinfo_ != nullptr; }
 
@@ -978,7 +978,7 @@ class CodeDescription {
 
  private:
   const char* name_;
-  SharedFunctionInfo shared_info_;
+  Tagged<SharedFunctionInfo> shared_info_;
   LineInfo* lineinfo_;
   bool is_function_;
   base::AddressRegion code_region_;
@@ -1077,7 +1077,7 @@ class DebugInfoSection : public DebugSection {
     w->WriteString("v8value");
 
     if (desc_->has_scope_info()) {
-      ScopeInfo scope = desc_->scope_info();
+      Tagged<ScopeInfo> scope = desc_->scope_info();
       w->WriteULEB128(2);
       w->WriteString(desc_->name());
       w->Write<intptr_t>(desc_->CodeStart());
@@ -1265,7 +1265,7 @@ class DebugAbbrevSection : public DebugSection {
     w->WriteULEB128(0);
 
     if (extra_info) {
-      ScopeInfo scope = desc_->scope_info();
+      Tagged<ScopeInfo> scope = desc_->scope_info();
       int params = scope->ParameterCount();
       int context_slots = scope->ContextLocalCount();
       // The real slot ID is internal_slots + context_slot_id.
@@ -2006,7 +2006,7 @@ static void AddJITCodeEntry(CodeMap* map, const base::AddressRegion region,
 }
 
 static void AddCode(const char* name, base::AddressRegion region,
-                    SharedFunctionInfo shared, LineInfo* lineinfo,
+                    Tagged<SharedFunctionInfo> shared, LineInfo* lineinfo,
                     Isolate* isolate, bool is_function) {
   DisallowGarbageCollection no_gc;
   CodeDescription code_desc(name, region, shared, lineinfo, is_function);
@@ -2051,9 +2051,9 @@ void EventHandler(const v8::JitCodeEvent* event) {
       LineInfo* lineinfo = GetLineInfo(addr);
       std::string event_name(event->name.str, event->name.len);
       // It's called UnboundScript in the API but it's a SharedFunctionInfo.
-      SharedFunctionInfo shared = event->script.IsEmpty()
-                                      ? Tagged<SharedFunctionInfo>()
-                                      : *Utils::OpenHandle(*event->script);
+      Tagged<SharedFunctionInfo> shared =
+          event->script.IsEmpty() ? Tagged<SharedFunctionInfo>()
+                                  : *Utils::OpenHandle(*event->script);
       Isolate* isolate = reinterpret_cast<Isolate*>(event->isolate);
       bool is_function = false;
       // TODO(zhin): See if we can use event->code_type to determine
@@ -2064,7 +2064,8 @@ void EventHandler(const v8::JitCodeEvent* event) {
       // use event->code_type here instead of finding the Code.
       // TODO(zhin): Rename is_function to be more accurate.
       if (event->code_type == v8::JitCodeEvent::JIT_CODE) {
-        Code lookup_result = isolate->heap()->FindCodeForInnerPointer(addr);
+        Tagged<Code> lookup_result =
+            isolate->heap()->FindCodeForInnerPointer(addr);
         is_function = CodeKindIsOptimizedJSFunction(lookup_result->kind());
       }
       AddCode(event_name.c_str(), {addr, event->code_len}, shared, lineinfo,
diff --git a/src/diagnostics/objects-printer.cc b/src/diagnostics/objects-printer.cc
index 5d9509112f1..24e00e450ea 100644
--- a/src/diagnostics/objects-printer.cc
+++ b/src/diagnostics/objects-printer.cc
@@ -3536,7 +3536,7 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_Code(void* object) {
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-  v8::base::Optional<i::Code> lookup_result =
+  v8::base::Optional<i::Tagged<i::Code>> lookup_result =
       isolate->heap()->TryFindCodeForInnerPointerForPrinting(address);
   if (!lookup_result.has_value()) {
     i::PrintF(
@@ -3547,12 +3547,12 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_Code(void* object) {
 
 #if defined(OBJECT_PRINT)
   i::StdoutStream os;
-  lookup_result->CodePrint(os, nullptr, address);
+  lookup_result.value()->CodePrint(os, nullptr, address);
 #elif defined(ENABLE_DISASSEMBLER)
   i::StdoutStream os;
-  lookup_result->Disassemble(nullptr, os, isolate, address);
+  lookup_result.value()->Disassemble(nullptr, os, isolate, address);
 #else
-  i::Print(*lookup_result);
+  i::Print(lookup_result.value());
 #endif
 }
 
@@ -3572,7 +3572,7 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_OnlyCode(void* object,
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
 
-  v8::base::Optional<i::Code> lookup_result =
+  v8::base::Optional<i::Tagged<i::Code>> lookup_result =
       isolate->heap()->TryFindCodeForInnerPointerForPrinting(address);
   if (!lookup_result.has_value()) {
     i::PrintF(
@@ -3583,8 +3583,8 @@ V8_EXPORT_PRIVATE extern void _v8_internal_Print_OnlyCode(void* object,
 
 #if defined(ENABLE_DISASSEMBLER)
   i::StdoutStream os;
-  lookup_result->DisassembleOnlyCode(nullptr, os, isolate, address,
-                                     range_limit);
+  lookup_result.value()->DisassembleOnlyCode(nullptr, os, isolate, address,
+                                             range_limit);
 #endif
 }
 
diff --git a/src/execution/arguments.h b/src/execution/arguments.h
index 248d0fd3ba5..7676b974049 100644
--- a/src/execution/arguments.h
+++ b/src/execution/arguments.h
@@ -152,8 +152,9 @@ FullObjectSlot Arguments<T>::slot_from_address_at(int index, int offset) const {
 #define BUILTIN_CONVERT_RESULT_PAIR(x) (x)
 #endif  // DEBUG
 
-#define RUNTIME_FUNCTION(Name) \
-  RUNTIME_FUNCTION_RETURNS_TYPE(Address, Object, BUILTIN_CONVERT_RESULT, Name)
+#define RUNTIME_FUNCTION(Name)                           \
+  RUNTIME_FUNCTION_RETURNS_TYPE(Address, Tagged<Object>, \
+                                BUILTIN_CONVERT_RESULT, Name)
 
 #define RUNTIME_FUNCTION_RETURN_PAIR(Name)              \
   RUNTIME_FUNCTION_RETURNS_TYPE(ObjectPair, ObjectPair, \
diff --git a/src/execution/frames-inl.h b/src/execution/frames-inl.h
index 2f5582bbc39..794c065dafc 100644
--- a/src/execution/frames-inl.h
+++ b/src/execution/frames-inl.h
@@ -19,7 +19,7 @@ class InnerPointerToCodeCache final {
  public:
   struct InnerPointerToCodeCacheEntry {
     Address inner_pointer;
-    base::Optional<GcSafeCode> code;
+    base::Optional<Tagged<GcSafeCode>> code;
     union {
       SafepointEntry safepoint_entry;
       MaglevSafepointEntry maglev_safepoint_entry;
diff --git a/src/execution/frames.cc b/src/execution/frames.cc
index 9c8b4d589f9..8459fb06c42 100644
--- a/src/execution/frames.cc
+++ b/src/execution/frames.cc
@@ -241,7 +241,7 @@ void DebuggableStackFrameIterator::Advance() {
 int DebuggableStackFrameIterator::FrameFunctionCount() const {
   DCHECK(!done());
   if (!iterator_.frame()->is_optimized()) return 1;
-  std::vector<SharedFunctionInfo> infos;
+  std::vector<Tagged<SharedFunctionInfo>> infos;
   TurbofanFrame::cast(iterator_.frame())->GetFunctions(&infos);
   return static_cast<int>(infos.size());
 }
@@ -595,16 +595,18 @@ void StackFrameIteratorForProfilerForTesting::Advance() {
 
 namespace {
 
-base::Optional<GcSafeCode> GetContainingCode(Isolate* isolate, Address pc) {
+base::Optional<Tagged<GcSafeCode>> GetContainingCode(Isolate* isolate,
+                                                     Address pc) {
   return isolate->inner_pointer_to_code_cache()->GetCacheEntry(pc)->code;
 }
 
 }  // namespace
 
 Tagged<GcSafeCode> StackFrame::GcSafeLookupCode() const {
-  base::Optional<GcSafeCode> result = GetContainingCode(isolate(), pc());
-  DCHECK_GE(pc(), result->InstructionStart(isolate(), pc()));
-  DCHECK_LT(pc(), result->InstructionEnd(isolate(), pc()));
+  base::Optional<Tagged<GcSafeCode>> result =
+      GetContainingCode(isolate(), pc());
+  DCHECK_GE(pc(), result.value()->InstructionStart(isolate(), pc()));
+  DCHECK_LT(pc(), result.value()->InstructionEnd(isolate(), pc()));
   return result.value();
 }
 
@@ -763,7 +765,8 @@ StackFrame::Type StackFrameIterator::ComputeStackFrameType(
 #endif  // V8_ENABLE_WEBASSEMBLY
 
   // Look up the code object to figure out the type of the stack frame.
-  base::Optional<GcSafeCode> lookup_result = GetContainingCode(isolate(), pc);
+  base::Optional<Tagged<GcSafeCode>> lookup_result =
+      GetContainingCode(isolate(), pc);
   if (!lookup_result.has_value()) return StackFrame::NATIVE;
 
   MSAN_MEMORY_IS_INITIALIZED(
@@ -771,7 +774,7 @@ StackFrame::Type StackFrameIterator::ComputeStackFrameType(
       kSystemPointerSize);
   const intptr_t marker = Memory<intptr_t>(
       state->fp + CommonFrameConstants::kContextOrFrameTypeOffset);
-  switch (lookup_result->kind()) {
+  switch (lookup_result.value()->kind()) {
     case CodeKind::BUILTIN: {
       if (StackFrame::IsTypeMarker(marker)) break;
       return ComputeBuiltinFrameType(lookup_result.value());
@@ -792,7 +795,7 @@ StackFrame::Type StackFrameIterator::ComputeStackFrameType(
       return StackFrame::TURBOFAN;
 #if V8_ENABLE_WEBASSEMBLY
     case CodeKind::JS_TO_WASM_FUNCTION:
-      if (lookup_result->builtin_id() == Builtin::kJSToWasmWrapperAsm) {
+      if (lookup_result.value()->builtin_id() == Builtin::kJSToWasmWrapperAsm) {
         return StackFrame::JS_TO_WASM;
       }
       return StackFrame::TURBOFAN_STUB_WITH_CONTEXT;
@@ -839,8 +842,9 @@ StackFrame::Type StackFrameIteratorForProfiler::ComputeStackFrameType(
   // fast_c_call_caller_pc_address, for which authentication does not work.
   const Address pc = StackFrame::unauthenticated_pc(state->pc_address);
 #if V8_ENABLE_WEBASSEMBLY
-  Code wrapper = isolate()->builtins()->code(Builtin::kWasmToJsWrapperCSA);
-  if (pc >= wrapper.instruction_start() && pc <= wrapper.instruction_end()) {
+  Tagged<Code> wrapper =
+      isolate()->builtins()->code(Builtin::kWasmToJsWrapperCSA);
+  if (pc >= wrapper->instruction_start() && pc <= wrapper->instruction_end()) {
     return StackFrame::WASM_TO_JS;
   }
 #endif  // V8_ENABLE_WEBASSEMBLY
@@ -1880,7 +1884,7 @@ bool CommonFrame::HasTaggedOutgoingParams(
 }
 
 Tagged<HeapObject> TurbofanStubWithContextFrame::unchecked_code() const {
-  base::Optional<GcSafeCode> code_lookup =
+  base::Optional<Tagged<GcSafeCode>> code_lookup =
       isolate()->heap()->GcSafeTryFindCodeForInnerPointer(pc());
   if (!code_lookup.has_value()) return {};
   return code_lookup.value();
@@ -1973,7 +1977,7 @@ void TurbofanFrame::Iterate(RootVisitor* v) const {
 }
 
 Tagged<HeapObject> StubFrame::unchecked_code() const {
-  base::Optional<GcSafeCode> code_lookup =
+  base::Optional<Tagged<GcSafeCode>> code_lookup =
       isolate()->heap()->GcSafeTryFindCodeForInnerPointer(pc());
   if (!code_lookup.has_value()) return {};
   return code_lookup.value();
@@ -2038,7 +2042,7 @@ Address JavaScriptFrame::GetCallerStackPointer() const {
 }
 
 void JavaScriptFrame::GetFunctions(
-    std::vector<SharedFunctionInfo>* functions) const {
+    std::vector<Tagged<SharedFunctionInfo>>* functions) const {
   DCHECK(functions->empty());
   functions->push_back(function()->shared());
 }
@@ -2046,7 +2050,7 @@ void JavaScriptFrame::GetFunctions(
 void JavaScriptFrame::GetFunctions(
     std::vector<Handle<SharedFunctionInfo>>* functions) const {
   DCHECK(functions->empty());
-  std::vector<SharedFunctionInfo> raw_functions;
+  std::vector<Tagged<SharedFunctionInfo>> raw_functions;
   GetFunctions(&raw_functions);
   for (const auto& raw_function : raw_functions) {
     functions->push_back(
@@ -2742,7 +2746,7 @@ Tagged<DeoptimizationData> OptimizedFrame::GetDeoptimizationData(
 }
 
 void OptimizedFrame::GetFunctions(
-    std::vector<SharedFunctionInfo>* functions) const {
+    std::vector<Tagged<SharedFunctionInfo>>* functions) const {
   DCHECK(functions->empty());
   DCHECK(is_optimized());
 
@@ -3424,7 +3428,7 @@ InnerPointerToCodeCache::GetCacheEntry(Address inner_pointer) {
     // the code has been computed.
     entry->code =
         isolate_->heap()->GcSafeFindCodeForInnerPointer(inner_pointer);
-    if (entry->code->is_maglevved()) {
+    if (entry->code.value()->is_maglevved()) {
       entry->maglev_safepoint_entry.Reset();
     } else {
       entry->safepoint_entry.Reset();
diff --git a/src/execution/frames.h b/src/execution/frames.h
index 378b7570f01..f5c30726182 100644
--- a/src/execution/frames.h
+++ b/src/execution/frames.h
@@ -700,7 +700,8 @@ class JavaScriptFrame : public CommonFrameWithJSLinkage {
              int index) const override;
 
   // Return a list with {SharedFunctionInfo} objects of this frame.
-  virtual void GetFunctions(std::vector<SharedFunctionInfo>* functions) const;
+  virtual void GetFunctions(
+      std::vector<Tagged<SharedFunctionInfo>>* functions) const;
 
   void GetFunctions(std::vector<Handle<SharedFunctionInfo>>* functions) const;
 
@@ -942,7 +943,8 @@ class OptimizedFrame : public JavaScriptFrame {
   // Return a list with {SharedFunctionInfo} objects of this frame.
   // The functions are ordered bottom-to-top (i.e. functions.last()
   // is the top-most activation)
-  void GetFunctions(std::vector<SharedFunctionInfo>* functions) const override;
+  void GetFunctions(
+      std::vector<Tagged<SharedFunctionInfo>>* functions) const override;
 
   void Summarize(std::vector<FrameSummary>* frames) const override;
 
diff --git a/src/execution/isolate-inl.h b/src/execution/isolate-inl.h
index d956e5f088c..91a0147173d 100644
--- a/src/execution/isolate-inl.h
+++ b/src/execution/isolate-inl.h
@@ -239,7 +239,7 @@ void Isolate::DidFinishModuleAsyncEvaluation(unsigned ordinal) {
   Handle<type> Isolate::name() {                             \
     return Handle<type>(raw_native_context()->name(), this); \
   }                                                          \
-  bool Isolate::is_##name(type value) {                      \
+  bool Isolate::is_##name(Tagged<type> value) {              \
     return raw_native_context()->is_##name(value);           \
   }
 NATIVE_CONTEXT_FIELDS(NATIVE_CONTEXT_FIELD_ACCESSOR)
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index e7232e5c1fd..c8dace0268b 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -442,7 +442,7 @@ size_t Isolate::HashIsolateForEmbeddedBlob() {
   // Hash data sections of builtin code objects.
   for (Builtin builtin = Builtins::kFirst; builtin <= Builtins::kLast;
        ++builtin) {
-    Code code = builtins()->code(builtin);
+    Tagged<Code> code = builtins()->code(builtin);
 
     DCHECK(Internals::HasHeapObjectTag(code.ptr()));
     uint8_t* const code_ptr = reinterpret_cast<uint8_t*>(code.address());
@@ -960,7 +960,7 @@ bool GetStackTraceLimit(Isolate* isolate, int* result) {
 bool IsBuiltinFunction(Isolate* isolate, Tagged<HeapObject> object,
                        Builtin builtin) {
   if (!IsJSFunction(object)) return false;
-  JSFunction const function = JSFunction::cast(object);
+  Tagged<JSFunction> const function = JSFunction::cast(object);
   return function->code() == isolate->builtins()->code(builtin);
 }
 
@@ -1518,7 +1518,8 @@ MaybeHandle<Object> Isolate::ReportFailedAccessCheck(
   Handle<Object> data;
   {
     DisallowGarbageCollection no_gc;
-    AccessCheckInfo access_check_info = AccessCheckInfo::Get(this, receiver);
+    Tagged<AccessCheckInfo> access_check_info =
+        AccessCheckInfo::Get(this, receiver);
     if (access_check_info.is_null()) {
       no_gc.Release();
       THROW_NEW_ERROR(this, NewTypeError(MessageTemplate::kNoAccess), Object);
@@ -1567,7 +1568,8 @@ bool Isolate::MayAccess(Handle<NativeContext> accessing_context,
   v8::AccessCheckCallback callback = nullptr;
   {
     DisallowGarbageCollection no_gc;
-    AccessCheckInfo access_check_info = AccessCheckInfo::Get(this, receiver);
+    Tagged<AccessCheckInfo> access_check_info =
+        AccessCheckInfo::Get(this, receiver);
     if (access_check_info.is_null()) return false;
     Tagged<Object> fun_obj = access_check_info->callback();
     callback = v8::ToCData<v8::AccessCheckCallback>(fun_obj);
@@ -1950,7 +1952,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
 #endif  // V8_ENABLE_WEBASSEMBLY
   Tagged<Object> exception = pending_exception();
 
-  auto FoundHandler = [&](Context context, Address instruction_start,
+  auto FoundHandler = [&](Tagged<Context> context, Address instruction_start,
                           intptr_t handler_offset,
                           Address constant_pool_address, Address handler_sp,
                           Address handler_fp, int num_frames_above_handler) {
@@ -2003,7 +2005,8 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
 #if V8_ENABLE_WEBASSEMBLY
     if (v8_flags.experimental_wasm_stack_switching &&
         iter.frame()->type() == StackFrame::STACK_SWITCH) {
-      Code code = builtins()->code(Builtin::kWasmReturnPromiseOnSuspendAsm);
+      Tagged<Code> code =
+          builtins()->code(Builtin::kWasmReturnPromiseOnSuspendAsm);
       HandlerTable table(code);
       Address instruction_start =
           code->InstructionStart(this, iter.frame()->pc());
@@ -2029,7 +2032,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
       CHECK(frame->is_java_script());
 
       if (frame->is_optimized()) {
-        Code code = frame->LookupCode();
+        Tagged<Code> code = frame->LookupCode();
         // The debugger triggers lazy deopt for the "to-be-restarted" frame
         // immediately when the CDP event arrives while paused.
         CHECK(code->marked_for_deoptimization());
@@ -2052,7 +2055,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
       }
 
       debug()->clear_restart_frame();
-      Code code = *BUILTIN_CODE(this, RestartFrameTrampoline);
+      Tagged<Code> code = *BUILTIN_CODE(this, RestartFrameTrampoline);
       return FoundHandler(Context(), code->instruction_start(), 0,
                           code->constant_pool(), kNullAddress, frame->fp(),
                           visited_frames);
@@ -2068,7 +2071,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
         thread_local_top()->handler_ = handler->next_address();
 
         // Gather information from the handler.
-        Code code = frame->LookupCode();
+        Tagged<Code> code = frame->LookupCode();
         HandlerTable table(code);
         return FoundHandler(Context(),
                             code->InstructionStart(this, frame->pc()),
@@ -2081,7 +2084,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
       case StackFrame::C_WASM_ENTRY: {
         StackHandler* handler = frame->top_handler();
         thread_local_top()->handler_ = handler->next_address();
-        Code code = frame->LookupCode();
+        Tagged<Code> code = frame->LookupCode();
         HandlerTable table(code);
         Address instruction_start = code->instruction_start();
         int return_offset = static_cast<int>(frame->pc() - instruction_start);
@@ -2154,7 +2157,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
         int offset = opt_frame->LookupExceptionHandlerInTable(nullptr, nullptr);
         if (offset < 0) break;
         // The code might be an optimized code or a turbofanned builtin.
-        Code code = frame->LookupCode();
+        Tagged<Code> code = frame->LookupCode();
         // Compute the stack pointer from the frame pointer. This ensures
         // that argument slots on the stack are dropped as returning would.
         Address return_sp = frame->fp() +
@@ -2188,7 +2191,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
 
         // The code might be a dynamically generated stub or a turbofanned
         // embedded builtin.
-        Code code = stub_frame->LookupCode();
+        Tagged<Code> code = stub_frame->LookupCode();
         if (!code->is_turbofanned() || !code->has_handler_table()) {
           break;
         }
@@ -2231,13 +2234,13 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
         // position of the exception handler. The special builtin below will
         // take care of continuing to dispatch at that position. Also restore
         // the correct context for the handler from the interpreter register.
-        Context context =
+        Tagged<Context> context =
             Context::cast(js_frame->ReadInterpreterRegister(context_reg));
         DCHECK(IsContext(context));
 
         if (frame->is_baseline()) {
           BaselineFrame* sp_frame = BaselineFrame::cast(js_frame);
-          Code code = sp_frame->LookupCode();
+          Tagged<Code> code = sp_frame->LookupCode();
           intptr_t pc_offset = sp_frame->GetPCForBytecodeOffset(offset);
           // Patch the context register directly on the frame, so that we don't
           // need to have a context read + write in the baseline code.
@@ -2249,7 +2252,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
           InterpretedFrame::cast(js_frame)->PatchBytecodeOffset(
               static_cast<int>(offset));
 
-          Code code = *BUILTIN_CODE(this, InterpreterEnterAtBytecode);
+          Tagged<Code> code = *BUILTIN_CODE(this, InterpreterEnterAtBytecode);
           // We subtract a frame from visited_frames because otherwise the
           // shadow stack will drop the underlying interpreter entry trampoline
           // in which the handler runs.
@@ -2281,7 +2284,7 @@ Tagged<Object> Isolate::UnwindAndFindHandler() {
 
         // Reconstruct the stack pointer from the frame pointer.
         Address return_sp = js_frame->fp() - js_frame->GetSPToFPDelta();
-        Code code = js_frame->LookupCode();
+        Tagged<Code> code = js_frame->LookupCode();
         return FoundHandler(Context(), code->instruction_start(), 0,
                             code->constant_pool(), return_sp, frame->fp(),
                             visited_frames);
@@ -2410,7 +2413,7 @@ Isolate::CatchType Isolate::PredictExceptionCatcher() {
       }
 
       case StackFrame::STUB: {
-        base::Optional<Code> code = frame->LookupCode();
+        Tagged<Code> code = *frame->LookupCode();
         if (code->kind() != CodeKind::BUILTIN || !code->has_handler_table() ||
             !code->is_turbofanned()) {
           break;
@@ -2422,7 +2425,7 @@ Isolate::CatchType Isolate::PredictExceptionCatcher() {
       }
 
       case StackFrame::JAVA_SCRIPT_BUILTIN_CONTINUATION_WITH_CATCH: {
-        base::Optional<Code> code = frame->LookupCode();
+        Tagged<Code> code = *frame->LookupCode();
         auto prediction = ToCatchType(CatchPredictionFor(code->builtin_id()));
         if (prediction != NOT_CAUGHT) return prediction;
         break;
@@ -2892,7 +2895,7 @@ Handle<Object> Isolate::GetPromiseOnStackOnThrow() {
     if (frame->is_java_script()) {
       catch_prediction = PredictException(JavaScriptFrame::cast(frame));
     } else if (frame->type() == StackFrame::STUB) {
-      base::Optional<Code> code = frame->LookupCode();
+      Tagged<Code> code = *frame->LookupCode();
       if (code->kind() != CodeKind::BUILTIN || !code->has_handler_table() ||
           !code->is_turbofanned()) {
         continue;
@@ -3104,7 +3107,7 @@ Handle<NativeContext> Isolate::GetIncumbentContext() {
           : 0;
   if (!it.done() &&
       (!top_backup_incumbent || it.frame()->sp() < top_backup_incumbent)) {
-    Context context = Context::cast(it.frame()->context());
+    Tagged<Context> context = Context::cast(it.frame()->context());
     return Handle<NativeContext>(context->native_context(), this);
   }
 
@@ -4310,7 +4313,7 @@ void Isolate::VerifyStaticRoots() {
   for (auto idx = RootIndex::kFirstRoot; idx <= RootIndex::kLastRoot; ++idx) {
     Tagged<Object> obj = roots_table().slot(idx).load(this);
     if (obj.ptr() == kNullAddress || !IsMap(obj)) continue;
-    Map map = Map::cast(obj);
+    Tagged<Map> map = Map::cast(obj);
 
 #define INSTANCE_TYPE_CHECKER_SINGLE(type, _)  \
   CHECK_EQ(InstanceTypeChecker::Is##type(map), \
@@ -5026,8 +5029,8 @@ void Isolate::MaybeInitializeVectorListFromHeap() {
          !current_obj.is_null(); current_obj = heap_iterator.Next()) {
       if (!IsFeedbackVector(current_obj)) continue;
 
-      FeedbackVector vector = FeedbackVector::cast(current_obj);
-      SharedFunctionInfo shared = vector->shared_function_info();
+      Tagged<FeedbackVector> vector = FeedbackVector::cast(current_obj);
+      Tagged<SharedFunctionInfo> shared = vector->shared_function_info();
 
       // No need to preserve the feedback vector for non-user-visible functions.
       if (!shared->IsSubjectToDebugging()) continue;
@@ -5054,7 +5057,7 @@ Isolate::KnownPrototype Isolate::IsArrayOrObjectOrStringPrototype(
     Tagged<Object> object) {
   Tagged<Object> context = heap()->native_contexts_list();
   while (!IsUndefined(context, this)) {
-    Context current_context = Context::cast(context);
+    Tagged<Context> current_context = Context::cast(context);
     if (current_context->initial_object_prototype() == object) {
       return KnownPrototype::kObject;
     } else if (current_context->initial_array_prototype() == object) {
@@ -5071,7 +5074,7 @@ bool Isolate::IsInAnyContext(Tagged<Object> object, uint32_t index) {
   DisallowGarbageCollection no_gc;
   Tagged<Object> context = heap()->native_contexts_list();
   while (!IsUndefined(context, this)) {
-    Context current_context = Context::cast(context);
+    Tagged<Context> current_context = Context::cast(context);
     if (current_context->get(index) == object) {
       return true;
     }
@@ -5996,7 +5999,7 @@ void Isolate::CollectSourcePositionsForAllBytecodeArrays() {
     for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
          obj = iterator.Next()) {
       if (!IsSharedFunctionInfo(obj)) continue;
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(obj);
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(obj);
       // If the script is a Smi, then the SharedFunctionInfo is in
       // the process of being deserialized.
       Tagged<Object> script = sfi->raw_script(kAcquireLoad);
@@ -6223,7 +6226,7 @@ bool Isolate::RequiresCodeRange() const {
 v8::metrics::Recorder::ContextId Isolate::GetOrRegisterRecorderContextId(
     Handle<NativeContext> context) {
   if (serializer_enabled_) return v8::metrics::Recorder::ContextId::Empty();
-  i::Object id = context->recorder_context_id();
+  i::Tagged<i::Object> id = context->recorder_context_id();
   if (IsNullOrUndefined(id)) {
     CHECK_LT(last_recorder_context_id_, i::Smi::kMaxValue);
     context->set_recorder_context_id(
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index cc472d79656..8412dcb2eb8 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -519,7 +519,7 @@ using DebugObjectCache = std::vector<Handle<HeapObject>>;
   /* State for Relocatable. */                                                \
   V(Relocatable*, relocatable_top, nullptr)                                   \
   V(DebugObjectCache*, string_stream_debug_object_cache, nullptr)             \
-  V(Object, string_stream_current_security_token, Object())                   \
+  V(Tagged<Object>, string_stream_current_security_token, Tagged<Object>())   \
   V(const intptr_t*, api_external_references, nullptr)                        \
   V(AddressToIndexHashMap*, external_reference_map, nullptr)                  \
   V(HeapObjectToIndexHashMap*, root_index_map, nullptr)                       \
@@ -763,7 +763,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // Access to top context (where the current function object was created).
   Tagged<Context> context() const { return thread_local_top()->context_; }
   inline void set_context(Tagged<Context> context);
-  Context* context_address() { return &thread_local_top()->context_; }
+  Tagged<Context>* context_address() { return &thread_local_top()->context_; }
 
   // Access to current thread id.
   inline void set_thread_id(ThreadId id) {
@@ -785,7 +785,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
   bool IsCompileHintsMagicEnabled(Handle<NativeContext> context);
 
-  THREAD_LOCAL_TOP_ADDRESS(Context, pending_handler_context)
+  THREAD_LOCAL_TOP_ADDRESS(Tagged<Context>, pending_handler_context)
   THREAD_LOCAL_TOP_ADDRESS(Address, pending_handler_entrypoint)
   THREAD_LOCAL_TOP_ADDRESS(Address, pending_handler_constant_pool)
   THREAD_LOCAL_TOP_ADDRESS(Address, pending_handler_fp)
@@ -801,19 +801,19 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   THREAD_LOCAL_TOP_ADDRESS(bool, external_caught_exception)
 
   // Interface to pending exception.
-  THREAD_LOCAL_TOP_ADDRESS(Object, pending_exception)
+  THREAD_LOCAL_TOP_ADDRESS(Tagged<Object>, pending_exception)
   inline Tagged<Object> pending_exception();
   inline void set_pending_exception(Tagged<Object> exception_obj);
   inline void clear_pending_exception();
   inline bool has_pending_exception();
 
-  THREAD_LOCAL_TOP_ADDRESS(Object, pending_message)
+  THREAD_LOCAL_TOP_ADDRESS(Tagged<Object>, pending_message)
   inline void clear_pending_message();
   inline Tagged<Object> pending_message();
   inline bool has_pending_message();
   inline void set_pending_message(Tagged<Object> message_obj);
 
-  THREAD_LOCAL_TOP_ADDRESS(Object, scheduled_exception)
+  THREAD_LOCAL_TOP_ADDRESS(Tagged<Object>, scheduled_exception)
   inline Tagged<Object> scheduled_exception();
   inline bool has_scheduled_exception();
   inline void clear_scheduled_exception();
@@ -1139,7 +1139,7 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
 
 #define NATIVE_CONTEXT_FIELD_ACCESSOR(index, type, name) \
   inline Handle<type> name();                            \
-  inline bool is_##name(type value);
+  inline bool is_##name(Tagged<type> value);
   NATIVE_CONTEXT_FIELDS(NATIVE_CONTEXT_FIELD_ACCESSOR)
 #undef NATIVE_CONTEXT_FIELD_ACCESSOR
 
@@ -1751,12 +1751,14 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   // Detach the environment from its outer global object.
   void DetachGlobal(Handle<Context> env);
 
-  std::vector<Object>* startup_object_cache() { return &startup_object_cache_; }
+  std::vector<Tagged<Object>>* startup_object_cache() {
+    return &startup_object_cache_;
+  }
 
   // When there is a shared space (i.e. when this is a client Isolate), the
   // shared heap object cache holds objects in shared among Isolates. Otherwise
   // this object cache is per-Isolate like the startup object cache.
-  std::vector<Object>* shared_heap_object_cache() {
+  std::vector<Tagged<Object>>* shared_heap_object_cache() {
     if (has_shared_space()) {
       return &shared_space_isolate()->shared_heap_object_cache_;
     }
@@ -2461,13 +2463,13 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   size_t last_long_task_stats_counter_ = 0;
   v8::metrics::LongTaskStats long_task_stats_;
 
-  std::vector<Object> startup_object_cache_;
+  std::vector<Tagged<Object>> startup_object_cache_;
 
   // When sharing data among Isolates (e.g. v8_flags.shared_string_table), only
   // the shared Isolate populates this and client Isolates reference that copy.
   //
   // Otherwise this is populated for all Isolates.
-  std::vector<Object> shared_heap_object_cache_;
+  std::vector<Tagged<Object>> shared_heap_object_cache_;
 
   // Used during builtins compilation to build the builtins constants table,
   // which is stored on the root list prior to serialization.
diff --git a/src/execution/local-isolate.h b/src/execution/local-isolate.h
index b9cdec9e703..dd80a956f57 100644
--- a/src/execution/local-isolate.h
+++ b/src/execution/local-isolate.h
@@ -149,7 +149,7 @@ class V8_EXPORT_PRIVATE LocalIsolate final : private HiddenLocalFactory {
   const v8::StartupData* snapshot_blob() const {
     return isolate_->snapshot_blob();
   }
-  Object* pending_message_address() {
+  Tagged<Object>* pending_message_address() {
     return isolate_->pending_message_address();
   }
 
diff --git a/src/execution/protectors-inl.h b/src/execution/protectors-inl.h
index 92bda9827b3..0b2f98835c7 100644
--- a/src/execution/protectors-inl.h
+++ b/src/execution/protectors-inl.h
@@ -14,7 +14,7 @@ namespace internal {
 
 #define DEFINE_PROTECTOR_ON_ISOLATE_CHECK(name, root_index, unused_cell) \
   bool Protectors::Is##name##Intact(Isolate* isolate) {                  \
-    PropertyCell cell =                                                  \
+    Tagged<PropertyCell> cell =                                          \
         PropertyCell::cast(isolate->root(RootIndex::k##root_index));     \
     return IsSmi(cell->value()) &&                                       \
            Smi::ToInt(cell->value()) == kProtectorValid;                 \
diff --git a/src/execution/thread-local-top.h b/src/execution/thread-local-top.h
index 15a0f64f94e..ec05ae7d770 100644
--- a/src/execution/thread-local-top.h
+++ b/src/execution/thread-local-top.h
@@ -99,12 +99,12 @@ class ThreadLocalTop {
   // object in the getter. Same for {pending_handler_context_} below. In the
   // meantime, assert that the memory layout is the same.
   static_assert(sizeof(Context) == kSystemPointerSize);
-  Context context_;
+  Tagged<Context> context_;
   std::atomic<ThreadId> thread_id_;
-  Object pending_exception_;
+  Tagged<Object> pending_exception_ = Smi::zero();
 
   // Communication channel between Isolate::FindHandler and the CEntry.
-  Context pending_handler_context_;
+  Tagged<Context> pending_handler_context_;
   Address pending_handler_entrypoint_;
   Address pending_handler_constant_pool_;
   Address pending_handler_fp_;
@@ -114,14 +114,14 @@ class ThreadLocalTop {
   Address last_api_entry_;
 
   // Communication channel between Isolate::Throw and message consumers.
-  Object pending_message_;
+  Tagged<Object> pending_message_ = Smi::zero();
   bool rethrowing_message_;
 
   // Use a separate value for scheduled exceptions to preserve the
   // invariants that hold about pending_exception.  We may want to
   // unify them later.
   bool external_caught_exception_;
-  Object scheduled_exception_;
+  Tagged<Object> scheduled_exception_ = Smi::zero();
 
   // Stack.
   // The frame pointer of the top c entry frame.
diff --git a/src/handles/global-handles-inl.h b/src/handles/global-handles-inl.h
index 0bee8ea54a3..c8011e35f4a 100644
--- a/src/handles/global-handles-inl.h
+++ b/src/handles/global-handles-inl.h
@@ -27,8 +27,8 @@ Handle<T> GlobalHandles::Create(T value) {
 }
 
 template <typename T>
-T GlobalHandleVector<T>::Pop() {
-  T obj = T::cast(Object(locations_.back()));
+Tagged<T> GlobalHandleVector<T>::Pop() {
+  Tagged<T> obj = T::cast(Tagged<Object>(locations_.back()));
   locations_.pop_back();
   return obj;
 }
diff --git a/src/handles/global-handles.h b/src/handles/global-handles.h
index 31f91ec5553..4e96f2bb33a 100644
--- a/src/handles/global-handles.h
+++ b/src/handles/global-handles.h
@@ -253,7 +253,7 @@ class GlobalHandleVector {
     bool operator==(const Iterator& that) const { return it_ == that.it_; }
     bool operator!=(const Iterator& that) const { return it_ != that.it_; }
 
-    T raw() { return T::cast(Object(*it_)); }
+    Tagged<T> raw() { return T::cast(Tagged<Object>(*it_)); }
 
    private:
     std::vector<Address, StrongRootBlockAllocator>::iterator it_;
@@ -268,10 +268,10 @@ class GlobalHandleVector {
   size_t size() const { return locations_.size(); }
   bool empty() const { return locations_.empty(); }
 
-  void Push(T val) { locations_.push_back(val.ptr()); }
+  void Push(Tagged<T> val) { locations_.push_back(val.ptr()); }
   // Handles into the GlobalHandleVector become invalid when they are removed,
   // so "pop" returns a raw object rather than a handle.
-  inline T Pop();
+  inline Tagged<T> Pop();
 
   Iterator begin() { return Iterator(locations_.begin()); }
   Iterator end() { return Iterator(locations_.end()); }
diff --git a/src/handles/handles-inl.h b/src/handles/handles-inl.h
index 8ddb49f17b0..fb86c0d7d78 100644
--- a/src/handles/handles-inl.h
+++ b/src/handles/handles-inl.h
@@ -230,7 +230,7 @@ void HandleScope::CloseScope(Isolate* isolate, Address* prev_next,
 template <typename T>
 Handle<T> HandleScope::CloseAndEscape(Handle<T> handle_value) {
   HandleScopeData* current = isolate_->handle_scope_data();
-  T value = *handle_value;
+  Tagged<T> value = *handle_value;
   // Throw away all handles in the current scope.
   CloseScope(isolate_, prev_next_, prev_limit_);
   // Allocate one handle in the parent scope.
diff --git a/src/handles/handles.cc b/src/handles/handles.cc
index 2dd1b68c305..f27be194a3d 100644
--- a/src/handles/handles.cc
+++ b/src/handles/handles.cc
@@ -47,9 +47,9 @@ ASSERT_TRIVIALLY_COPYABLE(MaybeDirectHandle<Object>);
 
 bool HandleBase::IsDereferenceAllowed() const {
   DCHECK_NOT_NULL(location_);
-  Object object(*location_);
+  Tagged<Object> object(*location_);
   if (IsSmi(object)) return true;
-  HeapObject heap_object = HeapObject::cast(object);
+  Tagged<HeapObject> heap_object = HeapObject::cast(object);
   if (IsReadOnlyHeapObject(heap_object)) return true;
   Isolate* isolate = GetIsolateFromWritableObject(heap_object);
   RootIndex root_index;
@@ -97,9 +97,9 @@ bool HandleBase::IsDereferenceAllowed() const {
 
 bool DirectHandleBase::IsDereferenceAllowed() const {
   DCHECK_NE(obj_, kTaggedNullAddress);
-  Object object(obj_);
+  Tagged<Object> object(obj_);
   if (IsSmi(object)) return true;
-  HeapObject heap_object = HeapObject::cast(object);
+  Tagged<HeapObject> heap_object = HeapObject::cast(object);
   if (IsReadOnlyHeapObject(heap_object)) return true;
   Isolate* isolate = GetIsolateFromWritableObject(heap_object);
   if (!AllowHandleDereference::IsAllowed()) return false;
diff --git a/src/handles/handles.h b/src/handles/handles.h
index f340210c704..2e0521a5d53 100644
--- a/src/handles/handles.h
+++ b/src/handles/handles.h
@@ -168,7 +168,7 @@ class Handle final : public HandleBase {
 
   // Patches this Handle's value, in-place, with a new value. All handles with
   // the same location will see this update.
-  void PatchValue(T new_value) {
+  void PatchValue(Tagged<T> new_value) {
     SLOW_DCHECK(location_ != nullptr && IsDereferenceAllowed());
     *location_ = new_value.ptr();
   }
diff --git a/src/handles/local-handles-inl.h b/src/handles/local-handles-inl.h
index 404a922ae0b..81307c1009c 100644
--- a/src/handles/local-handles-inl.h
+++ b/src/handles/local-handles-inl.h
@@ -59,7 +59,7 @@ LocalHandleScope::~LocalHandleScope() {
 template <typename T>
 Handle<T> LocalHandleScope::CloseAndEscape(Handle<T> handle_value) {
   HandleScopeData* current;
-  T value = *handle_value;
+  Tagged<T> value = *handle_value;
   // Throw away all handles in the current scope.
   if (local_heap_->is_main_thread()) {
     current = local_heap_->heap()->isolate()->handle_scope_data();
diff --git a/src/handles/maybe-handles-inl.h b/src/handles/maybe-handles-inl.h
index 6f4fdcc2091..83f848522e8 100644
--- a/src/handles/maybe-handles-inl.h
+++ b/src/handles/maybe-handles-inl.h
@@ -14,11 +14,11 @@ namespace v8 {
 namespace internal {
 
 template <typename T>
-MaybeHandle<T>::MaybeHandle(T object, Isolate* isolate)
+MaybeHandle<T>::MaybeHandle(Tagged<T> object, Isolate* isolate)
     : MaybeHandle(handle(object, isolate)) {}
 
 template <typename T>
-MaybeHandle<T>::MaybeHandle(T object, LocalHeap* local_heap)
+MaybeHandle<T>::MaybeHandle(Tagged<T> object, LocalHeap* local_heap)
     : MaybeHandle(handle(object, local_heap)) {}
 
 MaybeObjectHandle::MaybeObjectHandle(MaybeObject object, Isolate* isolate) {
@@ -122,11 +122,11 @@ inline std::ostream& operator<<(std::ostream& os, MaybeHandle<T> handle) {
 #ifdef V8_ENABLE_DIRECT_HANDLE
 
 template <typename T>
-MaybeDirectHandle<T>::MaybeDirectHandle(T object, Isolate* isolate)
+MaybeDirectHandle<T>::MaybeDirectHandle(Tagged<T> object, Isolate* isolate)
     : MaybeDirectHandle(direct_handle(object, isolate)) {}
 
 template <typename T>
-MaybeDirectHandle<T>::MaybeDirectHandle(T object, LocalHeap* local_heap)
+MaybeDirectHandle<T>::MaybeDirectHandle(Tagged<T> object, LocalHeap* local_heap)
     : MaybeDirectHandle(direct_handle(object, local_heap)) {}
 
 template <typename T>
@@ -164,18 +164,19 @@ MaybeObjectDirectHandle::MaybeObjectDirectHandle(MaybeObject object,
 MaybeObjectDirectHandle::MaybeObjectDirectHandle(DirectHandle<Object> object)
     : reference_type_(HeapObjectReferenceType::STRONG), handle_(object) {}
 
-MaybeObjectDirectHandle::MaybeObjectDirectHandle(Object object,
+MaybeObjectDirectHandle::MaybeObjectDirectHandle(Tagged<Object> object,
                                                  Isolate* isolate)
     : reference_type_(HeapObjectReferenceType::STRONG),
       handle_(object, isolate) {}
 
-MaybeObjectDirectHandle::MaybeObjectDirectHandle(Object object,
+MaybeObjectDirectHandle::MaybeObjectDirectHandle(Tagged<Object> object,
                                                  LocalHeap* local_heap)
     : reference_type_(HeapObjectReferenceType::STRONG),
       handle_(object, local_heap) {}
 
 MaybeObjectDirectHandle::MaybeObjectDirectHandle(
-    Object object, HeapObjectReferenceType reference_type, Isolate* isolate)
+    Tagged<Object> object, HeapObjectReferenceType reference_type,
+    Isolate* isolate)
     : reference_type_(reference_type), handle_(object, isolate) {}
 
 MaybeObjectDirectHandle::MaybeObjectDirectHandle(
@@ -187,7 +188,7 @@ MaybeObjectDirectHandle MaybeObjectDirectHandle::Weak(
   return MaybeObjectDirectHandle(object, HeapObjectReferenceType::WEAK);
 }
 
-MaybeObjectDirectHandle MaybeObjectDirectHandle::Weak(Object object,
+MaybeObjectDirectHandle MaybeObjectDirectHandle::Weak(Tagged<Object> object,
                                                       Isolate* isolate) {
   return MaybeObjectDirectHandle(object, HeapObjectReferenceType::WEAK,
                                  isolate);
diff --git a/src/handles/maybe-handles.h b/src/handles/maybe-handles.h
index b3134b6d757..e57a8b72b78 100644
--- a/src/handles/maybe-handles.h
+++ b/src/handles/maybe-handles.h
@@ -45,8 +45,8 @@ class MaybeHandle final {
   V8_INLINE MaybeHandle(MaybeHandle<S> maybe_handle)
       : location_(maybe_handle.location_) {}
 
-  V8_INLINE MaybeHandle(T object, Isolate* isolate);
-  V8_INLINE MaybeHandle(T object, LocalHeap* local_heap);
+  V8_INLINE MaybeHandle(Tagged<T> object, Isolate* isolate);
+  V8_INLINE MaybeHandle(Tagged<T> object, LocalHeap* local_heap);
 
   V8_INLINE void Assert() const { DCHECK_NOT_NULL(location_); }
   V8_INLINE void Check() const { CHECK_NOT_NULL(location_); }
@@ -173,8 +173,8 @@ class MaybeDirectHandle final {
                                                     : *maybe_handle.location_) {
   }
 
-  V8_INLINE MaybeDirectHandle(T object, Isolate* isolate);
-  V8_INLINE MaybeDirectHandle(T object, LocalHeap* local_heap);
+  V8_INLINE MaybeDirectHandle(Tagged<T> object, Isolate* isolate);
+  V8_INLINE MaybeDirectHandle(Tagged<T> object, LocalHeap* local_heap);
 
   V8_INLINE void Assert() const { DCHECK_NE(location_, kTaggedNullAddress); }
   V8_INLINE void Check() const { CHECK_NE(location_, kTaggedNullAddress); }
@@ -215,12 +215,13 @@ class MaybeObjectDirectHandle {
   inline MaybeObjectDirectHandle()
       : reference_type_(HeapObjectReferenceType::STRONG) {}
   inline MaybeObjectDirectHandle(MaybeObject object, Isolate* isolate);
-  inline MaybeObjectDirectHandle(Object object, Isolate* isolate);
+  inline MaybeObjectDirectHandle(Tagged<Object> object, Isolate* isolate);
   inline MaybeObjectDirectHandle(MaybeObject object, LocalHeap* local_heap);
-  inline MaybeObjectDirectHandle(Object object, LocalHeap* local_heap);
+  inline MaybeObjectDirectHandle(Tagged<Object> object, LocalHeap* local_heap);
   inline explicit MaybeObjectDirectHandle(DirectHandle<Object> object);
 
-  static inline MaybeObjectDirectHandle Weak(Object object, Isolate* isolate);
+  static inline MaybeObjectDirectHandle Weak(Tagged<Object> object,
+                                             Isolate* isolate);
   static inline MaybeObjectDirectHandle Weak(DirectHandle<Object> object);
 
   inline MaybeObject operator*() const;
@@ -231,7 +232,7 @@ class MaybeObjectDirectHandle {
   bool is_null() const { return handle_.is_null(); }
 
  private:
-  inline MaybeObjectDirectHandle(Object object,
+  inline MaybeObjectDirectHandle(Tagged<Object> object,
                                  HeapObjectReferenceType reference_type,
                                  Isolate* isolate);
   inline MaybeObjectDirectHandle(DirectHandle<Object> object,
diff --git a/src/heap/allocation-result.h b/src/heap/allocation-result.h
index e8fcd9dce24..160782be1ba 100644
--- a/src/heap/allocation-result.h
+++ b/src/heap/allocation-result.h
@@ -39,7 +39,7 @@ class AllocationResult final {
   bool IsFailure() const { return object_.is_null(); }
 
   template <typename T>
-  bool To(T* obj) const {
+  bool To(Tagged<T>* obj) const {
     if (IsFailure()) return false;
     *obj = T::cast(object_);
     return true;
@@ -64,7 +64,7 @@ class AllocationResult final {
   explicit AllocationResult(Tagged<HeapObject> heap_object)
       : object_(heap_object) {}
 
-  HeapObject object_;
+  Tagged<HeapObject> object_;
 };
 
 static_assert(sizeof(AllocationResult) == kSystemPointerSize);
diff --git a/src/heap/code-stats.cc b/src/heap/code-stats.cc
index f016c695f0e..098fdf03f44 100644
--- a/src/heap/code-stats.cc
+++ b/src/heap/code-stats.cc
@@ -67,7 +67,8 @@ void CodeStatistics::ResetCodeAndMetadataStatistics(Isolate* isolate) {
 void CodeStatistics::CollectCodeStatistics(PagedSpace* space,
                                            Isolate* isolate) {
   PagedSpaceObjectIterator obj_it(isolate->heap(), space);
-  for (HeapObject obj = obj_it.Next(); !obj.is_null(); obj = obj_it.Next()) {
+  for (Tagged<HeapObject> obj = obj_it.Next(); !obj.is_null();
+       obj = obj_it.Next()) {
     RecordCodeAndMetadataStatistics(obj, isolate);
   }
 }
@@ -79,7 +80,8 @@ void CodeStatistics::CollectCodeStatistics(PagedSpace* space,
 void CodeStatistics::CollectCodeStatistics(OldLargeObjectSpace* space,
                                            Isolate* isolate) {
   LargeObjectSpaceObjectIterator obj_it(space);
-  for (HeapObject obj = obj_it.Next(); !obj.is_null(); obj = obj_it.Next()) {
+  for (Tagged<HeapObject> obj = obj_it.Next(); !obj.is_null();
+       obj = obj_it.Next()) {
     RecordCodeAndMetadataStatistics(obj, isolate);
   }
 }
@@ -204,7 +206,7 @@ void CodeStatistics::CollectCodeCommentStatistics(Tagged<AbstractCode> obj,
   PtrComprCageBase cage_base{isolate};
   if (!IsCode(obj, cage_base)) return;
 
-  Code code = Code::cast(obj);
+  Tagged<Code> code = Code::cast(obj);
 
   // Off-heap builtins might contain comments but they are a part of binary so
   // it doesn't make sense to account them in the stats.
diff --git a/src/heap/conservative-stack-visitor.cc b/src/heap/conservative-stack-visitor.cc
index c50530ff1a6..e20ac4aae1b 100644
--- a/src/heap/conservative-stack-visitor.cc
+++ b/src/heap/conservative-stack-visitor.cc
@@ -41,7 +41,7 @@ Address ConservativeStackVisitor::FindBasePtr(Address maybe_inner_ptr) const {
   if (chunk->IsLargePage()) {
     // This could be simplified if we could guarantee that there are no free
     // space or filler objects in large pages. A few cctests violate this now.
-    HeapObject obj(static_cast<const LargePage*>(chunk)->GetObject());
+    Tagged<HeapObject> obj(static_cast<const LargePage*>(chunk)->GetObject());
     PtrComprCageBase cage_base{chunk->heap()->isolate()};
     return IsFreeSpaceOrFiller(obj, cage_base) ? kNullAddress : obj.address();
   }
@@ -63,8 +63,8 @@ Address ConservativeStackVisitor::FindBasePtr(Address maybe_inner_ptr) const {
   DCHECK_LE(base_ptr, maybe_inner_ptr);
   PtrComprCageBase cage_base{page->heap()->isolate()};
   while (true) {
-    HeapObject obj(HeapObject::FromAddress(base_ptr));
-    const int size = obj.Size(cage_base);
+    Tagged<HeapObject> obj(HeapObject::FromAddress(base_ptr));
+    const int size = obj->Size(cage_base);
     DCHECK_LT(0, size);
     if (maybe_inner_ptr < base_ptr + size)
       return IsFreeSpaceOrFiller(obj, cage_base) ? kNullAddress : base_ptr;
@@ -104,8 +104,8 @@ void ConservativeStackVisitor::VisitConservativelyIfPointer(Address address) {
   // Proceed with inner-pointer resolution.
   Address base_ptr = FindBasePtr(address);
   if (base_ptr == kNullAddress) return;
-  HeapObject obj = HeapObject::FromAddress(base_ptr);
-  Object root = obj;
+  Tagged<HeapObject> obj = HeapObject::FromAddress(base_ptr);
+  Tagged<Object> root = obj;
   DCHECK_NOT_NULL(delegate_);
   delegate_->VisitRootPointer(Root::kStackRoots, nullptr,
                               FullObjectSlot(&root));
diff --git a/src/heap/factory-base.cc b/src/heap/factory-base.cc
index e8146d99405..ba66e5a8a76 100644
--- a/src/heap/factory-base.cc
+++ b/src/heap/factory-base.cc
@@ -743,13 +743,13 @@ MaybeHandle<SeqStringT> FactoryBase<Impl>::NewRawStringWithMap(
   int size = SeqStringT::SizeFor(length);
   DCHECK_GE(SeqStringT::kMaxSize, size);
 
-  SeqStringT string =
+  Tagged<SeqStringT> string =
       SeqStringT::cast(AllocateRawWithImmortalMap(size, allocation, map));
   DisallowGarbageCollection no_gc;
-  string.clear_padding_destructively(length);
-  string.set_length(length);
-  string.set_raw_hash_field(String::kEmptyHashField);
-  DCHECK_EQ(size, string.Size());
+  string->clear_padding_destructively(length);
+  string->set_length(length);
+  string->set_raw_hash_field(String::kEmptyHashField);
+  DCHECK_EQ(size, string->Size());
   return handle(string, isolate());
 }
 
diff --git a/src/heap/free-list.h b/src/heap/free-list.h
index 374ee3a2ba1..3a43d0e1d10 100644
--- a/src/heap/free-list.h
+++ b/src/heap/free-list.h
@@ -111,7 +111,7 @@ class FreeListCategory {
   uint32_t available_ = 0;
 
   // |top_|: Points to the top FreeSpace in the free list category.
-  FreeSpace top_;
+  Tagged<FreeSpace> top_;
 
   FreeListCategory* prev_ = nullptr;
   FreeListCategory* next_ = nullptr;
diff --git a/src/heap/heap-verifier.cc b/src/heap/heap-verifier.cc
index 78834f07593..d666d729aae 100644
--- a/src/heap/heap-verifier.cc
+++ b/src/heap/heap-verifier.cc
@@ -105,7 +105,7 @@ void VerifyPointersVisitor::VisitPointers(Tagged<HeapObject> host,
 
 void VerifyPointersVisitor::VisitInstructionStreamPointer(
     Tagged<Code> host, InstructionStreamSlot slot) {
-  Object maybe_code = slot.load(code_cage_base());
+  Tagged<Object> maybe_code = slot.load(code_cage_base());
   Tagged<HeapObject> code;
   // The slot might contain smi during Code creation.
   if (maybe_code.GetHeapObject(&code)) {
@@ -220,7 +220,7 @@ class VerifySharedHeapObjectVisitor : public VerifyPointersVisitor {
   void VerifyPointers(Tagged<HeapObject> host, MaybeObjectSlot start,
                       MaybeObjectSlot end) override {
     if (!host.is_null()) {
-      Map map = host->map();
+      Tagged<Map> map = host->map();
       CHECK(ReadOnlyHeap::Contains(map) || shared_space_->Contains(map));
     }
     VerifyPointersVisitor::VerifyPointers(host, start, end);
@@ -308,7 +308,7 @@ void HeapVerification::Verify() {
 
   if (!isolate()->context().is_null() &&
       !isolate()->raw_native_context().is_null()) {
-    Object normalized_map_cache =
+    Tagged<Object> normalized_map_cache =
         isolate()->raw_native_context()->normalized_map_cache();
 
     if (IsNormalizedMapCache(normalized_map_cache)) {
@@ -413,7 +413,7 @@ void HeapVerification::VerifyOutgoingPointers(Tagged<HeapObject> object) {
 void HeapVerification::VerifyObjectMap(Tagged<HeapObject> object) {
   // The first word should be a map, and we expect all map pointers to be
   // in map space or read-only space.
-  Map map = object->map(cage_base_);
+  Tagged<Map> map = object->map(cage_base_);
   CHECK(IsMap(map, cage_base_));
   CHECK(ReadOnlyHeap::Contains(map) || old_space()->Contains(map) ||
         (shared_space() && shared_space()->Contains(map)));
@@ -446,7 +446,7 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
                      ObjectSlot end) override {
 #ifdef DEBUG
     for (ObjectSlot slot = start; slot < end; ++slot) {
-      Object obj = slot.load(cage_base());
+      Tagged<Object> obj = slot.load(cage_base());
       CHECK(!MapWord::IsPacked(obj.ptr()) || !HasWeakHeapObjectTag(obj));
     }
 #endif  // DEBUG
@@ -472,7 +472,7 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
 
   void VisitCodeTarget(Tagged<InstructionStream> host,
                        RelocInfo* rinfo) override {
-    Object target =
+    Tagged<Object> target =
         InstructionStream::FromTargetAddress(rinfo->target_address());
     if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kCodeEntry, rinfo->pc()) ||
@@ -484,7 +484,7 @@ class SlotVerifyingVisitor : public ObjectVisitorWithCageBases {
 
   void VisitEmbeddedPointer(Tagged<InstructionStream> host,
                             RelocInfo* rinfo) override {
-    Object target = rinfo->target_object(cage_base());
+    Tagged<Object> target = rinfo->target_object(cage_base());
     if (ShouldHaveBeenRecorded(host, MaybeObject::FromObject(target))) {
       CHECK(InTypedSet(SlotType::kEmbeddedObjectFull, rinfo->pc()) ||
             InTypedSet(SlotType::kEmbeddedObjectCompressed, rinfo->pc()) ||
@@ -533,9 +533,9 @@ class OldToNewSlotVerifyingVisitor : public SlotVerifyingVisitor {
     if (v8_flags.minor_ms) return;
     // Keys are handled separately and should never appear in this set.
     CHECK(!InUntypedSet(key));
-    Object k = *key;
+    Tagged<Object> k = *key;
     if (!ObjectInYoungGeneration(host) && ObjectInYoungGeneration(k)) {
-      EphemeronHashTable table = EphemeronHashTable::cast(host);
+      Tagged<EphemeronHashTable> table = EphemeronHashTable::cast(host);
       auto it = ephemeron_remembered_set_->find(table);
       CHECK(it != ephemeron_remembered_set_->end());
       int slot_index =
diff --git a/src/heap/heap-verifier.h b/src/heap/heap-verifier.h
index 08ebfc9ddf3..f2b27ff7fac 100644
--- a/src/heap/heap-verifier.h
+++ b/src/heap/heap-verifier.h
@@ -66,13 +66,13 @@ class HeapVerifier final {
   static void VerifyHeap(Heap* heap) {}
   static void VerifyReadOnlyHeap(Heap* heap) {}
   static void VerifySharedHeap(Heap* heap, Isolate* initiator) {}
-  static void VerifyRememberedSetFor(Heap* heap, HeapObject object) {}
-  static void VerifySafeMapTransition(Heap* heap, HeapObject object,
-                                      Map new_map) {}
-  static void VerifyObjectLayoutChange(Heap* heap, HeapObject object,
-                                       Map new_map) {}
-  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap, HeapObject object) {
-  }
+  static void VerifyRememberedSetFor(Heap* heap, Tagged<HeapObject> object) {}
+  static void VerifySafeMapTransition(Heap* heap, Tagged<HeapObject> object,
+                                      Tagged<Map> new_map) {}
+  static void VerifyObjectLayoutChange(Heap* heap, Tagged<HeapObject> object,
+                                       Tagged<Map> new_map) {}
+  static void VerifyObjectLayoutChangeIsAllowed(Heap* heap,
+                                                Tagged<HeapObject> object) {}
 #endif
 
   V8_INLINE static void VerifyHeapIfEnabled(Heap* heap) {
diff --git a/src/heap/heap.cc b/src/heap/heap.cc
index bb85d5bd8d6..3e824ea3e52 100644
--- a/src/heap/heap.cc
+++ b/src/heap/heap.cc
@@ -1047,11 +1047,12 @@ void Heap::PrintRetainingPath(Tagged<HeapObject> target,
   PrintF("-------------------------------------------------\n");
 }
 
-void UpdateRetainersMapAfterScavenge(UnorderedHeapObjectMap<HeapObject>* map) {
+void UpdateRetainersMapAfterScavenge(
+    UnorderedHeapObjectMap<Tagged<HeapObject>>* map) {
   // This is only used for Scavenger.
   DCHECK(!v8_flags.minor_ms);
 
-  UnorderedHeapObjectMap<HeapObject> updated_map;
+  UnorderedHeapObjectMap<Tagged<HeapObject>> updated_map;
 
   for (auto pair : *map) {
     Tagged<HeapObject> object = pair.first;
@@ -1534,7 +1535,7 @@ intptr_t CompareWords(int size, Tagged<HeapObject> a, Tagged<HeapObject> b) {
   return 0;
 }
 
-void ReportDuplicates(int size, std::vector<HeapObject>* objects) {
+void ReportDuplicates(int size, std::vector<Tagged<HeapObject>>* objects) {
   if (objects->size() == 0) return;
 
   sort(objects->begin(), objects->end(),
@@ -1544,7 +1545,7 @@ void ReportDuplicates(int size, std::vector<HeapObject>* objects) {
          return a < b;
        });
 
-  std::vector<std::pair<int, HeapObject>> duplicates;
+  std::vector<std::pair<int, Tagged<HeapObject>>> duplicates;
   Tagged<HeapObject> current = (*objects)[0];
   int count = 1;
   for (size_t i = 1; i < objects->size(); i++) {
@@ -1626,7 +1627,7 @@ void Heap::CollectAllAvailableGarbage(GarbageCollectionReason gc_reason) {
   EagerlyFreeExternalMemory();
 
   if (v8_flags.trace_duplicate_threshold_kb) {
-    std::map<int, std::vector<HeapObject>> objects_by_size;
+    std::map<int, std::vector<Tagged<HeapObject>>> objects_by_size;
     PagedSpaceIterator spaces(this);
     for (PagedSpace* space = spaces.Next(); space != nullptr;
          space = spaces.Next()) {
@@ -6225,7 +6226,7 @@ class UnreachableObjectsFilter : public HeapObjectsFilter {
   }
 
  private:
-  using BucketType = std::unordered_set<HeapObject, Object::Hasher>;
+  using BucketType = std::unordered_set<Tagged<HeapObject>, Object::Hasher>;
 
   bool MarkAsReachable(Tagged<HeapObject> object) {
     // If the bucket corresponding to the object's chunk does not exist, then
@@ -6713,8 +6714,8 @@ std::vector<Handle<NativeContext>> Heap::FindAllNativeContexts() {
   return result;
 }
 
-std::vector<WeakArrayList> Heap::FindAllRetainedMaps() {
-  std::vector<WeakArrayList> result;
+std::vector<Tagged<WeakArrayList>> Heap::FindAllRetainedMaps() {
+  std::vector<Tagged<WeakArrayList>> result;
   Tagged<Object> context = native_contexts_list();
   while (!IsUndefined(context, isolate())) {
     Tagged<NativeContext> native_context = Tagged<NativeContext>::cast(context);
@@ -6816,7 +6817,7 @@ bool Heap::GcSafeInstructionStreamContains(Tagged<InstructionStream> istream,
   return start <= addr && addr < end;
 }
 
-base::Optional<InstructionStream>
+base::Optional<Tagged<InstructionStream>>
 Heap::GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer) {
   if (V8_ENABLE_THIRD_PARTY_HEAP_BOOL) {
     Address start = tp_heap_->GetObjectFromInnerPointer(inner_pointer);
@@ -6832,7 +6833,7 @@ Heap::GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer) {
   return {};
 }
 
-base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
+base::Optional<Tagged<GcSafeCode>> Heap::GcSafeTryFindCodeForInnerPointer(
     Address inner_pointer) {
   Builtin maybe_builtin =
       OffHeapInstructionStream::TryLookupCode(isolate(), inner_pointer);
@@ -6840,7 +6841,7 @@ base::Optional<GcSafeCode> Heap::GcSafeTryFindCodeForInnerPointer(
     return GcSafeCode::cast(isolate()->builtins()->code(maybe_builtin));
   }
 
-  base::Optional<InstructionStream> maybe_istream =
+  base::Optional<Tagged<InstructionStream>> maybe_istream =
       GcSafeTryFindInstructionStreamForInnerPointer(inner_pointer);
   if (!maybe_istream) return {};
 
@@ -6852,22 +6853,22 @@ Tagged<Code> Heap::FindCodeForInnerPointer(Address inner_pointer) {
 }
 
 Tagged<GcSafeCode> Heap::GcSafeFindCodeForInnerPointer(Address inner_pointer) {
-  base::Optional<GcSafeCode> maybe_code =
+  base::Optional<Tagged<GcSafeCode>> maybe_code =
       GcSafeTryFindCodeForInnerPointer(inner_pointer);
   // Callers expect that the code object is found.
   CHECK(maybe_code.has_value());
   return GcSafeCode::unchecked_cast(maybe_code.value());
 }
 
-base::Optional<Code> Heap::TryFindCodeForInnerPointerForPrinting(
+base::Optional<Tagged<Code>> Heap::TryFindCodeForInnerPointerForPrinting(
     Address inner_pointer) {
   if (InSpaceSlow(inner_pointer, i::CODE_SPACE) ||
       InSpaceSlow(inner_pointer, i::CODE_LO_SPACE) ||
       i::OffHeapInstructionStream::PcIsOffHeap(isolate(), inner_pointer)) {
-    base::Optional<GcSafeCode> maybe_code =
+    base::Optional<Tagged<GcSafeCode>> maybe_code =
         GcSafeTryFindCodeForInnerPointer(inner_pointer);
     if (maybe_code.has_value()) {
-      return maybe_code->UnsafeCastToCode();
+      return maybe_code.value()->UnsafeCastToCode();
     }
   }
   return {};
diff --git a/src/heap/heap.h b/src/heap/heap.h
index 7da06e6048e..3cf0e3bd00d 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -180,11 +180,12 @@ struct CommentStatistic {
 };
 #endif
 
-// An alias for std::unordered_map<HeapObject, T> which also sets proper
-// Hash and KeyEqual functions.
+// An alias for std::unordered_map<Tagged<HeapObject>, T> which also
+// sets proper Hash and KeyEqual functions.
 template <typename T>
 using UnorderedHeapObjectMap =
-    std::unordered_map<HeapObject, T, Object::Hasher, Object::KeyEqualSafe>;
+    std::unordered_map<Tagged<HeapObject>, T, Object::Hasher,
+                       Object::KeyEqualSafe>;
 
 enum class GCFlag : uint8_t {
   kNoFlags = 0,
@@ -1484,12 +1485,12 @@ class Heap final {
   V8_EXPORT_PRIVATE Tagged<Code> FindCodeForInnerPointer(Address inner_pointer);
   // Use the GcSafe family of functions if called while GC is in progress.
   Tagged<GcSafeCode> GcSafeFindCodeForInnerPointer(Address inner_pointer);
-  base::Optional<GcSafeCode> GcSafeTryFindCodeForInnerPointer(
+  base::Optional<Tagged<GcSafeCode>> GcSafeTryFindCodeForInnerPointer(
       Address inner_pointer);
-  base::Optional<InstructionStream>
+  base::Optional<Tagged<InstructionStream>>
   GcSafeTryFindInstructionStreamForInnerPointer(Address inner_pointer);
   // Only intended for use from the `jco` gdb macro.
-  base::Optional<Code> TryFindCodeForInnerPointerForPrinting(
+  base::Optional<Tagged<Code>> TryFindCodeForInnerPointerForPrinting(
       Address inner_pointer);
 
   // Returns true if {addr} is contained within {instruction_stream} and false
@@ -2012,7 +2013,7 @@ class Heap final {
 #endif  // DEBUG
 
   std::vector<Handle<NativeContext>> FindAllNativeContexts();
-  std::vector<WeakArrayList> FindAllRetainedMaps();
+  std::vector<Tagged<WeakArrayList>> FindAllRetainedMaps();
   MemoryMeasurement* memory_measurement() { return memory_measurement_.get(); }
 
   AllocationType allocation_type_for_in_place_internalizable_strings() const {
@@ -2315,11 +2316,11 @@ class Heap final {
   bool force_gc_on_next_allocation_ = false;
   bool delay_sweeper_tasks_for_testing_ = false;
 
-  UnorderedHeapObjectMap<HeapObject> retainer_;
+  UnorderedHeapObjectMap<Tagged<HeapObject>> retainer_;
   UnorderedHeapObjectMap<Root> retaining_root_;
   // If an object is retained by an ephemeron, then the retaining key of the
   // ephemeron is stored in this map.
-  UnorderedHeapObjectMap<HeapObject> ephemeron_retainer_;
+  UnorderedHeapObjectMap<Tagged<HeapObject>> ephemeron_retainer_;
   // For each index in the retaining_path_targets_ array this map
   // stores the option of the corresponding target.
   std::unordered_map<int, RetainingPathOption> retaining_path_target_option_;
diff --git a/src/heap/incremental-marking.cc b/src/heap/incremental-marking.cc
index f078f63b7a3..75af6087fa5 100644
--- a/src/heap/incremental-marking.cc
+++ b/src/heap/incremental-marking.cc
@@ -484,7 +484,7 @@ void IncrementalMarking::UpdateMarkingWorklistAfterScavenge() {
   // Minor MS never runs during incremental marking.
   DCHECK(!v8_flags.minor_ms);
 
-  Map filler_map = ReadOnlyRoots(heap_).one_pointer_filler_map();
+  Tagged<Map> filler_map = ReadOnlyRoots(heap_).one_pointer_filler_map();
 
   MarkingState* marking_state = heap()->marking_state();
 
@@ -515,7 +515,7 @@ void IncrementalMarking::UpdateMarkingWorklistAfterScavenge() {
       // other pointer into the old space and we won't encounter them here in
       // this code path.
       DCHECK(!Heap::IsLargeObject(obj));
-      HeapObject dest = map_word.ToForwardingAddress(obj);
+      Tagged<HeapObject> dest = map_word.ToForwardingAddress(obj);
       DCHECK_IMPLIES(marking_state->IsUnmarked(obj), IsFreeSpaceOrFiller(obj));
       if (dest.InWritableSharedSpace() &&
           !isolate()->is_shared_space_isolate()) {
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index 189e023cc14..0b136f56c93 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -1635,9 +1635,9 @@ class EvacuateNewSpaceVisitor final : public EvacuateVisitorBase {
     return false;
   }
 
-  inline AllocationSpace AllocateTargetObject(Tagged<HeapObject> old_object,
-                                              int size,
-                                              HeapObject* target_object) {
+  inline AllocationSpace AllocateTargetObject(
+      Tagged<HeapObject> old_object, int size,
+      Tagged<HeapObject>* target_object) {
     AllocationAlignment alignment =
         HeapObject::RequiredAlignment(old_object->map());
     AllocationSpace space_allocated_in = NEW_SPACE;
@@ -2044,7 +2044,9 @@ void MarkCompactCollector::MarkTransitiveClosureLinear() {
            GCTracer::Scope::MC_MARK_WEAK_CLOSURE_EPHEMERON_LINEAR);
   // This phase doesn't support parallel marking.
   DCHECK(heap_->concurrent_marking()->IsStopped());
-  std::unordered_multimap<HeapObject, HeapObject, Object::Hasher> key_to_values;
+  std::unordered_multimap<Tagged<HeapObject>, Tagged<HeapObject>,
+                          Object::Hasher>
+      key_to_values;
   Ephemeron ephemeron;
 
   DCHECK(
diff --git a/src/heap/mark-sweep-utilities.cc b/src/heap/mark-sweep-utilities.cc
index 4ada3534fc0..53f03fa158e 100644
--- a/src/heap/mark-sweep-utilities.cc
+++ b/src/heap/mark-sweep-utilities.cc
@@ -43,7 +43,7 @@ void MarkingVerifierBase::VerifyMarkingOnPage(const Page* page, Address start,
     if (current >= end) break;
     CHECK(IsMarked(object));
     CHECK(current >= next_object_must_be_here_or_later);
-    object.Iterate(cage_base(), this);
+    object->Iterate(cage_base(), this);
     next_object_must_be_here_or_later = current + size;
     // The object is either part of a black area of black allocation or a
     // regular black object
diff --git a/src/heap/marking-visitor.h b/src/heap/marking-visitor.h
index 7cead5f2830..1c6c2b00b6f 100644
--- a/src/heap/marking-visitor.h
+++ b/src/heap/marking-visitor.h
@@ -20,7 +20,7 @@ namespace v8 {
 namespace internal {
 
 struct EphemeronMarking {
-  std::vector<HeapObject> newly_discovered;
+  std::vector<Tagged<HeapObject>> newly_discovered;
   bool newly_discovered_overflowed;
   size_t newly_discovered_limit;
 };
diff --git a/src/heap/marking.h b/src/heap/marking.h
index e3c66972777..e6824a92c19 100644
--- a/src/heap/marking.h
+++ b/src/heap/marking.h
@@ -232,7 +232,7 @@ class LiveObjectRange final {
  public:
   class iterator final {
    public:
-    using value_type = std::pair<HeapObject, int /* size */>;
+    using value_type = std::pair<Tagged<HeapObject>, int /* size */>;
     using pointer = const value_type*;
     using reference = const value_type&;
     using iterator_category = std::forward_iterator_tag;
@@ -261,8 +261,8 @@ class LiveObjectRange final {
     const PtrComprCageBase cage_base_;
     MarkingBitmap::CellIndex current_cell_index_ = 0;
     MarkingBitmap::CellType current_cell_ = 0;
-    HeapObject current_object_;
-    Map current_map_;
+    Tagged<HeapObject> current_object_;
+    Tagged<Map> current_map_;
     int current_size_ = 0;
   };
 
diff --git a/src/heap/object-lock.h b/src/heap/object-lock.h
index 4da0dd491ad..2eeffd16f8e 100644
--- a/src/heap/object-lock.h
+++ b/src/heap/object-lock.h
@@ -39,7 +39,7 @@ class ObjectLockGuard final {
   ~ObjectLockGuard() { LockType::Unlock(raw_object_); }
 
  private:
-  HeapObject raw_object_;
+  Tagged<HeapObject> raw_object_;
 };
 
 using ExclusiveObjectLockGuard = ObjectLockGuard<ExclusiveObjectLock>;
diff --git a/src/heap/object-stats.cc b/src/heap/object-stats.cc
index e832b381c81..1434e0f4f1f 100644
--- a/src/heap/object-stats.cc
+++ b/src/heap/object-stats.cc
@@ -124,7 +124,7 @@ class FieldStatsCollector : public ObjectVisitorWithCageBases {
     unsigned embedded_fields_count_ : kDescriptorIndexBitCount;
     unsigned smi_fields_count_ : kDescriptorIndexBitCount;
   };
-  std::unordered_map<Map, JSObjectFieldStats, Object::Hasher>
+  std::unordered_map<Tagged<Map>, JSObjectFieldStats, Object::Hasher>
       field_stats_cache_;
 
   JSObjectFieldStats GetInobjectFieldStats(Tagged<Map> map);
@@ -457,7 +457,7 @@ class ObjectStatsCollectorImpl {
   Heap* const heap_;
   ObjectStats* const stats_;
   NonAtomicMarkingState* const marking_state_;
-  std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>
+  std::unordered_set<Tagged<HeapObject>, Object::Hasher, Object::KeyEqualSafe>
       virtual_objects_;
   std::unordered_set<Address> external_resources_;
   FieldStatsCollector field_stats_collector_;
diff --git a/src/heap/objects-visiting-inl.h b/src/heap/objects-visiting-inl.h
index 87de1218134..2fd8e2330b4 100644
--- a/src/heap/objects-visiting-inl.h
+++ b/src/heap/objects-visiting-inl.h
@@ -173,7 +173,7 @@ void HeapVisitor<ResultType, ConcreteVisitor>::VisitMapPointerIfNeeded(
 #define VISIT(TypeName)                                                      \
   template <typename ResultType, typename ConcreteVisitor>                   \
   ResultType HeapVisitor<ResultType, ConcreteVisitor>::Visit##TypeName(      \
-      Map map, TypeName object) {                                            \
+      Tagged<Map> map, Tagged<TypeName> object) {                            \
     ConcreteVisitor* visitor = static_cast<ConcreteVisitor*>(this);          \
     /* If you see the following DCHECK fail, then the size computation of    \
      * BodyDescriptor doesn't match the size return via obj.Size(). This is  \
diff --git a/src/heap/objects-visiting.cc b/src/heap/objects-visiting.cc
index 08655597634..b283d1310a6 100644
--- a/src/heap/objects-visiting.cc
+++ b/src/heap/objects-visiting.cc
@@ -29,12 +29,12 @@ Tagged<Object> VisitWeakList(Heap* heap, Tagged<Object> list,
                              WeakObjectRetainer* retainer) {
   Tagged<HeapObject> undefined = ReadOnlyRoots(heap).undefined_value();
   Tagged<Object> head = undefined;
-  T tail;
+  Tagged<T> tail;
   bool record_slots = MustRecordSlots(heap);
 
   while (list != undefined) {
     // Check whether to keep the candidate in the list.
-    T candidate = T::cast(list);
+    Tagged<T> candidate = T::cast(list);
 
     Tagged<Object> retained = retainer->RetainAs(list);
 
@@ -80,7 +80,7 @@ template <class T>
 static void ClearWeakList(Heap* heap, Tagged<Object> list) {
   Tagged<Object> undefined = ReadOnlyRoots(heap).undefined_value();
   while (list != undefined) {
-    T candidate = T::cast(list);
+    Tagged<T> candidate = T::cast(list);
     list = WeakListVisitor<T>::WeakNext(candidate);
     WeakListVisitor<T>::SetWeakNext(candidate, undefined);
   }
diff --git a/src/heap/objects-visiting.h b/src/heap/objects-visiting.h
index a81fadb7ce9..1a11c6475dc 100644
--- a/src/heap/objects-visiting.h
+++ b/src/heap/objects-visiting.h
@@ -133,8 +133,9 @@ class HeapVisitor : public ObjectVisitorWithCageBases {
     return static_cast<const ConcreteVisitor*>(this);
   }
 
-#define VISIT(TypeName) \
-  V8_INLINE ResultType Visit##TypeName(Map map, TypeName object);
+#define VISIT(TypeName)                                 \
+  V8_INLINE ResultType Visit##TypeName(Tagged<Map> map, \
+                                       Tagged<TypeName> object);
   TYPED_VISITOR_ID_LIST(VISIT)
   TORQUE_VISITOR_ID_LIST(VISIT)
 #undef VISIT
diff --git a/src/heap/paged-spaces.cc b/src/heap/paged-spaces.cc
index b3394dcec84..4cd59f16e32 100644
--- a/src/heap/paged-spaces.cc
+++ b/src/heap/paged-spaces.cc
@@ -751,7 +751,7 @@ void PagedSpaceBase::VerifyCountersAfterSweeping(Heap* heap) const {
     DCHECK(page->SweepingDone());
     total_capacity += page->area_size();
     size_t real_allocated = 0;
-    for (HeapObject object : HeapObjectRange(page)) {
+    for (Tagged<HeapObject> object : HeapObjectRange(page)) {
       if (!IsFreeSpaceOrFiller(object)) {
         real_allocated +=
             ALIGN_TO_ALLOCATION_ALIGNMENT(object->Size(cage_base));
diff --git a/src/heap/paged-spaces.h b/src/heap/paged-spaces.h
index a7901aab660..475435d454d 100644
--- a/src/heap/paged-spaces.h
+++ b/src/heap/paged-spaces.h
@@ -39,7 +39,7 @@ class HeapObjectRange final {
  public:
   class iterator final {
    public:
-    using value_type = HeapObject;
+    using value_type = Tagged<HeapObject>;
     using pointer = const value_type*;
     using reference = const value_type&;
     using iterator_category = std::forward_iterator_tag;
diff --git a/src/heap/pretenuring-handler-inl.h b/src/heap/pretenuring-handler-inl.h
index 5ed6522fd76..1c51e90f8c1 100644
--- a/src/heap/pretenuring-handler-inl.h
+++ b/src/heap/pretenuring-handler-inl.h
@@ -39,7 +39,8 @@ void PretenuringHandler::UpdateAllocationSite(
   // to dereference the allocation site and rather have to postpone all checks
   // till actually merging the data.
   Address key = memento_candidate->GetAllocationSiteUnchecked();
-  (*pretenuring_feedback)[AllocationSite::unchecked_cast(Object(key))]++;
+  (*pretenuring_feedback)[AllocationSite::unchecked_cast(
+      Tagged<Object>(key))]++;
 }
 
 template <PretenuringHandler::FindMementoMode mode>
diff --git a/src/heap/pretenuring-handler.h b/src/heap/pretenuring-handler.h
index 87a1d4d01ff..4c1ef61cc30 100644
--- a/src/heap/pretenuring-handler.h
+++ b/src/heap/pretenuring-handler.h
@@ -23,7 +23,7 @@ class PretenuringHandler final {
   static constexpr int kInitialFeedbackCapacity = 256;
 
   using PretenuringFeedbackMap =
-      std::unordered_map<AllocationSite, size_t, Object::Hasher>;
+      std::unordered_map<Tagged<AllocationSite>, size_t, Object::Hasher>;
   enum FindMementoMode { kForRuntime, kForGC };
 
   explicit PretenuringHandler(Heap* heap);
diff --git a/src/heap/read-only-promotion.cc b/src/heap/read-only-promotion.cc
index c9e373d49f1..da5d20cf96e 100644
--- a/src/heap/read-only-promotion.cc
+++ b/src/heap/read-only-promotion.cc
@@ -18,8 +18,8 @@ namespace internal {
 namespace {
 
 // Convenience aliases:
-using HeapObjectSet =
-    std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>;
+using HeapObjectSet = std::unordered_set<Tagged<HeapObject>, Object::Hasher,
+                                         Object::KeyEqualSafe>;
 using HeapObjectMap = std::unordered_map<Tagged<HeapObject>, Tagged<HeapObject>,
                                          Object::Hasher, Object::KeyEqualSafe>;
 bool Contains(const HeapObjectSet& s, Tagged<HeapObject> o) {
@@ -31,7 +31,7 @@ bool Contains(const HeapObjectMap& s, Tagged<HeapObject> o) {
 
 class Committee final {
  public:
-  static std::vector<HeapObject> DeterminePromotees(
+  static std::vector<Tagged<HeapObject>> DeterminePromotees(
       Isolate* isolate, const DisallowGarbageCollection& no_gc,
       const SafepointScope& safepoint_scope) {
     return Committee(isolate).DeterminePromotees(safepoint_scope);
@@ -40,7 +40,7 @@ class Committee final {
  private:
   explicit Committee(Isolate* isolate) : isolate_(isolate) {}
 
-  std::vector<HeapObject> DeterminePromotees(
+  std::vector<Tagged<HeapObject>> DeterminePromotees(
       const SafepointScope& safepoint_scope) {
     DCHECK(promo_accepted_.empty());
     DCHECK(promo_rejected_.empty());
@@ -71,8 +71,8 @@ class Committee final {
     // Return promotees as a sorted list. Note that sorting uses object
     // addresses; the list order is deterministic only if heap layout
     // itself is deterministic (see v8_flags.predictable).
-    std::vector<HeapObject> promotees{promo_accepted_.begin(),
-                                      promo_accepted_.end()};
+    std::vector<Tagged<HeapObject>> promotees{promo_accepted_.begin(),
+                                              promo_accepted_.end()};
     std::sort(promotees.begin(), promotees.end(), Object::Comparer());
 
     return promotees;
@@ -139,8 +139,10 @@ class Committee final {
   }
 #undef PROMO_CANDIDATE_TYPE_LIST
 
-#define DEF_PROMO_CANDIDATE(Type) \
-  static bool IsPromoCandidate##Type(Isolate* isolate, Type o) { return true; }
+#define DEF_PROMO_CANDIDATE(Type)                                        \
+  static bool IsPromoCandidate##Type(Isolate* isolate, Tagged<Type> o) { \
+    return true;                                                         \
+  }
 
   DEF_PROMO_CANDIDATE(AccessCheckInfo)
   DEF_PROMO_CANDIDATE(AccessorInfo)
@@ -262,9 +264,9 @@ class Committee final {
 
 class ReadOnlyPromotionImpl final : public AllStatic {
  public:
-  static void CopyToReadOnlyHeap(Isolate* isolate,
-                                 const std::vector<HeapObject>& promotees,
-                                 HeapObjectMap* moves) {
+  static void CopyToReadOnlyHeap(
+      Isolate* isolate, const std::vector<Tagged<HeapObject>>& promotees,
+      HeapObjectMap* moves) {
     ReadOnlySpace* rospace = isolate->heap()->read_only_space();
     for (Tagged<HeapObject> src : promotees) {
       const int size = src->Size(isolate);
@@ -431,7 +433,7 @@ class ReadOnlyPromotionImpl final : public AllStatic {
       // We shouldn't have moved any string table contents (which is what
       // OffHeapObjectSlot currently refers to).
       for (OffHeapObjectSlot slot = start; slot < end; slot++) {
-        Object o = slot.load(isolate_);
+        Tagged<Object> o = slot.load(isolate_);
         if (!IsHeapObject(o)) continue;
         CHECK(!Contains(*moves_, HeapObject::cast(o)));
       }
@@ -522,7 +524,7 @@ void ReadOnlyPromotion::Promote(Isolate* isolate,
                                 const DisallowGarbageCollection& no_gc) {
   // Visit the mutable heap and determine the set of objects that can be
   // promoted to RO space.
-  std::vector<HeapObject> promotees =
+  std::vector<Tagged<HeapObject>> promotees =
       Committee::DeterminePromotees(isolate, no_gc, safepoint_scope);
   // Physically copy promotee objects to RO space and track all object moves.
   HeapObjectMap moves;
diff --git a/src/heap/reference-summarizer.cc b/src/heap/reference-summarizer.cc
index f0913fec6cb..f8f52bece97 100644
--- a/src/heap/reference-summarizer.cc
+++ b/src/heap/reference-summarizer.cc
@@ -80,7 +80,7 @@ class ReferenceSummarizerMarkingState final {
   }
 
   ReferenceSummary references_;
-  HeapObject primary_object_;
+  Tagged<HeapObject> primary_object_;
   MarkingWorklists marking_worklists_;
   MarkingWorklists::Local local_marking_worklists_;
   WeakObjects weak_objects_;
diff --git a/src/heap/reference-summarizer.h b/src/heap/reference-summarizer.h
index 6e5071b7cba..ac19362c707 100644
--- a/src/heap/reference-summarizer.h
+++ b/src/heap/reference-summarizer.h
@@ -28,7 +28,8 @@ class ReferenceSummary {
                                                   Tagged<HeapObject> obj);
 
   using UnorderedHeapObjectSet =
-      std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>;
+      std::unordered_set<Tagged<HeapObject>, Object::Hasher,
+                         Object::KeyEqualSafe>;
 
   // All objects which the chosen object has strong pointers to.
   UnorderedHeapObjectSet& strong_references() { return strong_references_; }
diff --git a/src/heap/scavenger.h b/src/heap/scavenger.h
index 80f4cbf7a4b..3440c5f834e 100644
--- a/src/heap/scavenger.h
+++ b/src/heap/scavenger.h
@@ -29,10 +29,11 @@ enum class CopyAndForwardResult {
   FAILURE
 };
 
-using ObjectAndSize = std::pair<HeapObject, int>;
+using ObjectAndSize = std::pair<Tagged<HeapObject>, int>;
 using SurvivingNewLargeObjectsMap =
-    std::unordered_map<HeapObject, Map, Object::Hasher>;
-using SurvivingNewLargeObjectMapEntry = std::pair<HeapObject, Map>;
+    std::unordered_map<Tagged<HeapObject>, Tagged<Map>, Object::Hasher>;
+using SurvivingNewLargeObjectMapEntry =
+    std::pair<Tagged<HeapObject>, Tagged<Map>>;
 
 class ScavengerCollector;
 
@@ -40,7 +41,7 @@ class Scavenger {
  public:
   struct PromotionListEntry {
     Tagged<HeapObject> heap_object;
-    Map map;
+    Tagged<Map> map;
     int size;
   };
 
diff --git a/src/heap/setup-heap-internal.cc b/src/heap/setup-heap-internal.cc
index 9f51a70c46c..fc7570907bc 100644
--- a/src/heap/setup-heap-internal.cc
+++ b/src/heap/setup-heap-internal.cc
@@ -216,7 +216,7 @@ bool Heap::CreateMutableHeapObjects() {
 
 #define ALLOCATE_MAP(instance_type, size, field_name)                       \
   {                                                                         \
-    Map map;                                                                \
+    Tagged<Map> map;                                                        \
     if (!AllocateMap(AllocationType::kMap, (instance_type), size).To(&map)) \
       return false;                                                         \
     set_##field_name##_map(map);                                            \
@@ -342,7 +342,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
 
 #define ALLOCATE_PARTIAL_MAP(instance_type, size, field_name)                \
   {                                                                          \
-    Map map;                                                                 \
+    Tagged<Map> map;                                                         \
     if (!AllocatePartialMap((instance_type), (size)).To(&map)) return false; \
     set_##field_name##_map(map);                                             \
   }
@@ -471,7 +471,7 @@ bool Heap::CreateEarlyReadOnlyMaps() {
 
 #define ALLOCATE_MAP(instance_type, size, field_name)                  \
   {                                                                    \
-    Map map;                                                           \
+    Tagged<Map> map;                                                   \
     if (!AllocateMap(AllocationType::kReadOnly, (instance_type), size) \
              .To(&map)) {                                              \
       return false;                                                    \
@@ -677,7 +677,7 @@ bool Heap::CreateLateReadOnlyJSReceiverMaps() {
 #define ALLOCATE_ALWAYS_SHARED_SPACE_JSOBJECT_MAP(instance_type, size, \
                                                   field_name)          \
   {                                                                    \
-    Map map;                                                           \
+    Tagged<Map> map;                                                   \
     if (!AllocateMap(AllocationType::kReadOnly, (instance_type), size, \
                      DICTIONARY_ELEMENTS)                              \
              .To(&map)) {                                              \
diff --git a/src/heap/young-generation-marking-visitor-inl.h b/src/heap/young-generation-marking-visitor-inl.h
index f57d3bbb926..39190af5e3f 100644
--- a/src/heap/young-generation-marking-visitor-inl.h
+++ b/src/heap/young-generation-marking-visitor-inl.h
@@ -250,7 +250,7 @@ V8_INLINE bool YoungGenerationMarkingVisitor<marking_mode>::ShortCutStrings(
           heap_object->map(ObjectVisitorWithCageBases::cage_base())
               ->visitor_id();
       if (visitor_id == VisitorId::kVisitShortcutCandidate) {
-        ConsString string = ConsString::cast(*heap_object);
+        Tagged<ConsString> string = ConsString::cast(*heap_object);
         if (static_cast<Tagged_t>(string->second().ptr()) ==
             StaticReadOnlyRoot::kempty_string) {
           *heap_object = string->first();
diff --git a/src/ic/call-optimization.cc b/src/ic/call-optimization.cc
index d0dfc92d342..bd1165fa37c 100644
--- a/src/ic/call-optimization.cc
+++ b/src/ic/call-optimization.cc
@@ -23,7 +23,7 @@ template CallOptimization::CallOptimization(Isolate* isolate,
 template CallOptimization::CallOptimization(LocalIsolate* isolate,
                                             Handle<Object> function);
 
-base::Optional<NativeContext> CallOptimization::GetAccessorContext(
+base::Optional<Tagged<NativeContext>> CallOptimization::GetAccessorContext(
     Tagged<Map> holder_map) const {
   if (is_constant_call()) {
     return constant_function_->native_context();
@@ -45,7 +45,8 @@ bool CallOptimization::IsCrossContextLazyAccessorPair(
     Tagged<NativeContext> native_context, Tagged<Map> holder_map) const {
   DCHECK(IsNativeContext(native_context));
   if (is_constant_call()) return false;
-  base::Optional<NativeContext> maybe_context = GetAccessorContext(holder_map);
+  base::Optional<Tagged<NativeContext>> maybe_context =
+      GetAccessorContext(holder_map);
   if (!maybe_context.has_value()) {
     // The holder is a remote object which doesn't have a creation context.
     return true;
diff --git a/src/ic/call-optimization.h b/src/ic/call-optimization.h
index ea6d4887ee0..a11b83e3839 100644
--- a/src/ic/call-optimization.h
+++ b/src/ic/call-optimization.h
@@ -21,7 +21,7 @@ class CallOptimization {
   // If the holder is a remote object returns empty optional.
   // This method must not be called for holder maps with null constructor
   // because they can't be holders for lazy accessor pairs anyway.
-  base::Optional<NativeContext> GetAccessorContext(
+  base::Optional<Tagged<NativeContext>> GetAccessorContext(
       Tagged<Map> holder_map) const;
 
   // Return true if the accessor context for given holder doesn't match
diff --git a/src/ic/handler-configuration.cc b/src/ic/handler-configuration.cc
index 10e8bb31099..a9e41cdc8e9 100644
--- a/src/ic/handler-configuration.cc
+++ b/src/ic/handler-configuration.cc
@@ -533,7 +533,7 @@ void LoadHandler::PrintHandler(Tagged<Object> handler, std::ostream& os) {
   } else if (IsSymbol(handler)) {
     os << "LoadHandler(Symbol)(" << Brief(Symbol::cast(handler)) << ")";
   } else if (IsLoadHandler(handler)) {
-    LoadHandler load_handler = LoadHandler::cast(handler);
+    Tagged<LoadHandler> load_handler = LoadHandler::cast(handler);
     int raw_handler = Smi::cast(load_handler->smi_handler()).value();
     os << "LoadHandler(do access check on lookup start object = "
        << DoAccessCheckOnLookupStartObjectBits::decode(raw_handler)
@@ -569,9 +569,9 @@ void StoreHandler::PrintHandler(Tagged<Object> handler, std::ostream& os) {
     os << ")" << std::endl;
   } else if (IsStoreHandler(handler)) {
     os << "StoreHandler(";
-    StoreHandler store_handler = StoreHandler::cast(handler);
+    Tagged<StoreHandler> store_handler = StoreHandler::cast(handler);
     if (IsCode(store_handler->smi_handler())) {
-      Code code = Code::cast(store_handler->smi_handler());
+      Tagged<Code> code = Code::cast(store_handler->smi_handler());
       os << "builtin = ";
       ShortPrint(code, os);
     } else {
diff --git a/src/ic/ic.cc b/src/ic/ic.cc
index 630f4db9c82..8348f60a970 100644
--- a/src/ic/ic.cc
+++ b/src/ic/ic.cc
@@ -567,7 +567,7 @@ bool AddOneReceiverMapIfMissing(
 Handle<NativeContext> GetAccessorContext(
     const CallOptimization& call_optimization, Tagged<Map> holder_map,
     Isolate* isolate) {
-  base::Optional<NativeContext> maybe_context =
+  base::Optional<Tagged<NativeContext>> maybe_context =
       call_optimization.GetAccessorContext(holder_map);
 
   // Holders which are remote objects are not expected in the IC system.
diff --git a/src/init/bootstrapper.h b/src/init/bootstrapper.h
index abcf80264e4..309079797d5 100644
--- a/src/init/bootstrapper.h
+++ b/src/init/bootstrapper.h
@@ -37,7 +37,7 @@ class SourceCodeCache final {
 
  private:
   Script::Type type_;
-  FixedArray cache_;
+  Tagged<FixedArray> cache_;
 };
 
 // The Boostrapper is the public interface for creating a JavaScript global
diff --git a/src/interpreter/bytecode-array-iterator.h b/src/interpreter/bytecode-array-iterator.h
index 3343e82a4ba..65aaa9fe793 100644
--- a/src/interpreter/bytecode-array-iterator.h
+++ b/src/interpreter/bytecode-array-iterator.h
@@ -46,7 +46,7 @@ class V8_EXPORT_PRIVATE JumpTableTargetOffsets final {
     void UpdateAndAdvanceToValid();
 
     const BytecodeArrayIterator* iterator_;
-    Smi current_;
+    Tagged<Smi> current_;
     int index_;
     int table_offset_;
     int table_end_;
diff --git a/src/interpreter/constant-array-builder.cc b/src/interpreter/constant-array-builder.cc
index e2cba36f6f5..8131ce44711 100644
--- a/src/interpreter/constant-array-builder.cc
+++ b/src/interpreter/constant-array-builder.cc
@@ -68,12 +68,12 @@ const ConstantArrayBuilder::Entry& ConstantArrayBuilder::ConstantArraySlice::At(
 template <typename IsolateT>
 void ConstantArrayBuilder::ConstantArraySlice::CheckAllElementsAreUnique(
     IsolateT* isolate) const {
-  std::set<Smi> smis;
+  std::set<Tagged<Smi>> smis;
   std::set<double> heap_numbers;
   std::set<const AstRawString*> strings;
   std::set<const char*> bigints;
   std::set<const Scope*> scopes;
-  std::set<Object, Object::Comparer> deferred_objects;
+  std::set<Tagged<Object>, Object::Comparer> deferred_objects;
   for (const Entry& entry : constants_) {
     bool duplicate = false;
     switch (entry.tag_) {
diff --git a/src/interpreter/constant-array-builder.h b/src/interpreter/constant-array-builder.h
index 76d970c93a7..d3b2158ee78 100644
--- a/src/interpreter/constant-array-builder.h
+++ b/src/interpreter/constant-array-builder.h
@@ -161,7 +161,7 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
 
     union {
       Handle<Object> handle_;
-      Smi smi_;
+      Tagged<Smi> smi_;
       double heap_number_;
       const AstRawString* raw_string_;
       AstBigInt bigint_;
@@ -234,8 +234,8 @@ class V8_EXPORT_PRIVATE ConstantArrayBuilder final {
                             base::KeyEqualityMatcher<intptr_t>,
                             ZoneAllocationPolicy>
       constants_map_;
-  ZoneMap<Smi, index_t> smi_map_;
-  ZoneVector<std::pair<Smi, index_t>> smi_pairs_;
+  ZoneMap<Tagged<Smi>, index_t> smi_map_;
+  ZoneVector<std::pair<Tagged<Smi>, index_t>> smi_pairs_;
   ZoneMap<double, index_t> heap_number_map_;
 
 #define SINGLETON_ENTRY_FIELD(NAME, LOWER_NAME) int LOWER_NAME##_ = -1;
diff --git a/src/json/json-stringifier.cc b/src/json/json-stringifier.cc
index 032b49435be..825c92e6756 100644
--- a/src/json/json-stringifier.cc
+++ b/src/json/json-stringifier.cc
@@ -190,7 +190,7 @@ class JsonStringifier {
   V8_INLINE void SerializeDeferredKey(bool deferred_comma,
                                       Handle<Object> deferred_key);
 
-  Result SerializeSmi(Smi object);
+  Result SerializeSmi(Tagged<Smi> object);
 
   Result SerializeDouble(double number);
   V8_INLINE Result SerializeHeapNumber(Handle<HeapNumber> object) {
@@ -264,17 +264,19 @@ class JsonStringifier {
       }
     }
 
-    void TryInsert(String string, Isolate* isolate) {
+    void TryInsert(Tagged<String> string, Isolate* isolate) {
       ReadOnlyRoots roots(isolate);
-      if (string.map(isolate) == roots.internalized_one_byte_string_map()) {
+      if (string->map(isolate) == roots.internalized_one_byte_string_map()) {
         keys_[GetIndex(string)].PatchValue(string);
       }
     }
 
-    bool Contains(String string) { return *keys_[GetIndex(string)] == string; }
+    bool Contains(Tagged<String> string) {
+      return *keys_[GetIndex(string)] == string;
+    }
 
    private:
-    size_t GetIndex(String string) {
+    size_t GetIndex(Tagged<String> string) {
       // Short strings are 16 bytes long in pointer-compression builds, so the
       // lower four bits of the pointer may not provide much entropy.
       return (string.ptr() >> 4) & kIndexMask;
@@ -299,7 +301,7 @@ class JsonStringifier {
   // Tries to do fast-path serialization for a property key, and returns whether
   // it was successful.
   template <typename DestChar>
-  bool TrySerializeSimplePropertyKey(String string);
+  bool TrySerializeSimplePropertyKey(Tagged<String> string);
 
   template <typename Char>
   V8_INLINE static bool DoNotEscape(Char c);
@@ -619,7 +621,7 @@ JsonStringifier::Result JsonStringifier::StackPush(Handle<Object> object,
 
   {
     DisallowGarbageCollection no_gc;
-    Object raw_obj = *object;
+    Tagged<Object> raw_obj = *object;
     size_t size = stack_.size();
     for (size_t i = 0; i < size; ++i) {
       if (*stack_[i].second == raw_obj) {
@@ -704,7 +706,7 @@ class CircularStructureMessageBuilder {
     }
   }
 
-  void AppendSmi(Smi smi) {
+  void AppendSmi(Tagged<Smi> smi) {
     static const int kBufferSize = 100;
     char chars[kBufferSize];
     base::Vector<char> buffer(chars, kBufferSize);
@@ -756,7 +758,7 @@ Handle<String> JsonStringifier::ConstructCircularStructureErrorMessage(
   return result;
 }
 
-bool MayHaveInterestingProperties(Isolate* isolate, JSReceiver object) {
+bool MayHaveInterestingProperties(Isolate* isolate, Tagged<JSReceiver> object) {
   for (PrototypeIterator iter(isolate, object, kStartAtReceiver);
        !iter.IsAtEnd(); iter.Advance()) {
     if (iter.GetCurrent()->map()->may_have_interesting_properties()) {
@@ -894,7 +896,7 @@ JsonStringifier::Result JsonStringifier::Serialize_(Handle<Object> object,
 
 JsonStringifier::Result JsonStringifier::SerializeJSPrimitiveWrapper(
     Handle<JSPrimitiveWrapper> object, Handle<Object> key) {
-  Object raw = object->value();
+  Tagged<Object> raw = object->value();
   if (IsString(raw)) {
     Handle<Object> value;
     ASSIGN_RETURN_ON_EXCEPTION_VALUE(
@@ -923,7 +925,7 @@ JsonStringifier::Result JsonStringifier::SerializeJSPrimitiveWrapper(
   return SUCCESS;
 }
 
-JsonStringifier::Result JsonStringifier::SerializeSmi(Smi object) {
+JsonStringifier::Result JsonStringifier::SerializeSmi(Tagged<Smi> object) {
   static const int kBufferSize = 100;
   char chars[kBufferSize];
   base::Vector<char> buffer(chars, kBufferSize);
@@ -1078,7 +1080,8 @@ JsonStringifier::Result JsonStringifier::SerializeArrayLikeSlow(
 
 namespace {
 V8_INLINE bool CanFastSerializeJSObject(PtrComprCageBase cage_base,
-                                        JSObject raw_object, Isolate* isolate) {
+                                        Tagged<JSObject> raw_object,
+                                        Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   if (IsCustomElementsReceiverMap(raw_object->map(cage_base))) return false;
   if (!raw_object->HasFastProperties(cage_base)) return false;
@@ -1124,8 +1127,9 @@ JsonStringifier::Result JsonStringifier::SerializeJSObject(
     PropertyDetails details = PropertyDetails::Empty();
     {
       DisallowGarbageCollection no_gc;
-      DescriptorArray descriptors = map->instance_descriptors(cage_base);
-      Name name = descriptors->GetKey(i);
+      Tagged<DescriptorArray> descriptors =
+          map->instance_descriptors(cage_base);
+      Tagged<Name> name = descriptors->GetKey(i);
       // TODO(rossberg): Should this throw?
       if (!IsString(name, cage_base)) continue;
       key_name = handle(String::cast(name), isolate_);
@@ -1361,16 +1365,16 @@ bool JsonStringifier::SerializeString_(Handle<String> string) {
 }
 
 template <typename DestChar>
-bool JsonStringifier::TrySerializeSimplePropertyKey(String key) {
+bool JsonStringifier::TrySerializeSimplePropertyKey(Tagged<String> key) {
   DisallowGarbageCollection no_gc;
   ReadOnlyRoots roots(isolate_);
-  if (key.map(isolate_) != roots.internalized_one_byte_string_map()) {
+  if (key->map(isolate_) != roots.internalized_one_byte_string_map()) {
     return false;
   }
   if (!key_cache_.Contains(key)) {
     return false;
   }
-  int length = key.length();
+  int length = key->length();
   int copy_length = length;
   if constexpr (sizeof(DestChar) == 1) {
     // CopyChars has fast paths for small integer lengths, and is generally a
@@ -1395,7 +1399,7 @@ bool JsonStringifier::TrySerializeSimplePropertyKey(String key) {
   base::Vector<const uint8_t> chars(
       SeqOneByteString::cast(key)->GetChars(no_gc), copy_length);
   DCHECK_LE(reinterpret_cast<Address>(chars.end()),
-            key.address() + key.Size(isolate_));
+            key.address() + key->Size(isolate_));
 #if DEBUG
   for (int i = 0; i < length; ++i) {
     DCHECK(DoNotEscape(chars[i]));
diff --git a/src/logging/log.cc b/src/logging/log.cc
index 8bd10449687..ae0ac0f0116 100644
--- a/src/logging/log.cc
+++ b/src/logging/log.cc
@@ -2017,11 +2017,13 @@ EnumerateCompiledFunctions(Heap* heap) {
   std::vector<std::pair<Handle<SharedFunctionInfo>, Handle<AbstractCode>>>
       compiled_funcs;
   Isolate* isolate = heap->isolate();
-  auto hash = [](const std::pair<SharedFunctionInfo, AbstractCode>& p) {
-    return base::hash_combine(p.first.address(), p.second.address());
-  };
-  std::unordered_set<std::pair<SharedFunctionInfo, AbstractCode>,
-                     decltype(hash)>
+  auto hash =
+      [](const std::pair<Tagged<SharedFunctionInfo>, Tagged<AbstractCode>>& p) {
+        return base::hash_combine(p.first.address(), p.second.address());
+      };
+  std::unordered_set<
+      std::pair<Tagged<SharedFunctionInfo>, Tagged<AbstractCode>>,
+      decltype(hash)>
       seen(8, hash);
 
   auto record = [&](Tagged<SharedFunctionInfo> sfi, Tagged<AbstractCode> c) {
@@ -2076,11 +2078,11 @@ static std::vector<Handle<SharedFunctionInfo>> EnumerateInterpretedFunctions(
   std::vector<Handle<SharedFunctionInfo>> interpreted_funcs;
   Isolate* isolate = heap->isolate();
 
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsSharedFunctionInfo(obj)) {
-      SharedFunctionInfo sfi = SharedFunctionInfo::cast(obj);
-      if (sfi.HasBytecodeArray()) {
+      Tagged<SharedFunctionInfo> sfi = SharedFunctionInfo::cast(obj);
+      if (sfi->HasBytecodeArray()) {
         interpreted_funcs.push_back(handle(sfi, isolate));
       }
     }
diff --git a/src/maglev/maglev-graph-builder.cc b/src/maglev/maglev-graph-builder.cc
index 524403051c1..2cd475e80b5 100644
--- a/src/maglev/maglev-graph-builder.cc
+++ b/src/maglev/maglev-graph-builder.cc
@@ -1210,7 +1210,7 @@ ValueNode* MaglevGraphBuilder::GetTruncatedInt32ForToNumber(ValueNode* value,
     case Opcode::kSmiConstant:
       return GetInt32Constant(value->Cast<SmiConstant>()->value().value());
     case Opcode::kRootConstant: {
-      Object root_object =
+      Tagged<Object> root_object =
           local_isolate_->root(value->Cast<RootConstant>()->index());
       if (!IsOddball(root_object, local_isolate_)) break;
       int32_t truncated_value =
@@ -1373,7 +1373,7 @@ ValueNode* MaglevGraphBuilder::GetFloat64ForToNumber(ValueNode* value,
     case Opcode::kInt32Constant:
       return GetFloat64Constant(value->Cast<Int32Constant>()->value());
     case Opcode::kRootConstant: {
-      Object root_object =
+      Tagged<Object> root_object =
           local_isolate_->root(value->Cast<RootConstant>()->index());
       if (hint != ToNumberHint::kDisallowToNumber && IsOddball(root_object)) {
         return GetFloat64Constant(Oddball::cast(root_object)->to_number_raw());
@@ -3360,7 +3360,7 @@ MaglevGraphBuilder::TryFoldLoadDictPrototypeConstant(
     if (!IsJSReceiverMap(*map_handle)) {
       // Perform the implicit ToObject for primitives here.
       // Implemented according to ES6 section 7.3.2 GetV (V, P).
-      JSFunction constructor =
+      Tagged<JSFunction> constructor =
           Map::GetConstructorFunction(
               *map_handle, *broker()->target_native_context().object())
               .value();
diff --git a/src/objects/bytecode-array.cc b/src/objects/bytecode-array.cc
index a05f7ac6f73..30d0d8d3463 100644
--- a/src/objects/bytecode-array.cc
+++ b/src/objects/bytecode-array.cc
@@ -62,7 +62,7 @@ void BytecodeArray::PrintJson(std::ostream& os) {
   if (constant_pool_lenght > 0) {
     os << ", \"constantPool\": [";
     for (int i = 0; i < constant_pool_lenght; i++) {
-      Object object = constant_pool()->get(i);
+      Tagged<Object> object = constant_pool()->get(i);
       if (i > 0) os << ", ";
       os << "\"" << object << "\"";
     }
@@ -146,7 +146,7 @@ void BytecodeArray::Disassemble(Handle<BytecodeArray> handle,
   }
 #endif
 
-  ByteArray source_position_table = handle->SourcePositionTable();
+  Tagged<ByteArray> source_position_table = handle->SourcePositionTable();
   os << "Source Position Table (size = " << source_position_table->length()
      << ")\n";
 #ifdef OBJECT_PRINT
diff --git a/src/objects/call-site-info.cc b/src/objects/call-site-info.cc
index 32c9042d36a..c96bea2a4f3 100644
--- a/src/objects/call-site-info.cc
+++ b/src/objects/call-site-info.cc
@@ -39,14 +39,14 @@ bool CallSiteInfo::IsNative() const {
   if (IsBuiltin()) return true;
 #endif
   if (auto script = GetScript()) {
-    return script->type() == Script::Type::kNative;
+    return script.value()->type() == Script::Type::kNative;
   }
   return false;
 }
 
 bool CallSiteInfo::IsEval() const {
   if (auto script = GetScript()) {
-    return script->compilation_type() == Script::CompilationType::kEval;
+    return script.value()->compilation_type() == Script::CompilationType::kEval;
   }
   return false;
 }
@@ -168,29 +168,29 @@ int CallSiteInfo::GetEnclosingColumnNumber(Handle<CallSiteInfo> info) {
 
 int CallSiteInfo::GetScriptId() const {
   if (auto script = GetScript()) {
-    return script->id();
+    return script.value()->id();
   }
   return Message::kNoScriptIdInfo;
 }
 
 Tagged<Object> CallSiteInfo::GetScriptName() const {
   if (auto script = GetScript()) {
-    return script->name();
+    return script.value()->name();
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
 
 Tagged<Object> CallSiteInfo::GetScriptNameOrSourceURL() const {
   if (auto script = GetScript()) {
-    return script->GetNameOrSourceURL();
+    return script.value()->GetNameOrSourceURL();
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
 
 Tagged<Object> CallSiteInfo::GetScriptSource() const {
   if (auto script = GetScript()) {
-    if (script->HasValidSource()) {
-      return script->source();
+    if (script.value()->HasValidSource()) {
+      return script.value()->source();
     }
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
@@ -198,7 +198,7 @@ Tagged<Object> CallSiteInfo::GetScriptSource() const {
 
 Tagged<Object> CallSiteInfo::GetScriptSourceMappingURL() const {
   if (auto script = GetScript()) {
-    return script->source_mapping_url();
+    return script.value()->source_mapping_url();
   }
   return ReadOnlyRoots(GetIsolate()).null_value();
 }
@@ -348,8 +348,8 @@ Tagged<PrimitiveHeapObject> InferMethodNameFromFastObject(
     Isolate* isolate, Tagged<JSObject> receiver, Tagged<JSFunction> fun,
     Tagged<PrimitiveHeapObject> name) {
   ReadOnlyRoots roots(isolate);
-  Map map = receiver->map();
-  DescriptorArray descriptors = map->instance_descriptors(isolate);
+  Tagged<Map> map = receiver->map();
+  Tagged<DescriptorArray> descriptors = map->instance_descriptors(isolate);
   for (auto i : map->IterateOwnDescriptors()) {
     Tagged<PrimitiveHeapObject> key = descriptors->GetKey(i);
     if (IsSymbol(key)) continue;
@@ -613,7 +613,7 @@ int CallSiteInfo::ComputeSourcePosition(Handle<CallSiteInfo> info, int offset) {
       ->SourcePosition(isolate, offset);
 }
 
-base::Optional<Script> CallSiteInfo::GetScript() const {
+base::Optional<Tagged<Script>> CallSiteInfo::GetScript() const {
 #if V8_ENABLE_WEBASSEMBLY
   if (IsWasm()) {
     return GetWasmInstance()->module_object()->script();
diff --git a/src/objects/call-site-info.h b/src/objects/call-site-info.h
index cea5fc0300e..5049f1d463e 100644
--- a/src/objects/call-site-info.h
+++ b/src/objects/call-site-info.h
@@ -100,7 +100,7 @@ class CallSiteInfo : public TorqueGeneratedCallSiteInfo<CallSiteInfo, Struct> {
  private:
   static int ComputeSourcePosition(Handle<CallSiteInfo> info, int offset);
 
-  base::Optional<Script> GetScript() const;
+  base::Optional<Tagged<Script>> GetScript() const;
   Tagged<SharedFunctionInfo> GetSharedFunctionInfo() const;
 
   TQ_OBJECT_CONSTRUCTORS(CallSiteInfo)
diff --git a/src/objects/compilation-cache-table-inl.h b/src/objects/compilation-cache-table-inl.h
index bcaf2399b09..bed240f2314 100644
--- a/src/objects/compilation-cache-table-inl.h
+++ b/src/objects/compilation-cache-table-inl.h
@@ -75,7 +75,7 @@ class ScriptCacheKey : public HashTableKey {
 
   Handle<Object> AsHandle(Isolate* isolate, Handle<SharedFunctionInfo> shared);
 
-  static base::Optional<String> SourceFromObject(Tagged<Object> obj) {
+  static base::Optional<Tagged<String>> SourceFromObject(Tagged<Object> obj) {
     DisallowGarbageCollection no_gc;
     DCHECK(IsWeakFixedArray(obj));
     Tagged<WeakFixedArray> array = WeakFixedArray::cast(obj);
diff --git a/src/objects/compilation-cache-table.cc b/src/objects/compilation-cache-table.cc
index cff8a03deb8..6b346f8c8f8 100644
--- a/src/objects/compilation-cache-table.cc
+++ b/src/objects/compilation-cache-table.cc
@@ -210,7 +210,7 @@ class RegExpKey : public HashTableKey {
   }
 
   Handle<String> string_;
-  Smi flags_;
+  Tagged<Smi> flags_;
 };
 
 // CodeKey carries the SharedFunctionInfo key associated with a
diff --git a/src/objects/compilation-cache-table.h b/src/objects/compilation-cache-table.h
index 9b36b54bff5..2fba683e741 100644
--- a/src/objects/compilation-cache-table.h
+++ b/src/objects/compilation-cache-table.h
@@ -74,8 +74,8 @@ class InfoCellPair {
 
  private:
   IsCompiledScope is_compiled_scope_;
-  SharedFunctionInfo shared_;
-  FeedbackCell feedback_cell_;
+  Tagged<SharedFunctionInfo> shared_;
+  Tagged<FeedbackCell> feedback_cell_;
 };
 
 // A lookup result from the compilation cache for scripts. There are three
@@ -93,7 +93,7 @@ class CompilationCacheScriptLookupResult {
   MaybeHandle<SharedFunctionInfo> toplevel_sfi() const { return toplevel_sfi_; }
   IsCompiledScope is_compiled_scope() const { return is_compiled_scope_; }
 
-  using RawObjects = std::pair<Script, SharedFunctionInfo>;
+  using RawObjects = std::pair<Tagged<Script>, Tagged<SharedFunctionInfo>>;
 
   RawObjects GetRawObjects() const;
 
diff --git a/src/objects/contexts-inl.h b/src/objects/contexts-inl.h
index 0119cab1215..7a064d73185 100644
--- a/src/objects/contexts-inl.h
+++ b/src/objects/contexts-inl.h
@@ -188,19 +188,19 @@ bool Context::HasSameSecurityTokenAs(Tagged<Context> that) const {
 bool Context::IsDetached() const { return global_object()->IsDetached(); }
 
 #define NATIVE_CONTEXT_FIELD_ACCESSORS(index, type, name)   \
-  void Context::set_##name(type value) {                    \
+  void Context::set_##name(Tagged<type> value) {            \
     DCHECK(IsNativeContext(*this));                         \
     set(index, value, UPDATE_WRITE_BARRIER, kReleaseStore); \
   }                                                         \
-  bool Context::is_##name(type value) const {               \
+  bool Context::is_##name(Tagged<type> value) const {       \
     DCHECK(IsNativeContext(*this));                         \
     return type::cast(get(index)) == value;                 \
   }                                                         \
-  type Context::name() const {                              \
+  Tagged<type> Context::name() const {                      \
     DCHECK(IsNativeContext(*this));                         \
     return type::cast(get(index));                          \
   }                                                         \
-  type Context::name(AcquireLoadTag tag) const {            \
+  Tagged<type> Context::name(AcquireLoadTag tag) const {    \
     DCHECK(IsNativeContext(*this));                         \
     return type::cast(get(index, tag));                     \
   }
diff --git a/src/objects/contexts.h b/src/objects/contexts.h
index cadf5e946ae..90b67033657 100644
--- a/src/objects/contexts.h
+++ b/src/objects/contexts.h
@@ -653,10 +653,10 @@ class Context : public TorqueGeneratedContext<Context, HeapObject> {
   static int IntrinsicIndexForName(const unsigned char* name, int length);
 
 #define NATIVE_CONTEXT_FIELD_ACCESSORS(index, type, name) \
-  inline void set_##name(type value);                     \
-  inline bool is_##name(type value) const;                \
-  inline type name() const;                               \
-  inline type name(AcquireLoadTag) const;
+  inline void set_##name(Tagged<type> value);             \
+  inline bool is_##name(Tagged<type> value) const;        \
+  inline Tagged<type> name() const;                       \
+  inline Tagged<type> name(AcquireLoadTag) const;
   NATIVE_CONTEXT_FIELDS(NATIVE_CONTEXT_FIELD_ACCESSORS)
 #undef NATIVE_CONTEXT_FIELD_ACCESSORS
 
diff --git a/src/objects/deoptimization-data.cc b/src/objects/deoptimization-data.cc
index 994d8643a86..7f878bc1162 100644
--- a/src/objects/deoptimization-data.cc
+++ b/src/objects/deoptimization-data.cc
@@ -161,7 +161,7 @@ DeoptimizationFrameTranslation::Iterator::Iterator(
     : buffer_(buffer), index_(index) {
 #ifdef V8_USE_ZLIB
   if (V8_UNLIKELY(v8_flags.turbo_compress_frame_translations)) {
-    const int size = buffer_.get_int(kUncompressedSizeOffset);
+    const int size = buffer_->get_int(kUncompressedSizeOffset);
     uncompressed_contents_.insert(uncompressed_contents_.begin(), size, 0);
 
     uLongf uncompressed_size =
@@ -171,8 +171,8 @@ DeoptimizationFrameTranslation::Iterator::Iterator(
                  zlib_internal::ZRAW,
                  base::bit_cast<Bytef*>(uncompressed_contents_.data()),
                  &uncompressed_size,
-                 buffer_.GetDataStartAddress() + kCompressedDataOffset,
-                 buffer_.DataSize()),
+                 buffer_->GetDataStartAddress() + kCompressedDataOffset,
+                 buffer_->DataSize()),
              Z_OK);
     DCHECK(index >= 0 && index < size);
     return;
@@ -183,7 +183,7 @@ DeoptimizationFrameTranslation::Iterator::Iterator(
   // Starting at a location other than a BEGIN would make
   // MATCH_PREVIOUS_TRANSLATION instructions not work.
   DCHECK(TranslationOpcodeIsBegin(
-      static_cast<TranslationOpcode>(buffer_.GetDataStartAddress()[index])));
+      static_cast<TranslationOpcode>(buffer_->GetDataStartAddress()[index])));
 }
 
 int32_t DeoptimizationFrameTranslation::Iterator::NextOperand() {
@@ -191,12 +191,12 @@ int32_t DeoptimizationFrameTranslation::Iterator::NextOperand() {
     return uncompressed_contents_[index_++];
   } else if (remaining_ops_to_use_from_previous_translation_) {
     int32_t value =
-        base::VLQDecode(buffer_.GetDataStartAddress(), &previous_index_);
+        base::VLQDecode(buffer_->GetDataStartAddress(), &previous_index_);
     DCHECK_LT(previous_index_, index_);
     return value;
   } else {
-    int32_t value = base::VLQDecode(buffer_.GetDataStartAddress(), &index_);
-    DCHECK_LE(index_, buffer_.length());
+    int32_t value = base::VLQDecode(buffer_->GetDataStartAddress(), &index_);
+    DCHECK_LE(index_, buffer_->length());
     return value;
   }
 }
@@ -204,7 +204,7 @@ int32_t DeoptimizationFrameTranslation::Iterator::NextOperand() {
 TranslationOpcode
 DeoptimizationFrameTranslation::Iterator::NextOpcodeAtPreviousIndex() {
   TranslationOpcode opcode =
-      static_cast<TranslationOpcode>(buffer_.get(previous_index_++));
+      static_cast<TranslationOpcode>(buffer_->get(previous_index_++));
   DCHECK_LT(static_cast<uint32_t>(opcode), kNumTranslationOpcodes);
   DCHECK_NE(opcode, TranslationOpcode::MATCH_PREVIOUS_TRANSLATION);
   DCHECK_LT(previous_index_, index_);
@@ -214,7 +214,7 @@ DeoptimizationFrameTranslation::Iterator::NextOpcodeAtPreviousIndex() {
 uint32_t
 DeoptimizationFrameTranslation::Iterator::NextUnsignedOperandAtPreviousIndex() {
   uint32_t value =
-      base::VLQDecodeUnsigned(buffer_.GetDataStartAddress(), &previous_index_);
+      base::VLQDecodeUnsigned(buffer_->GetDataStartAddress(), &previous_index_);
   DCHECK_LT(previous_index_, index_);
   return value;
 }
@@ -226,8 +226,8 @@ uint32_t DeoptimizationFrameTranslation::Iterator::NextOperandUnsigned() {
     return NextUnsignedOperandAtPreviousIndex();
   } else {
     uint32_t value =
-        base::VLQDecodeUnsigned(buffer_.GetDataStartAddress(), &index_);
-    DCHECK_LE(index_, buffer_.length());
+        base::VLQDecodeUnsigned(buffer_->GetDataStartAddress(), &index_);
+    DCHECK_LE(index_, buffer_->length());
     return value;
   }
 }
@@ -242,8 +242,8 @@ TranslationOpcode DeoptimizationFrameTranslation::Iterator::NextOpcode() {
   if (remaining_ops_to_use_from_previous_translation_) {
     return NextOpcodeAtPreviousIndex();
   }
-  CHECK_LT(index_, buffer_.length());
-  uint8_t opcode_byte = buffer_.get(index_++);
+  CHECK_LT(index_, buffer_->length());
+  uint8_t opcode_byte = buffer_->get(index_++);
 
   // If the opcode byte is greater than any valid opcode, then the opcode is
   // implicitly MATCH_PREVIOUS_TRANSLATION and the operand is the opcode byte
@@ -261,7 +261,7 @@ TranslationOpcode DeoptimizationFrameTranslation::Iterator::NextOpcode() {
   }
 
   TranslationOpcode opcode = static_cast<TranslationOpcode>(opcode_byte);
-  DCHECK_LE(index_, buffer_.length());
+  DCHECK_LE(index_, buffer_->length());
   DCHECK_LT(static_cast<uint32_t>(opcode), kNumTranslationOpcodes);
   if (TranslationOpcodeIsBegin(opcode)) {
     int temp_index = index_;
@@ -269,14 +269,14 @@ TranslationOpcode DeoptimizationFrameTranslation::Iterator::NextOpcode() {
     // previous BEGIN, or zero to indicate that MATCH_PREVIOUS_TRANSLATION will
     // not be used in this translation.
     uint32_t lookback_distance =
-        base::VLQDecodeUnsigned(buffer_.GetDataStartAddress(), &temp_index);
+        base::VLQDecodeUnsigned(buffer_->GetDataStartAddress(), &temp_index);
     if (lookback_distance) {
       previous_index_ = index_ - 1 - lookback_distance;
       DCHECK(TranslationOpcodeIsBegin(
-          static_cast<TranslationOpcode>(buffer_.get(previous_index_))));
+          static_cast<TranslationOpcode>(buffer_->get(previous_index_))));
       // The previous BEGIN should specify zero as its lookback distance,
       // meaning it won't use MATCH_PREVIOUS_TRANSLATION.
-      DCHECK_EQ(buffer_.get(previous_index_ + 1), 0);
+      DCHECK_EQ(buffer_->get(previous_index_ + 1), 0);
     }
     ops_since_previous_index_was_updated_ = 1;
   } else if (opcode == TranslationOpcode::MATCH_PREVIOUS_TRANSLATION) {
@@ -334,7 +334,7 @@ bool DeoptimizationFrameTranslation::Iterator::HasNextOpcode() const {
   if (V8_UNLIKELY(v8_flags.turbo_compress_frame_translations)) {
     return index_ < static_cast<int>(uncompressed_contents_.size());
   } else {
-    return index_ < buffer_.length() ||
+    return index_ < buffer_->length() ||
            remaining_ops_to_use_from_previous_translation_ > 1;
   }
 }
diff --git a/src/objects/deoptimization-data.h b/src/objects/deoptimization-data.h
index 2794e1eea80..625d9b1342a 100644
--- a/src/objects/deoptimization-data.h
+++ b/src/objects/deoptimization-data.h
@@ -97,7 +97,7 @@ class DeoptimizationFrameTranslation::Iterator {
   void SkipOpcodeAndItsOperandsAtPreviousIndex();
 
   std::vector<int32_t> uncompressed_contents_;
-  DeoptimizationFrameTranslation buffer_;
+  Tagged<DeoptimizationFrameTranslation> buffer_;
   int index_;
 
   // This decrementing counter indicates how many more times to read operations
diff --git a/src/objects/dependent-code.cc b/src/objects/dependent-code.cc
index 871d4babf0a..c1872cba47b 100644
--- a/src/objects/dependent-code.cc
+++ b/src/objects/dependent-code.cc
@@ -76,7 +76,8 @@ Handle<DependentCode> DependentCode::InsertWeakCode(
     Handle<Code> code) {
   if (entries->length() == entries->capacity()) {
     // We'd have to grow - try to compact first.
-    entries->IterateAndCompact([](Code, DependencyGroups) { return false; });
+    entries->IterateAndCompact(
+        [](Tagged<Code>, DependencyGroups) { return false; });
   }
 
   MaybeObjectHandle code_slot(HeapObjectReference::Weak(*code), isolate);
@@ -122,7 +123,7 @@ bool DependentCode::MarkCodeForDeoptimization(
   DisallowGarbageCollection no_gc;
 
   bool marked_something = false;
-  IterateAndCompact([&](Code code, DependencyGroups groups) {
+  IterateAndCompact([&](Tagged<Code> code, DependencyGroups groups) {
     if ((groups & deopt_groups) == 0) return false;
 
     if (!code->marked_for_deoptimization()) {
diff --git a/src/objects/elements.cc b/src/objects/elements.cc
index 948165de2bb..11c067decbf 100644
--- a/src/objects/elements.cc
+++ b/src/objects/elements.cc
@@ -219,8 +219,8 @@ void CopyObjectToObjectElements(Isolate* isolate,
   DCHECK((copy_size + static_cast<int>(to_start)) <= to_base->length() &&
          (copy_size + static_cast<int>(from_start)) <= from_base->length());
   if (copy_size == 0) return;
-  FixedArray from = FixedArray::cast(from_base);
-  FixedArray to = FixedArray::cast(to_base);
+  Tagged<FixedArray> from = FixedArray::cast(from_base);
+  Tagged<FixedArray> to = FixedArray::cast(to_base);
   DCHECK(IsSmiOrObjectElementsKind(from_kind));
   DCHECK(IsSmiOrObjectElementsKind(to_kind));
 
@@ -239,7 +239,7 @@ void CopyDictionaryToObjectElements(Isolate* isolate,
                                     ElementsKind to_kind, uint32_t to_start,
                                     int raw_copy_size) {
   DisallowGarbageCollection no_gc;
-  NumberDictionary from = NumberDictionary::cast(from_base);
+  Tagged<NumberDictionary> from = NumberDictionary::cast(from_base);
   int copy_size = raw_copy_size;
   if (raw_copy_size < 0) {
     DCHECK_EQ(kCopyToEndAndInitializeToHole, raw_copy_size);
@@ -254,7 +254,7 @@ void CopyDictionaryToObjectElements(Isolate* isolate,
   DCHECK(to_base != from_base);
   DCHECK(IsSmiOrObjectElementsKind(to_kind));
   if (copy_size == 0) return;
-  FixedArray to = FixedArray::cast(to_base);
+  Tagged<FixedArray> to = FixedArray::cast(to_base);
   uint32_t to_length = to->length();
   if (to_start + copy_size > to_length) {
     copy_size = to_length - to_start;
@@ -338,8 +338,8 @@ void CopyDoubleToDoubleElements(Tagged<FixedArrayBase> from_base,
   DCHECK((copy_size + static_cast<int>(to_start)) <= to_base->length() &&
          (copy_size + static_cast<int>(from_start)) <= from_base->length());
   if (copy_size == 0) return;
-  FixedDoubleArray from = FixedDoubleArray::cast(from_base);
-  FixedDoubleArray to = FixedDoubleArray::cast(to_base);
+  Tagged<FixedDoubleArray> from = FixedDoubleArray::cast(from_base);
+  Tagged<FixedDoubleArray> to = FixedDoubleArray::cast(to_base);
   Address to_address = to.address() + FixedDoubleArray::kHeaderSize;
   Address from_address = from.address() + FixedDoubleArray::kHeaderSize;
   to_address += kDoubleSize * to_start;
@@ -374,8 +374,8 @@ void CopySmiToDoubleElements(Tagged<FixedArrayBase> from_base,
   DCHECK((copy_size + static_cast<int>(to_start)) <= to_base->length() &&
          (copy_size + static_cast<int>(from_start)) <= from_base->length());
   if (copy_size == 0) return;
-  FixedArray from = FixedArray::cast(from_base);
-  FixedDoubleArray to = FixedDoubleArray::cast(to_base);
+  Tagged<FixedArray> from = FixedArray::cast(from_base);
+  Tagged<FixedDoubleArray> to = FixedDoubleArray::cast(to_base);
   Tagged<Object> the_hole = from->GetReadOnlyRoots().the_hole_value();
   for (uint32_t from_end = from_start + static_cast<uint32_t>(copy_size);
        from_start < from_end; from_start++, to_start++) {
@@ -411,8 +411,8 @@ void CopyPackedSmiToDoubleElements(Tagged<FixedArrayBase> from_base,
   DCHECK((copy_size + static_cast<int>(to_start)) <= to_base->length() &&
          (copy_size + static_cast<int>(from_start)) <= from_base->length());
   if (copy_size == 0) return;
-  FixedArray from = FixedArray::cast(from_base);
-  FixedDoubleArray to = FixedDoubleArray::cast(to_base);
+  Tagged<FixedArray> from = FixedArray::cast(from_base);
+  Tagged<FixedDoubleArray> to = FixedDoubleArray::cast(to_base);
   for (uint32_t from_end = from_start + static_cast<uint32_t>(packed_size);
        from_start < from_end; from_start++, to_start++) {
     Tagged<Object> smi = from->get(from_start);
@@ -437,8 +437,8 @@ void CopyObjectToDoubleElements(Tagged<FixedArrayBase> from_base,
   DCHECK((copy_size + static_cast<int>(to_start)) <= to_base->length() &&
          (copy_size + static_cast<int>(from_start)) <= from_base->length());
   if (copy_size == 0) return;
-  FixedArray from = FixedArray::cast(from_base);
-  FixedDoubleArray to = FixedDoubleArray::cast(to_base);
+  Tagged<FixedArray> from = FixedArray::cast(from_base);
+  Tagged<FixedDoubleArray> to = FixedDoubleArray::cast(to_base);
   Tagged<Object> the_hole = from->GetReadOnlyRoots().the_hole_value();
   for (uint32_t from_end = from_start + copy_size; from_start < from_end;
        from_start++, to_start++) {
@@ -457,7 +457,7 @@ void CopyDictionaryToDoubleElements(Isolate* isolate,
                                     Tagged<FixedArrayBase> to_base,
                                     uint32_t to_start, int raw_copy_size) {
   DisallowGarbageCollection no_gc;
-  NumberDictionary from = NumberDictionary::cast(from_base);
+  Tagged<NumberDictionary> from = NumberDictionary::cast(from_base);
   int copy_size = raw_copy_size;
   if (copy_size < 0) {
     DCHECK_EQ(kCopyToEndAndInitializeToHole, copy_size);
@@ -467,7 +467,7 @@ void CopyDictionaryToDoubleElements(Isolate* isolate,
     }
   }
   if (copy_size == 0) return;
-  FixedDoubleArray to = FixedDoubleArray::cast(to_base);
+  Tagged<FixedDoubleArray> to = FixedDoubleArray::cast(to_base);
   uint32_t to_length = to->length();
   if (to_start + copy_size > to_length) {
     copy_size = to_length - to_start;
@@ -1502,7 +1502,7 @@ class DictionaryElementsAccessor
 
   static uint32_t NumberOfElementsImpl(Tagged<JSObject> receiver,
                                        Tagged<FixedArrayBase> backing_store) {
-    NumberDictionary dict = NumberDictionary::cast(backing_store);
+    Tagged<NumberDictionary> dict = NumberDictionary::cast(backing_store);
     return dict->NumberOfElements();
   }
 
@@ -1579,13 +1579,13 @@ class DictionaryElementsAccessor
   static bool HasAccessorsImpl(Tagged<JSObject> holder,
                                Tagged<FixedArrayBase> backing_store) {
     DisallowGarbageCollection no_gc;
-    NumberDictionary dict = NumberDictionary::cast(backing_store);
+    Tagged<NumberDictionary> dict = NumberDictionary::cast(backing_store);
     if (!dict->requires_slow_elements()) return false;
     PtrComprCageBase cage_base = GetPtrComprCageBase(holder);
     ReadOnlyRoots roots = holder->GetReadOnlyRoots(cage_base);
     for (InternalIndex i : dict->IterateEntries()) {
       Tagged<Object> key = dict->KeyAt(cage_base, i);
-      if (!dict.IsKey(roots, key)) continue;
+      if (!dict->IsKey(roots, key)) continue;
       PropertyDetails details = dict->DetailsAt(i);
       if (details.kind() == PropertyKind::kAccessor) return true;
     }
@@ -1594,7 +1594,7 @@ class DictionaryElementsAccessor
 
   static Tagged<Object> GetRaw(Tagged<FixedArrayBase> store,
                                InternalIndex entry) {
-    NumberDictionary backing_store = NumberDictionary::cast(store);
+    Tagged<NumberDictionary> backing_store = NumberDictionary::cast(store);
     return backing_store->ValueAt(entry);
   }
 
@@ -1646,7 +1646,7 @@ class DictionaryElementsAccessor
                               Handle<FixedArrayBase> store, InternalIndex entry,
                               Handle<Object> value,
                               PropertyAttributes attributes) {
-    NumberDictionary dictionary = NumberDictionary::cast(*store);
+    Tagged<NumberDictionary> dictionary = NumberDictionary::cast(*store);
     if (attributes != NONE) object->RequireSlowElements(dictionary);
     dictionary->ValueAtPut(entry, *value);
     PropertyDetails details = dictionary->DetailsAt(entry);
@@ -1680,7 +1680,7 @@ class DictionaryElementsAccessor
   static bool HasEntryImpl(Isolate* isolate, Tagged<FixedArrayBase> store,
                            InternalIndex entry) {
     DisallowGarbageCollection no_gc;
-    NumberDictionary dict = NumberDictionary::cast(store);
+    Tagged<NumberDictionary> dict = NumberDictionary::cast(store);
     Tagged<Object> index = dict->KeyAt(isolate, entry);
     return !IsTheHole(index, isolate);
   }
@@ -1691,7 +1691,7 @@ class DictionaryElementsAccessor
                                             size_t index,
                                             PropertyFilter filter) {
     DisallowGarbageCollection no_gc;
-    NumberDictionary dictionary = NumberDictionary::cast(store);
+    Tagged<NumberDictionary> dictionary = NumberDictionary::cast(store);
     DCHECK_LE(index, std::numeric_limits<uint32_t>::max());
     InternalIndex entry =
         dictionary->FindEntry(isolate, static_cast<uint32_t>(index));
@@ -1812,7 +1812,8 @@ class DictionaryElementsAccessor
                                     Handle<Object> value, size_t start_from,
                                     size_t length, Maybe<bool>* result) {
     DisallowGarbageCollection no_gc;
-    NumberDictionary dictionary = NumberDictionary::cast(receiver->elements());
+    Tagged<NumberDictionary> dictionary =
+        NumberDictionary::cast(receiver->elements());
     Tagged<Object> the_hole = ReadOnlyRoots(isolate).the_hole_value();
     Tagged<Object> undefined = ReadOnlyRoots(isolate).undefined_value();
 
@@ -1997,7 +1998,8 @@ class DictionaryElementsAccessor
     DCHECK_EQ(holder->map()->elements_kind(), DICTIONARY_ELEMENTS);
     if (!v8_flags.enable_slow_asserts) return;
     ReadOnlyRoots roots = holder->GetReadOnlyRoots();
-    NumberDictionary dictionary = NumberDictionary::cast(holder->elements());
+    Tagged<NumberDictionary> dictionary =
+        NumberDictionary::cast(holder->elements());
     // Validate the requires_slow_elements and max_number_key values.
     bool requires_slow_elements = false;
     int max_key = 0;
@@ -2072,7 +2074,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
       if (!backing_store->is_the_hole(isolate, entry - 1)) break;
     }
     if (entry == 0) {
-      FixedArray empty = ReadOnlyRoots(isolate).empty_fixed_array();
+      Tagged<FixedArray> empty = ReadOnlyRoots(isolate).empty_fixed_array();
       // Dynamically ask for the elements kind here since we manually redirect
       // the operations for argument backing stores.
       if (obj->GetElementsKind() == FAST_SLOPPY_ARGUMENTS_ELEMENTS) {
@@ -2252,7 +2254,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
     Isolate* isolate = holder->GetIsolate();
     Heap* heap = isolate->heap();
     Tagged<FixedArrayBase> elements = holder->elements();
-    Map map = elements->map();
+    Tagged<Map> map = elements->map();
     if (IsSmiOrObjectElementsKind(KindTraits::Kind)) {
       DCHECK_NE(map, ReadOnlyRoots(heap).fixed_double_array_map());
     } else if (IsDoubleElementsKind(KindTraits::Kind)) {
@@ -2397,7 +2399,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
         // JSArray::length.
         if (IsSmiOrObjectElementsKind(Subclass::kind()) ||
             IsAnyNonextensibleElementsKind(Subclass::kind())) {
-          FixedArray elements = FixedArray::cast(receiver->elements());
+          Tagged<FixedArray> elements = FixedArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
             Tagged<Object> element_k = elements->get(static_cast<int>(k));
@@ -2411,7 +2413,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           // Search for The Hole in HOLEY_DOUBLE_ELEMENTS or
           // PACKED_DOUBLE_ELEMENTS.
           DCHECK(IsDoubleElementsKind(Subclass::kind()));
-          FixedDoubleArray elements =
+          Tagged<FixedDoubleArray> elements =
               FixedDoubleArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
@@ -2431,7 +2433,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
         // PACKED_ELEMENTS or HOLEY_ELEMENTS.
         DCHECK(IsObjectElementsKind(Subclass::kind()) ||
                IsAnyNonextensibleElementsKind(Subclass::kind()));
-        FixedArray elements = FixedArray::cast(receiver->elements());
+        Tagged<FixedArray> elements = FixedArray::cast(receiver->elements());
 
         for (size_t k = start_from; k < length; ++k) {
           Tagged<Object> element_k = elements->get(static_cast<int>(k));
@@ -2447,7 +2449,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           // Search for non-NaN Number in PACKED_DOUBLE_ELEMENTS or
           // HOLEY_DOUBLE_ELEMENTS --- Skip TheHole, and trust UCOMISD or
           // similar operation for result.
-          FixedDoubleArray elements =
+          Tagged<FixedDoubleArray> elements =
               FixedDoubleArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
@@ -2461,7 +2463,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           // Search for non-NaN Number in PACKED_ELEMENTS, HOLEY_ELEMENTS,
           // PACKED_SMI_ELEMENTS or HOLEY_SMI_ELEMENTS --- Skip non-Numbers,
           // and trust UCOMISD or similar operation for result
-          FixedArray elements = FixedArray::cast(receiver->elements());
+          Tagged<FixedArray> elements = FixedArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
             Tagged<Object> element_k = elements->get(static_cast<int>(k));
@@ -2481,7 +2483,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           // Search for NaN in PACKED_DOUBLE_ELEMENTS or
           // HOLEY_DOUBLE_ELEMENTS --- Skip The Hole and trust
           // std::isnan(elementK) for result
-          FixedDoubleArray elements =
+          Tagged<FixedDoubleArray> elements =
               FixedDoubleArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
@@ -2496,7 +2498,7 @@ class FastElementsAccessor : public ElementsAccessorBase<Subclass, KindTraits> {
           // if elementK->IsHeapNumber() && std::isnan(elementK->Number())
           DCHECK(IsObjectElementsKind(Subclass::kind()) ||
                  IsAnyNonextensibleElementsKind(Subclass::kind()));
-          FixedArray elements = FixedArray::cast(receiver->elements());
+          Tagged<FixedArray> elements = FixedArray::cast(receiver->elements());
 
           for (size_t k = start_from; k < length; ++k) {
             if (IsNaN(elements->get(static_cast<int>(k)))) return Just(true);
@@ -2630,7 +2632,8 @@ class FastSmiOrObjectElementsAccessor
     FixedArray::cast(backing_store)->set(entry.as_int(), value, mode);
   }
 
-  static Tagged<Object> GetRaw(FixedArray backing_store, InternalIndex entry) {
+  static Tagged<Object> GetRaw(Tagged<FixedArray> backing_store,
+                               InternalIndex entry) {
     return backing_store->get(entry.as_int());
   }
 
@@ -2709,7 +2712,7 @@ class FastSmiOrObjectElementsAccessor
     } else {
       // No allocations here, so we can avoid handlification overhead.
       DisallowGarbageCollection no_gc;
-      FixedArray elements = FixedArray::cast(object->elements());
+      Tagged<FixedArray> elements = FixedArray::cast(object->elements());
       uint32_t length = elements->length();
       for (uint32_t index = 0; index < length; ++index) {
         InternalIndex entry(index);
@@ -2747,7 +2750,7 @@ class FastSmiOrObjectElementsAccessor
     // elements_base->length() so we never read out of bounds. This means that
     // elements->get(k) can return the hole, for which the StrictEquals will
     // always fail.
-    FixedArray elements = FixedArray::cast(receiver->elements());
+    Tagged<FixedArray> elements = FixedArray::cast(receiver->elements());
     static_assert(FixedArray::kMaxLength <=
                   std::numeric_limits<uint32_t>::max());
     for (size_t k = start_from; k < length; ++k) {
@@ -3186,7 +3189,8 @@ class FastDoubleElementsAccessor
       return Just<int64_t>(-1);
     }
     double numeric_search_value = Object::Number(value);
-    FixedDoubleArray elements = FixedDoubleArray::cast(receiver->elements());
+    Tagged<FixedDoubleArray> elements =
+        FixedDoubleArray::cast(receiver->elements());
 
     static_assert(FixedDoubleArray::kMaxLength <=
                   std::numeric_limits<int>::max());
@@ -3433,7 +3437,7 @@ class TypedElementsAccessor
 
   static size_t GetCapacityImpl(Tagged<JSObject> holder,
                                 Tagged<FixedArrayBase> backing_store) {
-    JSTypedArray typed_array = JSTypedArray::cast(holder);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(holder);
     return typed_array->GetLength();
   }
 
@@ -3522,7 +3526,7 @@ class TypedElementsAccessor
                                        Handle<Object> value, size_t start_from,
                                        size_t length) {
     DisallowGarbageCollection no_gc;
-    JSTypedArray typed_array = JSTypedArray::cast(*receiver);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(*receiver);
 
     if (typed_array->WasDetached()) {
       return Just(IsUndefined(*value, isolate) && length > start_from);
@@ -3596,7 +3600,7 @@ class TypedElementsAccessor
                                          Handle<Object> value,
                                          size_t start_from, size_t length) {
     DisallowGarbageCollection no_gc;
-    JSTypedArray typed_array = JSTypedArray::cast(*receiver);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(*receiver);
 
     // If this is called via Array.prototype.indexOf (not
     // TypedArray.prototype.indexOf), it's possible that the TypedArray is
@@ -3659,7 +3663,7 @@ class TypedElementsAccessor
                                              Handle<Object> value,
                                              size_t start_from) {
     DisallowGarbageCollection no_gc;
-    JSTypedArray typed_array = JSTypedArray::cast(*receiver);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(*receiver);
 
     DCHECK(!typed_array->IsDetachedOrOutOfBounds());
 
@@ -3716,7 +3720,7 @@ class TypedElementsAccessor
 
   static void ReverseImpl(Tagged<JSObject> receiver) {
     DisallowGarbageCollection no_gc;
-    JSTypedArray typed_array = JSTypedArray::cast(receiver);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(receiver);
 
     DCHECK(!typed_array->IsDetachedOrOutOfBounds());
 
@@ -3758,8 +3762,8 @@ class TypedElementsAccessor
     return result;
   }
 
-  static void CopyTypedArrayElementsSliceImpl(JSTypedArray source,
-                                              JSTypedArray destination,
+  static void CopyTypedArrayElementsSliceImpl(Tagged<JSTypedArray> source,
+                                              Tagged<JSTypedArray> destination,
                                               size_t start, size_t end) {
     DisallowGarbageCollection no_gc;
     DCHECK_EQ(destination->GetElementsKind(), AccessorClass::kind());
@@ -3822,8 +3826,8 @@ class TypedElementsAccessor
     }
   }
 
-  static void CopyElementsFromTypedArray(JSTypedArray source,
-                                         JSTypedArray destination,
+  static void CopyElementsFromTypedArray(Tagged<JSTypedArray> source,
+                                         Tagged<JSTypedArray> destination,
                                          size_t length, size_t offset) {
     // The source is a typed array, so we know we don't need to do ToNumber
     // side-effects, as the source elements will always be a number.
@@ -3904,8 +3908,9 @@ class TypedElementsAccessor
     }
   }
 
-  static bool HoleyPrototypeLookupRequired(Isolate* isolate, Context context,
-                                           JSArray source) {
+  static bool HoleyPrototypeLookupRequired(Isolate* isolate,
+                                           Tagged<Context> context,
+                                           Tagged<JSArray> source) {
     DisallowGarbageCollection no_gc;
     DisallowJavascriptExecution no_js(isolate);
 
@@ -3927,9 +3932,10 @@ class TypedElementsAccessor
     return !Protectors::IsNoElementsIntact(isolate);
   }
 
-  static bool TryCopyElementsFastNumber(Context context, JSArray source,
-                                        JSTypedArray destination, size_t length,
-                                        size_t offset) {
+  static bool TryCopyElementsFastNumber(Tagged<Context> context,
+                                        Tagged<JSArray> source,
+                                        Tagged<JSTypedArray> destination,
+                                        size_t length, size_t offset) {
     if (IsBigIntTypedArrayElementsKind(Kind)) return false;
     Isolate* isolate = source->GetIsolate();
     DisallowGarbageCollection no_gc;
@@ -3962,13 +3968,13 @@ class TypedElementsAccessor
     // the hole into undefined.
     if (HoleyPrototypeLookupRequired(isolate, context, source)) return false;
 
-    Oddball undefined = ReadOnlyRoots(isolate).undefined_value();
+    Tagged<Oddball> undefined = ReadOnlyRoots(isolate).undefined_value();
     ElementType* dest_data =
         reinterpret_cast<ElementType*>(destination->DataPtr()) + offset;
 
     // Fast-path for packed Smi kind.
     if (kind == PACKED_SMI_ELEMENTS) {
-      FixedArray source_store = FixedArray::cast(source->elements());
+      Tagged<FixedArray> source_store = FixedArray::cast(source->elements());
 
       for (size_t i = 0; i < length; i++) {
         Tagged<Object> elem = source_store->get(static_cast<int>(i));
@@ -3977,7 +3983,7 @@ class TypedElementsAccessor
       }
       return true;
     } else if (kind == HOLEY_SMI_ELEMENTS) {
-      FixedArray source_store = FixedArray::cast(source->elements());
+      Tagged<FixedArray> source_store = FixedArray::cast(source->elements());
       for (size_t i = 0; i < length; i++) {
         if (source_store->is_the_hole(isolate, static_cast<int>(i))) {
           SetImpl(dest_data + i, FromObject(undefined), destination_shared);
@@ -3991,7 +3997,7 @@ class TypedElementsAccessor
     } else if (kind == PACKED_DOUBLE_ELEMENTS) {
       // Fast-path for packed double kind. We avoid boxing and then immediately
       // unboxing the double here by using get_scalar.
-      FixedDoubleArray source_store =
+      Tagged<FixedDoubleArray> source_store =
           FixedDoubleArray::cast(source->elements());
 
       for (size_t i = 0; i < length; i++) {
@@ -4002,7 +4008,7 @@ class TypedElementsAccessor
       }
       return true;
     } else if (kind == HOLEY_DOUBLE_ELEMENTS) {
-      FixedDoubleArray source_store =
+      Tagged<FixedDoubleArray> source_store =
           FixedDoubleArray::cast(source->elements());
       for (size_t i = 0; i < length; i++) {
         if (source_store->is_the_hole(static_cast<int>(i))) {
@@ -4567,7 +4573,7 @@ class SloppyArgumentsElementsAccessor
       Tagged<Object> probe =
           elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
       DCHECK(!IsTheHole(probe, isolate));
-      Context context = elements->context();
+      Tagged<Context> context = elements->context();
       int context_entry = Smi::ToInt(probe);
       DCHECK(!IsTheHole(context->get(context_entry), isolate));
       return handle(context->get(context_entry), isolate);
@@ -4605,18 +4611,19 @@ class SloppyArgumentsElementsAccessor
       Tagged<Object> probe =
           elements->mapped_entries(entry.as_uint32(), kRelaxedLoad);
       DCHECK(!IsTheHole(probe));
-      Context context = Context::cast(elements->context());
+      Tagged<Context> context = Context::cast(elements->context());
       int context_entry = Smi::ToInt(probe);
       DCHECK(!IsTheHole(context->get(context_entry)));
       context->set(context_entry, value);
     } else {
       //  Entry is not context mapped defer to arguments.
-      FixedArray arguments = elements->arguments();
+      Tagged<FixedArray> arguments = elements->arguments();
       Tagged<Object> current =
           ArgumentsAccessor::GetRaw(arguments, entry.adjust_down(length));
       if (IsAliasedArgumentsEntry(current)) {
-        AliasedArgumentsEntry alias = AliasedArgumentsEntry::cast(current);
-        Context context = Context::cast(elements->context());
+        Tagged<AliasedArgumentsEntry> alias =
+            AliasedArgumentsEntry::cast(current);
+        Tagged<Context> context = Context::cast(elements->context());
         int context_entry = alias->aliased_context_slot();
         DCHECK(!IsTheHole(context->get(context_entry)));
         context->set(context_entry, value);
diff --git a/src/objects/feedback-vector-inl.h b/src/objects/feedback-vector-inl.h
index bc5a2e69141..812ae2c0aa5 100644
--- a/src/objects/feedback-vector-inl.h
+++ b/src/objects/feedback-vector-inl.h
@@ -216,8 +216,8 @@ void FeedbackVector::set_log_next_execution(bool value) {
   set_flags(LogNextExecutionBit::update(flags(), value));
 }
 
-base::Optional<Code> FeedbackVector::GetOptimizedOsrCode(Isolate* isolate,
-                                                         FeedbackSlot slot) {
+base::Optional<Tagged<Code>> FeedbackVector::GetOptimizedOsrCode(
+    Isolate* isolate, FeedbackSlot slot) {
   MaybeObject maybe_code = Get(isolate, slot);
   if (maybe_code->IsCleared()) return {};
 
diff --git a/src/objects/feedback-vector.cc b/src/objects/feedback-vector.cc
index 2d200bcc6a5..320da874cd0 100644
--- a/src/objects/feedback-vector.cc
+++ b/src/objects/feedback-vector.cc
@@ -397,7 +397,7 @@ void FeedbackVector::SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot,
   DCHECK(CodeKindIsOptimizedJSFunction(code->kind()));
   DCHECK(!slot.IsInvalid());
   auto current = GetOptimizedOsrCode(isolate, slot);
-  if (V8_UNLIKELY(current && current->kind() > code->kind())) {
+  if (V8_UNLIKELY(current && current.value()->kind() > code->kind())) {
     return;
   }
   Set(slot, HeapObjectReference::Weak(code));
diff --git a/src/objects/feedback-vector.h b/src/objects/feedback-vector.h
index 4e446d809ed..d3a71873924 100644
--- a/src/objects/feedback-vector.h
+++ b/src/objects/feedback-vector.h
@@ -273,8 +273,8 @@ class FeedbackVector
 
   // Optimized OSR'd code is cached in JumpLoop feedback vector slots. The
   // slots either contain a Code object or the ClearedValue.
-  inline base::Optional<Code> GetOptimizedOsrCode(Isolate* isolate,
-                                                  FeedbackSlot slot);
+  inline base::Optional<Tagged<Code>> GetOptimizedOsrCode(Isolate* isolate,
+                                                          FeedbackSlot slot);
   void SetOptimizedOsrCode(Isolate* isolate, FeedbackSlot slot,
                            Tagged<Code> code);
 
@@ -953,7 +953,7 @@ class V8_EXPORT_PRIVATE FeedbackIterator final {
   enum State { kMonomorphic, kPolymorphic, kOther };
 
   Handle<WeakFixedArray> polymorphic_feedback_;
-  Map map_;
+  Tagged<Map> map_;
   MaybeObject handler_;
   bool done_;
   int index_;
diff --git a/src/objects/fixed-array-inl.h b/src/objects/fixed-array-inl.h
index 429e29459e3..7d9f98c4ba7 100644
--- a/src/objects/fixed-array-inl.h
+++ b/src/objects/fixed-array-inl.h
@@ -85,7 +85,7 @@ bool FixedArray::is_the_hole(Isolate* isolate, int index) {
 void FixedArray::set(int index, Tagged<Smi> value) {
   DCHECK_NE(map(), EarlyGetReadOnlyRoots().unchecked_fixed_cow_array_map());
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   int offset = OffsetOfElementAt(index);
   RELAXED_WRITE_FIELD(*this, offset, value);
 }
@@ -138,7 +138,7 @@ void FixedArray::set(int index, Tagged<Object> value, RelaxedStoreTag,
 }
 
 void FixedArray::set(int index, Tagged<Smi> value, RelaxedStoreTag tag) {
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   set(index, value, tag, SKIP_WRITE_BARRIER);
 }
 
@@ -162,7 +162,7 @@ void FixedArray::set(int index, Tagged<Object> value, SeqCstAccessTag,
 }
 
 void FixedArray::set(int index, Tagged<Smi> value, SeqCstAccessTag tag) {
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   set(index, value, tag, SKIP_WRITE_BARRIER);
 }
 
@@ -186,7 +186,7 @@ void FixedArray::set(int index, Tagged<Object> value, ReleaseStoreTag,
 }
 
 void FixedArray::set(int index, Tagged<Smi> value, ReleaseStoreTag tag) {
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   set(index, value, tag, SKIP_WRITE_BARRIER);
 }
 
@@ -602,7 +602,7 @@ void ArrayList::Set(int index, Tagged<Object> obj, WriteBarrierMode mode) {
 }
 
 void ArrayList::Set(int index, Tagged<Smi> value) {
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   Set(index, value, SKIP_WRITE_BARRIER);
 }
 void ArrayList::Clear(int index, Tagged<Object> undefined) {
diff --git a/src/objects/fixed-array.h b/src/objects/fixed-array.h
index 1c186c4135c..47d701f9876 100644
--- a/src/objects/fixed-array.h
+++ b/src/objects/fixed-array.h
@@ -401,7 +401,7 @@ class WeakArrayList::Iterator {
 
  private:
   int index_;
-  WeakArrayList array_;
+  Tagged<WeakArrayList> array_;
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
 };
 
diff --git a/src/objects/hash-table.h b/src/objects/hash-table.h
index bf52429a715..92a11483341 100644
--- a/src/objects/hash-table.h
+++ b/src/objects/hash-table.h
@@ -40,11 +40,12 @@ class Impl;
 //   class ExampleShape {
 //    public:
 //     // Tells whether key matches other.
-//     static bool IsMatch(Key key, Object other);
+//     static bool IsMatch(Key key, Tagged<Object> other);
 //     // Returns the hash value for key.
 //     static uint32_t Hash(ReadOnlyRoots roots, Key key);
 //     // Returns the hash value for object.
-//     static uint32_t HashForObject(ReadOnlyRoots roots, Object object);
+//     static uint32_t HashForObject(ReadOnlyRoots roots,
+//                                   Tagged<Object> object);
 //     // Convert key to an object.
 //     static inline Handle<Object> AsHandle(Isolate* isolate, Key key);
 //     // The prefix size indicates number of elements in the beginning
@@ -248,7 +249,7 @@ class EXPORT_TEMPLATE_DECLARE(V8_EXPORT_PRIVATE) HashTable
       Isolate* isolate, Handle<Derived> table, int additionalCapacity = 0);
 
   // Rehashes this hash-table into the new table.
-  void Rehash(PtrComprCageBase cage_base, Derived new_table);
+  void Rehash(PtrComprCageBase cage_base, Tagged<Derived> new_table);
 
   inline void set_key(int index, Tagged<Object> value);
   inline void set_key(int index, Tagged<Object> value, WriteBarrierMode mode);
@@ -445,8 +446,9 @@ class ObjectMultiHashTableBase
 
   // Returns the values associated with the given key. Return an std::array of
   // holes if not found.
-  std::array<Object, N> Lookup(Handle<Object> key);
-  std::array<Object, N> Lookup(PtrComprCageBase cage_base, Handle<Object> key);
+  std::array<Tagged<Object>, N> Lookup(Handle<Object> key);
+  std::array<Tagged<Object>, N> Lookup(PtrComprCageBase cage_base,
+                                       Handle<Object> key);
 
   // Adds or overwrites the values associated with the given key.
   static Handle<Derived> Put(Isolate* isolate, Handle<Derived> table,
diff --git a/src/objects/heap-object-inl.h b/src/objects/heap-object-inl.h
index 608152ac998..12f9180061a 100644
--- a/src/objects/heap-object-inl.h
+++ b/src/objects/heap-object-inl.h
@@ -25,7 +25,7 @@ namespace internal {
   /* The cage_base passed here must be the base of the main pointer */        \
   /* compression cage, i.e. the one where the Map space is allocated. */      \
   bool Is##type(Tagged<HeapObject> obj, PtrComprCageBase cage_base) {         \
-    Map map_object = obj->map(cage_base);                                     \
+    Tagged<Map> map_object = obj->map(cage_base);                             \
     return InstanceTypeChecker::Is##type(map_object);                         \
   }
 
diff --git a/src/objects/heap-object.h b/src/objects/heap-object.h
index ba996bd8307..97ce19a86cd 100644
--- a/src/objects/heap-object.h
+++ b/src/objects/heap-object.h
@@ -343,13 +343,6 @@ constexpr detail::TaggedOperatorArrowRef<HeapObject>
 Tagged<HeapObject>::operator->() const {
   return detail::TaggedOperatorArrowRef<HeapObject>{ToRawPtr()};
 }
-// Implicit conversions and explicit casts to/from raw pointers
-// TODO(leszeks): Remove once we're using Tagged everywhere.
-// NOLINTNEXTLINE
-constexpr Tagged<HeapObject>::operator HeapObject() {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return ToRawPtr();
-}
 constexpr HeapObject Tagged<HeapObject>::ToRawPtr() const {
   return HeapObject(this->ptr(), HeapObject::SkipTypeCheckTag{});
 }
diff --git a/src/objects/js-function.cc b/src/objects/js-function.cc
index 64cb8265b93..2859ef1cb9d 100644
--- a/src/objects/js-function.cc
+++ b/src/objects/js-function.cc
@@ -1192,7 +1192,7 @@ void JSFunction::PrintName(FILE* out) {
 
 namespace {
 
-bool UseFastFunctionNameLookup(Isolate* isolate, Map map) {
+bool UseFastFunctionNameLookup(Isolate* isolate, Tagged<Map> map) {
   DCHECK(IsJSFunctionMap(map));
   if (map->NumberOfOwnDescriptors() <
       JSFunction::kMinDescriptorsForFastBindAndWrap) {
@@ -1288,7 +1288,7 @@ Handle<String> JSFunction::ToString(Handle<JSFunction> function) {
     Handle<Object> maybe_class_positions = JSReceiver::GetDataProperty(
         isolate, function, isolate->factory()->class_positions_symbol());
     if (IsClassPositions(*maybe_class_positions)) {
-      ClassPositions class_positions =
+      Tagged<ClassPositions> class_positions =
           ClassPositions::cast(*maybe_class_positions);
       int start_position = class_positions->start();
       int end_position = class_positions->end();
@@ -1411,7 +1411,7 @@ void JSFunction::CalculateInstanceSizeHelper(InstanceType instance_type,
 void JSFunction::ClearAllTypeFeedbackInfoForTesting() {
   ResetIfCodeFlushed();
   if (has_feedback_vector()) {
-    FeedbackVector vector = feedback_vector();
+    Tagged<FeedbackVector> vector = feedback_vector();
     Isolate* isolate = GetIsolate();
     if (vector->ClearAllSlotsForTesting(isolate)) {
       IC::OnFeedbackChanged(isolate, vector, FeedbackSlot::Invalid(),
diff --git a/src/objects/js-objects.cc b/src/objects/js-objects.cc
index 6094323ee09..692aa0b0252 100644
--- a/src/objects/js-objects.cc
+++ b/src/objects/js-objects.cc
@@ -636,7 +636,7 @@ Handle<String> JSReceiver::GetConstructorName(Isolate* isolate,
   return GetConstructorHelper(isolate, receiver).second;
 }
 
-base::Optional<NativeContext> JSReceiver::GetCreationContextRaw() {
+base::Optional<Tagged<NativeContext>> JSReceiver::GetCreationContextRaw() {
   DisallowGarbageCollection no_gc;
   Tagged<JSFunction> function;
   {
@@ -667,7 +667,7 @@ base::Optional<NativeContext> JSReceiver::GetCreationContextRaw() {
 }
 
 MaybeHandle<NativeContext> JSReceiver::GetCreationContext() {
-  base::Optional<NativeContext> maybe_context = GetCreationContextRaw();
+  base::Optional<Tagged<NativeContext>> maybe_context = GetCreationContextRaw();
   if (!maybe_context.has_value()) return {};
   return handle(maybe_context.value(), GetIsolate());
 }
diff --git a/src/objects/js-objects.h b/src/objects/js-objects.h
index 82d367affe3..ba159d81ad3 100644
--- a/src/objects/js-objects.h
+++ b/src/objects/js-objects.h
@@ -248,7 +248,8 @@ class JSReceiver : public TorqueGeneratedJSReceiver<JSReceiver, HeapObject> {
   static Handle<String> GetConstructorName(Isolate* isolate,
                                            Handle<JSReceiver> receiver);
 
-  V8_EXPORT_PRIVATE base::Optional<NativeContext> GetCreationContextRaw();
+  V8_EXPORT_PRIVATE base::Optional<Tagged<NativeContext>>
+  GetCreationContextRaw();
   V8_EXPORT_PRIVATE MaybeHandle<NativeContext> GetCreationContext();
 
   V8_WARN_UNUSED_RESULT static inline Maybe<PropertyAttributes>
diff --git a/src/objects/js-struct.cc b/src/objects/js-struct.cc
index b8a583231ea..c55b5d09ec1 100644
--- a/src/objects/js-struct.cc
+++ b/src/objects/js-struct.cc
@@ -13,7 +13,7 @@ namespace internal {
 
 namespace {
 
-void PrepareMapCommon(Map map) {
+void PrepareMapCommon(Tagged<Map> map) {
   DCHECK(IsAlwaysSharedSpaceJSObjectMap(map));
   DisallowGarbageCollection no_gc;
   // Shared objects have fixed layout ahead of time, so there's no slack.
diff --git a/src/objects/keys.cc b/src/objects/keys.cc
index 15a2fcd6077..c35ff86f991 100644
--- a/src/objects/keys.cc
+++ b/src/objects/keys.cc
@@ -897,7 +897,7 @@ void CopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
   int length = storage->length();
 
   DisallowGarbageCollection no_gc;
-  Dictionary raw_dictionary = *dictionary;
+  Tagged<Dictionary> raw_dictionary = *dictionary;
   Tagged<FixedArray> raw_storage = *storage;
   EnumIndexComparator<Dictionary> cmp(raw_dictionary);
   // Use AtomicSlot wrapper to ensure that std::sort uses atomic load and
@@ -906,7 +906,7 @@ void CopyEnumKeysTo(Isolate* isolate, Handle<Dictionary> dictionary,
   std::sort(start, start + length, cmp);
   for (int i = 0; i < length; i++) {
     InternalIndex index(Smi::ToInt(raw_storage->get(i)));
-    raw_storage->set(i, raw_dictionary.NameAt(index));
+    raw_storage->set(i, raw_dictionary->NameAt(index));
   }
 }
 
@@ -957,10 +957,10 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
     DisallowGarbageCollection no_gc;
     for (InternalIndex i : dictionary->IterateEntries()) {
       Tagged<Object> key;
-      Dictionary raw_dictionary = *dictionary;
-      if (!raw_dictionary.ToKey(roots, i, &key)) continue;
+      Tagged<Dictionary> raw_dictionary = *dictionary;
+      if (!raw_dictionary->ToKey(roots, i, &key)) continue;
       if (Object::FilterKey(key, filter)) continue;
-      PropertyDetails details = raw_dictionary.DetailsAt(i);
+      PropertyDetails details = raw_dictionary->DetailsAt(i);
       if ((int{details.attributes()} & filter) != 0) {
         AllowGarbageCollection gc;
         // This might allocate, but {key} is not used afterwards.
@@ -969,7 +969,7 @@ ExceptionStatus CollectKeysFromDictionary(Handle<Dictionary> dictionary,
       }
       if (filter & ONLY_ALL_CAN_READ) {
         if (details.kind() != PropertyKind::kAccessor) continue;
-        Tagged<Object> accessors = raw_dictionary.ValueAt(i);
+        Tagged<Object> accessors = raw_dictionary->ValueAt(i);
         if (!IsAccessorInfo(accessors)) continue;
         if (!AccessorInfo::cast(accessors)->all_can_read()) continue;
       }
diff --git a/src/objects/lookup-cache.h b/src/objects/lookup-cache.h
index 0949ec90ddb..a1618ae0846 100644
--- a/src/objects/lookup-cache.h
+++ b/src/objects/lookup-cache.h
@@ -45,8 +45,8 @@ class DescriptorLookupCache {
 
   static const int kLength = 64;
   struct Key {
-    Map source;
-    Name name;
+    Tagged<Map> source;
+    Tagged<Name> name;
   };
 
   Key keys_[kLength];
diff --git a/src/objects/lookup.cc b/src/objects/lookup.cc
index a5fd2387674..abf82b6dda4 100644
--- a/src/objects/lookup.cc
+++ b/src/objects/lookup.cc
@@ -1398,7 +1398,7 @@ bool LookupIterator::LookupCachedProperty(Handle<AccessorPair> accessor_pair) {
   DCHECK(IsAccessorPair(*GetAccessors(), isolate_));
 
   Tagged<Object> getter = accessor_pair->getter(isolate_);
-  base::Optional<Name> maybe_name =
+  base::Optional<Tagged<Name>> maybe_name =
       FunctionTemplateInfo::TryGetCachedPropertyName(isolate(), getter);
   if (!maybe_name.has_value()) return false;
 
@@ -1589,7 +1589,7 @@ ConcurrentLookupIterator::TryGetPropertyCell(Isolate* isolate,
     Tagged<Object> maybe_accessor_pair = cell->value(kAcquireLoad);
     if (!IsAccessorPair(maybe_accessor_pair)) return {};
 
-    base::Optional<Name> maybe_cached_property_name =
+    base::Optional<Tagged<Name>> maybe_cached_property_name =
         FunctionTemplateInfo::TryGetCachedPropertyName(
             isolate, AccessorPair::cast(maybe_accessor_pair)
                          ->getter(isolate, kAcquireLoad));
diff --git a/src/objects/map-inl.h b/src/objects/map-inl.h
index 4856ecb7a8d..fdf7b5e38b1 100644
--- a/src/objects/map-inl.h
+++ b/src/objects/map-inl.h
@@ -791,7 +791,7 @@ void Map::AppendDescriptor(Isolate* isolate, Descriptor* desc) {
 }
 
 bool Map::ConcurrentIsMap(PtrComprCageBase cage_base,
-                          const Object& object) const {
+                          Tagged<Object> object) const {
   return IsHeapObject(object) && HeapObject::cast(object)->map(cage_base) ==
                                      GetReadOnlyRoots(cage_base).meta_map();
 }
diff --git a/src/objects/map-updater.cc b/src/objects/map-updater.cc
index 613a4921637..07b5e753754 100644
--- a/src/objects/map-updater.cc
+++ b/src/objects/map-updater.cc
@@ -342,9 +342,9 @@ IntegrityLevelTransitionInfo DetectIntegrityLevelTransitions(
 }  // namespace
 
 // static
-base::Optional<Map> MapUpdater::TryUpdateNoLock(Isolate* isolate,
-                                                Tagged<Map> old_map,
-                                                ConcurrencyMode cmode) {
+base::Optional<Tagged<Map>> MapUpdater::TryUpdateNoLock(Isolate* isolate,
+                                                        Tagged<Map> old_map,
+                                                        ConcurrencyMode cmode) {
   DisallowGarbageCollection no_gc;
 
   // Check the state of the root map.
@@ -1142,7 +1142,7 @@ void MapUpdater::UpdateFieldType(Isolate* isolate, Handle<Map> map,
     JSObject::InvalidatePrototypeChains(*map);
   }
 
-  std::queue<Map> backlog;
+  std::queue<Tagged<Map>> backlog;
   backlog.push(*map);
 
   while (!backlog.empty()) {
diff --git a/src/objects/map-updater.h b/src/objects/map-updater.h
index f590ed38bbb..958b050e50f 100644
--- a/src/objects/map-updater.h
+++ b/src/objects/map-updater.h
@@ -69,7 +69,7 @@ class V8_EXPORT_PRIVATE MapUpdater {
 
   // As above but does not mutate maps; instead, we attempt to replay existing
   // transitions to find an updated map. No lock is taken.
-  static base::Optional<Map> TryUpdateNoLock(
+  static base::Optional<Tagged<Map>> TryUpdateNoLock(
       Isolate* isolate, Tagged<Map> old_map,
       ConcurrencyMode cmode) V8_WARN_UNUSED_RESULT;
 
diff --git a/src/objects/map.cc b/src/objects/map.cc
index 798c9c59535..0a252e04205 100644
--- a/src/objects/map.cc
+++ b/src/objects/map.cc
@@ -48,7 +48,7 @@ Tagged<Map> Map::GetPrototypeChainRootMap(Isolate* isolate) const {
 }
 
 // static
-base::Optional<JSFunction> Map::GetConstructorFunction(
+base::Optional<Tagged<JSFunction>> Map::GetConstructorFunction(
     Tagged<Map> map, Tagged<Context> native_context) {
   DisallowGarbageCollection no_gc;
   if (IsPrimitiveMap(map)) {
@@ -716,7 +716,7 @@ MaybeHandle<Map> Map::TryUpdate(Isolate* isolate, Handle<Map> old_map) {
     }
   }
 
-  base::Optional<Map> new_map = MapUpdater::TryUpdateNoLock(
+  base::Optional<Tagged<Map>> new_map = MapUpdater::TryUpdateNoLock(
       isolate, *old_map, ConcurrencyMode::kSynchronous);
   if (!new_map.has_value()) return MaybeHandle<Map>();
   if (v8_flags.fast_map_update) {
@@ -1076,9 +1076,10 @@ static Handle<Map> AddMissingElementsTransitions(Isolate* isolate,
 }
 
 // static
-base::Optional<Map> Map::TryAsElementsKind(Isolate* isolate, Handle<Map> map,
-                                           ElementsKind kind,
-                                           ConcurrencyMode cmode) {
+base::Optional<Tagged<Map>> Map::TryAsElementsKind(Isolate* isolate,
+                                                   Handle<Map> map,
+                                                   ElementsKind kind,
+                                                   ConcurrencyMode cmode) {
   Tagged<Map> closest_map =
       FindClosestElementsTransition(isolate, *map, kind, cmode);
   if (closest_map->elements_kind() != kind) return {};
diff --git a/src/objects/map.h b/src/objects/map.h
index 2853bdc738f..88956b22848 100644
--- a/src/objects/map.h
+++ b/src/objects/map.h
@@ -234,7 +234,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   static const int kNoConstructorFunctionIndex = 0;
   inline int GetConstructorFunctionIndex() const;
   inline void SetConstructorFunctionIndex(int value);
-  static base::Optional<JSFunction> GetConstructorFunction(
+  static base::Optional<Tagged<JSFunction>> GetConstructorFunction(
       Tagged<Map> map, Tagged<Context> native_context);
 
   // Retrieve interceptors.
@@ -751,10 +751,10 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   static Handle<Map> TransitionElementsTo(Isolate* isolate, Handle<Map> map,
                                           ElementsKind to_kind);
 
-  static base::Optional<Map> TryAsElementsKind(Isolate* isolate,
-                                               Handle<Map> map,
-                                               ElementsKind kind,
-                                               ConcurrencyMode cmode);
+  static base::Optional<Tagged<Map>> TryAsElementsKind(Isolate* isolate,
+                                                       Handle<Map> map,
+                                                       ElementsKind kind,
+                                                       ConcurrencyMode cmode);
   V8_EXPORT_PRIVATE static Handle<Map> AsElementsKind(Isolate* isolate,
                                                       Handle<Map> map,
                                                       ElementsKind kind);
@@ -988,7 +988,7 @@ class Map : public TorqueGeneratedMap<Map, HeapObject> {
   // This is the equivalent of IsMap() but avoids reading the instance type so
   // it can be used concurrently without acquire load.
   V8_INLINE bool ConcurrentIsMap(PtrComprCageBase cage_base,
-                                 const Object& object) const;
+                                 Tagged<Object> object) const;
 
   // Use the high-level instance_descriptors/SetInstanceDescriptors instead.
   DECL_RELEASE_SETTER(instance_descriptors, Tagged<DescriptorArray>)
diff --git a/src/objects/module-inl.h b/src/objects/module-inl.h
index 347ccdc33a8..c0452da13e8 100644
--- a/src/objects/module-inl.h
+++ b/src/objects/module-inl.h
@@ -36,7 +36,7 @@ ACCESSORS(SourceTextModule, async_parent_modules, Tagged<ArrayList>,
           kAsyncParentModulesOffset)
 
 struct Module::Hash {
-  V8_INLINE size_t operator()(Module const& module) const {
+  V8_INLINE size_t operator()(Tagged<Module> module) const {
     return module->hash();
   }
 };
diff --git a/src/objects/module.cc b/src/objects/module.cc
index d4b170e2f63..22b0e592240 100644
--- a/src/objects/module.cc
+++ b/src/objects/module.cc
@@ -459,8 +459,8 @@ bool Module::IsGraphAsync(Isolate* isolate) const {
 
   Zone zone(isolate->allocator(), ZONE_NAME);
   const size_t bucket_count = 2;
-  ZoneUnorderedSet<Module, Module::Hash> visited(&zone, bucket_count);
-  ZoneVector<SourceTextModule> worklist(&zone);
+  ZoneUnorderedSet<Tagged<Module>, Module::Hash> visited(&zone, bucket_count);
+  ZoneVector<Tagged<SourceTextModule>> worklist(&zone);
   visited.insert(root);
   worklist.push_back(root);
 
diff --git a/src/objects/object-macros.h b/src/objects/object-macros.h
index ab2467ef41e..1f1962f1bb1 100644
--- a/src/objects/object-macros.h
+++ b/src/objects/object-macros.h
@@ -382,7 +382,7 @@
 #define SMI_ACCESSORS_CHECKED(holder, name, offset, condition)   \
   int holder::name() const {                                     \
     DCHECK(condition);                                           \
-    Smi value = TaggedField<Smi, offset>::load(*this);           \
+    Tagged<Smi> value = TaggedField<Smi, offset>::load(*this);   \
     return value.value();                                        \
   }                                                              \
   void holder::set_##name(int value) {                           \
@@ -399,7 +399,7 @@
 
 #define RELEASE_ACQUIRE_SMI_ACCESSORS(holder, name, offset)              \
   int holder::name(AcquireLoadTag) const {                               \
-    Smi value = TaggedField<Smi, offset>::Acquire_Load(*this);           \
+    Tagged<Smi> value = TaggedField<Smi, offset>::Acquire_Load(*this);   \
     return value.value();                                                \
   }                                                                      \
   void holder::set_##name(int value, ReleaseStoreTag) {                  \
@@ -412,7 +412,7 @@
 
 #define RELAXED_SMI_ACCESSORS(holder, name, offset)                      \
   int holder::name(RelaxedLoadTag) const {                               \
-    Smi value = TaggedField<Smi, offset>::Relaxed_Load(*this);           \
+    Tagged<Smi> value = TaggedField<Smi, offset>::Relaxed_Load(*this);   \
     return value.value();                                                \
   }                                                                      \
   void holder::set_##name(int value, RelaxedStoreTag) {                  \
diff --git a/src/objects/object-type.cc b/src/objects/object-type.cc
index 954dcbfc356..2424f4aa107 100644
--- a/src/objects/object-type.cc
+++ b/src/objects/object-type.cc
@@ -15,7 +15,7 @@ Address CheckObjectType(Address raw_value, Address raw_type,
                         Address raw_location) {
 #ifdef DEBUG
   ObjectType type = static_cast<ObjectType>(Smi(raw_type).value());
-  String location = String::cast(Object(raw_location));
+  Tagged<String> location = String::cast(Object(raw_location));
   const char* expected;
 
   if (HAS_WEAK_HEAP_OBJECT_TAG(raw_value)) {
@@ -44,7 +44,7 @@ Address CheckObjectType(Address raw_value, Address raw_type,
 #undef TYPE_STRUCT_CASE
     }
   } else {
-    Object value(raw_value);
+    Tagged<Object> value(raw_value);
     switch (type) {
       case ObjectType::kHeapObjectReference:
         if (!IsSmi(value)) return Smi::FromInt(0).ptr();
diff --git a/src/objects/objects-body-descriptors-inl.h b/src/objects/objects-body-descriptors-inl.h
index 9b1d7b00a8b..1d6189a344e 100644
--- a/src/objects/objects-body-descriptors-inl.h
+++ b/src/objects/objects-body-descriptors-inl.h
@@ -409,15 +409,15 @@ class V8_EXPORT_PRIVATE SmallOrderedHashTable<Derived>::BodyDescriptor final
   template <typename ObjectVisitor>
   static inline void IterateBody(Tagged<Map> map, Tagged<HeapObject> obj,
                                  int object_size, ObjectVisitor* v) {
-    Derived table = Derived::cast(obj);
+    Tagged<Derived> table = Derived::cast(obj);
     int start_offset = DataTableStartOffset();
-    int end_offset = table.GetBucketsStartOffset();
+    int end_offset = table->GetBucketsStartOffset();
     IteratePointers(obj, start_offset, end_offset, v);
   }
 
   static inline int SizeOf(Tagged<Map> map, Tagged<HeapObject> obj) {
-    Derived table = Derived::cast(obj);
-    return Derived::SizeFor(table.Capacity());
+    Tagged<Derived> table = Derived::cast(obj);
+    return Derived::SizeFor(table->Capacity());
   }
 };
 
diff --git a/src/objects/objects-body-descriptors.h b/src/objects/objects-body-descriptors.h
index eaeca1e5a3b..41da88f07c3 100644
--- a/src/objects/objects-body-descriptors.h
+++ b/src/objects/objects-body-descriptors.h
@@ -18,7 +18,8 @@ namespace internal {
 // 1) Iterate object's body using stateful object visitor.
 //
 //   template <typename ObjectVisitor>
-//   static inline void IterateBody(Map map, HeapObject obj, int object_size,
+//   static inline void IterateBody(Tagged<Map> map, HeapObject obj, int
+//   object_size,
 //                                  ObjectVisitor* v);
 class BodyDescriptorBase {
  public:
diff --git a/src/objects/objects.cc b/src/objects/objects.cc
index d55f192de84..84e1892ac8f 100644
--- a/src/objects/objects.cc
+++ b/src/objects/objects.cc
@@ -150,9 +150,9 @@ ShouldThrow GetShouldThrow(Isolate* isolate, Maybe<ShouldThrow> should_throw) {
 
     // Get the language mode from closure.
     JavaScriptFrame* js_frame = static_cast<JavaScriptFrame*>(it.frame());
-    std::vector<SharedFunctionInfo> functions;
+    std::vector<Tagged<SharedFunctionInfo>> functions;
     js_frame->GetFunctions(&functions);
-    LanguageMode closure_language_mode = functions.back().language_mode();
+    LanguageMode closure_language_mode = functions.back()->language_mode();
     if (closure_language_mode > mode) {
       mode = closure_language_mode;
     }
@@ -5083,15 +5083,15 @@ Handle<Derived> HashTable<Derived, Shape>::NewInternal(
 
 template <typename Derived, typename Shape>
 void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base,
-                                       Derived new_table) {
+                                       Tagged<Derived> new_table) {
   DisallowGarbageCollection no_gc;
-  WriteBarrierMode mode = new_table.GetWriteBarrierMode(no_gc);
+  WriteBarrierMode mode = new_table->GetWriteBarrierMode(no_gc);
 
-  DCHECK_LT(NumberOfElements(), new_table.Capacity());
+  DCHECK_LT(NumberOfElements(), new_table->Capacity());
 
   // Copy prefix to new array.
   for (int i = kPrefixStartIndex; i < kElementsStartIndex; i++) {
-    new_table.set(i, get(cage_base, i), mode);
+    new_table->set(i, get(cage_base, i), mode);
   }
 
   // Rehash the elements.
@@ -5102,14 +5102,14 @@ void HashTable<Derived, Shape>::Rehash(PtrComprCageBase cage_base,
     if (!IsKey(roots, k)) continue;
     uint32_t hash = Shape::HashForObject(roots, k);
     uint32_t insertion_index =
-        EntryToIndex(new_table.FindInsertionEntry(cage_base, roots, hash));
-    new_table.set_key(insertion_index, get(cage_base, from_index), mode);
+        EntryToIndex(new_table->FindInsertionEntry(cage_base, roots, hash));
+    new_table->set_key(insertion_index, get(cage_base, from_index), mode);
     for (int j = 1; j < Shape::kEntrySize; j++) {
-      new_table.set(insertion_index + j, get(cage_base, from_index + j), mode);
+      new_table->set(insertion_index + j, get(cage_base, from_index + j), mode);
     }
   }
-  new_table.SetNumberOfElements(NumberOfElements());
-  new_table.SetNumberOfDeletedElements(0);
+  new_table->SetNumberOfElements(NumberOfElements());
+  new_table->SetNumberOfDeletedElements(0);
 }
 
 template <typename Derived, typename Shape>
@@ -5132,7 +5132,7 @@ void HashTable<Derived, Shape>::Swap(InternalIndex entry1, InternalIndex entry2,
                                      WriteBarrierMode mode) {
   int index1 = EntryToIndex(entry1);
   int index2 = EntryToIndex(entry2);
-  Object temp[Shape::kEntrySize];
+  Tagged<Object> temp[Shape::kEntrySize];
   Derived* self = static_cast<Derived*>(this);
   for (int j = 0; j < Shape::kEntrySize; j++) {
     temp[j] = get(index1 + j);
@@ -5659,12 +5659,12 @@ Handle<FixedArray> BaseNameDictionary<Derived, Shape>::IterationIndices(
 template <typename Derived, typename Shape>
 Tagged<Object> Dictionary<Derived, Shape>::SlowReverseLookup(
     Tagged<Object> value) {
-  Derived dictionary = Derived::cast(*this);
-  ReadOnlyRoots roots = dictionary.GetReadOnlyRoots();
-  for (InternalIndex i : dictionary.IterateEntries()) {
+  Tagged<Derived> dictionary = Derived::cast(*this);
+  ReadOnlyRoots roots = dictionary->GetReadOnlyRoots();
+  for (InternalIndex i : dictionary->IterateEntries()) {
     Tagged<Object> k;
-    if (!dictionary.ToKey(roots, i, &k)) continue;
-    Tagged<Object> e = dictionary.ValueAt(i);
+    if (!dictionary->ToKey(roots, i, &k)) continue;
+    Tagged<Object> e = dictionary->ValueAt(i);
     if (e == value) return k;
   }
   return roots.undefined_value();
@@ -5867,13 +5867,13 @@ void ObjectHashTableBase<Derived, Shape>::RemoveEntry(InternalIndex entry) {
 }
 
 template <typename Derived, int N>
-std::array<Object, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
+std::array<Tagged<Object>, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
     Handle<Object> key) {
   return Lookup(GetPtrComprCageBase(*this), key);
 }
 
 template <typename Derived, int N>
-std::array<Object, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
+std::array<Tagged<Object>, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
     PtrComprCageBase cage_base, Handle<Object> key) {
   DisallowGarbageCollection no_gc;
 
@@ -5893,7 +5893,7 @@ std::array<Object, N> ObjectMultiHashTableBase<Derived, N>::Lookup(
 
   int start_index = this->EntryToIndex(entry) +
                     ObjectMultiHashTableShape<N>::kEntryValueIndex;
-  std::array<Object, N> values;
+  std::array<Tagged<Object>, N> values;
   for (int i = 0; i < N; i++) {
     values[i] = this->get(start_index + i);
     DCHECK(!IsTheHole(values[i]));
diff --git a/src/objects/objects.h b/src/objects/objects.h
index 31f7766930a..33d29946ef2 100644
--- a/src/objects/objects.h
+++ b/src/objects/objects.h
@@ -746,12 +746,6 @@ class Object : public TaggedImpl<HeapObjectReferenceType::STRONG, Address> {
 constexpr Tagged<Object>::Tagged(Object raw) : TaggedBase(raw.ptr()) {
   static_assert(kTaggedCanConvertToRawObjects);
 }
-template <typename U, typename>
-// NOLINTNEXTLINE
-constexpr Tagged<Object>::operator U() {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return ToRawPtr();
-}
 
 constexpr Object Tagged<Object>::ToRawPtr() const { return Object(ptr()); }
 
diff --git a/src/objects/ordered-hash-table-inl.h b/src/objects/ordered-hash-table-inl.h
index 84fb8a49d40..1eebb812c81 100644
--- a/src/objects/ordered-hash-table-inl.h
+++ b/src/objects/ordered-hash-table-inl.h
@@ -213,11 +213,11 @@ void SmallOrderedHashTable<Derived>::SetDataEntry(int entry, int relative_index,
 
 template <class Derived, class TableType>
 Tagged<Object> OrderedHashTableIterator<Derived, TableType>::CurrentKey() {
-  TableType table = TableType::cast(this->table());
+  Tagged<TableType> table = TableType::cast(this->table());
   int index = Smi::ToInt(this->index());
   DCHECK_LE(0, index);
   InternalIndex entry(index);
-  Tagged<Object> key = table.KeyAt(entry);
+  Tagged<Object> key = table->KeyAt(entry);
   DCHECK(!IsTheHole(key));
   return key;
 }
diff --git a/src/objects/ordered-hash-table.cc b/src/objects/ordered-hash-table.cc
index a1902775834..319007c96a0 100644
--- a/src/objects/ordered-hash-table.cc
+++ b/src/objects/ordered-hash-table.cc
@@ -1477,7 +1477,7 @@ bool OrderedHashTableIterator<Derived, TableType>::HasMore() {
 
   Transition();
 
-  TableType table = TableType::cast(this->table());
+  Tagged<TableType> table = TableType::cast(this->table());
   int index = Smi::ToInt(this->index());
   int used_capacity = table->UsedCapacity();
 
diff --git a/src/objects/ordered-hash-table.h b/src/objects/ordered-hash-table.h
index d07c7d631b8..299f8eaf783 100644
--- a/src/objects/ordered-hash-table.h
+++ b/src/objects/ordered-hash-table.h
@@ -254,8 +254,9 @@ class OrderedHashTable : public FixedArray {
     set(NumberOfDeletedElementsIndex(), Smi::FromInt(num));
   }
 
-
-  void SetNextTable(Derived next_table) { set(NextTableIndex(), next_table); }
+  void SetNextTable(Tagged<Derived> next_table) {
+    set(NextTableIndex(), next_table);
+  }
 
   void SetRemovedIndexAt(int index, int removed_index) {
     return set(RemovedHolesIndex() + index, Smi::FromInt(removed_index));
diff --git a/src/objects/prototype.h b/src/objects/prototype.h
index d526c33e9ca..1728264dd44 100644
--- a/src/objects/prototype.h
+++ b/src/objects/prototype.h
@@ -48,7 +48,7 @@ class PrototypeIterator {
   inline bool HasAccess() const;
 
   template <typename T = HeapObject>
-  T GetCurrent() const {
+  Tagged<T> GetCurrent() const {
     DCHECK(handle_.is_null());
     return T::cast(object_);
   }
@@ -56,7 +56,7 @@ class PrototypeIterator {
   template <typename T = HeapObject>
   static Handle<T> GetCurrent(const PrototypeIterator& iterator) {
     DCHECK(!iterator.handle_.is_null());
-    DCHECK_EQ(iterator.object_, Object());
+    DCHECK_EQ(iterator.object_, Tagged<HeapObject>());
     return Handle<T>::cast(iterator.handle_);
   }
 
@@ -75,7 +75,7 @@ class PrototypeIterator {
 
  private:
   Isolate* isolate_;
-  Object object_;
+  Tagged<Object> object_ = Tagged<HeapObject>();
   Handle<HeapObject> handle_;
   WhereToEnd where_to_end_;
   bool is_at_end_;
diff --git a/src/objects/scope-info.cc b/src/objects/scope-info.cc
index 586c7a5d16f..f9b1831b3d5 100644
--- a/src/objects/scope-info.cc
+++ b/src/objects/scope-info.cc
@@ -612,7 +612,7 @@ Tagged<Object> ScopeInfo::get(PtrComprCageBase cage_base, int index) const {
 
 void ScopeInfo::set(int index, Tagged<Smi> value) {
   DCHECK_LT(static_cast<unsigned>(index), static_cast<unsigned>(length()));
-  DCHECK(IsSmi(Object(value)));
+  DCHECK(IsSmi(Tagged<Object>(value)));
   int offset = OffsetOfElementAt(index);
   RELAXED_WRITE_FIELD(*this, offset, value);
 }
@@ -1006,7 +1006,7 @@ int ScopeInfo::ContextSlotIndex(Handle<String> name) {
   return ContextSlotIndex(name, &lookup_result);
 }
 
-std::pair<String, int> ScopeInfo::SavedClassVariable() const {
+std::pair<Tagged<String>, int> ScopeInfo::SavedClassVariable() const {
   DCHECK(HasSavedClassVariableBit::decode(Flags()));
   if (HasInlinedLocalNames()) {
     // The saved class variable info corresponds to the context slot index.
@@ -1019,7 +1019,7 @@ std::pair<String, int> ScopeInfo::SavedClassVariable() const {
     // The saved class variable info corresponds to the offset in the hash
     // table storage.
     InternalIndex entry(saved_class_variable_info());
-    NameToIndexHashTable table = context_local_names_hashtable();
+    Tagged<NameToIndexHashTable> table = context_local_names_hashtable();
     Tagged<Object> name = table->KeyAt(entry);
     DCHECK(IsString(name));
     return std::make_pair(String::cast(name), table->IndexAt(entry));
diff --git a/src/objects/scope-info.h b/src/objects/scope-info.h
index 6cb045709a9..91b0d88864e 100644
--- a/src/objects/scope-info.h
+++ b/src/objects/scope-info.h
@@ -212,7 +212,7 @@ class ScopeInfo : public TorqueGeneratedScopeInfo<ScopeInfo, HeapObject> {
   // Lookup support for serialized scope info.  Returns the name and index of
   // the saved class variable in context local slots if scope is a class scope
   // and it contains static private methods that may be accessed.
-  std::pair<String, int> SavedClassVariable() const;
+  std::pair<Tagged<String>, int> SavedClassVariable() const;
 
   FunctionKind function_kind() const;
 
diff --git a/src/objects/shared-function-info-inl.h b/src/objects/shared-function-info-inl.h
index c5a7cb6e29e..b8861c24eb7 100644
--- a/src/objects/shared-function-info-inl.h
+++ b/src/objects/shared-function-info-inl.h
@@ -614,10 +614,11 @@ Tagged<BytecodeArray> SharedFunctionInfo::GetBytecodeArray(
 
   DCHECK(HasBytecodeArray());
 
-  base::Optional<DebugInfo> debug_info =
+  base::Optional<Tagged<DebugInfo>> debug_info =
       TryGetDebugInfo(isolate->GetMainThreadIsolateUnsafe());
-  if (debug_info.has_value() && debug_info->HasInstrumentedBytecodeArray()) {
-    return debug_info->OriginalBytecodeArray();
+  if (debug_info.has_value() &&
+      debug_info.value()->HasInstrumentedBytecodeArray()) {
+    return debug_info.value()->OriginalBytecodeArray();
   }
 
   return GetActiveBytecodeArray();
@@ -757,23 +758,26 @@ void SharedFunctionInfo::set_asm_wasm_data(Tagged<AsmWasmData> data,
 
 const wasm::WasmModule* SharedFunctionInfo::wasm_module() const {
   if (!HasWasmExportedFunctionData()) return nullptr;
-  const WasmExportedFunctionData& function_data = wasm_exported_function_data();
-  const WasmInstanceObject& wasm_instance = function_data->instance();
-  const WasmModuleObject& wasm_module_object = wasm_instance->module_object();
+  Tagged<WasmExportedFunctionData> function_data =
+      wasm_exported_function_data();
+  Tagged<WasmInstanceObject> wasm_instance = function_data->instance();
+  Tagged<WasmModuleObject> wasm_module_object = wasm_instance->module_object();
   return wasm_module_object->module();
 }
 
 const wasm::FunctionSig* SharedFunctionInfo::wasm_function_signature() const {
   const wasm::WasmModule* module = wasm_module();
   if (!module) return nullptr;
-  const WasmExportedFunctionData& function_data = wasm_exported_function_data();
+  Tagged<WasmExportedFunctionData> function_data =
+      wasm_exported_function_data();
   DCHECK_LT(function_data->function_index(), module->functions.size());
   return module->functions[function_data->function_index()].sig;
 }
 
 int SharedFunctionInfo::wasm_function_index() const {
   if (!HasWasmExportedFunctionData()) return -1;
-  const WasmExportedFunctionData& function_data = wasm_exported_function_data();
+  Tagged<WasmExportedFunctionData> function_data =
+      wasm_exported_function_data();
   DCHECK_GE(function_data->function_index(), 0);
   DCHECK_LT(function_data->function_index(), wasm_module()->functions.size());
   return function_data->function_index();
diff --git a/src/objects/shared-function-info.h b/src/objects/shared-function-info.h
index fd4a93d7bce..ae342178b49 100644
--- a/src/objects/shared-function-info.h
+++ b/src/objects/shared-function-info.h
@@ -757,7 +757,7 @@ static_assert(SharedFunctionInfo::kSize == kStaticRootsSFISize);
 struct SourceCodeOf {
   explicit SourceCodeOf(Tagged<SharedFunctionInfo> v, int max = -1)
       : value(v), max_length(max) {}
-  const SharedFunctionInfo value;
+  const Tagged<SharedFunctionInfo> value;
   int max_length;
 };
 
diff --git a/src/objects/smi.h b/src/objects/smi.h
index 39357f0faa0..fba42f6d9e7 100644
--- a/src/objects/smi.h
+++ b/src/objects/smi.h
@@ -126,11 +126,6 @@ CAST_ACCESSOR(Smi)
 constexpr Tagged<Smi>::Tagged(Smi raw) : TaggedBase(raw.ptr()) {
   static_assert(kTaggedCanConvertToRawObjects);
 }
-// NOLINTNEXTLINE
-constexpr Tagged<Smi>::operator Smi() {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return Smi(ptr());
-}
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/string-inl.h b/src/objects/string-inl.h
index cf47e6fbda7..b9ec3958e7a 100644
--- a/src/objects/string-inl.h
+++ b/src/objects/string-inl.h
@@ -290,10 +290,10 @@ inline TResult StringShape::DispatchToSpecificType(Tagged<String> str,
                                                    TArgs&&... args) {
   class CastingDispatcher : public AllStatic {
    public:
-#define DEFINE_METHOD(Type)                                         \
-  static inline TResult Handle##Type(String str, TArgs&&... args) { \
-    return TDispatcher::Handle##Type(Type::cast(str),               \
-                                     std::forward<TArgs>(args)...); \
+#define DEFINE_METHOD(Type)                                                 \
+  static inline TResult Handle##Type(Tagged<String> str, TArgs&&... args) { \
+    return TDispatcher::Handle##Type(Type::cast(str),                       \
+                                     std::forward<TArgs>(args)...);         \
   }
     STRING_CLASS_TYPES(DEFINE_METHOD)
 #undef DEFINE_METHOD
@@ -833,11 +833,11 @@ uint16_t String::GetImpl(
 
   class StringGetDispatcher : public AllStatic {
    public:
-#define DEFINE_METHOD(Type)                                  \
-  static inline uint16_t Handle##Type(                       \
-      Type str, int index, PtrComprCageBase cage_base,       \
-      const SharedStringAccessGuardIfNeeded& access_guard) { \
-    return str->Get(index, cage_base, access_guard);         \
+#define DEFINE_METHOD(Type)                                    \
+  static inline uint16_t Handle##Type(                         \
+      Tagged<Type> str, int index, PtrComprCageBase cage_base, \
+      const SharedStringAccessGuardIfNeeded& access_guard) {   \
+    return str->Get(index, cage_base, access_guard);           \
   }
     STRING_CLASS_TYPES(DEFINE_METHOD)
 #undef DEFINE_METHOD
diff --git a/src/objects/string-set.h b/src/objects/string-set.h
index 7edfa8fd39c..c4267f2672a 100644
--- a/src/objects/string-set.h
+++ b/src/objects/string-set.h
@@ -13,7 +13,7 @@
 namespace v8 {
 namespace internal {
 
-class StringSetShape : public BaseShape<String> {
+class StringSetShape : public BaseShape<Tagged<String>> {
  public:
   static inline bool IsMatch(Tagged<String> key, Tagged<Object> value);
   static inline uint32_t Hash(ReadOnlyRoots roots, Tagged<String> key);
diff --git a/src/objects/string-table.cc b/src/objects/string-table.cc
index 83dc059d992..0af2a576730 100644
--- a/src/objects/string-table.cc
+++ b/src/objects/string-table.cc
@@ -70,7 +70,7 @@ int ComputeStringTableCapacityWithShrink(int current_capacity,
 }
 
 template <typename IsolateT, typename StringTableKey>
-bool KeyIsMatch(IsolateT* isolate, StringTableKey* key, String string) {
+bool KeyIsMatch(IsolateT* isolate, StringTableKey* key, Tagged<String> string) {
   if (string->hash() != key->hash()) return false;
   if (string->length() != key->length()) return false;
   return key->IsMatch(isolate, string);
@@ -98,11 +98,11 @@ class StringTable::Data {
     return OffHeapObjectSlot(&elements_[index.as_uint32()]);
   }
 
-  Object Get(PtrComprCageBase cage_base, InternalIndex index) const {
+  Tagged<Object> Get(PtrComprCageBase cage_base, InternalIndex index) const {
     return slot(index).Acquire_Load(cage_base);
   }
 
-  void Set(InternalIndex index, String entry) {
+  void Set(InternalIndex index, Tagged<String> entry) {
     slot(index).Release_Store(entry);
   }
 
@@ -150,7 +150,8 @@ class StringTable::Data {
   // Helper method for StringTable::TryStringToIndexOrLookupExisting.
   template <typename Char>
   static Address TryStringToIndexOrLookupExisting(Isolate* isolate,
-                                                  String string, String source,
+                                                  Tagged<String> string,
+                                                  Tagged<String> source,
                                                   size_t start);
 
   void IterateElements(RootVisitor* visitor);
@@ -235,9 +236,9 @@ std::unique_ptr<StringTable::Data> StringTable::Data::Resize(
 
   // Rehash the elements.
   for (InternalIndex i : InternalIndex::Range(data->capacity())) {
-    Object element = data->Get(cage_base, i);
+    Tagged<Object> element = data->Get(cage_base, i);
     if (element == empty_element() || element == deleted_element()) continue;
-    String string = String::cast(element);
+    Tagged<String> string = String::cast(element);
     uint32_t hash = string->hash();
     InternalIndex insertion_index =
         new_data->FindInsertionEntry(cage_base, hash);
@@ -259,10 +260,10 @@ InternalIndex StringTable::Data::FindEntry(IsolateT* isolate,
        entry = NextProbe(entry, count++, capacity_)) {
     // TODO(leszeks): Consider delaying the decompression until after the
     // comparisons against empty/deleted.
-    Object element = Get(isolate, entry);
+    Tagged<Object> element = Get(isolate, entry);
     if (element == empty_element()) return InternalIndex::NotFound();
     if (element == deleted_element()) continue;
-    String string = String::cast(element);
+    Tagged<String> string = String::cast(element);
     if (KeyIsMatch(isolate, key, string)) return entry;
   }
 }
@@ -275,7 +276,7 @@ InternalIndex StringTable::Data::FindInsertionEntry(PtrComprCageBase cage_base,
        entry = NextProbe(entry, count++, capacity_)) {
     // TODO(leszeks): Consider delaying the decompression until after the
     // comparisons against empty/deleted.
-    Object element = Get(cage_base, entry);
+    Tagged<Object> element = Get(cage_base, entry);
     if (element == empty_element() || element == deleted_element())
       return entry;
   }
@@ -291,7 +292,7 @@ InternalIndex StringTable::Data::FindEntryOrInsertionEntry(
        entry = NextProbe(entry, count++, capacity_)) {
     // TODO(leszeks): Consider delaying the decompression until after the
     // comparisons against empty/deleted.
-    Object element = Get(isolate, entry);
+    Tagged<Object> element = Get(isolate, entry);
     if (element == empty_element()) {
       // Empty entry, it's our insertion entry if there was no previous Hole.
       if (insertion_entry.is_not_found()) return entry;
@@ -305,7 +306,7 @@ InternalIndex StringTable::Data::FindEntryOrInsertionEntry(
       continue;
     }
 
-    String string = String::cast(element);
+    Tagged<String> string = String::cast(element);
     if (KeyIsMatch(isolate, key, string)) return entry;
   }
 }
@@ -355,7 +356,7 @@ class InternalizedStringKey final : public StringTableKey {
     DCHECK(String::IsHashFieldComputed(hash));
   }
 
-  bool IsMatch(Isolate* isolate, String string) {
+  bool IsMatch(Isolate* isolate, Tagged<String> string) {
     DCHECK(!SharedStringAccessGuardIfNeeded::IsNeeded(string));
     return string_->SlowEquals(string);
   }
@@ -452,8 +453,8 @@ class InternalizedStringKey final : public StringTableKey {
 
 namespace {
 
-void SetInternalizedReference(Isolate* isolate, String string,
-                              String internalized) {
+void SetInternalizedReference(Isolate* isolate, Tagged<String> string,
+                              Tagged<String> internalized) {
   DCHECK(!IsThinString(string));
   DCHECK(!IsInternalizedString(string));
   DCHECK(IsInternalizedString(internalized));
@@ -614,7 +615,7 @@ Handle<String> StringTable::LookupKey(IsolateT* isolate, StringTableKey* key) {
     // added after the check.
     entry = data->FindEntryOrInsertionEntry(isolate, key, key->hash());
 
-    Object element = data->Get(isolate, entry);
+    Tagged<Object> element = data->Get(isolate, entry);
     if (element == empty_element()) {
       // This entry is empty, so write it and register that we added an
       // element.
@@ -702,10 +703,9 @@ StringTable::Data* StringTable::EnsureCapacity(PtrComprCageBase cage_base,
 
 // static
 template <typename Char>
-Address StringTable::Data::TryStringToIndexOrLookupExisting(Isolate* isolate,
-                                                            String string,
-                                                            String source,
-                                                            size_t start) {
+Address StringTable::Data::TryStringToIndexOrLookupExisting(
+    Isolate* isolate, Tagged<String> string, Tagged<String> source,
+    size_t start) {
   // TODO(leszeks): This method doesn't really belong on StringTable::Data.
   // Ideally it would be a free function in an anonymous namespace, but that
   // causes issues around method and class visibility.
@@ -723,7 +723,7 @@ Address StringTable::Data::TryStringToIndexOrLookupExisting(Isolate* isolate,
   if (Name::IsInternalizedForwardingIndex(raw_hash_field) &&
       is_source_hash_usable) {
     const int index = Name::ForwardingIndexValueBits::decode(raw_hash_field);
-    String internalized =
+    Tagged<String> internalized =
         isolate->string_forwarding_table()->GetForwardString(isolate, index);
     return internalized.ptr();
   }
@@ -740,8 +740,8 @@ Address StringTable::Data::TryStringToIndexOrLookupExisting(Isolate* isolate,
     String::WriteToFlat(source, buffer.get(), 0, length, isolate, access_guard);
     chars = buffer.get();
   } else {
-    chars =
-        source.GetDirectStringChars<Char>(isolate, no_gc, access_guard) + start;
+    chars = source->GetDirectStringChars<Char>(isolate, no_gc, access_guard) +
+            start;
   }
 
   if (!Name::IsHashFieldComputed(raw_hash_field) || !is_source_hash_usable) {
@@ -773,7 +773,8 @@ Address StringTable::Data::TryStringToIndexOrLookupExisting(Isolate* isolate,
     return Smi::FromInt(ResultSentinel::kNotFound).ptr();
   }
 
-  String internalized = String::cast(string_table_data->Get(isolate, entry));
+  Tagged<String> internalized =
+      String::cast(string_table_data->Get(isolate, entry));
   // string can be internalized here, if another thread internalized it.
   // If we found and entry in the string table and string is not internalized,
   // there is no way that it can transition to internalized later on. So a last
@@ -789,7 +790,7 @@ Address StringTable::Data::TryStringToIndexOrLookupExisting(Isolate* isolate,
 // static
 Address StringTable::TryStringToIndexOrLookupExisting(Isolate* isolate,
                                                       Address raw_string) {
-  String string = String::cast(Object(raw_string));
+  Tagged<String> string = String::cast(Object(raw_string));
   if (IsInternalizedString(string)) {
     // string could be internalized, if the string table is shared and another
     // thread internalized it.
@@ -805,9 +806,9 @@ Address StringTable::TryStringToIndexOrLookupExisting(Isolate* isolate,
       !String::ArrayIndexValueBits::is_valid(ResultSentinel::kNotFound));
 
   size_t start = 0;
-  String source = string;
+  Tagged<String> source = string;
   if (IsSlicedString(source)) {
-    SlicedString sliced = SlicedString::cast(source);
+    Tagged<SlicedString> sliced = SlicedString::cast(source);
     start = sliced->offset();
     source = sliced->parent();
   } else if (IsConsString(source) && source->IsFlat()) {
diff --git a/src/objects/string.h b/src/objects/string.h
index 3d04b0dae7a..8a0794bfa93 100644
--- a/src/objects/string.h
+++ b/src/objects/string.h
@@ -699,7 +699,7 @@ class SubStringRange {
   inline iterator end();
 
  private:
-  String string_;
+  Tagged<String> string_;
   int first_;
   int length_;
   const DisallowGarbageCollection& no_gc_;
@@ -1106,7 +1106,7 @@ class ConsStringIterator {
   // a non-zero offset.
   inline Tagged<String> Next(int* offset_out) {
     *offset_out = 0;
-    if (depth_ == 0) return String();
+    if (depth_ == 0) return Tagged<String>();
     return Continue(offset_out);
   }
 
@@ -1130,8 +1130,8 @@ class ConsStringIterator {
 
   // Stack must always contain only frames for which right traversal
   // has not yet been performed.
-  ConsString frames_[kStackSize];
-  ConsString root_;
+  Tagged<ConsString> frames_[kStackSize];
+  Tagged<ConsString> root_;
   int depth_;
   int maximum_depth_;
   int consumed_;
diff --git a/src/objects/swiss-name-dictionary.cc b/src/objects/swiss-name-dictionary.cc
index 58cea831365..4417eb24492 100644
--- a/src/objects/swiss-name-dictionary.cc
+++ b/src/objects/swiss-name-dictionary.cc
@@ -212,14 +212,14 @@ void SwissNameDictionary::Rehash(IsolateT* isolate) {
   DisallowHeapAllocation no_gc;
 
   struct Entry {
-    Name key;
-    Object value;
+    Tagged<Name> key;
+    Tagged<Object> value;
     PropertyDetails details;
   };
 
   if (Capacity() == 0) return;
 
-  Entry dummy{Name(), Object(), PropertyDetails::Empty()};
+  Entry dummy{Tagged<Name>(), Tagged<Object>(), PropertyDetails::Empty()};
   std::vector<Entry> data(NumberOfElements(), dummy);
 
   ReadOnlyRoots roots(isolate);
diff --git a/src/objects/tagged-impl.cc b/src/objects/tagged-impl.cc
index 58c639a62f9..a35a0e1e180 100644
--- a/src/objects/tagged-impl.cc
+++ b/src/objects/tagged-impl.cc
@@ -25,8 +25,8 @@ bool CheckObjectComparisonAllowed(Address a, Address b) {
   if (!HAS_STRONG_HEAP_OBJECT_TAG(a) || !HAS_STRONG_HEAP_OBJECT_TAG(b)) {
     return true;
   }
-  HeapObject obj_a = HeapObject::unchecked_cast(Object(a));
-  HeapObject obj_b = HeapObject::unchecked_cast(Object(b));
+  Tagged<HeapObject> obj_a = HeapObject::unchecked_cast(Object(a));
+  Tagged<HeapObject> obj_b = HeapObject::unchecked_cast(Object(b));
   // This check might fail when we try to compare InstructionStream object with
   // non-InstructionStream object. The main legitimate case when such "mixed"
   // comparison could happen is comparing two AbstractCode objects. If that's
diff --git a/src/objects/tagged-impl.h b/src/objects/tagged-impl.h
index 5666035e203..1e8579a357c 100644
--- a/src/objects/tagged-impl.h
+++ b/src/objects/tagged-impl.h
@@ -208,10 +208,10 @@ class TaggedImpl {
 
   // Cast operation is available only for full non-weak tagged values.
   template <typename T>
-  T cast() const {
+  Tagged<T> cast() const {
     CHECK(kIsFull);
     DCHECK(!HAS_WEAK_HEAP_OBJECT_TAG(ptr_));
-    return T::cast(Object(ptr_));
+    return T::cast(Tagged<Object>(ptr_));
   }
 
  private:
diff --git a/src/objects/tagged-index.h b/src/objects/tagged-index.h
index c1d1a250a16..2fb316c8893 100644
--- a/src/objects/tagged-index.h
+++ b/src/objects/tagged-index.h
@@ -83,11 +83,6 @@ CAST_ACCESSOR(TaggedIndex)
 constexpr Tagged<TaggedIndex>::Tagged(TaggedIndex raw) : TaggedBase(raw.ptr()) {
   static_assert(kTaggedCanConvertToRawObjects);
 }
-// NOLINTNEXTLINE
-constexpr Tagged<TaggedIndex>::operator TaggedIndex() {
-  static_assert(kTaggedCanConvertToRawObjects);
-  return TaggedIndex(ptr());
-}
 
 }  // namespace internal
 }  // namespace v8
diff --git a/src/objects/tagged.h b/src/objects/tagged.h
index ce955b538e2..fdbf6dde8d1 100644
--- a/src/objects/tagged.h
+++ b/src/objects/tagged.h
@@ -150,11 +150,6 @@ class Tagged<Object> : public TaggedBase {
   // TODO(leszeks): Remove once we're using Tagged everywhere.
   // NOLINTNEXTLINE
   inline constexpr Tagged(Object raw);
-  template <typename U,
-            typename = std::enable_if_t<std::is_base_of_v<U, Object> ||
-                                        std::is_convertible_v<Object*, U*>>>
-  // NOLINTNEXTLINE
-  inline constexpr operator U();
 
  private:
   friend class Object;
@@ -194,8 +189,6 @@ class Tagged<Smi> : public TaggedBase {
   // TODO(leszeks): Remove once we're using Tagged everywhere.
   // NOLINTNEXTLINE
   inline constexpr Tagged(Smi raw);
-  // NOLINTNEXTLINE
-  inline constexpr operator Smi();
 
  private:
   friend class Smi;
@@ -245,8 +238,6 @@ class Tagged<TaggedIndex> : public TaggedBase {
   // TODO(leszeks): Remove once we're using Tagged everywhere.
   // NOLINTNEXTLINE
   inline constexpr Tagged(TaggedIndex raw);
-  // NOLINTNEXTLINE
-  inline constexpr operator TaggedIndex();
 
  private:
   friend class TaggedIndex;
@@ -321,8 +312,6 @@ class Tagged<HeapObject> : public TaggedBase {
   constexpr Tagged(U raw) : Base(raw.ptr()) {
     static_assert(kTaggedCanConvertToRawObjects);
   }
-  // NOLINTNEXTLINE
-  constexpr operator HeapObject();
   template <typename U>
   static constexpr Tagged<HeapObject> cast(U other) {
     static_assert(kTaggedCanConvertToRawObjects);
@@ -400,14 +389,6 @@ class Tagged : public detail::BaseForTagged<T>::type {
   constexpr Tagged(U raw) : Base(raw.ptr()) {
     static_assert(kTaggedCanConvertToRawObjects);
   }
-  template <typename U,
-            typename = std::enable_if_t<std::is_base_of_v<U, T> ||
-                                        std::is_convertible_v<T*, U*>>>
-  // NOLINTNEXTLINE
-  constexpr operator U() const {
-    static_assert(kTaggedCanConvertToRawObjects);
-    return ToRawPtr();
-  }
   template <typename U>
   static constexpr Tagged<T> cast(U other) {
     static_assert(kTaggedCanConvertToRawObjects);
diff --git a/src/objects/templates.cc b/src/objects/templates.cc
index c197cde6d45..a9a8f581a82 100644
--- a/src/objects/templates.cc
+++ b/src/objects/templates.cc
@@ -112,7 +112,7 @@ bool FunctionTemplateInfo::IsLeafTemplateForApiObject(
   Tagged<Map> map = HeapObject::cast(object)->map();
   Tagged<Object> constructor_obj = map->GetConstructor();
   if (IsJSFunction(constructor_obj)) {
-    JSFunction fun = JSFunction::cast(constructor_obj);
+    Tagged<JSFunction> fun = JSFunction::cast(constructor_obj);
     result = (*this == fun->shared()->function_data(kAcquireLoad));
   } else if (IsFunctionTemplateInfo(constructor_obj)) {
     result = (*this == constructor_obj);
@@ -137,7 +137,7 @@ base::Optional<Tagged<Name>> FunctionTemplateInfo::TryGetCachedPropertyName(
   DisallowGarbageCollection no_gc;
   if (!IsFunctionTemplateInfo(getter)) {
     if (!IsJSFunction(getter)) return {};
-    SharedFunctionInfo info = JSFunction::cast(getter)->shared();
+    Tagged<SharedFunctionInfo> info = JSFunction::cast(getter)->shared();
     if (!info->IsApiFunction()) return {};
     getter = info->api_func_data();
   }
diff --git a/src/objects/transitions.cc b/src/objects/transitions.cc
index a1bfb38c414..66ac57bc4cd 100644
--- a/src/objects/transitions.cc
+++ b/src/objects/transitions.cc
@@ -139,7 +139,7 @@ void TransitionsAccessor::Insert(Isolate* isolate, Handle<Map> map,
 
   {
     DisallowGarbageCollection no_gc;
-    TransitionArray array = GetTransitionArray(isolate, map);
+    Tagged<TransitionArray> array = GetTransitionArray(isolate, map);
     number_of_transitions = array->number_of_transitions();
 
     int index =
@@ -171,7 +171,7 @@ void TransitionsAccessor::Insert(Isolate* isolate, Handle<Map> map,
       }
       array->SetKey(insertion_index, *name);
       array->SetRawTarget(insertion_index, HeapObjectReference::Weak(*target));
-      SLOW_DCHECK(array.IsSortedNoDuplicates());
+      SLOW_DCHECK(array->IsSortedNoDuplicates());
       return;
     }
   }
@@ -185,7 +185,7 @@ void TransitionsAccessor::Insert(Isolate* isolate, Handle<Map> map,
   // it was weakly traversed, though it is guaranteed not to disappear. Trim the
   // result copy if needed, and recompute variables.
   DisallowGarbageCollection no_gc;
-  TransitionArray array = GetTransitionArray(isolate, map);
+  Tagged<TransitionArray> array = GetTransitionArray(isolate, map);
   if (array->number_of_transitions() != number_of_transitions) {
     DCHECK_LT(array->number_of_transitions(), number_of_transitions);
 
@@ -292,7 +292,8 @@ void TransitionsAccessor::ForEachTransitionTo(
       Tagged<Map> target =
           Map::cast(raw_transitions_.GetHeapObjectAssumeWeak());
       InternalIndex descriptor = target->LastAdded();
-      DescriptorArray descriptors = target->instance_descriptors(kRelaxedLoad);
+      Tagged<DescriptorArray> descriptors =
+          target->instance_descriptors(kRelaxedLoad);
       Tagged<Name> key = descriptors->GetKey(descriptor);
       if (key == name) {
         callback(target);
@@ -325,7 +326,8 @@ bool TransitionsAccessor::IsMatchingMap(Tagged<Map> target, Tagged<Name> name,
                                         PropertyKind kind,
                                         PropertyAttributes attributes) {
   InternalIndex descriptor = target->LastAdded();
-  DescriptorArray descriptors = target->instance_descriptors(kRelaxedLoad);
+  Tagged<DescriptorArray> descriptors =
+      target->instance_descriptors(kRelaxedLoad);
   Tagged<Name> key = descriptors->GetKey(descriptor);
   if (key != name) return false;
   return descriptors->GetDetails(descriptor)
@@ -446,7 +448,7 @@ Handle<Map> TransitionsAccessor::GetPrototypeTransition(
     Isolate* isolate, Handle<Map> map, Handle<Object> prototype_handle,
     bool new_target_is_base) {
   DisallowGarbageCollection no_gc;
-  Object prototype = *prototype_handle;
+  Tagged<Object> prototype = *prototype_handle;
   Tagged<WeakFixedArray> cache = GetPrototypeTransitions(isolate, map);
   int length = TransitionArray::NumberOfPrototypeTransitions(cache);
   for (int i = 0; i < length; i++) {
@@ -472,7 +474,7 @@ Tagged<WeakFixedArray> TransitionsAccessor::GetPrototypeTransitions(
   if (GetEncoding(isolate, raw_transitions) != kFullTransitionArray) {
     return ReadOnlyRoots(isolate).empty_weak_fixed_array();
   }
-  TransitionArray transition_array =
+  Tagged<TransitionArray> transition_array =
       GetTransitionArray(isolate, raw_transitions);
   if (!transition_array->HasPrototypeTransitions()) {
     return ReadOnlyRoots(isolate).empty_weak_fixed_array();
@@ -581,7 +583,7 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
   // Mostly arbitrary but more than enough to run the test suite in static
   // memory.
   static constexpr int kStaticStackSize = 16;
-  base::SmallVector<Map, kStaticStackSize> stack;
+  base::SmallVector<Tagged<Map>, kStaticStackSize> stack;
   stack.emplace_back(map_);
 
   // Pre-order iterative depth-first-search.
@@ -606,7 +608,7 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
         break;
       }
       case kFullTransitionArray: {
-        TransitionArray transitions =
+        Tagged<TransitionArray> transitions =
             TransitionArray::cast(raw_transitions.GetHeapObjectAssumeStrong());
         if (transitions->HasPrototypeTransitions()) {
           Tagged<WeakFixedArray> proto_trans =
@@ -638,9 +640,9 @@ void TransitionsAccessor::TraverseTransitionTreeInternal(
 void TransitionsAccessor::CheckNewTransitionsAreConsistent(
     Isolate* isolate, Handle<Map> map, Tagged<Object> transitions) {
   // This function only handles full transition arrays.
-  TransitionArray old_transitions = GetTransitionArray(isolate, map);
+  Tagged<TransitionArray> old_transitions = GetTransitionArray(isolate, map);
   DCHECK_EQ(kFullTransitionArray, GetEncoding(isolate, old_transitions));
-  TransitionArray new_transitions = TransitionArray::cast(transitions);
+  Tagged<TransitionArray> new_transitions = TransitionArray::cast(transitions);
   for (int i = 0; i < old_transitions->number_of_transitions(); i++) {
     Tagged<Map> target = old_transitions->GetTarget(i);
     if (target->instance_descriptors(isolate) ==
diff --git a/src/objects/transitions.h b/src/objects/transitions.h
index 66e80f1704b..313918b2a2d 100644
--- a/src/objects/transitions.h
+++ b/src/objects/transitions.h
@@ -225,7 +225,7 @@ class V8_EXPORT_PRIVATE TransitionsAccessor {
                                       DisallowGarbageCollection* no_gc);
 
   Isolate* isolate_;
-  Map map_;
+  Tagged<Map> map_;
   MaybeObject raw_transitions_;
   Encoding encoding_;
   bool concurrent_access_;
diff --git a/src/parsing/scanner-character-streams.cc b/src/parsing/scanner-character-streams.cc
index 245d7684875..c19f2ae808d 100644
--- a/src/parsing/scanner-character-streams.cc
+++ b/src/parsing/scanner-character-streams.cc
@@ -100,10 +100,10 @@ class ExternalStringStream {
   using ExternalString = typename CharTraits<Char>::ExternalString;
 
  public:
-  ExternalStringStream(ExternalString string, size_t start_offset,
+  ExternalStringStream(Tagged<ExternalString> string, size_t start_offset,
                        size_t length)
       : lock_(string),
-        data_(string.GetChars(GetPtrComprCageBase(string)) + start_offset),
+        data_(string->GetChars(GetPtrComprCageBase(string)) + start_offset),
         length_(length) {}
 
   ExternalStringStream(const ExternalStringStream& other) V8_NOEXCEPT
diff --git a/src/profiler/heap-snapshot-generator.cc b/src/profiler/heap-snapshot-generator.cc
index b25e59552b7..359afa9a864 100644
--- a/src/profiler/heap-snapshot-generator.cc
+++ b/src/profiler/heap-snapshot-generator.cc
@@ -145,7 +145,8 @@ class HeapEntryVerifier {
 
  private:
   using UnorderedHeapObjectSet =
-      std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>;
+      std::unordered_set<Tagged<HeapObject>, Object::Hasher,
+                         Object::KeyEqualSafe>;
 
   const UnorderedHeapObjectSet& GetIndirectStrongReferences(size_t level) {
     CHECK_GE(indirect_strong_references_.size(), level);
@@ -184,7 +185,7 @@ class HeapEntryVerifier {
 
   DISALLOW_GARBAGE_COLLECTION(no_gc)
   HeapSnapshotGenerator* generator_;
-  HeapObject primary_object_;
+  Tagged<HeapObject> primary_object_;
 
   // All objects referred to by primary_object_, according to a marking visitor.
   ReferenceSummary reference_summary_;
@@ -192,7 +193,7 @@ class HeapEntryVerifier {
   // Objects that have been checked via a call to CheckStrongReference or
   // CheckWeakReference, or deliberately skipped via a call to
   // MarkReferenceCheckedWithoutChecking.
-  std::unordered_set<HeapObject, Object::Hasher, Object::KeyEqualSafe>
+  std::unordered_set<Tagged<HeapObject>, Object::Hasher, Object::KeyEqualSafe>
       checked_objects_;
 
   // Objects transitively retained by the primary object. The objects in the set
@@ -1084,7 +1085,7 @@ uint32_t V8HeapExplorer::EstimateObjectsCount() {
 #ifdef V8_TARGET_BIG_ENDIAN
 namespace {
 int AdjustEmbedderFieldIndex(HeapObject heap_obj, int field_index) {
-  Map map = heap_obj.map();
+  Tagged<Map> map = heap_obj.map();
   if (JSObject::MayHaveEmbedderFields(map)) {
     int emb_start_index = (JSObject::GetEmbedderFieldsStartOffset(map) +
                            EmbedderDataSlot::kTaggedPayloadOffset) /
@@ -1196,7 +1197,7 @@ class IndexedReferencesExtractor : public ObjectVisitorWithCageBases {
   }
 
   V8HeapExplorer* generator_;
-  HeapObject parent_obj_;
+  Tagged<HeapObject> parent_obj_;
   MaybeObjectSlot parent_start_;
   MaybeObjectSlot parent_end_;
   HeapEntry* parent_;
@@ -2676,7 +2677,7 @@ class EmbedderGraphImpl : public EmbedderGraph {
     }
 
    private:
-    Object object_;
+    Tagged<Object> object_;
   };
 
   Node* V8Node(const v8::Local<v8::Value>& value) final {
@@ -2912,7 +2913,7 @@ class V8_NODISCARD NullContextForSnapshotScope {
 
  private:
   Isolate* isolate_;
-  Context prev_;
+  Tagged<Context> prev_;
 };
 }  // namespace
 
diff --git a/src/profiler/heap-snapshot-generator.h b/src/profiler/heap-snapshot-generator.h
index ef5241d0fb1..26e712714e9 100644
--- a/src/profiler/heap-snapshot-generator.h
+++ b/src/profiler/heap-snapshot-generator.h
@@ -568,10 +568,10 @@ class V8_EXPORT_PRIVATE V8HeapExplorer : public HeapEntriesAllocator {
   HeapObjectsMap* heap_object_map_;
   SnapshottingProgressReportingInterface* progress_;
   HeapSnapshotGenerator* generator_ = nullptr;
-  std::unordered_map<JSGlobalObject, const char*, Object::Hasher>
+  std::unordered_map<Tagged<JSGlobalObject>, const char*, Object::Hasher>
       global_object_tag_map_;
   UnorderedHeapObjectMap<const char*> strong_gc_subroot_names_;
-  std::unordered_set<JSGlobalObject, Object::Hasher> user_roots_;
+  std::unordered_set<Tagged<JSGlobalObject>, Object::Hasher> user_roots_;
   v8::HeapProfiler::ObjectNameResolver* global_object_name_resolver_;
 
   std::vector<bool> visited_fields_;
diff --git a/src/profiler/profiler-listener.h b/src/profiler/profiler-listener.h
index a3dab7ea929..c6b428cd90a 100644
--- a/src/profiler/profiler-listener.h
+++ b/src/profiler/profiler-listener.h
@@ -77,7 +77,7 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
   // Invoked after a mark-sweep cycle.
   void CodeSweepEvent();
 
-  const char* GetName(Name name) {
+  const char* GetName(Tagged<Name> name) {
     return code_entries_.strings().GetName(name);
   }
   const char* GetName(int args_count) {
@@ -87,7 +87,7 @@ class V8_EXPORT_PRIVATE ProfilerListener : public LogEventListener,
     return code_entries_.strings().GetCopy(name);
   }
   const char* GetName(base::Vector<const char> name);
-  const char* GetConsName(const char* prefix, Name name) {
+  const char* GetConsName(const char* prefix, Tagged<Name> name) {
     return code_entries_.strings().GetConsName(prefix, name);
   }
 
diff --git a/src/profiler/sampling-heap-profiler.cc b/src/profiler/sampling-heap-profiler.cc
index 9da56aa3ea5..bd09319b140 100644
--- a/src/profiler/sampling-heap-profiler.cc
+++ b/src/profiler/sampling-heap-profiler.cc
@@ -149,7 +149,7 @@ SamplingHeapProfiler::AllocationNode* SamplingHeapProfiler::FindOrAddChildNode(
 SamplingHeapProfiler::AllocationNode* SamplingHeapProfiler::AddStack() {
   AllocationNode* node = &profile_root_;
 
-  std::vector<SharedFunctionInfo> stack;
+  std::vector<Tagged<SharedFunctionInfo>> stack;
   JavaScriptStackFrameIterator frame_it(isolate_);
   int frames_captured = 0;
   bool found_arguments_marker_frames = false;
diff --git a/src/regexp/experimental/experimental-interpreter.cc b/src/regexp/experimental/experimental-interpreter.cc
index f200b78f194..2342c88e44f 100644
--- a/src/regexp/experimental/experimental-interpreter.cc
+++ b/src/regexp/experimental/experimental-interpreter.cc
@@ -516,13 +516,13 @@ class NfaInterpreter {
 
   DisallowGarbageCollection no_gc_;
 
-  ByteArray bytecode_object_;
+  Tagged<ByteArray> bytecode_object_;
   base::Vector<const RegExpInstruction> bytecode_;
 
   // Number of registers used per thread.
   const int register_count_per_match_;
 
-  String input_object_;
+  Tagged<String> input_object_;
   base::Vector<const Character> input_;
   int input_index_;
 
diff --git a/src/regexp/loong64/regexp-macro-assembler-loong64.cc b/src/regexp/loong64/regexp-macro-assembler-loong64.cc
index bc7657c051c..e13e7bd1698 100644
--- a/src/regexp/loong64/regexp-macro-assembler-loong64.cc
+++ b/src/regexp/loong64/regexp-macro-assembler-loong64.cc
@@ -1204,7 +1204,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 
 int64_t RegExpMacroAssemblerLOONG64::CheckStackGuardState(
     Address* return_address, Address raw_code, Address re_frame) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset),
       static_cast<int>(frame_entry<int64_t>(re_frame, kStartIndexOffset)),
diff --git a/src/regexp/mips64/regexp-macro-assembler-mips64.cc b/src/regexp/mips64/regexp-macro-assembler-mips64.cc
index 7e8a2646df5..2ae0b0c49f2 100644
--- a/src/regexp/mips64/regexp-macro-assembler-mips64.cc
+++ b/src/regexp/mips64/regexp-macro-assembler-mips64.cc
@@ -1255,7 +1255,7 @@ static T* frame_entry_address(Address re_frame, int frame_offset) {
 int64_t RegExpMacroAssemblerMIPS::CheckStackGuardState(Address* return_address,
                                                        Address raw_code,
                                                        Address re_frame) {
-  InstructionStream re_code = InstructionStream::cast(Object(raw_code));
+  Tagged<InstructionStream> re_code = InstructionStream::cast(Object(raw_code));
   return NativeRegExpMacroAssembler::CheckStackGuardState(
       frame_entry<Isolate*>(re_frame, kIsolateOffset),
       static_cast<int>(frame_entry<int64_t>(re_frame, kStartIndexOffset)),
diff --git a/src/regexp/regexp-interpreter.cc b/src/regexp/regexp-interpreter.cc
index 33a75b7e76d..dd264d43796 100644
--- a/src/regexp/regexp-interpreter.cc
+++ b/src/regexp/regexp-interpreter.cc
@@ -286,7 +286,7 @@ IrregexpInterpreter::Result HandleInterrupts(
     } else if (check.InterruptRequested()) {
       const bool was_one_byte =
           String::IsOneByteRepresentationUnderneath(*subject_string_out);
-      Object result;
+      Tagged<Object> result;
       {
         AllowGarbageCollection yes_gc;
         result = isolate->stack_guard()->HandleInterrupts();
diff --git a/src/regexp/regexp-macro-assembler.cc b/src/regexp/regexp-macro-assembler.cc
index dea1568829c..704c7edf12a 100644
--- a/src/regexp/regexp-macro-assembler.cc
+++ b/src/regexp/regexp-macro-assembler.cc
@@ -188,24 +188,25 @@ uint32_t RegExpMacroAssembler::IsCharacterInRangeArray(uint32_t current_char,
   static constexpr uint32_t kTrue = 1;
   static constexpr uint32_t kFalse = 0;
 
-  FixedUInt16Array ranges = FixedUInt16Array::cast(Object(raw_byte_array));
-  DCHECK_GE(ranges.length(), 1);
+  Tagged<FixedUInt16Array> ranges =
+      FixedUInt16Array::cast(Tagged<Object>(raw_byte_array));
+  DCHECK_GE(ranges->length(), 1);
 
   // Shortcut for fully out of range chars.
-  if (current_char < ranges.get(0)) return kFalse;
-  if (current_char >= ranges.get(ranges.length() - 1)) {
+  if (current_char < ranges->get(0)) return kFalse;
+  if (current_char >= ranges->get(ranges->length() - 1)) {
     // The last range may be open-ended.
-    return (ranges.length() % 2) == 0 ? kFalse : kTrue;
+    return (ranges->length() % 2) == 0 ? kFalse : kTrue;
   }
 
   // Binary search for the matching range. `ranges` is encoded as
   // [from0, to0, from1, to1, ..., fromN, toN], or
   // [from0, to0, from1, to1, ..., fromN] (open-ended last interval).
 
-  int mid, lower = 0, upper = ranges.length();
+  int mid, lower = 0, upper = ranges->length();
   do {
     mid = lower + (upper - lower) / 2;
-    const base::uc16 elem = ranges.get(mid);
+    const base::uc16 elem = ranges->get(mid);
     if (current_char < elem) {
       upper = mid;
     } else if (current_char > elem) {
@@ -216,7 +217,7 @@ uint32_t RegExpMacroAssembler::IsCharacterInRangeArray(uint32_t current_char,
     }
   } while (lower < upper);
 
-  const bool current_char_ge_last_elem = current_char >= ranges.get(mid);
+  const bool current_char_ge_last_elem = current_char >= ranges->get(mid);
   const int current_range_start_index =
       current_char_ge_last_elem ? mid : mid - 1;
 
diff --git a/src/regexp/regexp-utils.cc b/src/regexp/regexp-utils.cc
index bf58d114ecd..d1854046fe9 100644
--- a/src/regexp/regexp-utils.cc
+++ b/src/regexp/regexp-utils.cc
@@ -37,7 +37,7 @@ Handle<String> RegExpUtils::GenericCaptureGetter(
 
 namespace {
 
-V8_INLINE bool HasInitialRegExpMap(Isolate* isolate, JSReceiver recv) {
+V8_INLINE bool HasInitialRegExpMap(Isolate* isolate, Tagged<JSReceiver> recv) {
   return recv->map() == isolate->regexp_function()->initial_map();
 }
 
@@ -128,16 +128,16 @@ bool RegExpUtils::IsUnmodifiedRegExp(Isolate* isolate, Handle<Object> obj) {
 
   if (!IsJSReceiver(*obj)) return false;
 
-  JSReceiver recv = JSReceiver::cast(*obj);
+  Tagged<JSReceiver> recv = JSReceiver::cast(*obj);
 
   if (!HasInitialRegExpMap(isolate, recv)) return false;
 
   // Check the receiver's prototype's map.
-  Object proto = recv->map()->prototype();
+  Tagged<Object> proto = recv->map()->prototype();
   if (!IsJSReceiver(proto)) return false;
 
   Handle<Map> initial_proto_initial_map = isolate->regexp_prototype_map();
-  Map proto_map = JSReceiver::cast(proto)->map();
+  Tagged<Map> proto_map = JSReceiver::cast(proto)->map();
   if (proto_map != *initial_proto_initial_map) {
     return false;
   }
@@ -164,7 +164,7 @@ bool RegExpUtils::IsUnmodifiedRegExp(Isolate* isolate, Handle<Object> obj) {
 
   // The smi check is required to omit ToLength(lastIndex) calls with possible
   // user-code execution on the fast path.
-  Object last_index = JSRegExp::cast(recv)->last_index();
+  Tagged<Object> last_index = JSRegExp::cast(recv)->last_index();
   return IsSmi(last_index) && Smi::ToInt(last_index) >= 0;
 }
 
diff --git a/src/regexp/regexp.cc b/src/regexp/regexp.cc
index b3610922b49..6b28b417665 100644
--- a/src/regexp/regexp.cc
+++ b/src/regexp/regexp.cc
@@ -1223,7 +1223,7 @@ int32_t* RegExpGlobalCache::LastSuccessfulMatch() {
 
 Tagged<Object> RegExpResultsCache::Lookup(Heap* heap, Tagged<String> key_string,
                                           Tagged<Object> key_pattern,
-                                          FixedArray* last_match_cache,
+                                          Tagged<FixedArray>* last_match_cache,
                                           ResultsCacheType type) {
   Tagged<FixedArray> cache;
   if (!IsInternalizedString(key_string)) return Smi::zero();
diff --git a/src/regexp/regexp.h b/src/regexp/regexp.h
index b82b1b6fe80..94596424b41 100644
--- a/src/regexp/regexp.h
+++ b/src/regexp/regexp.h
@@ -216,7 +216,7 @@ class RegExpResultsCache final : public AllStatic {
   // On success, the returned result is guaranteed to be a COW-array.
   static Tagged<Object> Lookup(Heap* heap, Tagged<String> key_string,
                                Tagged<Object> key_pattern,
-                               FixedArray* last_match_out,
+                               Tagged<FixedArray>* last_match_out,
                                ResultsCacheType type);
   // Attempt to add value_array to the cache specified by type.  On success,
   // value_array is turned into a COW-array.
diff --git a/src/runtime/runtime-atomics.cc b/src/runtime/runtime-atomics.cc
index e72f6ef3bf1..a0cba7fa5c4 100644
--- a/src/runtime/runtime-atomics.cc
+++ b/src/runtime/runtime-atomics.cc
@@ -396,8 +396,9 @@ struct Xor {
 // but also includes the ToInteger/ToBigInt conversion that's part of
 // https://tc39.github.io/ecma262/#sec-atomicreadmodifywrite
 template <template <typename> class Op>
-Object GetModifySetValueInBuffer(RuntimeArguments args, Isolate* isolate,
-                                 const char* method_name) {
+Tagged<Object> GetModifySetValueInBuffer(RuntimeArguments args,
+                                         Isolate* isolate,
+                                         const char* method_name) {
   HandleScope scope(isolate);
   DCHECK_EQ(3, args.length());
   Handle<JSTypedArray> sta = args.at<JSTypedArray>(0);
@@ -632,9 +633,9 @@ RUNTIME_FUNCTION(Runtime_AtomicsLoadSharedStructOrArray) {
 namespace {
 
 template <typename WriteOperation>
-Object AtomicFieldWrite(Isolate* isolate, Handle<JSObject> object,
-                        Handle<Name> field_name, Handle<Object> value,
-                        WriteOperation write_operation) {
+Tagged<Object> AtomicFieldWrite(Isolate* isolate, Handle<JSObject> object,
+                                Handle<Name> field_name, Handle<Object> value,
+                                WriteOperation write_operation) {
   LookupIterator it(isolate, object, PropertyKey(isolate, field_name),
                     LookupIterator::OWN);
   Maybe<bool> result = Nothing<bool>();
diff --git a/src/runtime/runtime-classes.cc b/src/runtime/runtime-classes.cc
index 4154730f9fa..8513c356e34 100644
--- a/src/runtime/runtime-classes.cc
+++ b/src/runtime/runtime-classes.cc
@@ -75,8 +75,9 @@ RUNTIME_FUNCTION(Runtime_ThrowSuperNotCalled) {
 
 namespace {
 
-Object ThrowNotSuperConstructor(Isolate* isolate, Handle<Object> constructor,
-                                Handle<JSFunction> function) {
+Tagged<Object> ThrowNotSuperConstructor(Isolate* isolate,
+                                        Handle<Object> constructor,
+                                        Handle<JSFunction> function) {
   Handle<String> super_name;
   if (IsJSFunction(*constructor)) {
     super_name = handle(Handle<JSFunction>::cast(constructor)->shared()->Name(),
@@ -138,7 +139,8 @@ Handle<Name> KeyToName<NumberDictionary>(Isolate* isolate, Handle<Object> key) {
 //    shared name.
 template <typename Dictionary>
 MaybeHandle<Object> GetMethodAndSetName(Isolate* isolate,
-                                        RuntimeArguments& args, Smi index,
+                                        RuntimeArguments& args,
+                                        Tagged<Smi> index,
                                         Handle<String> name_prefix,
                                         Handle<Object> key) {
   int int_index = index.value();
@@ -169,8 +171,8 @@ MaybeHandle<Object> GetMethodAndSetName(Isolate* isolate,
 // This is a simplified version of GetMethodAndSetName()
 // function above that is used when it's guaranteed that the method has
 // shared name.
-Object GetMethodWithSharedName(Isolate* isolate, RuntimeArguments& args,
-                               Object index) {
+Tagged<Object> GetMethodWithSharedName(Isolate* isolate, RuntimeArguments& args,
+                                       Tagged<Object> index) {
   DisallowGarbageCollection no_gc;
   int int_index = Smi::ToInt(index);
 
@@ -191,7 +193,7 @@ Handle<Dictionary> ShallowCopyDictionaryTemplate(
       Dictionary::ShallowCopy(isolate, dictionary_template);
   // Clone all AccessorPairs in the dictionary.
   for (InternalIndex i : dictionary->IterateEntries()) {
-    Object value = dictionary->ValueAt(i);
+    Tagged<Object> value = dictionary->ValueAt(i);
     if (IsAccessorPair(value)) {
       Handle<AccessorPair> pair(AccessorPair::cast(value), isolate);
       pair = AccessorPair::Copy(isolate, pair);
@@ -207,13 +209,13 @@ bool SubstituteValues(Isolate* isolate, Handle<Dictionary> dictionary,
   // Replace all indices with proper methods.
   ReadOnlyRoots roots(isolate);
   for (InternalIndex i : dictionary->IterateEntries()) {
-    Object maybe_key = dictionary->KeyAt(i);
+    Tagged<Object> maybe_key = dictionary->KeyAt(i);
     if (!Dictionary::IsKey(roots, maybe_key)) continue;
     Handle<Object> key(maybe_key, isolate);
     Handle<Object> value(dictionary->ValueAt(i), isolate);
     if (IsAccessorPair(*value)) {
       Handle<AccessorPair> pair = Handle<AccessorPair>::cast(value);
-      Object tmp = pair->getter();
+      Tagged<Object> tmp = pair->getter();
       if (IsSmi(tmp)) {
         Handle<Object> result;
         ASSIGN_RETURN_ON_EXCEPTION_VALUE(
@@ -254,7 +256,7 @@ void UpdateProtectors(Isolate* isolate, Handle<JSObject> receiver,
                       Handle<Dictionary> properties_dictionary) {
   ReadOnlyRoots roots(isolate);
   for (InternalIndex i : properties_dictionary->IterateEntries()) {
-    Object maybe_key = properties_dictionary->KeyAt(i);
+    Tagged<Object> maybe_key = properties_dictionary->KeyAt(i);
     if (!Dictionary::IsKey(roots, maybe_key)) continue;
     Handle<Name> name(Name::cast(maybe_key), isolate);
     LookupIterator::UpdateProtector(isolate, receiver, name);
@@ -304,14 +306,14 @@ bool AddDescriptorsByTemplate(
   // values into "instantiated" |descriptors| array.
   int field_index = 0;
   for (InternalIndex i : InternalIndex::Range(nof_descriptors)) {
-    Object value = descriptors_template->GetStrongValue(i);
+    Tagged<Object> value = descriptors_template->GetStrongValue(i);
     if (IsAccessorPair(value)) {
       Handle<AccessorPair> pair = AccessorPair::Copy(
           isolate, handle(AccessorPair::cast(value), isolate));
       value = *pair;
     }
     DisallowGarbageCollection no_gc;
-    Name name = descriptors_template->GetKey(i);
+    Tagged<Name> name = descriptors_template->GetKey(i);
     // TODO(v8:5799): consider adding a ClassBoilerplate flag
     // "has_interesting_properties".
     if (name->IsInteresting(isolate)) {
@@ -329,8 +331,8 @@ bool AddDescriptorsByTemplate(
       } else {
         DCHECK_EQ(PropertyKind::kAccessor, details.kind());
         if (IsAccessorPair(value)) {
-          AccessorPair pair = AccessorPair::cast(value);
-          Object tmp = pair->getter();
+          Tagged<AccessorPair> pair = AccessorPair::cast(value);
+          Tagged<Object> tmp = pair->getter();
           if (IsSmi(tmp)) {
             pair->set_getter(GetMethodWithSharedName(isolate, args, tmp));
           }
@@ -422,7 +424,7 @@ bool AddDescriptorsByTemplate(
 
     ValueKind value_kind = ComputedEntryFlags::ValueKindBits::decode(flags);
     int key_index = ComputedEntryFlags::KeyIndexBits::decode(flags);
-    Smi value = Smi::FromInt(key_index + 1);  // Value follows name.
+    Tagged<Smi> value = Smi::FromInt(key_index + 1);  // Value follows name.
 
     Handle<Object> key = args.at(key_index);
     DCHECK(IsName(*key));
diff --git a/src/runtime/runtime-compiler.cc b/src/runtime/runtime-compiler.cc
index f2d3fb50582..1bd0ce1ea30 100644
--- a/src/runtime/runtime-compiler.cc
+++ b/src/runtime/runtime-compiler.cc
@@ -262,7 +262,7 @@ void DeoptAllOsrLoopsContainingDeoptExit(Isolate* isolate,
 
   Tagged<FeedbackVector> vector = function->feedback_vector();
   Tagged<Code> code;
-  base::SmallVector<Code, 8> osr_codes;
+  base::SmallVector<Tagged<Code>, 8> osr_codes;
   // Visit before the first loop-with-deopt is found
   for (; !it.done(); it.Advance()) {
     // We're only interested in loop ranges.
diff --git a/src/runtime/runtime-generator.cc b/src/runtime/runtime-generator.cc
index 8e867c5f21e..5ccd6cf174e 100644
--- a/src/runtime/runtime-generator.cc
+++ b/src/runtime/runtime-generator.cc
@@ -137,7 +137,7 @@ RUNTIME_FUNCTION(Runtime_AsyncGeneratorHasCatchHandlerForPC) {
   // not reach a catch handler.
   if (state < 1) return ReadOnlyRoots(isolate).false_value();
 
-  SharedFunctionInfo shared = generator->function()->shared();
+  Tagged<SharedFunctionInfo> shared = generator->function()->shared();
   DCHECK(shared->HasBytecodeArray());
   HandlerTable handler_table(shared->GetBytecodeArray(isolate));
 
diff --git a/src/runtime/runtime-module.cc b/src/runtime/runtime-module.cc
index 2d31ac847d6..0e487e5e741 100644
--- a/src/runtime/runtime-module.cc
+++ b/src/runtime/runtime-module.cc
@@ -10,10 +10,11 @@ namespace v8 {
 namespace internal {
 
 namespace {
-Handle<Script> GetEvalOrigin(Isolate* isolate, Script origin_script) {
+Handle<Script> GetEvalOrigin(Isolate* isolate, Tagged<Script> origin_script) {
   DisallowGarbageCollection no_gc;
   while (origin_script->has_eval_from_shared()) {
-    HeapObject maybe_script = origin_script->eval_from_shared()->script();
+    Tagged<HeapObject> maybe_script =
+        origin_script->eval_from_shared()->script();
     CHECK(IsScript(maybe_script));
     origin_script = Script::cast(maybe_script);
   }
diff --git a/src/runtime/runtime-numbers.cc b/src/runtime/runtime-numbers.cc
index eee0b01d891..aeabb2fcf7f 100644
--- a/src/runtime/runtime-numbers.cc
+++ b/src/runtime/runtime-numbers.cc
@@ -74,7 +74,7 @@ RUNTIME_FUNCTION(Runtime_MaxSmi) {
 RUNTIME_FUNCTION(Runtime_IsSmi) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(IsSmi(obj));
 }
 
diff --git a/src/runtime/runtime-object.cc b/src/runtime/runtime-object.cc
index 770f8efa8f9..5327b32a1aa 100644
--- a/src/runtime/runtime-object.cc
+++ b/src/runtime/runtime-object.cc
@@ -78,7 +78,8 @@ namespace {
 // complete at any time. For this reason we use the filler map word.
 // If V8_MAP_PACKING is enabled, then the filler map word is a packed filler
 // map. Otherwise, the filler map word is the same as the filler map.
-inline void ClearField(Isolate* isolate, JSObject object, FieldIndex index) {
+inline void ClearField(Isolate* isolate, Tagged<JSObject> object,
+                       FieldIndex index) {
   if (index.is_inobject()) {
     MapWord filler_map_word =
         ReadOnlyRoots(isolate).one_pointer_filler_map_word();
@@ -108,7 +109,7 @@ void GeneralizeAllTransitionsToFieldAsMutable(Isolate* isolate, Handle<Map> map,
     TransitionsAccessor transitions(isolate, *map);
     transitions.ForEachTransitionTo(
         *name,
-        [&](Map target) {
+        [&](Tagged<Map> target) {
           DCHECK_EQ(descriptor, target->LastAdded());
           DCHECK_EQ(*name, target->GetLastDescriptorName(isolate));
           PropertyDetails details = target->GetLastDescriptorDetails(isolate);
@@ -365,7 +366,7 @@ RUNTIME_FUNCTION(Runtime_ObjectHasOwnProperty) {
       if (maybe.FromJust()) return ReadOnlyRoots(isolate).true_value();
     }
 
-    Map map = js_obj->map();
+    Tagged<Map> map = js_obj->map();
     if (!IsJSGlobalProxyMap(map) &&
         (key.is_element() && key.index() <= JSObject::kMaxElementIndex
              ? !map->has_indexed_interceptor()
@@ -806,13 +807,14 @@ RUNTIME_FUNCTION(Runtime_GetProperty) {
       DisallowGarbageCollection no_gc;
       if (IsJSGlobalObject(*lookup_start_object)) {
         // Attempt dictionary lookup.
-        GlobalDictionary dictionary = JSGlobalObject::cast(*lookup_start_object)
-                                          ->global_dictionary(kAcquireLoad);
+        Tagged<GlobalDictionary> dictionary =
+            JSGlobalObject::cast(*lookup_start_object)
+                ->global_dictionary(kAcquireLoad);
         InternalIndex entry = dictionary->FindEntry(isolate, key);
         if (entry.is_found()) {
-          PropertyCell cell = dictionary->CellAt(entry);
+          Tagged<PropertyCell> cell = dictionary->CellAt(entry);
           if (cell->property_details().kind() == PropertyKind::kData) {
-            Object value = cell->value();
+            Tagged<Object> value = cell->value();
             if (!IsPropertyCellHole(value, isolate)) return value;
             // If value is the hole (meaning, absent) do the general lookup.
           }
@@ -820,7 +822,7 @@ RUNTIME_FUNCTION(Runtime_GetProperty) {
       } else if (!lookup_start_object->HasFastProperties()) {
         // Attempt dictionary lookup.
         if (V8_ENABLE_SWISS_NAME_DICTIONARY_BOOL) {
-          SwissNameDictionary dictionary =
+          Tagged<SwissNameDictionary> dictionary =
               lookup_start_object->property_dictionary_swiss();
           InternalIndex entry = dictionary->FindEntry(isolate, *key);
           if (entry.is_found() &&
@@ -828,7 +830,7 @@ RUNTIME_FUNCTION(Runtime_GetProperty) {
             return dictionary->ValueAt(entry);
           }
         } else {
-          NameDictionary dictionary =
+          Tagged<NameDictionary> dictionary =
               lookup_start_object->property_dictionary();
           InternalIndex entry = dictionary->FindEntry(isolate, key);
           if ((entry.is_found()) &&
@@ -915,8 +917,8 @@ RUNTIME_FUNCTION(Runtime_SetNamedProperty) {
 namespace {
 
 // ES6 section 12.5.4.
-Object DeleteProperty(Isolate* isolate, Handle<Object> object,
-                      Handle<Object> key, LanguageMode language_mode) {
+Tagged<Object> DeleteProperty(Isolate* isolate, Handle<Object> object,
+                              Handle<Object> key, LanguageMode language_mode) {
   Handle<JSReceiver> receiver;
   ASSIGN_RETURN_FAILURE_ON_EXCEPTION(isolate, receiver,
                                      Object::ToObject(isolate, object));
@@ -1186,7 +1188,7 @@ RUNTIME_FUNCTION(Runtime_HasFastPackedElements) {
 RUNTIME_FUNCTION(Runtime_IsJSReceiver) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(IsJSReceiver(obj));
 }
 
@@ -1770,7 +1772,7 @@ RUNTIME_FUNCTION(Runtime_SwissTableFindEntry) {
   HandleScope scope(isolate);
   DisallowGarbageCollection no_gc;
   auto table = SwissNameDictionary::cast(args[0]);
-  Name key = Name::cast(args[1]);
+  Tagged<Name> key = Name::cast(args[1]);
   InternalIndex index = table->FindEntry(isolate, key);
   return Smi::FromInt(index.is_found()
                           ? index.as_int()
@@ -1784,7 +1786,7 @@ RUNTIME_FUNCTION(Runtime_SwissTableUpdate) {
   DisallowGarbageCollection no_gc;
   auto table = SwissNameDictionary::cast(args[0]);
   InternalIndex index(args.smi_value_at(1));
-  Object value = args[2];
+  Tagged<Object> value = args[2];
   table->ValueAtPut(index, value);
 
   PropertyDetails details(Smi::cast(args[3]));
diff --git a/src/runtime/runtime-operators.cc b/src/runtime/runtime-operators.cc
index d009f770da6..dc022212965 100644
--- a/src/runtime/runtime-operators.cc
+++ b/src/runtime/runtime-operators.cc
@@ -41,24 +41,24 @@ RUNTIME_FUNCTION(Runtime_NotEqual) {
 RUNTIME_FUNCTION(Runtime_StrictEqual) {
   SealHandleScope scope(isolate);
   DCHECK_EQ(2, args.length());
-  Object x = args[0];
-  Object y = args[1];
+  Tagged<Object> x = args[0];
+  Tagged<Object> y = args[1];
   return isolate->heap()->ToBoolean(Object::StrictEquals(x, y));
 }
 
 RUNTIME_FUNCTION(Runtime_StrictNotEqual) {
   SealHandleScope scope(isolate);
   DCHECK_EQ(2, args.length());
-  Object x = args[0];
-  Object y = args[1];
+  Tagged<Object> x = args[0];
+  Tagged<Object> y = args[1];
   return isolate->heap()->ToBoolean(!Object::StrictEquals(x, y));
 }
 
 RUNTIME_FUNCTION(Runtime_ReferenceEqual) {
   SealHandleScope scope(isolate);
   DCHECK_EQ(2, args.length());
-  Object x = args[0];
-  Object y = args[1];
+  Tagged<Object> x = args[0];
+  Tagged<Object> y = args[1];
   return isolate->heap()->ToBoolean(x == y);
 }
 
diff --git a/src/runtime/runtime-regexp.cc b/src/runtime/runtime-regexp.cc
index 0095b6f2b54..5971b6d6f83 100644
--- a/src/runtime/runtime-regexp.cc
+++ b/src/runtime/runtime-regexp.cc
@@ -49,8 +49,8 @@ uint32_t GetArgcForReplaceCallable(uint32_t num_captures,
 
 // Looks up the capture of the given name. Returns the (1-based) numbered
 // capture index or -1 on failure.
-int LookupNamedCapture(const std::function<bool(String)>& name_matches,
-                       FixedArray capture_name_map) {
+int LookupNamedCapture(const std::function<bool(Tagged<String>)>& name_matches,
+                       Tagged<FixedArray> capture_name_map) {
   // TODO(jgruber): Sort capture_name_map and do binary search via
   // internalized strings.
 
@@ -62,7 +62,7 @@ int LookupNamedCapture(const std::function<bool(String)>& name_matches,
     const int name_ix = j * 2;
     const int index_ix = j * 2 + 1;
 
-    String capture_name = String::cast(capture_name_map->get(name_ix));
+    Tagged<String> capture_name = String::cast(capture_name_map->get(name_ix));
     if (!name_matches(capture_name)) continue;
 
     maybe_capture_index = Smi::ToInt(capture_name_map->get(index_ix));
@@ -150,8 +150,8 @@ class CompiledReplacement {
 
   template <typename Char>
   bool ParseReplacementPattern(base::Vector<Char> characters,
-                               FixedArray capture_name_map, int capture_count,
-                               int subject_length) {
+                               Tagged<FixedArray> capture_name_map,
+                               int capture_count, int subject_length) {
     // Equivalent to String::GetSubstitution, except that this method converts
     // the replacement string into an internal representation that avoids
     // repeated parsing when used repeatedly.
@@ -275,8 +275,8 @@ class CompiledReplacement {
             // Let capture be ? Get(namedCaptures, groupName).
 
             const int capture_index = LookupNamedCapture(
-                [=](String capture_name) {
-                  return capture_name.IsEqualTo(requested_name);
+                [=](Tagged<String> capture_name) {
+                  return capture_name->IsEqualTo(requested_name);
                 },
                 capture_name_map);
 
@@ -330,10 +330,10 @@ bool CompiledReplacement::Compile(Isolate* isolate, Handle<JSRegExp> regexp,
     String::FlatContent content = replacement->GetFlatContent(no_gc);
     DCHECK(content.IsFlat());
 
-    FixedArray capture_name_map;
+    Tagged<FixedArray> capture_name_map;
     if (capture_count > 0) {
       DCHECK(JSRegExp::TypeSupportsCaptures(regexp->type_tag()));
-      Object maybe_capture_name_map = regexp->capture_name_map();
+      Tagged<Object> maybe_capture_name_map = regexp->capture_name_map();
       if (IsFixedArray(maybe_capture_name_map)) {
         capture_name_map = FixedArray::cast(maybe_capture_name_map);
       }
@@ -464,7 +464,8 @@ void FindStringIndices(Isolate* isolate,
   }
 }
 
-void FindStringIndicesDispatch(Isolate* isolate, String subject, String pattern,
+void FindStringIndicesDispatch(Isolate* isolate, Tagged<String> subject,
+                               Tagged<String> pattern,
                                std::vector<int>* indices, unsigned int limit) {
   {
     DisallowGarbageCollection no_gc;
@@ -541,7 +542,8 @@ void TruncateRegexpIndicesList(Isolate* isolate) {
 }  // namespace
 
 template <typename ResultSeqString>
-V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalAtomRegExpWithString(
+V8_WARN_UNUSED_RESULT static Tagged<Object>
+StringReplaceGlobalAtomRegExpWithString(
     Isolate* isolate, Handle<String> subject, Handle<JSRegExp> pattern_regexp,
     Handle<String> replacement, Handle<RegExpMatchInfo> last_match_info) {
   DCHECK(subject->IsFlat());
@@ -550,7 +552,7 @@ V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalAtomRegExpWithString(
   std::vector<int>* indices = GetRewoundRegexpIndicesList(isolate);
 
   DCHECK_EQ(JSRegExp::ATOM, pattern_regexp->type_tag());
-  String pattern = pattern_regexp->atom_pattern();
+  Tagged<String> pattern = pattern_regexp->atom_pattern();
   int subject_len = subject->length();
   int pattern_len = pattern->length();
   int replacement_len = replacement->length();
@@ -620,7 +622,7 @@ V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalAtomRegExpWithString(
   return *result;
 }
 
-V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalRegExpWithString(
+V8_WARN_UNUSED_RESULT static Tagged<Object> StringReplaceGlobalRegExpWithString(
     Isolate* isolate, Handle<String> subject, Handle<JSRegExp> regexp,
     Handle<String> replacement, Handle<RegExpMatchInfo> last_match_info) {
   DCHECK(subject->IsFlat());
@@ -700,7 +702,8 @@ V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalRegExpWithString(
 }
 
 template <typename ResultSeqString>
-V8_WARN_UNUSED_RESULT static Object StringReplaceGlobalRegExpWithEmptyString(
+V8_WARN_UNUSED_RESULT static Tagged<Object>
+StringReplaceGlobalRegExpWithEmptyString(
     Isolate* isolate, Handle<String> subject, Handle<JSRegExp> regexp,
     Handle<RegExpMatchInfo> last_match_info) {
   DCHECK(subject->IsFlat());
@@ -811,7 +814,7 @@ RUNTIME_FUNCTION(Runtime_StringSplit) {
   CHECK_LT(0, pattern_length);
 
   if (limit == 0xFFFFFFFFu) {
-    FixedArray last_match_cache_unused;
+    Tagged<FixedArray> last_match_cache_unused;
     Handle<Object> cached_answer(
         RegExpResultsCache::Lookup(isolate->heap(), *subject, *pattern,
                                    &last_match_cache_unused,
@@ -988,7 +991,7 @@ class MatchInfoBackedMatch : public String::Match {
     subject_ = String::Flatten(isolate, subject);
 
     if (JSRegExp::TypeSupportsCaptures(regexp->type_tag())) {
-      Object o = regexp->capture_name_map();
+      Tagged<Object> o = regexp->capture_name_map();
       has_named_captures_ = IsFixedArray(o);
       if (has_named_captures_) {
         capture_name_map_ = handle(FixedArray::cast(o), isolate);
@@ -1030,7 +1033,9 @@ class MatchInfoBackedMatch : public String::Match {
                                       CaptureState* state) override {
     DCHECK(has_named_captures_);
     const int capture_index = LookupNamedCapture(
-        [=](String capture_name) { return capture_name->Equals(*name); },
+        [=](Tagged<String> capture_name) {
+          return capture_name->Equals(*name);
+        },
         *capture_name_map_);
 
     if (capture_index == -1) {
@@ -1139,7 +1144,7 @@ class VectorBackedMatch : public String::Match {
 // RegExpBuiltinsAssembler::ConstructNewResultFromMatchInfo).
 Handle<JSObject> ConstructNamedCaptureGroupsObject(
     Isolate* isolate, Handle<FixedArray> capture_map,
-    const std::function<Object(int)>& f_get_capture) {
+    const std::function<Tagged<Object>(int)>& f_get_capture) {
   Handle<JSObject> groups = isolate->factory()->NewJSObjectWithNullProto();
 
   const int named_capture_count = capture_map->length() >> 1;
@@ -1164,9 +1169,9 @@ Handle<JSObject> ConstructNamedCaptureGroupsObject(
 // Only called from Runtime_RegExpExecMultiple so it doesn't need to maintain
 // separate last match info.  See comment on that function.
 template <bool has_capture>
-static Object SearchRegExpMultiple(Isolate* isolate, Handle<String> subject,
-                                   Handle<JSRegExp> regexp,
-                                   Handle<RegExpMatchInfo> last_match_array) {
+static Tagged<Object> SearchRegExpMultiple(
+    Isolate* isolate, Handle<String> subject, Handle<JSRegExp> regexp,
+    Handle<RegExpMatchInfo> last_match_array) {
   DCHECK(RegExpUtils::IsUnmodifiedRegExp(isolate, regexp));
   DCHECK_NE(has_capture, regexp->capture_count() == 0);
   DCHECK(subject->IsFlat());
@@ -1190,8 +1195,8 @@ static Object SearchRegExpMultiple(Isolate* isolate, Handle<String> subject,
   static const int kMinLengthToCache = 0x1000;
 
   if (subject_length > kMinLengthToCache) {
-    FixedArray last_match_cache;
-    Object cached_answer = RegExpResultsCache::Lookup(
+    Tagged<FixedArray> last_match_cache;
+    Tagged<Object> cached_answer = RegExpResultsCache::Lookup(
         isolate->heap(), *subject, regexp->data(), &last_match_cache,
         RegExpResultsCache::REGEXP_MULTIPLE_INDICES);
     if (IsFixedArray(cached_answer)) {
@@ -1426,19 +1431,19 @@ V8_WARN_UNUSED_RESULT MaybeHandle<String> RegExpReplace(
 
     if (replace->length() == 0) {
       if (string->IsOneByteRepresentation()) {
-        Object result =
+        Tagged<Object> result =
             StringReplaceGlobalRegExpWithEmptyString<SeqOneByteString>(
                 isolate, string, regexp, last_match_info);
         return handle(String::cast(result), isolate);
       } else {
-        Object result =
+        Tagged<Object> result =
             StringReplaceGlobalRegExpWithEmptyString<SeqTwoByteString>(
                 isolate, string, regexp, last_match_info);
         return handle(String::cast(result), isolate);
       }
     }
 
-    Object result = StringReplaceGlobalRegExpWithString(
+    Tagged<Object> result = StringReplaceGlobalRegExpWithString(
         isolate, string, regexp, replace, last_match_info);
     if (IsString(result)) {
       return handle(String::cast(result), isolate);
@@ -1466,7 +1471,7 @@ RUNTIME_FUNCTION(Runtime_RegExpExecMultiple) {
   subject = String::Flatten(isolate, subject);
   CHECK(regexp->flags() & JSRegExp::kGlobal);
 
-  Object result;
+  Tagged<Object> result;
   if (regexp->capture_count() == 0) {
     result =
         SearchRegExpMultiple<false>(isolate, subject, regexp, last_match_info);
@@ -1546,7 +1551,7 @@ RUNTIME_FUNCTION(Runtime_StringReplaceNonGlobalRegExpWithFunction) {
   if (m > 1) {
     DCHECK(JSRegExp::TypeSupportsCaptures(regexp->type_tag()));
 
-    Object maybe_capture_map = regexp->capture_name_map();
+    Tagged<Object> maybe_capture_map = regexp->capture_name_map();
     if (IsFixedArray(maybe_capture_map)) {
       has_named_captures = true;
       capture_map = handle(FixedArray::cast(maybe_capture_map), isolate);
@@ -1999,7 +2004,7 @@ RUNTIME_FUNCTION(Runtime_RegExpInitializeAndCompile) {
 RUNTIME_FUNCTION(Runtime_IsRegExp) {
   SealHandleScope shs(isolate);
   DCHECK_EQ(1, args.length());
-  Object obj = args[0];
+  Tagged<Object> obj = args[0];
   return isolate->heap()->ToBoolean(IsJSRegExp(obj));
 }
 
diff --git a/src/runtime/runtime-scopes.cc b/src/runtime/runtime-scopes.cc
index 589faad0f42..c2072986289 100644
--- a/src/runtime/runtime-scopes.cc
+++ b/src/runtime/runtime-scopes.cc
@@ -29,8 +29,8 @@ namespace {
 
 enum class RedeclarationType { kSyntaxError = 0, kTypeError = 1 };
 
-Object ThrowRedeclarationError(Isolate* isolate, Handle<String> name,
-                               RedeclarationType redeclaration_type) {
+Tagged<Object> ThrowRedeclarationError(Isolate* isolate, Handle<String> name,
+                                       RedeclarationType redeclaration_type) {
   HandleScope scope(isolate);
   if (redeclaration_type == RedeclarationType::kSyntaxError) {
     THROW_NEW_ERROR_RETURN_FAILURE(
@@ -42,10 +42,10 @@ Object ThrowRedeclarationError(Isolate* isolate, Handle<String> name,
 }
 
 // May throw a RedeclarationError.
-Object DeclareGlobal(Isolate* isolate, Handle<JSGlobalObject> global,
-                     Handle<String> name, Handle<Object> value,
-                     PropertyAttributes attr, bool is_var,
-                     RedeclarationType redeclaration_type) {
+Tagged<Object> DeclareGlobal(Isolate* isolate, Handle<JSGlobalObject> global,
+                             Handle<String> name, Handle<Object> value,
+                             PropertyAttributes attr, bool is_var,
+                             RedeclarationType redeclaration_type) {
   Handle<ScriptContextTable> script_contexts(
       global->native_context()->script_context_table(), isolate);
   VariableLookupResult lookup;
@@ -140,9 +140,9 @@ RUNTIME_FUNCTION(Runtime_DeclareModuleExports) {
 
   int length = declarations->length();
   FOR_WITH_HANDLE_SCOPE(isolate, int, i = 0, i, i < length, i++, {
-    Object decl = declarations->get(i);
+    Tagged<Object> decl = declarations->get(i);
     int index;
-    Object value;
+    Tagged<Object> value;
     if (IsSmi(decl)) {
       index = Smi::ToInt(decl);
       value = ReadOnlyRoots(isolate).the_hole_value();
@@ -208,7 +208,7 @@ RUNTIME_FUNCTION(Runtime_DeclareGlobals) {
 
     // Compute the property attributes. According to ECMA-262,
     // the property must be non-configurable except in eval.
-    Script script = Script::cast(closure->shared()->script());
+    Tagged<Script> script = Script::cast(closure->shared()->script());
     PropertyAttributes attr =
         script->compilation_type() == Script::CompilationType::kEval
             ? NONE
@@ -216,8 +216,9 @@ RUNTIME_FUNCTION(Runtime_DeclareGlobals) {
 
     // ES#sec-globaldeclarationinstantiation 5.d:
     // If hasRestrictedGlobal is true, throw a SyntaxError exception.
-    Object result = DeclareGlobal(isolate, global, name, value, attr, is_var,
-                                  RedeclarationType::kSyntaxError);
+    Tagged<Object> result =
+        DeclareGlobal(isolate, global, name, value, attr, is_var,
+                      RedeclarationType::kSyntaxError);
     if (isolate->has_pending_exception()) return result;
   });
 
@@ -226,8 +227,8 @@ RUNTIME_FUNCTION(Runtime_DeclareGlobals) {
 
 namespace {
 
-Object DeclareEvalHelper(Isolate* isolate, Handle<String> name,
-                         Handle<Object> value) {
+Tagged<Object> DeclareEvalHelper(Isolate* isolate, Handle<String> name,
+                                 Handle<Object> value) {
   // Declarations are always made in a function, native, eval, or script
   // context, or a declaration block scope. Since this is called from eval, the
   // context passed is the context of the caller, which may be some nested
@@ -350,7 +351,7 @@ std::unique_ptr<Handle<Object>[]> GetCallerArguments(Isolate* isolate,
   // Find frame containing arguments passed to the caller.
   JavaScriptStackFrameIterator it(isolate);
   JavaScriptFrame* frame = it.frame();
-  std::vector<SharedFunctionInfo> functions;
+  std::vector<Tagged<SharedFunctionInfo>> functions;
   frame->GetFunctions(&functions);
   if (functions.size() > 1) {
     int inlined_jsframe_index = static_cast<int>(functions.size()) - 1;
@@ -455,7 +456,7 @@ Handle<JSObject> NewSloppyArguments(Isolate* isolate, Handle<JSFunction> callee,
         int parameter = scope_info->ContextLocalParameterNumber(i);
         if (parameter >= mapped_count) continue;
         arguments->set_the_hole(parameter);
-        Smi slot = Smi::FromInt(scope_info->ContextHeaderLength() + i);
+        Tagged<Smi> slot = Smi::FromInt(scope_info->ContextHeaderLength() + i);
         parameter_map->set_mapped_entries(parameter, slot);
       }
     } else {
@@ -475,7 +476,7 @@ Handle<JSObject> NewSloppyArguments(Isolate* isolate, Handle<JSFunction> callee,
 class HandleArguments {
  public:
   explicit HandleArguments(Handle<Object>* array) : array_(array) {}
-  Object operator[](int index) { return *array_[index]; }
+  Tagged<Object> operator[](int index) { return *array_[index]; }
 
  private:
   Handle<Object>* array_;
@@ -484,7 +485,7 @@ class HandleArguments {
 class ParameterArguments {
  public:
   explicit ParameterArguments(Address parameters) : parameters_(parameters) {}
-  Object operator[](int index) {
+  Tagged<Object> operator[](int index) {
     return *FullObjectSlot(parameters_ - (index + 1) * kSystemPointerSize);
   }
 
@@ -549,7 +550,7 @@ RUNTIME_FUNCTION(Runtime_NewRestParameter) {
       ArrayStorageAllocationMode::DONT_INITIALIZE_ARRAY_ELEMENTS);
   {
     DisallowGarbageCollection no_gc;
-    FixedArray elements = FixedArray::cast(result->elements());
+    Tagged<FixedArray> elements = FixedArray::cast(result->elements());
     WriteBarrierMode mode = elements->GetWriteBarrierMode(no_gc);
     for (int i = 0; i < num_elements; i++) {
       elements->set(i, *arguments[i + start_index], mode);
diff --git a/src/runtime/runtime-strings.cc b/src/runtime/runtime-strings.cc
index eb798b12b7c..bbddcf80fe8 100644
--- a/src/runtime/runtime-strings.cc
+++ b/src/runtime/runtime-strings.cc
@@ -109,7 +109,7 @@ MaybeHandle<String> StringReplaceOneCharWithString(
   }
   recursion_limit--;
   if (IsConsString(*subject)) {
-    ConsString cons = ConsString::cast(*subject);
+    Tagged<ConsString> cons = ConsString::cast(*subject);
     Handle<String> first = handle(cons->first(), isolate);
     Handle<String> second = handle(cons->second(), isolate);
     Handle<String> new_first;
@@ -283,12 +283,12 @@ RUNTIME_FUNCTION(Runtime_StringBuilderConcat) {
 
   {
     DisallowGarbageCollection no_gc;
-    FixedArray fixed_array = *array;
+    Tagged<FixedArray> fixed_array = *array;
 
     if (array_length == 0) {
       return ReadOnlyRoots(isolate).empty_string();
     } else if (array_length == 1) {
-      Object first = fixed_array->get(0);
+      Tagged<Object> first = fixed_array->get(0);
       if (IsString(first)) return first;
     }
     length = StringBuilderConcatLength(special_length, fixed_array,
@@ -345,10 +345,10 @@ RUNTIME_FUNCTION(Runtime_StringToArray) {
     // a LookupSingleCharacterStringFromCode for each of the characters.
     if (content.IsOneByte()) {
       base::Vector<const uint8_t> chars = content.ToOneByteVector();
-      FixedArray one_byte_table =
+      Tagged<FixedArray> one_byte_table =
           isolate->heap()->single_character_string_table();
       for (int i = 0; i < length; ++i) {
-        Object value = one_byte_table->get(chars[i]);
+        Tagged<Object> value = one_byte_table->get(chars[i]);
         DCHECK(IsString(value));
         DCHECK(ReadOnlyHeap::Contains(HeapObject::cast(value)));
         // The single-character strings are in RO space so it should
diff --git a/src/runtime/runtime-wasm.cc b/src/runtime/runtime-wasm.cc
index 60d9e051684..5d27a4bb0d5 100644
--- a/src/runtime/runtime-wasm.cc
+++ b/src/runtime/runtime-wasm.cc
@@ -73,7 +73,7 @@ class FrameFinder {
   StackFrameIterator frame_iterator_;
 };
 
-WasmInstanceObject GetWasmInstanceOnStackTop(
+Tagged<WasmInstanceObject> GetWasmInstanceOnStackTop(
     Isolate* isolate,
     std::initializer_list<StackFrame::Type> skipped_frame_types = {
         StackFrame::EXIT}) {
@@ -82,7 +82,7 @@ WasmInstanceObject GetWasmInstanceOnStackTop(
       ->wasm_instance();
 }
 
-Context GetNativeContextFromWasmInstanceOnStackTop(Isolate* isolate) {
+Tagged<Context> GetNativeContextFromWasmInstanceOnStackTop(Isolate* isolate) {
   return GetWasmInstanceOnStackTop(isolate)->native_context();
 }
 
@@ -111,8 +111,8 @@ class V8_NODISCARD ClearThreadInWasmScope {
   const bool is_thread_in_wasm_;
 };
 
-Object ThrowWasmError(Isolate* isolate, MessageTemplate message,
-                      Handle<Object> arg0 = Handle<Object>()) {
+Tagged<Object> ThrowWasmError(Isolate* isolate, MessageTemplate message,
+                              Handle<Object> arg0 = Handle<Object>()) {
   Handle<JSObject> error_obj =
       isolate->factory()->NewWasmRuntimeError(message, arg0);
   JSObject::AddProperty(isolate, error_obj,
@@ -191,9 +191,10 @@ RUNTIME_FUNCTION(Runtime_WasmJSToWasmObject) {
   bool success =
       JSToWasmObject(isolate, value, expected_canonical, &error_message)
           .ToHandle(&result);
-  Object ret = success ? *result
-                       : isolate->Throw(*isolate->factory()->NewTypeError(
-                             MessageTemplate::kWasmTrapJSTypeError));
+  Tagged<Object> ret = success
+                           ? *result
+                           : isolate->Throw(*isolate->factory()->NewTypeError(
+                                 MessageTemplate::kWasmTrapJSTypeError));
   if (thread_in_wasm && !isolate->has_pending_exception()) {
     trap_handler::SetThreadInWasm();
   }
@@ -204,7 +205,7 @@ RUNTIME_FUNCTION(Runtime_WasmMemoryGrow) {
   ClearThreadInWasmScope flag_scope(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(3, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   // {memory_index} and {delta_pages} are checked to be positive Smis in the
   // WasmMemoryGrow builtin which calls this runtime function.
   uint32_t memory_index = args.positive_smi_value_at(1);
@@ -301,7 +302,7 @@ RUNTIME_FUNCTION(Runtime_WasmStackGuard) {
 RUNTIME_FUNCTION(Runtime_WasmCompileLazy) {
   ClearThreadInWasmScope wasm_flag(isolate);
   DCHECK_EQ(2, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   int func_index = args.smi_value_at(1);
 
   TRACE_EVENT1("v8.wasm", "wasm.CompileLazy", "func_index", func_index);
@@ -442,7 +443,8 @@ RUNTIME_FUNCTION(Runtime_TierUpWasmToJSWrapper) {
     size_t expected_arity = sig.parameter_count();
     wasm::ImportCallKind kind = wasm::kDefaultImportCallKind;
     if (IsJSFunction(ref->callable())) {
-      SharedFunctionInfo shared = JSFunction::cast(ref->callable())->shared();
+      Tagged<SharedFunctionInfo> shared =
+          JSFunction::cast(ref->callable())->shared();
       expected_arity =
           shared->internal_formal_parameter_count_without_receiver();
       if (expected_arity != sig.parameter_count()) {
@@ -551,7 +553,7 @@ RUNTIME_FUNCTION(Runtime_WasmTriggerTierUp) {
   {
     DisallowGarbageCollection no_gc;
     DCHECK_EQ(1, args.length());
-    WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+    Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
 
     FrameFinder<WasmFrame> frame_finder(isolate);
     int func_index = frame_finder.frame()->function_index();
@@ -571,7 +573,7 @@ RUNTIME_FUNCTION(Runtime_WasmTriggerTierUp) {
   if (check.InterruptRequested()) {
     // Note: This might trigger a GC, which invalidates the {args} object (see
     // https://crbug.com/v8/13036#2).
-    Object result = isolate->stack_guard()->HandleInterrupts();
+    Tagged<Object> result = isolate->stack_guard()->HandleInterrupts();
     if (IsException(result)) return result;
   }
 
@@ -582,7 +584,7 @@ RUNTIME_FUNCTION(Runtime_WasmAtomicNotify) {
   ClearThreadInWasmScope clear_wasm_flag(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(4, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   int memory_index = args.smi_value_at(1);
   double offset_double = args.number_value_at(2);
   uintptr_t offset = static_cast<uintptr_t>(offset_double);
@@ -600,12 +602,12 @@ RUNTIME_FUNCTION(Runtime_WasmI32AtomicWait) {
   ClearThreadInWasmScope clear_wasm_flag(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(5, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   int memory_index = args.smi_value_at(1);
   double offset_double = args.number_value_at(2);
   uintptr_t offset = static_cast<uintptr_t>(offset_double);
   int32_t expected_value = NumberToInt32(args[3]);
-  BigInt timeout_ns = BigInt::cast(args[4]);
+  Tagged<BigInt> timeout_ns = BigInt::cast(args[4]);
 
   Handle<JSArrayBuffer> array_buffer{
       instance->memory_object(memory_index)->array_buffer(), isolate};
@@ -626,12 +628,12 @@ RUNTIME_FUNCTION(Runtime_WasmI64AtomicWait) {
   ClearThreadInWasmScope clear_wasm_flag(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(5, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   int memory_index = args.smi_value_at(1);
   double offset_double = args.number_value_at(2);
   uintptr_t offset = static_cast<uintptr_t>(offset_double);
-  BigInt expected_value = BigInt::cast(args[3]);
-  BigInt timeout_ns = BigInt::cast(args[4]);
+  Tagged<BigInt> expected_value = BigInt::cast(args[3]);
+  Tagged<BigInt> timeout_ns = BigInt::cast(args[4]);
 
   Handle<JSArrayBuffer> array_buffer{
       instance->memory_object(memory_index)->array_buffer(), isolate};
@@ -650,8 +652,8 @@ RUNTIME_FUNCTION(Runtime_WasmI64AtomicWait) {
 }
 
 namespace {
-Object ThrowTableOutOfBounds(Isolate* isolate,
-                             Handle<WasmInstanceObject> instance) {
+Tagged<Object> ThrowTableOutOfBounds(Isolate* isolate,
+                                     Handle<WasmInstanceObject> instance) {
   // Handle out-of-bounds access here in the runtime call, rather
   // than having the lower-level layers deal with JS exceptions.
   if (isolate->context().is_null()) {
@@ -686,7 +688,7 @@ RUNTIME_FUNCTION(Runtime_WasmFunctionTableGet) {
   ClearThreadInWasmScope flag_scope(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(3, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t table_index = args.positive_smi_value_at(1);
   uint32_t entry_index = args.positive_smi_value_at(2);
   DCHECK_LT(table_index, instance->tables()->length());
@@ -710,7 +712,7 @@ RUNTIME_FUNCTION(Runtime_WasmFunctionTableSet) {
   ClearThreadInWasmScope flag_scope(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(4, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t table_index = args.positive_smi_value_at(1);
   uint32_t entry_index = args.positive_smi_value_at(2);
   Handle<Object> element(args[3], isolate);
@@ -784,7 +786,7 @@ RUNTIME_FUNCTION(Runtime_WasmTableGrow) {
   ClearThreadInWasmScope flag_scope(isolate);
   HandleScope scope(isolate);
   DCHECK_EQ(4, args.length());
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t table_index = args.positive_smi_value_at(1);
   Handle<Object> value(args[2], isolate);
   uint32_t delta = args.positive_smi_value_at(3);
@@ -849,7 +851,8 @@ bool ExecuteWasmDebugBreaks(Isolate* isolate,
                                      frame->id());
     script->set_break_on_entry(false);
     // Update the "break_on_entry" flag on all live instances.
-    i::WeakArrayList weak_instance_list = script->wasm_weak_instance_list();
+    i::Tagged<i::WeakArrayList> weak_instance_list =
+        script->wasm_weak_instance_list();
     for (int i = 0; i < weak_instance_list->length(); ++i) {
       if (weak_instance_list->Get(i)->IsCleared()) continue;
       i::WasmInstanceObject::cast(weak_instance_list->Get(i).GetHeapObject())
@@ -914,7 +917,8 @@ RUNTIME_FUNCTION(Runtime_WasmDebugBreak) {
   // code GC to get rid of temporarily created Wasm code.
   StackLimitCheck check(isolate);
   if (check.InterruptRequested()) {
-    Object interrupt_object = isolate->stack_guard()->HandleInterrupts();
+    Tagged<Object> interrupt_object =
+        isolate->stack_guard()->HandleInterrupts();
     // Interrupt handling can create an exception, including the
     // termination exception.
     if (IsException(interrupt_object, isolate)) return interrupt_object;
@@ -932,9 +936,9 @@ RUNTIME_FUNCTION(Runtime_WasmArrayCopy) {
   HandleScope scope(isolate);
   DisallowGarbageCollection no_gc;
   DCHECK_EQ(5, args.length());
-  WasmArray dst_array = WasmArray::cast(args[0]);
+  Tagged<WasmArray> dst_array = WasmArray::cast(args[0]);
   uint32_t dst_index = args.positive_smi_value_at(1);
-  WasmArray src_array = WasmArray::cast(args[2]);
+  Tagged<WasmArray> src_array = WasmArray::cast(args[2]);
   uint32_t src_index = args.positive_smi_value_at(3);
   uint32_t length = args.positive_smi_value_at(4);
   DCHECK_GT(length, 0);
@@ -1178,7 +1182,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringNewWtf8) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(5, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t memory = args.positive_smi_value_at(1);
   uint32_t utf8_variant_value = args.positive_smi_value_at(2);
   uint32_t offset = NumberToUint32(args[3]);
@@ -1240,7 +1244,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringNewWtf16) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(4, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t memory = args.positive_smi_value_at(1);
   uint32_t offset = NumberToUint32(args[2]);
   uint32_t size_in_codeunits = NumberToUint32(args[3]);
@@ -1293,7 +1297,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringConst) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(2, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   static_assert(
       base::IsInRange(wasm::kV8MaxWasmStringLiterals, 0, Smi::kMaxValue));
   uint32_t index = args.positive_smi_value_at(1);
@@ -1415,9 +1419,10 @@ int EncodeWtf8(base::Vector<char> bytes, size_t offset,
   return static_cast<int>(dst - dst_start);
 }
 template <typename GetWritableBytes>
-Object EncodeWtf8(Isolate* isolate, unibrow::Utf8Variant variant,
-                  Handle<String> string, GetWritableBytes get_writable_bytes,
-                  size_t offset, MessageTemplate out_of_bounds_message) {
+Tagged<Object> EncodeWtf8(Isolate* isolate, unibrow::Utf8Variant variant,
+                          Handle<String> string,
+                          GetWritableBytes get_writable_bytes, size_t offset,
+                          MessageTemplate out_of_bounds_message) {
   string = String::Flatten(isolate, string);
   MessageTemplate message;
   int written;
@@ -1480,7 +1485,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringEncodeWtf8) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(5, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t memory = args.positive_smi_value_at(1);
   uint32_t utf8_variant_value = args.positive_smi_value_at(2);
   Handle<String> string(String::cast(args[3]), isolate);
@@ -1526,9 +1531,9 @@ RUNTIME_FUNCTION(Runtime_WasmStringEncodeWtf16) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(6, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t memory = args.positive_smi_value_at(1);
-  String string = String::cast(args[2]);
+  Tagged<String> string = String::cast(args[2]);
   uint32_t offset = NumberToUint32(args[3]);
   uint32_t start = args.positive_smi_value_at(4);
   uint32_t length = args.positive_smi_value_at(5);
@@ -1589,7 +1594,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringViewWtf8Encode) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(6, args.length());
   HandleScope scope(isolate);
-  WasmInstanceObject instance = WasmInstanceObject::cast(args[0]);
+  Tagged<WasmInstanceObject> instance = WasmInstanceObject::cast(args[0]);
   uint32_t utf8_variant_value = args.positive_smi_value_at(1);
   Handle<ByteArray> array(ByteArray::cast(args[2]), isolate);
   uint32_t addr = NumberToUint32(args[3]);
@@ -1685,7 +1690,7 @@ RUNTIME_FUNCTION(Runtime_WasmStringFromCodePoint) {
 RUNTIME_FUNCTION(Runtime_WasmStringHash) {
   ClearThreadInWasmScope flag_scope(isolate);
   DCHECK_EQ(1, args.length());
-  String string(String::cast(args[0]));
+  Tagged<String> string(String::cast(args[0]));
   uint32_t hash = string->EnsureHash();
   return Smi::FromInt(static_cast<int>(hash));
 }
diff --git a/src/snapshot/context-serializer.cc b/src/snapshot/context-serializer.cc
index 384a30c208e..47407d3297f 100644
--- a/src/snapshot/context-serializer.cc
+++ b/src/snapshot/context-serializer.cc
@@ -52,7 +52,7 @@ class V8_NODISCARD SanitizeNativeContextScope final {
   }
 
  private:
-  NativeContext native_context_;
+  Tagged<NativeContext> native_context_;
   ExternalPointerSlot::RawContent microtask_queue_external_pointer_;
   const DisallowGarbageCollection& no_gc_;
 };
@@ -74,7 +74,7 @@ ContextSerializer::~ContextSerializer() {
   OutputStatistics("ContextSerializer");
 }
 
-void ContextSerializer::Serialize(Context* o,
+void ContextSerializer::Serialize(Tagged<Context>* o,
                                   const DisallowGarbageCollection& no_gc) {
   context_ = *o;
   DCHECK(IsNativeContext(context_));
diff --git a/src/snapshot/context-serializer.h b/src/snapshot/context-serializer.h
index 2d4214a7ac0..012125d6a77 100644
--- a/src/snapshot/context-serializer.h
+++ b/src/snapshot/context-serializer.h
@@ -24,7 +24,7 @@ class V8_EXPORT_PRIVATE ContextSerializer : public Serializer {
   ContextSerializer& operator=(const ContextSerializer&) = delete;
 
   // Serialize the objects reachable from a single object pointer.
-  void Serialize(Context* o, const DisallowGarbageCollection& no_gc);
+  void Serialize(Tagged<Context>* o, const DisallowGarbageCollection& no_gc);
 
   bool can_be_rehashed() const { return can_be_rehashed_; }
 
@@ -40,7 +40,7 @@ class V8_EXPORT_PRIVATE ContextSerializer : public Serializer {
   // Indicates whether we only serialized hash tables that we can rehash.
   // TODO(yangguo): generalize rehashing, and remove this flag.
   bool can_be_rehashed_;
-  Context context_;
+  Tagged<Context> context_;
 
   // Used to store serialized data for embedder fields.
   SnapshotByteSink embedder_fields_sink_;
diff --git a/src/snapshot/serializer-deserializer.cc b/src/snapshot/serializer-deserializer.cc
index eb49a49b365..828861ccaba 100644
--- a/src/snapshot/serializer-deserializer.cc
+++ b/src/snapshot/serializer-deserializer.cc
@@ -11,7 +11,7 @@ namespace internal {
 
 namespace {
 DISABLE_CFI_PERF
-void IterateObjectCache(Isolate* isolate, std::vector<Object>* cache,
+void IterateObjectCache(Isolate* isolate, std::vector<Tagged<Object>>* cache,
                         Root root_id, RootVisitor* visitor) {
   for (size_t i = 0;; ++i) {
     // Extend the array ready to get a value when deserializing.
diff --git a/src/snapshot/serializer-inl.h b/src/snapshot/serializer-inl.h
index 487efe35a99..3b63bbcb763 100644
--- a/src/snapshot/serializer-inl.h
+++ b/src/snapshot/serializer-inl.h
@@ -12,7 +12,8 @@ namespace v8 {
 namespace internal {
 
 bool Serializer::IsNotMappedSymbol(Tagged<HeapObject> obj) const {
-  Object not_mapped_symbol = ReadOnlyRoots(isolate()).not_mapped_symbol();
+  Tagged<Object> not_mapped_symbol =
+      ReadOnlyRoots(isolate()).not_mapped_symbol();
   if (V8_EXTERNAL_CODE_SPACE_BOOL) {
     // It's possible that a InstructionStream object might have the same
     // compressed value as the not_mapped_symbol, so we must compare full
diff --git a/src/snapshot/serializer.cc b/src/snapshot/serializer.cc
index cb1120e3fdf..a2ee51f8e0f 100644
--- a/src/snapshot/serializer.cc
+++ b/src/snapshot/serializer.cc
@@ -156,7 +156,7 @@ void Serializer::SerializeObject(Handle<HeapObject> obj, SlotType slot_type) {
   if (IsThinString(*obj, isolate())) {
     obj = handle(ThinString::cast(*obj)->actual(isolate()), isolate());
   } else if (IsCode(*obj, isolate())) {
-    Code code = Code::cast(*obj);
+    Tagged<Code> code = Code::cast(*obj);
     if (code->kind() == CodeKind::BASELINE) {
       // For now just serialize the BytecodeArray instead of baseline code.
       // TODO(v8:11429,pthier): Handle Baseline code in cases we want to
@@ -177,7 +177,7 @@ void Serializer::VisitRootPointers(Root root, const char* description,
 }
 
 void Serializer::SerializeRootObject(FullObjectSlot slot) {
-  Object o = *slot;
+  Tagged<Object> o = *slot;
   if (IsSmi(o)) {
     PutSmiRoot(slot);
   } else {
@@ -553,13 +553,14 @@ uint32_t Serializer::ObjectSerializer::SerializeBackingStore(
 void Serializer::ObjectSerializer::SerializeJSTypedArray() {
   {
     DisallowGarbageCollection no_gc;
-    JSTypedArray typed_array = JSTypedArray::cast(*object_);
+    Tagged<JSTypedArray> typed_array = JSTypedArray::cast(*object_);
     if (typed_array->is_on_heap()) {
       typed_array->RemoveExternalPointerCompensationForSerialization(isolate());
     } else {
       if (!typed_array->IsDetachedOrOutOfBounds()) {
         // Explicitly serialize the backing store now.
-        JSArrayBuffer buffer = JSArrayBuffer::cast(typed_array->buffer());
+        Tagged<JSArrayBuffer> buffer =
+            JSArrayBuffer::cast(typed_array->buffer());
         // We cannot store byte_length or max_byte_length larger than uint32
         // range in the snapshot.
         size_t byte_length_size = buffer->GetByteLength();
@@ -596,7 +597,7 @@ void Serializer::ObjectSerializer::SerializeJSArrayBuffer() {
   void* backing_store;
   {
     DisallowGarbageCollection no_gc;
-    JSArrayBuffer buffer = JSArrayBuffer::cast(*object_);
+    Tagged<JSArrayBuffer> buffer = JSArrayBuffer::cast(*object_);
     backing_store = buffer->backing_store();
     // We cannot store byte_length or max_byte_length larger than uint32 range
     // in the snapshot.
@@ -624,7 +625,7 @@ void Serializer::ObjectSerializer::SerializeJSArrayBuffer() {
   }
   SerializeObject();
   {
-    JSArrayBuffer buffer = JSArrayBuffer::cast(*object_);
+    Tagged<JSArrayBuffer> buffer = JSArrayBuffer::cast(*object_);
     buffer->set_backing_store(isolate(), backing_store);
     buffer->set_extension(extension);
   }
@@ -738,7 +739,7 @@ class V8_NODISCARD UnlinkWeakNextScope {
 
  private:
   Tagged<HeapObject> object_;
-  Object next_ = Smi::zero();
+  Tagged<Object> next_ = Smi::zero();
   DISALLOW_GARBAGE_COLLECTION(no_gc_)
 };
 
@@ -1252,7 +1253,7 @@ Handle<FixedArray> ObjectCacheIndexMap::Values(Isolate* isolate) {
   }
   Handle<FixedArray> externals = isolate->factory()->NewFixedArray(size());
   DisallowGarbageCollection no_gc;
-  FixedArray raw = *externals;
+  Tagged<FixedArray> raw = *externals;
   IdentityMap<int, base::DefaultAllocationPolicy>::IteratableScope it_scope(
       &map_);
   for (auto it = it_scope.begin(); it != it_scope.end(); ++it) {
diff --git a/src/snapshot/shared-heap-serializer.cc b/src/snapshot/shared-heap-serializer.cc
index 853c3a88d52..444b4c17890 100644
--- a/src/snapshot/shared-heap-serializer.cc
+++ b/src/snapshot/shared-heap-serializer.cc
@@ -88,7 +88,7 @@ bool SharedHeapSerializer::SerializeUsingSharedHeapObjectCache(
   // because the live isolate may have had new internalized strings that were
   // not present in the startup snapshot to be serialized.
   if (ShouldReconstructSharedHeapObjectCacheForTesting()) {
-    std::vector<Object>* existing_cache =
+    std::vector<Tagged<Object>>* existing_cache =
         isolate()->shared_space_isolate()->shared_heap_object_cache();
     const size_t existing_cache_size = existing_cache->size();
     // This is strictly < because the existing cache contains the terminating
@@ -201,7 +201,7 @@ bool SharedHeapSerializer::ShouldReconstructSharedHeapObjectCacheForTesting()
 }
 
 void SharedHeapSerializer::ReconstructSharedHeapObjectCacheForTesting() {
-  std::vector<Object>* cache =
+  std::vector<Tagged<Object>>* cache =
       isolate()->shared_space_isolate()->shared_heap_object_cache();
   // Don't reconstruct the final element, which is always undefined and marks
   // the end of the cache, since serializing the live Isolate may extend the
diff --git a/src/snapshot/snapshot.cc b/src/snapshot/snapshot.cc
index 45f11e89463..03340e2cc33 100644
--- a/src/snapshot/snapshot.cc
+++ b/src/snapshot/snapshot.cc
@@ -383,7 +383,7 @@ void Snapshot::SerializeDeserializeAndVerifyForTesting(
         ((isolate->has_shared_space() || ReadOnlyHeap::IsReadOnlySpaceShared())
              ? Snapshot::kReconstructReadOnlyAndSharedObjectCachesForTesting
              : 0));
-    std::vector<Context> contexts{*default_context};
+    std::vector<Tagged<Context>> contexts{*default_context};
     std::vector<SerializeInternalFieldsCallback> callbacks{{}};
     serialized_data = Snapshot::Create(isolate, &contexts, callbacks,
                                        safepoint_scope, no_gc, flags);
@@ -426,7 +426,7 @@ void Snapshot::SerializeDeserializeAndVerifyForTesting(
 
 // static
 v8::StartupData Snapshot::Create(
-    Isolate* isolate, std::vector<Context>* contexts,
+    Isolate* isolate, std::vector<Tagged<Context>>* contexts,
     const std::vector<SerializeInternalFieldsCallback>&
         embedder_fields_serializers,
     const SafepointScope& safepoint_scope,
@@ -1090,7 +1090,7 @@ StartupData SnapshotCreatorImpl::CreateBlob(
   // Note these contexts may be dead after calling Clear(), but will not be
   // collected until serialization completes and the DisallowGarbageCollection
   // scope above goes out of scope.
-  std::vector<Context> raw_contexts;
+  std::vector<Tagged<Context>> raw_contexts;
   raw_contexts.reserve(num_contexts);
   {
     HandleScope scope(isolate_);
diff --git a/src/snapshot/snapshot.h b/src/snapshot/snapshot.h
index dcbb33fa17e..f792f722364 100644
--- a/src/snapshot/snapshot.h
+++ b/src/snapshot/snapshot.h
@@ -67,7 +67,7 @@ class Snapshot : public AllStatic {
   // associated callback to serialize internal fields. The default context must
   // be passed at index 0.
   static v8::StartupData Create(
-      Isolate* isolate, std::vector<Context>* contexts,
+      Isolate* isolate, std::vector<Tagged<Context>>* contexts,
       const std::vector<SerializeInternalFieldsCallback>&
           embedder_fields_serializers,
       const SafepointScope& safepoint_scope,
diff --git a/src/snapshot/startup-serializer.cc b/src/snapshot/startup-serializer.cc
index 98e9d8a6a53..34c98eec754 100644
--- a/src/snapshot/startup-serializer.cc
+++ b/src/snapshot/startup-serializer.cc
@@ -54,8 +54,8 @@ class V8_NODISCARD SanitizeIsolateScope final {
 
  private:
   Isolate* isolate_;
-  const Object feedback_vectors_for_profiling_tools_;
-  const WeakArrayList detached_contexts_;
+  const Tagged<Object> feedback_vectors_for_profiling_tools_;
+  const Tagged<WeakArrayList> detached_contexts_;
 };
 
 }  // namespace
@@ -166,8 +166,8 @@ void StartupSerializer::SerializeStrongReferences(
                                     SkipRoot::kTracedHandles});
 }
 
-SerializedHandleChecker::SerializedHandleChecker(Isolate* isolate,
-                                                 std::vector<Context>* contexts)
+SerializedHandleChecker::SerializedHandleChecker(
+    Isolate* isolate, std::vector<Tagged<Context>>* contexts)
     : isolate_(isolate) {
   AddToSet(isolate->heap()->serialized_objects());
   for (auto const& context : *contexts) {
diff --git a/src/snapshot/startup-serializer.h b/src/snapshot/startup-serializer.h
index e16465d8253..4fb40baa2a7 100644
--- a/src/snapshot/startup-serializer.h
+++ b/src/snapshot/startup-serializer.h
@@ -59,7 +59,8 @@ class V8_EXPORT_PRIVATE StartupSerializer : public RootsSerializer {
 
 class SerializedHandleChecker : public RootVisitor {
  public:
-  SerializedHandleChecker(Isolate* isolate, std::vector<Context>* contexts);
+  SerializedHandleChecker(Isolate* isolate,
+                          std::vector<Tagged<Context>>* contexts);
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override;
   bool CheckGlobalAndEternalHandles();
@@ -68,7 +69,7 @@ class SerializedHandleChecker : public RootVisitor {
   void AddToSet(Tagged<FixedArray> serialized);
 
   Isolate* isolate_;
-  std::unordered_set<Object, Object::Hasher> serialized_;
+  std::unordered_set<Tagged<Object>, Object::Hasher> serialized_;
   bool ok_ = true;
 };
 
diff --git a/src/strings/string-builder-inl.h b/src/strings/string-builder-inl.h
index 66f665b0a02..e022ab10b9f 100644
--- a/src/strings/string-builder-inl.h
+++ b/src/strings/string-builder-inl.h
@@ -249,7 +249,7 @@ class IncrementalStringBuilder {
     DestChar* start_;
     DestChar* cursor_;
 #ifdef DEBUG
-    String string_;
+    Tagged<String> string_;
 #endif
     DISALLOW_GARBAGE_COLLECTION(no_gc_)
   };
diff --git a/src/torque/cc-generator.cc b/src/torque/cc-generator.cc
index 53f1d368d11..9d3cf92b3fd 100644
--- a/src/torque/cc-generator.cc
+++ b/src/torque/cc-generator.cc
@@ -398,10 +398,10 @@ void CCGenerator::EmitInstruction(const LoadReferenceInstruction& instruction,
       // HeapObject|TaggedZeroPattern, which is output as "Object". TaggedField
       // requires HeapObject, so we need a cast.
       out() << "TaggedField<" << result_type
-            << ">::load(*static_cast<HeapObject*>(&" << object
+            << ">::load(Tagged<HeapObject>::unchecked_cast(" << object
             << "), static_cast<int>(" << offset << "));\n";
     } else {
-      out() << "(" << object << ").ReadField<" << result_type << ">(" << offset
+      out() << "(" << object << ")->ReadField<" << result_type << ">(" << offset
             << ");\n";
     }
   } else {
diff --git a/src/torque/implementation-visitor.cc b/src/torque/implementation-visitor.cc
index 50d7b854bfe..fcac0fdeb20 100644
--- a/src/torque/implementation-visitor.cc
+++ b/src/torque/implementation-visitor.cc
@@ -5027,8 +5027,8 @@ void ImplementationVisitor::GenerateBodyDescriptors(
 
         h_contents << "  template <typename ObjectVisitor>\n";
         h_contents
-            << "  static inline void IterateBody(Map map, HeapObject obj, "
-               "int object_size, ObjectVisitor* v) {\n";
+            << "  static inline void IterateBody(Tagged<Map> map, "
+               "Tagged<HeapObject> obj, int object_size, ObjectVisitor* v) {\n";
 
         std::vector<ObjectSlotKind> slots = std::move(header_slot_kinds);
         if (has_array_fields) slots.push_back(*array_slot_kind);
@@ -5093,8 +5093,8 @@ void ImplementationVisitor::GenerateBodyDescriptors(
         h_contents << "  }\n\n";
       }
 
-      h_contents
-          << "  static inline int SizeOf(Map map, HeapObject raw_object) {\n";
+      h_contents << "  static inline int SizeOf(Tagged<Map> map, "
+                    "Tagged<HeapObject> raw_object) {\n";
       if (type->size().SingleValue()) {
         h_contents << "    return " << *type->size().SingleValue() << ";\n";
       } else {
@@ -5125,6 +5125,8 @@ void GenerateFieldValueVerifier(const std::string& class_name, bool indexed,
   bool maybe_object =
       !field_type->IsSubtypeOf(TypeOracle::GetStrongTaggedType());
   const char* object_type = maybe_object ? "MaybeObject" : "Object";
+  const char* tagged_object_type =
+      maybe_object ? "MaybeObject" : "Tagged<Object>";
   const char* verify_fn =
       maybe_object ? "VerifyMaybeObjectPointer" : "VerifyPointer";
   if (indexed) {
@@ -5135,10 +5137,12 @@ void GenerateFieldValueVerifier(const std::string& class_name, bool indexed,
 
   // Read the field.
   if (is_map) {
-    cc_contents << "    " << object_type << " " << value << " = o.map();\n";
+    cc_contents << "    " << tagged_object_type << " " << value
+                << " = o->map();\n";
   } else {
-    cc_contents << "    " << object_type << " " << value << " = TaggedField<"
-                << object_type << ">::load(o, " << offset << ");\n";
+    cc_contents << "    " << tagged_object_type << " " << value
+                << " = TaggedField<" << object_type << ">::load(o, " << offset
+                << ");\n";
   }
 
   // Call VerifyPointer or VerifyMaybeObjectPointer on it.
@@ -5244,6 +5248,7 @@ void ImplementationVisitor::GenerateClassVerifiers(
 
     // Generate forward declarations to avoid including any headers.
     h_contents << "class Isolate;\n";
+    h_contents << "template<typename T>\nclass Tagged;\n";
     for (const ClassType* type : TypeOracle::GetClasses()) {
       if (!type->ShouldGenerateVerify()) continue;
       h_contents << "class " << type->name() << ";\n";
@@ -5256,15 +5261,16 @@ void ImplementationVisitor::GenerateClassVerifiers(
 
     for (const ClassType* type : TypeOracle::GetClasses()) {
       std::string name = type->name();
+      std::string cpp_name = type->TagglifiedCppTypeName();
       if (!type->ShouldGenerateVerify()) continue;
 
       std::string method_name = name + "Verify";
 
-      h_contents << "  static void " << method_name << "(" << name
+      h_contents << "  static void " << method_name << "(" << cpp_name
                  << " o, Isolate* isolate);\n";
 
       cc_contents << "void " << verifier_class << "::" << method_name << "("
-                  << name << " o, Isolate* isolate) {\n";
+                  << cpp_name << " o, Isolate* isolate) {\n";
 
       // First, do any verification for the super class. Not all classes have
       // verifiers, so skip to the nearest super class that has one.
@@ -5274,7 +5280,7 @@ void ImplementationVisitor::GenerateClassVerifiers(
       }
       if (super_type) {
         std::string super_name = super_type->name();
-        cc_contents << "  o." << super_name << "Verify(isolate);\n";
+        cc_contents << "  o->" << super_name << "Verify(isolate);\n";
       }
 
       // Second, verify that this object is what it claims to be.
diff --git a/src/torque/types.cc b/src/torque/types.cc
index 06bcc789d19..7c89026821e 100644
--- a/src/torque/types.cc
+++ b/src/torque/types.cc
@@ -1333,9 +1333,9 @@ base::Optional<NameAndType> ExtractSimpleFieldArraySize(
 }
 
 std::string Type::GetRuntimeType() const {
-  if (IsSubtypeOf(TypeOracle::GetSmiType())) return "Smi";
+  if (IsSubtypeOf(TypeOracle::GetSmiType())) return "Tagged<Smi>";
   if (IsSubtypeOf(TypeOracle::GetTaggedType())) {
-    return GetGeneratedTNodeTypeName();
+    return "Tagged<" + GetGeneratedTNodeTypeName() + ">";
   }
   if (base::Optional<const StructType*> struct_type = StructSupertype()) {
     std::stringstream result;
diff --git a/src/utils/address-map.cc b/src/utils/address-map.cc
index 111d0e97736..25aa828b52a 100644
--- a/src/utils/address-map.cc
+++ b/src/utils/address-map.cc
@@ -16,14 +16,14 @@ RootIndexMap::RootIndexMap(Isolate* isolate) {
   map_ = new HeapObjectToIndexHashMap();
   for (RootIndex root_index = RootIndex::kFirstStrongOrReadOnlyRoot;
        root_index <= RootIndex::kLastStrongOrReadOnlyRoot; ++root_index) {
-    Object root = isolate->root(root_index);
+    Tagged<Object> root = isolate->root(root_index);
     if (!IsHeapObject(root)) continue;
     // Omit root entries that can be written after initialization. They must
     // not be referenced through the root list in the snapshot.
     // Since we map the raw address of an root item to its root list index, the
     // raw address must be constant, i.e. the object must be immovable.
     if (RootsTable::IsImmortalImmovable(root_index)) {
-      HeapObject heap_object = HeapObject::cast(root);
+      Tagged<HeapObject> heap_object = HeapObject::cast(root);
       Maybe<uint32_t> maybe_index = map_->Get(heap_object);
       uint32_t index = static_cast<uint32_t>(root_index);
       if (maybe_index.IsJust()) {
diff --git a/src/wasm/module-compiler.cc b/src/wasm/module-compiler.cc
index f6ac958579a..b5c0f9a0d62 100644
--- a/src/wasm/module-compiler.cc
+++ b/src/wasm/module-compiler.cc
@@ -1313,7 +1313,7 @@ class TransitiveTypeFeedbackProcessor {
   }
 
   DisallowGarbageCollection no_gc_scope_;
-  WasmInstanceObject instance_;
+  Tagged<WasmInstanceObject> instance_;
   const WasmModule* const module_;
   // TODO(jkummerow): Check if it makes a difference to apply any updates
   // as a single batch at the end.
@@ -1392,7 +1392,7 @@ class FeedbackMaker {
   std::vector<CallSiteFeedback>&& GetResult() && { return std::move(result_); }
 
  private:
-  const WasmInstanceObject instance_;
+  const Tagged<WasmInstanceObject> instance_;
   std::vector<CallSiteFeedback> result_;
   const int num_imported_functions_;
   const int func_index_;
diff --git a/src/wasm/wasm-external-refs.cc b/src/wasm/wasm-external-refs.cc
index b75097ef363..44ee16e5343 100644
--- a/src/wasm/wasm-external-refs.cc
+++ b/src/wasm/wasm-external-refs.cc
@@ -457,7 +457,7 @@ class V8_NODISCARD ThreadNotInWasmScope {
 #endif
 };
 
-inline uint8_t* EffectiveAddress(WasmInstanceObject instance,
+inline uint8_t* EffectiveAddress(Tagged<WasmInstanceObject> instance,
                                  uint32_t mem_index, uintptr_t index) {
   return instance->memory_base(mem_index) + index;
 }
@@ -477,7 +477,7 @@ int32_t memory_init_wrapper(Address data) {
   ThreadNotInWasmScope thread_not_in_wasm_scope;
   DisallowGarbageCollection no_gc;
   size_t offset = 0;
-  WasmInstanceObject instance =
+  Tagged<WasmInstanceObject> instance =
       WasmInstanceObject::cast(ReadAndIncrementOffset<Object>(data, &offset));
   uint32_t mem_index = ReadAndIncrementOffset<uint32_t>(data, &offset);
   uintptr_t dst = ReadAndIncrementOffset<uintptr_t>(data, &offset);
@@ -502,7 +502,7 @@ int32_t memory_copy_wrapper(Address data) {
   ThreadNotInWasmScope thread_not_in_wasm_scope;
   DisallowGarbageCollection no_gc;
   size_t offset = 0;
-  WasmInstanceObject instance =
+  Tagged<WasmInstanceObject> instance =
       WasmInstanceObject::cast(ReadAndIncrementOffset<Object>(data, &offset));
   uint32_t dst_mem_index = ReadAndIncrementOffset<uint32_t>(data, &offset);
   uint32_t src_mem_index = ReadAndIncrementOffset<uint32_t>(data, &offset);
@@ -526,7 +526,7 @@ int32_t memory_fill_wrapper(Address data) {
   DisallowGarbageCollection no_gc;
 
   size_t offset = 0;
-  WasmInstanceObject instance =
+  Tagged<WasmInstanceObject> instance =
       WasmInstanceObject::cast(ReadAndIncrementOffset<Object>(data, &offset));
   uint32_t mem_index = ReadAndIncrementOffset<uint32_t>(data, &offset);
   uintptr_t dst = ReadAndIncrementOffset<uintptr_t>(data, &offset);
@@ -547,7 +547,7 @@ inline void* ArrayElementAddress(Address array, uint32_t index,
   return reinterpret_cast<void*>(array + WasmArray::kHeaderSize -
                                  kHeapObjectTag + index * element_size_bytes);
 }
-inline void* ArrayElementAddress(WasmArray array, uint32_t index,
+inline void* ArrayElementAddress(Tagged<WasmArray> array, uint32_t index,
                                  int element_size_bytes) {
   return ArrayElementAddress(array.ptr(), index, element_size_bytes);
 }
@@ -559,8 +559,8 @@ void array_copy_wrapper(Address raw_instance, Address raw_dst_array,
   DCHECK_GT(length, 0);
   ThreadNotInWasmScope thread_not_in_wasm_scope;
   DisallowGarbageCollection no_gc;
-  WasmArray dst_array = WasmArray::cast(Object(raw_dst_array));
-  WasmArray src_array = WasmArray::cast(Object(raw_src_array));
+  Tagged<WasmArray> dst_array = WasmArray::cast(Object(raw_dst_array));
+  Tagged<WasmArray> src_array = WasmArray::cast(Object(raw_src_array));
 
   bool overlapping_ranges =
       dst_array.ptr() == src_array.ptr() &&
@@ -568,7 +568,7 @@ void array_copy_wrapper(Address raw_instance, Address raw_dst_array,
                              : src_index + length > dst_index);
   wasm::ValueType element_type = src_array->type()->element_type();
   if (element_type.is_reference()) {
-    WasmInstanceObject instance =
+    Tagged<WasmInstanceObject> instance =
         WasmInstanceObject::cast(Object(raw_instance));
     Isolate* isolate = instance->GetIsolate();
     ObjectSlot dst_slot = dst_array->ElementSlot(dst_index);
@@ -671,7 +671,7 @@ void array_fill_wrapper(Address raw_array, uint32_t index, uint32_t length,
 
   if (emit_write_barrier) {
     DCHECK(type.is_reference());
-    WasmArray array = WasmArray::cast(Object(raw_array));
+    Tagged<WasmArray> array = WasmArray::cast(Object(raw_array));
     Isolate* isolate = array->GetIsolate();
     ObjectSlot start(reinterpret_cast<Address>(initial_element_address));
     ObjectSlot end(
@@ -681,7 +681,7 @@ void array_fill_wrapper(Address raw_array, uint32_t index, uint32_t length,
 }
 
 double flat_string_to_f64(Address string_address) {
-  String s = String::cast(Object(string_address));
+  Tagged<String> s = String::cast(Object(string_address));
   return FlatStringToDouble(s, ALLOW_TRAILING_JUNK,
                             std::numeric_limits<double>::quiet_NaN());
 }
diff --git a/src/wasm/wasm-objects-inl.h b/src/wasm/wasm-objects-inl.h
index 0214a536fa2..57925fa4aa2 100644
--- a/src/wasm/wasm-objects-inl.h
+++ b/src/wasm/wasm-objects-inl.h
@@ -60,12 +60,13 @@ TQ_OBJECT_CONSTRUCTORS_IMPL(WasmNull)
 
 CAST_ACCESSOR(WasmInstanceObject)
 
-#define OPTIONAL_ACCESSORS(holder, name, type, offset)                  \
-  DEF_GETTER(holder, has_##name, bool) {                                \
-    Object value = TaggedField<Object, offset>::load(cage_base, *this); \
-    return !IsUndefined(value, GetReadOnlyRoots(cage_base));            \
-  }                                                                     \
-  ACCESSORS_CHECKED2(holder, name, Tagged<type>, offset,                \
+#define OPTIONAL_ACCESSORS(holder, name, type, offset)       \
+  DEF_GETTER(holder, has_##name, bool) {                     \
+    Tagged<Object> value =                                   \
+        TaggedField<Object, offset>::load(cage_base, *this); \
+    return !IsUndefined(value, GetReadOnlyRoots(cage_base)); \
+  }                                                          \
+  ACCESSORS_CHECKED2(holder, name, type, offset,             \
                      !IsUndefined(value, GetReadOnlyRoots(cage_base)), true)
 
 #define PRIMITIVE_ACCESSORS(holder, name, type, offset)               \
@@ -107,7 +108,8 @@ bool WasmModuleObject::is_asm_js() {
 }
 
 // WasmMemoryObject
-OPTIONAL_ACCESSORS(WasmMemoryObject, instances, WeakArrayList, kInstancesOffset)
+OPTIONAL_ACCESSORS(WasmMemoryObject, instances, Tagged<WeakArrayList>,
+                   kInstancesOffset)
 
 // WasmGlobalObject
 ACCESSORS(WasmGlobalObject, untagged_buffer, Tagged<JSArrayBuffer>,
@@ -234,20 +236,22 @@ ACCESSORS(WasmInstanceObject, native_context, Tagged<Context>,
           kNativeContextOffset)
 ACCESSORS(WasmInstanceObject, memory_objects, Tagged<FixedArray>,
           kMemoryObjectsOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, untagged_globals_buffer, JSArrayBuffer,
-                   kUntaggedGlobalsBufferOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, tagged_globals_buffer, FixedArray,
-                   kTaggedGlobalsBufferOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, untagged_globals_buffer,
+                   Tagged<JSArrayBuffer>, kUntaggedGlobalsBufferOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, tagged_globals_buffer,
+                   Tagged<FixedArray>, kTaggedGlobalsBufferOffset)
 OPTIONAL_ACCESSORS(WasmInstanceObject, imported_mutable_globals_buffers,
-                   FixedArray, kImportedMutableGlobalsBuffersOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, tables, FixedArray, kTablesOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, indirect_function_tables, FixedArray,
-                   kIndirectFunctionTablesOffset)
+                   Tagged<FixedArray>, kImportedMutableGlobalsBuffersOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, tables, Tagged<FixedArray>,
+                   kTablesOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, indirect_function_tables,
+                   Tagged<FixedArray>, kIndirectFunctionTablesOffset)
 ACCESSORS(WasmInstanceObject, imported_function_refs, Tagged<FixedArray>,
           kImportedFunctionRefsOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, indirect_function_table_refs, FixedArray,
-                   kIndirectFunctionTableRefsOffset)
-OPTIONAL_ACCESSORS(WasmInstanceObject, tags_table, FixedArray, kTagsTableOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, indirect_function_table_refs,
+                   Tagged<FixedArray>, kIndirectFunctionTableRefsOffset)
+OPTIONAL_ACCESSORS(WasmInstanceObject, tags_table, Tagged<FixedArray>,
+                   kTagsTableOffset)
 ACCESSORS(WasmInstanceObject, wasm_internal_functions, Tagged<FixedArray>,
           kWasmInternalFunctionsOffset)
 ACCESSORS(WasmInstanceObject, managed_object_maps, Tagged<FixedArray>,
diff --git a/src/wasm/wasm-objects.h b/src/wasm/wasm-objects.h
index ba01c79b48c..63e9fae1e9e 100644
--- a/src/wasm/wasm-objects.h
+++ b/src/wasm/wasm-objects.h
@@ -62,7 +62,7 @@ class Managed;
 
 #define DECL_OPTIONAL_ACCESSORS(name, type) \
   DECL_GETTER(has_##name, bool)             \
-  DECL_ACCESSORS(name, Tagged<type>)
+  DECL_ACCESSORS(name, type)
 
 class V8_EXPORT_PRIVATE FunctionTargetAndRef {
  public:
@@ -261,7 +261,7 @@ class WasmTableObject
 class WasmMemoryObject
     : public TorqueGeneratedWasmMemoryObject<WasmMemoryObject, JSObject> {
  public:
-  DECL_OPTIONAL_ACCESSORS(instances, WeakArrayList)
+  DECL_OPTIONAL_ACCESSORS(instances, Tagged<WeakArrayList>)
 
   // Add a use of this memory object to the given instance. This updates the
   // internal weak list of instances that use this memory and also updates the
@@ -346,18 +346,18 @@ class V8_EXPORT_PRIVATE WasmInstanceObject : public JSObject {
   DECL_ACCESSORS(exports_object, Tagged<JSObject>)
   DECL_ACCESSORS(native_context, Tagged<Context>)
   DECL_ACCESSORS(memory_objects, Tagged<FixedArray>)
-  DECL_OPTIONAL_ACCESSORS(untagged_globals_buffer, JSArrayBuffer)
-  DECL_OPTIONAL_ACCESSORS(tagged_globals_buffer, FixedArray)
-  DECL_OPTIONAL_ACCESSORS(imported_mutable_globals_buffers, FixedArray)
-  DECL_OPTIONAL_ACCESSORS(tables, FixedArray)
-  DECL_OPTIONAL_ACCESSORS(indirect_function_tables, FixedArray)
+  DECL_OPTIONAL_ACCESSORS(untagged_globals_buffer, Tagged<JSArrayBuffer>)
+  DECL_OPTIONAL_ACCESSORS(tagged_globals_buffer, Tagged<FixedArray>)
+  DECL_OPTIONAL_ACCESSORS(imported_mutable_globals_buffers, Tagged<FixedArray>)
+  DECL_OPTIONAL_ACCESSORS(tables, Tagged<FixedArray>)
+  DECL_OPTIONAL_ACCESSORS(indirect_function_tables, Tagged<FixedArray>)
   DECL_ACCESSORS(imported_function_refs, Tagged<FixedArray>)
   DECL_ACCESSORS(imported_mutable_globals, Tagged<FixedAddressArray>)
   DECL_ACCESSORS(imported_function_targets, Tagged<FixedAddressArray>)
-  DECL_OPTIONAL_ACCESSORS(indirect_function_table_refs, FixedArray)
+  DECL_OPTIONAL_ACCESSORS(indirect_function_table_refs, Tagged<FixedArray>)
   DECL_ACCESSORS(indirect_function_table_sig_ids, Tagged<FixedUInt32Array>)
   DECL_ACCESSORS(indirect_function_table_targets, Tagged<ExternalPointerArray>)
-  DECL_OPTIONAL_ACCESSORS(tags_table, FixedArray)
+  DECL_OPTIONAL_ACCESSORS(tags_table, Tagged<FixedArray>)
   DECL_ACCESSORS(wasm_internal_functions, Tagged<FixedArray>)
   DECL_ACCESSORS(managed_object_maps, Tagged<FixedArray>)
   DECL_ACCESSORS(feedback_vectors, Tagged<FixedArray>)
diff --git a/test/cctest/compiler/test-run-load-store.cc b/test/cctest/compiler/test-run-load-store.cc
index 78b9d3c0cd1..e127dab88c1 100644
--- a/test/cctest/compiler/test-run-load-store.cc
+++ b/test/cctest/compiler/test-run-load-store.cc
@@ -239,7 +239,7 @@ void InitBuffer(CType* buffer, size_t length, MachineType type) {
   // Tagged field loads require values to be properly tagged because of
   // pointer decompression that may be happenning during load.
   Isolate* isolate = CcTest::InitIsolateOnce();
-  Smi* smi_view = reinterpret_cast<Smi*>(&buffer[0]);
+  Tagged<Smi>* smi_view = reinterpret_cast<Tagged<Smi>*>(&buffer[0]);
   if (type.IsTaggedSigned()) {
     for (size_t i = 0; i < length; i++) {
       smi_view[i] = Smi::FromInt(static_cast<int>(i + kBufferSize) ^ 0xABCDEF0);
diff --git a/test/cctest/heap/test-concurrent-allocation.cc b/test/cctest/heap/test-concurrent-allocation.cc
index c46814e15ca..6916b09ef56 100644
--- a/test/cctest/heap/test-concurrent-allocation.cc
+++ b/test/cctest/heap/test-concurrent-allocation.cc
@@ -432,8 +432,8 @@ class ConcurrentWriteBarrierThread final : public v8::base::Thread {
   }
 
   Heap* heap_;
-  FixedArray fixed_array_;
-  HeapObject value_;
+  Tagged<FixedArray> fixed_array_;
+  Tagged<HeapObject> value_;
 };
 
 UNINITIALIZED_TEST(ConcurrentWriteBarrier) {
@@ -505,8 +505,8 @@ class ConcurrentRecordRelocSlotThread final : public v8::base::Thread {
   }
 
   Heap* heap_;
-  Code code_;
-  HeapObject value_;
+  Tagged<Code> code_;
+  Tagged<HeapObject> value_;
 };
 
 UNINITIALIZED_TEST(ConcurrentRecordRelocSlot) {
diff --git a/test/cctest/heap/test-heap.cc b/test/cctest/heap/test-heap.cc
index bcf0106da2c..a61eae7205c 100644
--- a/test/cctest/heap/test-heap.cc
+++ b/test/cctest/heap/test-heap.cc
@@ -90,7 +90,7 @@ namespace heap {
 static const int kPretenureCreationCount =
     PretenuringHandler::GetMinMementoCountForTesting() + 1;
 
-static void CheckMap(Map map, int type, int instance_size) {
+static void CheckMap(Tagged<Map> map, int type, int instance_size) {
   CHECK(IsHeapObject(map));
   DCHECK(IsValidHeapObject(CcTest::heap(), map));
   CHECK_EQ(ReadOnlyRoots(CcTest::heap()).meta_map(), map->map());
@@ -98,7 +98,6 @@ static void CheckMap(Map map, int type, int instance_size) {
   CHECK_EQ(instance_size, map->instance_size());
 }
 
-
 TEST(HeapMaps) {
   CcTest::InitializeVM();
   ReadOnlyRoots roots(CcTest::heap());
@@ -166,16 +165,19 @@ TEST(InitialObjects) {
            *v8::Utils::OpenHandle(*CompileRun("Object.prototype")));
 }
 
-static void CheckOddball(Isolate* isolate, Object obj, const char* string) {
+static void CheckOddball(Isolate* isolate, Tagged<Object> obj,
+                         const char* string) {
   CHECK(IsOddball(obj));
   Handle<Object> handle(obj, isolate);
-  Object print_string = *Object::ToString(isolate, handle).ToHandleChecked();
+  Tagged<Object> print_string =
+      *Object::ToString(isolate, handle).ToHandleChecked();
   CHECK(String::cast(print_string)->IsOneByteEqualTo(base::CStrVector(string)));
 }
 
 static void CheckSmi(Isolate* isolate, int value, const char* string) {
   Handle<Object> handle(Smi::FromInt(value), isolate);
-  Object print_string = *Object::ToString(isolate, handle).ToHandleChecked();
+  Tagged<Object> print_string =
+      *Object::ToString(isolate, handle).ToHandleChecked();
   CHECK(String::cast(print_string)->IsOneByteEqualTo(base::CStrVector(string)));
 }
 
@@ -224,11 +226,12 @@ static void CheckGcSafeFindCodeForInnerPointer(Isolate* isolate) {
       isolate);
   CHECK(IsInstructionStream(*code, cage_base));
 
-  HeapObject obj = HeapObject::cast(*code);
+  Tagged<HeapObject> obj = HeapObject::cast(*code);
   Address obj_addr = obj.address();
 
   for (int i = 0; i < obj->Size(cage_base); i += kTaggedSize) {
-    Code lookup_result = isolate->heap()->FindCodeForInnerPointer(obj_addr + i);
+    Tagged<Code> lookup_result =
+        isolate->heap()->FindCodeForInnerPointer(obj_addr + i);
     CHECK_EQ(*code, lookup_result->instruction_stream());
   }
 
@@ -237,8 +240,8 @@ static void CheckGcSafeFindCodeForInnerPointer(Isolate* isolate) {
           .Build()
           ->instruction_stream(),
       isolate);
-  HeapObject obj_copy = HeapObject::cast(*copy);
-  Code not_right = isolate->heap()->FindCodeForInnerPointer(
+  Tagged<HeapObject> obj_copy = HeapObject::cast(*copy);
+  Tagged<Code> not_right = isolate->heap()->FindCodeForInnerPointer(
       obj_copy.address() + obj_copy->Size(cage_base) / 2);
   CHECK_NE(not_right->instruction_stream(), *code);
   CHECK_EQ(not_right->instruction_stream(), *copy);
@@ -655,7 +658,7 @@ TEST(BytecodeArray) {
     CHECK_EQ(array->get(i), kRawBytes[i]);
   }
 
-  FixedArray old_constant_pool_address = *constant_pool;
+  Tagged<FixedArray> old_constant_pool_address = *constant_pool;
 
   // Perform a full garbage collection and force the constant pool to be on an
   // evacuation candidate.
@@ -1015,7 +1018,7 @@ static int ObjectsFoundInHeap(Heap* heap, Handle<Object> objs[], int size) {
   // Count the number of objects found in the heap.
   int found_count = 0;
   HeapObjectIterator iterator(heap);
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     for (int i = 0; i < size; i++) {
       // V8_EXTERNAL_CODE_SPACE specific: we might be comparing
@@ -1844,7 +1847,7 @@ static void OptimizeEmptyFunction(const char* name) {
 // Count the number of native contexts in the weak list of native contexts.
 int CountNativeContexts() {
   int count = 0;
-  Object object = CcTest::heap()->native_contexts_list();
+  Tagged<Object> object = CcTest::heap()->native_contexts_list();
   while (!IsUndefined(object, CcTest::i_isolate())) {
     count++;
     object = Context::cast(object)->next_context_link();
@@ -2088,12 +2091,12 @@ TEST(TestAlignmentCalculations) {
   CHECK_EQ(0, fill);
 }
 
-static HeapObject NewSpaceAllocateAligned(int size,
-                                          AllocationAlignment alignment) {
+static Tagged<HeapObject> NewSpaceAllocateAligned(
+    int size, AllocationAlignment alignment) {
   Heap* heap = CcTest::heap();
   AllocationResult allocation =
       heap->new_space()->AllocateRawAligned(size, alignment);
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   allocation.To(&obj);
   heap->CreateFillerObjectAt(obj.address(), size);
   return obj;
@@ -2118,8 +2121,8 @@ TEST(TestAlignedAllocation) {
   const intptr_t double_misalignment = kDoubleSize - kTaggedSize;
   Address* top_addr = CcTest::heap()->new_space()->allocation_top_address();
   Address start;
-  HeapObject obj;
-  HeapObject filler;
+  Tagged<HeapObject> obj;
+  Tagged<HeapObject> filler;
   if (double_misalignment) {
     if (v8_flags.minor_ms) {
       // Make one allocation to force allocating an allocation area. Using
@@ -2161,12 +2164,12 @@ TEST(TestAlignedAllocation) {
   }
 }
 
-static HeapObject OldSpaceAllocateAligned(int size,
-                                          AllocationAlignment alignment) {
+static Tagged<HeapObject> OldSpaceAllocateAligned(
+    int size, AllocationAlignment alignment) {
   Heap* heap = CcTest::heap();
   AllocationResult allocation =
       heap->old_space()->AllocateRawAligned(size, alignment);
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   allocation.To(&obj);
   heap->CreateFillerObjectAt(obj.address(), size);
   return obj;
@@ -2205,8 +2208,8 @@ TEST(TestAlignedOverAllocation) {
   // is enabled, 0 on 64-bit ones when pointer compression is disabled.
   const intptr_t double_misalignment = kDoubleSize - kTaggedSize;
   Address start;
-  HeapObject obj;
-  HeapObject filler;
+  Tagged<HeapObject> obj;
+  Tagged<HeapObject> filler;
   if (double_misalignment) {
     start = AlignOldSpace(kDoubleAligned, 0);
     obj = OldSpaceAllocateAligned(kTaggedSize, kDoubleAligned);
@@ -2280,7 +2283,7 @@ TEST(TestSizeOfObjectsVsHeapObjectIteratorPrecision) {
   PtrComprCageBase cage_base(CcTest::i_isolate());
   intptr_t size_of_objects_1 = CcTest::heap()->SizeOfObjects();
   intptr_t size_of_objects_2 = 0;
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (!IsFreeSpace(obj, cage_base)) {
       size_of_objects_2 += obj->Size(cage_base);
@@ -2314,7 +2317,7 @@ TEST(TestSizeOfObjectsVsHeapObjectIteratorPrecision) {
 static int NumberOfGlobalObjects() {
   int count = 0;
   HeapObjectIterator iterator(CcTest::heap());
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsJSGlobalObject(obj)) count++;
   }
@@ -2854,7 +2857,7 @@ TEST(OptimizedPretenuringMixedInObjectProperties) {
   CHECK(CcTest::heap()->InOldSpace(o->RawFastPropertyAt(idx1)));
   CHECK(CcTest::heap()->InOldSpace(o->RawFastPropertyAt(idx2)));
 
-  JSObject inner_object = JSObject::cast(o->RawFastPropertyAt(idx1));
+  Tagged<JSObject> inner_object = JSObject::cast(o->RawFastPropertyAt(idx1));
   CHECK(CcTest::heap()->InOldSpace(inner_object));
   CHECK(CcTest::heap()->InOldSpace(inner_object->RawFastPropertyAt(idx1)));
   CHECK(CcTest::heap()->InOldSpace(inner_object->RawFastPropertyAt(idx2)));
@@ -3120,11 +3123,10 @@ TEST(OptimizedAllocationArrayLiterals) {
   CHECK(InCorrectGeneration(o->elements()));
 }
 
-static int CountMapTransitions(i::Isolate* isolate, Map map) {
+static int CountMapTransitions(i::Isolate* isolate, Tagged<Map> map) {
   return TransitionsAccessor(isolate, map).NumberOfTransitions();
 }
 
-
 // Test that map transitions are cleared and maps are collected with
 // incremental marking as well.
 TEST(Regress1465) {
@@ -3747,7 +3749,8 @@ void DetailedErrorStackTraceTest(const char* src,
       Handle<JSReceiver>::cast(exception)));
 }
 
-FixedArray ParametersOf(Handle<FixedArray> stack_trace, int frame_index) {
+Tagged<FixedArray> ParametersOf(Handle<FixedArray> stack_trace,
+                                int frame_index) {
   return CallSiteInfo::cast(stack_trace->get(frame_index))->parameters();
 }
 
@@ -3769,12 +3772,12 @@ TEST(DetailedErrorStackTrace) {
       "main(foo);                   ";
 
   DetailedErrorStackTraceTest(source, [](Handle<FixedArray> stack_trace) {
-    FixedArray foo_parameters = ParametersOf(stack_trace, 0);
+    Tagged<FixedArray> foo_parameters = ParametersOf(stack_trace, 0);
     CHECK_EQ(foo_parameters->length(), 1);
     CHECK(IsSmi(foo_parameters->get(0)));
     CHECK_EQ(Smi::ToInt(foo_parameters->get(0)), 42);
 
-    FixedArray bar_parameters = ParametersOf(stack_trace, 1);
+    Tagged<FixedArray> bar_parameters = ParametersOf(stack_trace, 1);
     CHECK_EQ(bar_parameters->length(), 2);
     CHECK(IsJSObject(bar_parameters->get(0)));
     CHECK(IsBoolean(bar_parameters->get(1)));
@@ -3782,7 +3785,7 @@ TEST(DetailedErrorStackTrace) {
     CHECK_EQ(bar_parameters->get(0), *foo);
     CHECK(!Object::BooleanValue(bar_parameters->get(1), CcTest::i_isolate()));
 
-    FixedArray main_parameters = ParametersOf(stack_trace, 2);
+    Tagged<FixedArray> main_parameters = ParametersOf(stack_trace, 2);
     CHECK_EQ(main_parameters->length(), 2);
     CHECK(IsJSObject(main_parameters->get(0)));
     CHECK(IsUndefined(main_parameters->get(1)));
@@ -3810,12 +3813,12 @@ TEST(DetailedErrorStackTraceInline) {
       "foo(41);                              ";
 
   DetailedErrorStackTraceTest(source, [](Handle<FixedArray> stack_trace) {
-    FixedArray parameters_add = ParametersOf(stack_trace, 0);
+    Tagged<FixedArray> parameters_add = ParametersOf(stack_trace, 0);
     CHECK_EQ(parameters_add->length(), 1);
     CHECK(IsSmi(parameters_add->get(0)));
     CHECK_EQ(Smi::ToInt(parameters_add->get(0)), 42);
 
-    FixedArray parameters_foo = ParametersOf(stack_trace, 1);
+    Tagged<FixedArray> parameters_foo = ParametersOf(stack_trace, 1);
     CHECK_EQ(parameters_foo->length(), 1);
     CHECK(IsSmi(parameters_foo->get(0)));
     CHECK_EQ(Smi::ToInt(parameters_foo->get(0)), 41);
@@ -3831,7 +3834,7 @@ TEST(DetailedErrorStackTraceBuiltinExit) {
       "test(9999);                     ";
 
   DetailedErrorStackTraceTest(source, [](Handle<FixedArray> stack_trace) {
-    FixedArray parameters = ParametersOf(stack_trace, 0);
+    Tagged<FixedArray> parameters = ParametersOf(stack_trace, 0);
 
     CHECK_EQ(parameters->length(), 2);
     CHECK(IsSmi(parameters->get(1)));
@@ -3901,7 +3904,7 @@ TEST(Regress169928) {
 
   // We need filler the size of AllocationMemento object, plus an extra
   // fill pointer value.
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   AllocationResult allocation = CcTest::heap()->new_space()->AllocateRaw(
       AllocationMemento::kSize + kTaggedSize, kTaggedAligned);
   CHECK(allocation.To(&obj));
@@ -3939,7 +3942,7 @@ TEST(LargeObjectSlotRecording) {
       isolate->factory()->NewFixedArray(4, AllocationType::kOld);
   Page* evac_page = Page::FromHeapObject(*lit);
   heap::ForceEvacuationCandidate(evac_page);
-  FixedArray old_location = *lit;
+  Tagged<FixedArray> old_location = *lit;
 
   // Allocate a large object.
   int size = std::max(1000000, kMaxRegularHeapObjectSize + KB);
@@ -4108,8 +4111,9 @@ TEST(DisableInlineAllocation) {
 
 static int AllocationSitesCount(Heap* heap) {
   int count = 0;
-  for (Object site = heap->allocation_sites_list(); IsAllocationSite(site);) {
-    AllocationSite cur = AllocationSite::cast(site);
+  for (Tagged<Object> site = heap->allocation_sites_list();
+       IsAllocationSite(site);) {
+    Tagged<AllocationSite> cur = AllocationSite::cast(site);
     CHECK(cur->HasWeakNext());
     site = cur->weak_next();
     count++;
@@ -4119,11 +4123,12 @@ static int AllocationSitesCount(Heap* heap) {
 
 static int SlimAllocationSiteCount(Heap* heap) {
   int count = 0;
-  for (Object weak_list = heap->allocation_sites_list();
+  for (Tagged<Object> weak_list = heap->allocation_sites_list();
        IsAllocationSite(weak_list);) {
-    AllocationSite weak_cur = AllocationSite::cast(weak_list);
-    for (Object site = weak_cur->nested_site(); IsAllocationSite(site);) {
-      AllocationSite cur = AllocationSite::cast(site);
+    Tagged<AllocationSite> weak_cur = AllocationSite::cast(weak_list);
+    for (Tagged<Object> site = weak_cur->nested_site();
+         IsAllocationSite(site);) {
+      Tagged<AllocationSite> cur = AllocationSite::cast(site);
       CHECK(!cur->HasWeakNext());
       site = cur->nested_site();
       count++;
@@ -4176,14 +4181,15 @@ TEST(EnsureAllocationSiteDependentCodesProcessed) {
                 .ToLocalChecked())));
 
     // Expect a dependent code object for transitioning and pretenuring.
-    DependentCode dependency = site->dependent_code();
+    Tagged<DependentCode> dependency = site->dependent_code();
     CHECK_NE(dependency,
              DependentCode::empty_dependent_code(ReadOnlyRoots(isolate)));
     CHECK_EQ(dependency->length(), DependentCode::kSlotsPerEntry);
     MaybeObject code = dependency->Get(0 + DependentCode::kCodeSlotOffset);
     CHECK(code->IsWeak());
     CHECK_EQ(bar_handle->code(), Code::cast(code.GetHeapObjectAssumeWeak()));
-    Smi groups = dependency->Get(0 + DependentCode::kGroupsSlotOffset).ToSmi();
+    Tagged<Smi> groups =
+        dependency->Get(0 + DependentCode::kGroupsSlotOffset).ToSmi();
     CHECK_EQ(static_cast<DependentCode::DependencyGroups>(groups.value()),
              DependentCode::kAllocationSiteTransitionChangedGroup |
                  DependentCode::kAllocationSiteTenuringChangedGroup);
@@ -4814,7 +4820,7 @@ Handle<JSFunction> GetFunctionByName(Isolate* isolate, const char* name) {
 
 void CheckIC(Handle<JSFunction> function, int slot_index,
              InlineCacheState state) {
-  FeedbackVector vector = function->feedback_vector();
+  Tagged<FeedbackVector> vector = function->feedback_vector();
   FeedbackSlot slot(slot_index);
   FeedbackNexus nexus(vector, slot);
   CHECK_EQ(nexus.ic_state(), state);
@@ -5048,7 +5054,7 @@ TEST(Regress507979) {
   // way the filler object shares the mark bits with the following live object.
   o1->Shrink(isolate, kFixedArrayLen - 1);
 
-  for (HeapObject obj = it.Next(); !obj.is_null(); obj = it.Next()) {
+  for (Tagged<HeapObject> obj = it.Next(); !obj.is_null(); obj = it.Next()) {
     // Let's not optimize the loop away.
     CHECK_NE(obj.address(), kNullAddress);
   }
@@ -5405,7 +5411,7 @@ TEST(OldSpaceAllocationCounter) {
 static void CheckLeak(const v8::FunctionCallbackInfo<v8::Value>& info) {
   CHECK(i::ValidateCallbackInfo(info));
   Isolate* isolate = CcTest::i_isolate();
-  Object message(
+  Tagged<Object> message(
       *reinterpret_cast<Address*>(isolate->pending_message_address()));
   CHECK(IsTheHole(message, isolate));
 }
@@ -5501,14 +5507,14 @@ TEST(ScriptIterator) {
   int script_count = 0;
   {
     HeapObjectIterator it(heap);
-    for (HeapObject obj = it.Next(); !obj.is_null(); obj = it.Next()) {
+    for (Tagged<HeapObject> obj = it.Next(); !obj.is_null(); obj = it.Next()) {
       if (IsScript(obj)) script_count++;
     }
   }
 
   {
     Script::Iterator iterator(isolate);
-    for (Script script = iterator.Next(); !script.is_null();
+    for (Tagged<Script> script = iterator.Next(); !script.is_null();
          script = iterator.Next()) {
       script_count--;
     }
@@ -5523,7 +5529,7 @@ AllocationResult HeapTester::AllocateByteArrayForTest(
     Heap* heap, int length, AllocationType allocation_type) {
   DCHECK(length >= 0 && length <= ByteArray::kMaxLength);
   int size = ByteArray::SizeFor(length);
-  HeapObject result;
+  Tagged<HeapObject> result;
   {
     AllocationResult allocation = heap->AllocateRaw(size, allocation_type);
     if (!allocation.To(&result)) return allocation;
@@ -5568,7 +5574,7 @@ HEAP_TEST(Regress587004) {
   heap::SimulateFullSpace(heap->old_space());
   heap->RightTrimFixedArray(*array, N - 1);
   heap->EnsureSweepingCompleted(Heap::SweepingForcedFinalizationMode::kV8Only);
-  ByteArray byte_array;
+  Tagged<ByteArray> byte_array;
   const int M = 256;
   // Don't allow old space expansion. The test works without this flag too,
   // but becomes very slow.
@@ -5602,7 +5608,7 @@ HEAP_TEST(Regress589413) {
   Factory* factory = isolate->factory();
   // Fill the new space with byte arrays with elements looking like pointers.
   const int M = 256;
-  ByteArray byte_array;
+  Tagged<ByteArray> byte_array;
   Page* young_page = nullptr;
   while (AllocateByteArrayForTest(heap, M, AllocationType::kYoung)
              .To(&byte_array)) {
@@ -5634,9 +5640,9 @@ HEAP_TEST(Regress589413) {
     // This number is close to large free list category threshold.
     const int N = 0x3EEE;
 
-    std::vector<FixedArray> arrays;
+    std::vector<Tagged<FixedArray>> arrays;
     std::set<Page*> pages;
-    FixedArray array;
+    Tagged<FixedArray> array;
     // Fill all pages with fixed arrays.
     heap->set_force_oom(true);
     while (
@@ -5723,7 +5729,7 @@ TEST(Regress598319) {
                         Utils::ToLocal(Handle<Object>::cast(root)));
     }
 
-    FixedArray get() { return FixedArray::cast(root->get(0)); }
+    Tagged<FixedArray> get() { return FixedArray::cast(root->get(0)); }
 
     Handle<FixedArray> root;
 
@@ -5749,7 +5755,7 @@ TEST(Regress598319) {
   MarkingState* marking_state = heap->marking_state();
   CHECK(marking_state->IsUnmarked(arr.get()));
   for (int i = 0; i < arr.get()->length(); i++) {
-    HeapObject arr_value = HeapObject::cast(arr.get()->get(i));
+    Tagged<HeapObject> arr_value = HeapObject::cast(arr.get()->get(i));
     CHECK(marking_state->IsUnmarked(arr_value));
   }
 
@@ -5763,7 +5769,7 @@ TEST(Regress598319) {
 
   // Check that we have not marked the interesting array during root scanning.
   for (int i = 0; i < arr.get()->length(); i++) {
-    HeapObject arr_value = HeapObject::cast(arr.get()->get(i));
+    Tagged<HeapObject> arr_value = HeapObject::cast(arr.get()->get(i));
     CHECK(marking_state->IsUnmarked(arr_value));
   }
 
@@ -5803,7 +5809,7 @@ TEST(Regress598319) {
   // All objects need to be black after marking. If a white object crossed the
   // progress bar, we would fail here.
   for (int i = 0; i < arr.get()->length(); i++) {
-    HeapObject arr_value = HeapObject::cast(arr.get()->get(i));
+    Tagged<HeapObject> arr_value = HeapObject::cast(arr.get()->get(i));
     CHECK(arr_value.InReadOnlySpace() || marking_state->IsMarked(arr_value));
   }
 }
@@ -5997,7 +6003,7 @@ TEST(ContinuousRightTrimFixedArrayInBlackArea) {
   Address previous = end_address - kTaggedSize;
   isolate->heap()->RightTrimFixedArray(*array, 1);
 
-  HeapObject filler = HeapObject::FromAddress(previous);
+  Tagged<HeapObject> filler = HeapObject::FromAddress(previous);
   CHECK(IsFreeSpaceOrFiller(filler));
 
   // Trim 10 times by one, two, and three word.
@@ -6144,7 +6150,7 @@ TEST(UncommitUnusedLargeObjectMemory) {
 }
 
 template <RememberedSetType direction>
-static size_t GetRememberedSetSize(HeapObject obj) {
+static size_t GetRememberedSetSize(Tagged<HeapObject> obj) {
   size_t count = 0;
   auto chunk = MemoryChunk::FromHeapObject(obj);
   RememberedSet<direction>::Iterate(
@@ -7004,7 +7010,7 @@ TEST(Regress10698) {
   // Step 3. Allocate another byte array. It will be black.
   factory->NewByteArray(kTaggedSize, AllocationType::kOld);
   Address address = reinterpret_cast<Address>(array->GetDataStartAddress());
-  HeapObject filler = HeapObject::FromAddress(address);
+  Tagged<HeapObject> filler = HeapObject::FromAddress(address);
   // Step 4. Set the filler at the end of the first array.
   // It will have an impossible markbit pattern because the second markbit
   // will be taken from the second array.
@@ -7035,7 +7041,7 @@ HEAP_TEST(CodeLargeObjectSpace) {
   TestAllocationTracker allocation_tracker{size_in_bytes};
   heap->AddHeapObjectAllocationTracker(&allocation_tracker);
 
-  HeapObject obj;
+  Tagged<HeapObject> obj;
   {
     AllocationResult allocation = heap->AllocateRaw(
         size_in_bytes, AllocationType::kCode, AllocationOrigin::kRuntime);
@@ -7070,7 +7076,7 @@ UNINITIALIZED_HEAP_TEST(CodeLargeObjectSpace64k) {
     TestAllocationTracker allocation_tracker{size_in_bytes};
     heap->AddHeapObjectAllocationTracker(&allocation_tracker);
 
-    HeapObject obj;
+    Tagged<HeapObject> obj;
     {
       AllocationResult allocation = heap->AllocateRaw(
           size_in_bytes, AllocationType::kCode, AllocationOrigin::kRuntime);
@@ -7093,7 +7099,7 @@ UNINITIALIZED_HEAP_TEST(CodeLargeObjectSpace64k) {
     TestAllocationTracker allocation_tracker{size_in_bytes};
     heap->AddHeapObjectAllocationTracker(&allocation_tracker);
 
-    HeapObject obj;
+    Tagged<HeapObject> obj;
     {
       AllocationResult allocation = heap->AllocateRaw(
           size_in_bytes, AllocationType::kCode, AllocationOrigin::kRuntime);
diff --git a/test/cctest/test-accessors.cc b/test/cctest/test-accessors.cc
index 5a249d66c22..e7e2cd348d6 100644
--- a/test/cctest/test-accessors.cc
+++ b/test/cctest/test-accessors.cc
@@ -534,7 +534,7 @@ static void StackCheck(Local<String> name,
   for (int i = 0; !iter.done(); i++) {
     i::StackFrame* frame = iter.frame();
     CHECK(i != 0 || (frame->type() == i::StackFrame::EXIT));
-    i::Code code = frame->LookupCode();
+    i::Tagged<i::Code> code = frame->LookupCode();
     CHECK(code->contains(isolate, frame->pc()));
     iter.Advance();
   }
diff --git a/test/cctest/test-api.cc b/test/cctest/test-api.cc
index adb4b4277fb..ac3140d9a17 100644
--- a/test/cctest/test-api.cc
+++ b/test/cctest/test-api.cc
@@ -3262,7 +3262,7 @@ void GlobalProxyIdentityHash(bool set_in_js) {
   int32_t hash1;
   if (set_in_js) {
     CompileRun("var m = new Set(); m.add(global);");
-    i::Object original_hash = i::Object::GetHash(*i_global_proxy);
+    i::Tagged<i::Object> original_hash = i::Object::GetHash(*i_global_proxy);
     CHECK(IsSmi(original_hash));
     hash1 = i::Smi::ToInt(original_hash);
   } else {
@@ -12657,7 +12657,7 @@ TEST(CallHandlerAsFunctionHasNoSideEffectNotSupported) {
   i::Tagged<i::FunctionTemplateInfo> cons = i::FunctionTemplateInfo::cast(
       v8::Utils::OpenHandle(*templ)->constructor());
   i::Heap* heap = reinterpret_cast<i::Isolate*>(isolate)->heap();
-  i::CallHandlerInfo handler_info =
+  i::Tagged<i::CallHandlerInfo> handler_info =
       i::CallHandlerInfo::cast(cons->GetInstanceCallHandler());
   CHECK(!handler_info->IsSideEffectFreeCallHandlerInfo());
   handler_info->set_map(
@@ -13365,10 +13365,10 @@ THREADED_TEST(LockUnlockLock) {
 static int GetGlobalObjectsCount() {
   int count = 0;
   i::HeapObjectIterator it(CcTest::heap());
-  for (i::HeapObject object = it.Next(); !object.is_null();
+  for (i::Tagged<i::HeapObject> object = it.Next(); !object.is_null();
        object = it.Next()) {
     if (IsJSGlobalObject(object)) {
-      i::JSGlobalObject g = i::JSGlobalObject::cast(object);
+      i::Tagged<i::JSGlobalObject> g = i::JSGlobalObject::cast(object);
       // Skip dummy global object.
       if (g->global_dictionary(v8::kAcquireLoad)->NumberOfElements() != 0) {
         count++;
@@ -14848,7 +14848,7 @@ class UC16VectorResource : public v8::String::ExternalStringResource {
   v8::base::Vector<const v8::base::uc16> data_;
 };
 
-static void MorphAString(i::String string,
+static void MorphAString(i::Tagged<i::String> string,
                          OneByteVectorResource* one_byte_resource,
                          UC16VectorResource* uc16_resource) {
   i::Isolate* isolate = CcTest::i_isolate();
@@ -19532,8 +19532,9 @@ THREADED_TEST(ReadOnlyIndexedProperties) {
             .FromJust());
 }
 
-static int CountLiveMapsInMapCache(i::Context context) {
-  i::WeakFixedArray map_cache = i::WeakFixedArray::cast(context->map_cache());
+static int CountLiveMapsInMapCache(i::Tagged<i::Context> context) {
+  i::Tagged<i::WeakFixedArray> map_cache =
+      i::WeakFixedArray::cast(context->map_cache());
   int length = map_cache->length();
   int count = 0;
   for (int i = 0; i < length; i++) {
@@ -19542,7 +19543,6 @@ static int CountLiveMapsInMapCache(i::Context context) {
   return count;
 }
 
-
 THREADED_TEST(Regress1516) {
   LocalContext context;
   v8::HandleScope scope(context->GetIsolate());
diff --git a/test/cctest/test-assembler-loong64.cc b/test/cctest/test-assembler-loong64.cc
index 29e9ea79b1d..220f583f596 100644
--- a/test/cctest/test-assembler-loong64.cc
+++ b/test/cctest/test-assembler-loong64.cc
@@ -4203,7 +4203,7 @@ TEST(jump_tables3) {
     values[i] = isolate->factory()->NewHeapNumber<AllocationType::kOld>(value);
   }
   Label labels[kNumCases];
-  Object obj;
+  Tagged<Object> obj;
   int64_t imm64;
 
   __ addi_d(sp, sp, -8);
diff --git a/test/cctest/test-assembler-mips64.cc b/test/cctest/test-assembler-mips64.cc
index 88f3f9ff059..58a90bc248c 100644
--- a/test/cctest/test-assembler-mips64.cc
+++ b/test/cctest/test-assembler-mips64.cc
@@ -3370,7 +3370,7 @@ TEST(jump_tables3) {
     values[i] = isolate->factory()->NewHeapNumber<AllocationType::kOld>(value);
   }
   Label labels[kNumCases];
-  Object obj;
+  Tagged<Object> obj;
   int64_t imm64;
 
   __ daddiu(sp, sp, -8);
diff --git a/test/cctest/test-assembler-ppc.cc b/test/cctest/test-assembler-ppc.cc
index d8980c24615..5363405a36a 100644
--- a/test/cctest/test-assembler-ppc.cc
+++ b/test/cctest/test-assembler-ppc.cc
@@ -325,7 +325,7 @@ TEST(4) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -385,7 +385,7 @@ TEST(5) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -420,7 +420,7 @@ TEST(6) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -495,7 +495,7 @@ static void TestRoundingMode(VCVTTypes types,
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -682,7 +682,7 @@ TEST(8) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -797,7 +797,7 @@ TEST(9) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -908,7 +908,7 @@ TEST(10) {
 
     CodeDesc desc;
     assm.GetCode(isolate, &desc);
-    Object code = isolate->heap()->CreateCode(
+    Tagged<Object> code = isolate->heap()->CreateCode(
         desc,
         CodeKind::FOR_TESTING,
         Handle<Code>())->ToObjectChecked();
@@ -1005,7 +1005,7 @@ TEST(11) {
 
   CodeDesc desc;
   assm.GetCode(isolate, &desc);
-  Object code = isolate->heap()->CreateCode(
+  Tagged<Object> code = isolate->heap()->CreateCode(
       desc,
       CodeKind::FOR_TESTING,
       Handle<Code>())->ToObjectChecked();
diff --git a/test/cctest/test-assembler-riscv32.cc b/test/cctest/test-assembler-riscv32.cc
index 719836ab7ca..dbbfaca3389 100644
--- a/test/cctest/test-assembler-riscv32.cc
+++ b/test/cctest/test-assembler-riscv32.cc
@@ -1683,7 +1683,7 @@ TEST(jump_tables3) {
     values[i] = isolate->factory()->NewHeapNumber<AllocationType::kOld>(value);
   }
   Label labels[kNumCases], done, dispatch;
-  Object obj;
+  Tagged<Object> obj;
   int32_t imm32;
 
   auto fn = [&labels, &done, &dispatch, values, &obj,
diff --git a/test/cctest/test-assembler-riscv64.cc b/test/cctest/test-assembler-riscv64.cc
index c05b01f3cce..8b3df261159 100644
--- a/test/cctest/test-assembler-riscv64.cc
+++ b/test/cctest/test-assembler-riscv64.cc
@@ -1960,7 +1960,7 @@ TEST(jump_tables3) {
     values[i] = isolate->factory()->NewHeapNumber<AllocationType::kOld>(value);
   }
   Label labels[kNumCases], done, dispatch;
-  Object obj;
+  Tagged<Object> obj;
   int64_t imm64;
 
   auto fn = [&labels, &done, &dispatch, values, &obj,
diff --git a/test/cctest/test-cpu-profiler.cc b/test/cctest/test-cpu-profiler.cc
index 110f0bf0cb7..7e23c4ff3a5 100644
--- a/test/cctest/test-cpu-profiler.cc
+++ b/test/cctest/test-cpu-profiler.cc
@@ -141,7 +141,7 @@ class TestSetup {
 
 }  // namespace
 
-i::AbstractCode CreateCode(i::Isolate* isolate, LocalContext* env) {
+i::Tagged<i::AbstractCode> CreateCode(i::Isolate* isolate, LocalContext* env) {
   static int counter = 0;
   base::EmbeddedVector<char, 256> script;
   base::EmbeddedVector<char, 32> name;
@@ -4793,7 +4793,7 @@ TEST(BytecodeFlushEventsEagerLogging) {
     Handle<JSFunction> function = Handle<JSFunction>::cast(func_value);
     CHECK(function->shared()->is_compiled());
 
-    i::BytecodeArray compiled_data =
+    Tagged<BytecodeArray> compiled_data =
         function->shared()->GetBytecodeArray(i_isolate);
     i::Address bytecode_start = compiled_data->GetFirstBytecodeAddress();
 
diff --git a/test/cctest/test-heap-profiler.cc b/test/cctest/test-heap-profiler.cc
index 1ca4a4808de..21064198c2f 100644
--- a/test/cctest/test-heap-profiler.cc
+++ b/test/cctest/test-heap-profiler.cc
@@ -2064,7 +2064,7 @@ TEST(GetHeapValueForDeletedObject) {
   CHECK(heap_profiler->FindObjectById(prop->GetId()).IsEmpty());
 }
 
-static int StringCmp(const char* ref, i::String act) {
+static int StringCmp(const char* ref, i::Tagged<i::String> act) {
   std::unique_ptr<char[]> s_act = act->ToCString();
   int result = strcmp(ref, s_act.get());
   if (result != 0)
diff --git a/test/cctest/test-serialize.cc b/test/cctest/test-serialize.cc
index 4a7e5e84e88..7872ebecb7a 100644
--- a/test/cctest/test-serialize.cc
+++ b/test/cctest/test-serialize.cc
@@ -405,7 +405,8 @@ static void SerializeContext(base::Vector<const uint8_t>* startup_blob_out,
     }
 
     HandleScope scope(isolate);
-    i::Context raw_context = i::Context::cast(*v8::Utils::OpenPersistent(env));
+    i::Tagged<i::Context> raw_context =
+        i::Context::cast(*v8::Utils::OpenPersistent(env));
 
     env.Reset();
 
@@ -589,7 +590,8 @@ static void SerializeCustomContext(
     }
 
     HandleScope scope(i_isolate);
-    i::Context raw_context = i::Context::cast(*v8::Utils::OpenPersistent(env));
+    i::Tagged<i::Context> raw_context =
+        i::Context::cast(*v8::Utils::OpenPersistent(env));
 
     env.Reset();
 
@@ -841,7 +843,7 @@ UNINITIALIZED_TEST(CustomSnapshotDataBlobStringNotInternalized) {
     v8::Context::Scope c_scope(context);
     v8::Local<v8::Value> result = CompileRun("f()").As<v8::Value>();
     CHECK(result->IsString());
-    i::String str = *v8::Utils::OpenHandle(*result.As<v8::String>());
+    i::Tagged<i::String> str = *v8::Utils::OpenHandle(*result.As<v8::String>());
     CHECK_EQ(std::string(str->ToCString().get()), "AB");
     CHECK(!IsInternalizedString(str));
     CHECK(!i::ReadOnlyHeap::Contains(str));
@@ -1669,7 +1671,7 @@ int CountBuiltins() {
   HeapObjectIterator iterator(CcTest::heap());
   DisallowGarbageCollection no_gc;
   int counter = 0;
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     if (IsCode(obj) && Code::cast(obj)->kind() == CodeKind::BUILTIN) counter++;
   }
@@ -4932,7 +4934,7 @@ UNINITIALIZED_TEST(ClassFieldsWithBindings) {
   FreeCurrentEmbeddedBlob();
 }
 
-void CheckSFIsAreWeak(WeakFixedArray sfis, Isolate* isolate) {
+void CheckSFIsAreWeak(Tagged<WeakFixedArray> sfis, Isolate* isolate) {
   CHECK_GT(sfis->length(), 0);
   int no_of_weak = 0;
   for (int i = 0; i < sfis->length(); ++i) {
@@ -4992,7 +4994,7 @@ UNINITIALIZED_TEST(WeakArraySerializationInSnapshot) {
         Handle<JSFunction>::cast(v8::Utils::OpenHandle(*x));
 
     // Verify that the pointers in shared_function_infos are weak.
-    WeakFixedArray sfis =
+    Tagged<WeakFixedArray> sfis =
         Script::cast(function->shared()->script())->shared_function_infos();
     CheckSFIsAreWeak(sfis, reinterpret_cast<i::Isolate*>(isolate));
   }
@@ -5025,7 +5027,8 @@ TEST(WeakArraySerializationInCodeCache) {
                     v8::ScriptCompiler::kConsumeCodeCache);
 
   // Verify that the pointers in shared_function_infos are weak.
-  WeakFixedArray sfis = Script::cast(copy->script())->shared_function_infos();
+  Tagged<WeakFixedArray> sfis =
+      Script::cast(copy->script())->shared_function_infos();
   CheckSFIsAreWeak(sfis, isolate);
 
   delete cache;
@@ -5215,7 +5218,7 @@ void CheckObjectsAreInSharedHeap(Isolate* isolate) {
   Heap* heap = isolate->heap();
   HeapObjectIterator iterator(heap);
   DisallowGarbageCollection no_gc;
-  for (HeapObject obj = iterator.Next(); !obj.is_null();
+  for (Tagged<HeapObject> obj = iterator.Next(); !obj.is_null();
        obj = iterator.Next()) {
     const bool expected_in_shared_old =
         heap->MustBeInSharedOldSpace(obj) ||
diff --git a/test/cctest/test-smi-lexicographic-compare.cc b/test/cctest/test-smi-lexicographic-compare.cc
index 973a9155f80..333e6472f48 100644
--- a/test/cctest/test-smi-lexicographic-compare.cc
+++ b/test/cctest/test-smi-lexicographic-compare.cc
@@ -13,7 +13,7 @@ namespace internal {
 
 namespace {
 
-void AddSigned(std::set<Smi>* smis, int64_t x) {
+void AddSigned(std::set<Tagged<Smi>>* smis, int64_t x) {
   if (!Smi::IsValid(x)) return;
 
   smis->insert(Smi::FromInt(static_cast<int>(x)));
@@ -52,7 +52,7 @@ TEST(TestSmiLexicographicCompare) {
   Isolate* isolate = CcTest::InitIsolateOnce();
   HandleScope scope(isolate);
 
-  std::set<Smi> smis;
+  std::set<Tagged<Smi>> smis;
 
   for (int64_t xb = 1; xb <= Smi::kMaxValue; xb *= 10) {
     for (int64_t xf = 0; xf <= 9; ++xf) {
diff --git a/test/cctest/test-strings.cc b/test/cctest/test-strings.cc
index c9db5aa25f5..37a5dffb72a 100644
--- a/test/cctest/test-strings.cc
+++ b/test/cctest/test-strings.cc
@@ -272,7 +272,7 @@ class ConsStringGenerationData {
   int max_leaves_;
   // Cached data.
   Handle<String> building_blocks_[kNumberOfBuildingBlocks];
-  String empty_string_;
+  Tagged<String> empty_string_;
   MyRandomNumberGenerator rng_;
   // Stats.
   ConsStringStats stats_;
diff --git a/test/common/c-signature.h b/test/common/c-signature.h
index a638348fb82..04cc5a62efc 100644
--- a/test/common/c-signature.h
+++ b/test/common/c-signature.h
@@ -33,8 +33,9 @@ namespace compiler {
 
 template <typename T>
 inline constexpr MachineType MachineTypeForC() {
-  static_assert(std::is_convertible<T, Object>::value,
-                "all non-specialized types must be convertible to Object");
+  static_assert(
+      std::is_convertible<T, Tagged<Object>>::value,
+      "all non-specialized types must be convertible to Tagged<Object>");
   return MachineType::AnyTagged();
 }
 
diff --git a/test/mkgrokdump/mkgrokdump.cc b/test/mkgrokdump/mkgrokdump.cc
index 76cfb882708..377374c3691 100644
--- a/test/mkgrokdump/mkgrokdump.cc
+++ b/test/mkgrokdump/mkgrokdump.cc
@@ -163,8 +163,8 @@ static int DumpHeapConstants(FILE* out, const char* argv0) {
       i::PrintF(out, "\n# List of known V8 objects.\n");
       i::PrintF(out, "KNOWN_OBJECTS = {\n");
       i::ReadOnlyHeapObjectIterator ro_iterator(read_only_heap);
-      for (i::HeapObject object = ro_iterator.Next(); !object.is_null();
-           object = ro_iterator.Next()) {
+      for (i::Tagged<i::HeapObject> object = ro_iterator.Next();
+           !object.is_null(); object = ro_iterator.Next()) {
         // Skip read-only heap maps, they will be reported elsewhere.
         if (IsMap(object)) continue;
         DumpKnownObject(out, heap, i::ToString(i::RO_SPACE), object);
@@ -176,7 +176,8 @@ static int DumpHeapConstants(FILE* out, const char* argv0) {
         // Code objects are generally platform-dependent.
         if (s->identity() == i::CODE_SPACE) continue;
         const char* sname = i::ToString(s->identity());
-        for (i::HeapObject o = it.Next(); !o.is_null(); o = it.Next()) {
+        for (i::Tagged<i::HeapObject> o = it.Next(); !o.is_null();
+             o = it.Next()) {
           DumpKnownObject(out, heap, sname, o);
         }
       }
diff --git a/test/unittests/execution/microtask-queue-unittest.cc b/test/unittests/execution/microtask-queue-unittest.cc
index e0cbd44206d..f89bd5cebf3 100644
--- a/test/unittests/execution/microtask-queue-unittest.cc
+++ b/test/unittests/execution/microtask-queue-unittest.cc
@@ -135,10 +135,10 @@ class RecordingVisitor : public RootVisitor {
     }
   }
 
-  const std::vector<Object>& visited() const { return visited_; }
+  const std::vector<Tagged<Object>>& visited() const { return visited_; }
 
  private:
-  std::vector<Object> visited_;
+  std::vector<Tagged<Object>> visited_;
 };
 
 // Sanity check. Ensure a microtask is stored in a queue and run.
@@ -232,7 +232,7 @@ TEST_P(MicrotaskQueueTest, VisitRoot) {
   EXPECT_EQ(MicrotaskQueue::kMinimumCapacity / 2 + 1,
             microtask_queue()->RunMicrotasks(isolate()));
 
-  std::vector<Object> expected;
+  std::vector<Tagged<Object>> expected;
   for (int i = 0; i < MicrotaskQueue::kMinimumCapacity / 2 + 1; ++i) {
     Handle<Microtask> microtask = NewMicrotask([] {});
     expected.push_back(*microtask);
@@ -244,7 +244,7 @@ TEST_P(MicrotaskQueueTest, VisitRoot) {
   RecordingVisitor visitor;
   microtask_queue()->IterateMicrotasks(&visitor);
 
-  std::vector<Object> actual = visitor.visited();
+  std::vector<Tagged<Object>> actual = visitor.visited();
   std::sort(expected.begin(), expected.end());
   std::sort(actual.begin(), actual.end());
   EXPECT_EQ(expected, actual);
diff --git a/test/unittests/heap/conservative-stack-visitor-unittest.cc b/test/unittests/heap/conservative-stack-visitor-unittest.cc
index d1ed400e79e..47b17057fab 100644
--- a/test/unittests/heap/conservative-stack-visitor-unittest.cc
+++ b/test/unittests/heap/conservative-stack-visitor-unittest.cc
@@ -50,7 +50,7 @@ class RecordingVisitor final : public RootVisitor {
 
  private:
   // Some heap object that we want to check if it is visited or not.
-  HeapObject the_object_;
+  Tagged<HeapObject> the_object_;
 
   // Addresses of this object.
   Address base_address_;    // Uncompressed base address
diff --git a/test/unittests/heap/heap-unittest.cc b/test/unittests/heap/heap-unittest.cc
index eac5640e29a..871ff751d41 100644
--- a/test/unittests/heap/heap-unittest.cc
+++ b/test/unittests/heap/heap-unittest.cc
@@ -391,7 +391,7 @@ TEST_F(HeapTest, OptimizedAllocationAlwaysInNewSpace) {
 
 namespace {
 template <RememberedSetType direction>
-static size_t GetRememberedSetSize(HeapObject obj) {
+static size_t GetRememberedSetSize(Tagged<HeapObject> obj) {
   size_t count = 0;
   auto chunk = MemoryChunk::FromHeapObject(obj);
   RememberedSet<direction>::Iterate(
@@ -468,7 +468,7 @@ TEST_F(HeapTest, Regress978156) {
   CHECK_GT(last->length(), 0);
   heap->RightTrimFixedArray(*last, 1);
   // 4. Get the last filler on the page.
-  HeapObject filler = HeapObject::FromAddress(
+  Tagged<HeapObject> filler = HeapObject::FromAddress(
       MemoryChunk::FromHeapObject(*last)->area_end() - kTaggedSize);
   HeapObject::FromAddress(last->address() + last->Size());
   CHECK(IsFiller(filler));
diff --git a/test/unittests/heap/inner-pointer-resolution-unittest.cc b/test/unittests/heap/inner-pointer-resolution-unittest.cc
index 166ba5495ea..23f9a2857ff 100644
--- a/test/unittests/heap/inner-pointer-resolution-unittest.cc
+++ b/test/unittests/heap/inner-pointer-resolution-unittest.cc
@@ -199,12 +199,12 @@ class InnerPointerResolutionTest
       case ObjectRequest::LARGE: {
         DCHECK_LE(2 * kTaggedSize, object.size);
         ReadOnlyRoots roots(heap());
-        HeapObject heap_object(HeapObject::FromAddress(object.address));
-        heap_object.set_map_after_allocation(roots.unchecked_fixed_array_map(),
-                                             SKIP_WRITE_BARRIER);
-        FixedArray arr(FixedArray::cast(heap_object));
-        arr.set_length((object.size - FixedArray::SizeFor(0)) / kTaggedSize);
-        DCHECK_EQ(object.size, arr.AllocatedSize());
+        Tagged<HeapObject> heap_object(HeapObject::FromAddress(object.address));
+        heap_object->set_map_after_allocation(roots.unchecked_fixed_array_map(),
+                                              SKIP_WRITE_BARRIER);
+        Tagged<FixedArray> arr(FixedArray::cast(heap_object));
+        arr->set_length((object.size - FixedArray::SizeFor(0)) / kTaggedSize);
+        DCHECK_EQ(object.size, arr->AllocatedSize());
         break;
       }
       case ObjectRequest::FREE:
diff --git a/test/unittests/heap/persistent-handles-unittest.cc b/test/unittests/heap/persistent-handles-unittest.cc
index bab9f436b0b..b1fcc1de4d5 100644
--- a/test/unittests/heap/persistent-handles-unittest.cc
+++ b/test/unittests/heap/persistent-handles-unittest.cc
@@ -163,7 +163,7 @@ class PersistentHandlesThread final : public v8::base::Thread {
   Heap* heap_;
   std::vector<Handle<HeapNumber>> handles_;
   std::unique_ptr<PersistentHandles> ph_;
-  HeapNumber number_;
+  Tagged<HeapNumber> number_;
   base::Semaphore* sema_started_;
   base::Semaphore* sema_gc_finished_;
 };
diff --git a/test/unittests/objects/roots-unittest.cc b/test/unittests/objects/roots-unittest.cc
index f4acd2ad70a..79137d0b67e 100644
--- a/test/unittests/objects/roots-unittest.cc
+++ b/test/unittests/objects/roots-unittest.cc
@@ -28,7 +28,7 @@ AllocationSpace GetSpaceFromObject(Tagged<Object> object) {
 }  // namespace
 
 #define CHECK_IN_RO_SPACE(type, name, CamelName) \
-  HeapObject name = roots.name();                \
+  Tagged<HeapObject> name = roots.name();        \
   CHECK_EQ(RO_SPACE, GetSpaceFromObject(name));
 
 // The following tests check that all the roots accessible via ReadOnlyRoots are
diff --git a/test/unittests/parser/parsing-unittest.cc b/test/unittests/parser/parsing-unittest.cc
index ab6b6805ed9..3bc8b23d58e 100644
--- a/test/unittests/parser/parsing-unittest.cc
+++ b/test/unittests/parser/parsing-unittest.cc
@@ -82,7 +82,7 @@ struct Input {
             .ToHandleChecked());                                              \
     (isolate)->clear_pending_exception();                                     \
                                                                               \
-    String script_source = String::cast((script)->source());                  \
+    Tagged<String> script_source = String::cast((script)->source());          \
                                                                               \
     FATAL(                                                                    \
         "Parser failed on:\n"                                                 \
diff --git a/test/unittests/test-helpers.cc b/test/unittests/test-helpers.cc
index cc6f3943aba..084e9a15dab 100644
--- a/test/unittests/test-helpers.cc
+++ b/test/unittests/test-helpers.cc
@@ -56,7 +56,7 @@ std::unique_ptr<Utf16CharacterStream> SourceCharacterStreamForShared(
     Isolate* isolate, Handle<SharedFunctionInfo> shared) {
   // Create a character stream to simulate the parser having done so for the
   // top-level ParseProgram.
-  Script script = Script::cast(shared->script());
+  Tagged<Script> script = Script::cast(shared->script());
   Handle<String> source(String::cast(script->source()), isolate);
   std::unique_ptr<Utf16CharacterStream> stream(
       ScannerStream::For(isolate, source));
diff --git a/test/unittests/torque/torque-unittest.cc b/test/unittests/torque/torque-unittest.cc
index 1cab0265b1e..51b300e5712 100644
--- a/test/unittests/torque/torque-unittest.cc
+++ b/test/unittests/torque/torque-unittest.cc
@@ -42,7 +42,7 @@ namespace torque_internal {
 
 type Tagged generates 'TNode<MaybeObject>' constexpr 'MaybeObject';
 type StrongTagged extends Tagged
-    generates 'TNode<Object>' constexpr 'ObjectPtr';
+    generates 'TNode<Object>' constexpr 'Object';
 type Smi extends StrongTagged generates 'TNode<Smi>' constexpr 'Smi';
 type WeakHeapObject extends Tagged;
 type Weak<T : type extends HeapObject> extends WeakHeapObject;
diff --git a/tools/gcmole/gcmole-test.cc b/tools/gcmole/gcmole-test.cc
index b08f1bec8a1..0879c57e332 100644
--- a/tools/gcmole/gcmole-test.cc
+++ b/tools/gcmole/gcmole-test.cc
@@ -24,19 +24,19 @@ Handle<Object> CauseGC(Handle<Object> obj, Isolate* isolate) {
   return obj;
 }
 
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
   isolate->heap()->CollectGarbage(OLD_SPACE, GarbageCollectionReason::kTesting);
 
   return obj;
 }
 
-Managed<Smi> CauseGCManaged(int i, Isolate* isolate) {
+Tagged<Managed<int>> CauseGCManaged(int i, Isolate* isolate) {
   isolate->heap()->CollectGarbage(OLD_SPACE, GarbageCollectionReason::kTesting);
 
-  return Managed<Smi>::cast(Smi::FromInt(i));
+  return Managed<int>::cast(Smi::FromInt(i));
 }
 
-void TwoArgumentsFunction(Object a, Object b) {
+void TwoArgumentsFunction(Tagged<Object> a, Tagged<Object> b) {
   Print(a);
   Print(b);
 }
@@ -61,16 +61,11 @@ void TestTwoSizeTArguments(Isolate* isolate) {
                             sizeof(*CauseGC(obj2, isolate)));
 }
 
-// --------- Test problems with method arguments ----------
+// --------- Test problFems with method arguments ----------
 
 class SomeObject : public Object {
  public:
-  void Method(Object a) { Print(a); }
-
-  SomeObject& operator=(const Object& b) {
-    Print(*this);
-    return *this;
-  }
+  void Method(Tagged<Object> a) { Print(a); }
 
   DECL_CAST(SomeObject)
 
@@ -78,7 +73,7 @@ class SomeObject : public Object {
 };
 
 void TestMethodCall(Isolate* isolate) {
-  SomeObject obj;
+  Tagged<SomeObject> obj;
   Handle<SomeObject> so = handle(obj, isolate);
   Handle<JSObject> obj1 = isolate->factory()->NewJSObjectWithNullProto();
   // Should cause warning.
@@ -88,10 +83,10 @@ void TestMethodCall(Isolate* isolate) {
 }
 
 void TestOperatorCall(Isolate* isolate) {
-  SomeObject obj;
+  Tagged<SomeObject> obj;
   Handle<JSObject> obj1 = isolate->factory()->NewJSObjectWithNullProto();
   // Should not cause warning.
-  obj = *CauseGC(obj1, isolate);
+  obj = Tagged<SomeObject>::unchecked_cast(*CauseGC(obj1, isolate));
 }
 
 // --------- Test for templated sub-classes of Object ----------
@@ -125,7 +120,7 @@ void TestFollowingVirtualFunctions(Isolate* isolate) {
   BaseObject* base = &derived;
   Handle<JSObject> obj1 = isolate->factory()->NewJSObjectWithNullProto();
 
-  SomeObject so;
+  Tagged<SomeObject> so;
   Handle<SomeObject> so_handle = handle(so, isolate);
   // Should cause warning.
   so_handle->Method(*derived.VirtualCauseGC(obj1, isolate));
@@ -146,7 +141,7 @@ class SomeClass {
 };
 
 void TestFollowingStaticFunctions(Isolate* isolate) {
-  SomeObject so;
+  Tagged<SomeObject> so;
   Handle<SomeObject> so_handle = handle(so, isolate);
 
   Handle<JSObject> obj1 = isolate->factory()->NewJSObjectWithNullProto();
@@ -157,7 +152,7 @@ void TestFollowingStaticFunctions(Isolate* isolate) {
 // --------- Test basic dead variable analysis ----------
 
 void TestDeadVarAnalysis(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   CauseGCRaw(raw_obj, isolate);
 
   // Should cause warning.
@@ -165,7 +160,7 @@ void TestDeadVarAnalysis(Isolate* isolate) {
 }
 
 void TestDeadVarBecauseOfSafepointAnalysis(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   Safepoint();
 
   // Should cause warning.
@@ -173,7 +168,7 @@ void TestDeadVarBecauseOfSafepointAnalysis(Isolate* isolate) {
 }
 
 void TestGuardedDeadVarAnalysis(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
 
   // Note: having DisableGCMole with the same function as CauseGC
   // normally doesn't make sense, but we want to test whether the guards
@@ -186,7 +181,7 @@ void TestGuardedDeadVarAnalysis(Isolate* isolate) {
 }
 
 void TestGuardedDeadVarAnalysis2(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
 
   // Note: having DisallowGarbageCollection with the same function as CauseGC
   // normally doesn't make sense, but we want to test whether the guards
@@ -199,7 +194,7 @@ void TestGuardedDeadVarAnalysis2(Isolate* isolate) {
 }
 
 void TestGuardedAgainstSafepointDeadVarAnalysis(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
 
   // Note: having DisableGCMole with the same function as CauseGC
   // normally doesn't make sense, but we want to test whether the guards
@@ -212,7 +207,7 @@ void TestGuardedAgainstSafepointDeadVarAnalysis(Isolate* isolate) {
 }
 
 void TestGuardedAgainstSafepointDeadVarAnalysis2(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
 
   // Note: having DisallowGarbageCollection with the same function as CauseGC
   // normally doesn't make sense, but we want to test whether the guards
@@ -225,7 +220,7 @@ void TestGuardedAgainstSafepointDeadVarAnalysis2(Isolate* isolate) {
 }
 
 void TestGuardedAgainstSafepointDeadVarAnalysis3(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   // Note: having DisallowGarbageCollection with the same function as CauseGC
   // normally doesn't make sense, but we want to test whether the guards
   // are recognized by GCMole.
@@ -243,7 +238,7 @@ void TestGuardedAgainstSafepointDeadVarAnalysis3(Isolate* isolate) {
 }
 
 void TestOnlyHeapGuardedDeadVarAnalysisInCompound(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   // {DisallowHeapAccess} has a {DisallowHeapAllocation}, but no
   // {DisallowSafepoints}, so it could see objects move due to safepoints.
   DisallowHeapAccess no_gc;
@@ -253,7 +248,7 @@ void TestOnlyHeapGuardedDeadVarAnalysisInCompound(Isolate* isolate) {
 }
 
 void TestOnlyHeapGuardedDeadVarAnalysisInCompound2(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   // {DisallowHeapAccess} has a {DisallowHeapAllocation}, but no
   // {DisallowSafepoints}, so it could see objects move due to safepoints.
   DisallowHeapAccess no_gc;
@@ -265,7 +260,8 @@ void TestOnlyHeapGuardedDeadVarAnalysisInCompound2(Isolate* isolate) {
   Print(raw_obj);
 }
 
-void TestGuardedDeadVarAnalysisNested(JSObject raw_obj, Isolate* isolate) {
+void TestGuardedDeadVarAnalysisNested(Tagged<JSObject> raw_obj,
+                                      Isolate* isolate) {
   CauseGCRaw(raw_obj, isolate);
   // Should cause warning.
   Print(raw_obj);
@@ -273,7 +269,7 @@ void TestGuardedDeadVarAnalysisNested(JSObject raw_obj, Isolate* isolate) {
 
 void TestGuardedDeadVarAnalysisCaller(Isolate* isolate) {
   DisableGCMole no_gc_mole;
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   // Shouldn't cause warning.
   Print(raw_obj);
@@ -281,7 +277,7 @@ void TestGuardedDeadVarAnalysisCaller(Isolate* isolate) {
 
 void TestGuardedDeadVarAnalysisCaller2(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   // Should cause warning.
   Print(raw_obj);
@@ -289,31 +285,31 @@ void TestGuardedDeadVarAnalysisCaller2(Isolate* isolate) {
 
 void TestGuardedDeadVarAnalysisCaller3(Isolate* isolate) {
   DisallowHeapAccess no_gc;
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   // Should cause warning.
   Print(raw_obj);
 }
 
 void TestGuardedDeadVarAnalysisCaller4(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   // Should cause warning.
   Print(raw_obj);
 }
 
-JSObject GuardedAllocation(Isolate* isolate) {
+Tagged<JSObject> GuardedAllocation(Isolate* isolate) {
   DisallowGarbageCollection no_gc;
   return *isolate->factory()->NewJSObjectWithNullProto();
 }
 
-JSObject GuardedAllocation2(Isolate* isolate) {
+Tagged<JSObject> GuardedAllocation2(Isolate* isolate) {
   DisableGCMole no_gc_mole;
   return *isolate->factory()->NewJSObjectWithNullProto();
 }
 
 void TestNestedDeadVarAnalysis(Isolate* isolate) {
-  JSObject raw_obj = GuardedAllocation(isolate);
+  Tagged<JSObject> raw_obj = GuardedAllocation(isolate);
   CauseGCRaw(raw_obj, isolate);
   // Should cause warning.
   Print(raw_obj);
@@ -321,7 +317,7 @@ void TestNestedDeadVarAnalysis(Isolate* isolate) {
 
 void TestNestedDeadVarAnalysis2(Isolate* isolate) {
   DisableGCMole no_gc_mole;
-  JSObject raw_obj = GuardedAllocation(isolate);
+  Tagged<JSObject> raw_obj = GuardedAllocation(isolate);
   CauseGCRaw(raw_obj, isolate);
   // Shouldn't cause warning.
   Print(raw_obj);
@@ -330,7 +326,7 @@ void TestNestedDeadVarAnalysis2(Isolate* isolate) {
 // Test that putting a guard in the middle of the function doesn't
 // mistakenly cover the whole scope of the raw variable.
 void TestGuardedDeadVarAnalysisMidFunction(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   CauseGCRaw(raw_obj, isolate);
   // Guarding the rest of the function from triggering a GC.
   DisallowGarbageCollection no_gc;
@@ -341,7 +337,7 @@ void TestGuardedDeadVarAnalysisMidFunction(Isolate* isolate) {
 // Test that putting a guard in the middle of the function doesn't
 // mistakenly cover the whole scope of the raw variable.
 void TestGuardedDeadVarAnalysisMidFunction2(Isolate* isolate) {
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   CauseGCRaw(raw_obj, isolate);
   // Guarding the rest of the function from triggering a GC.
   DisableGCMole no_gc_mole;
@@ -353,7 +349,7 @@ void TestGuardedDeadVarAnalysisMultipleSafepoints(Isolate* isolate) {
   // TODO(https://crbug.com/v8/13536): The analysis points to this safepoint,
   // while it should point to the one below.
   Safepoint();
-  JSObject raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
+  Tagged<JSObject> raw_obj = *isolate->factory()->NewJSObjectWithNullProto();
   DisallowGarbageCollection no_gc;
   Safepoint();
   Print(raw_obj);
diff --git a/tools/gcmole/test-expectations.txt b/tools/gcmole/test-expectations.txt
index c09089ac51c..1851021955f 100644
--- a/tools/gcmole/test-expectations.txt
+++ b/tools/gcmole/test-expectations.txt
@@ -25,199 +25,199 @@ tools/gcmole/gcmole-test.cc:60:37: note: Call might cause unexpected GC.
 tools/gcmole/gcmole-test.cc:21:1: note: GC call here.
 Handle<Object> CauseGC(Handle<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:85:7: warning: Possible problem with evaluation order with interleaved GCs.
+tools/gcmole/gcmole-test.cc:80:7: warning: Possible problem with evaluation order with interleaved GCs.
   so->Method(*CauseGC(obj1, isolate));
       ^
-tools/gcmole/gcmole-test.cc:85:15: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:80:15: note: Call might cause unexpected GC.
   so->Method(*CauseGC(obj1, isolate));
               ^
 tools/gcmole/gcmole-test.cc:21:1: note: GC call here.
 Handle<Object> CauseGC(Handle<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:87:7: warning: Possible problem with evaluation order with interleaved GCs.
+tools/gcmole/gcmole-test.cc:82:7: warning: Possible problem with evaluation order with interleaved GCs.
   so->Method(CauseGCRaw(*obj1, isolate));
       ^
-tools/gcmole/gcmole-test.cc:85:15: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:80:15: note: Call might cause unexpected GC.
   so->Method(*CauseGC(obj1, isolate));
               ^
 tools/gcmole/gcmole-test.cc:21:1: note: GC call here.
 Handle<Object> CauseGC(Handle<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:131:14: warning: Possible problem with evaluation order with interleaved GCs.
+tools/gcmole/gcmole-test.cc:126:14: warning: Possible problem with evaluation order with interleaved GCs.
   so_handle->Method(*derived.VirtualCauseGC(obj1, isolate));
              ^
-tools/gcmole/gcmole-test.cc:131:30: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:126:30: note: Call might cause unexpected GC.
   so_handle->Method(*derived.VirtualCauseGC(obj1, isolate));
                              ^
-tools/gcmole/gcmole-test.cc:115:3: note: GC call here.
+tools/gcmole/gcmole-test.cc:110:3: note: GC call here.
   Handle<Object> VirtualCauseGC(Handle<Object> obj, Isolate* isolate) override {
   ^
-tools/gcmole/gcmole-test.cc:133:14: warning: Possible problem with evaluation order with interleaved GCs.
+tools/gcmole/gcmole-test.cc:128:14: warning: Possible problem with evaluation order with interleaved GCs.
   so_handle->Method(*base->VirtualCauseGC(obj1, isolate));
              ^
-tools/gcmole/gcmole-test.cc:131:30: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:126:30: note: Call might cause unexpected GC.
   so_handle->Method(*derived.VirtualCauseGC(obj1, isolate));
                              ^
-tools/gcmole/gcmole-test.cc:115:3: note: GC call here.
+tools/gcmole/gcmole-test.cc:110:3: note: GC call here.
   Handle<Object> VirtualCauseGC(Handle<Object> obj, Isolate* isolate) override {
   ^
-tools/gcmole/gcmole-test.cc:154:14: warning: Possible problem with evaluation order with interleaved GCs.
+tools/gcmole/gcmole-test.cc:149:14: warning: Possible problem with evaluation order with interleaved GCs.
   so_handle->Method(*SomeClass::StaticCauseGC(obj1, isolate));
              ^
-tools/gcmole/gcmole-test.cc:154:22: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:149:22: note: Call might cause unexpected GC.
   so_handle->Method(*SomeClass::StaticCauseGC(obj1, isolate));
                      ^
-tools/gcmole/gcmole-test.cc:140:3: note: GC call here.
+tools/gcmole/gcmole-test.cc:135:3: note: GC call here.
   static Handle<Object> StaticCauseGC(Handle<Object> obj, Isolate* isolate) {
   ^
-tools/gcmole/gcmole-test.cc:164:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:159:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:161:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:156:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:172:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:167:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:169:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:164:3: note: Call might cause unexpected GC.
   Safepoint();
   ^
 tools/gcmole/gcmole-test.cc:19:1: note: GC call here.
 void Safepoint() { LocalHeap::Current()->Safepoint(); }
 ^
-tools/gcmole/gcmole-test.cc:198:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:193:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:195:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:190:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:224:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:219:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:221:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:216:3: note: Call might cause unexpected GC.
   Safepoint();
   ^
 tools/gcmole/gcmole-test.cc:19:1: note: GC call here.
 void Safepoint() { LocalHeap::Current()->Safepoint(); }
 ^
-tools/gcmole/gcmole-test.cc:235:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:230:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:233:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:228:3: note: Call might cause unexpected GC.
   Safepoint();
   ^
 tools/gcmole/gcmole-test.cc:19:1: note: GC call here.
 void Safepoint() { LocalHeap::Current()->Safepoint(); }
 ^
-tools/gcmole/gcmole-test.cc:242:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:237:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:233:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:228:3: note: Call might cause unexpected GC.
   Safepoint();
   ^
 tools/gcmole/gcmole-test.cc:19:1: note: GC call here.
 void Safepoint() { LocalHeap::Current()->Safepoint(); }
 ^
-tools/gcmole/gcmole-test.cc:252:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:247:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:250:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:245:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:262:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:257:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:260:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:255:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:265:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:260:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:260:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:255:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:271:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:267:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:269:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:265:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:287:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:283:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:285:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:281:3: note: Call might cause unexpected GC.
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   ^
-tools/gcmole/gcmole-test.cc:268:1: note: GC call here.
-void TestGuardedDeadVarAnalysisNested(JSObject raw_obj, Isolate* isolate) {
+tools/gcmole/gcmole-test.cc:263:1: note: GC call here.
+void TestGuardedDeadVarAnalysisNested(Tagged<JSObject> raw_obj,
 ^
-tools/gcmole/gcmole-test.cc:295:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:291:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:293:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:289:3: note: Call might cause unexpected GC.
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   ^
-tools/gcmole/gcmole-test.cc:268:1: note: GC call here.
-void TestGuardedDeadVarAnalysisNested(JSObject raw_obj, Isolate* isolate) {
+tools/gcmole/gcmole-test.cc:263:1: note: GC call here.
+void TestGuardedDeadVarAnalysisNested(Tagged<JSObject> raw_obj,
 ^
-tools/gcmole/gcmole-test.cc:302:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:298:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:300:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:296:3: note: Call might cause unexpected GC.
   TestGuardedDeadVarAnalysisNested(raw_obj, isolate);
   ^
-tools/gcmole/gcmole-test.cc:268:1: note: GC call here.
-void TestGuardedDeadVarAnalysisNested(JSObject raw_obj, Isolate* isolate) {
+tools/gcmole/gcmole-test.cc:263:1: note: GC call here.
+void TestGuardedDeadVarAnalysisNested(Tagged<JSObject> raw_obj,
 ^
-tools/gcmole/gcmole-test.cc:319:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:315:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:317:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:313:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:338:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:334:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:334:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:330:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:349:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:345:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:345:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:341:3: note: Call might cause unexpected GC.
   CauseGCRaw(raw_obj, isolate);
   ^
 tools/gcmole/gcmole-test.cc:27:1: note: GC call here.
-Object CauseGCRaw(Object obj, Isolate* isolate) {
+Tagged<Object> CauseGCRaw(Tagged<Object> obj, Isolate* isolate) {
 ^
-tools/gcmole/gcmole-test.cc:359:9: warning: Possibly stale variable due to GCs.
+tools/gcmole/gcmole-test.cc:355:9: warning: Possibly stale variable due to GCs.
   Print(raw_obj);
         ^
-tools/gcmole/gcmole-test.cc:355:3: note: Call might cause unexpected GC.
+tools/gcmole/gcmole-test.cc:351:3: note: Call might cause unexpected GC.
   Safepoint();
   ^
 tools/gcmole/gcmole-test.cc:19:1: note: GC call here.
-- 
2.35.1

