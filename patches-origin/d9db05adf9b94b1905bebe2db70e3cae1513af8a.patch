From d9db05adf9b94b1905bebe2db70e3cae1513af8a Mon Sep 17 00:00:00 2001
From: Nico Hartmann <nicohartmann@chromium.org>
Date: Mon, 28 Aug 2023 11:02:53 +0200
Subject: [PATCH] [compiler] Generalize InstructionSelectorT for Turboshaft
 (part 13)

This CL ports more of the instruction selector implementation to
support Turboshaft.


Bug: v8:12783
Change-Id: Ifd8c7ddeefb77dd9b720d01151ca7f328d357a3d
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4809945
Commit-Queue: Nico Hartmann <nicohartmann@chromium.org>
Reviewed-by: Darius Mercadier <dmercadier@chromium.org>
Cr-Commit-Position: refs/heads/main@{#89646}
---
 .../backend/arm/instruction-selector-arm.cc   | 165 +++++++-----
 .../arm64/instruction-selector-arm64.cc       | 113 ++++----
 .../backend/ia32/instruction-selector-ia32.cc | 126 +++++----
 .../backend/instruction-selector-adapter.h    |  17 +-
 src/compiler/backend/instruction-selector.cc  | 251 +++++++++---------
 src/compiler/backend/instruction-selector.h   |  70 ++---
 .../backend/x64/instruction-selector-x64.cc   | 106 ++++----
 src/compiler/turboshaft/operations.h          |   1 +
 8 files changed, 463 insertions(+), 386 deletions(-)

diff --git a/src/compiler/backend/arm/instruction-selector-arm.cc b/src/compiler/backend/arm/instruction-selector-arm.cc
index f0bd6a23f61..f9a7e144f1d 100644
--- a/src/compiler/backend/arm/instruction-selector-arm.cc
+++ b/src/compiler/backend/arm/instruction-selector-arm.cc
@@ -576,9 +576,14 @@ void InstructionSelectorT<Adapter>::VisitStackSlot(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
-  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), r1));
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // This is currently not used by Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    ArmOperandGeneratorT<Adapter> g(this);
+    Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), r1));
+  }
 }
 
 template <typename Adapter>
@@ -723,7 +728,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
@@ -874,7 +879,7 @@ void VisitStoreCommon(InstructionSelectorT<Adapter>* selector, Node* node,
 }  // namespace
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {
   UNREACHABLE();
 }
 
@@ -895,7 +900,7 @@ void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {
   if constexpr (Adapter::IsTurboshaft) {
     UNIMPLEMENTED();
   } else {
@@ -964,9 +969,14 @@ void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitUnalignedStore(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitUnalignedStore(Node* node) {
+  ArmOperandGeneratorT<TurbofanAdapter> g(this);
   Node* base = node->InputAt(0);
   Node* index = node->InputAt(1);
   Node* value = node->InputAt(2);
@@ -1366,9 +1376,14 @@ void InstructionSelectorT<Adapter>::VisitWord32Sar(node_t node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairAdd(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairAdd(node_t node) {
+  ArmOperandGeneratorT<TurbofanAdapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1392,9 +1407,14 @@ void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairSub(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairSub(node_t node) {
+  ArmOperandGeneratorT<TurbofanAdapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1418,9 +1438,14 @@ void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairMul(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairMul(node_t node) {
+  ArmOperandGeneratorT<TurbofanAdapter> g(this);
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
     InstructionOperand inputs[] = {g.UseUniqueRegister(node->InputAt(0)),
@@ -1480,18 +1505,30 @@ void VisitWord32PairShift(InstructionSelectorT<Adapter>* selector,
 }
 }  // namespace
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
-  VisitWord32PairShift(this, kArmLslPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairShl(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kArmLslPair, node);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
-  VisitWord32PairShift(this, kArmLsrPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairShr(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kArmLsrPair, node);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
-  VisitWord32PairShift(this, kArmAsrPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairSar(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kArmAsrPair, node);
+  }
 }
 
 template <typename Adapter>
@@ -1510,7 +1547,7 @@ void InstructionSelectorT<Adapter>::VisitWord32Ctz(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(node_t node) {
   DCHECK(IsSupported(ARMv7));
   VisitRR(this, kArmRbit, node);
 }
@@ -1773,6 +1810,7 @@ void InstructionSelectorT<Adapter>::VisitUint32Mod(node_t node) {
   V(Float64ExtractHighWord32, kArmVmovHighU32F64)    \
   V(TruncateFloat64ToFloat32, kArmVcvtF32F64)        \
   V(TruncateFloat64ToWord32, kArchTruncateDoubleToI) \
+  V(TruncateFloat64ToUint32, kArmVcvtU32F64)         \
   V(BitcastFloat32ToInt32, kArmVmovU32F32)           \
   V(BitcastInt32ToFloat32, kArmVmovF32U32)           \
   V(RoundFloat64ToInt32, kArmVcvtS32F64)             \
@@ -1785,8 +1823,6 @@ void InstructionSelectorT<Adapter>::VisitUint32Mod(node_t node) {
   V(Float64Sqrt, kArmVsqrtF64)                       \
   V(Word32Clz, kArmClz)
 
-#define RR_OP_LIST(V) V(TruncateFloat64ToUint32, kArmVcvtU32F64)
-
 #define RR_OP_T_LIST_V8(V)               \
   V(Float32RoundDown, kArmVrintmF32)     \
   V(Float64RoundDown, kArmVrintmF64)     \
@@ -1819,15 +1855,6 @@ void InstructionSelectorT<Adapter>::VisitUint32Mod(node_t node) {
   V(Float64Min, kArmFloat64Min) \
   V(Int32MulHigh, kArmSmmul)
 
-#define RR_VISITOR(Name, opcode)                                \
-  template <typename Adapter>                                   \
-  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
-    VisitRR(this, opcode, node);                                \
-  }
-RR_OP_LIST(RR_VISITOR)
-#undef RR_VISITOR
-#undef RR_OP_LIST
-
 #define RR_VISITOR(Name, opcode)                                 \
   template <typename Adapter>                                    \
   void InstructionSelectorT<Adapter>::Visit##Name(node_t node) { \
@@ -2605,39 +2632,53 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
-      CanCover(node, left)) {
-    left = left->InputAt(1);
-    Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), g.UseRegister(right),
-         g.UseRegister(left));
-    return;
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    ArmOperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
+        CanCover(node, left)) {
+      left = left->InputAt(1);
+      Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), g.UseRegister(right),
+           g.UseRegister(left));
+      return;
+    }
+    Emit(kArmVmovLowF64U32, g.DefineSameAsFirst(node), g.UseRegister(left),
+         g.UseRegister(right));
   }
-  Emit(kArmVmovLowF64U32, g.DefineSameAsFirst(node), g.UseRegister(left),
-       g.UseRegister(right));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
-  ArmOperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
-      CanCover(node, left)) {
-    left = left->InputAt(1);
-    Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), g.UseRegister(left),
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    ArmOperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
+        CanCover(node, left)) {
+      left = left->InputAt(1);
+      Emit(kArmVmovF64U32U32, g.DefineAsRegister(node), g.UseRegister(left),
+           g.UseRegister(right));
+      return;
+    }
+    Emit(kArmVmovHighF64U32, g.DefineSameAsFirst(node), g.UseRegister(left),
          g.UseRegister(right));
-    return;
   }
-  Emit(kArmVmovHighF64U32, g.DefineSameAsFirst(node), g.UseRegister(left),
-       g.UseRegister(right));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
+void InstructionSelectorT<Adapter>::VisitBitcastWord32PairToFloat64(
+    node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(node_t node) {
   // Use DMB ISH for both acquire-release and sequentially consistent barriers.
   ArmOperandGeneratorT<Adapter> g(this);
   Emit(kArmDmbIsh, g.NoOutput());
@@ -3552,12 +3593,12 @@ void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt32(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
diff --git a/src/compiler/backend/arm64/instruction-selector-arm64.cc b/src/compiler/backend/arm64/instruction-selector-arm64.cc
index 2952fbfda73..2225f726707 100644
--- a/src/compiler/backend/arm64/instruction-selector-arm64.cc
+++ b/src/compiler/backend/arm64/instruction-selector-arm64.cc
@@ -646,7 +646,7 @@ int32_t LeftShiftForReducedMultiply(Matcher* m) {
 }  // namespace
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTraceInstruction(Node* node) {}
+void InstructionSelectorT<Adapter>::VisitTraceInstruction(node_t node) {}
 
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitStackSlot(node_t node) {
@@ -663,9 +663,13 @@ void InstructionSelectorT<Adapter>::VisitStackSlot(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
-  Arm64OperandGeneratorT<Adapter> g(this);
-  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), x1));
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Arm64OperandGeneratorT<Adapter> g(this);
+    Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), x1));
+  }
 }
 
 template <typename Adapter>
@@ -958,12 +962,12 @@ void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {
   VisitLoad(node);
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {
   if constexpr (Adapter::IsTurboshaft) {
   UNIMPLEMENTED();
   } else {
@@ -1231,13 +1235,13 @@ void InstructionSelectorT<Adapter>::VisitSimd128ReverseBytes(Node* node) {
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {
   UNREACHABLE();
 }
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) {
   UNREACHABLE();
 }
 
@@ -1813,26 +1817,26 @@ void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) {
   V(BitcastInt64ToFloat64, kArm64Float64MoveU64)              \
   V(TruncateFloat64ToFloat32, kArm64Float64ToFloat32)         \
   V(TruncateFloat64ToWord32, kArchTruncateDoubleToI)          \
+  V(TruncateFloat64ToUint32, kArm64Float64ToUint32)           \
   V(Float64ExtractLowWord32, kArm64Float64ExtractLowWord32)   \
   V(Float64ExtractHighWord32, kArm64Float64ExtractHighWord32) \
   V(Word64Clz, kArm64Clz)                                     \
   V(Word32Clz, kArm64Clz32)                                   \
   V(Word32Popcnt, kArm64Cnt32)                                \
   V(Word64Popcnt, kArm64Cnt64)                                \
+  V(Word32ReverseBits, kArm64Rbit32)                          \
+  V(Word64ReverseBits, kArm64Rbit)                            \
   V(Word32ReverseBytes, kArm64Rev32)                          \
   V(Word64ReverseBytes, kArm64Rev)
 
-#define RR_OP_LIST(V)                               \
-  V(Word32ReverseBits, kArm64Rbit32)                \
-  V(Word64ReverseBits, kArm64Rbit)                  \
-  V(TruncateFloat64ToUint32, kArm64Float64ToUint32) \
-  V(F32x4Ceil, kArm64Float32RoundUp)                \
-  V(F32x4Floor, kArm64Float32RoundDown)             \
-  V(F32x4Trunc, kArm64Float32RoundTruncate)         \
-  V(F32x4NearestInt, kArm64Float32RoundTiesEven)    \
-  V(F64x2Ceil, kArm64Float64RoundUp)                \
-  V(F64x2Floor, kArm64Float64RoundDown)             \
-  V(F64x2Trunc, kArm64Float64RoundTruncate)         \
+#define RR_OP_LIST(V)                            \
+  V(F32x4Ceil, kArm64Float32RoundUp)             \
+  V(F32x4Floor, kArm64Float32RoundDown)          \
+  V(F32x4Trunc, kArm64Float32RoundTruncate)      \
+  V(F32x4NearestInt, kArm64Float32RoundTiesEven) \
+  V(F64x2Ceil, kArm64Float64RoundUp)             \
+  V(F64x2Floor, kArm64Float64RoundDown)          \
+  V(F64x2Trunc, kArm64Float64RoundTruncate)      \
   V(F64x2NearestInt, kArm64Float64RoundTiesEven)
 
 #define RRR_OP_T_LIST(V)          \
@@ -4103,38 +4107,47 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
-  Arm64OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
-      CanCover(node, left)) {
-    Node* right_of_left = left->InputAt(1);
-    Emit(kArm64Bfi, g.DefineSameAsFirst(right), g.UseRegister(right),
-         g.UseRegister(right_of_left), g.TempImmediate(32),
-         g.TempImmediate(32));
-    Emit(kArm64Float64MoveU64, g.DefineAsRegister(node), g.UseRegister(right));
-    return;
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Arm64OperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    if (left->opcode() == IrOpcode::kFloat64InsertHighWord32 &&
+        CanCover(node, left)) {
+      Node* right_of_left = left->InputAt(1);
+      Emit(kArm64Bfi, g.DefineSameAsFirst(right), g.UseRegister(right),
+           g.UseRegister(right_of_left), g.TempImmediate(32),
+           g.TempImmediate(32));
+      Emit(kArm64Float64MoveU64, g.DefineAsRegister(node),
+           g.UseRegister(right));
+      return;
+    }
+    Emit(kArm64Float64InsertLowWord32, g.DefineSameAsFirst(node),
+         g.UseRegister(left), g.UseRegister(right));
   }
-  Emit(kArm64Float64InsertLowWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.UseRegister(right));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
-  Arm64OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
-      CanCover(node, left)) {
-    Node* right_of_left = left->InputAt(1);
-    Emit(kArm64Bfi, g.DefineSameAsFirst(left), g.UseRegister(right_of_left),
-         g.UseRegister(right), g.TempImmediate(32), g.TempImmediate(32));
-    Emit(kArm64Float64MoveU64, g.DefineAsRegister(node), g.UseRegister(left));
-    return;
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    Arm64OperandGeneratorT<Adapter> g(this);
+    Node* left = node->InputAt(0);
+    Node* right = node->InputAt(1);
+    if (left->opcode() == IrOpcode::kFloat64InsertLowWord32 &&
+        CanCover(node, left)) {
+      Node* right_of_left = left->InputAt(1);
+      Emit(kArm64Bfi, g.DefineSameAsFirst(left), g.UseRegister(right_of_left),
+           g.UseRegister(right), g.TempImmediate(32), g.TempImmediate(32));
+      Emit(kArm64Float64MoveU64, g.DefineAsRegister(node), g.UseRegister(left));
+      return;
+    }
+    Emit(kArm64Float64InsertHighWord32, g.DefineSameAsFirst(node),
+         g.UseRegister(left), g.UseRegister(right));
   }
-  Emit(kArm64Float64InsertHighWord32, g.DefineSameAsFirst(node),
-       g.UseRegister(left), g.UseRegister(right));
 }
 
 template <typename Adapter>
@@ -4180,7 +4193,7 @@ void InstructionSelectorT<Adapter>::VisitFloat64Mul(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(node_t node) {
   // Use DMB ISH for both acquire-release and sequentially consistent barriers.
   Arm64OperandGeneratorT<Adapter> g(this);
   Emit(kArm64DmbIsh, g.NoOutput());
@@ -4368,12 +4381,12 @@ VISIT_ATOMIC_BINOP(Xor)
 #undef VISIT_ATOMIC_BINOP
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
@@ -5267,7 +5280,7 @@ void InstructionSelectorT<Adapter>::VisitSignExtendWord16ToInt64(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
+void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(node_t node) {
   VisitRR(this, kArm64Sxtw, node);
 }
 
diff --git a/src/compiler/backend/ia32/instruction-selector-ia32.cc b/src/compiler/backend/ia32/instruction-selector-ia32.cc
index bfeee858832..630a9fd5d9f 100644
--- a/src/compiler/backend/ia32/instruction-selector-ia32.cc
+++ b/src/compiler/backend/ia32/instruction-selector-ia32.cc
@@ -550,9 +550,13 @@ void InstructionSelectorT<Adapter>::VisitStackSlot(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
-  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), edx));
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    IA32OperandGeneratorT<Adapter> g(this);
+    Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), edx));
+  }
 }
 
 template <typename Adapter>
@@ -686,7 +690,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {
   // TODO(eholk)
   UNIMPLEMENTED();
 }
@@ -830,7 +834,7 @@ void VisitStoreCommon(InstructionSelectorT<Adapter>* selector, Node* node,
 }  // namespace
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {
   UNREACHABLE();
 }
 
@@ -888,13 +892,13 @@ void InstructionSelectorT<Adapter>::VisitStoreLane(Node* node) {
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {
   UNREACHABLE();
 }
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) {
   UNREACHABLE();
 }
 
@@ -1168,9 +1172,14 @@ void InstructionSelectorT<Adapter>::VisitWord32Sar(node_t node) {
   VisitShift(this, node, kIA32Sar);
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairAdd(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairAdd(node_t node) {
+  IA32OperandGeneratorT<TurbofanAdapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1195,9 +1204,14 @@ void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairSub(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairSub(node_t node) {
+  IA32OperandGeneratorT<TurbofanAdapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1222,9 +1236,14 @@ void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
   }
 }
 
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
-  IA32OperandGeneratorT<Adapter> g(this);
+template <>
+void InstructionSelectorT<TurboshaftAdapter>::VisitInt32PairMul(node_t node) {
+  UNIMPLEMENTED();
+}
+
+template <>
+void InstructionSelectorT<TurbofanAdapter>::VisitInt32PairMul(node_t node) {
+  IA32OperandGeneratorT<TurbofanAdapter> g(this);
 
   Node* projection1 = NodeProperties::FindProjection(node, 1);
   if (projection1) {
@@ -1283,18 +1302,30 @@ void VisitWord32PairShift(InstructionSelectorT<Adapter>* selector,
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
-  VisitWord32PairShift(this, kIA32ShlPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairShl(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kIA32ShlPair, node);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
-  VisitWord32PairShift(this, kIA32ShrPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairShr(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kIA32ShrPair, node);
+  }
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
-  VisitWord32PairShift(this, kIA32SarPair, node);
+void InstructionSelectorT<Adapter>::VisitWord32PairSar(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    VisitWord32PairShift(this, kIA32SarPair, node);
+  }
 }
 
 template <typename Adapter>
@@ -1332,12 +1363,10 @@ void InstructionSelectorT<Adapter>::VisitWord32Ror(node_t node) {
 #define RO_WITH_TEMP_OP_T_LIST(V) V(ChangeUint32ToFloat64, kIA32Uint32ToFloat64)
 
 #define RO_WITH_TEMP_SIMD_OP_T_LIST(V)             \
+  V(TruncateFloat64ToUint32, kIA32Float64ToUint32) \
   V(TruncateFloat32ToUint32, kIA32Float32ToUint32) \
   V(ChangeFloat64ToUint32, kIA32Float64ToUint32)
 
-#define RO_WITH_TEMP_SIMD_OP_LIST(V) \
-  V(TruncateFloat64ToUint32, kIA32Float64ToUint32)
-
 #define RR_OP_T_LIST(V)                                                        \
   V(Float32RoundDown, kIA32Float32Round | MiscField::encode(kRoundDown))       \
   V(Float64RoundDown, kIA32Float64Round | MiscField::encode(kRoundDown))       \
@@ -1420,15 +1449,6 @@ RO_WITH_TEMP_OP_T_LIST(RO_WITH_TEMP_VISITOR)
 #undef RO_WITH_TEMP_VISITOR
 #undef RO_WITH_TEMP_OP_T_LIST
 
-#define RO_WITH_TEMP_SIMD_VISITOR(Name, opcode)                 \
-  template <typename Adapter>                                   \
-  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
-    VisitROWithTempSimd(this, node, opcode);                    \
-  }
-RO_WITH_TEMP_SIMD_OP_LIST(RO_WITH_TEMP_SIMD_VISITOR)
-#undef RO_WITH_TEMP_SIMD_VISITOR
-#undef RO_WITH_TEMP_SIMD_OP_LIST
-
 #define RO_WITH_TEMP_SIMD_VISITOR(Name, opcode)                  \
   template <typename Adapter>                                    \
   void InstructionSelectorT<Adapter>::Visit##Name(node_t node) { \
@@ -1494,7 +1514,7 @@ FLOAT_UNOP_T_LIST(FLOAT_UNOP_VISITOR)
 #undef FLOAT_UNOP_T_LIST
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(node_t node) {
   UNREACHABLE();
 }
 
@@ -2369,7 +2389,7 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(node_t node) {
   if constexpr (Adapter::IsTurboshaft) {
     UNIMPLEMENTED();
   } else {
@@ -2388,7 +2408,7 @@ void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(node_t node) {
   if constexpr (Adapter::IsTurboshaft) {
     UNIMPLEMENTED();
   } else {
@@ -2400,6 +2420,12 @@ void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
   }
 }
 
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitBitcastWord32PairToFloat64(
+    node_t node) {
+  UNIMPLEMENTED();
+}
+
 template <typename Adapter>
 void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(node_t node) {
   if constexpr (Adapter::IsTurboshaft) {
@@ -2412,16 +2438,20 @@ void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
-  // ia32 is no weaker than release-acquire and only needs to emit an
-  // instruction for SeqCst memory barriers.
-  AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
-  if (order == AtomicMemoryOrder::kSeqCst) {
-    IA32OperandGeneratorT<Adapter> g(this);
-    Emit(kIA32MFence, g.NoOutput());
-    return;
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    UNIMPLEMENTED();
+  } else {
+    // ia32 is no weaker than release-acquire and only needs to emit an
+    // instruction for SeqCst memory barriers.
+    AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
+    if (order == AtomicMemoryOrder::kSeqCst) {
+      IA32OperandGeneratorT<Adapter> g(this);
+      Emit(kIA32MFence, g.NoOutput());
+      return;
+    }
+    DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
   }
-  DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
 }
 
 template <typename Adapter>
@@ -3193,12 +3223,12 @@ void InstructionSelectorT<Adapter>::VisitI8x16ShrU(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
diff --git a/src/compiler/backend/instruction-selector-adapter.h b/src/compiler/backend/instruction-selector-adapter.h
index 42e718a284f..641b8c0149d 100644
--- a/src/compiler/backend/instruction-selector-adapter.h
+++ b/src/compiler/backend/instruction-selector-adapter.h
@@ -544,12 +544,10 @@ struct TurboshaftAdapter
   class CallView {
    public:
     explicit CallView(turboshaft::Graph* graph, node_t node) : node_(node) {
-      if ((call_op_ = graph->Get(node_).TryCast<turboshaft::CallOp>())) {
-        return;
-      }
-      if (graph->Get(node_).Is<turboshaft::TailCallOp>()) {
-        UNIMPLEMENTED();
-      }
+      call_op_ = graph->Get(node_).TryCast<turboshaft::CallOp>();
+      if (call_op_ != nullptr) return;
+      tail_call_op_ = graph->Get(node_).TryCast<turboshaft::TailCallOp>();
+      if (tail_call_op_ != nullptr) return;
       UNREACHABLE();
     }
 
@@ -557,10 +555,14 @@ struct TurboshaftAdapter
       if (call_op_) {
         return static_cast<int>(call_op_->results_rep().size());
       }
+      if (tail_call_op_) {
+        return static_cast<int>(tail_call_op_->outputs_rep().size());
+      }
       UNREACHABLE();
     }
     node_t callee() const {
       if (call_op_) return call_op_->callee();
+      if (tail_call_op_) return tail_call_op_->callee();
       UNREACHABLE();
     }
     node_t frame_state() const {
@@ -569,10 +571,12 @@ struct TurboshaftAdapter
     }
     base::Vector<const node_t> arguments() const {
       if (call_op_) return call_op_->arguments();
+      if (tail_call_op_) return tail_call_op_->arguments();
       UNREACHABLE();
     }
     const CallDescriptor* call_descriptor() const {
       if (call_op_) return call_op_->descriptor->descriptor;
+      if (tail_call_op_) return tail_call_op_->descriptor->descriptor;
       UNREACHABLE();
     }
 
@@ -581,6 +585,7 @@ struct TurboshaftAdapter
    private:
     node_t node_;
     const turboshaft::CallOp* call_op_;
+    const turboshaft::TailCallOp* tail_call_op_;
   };
 
   class BranchView {
diff --git a/src/compiler/backend/instruction-selector.cc b/src/compiler/backend/instruction-selector.cc
index d56f8845d90..0999380f363 100644
--- a/src/compiler/backend/instruction-selector.cc
+++ b/src/compiler/backend/instruction-selector.cc
@@ -1656,13 +1656,13 @@ void InstructionSelectorT<Adapter>::VisitBlock(block_t block) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::MarkPairProjectionsAsWord32(Node* node) {
-  Node* projection0 = NodeProperties::FindProjection(node, 0);
-  if (projection0) {
+void InstructionSelectorT<Adapter>::MarkPairProjectionsAsWord32(node_t node) {
+  node_t projection0 = FindProjection(node, 0);
+  if (Adapter::valid(projection0)) {
     MarkAsWord32(projection0);
   }
-  Node* projection1 = NodeProperties::FindProjection(node, 1);
-  if (projection1) {
+  node_t projection1 = FindProjection(node, 1);
+  if (Adapter::valid(projection1)) {
     MarkAsWord32(projection1);
   }
 }
@@ -1674,21 +1674,20 @@ void InstructionSelectorT<Adapter>::VisitStackPointerGreaterThan(node_t node) {
   VisitStackPointerGreaterThan(node, &cont);
 }
 
-template <>
-void InstructionSelectorT<TurbofanAdapter>::VisitLoadStackCheckOffset(
-    Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadStackCheckOffset(node_t node) {
   OperandGenerator g(this);
   Emit(kArchStackCheckOffset, g.DefineAsRegister(node));
 }
 
-template <>
-void InstructionSelectorT<TurbofanAdapter>::VisitLoadFramePointer(Node* node) {
+template <typename Adapter>
+void InstructionSelectorT<Adapter>::VisitLoadFramePointer(node_t node) {
   OperandGenerator g(this);
   Emit(kArchFramePointer, g.DefineAsRegister(node));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitLoadParentFramePointer(Node* node) {
+void InstructionSelectorT<Adapter>::VisitLoadParentFramePointer(node_t node) {
   OperandGenerator g(this);
   Emit(kArchParentFramePointer, g.DefineAsRegister(node));
 }
@@ -1877,12 +1876,7 @@ VISIT_UNSUPPORTED_OP(Word64Rol)
 VISIT_UNSUPPORTED_OP(Word64Ror)
 VISIT_UNSUPPORTED_OP(Word64Clz)
 VISIT_UNSUPPORTED_OP(Word64Ctz)
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord64ReverseBits(Node* node) {
-  UNIMPLEMENTED();
-}
-
+VISIT_UNSUPPORTED_OP(Word64ReverseBits)
 VISIT_UNSUPPORTED_OP(Word64Popcnt)
 VISIT_UNSUPPORTED_OP(Word64Equal)
 VISIT_UNSUPPORTED_OP(Int64Add)
@@ -1923,44 +1917,18 @@ VISIT_UNSUPPORTED_OP(BitcastFloat64ToInt64)
 VISIT_UNSUPPORTED_OP(BitcastInt64ToFloat64)
 VISIT_UNSUPPORTED_OP(SignExtendWord8ToInt64)
 VISIT_UNSUPPORTED_OP(SignExtendWord16ToInt64)
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitSignExtendWord32ToInt64(Node* node) {
-  UNIMPLEMENTED();
-}
+VISIT_UNSUPPORTED_OP(SignExtendWord32ToInt64)
 #endif  // V8_TARGET_ARCH_32_BIT
 
 // 64 bit targets do not implement the following instructions.
 #if V8_TARGET_ARCH_64_BIT
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairAdd(Node* node) {
-  UNIMPLEMENTED();
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairSub(Node* node) {
-  UNIMPLEMENTED();
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32PairMul(Node* node) {
-  UNIMPLEMENTED();
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShl(Node* node) {
-  UNIMPLEMENTED();
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairShr(Node* node) {
-  UNIMPLEMENTED();
-}
-
-template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32PairSar(Node* node) {
-  UNIMPLEMENTED();
-}
+VISIT_UNSUPPORTED_OP(Int32PairAdd)
+VISIT_UNSUPPORTED_OP(Int32PairSub)
+VISIT_UNSUPPORTED_OP(Int32PairMul)
+VISIT_UNSUPPORTED_OP(Word32PairShl)
+VISIT_UNSUPPORTED_OP(Word32PairShr)
+VISIT_UNSUPPORTED_OP(Word32PairSar)
+VISIT_UNSUPPORTED_OP(BitcastWord32PairToFloat64)
 #endif  // V8_TARGET_ARCH_64_BIT
 
 #if !V8_TARGET_ARCH_IA32 && !V8_TARGET_ARCH_ARM && !V8_TARGET_ARCH_RISCV32
@@ -2354,41 +2322,38 @@ void InstructionSelectorT<Adapter>::VisitCall(node_t node, block_t handler) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
-  if constexpr (Adapter::IsTurboshaft) {
-    // TODO(nicohartmann@): Implement for Turboshaft.
-    UNIMPLEMENTED();
-  } else {
-    OperandGenerator g(this);
-
-    auto caller = linkage()->GetIncomingDescriptor();
-    auto callee = CallDescriptorOf(node->op());
-    DCHECK(caller->CanTailCall(callee));
-    const int stack_param_delta = callee->GetStackParameterDelta(caller);
-    CallBuffer buffer(zone(), callee, nullptr);
-
-    // Compute InstructionOperands for inputs and outputs.
-    CallBufferFlags flags(kCallCodeImmediate | kCallTail);
-    if (IsTailCallAddressImmediate()) {
-      flags |= kCallAddressImmediate;
-    }
-    if (callee->flags() & CallDescriptor::kFixedTargetRegister) {
-      flags |= kCallFixedTargetRegister;
-    }
-    InitializeCallBuffer(node, &buffer, flags, stack_param_delta);
-    UpdateMaxPushedArgumentCount(stack_param_delta);
-
-    // Select the appropriate opcode based on the call type.
-    InstructionCode opcode;
-    InstructionOperandVector temps(zone());
-    switch (callee->kind()) {
-      case CallDescriptor::kCallCodeObject:
-        opcode = kArchTailCallCodeObject;
-        break;
-      case CallDescriptor::kCallAddress:
-        DCHECK(!caller->IsJSFunctionCall());
-        opcode = kArchTailCallAddress;
-        break;
+void InstructionSelectorT<Adapter>::VisitTailCall(node_t node) {
+  OperandGenerator g(this);
+
+  auto call = this->call_view(node);
+  auto caller = linkage()->GetIncomingDescriptor();
+  auto callee = call.call_descriptor();
+  DCHECK(caller->CanTailCall(callee));
+  const int stack_param_delta = callee->GetStackParameterDelta(caller);
+  CallBuffer buffer(zone(), callee, nullptr);
+
+  // Compute InstructionOperands for inputs and outputs.
+  CallBufferFlags flags(kCallCodeImmediate | kCallTail);
+  if (IsTailCallAddressImmediate()) {
+    flags |= kCallAddressImmediate;
+  }
+  if (callee->flags() & CallDescriptor::kFixedTargetRegister) {
+    flags |= kCallFixedTargetRegister;
+  }
+  InitializeCallBuffer(node, &buffer, flags, stack_param_delta);
+  UpdateMaxPushedArgumentCount(stack_param_delta);
+
+  // Select the appropriate opcode based on the call type.
+  InstructionCode opcode;
+  InstructionOperandVector temps(zone());
+  switch (callee->kind()) {
+    case CallDescriptor::kCallCodeObject:
+      opcode = kArchTailCallCodeObject;
+      break;
+    case CallDescriptor::kCallAddress:
+      DCHECK(!caller->IsJSFunctionCall());
+      opcode = kArchTailCallAddress;
+      break;
 #if V8_ENABLE_WEBASSEMBLY
       case CallDescriptor::kCallWasmFunction:
         DCHECK(!caller->IsJSFunctionCall());
@@ -2397,7 +2362,7 @@ void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
 #endif  // V8_ENABLE_WEBASSEMBLY
       default:
         UNREACHABLE();
-    }
+  }
     opcode = EncodeCallDescriptorFlags(opcode, callee->flags());
 
     Emit(kArchPrepareTailCall, g.NoOutput());
@@ -2419,7 +2384,6 @@ void InstructionSelectorT<Adapter>::VisitTailCall(Node* node) {
     Emit(opcode, 0, nullptr, buffer.instruction_args.size(),
          &buffer.instruction_args.front(), temps.size(),
          temps.empty() ? nullptr : &temps.front());
-  }
 }
 
 template <typename Adapter>
@@ -2629,15 +2593,11 @@ void InstructionSelectorT<Adapter>::VisitDeoptimizeUnless(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitSelect(Node* node) {
-  if constexpr (Adapter::IsTurboshaft) {
-    // TODO(nicohartmann@): Implement for Turboshaft.
-    UNIMPLEMENTED();
-  } else {
-    FlagsContinuation cont = FlagsContinuation::ForSelect(
-        kNotEqual, node, node->InputAt(1), node->InputAt(2));
-    VisitWordCompareZero(node, node->InputAt(0), &cont);
-  }
+void InstructionSelectorT<Adapter>::VisitSelect(node_t node) {
+  DCHECK_EQ(this->value_input_count(node), 3);
+  FlagsContinuation cont = FlagsContinuation::ForSelect(
+      kNotEqual, node, this->input_at(node, 1), this->input_at(node, 2));
+  VisitWordCompareZero(node, this->input_at(node, 0), &cont);
 }
 
 template <typename Adapter>
@@ -2717,7 +2677,7 @@ void InstructionSelectorT<Adapter>::VisitDeadValue(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitComment(Node* node) {
+void InstructionSelectorT<Adapter>::VisitComment(node_t node) {
   OperandGenerator g(this);
   InstructionOperand operand(g.UseImmediate(node));
   Emit(kArchComment, 0, nullptr, 1, &operand);
@@ -2736,10 +2696,10 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitControl(block_t block) {
 #ifdef DEBUG
   // SSA deconstruction requires targets of branches not to have phis.
   // Edge split form guarantees this property, but is more strict.
-  if (auto successors = turboshaft::SuccessorBlocks(
-          block->LastOperation(*turboshaft_graph()));
+  if (auto successors =
+          SuccessorBlocks(block->LastOperation(*turboshaft_graph()));
       successors.size() > 1) {
-    for (turboshaft::Block* successor : successors) {
+    for (Block* successor : successors) {
       if (successor->HasPhis(*turboshaft_graph())) {
         std::ostringstream str;
         str << "You might have specified merged variables for a label with "
@@ -2760,6 +2720,9 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitControl(block_t block) {
     case Opcode::kReturn:
       VisitReturn(node);
       break;
+    case Opcode::kTailCall:
+      VisitTailCall(node);
+      break;
     case Opcode::kDeoptimize: {
       const DeoptimizeOp& deoptimize = op.Cast<DeoptimizeOp>();
       VisitDeoptimize(deoptimize.parameters->reason(), node.id(),
@@ -4243,6 +4206,7 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitNode(
     case Opcode::kBranch:
     case Opcode::kGoto:
     case Opcode::kReturn:
+    case Opcode::kTailCall:
     case Opcode::kUnreachable:
     case Opcode::kDeoptimize:
     case Opcode::kSwitch:
@@ -4846,28 +4810,25 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitNode(
       UNREACHABLE();
     }
     case Opcode::kLoad: {
-      MachineRepresentation rep = op.Cast<turboshaft::LoadOp>()
-                                      .loaded_rep.ToMachineType()
-                                      .representation();
+      MachineRepresentation rep =
+          op.Cast<LoadOp>().loaded_rep.ToMachineType().representation();
       MarkAsRepresentation(rep, node);
       return VisitLoad(node);
     }
     case Opcode::kStore:
       return VisitStore(node);
     case Opcode::kTaggedBitcast: {
-      const turboshaft::TaggedBitcastOp& cast =
-          op.Cast<turboshaft::TaggedBitcastOp>();
-      if (cast.from == turboshaft::RegisterRepresentation::Tagged() &&
-          cast.to == turboshaft::RegisterRepresentation::PointerSized()) {
+      const TaggedBitcastOp& cast = op.Cast<TaggedBitcastOp>();
+      if (cast.from == RegisterRepresentation::Tagged() &&
+          cast.to == RegisterRepresentation::PointerSized()) {
         MarkAsRepresentation(MachineType::PointerRepresentation(), node);
         return VisitBitcastTaggedToWord(node);
       } else if (cast.from.IsWord() &&
-                 cast.to == turboshaft::RegisterRepresentation::Tagged()) {
+                 cast.to == RegisterRepresentation::Tagged()) {
         MarkAsTagged(node);
         return VisitBitcastWordToTagged(node);
-      } else if (cast.from ==
-                     turboshaft::RegisterRepresentation::Compressed() &&
-                 cast.to == turboshaft::RegisterRepresentation::Word32()) {
+      } else if (cast.from == RegisterRepresentation::Compressed() &&
+                 cast.to == RegisterRepresentation::Word32()) {
         MarkAsRepresentation(MachineType::PointerRepresentation(), node);
         return VisitBitcastTaggedToWord(node);
       } else {
@@ -4875,15 +4836,22 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitNode(
       }
     }
     case Opcode::kPhi:
-      MarkAsRepresentation(op.Cast<turboshaft::PhiOp>().rep, node);
+      MarkAsRepresentation(op.Cast<PhiOp>().rep, node);
       return VisitPhi(node);
     case Opcode::kProjection:
       return VisitProjection(node);
     case Opcode::kDeoptimizeIf:
-      if (Get(node).Cast<turboshaft::DeoptimizeIfOp>().negated) {
+      if (Get(node).Cast<DeoptimizeIfOp>().negated) {
         return VisitDeoptimizeUnless(node);
       }
       return VisitDeoptimizeIf(node);
+    case Opcode::kTrapIf: {
+      const TrapIfOp& trap_if = op.Cast<TrapIfOp>();
+      if (trap_if.negated) {
+        return VisitTrapUnless(node, trap_if.trap_id);
+      }
+      return VisitTrapIf(node, trap_if.trap_id);
+    }
     case Opcode::kCatchBlockBegin:
       MarkAsTagged(node);
       return VisitIfException(node);
@@ -4905,28 +4873,55 @@ void InstructionSelectorT<TurboshaftAdapter>::VisitNode(
       return;
     case Opcode::kDebugBreak:
       return VisitDebugBreak(node);
+    case Opcode::kSelect: {
+      const SelectOp& select = op.Cast<SelectOp>();
+      // If there is a Select, then it should only be one that is supported by
+      // the machine, and it should be meant to be implementation with cmove.
+      DCHECK_EQ(select.implem, SelectOp::Implementation::kCMove);
+      MarkAsRepresentation(select.rep, node);
+      return VisitSelect(node);
+    }
+    case Opcode::kWord32PairBinop: {
+      const Word32PairBinopOp& binop = op.Cast<Word32PairBinopOp>();
+      MarkAsWord32(node);
+      MarkPairProjectionsAsWord32(node);
+      switch (binop.kind) {
+        case Word32PairBinopOp::Kind::kAdd:
+          return VisitInt32PairAdd(node);
+        case Word32PairBinopOp::Kind::kSub:
+          return VisitInt32PairSub(node);
+        case Word32PairBinopOp::Kind::kMul:
+          return VisitInt32PairMul(node);
+        case Word32PairBinopOp::Kind::kShiftLeft:
+          return VisitWord32PairShl(node);
+        case Word32PairBinopOp::Kind::kShiftRightLogical:
+          return VisitWord32PairShr(node);
+        case Word32PairBinopOp::Kind::kShiftRightArithmetic:
+          return VisitWord32PairSar(node);
+      }
+      UNREACHABLE();
+    }
+    case Opcode::kBitcastWord32PairToFloat64:
+      return VisitBitcastWord32PairToFloat64(node);
+
+#define UNIMPLEMENTED_CASE(op) case Opcode::k##op:
+      TURBOSHAFT_WASM_OPERATION_LIST(UNIMPLEMENTED_CASE)
+      TURBOSHAFT_WASM_GC_OPERATION_LIST(UNIMPLEMENTED_CASE)
+      TURBOSHAFT_SIMD_OPERATION_LIST(UNIMPLEMENTED_CASE)
+#undef UNIMPLEMENTED_CASE
+    case Opcode::kAtomicRMW: {
+      const std::string op_string = op.ToString();
+      PrintF("\033[31mNo ISEL support for: %s\033[m\n", op_string.c_str());
+      FATAL("Unexpected operation #%d:%s", node.id(), op_string.c_str());
+    }
 
-    case Opcode::kAtomicRMW:
 #define UNREACHABLE_CASE(op) case Opcode::k##op:
       TURBOSHAFT_SIMPLIFIED_OPERATION_LIST(UNREACHABLE_CASE)
       TURBOSHAFT_OTHER_OPERATION_LIST(UNREACHABLE_CASE)
-      TURBOSHAFT_WASM_OPERATION_LIST(UNREACHABLE_CASE)
-      TURBOSHAFT_WASM_GC_OPERATION_LIST(UNREACHABLE_CASE)
-      TURBOSHAFT_SIMD_OPERATION_LIST(UNREACHABLE_CASE)
       UNREACHABLE_CASE(PendingLoopPhi)
       UNREACHABLE_CASE(Tuple)
       UNREACHABLE();
 #undef UNREACHABLE_CASE
-
-    case Opcode::kTailCall:
-    case Opcode::kWord32PairBinop:
-    case Opcode::kBitcastWord32PairToFloat64:
-    case Opcode::kSelect:
-    case Opcode::kTrapIf: {
-      const std::string op_string = op.ToString();
-      PrintF("\033[31mNo ISEL support for: %s\033[m\n", op_string.c_str());
-      FATAL("Unexpected operation #%d:%s", node.id(), op_string.c_str());
-    }
   }
 }
 
diff --git a/src/compiler/backend/instruction-selector.h b/src/compiler/backend/instruction-selector.h
index 33b8e056917..3e94f1ad063 100644
--- a/src/compiler/backend/instruction-selector.h
+++ b/src/compiler/backend/instruction-selector.h
@@ -886,13 +886,34 @@ class InstructionSelectorT final : public Adapter {
   DECLARE_GENERATOR_T(TryTruncateFloat64ToUint64)
   DECLARE_GENERATOR_T(TryTruncateFloat64ToInt32)
   DECLARE_GENERATOR_T(TryTruncateFloat64ToUint32)
+  DECLARE_GENERATOR_T(Int32PairAdd)
+  DECLARE_GENERATOR_T(Int32PairSub)
+  DECLARE_GENERATOR_T(Int32PairMul)
+  DECLARE_GENERATOR_T(Word32PairShl)
+  DECLARE_GENERATOR_T(Word32PairShr)
+  DECLARE_GENERATOR_T(Word32PairSar)
+  DECLARE_GENERATOR_T(Float64InsertLowWord32)
+  DECLARE_GENERATOR_T(Float64InsertHighWord32)
+  DECLARE_GENERATOR_T(Comment)
+  DECLARE_GENERATOR_T(Word32ReverseBits)
+  DECLARE_GENERATOR_T(Word64ReverseBits)
+  DECLARE_GENERATOR_T(AbortCSADcheck)
+  DECLARE_GENERATOR_T(StorePair)
+  DECLARE_GENERATOR_T(UnalignedLoad)
+  DECLARE_GENERATOR_T(UnalignedStore)
+  DECLARE_GENERATOR_T(Int32AbsWithOverflow)
+  DECLARE_GENERATOR_T(Int64AbsWithOverflow)
+  DECLARE_GENERATOR_T(TruncateFloat64ToUint32)
+  DECLARE_GENERATOR_T(SignExtendWord32ToInt64)
+  DECLARE_GENERATOR_T(TraceInstruction)
+  DECLARE_GENERATOR_T(MemoryBarrier)
+  DECLARE_GENERATOR_T(LoadStackCheckOffset)
+  DECLARE_GENERATOR_T(LoadFramePointer)
+  DECLARE_GENERATOR_T(LoadParentFramePointer)
+  DECLARE_GENERATOR_T(ProtectedLoad)
 #undef DECLARE_GENERATOR_T
 
 #define DECLARE_GENERATOR(x) void Visit##x(Node* node);
-  DECLARE_GENERATOR(Int32AbsWithOverflow)
-  DECLARE_GENERATOR(Word32ReverseBits)
-  DECLARE_GENERATOR(Word64RolLowerable)
-  DECLARE_GENERATOR(Word64RorLowerable)
   DECLARE_GENERATOR(Word32AtomicLoad)
   DECLARE_GENERATOR(Word32AtomicExchange)
   DECLARE_GENERATOR(Word32AtomicCompareExchange)
@@ -918,41 +939,7 @@ class InstructionSelectorT final : public Adapter {
   DECLARE_GENERATOR(Word64AtomicXor)
   DECLARE_GENERATOR(Word64AtomicExchange)
   DECLARE_GENERATOR(Word64AtomicCompareExchange)
-  DECLARE_GENERATOR(AbortCSADcheck)
-  DECLARE_GENERATOR(Comment)
-  DECLARE_GENERATOR(LoadImmutable)
-  DECLARE_GENERATOR(StorePair)
-  DECLARE_GENERATOR(Word64ClzLowerable)
-  DECLARE_GENERATOR(Word64CtzLowerable)
-  DECLARE_GENERATOR(Word64ReverseBits)
   DECLARE_GENERATOR(Simd128ReverseBytes)
-  DECLARE_GENERATOR(Int64AbsWithOverflow)
-  DECLARE_GENERATOR(BitcastTaggedToWordForTagAndSmiBits)
-  DECLARE_GENERATOR(BitcastWordToTaggedSigned)
-  DECLARE_GENERATOR(TruncateFloat64ToUint32)
-  DECLARE_GENERATOR(Float64InsertLowWord32)
-  DECLARE_GENERATOR(Float64InsertHighWord32)
-  DECLARE_GENERATOR(Word32Select)
-  DECLARE_GENERATOR(Word64Select)
-  DECLARE_GENERATOR(Float32Select)
-  DECLARE_GENERATOR(Float64Select)
-  DECLARE_GENERATOR(LoadStackCheckOffset)
-  DECLARE_GENERATOR(LoadFramePointer)
-  DECLARE_GENERATOR(LoadParentFramePointer)
-  DECLARE_GENERATOR(UnalignedLoad)
-  DECLARE_GENERATOR(UnalignedStore)
-  DECLARE_GENERATOR(Int32PairAdd)
-  DECLARE_GENERATOR(Int32PairSub)
-  DECLARE_GENERATOR(Int32PairMul)
-  DECLARE_GENERATOR(Word32PairShl)
-  DECLARE_GENERATOR(Word32PairShr)
-  DECLARE_GENERATOR(Word32PairSar)
-  DECLARE_GENERATOR(ProtectedLoad)
-  DECLARE_GENERATOR(LoadTrapOnNull)
-  DECLARE_GENERATOR(StoreTrapOnNull)
-  DECLARE_GENERATOR(MemoryBarrier)
-  DECLARE_GENERATOR(SignExtendWord32ToInt64)
-  DECLARE_GENERATOR(TraceInstruction)
   MACHINE_SIMD128_OP_LIST(DECLARE_GENERATOR)
   MACHINE_SIMD256_OP_LIST(DECLARE_GENERATOR)
 #undef DECLARE_GENERATOR
@@ -974,19 +961,20 @@ class InstructionSelectorT final : public Adapter {
   void VisitDynamicCheckMapsWithDeoptUnless(Node* node);
   void VisitTrapIf(node_t node, TrapId trap_id);
   void VisitTrapUnless(node_t node, TrapId trap_id);
-  void VisitTailCall(Node* call);
+  void VisitTailCall(node_t call);
   void VisitGoto(block_t target);
   void VisitBranch(node_t input, block_t tbranch, block_t fbranch);
   void VisitSwitch(node_t node, const SwitchInfo& sw);
   void VisitDeoptimize(DeoptimizeReason reason, id_t node_id,
                        FeedbackSource const& feedback, node_t frame_state);
-  void VisitSelect(Node* node);
+  void VisitSelect(node_t node);
   void VisitReturn(node_t node);
   void VisitThrow(Node* node);
   void VisitRetain(node_t node);
   void VisitUnreachable(node_t node);
   void VisitStaticAssert(Node* node);
   void VisitDeadValue(Node* node);
+  void VisitBitcastWord32PairToFloat64(node_t node);
 
   void TryPrepareScheduleFirstProjection(node_t maybe_projection);
 
@@ -1073,7 +1061,7 @@ class InstructionSelectorT final : public Adapter {
   }
   bool instruction_selection_failed() { return instruction_selection_failed_; }
 
-  void MarkPairProjectionsAsWord32(Node* node);
+  void MarkPairProjectionsAsWord32(node_t node);
   bool IsSourcePositionUsed(node_t node);
   DECLARE_UNREACHABLE_TURBOSHAFT_FALLBACK(void,
                                           VisitWord32AtomicBinaryOperation)
diff --git a/src/compiler/backend/x64/instruction-selector-x64.cc b/src/compiler/backend/x64/instruction-selector-x64.cc
index 3fad1efe741..d5a160b985d 100644
--- a/src/compiler/backend/x64/instruction-selector-x64.cc
+++ b/src/compiler/backend/x64/instruction-selector-x64.cc
@@ -945,10 +945,15 @@ ArchOpcode GetSeqCstStoreOpcode(StoreRepresentation store_rep) {
 }  // namespace
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitTraceInstruction(Node* node) {
-  X64OperandGeneratorT<Adapter> g(this);
-  uint32_t markid = OpParameter<uint32_t>(node->op());
-  Emit(kX64TraceInstruction, g.Use(node), g.UseImmediate(markid));
+void InstructionSelectorT<Adapter>::VisitTraceInstruction(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // Currently not used by Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    X64OperandGeneratorT<Adapter> g(this);
+    uint32_t markid = OpParameter<uint32_t>(node->op());
+    Emit(kX64TraceInstruction, g.Use(node), g.UseImmediate(markid));
+  }
 }
 
 template <typename Adapter>
@@ -962,9 +967,11 @@ void InstructionSelectorT<Adapter>::VisitStackSlot(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(Node* node) {
+void InstructionSelectorT<Adapter>::VisitAbortCSADcheck(node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
-  Emit(kArchAbortCSADcheck, g.NoOutput(), g.UseFixed(node->InputAt(0), rdx));
+  DCHECK_EQ(this->value_input_count(node), 1);
+  Emit(kArchAbortCSADcheck, g.NoOutput(),
+       g.UseFixed(this->input_at(node, 0), rdx));
 }
 
 template <typename Adapter>
@@ -1138,7 +1145,7 @@ void InstructionSelectorT<Adapter>::VisitLoad(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitProtectedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitProtectedLoad(node_t node) {
   VisitLoad(node);
 }
 
@@ -1295,7 +1302,7 @@ void VisitStoreCommon(InstructionSelectorT<Adapter>* selector,
 }  // namespace
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitStorePair(Node* node) {
+void InstructionSelectorT<Adapter>::VisitStorePair(node_t node) {
   UNREACHABLE();
 }
 
@@ -1311,13 +1318,13 @@ void InstructionSelectorT<Adapter>::VisitProtectedStore(node_t node) {
 
 // Architecture supports unaligned access, therefore VisitLoad is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedLoad(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedLoad(node_t node) {
   UNREACHABLE();
 }
 
 // Architecture supports unaligned access, therefore VisitStore is used instead
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitUnalignedStore(Node* node) {
+void InstructionSelectorT<Adapter>::VisitUnalignedStore(node_t node) {
   UNREACHABLE();
 }
 
@@ -1985,12 +1992,12 @@ void InstructionSelectorT<Adapter>::VisitWord64Ror(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(Node* node) {
+void InstructionSelectorT<Adapter>::VisitWord32ReverseBits(node_t node) {
   UNREACHABLE();
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitWord64ReverseBits(Node* node) {
+void InstructionSelectorT<Adapter>::VisitWord64ReverseBits(node_t node) {
   UNREACHABLE();
 }
 
@@ -2735,10 +2742,6 @@ void InstructionSelectorT<Adapter>::VisitChangeUint32ToUint64(node_t node) {
 
 namespace {
 
-void VisitRO(InstructionSelectorT<TurboshaftAdapter>*, Node*, InstructionCode) {
-  UNREACHABLE();
-}
-
 template <typename Adapter>
 void VisitRO(InstructionSelectorT<Adapter>* selector,
              typename Adapter::node_t node, InstructionCode opcode) {
@@ -2888,11 +2891,9 @@ void VisitFloatUnop(InstructionSelectorT<Adapter>* selector,
   V(TruncateFloat32ToInt32, kSSEFloat32ToInt32)                        \
   V(TruncateFloat32ToUint32, kSSEFloat32ToUint32)
 
-#define RO_OP_LIST(V)                                                    \
-  V(TruncateFloat64ToUint32, kSSEFloat64ToUint32 | MiscField::encode(0)) \
-  V(SignExtendWord32ToInt64, kX64Movsxlq)
-
 #define RR_OP_T_LIST(V)                                                       \
+  V(TruncateFloat64ToUint32, kSSEFloat64ToUint32 | MiscField::encode(0))      \
+  V(SignExtendWord32ToInt64, kX64Movsxlq)                                     \
   V(Float32RoundDown, kSSEFloat32Round | MiscField::encode(kRoundDown))       \
   V(Float64RoundDown, kSSEFloat64Round | MiscField::encode(kRoundDown))       \
   V(Float32RoundUp, kSSEFloat32Round | MiscField::encode(kRoundUp))           \
@@ -2913,15 +2914,6 @@ void VisitFloatUnop(InstructionSelectorT<Adapter>* selector,
   V(F64x2Trunc, kX64F64x2Round | MiscField::encode(kRoundToZero))         \
   V(F64x2NearestInt, kX64F64x2Round | MiscField::encode(kRoundToNearest))
 
-#define RO_VISITOR(Name, opcode)                                \
-  template <typename Adapter>                                   \
-  void InstructionSelectorT<Adapter>::Visit##Name(Node* node) { \
-    VisitRO(this, node, opcode);                                \
-  }
-RO_OP_LIST(RO_VISITOR)
-#undef RO_VISITOR
-#undef RO_OP_LIST
-
 #define RO_VISITOR(Name, opcode)                                 \
   template <typename Adapter>                                    \
   void InstructionSelectorT<Adapter>::Visit##Name(node_t node) { \
@@ -4360,25 +4352,32 @@ void InstructionSelectorT<Adapter>::VisitFloat64LessThanOrEqual(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64InsertLowWord32(node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
-  Float64Matcher mleft(left);
-  if (mleft.HasResolvedValue() &&
-      (base::bit_cast<uint64_t>(mleft.ResolvedValue()) >> 32) == 0u) {
-    Emit(kSSEFloat64LoadLowWord32, g.DefineAsRegister(node), g.Use(right));
-    return;
+  DCHECK_EQ(this->value_input_count(node), 2);
+  node_t left = this->input_at(node, 0);
+  node_t right = this->input_at(node, 1);
+  if constexpr (Adapter::IsTurboshaft) {
+    // TODO(nicohartmann@): Might want to provide this optimization for
+    // turboshaft.
+  } else {
+    Float64Matcher mleft(left);
+    if (mleft.HasResolvedValue() &&
+        (base::bit_cast<uint64_t>(mleft.ResolvedValue()) >> 32) == 0u) {
+      Emit(kSSEFloat64LoadLowWord32, g.DefineAsRegister(node), g.Use(right));
+      return;
+    }
   }
   Emit(kSSEFloat64InsertLowWord32, g.DefineSameAsFirst(node),
        g.UseRegister(left), g.Use(right));
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(Node* node) {
+void InstructionSelectorT<Adapter>::VisitFloat64InsertHighWord32(node_t node) {
   X64OperandGeneratorT<Adapter> g(this);
-  Node* left = node->InputAt(0);
-  Node* right = node->InputAt(1);
+  DCHECK_EQ(this->value_input_count(node), 2);
+  node_t left = this->input_at(node, 0);
+  node_t right = this->input_at(node, 1);
   Emit(kSSEFloat64InsertHighWord32, g.DefineSameAsFirst(node),
        g.UseRegister(left), g.Use(right));
 }
@@ -4392,16 +4391,21 @@ void InstructionSelectorT<Adapter>::VisitFloat64SilenceNaN(node_t node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitMemoryBarrier(Node* node) {
-  // x64 is no weaker than release-acquire and only needs to emit an instruction
-  // for SeqCst memory barriers.
-  AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
-  if (order == AtomicMemoryOrder::kSeqCst) {
-    X64OperandGeneratorT<Adapter> g(this);
-    Emit(kX64MFence, g.NoOutput());
-    return;
+void InstructionSelectorT<Adapter>::VisitMemoryBarrier(node_t node) {
+  if constexpr (Adapter::IsTurboshaft) {
+    // Currently not used by Turboshaft.
+    UNIMPLEMENTED();
+  } else {
+    // x64 is no weaker than release-acquire and only needs to emit an
+    // instruction for SeqCst memory barriers.
+    AtomicMemoryOrder order = OpParameter<AtomicMemoryOrder>(node->op());
+    if (order == AtomicMemoryOrder::kSeqCst) {
+      X64OperandGeneratorT<Adapter> g(this);
+      Emit(kX64MFence, g.NoOutput());
+      return;
+    }
+    DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
   }
-  DCHECK_EQ(AtomicMemoryOrder::kAcqRel, order);
 }
 
 template <typename Adapter>
@@ -5404,12 +5408,12 @@ void InstructionSelectorT<Adapter>::VisitI32x8UConvertF32x8(Node* node) {
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt32AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
 template <typename Adapter>
-void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(Node* node) {
+void InstructionSelectorT<Adapter>::VisitInt64AbsWithOverflow(node_t node) {
   UNREACHABLE();
 }
 
diff --git a/src/compiler/turboshaft/operations.h b/src/compiler/turboshaft/operations.h
index ba334fb94f0..adedb5ea00e 100644
--- a/src/compiler/turboshaft/operations.h
+++ b/src/compiler/turboshaft/operations.h
@@ -3572,6 +3572,7 @@ inline base::SmallVector<Block*, 4> SuccessorBlocks(const Operation& op) {
       return {casted.if_true, casted.if_false};
     }
     case Opcode::kReturn:
+    case Opcode::kTailCall:
     case Opcode::kDeoptimize:
     case Opcode::kUnreachable:
       return base::SmallVector<Block*, 4>{};
-- 
2.35.1

