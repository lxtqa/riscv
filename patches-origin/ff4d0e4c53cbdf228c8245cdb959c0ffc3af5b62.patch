From ff4d0e4c53cbdf228c8245cdb959c0ffc3af5b62 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Samuel=20Gro=C3=9F?= <saelo@chromium.org>
Date: Fri, 23 Jun 2023 13:21:46 +0000
Subject: [PATCH] [sandbox] Introduce ExternalPointerTable Spaces
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

A Space is a list of table segments (currently 64kb chunks) that share
the same freelist. A space is therefore the unit on which entry
allocation and garbage collection happen.

Currently, there is only one space per table, but in the future we will
have multiple spaces per table.
Some of the things that this enables include:

1. It allows adding minor-gc support for the ExternalPointerTable: we
   create a new "YoungGenSpace" for the EPT and use it for allocating
   entries whenever the owning object is in the young generation. We can
   then sweep this space independently and promote live objects to the
   regular space.

2. It allows using a single table for multiple client Isolates. This
   will likely be needed for the CodePointerTable: there is currently
   one CPT per process and so every client Isolate will get its own
   space in it.

3. It makes it possible to support EPT entries containing more than a
   single pointer. For example, for things like external strings it
   might make sense to store both the data pointer and the size in a
   single entry since both need to be in sync and must not be modified
   by an attacker. This now becomes possible:
      1. Add a new EPT::Space in which every entry will be 16 bytes
         large (i.e. two pointer-sized words)
      2. Allocate a new tag for the size field of the new entries
      3. Store the pointer and size together in one entry and make sure
         to tag both values

While this change is a net increases in lines of code, mostly due to the
new structs and because it now requires a Space to be passed into the
entry allocation routines, it also simplifies/unifies a few things:
* The compaction threshold can now move into the EPT::Space struct, no
  need for an extra_ field in the ExternalEntityTable class.
* The table memory managements (i.e. of the table's segments) becomes a
  little simpler as it now uses a VirtualAddressSubspace, which
  automatically takes care of managing free table segments.
* We have slightly cleaner locking semantics: every Space has a lock for
  its segments_ set, and the space takes care of taking the lock when
  necessary. There is no longer a global table lock.
* The ExternalEntityTable class becomes smaller: it now only consists of
  a pointer to a virtual address space to manage the memory plus an
  Entry* pointer to the base of the table for entry access. There is for
  example no longer any need to keep track of the table's capacity.
* The null entry no longer needs special handling. Instead the first
  segment of a table is now mapped read-only and doesn't belong to any
  space, so will never be processed by e.g. sweeping.

Bug: v8:10391, v8:13640
Change-Id: I6b8121a7ef85876fbdc228732784eec9cc59c5fe
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/4537358
Reviewed-by: Michael Lippautz <mlippautz@chromium.org>
Reviewed-by: Igor Sheludko <ishell@chromium.org>
Commit-Queue: Samuel Gro√ü <saelo@chromium.org>
Cr-Commit-Position: refs/heads/main@{#88518}
---
 include/v8-internal.h                         |   8 +-
 src/codegen/arm64/macro-assembler-arm64.cc    |   2 +-
 src/codegen/code-stub-assembler.cc            |   4 +-
 src/codegen/external-reference.cc             |   5 +-
 .../loong64/macro-assembler-loong64.cc        |   2 +-
 src/codegen/riscv/macro-assembler-riscv.cc    |   2 +-
 src/codegen/x64/macro-assembler-x64.cc        |  12 +-
 src/compiler/memory-lowering.cc               |   2 +-
 .../turboshaft/memory-optimization-reducer.h  |   2 +-
 src/compiler/wasm-graph-assembler.cc          |   4 +-
 src/execution/isolate.cc                      |  11 +-
 src/execution/isolate.h                       |   8 +
 src/heap/heap.h                               |  12 +
 src/heap/incremental-marking.cc               |   2 +-
 src/heap/mark-compact.cc                      |  27 +-
 src/heap/marking-visitor-inl.h                |   5 +-
 src/heap/marking-visitor.h                    |   5 +-
 src/objects/js-array-buffer-inl.h             |   1 +
 src/objects/slots-inl.h                       |  11 +-
 src/objects/slots.h                           |   2 +
 src/sandbox/code-pointer-table-inl.h          |   5 +-
 src/sandbox/code-pointer-table.h              |   9 +-
 src/sandbox/external-entity-table-inl.h       | 222 +++++++------
 src/sandbox/external-entity-table.h           | 295 +++++++++++-------
 src/sandbox/external-pointer-inl.h            |  13 +-
 src/sandbox/external-pointer-table-inl.h      |  82 +++--
 src/sandbox/external-pointer-table.cc         | 213 ++++++-------
 src/sandbox/external-pointer-table.h          | 246 ++++++++-------
 28 files changed, 701 insertions(+), 511 deletions(-)

diff --git a/include/v8-internal.h b/include/v8-internal.h
index 9d163de1efe..bb61cfb6be4 100644
--- a/include/v8-internal.h
+++ b/include/v8-internal.h
@@ -566,8 +566,8 @@ class Internals {
       2 * kApiSystemPointerSize + 2 * kApiInt32Size;
 
   // ExternalPointerTable layout guarantees.
-  static const int kExternalPointerTableBufferOffset = 0;
-  static const int kExternalPointerTableSize = 4 * kApiSystemPointerSize;
+  static const int kExternalPointerTableBasePointerOffset = 0;
+  static const int kExternalPointerTableSize = 2 * kApiSystemPointerSize;
 
   // IsolateData layout guarantees.
   static const int kIsolateCageBaseOffset = 0;
@@ -816,7 +816,7 @@ class Internals {
   V8_INLINE static Address* GetExternalPointerTableBase(v8::Isolate* isolate) {
     Address addr = reinterpret_cast<Address>(isolate) +
                    kIsolateExternalPointerTableOffset +
-                   kExternalPointerTableBufferOffset;
+                   kExternalPointerTableBasePointerOffset;
     return *reinterpret_cast<Address**>(addr);
   }
 
@@ -825,7 +825,7 @@ class Internals {
     Address addr = reinterpret_cast<Address>(isolate) +
                    kIsolateSharedExternalPointerTableAddressOffset;
     addr = *reinterpret_cast<Address*>(addr);
-    addr += kExternalPointerTableBufferOffset;
+    addr += kExternalPointerTableBasePointerOffset;
     return *reinterpret_cast<Address**>(addr);
   }
 #endif
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index e0f78c08455..6bcd96e296b 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -3438,7 +3438,7 @@ void MacroAssembler::LoadExternalPointerField(Register destination,
   Ldr(external_table,
       MemOperand(isolate_root,
                  IsolateData::external_pointer_table_offset() +
-                     Internals::kExternalPointerTableBufferOffset));
+                     Internals::kExternalPointerTableBasePointerOffset));
   Ldr(destination.W(), field_operand);
   Mov(destination, Operand(destination, LSR, kExternalPointerIndexShift));
   Ldr(destination, MemOperand(external_table, destination, LSL,
diff --git a/src/codegen/code-stub-assembler.cc b/src/codegen/code-stub-assembler.cc
index 2d0fff9d8df..2e45e60f8d8 100644
--- a/src/codegen/code-stub-assembler.cc
+++ b/src/codegen/code-stub-assembler.cc
@@ -1740,7 +1740,7 @@ TNode<RawPtrT> CodeStubAssembler::LoadExternalPointerFromObject(
       ExternalPointerTableAddress(tag);
   TNode<RawPtrT> table = UncheckedCast<RawPtrT>(
       Load(MachineType::Pointer(), external_pointer_table_address,
-           UintPtrConstant(Internals::kExternalPointerTableBufferOffset)));
+           UintPtrConstant(Internals::kExternalPointerTableBasePointerOffset)));
 
   TNode<ExternalPointerHandleT> handle =
       LoadObjectField<ExternalPointerHandleT>(object, offset);
@@ -1771,7 +1771,7 @@ void CodeStubAssembler::StoreExternalPointerToObject(TNode<HeapObject> object,
       ExternalPointerTableAddress(tag);
   TNode<RawPtrT> table = UncheckedCast<RawPtrT>(
       Load(MachineType::Pointer(), external_pointer_table_address,
-           UintPtrConstant(Internals::kExternalPointerTableBufferOffset)));
+           UintPtrConstant(Internals::kExternalPointerTableBasePointerOffset)));
   TNode<ExternalPointerHandleT> handle =
       LoadObjectField<ExternalPointerHandleT>(object, offset);
 
diff --git a/src/codegen/external-reference.cc b/src/codegen/external-reference.cc
index 0bc8abfcc97..e465480eb51 100644
--- a/src/codegen/external-reference.cc
+++ b/src/codegen/external-reference.cc
@@ -250,7 +250,7 @@ ExternalReference::shared_external_pointer_table_address_address(
 }
 
 ExternalReference ExternalReference::code_pointer_table_address() {
-  return ExternalReference(GetProcessWideCodePointerTable()->buffer_address());
+  return ExternalReference(GetProcessWideCodePointerTable()->base_address());
 }
 
 #endif  // V8_ENABLE_SANDBOX
@@ -364,7 +364,8 @@ ExternalPointerHandle AllocateAndInitializeExternalPointerTableEntry(
     Isolate* isolate, Address pointer) {
 #ifdef V8_ENABLE_SANDBOX
   return isolate->external_pointer_table().AllocateAndInitializeEntry(
-      pointer, kExternalObjectValueTag);
+      isolate->heap()->external_pointer_space(), pointer,
+      kExternalObjectValueTag);
 #else
   return 0;
 #endif  // V8_ENABLE_SANDBOX
diff --git a/src/codegen/loong64/macro-assembler-loong64.cc b/src/codegen/loong64/macro-assembler-loong64.cc
index 62047a52449..3777707844a 100644
--- a/src/codegen/loong64/macro-assembler-loong64.cc
+++ b/src/codegen/loong64/macro-assembler-loong64.cc
@@ -234,7 +234,7 @@ void MacroAssembler::LoadExternalPointerField(Register destination,
   Ld_d(external_table,
        MemOperand(isolate_root,
                   IsolateData::external_pointer_table_offset() +
-                      Internals::kExternalPointerTableBufferOffset));
+                      Internals::kExternalPointerTableBasePointerOffset));
   Ld_wu(destination, field_operand);
   srli_d(destination, destination, kExternalPointerIndexShift);
   slli_d(destination, destination, kExternalPointerTableEntrySizeLog2);
diff --git a/src/codegen/riscv/macro-assembler-riscv.cc b/src/codegen/riscv/macro-assembler-riscv.cc
index 6baa857fa7e..6e608d51ea6 100644
--- a/src/codegen/riscv/macro-assembler-riscv.cc
+++ b/src/codegen/riscv/macro-assembler-riscv.cc
@@ -389,7 +389,7 @@ void MacroAssembler::LoadExternalPointerField(Register destination,
   LoadWord(external_table,
            MemOperand(isolate_root,
                       IsolateData::external_pointer_table_offset() +
-                          Internals::kExternalPointerTableBufferOffset));
+                          Internals::kExternalPointerTableBasePointerOffset));
   lwu(destination, field_operand);
   srli(destination, destination, kExternalPointerIndexShift);
   slli(destination, destination, kExternalPointerTableEntrySizeLog2);
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index e64952503ae..b76e08a44d6 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -447,14 +447,16 @@ void MacroAssembler::LoadExternalPointerField(
   DCHECK(!field_operand.AddressUsesRegister(scratch));
   if (isolateRootLocation == IsolateRootLocation::kInRootRegister) {
     DCHECK(root_array_available_);
-    movq(scratch, Operand(kRootRegister,
-                          IsolateData::external_pointer_table_offset() +
-                              Internals::kExternalPointerTableBufferOffset));
+    movq(scratch,
+         Operand(kRootRegister,
+                 IsolateData::external_pointer_table_offset() +
+                     Internals::kExternalPointerTableBasePointerOffset));
   } else {
     DCHECK(isolateRootLocation == IsolateRootLocation::kInScratchRegister);
     movq(scratch,
-         Operand(scratch, IsolateData::external_pointer_table_offset() +
-                              Internals::kExternalPointerTableBufferOffset));
+         Operand(scratch,
+                 IsolateData::external_pointer_table_offset() +
+                     Internals::kExternalPointerTableBasePointerOffset));
   }
   movl(destination, field_operand);
   shrq(destination, Immediate(kExternalPointerIndexShift));
diff --git a/src/compiler/memory-lowering.cc b/src/compiler/memory-lowering.cc
index 3593c6e9616..f490b2273db 100644
--- a/src/compiler/memory-lowering.cc
+++ b/src/compiler/memory-lowering.cc
@@ -483,7 +483,7 @@ Reduction MemoryLowering::ReduceLoadExternalPointerField(Node* node) {
           : __ ExternalConstant(
                 ExternalReference::external_pointer_table_address(isolate()));
   Node* table = __ Load(MachineType::Pointer(), table_address,
-                        Internals::kExternalPointerTableBufferOffset);
+                        Internals::kExternalPointerTableBasePointerOffset);
   Node* pointer =
       __ Load(MachineType::Pointer(), table, __ ChangeUint32ToUint64(offset));
   pointer = __ WordAnd(pointer, __ IntPtrConstant(~tag));
diff --git a/src/compiler/turboshaft/memory-optimization-reducer.h b/src/compiler/turboshaft/memory-optimization-reducer.h
index f3679e7af50..b0d81b9d9a1 100644
--- a/src/compiler/turboshaft/memory-optimization-reducer.h
+++ b/src/compiler/turboshaft/memory-optimization-reducer.h
@@ -238,7 +238,7 @@ class MemoryOptimizationReducer : public Next {
             : Asm().ExternalConstant(
                   ExternalReference::external_pointer_table_address(isolate_));
     OpIndex table = Asm().LoadOffHeap(
-        table_address, Internals::kExternalPointerTableBufferOffset,
+        table_address, Internals::kExternalPointerTableBasePointerOffset,
         MemoryRepresentation::PointerSized());
     OpIndex index = Asm().ShiftRightLogical(handle, kExternalPointerIndexShift,
                                             WordRepresentation::Word32());
diff --git a/src/compiler/wasm-graph-assembler.cc b/src/compiler/wasm-graph-assembler.cc
index b917cc62084..4174244dd00 100644
--- a/src/compiler/wasm-graph-assembler.cc
+++ b/src/compiler/wasm-graph-assembler.cc
@@ -189,11 +189,11 @@ Node* WasmGraphAssembler::BuildLoadExternalPointerFromObject(
         Load(MachineType::Pointer(), isolate_root,
              IsolateData::shared_external_pointer_table_offset());
     table = Load(MachineType::Pointer(), table_address,
-                 Internals::kExternalPointerTableBufferOffset);
+                 Internals::kExternalPointerTableBasePointerOffset);
   } else {
     table = Load(MachineType::Pointer(), isolate_root,
                  IsolateData::external_pointer_table_offset() +
-                     Internals::kExternalPointerTableBufferOffset);
+                     Internals::kExternalPointerTableBasePointerOffset);
   }
   Node* decoded_ptr = Load(MachineType::Pointer(), table, offset);
   return WordAnd(decoded_ptr, IntPtrConstant(~tag));
diff --git a/src/execution/isolate.cc b/src/execution/isolate.cc
index 01c0ba8b5c5..5a4e485b6d1 100644
--- a/src/execution/isolate.cc
+++ b/src/execution/isolate.cc
@@ -3433,8 +3433,8 @@ Isolate::Isolate(std::unique_ptr<i::IsolateAllocator> isolate_allocator)
 
 void Isolate::CheckIsolateLayout() {
 #ifdef V8_ENABLE_SANDBOX
-  CHECK_EQ(static_cast<int>(OFFSET_OF(ExternalPointerTable, buffer_)),
-           Internals::kExternalPointerTableBufferOffset);
+  CHECK_EQ(static_cast<int>(OFFSET_OF(ExternalPointerTable, base_)),
+           Internals::kExternalPointerTableBasePointerOffset);
   CHECK_EQ(static_cast<int>(sizeof(ExternalPointerTable)),
            Internals::kExternalPointerTableSize);
   CHECK_EQ(static_cast<int>(sizeof(ExternalPointerTable)),
@@ -3686,6 +3686,8 @@ void Isolate::Deinit() {
     shared_external_pointer_table().TearDown();
     delete isolate_data_.shared_external_pointer_table_;
     isolate_data_.shared_external_pointer_table_ = nullptr;
+    delete shared_external_pointer_space_;
+    shared_external_pointer_space_ = nullptr;
   }
 #endif  // V8_COMPRESS_POINTERS
 
@@ -4427,10 +4429,13 @@ bool Isolate::Init(SnapshotData* startup_snapshot_data,
   if (owns_shareable_data()) {
     isolate_data_.shared_external_pointer_table_ = new ExternalPointerTable();
     shared_external_pointer_table().Init();
+    shared_external_pointer_space_ = new ExternalPointerTable::Space();
   } else {
     DCHECK(has_shared_space());
     isolate_data_.shared_external_pointer_table_ =
         shared_space_isolate()->isolate_data_.shared_external_pointer_table_;
+    shared_external_pointer_space_ =
+        shared_space_isolate()->shared_external_pointer_space_;
   }
 #endif  // V8_COMPRESS_POINTERS
 
@@ -6137,7 +6142,7 @@ ExternalPointerHandle Isolate::GetOrCreateWaiterQueueNodeExternalPointer() {
     handle = waiter_queue_node_external_pointer_handle_;
   } else {
     handle = shared_external_pointer_table().AllocateAndInitializeEntry(
-        kNullAddress, kWaiterQueueNodeTag);
+        shared_external_pointer_space(), kNullAddress, kWaiterQueueNodeTag);
     waiter_queue_node_external_pointer_handle_ = handle;
   }
   DCHECK_NE(0, handle);
diff --git a/src/execution/isolate.h b/src/execution/isolate.h
index a0bb8886b0f..721e06854a6 100644
--- a/src/execution/isolate.h
+++ b/src/execution/isolate.h
@@ -1980,6 +1980,10 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
     return *isolate_data_.shared_external_pointer_table_;
   }
 
+  ExternalPointerTable::Space* shared_external_pointer_space() {
+    return shared_external_pointer_space_;
+  }
+
   Address shared_external_pointer_table_address_address() {
     return reinterpret_cast<Address>(
         &isolate_data_.shared_external_pointer_table_);
@@ -2467,6 +2471,10 @@ class V8_EXPORT_PRIVATE Isolate final : private HiddenFactory {
   base::Optional<Isolate*> shared_space_isolate_;
 
 #ifdef V8_COMPRESS_POINTERS
+  // Stores the external pointer table space for the shared external pointer
+  // table.
+  ExternalPointerTable::Space* shared_external_pointer_space_ = nullptr;
+
   // The external pointer handle to the Isolate's main thread's WaiterQueueNode.
   // It is used to wait for JS-exposed mutex or condition variable.
   ExternalPointerHandle waiter_queue_node_external_pointer_handle_ =
diff --git a/src/heap/heap.h b/src/heap/heap.h
index 4743cd9ca0f..fee6dc04244 100644
--- a/src/heap/heap.h
+++ b/src/heap/heap.h
@@ -858,6 +858,12 @@ class Heap final {
   inline PagedSpace* paged_space(int idx) const;
   inline Space* space(int idx) const;
 
+#ifdef V8_COMPRESS_POINTERS
+  ExternalPointerTable::Space* external_pointer_space() {
+    return &external_pointer_space_;
+  }
+#endif  // V8_COMPRESS_POINTERS
+
   // ===========================================================================
   // Getters to other components. ==============================================
   // ===========================================================================
@@ -2171,6 +2177,12 @@ class Heap final {
   // Map from the space id to the space.
   std::unique_ptr<Space> space_[LAST_SPACE + 1];
 
+#ifdef V8_COMPRESS_POINTERS
+  // The space in the ExternalPointerTable containing entries owned by objects
+  // in this heap.
+  ExternalPointerTable::Space external_pointer_space_;
+#endif  // V8_COMPRESS_POINTERS
+
   LocalHeap* main_thread_local_heap_ = nullptr;
 
   // Determines whether code space is write-protected. This is essentially a
diff --git a/src/heap/incremental-marking.cc b/src/heap/incremental-marking.cc
index 0e1b014752f..f185f2664fd 100644
--- a/src/heap/incremental-marking.cc
+++ b/src/heap/incremental-marking.cc
@@ -290,7 +290,7 @@ void IncrementalMarking::StartMarkingMajor() {
       MarkCompactCollector::StartCompactionMode::kIncremental);
 
 #ifdef V8_COMPRESS_POINTERS
-  isolate()->external_pointer_table().StartCompactingIfNeeded();
+  heap_->external_pointer_space()->StartCompactingIfNeeded();
 #endif  // V8_COMPRESS_POINTERS
 
   if (heap_->cpp_heap()) {
diff --git a/src/heap/mark-compact.cc b/src/heap/mark-compact.cc
index 44357c5f31f..fc5be7fc49a 100644
--- a/src/heap/mark-compact.cc
+++ b/src/heap/mark-compact.cc
@@ -828,7 +828,7 @@ void MarkCompactCollector::Prepare() {
       CppHeap::From(heap()->cpp_heap())->StartTracing();
     }
 #ifdef V8_COMPRESS_POINTERS
-    heap_->isolate()->external_pointer_table().StartCompactingIfNeeded();
+    heap_->external_pointer_space()->StartCompactingIfNeeded();
 #endif  // V8_COMPRESS_POINTERS
   }
 
@@ -1261,8 +1261,8 @@ class ExternalStringTableCleaner : public RootVisitor {
 class MarkExternalPointerFromExternalStringTable : public RootVisitor {
  public:
   explicit MarkExternalPointerFromExternalStringTable(
-      ExternalPointerTable* shared_table)
-      : visitor(shared_table) {}
+      ExternalPointerTable* shared_table, ExternalPointerTable::Space* space)
+      : visitor(shared_table, space) {}
 
   void VisitRootPointers(Root root, const char* description,
                          FullObjectSlot start, FullObjectSlot end) override {
@@ -1285,14 +1285,15 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
  private:
   class MarkExternalPointerTableVisitor : public ObjectVisitor {
    public:
-    explicit MarkExternalPointerTableVisitor(ExternalPointerTable* table)
-        : table_(table) {}
+    explicit MarkExternalPointerTableVisitor(ExternalPointerTable* table,
+                                             ExternalPointerTable::Space* space)
+        : table_(table), space_(space) {}
     void VisitExternalPointer(HeapObject host, ExternalPointerSlot slot,
                               ExternalPointerTag tag) override {
       DCHECK_NE(tag, kExternalPointerNullTag);
       DCHECK(IsSharedExternalPointerType(tag));
       ExternalPointerHandle handle = slot.Relaxed_LoadHandle();
-      table_->Mark(handle, slot.address());
+      table_->Mark(space_, handle, slot.address());
     }
     void VisitPointers(HeapObject host, ObjectSlot start,
                        ObjectSlot end) override {
@@ -1316,6 +1317,7 @@ class MarkExternalPointerFromExternalStringTable : public RootVisitor {
 
    private:
     ExternalPointerTable* table_;
+    ExternalPointerTable::Space* space_;
   };
 
   MarkExternalPointerTableVisitor visitor;
@@ -2050,8 +2052,10 @@ void MarkCompactCollector::MarkObjectsFromClientHeap(Isolate* client) {
   // All ExternalString resources are stored in the shared external pointer
   // table. Mark entries from client heaps.
   ExternalPointerTable& shared_table = client->shared_external_pointer_table();
+  ExternalPointerTable::Space* shared_space =
+      client->shared_external_pointer_space();
   MarkExternalPointerFromExternalStringTable external_string_visitor(
-      &shared_table);
+      &shared_table, shared_space);
   heap->external_string_table_.IterateAll(&external_string_visitor);
 #endif  // V8_ENABLE_SANDBOX
 }
@@ -2068,7 +2072,8 @@ void MarkCompactCollector::MarkWaiterQueueNode(Isolate* isolate) {
   ExternalPointerHandle handle =
       base::AsAtomic32::Relaxed_Load(handle_location);
   if (handle) {
-    shared_table.Mark(handle, reinterpret_cast<Address>(handle_location));
+    shared_table.Mark(isolate->shared_external_pointer_space(), handle,
+                      reinterpret_cast<Address>(handle_location));
   }
 #endif  // V8_COMPRESS_POINTERS
 }
@@ -2916,9 +2921,11 @@ void MarkCompactCollector::ClearNonLiveReferences() {
     // External pointer table sweeping needs to happen before evacuating live
     // objects as it may perform table compaction, which requires objects to
     // still be at the same location as during marking.
-    isolate()->external_pointer_table().SweepAndCompact(isolate());
+    isolate()->external_pointer_table().SweepAndCompact(
+        isolate()->heap()->external_pointer_space(), isolate()->counters());
     if (isolate()->owns_shareable_data()) {
-      isolate()->shared_external_pointer_table().SweepAndCompact(isolate());
+      isolate()->shared_external_pointer_table().SweepAndCompact(
+          isolate()->shared_external_pointer_space(), isolate()->counters());
     }
   }
 #endif  // V8_ENABLE_SANDBOX
diff --git a/src/heap/marking-visitor-inl.h b/src/heap/marking-visitor-inl.h
index ada78f2447f..3520d4eac0a 100644
--- a/src/heap/marking-visitor-inl.h
+++ b/src/heap/marking-visitor-inl.h
@@ -158,7 +158,10 @@ void MarkingVisitorBase<ConcreteVisitor, MarkingState>::VisitExternalPointer(
   ExternalPointerTable* table = IsSharedExternalPointerType(tag)
                                     ? shared_external_pointer_table_
                                     : external_pointer_table_;
-  table->Mark(handle, slot.address());
+  ExternalPointerTable::Space* space = IsSharedExternalPointerType(tag)
+                                           ? shared_external_pointer_space_
+                                           : heap_->external_pointer_space();
+  table->Mark(space, handle, slot.address());
 #endif  // V8_ENABLE_SANDBOX
 }
 
diff --git a/src/heap/marking-visitor.h b/src/heap/marking-visitor.h
index 2fee17a4f7c..caf53ebb518 100644
--- a/src/heap/marking-visitor.h
+++ b/src/heap/marking-visitor.h
@@ -61,7 +61,9 @@ class MarkingVisitorBase : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
         ,
         external_pointer_table_(&heap->isolate()->external_pointer_table()),
         shared_external_pointer_table_(
-            &heap->isolate()->shared_external_pointer_table())
+            &heap->isolate()->shared_external_pointer_table()),
+        shared_external_pointer_space_(
+            heap->isolate()->shared_external_pointer_space())
 #endif  // V8_ENABLE_SANDBOX
   {
   }
@@ -211,6 +213,7 @@ class MarkingVisitorBase : public ConcurrentHeapVisitor<int, ConcreteVisitor> {
 #ifdef V8_ENABLE_SANDBOX
   ExternalPointerTable* const external_pointer_table_;
   ExternalPointerTable* const shared_external_pointer_table_;
+  ExternalPointerTable::Space* const shared_external_pointer_space_;
 #endif  // V8_ENABLE_SANDBOX
 };
 
diff --git a/src/objects/js-array-buffer-inl.h b/src/objects/js-array-buffer-inl.h
index 61d2d53da01..4a2e5f8bb53 100644
--- a/src/objects/js-array-buffer-inl.h
+++ b/src/objects/js-array-buffer-inl.h
@@ -116,6 +116,7 @@ void JSArrayBuffer::set_extension(ArrayBufferExtension* extension) {
 
     // We need Release semantics here, see above.
     ExternalPointerHandle handle = table.AllocateAndInitializeEntry(
+        isolate->heap()->external_pointer_space(),
         reinterpret_cast<Address>(extension), kArrayBufferExtensionTag);
     base::AsAtomic32::Release_Store(extension_handle_location(), handle);
   } else {
diff --git a/src/objects/slots-inl.h b/src/objects/slots-inl.h
index de4ab2a6fce..45c5bc75cca 100644
--- a/src/objects/slots-inl.h
+++ b/src/objects/slots-inl.h
@@ -159,7 +159,8 @@ void ExternalPointerSlot::init(Isolate* isolate, Address value,
 #ifdef V8_ENABLE_SANDBOX
   DCHECK_NE(tag, kExternalPointerNullTag);
   ExternalPointerTable& table = GetExternalPointerTableForTag(isolate, tag);
-  ExternalPointerHandle handle = table.AllocateAndInitializeEntry(value, tag);
+  ExternalPointerHandle handle = table.AllocateAndInitializeEntry(
+      GetDefaultExternalPointerSpace(isolate, tag), value, tag);
   // Use a Release_Store to ensure that the store of the pointer into the
   // table is not reordered after the store of the handle. Otherwise, other
   // threads may access an uninitialized table entry and crash.
@@ -247,6 +248,14 @@ ExternalPointerTable& ExternalPointerSlot::GetExternalPointerTableForTag(
              ? isolate->shared_external_pointer_table()
              : isolate->external_pointer_table();
 }
+
+ExternalPointerTable::Space*
+ExternalPointerSlot::GetDefaultExternalPointerSpace(Isolate* isolate,
+                                                    ExternalPointerTag tag) {
+  return IsSharedExternalPointerType(tag)
+             ? isolate->shared_external_pointer_space()
+             : isolate->heap()->external_pointer_space();
+}
 #endif  // V8_ENABLE_SANDBOX
 
 //
diff --git a/src/objects/slots.h b/src/objects/slots.h
index 0815e0b54f8..4926c4e32f2 100644
--- a/src/objects/slots.h
+++ b/src/objects/slots.h
@@ -330,6 +330,8 @@ class ExternalPointerSlot
       const Isolate* isolate, ExternalPointerTag tag);
   inline ExternalPointerTable& GetExternalPointerTableForTag(
       Isolate* isolate, ExternalPointerTag tag);
+  inline ExternalPointerTable::Space* GetDefaultExternalPointerSpace(
+      Isolate* isolate, ExternalPointerTag tag);
 #endif  // V8_ENABLE_SANDBOX
 };
 
diff --git a/src/sandbox/code-pointer-table-inl.h b/src/sandbox/code-pointer-table-inl.h
index fd1c416b55c..badca3d5d5d 100644
--- a/src/sandbox/code-pointer-table-inl.h
+++ b/src/sandbox/code-pointer-table-inl.h
@@ -50,8 +50,8 @@ void CodePointerTable::Set(CodePointerHandle handle, Address value) {
 }
 
 CodePointerHandle CodePointerTable::AllocateAndInitializeEntry(
-    Address initial_value) {
-  uint32_t index = AllocateEntry();
+    Space* space, Address initial_value) {
+  uint32_t index = AllocateEntry(space);
   at(index).MakeCodePointerEntry(initial_value);
   return IndexToHandle(index);
 }
@@ -59,7 +59,6 @@ CodePointerHandle CodePointerTable::AllocateAndInitializeEntry(
 uint32_t CodePointerTable::HandleToIndex(CodePointerHandle handle) const {
   uint32_t index = handle >> kCodePointerIndexShift;
   DCHECK_EQ(handle, index << kCodePointerIndexShift);
-  DCHECK_LT(index, capacity());
   return index;
 }
 
diff --git a/src/sandbox/code-pointer-table.h b/src/sandbox/code-pointer-table.h
index aa30381c6ad..f3348da4bf3 100644
--- a/src/sandbox/code-pointer-table.h
+++ b/src/sandbox/code-pointer-table.h
@@ -72,7 +72,7 @@ class V8_EXPORT_PRIVATE CodePointerTable
                                  kCodePointerTableReservationSize> {
  public:
   // Size of a CodePointerTable, for layout computation in IsolateData.
-  static int constexpr kSize = 4 * kSystemPointerSize;
+  static int constexpr kSize = 2 * kSystemPointerSize;
   static_assert(kMaxCodePointers == kMaxCapacity);
 
   CodePointerTable() = default;
@@ -99,10 +99,11 @@ class V8_EXPORT_PRIVATE CodePointerTable
   // value and tag.
   //
   // This method is atomic and can be called from background threads.
-  inline CodePointerHandle AllocateAndInitializeEntry(Address initial_value);
+  inline CodePointerHandle AllocateAndInitializeEntry(Space* space,
+                                                      Address initial_value);
 
-  // The address of the backing buffer, for use in JIT compilers.
-  Address buffer_address() const { return reinterpret_cast<Address>(buffer_); }
+  // The base address of this table, for use in JIT compilers.
+  Address base_address() const { return reinterpret_cast<Address>(base_); }
 
  private:
   inline uint32_t HandleToIndex(CodePointerHandle handle) const;
diff --git a/src/sandbox/external-entity-table-inl.h b/src/sandbox/external-entity-table-inl.h
index cecbf6f3f9a..66d1f1c0f03 100644
--- a/src/sandbox/external-entity-table-inl.h
+++ b/src/sandbox/external-entity-table-inl.h
@@ -6,6 +6,7 @@
 #define V8_SANDBOX_EXTERNAL_ENTITY_TABLE_INL_H_
 
 #include "src/base/atomicops.h"
+#include "src/base/emulated-virtual-address-subspace.h"
 #include "src/common/assert-scope.h"
 #include "src/sandbox/external-entity-table.h"
 #include "src/utils/allocation.h"
@@ -16,32 +17,53 @@ namespace v8 {
 namespace internal {
 
 template <typename Entry, size_t size>
-Entry& ExternalEntityTable<Entry, size>::at(uint32_t index) {
-  DCHECK_LT(index, capacity());
-  return buffer_[index];
+typename ExternalEntityTable<Entry, size>::Segment
+ExternalEntityTable<Entry, size>::Segment::At(uint32_t offset) {
+  DCHECK(IsAligned(offset, kSegmentSize));
+  uint32_t number = offset / kSegmentSize;
+  return Segment(number);
 }
 
 template <typename Entry, size_t size>
-const Entry& ExternalEntityTable<Entry, size>::at(uint32_t index) const {
-  DCHECK_LT(index, capacity());
-  return buffer_[index];
+typename ExternalEntityTable<Entry, size>::Segment
+ExternalEntityTable<Entry, size>::Segment::Containing(uint32_t entry_index) {
+  uint32_t number = entry_index / kEntriesPerSegment;
+  return Segment(number);
 }
 
 template <typename Entry, size_t size>
-bool ExternalEntityTable<Entry, size>::is_initialized() const {
-  return buffer_ != nullptr;
+uint32_t ExternalEntityTable<Entry, size>::Space::freelist_length() const {
+  auto freelist = freelist_head_.load(std::memory_order_relaxed);
+  return freelist.length();
 }
 
 template <typename Entry, size_t size>
-uint32_t ExternalEntityTable<Entry, size>::capacity() const {
-  return capacity_.load(std::memory_order_relaxed);
+uint32_t ExternalEntityTable<Entry, size>::Space::num_segments() {
+  base::MutexGuard guard(&mutex_);
+  return static_cast<uint32_t>(segments_.size());
 }
 
 template <typename Entry, size_t size>
-uint32_t ExternalEntityTable<Entry, size>::freelist_length() const {
-  auto freelist = freelist_head_.load(std::memory_order_relaxed);
-  DCHECK_LE(freelist.length(), capacity());
-  return freelist.length();
+bool ExternalEntityTable<Entry, size>::Space::Contains(uint32_t index) {
+  base::MutexGuard guard(&mutex_);
+  Segment segment = Segment::Containing(index);
+  return segments_.find(segment) != segments_.end();
+}
+
+template <typename Entry, size_t size>
+Entry& ExternalEntityTable<Entry, size>::at(uint32_t index) {
+  return base_[index];
+}
+
+template <typename Entry, size_t size>
+const Entry& ExternalEntityTable<Entry, size>::at(uint32_t index) const {
+  return base_[index];
+}
+
+template <typename Entry, size_t size>
+bool ExternalEntityTable<Entry, size>::is_initialized() const {
+  DCHECK(!base_ || reinterpret_cast<Address>(base_) == vas_->base());
+  return base_ != nullptr;
 }
 
 template <typename Entry, size_t size>
@@ -51,44 +73,50 @@ void ExternalEntityTable<Entry, size>::InitializeTable() {
   VirtualAddressSpace* root_space = GetPlatformVirtualAddressSpace();
   DCHECK(IsAligned(kReservationSize, root_space->allocation_granularity()));
 
-  Address buffer_start = root_space->AllocatePages(
-      VirtualAddressSpace::kNoHint, kReservationSize,
-      root_space->allocation_granularity(), PagePermissions::kNoAccess);
-  if (!buffer_start) {
-    V8::FatalProcessOutOfMemory(nullptr,
-                                "ExternalEntityTable::InitializeTable");
+  if (root_space->CanAllocateSubspaces()) {
+    vas_ = root_space->AllocateSubspace(VirtualAddressSpace::kNoHint,
+                                        kReservationSize, kSegmentSize,
+                                        PagePermissions::kReadWrite);
+  } else {
+    // This may be required on old Windows versions that don't support
+    // VirtualAlloc2, which is required for subspaces. In that case, just use a
+    // fully-backed emulated subspace.
+    Address reservation_base = root_space->AllocatePages(
+        VirtualAddressSpace::kNoHint, kReservationSize, kSegmentSize,
+        PagePermissions::kReadWrite);
+    if (reservation_base) {
+      vas_ = std::make_unique<base::EmulatedVirtualAddressSubspace>(
+          root_space, reservation_base, kReservationSize, kReservationSize);
+    }
   }
-  buffer_ = reinterpret_cast<Entry*>(buffer_start);
-
-  mutex_ = new base::Mutex;
-  if (!mutex_) {
-    V8::FatalProcessOutOfMemory(nullptr,
-                                "ExternalEntityTable::InitializeTable");
+  if (!vas_) {
+    V8::FatalProcessOutOfMemory(
+        nullptr, "ExternalEntityTable::InitializeTable (subspace allocation)");
+  }
+  base_ = reinterpret_cast<Entry*>(vas_->base());
+
+  // Allocate the first segment of the table as read-only memory. This segment
+  // will contain the null entry, which should always contain nullptr.
+  auto first_segment = vas_->AllocatePages(
+      vas_->base(), kSegmentSize, kSegmentSize, PagePermissions::kRead);
+  if (first_segment != vas_->base()) {
+    V8::FatalProcessOutOfMemory(
+        nullptr,
+        "ExternalEntityTable::InitializeTable (first segment allocation)");
   }
-
-  // Allocate the initial block. Mutex must be held for that.
-  base::MutexGuard guard(mutex_);
-  Grow();
 }
 
 template <typename Entry, size_t size>
 void ExternalEntityTable<Entry, size>::TearDownTable() {
   DCHECK(is_initialized());
-
-  Address buffer_start = reinterpret_cast<Address>(buffer_);
-  GetPlatformVirtualAddressSpace()->FreePages(buffer_start, kReservationSize);
-  delete mutex_;
-
-  buffer_ = nullptr;
-  mutex_ = nullptr;
-  freelist_head_ = FreelistHead();
-  capacity_ = 0;
-  extra_ = 0;
+  base_ = nullptr;
+  vas_.reset();
 }
 
 template <typename Entry, size_t size>
-uint32_t ExternalEntityTable<Entry, size>::AllocateEntry() {
+uint32_t ExternalEntityTable<Entry, size>::AllocateEntry(Space* space) {
   DCHECK(is_initialized());
+  DCHECK(space->BelongsTo(this));
 
   // We currently don't want entry allocation to trigger garbage collection as
   // this may cause seemingly harmless pointer field assignments to trigger
@@ -105,49 +133,49 @@ uint32_t ExternalEntityTable<Entry, size>::AllocateEntry() {
     // and so requires an acquire load as well as a release store in Grow() to
     // prevent reordering of memory accesses, which could for example cause one
     // thread to read a freelist entry before it has been properly initialized.
-    freelist = freelist_head_.load(std::memory_order_acquire);
+    freelist = space->freelist_head_.load(std::memory_order_acquire);
     if (freelist.is_empty()) {
       // Freelist is empty. Need to take the lock, then attempt to grow the
       // table if no other thread has done it in the meantime.
-      base::MutexGuard guard(mutex_);
+      base::MutexGuard guard(&space->mutex_);
 
       // Reload freelist head in case another thread already grew the table.
-      freelist = freelist_head_.load(std::memory_order_relaxed);
+      freelist = space->freelist_head_.load(std::memory_order_relaxed);
 
       if (freelist.is_empty()) {
-        // Freelist is (still) empty so grow the table.
-        freelist = Grow();
-        // Grow() adds one block to the table and so to the freelist.
-        DCHECK_EQ(freelist.length(), kEntriesPerBlock);
+        // Freelist is (still) empty so extend this space by another segment.
+        freelist = Extend(space);
+        // Extend() adds one segment to the table and so to the freelist.
+        DCHECK_EQ(freelist.length(), kEntriesPerSegment);
       }
     }
 
-    success = TryAllocateEntryFromFreelist(freelist);
+    success = TryAllocateEntryFromFreelist(space, freelist);
   }
 
   uint32_t allocated_entry = freelist.next();
+  DCHECK(space->Contains(allocated_entry));
   DCHECK_NE(allocated_entry, 0);
-  DCHECK_LT(allocated_entry, capacity());
   return allocated_entry;
 }
 
 template <typename Entry, size_t size>
 uint32_t ExternalEntityTable<Entry, size>::AllocateEntryBelow(
-    uint32_t threshold_index) {
+    Space* space, uint32_t threshold_index) {
   DCHECK(is_initialized());
-  DCHECK_LE(threshold_index, capacity());
 
   FreelistHead freelist;
   bool success = false;
   while (!success) {
-    freelist = freelist_head_.load(std::memory_order_acquire);
+    freelist = space->freelist_head_.load(std::memory_order_acquire);
     // Check that the next free entry is below the threshold.
     if (freelist.is_empty() || freelist.next() >= threshold_index) return 0;
 
-    success = TryAllocateEntryFromFreelist(freelist);
+    success = TryAllocateEntryFromFreelist(space, freelist);
   }
 
   uint32_t allocated_entry = freelist.next();
+  DCHECK(space->Contains(allocated_entry));
   DCHECK_NE(allocated_entry, 0);
   DCHECK_LT(allocated_entry, threshold_index);
   return allocated_entry;
@@ -155,23 +183,20 @@ uint32_t ExternalEntityTable<Entry, size>::AllocateEntryBelow(
 
 template <typename Entry, size_t size>
 bool ExternalEntityTable<Entry, size>::TryAllocateEntryFromFreelist(
-    FreelistHead freelist) {
+    Space* space, FreelistHead freelist) {
   DCHECK(!freelist.is_empty());
-  DCHECK_LT(freelist.next(), capacity());
-  DCHECK_LT(freelist.length(), capacity());
+  DCHECK(space->Contains(freelist.next()));
 
   Entry& freelist_entry = at(freelist.next());
   uint32_t next_freelist_entry = freelist_entry.GetNextFreelistEntryIndex();
   FreelistHead new_freelist(next_freelist_entry, freelist.length() - 1);
-  bool success = freelist_head_.compare_exchange_strong(
+  bool success = space->freelist_head_.compare_exchange_strong(
       freelist, new_freelist, std::memory_order_relaxed);
 
   // When the CAS succeeded, the entry must've been a freelist entry.
   // Otherwise, this is not guaranteed as another thread may have allocated
   // and overwritten the same entry in the meantime.
   if (success) {
-    DCHECK_LT(new_freelist.next(), capacity());
-    DCHECK_LT(new_freelist.length(), capacity());
     DCHECK_IMPLIES(freelist.length() > 1, !new_freelist.is_empty());
     DCHECK_IMPLIES(freelist.length() == 1, new_freelist.is_empty());
   }
@@ -180,33 +205,21 @@ bool ExternalEntityTable<Entry, size>::TryAllocateEntryFromFreelist(
 
 template <typename Entry, size_t size>
 typename ExternalEntityTable<Entry, size>::FreelistHead
-ExternalEntityTable<Entry, size>::Grow() {
+ExternalEntityTable<Entry, size>::Extend(Space* space) {
   // Freelist should be empty when calling this method.
-  DCHECK_EQ(freelist_length(), 0);
-  // The caller must lock the mutex before calling Grow().
-  mutex_->AssertHeld();
-
-  // Grow the table by one block.
-  VirtualAddressSpace* root_space = GetPlatformVirtualAddressSpace();
-  DCHECK(IsAligned(kBlockSize, root_space->page_size()));
-  uint32_t old_capacity = capacity();
-  uint32_t new_capacity = old_capacity + kEntriesPerBlock;
-  if (new_capacity > kMaxCapacity) {
-    V8::FatalProcessOutOfMemory(nullptr, "ExternalEntityTable::Grow");
-  }
-  Address buffer_start = reinterpret_cast<Address>(buffer_);
-  if (!root_space->SetPagePermissions(buffer_start + old_capacity * kEntrySize,
-                                      kBlockSize,
-                                      PagePermissions::kReadWrite)) {
-    V8::FatalProcessOutOfMemory(nullptr, "ExternalEntityTable::Grow");
-  }
-
-  capacity_.store(new_capacity, std::memory_order_relaxed);
-
-  // Build the freelist bottom to top but skip entry zero, which is reserved.
-  uint32_t start = std::max<uint32_t>(old_capacity, 1);
-  uint32_t last = new_capacity - 1;
-  for (uint32_t i = start; i < last; i++) {
+  DCHECK_EQ(space->freelist_length(), 0);
+  // The caller must lock the space's mutex before extending it.
+  space->mutex_.AssertHeld();
+
+  // Allocate the new segment.
+  Segment segment = AllocateTableSegment();
+  space->segments_.insert(segment);
+  DCHECK_NE(segment.number(), 0);
+
+  // Refill the freelist with the entries in the newly allocated segment.
+  uint32_t first = segment.first_entry();
+  uint32_t last = segment.last_entry();
+  for (uint32_t i = first; i < last; i++) {
     uint32_t next_free_entry = i + 1;
     at(i).MakeFreelistEntry(next_free_entry);
   }
@@ -215,29 +228,32 @@ ExternalEntityTable<Entry, size>::Grow() {
   // This must be a release store to prevent reordering of the preceeding
   // stores to the freelist from being reordered past this store. See
   // AllocateEntry() for more details.
-  FreelistHead new_freelist_head(start, last - start + 1);
-  freelist_head_.store(new_freelist_head, std::memory_order_release);
+  FreelistHead new_freelist_head(first, last - first + 1);
+  space->freelist_head_.store(new_freelist_head, std::memory_order_release);
 
   return new_freelist_head;
 }
 
 template <typename Entry, size_t size>
-void ExternalEntityTable<Entry, size>::Shrink(uint32_t new_capacity) {
-  uint32_t old_capacity = capacity();
-  DCHECK(IsAligned(new_capacity, kEntriesPerBlock));
-  DCHECK_GT(new_capacity, 0);
-  DCHECK_LT(new_capacity, old_capacity);
-
-  capacity_.store(new_capacity, std::memory_order_relaxed);
+typename ExternalEntityTable<Entry, size>::Segment
+ExternalEntityTable<Entry, size>::AllocateTableSegment() {
+  Address start =
+      vas_->AllocatePages(VirtualAddressSpace::kNoHint, kSegmentSize,
+                          kSegmentSize, PagePermissions::kReadWrite);
+  if (!start) {
+    V8::FatalProcessOutOfMemory(nullptr,
+                                "ExternalEntityTable::AllocateSegment");
+  }
+  uint32_t offset = static_cast<uint32_t>((start - vas_->base()));
+  return Segment::At(offset);
+}
 
-  Address buffer_start = reinterpret_cast<Address>(buffer_);
-  Address new_table_end = buffer_start + new_capacity * kEntrySize;
-  uint32_t bytes_to_decommit = (old_capacity - new_capacity) * kEntrySize;
-  // The pages may contain stale pointers which could be abused by an
-  // attacker if they are still accessible, so use Decommit here which
-  // guarantees that the pages become inaccessible and will be zeroed out.
-  VirtualAddressSpace* root_space = GetPlatformVirtualAddressSpace();
-  CHECK(root_space->DecommitPages(new_table_end, bytes_to_decommit));
+template <typename Entry, size_t size>
+void ExternalEntityTable<Entry, size>::FreeTableSegment(Segment segment) {
+  // Segment zero is reserved.
+  DCHECK_NE(segment.number(), 0);
+  Address segment_start = vas_->base() + segment.offset();
+  vas_->FreePages(segment_start, kSegmentSize);
 }
 
 }  // namespace internal
diff --git a/src/sandbox/external-entity-table.h b/src/sandbox/external-entity-table.h
index f1a0d283c94..ead8a47c71e 100644
--- a/src/sandbox/external-entity-table.h
+++ b/src/sandbox/external-entity-table.h
@@ -5,6 +5,9 @@
 #ifndef V8_SANDBOX_EXTERNAL_ENTITY_TABLE_H_
 #define V8_SANDBOX_EXTERNAL_ENTITY_TABLE_H_
 
+#include <set>
+
+#include "include/v8-platform.h"
 #include "include/v8config.h"
 #include "src/base/atomicops.h"
 #include "src/base/memory.h"
@@ -30,10 +33,10 @@ class Isolate;
  * The ExternalEntityTable class should be seen an an incomplete class that
  * needs to be extended by a concrete implementation class, such as the
  * ExternalPointerTable class, as it is lacking some functionality. In
- * particular, while the ExternalEntityTable implements the reserving, growing,
- * and shrinking of the backing memory as well as entry allocation routines, it
- * does not implement any logic for reclaiming entries such as garbage
- * collection. This must be done by the child classes.
+ * particular, while the ExternalEntityTable implements basic table memory
+ * management as well as entry allocation routines, it does not implement any
+ * logic for reclaiming entries such as garbage collection. This must be done
+ * by the child classes.
  */
 template <typename Entry, size_t size>
 class V8_EXPORT_PRIVATE ExternalEntityTable {
@@ -42,60 +45,54 @@ class V8_EXPORT_PRIVATE ExternalEntityTable {
   static constexpr size_t kReservationSize = size;
   static constexpr size_t kMaxCapacity = kReservationSize / kEntrySize;
 
-  ExternalEntityTable() = default;
-  ExternalEntityTable(const ExternalEntityTable&) = delete;
-  ExternalEntityTable& operator=(const ExternalEntityTable&) = delete;
+  // For managing the table's backing memory, the table is partitioned into
+  // segments of this size. Segments can then be allocated and freed using the
+  // AllocateTableSegment() and FreeTableSegment() routines.
+  static constexpr size_t kSegmentSize = 64 * KB;
+  static constexpr size_t kEntriesPerSegment = kSegmentSize / kEntrySize;
 
-  // Access the entry at the specified index.
-  // The index must be less than the current capacity.
-  Entry& at(uint32_t index);
-  const Entry& at(uint32_t index) const;
+  // Struct representing a segment of the table.
+  struct Segment {
+   public:
+    // Initialize a segment given its number.
+    explicit Segment(uint32_t number) : number_(number) {}
 
-  // Returns true if this table has been initialized.
-  bool is_initialized() const;
+    // Returns the segment starting at the specified offset from the base of the
+    // table.
+    static Segment At(uint32_t offset);
 
-  // Returns the current capacity of the table, expressed as number of entries.
-  //
-  // The capacity of the table may increase during entry allocation (if the
-  // table is grown) and may decrease during sweeping (if blocks at the end are
-  // free). As the former may happen concurrently, the capacity can only be
-  // used reliably if either the table mutex is held or if all mutator threads
-  // are currently stopped. However, it is fine to use this value to
-  // sanity-check incoming ExternalPointerHandles in debug builds (there's no
-  // need for actual bounds-checks because out-of-bounds accesses are guaranteed
-  // to result in a harmless crash).
-  uint32_t capacity() const;
-
-  // Determines the number of entries currently on the freelist.
-  // As table entries can be allocated from other threads, the freelist size
-  // may have changed by the time this method returns. As such, the returned
-  // value should only be treated as an approximation.
-  uint32_t freelist_length() const;
+    // Returns the segment containing the entry at the given index.
+    static Segment Containing(uint32_t entry_index);
 
-  // Initializes the table by reserving the backing memory, allocating an
-  // initial block, and populating the freelist.
-  void InitializeTable();
+    // The segments of a table are numbered sequentially. This method returns
+    // the number of this segment.
+    uint32_t number() const { return number_; }
 
-  // Deallocates all memory associated with this table.
-  void TearDownTable();
+    // Returns the offset of this segment from the table base.
+    uint32_t offset() const { return number_ * kSegmentSize; }
 
-  // Allocates a new entry and return its index.
-  //
-  // If there are no free entries, then this will grow the table.
-  // This method is atomic and can be called from background threads.
-  uint32_t AllocateEntry();
+    // Returns the index of the first entry in this segment.
+    uint32_t first_entry() const { return number_ * kEntriesPerSegment; }
 
-  // Attempts to allocate an entry below the specified index.
-  //
-  // If there are no free entries at a lower index, this method will fail and
-  // return zero. The threshold index must be at or below the current capacity.
-  // This method will therefore never grow the table.
-  // This method is atomic and can be called from background threads.
-  uint32_t AllocateEntryBelow(uint32_t threshold_index);
+    // Return the index of the last entry in this segment.
+    uint32_t last_entry() const {
+      return first_entry() + kEntriesPerSegment - 1;
+    }
 
-  // Struct representing the head of the freelist of a table.
+    // Segments are ordered by their id/offset.
+    bool operator<(const Segment& other) const {
+      return number_ < other.number_;
+    }
+
+   private:
+    // A segment is identified by its number, which is its offset from the base
+    // of the table divided by the segment size.
+    const uint32_t number_;
+  };
+
+  // Struct representing the head of the freelist.
   //
-  // An external entity table uses a simple, singly-linked list to manage free
+  // An external entity table uses simple, singly-linked lists to manage free
   // entries. Each entry on the freelist contains the 32-bit index of the next
   // entry. The last entry points to zero.
   struct FreelistHead {
@@ -124,28 +121,141 @@ class V8_EXPORT_PRIVATE ExternalEntityTable {
     uint32_t length_;
   };
 
+  // We expect the FreelistHead struct to fit into a single atomic word.
+  // Otherwise, access to it would be slow.
+  static_assert(std::atomic<FreelistHead>::is_always_lock_free);
+
+  // A collection of segments in an external entity table.
+  //
+  // For the purpose of memory management, a table is partitioned into segments
+  // of a fixed size (e.g. 64kb). A Space is a collection of segments that all
+  // share the same freelist. As such, entry allocation and freeing (e.g.
+  // through garbage collection) all happen on the level of spaces.
+  //
+  // Spaces allow implementing features such as:
+  // * Young generation GC support (a separate space is used for all entries
+  //   belonging to the young generation)
+  // * Having double-width entries in a table (a dedicated space is used that
+  //   contains only double-width entries)
+  // * Sharing one table between multiple isolates that perform GC independently
+  //   (each Isolate owns one space)
+  struct Space {
+   public:
+    Space() = default;
+    Space(const Space&) = delete;
+    Space& operator=(const Space&) = delete;
+
+    // Determines the number of entries currently on the freelist.
+    // As entries can be allocated from other threads, the freelist size may
+    // have changed by the time this method returns. As such, the returned
+    // value should only be treated as an approximation.
+    uint32_t freelist_length() const;
+
+    // Returns the current number of segments currently associated with this
+    // space. As entries can be allocated from other threads, and as that may
+    // cause new segments to be added to this space, the returned value should
+    // generally only be treated as an approximation.
+    uint32_t num_segments();
+
+    // Returns whether this space is currently empty.
+    bool is_empty() { return num_segments() == 0; }
+
+    // Returns the current capacity of this space.
+    // The capacity of a space is the total number of entries it can contain.
+    uint32_t capacity() { return num_segments() * kEntriesPerSegment; }
+
+    // Returns true if this space contains the entry with the given index.
+    bool Contains(uint32_t index);
+
+   protected:
+    friend class ExternalEntityTable<Entry, size>;
+
+#ifdef DEBUG
+    // In debug builds we keep track of which table a space belongs to to be
+    // able to insert additional DCHECKs that verify that spaces are always used
+    // with the correct table.
+    std::atomic<void*> owning_table_ = nullptr;
+
+    // Check whether this space belongs to the given external entity table.
+    bool BelongsTo(void* table) {
+      // To simplify things, we set the owning table the first time this method
+      // is called. This way we avoid having to add space initialization
+      // routines just for this feature.
+      if (owning_table_ == nullptr) owning_table_ = table;
+      return owning_table_ == table;
+    }
+#endif  // DEBUG
+
+    // The freelist used by this space.
+    // This contains both the index of the first entry in the freelist and the
+    // total length of the freelist as both values need to be updated together
+    // in a single atomic operation to stay consistent in the case of concurrent
+    // entry allocations.
+    std::atomic<FreelistHead> freelist_head_ = FreelistHead();
+
+    // The collection of segments belonging to this space.
+    std::set<Segment> segments_;
+
+    // Mutex guarding access to the segments_ set.
+    base::Mutex mutex_;
+  };
+
+  ExternalEntityTable() = default;
+  ExternalEntityTable(const ExternalEntityTable&) = delete;
+  ExternalEntityTable& operator=(const ExternalEntityTable&) = delete;
+
+  // Access the entry at the specified index.
+  Entry& at(uint32_t index);
+  const Entry& at(uint32_t index) const;
+
+  // Returns true if this table has been initialized.
+  bool is_initialized() const;
+
+  // Initializes the table by reserving the backing memory, allocating an
+  // initial segment, and populating the freelist.
+  void InitializeTable();
+
+  // Deallocates all memory associated with this table.
+  void TearDownTable();
+
+  // Allocates a new entry in the given space and return its index.
+  //
+  // If there are no free entries, then this will extend the space by
+  // allocating a new segment.
+  // This method is atomic and can be called from background threads.
+  uint32_t AllocateEntry(Space* space);
+
+  // Attempts to allocate an entry in the given space below the specified index.
+  //
+  // If there are no free entries at a lower index, this method will fail and
+  // return zero. This method will therefore never allocate a new segment.
+  // This method is atomic and can be called from background threads.
+  uint32_t AllocateEntryBelow(Space* space, uint32_t threshold_index);
+
   // Try to allocate the first entry of the freelist.
   //
   // This method is mostly a wrapper around an atomic compare-and-swap which
   // replaces the current freelist head with the next entry in the freelist,
   // thereby allocating the entry at the start of the freelist.
-  bool TryAllocateEntryFromFreelist(FreelistHead freelist);
-
-  // Extends the table and adds newly created entries to the freelist.
-  // Returns the new freelist head.
-  // When calling this method, mutex_ must be locked.
-  // If the table cannot be grown, either because it is already at its maximum
-  // size or because the memory for it could not be allocated, this method will
-  // fail with an OOM crash.
-  FreelistHead Grow();
-
-  // Shrink the table to the new capacity.
-  // The new capacity must be less than the current capacity and must be a
-  // multiple of the block size. The now-unused blocks at the end of the table
-  // are decommitted from memory. It is therefore guaranteed that they will be
-  // inaccessible afterwards, and that they will be zero-initialized when they
-  // are "brought back".
-  void Shrink(uint32_t new_capacity);
+  bool TryAllocateEntryFromFreelist(Space* space, FreelistHead freelist);
+
+  // Allocate a new segment and add it to the given space.
+  //
+  // This should only be called when the freelist of the space is currently
+  // empty. It will then refill the freelist with all entries in the newly
+  // allocated segment.
+  FreelistHead Extend(Space* space);
+
+  // Allocate a new segment in this table.
+  //
+  // The memory of the newly allocated segment is guaranteed to be
+  // zero-initialized.
+  Segment AllocateTableSegment();
+
+  // Free the specified segment of this table.
+  //
+  // The memory of this segment will afterwards be inaccessible.
+  void FreeTableSegment(Segment segment);
 
   // Marker value for the freelist_head_ member to indicate that entry
   // allocation is currently forbidden, for example because the table is being
@@ -155,46 +265,19 @@ class V8_EXPORT_PRIVATE ExternalEntityTable {
   static constexpr FreelistHead kEntryAllocationIsForbiddenMarker =
       FreelistHead(-1, -1);
 
-  // The table grows and shrinks in blocks of this size. This is also the
-  // initial size of the table.
-#if V8_TARGET_ARCH_PPC64
-  // PPC64 uses 64KB pages, and this must be a multiple of the page size.
-  static constexpr size_t kBlockSize = 64 * KB;
-#else
-  static constexpr size_t kBlockSize = 16 * KB;
-#endif
-  static constexpr size_t kEntriesPerBlock = kBlockSize / kEntrySize;
-
-  // The buffer backing this table.
-  // This is effectively const after initialization: the underlying buffer is
-  // never reallocated, only grown/shrunk in place.
-  Entry* buffer_ = nullptr;
-
-  // Lock protecting the slow path for entry allocation, in particular Grow().
-  // As the size of this class must be predictable (it is e.g. part of
-  // IsolateData), it cannot directly contain a Mutex and so instead contains a
-  // pointer to one.
-  base::Mutex* mutex_ = nullptr;
-
-  // The freelist used by this table.
-  // This contains both the index of the first entry in the freelist and the
-  // total length of the freelist as both values need to be updated together in
-  // a single atomic operation to stay consistent in the case of concurrent
-  // entry allocations.
-  // We expect the FreelistHead struct to fit into a single atomic word.
-  // Otherwise, access to it would be slow.
-  static_assert(std::atomic<FreelistHead>::is_always_lock_free);
-  std::atomic<FreelistHead> freelist_head_ = FreelistHead();
-
-  // The current capacity of this table, as number of entries.
-  std::atomic<uint32_t> capacity_{0};
-
-  // An additional 32-bit atomic word that derived classes can use. For example,
-  // the ExternalPointerTable uses this for the table compaction algorithm. This
-  // is stored in this class so that std::is_standard_layout is true for derived
-  // classes (for a class to have standard layout, only one class in the
-  // inheritance hierarchy must have non-static data properties).
-  std::atomic<uint32_t> extra_{0};
+  // The pointer to the base of the virtual address space backing this table.
+  // All entry accesses happen through this pointer.
+  // It is equivalent to |vas_->base()| and is effectively const after
+  // initialization since the backing memory is never reallocated.
+  Entry* base_ = nullptr;
+
+  // The virtual address space backing this table.
+  // This is used to manage the underlying OS pages, in particular to allocate
+  // and free the segments that make up the table.
+  std::unique_ptr<VirtualAddressSpace> vas_ = nullptr;
+  // The size and layout of this class must be deterministic.
+  static_assert(sizeof(std::unique_ptr<VirtualAddressSpace>) ==
+                kSystemPointerSize);
 };
 
 }  // namespace internal
diff --git a/src/sandbox/external-pointer-inl.h b/src/sandbox/external-pointer-inl.h
index f2f5a73a603..5a1db3ac6f0 100644
--- a/src/sandbox/external-pointer-inl.h
+++ b/src/sandbox/external-pointer-inl.h
@@ -28,6 +28,13 @@ ExternalPointerTable& GetExternalPointerTable(Isolate* isolate) {
              ? isolate->shared_external_pointer_table()
              : isolate->external_pointer_table();
 }
+
+template <ExternalPointerTag tag>
+ExternalPointerTable::Space* GetDefaultExternalPointerSpace(Isolate* isolate) {
+  return IsSharedExternalPointerType(tag)
+             ? isolate->shared_external_pointer_space()
+             : isolate->heap()->external_pointer_space();
+}
 #endif  // V8_ENABLE_SANDBOX
 
 template <ExternalPointerTag tag>
@@ -36,7 +43,8 @@ V8_INLINE void InitExternalPointerField(Address field_address, Isolate* isolate,
 #ifdef V8_ENABLE_SANDBOX
   static_assert(tag != kExternalPointerNullTag);
   ExternalPointerTable& table = GetExternalPointerTable<tag>(isolate);
-  ExternalPointerHandle handle = table.AllocateAndInitializeEntry(value, tag);
+  ExternalPointerHandle handle = table.AllocateAndInitializeEntry(
+      GetDefaultExternalPointerSpace<tag>(isolate), value, tag);
   // Use a Release_Store to ensure that the store of the pointer into the
   // table is not reordered after the store of the handle. Otherwise, other
   // threads may access an uninitialized table entry and crash.
@@ -91,7 +99,8 @@ V8_INLINE void WriteLazilyInitializedExternalPointerField(Address field_address,
   ExternalPointerHandle handle = base::AsAtomic32::Relaxed_Load(location);
   if (handle == kNullExternalPointerHandle) {
     // Field has not been initialized yet.
-    ExternalPointerHandle handle = table.AllocateAndInitializeEntry(value, tag);
+    ExternalPointerHandle handle = table.AllocateAndInitializeEntry(
+        GetDefaultExternalPointerSpace<tag>(isolate), value, tag);
     base::AsAtomic32::Release_Store(location, handle);
   } else {
     table.Set(handle, value, tag);
diff --git a/src/sandbox/external-pointer-table-inl.h b/src/sandbox/external-pointer-table-inl.h
index 983a6648d44..e3578c92c0a 100644
--- a/src/sandbox/external-pointer-table-inl.h
+++ b/src/sandbox/external-pointer-table-inl.h
@@ -140,47 +140,52 @@ Address ExternalPointerTable::Exchange(ExternalPointerHandle handle,
 }
 
 ExternalPointerHandle ExternalPointerTable::AllocateAndInitializeEntry(
-    Address initial_value, ExternalPointerTag tag) {
-  uint32_t index = AllocateEntry();
+    Space* space, Address initial_value, ExternalPointerTag tag) {
+  DCHECK(space->BelongsTo(this));
+  uint32_t index = AllocateEntry(space);
   at(index).MakeExternalPointerEntry(initial_value, tag);
   return IndexToHandle(index);
 }
 
-void ExternalPointerTable::Mark(ExternalPointerHandle handle,
+void ExternalPointerTable::Mark(Space* space, ExternalPointerHandle handle,
                                 Address handle_location) {
+  DCHECK(space->BelongsTo(this));
+  // The null entry is immortal and immutable, so no need to mark it as alive.
+  if (handle == kNullExternalPointerHandle) return;
+
   // The handle_location must contain the given handle. The only exception to
-  // this is when the handle is zero, which means that it hasn't yet been
-  // initialized. In that case, the handle may be initialized between the
-  // caller loading it and this DCHECK loading it again, in which case the two
-  // values would not be the same. This scenario is unproblematic though as the
-  // new entry will already be marked as alive as it has just been allocated.
-  DCHECK(handle == kNullExternalPointerHandle ||
-         handle ==
-             base::AsAtomic32::Acquire_Load(
-                 reinterpret_cast<ExternalPointerHandle*>(handle_location)));
+  // this is when the handle is zero, which may mean that it hasn't yet been
+  // initialized. However, in that case we'll have already returned from this
+  // function above.
+  DCHECK(handle ==
+         base::AsAtomic32::Acquire_Load(
+             reinterpret_cast<ExternalPointerHandle*>(handle_location)));
 
   uint32_t index = HandleToIndex(handle);
+  DCHECK(space->Contains(index));
 
   // If the table is being compacted and the entry is inside the evacuation
   // area, then allocate and set up an evacuation entry for it.
-  MaybeCreateEvacuationEntry(index, handle_location);
+  MaybeCreateEvacuationEntry(space, index, handle_location);
 
   // Even if the entry is marked for evacuation, it still needs to be marked as
   // alive as it may be visited during sweeping before being evacuation.
   at(index).Mark();
 }
 
-void ExternalPointerTable::MaybeCreateEvacuationEntry(uint32_t index,
+void ExternalPointerTable::MaybeCreateEvacuationEntry(Space* space,
+                                                      uint32_t index,
                                                       Address handle_location) {
   // Check if the entry should be evacuated for table compaction.
   // The current value of the start of the evacuation area is cached in a local
   // variable here as it otherwise may be changed by another marking thread
   // while this method runs, causing non-optimal behaviour (for example, the
   // allocation of an evacuation entry _after_ the entry that is evacuated).
-  uint32_t start_of_evacuation_area = extra_.load(std::memory_order_relaxed);
+  uint32_t start_of_evacuation_area =
+      space->start_of_evacuation_area_.load(std::memory_order_relaxed);
   if (index >= start_of_evacuation_area) {
-    DCHECK(IsCompacting());
-    uint32_t new_index = AllocateEntryBelow(start_of_evacuation_area);
+    DCHECK(space->IsCompacting());
+    uint32_t new_index = AllocateEntryBelow(space, start_of_evacuation_area);
     if (new_index) {
       DCHECK_LT(new_index, start_of_evacuation_area);
       // Even though the new entry will only be accessed during sweeping, this
@@ -205,16 +210,23 @@ void ExternalPointerTable::MaybeCreateEvacuationEntry(uint32_t index,
       // abort compaction here. Entries that have already been visited will
       // still be compacted during Sweep, but there is no guarantee that any
       // blocks at the end of the table will now be completely free.
-      AbortCompacting(start_of_evacuation_area);
+      space->AbortCompacting(start_of_evacuation_area);
     }
   }
 }
 
+bool ExternalPointerTable::IsValidHandle(ExternalPointerHandle handle) const {
+#ifdef DEBUG
+  handle &= ~kVisitedHandleMarker;
+#endif  // DEBUG
+  uint32_t index = handle >> kExternalPointerIndexShift;
+  return handle == index << kExternalPointerIndexShift;
+}
+
 uint32_t ExternalPointerTable::HandleToIndex(
     ExternalPointerHandle handle) const {
+  DCHECK(IsValidHandle(handle));
   uint32_t index = handle >> kExternalPointerIndexShift;
-  DCHECK_EQ(handle & ~kVisitedHandleMarker,
-            index << kExternalPointerIndexShift);
 #if defined(LEAK_SANITIZER)
   // When LSan is active, we use "fat" entries that also store the raw pointer
   // to that LSan can find live references. However, we do this transparently:
@@ -227,41 +239,47 @@ uint32_t ExternalPointerTable::HandleToIndex(
   // that the entries are 16 bytes large when LSan is active.
   index /= 2;
 #endif  // LEAK_SANITIZER
-  DCHECK_LT(index, capacity());
+  DCHECK_LE(index, kMaxExternalPointers);
   return index;
 }
 
 ExternalPointerHandle ExternalPointerTable::IndexToHandle(
     uint32_t index) const {
+  DCHECK_LE(index, kMaxExternalPointers);
   ExternalPointerHandle handle = index << kExternalPointerIndexShift;
-  DCHECK_EQ(index, handle >> kExternalPointerIndexShift);
 #if defined(LEAK_SANITIZER)
   handle *= 2;
 #endif  // LEAK_SANITIZER
   return handle;
 }
 
-void ExternalPointerTable::StartCompacting(uint32_t start_of_evacuation_area) {
-  extra_.store(start_of_evacuation_area, std::memory_order_relaxed);
+void ExternalPointerTable::Space::StartCompacting(
+    uint32_t start_of_evacuation_area) {
+  start_of_evacuation_area_.store(start_of_evacuation_area,
+                                  std::memory_order_relaxed);
 }
 
-void ExternalPointerTable::StopCompacting() {
-  extra_.store(kNotCompactingMarker, std::memory_order_relaxed);
+void ExternalPointerTable::Space::StopCompacting() {
+  start_of_evacuation_area_.store(kNotCompactingMarker,
+                                  std::memory_order_relaxed);
 }
 
-void ExternalPointerTable::AbortCompacting(uint32_t start_of_evacuation_area) {
+void ExternalPointerTable::Space::AbortCompacting(
+    uint32_t start_of_evacuation_area) {
   uint32_t compaction_aborted_marker =
       start_of_evacuation_area | kCompactionAbortedMarker;
   DCHECK_NE(compaction_aborted_marker, kNotCompactingMarker);
-  extra_.store(compaction_aborted_marker, std::memory_order_relaxed);
+  start_of_evacuation_area_.store(compaction_aborted_marker,
+                                  std::memory_order_relaxed);
 }
 
-bool ExternalPointerTable::IsCompacting() {
-  return extra_.load(std::memory_order_relaxed) != kNotCompactingMarker;
+bool ExternalPointerTable::Space::IsCompacting() {
+  return start_of_evacuation_area_.load(std::memory_order_relaxed) !=
+         kNotCompactingMarker;
 }
 
-bool ExternalPointerTable::CompactingWasAbortedDuringMarking() {
-  auto value = extra_.load(std::memory_order_relaxed);
+bool ExternalPointerTable::Space::CompactingWasAbortedDuringMarking() {
+  auto value = start_of_evacuation_area_.load(std::memory_order_relaxed);
   return (value & kCompactionAbortedMarker) == kCompactionAbortedMarker;
 }
 
diff --git a/src/sandbox/external-pointer-table.cc b/src/sandbox/external-pointer-table.cc
index eb6b7f16d8d..514a532746c 100644
--- a/src/sandbox/external-pointer-table.cc
+++ b/src/sandbox/external-pointer-table.cc
@@ -13,169 +13,148 @@
 namespace v8 {
 namespace internal {
 
-void ExternalPointerTable::Init() {
-  InitializeTable();
-
-  extra_.store(kNotCompactingMarker);
-
-  // Set up the special null entry. This entry must contain nullptr so that
-  // empty EmbedderDataSlots represent nullptr.
-  static_assert(kNullExternalPointerHandle == 0);
-  at(0).MakeExternalPointerEntry(kNullAddress, kExternalPointerNullTag);
-}
+void ExternalPointerTable::Init() { InitializeTable(); }
 
 void ExternalPointerTable::TearDown() { TearDownTable(); }
 
-uint32_t ExternalPointerTable::SweepAndCompact(Isolate* isolate) {
+uint32_t ExternalPointerTable::SweepAndCompact(Space* space,
+                                               Counters* counters) {
   // There must not be any entry allocations while the table is being swept as
   // that would not be safe. Set the freelist to this special marker value to
   // easily catch any violation of this requirement.
-  FreelistHead old_freelist = freelist_head_.load(std::memory_order_relaxed);
-  freelist_head_.store(kEntryAllocationIsForbiddenMarker,
-                       std::memory_order_relaxed);
+  FreelistHead old_freelist =
+      space->freelist_head_.load(std::memory_order_relaxed);
+  space->freelist_head_.store(kEntryAllocationIsForbiddenMarker,
+                              std::memory_order_relaxed);
 
-  // Keep track of the last block (identified by the index of its first entry)
-  // that has live entries. Used to decommit empty blocks at the end.
-  DCHECK_GE(capacity(), kEntriesPerBlock);
-  const uint32_t last_block = capacity() - kEntriesPerBlock;
-  uint32_t last_in_use_block = last_block;
-
-  // When compacting, we can compute the number of unused blocks at the end of
+  // When compacting, we can compute the number of unused segments at the end of
   // the table and skip those during sweeping.
-  uint32_t start_of_evacuation_area = extra_.load(std::memory_order_relaxed);
-  if (IsCompacting()) {
+  uint32_t start_of_evacuation_area =
+      space->start_of_evacuation_area_.load(std::memory_order_relaxed);
+  if (space->IsCompacting()) {
     TableCompactionOutcome outcome;
-    if (CompactingWasAbortedDuringMarking()) {
+    if (space->CompactingWasAbortedDuringMarking()) {
       // Compaction was aborted during marking because the freelist grew to
-      // short. This is not great because now there is no guarantee that any
-      // blocks will be emtpy and so the entire table needs to be swept.
+      // short. In this case, it is not guaranteed that any segments will now
+      // be completely free so that they can be deallocated.
       outcome = TableCompactionOutcome::kAbortedDuringMarking;
       // Extract the original start_of_evacuation_area value so that the
-      // DCHECKs below work correctly.
-      start_of_evacuation_area &= ~kCompactionAbortedMarker;
+      // DCHECKs below and in ResolveEvacuationEntryDuringSweeping work.
+      start_of_evacuation_area &= ~Space::kCompactionAbortedMarker;
     } else if (old_freelist.is_empty() ||
                old_freelist.next() > start_of_evacuation_area) {
-      // In this case, marking finished successfully, but the application
-      // afterwards allocated entries inside the area that is being compacted.
-      // In this case, we can still compute how many blocks at the end of the
-      // table are now empty.
-      if (!old_freelist.is_empty()) {
-        last_in_use_block = RoundDown(old_freelist.next(), kEntriesPerBlock);
-      }
       outcome = TableCompactionOutcome::kPartialSuccess;
     } else {
       // Marking was successful so the entire evacuation area is now free.
-      last_in_use_block = start_of_evacuation_area - kEntriesPerBlock;
       outcome = TableCompactionOutcome::kSuccess;
     }
-    DCHECK(IsAligned(start_of_evacuation_area, kEntriesPerBlock));
+    DCHECK(IsAligned(start_of_evacuation_area, kEntriesPerSegment));
 
-    StopCompacting();
+    space->StopCompacting();
 
-    isolate->counters()->external_pointer_table_compaction_outcome()->AddSample(
+    counters->external_pointer_table_compaction_outcome()->AddSample(
         static_cast<int>(outcome));
   }
 
-  uint32_t table_end = last_in_use_block + kEntriesPerBlock;
-  DCHECK(IsAligned(table_end, kEntriesPerBlock));
-
   // Sweep top to bottom and rebuild the freelist from newly dead and
   // previously freed entries while also clearing the marking bit on live
   // entries and resolving evacuation entries table when compacting the table.
   // This way, the freelist ends up sorted by index which already makes the
   // table somewhat self-compacting and is required for the compaction
-  // algorithm so that evacuated entries are evacuated to the start of the
-  // table. This method must run either on the mutator thread or while the
-  // mutator is stopped.
+  // algorithm so that evacuated entries are evacuated to the start of a space.
+  // This method must run either on the mutator thread or while the mutator is
+  // stopped.
+  // Here we can iterate over the segments collection without taking a lock
+  // because no other thread can currently allocate entries in this space.
   uint32_t current_freelist_head = 0;
   uint32_t current_freelist_length = 0;
-
-  // Skip the special null entry. This also guarantees that the first block
-  // will never be decommitted.
-  // The null entry may have been marked as alive (if any live object was
-  // referencing it), which is fine, the entry will just keep the bit set.
-  for (uint32_t i = table_end - 1; i > 0; i--) {
-    auto payload = at(i).GetRawPayload();
-    if (payload.ContainsEvacuationEntry()) {
-      // Resolve the evacuation entry: take the pointer to the handle from the
-      // evacuation entry, copy the entry to its new location, and finally
-      // update the handle to point to the new entry.
-      // While we now know that the entry being evacuated is free, we don't add
-      // it to (the start of) the freelist because that would immediately cause
-      // new fragmentation when the next entry is allocated. Instead, we assume
-      // that the blocks out of which entries are evacuated will all be
-      // decommitted anyway after this loop, which is usually the case unless
-      // compaction was already aborted during marking.
-      ExternalPointerHandle* handle_location =
-          reinterpret_cast<ExternalPointerHandle*>(
-              payload.ExtractEvacuationEntryHandleLocation());
-      ResolveEvacuationEntryDuringSweeping(i, handle_location,
-                                           start_of_evacuation_area);
-    } else if (!payload.HasMarkBitSet()) {
-      static_assert(
-          (kExternalPointerFreeEntryTag & kExternalPointerMarkBit) == 0,
-          "Freelist entries should not have their marking bit set");
-      at(i).MakeFreelistEntry(current_freelist_head);
-      current_freelist_head = i;
-      current_freelist_length++;
-    } else {
-      auto new_payload = payload;
-      new_payload.ClearMarkBit();
-      at(i).SetRawPayload(new_payload);
+  std::vector<Segment> freed_segments;
+  for (auto segment : base::Reversed(space->segments_)) {
+    uint32_t previous_freelist_head = current_freelist_head;
+    uint32_t previous_freelist_length = current_freelist_length;
+    for (uint32_t i = segment.last_entry(); i >= segment.first_entry(); i--) {
+      auto payload = at(i).GetRawPayload();
+      if (payload.ContainsEvacuationEntry()) {
+        // Resolve the evacuation entry: take the pointer to the handle from the
+        // evacuation entry, copy the entry to its new location, and finally
+        // update the handle to point to the new entry.
+        // While we now know that the entry being evacuated is free, we don't
+        // add it to (the start of) the freelist because that would immediately
+        // cause new fragmentation when the next entry is allocated. Instead, we
+        // assume that the segments out of which entries are evacuated will all
+        // be decommitted anyway after this loop, which is usually the case
+        // unless compaction was already aborted during marking.
+        ExternalPointerHandle* handle_location =
+            reinterpret_cast<ExternalPointerHandle*>(
+                payload.ExtractEvacuationEntryHandleLocation());
+        ResolveEvacuationEntryDuringSweeping(i, handle_location,
+                                             start_of_evacuation_area);
+      } else if (!payload.HasMarkBitSet()) {
+        at(i).MakeFreelistEntry(current_freelist_head);
+        current_freelist_head = i;
+        current_freelist_length++;
+      } else {
+        auto new_payload = payload;
+        new_payload.ClearMarkBit();
+        at(i).SetRawPayload(new_payload);
+      }
     }
 
-    // If a block at the end of the table is completely empty, we can decommit
-    // it afterwards, thereby shrinking the table.
-    if (last_in_use_block == i && current_freelist_length == kEntriesPerBlock) {
-      last_in_use_block -= kEntriesPerBlock;
-      // This block will be decommitted, so the freelist is now empty again.
-      current_freelist_head = 0;
-      current_freelist_length = 0;
+    // If a segment is completely empty, free it.
+    uint32_t free_entries = current_freelist_length - previous_freelist_length;
+    bool segment_is_empty = free_entries == kEntriesPerSegment;
+    if (segment_is_empty) {
+      FreeTableSegment(segment);
+      freed_segments.push_back(segment);
+      // Restore the state of the freelist before this segment.
+      current_freelist_head = previous_freelist_head;
+      current_freelist_length = previous_freelist_length;
     }
   }
 
-  // Decommit all blocks at the end of the table that are not used anymore.
-  if (last_in_use_block != last_block) {
-    uint32_t new_capacity = last_in_use_block + kEntriesPerBlock;
-    Shrink(new_capacity);
-    DCHECK_EQ(new_capacity, capacity());
+  // We cannot remove the segments while iterating over the segments set, so
+  // only do that now.
+  for (auto segment : freed_segments) {
+    space->segments_.erase(segment);
   }
 
   FreelistHead new_freelist(current_freelist_head, current_freelist_length);
-  freelist_head_.store(new_freelist, std::memory_order_release);
-  DCHECK_EQ(freelist_length(), current_freelist_length);
+  space->freelist_head_.store(new_freelist, std::memory_order_release);
+  DCHECK_EQ(space->freelist_length(), current_freelist_length);
 
-  uint32_t num_live_entries = capacity() - current_freelist_length;
-  isolate->counters()->external_pointers_count()->AddSample(num_live_entries);
+  uint32_t num_live_entries = space->capacity() - current_freelist_length;
+  counters->external_pointers_count()->AddSample(num_live_entries);
   return num_live_entries;
 }
 
-void ExternalPointerTable::StartCompactingIfNeeded() {
+void ExternalPointerTable::Space::StartCompactingIfNeeded() {
   // This method may be executed while other threads allocate entries from the
-  // freelist or even grow the table, thereby increasing the capacity. In that
-  // case, this method may use incorrect data to determine if table compaction
-  // is necessary. That's fine however since in the worst case, compaction will
-  // simply be aborted right away if the freelist became too small.
-  uint32_t current_freelist_length = freelist_length();
-  uint32_t current_capacity = capacity();
-
-  // Current (somewhat arbitrary) heuristic: need compacting if the table is
-  // more than 1MB in size, is at least 10% empty, and if at least one block
-  // can be decommitted after successful compaction.
-  uint32_t table_size = current_capacity * kSystemPointerSize;
-  double free_ratio = static_cast<double>(current_freelist_length) /
-                      static_cast<double>(current_capacity);
-  uint32_t num_blocks_to_evacuate =
-      (current_freelist_length / 2) / kEntriesPerBlock;
-
-  bool should_compact = (table_size >= 1 * MB) && (free_ratio >= 0.10) &&
-                        (num_blocks_to_evacuate >= 1);
+  // freelist or even expand the space. In that case, this method may use
+  // incorrect data to determine if table compaction is necessary. That's fine
+  // however since in the worst case, compaction will simply be aborted right
+  // away if the freelist became too small.
+  uint32_t num_free_entries = freelist_length();
+  uint32_t num_total_entries = num_segments() * kEntriesPerSegment;
+
+  // Current (somewhat arbitrary) heuristic: need compacting if the space is
+  // more than 1MB in size, is at least 10% empty, and if at least one segment
+  // can be freed after successful compaction.
+  double free_ratio = static_cast<double>(num_free_entries) /
+                      static_cast<double>(num_total_entries);
+  uint32_t num_segments_to_evacuate =
+      (num_free_entries / 2) / kEntriesPerSegment;
+
+  uint32_t space_size = num_total_entries * kEntrySize;
+  bool should_compact = (space_size >= 1 * MB) && (free_ratio >= 0.10) &&
+                        (num_segments_to_evacuate >= 1);
 
   if (should_compact) {
-    uint32_t num_entries_to_evacuate =
-        num_blocks_to_evacuate * kEntriesPerBlock;
-    uint32_t start_of_evacuation_area =
-        current_capacity - num_entries_to_evacuate;
+    // If we're compacting, attempt to free up the last N segments so that they
+    // can be decommitted afterwards.
+    base::MutexGuard guard(&mutex_);
+    Segment first_segment_to_evacuate =
+        *std::prev(segments_.end(), num_segments_to_evacuate);
+    uint32_t start_of_evacuation_area = first_segment_to_evacuate.first_entry();
     StartCompacting(start_of_evacuation_area);
   }
 }
diff --git a/src/sandbox/external-pointer-table.h b/src/sandbox/external-pointer-table.h
index 4a70eea6bb5..c50397f3f2a 100644
--- a/src/sandbox/external-pointer-table.h
+++ b/src/sandbox/external-pointer-table.h
@@ -18,6 +18,7 @@ namespace v8 {
 namespace internal {
 
 class Isolate;
+class Counters;
 
 /**
  * The entries of an ExternalPointerTable.
@@ -184,7 +185,11 @@ constexpr size_t kExternalPointerTableReservationSizeAfterAccountingForLSan =
  *
  * When V8_ENABLE_SANDBOX, its primary use is for pointing to objects outside
  * the sandbox, as described below.
+ * When V8_COMPRESS_POINTERS, external pointer tables are also used to ease
+ * alignment requirements in heap object fields via indirection.
  *
+ * A table's role for the V8 Sandbox:
+ * --------------------------------
  * An external pointer table provides the basic mechanisms to ensure
  * memory-safe access to objects located outside the sandbox, but referenced
  * from within it. When an external pointer table is used, objects located
@@ -200,11 +205,18 @@ constexpr size_t kExternalPointerTableReservationSizeAfterAccountingForLSan =
  * which ensures that every entry is either an invalid pointer or a valid
  * pointer pointing to a live object.
  *
- * Spatial memory safety can, if necessary, be ensured by storing the size of a
- * referenced object together with the object itself outside the sandbox, and
- * referencing both through a single entry in the table.
+ * Spatial memory safety can, if necessary, be ensured either by storing the
+ * size of the referenced object together with the object itself outside the
+ * sandbox, or by storing both the pointer and the size in one (double-width)
+ * table entry.
  *
- * The garbage collection algorithm for the table works as follows:
+ * Table memory management:
+ * ------------------------
+ * For the purpose of memory management, the table is partitioned into Segments
+ * (for example 64kb memory chunks) that are grouped together in "Spaces". All
+ * segments in a space share a freelist, and so entry allocation and garbage
+ * collection happen on the level of spaces. The garbage collection algorithm
+ * then works as follows:
  *  - One bit of every entry is reserved for the marking bit.
  *  - Every store to an entry automatically sets the marking bit when ORing
  *    with the tag. This avoids the need for write barriers.
@@ -213,28 +225,129 @@ constexpr size_t kExternalPointerTableReservationSizeAfterAccountingForLSan =
  *  - When the GC marking visitor finds a live object with an external pointer,
  *    it marks the corresponding entry as alive through Mark(), which sets the
  *    marking bit using an atomic CAS operation.
- *  - When marking is finished, SweepAndCompact() iterates of the table once
+ *  - When marking is finished, SweepAndCompact() iterates over a Space once
  *    while the mutator is stopped and builds a freelist from all dead entries
  *    while also removing the marking bit from any live entry.
  *
- * When V8_COMPRESS_POINTERS, external pointer tables are also used to ease
- * alignment requirements in heap object fields via indirection.
+ * Table compaction:
+ * -----------------
+ * The table's spaces are to some degree self-compacting: since the freelists
+ * are sorted in ascending order (see SweepAndCompact()), segments at the start
+ * of the table will usually be fairly well utilized, while later segments
+ * might become completely free, in which case they will be deallocated.
+ * However, as a single live entry may keep an entire segment alive, the
+ * following simple algorithm is used to compact a space if that is deemed
+ * necessary:
+ *  - At the start of the GC marking phase, determine if a space needs to be
+ *    compacted. This decisiont is mostly based on the absolute and relative
+ *    size of the freelist.
+ *  - If compaction is needed, this algorithm determines by how many segments
+ *    it would like to shrink the space (N). It will then attempts to move all
+ *    live entries out of these segments so that they can be deallocated
+ *    afterwards during sweeping.
+ *  - The algorithm then simply selects the last N segments for evacuation, and
+ *    it "marks" them for evacuation simply by remembering the start of the
+ *    first selected segment. Everything after this threshold value then
+ *    becomes the evacuation area. In this way, it becomes very cheap to test
+ *    if an entry or segment should be evacuated: only a single integer
+ *    comparison against the threshold is required. It also establishes a
+ *    simple compaction invariant that can be verified with a few DCHECKs:
+ *    compaction always moves an entry at or above the threshold to a new
+ *    position before the threshold.
+ *  - During marking, whenever a live entry inside the evacuation area is
+ *    found, a new "evacuation entry" is allocated from the freelist (which is
+ *    assumed to have enough free slots) and the address of the handle in the
+ *    object owning the table entry is written into it.
+ *  - During sweeping, these evacuation entries are resolved: the content of
+ *    the old entry is copied into the new entry and the handle in the object
+ *    is updated to point to the new entry.
+ *
+ * When compacting, it is expected that the evacuation area contains few live
+ * entries and that the freelist will be able to serve all evacuation entry
+ * allocations. In that case, compaction is essentially free (very little
+ * marking overhead, no memory overhead). However, it can happen that the
+ * application allocates a large number of table entries during marking, in
+ * which case we might end up allocating new entries inside the evacuation area
+ * or even allocate entire new segments for the space that's being compacted.
+ * If that situation is detected, compaction is aborted during marking.
+ *
+ * This algorithm assumes that table entries (except for the null entry) are
+ * never shared between multiple objects. Otherwise, the following could
+ * happen: object A initially has handle H1 and is scanned during incremental
+ * marking. Next, object B with handle H2 is scanned and marked for
+ * evacuation. Afterwards, object A copies the handle H2 from object B.
+ * During sweeping, only object B's handle will be updated to point to the
+ * new entry while object A's handle is now dangling. If shared entries ever
+ * become necessary, setting external pointer handles would have to be
+ * guarded by write barriers to avoid this scenario.
  */
 class V8_EXPORT_PRIVATE ExternalPointerTable
     : public ExternalEntityTable<
           ExternalPointerTableEntry,
           kExternalPointerTableReservationSizeAfterAccountingForLSan> {
+  static_assert(kMaxExternalPointers == kMaxCapacity);
+
  public:
   // Size of an ExternalPointerTable, for layout computation in IsolateData.
-  static int constexpr kSize = 4 * kSystemPointerSize;
-  static_assert(kMaxExternalPointers == kMaxCapacity);
+  static int constexpr kSize = 2 * kSystemPointerSize;
 
   ExternalPointerTable() = default;
   ExternalPointerTable(const ExternalPointerTable&) = delete;
   ExternalPointerTable& operator=(const ExternalPointerTable&) = delete;
 
-  // Initializes this external pointer table by reserving the backing memory
-  // and initializing the freelist.
+  // The Spaces used by an ExternalPointerTable also contain the state related
+  // to compaction.
+  struct Space
+      : public ExternalEntityTable<
+            ExternalPointerTableEntry,
+            kExternalPointerTableReservationSizeAfterAccountingForLSan>::Space {
+   public:
+    Space() : start_of_evacuation_area_(kNotCompactingMarker) {}
+
+    // Determine if compaction is needed and if so start the compaction.
+    // This is expected to be called at the start of the GC marking phase.
+    void StartCompactingIfNeeded();
+
+   private:
+    friend class ExternalPointerTable;
+
+    // Routines for compaction. See the comment about table compaction above.
+    inline bool IsCompacting();
+    inline void StartCompacting(uint32_t start_of_evacuation_area);
+    inline void StopCompacting();
+    inline void AbortCompacting(uint32_t start_of_evacuation_area);
+    inline bool CompactingWasAbortedDuringMarking();
+
+    // This value indicates that this space is not currently being compacted. It
+    // is set to uint32_t max so that determining whether an entry should be
+    // evacuated becomes a single comparison:
+    // `bool should_be_evacuated = index >= start_of_evacuation_area`.
+    static constexpr uint32_t kNotCompactingMarker =
+        std::numeric_limits<uint32_t>::max();
+
+    // This value may be ORed into the start of evacuation area threshold
+    // during the GC marking phase to indicate that compaction has been
+    // aborted because the freelist grew to short and so evacuation entry
+    // allocation is no longer possible. This will prevent any further
+    // evacuation attempts as entries will be evacuated if their index is at or
+    // above the start of the evacuation area, which is now a huge value.
+    static constexpr uint32_t kCompactionAbortedMarker = 0xf0000000;
+
+    // When compacting this space, this field contains the index of the first
+    // entry in the evacuation area. The evacuation area then consists of all
+    // segments above this threshold, and the goal of compaction is to move all
+    // live entries out of these segments so that they can be deallocated after
+    // sweeping. The field can have the following values:
+    // - kNotCompactingMarker: compaction is not currently running.
+    // - A kEntriesPerSegment aligned value within: compaction is running and
+    //   all entries after this value should be evacuated.
+    // - A value that has kCompactionAbortedMarker in its top bits:
+    //   compaction has been aborted during marking. The original start of the
+    //   evacuation area is still contained in the lower bits.
+    std::atomic<uint32_t> start_of_evacuation_area_;
+  };
+
+  // Initializes this external pointer table by reserving the backing memory.
   void Init();
 
   // Resets this external pointer table and deletes all associated memory.
@@ -260,108 +373,54 @@ class V8_EXPORT_PRIVATE ExternalPointerTable
   inline Address Exchange(ExternalPointerHandle handle, Address value,
                           ExternalPointerTag tag);
 
-  // Allocates a new entry in the external pointer table. The caller must
-  // provide the initial value and tag.
+  // Allocates a new entry in the given space. The caller must provide the
+  // initial value and tag for the entry.
   //
   // This method is atomic and can be called from background threads.
   inline ExternalPointerHandle AllocateAndInitializeEntry(
-      Address initial_value, ExternalPointerTag tag);
+      Space* space, Address initial_value, ExternalPointerTag tag);
 
   // Marks the specified entry as alive.
   //
-  // If the table is currently being compacted, this may also mark the entry
-  // for evacuation for which the location of the handle is required. See the
-  // comments about table compaction below for more details.
+  // If the space to which the entry belongs is currently being compacted, this
+  // may also mark the entry for evacuation for which the location of the
+  // handle is required. See the comments about the compaction algorithm for
+  // more details.
   //
   // This method is atomic and can be called from background threads.
-  inline void Mark(ExternalPointerHandle handle, Address handle_location);
+  inline void Mark(Space* space, ExternalPointerHandle handle,
+                   Address handle_location);
 
-  // Table compaction.
-  //
-  // The table is to some degree self-compacting: since the freelist is
-  // sorted in ascending order (see SweepAndCompact()), empty slots at the start
-  // of the table will usually quickly be filled. Further, empty blocks at the
-  // end of the table will be decommitted to reduce memory usage. However, live
-  // entries at the end of the table can prevent this decommitting and cause
-  // fragmentation. The following simple algorithm is therefore used to
-  // compact the table if that is deemed necessary:
-  //  - At the start of the GC marking phase, determine if the table needs to be
-  //    compacted. This decisiont is mostly based on the absolute and relative
-  //    size of the freelist.
-  //  - If compaction is needed, this algorithm attempts to shrink the table by
-  //    FreelistSize/2 entries during compaction by moving all live entries out
-  //    of the evacuation area (the last FreelistSize/2 entries of the table),
-  //    then decommitting those blocks at the end of SweepAndCompact().
-  //  - During marking, whenever a live entry inside the evacuation area is
-  //    found, a new "evacuation entry" is allocated from the freelist (which is
-  //    assumed to have enough free slots) and the address of the handle is
-  //    written into it.
-  //  - During sweeping, these evacuation entries are resolved: the content of
-  //    the old entry is copied into the new entry and the handle in the object
-  //    is updated to point to the new entry.
-  //
-  // When compacting, it is expected that the evacuation area contains few live
-  // entries and that the freelist will be able to serve all evacuation entry
-  // allocations. In that case, compaction is essentially free (very little
-  // marking overhead, no memory overhead). However, it can happen that the
-  // application allocates a large number of entries from the table during
-  // marking, in which case the freelist would no longer be able to serve all
-  // allocation without growing. If that situation is detected, compaction is
-  // aborted during marking.
-  //
-  // This algorithm assumes that table entries (except for the null entry) are
-  // never shared between multiple objects. Otherwise, the following could
-  // happen: object A initially has handle H1 and is scanned during incremental
-  // marking. Next, object B with handle H2 is scanned and marked for
-  // evacuation. Afterwards, object A copies the handle H2 from object B.
-  // During sweeping, only object B's handle will be updated to point to the
-  // new entry while object A's handle is now dangling. If shared entries ever
-  // become necessary, setting external pointer handles would have to be
-  // guarded by write barriers to avoid this scenario.
-
-  // Frees unmarked entries and finishes table compaction (if running).
+  // Frees unmarked entries and finishes space compaction (if running).
   //
   // This method must only be called while mutator threads are stopped as it is
   // not safe to allocate table entries while the table is being swept.
   //
   // Returns the number of live entries after sweeping.
-  uint32_t SweepAndCompact(Isolate* isolate);
-
-  // Determine if table compaction is needed and if so start the compaction.
-  // This is expected to be called at the start of the GC marking phase.
-  void StartCompactingIfNeeded();
+  uint32_t SweepAndCompact(Space* space, Counters* counters);
 
  private:
   // Required for Isolate::CheckIsolateLayout().
   friend class Isolate;
 
+  inline bool IsValidHandle(ExternalPointerHandle handle) const;
   inline uint32_t HandleToIndex(ExternalPointerHandle handle) const;
   inline ExternalPointerHandle IndexToHandle(uint32_t index) const;
 
-  inline void MaybeCreateEvacuationEntry(uint32_t index,
+  inline void MaybeCreateEvacuationEntry(Space* space, uint32_t index,
                                          Address handle_location);
 
   void ResolveEvacuationEntryDuringSweeping(
       uint32_t index, ExternalPointerHandle* handle_location,
       uint32_t start_of_evacuation_area);
 
-  inline bool IsCompacting();
-
-  inline void StartCompacting(uint32_t start_of_evacuation_area);
-
-  inline void StopCompacting();
-
-  inline void AbortCompacting(uint32_t start_of_evacuation_area);
-
-  inline bool CompactingWasAbortedDuringMarking();
-
 #ifdef DEBUG
   // In debug builds during GC marking, this value is ORed into
   // ExternalPointerHandles whose entries are marked for evacuation. During
   // sweeping, the Handles for evacuated entries are checked to have this
   // marker value. This allows detecting re-initialized entries, which are
-  // problematic for table compaction. This is only possible for entries marked
-  // for evacuation as the location of the Handle is only known for those.
+  // problematic for compaction. This is only possible for entries marked for
+  // evacuation as the location of the Handle is only known for those.
   static constexpr uint32_t kVisitedHandleMarker = 0x1;
   static_assert(kExternalPointerIndexShift >= 1);
 
@@ -370,33 +429,6 @@ class V8_EXPORT_PRIVATE ExternalPointerTable
   }
 #endif  // DEBUG
 
-  // When compacting the table, the extra_ field in our parent class contains
-  // the index of the first entry in the evacuation area. The evacuation area is
-  // the region at the end of the table from which entries are moved out of so
-  // that the underyling memory pages can be freed after sweeping. The field can
-  // have the following values:
-  // - kNotCompactingMarker: compaction is not currently running.
-  // - A kEntriesPerBlock aligned value within (0, capacity): table compaction
-  //   is running and all entries after this value should be evacuated.
-  // - A value that has kCompactionAbortedMarker in its top bits: table
-  //   compaction has been aborted during marking. The original start of the
-  //   evacuation area is still contained in the lower bits.
-  //
-  // This value indicates that the table is not currently being compacted. It
-  // is set to uint32_t max so that determining whether an entry should be
-  // evacuated becomes a single comparison:
-  // `bool should_be_evacuated = index >= start_of_evacuation_area`.
-  static constexpr uint32_t kNotCompactingMarker =
-      std::numeric_limits<uint32_t>::max();
-
-  // This value may be ORed into the start of evacuation area threshold (stored
-  // in the extra_ field) during the GC marking phase to indicate that table
-  // compaction has been aborted because the freelist grew to short and so
-  // evacuation entry allocation is no longer possible. This will prevent any
-  // further evacuation attempts as entries will be evacuated if their index is
-  // at or above the start of the evacuation area, which is now a huge value.
-  static constexpr uint32_t kCompactionAbortedMarker = 0xf0000000;
-
   // Outcome of external pointer table compaction to use for the
   // ExternalPointerTableCompactionOutcome histogram.
   enum class TableCompactionOutcome {
-- 
2.35.1

