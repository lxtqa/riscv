From 7d3c3cfb1ad4856ea98b8c6687a89b08b3c1fe85 Mon Sep 17 00:00:00 2001
From: Camillo Bruni <cbruni@chromium.org>
Date: Wed, 23 Jun 2021 17:17:55 +0200
Subject: [PATCH] Reland "[assembler][arm64][arm][ia32] Add more
 Assembler::CodeComment"

This is a reland of 2588380d2cb0ef95527eb056d2b6195f77694d8e

Fixes arm compilation.

Original change's description:
> [assembler][arm64][arm][ia32] Add more Assembler::CodeComment
>
> Use more code comments to better document generated assembler code.
> This CL also attaches a CodeComment to FrameScope.
>
> Change-Id: I45afc3e4078dc161316b877fbbd1ae8bcd6e821d
> Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/2964392
> Reviewed-by: Nico Hartmann <nicohartmann@chromium.org>
> Reviewed-by: Victor Gomes <victorgomes@chromium.org>
> Commit-Queue: Camillo Bruni <cbruni@chromium.org>
> Cr-Commit-Position: refs/heads/master@{#75277}

Change-Id: I150c9166e0d31e4af857f76c567b829d16da87e4
Reviewed-on: https://chromium-review.googlesource.com/c/v8/v8/+/2976651
Reviewed-by: Victor Gomes <victorgomes@chromium.org>
Reviewed-by: Nico Hartmann <nicohartmann@chromium.org>
Commit-Queue: Camillo Bruni <cbruni@chromium.org>
Cr-Commit-Position: refs/heads/master@{#75351}
---
 src/baseline/arm/baseline-assembler-arm-inl.h |  45 +--
 src/baseline/arm/baseline-compiler-arm-inl.h  |   3 +-
 .../arm64/baseline-assembler-arm64-inl.h      |  17 +-
 .../arm64/baseline-compiler-arm64-inl.h       |   5 +-
 src/baseline/baseline-compiler.cc             |   9 +-
 .../ia32/baseline-assembler-ia32-inl.h        |  26 +-
 .../ia32/baseline-compiler-ia32-inl.h         |   3 +-
 .../mips/baseline-assembler-mips-inl.h        |  25 +-
 .../mips/baseline-compiler-mips-inl.h         |   5 +-
 .../mips64/baseline-assembler-mips64-inl.h    |  25 +-
 .../mips64/baseline-compiler-mips64-inl.h     |   5 +-
 .../riscv64/baseline-assembler-riscv64-inl.h  |  26 +-
 .../riscv64/baseline-compiler-riscv64-inl.h   |   5 +-
 src/baseline/x64/baseline-assembler-x64-inl.h |  33 ++-
 src/builtins/arm/builtins-arm.cc              | 123 ++++----
 src/builtins/arm64/builtins-arm64.cc          | 128 ++++----
 src/builtins/ia32/builtins-ia32.cc            | 102 ++++---
 src/builtins/mips/builtins-mips.cc            | 125 ++++----
 src/builtins/mips64/builtins-mips64.cc        | 115 ++++----
 src/builtins/ppc/builtins-ppc.cc              |  12 +-
 src/builtins/riscv64/builtins-riscv64.cc      | 128 ++++----
 src/builtins/s390/builtins-s390.cc            |  12 +-
 src/builtins/setup-builtins-internal.cc       |   6 +-
 src/builtins/x64/builtins-x64.cc              |  83 +++---
 src/codegen/arm/assembler-arm.cc              |   4 +-
 src/codegen/arm/macro-assembler-arm.cc        | 171 +++++++----
 src/codegen/arm64/assembler-arm64.cc          |   4 +-
 src/codegen/arm64/macro-assembler-arm64.cc    | 276 +++++++++++-------
 src/codegen/ia32/macro-assembler-ia32.cc      | 185 ++++++++----
 src/codegen/macro-assembler.h                 |  33 ++-
 src/codegen/x64/macro-assembler-x64.cc        |   8 +-
 31 files changed, 1022 insertions(+), 725 deletions(-)

diff --git a/src/baseline/arm/baseline-assembler-arm-inl.h b/src/baseline/arm/baseline-assembler-arm-inl.h
index 60b28ce8eb9..040761091ab 100644
--- a/src/baseline/arm/baseline-assembler-arm-inl.h
+++ b/src/baseline/arm/baseline-assembler-arm-inl.h
@@ -116,21 +116,21 @@ void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,
 
 void BaselineAssembler::CallBuiltin(Builtin builtin) {
   //  __ CallBuiltin(static_cast<int>(builtin));
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("call", builtin));
   ScratchRegisterScope temps(this);
   Register temp = temps.AcquireScratch();
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Call(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("tail call", builtin));
   ScratchRegisterScope temps(this);
   Register temp = temps.AcquireScratch();
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Jump(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,
@@ -360,6 +360,7 @@ void BaselineAssembler::LoadByteField(Register output, Register source,
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
                                                Smi value) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
   __ mov(tmp, Operand(value));
@@ -368,6 +369,7 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   DCHECK(!AreAliased(target, value));
   __ str(value, FieldMemOperand(target, offset));
   __ RecordWriteField(target, offset, value, kLRHasNotBeenSaved,
@@ -381,6 +383,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -403,6 +406,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -425,6 +429,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base != 0) {
     __ sub(reg, reg, Operand(case_value_base));
@@ -451,30 +456,32 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
-    __ masm()->SmiTag(params_size);
-    __ Push(params_size, kInterpreterAccumulatorRegister);
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
 
-    __ LoadContext(kContextRegister);
-    __ LoadFunction(kJSFunctionRegister);
-    __ Push(kJSFunctionRegister);
-    __ CallRuntime(Runtime::kBytecodeBudgetInterruptFromBytecode, 1);
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
+    {
+      __ masm()->SmiTag(params_size);
+      __ Push(params_size, kInterpreterAccumulatorRegister);
 
-    __ Pop(kInterpreterAccumulatorRegister, params_size);
-    __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
+      __ LoadContext(kContextRegister);
+      __ LoadFunction(kJSFunctionRegister);
+      __ Push(kJSFunctionRegister);
+      __ CallRuntime(Runtime::kBytecodeBudgetInterruptFromBytecode, 1);
 
-  __ Bind(&skip_interrupt_label);
+      __ Pop(kInterpreterAccumulatorRegister, params_size);
+      __ masm()->SmiUntag(params_size);
+    }
+
+    __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope temps(&basm);
   Register actual_params_size = temps.AcquireScratch();
diff --git a/src/baseline/arm/baseline-compiler-arm-inl.h b/src/baseline/arm/baseline-compiler-arm-inl.h
index 3e0b5bfcd18..0fc2389c35c 100644
--- a/src/baseline/arm/baseline-compiler-arm-inl.h
+++ b/src/baseline/arm/baseline-compiler-arm-inl.h
@@ -27,7 +27,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -77,7 +77,6 @@ void BaselineCompiler::PrologueFillFrame() {
     __ masm()->sub(scratch, scratch, Operand(1), SetCC);
     __ masm()->b(gt, &loop);
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
diff --git a/src/baseline/arm64/baseline-assembler-arm64-inl.h b/src/baseline/arm64/baseline-assembler-arm64-inl.h
index 10135afe9f7..cda21083270 100644
--- a/src/baseline/arm64/baseline-assembler-arm64-inl.h
+++ b/src/baseline/arm64/baseline-assembler-arm64-inl.h
@@ -433,6 +433,7 @@ void BaselineAssembler::LoadByteField(Register output, Register source,
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
                                                Smi value) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
   __ Mov(tmp, Operand(value));
@@ -441,6 +442,7 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   __ StoreTaggedField(value, FieldMemOperand(target, offset));
   __ RecordWriteField(target, offset, value, kLRHasNotBeenSaved,
                       SaveFPRegsMode::kIgnore);
@@ -453,6 +455,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -475,6 +478,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -502,6 +506,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base != 0) {
     __ Sub(reg, reg, Immediate(case_value_base));
@@ -535,16 +540,17 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
+
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
     __ masm()->SmiTag(params_size);
     __ masm()->Push(params_size, kInterpreterAccumulatorRegister);
 
@@ -555,10 +561,9 @@ void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
 
     __ masm()->Pop(kInterpreterAccumulatorRegister, params_size);
     __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
 
   __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope temps(&basm);
   Register actual_params_size = temps.AcquireScratch();
diff --git a/src/baseline/arm64/baseline-compiler-arm64-inl.h b/src/baseline/arm64/baseline-compiler-arm64-inl.h
index 1637c259cc4..333c0a2a342 100644
--- a/src/baseline/arm64/baseline-compiler-arm64-inl.h
+++ b/src/baseline/arm64/baseline-compiler-arm64-inl.h
@@ -14,6 +14,7 @@ namespace baseline {
 #define __ basm_.
 
 void BaselineCompiler::Prologue() {
+  ASM_CODE_COMMENT(&masm_);
   // Enter the frame here, since CallBuiltin will override lr.
   __ masm()->EnterFrame(StackFrame::BASELINE);
   DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);
@@ -28,7 +29,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -98,10 +99,10 @@ void BaselineCompiler::PrologueFillFrame() {
     __ masm()->Subs(scratch, scratch, 1);
     __ masm()->B(gt, &loop);
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->Add(x15, sp,
                  RoundUp(InterpreterFrameConstants::kFixedFrameSizeFromFp +
                              bytecode_->frame_size(),
diff --git a/src/baseline/baseline-compiler.cc b/src/baseline/baseline-compiler.cc
index ca795ab13b6..899ecf68aa4 100644
--- a/src/baseline/baseline-compiler.cc
+++ b/src/baseline/baseline-compiler.cc
@@ -529,16 +529,15 @@ void BaselineCompiler::VerifyFrame() {
 #ifdef V8_TRACE_UNOPTIMIZED
 void BaselineCompiler::TraceBytecode(Runtime::FunctionId function_id) {
   if (!FLAG_trace_baseline_exec) return;
-
-  __ RecordComment(function_id == Runtime::kTraceUnoptimizedBytecodeEntry
-                       ? "[ Trace bytecode entry"
-                       : "[ Trace bytecode exit");
+  ASM_CODE_COMMENT_STRING(&masm_,
+                          function_id == Runtime::kTraceUnoptimizedBytecodeEntry
+                              ? "Trace bytecode entry"
+                              : "Trace bytecode exit");
   SaveAccumulatorScope accumulator_scope(&basm_);
   CallRuntime(function_id, bytecode_,
               Smi::FromInt(BytecodeArray::kHeaderSize - kHeapObjectTag +
                            iterator().current_offset()),
               kInterpreterAccumulatorRegister);
-  __ RecordComment("]");
 }
 #endif
 
diff --git a/src/baseline/ia32/baseline-assembler-ia32-inl.h b/src/baseline/ia32/baseline-assembler-ia32-inl.h
index aaa2ef93a44..e3f991886db 100644
--- a/src/baseline/ia32/baseline-assembler-ia32-inl.h
+++ b/src/baseline/ia32/baseline-assembler-ia32-inl.h
@@ -116,15 +116,15 @@ void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,
 }
 
 void BaselineAssembler::CallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("call", builtin));
   __ Call(__ EntryFromBuiltinAsOperand(builtin));
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("tail call", builtin));
   __ jmp(__ EntryFromBuiltinAsOperand(builtin));
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,
@@ -336,8 +336,8 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 }
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
-
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   BaselineAssembler::ScratchRegisterScope scratch_scope(this);
   Register scratch = scratch_scope.AcquireScratch();
   DCHECK(!AreAliased(scratch, target, value));
@@ -353,6 +353,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -368,6 +369,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   DCHECK(!AreAliased(feedback_cell, weight));
@@ -386,6 +388,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scope(this);
   Register table = scope.AcquireScratch();
   DCHECK(!AreAliased(reg, table));
@@ -410,16 +413,16 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
-
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
+
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
     __ masm()->SmiTag(params_size);
     __ Push(params_size, kInterpreterAccumulatorRegister);
 
@@ -429,10 +432,9 @@ void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
 
     __ Pop(kInterpreterAccumulatorRegister, params_size);
     __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
 
   __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope scope(&basm);
   Register scratch = scope.AcquireScratch();
diff --git a/src/baseline/ia32/baseline-compiler-ia32-inl.h b/src/baseline/ia32/baseline-compiler-ia32-inl.h
index dc9cea7d34b..6ce19ec60d4 100644
--- a/src/baseline/ia32/baseline-compiler-ia32-inl.h
+++ b/src/baseline/ia32/baseline-compiler-ia32-inl.h
@@ -26,7 +26,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -74,7 +74,6 @@ void BaselineCompiler::PrologueFillFrame() {
     __ masm()->dec(scratch);
     __ masm()->j(greater, &loop);
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
diff --git a/src/baseline/mips/baseline-assembler-mips-inl.h b/src/baseline/mips/baseline-assembler-mips-inl.h
index 581fad2d7a5..f0ea4b04a19 100644
--- a/src/baseline/mips/baseline-assembler-mips-inl.h
+++ b/src/baseline/mips/baseline-assembler-mips-inl.h
@@ -112,19 +112,19 @@ void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,
 }
 
 void BaselineAssembler::CallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("call", builtin));
   Register temp = t9;
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Call(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("tail call", builtin));
   Register temp = t9;
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Jump(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,
@@ -357,6 +357,7 @@ void BaselineAssembler::LoadByteField(Register output, Register source,
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
                                                Smi value) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
   __ li(scratch, Operand(value));
@@ -365,6 +366,7 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   __ Sw(value, FieldMemOperand(target, offset));
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
@@ -379,6 +381,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -398,6 +401,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 }
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -420,6 +424,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base > 0) {
     __ Subu(reg, reg, Operand(case_value_base));
@@ -451,16 +456,17 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
+
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
     __ masm()->SmiTag(params_size);
     __ masm()->Push(params_size, kInterpreterAccumulatorRegister);
 
@@ -471,10 +477,9 @@ void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
 
     __ masm()->Pop(params_size, kInterpreterAccumulatorRegister);
     __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
 
   __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope temps(&basm);
   Register actual_params_size = temps.AcquireScratch();
diff --git a/src/baseline/mips/baseline-compiler-mips-inl.h b/src/baseline/mips/baseline-compiler-mips-inl.h
index 25108d37c7e..6897d9b48cb 100644
--- a/src/baseline/mips/baseline-compiler-mips-inl.h
+++ b/src/baseline/mips/baseline-compiler-mips-inl.h
@@ -15,6 +15,7 @@ namespace baseline {
 #define __ basm_.
 
 void BaselineCompiler::Prologue() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->EnterFrame(StackFrame::BASELINE);
   DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);
   int max_frame_size = bytecode_->frame_size() + max_call_args_;
@@ -26,7 +27,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -58,10 +59,10 @@ void BaselineCompiler::PrologueFillFrame() {
       __ masm()->Sw(kInterpreterAccumulatorRegister, MemOperand(sp, i * 4));
     }
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->Addu(kScratchReg, sp,
                   Operand(InterpreterFrameConstants::kFixedFrameSizeFromFp +
                           bytecode_->frame_size()));
diff --git a/src/baseline/mips64/baseline-assembler-mips64-inl.h b/src/baseline/mips64/baseline-assembler-mips64-inl.h
index 1cbb03c9741..69c70d961e8 100644
--- a/src/baseline/mips64/baseline-assembler-mips64-inl.h
+++ b/src/baseline/mips64/baseline-assembler-mips64-inl.h
@@ -110,19 +110,19 @@ void BaselineAssembler::JumpIfNotSmi(Register value, Label* target,
 }
 
 void BaselineAssembler::CallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("call", builtin));
   Register temp = t9;
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Call(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
-  __ RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(masm_,
+                          __ CommentForOffHeapTrampoline("tail call", builtin));
   Register temp = t9;
   __ LoadEntryFromBuiltin(builtin, temp);
   __ Jump(temp);
-  __ RecordComment("]");
 }
 
 void BaselineAssembler::TestAndBranch(Register value, int mask, Condition cc,
@@ -355,6 +355,7 @@ void BaselineAssembler::LoadByteField(Register output, Register source,
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
                                                Smi value) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
   __ li(scratch, Operand(value));
@@ -363,6 +364,7 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   __ Sd(value, FieldMemOperand(target, offset));
   ScratchRegisterScope temps(this);
   Register scratch = temps.AcquireScratch();
@@ -377,6 +379,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -396,6 +399,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 }
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -418,6 +422,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base > 0) {
     __ Dsubu(reg, reg, Operand(case_value_base));
@@ -449,16 +454,17 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
+
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
     __ masm()->SmiTag(params_size);
     __ masm()->Push(params_size, kInterpreterAccumulatorRegister);
 
@@ -469,10 +475,9 @@ void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
 
     __ masm()->Pop(params_size, kInterpreterAccumulatorRegister);
     __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
 
   __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope temps(&basm);
   Register actual_params_size = temps.AcquireScratch();
diff --git a/src/baseline/mips64/baseline-compiler-mips64-inl.h b/src/baseline/mips64/baseline-compiler-mips64-inl.h
index de8e9fcc42e..5971e17aa42 100644
--- a/src/baseline/mips64/baseline-compiler-mips64-inl.h
+++ b/src/baseline/mips64/baseline-compiler-mips64-inl.h
@@ -15,6 +15,7 @@ namespace baseline {
 #define __ basm_.
 
 void BaselineCompiler::Prologue() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->EnterFrame(StackFrame::BASELINE);
   DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);
   int max_frame_size = bytecode_->frame_size() + max_call_args_;
@@ -26,7 +27,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -58,10 +59,10 @@ void BaselineCompiler::PrologueFillFrame() {
       __ masm()->Sd(kInterpreterAccumulatorRegister, MemOperand(sp, i * 8));
     }
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->Daddu(kScratchReg, sp,
                    Operand(InterpreterFrameConstants::kFixedFrameSizeFromFp +
                            bytecode_->frame_size()));
diff --git a/src/baseline/riscv64/baseline-assembler-riscv64-inl.h b/src/baseline/riscv64/baseline-assembler-riscv64-inl.h
index 77d861b173b..27970bd6181 100644
--- a/src/baseline/riscv64/baseline-assembler-riscv64-inl.h
+++ b/src/baseline/riscv64/baseline-assembler-riscv64-inl.h
@@ -112,11 +112,11 @@ void BaselineAssembler::CallBuiltin(Builtin builtin) {
   if (masm()->options().short_builtin_calls) {
     __ CallBuiltin(builtin);
   } else {
-    __ RecordCommentForOffHeapTrampoline(builtin);
+    ASM_CODE_COMMENT_STRING(masm_,
+                            __ CommentForOffHeapTrampoline("call", builtin));
     Register temp = t6;
     __ LoadEntryFromBuiltin(builtin, temp);
     __ Call(temp);
-    __ RecordComment("]");
   }
 }
 
@@ -125,13 +125,13 @@ void BaselineAssembler::TailCallBuiltin(Builtin builtin) {
     // Generate pc-relative jump.
     __ TailCallBuiltin(builtin);
   } else {
-    __ RecordCommentForOffHeapTrampoline(builtin);
+    ASM_CODE_COMMENT_STRING(
+        masm_, __ CommentForOffHeapTrampoline("tail call", builtin));
     // t6 be used for function call in RISCV64
     // For example 'jalr t6' or 'jal t6'
     Register temp = t6;
     __ LoadEntryFromBuiltin(builtin, temp);
     __ Jump(temp);
-    __ RecordComment("]");
   }
 }
 
@@ -469,6 +469,7 @@ void BaselineAssembler::LoadByteField(Register output, Register source,
 }
 void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
                                                Smi value) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope temps(this);
   Register tmp = temps.AcquireScratch();
   __ li(tmp, Operand(value));
@@ -479,6 +480,7 @@ void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
   // FIXME(riscv64): riscv64 don't implement pointer compressed
+  ASM_CODE_COMMENT(masm_);
   __ Sd(value, FieldMemOperand(target, offset));
   __ RecordWriteField(target, offset, value, kRAHasNotBeenSaved,
                       SaveFPRegsMode::kIgnore);
@@ -492,6 +494,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -513,6 +516,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -531,6 +535,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 }
 
 void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
+  ASM_CODE_COMMENT(&masm_);
   if (SmiValuesAre31Bits()) {
     __ Add32(lhs, lhs, Operand(rhs));
   } else {
@@ -540,6 +545,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   Label fallthrough;
   if (case_value_base != 0) {
     __ Sub64(reg, reg, Operand(case_value_base));
@@ -580,16 +586,17 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
+
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
     __ masm()->SmiTag(params_size);
     __ masm()->Push(params_size, kInterpreterAccumulatorRegister);
 
@@ -600,10 +607,9 @@ void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
 
     __ masm()->Pop(kInterpreterAccumulatorRegister, params_size);
     __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
 
   __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope temps(&basm);
   Register actual_params_size = temps.AcquireScratch();
diff --git a/src/baseline/riscv64/baseline-compiler-riscv64-inl.h b/src/baseline/riscv64/baseline-compiler-riscv64-inl.h
index c631c4c33f6..deb5aba0efb 100644
--- a/src/baseline/riscv64/baseline-compiler-riscv64-inl.h
+++ b/src/baseline/riscv64/baseline-compiler-riscv64-inl.h
@@ -14,6 +14,7 @@ namespace baseline {
 #define __ basm_.
 
 void BaselineCompiler::Prologue() {
+  ASM_CODE_COMMENT(&masm_);
   // Enter the frame here, since CallBuiltin will override lr.
   __ masm()->EnterFrame(StackFrame::BASELINE);
   DCHECK_EQ(kJSFunctionRegister, kJavaScriptCallTargetRegister);
@@ -25,7 +26,7 @@ void BaselineCompiler::Prologue() {
 }
 
 void BaselineCompiler::PrologueFillFrame() {
-  __ RecordComment("[ Fill frame");
+  ASM_CODE_COMMENT(&masm_);
   // Inlined register frame fill
   interpreter::Register new_target_or_generator_register =
       bytecode_->incoming_new_target_or_generator_register();
@@ -90,10 +91,10 @@ void BaselineCompiler::PrologueFillFrame() {
     }
     __ masm()->Branch(&loop, gt, scratch, Operand(1));
   }
-  __ RecordComment("]");
 }
 
 void BaselineCompiler::VerifyFrameSize() {
+  ASM_CODE_COMMENT(&masm_);
   __ masm()->Add64(kScratchReg, sp,
                    RoundUp(InterpreterFrameConstants::kFixedFrameSizeFromFp +
                                bytecode_->frame_size(),
diff --git a/src/baseline/x64/baseline-assembler-x64-inl.h b/src/baseline/x64/baseline-assembler-x64-inl.h
index 20228431cdc..f18ac84eaee 100644
--- a/src/baseline/x64/baseline-assembler-x64-inl.h
+++ b/src/baseline/x64/baseline-assembler-x64-inl.h
@@ -344,6 +344,7 @@ void BaselineAssembler::StoreTaggedSignedField(Register target, int offset,
 void BaselineAssembler::StoreTaggedFieldWithWriteBarrier(Register target,
                                                          int offset,
                                                          Register value) {
+  ASM_CODE_COMMENT(masm_);
   Register scratch = WriteBarrierDescriptor::SlotAddressRegister();
   DCHECK(!AreAliased(target, value, scratch));
   __ StoreTaggedField(FieldOperand(target, offset), value);
@@ -357,6 +358,7 @@ void BaselineAssembler::StoreTaggedFieldNoWriteBarrier(Register target,
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     int32_t weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -372,6 +374,7 @@ void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
 
 void BaselineAssembler::AddToInterruptBudgetAndJumpIfNotExceeded(
     Register weight, Label* skip_interrupt_label) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scratch_scope(this);
   Register feedback_cell = scratch_scope.AcquireScratch();
   LoadFunction(feedback_cell);
@@ -396,6 +399,7 @@ void BaselineAssembler::AddSmi(Register lhs, Smi rhs) {
 
 void BaselineAssembler::Switch(Register reg, int case_value_base,
                                Label** labels, int num_labels) {
+  ASM_CODE_COMMENT(masm_);
   ScratchRegisterScope scope(this);
   Register table = scope.AcquireScratch();
   Label fallthrough, jump_table;
@@ -419,29 +423,30 @@ void BaselineAssembler::Switch(Register reg, int case_value_base,
 #define __ basm.
 
 void BaselineAssembler::EmitReturn(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   BaselineAssembler basm(masm);
 
   Register weight = BaselineLeaveFrameDescriptor::WeightRegister();
   Register params_size = BaselineLeaveFrameDescriptor::ParamsSizeRegister();
 
-  __ RecordComment("[ Update Interrupt Budget");
-
-  Label skip_interrupt_label;
-  __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
   {
-    __ masm()->SmiTag(params_size);
-    __ Push(params_size, kInterpreterAccumulatorRegister);
+    ASM_CODE_COMMENT_STRING(masm, "Update Interrupt Budget");
 
-    __ LoadContext(kContextRegister);
-    __ Push(MemOperand(rbp, InterpreterFrameConstants::kFunctionOffset));
-    __ CallRuntime(Runtime::kBytecodeBudgetInterruptFromBytecode, 1);
+    Label skip_interrupt_label;
+    __ AddToInterruptBudgetAndJumpIfNotExceeded(weight, &skip_interrupt_label);
+    {
+      __ masm()->SmiTag(params_size);
+      __ Push(params_size, kInterpreterAccumulatorRegister);
 
-    __ Pop(kInterpreterAccumulatorRegister, params_size);
-    __ masm()->SmiUntag(params_size);
-  }
-  __ RecordComment("]");
+      __ LoadContext(kContextRegister);
+      __ Push(MemOperand(rbp, InterpreterFrameConstants::kFunctionOffset));
+      __ CallRuntime(Runtime::kBytecodeBudgetInterruptFromBytecode, 1);
 
-  __ Bind(&skip_interrupt_label);
+      __ Pop(kInterpreterAccumulatorRegister, params_size);
+      __ masm()->SmiUntag(params_size);
+    }
+    __ Bind(&skip_interrupt_label);
+  }
 
   BaselineAssembler::ScratchRegisterScope scope(&basm);
   Register scratch = scope.AcquireScratch();
diff --git a/src/builtins/arm/builtins-arm.cc b/src/builtins/arm/builtins-arm.cc
index 4dfe7ccb38f..6ebd820d771 100644
--- a/src/builtins/arm/builtins-arm.cc
+++ b/src/builtins/arm/builtins-arm.cc
@@ -314,8 +314,8 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Register sfi_data,
                                                     Register scratch1,
                                                     Label* is_baseline) {
+  ASM_CODE_COMMENT(masm);
   Label done;
-
   __ CompareObjectType(sfi_data, scratch1, scratch1, BASELINE_DATA_TYPE);
   __ b(eq, is_baseline);
   __ cmp(scratch1, Operand(INTERPRETER_DATA_TYPE));
@@ -793,6 +793,7 @@ void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {
 static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
                                                 Register optimized_code,
                                                 Register closure) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimized_code, closure));
   // Store code entry in the closure.
   __ str(optimized_code, FieldMemOperand(closure, JSFunction::kCodeOffset));
@@ -803,6 +804,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
 
 static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
                                   Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   Register params_size = scratch1;
   // Get the size of the formal parameters + receiver (in bytes).
   __ ldr(params_size,
@@ -834,6 +836,7 @@ static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
                                           Register actual_marker,
                                           OptimizationMarker expected_marker,
                                           Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   Label no_match;
   __ cmp_raw_immediate(actual_marker, expected_marker);
   __ b(ne, &no_match);
@@ -925,6 +928,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
                                           Register bytecode_offset,
                                           Register bytecode, Register scratch1,
                                           Register scratch2, Label* if_return) {
+  ASM_CODE_COMMENT(masm);
   Register bytecode_size_table = scratch1;
 
   // The bytecode offset value will be increased by one in wide and extra wide
@@ -1000,22 +1004,20 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state, Register feedback_vector,
     Label* has_optimized_code_or_marker) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
-  __ RecordComment("[ Check optimization state");
-
   __ ldr(optimization_state,
          FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));
   __ tst(
       optimization_state,
       Operand(FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask));
   __ b(ne, has_optimized_code_or_marker);
-
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     Register feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
   Label maybe_has_optimized_code;
   // Check if optimized code is available
@@ -1085,60 +1087,61 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                            FeedbackVector::kInvocationCountOffset));
   }
 
-  __ RecordComment("[ Frame Setup");
   FrameScope frame_scope(masm, StackFrame::MANUAL);
-  // Normally the first thing we'd do here is Push(lr, fp), but we already
-  // entered the frame in BaselineCompiler::Prologue, as we had to use the
-  // value lr before the call to this BaselineOutOfLinePrologue builtin.
-
-  Register callee_context = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext);
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  __ Push(callee_context, callee_js_function);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-
-  Register argc = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecodeArray = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
   {
-    UseScratchRegisterScope temps(masm);
-    Register scratch = temps.Acquire();
-    __ mov(scratch, Operand(0));
-    __ strh(scratch, FieldMemOperand(bytecodeArray,
-                                     BytecodeArray::kOsrNestingLevelOffset));
-  }
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    // Normally the first thing we'd do here is Push(lr, fp), but we already
+    // entered the frame in BaselineCompiler::Prologue, as we had to use the
+    // value lr before the call to this BaselineOutOfLinePrologue builtin.
+
+    Register callee_context = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext);
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    __ Push(callee_context, callee_js_function);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+
+    Register argc = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecodeArray = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    {
+      UseScratchRegisterScope temps(masm);
+      Register scratch = temps.Acquire();
+      __ mov(scratch, Operand(0));
+      __ strh(scratch, FieldMemOperand(bytecodeArray,
+                                       BytecodeArray::kOsrNestingLevelOffset));
+    }
 
-  __ Push(argc, bytecodeArray);
+    __ Push(argc, bytecodeArray);
 
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  if (FLAG_debug_code) {
-    UseScratchRegisterScope temps(masm);
-    Register scratch = temps.Acquire();
-    __ CompareObjectType(feedback_vector, scratch, scratch,
-                         FEEDBACK_VECTOR_TYPE);
-    __ Assert(eq, AbortReason::kExpectedFeedbackVector);
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    if (FLAG_debug_code) {
+      UseScratchRegisterScope temps(masm);
+      Register scratch = temps.Acquire();
+      __ CompareObjectType(feedback_vector, scratch, scratch,
+                           FEEDBACK_VECTOR_TYPE);
+      __ Assert(eq, AbortReason::kExpectedFeedbackVector);
+    }
+    __ Push(feedback_vector);
   }
-  __ Push(feedback_vector);
-  __ RecordComment("]");
 
-  __ RecordComment("[ Stack/interrupt check");
   Label call_stack_guard;
   Register frame_size = descriptor.GetRegisterParameter(
       BaselineOutOfLinePrologueDescriptor::kStackFrameSize);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check");
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1152,7 +1155,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     __ LoadStackLimit(interrupt_limit, StackLimitKind::kInterruptStackLimit);
     __ cmp(sp_minus_frame_size, interrupt_limit);
     __ b(&call_stack_guard, lo);
-    __ RecordComment("]");
   }
 
   // Do "fast" return to the caller pc in lr.
@@ -1161,30 +1163,28 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     UseScratchRegisterScope temps(masm);
     // Ensure the optimization_state is not allocated again.
     temps.Exclude(optimization_state);
 
-    __ RecordComment("[ Optimized marker check");
     // Drop the frame created by the baseline call.
     __ ldm(ia_w, sp, fp.bit() | lr.bit());
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     FrameScope frame_scope(masm, StackFrame::INTERNAL);
-    __ RecordComment("[ Stack/interrupt call");
     // Save incoming new target or generator
     __ Push(kJavaScriptCallNewTargetRegister);
     __ SmiTag(frame_size);
     __ Push(frame_size);
     __ CallRuntime(Runtime::kStackGuardWithGap);
     __ Pop(kJavaScriptCallNewTargetRegister);
-    __ RecordComment("]");
   }
 
   __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
@@ -1432,10 +1432,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ bkpt(0);  // Should not return.
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch) {
+  ASM_CODE_COMMENT(masm);
   // Find the argument with lowest address.
   __ sub(scratch, num_args, Operand(1));
   __ mov(scratch, Operand(scratch, LSL, kSystemPointerSizeLog2));
@@ -1474,7 +1474,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // Push the arguments. r2 and r4 will be modified.
-  Generate_InterpreterPushArgs(masm, r3, r2, r4);
+  GenerateInterpreterPushArgs(masm, r3, r2, r4);
 
   // Push "undefined" as the receiver arg if we need to.
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
@@ -1528,7 +1528,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // Push the arguments. r4 and r5 will be modified.
-  Generate_InterpreterPushArgs(masm, r0, r4, r5);
+  GenerateInterpreterPushArgs(masm, r0, r4, r5);
 
   // Push a slot for the receiver to be constructed.
   __ mov(r5, Operand::Zero());
@@ -1793,6 +1793,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
 }
 
 void OnStackReplacement(MacroAssembler* masm, bool is_interpreter) {
+  ASM_CODE_COMMENT(masm);
   {
     FrameAndConstantPoolScope scope(masm, StackFrame::INTERNAL);
     __ CallRuntime(Runtime::kCompileForOnStackReplacement);
@@ -2289,6 +2290,7 @@ void Builtins::Generate_CallFunction(MacroAssembler* masm,
 namespace {
 
 void Generate_PushBoundArguments(MacroAssembler* masm) {
+  ASM_CODE_COMMENT(masm);
   // ----------- S t a t e -------------
   //  -- r0 : the number of arguments (not including the receiver)
   //  -- r1 : target (checked to be a JSBoundFunction)
@@ -2906,6 +2908,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
                               ExternalReference thunk_ref, int stack_space,
                               MemOperand* stack_space_operand,
                               MemOperand return_value_operand) {
+  ASM_CODE_COMMENT(masm);
   Isolate* isolate = masm->isolate();
   ExternalReference next_address =
       ExternalReference::handle_scope_next_address(isolate);
diff --git a/src/builtins/arm64/builtins-arm64.cc b/src/builtins/arm64/builtins-arm64.cc
index fae4e8d9bdb..80ad1aa101e 100644
--- a/src/builtins/arm64/builtins-arm64.cc
+++ b/src/builtins/arm64/builtins-arm64.cc
@@ -49,6 +49,7 @@ void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) {
 
 static void GenerateTailCallToReturnedCode(MacroAssembler* masm,
                                            Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   // ----------- S t a t e -------------
   //  -- x0 : actual argument count
   //  -- x1 : target function (preserved for callee)
@@ -419,6 +420,7 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Register sfi_data,
                                                     Register scratch1,
                                                     Label* is_baseline) {
+  ASM_CODE_COMMENT(masm);
   Label done;
   __ CompareObjectType(sfi_data, scratch1, scratch1, BASELINE_DATA_TYPE);
   __ B(eq, is_baseline);
@@ -964,6 +966,7 @@ void Builtins::Generate_RunMicrotasksTrampoline(MacroAssembler* masm) {
 static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
                                                 Register optimized_code,
                                                 Register closure) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimized_code, closure));
   // Store code entry in the closure.
   __ StoreTaggedField(optimized_code,
@@ -975,6 +978,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
 
 static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
                                   Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   Register params_size = scratch1;
   // Get the size of the formal parameters + receiver (in bytes).
   __ Ldr(params_size,
@@ -1014,6 +1018,7 @@ static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
                                           Register actual_marker,
                                           OptimizationMarker expected_marker,
                                           Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   Label no_match;
   __ CompareAndBranch(actual_marker, Operand(expected_marker), ne, &no_match);
   GenerateTailCallToReturnedCode(masm, function_id);
@@ -1028,6 +1033,7 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   //  -- x3 : new target (preserved for callee if needed, and caller)
   //  -- x1 : target function (preserved for callee if needed, and caller)
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(x1, x3, optimized_code_entry, scratch));
 
   Register closure = x1;
@@ -1077,6 +1083,7 @@ static void MaybeOptimizeCode(MacroAssembler* masm, Register feedback_vector,
   //  -- feedback vector (preserved for caller if needed)
   //  -- optimization_marker : int32 containing non-zero optimization marker.
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(feedback_vector, x1, x3, optimization_marker));
 
   // TODO(v8:8394): The logging of first execution will break if
@@ -1110,6 +1117,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
                                           Register bytecode_offset,
                                           Register bytecode, Register scratch1,
                                           Register scratch2, Label* if_return) {
+  ASM_CODE_COMMENT(masm);
   Register bytecode_size_table = scratch1;
 
   // The bytecode offset value will be increased by one in wide and extra wide
@@ -1181,22 +1189,20 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state, Register feedback_vector,
     Label* has_optimized_code_or_marker) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
-  __ RecordComment("[ Check optimization state");
-
   __ Ldr(optimization_state,
          FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));
   __ TestAndBranchIfAnySet(
       optimization_state,
       FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask,
       has_optimized_code_or_marker);
-
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     Register feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
   Label maybe_has_optimized_code;
   // Check if optimized code is available
@@ -1259,56 +1265,57 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                            FeedbackVector::kInvocationCountOffset));
   }
 
-  __ RecordComment("[ Frame Setup");
   FrameScope frame_scope(masm, StackFrame::MANUAL);
-  // Normally the first thing we'd do here is Push(lr, fp), but we already
-  // entered the frame in BaselineCompiler::Prologue, as we had to use the
-  // value lr before the call to this BaselineOutOfLinePrologue builtin.
-
-  Register callee_context = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext);
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  __ Push(callee_context, callee_js_function);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-
-  Register argc = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecode_array = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-  __ Strh(wzr, FieldMemOperand(bytecode_array,
-                               BytecodeArray::kOsrNestingLevelOffset));
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    // Normally the first thing we'd do here is Push(lr, fp), but we already
+    // entered the frame in BaselineCompiler::Prologue, as we had to use the
+    // value lr before the call to this BaselineOutOfLinePrologue builtin.
+
+    Register callee_context = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext);
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    __ Push(callee_context, callee_js_function);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+
+    Register argc = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecode_array = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    __ Strh(wzr, FieldMemOperand(bytecode_array,
+                                 BytecodeArray::kOsrNestingLevelOffset));
 
-  __ Push(argc, bytecode_array);
+    __ Push(argc, bytecode_array);
 
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  if (FLAG_debug_code) {
-    __ CompareObjectType(feedback_vector, x4, x4, FEEDBACK_VECTOR_TYPE);
-    __ Assert(eq, AbortReason::kExpectedFeedbackVector);
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    if (FLAG_debug_code) {
+      __ CompareObjectType(feedback_vector, x4, x4, FEEDBACK_VECTOR_TYPE);
+      __ Assert(eq, AbortReason::kExpectedFeedbackVector);
+    }
+    // Our stack is currently aligned. We have have to push something along with
+    // the feedback vector to keep it that way -- we may as well start
+    // initialising the register frame.
+    __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
+    __ Push(feedback_vector, kInterpreterAccumulatorRegister);
   }
-  // Our stack is currently aligned. We have have to push something along with
-  // the feedback vector to keep it that way -- we may as well start
-  // initialising the register frame.
-  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
-  __ Push(feedback_vector, kInterpreterAccumulatorRegister);
-  __ RecordComment("]");
 
-  __ RecordComment("[ Stack/interrupt check");
   Label call_stack_guard;
   Register frame_size = descriptor.GetRegisterParameter(
       BaselineOutOfLinePrologueDescriptor::kStackFrameSize);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check");
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1322,7 +1329,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     __ LoadStackLimit(interrupt_limit, StackLimitKind::kInterruptStackLimit);
     __ Cmp(sp_minus_frame_size, interrupt_limit);
     __ B(lo, &call_stack_guard);
-    __ RecordComment("]");
   }
 
   // Do "fast" return to the caller pc in lr.
@@ -1335,29 +1341,27 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
-    __ RecordComment("[ Optimized marker check");
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     // Drop the frame created by the baseline call.
     __ Pop<TurboAssembler::kAuthLR>(fp, lr);
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     Register new_target = descriptor.GetRegisterParameter(
         BaselineOutOfLinePrologueDescriptor::kJavaScriptCallNewTarget);
 
     FrameScope frame_scope(masm, StackFrame::INTERNAL);
-    __ RecordComment("[ Stack/interrupt call");
     // Save incoming new target or generator
     __ Push(padreg, new_target);
     __ SmiTag(frame_size);
     __ PushArgument(frame_size);
     __ CallRuntime(Runtime::kStackGuardWithGap);
     __ Pop(new_target, padreg);
-    __ RecordComment("]");
   }
   __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
   __ Ret();
@@ -1624,12 +1628,12 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ Unreachable();  // Should not return.
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register first_arg_index,
-                                         Register spread_arg_out,
-                                         ConvertReceiverMode receiver_mode,
-                                         InterpreterPushArgsMode mode) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register first_arg_index,
+                                        Register spread_arg_out,
+                                        ConvertReceiverMode receiver_mode,
+                                        InterpreterPushArgsMode mode) {
+  ASM_CODE_COMMENT(masm);
   Register last_arg_addr = x10;
   Register stack_addr = x11;
   Register slots_to_claim = x12;
@@ -1722,8 +1726,8 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   Register first_arg_index = x2;
   Register spread_arg_out =
       (mode == InterpreterPushArgsMode::kWithFinalSpread) ? x2 : no_reg;
-  Generate_InterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,
-                               receiver_mode, mode);
+  GenerateInterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,
+                              receiver_mode, mode);
 
   // Call the target.
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
@@ -1754,8 +1758,8 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   Register first_arg_index = x4;
   Register spread_arg_out =
       (mode == InterpreterPushArgsMode::kWithFinalSpread) ? x2 : no_reg;
-  Generate_InterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,
-                               ConvertReceiverMode::kNullOrUndefined, mode);
+  GenerateInterpreterPushArgs(masm, num_args, first_arg_index, spread_arg_out,
+                              ConvertReceiverMode::kNullOrUndefined, mode);
 
   if (mode == InterpreterPushArgsMode::kArrayFunction) {
     __ AssertFunction(x1);
@@ -2046,6 +2050,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
 }
 
 void OnStackReplacement(MacroAssembler* masm, bool is_interpreter) {
+  ASM_CODE_COMMENT(masm);
   {
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallRuntime(Runtime::kCompileForOnStackReplacement);
@@ -3353,6 +3358,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
                               ExternalReference thunk_ref, int stack_space,
                               MemOperand* stack_space_operand, int spill_offset,
                               MemOperand return_value_operand) {
+  ASM_CODE_COMMENT(masm);
   ASM_LOCATION("CallApiFunctionAndReturn");
   Isolate* isolate = masm->isolate();
   ExternalReference next_address =
@@ -3700,6 +3706,7 @@ void CopyRegListToFrame(MacroAssembler* masm, const Register& dst,
                         int dst_offset, const CPURegList& reg_list,
                         const Register& temp0, const Register& temp1,
                         int src_offset = 0) {
+  ASM_CODE_COMMENT(masm);
   DCHECK_EQ(reg_list.Count() % 2, 0);
   UseScratchRegisterScope temps(masm);
   CPURegList copy_to_input = reg_list;
@@ -3736,6 +3743,7 @@ void CopyRegListToFrame(MacroAssembler* masm, const Register& dst,
 
 void RestoreRegList(MacroAssembler* masm, const CPURegList& reg_list,
                     const Register& src_base, int src_offset) {
+  ASM_CODE_COMMENT(masm);
   DCHECK_EQ(reg_list.Count() % 2, 0);
   UseScratchRegisterScope temps(masm);
   CPURegList restore_list = reg_list;
diff --git a/src/builtins/ia32/builtins-ia32.cc b/src/builtins/ia32/builtins-ia32.cc
index 0d57121a3ed..3bbd350d036 100644
--- a/src/builtins/ia32/builtins-ia32.cc
+++ b/src/builtins/ia32/builtins-ia32.cc
@@ -49,6 +49,7 @@ static void GenerateTailCallToReturnedCode(MacroAssembler* masm,
   //  -- edx : new target (preserved for callee)
   //  -- edi : target function (preserved for callee)
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   {
     FrameScope scope(masm, StackFrame::INTERNAL);
     // Push a copy of the target function, the new target and the actual
@@ -570,6 +571,7 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Register sfi_data,
                                                     Register scratch1,
                                                     Label* is_baseline) {
+  ASM_CODE_COMMENT(masm);
   Label done;
   __ LoadMap(scratch1, sfi_data);
 
@@ -736,6 +738,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
                                                 Register closure,
                                                 Register value,
                                                 Register slot_address) {
+  ASM_CODE_COMMENT(masm);
   // Store the optimized code in the closure.
   __ mov(FieldOperand(closure, JSFunction::kCodeOffset), optimized_code);
   __ mov(value, optimized_code);  // Write barrier clobbers slot_address below.
@@ -746,6 +749,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
 
 static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
                                   Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   Register params_size = scratch1;
   // Get the size of the formal parameters + receiver (in bytes).
   __ mov(params_size,
@@ -783,6 +787,7 @@ static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
                                           Register actual_marker,
                                           OptimizationMarker expected_marker,
                                           Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   Label no_match;
   __ cmp(actual_marker, expected_marker);
   __ j(not_equal, &no_match, Label::kNear);
@@ -797,6 +802,7 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   //  -- edx : new target (preserved for callee if needed, and caller)
   //  -- edi : target function (preserved for callee if needed, and caller)
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(edx, edi, optimized_code_entry));
 
   Register closure = edi;
@@ -846,6 +852,7 @@ static void MaybeOptimizeCode(MacroAssembler* masm,
   //  -- edi : target function (preserved for callee if needed, and caller)
   //  -- optimization_marker : a Smi containing a non-zero optimization marker.
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(edx, edi, optimization_marker));
 
   // TODO(v8:8394): The logging of first execution will break if
@@ -879,6 +886,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
                                           Register bytecode_offset,
                                           Register scratch1, Register scratch2,
                                           Register scratch3, Label* if_return) {
+  ASM_CODE_COMMENT(masm);
   Register bytecode_size_table = scratch1;
   Register bytecode = scratch2;
 
@@ -960,8 +968,8 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state,
     XMMRegister saved_feedback_vector, Label* has_optimized_code_or_marker) {
+  ASM_CODE_COMMENT(masm);
   Register feedback_vector = optimization_state;
-  __ RecordComment("[ Check optimization state");
 
   // Store feedback_vector. We may need it if we need to load the optimize code
   // slot entry.
@@ -975,13 +983,12 @@ static void LoadOptimizationStateAndJumpIfNeedsProcessing(
       optimization_state,
       Immediate(FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask));
   __ j(not_zero, has_optimized_code_or_marker);
-
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     XMMRegister saved_feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   Label maybe_has_optimized_code;
   // Check if optimized code is available
   __ test(
@@ -1278,15 +1285,15 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ int3();  // Should not return.
 }
 
-
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register array_limit,
-                                         Register start_address) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm,
+                                        Register array_limit,
+                                        Register start_address) {
   // ----------- S t a t e -------------
   //  -- start_address : Pointer to the last argument in the args array.
   //  -- array_limit : Pointer to one before the first argument in the
   //                   args array.
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   Label loop_header, loop_check;
   __ jmp(&loop_check);
   __ bind(&loop_header);
@@ -1341,12 +1348,12 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
 
   if (mode == InterpreterPushArgsMode::kWithFinalSpread) {
     __ movd(xmm1, scratch);
-    Generate_InterpreterPushArgs(masm, scratch, argv);
+    GenerateInterpreterPushArgs(masm, scratch, argv);
     // Pass the spread in the register ecx.
     __ movd(ecx, xmm1);
     __ mov(ecx, Operand(ecx, 0));
   } else {
-    Generate_InterpreterPushArgs(masm, scratch, argv);
+    GenerateInterpreterPushArgs(masm, scratch, argv);
   }
 
   // Push "undefined" as the receiver arg if we need to.
@@ -1690,47 +1697,49 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
   __ inc(FieldOperand(feedback_vector, FeedbackVector::kInvocationCountOffset));
 
   XMMRegister return_address = xmm4;
-  __ RecordComment("[ Frame Setup");
   // Save the return address, so that we can push it to the end of the newly
   // set-up frame once we're done setting it up.
   __ PopReturnAddressTo(return_address, scratch);
   // The bytecode array was pushed to the stack by the caller.
   __ Pop(saved_bytecode_array, scratch);
   FrameScope frame_scope(masm, StackFrame::MANUAL);
-  __ EnterFrame(StackFrame::BASELINE);
-
-  __ Push(descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext));  // Callee's
-                                                              // context.
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-  __ Push(callee_js_function);        // Callee's JS function.
-  __ Push(saved_arg_count, scratch);  // Push actual argument count.
-
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecode_array = scratch;
-  __ movd(bytecode_array, saved_bytecode_array);
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-  __ mov_w(FieldOperand(bytecode_array, BytecodeArray::kOsrNestingLevelOffset),
-           Immediate(0));
-  __ Push(bytecode_array);
-
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  __ Push(saved_feedback_vector, scratch);
-  __ RecordComment("]");
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    __ EnterFrame(StackFrame::BASELINE);
+
+    __ Push(descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext));  // Callee's
+                                                                // context.
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+    __ Push(callee_js_function);        // Callee's JS function.
+    __ Push(saved_arg_count, scratch);  // Push actual argument count.
+
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecode_array = scratch;
+    __ movd(bytecode_array, saved_bytecode_array);
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    __ mov_w(
+        FieldOperand(bytecode_array, BytecodeArray::kOsrNestingLevelOffset),
+        Immediate(0));
+    __ Push(bytecode_array);
+
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    __ Push(saved_feedback_vector, scratch);
+  }
 
-  __ RecordComment("[ Stack/interrupt check");
   Label call_stack_guard;
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check");
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1745,7 +1754,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     __ sub(scratch, frame_size);
     __ CompareStackLimit(scratch, StackLimitKind::kInterruptStackLimit);
     __ j(below, &call_stack_guard);
-    __ RecordComment("]");
   }
 
   // Push the return address back onto the stack for return.
@@ -1756,7 +1764,7 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
-    __ RecordComment("[ Optimized marker check");
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     // Drop the return address and bytecode array, rebalancing the return stack
     // buffer by using JumpMode::kPushAndReturn. We can't leave the slot and
     // overwrite it on return since we may do a runtime call along the way that
@@ -1766,12 +1774,11 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  saved_feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
-    __ RecordComment("[ Stack/interrupt call");
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     {
       // Push the baseline code return address now, as if it had been pushed by
       // the call to this builtin.
@@ -1788,7 +1795,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     // Return to caller pushed pc, without any frame teardown.
     __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
     __ Ret();
-    __ RecordComment("]");
   }
 }
 
@@ -2744,6 +2750,7 @@ void Builtins::Generate_Construct(MacroAssembler* masm) {
 namespace {
 
 void Generate_OSREntry(MacroAssembler* masm, Register entry_address) {
+  ASM_CODE_COMMENT(masm);
   // Overwrite the return address on the stack.
   __ mov(Operand(esp, 0), entry_address);
 
@@ -2752,6 +2759,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address) {
 }
 
 void OnStackReplacement(MacroAssembler* masm, bool is_interpreter) {
+  ASM_CODE_COMMENT(masm);
   {
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallRuntime(Runtime::kCompileForOnStackReplacement);
@@ -3160,6 +3168,7 @@ Operand ApiParameterOperand(int index) {
 // etc. Saves context (esi). If space was reserved for return value then
 // stores the pointer to the reserved slot into esi.
 void PrepareCallApiFunction(MacroAssembler* masm, int argc, Register scratch) {
+  ASM_CODE_COMMENT(masm);
   __ EnterApiExitFrame(argc, scratch);
   if (FLAG_debug_code) {
     __ mov(esi, Immediate(bit_cast<int32_t>(kZapValue)));
@@ -3520,6 +3529,7 @@ enum Alignment { MOVE_ALIGNED, MOVE_UNALIGNED };
 // edx - loop count (number of 64 byte chunks)
 void MemMoveEmitMainLoop(MacroAssembler* masm, Label* move_last_15,
                          Direction direction, Alignment alignment) {
+  ASM_CODE_COMMENT(masm);
   Register src = esi;
   Register dst = edi;
   Register count = ecx;
diff --git a/src/builtins/mips/builtins-mips.cc b/src/builtins/mips/builtins-mips.cc
index 6434870feda..b26770e0b3c 100644
--- a/src/builtins/mips/builtins-mips.cc
+++ b/src/builtins/mips/builtins-mips.cc
@@ -309,6 +309,7 @@ void Builtins::Generate_ConstructedNonConstructable(MacroAssembler* masm) {
 // Clobbers scratch1 and scratch2; preserves all other registers.
 static void Generate_CheckStackOverflow(MacroAssembler* masm, Register argc,
                                         Register scratch1, Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   // Check the stack for overflow. We are not trying to catch
   // interruptions (e.g. debug break and preemption) here, so the "real stack
   // limit" is checked.
@@ -615,6 +616,7 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Register sfi_data,
                                                     Register scratch1,
                                                     Label* is_baseline) {
+  ASM_CODE_COMMENT(masm);
   Label done;
 
   __ GetObjectType(sfi_data, scratch1, scratch1);
@@ -761,6 +763,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
                                                 Register closure,
                                                 Register scratch1,
                                                 Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   // Store code entry in the closure.
   __ sw(optimized_code, FieldMemOperand(closure, JSFunction::kCodeOffset));
   __ mov(scratch1, optimized_code);  // Write barrier clobbers scratch1 below.
@@ -771,6 +774,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
 
 static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
                                   Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   Register params_size = scratch1;
 
   // Get the size of the formal parameters + receiver (in bytes).
@@ -803,6 +807,7 @@ static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
                                           Register actual_marker,
                                           OptimizationMarker expected_marker,
                                           Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   Label no_match;
   __ Branch(&no_match, ne, actual_marker, Operand(expected_marker));
   GenerateTailCallToReturnedCode(masm, function_id);
@@ -863,6 +868,7 @@ static void MaybeOptimizeCode(MacroAssembler* masm, Register feedback_vector,
   //  -- optimization_marker : a int32 containing a non-zero optimization
   //  marker.
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(feedback_vector, a1, a3, optimization_marker));
 
   // TODO(v8:8394): The logging of first execution will break if
@@ -897,6 +903,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
                                           Register bytecode, Register scratch1,
                                           Register scratch2, Register scratch3,
                                           Label* if_return) {
+  ASM_CODE_COMMENT(masm);
   Register bytecode_size_table = scratch1;
 
   // The bytecode offset value will be increased by one in wide and extra wide
@@ -969,7 +976,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state, Register feedback_vector,
     Label* has_optimized_code_or_marker) {
-  __ RecordComment("[ Check optimization state");
+  ASM_CODE_COMMENT(masm);
   Register scratch = t6;
   __ Lw(optimization_state,
         FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));
@@ -977,12 +984,12 @@ static void LoadOptimizationStateAndJumpIfNeedsProcessing(
       scratch, optimization_state,
       Operand(FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask));
   __ Branch(has_optimized_code_or_marker, ne, scratch, Operand(zero_reg));
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     Register feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   Label maybe_has_optimized_code;
   // Check if optimized code marker is available
   {
@@ -1052,61 +1059,61 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                           FeedbackVector::kInvocationCountOffset));
   }
 
-  __ RecordComment("[ Frame Setup");
   FrameScope frame_scope(masm, StackFrame::MANUAL);
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    // Normally the first thing we'd do here is Push(ra, fp), but we already
+    // entered the frame in BaselineCompiler::Prologue, as we had to use the
+    // value ra before the call to this BaselineOutOfLinePrologue builtin.
+    Register callee_context = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext);
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    __ Push(callee_context, callee_js_function);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+
+    Register argc = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecodeArray = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    __ sh(zero_reg, FieldMemOperand(bytecodeArray,
+                                    BytecodeArray::kOsrNestingLevelOffset));
 
-  // Normally the first thing we'd do here is Push(ra, fp), but we already
-  // entered the frame in BaselineCompiler::Prologue, as we had to use the
-  // value ra before the call to this BaselineOutOfLinePrologue builtin.
-  Register callee_context = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext);
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  __ Push(callee_context, callee_js_function);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-
-  Register argc = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecodeArray = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-  __ sh(zero_reg,
-        FieldMemOperand(bytecodeArray, BytecodeArray::kOsrNestingLevelOffset));
-
-  __ Push(argc, bytecodeArray);
+    __ Push(argc, bytecodeArray);
 
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  if (FLAG_debug_code) {
-    UseScratchRegisterScope temps(masm);
-    Register invocation_count = temps.Acquire();
-    __ GetObjectType(feedback_vector, invocation_count, invocation_count);
-    __ Assert(eq, AbortReason::kExpectedFeedbackVector, invocation_count,
-              Operand(FEEDBACK_VECTOR_TYPE));
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    if (FLAG_debug_code) {
+      UseScratchRegisterScope temps(masm);
+      Register invocation_count = temps.Acquire();
+      __ GetObjectType(feedback_vector, invocation_count, invocation_count);
+      __ Assert(eq, AbortReason::kExpectedFeedbackVector, invocation_count,
+                Operand(FEEDBACK_VECTOR_TYPE));
+    }
+    // Our stack is currently aligned. We have have to push something along with
+    // the feedback vector to keep it that way -- we may as well start
+    // initialising the register frame.
+    // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
+    // `undefined` in the accumulator register, to skip the load in the baseline
+    // code.
+    __ Push(feedback_vector);
   }
-  // Our stack is currently aligned. We have have to push something along with
-  // the feedback vector to keep it that way -- we may as well start
-  // initialising the register frame.
-  // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
-  // `undefined` in the accumulator register, to skip the load in the baseline
-  // code.
-  __ Push(feedback_vector);
-  __ RecordComment("]");
-
-  __ RecordComment("[ Stack/interrupt check");
+
   Label call_stack_guard;
   Register frame_size = descriptor.GetRegisterParameter(
       BaselineOutOfLinePrologueDescriptor::kStackFrameSize);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check");
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1120,7 +1127,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                       MacroAssembler::StackLimitKind::kInterruptStackLimit);
     __ Branch(&call_stack_guard, Uless, sp_minus_frame_size,
               Operand(interrupt_limit));
-    __ RecordComment("]");
   }
 
   // Do "fast" return to the caller pc in ra.
@@ -1129,29 +1135,27 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     UseScratchRegisterScope temps(masm);
     temps.Exclude(optimization_state);
     // Ensure the optimization_state is not allocated again.
-    __ RecordComment("[ Optimized marker check");
     // Drop the frame created by the baseline call.
     __ Pop(ra, fp);
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     FrameScope frame_scope(masm, StackFrame::INTERNAL);
-    __ RecordComment("[ Stack/interrupt call");
     // Save incoming new target or generator
     __ Push(kJavaScriptCallNewTargetRegister);
     __ SmiTag(frame_size);
     __ Push(frame_size);
     __ CallRuntime(Runtime::kStackGuardWithGap);
     __ Pop(kJavaScriptCallNewTargetRegister);
-    __ RecordComment("]");
   }
   __ Ret();
   temps.Exclude(kScratchReg.bit() | kScratchReg2.bit());
@@ -1405,10 +1409,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ break_(0xCC);
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch, Register scratch2) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch, Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   // Find the address of the last argument.
   __ Subu(scratch, num_args, Operand(1));
   __ sll(scratch, scratch, kPointerSizeLog2);
@@ -1447,7 +1451,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // This function modifies a2, t4 and t1.
-  Generate_InterpreterPushArgs(masm, t0, a2, t4, t1);
+  GenerateInterpreterPushArgs(masm, t0, a2, t4, t1);
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
@@ -1496,7 +1500,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
     __ Subu(a0, a0, Operand(1));
   }
 
-  Generate_InterpreterPushArgs(masm, a0, t4, t1, t0);
+  GenerateInterpreterPushArgs(masm, a0, t4, t1, t0);
 
   // Push a slot for the receiver.
   __ push(zero_reg);
@@ -2876,6 +2880,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
                               ExternalReference thunk_ref, int stack_space,
                               MemOperand* stack_space_operand,
                               MemOperand return_value_operand) {
+  ASM_CODE_COMMENT(masm);
   Isolate* isolate = masm->isolate();
   ExternalReference next_address =
       ExternalReference::handle_scope_next_address(isolate);
diff --git a/src/builtins/mips64/builtins-mips64.cc b/src/builtins/mips64/builtins-mips64.cc
index aa794e5c84e..22bc8bde47b 100644
--- a/src/builtins/mips64/builtins-mips64.cc
+++ b/src/builtins/mips64/builtins-mips64.cc
@@ -989,7 +989,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state, Register feedback_vector,
     Label* has_optimized_code_or_marker) {
-  __ RecordComment("[ Check optimization state");
+  ASM_CODE_COMMENT(masm);
   Register scratch = t2;
   __ Lw(optimization_state,
         FieldMemOperand(feedback_vector, FeedbackVector::kFlagsOffset));
@@ -997,12 +997,12 @@ static void LoadOptimizationStateAndJumpIfNeedsProcessing(
       scratch, optimization_state,
       Operand(FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask));
   __ Branch(has_optimized_code_or_marker, ne, scratch, Operand(zero_reg));
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     Register feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   Label maybe_has_optimized_code;
   // Check if optimized code marker is available
   {
@@ -1071,61 +1071,61 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                           FeedbackVector::kInvocationCountOffset));
   }
 
-  __ RecordComment("[ Frame Setup");
   FrameScope frame_scope(masm, StackFrame::MANUAL);
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    // Normally the first thing we'd do here is Push(ra, fp), but we already
+    // entered the frame in BaselineCompiler::Prologue, as we had to use the
+    // value lr before the call to this BaselineOutOfLinePrologue builtin.
+    Register callee_context = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext);
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    __ Push(callee_context, callee_js_function);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+
+    Register argc = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecodeArray = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    __ Sh(zero_reg, FieldMemOperand(bytecodeArray,
+                                    BytecodeArray::kOsrNestingLevelOffset));
 
-  // Normally the first thing we'd do here is Push(ra, fp), but we already
-  // entered the frame in BaselineCompiler::Prologue, as we had to use the
-  // value lr before the call to this BaselineOutOfLinePrologue builtin.
-  Register callee_context = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext);
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  __ Push(callee_context, callee_js_function);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-
-  Register argc = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecodeArray = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-  __ Sh(zero_reg,
-        FieldMemOperand(bytecodeArray, BytecodeArray::kOsrNestingLevelOffset));
-
-  __ Push(argc, bytecodeArray);
+    __ Push(argc, bytecodeArray);
 
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  if (FLAG_debug_code) {
-    UseScratchRegisterScope temps(masm);
-    Register invocation_count = temps.Acquire();
-    __ GetObjectType(feedback_vector, invocation_count, invocation_count);
-    __ Assert(eq, AbortReason::kExpectedFeedbackVector, invocation_count,
-              Operand(FEEDBACK_VECTOR_TYPE));
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    if (FLAG_debug_code) {
+      UseScratchRegisterScope temps(masm);
+      Register invocation_count = temps.Acquire();
+      __ GetObjectType(feedback_vector, invocation_count, invocation_count);
+      __ Assert(eq, AbortReason::kExpectedFeedbackVector, invocation_count,
+                Operand(FEEDBACK_VECTOR_TYPE));
+    }
+    // Our stack is currently aligned. We have have to push something along with
+    // the feedback vector to keep it that way -- we may as well start
+    // initialising the register frame.
+    // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
+    // `undefined` in the accumulator register, to skip the load in the baseline
+    // code.
+    __ Push(feedback_vector);
   }
-  // Our stack is currently aligned. We have have to push something along with
-  // the feedback vector to keep it that way -- we may as well start
-  // initialising the register frame.
-  // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
-  // `undefined` in the accumulator register, to skip the load in the baseline
-  // code.
-  __ Push(feedback_vector);
-  __ RecordComment("]");
 
-  __ RecordComment("[ Stack/interrupt check");
   Label call_stack_guard;
   Register frame_size = descriptor.GetRegisterParameter(
       BaselineOutOfLinePrologueDescriptor::kStackFrameSize);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check");
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1139,7 +1139,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                       MacroAssembler::StackLimitKind::kInterruptStackLimit);
     __ Branch(&call_stack_guard, Uless, sp_minus_frame_size,
               Operand(interrupt_limit));
-    __ RecordComment("]");
   }
 
   // Do "fast" return to the caller pc in ra.
@@ -1148,29 +1147,27 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     UseScratchRegisterScope temps(masm);
     temps.Exclude(optimization_state);
     // Ensure the optimization_state is not allocated again.
-    __ RecordComment("[ Optimized marker check");
     // Drop the frame created by the baseline call.
     __ Pop(ra, fp);
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     FrameScope frame_scope(masm, StackFrame::INTERNAL);
-    __ RecordComment("[ Stack/interrupt call");
     // Save incoming new target or generator
     __ Push(kJavaScriptCallNewTargetRegister);
     __ SmiTag(frame_size);
     __ Push(frame_size);
     __ CallRuntime(Runtime::kStackGuardWithGap);
     __ Pop(kJavaScriptCallNewTargetRegister);
-    __ RecordComment("]");
   }
   __ Ret();
   temps.Exclude(kScratchReg.bit() | kScratchReg2.bit());
@@ -1424,11 +1421,9 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ break_(0xCC);
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch,
-                                         Register scratch2) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch, Register scratch2) {
   // Find the address of the last argument.
   __ Dsubu(scratch, num_args, Operand(1));
   __ dsll(scratch, scratch, kPointerSizeLog2);
@@ -1467,7 +1462,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // This function modifies a2, t0 and a4.
-  Generate_InterpreterPushArgs(masm, a3, a2, a4, t0);
+  GenerateInterpreterPushArgs(masm, a3, a2, a4, t0);
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
@@ -1517,7 +1512,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // Push the arguments, This function modifies t0, a4 and a5.
-  Generate_InterpreterPushArgs(masm, a0, a4, a5, t0);
+  GenerateInterpreterPushArgs(masm, a0, a4, a5, t0);
 
   // Push a slot for the receiver.
   __ push(zero_reg);
diff --git a/src/builtins/ppc/builtins-ppc.cc b/src/builtins/ppc/builtins-ppc.cc
index c319b01b747..6b9030e6323 100644
--- a/src/builtins/ppc/builtins-ppc.cc
+++ b/src/builtins/ppc/builtins-ppc.cc
@@ -1272,10 +1272,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ bkpt(0);  // Should not return.
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch) {
+  ASM_CODE_COMMENT(masm);
   __ subi(scratch, num_args, Operand(1));
   __ ShiftLeftImm(scratch, scratch, Operand(kSystemPointerSizeLog2));
   __ sub(start_address, start_address, scratch);
@@ -1313,7 +1313,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // Push the arguments.
-  Generate_InterpreterPushArgs(masm, r6, r5, r7);
+  GenerateInterpreterPushArgs(masm, r6, r5, r7);
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
@@ -1363,7 +1363,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // Push the arguments.
-  Generate_InterpreterPushArgs(masm, r3, r7, r8);
+  GenerateInterpreterPushArgs(masm, r3, r7, r8);
 
   // Push a slot for the receiver to be constructed.
   __ li(r0, Operand::Zero());
diff --git a/src/builtins/riscv64/builtins-riscv64.cc b/src/builtins/riscv64/builtins-riscv64.cc
index 149f1732ac3..d5332611182 100644
--- a/src/builtins/riscv64/builtins-riscv64.cc
+++ b/src/builtins/riscv64/builtins-riscv64.cc
@@ -33,6 +33,7 @@ namespace internal {
 #define __ ACCESS_MASM(masm)
 
 void Builtins::Generate_Adaptor(MacroAssembler* masm, Address address) {
+  ASM_CODE_COMMENT(masm);
   __ li(kJavaScriptCallExtraArg1Register, ExternalReference::Create(address));
   __ Jump(BUILTIN_CODE(masm->isolate(), AdaptorWithBuiltinExitFrame),
           RelocInfo::CODE_TARGET);
@@ -325,6 +326,7 @@ static void GetSharedFunctionInfoBytecodeOrBaseline(MacroAssembler* masm,
                                                     Register sfi_data,
                                                     Register scratch1,
                                                     Label* is_baseline) {
+  ASM_CODE_COMMENT(masm);
   Label done;
 
   __ GetObjectType(sfi_data, scratch1, scratch1);
@@ -808,6 +810,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
                                                 Register closure,
                                                 Register scratch1,
                                                 Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimized_code, closure));
   // Store code entry in the closure.
   __ StoreTaggedField(optimized_code,
@@ -820,6 +823,7 @@ static void ReplaceClosureCodeWithOptimizedCode(MacroAssembler* masm,
 
 static void LeaveInterpreterFrame(MacroAssembler* masm, Register scratch1,
                                   Register scratch2) {
+  ASM_CODE_COMMENT(masm);
   Register params_size = scratch1;
 
   // Get the size of the formal parameters + receiver (in bytes).
@@ -854,6 +858,7 @@ static void TailCallRuntimeIfMarkerEquals(MacroAssembler* masm,
                                           Register actual_marker,
                                           OptimizationMarker expected_marker,
                                           Runtime::FunctionId function_id) {
+  ASM_CODE_COMMENT(masm);
   Label no_match;
   __ BranchShort(&no_match, ne, actual_marker, Operand(expected_marker));
   GenerateTailCallToReturnedCode(masm, function_id);
@@ -868,6 +873,7 @@ static void TailCallOptimizedCodeSlot(MacroAssembler* masm,
   //  -- a3 : new target (preserved for callee if needed, and caller)
   //  -- a1 : target function (preserved for callee if needed, and caller)
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimized_code_entry, a1, a3, scratch1, scratch2));
 
   Register closure = a1;
@@ -915,6 +921,7 @@ static void MaybeOptimizeCode(MacroAssembler* masm, Register feedback_vector,
   //  -- optimization_marker : a int32 containing a non-zero optimization
   //  marker.
   // -----------------------------------
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(feedback_vector, a1, a3, optimization_marker));
 
   // TODO(v8:8394): The logging of first execution will break if
@@ -949,6 +956,7 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
                                           Register bytecode, Register scratch1,
                                           Register scratch2, Register scratch3,
                                           Label* if_return) {
+  ASM_CODE_COMMENT(masm);
   Register bytecode_size_table = scratch1;
 
   // The bytecode offset value will be increased by one in wide and extra wide
@@ -1021,8 +1029,8 @@ static void AdvanceBytecodeOffsetOrReturn(MacroAssembler* masm,
 static void LoadOptimizationStateAndJumpIfNeedsProcessing(
     MacroAssembler* masm, Register optimization_state, Register feedback_vector,
     Label* has_optimized_code_or_marker) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
-  __ RecordComment("[ Check optimization state");
   UseScratchRegisterScope temps(masm);
   Register scratch = temps.Acquire();
   __ Lw(optimization_state,
@@ -1031,12 +1039,12 @@ static void LoadOptimizationStateAndJumpIfNeedsProcessing(
       scratch, optimization_state,
       Operand(FeedbackVector::kHasOptimizedCodeOrCompileOptimizedMarkerMask));
   __ Branch(has_optimized_code_or_marker, ne, scratch, Operand(zero_reg));
-  __ RecordComment("]");
 }
 
 static void MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(
     MacroAssembler* masm, Register optimization_state,
     Register feedback_vector) {
+  ASM_CODE_COMMENT(masm);
   DCHECK(!AreAliased(optimization_state, feedback_vector));
   UseScratchRegisterScope temps(masm);
   temps.Include(t0, t1);
@@ -1104,60 +1112,61 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                           FeedbackVector::kInvocationCountOffset));
   }
 
-  __ RecordComment("[ Frame Setup");
   FrameScope frame_scope(masm, StackFrame::MANUAL);
-  // Normally the first thing we'd do here is Push(lr, fp), but we already
-  // entered the frame in BaselineCompiler::Prologue, as we had to use the
-  // value lr before the call to this BaselineOutOfLinePrologue builtin.
-
-  Register callee_context = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kCalleeContext);
-  Register callee_js_function = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kClosure);
-  __ Push(callee_context, callee_js_function);
-  DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-  DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-
-  Register argc = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
-  // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-  // the frame, so load it into a register.
-  Register bytecodeArray = descriptor.GetRegisterParameter(
-      BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-  // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-  // are 8-bit fields next to each other, so we could just optimize by writing
-  // a 16-bit. These static asserts guard our assumption is valid.
-  STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-  STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-  __ Sh(zero_reg,
-        FieldMemOperand(bytecodeArray, BytecodeArray::kOsrNestingLevelOffset));
+  {
+    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+    // Normally the first thing we'd do here is Push(lr, fp), but we already
+    // entered the frame in BaselineCompiler::Prologue, as we had to use the
+    // value lr before the call to this BaselineOutOfLinePrologue builtin.
+
+    Register callee_context = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kCalleeContext);
+    Register callee_js_function = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kClosure);
+    __ Push(callee_context, callee_js_function);
+    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+
+    Register argc = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kJavaScriptCallArgCount);
+    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
+    // the frame, so load it into a register.
+    Register bytecodeArray = descriptor.GetRegisterParameter(
+        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+    // are 8-bit fields next to each other, so we could just optimize by writing
+    // a 16-bit. These static asserts guard our assumption is valid.
+    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+    __ Sh(zero_reg, FieldMemOperand(bytecodeArray,
+                                    BytecodeArray::kOsrNestingLevelOffset));
 
-  __ Push(argc, bytecodeArray);
+    __ Push(argc, bytecodeArray);
 
-  // Baseline code frames store the feedback vector where interpreter would
-  // store the bytecode offset.
-  if (FLAG_debug_code) {
-    UseScratchRegisterScope temps(masm);
-    Register type = temps.Acquire();
-    __ GetObjectType(feedback_vector, type, type);
-    __ Assert(eq, AbortReason::kExpectedFeedbackVector, type,
-              Operand(FEEDBACK_VECTOR_TYPE));
+    // Baseline code frames store the feedback vector where interpreter would
+    // store the bytecode offset.
+    if (FLAG_debug_code) {
+      UseScratchRegisterScope temps(masm);
+      Register type = temps.Acquire();
+      __ GetObjectType(feedback_vector, type, type);
+      __ Assert(eq, AbortReason::kExpectedFeedbackVector, type,
+                Operand(FEEDBACK_VECTOR_TYPE));
+    }
+    // Our stack is currently aligned. We have have to push something along with
+    // the feedback vector to keep it that way -- we may as well start
+    // initialising the register frame.
+    // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
+    // `undefined` in the accumulator register, to skip the load in the baseline
+    // code.
+    __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
+    __ Push(feedback_vector, kInterpreterAccumulatorRegister);
   }
-  // Our stack is currently aligned. We have have to push something along with
-  // the feedback vector to keep it that way -- we may as well start
-  // initialising the register frame.
-  // TODO(v8:11429,leszeks): Consider guaranteeing that this call leaves
-  // `undefined` in the accumulator register, to skip the load in the baseline
-  // code.
-  __ LoadRoot(kInterpreterAccumulatorRegister, RootIndex::kUndefinedValue);
-  __ Push(feedback_vector, kInterpreterAccumulatorRegister);
-  __ RecordComment("]");
 
-  __ RecordComment("[ Stack/interrupt check");
   Label call_stack_guard;
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt check")
     // Stack check. This folds the checks for both the interrupt stack limit
     // check and the real stack limit into one by just checking for the
     // interrupt limit. The interrupt limit is either equal to the real stack
@@ -1174,7 +1183,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
                       MacroAssembler::StackLimitKind::kInterruptStackLimit);
     __ BranchShort(&call_stack_guard, Uless, sp_minus_frame_size,
               Operand(interrupt_limit));
-    __ RecordComment("]");
   }
 
   // Do "fast" return to the caller pc in lr.
@@ -1183,27 +1191,25 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
 
   __ bind(&has_optimized_code_or_marker);
   {
-    __ RecordComment("[ Optimized marker check");
+    ASM_CODE_COMMENT_STRING(masm, "Optimized marker check");
     // Drop the frame created by the baseline call.
     __ Pop(fp, ra);
     MaybeOptimizeCodeOrTailCallOptimizedCodeSlot(masm, optimization_state,
                                                  feedback_vector);
     __ Trap();
-    __ RecordComment("]");
   }
 
   __ bind(&call_stack_guard);
   {
+    ASM_CODE_COMMENT_STRING(masm, "Stack/interrupt call");
     Register new_target = descriptor.GetRegisterParameter(
         BaselineOutOfLinePrologueDescriptor::kJavaScriptCallNewTarget);
 
     FrameScope frame_scope(masm, StackFrame::INTERNAL);
-    __ RecordComment("[ Stack/interrupt call");
     // Save incoming new target or generator
     __ Push(zero_reg, new_target);
     __ CallRuntime(Runtime::kStackGuard);
     __ Pop(new_target, zero_reg);
-    __ RecordComment("]");
   }
   __ Ret();
   temps.Exclude(kScratchReg.bit() | kScratchReg2.bit());
@@ -1496,10 +1502,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ break_(0xCC);
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch) {
+  ASM_CODE_COMMENT(masm);
   // Find the address of the last argument.
   __ Sub64(scratch, num_args, Operand(1));
   __ Sll64(scratch, scratch, kSystemPointerSizeLog2);
@@ -1538,7 +1544,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // This function modifies a2 and a4.
-  Generate_InterpreterPushArgs(masm, a3, a2, a4);
+  GenerateInterpreterPushArgs(masm, a3, a2, a4);
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
@@ -1588,7 +1594,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // Push the arguments, This function modifies a4 and a5.
-  Generate_InterpreterPushArgs(masm, a0, a4, a5);
+  GenerateInterpreterPushArgs(masm, a0, a4, a5);
 
   // Push a slot for the receiver.
   __ push(zero_reg);
@@ -1852,6 +1858,7 @@ void Generate_OSREntry(MacroAssembler* masm, Register entry_address,
 }
 
 void OnStackReplacement(MacroAssembler* masm, bool is_interpreter) {
+  ASM_CODE_COMMENT(masm);
   {
     FrameScope scope(masm, StackFrame::INTERNAL);
     __ CallRuntime(Runtime::kCompileForOnStackReplacement);
@@ -3040,6 +3047,7 @@ void CallApiFunctionAndReturn(MacroAssembler* masm, Register function_address,
                               ExternalReference thunk_ref, int stack_space,
                               MemOperand* stack_space_operand,
                               MemOperand return_value_operand) {
+  ASM_CODE_COMMENT(masm);
   Isolate* isolate = masm->isolate();
   ExternalReference next_address =
       ExternalReference::handle_scope_next_address(isolate);
diff --git a/src/builtins/s390/builtins-s390.cc b/src/builtins/s390/builtins-s390.cc
index d3366bb8483..5f6814c5f60 100644
--- a/src/builtins/s390/builtins-s390.cc
+++ b/src/builtins/s390/builtins-s390.cc
@@ -1318,10 +1318,10 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ bkpt(0);  // Should not return.
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch) {
+  ASM_CODE_COMMENT(masm);
   __ SubS64(scratch, num_args, Operand(1));
   __ ShiftLeftU64(scratch, scratch, Operand(kSystemPointerSizeLog2));
   __ SubS64(start_address, start_address, scratch);
@@ -1358,7 +1358,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // Push the arguments.
-  Generate_InterpreterPushArgs(masm, r5, r4, r6);
+  GenerateInterpreterPushArgs(masm, r5, r4, r6);
 
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
     __ PushRoot(RootIndex::kUndefinedValue);
@@ -1408,7 +1408,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // Push the arguments. r4 and r5 will be modified.
-  Generate_InterpreterPushArgs(masm, r2, r6, r7);
+  GenerateInterpreterPushArgs(masm, r2, r6, r7);
 
   // Push a slot for the receiver to be constructed.
   __ mov(r0, Operand::Zero());
diff --git a/src/builtins/setup-builtins-internal.cc b/src/builtins/setup-builtins-internal.cc
index e2c1a9922f0..37910d03af6 100644
--- a/src/builtins/setup-builtins-internal.cc
+++ b/src/builtins/setup-builtins-internal.cc
@@ -59,7 +59,7 @@ using CodeAssemblerGenerator = void (*)(compiler::CodeAssemblerState*);
 
 Handle<Code> BuildPlaceholder(Isolate* isolate, Builtin builtin) {
   HandleScope scope(isolate);
-  constexpr int kBufferSize = 1 * KB;
+  const int kBufferSize = FLAG_code_comments ? 128 * KB : 1 * KB;
   byte buffer[kBufferSize];
   MacroAssembler masm(isolate, CodeObjectRequired::kYes,
                       ExternalAssemblerBuffer(buffer, kBufferSize));
@@ -87,7 +87,7 @@ Code BuildWithMacroAssembler(Isolate* isolate, Builtin builtin,
   // Canonicalize handles, so that we can share constant pool entries pointing
   // to code targets without dereferencing their handles.
   CanonicalHandleScope canonical(isolate);
-  constexpr int kBufferSize = 32 * KB;
+  const int kBufferSize = FLAG_code_comments ? 128 * KB : 32 * KB;
   byte buffer[kBufferSize];
 
   MacroAssembler masm(isolate, BuiltinAssemblerOptions(isolate, builtin),
@@ -130,7 +130,7 @@ Code BuildAdaptor(Isolate* isolate, Builtin builtin, Address builtin_address,
   // Canonicalize handles, so that we can share constant pool entries pointing
   // to code targets without dereferencing their handles.
   CanonicalHandleScope canonical(isolate);
-  constexpr int kBufferSize = 32 * KB;
+  const int kBufferSize = FLAG_code_comments ? 128 * KB : 32 * KB;
   byte buffer[kBufferSize];
   MacroAssembler masm(isolate, BuiltinAssemblerOptions(isolate, builtin),
                       CodeObjectRequired::kYes,
diff --git a/src/builtins/x64/builtins-x64.cc b/src/builtins/x64/builtins-x64.cc
index 35c075f26e5..981142ba6ad 100644
--- a/src/builtins/x64/builtins-x64.cc
+++ b/src/builtins/x64/builtins-x64.cc
@@ -1361,10 +1361,9 @@ void Builtins::Generate_InterpreterEntryTrampoline(MacroAssembler* masm) {
   __ int3();  // Should not return.
 }
 
-static void Generate_InterpreterPushArgs(MacroAssembler* masm,
-                                         Register num_args,
-                                         Register start_address,
-                                         Register scratch) {
+static void GenerateInterpreterPushArgs(MacroAssembler* masm, Register num_args,
+                                        Register start_address,
+                                        Register scratch) {
   ASM_CODE_COMMENT(masm);
   // Find the argument with lowest address.
   __ movq(scratch, num_args);
@@ -1410,7 +1409,7 @@ void Builtins::Generate_InterpreterPushArgsThenCallImpl(
   }
 
   // rbx and rdx will be modified.
-  Generate_InterpreterPushArgs(masm, rcx, rbx, rdx);
+  GenerateInterpreterPushArgs(masm, rcx, rbx, rdx);
 
   // Push "undefined" as the receiver arg if we need to.
   if (receiver_mode == ConvertReceiverMode::kNullOrUndefined) {
@@ -1471,7 +1470,7 @@ void Builtins::Generate_InterpreterPushArgsThenConstructImpl(
   }
 
   // rcx and r8 will be modified.
-  Generate_InterpreterPushArgs(masm, rax, rcx, r8);
+  GenerateInterpreterPushArgs(masm, rax, rcx, r8);
 
   // Push slot for the receiver to be constructed.
   __ Push(Immediate(0));
@@ -1681,46 +1680,47 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
   __ incl(
       FieldOperand(feedback_vector, FeedbackVector::kInvocationCountOffset));
 
-  {
-    ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
     // Save the return address, so that we can push it to the end of the newly
     // set-up frame once we're done setting it up.
     __ PopReturnAddressTo(return_address);
     FrameScope frame_scope(masm, StackFrame::MANUAL);
-    __ EnterFrame(StackFrame::BASELINE);
-
-    __ Push(descriptor.GetRegisterParameter(
-        BaselineOutOfLinePrologueDescriptor::kCalleeContext));  // Callee's
-                                                                // context.
-    Register callee_js_function = descriptor.GetRegisterParameter(
-        BaselineOutOfLinePrologueDescriptor::kClosure);
-    DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
-    DCHECK_EQ(callee_js_function, kJSFunctionRegister);
-    __ Push(callee_js_function);  // Callee's JS function.
-    __ Push(descriptor.GetRegisterParameter(
-        BaselineOutOfLinePrologueDescriptor::
-            kJavaScriptCallArgCount));  // Actual argument
-                                        // count.
-
-    // We'll use the bytecode for both code age/OSR resetting, and pushing onto
-    // the frame, so load it into a register.
-    Register bytecode_array = descriptor.GetRegisterParameter(
-        BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
-
-    // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
-    // are 8-bit fields next to each other, so we could just optimize by writing
-    // a 16-bit. These static asserts guard our assumption is valid.
-    STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
-                  BytecodeArray::kOsrNestingLevelOffset + kCharSize);
-    STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
-    __ movw(FieldOperand(bytecode_array, BytecodeArray::kOsrNestingLevelOffset),
-            Immediate(0));
-    __ Push(bytecode_array);
+    {
+      ASM_CODE_COMMENT_STRING(masm, "Frame Setup");
+      __ EnterFrame(StackFrame::BASELINE);
+
+      __ Push(descriptor.GetRegisterParameter(
+          BaselineOutOfLinePrologueDescriptor::kCalleeContext));  // Callee's
+                                                                  // context.
+      Register callee_js_function = descriptor.GetRegisterParameter(
+          BaselineOutOfLinePrologueDescriptor::kClosure);
+      DCHECK_EQ(callee_js_function, kJavaScriptCallTargetRegister);
+      DCHECK_EQ(callee_js_function, kJSFunctionRegister);
+      __ Push(callee_js_function);  // Callee's JS function.
+      __ Push(descriptor.GetRegisterParameter(
+          BaselineOutOfLinePrologueDescriptor::
+              kJavaScriptCallArgCount));  // Actual argument
+                                          // count.
+
+      // We'll use the bytecode for both code age/OSR resetting, and pushing
+      // onto the frame, so load it into a register.
+      Register bytecode_array = descriptor.GetRegisterParameter(
+          BaselineOutOfLinePrologueDescriptor::kInterpreterBytecodeArray);
+
+      // Reset code age and the OSR arming. The OSR field and BytecodeAgeOffset
+      // are 8-bit fields next to each other, so we could just optimize by
+      // writing a 16-bit. These static asserts guard our assumption is valid.
+      STATIC_ASSERT(BytecodeArray::kBytecodeAgeOffset ==
+                    BytecodeArray::kOsrNestingLevelOffset + kCharSize);
+      STATIC_ASSERT(BytecodeArray::kNoAgeBytecodeAge == 0);
+      __ movw(
+          FieldOperand(bytecode_array, BytecodeArray::kOsrNestingLevelOffset),
+          Immediate(0));
+      __ Push(bytecode_array);
 
-    // Baseline code frames store the feedback vector where interpreter would
-    // store the bytecode offset.
-    __ Push(feedback_vector);
-  }
+      // Baseline code frames store the feedback vector where interpreter would
+      // store the bytecode offset.
+      __ Push(feedback_vector);
+    }
 
   Register new_target = descriptor.GetRegisterParameter(
       BaselineOutOfLinePrologueDescriptor::kJavaScriptCallNewTarget);
@@ -1744,7 +1744,6 @@ void Builtins::Generate_BaselineOutOfLinePrologue(MacroAssembler* masm) {
     __ cmpq(kScratchRegister,
             __ StackLimitAsOperand(StackLimitKind::kInterruptStackLimit));
     __ j(below, &call_stack_guard);
-    __ RecordComment("]");
   }
 
   // Push the return address back onto the stack for return.
diff --git a/src/codegen/arm/assembler-arm.cc b/src/codegen/arm/assembler-arm.cc
index ea2f8537b50..a81ef98f2af 100644
--- a/src/codegen/arm/assembler-arm.cc
+++ b/src/codegen/arm/assembler-arm.cc
@@ -5354,9 +5354,9 @@ void Assembler::CheckConstPool(bool force_emit, bool require_jump) {
   while (buffer_space() <= needed_space) GrowBuffer();
 
   {
+    ASM_CODE_COMMENT_STRING(this, "Constant Pool");
     // Block recursive calls to CheckConstPool.
     BlockConstPoolScope block_const_pool(this);
-    RecordComment("[ Constant Pool");
     RecordConstPool(size);
 
     Label size_check;
@@ -5421,8 +5421,6 @@ void Assembler::CheckConstPool(bool force_emit, bool require_jump) {
 
     first_const_pool_32_use_ = -1;
 
-    RecordComment("]");
-
     DCHECK_EQ(size, SizeOfCodeGeneratedSince(&size_check));
 
     if (after_pool.is_linked()) {
diff --git a/src/codegen/arm/macro-assembler-arm.cc b/src/codegen/arm/macro-assembler-arm.cc
index 521cee41f7d..f118381e0f5 100644
--- a/src/codegen/arm/macro-assembler-arm.cc
+++ b/src/codegen/arm/macro-assembler-arm.cc
@@ -68,6 +68,7 @@ int TurboAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
 
 int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
                                     Register exclusion2, Register exclusion3) {
+  ASM_CODE_COMMENT(this);
   int bytes = 0;
   RegList exclusions = 0;
   if (exclusion1 != no_reg) {
@@ -95,6 +96,7 @@ int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
 
 int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
                                    Register exclusion2, Register exclusion3) {
+  ASM_CODE_COMMENT(this);
   int bytes = 0;
   if (fp_mode == SaveFPRegsMode::kSave) {
     RestoreFPRegs(sp, lr);
@@ -282,6 +284,7 @@ void TurboAssembler::Call(Handle<Code> code, RelocInfo::Mode rmode,
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
+  ASM_CODE_COMMENT(this);
   STATIC_ASSERT(kSystemPointerSize == 4);
   STATIC_ASSERT(kSmiShiftSize == 0);
   STATIC_ASSERT(kSmiTagSize == 1);
@@ -303,17 +306,20 @@ void TurboAssembler::CallBuiltinByIndex(Register builtin_index) {
 
 void TurboAssembler::LoadEntryFromBuiltin(Builtin builtin,
                                           Register destination) {
+  ASM_CODE_COMMENT(this);
   ldr(destination, EntryFromBuiltinAsOperand(builtin));
 }
 
 MemOperand TurboAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   return MemOperand(kRootRegister,
                     IsolateData::builtin_entry_slot_offset(builtin));
 }
 
 void TurboAssembler::CallBuiltin(Builtin builtin, Condition cond) {
-  RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(this, CommentForOffHeapTrampoline("call", builtin));
+  DCHECK(Builtins::IsBuiltinId(builtin));
   // Use ip directly instead of using UseScratchRegisterScope, as we do not
   // preserve scratch registers across calls.
   mov(ip, Operand(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET));
@@ -322,6 +328,7 @@ void TurboAssembler::CallBuiltin(Builtin builtin, Condition cond) {
 
 void TurboAssembler::LoadCodeObjectEntry(Register destination,
                                          Register code_object) {
+  ASM_CODE_COMMENT(this);
   // Code objects are called differently depending on whether we are generating
   // builtin code (which will later be embedded into the binary) or compiling
   // user JS code at runtime.
@@ -369,17 +376,20 @@ void TurboAssembler::LoadCodeObjectEntry(Register destination,
 }
 
 void TurboAssembler::CallCodeObject(Register code_object) {
+  ASM_CODE_COMMENT(this);
   LoadCodeObjectEntry(code_object, code_object);
   Call(code_object);
 }
 
 void TurboAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
   LoadCodeObjectEntry(code_object, code_object);
   Jump(code_object);
 }
 
 void TurboAssembler::StoreReturnAddressAndCall(Register target) {
+  ASM_CODE_COMMENT(this);
   // This generates the final instruction sequence for calls to C functions
   // once an exit frame has been constructed.
   //
@@ -431,6 +441,7 @@ void TurboAssembler::Push(Smi smi) {
 
 void TurboAssembler::PushArray(Register array, Register size, Register scratch,
                                PushArrayOrder order) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register counter = scratch;
   Register tmp = temps.Acquire();
@@ -647,6 +658,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
                                       SaveFPRegsMode save_fp,
                                       RememberedSetAction remembered_set_action,
                                       SmiCheck smi_check) {
+  ASM_CODE_COMMENT(this);
   // First, check if a write barrier is even needed. The tests below
   // catch stores of Smis.
   Label done;
@@ -661,6 +673,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kPointerSize));
 
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
@@ -680,6 +693,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
 void TurboAssembler::MaybeSaveRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   RegList regs = 0;
   for (int i = 0; i < Register::kNumRegisters; ++i) {
     if ((registers >> i) & 1u) {
@@ -691,6 +705,7 @@ void TurboAssembler::MaybeSaveRegisters(RegList registers) {
 
 void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   RegList regs = 0;
   for (int i = 0; i < Register::kNumRegisters; ++i) {
     if ((registers >> i) & 1u) {
@@ -702,6 +717,7 @@ void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
 
 void TurboAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,
                                              SaveFPRegsMode fp_mode) {
+  ASM_CODE_COMMENT(this);
   RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);
   MaybeSaveRegisters(registers);
 
@@ -719,6 +735,7 @@ void TurboAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,
 void TurboAssembler::CallRecordWriteStubSaveRegisters(
     Register object, Operand offset, RememberedSetAction remembered_set_action,
     SaveFPRegsMode fp_mode, StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);
   MaybeSaveRegisters(registers);
 
@@ -737,6 +754,7 @@ void TurboAssembler::CallRecordWriteStub(
     Register object, Register slot_address,
     RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
     StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK_EQ(WriteBarrierDescriptor::ObjectRegister(), object);
   DCHECK_EQ(WriteBarrierDescriptor::SlotAddressRegister(), slot_address);
 #if V8_ENABLE_WEBASSEMBLY
@@ -802,6 +820,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
                                  SmiCheck smi_check) {
   DCHECK(!AreAliased(object, value));
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
     DCHECK(!AreAliased(object, value, scratch));
@@ -850,6 +869,7 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
 }
 
 void TurboAssembler::PushCommonFrame(Register marker_reg) {
+  ASM_CODE_COMMENT(this);
   if (marker_reg.is_valid()) {
     if (marker_reg.code() > fp.code()) {
       stm(db_w, sp, fp.bit() | lr.bit());
@@ -866,6 +886,7 @@ void TurboAssembler::PushCommonFrame(Register marker_reg) {
 }
 
 void TurboAssembler::PushStandardFrame(Register function_reg) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!function_reg.is_valid() || function_reg.code() < cp.code());
   stm(db_w, sp,
       (function_reg.is_valid() ? function_reg.bit() : 0) | cp.bit() | fp.bit() |
@@ -1328,6 +1349,7 @@ void TurboAssembler::AsrPair(Register dst_low, Register dst_high,
 }
 
 void TurboAssembler::StubPrologue(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   mov(scratch, Operand(StackFrame::TypeToMarker(type)));
@@ -1338,6 +1360,7 @@ void TurboAssembler::Prologue() { PushStandardFrame(r1); }
 
 void TurboAssembler::EnterFrame(StackFrame::Type type,
                                 bool load_constant_pool_pointer_reg) {
+  ASM_CODE_COMMENT(this);
   // r0-r3: preserved
   UseScratchRegisterScope temps(this);
   Register scratch = no_reg;
@@ -1349,6 +1372,7 @@ void TurboAssembler::EnterFrame(StackFrame::Type type,
 }
 
 int TurboAssembler::LeaveFrame(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   // r0: preserved
   // r1: preserved
   // r2: preserved
@@ -1366,6 +1390,7 @@ void TurboAssembler::AllocateStackSpace(Register bytes_scratch) {
   // "Functions that allocate 4 KB or more on the stack must ensure that each
   // page prior to the final page is touched in order." Source:
   // https://docs.microsoft.com/en-us/cpp/build/overview-of-arm-abi-conventions?view=vs-2019#stack
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   DwVfpRegister scratch = temps.AcquireD();
   Label check_offset;
@@ -1385,6 +1410,7 @@ void TurboAssembler::AllocateStackSpace(Register bytes_scratch) {
 }
 
 void TurboAssembler::AllocateStackSpace(int bytes) {
+  ASM_CODE_COMMENT(this);
   DCHECK_GE(bytes, 0);
   UseScratchRegisterScope temps(this);
   DwVfpRegister scratch = no_dreg;
@@ -1403,6 +1429,7 @@ void TurboAssembler::AllocateStackSpace(int bytes) {
 
 void MacroAssembler::EnterExitFrame(bool save_doubles, int stack_space,
                                     StackFrame::Type frame_type) {
+  ASM_CODE_COMMENT(this);
   DCHECK(frame_type == StackFrame::EXIT ||
          frame_type == StackFrame::BUILTIN_EXIT);
   UseScratchRegisterScope temps(this);
@@ -1471,6 +1498,7 @@ int TurboAssembler::ActivationFrameAlignment() {
 
 void MacroAssembler::LeaveExitFrame(bool save_doubles, Register argument_count,
                                     bool argument_count_is_length) {
+  ASM_CODE_COMMENT(this);
   ConstantPoolUnavailableScope constant_pool_unavailable(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
@@ -1528,6 +1556,7 @@ void TurboAssembler::MovFromFloatParameter(DwVfpRegister dst) {
 void TurboAssembler::PrepareForTailCall(Register callee_args_count,
                                         Register caller_args_count,
                                         Register scratch0, Register scratch1) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(callee_args_count, caller_args_count, scratch0, scratch1));
 
   // Calculate the end of destination area where we will put the arguments
@@ -1573,6 +1602,7 @@ void TurboAssembler::PrepareForTailCall(Register callee_args_count,
 }
 
 void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   Isolate* isolate = this->isolate();
   ExternalReference limit =
@@ -1589,6 +1619,7 @@ void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind) {
 
 void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch,
                                         Label* stack_overflow) {
+  ASM_CODE_COMMENT(this);
   // Check the stack for overflow. We are not trying to catch
   // interruptions (e.g. debug break and preemption) here, so the "real stack
   // limit" is checked.
@@ -1604,6 +1635,7 @@ void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch,
 void MacroAssembler::InvokePrologue(Register expected_parameter_count,
                                     Register actual_parameter_count,
                                     Label* done, InvokeType type) {
+  ASM_CODE_COMMENT(this);
   Label regular_invoke;
   //  r0: actual arguments count
   //  r1: function (passed through to callee)
@@ -1671,6 +1703,7 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
 void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
                                              Register expected_parameter_count,
                                              Register actual_parameter_count) {
+  ASM_CODE_COMMENT(this);
   // Load receiver to pass it later to DebugOnFunctionCall hook.
   ldr(r4, ReceiverOperand(actual_parameter_count));
   FrameScope frame(this, has_frame() ? StackFrame::NONE : StackFrame::INTERNAL);
@@ -1704,6 +1737,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
                                         Register expected_parameter_count,
                                         Register actual_parameter_count,
                                         InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
   DCHECK_EQ(function, r1);
@@ -1757,6 +1791,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
 void MacroAssembler::InvokeFunctionWithNewTarget(
     Register fun, Register new_target, Register actual_parameter_count,
     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
 
@@ -1780,6 +1815,7 @@ void MacroAssembler::InvokeFunction(Register function,
                                     Register expected_parameter_count,
                                     Register actual_parameter_count,
                                     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
 
@@ -1794,6 +1830,7 @@ void MacroAssembler::InvokeFunction(Register function,
 }
 
 void MacroAssembler::PushStackHandler() {
+  ASM_CODE_COMMENT(this);
   // Adjust this code if not the case.
   STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kPointerSize);
   STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0 * kPointerSize);
@@ -1809,6 +1846,7 @@ void MacroAssembler::PushStackHandler() {
 }
 
 void MacroAssembler::PopStackHandler() {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
@@ -1821,6 +1859,7 @@ void MacroAssembler::PopStackHandler() {
 
 void MacroAssembler::CompareObjectType(Register object, Register map,
                                        Register type_reg, InstanceType type) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   const Register temp = type_reg == no_reg ? temps.Acquire() : type_reg;
 
@@ -1837,6 +1876,7 @@ void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
 void MacroAssembler::CompareInstanceTypeRange(Register map, Register type_reg,
                                               InstanceType lower_limit,
                                               InstanceType higher_limit) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LT(lower_limit, higher_limit);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
@@ -1856,6 +1896,7 @@ void MacroAssembler::CompareRoot(Register obj, RootIndex index) {
 void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
                                      unsigned higher_limit,
                                      Label* on_in_range) {
+  ASM_CODE_COMMENT(this);
   if (lower_limit != 0) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.Acquire();
@@ -1870,6 +1911,7 @@ void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
 void TurboAssembler::TryInlineTruncateDoubleToI(Register result,
                                                 DwVfpRegister double_input,
                                                 Label* done) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   SwVfpRegister single_scratch = SwVfpRegister::no_reg();
   if (temps.CanAcquireVfp<SwVfpRegister>()) {
@@ -1896,6 +1938,7 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
                                        Register result,
                                        DwVfpRegister double_input,
                                        StubCallMode stub_mode) {
+  ASM_CODE_COMMENT(this);
   Label done;
 
   TryInlineTruncateDoubleToI(result, double_input, &done);
@@ -1927,6 +1970,7 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
 
 void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
                                  SaveFPRegsMode save_doubles) {
+  ASM_CODE_COMMENT(this);
   // All parameters are on the stack.  r0 has the return value after call.
 
   // If the expected number of arguments of the runtime function is
@@ -1946,6 +1990,7 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
 }
 
 void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
+  ASM_CODE_COMMENT(this);
   const Runtime::Function* function = Runtime::FunctionForId(fid);
   DCHECK_EQ(1, function->result_size);
   if (function->nargs >= 0) {
@@ -1988,6 +2033,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
   if (FLAG_native_code_counters && counter->Enabled()) {
+    ASM_CODE_COMMENT(this);
     Move(scratch2, ExternalReference::Create(counter));
     ldr(scratch1, MemOperand(scratch2));
     add(scratch1, scratch1, Operand(value));
@@ -2000,6 +2046,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch2) {
   DCHECK_GT(value, 0);
   if (FLAG_native_code_counters && counter->Enabled()) {
+    ASM_CODE_COMMENT(this);
     Move(scratch2, ExternalReference::Create(counter));
     ldr(scratch1, MemOperand(scratch2));
     sub(scratch1, scratch1, Operand(value));
@@ -2024,6 +2071,7 @@ void TurboAssembler::Check(Condition cond, AbortReason reason) {
 }
 
 void TurboAssembler::Abort(AbortReason reason) {
+  ASM_CODE_COMMENT(this);
   Label abort_start;
   bind(&abort_start);
   if (FLAG_code_comments) {
@@ -2069,10 +2117,12 @@ void TurboAssembler::LoadMap(Register destination, Register object) {
 }
 
 void MacroAssembler::LoadGlobalProxy(Register dst) {
+  ASM_CODE_COMMENT(this);
   LoadNativeContextSlot(dst, Context::GLOBAL_PROXY_INDEX);
 }
 
 void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
+  ASM_CODE_COMMENT(this);
   LoadMap(dst, cp);
   ldr(dst, FieldMemOperand(
                dst, Map::kConstructorOrBackPointerOrNativeContextOffset));
@@ -2080,6 +2130,7 @@ void MacroAssembler::LoadNativeContextSlot(Register dst, int index) {
 }
 
 void TurboAssembler::InitializeRootRegister() {
+  ASM_CODE_COMMENT(this);
   ExternalReference isolate_root = ExternalReference::isolate_root(isolate());
   mov(kRootRegister, Operand(isolate_root));
 }
@@ -2117,63 +2168,64 @@ void MacroAssembler::JumpIfNotSmi(Register value, Label* not_smi_label) {
 }
 
 void MacroAssembler::AssertNotSmi(Register object) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    tst(object, Operand(kSmiTagMask));
-    Check(ne, AbortReason::kOperandIsASmi);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  tst(object, Operand(kSmiTagMask));
+  Check(ne, AbortReason::kOperandIsASmi);
 }
 
 void MacroAssembler::AssertSmi(Register object) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    tst(object, Operand(kSmiTagMask));
-    Check(eq, AbortReason::kOperandIsNotASmi);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  tst(object, Operand(kSmiTagMask));
+  Check(eq, AbortReason::kOperandIsNotASmi);
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    tst(object, Operand(kSmiTagMask));
-    Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor);
-    push(object);
-    LoadMap(object, object);
-    ldrb(object, FieldMemOperand(object, Map::kBitFieldOffset));
-    tst(object, Operand(Map::Bits1::IsConstructorBit::kMask));
-    pop(object);
-    Check(ne, AbortReason::kOperandIsNotAConstructor);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  tst(object, Operand(kSmiTagMask));
+  Check(ne, AbortReason::kOperandIsASmiAndNotAConstructor);
+  push(object);
+  LoadMap(object, object);
+  ldrb(object, FieldMemOperand(object, Map::kBitFieldOffset));
+  tst(object, Operand(Map::Bits1::IsConstructorBit::kMask));
+  pop(object);
+  Check(ne, AbortReason::kOperandIsNotAConstructor);
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    tst(object, Operand(kSmiTagMask));
-    Check(ne, AbortReason::kOperandIsASmiAndNotAFunction);
-    push(object);
-    LoadMap(object, object);
-    CompareInstanceTypeRange(object, object, FIRST_JS_FUNCTION_TYPE,
-                             LAST_JS_FUNCTION_TYPE);
-    pop(object);
-    Check(ls, AbortReason::kOperandIsNotAFunction);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  tst(object, Operand(kSmiTagMask));
+  Check(ne, AbortReason::kOperandIsASmiAndNotAFunction);
+  push(object);
+  LoadMap(object, object);
+  CompareInstanceTypeRange(object, object, FIRST_JS_FUNCTION_TYPE,
+                           LAST_JS_FUNCTION_TYPE);
+  pop(object);
+  Check(ls, AbortReason::kOperandIsNotAFunction);
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    tst(object, Operand(kSmiTagMask));
-    Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction);
-    push(object);
-    CompareObjectType(object, object, object, JS_BOUND_FUNCTION_TYPE);
-    pop(object);
-    Check(eq, AbortReason::kOperandIsNotABoundFunction);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  tst(object, Operand(kSmiTagMask));
+  Check(ne, AbortReason::kOperandIsASmiAndNotABoundFunction);
+  push(object);
+  CompareObjectType(object, object, object, JS_BOUND_FUNCTION_TYPE);
+  pop(object);
+  Check(eq, AbortReason::kOperandIsNotABoundFunction);
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
   tst(object, Operand(kSmiTagMask));
   Check(ne, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
 
@@ -2203,25 +2255,27 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
-  if (FLAG_debug_code) {
-    Label done_checking;
-    AssertNotSmi(object);
-    CompareRoot(object, RootIndex::kUndefinedValue);
-    b(eq, &done_checking);
-    LoadMap(scratch, object);
-    CompareInstanceType(scratch, scratch, ALLOCATION_SITE_TYPE);
-    Assert(eq, AbortReason::kExpectedUndefinedOrCell);
-    bind(&done_checking);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  Label done_checking;
+  AssertNotSmi(object);
+  CompareRoot(object, RootIndex::kUndefinedValue);
+  b(eq, &done_checking);
+  LoadMap(scratch, object);
+  CompareInstanceType(scratch, scratch, ALLOCATION_SITE_TYPE);
+  Assert(eq, AbortReason::kExpectedUndefinedOrCell);
+  bind(&done_checking);
 }
 
 void TurboAssembler::CheckFor32DRegs(Register scratch) {
+  ASM_CODE_COMMENT(this);
   Move(scratch, ExternalReference::cpu_features());
   ldr(scratch, MemOperand(scratch));
   tst(scratch, Operand(1u << VFP32DREGS));
 }
 
 void TurboAssembler::SaveFPRegs(Register location, Register scratch) {
+  ASM_CODE_COMMENT(this);
   CpuFeatureScope scope(this, VFP32DREGS, CpuFeatureScope::kDontCheckSupported);
   CheckFor32DRegs(scratch);
   vstm(db_w, location, d16, d31, ne);
@@ -2230,6 +2284,7 @@ void TurboAssembler::SaveFPRegs(Register location, Register scratch) {
 }
 
 void TurboAssembler::RestoreFPRegs(Register location, Register scratch) {
+  ASM_CODE_COMMENT(this);
   CpuFeatureScope scope(this, VFP32DREGS, CpuFeatureScope::kDontCheckSupported);
   CheckFor32DRegs(scratch);
   vldm(ia_w, location, d0, d15);
@@ -2238,6 +2293,7 @@ void TurboAssembler::RestoreFPRegs(Register location, Register scratch) {
 }
 
 void TurboAssembler::SaveFPRegsToHeap(Register location, Register scratch) {
+  ASM_CODE_COMMENT(this);
   CpuFeatureScope scope(this, VFP32DREGS, CpuFeatureScope::kDontCheckSupported);
   CheckFor32DRegs(scratch);
   vstm(ia_w, location, d0, d15);
@@ -2247,6 +2303,7 @@ void TurboAssembler::SaveFPRegsToHeap(Register location, Register scratch) {
 
 void TurboAssembler::RestoreFPRegsFromHeap(Register location,
                                            Register scratch) {
+  ASM_CODE_COMMENT(this);
   CpuFeatureScope scope(this, VFP32DREGS, CpuFeatureScope::kDontCheckSupported);
   CheckFor32DRegs(scratch);
   vldm(ia_w, location, d0, d15);
@@ -2422,6 +2479,7 @@ int TurboAssembler::CalculateStackPassedWords(int num_reg_arguments,
 void TurboAssembler::PrepareCallCFunction(int num_reg_arguments,
                                           int num_double_arguments,
                                           Register scratch) {
+  ASM_CODE_COMMENT(this);
   int frame_alignment = ActivationFrameAlignment();
   int stack_passed_arguments =
       CalculateStackPassedWords(num_reg_arguments, num_double_arguments);
@@ -2488,6 +2546,7 @@ void TurboAssembler::CallCFunction(Register function, int num_arguments) {
 void TurboAssembler::CallCFunctionHelper(Register function,
                                          int num_reg_arguments,
                                          int num_double_arguments) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LE(num_reg_arguments + num_double_arguments, kMaxCParameters);
   DCHECK(has_frame());
   // Make sure that the stack is aligned before calling a C function unless
@@ -2498,6 +2557,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
     int frame_alignment = base::OS::ActivationFrameAlignment();
     int frame_alignment_mask = frame_alignment - 1;
     if (frame_alignment > kPointerSize) {
+      ASM_CODE_COMMENT_STRING(this, "Check stack alignment");
       DCHECK(base::bits::IsPowerOfTwo(frame_alignment));
       Label alignment_as_expected;
       tst(sp, Operand(frame_alignment_mask));
@@ -2568,6 +2628,7 @@ void TurboAssembler::CallCFunctionHelper(Register function,
 
 void TurboAssembler::CheckPageFlag(Register object, int mask, Condition cc,
                                    Label* condition_met) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.Acquire();
   DCHECK(!AreAliased(object, scratch));
@@ -2600,6 +2661,7 @@ Register GetRegisterThatIsNotOneOf(Register reg1, Register reg2, Register reg3,
 }
 
 void TurboAssembler::ComputeCodeStartAddress(Register dst) {
+  ASM_CODE_COMMENT(this);
   // We can use the register pc - 8 for the address of the current instruction.
   sub(dst, pc, Operand(pc_offset() + Instruction::kPcLoadDelta));
 }
@@ -2611,6 +2673,7 @@ void TurboAssembler::ResetSpeculationPoisonRegister() {
 void TurboAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                            DeoptimizeKind kind, Label* ret,
                                            Label*) {
+  ASM_CODE_COMMENT(this);
   BlockConstPoolScope block_const_pool(this);
   ldr(ip, MemOperand(kRootRegister,
                      IsolateData::builtin_entry_slot_offset(target)));
@@ -2662,18 +2725,21 @@ void TurboAssembler::I64x2Ne(QwNeonRegister dst, QwNeonRegister src1,
 
 void TurboAssembler::I64x2GtS(QwNeonRegister dst, QwNeonRegister src1,
                               QwNeonRegister src2) {
+  ASM_CODE_COMMENT(this);
   vqsub(NeonS64, dst, src2, src1);
   vshr(NeonS64, dst, dst, 63);
 }
 
 void TurboAssembler::I64x2GeS(QwNeonRegister dst, QwNeonRegister src1,
                               QwNeonRegister src2) {
+  ASM_CODE_COMMENT(this);
   vqsub(NeonS64, dst, src1, src2);
   vshr(NeonS64, dst, dst, 63);
   vmvn(dst, dst);
 }
 
 void TurboAssembler::I64x2AllTrue(Register dst, QwNeonRegister src) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   QwNeonRegister tmp = temps.AcquireQ();
   // src = | a | b | c | d |
@@ -2697,6 +2763,7 @@ void TurboAssembler::I64x2AllTrue(Register dst, QwNeonRegister src) {
 }
 
 void TurboAssembler::I64x2Abs(QwNeonRegister dst, QwNeonRegister src) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Simd128Register tmp = temps.AcquireQ();
   vshr(NeonS64, tmp, src, 63);
diff --git a/src/codegen/arm64/assembler-arm64.cc b/src/codegen/arm64/assembler-arm64.cc
index cc975913b96..07bb66383df 100644
--- a/src/codegen/arm64/assembler-arm64.cc
+++ b/src/codegen/arm64/assembler-arm64.cc
@@ -4493,8 +4493,8 @@ void Assembler::RecordVeneerPool(int location_offset, int size) {
 
 void Assembler::EmitVeneers(bool force_emit, bool need_protection,
                             size_t margin) {
+  ASM_CODE_COMMENT(this);
   BlockPoolsScope scope(this, PoolEmissionCheck::kSkip);
-  RecordComment("[ Veneers");
 
   // The exact size of the veneer pool must be recorded (see the comment at the
   // declaration site of RecordConstPool()), but computing the number of
@@ -4587,8 +4587,6 @@ void Assembler::EmitVeneers(bool force_emit, bool need_protection,
   RecordVeneerPool(veneer_pool_relocinfo_loc, pool_size);
 
   bind(&end);
-
-  RecordComment("]");
 }
 
 void Assembler::CheckVeneerPool(bool force_emit, bool require_jump,
diff --git a/src/codegen/arm64/macro-assembler-arm64.cc b/src/codegen/arm64/macro-assembler-arm64.cc
index c3a8aac0776..1bec745dc07 100644
--- a/src/codegen/arm64/macro-assembler-arm64.cc
+++ b/src/codegen/arm64/macro-assembler-arm64.cc
@@ -74,6 +74,7 @@ int TurboAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
 
 int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode,
                                     Register exclusion) {
+  ASM_CODE_COMMENT(this);
   auto list = kCallerSaved;
   list.Remove(exclusion);
   list.Align();
@@ -92,6 +93,7 @@ int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode,
 }
 
 int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion) {
+  ASM_CODE_COMMENT(this);
   int bytes = 0;
   if (fp_mode == SaveFPRegsMode::kSave) {
     auto fp_list = CPURegList::GetCallerSavedV(kStackSavedSavedFPSizeInBits);
@@ -112,6 +114,7 @@ int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion) {
 
 void TurboAssembler::LogicalMacro(const Register& rd, const Register& rn,
                                   const Operand& operand, LogicalOp op) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
 
   if (operand.NeedsRelocation(this)) {
@@ -1218,26 +1221,26 @@ void MacroAssembler::PeekPair(const CPURegister& dst1, const CPURegister& dst2,
 }
 
 void MacroAssembler::PushCalleeSavedRegisters() {
-  {
-    // Ensure that the macro-assembler doesn't use any scratch registers.
-    InstructionAccurateScope scope(this);
+  ASM_CODE_COMMENT(this);
+  // Ensure that the macro-assembler doesn't use any scratch registers.
+  InstructionAccurateScope scope(this);
 
-    MemOperand tos(sp, -2 * static_cast<int>(kXRegSize), PreIndex);
+  MemOperand tos(sp, -2 * static_cast<int>(kXRegSize), PreIndex);
 
-    stp(d14, d15, tos);
-    stp(d12, d13, tos);
-    stp(d10, d11, tos);
-    stp(d8, d9, tos);
+  stp(d14, d15, tos);
+  stp(d12, d13, tos);
+  stp(d10, d11, tos);
+  stp(d8, d9, tos);
 
-    stp(x27, x28, tos);
-    stp(x25, x26, tos);
-    stp(x23, x24, tos);
-    stp(x21, x22, tos);
-    stp(x19, x20, tos);
+  stp(x27, x28, tos);
+  stp(x25, x26, tos);
+  stp(x23, x24, tos);
+  stp(x21, x22, tos);
+  stp(x19, x20, tos);
 
-    STATIC_ASSERT(
-        EntryFrameConstants::kCalleeSavedRegisterBytesPushedBeforeFpLrPair ==
-        18 * kSystemPointerSize);
+  STATIC_ASSERT(
+      EntryFrameConstants::kCalleeSavedRegisterBytesPushedBeforeFpLrPair ==
+      18 * kSystemPointerSize);
 
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
     // Use the stack pointer's value immediately before pushing the LR as the
@@ -1249,22 +1252,22 @@ void MacroAssembler::PushCalleeSavedRegisters() {
 
     STATIC_ASSERT(
         EntryFrameConstants::kCalleeSavedRegisterBytesPushedAfterFpLrPair == 0);
-  }
 }
 
 void MacroAssembler::PopCalleeSavedRegisters() {
-  {
-    // Ensure that the macro-assembler doesn't use any scratch registers.
-    InstructionAccurateScope scope(this);
+  ASM_CODE_COMMENT(this);
+  // Ensure that the macro-assembler doesn't use any scratch registers.
+  InstructionAccurateScope scope(this);
 
-    MemOperand tos(sp, 2 * kXRegSize, PostIndex);
+  MemOperand tos(sp, 2 * kXRegSize, PostIndex);
 
-    ldp(x29, x30, tos);  // fp, lr
+  ldp(x29, x30, tos);  // fp, lr
 
 #ifdef V8_ENABLE_CONTROL_FLOW_INTEGRITY
-    // The context (stack pointer value) for authenticating the LR here must
-    // match the one used for signing it (see `PushCalleeSavedRegisters`).
-    autibsp();
+                       // The context (stack pointer value) for authenticating
+                       // the LR here must
+  // match the one used for signing it (see `PushCalleeSavedRegisters`).
+  autibsp();
 #endif
 
     ldp(x19, x20, tos);
@@ -1277,20 +1280,19 @@ void MacroAssembler::PopCalleeSavedRegisters() {
     ldp(d10, d11, tos);
     ldp(d12, d13, tos);
     ldp(d14, d15, tos);
-  }
 }
 
 void TurboAssembler::AssertSpAligned() {
-  if (FLAG_debug_code) {
-    HardAbortScope hard_abort(this);  // Avoid calls to Abort.
-    // Arm64 requires the stack pointer to be 16-byte aligned prior to address
-    // calculation.
-    UseScratchRegisterScope scope(this);
-    Register temp = scope.AcquireX();
-    Mov(temp, sp);
-    Tst(temp, 15);
-    Check(eq, AbortReason::kUnexpectedStackPointer);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  HardAbortScope hard_abort(this);  // Avoid calls to Abort.
+  // Arm64 requires the stack pointer to be 16-byte aligned prior to address
+  // calculation.
+  UseScratchRegisterScope scope(this);
+  Register temp = scope.AcquireX();
+  Mov(temp, sp);
+  Tst(temp, 15);
+  Check(eq, AbortReason::kUnexpectedStackPointer);
 }
 
 void TurboAssembler::CopySlots(int dst, Register src, Register slot_count) {
@@ -1312,6 +1314,7 @@ void TurboAssembler::CopySlots(Register dst, Register src,
 
 void TurboAssembler::CopyDoubleWords(Register dst, Register src, Register count,
                                      CopyDoubleWordsMode mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(dst, src, count));
 
   if (FLAG_debug_code) {
@@ -1389,13 +1392,14 @@ void TurboAssembler::SlotAddress(Register dst, Register slot_offset) {
 }
 
 void TurboAssembler::AssertFPCRState(Register fpcr) {
-  if (FLAG_debug_code) {
-    Label unexpected_mode, done;
-    UseScratchRegisterScope temps(this);
-    if (fpcr.IsNone()) {
-      fpcr = temps.AcquireX();
-      Mrs(fpcr, FPCR);
-    }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  Label unexpected_mode, done;
+  UseScratchRegisterScope temps(this);
+  if (fpcr.IsNone()) {
+    fpcr = temps.AcquireX();
+    Mrs(fpcr, FPCR);
+  }
 
     // Settings left to their default values:
     //   - Assert that flush-to-zero is not set.
@@ -1409,7 +1413,6 @@ void TurboAssembler::AssertFPCRState(Register fpcr) {
     Abort(AbortReason::kUnexpectedFPCRMode);
 
     Bind(&done);
-  }
 }
 
 void TurboAssembler::CanonicalizeNaN(const VRegister& dst,
@@ -1423,6 +1426,7 @@ void TurboAssembler::CanonicalizeNaN(const VRegister& dst,
 }
 
 void TurboAssembler::LoadRoot(Register destination, RootIndex index) {
+  ASM_CODE_COMMENT(this);
   // TODO(jbramley): Most root values are constants, and can be synthesized
   // without a load. Refer to the ARM back end for details.
   Ldr(destination,
@@ -1430,6 +1434,7 @@ void TurboAssembler::LoadRoot(Register destination, RootIndex index) {
 }
 
 void TurboAssembler::PushRoot(RootIndex index) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register tmp = temps.AcquireX();
   LoadRoot(tmp, index);
@@ -1488,63 +1493,64 @@ void TurboAssembler::Swap(VRegister lhs, VRegister rhs) {
 }
 
 void TurboAssembler::AssertSmi(Register object, AbortReason reason) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    Tst(object, kSmiTagMask);
-    Check(eq, reason);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  Tst(object, kSmiTagMask);
+  Check(eq, reason);
 }
 
 void MacroAssembler::AssertNotSmi(Register object, AbortReason reason) {
-  if (FLAG_debug_code) {
-    STATIC_ASSERT(kSmiTag == 0);
-    Tst(object, kSmiTagMask);
-    Check(ne, reason);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  STATIC_ASSERT(kSmiTag == 0);
+  Tst(object, kSmiTagMask);
+  Check(ne, reason);
 }
 
 void MacroAssembler::AssertConstructor(Register object) {
-  if (FLAG_debug_code) {
-    AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAConstructor);
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAConstructor);
 
-    UseScratchRegisterScope temps(this);
-    Register temp = temps.AcquireX();
+  UseScratchRegisterScope temps(this);
+  Register temp = temps.AcquireX();
 
-    LoadMap(temp, object);
-    Ldrb(temp, FieldMemOperand(temp, Map::kBitFieldOffset));
-    Tst(temp, Operand(Map::Bits1::IsConstructorBit::kMask));
+  LoadMap(temp, object);
+  Ldrb(temp, FieldMemOperand(temp, Map::kBitFieldOffset));
+  Tst(temp, Operand(Map::Bits1::IsConstructorBit::kMask));
 
-    Check(ne, AbortReason::kOperandIsNotAConstructor);
-  }
+  Check(ne, AbortReason::kOperandIsNotAConstructor);
 }
 
 void MacroAssembler::AssertFunction(Register object) {
-  if (FLAG_debug_code) {
-    AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAFunction);
 
-    UseScratchRegisterScope temps(this);
-    Register temp = temps.AcquireX();
-    LoadMap(temp, object);
-    CompareInstanceTypeRange(temp, temp, FIRST_JS_FUNCTION_TYPE,
-                             LAST_JS_FUNCTION_TYPE);
-    Check(ls, AbortReason::kOperandIsNotAFunction);
-  }
+  UseScratchRegisterScope temps(this);
+  Register temp = temps.AcquireX();
+  LoadMap(temp, object);
+  CompareInstanceTypeRange(temp, temp, FIRST_JS_FUNCTION_TYPE,
+                           LAST_JS_FUNCTION_TYPE);
+  Check(ls, AbortReason::kOperandIsNotAFunction);
 }
 
 void MacroAssembler::AssertBoundFunction(Register object) {
-  if (FLAG_debug_code) {
-    AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotABoundFunction);
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotABoundFunction);
 
-    UseScratchRegisterScope temps(this);
-    Register temp = temps.AcquireX();
+  UseScratchRegisterScope temps(this);
+  Register temp = temps.AcquireX();
 
-    CompareObjectType(object, temp, temp, JS_BOUND_FUNCTION_TYPE);
-    Check(eq, AbortReason::kOperandIsNotABoundFunction);
-  }
+  CompareObjectType(object, temp, temp, JS_BOUND_FUNCTION_TYPE);
+  Check(eq, AbortReason::kOperandIsNotABoundFunction);
 }
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
   AssertNotSmi(object, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
 
   // Load map
@@ -1570,31 +1576,32 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 }
 
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object) {
-  if (FLAG_debug_code) {
-    UseScratchRegisterScope temps(this);
-    Register scratch = temps.AcquireX();
-    Label done_checking;
-    AssertNotSmi(object);
-    JumpIfRoot(object, RootIndex::kUndefinedValue, &done_checking);
-    LoadMap(scratch, object);
-    CompareInstanceType(scratch, scratch, ALLOCATION_SITE_TYPE);
-    Assert(eq, AbortReason::kExpectedUndefinedOrCell);
-    Bind(&done_checking);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  UseScratchRegisterScope temps(this);
+  Register scratch = temps.AcquireX();
+  Label done_checking;
+  AssertNotSmi(object);
+  JumpIfRoot(object, RootIndex::kUndefinedValue, &done_checking);
+  LoadMap(scratch, object);
+  CompareInstanceType(scratch, scratch, ALLOCATION_SITE_TYPE);
+  Assert(eq, AbortReason::kExpectedUndefinedOrCell);
+  Bind(&done_checking);
 }
 
 void TurboAssembler::AssertPositiveOrZero(Register value) {
-  if (FLAG_debug_code) {
-    Label done;
-    int sign_bit = value.Is64Bits() ? kXSignBit : kWSignBit;
-    Tbz(value, sign_bit, &done);
-    Abort(AbortReason::kUnexpectedNegativeValue);
-    Bind(&done);
-  }
+  if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
+  Label done;
+  int sign_bit = value.Is64Bits() ? kXSignBit : kWSignBit;
+  Tbz(value, sign_bit, &done);
+  Abort(AbortReason::kUnexpectedNegativeValue);
+  Bind(&done);
 }
 
 void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
                                  SaveFPRegsMode save_doubles) {
+  ASM_CODE_COMMENT(this);
   // All arguments must be on the stack before this function is called.
   // x0 holds the return value after the call.
 
@@ -1613,6 +1620,7 @@ void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
 
 void MacroAssembler::JumpToExternalReference(const ExternalReference& builtin,
                                              bool builtin_exit_frame) {
+  ASM_CODE_COMMENT(this);
   Mov(x1, builtin);
   Handle<Code> code = CodeFactory::CEntry(isolate(), 1, SaveFPRegsMode::kIgnore,
                                           ArgvMode::kStack, builtin_exit_frame);
@@ -1625,6 +1633,7 @@ void MacroAssembler::JumpToInstructionStream(Address entry) {
 }
 
 void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
+  ASM_CODE_COMMENT(this);
   const Runtime::Function* function = Runtime::FunctionForId(fid);
   DCHECK_EQ(1, function->result_size);
   if (function->nargs >= 0) {
@@ -1661,6 +1670,7 @@ void TurboAssembler::CallCFunction(ExternalReference function,
 void TurboAssembler::CallCFunction(ExternalReference function,
                                    int num_of_reg_args,
                                    int num_of_double_args) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register temp = temps.AcquireX();
   Mov(temp, function);
@@ -1671,6 +1681,7 @@ static const int kRegisterPassedArguments = 8;
 
 void TurboAssembler::CallCFunction(Register function, int num_of_reg_args,
                                    int num_of_double_args) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LE(num_of_reg_args + num_of_double_args, kMaxCParameters);
   DCHECK(has_frame());
 
@@ -1739,6 +1750,7 @@ void TurboAssembler::CallCFunction(Register function, int num_of_reg_args,
 
 void TurboAssembler::LoadFromConstantsTable(Register destination,
                                             int constant_index) {
+  ASM_CODE_COMMENT(this);
   DCHECK(RootsTable::IsImmortalImmovable(RootIndex::kBuiltinsConstantsTable));
   LoadRoot(destination, RootIndex::kBuiltinsConstantsTable);
   LoadTaggedPointerField(
@@ -1902,6 +1914,7 @@ void TurboAssembler::Call(ExternalReference target) {
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
+  ASM_CODE_COMMENT(this);
   // The builtin_index register contains the builtin index as a Smi.
   // Untagging is folded into the indexing operand below.
   if (SmiValuesAre32Bits()) {
@@ -1929,17 +1942,20 @@ void TurboAssembler::LoadEntryFromBuiltin(Builtin builtin,
 }
 
 MemOperand TurboAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   return MemOperand(kRootRegister,
                     IsolateData::builtin_entry_slot_offset(builtin));
 }
 
 void TurboAssembler::CallBuiltinByIndex(Register builtin_index) {
+  ASM_CODE_COMMENT(this);
   LoadEntryFromBuiltinIndex(builtin_index);
   Call(builtin_index);
 }
 
 void TurboAssembler::CallBuiltin(Builtin builtin) {
+  ASM_CODE_COMMENT(this);
   DCHECK(Builtins::IsBuiltinId(builtin));
   RecordCommentForOffHeapTrampoline(builtin);
   CHECK_NE(builtin, Builtin::kNoBuiltinId);
@@ -1952,10 +1968,10 @@ void TurboAssembler::CallBuiltin(Builtin builtin) {
     Ldr(scratch, Operand(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET));
     Call(scratch);
   }
-  RecordComment("]");
 }
 
 void TurboAssembler::TailCallBuiltin(Builtin builtin) {
+  ASM_CODE_COMMENT(this);
   DCHECK(Builtins::IsBuiltinId(builtin));
   RecordCommentForOffHeapTrampoline(builtin);
   CHECK_NE(builtin, Builtin::kNoBuiltinId);
@@ -1977,11 +1993,11 @@ void TurboAssembler::TailCallBuiltin(Builtin builtin) {
     Ldr(temp, Operand(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET));
     Jump(temp);
   }
-  RecordComment("]");
 }
 
 void TurboAssembler::LoadCodeObjectEntry(Register destination,
                                          Register code_object) {
+  ASM_CODE_COMMENT(this);
   // Code objects are called differently depending on whether we are generating
   // builtin code (which will later be embedded into the binary) or compiling
   // user JS code at runtime.
@@ -2030,11 +2046,13 @@ void TurboAssembler::LoadCodeObjectEntry(Register destination,
 }
 
 void TurboAssembler::CallCodeObject(Register code_object) {
+  ASM_CODE_COMMENT(this);
   LoadCodeObjectEntry(code_object, code_object);
   Call(code_object);
 }
 
 void TurboAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK_EQ(JumpMode::kJump, jump_mode);
   LoadCodeObjectEntry(code_object, code_object);
 
@@ -2047,6 +2065,7 @@ void TurboAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
 }
 
 void TurboAssembler::StoreReturnAddressAndCall(Register target) {
+  ASM_CODE_COMMENT(this);
   // This generates the final instruction sequence for calls to C functions
   // once an exit frame has been constructed.
   //
@@ -2066,6 +2085,7 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
   Poke(x17, 0);
 
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify fp[kSPOffset]-8");
     // Verify that the slot below fp[kSPOffset]-8 points to the signed return
     // location.
     Ldr(x16, MemOperand(fp, ExitFrameConstants::kSPOffset));
@@ -2079,6 +2099,7 @@ void TurboAssembler::StoreReturnAddressAndCall(Register target) {
 }
 
 void TurboAssembler::IndirectCall(Address target, RelocInfo::Mode rmode) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register temp = temps.AcquireX();
   Mov(temp, Immediate(target, rmode));
@@ -2092,6 +2113,7 @@ bool TurboAssembler::IsNearCallOffset(int64_t offset) {
 void TurboAssembler::CallForDeoptimization(
     Builtin target, int deopt_id, Label* exit, DeoptimizeKind kind, Label* ret,
     Label* jump_deoptimization_entry_label) {
+  ASM_CODE_COMMENT(this);
   BlockPoolsScope scope(this);
   bl(jump_deoptimization_entry_label);
   DCHECK_EQ(SizeOfCodeGeneratedSince(exit),
@@ -2109,6 +2131,7 @@ void TurboAssembler::CallForDeoptimization(
 void TurboAssembler::PrepareForTailCall(Register callee_args_count,
                                         Register caller_args_count,
                                         Register scratch0, Register scratch1) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(callee_args_count, caller_args_count, scratch0, scratch1));
 
   // Calculate the end of destination area where we will put the arguments
@@ -2162,6 +2185,7 @@ void TurboAssembler::PrepareForTailCall(Register callee_args_count,
 }
 
 void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   Isolate* isolate = this->isolate();
   ExternalReference limit =
@@ -2177,6 +2201,7 @@ void MacroAssembler::LoadStackLimit(Register destination, StackLimitKind kind) {
 
 void MacroAssembler::StackOverflowCheck(Register num_args,
                                         Label* stack_overflow) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.AcquireX();
 
@@ -2196,6 +2221,7 @@ void MacroAssembler::StackOverflowCheck(Register num_args,
 void MacroAssembler::InvokePrologue(Register formal_parameter_count,
                                     Register actual_argument_count, Label* done,
                                     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   //  x0: actual arguments count.
   //  x1: function (passed through to callee).
   //  x2: expected arguments count.
@@ -2304,6 +2330,7 @@ void MacroAssembler::InvokePrologue(Register formal_parameter_count,
 void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
                                              Register expected_parameter_count,
                                              Register actual_parameter_count) {
+  ASM_CODE_COMMENT(this);
   // Load receiver to pass it later to DebugOnFunctionCall hook.
   Peek(x4, ReceiverOperand(actual_parameter_count));
   FrameScope frame(this, has_frame() ? StackFrame::NONE : StackFrame::INTERNAL);
@@ -2327,6 +2354,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
                                         Register expected_parameter_count,
                                         Register actual_parameter_count,
                                         InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
   DCHECK_EQ(function, x1);
@@ -2386,6 +2414,7 @@ Operand MacroAssembler::ReceiverOperand(Register arg_count) {
 void MacroAssembler::InvokeFunctionWithNewTarget(
     Register function, Register new_target, Register actual_parameter_count,
     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK(type == InvokeType::kJump || has_frame());
 
@@ -2415,6 +2444,7 @@ void MacroAssembler::InvokeFunction(Register function,
                                     Register expected_parameter_count,
                                     Register actual_parameter_count,
                                     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK(type == InvokeType::kJump || has_frame());
 
@@ -2433,6 +2463,7 @@ void MacroAssembler::InvokeFunction(Register function,
 void TurboAssembler::TryConvertDoubleToInt64(Register result,
                                              DoubleRegister double_input,
                                              Label* done) {
+  ASM_CODE_COMMENT(this);
   // Try to convert with an FPU convert instruction. It's trivial to compute
   // the modulo operation on an integer register so we convert to a 64-bit
   // integer.
@@ -2459,6 +2490,7 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
                                        DoubleRegister double_input,
                                        StubCallMode stub_mode,
                                        LinkRegisterStatus lr_status) {
+  ASM_CODE_COMMENT(this);
   if (CpuFeatures::IsSupported(JSCVT)) {
     Fjcvtzs(result.W(), double_input);
     return;
@@ -2507,6 +2539,7 @@ void TurboAssembler::TruncateDoubleToI(Isolate* isolate, Zone* zone,
 }
 
 void TurboAssembler::Prologue() {
+  ASM_CODE_COMMENT(this);
   Push<TurboAssembler::kSignLR>(lr, fp);
   mov(fp, sp);
   STATIC_ASSERT(kExtraSlotClaimedByPrologue == 1);
@@ -2573,6 +2606,7 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
 }
 
 void TurboAssembler::LeaveFrame(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   // Drop the execution stack down to the frame pointer and restore
   // the caller frame pointer and return address.
   Mov(sp, fp);
@@ -2580,6 +2614,7 @@ void TurboAssembler::LeaveFrame(StackFrame::Type type) {
 }
 
 void MacroAssembler::ExitFramePreserveFPRegs() {
+  ASM_CODE_COMMENT(this);
   DCHECK_EQ(kCallerSavedV.Count() % 2, 0);
   PushCPURegList(kCallerSavedV);
 }
@@ -2587,6 +2622,7 @@ void MacroAssembler::ExitFramePreserveFPRegs() {
 void MacroAssembler::ExitFrameRestoreFPRegs() {
   // Read the registers from the stack without popping them. The stack pointer
   // will be reset as part of the unwinding process.
+  ASM_CODE_COMMENT(this);
   CPURegList saved_fp_regs = kCallerSavedV;
   DCHECK_EQ(saved_fp_regs.Count() % 2, 0);
 
@@ -2602,6 +2638,7 @@ void MacroAssembler::ExitFrameRestoreFPRegs() {
 void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
                                     int extra_space,
                                     StackFrame::Type frame_type) {
+  ASM_CODE_COMMENT(this);
   DCHECK(frame_type == StackFrame::EXIT ||
          frame_type == StackFrame::BUILTIN_EXIT);
 
@@ -2663,6 +2700,7 @@ void MacroAssembler::EnterExitFrame(bool save_doubles, const Register& scratch,
 void MacroAssembler::LeaveExitFrame(bool restore_doubles,
                                     const Register& scratch,
                                     const Register& scratch2) {
+  ASM_CODE_COMMENT(this);
   if (restore_doubles) {
     ExitFrameRestoreFPRegs();
   }
@@ -2693,11 +2731,13 @@ void MacroAssembler::LeaveExitFrame(bool restore_doubles,
 }
 
 void MacroAssembler::LoadGlobalProxy(Register dst) {
+  ASM_CODE_COMMENT(this);
   LoadNativeContextSlot(dst, Context::GLOBAL_PROXY_INDEX);
 }
 
 void MacroAssembler::LoadWeakValue(Register out, Register in,
                                    Label* target_if_cleared) {
+  ASM_CODE_COMMENT(this);
   CompareAndBranch(in.W(), Operand(kClearedWeakHeapObjectLower32), eq,
                    target_if_cleared);
 
@@ -2707,6 +2747,7 @@ void MacroAssembler::LoadWeakValue(Register out, Register in,
 void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch1,
                                           Register scratch2) {
+  ASM_CODE_COMMENT(this);
   DCHECK_NE(value, 0);
   if (FLAG_native_code_counters && counter->Enabled()) {
     // This operation has to be exactly 32-bit wide in case the external
@@ -2722,6 +2763,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
 void MacroAssembler::JumpIfObjectType(Register object, Register map,
                                       Register type_reg, InstanceType type,
                                       Label* if_cond_pass, Condition cond) {
+  ASM_CODE_COMMENT(this);
   CompareObjectType(object, map, type_reg, type);
   B(cond, if_cond_pass);
 }
@@ -2729,17 +2771,20 @@ void MacroAssembler::JumpIfObjectType(Register object, Register map,
 // Sets condition flags based on comparison, and returns type in type_reg.
 void MacroAssembler::CompareObjectType(Register object, Register map,
                                        Register type_reg, InstanceType type) {
+  ASM_CODE_COMMENT(this);
   LoadMap(map, object);
   CompareInstanceType(map, type_reg, type);
 }
 
 void TurboAssembler::LoadMap(Register dst, Register object) {
+  ASM_CODE_COMMENT(this);
   LoadTaggedPointerField(dst, FieldMemOperand(object, HeapObject::kMapOffset));
 }
 
 // Sets condition flags based on comparison, and returns type in type_reg.
 void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
                                          InstanceType type) {
+  ASM_CODE_COMMENT(this);
   Ldrh(type_reg, FieldMemOperand(map, Map::kInstanceTypeOffset));
   Cmp(type_reg, type);
 }
@@ -2748,6 +2793,7 @@ void MacroAssembler::CompareInstanceType(Register map, Register type_reg,
 void MacroAssembler::CompareInstanceTypeRange(Register map, Register type_reg,
                                               InstanceType lower_limit,
                                               InstanceType higher_limit) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LT(lower_limit, higher_limit);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.AcquireX();
@@ -2757,6 +2803,7 @@ void MacroAssembler::CompareInstanceTypeRange(Register map, Register type_reg,
 }
 
 void MacroAssembler::LoadElementsKindFromMap(Register result, Register map) {
+  ASM_CODE_COMMENT(this);
   // Load the map's "bit field 2".
   Ldrb(result, FieldMemOperand(map, Map::kBitField2Offset));
   // Retrieve elements_kind from bit field 2.
@@ -2764,6 +2811,7 @@ void MacroAssembler::LoadElementsKindFromMap(Register result, Register map) {
 }
 
 void MacroAssembler::CompareRoot(const Register& obj, RootIndex index) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register temp = temps.AcquireX();
   DCHECK(!AreAliased(obj, temp));
@@ -2787,6 +2835,7 @@ void MacroAssembler::JumpIfIsInRange(const Register& value,
                                      unsigned lower_limit,
                                      unsigned higher_limit,
                                      Label* on_in_range) {
+  ASM_CODE_COMMENT(this);
   if (lower_limit != 0) {
     UseScratchRegisterScope temps(this);
     Register scratch = temps.AcquireW();
@@ -2841,41 +2890,38 @@ void TurboAssembler::StoreTaggedField(const Register& value,
 
 void TurboAssembler::DecompressTaggedSigned(const Register& destination,
                                             const MemOperand& field_operand) {
-  RecordComment("[ DecompressTaggedSigned");
+  ASM_CODE_COMMENT(this);
   Ldr(destination.W(), field_operand);
   if (FLAG_debug_code) {
     // Corrupt the top 32 bits. Made up of 16 fixed bits and 16 pc offset bits.
     Add(destination, destination,
         ((kDebugZapValue << 16) | (pc_offset() & 0xffff)) << 32);
   }
-  RecordComment("]");
 }
 
 void TurboAssembler::DecompressTaggedPointer(const Register& destination,
                                              const MemOperand& field_operand) {
-  RecordComment("[ DecompressTaggedPointer");
+  ASM_CODE_COMMENT(this);
   Ldr(destination.W(), field_operand);
   Add(destination, kPtrComprCageBaseRegister, destination);
-  RecordComment("]");
 }
 
 void TurboAssembler::DecompressTaggedPointer(const Register& destination,
                                              const Register& source) {
-  RecordComment("[ DecompressTaggedPointer");
+  ASM_CODE_COMMENT(this);
   Add(destination, kPtrComprCageBaseRegister, Operand(source, UXTW));
-  RecordComment("]");
 }
 
 void TurboAssembler::DecompressAnyTagged(const Register& destination,
                                          const MemOperand& field_operand) {
-  RecordComment("[ DecompressAnyTagged");
+  ASM_CODE_COMMENT(this);
   Ldr(destination.W(), field_operand);
   Add(destination, kPtrComprCageBaseRegister, destination);
-  RecordComment("]");
 }
 
 void TurboAssembler::CheckPageFlag(const Register& object, int mask,
                                    Condition cc, Label* condition_met) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope temps(this);
   Register scratch = temps.AcquireX();
   And(scratch, object, ~kPageAlignmentMask);
@@ -2894,6 +2940,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
                                       SaveFPRegsMode save_fp,
                                       RememberedSetAction remembered_set_action,
                                       SmiCheck smi_check) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(object, value));
   // First, check if a write barrier is even needed. The tests below
   // catch stores of Smis.
@@ -2909,6 +2956,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
   DCHECK(IsAligned(offset, kTaggedSize));
 
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     UseScratchRegisterScope temps(this);
     Register scratch = temps.AcquireX();
@@ -2928,6 +2976,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
 void TurboAssembler::MaybeSaveRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   CPURegList regs(CPURegister::kRegister, kXRegSizeInBits, registers);
   // If we were saving LR, we might need to sign it.
   DCHECK(!regs.IncludesAliasOf(lr));
@@ -2937,6 +2986,7 @@ void TurboAssembler::MaybeSaveRegisters(RegList registers) {
 
 void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   CPURegList regs(CPURegister::kRegister, kXRegSizeInBits, registers);
   // If we were saving LR, we might need to sign it.
   DCHECK(!regs.IncludesAliasOf(lr));
@@ -2946,6 +2996,7 @@ void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
 
 void TurboAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,
                                              SaveFPRegsMode fp_mode) {
+  ASM_CODE_COMMENT(this);
   RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);
   MaybeSaveRegisters(registers);
 
@@ -2962,6 +3013,7 @@ void TurboAssembler::CallEphemeronKeyBarrier(Register object, Operand offset,
 void TurboAssembler::CallRecordWriteStubSaveRegisters(
     Register object, Operand offset, RememberedSetAction remembered_set_action,
     SaveFPRegsMode fp_mode, StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   RegList registers = WriteBarrierDescriptor::ComputeSavedRegisters(object);
   MaybeSaveRegisters(registers);
 
@@ -2980,6 +3032,7 @@ void TurboAssembler::CallRecordWriteStub(
     Register object, Register slot_address,
     RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
     StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK_EQ(WriteBarrierDescriptor::ObjectRegister(), object);
   DCHECK_EQ(WriteBarrierDescriptor::SlotAddressRegister(), slot_address);
 #if V8_ENABLE_WEBASSEMBLY
@@ -3004,6 +3057,7 @@ void TurboAssembler::CallRecordWriteStub(
 
 void TurboAssembler::MoveObjectAndSlot(Register dst_object, Register dst_slot,
                                        Register object, Operand offset) {
+  ASM_CODE_COMMENT(this);
   DCHECK_NE(dst_object, dst_slot);
   // If `offset` is a register, it cannot overlap with `object`.
   DCHECK_IMPLIES(!offset.IsImmediate(), offset.reg() != object);
@@ -3044,10 +3098,12 @@ void MacroAssembler::RecordWrite(Register object, Operand offset,
                                  SaveFPRegsMode fp_mode,
                                  RememberedSetAction remembered_set_action,
                                  SmiCheck smi_check) {
+  ASM_CODE_COMMENT(this);
   ASM_LOCATION_IN_ASSEMBLER("MacroAssembler::RecordWrite");
   DCHECK(!AreAliased(object, value));
 
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     UseScratchRegisterScope temps(this);
     Register temp = temps.AcquireX();
     DCHECK(!AreAliased(object, value, temp));
@@ -3117,6 +3173,7 @@ void TurboAssembler::Trap() { Brk(0); }
 void TurboAssembler::DebugBreak() { Debug("DebugBreak", 0, BREAK); }
 
 void TurboAssembler::Abort(AbortReason reason) {
+  ASM_CODE_COMMENT(this);
   if (FLAG_code_comments) {
     RecordComment("Abort message: ");
     RecordComment(GetAbortReason(reason));
@@ -3173,6 +3230,7 @@ void TurboAssembler::PrintfNoPreserve(const char* format,
                                       const CPURegister& arg1,
                                       const CPURegister& arg2,
                                       const CPURegister& arg3) {
+  ASM_CODE_COMMENT(this);
   // We cannot handle a caller-saved stack pointer. It doesn't make much sense
   // in most cases anyway, so this restriction shouldn't be too serious.
   DCHECK(!kCallerSaved.IncludesAliasOf(sp));
@@ -3318,6 +3376,7 @@ void TurboAssembler::PrintfNoPreserve(const char* format,
 }
 
 void TurboAssembler::CallPrintf(int arg_count, const CPURegister* args) {
+  ASM_CODE_COMMENT(this);
   // A call to printf needs special handling for the simulator, since the system
   // printf function will use a different instruction set and the procedure-call
   // standard will not be compatible.
@@ -3349,6 +3408,7 @@ void TurboAssembler::CallPrintf(int arg_count, const CPURegister* args) {
 void TurboAssembler::Printf(const char* format, CPURegister arg0,
                             CPURegister arg1, CPURegister arg2,
                             CPURegister arg3) {
+  ASM_CODE_COMMENT(this);
   // Printf is expected to preserve all registers, so make sure that none are
   // available as scratch registers until we've preserved them.
   RegList old_tmp_list = TmpList()->list();
@@ -3485,6 +3545,7 @@ void TurboAssembler::StoreReturnAddressInWasmExitFrame(Label* return_location) {
 #endif  // V8_ENABLE_WEBASSEMBLY
 
 void TurboAssembler::I64x2BitMask(Register dst, VRegister src) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope scope(this);
   VRegister tmp1 = scope.AcquireV(kFormat2D);
   Register tmp2 = scope.AcquireX();
@@ -3495,6 +3556,7 @@ void TurboAssembler::I64x2BitMask(Register dst, VRegister src) {
 }
 
 void TurboAssembler::I64x2AllTrue(Register dst, VRegister src) {
+  ASM_CODE_COMMENT(this);
   UseScratchRegisterScope scope(this);
   VRegister tmp = scope.AcquireV(kFormat2D);
   Cmeq(tmp.V2D(), src.V2D(), 0);
diff --git a/src/codegen/ia32/macro-assembler-ia32.cc b/src/codegen/ia32/macro-assembler-ia32.cc
index 08c2e80ddb3..16298ed5364 100644
--- a/src/codegen/ia32/macro-assembler-ia32.cc
+++ b/src/codegen/ia32/macro-assembler-ia32.cc
@@ -76,11 +76,13 @@ Operand StackArgumentsAccessor::GetArgumentOperand(int index) const {
 // MacroAssembler implementation.
 
 void TurboAssembler::InitializeRootRegister() {
+  ASM_CODE_COMMENT(this);
   ExternalReference isolate_root = ExternalReference::isolate_root(isolate());
   Move(kRootRegister, Immediate(isolate_root));
 }
 
 void TurboAssembler::LoadRoot(Register destination, RootIndex index) {
+  ASM_CODE_COMMENT(this);
   if (root_array_available()) {
     mov(destination,
         Operand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
@@ -107,6 +109,7 @@ void TurboAssembler::LoadRoot(Register destination, RootIndex index) {
 
 void TurboAssembler::CompareRoot(Register with, Register scratch,
                                  RootIndex index) {
+  ASM_CODE_COMMENT(this);
   if (root_array_available()) {
     CompareRoot(with, index);
   } else {
@@ -118,6 +121,7 @@ void TurboAssembler::CompareRoot(Register with, Register scratch,
 }
 
 void TurboAssembler::CompareRoot(Register with, RootIndex index) {
+  ASM_CODE_COMMENT(this);
   if (root_array_available()) {
     cmp(with, Operand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
     return;
@@ -133,6 +137,7 @@ void TurboAssembler::CompareRoot(Register with, RootIndex index) {
 }
 
 void MacroAssembler::PushRoot(RootIndex index) {
+  ASM_CODE_COMMENT(this);
   if (root_array_available()) {
     DCHECK(RootsTable::IsImmortalImmovable(index));
     push(Operand(kRootRegister, RootRegisterOffsetForRootIndex(index)));
@@ -164,6 +169,7 @@ void MacroAssembler::JumpIfIsInRange(Register value, unsigned lower_limit,
 
 void TurboAssembler::PushArray(Register array, Register size, Register scratch,
                                PushArrayOrder order) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(array, size, scratch));
   Register counter = scratch;
   Label loop, entry;
@@ -243,6 +249,7 @@ Operand TurboAssembler::HeapObjectAsOperand(Handle<HeapObject> object) {
 
 void TurboAssembler::LoadFromConstantsTable(Register destination,
                                             int constant_index) {
+  ASM_CODE_COMMENT(this);
   DCHECK(RootsTable::IsImmortalImmovable(RootIndex::kBuiltinsConstantsTable));
   LoadRoot(destination, RootIndex::kBuiltinsConstantsTable);
   mov(destination,
@@ -251,6 +258,7 @@ void TurboAssembler::LoadFromConstantsTable(Register destination,
 
 void TurboAssembler::LoadRootRegisterOffset(Register destination,
                                             intptr_t offset) {
+  ASM_CODE_COMMENT(this);
   DCHECK(is_int32(offset));
   DCHECK(root_array_available());
   if (offset == 0) {
@@ -261,6 +269,7 @@ void TurboAssembler::LoadRootRegisterOffset(Register destination,
 }
 
 void TurboAssembler::LoadRootRelative(Register destination, int32_t offset) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   mov(destination, Operand(kRootRegister, offset));
 }
@@ -301,6 +310,7 @@ int TurboAssembler::RequiredStackSizeForCallerSaved(SaveFPRegsMode fp_mode,
 
 int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
                                     Register exclusion2, Register exclusion3) {
+  ASM_CODE_COMMENT(this);
   // We don't allow a GC during a store buffer overflow so there is no need to
   // store the registers in any particular way, but we do have to store and
   // restore them.
@@ -333,6 +343,7 @@ int TurboAssembler::PushCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
 
 int TurboAssembler::PopCallerSaved(SaveFPRegsMode fp_mode, Register exclusion1,
                                    Register exclusion2, Register exclusion3) {
+  ASM_CODE_COMMENT(this);
   int bytes = 0;
   if (fp_mode == SaveFPRegsMode::kSave) {
     // Restore all XMM registers except XMM0.
@@ -365,6 +376,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
                                       SaveFPRegsMode save_fp,
                                       RememberedSetAction remembered_set_action,
                                       SmiCheck smi_check) {
+  ASM_CODE_COMMENT(this);
   // First, check if a write barrier is even needed. The tests below
   // catch stores of Smis.
   Label done;
@@ -402,6 +414,7 @@ void MacroAssembler::RecordWriteField(Register object, int offset,
 
 void TurboAssembler::MaybeSaveRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   for (int i = 0; i < Register::kNumRegisters; ++i) {
     if ((registers >> i) & 1u) {
       push(Register::from_code(i));
@@ -411,6 +424,7 @@ void TurboAssembler::MaybeSaveRegisters(RegList registers) {
 
 void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
   if (registers == 0) return;
+  ASM_CODE_COMMENT(this);
   for (int i = Register::kNumRegisters - 1; i >= 0; --i) {
     if ((registers >> i) & 1u) {
       pop(Register::from_code(i));
@@ -421,6 +435,7 @@ void TurboAssembler::MaybeRestoreRegisters(RegList registers) {
 void TurboAssembler::CallEphemeronKeyBarrier(Register object,
                                              Register slot_address,
                                              SaveFPRegsMode fp_mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(object, slot_address));
   RegList registers =
       WriteBarrierDescriptor::ComputeSavedRegisters(object, slot_address);
@@ -446,6 +461,7 @@ void TurboAssembler::CallRecordWriteStubSaveRegisters(
     Register object, Register slot_address,
     RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
     StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(object, slot_address));
   RegList registers =
       WriteBarrierDescriptor::ComputeSavedRegisters(object, slot_address);
@@ -470,6 +486,7 @@ void TurboAssembler::CallRecordWriteStub(
     Register object, Register slot_address,
     RememberedSetAction remembered_set_action, SaveFPRegsMode fp_mode,
     StubCallMode mode) {
+  ASM_CODE_COMMENT(this);
   // Use CallRecordWriteStubSaveRegisters if the object and slot registers
   // need to be caller saved.
   DCHECK_EQ(WriteBarrierDescriptor::ObjectRegister(), object);
@@ -499,6 +516,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
                                  Register value, SaveFPRegsMode fp_mode,
                                  RememberedSetAction remembered_set_action,
                                  SmiCheck smi_check) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(object, value, slot_address));
   AssertNotSmi(object);
 
@@ -509,6 +527,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
   }
 
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Verify slot_address");
     Label ok;
     cmp(value, Operand(slot_address, 0));
     j(equal, &ok, Label::kNear);
@@ -542,6 +561,7 @@ void MacroAssembler::RecordWrite(Register object, Register slot_address,
   // Clobber clobbered registers when running with the debug-code flag
   // turned on to provoke errors.
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT_STRING(this, "Clobber slot_address and value");
     mov(slot_address, Immediate(bit_cast<int32_t>(kZapValue)));
     mov(value, Immediate(bit_cast<int32_t>(kZapValue)));
   }
@@ -623,6 +643,7 @@ void TurboAssembler::Pmulhrsw(XMMRegister dst, XMMRegister src1,
 
 void TurboAssembler::I16x8Q15MulRSatS(XMMRegister dst, XMMRegister src1,
                                       XMMRegister src2, XMMRegister scratch) {
+  ASM_CODE_COMMENT(this);
   // k = i16x8.splat(0x8000)
   Pcmpeqd(scratch, scratch);
   Psllw(scratch, scratch, byte{15});
@@ -635,6 +656,7 @@ void TurboAssembler::I16x8Q15MulRSatS(XMMRegister dst, XMMRegister src1,
 void TurboAssembler::I8x16Popcnt(XMMRegister dst, XMMRegister src,
                                  XMMRegister tmp1, XMMRegister tmp2,
                                  Register scratch) {
+  ASM_CODE_COMMENT(this);
   DCHECK_NE(dst, tmp1);
   DCHECK_NE(src, tmp1);
   DCHECK_NE(dst, tmp2);
@@ -723,6 +745,7 @@ void TurboAssembler::F64x2ConvertLowI32x4U(XMMRegister dst, XMMRegister src,
 void TurboAssembler::I32x4TruncSatF64x2SZero(XMMRegister dst, XMMRegister src,
                                              XMMRegister scratch,
                                              Register tmp) {
+  ASM_CODE_COMMENT(this);
   if (CpuFeatures::IsSupported(AVX)) {
     CpuFeatureScope avx_scope(this, AVX);
     XMMRegister original_dst = dst;
@@ -763,6 +786,7 @@ void TurboAssembler::I32x4TruncSatF64x2SZero(XMMRegister dst, XMMRegister src,
 void TurboAssembler::I32x4TruncSatF64x2UZero(XMMRegister dst, XMMRegister src,
                                              XMMRegister scratch,
                                              Register tmp) {
+  ASM_CODE_COMMENT(this);
   if (CpuFeatures::IsSupported(AVX)) {
     CpuFeatureScope avx_scope(this, AVX);
     vxorpd(scratch, scratch, scratch);
@@ -851,6 +875,7 @@ void TurboAssembler::I32x4ExtAddPairwiseI16x8S(XMMRegister dst, XMMRegister src,
 
 void TurboAssembler::I32x4ExtAddPairwiseI16x8U(XMMRegister dst, XMMRegister src,
                                                XMMRegister tmp) {
+  ASM_CODE_COMMENT(this);
   if (CpuFeatures::IsSupported(AVX)) {
     CpuFeatureScope avx_scope(this, AVX);
     // src = |a|b|c|d|e|f|g|h| (low)
@@ -928,6 +953,7 @@ void TurboAssembler::ShlPair(Register high, Register low, uint8_t shift) {
 }
 
 void TurboAssembler::ShlPair_cl(Register high, Register low) {
+  ASM_CODE_COMMENT(this);
   shld_cl(high, low);
   shl_cl(low);
   Label done;
@@ -951,6 +977,7 @@ void TurboAssembler::ShrPair(Register high, Register low, uint8_t shift) {
 }
 
 void TurboAssembler::ShrPair_cl(Register high, Register low) {
+  ASM_CODE_COMMENT(this);
   shrd_cl(low, high);
   shr_cl(high);
   Label done;
@@ -962,6 +989,7 @@ void TurboAssembler::ShrPair_cl(Register high, Register low) {
 }
 
 void TurboAssembler::SarPair(Register high, Register low, uint8_t shift) {
+  ASM_CODE_COMMENT(this);
   DCHECK_GE(63, shift);
   if (shift >= 32) {
     mov(low, high);
@@ -974,6 +1002,7 @@ void TurboAssembler::SarPair(Register high, Register low, uint8_t shift) {
 }
 
 void TurboAssembler::SarPair_cl(Register high, Register low) {
+  ASM_CODE_COMMENT(this);
   shrd_cl(low, high);
   sar_cl(high);
   Label done;
@@ -990,6 +1019,7 @@ void TurboAssembler::LoadMap(Register destination, Register object) {
 
 void MacroAssembler::CmpObjectType(Register heap_object, InstanceType type,
                                    Register map) {
+  ASM_CODE_COMMENT(this);
   LoadMap(map, heap_object);
   CmpInstanceType(map, type);
 }
@@ -1001,6 +1031,7 @@ void MacroAssembler::CmpInstanceType(Register map, InstanceType type) {
 void MacroAssembler::CmpInstanceTypeRange(Register map, Register scratch,
                                           InstanceType lower_limit,
                                           InstanceType higher_limit) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LT(lower_limit, higher_limit);
   movzx_w(scratch, FieldOperand(map, Map::kInstanceTypeOffset));
   lea(scratch, Operand(scratch, 0u - lower_limit));
@@ -1009,6 +1040,7 @@ void MacroAssembler::CmpInstanceTypeRange(Register map, Register scratch,
 
 void MacroAssembler::AssertSmi(Register object) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(equal, AbortReason::kOperandIsNotASmi);
   }
@@ -1016,6 +1048,7 @@ void MacroAssembler::AssertSmi(Register object) {
 
 void MacroAssembler::AssertConstructor(Register object) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotAConstructor);
     Push(object);
@@ -1029,6 +1062,7 @@ void MacroAssembler::AssertConstructor(Register object) {
 
 void MacroAssembler::AssertFunction(Register object, Register scratch) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotAFunction);
     Push(object);
@@ -1042,6 +1076,7 @@ void MacroAssembler::AssertFunction(Register object, Register scratch) {
 
 void MacroAssembler::AssertBoundFunction(Register object) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmiAndNotABoundFunction);
     Push(object);
@@ -1053,6 +1088,7 @@ void MacroAssembler::AssertBoundFunction(Register object) {
 
 void MacroAssembler::AssertGeneratorObject(Register object) {
   if (!FLAG_debug_code) return;
+  ASM_CODE_COMMENT(this);
 
   test(object, Immediate(kSmiTagMask));
   Check(not_equal, AbortReason::kOperandIsASmiAndNotAGeneratorObject);
@@ -1085,6 +1121,7 @@ void MacroAssembler::AssertGeneratorObject(Register object) {
 void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
                                                      Register scratch) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     Label done_checking;
     AssertNotSmi(object);
     CompareRoot(object, scratch, RootIndex::kUndefinedValue);
@@ -1098,18 +1135,21 @@ void MacroAssembler::AssertUndefinedOrAllocationSite(Register object,
 
 void MacroAssembler::AssertNotSmi(Register object) {
   if (FLAG_debug_code) {
+    ASM_CODE_COMMENT(this);
     test(object, Immediate(kSmiTagMask));
     Check(not_equal, AbortReason::kOperandIsASmi);
   }
 }
 
 void TurboAssembler::StubPrologue(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   push(ebp);  // Caller's frame pointer.
   mov(ebp, esp);
   push(Immediate(StackFrame::TypeToMarker(type)));
 }
 
 void TurboAssembler::Prologue() {
+  ASM_CODE_COMMENT(this);
   push(ebp);  // Caller's frame pointer.
   mov(ebp, esp);
   push(kContextRegister);                 // Callee's context.
@@ -1118,6 +1158,7 @@ void TurboAssembler::Prologue() {
 }
 
 void TurboAssembler::EnterFrame(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   push(ebp);
   mov(ebp, esp);
   if (!StackFrame::IsJavaScript(type)) {
@@ -1126,6 +1167,7 @@ void TurboAssembler::EnterFrame(StackFrame::Type type) {
 }
 
 void TurboAssembler::LeaveFrame(StackFrame::Type type) {
+  ASM_CODE_COMMENT(this);
   if (FLAG_debug_code && !StackFrame::IsJavaScript(type)) {
     cmp(Operand(ebp, CommonFrameConstants::kContextOrFrameTypeOffset),
         Immediate(StackFrame::TypeToMarker(type)));
@@ -1136,6 +1178,7 @@ void TurboAssembler::LeaveFrame(StackFrame::Type type) {
 
 #ifdef V8_OS_WIN
 void TurboAssembler::AllocateStackSpace(Register bytes_scratch) {
+  ASM_CODE_COMMENT(this);
   // In windows, we cannot increment the stack size by more than one page
   // (minimum page size is 4KB) without accessing at least one byte on the
   // page. Check this:
@@ -1157,6 +1200,7 @@ void TurboAssembler::AllocateStackSpace(Register bytes_scratch) {
 }
 
 void TurboAssembler::AllocateStackSpace(int bytes) {
+  ASM_CODE_COMMENT(this);
   DCHECK_GE(bytes, 0);
   while (bytes > kStackPageSize) {
     sub(esp, Immediate(kStackPageSize));
@@ -1170,6 +1214,7 @@ void TurboAssembler::AllocateStackSpace(int bytes) {
 
 void MacroAssembler::EnterExitFramePrologue(StackFrame::Type frame_type,
                                             Register scratch) {
+  ASM_CODE_COMMENT(this);
   DCHECK(frame_type == StackFrame::EXIT ||
          frame_type == StackFrame::BUILTIN_EXIT);
 
@@ -1203,6 +1248,7 @@ void MacroAssembler::EnterExitFramePrologue(StackFrame::Type frame_type,
 }
 
 void MacroAssembler::EnterExitFrameEpilogue(int argc, bool save_doubles) {
+  ASM_CODE_COMMENT(this);
   // Optionally save all XMM registers.
   if (save_doubles) {
     int space =
@@ -1230,6 +1276,7 @@ void MacroAssembler::EnterExitFrameEpilogue(int argc, bool save_doubles) {
 
 void MacroAssembler::EnterExitFrame(int argc, bool save_doubles,
                                     StackFrame::Type frame_type) {
+  ASM_CODE_COMMENT(this);
   EnterExitFramePrologue(frame_type, edi);
 
   // Set up argc and argv in callee-saved registers.
@@ -1247,6 +1294,7 @@ void MacroAssembler::EnterApiExitFrame(int argc, Register scratch) {
 }
 
 void MacroAssembler::LeaveExitFrame(bool save_doubles, bool pop_arguments) {
+  ASM_CODE_COMMENT(this);
   // Optionally restore all XMM registers.
   if (save_doubles) {
     const int offset = -ExitFrameConstants::kFixedFrameSizeFromFp;
@@ -1275,6 +1323,7 @@ void MacroAssembler::LeaveExitFrame(bool save_doubles, bool pop_arguments) {
 }
 
 void MacroAssembler::LeaveExitFrameEpilogue() {
+  ASM_CODE_COMMENT(this);
   // Clear the top frame.
   ExternalReference c_entry_fp_address =
       ExternalReference::Create(IsolateAddressId::kCEntryFPAddress, isolate());
@@ -1293,6 +1342,7 @@ void MacroAssembler::LeaveExitFrameEpilogue() {
 }
 
 void MacroAssembler::LeaveApiExitFrame() {
+  ASM_CODE_COMMENT(this);
   mov(esp, ebp);
   pop(ebp);
 
@@ -1300,6 +1350,7 @@ void MacroAssembler::LeaveApiExitFrame() {
 }
 
 void MacroAssembler::PushStackHandler(Register scratch) {
+  ASM_CODE_COMMENT(this);
   // Adjust this code if not the case.
   STATIC_ASSERT(StackHandlerConstants::kSize == 2 * kSystemPointerSize);
   STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
@@ -1316,6 +1367,7 @@ void MacroAssembler::PushStackHandler(Register scratch) {
 }
 
 void MacroAssembler::PopStackHandler(Register scratch) {
+  ASM_CODE_COMMENT(this);
   STATIC_ASSERT(StackHandlerConstants::kNextOffset == 0);
   ExternalReference handler_address =
       ExternalReference::Create(IsolateAddressId::kHandlerAddress, isolate());
@@ -1325,6 +1377,7 @@ void MacroAssembler::PopStackHandler(Register scratch) {
 
 void MacroAssembler::CallRuntime(const Runtime::Function* f, int num_arguments,
                                  SaveFPRegsMode save_doubles) {
+  ASM_CODE_COMMENT(this);
   // If the expected number of arguments of the runtime function is
   // constant, we check that the actual number of arguments match the
   // expectation.
@@ -1351,7 +1404,7 @@ void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
   //  For runtime functions with variable arguments:
   //  -- eax                    : number of  arguments
   // -----------------------------------
-
+  ASM_CODE_COMMENT(this);
   const Runtime::Function* function = Runtime::FunctionForId(fid);
   DCHECK_EQ(1, function->result_size);
   if (function->nargs >= 0) {
@@ -1366,6 +1419,7 @@ void MacroAssembler::TailCallRuntime(Runtime::FunctionId fid) {
 
 void MacroAssembler::JumpToExternalReference(const ExternalReference& ext,
                                              bool builtin_exit_frame) {
+  ASM_CODE_COMMENT(this);
   // Set the entry point and jump to the C entry runtime stub.
   Move(kRuntimeCallFunctionRegister, Immediate(ext));
   Handle<Code> code = CodeFactory::CEntry(isolate(), 1, SaveFPRegsMode::kIgnore,
@@ -1380,6 +1434,7 @@ void MacroAssembler::JumpToInstructionStream(Address entry) {
 void TurboAssembler::PrepareForTailCall(
     Register callee_args_count, Register caller_args_count, Register scratch0,
     Register scratch1, int number_of_temp_values_after_return_address) {
+  ASM_CODE_COMMENT(this);
   DCHECK(!AreAliased(callee_args_count, caller_args_count, scratch0, scratch1));
 
   // Calculate the destination address where we will put the return address
@@ -1431,6 +1486,7 @@ void TurboAssembler::PrepareForTailCall(
 }
 
 void MacroAssembler::CompareStackLimit(Register with, StackLimitKind kind) {
+  ASM_CODE_COMMENT(this);
   DCHECK(root_array_available());
   Isolate* isolate = this->isolate();
   // Address through the root register. No load is needed.
@@ -1448,6 +1504,7 @@ void MacroAssembler::CompareStackLimit(Register with, StackLimitKind kind) {
 void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch,
                                         Label* stack_overflow,
                                         bool include_receiver) {
+  ASM_CODE_COMMENT(this);
   DCHECK_NE(num_args, scratch);
   // Check the stack for overflow. We are not trying to catch
   // interruptions (e.g. debug break and preemption) here, so the "real stack
@@ -1474,55 +1531,56 @@ void MacroAssembler::StackOverflowCheck(Register num_args, Register scratch,
 void MacroAssembler::InvokePrologue(Register expected_parameter_count,
                                     Register actual_parameter_count,
                                     Label* done, InvokeType type) {
-  if (expected_parameter_count != actual_parameter_count) {
-    DCHECK_EQ(actual_parameter_count, eax);
-    DCHECK_EQ(expected_parameter_count, ecx);
-    Label regular_invoke;
-
-    // If the expected parameter count is equal to the adaptor sentinel, no need
-    // to push undefined value as arguments.
-    cmp(expected_parameter_count, Immediate(kDontAdaptArgumentsSentinel));
-    j(equal, &regular_invoke, Label::kFar);
-
-    // If overapplication or if the actual argument count is equal to the
-    // formal parameter count, no need to push extra undefined values.
-    sub(expected_parameter_count, actual_parameter_count);
-    j(less_equal, &regular_invoke, Label::kFar);
-
-    // We need to preserve edx, edi, esi and ebx.
-    movd(xmm0, edx);
-    movd(xmm1, edi);
-    movd(xmm2, esi);
-    movd(xmm3, ebx);
-
-    Label stack_overflow;
-    StackOverflowCheck(expected_parameter_count, edx, &stack_overflow);
-
-    Register scratch = esi;
-
-    // Underapplication. Move the arguments already in the stack, including the
-    // receiver and the return address.
-    {
-      Label copy, check;
-      Register src = edx, dest = esp, num = edi, current = ebx;
-      mov(src, esp);
-      lea(scratch,
-          Operand(expected_parameter_count, times_system_pointer_size, 0));
-      AllocateStackSpace(scratch);
-      // Extra words are the receiver and the return address (if a jump).
-      int extra_words = type == InvokeType::kCall ? 1 : 2;
-      lea(num, Operand(eax, extra_words));  // Number of words to copy.
-      Move(current, 0);
-      // Fall-through to the loop body because there are non-zero words to copy.
-      bind(&copy);
-      mov(scratch, Operand(src, current, times_system_pointer_size, 0));
-      mov(Operand(dest, current, times_system_pointer_size, 0), scratch);
-      inc(current);
-      bind(&check);
-      cmp(current, num);
-      j(less, &copy);
-      lea(edx, Operand(esp, num, times_system_pointer_size, 0));
-    }
+  if (expected_parameter_count == actual_parameter_count) return;
+  ASM_CODE_COMMENT(this);
+  DCHECK_EQ(actual_parameter_count, eax);
+  DCHECK_EQ(expected_parameter_count, ecx);
+  Label regular_invoke;
+
+  // If the expected parameter count is equal to the adaptor sentinel, no need
+  // to push undefined value as arguments.
+  cmp(expected_parameter_count, Immediate(kDontAdaptArgumentsSentinel));
+  j(equal, &regular_invoke, Label::kFar);
+
+  // If overapplication or if the actual argument count is equal to the
+  // formal parameter count, no need to push extra undefined values.
+  sub(expected_parameter_count, actual_parameter_count);
+  j(less_equal, &regular_invoke, Label::kFar);
+
+  // We need to preserve edx, edi, esi and ebx.
+  movd(xmm0, edx);
+  movd(xmm1, edi);
+  movd(xmm2, esi);
+  movd(xmm3, ebx);
+
+  Label stack_overflow;
+  StackOverflowCheck(expected_parameter_count, edx, &stack_overflow);
+
+  Register scratch = esi;
+
+  // Underapplication. Move the arguments already in the stack, including the
+  // receiver and the return address.
+  {
+    Label copy, check;
+    Register src = edx, dest = esp, num = edi, current = ebx;
+    mov(src, esp);
+    lea(scratch,
+        Operand(expected_parameter_count, times_system_pointer_size, 0));
+    AllocateStackSpace(scratch);
+    // Extra words are the receiver and the return address (if a jump).
+    int extra_words = type == InvokeType::kCall ? 1 : 2;
+    lea(num, Operand(eax, extra_words));  // Number of words to copy.
+    Move(current, 0);
+    // Fall-through to the loop body because there are non-zero words to copy.
+    bind(&copy);
+    mov(scratch, Operand(src, current, times_system_pointer_size, 0));
+    mov(Operand(dest, current, times_system_pointer_size, 0), scratch);
+    inc(current);
+    bind(&check);
+    cmp(current, num);
+    j(less, &copy);
+    lea(edx, Operand(esp, num, times_system_pointer_size, 0));
+  }
 
     // Fill remaining expected arguments with undefined values.
     movd(ebx, xmm3);  // Restore root.
@@ -1552,12 +1610,12 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
     }
 
     bind(&regular_invoke);
-  }
 }
 
 void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
                                              Register expected_parameter_count,
                                              Register actual_parameter_count) {
+  ASM_CODE_COMMENT(this);
   FrameScope frame(this, has_frame() ? StackFrame::NONE : StackFrame::INTERNAL);
   SmiTag(expected_parameter_count);
   Push(expected_parameter_count);
@@ -1590,6 +1648,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
                                         Register expected_parameter_count,
                                         Register actual_parameter_count,
                                         InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK_IMPLIES(type == InvokeType::kCall, has_frame());
   DCHECK_EQ(function, edi);
@@ -1643,6 +1702,7 @@ void MacroAssembler::InvokeFunctionCode(Register function, Register new_target,
 void MacroAssembler::InvokeFunction(Register fun, Register new_target,
                                     Register actual_parameter_count,
                                     InvokeType type) {
+  ASM_CODE_COMMENT(this);
   // You can't call a function without a valid frame.
   DCHECK(type == InvokeType::kJump || has_frame());
 
@@ -1660,6 +1720,7 @@ void MacroAssembler::LoadGlobalProxy(Register dst) {
 }
 
 void MacroAssembler::LoadNativeContextSlot(Register destination, int index) {
+  ASM_CODE_COMMENT(this);
   // Load the native context from the current context.
   LoadMap(destination, esi);
   mov(destination,
@@ -1984,6 +2045,7 @@ void TurboAssembler::Popcnt(Register dst, Operand src) {
 }
 
 void MacroAssembler::LoadWeakValue(Register in_out, Label* target_if_cleared) {
+  ASM_CODE_COMMENT(this);
   cmp(in_out, Immediate(kClearedWeakHeapObjectLower32));
   j(equal, target_if_cleared);
 
@@ -1994,6 +2056,7 @@ void MacroAssembler::EmitIncrementCounter(StatsCounter* counter, int value,
                                           Register scratch) {
   DCHECK_GT(value, 0);
   if (FLAG_native_code_counters && counter->Enabled()) {
+    ASM_CODE_COMMENT(this);
     Operand operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter), scratch);
     if (value == 1) {
@@ -2008,6 +2071,7 @@ void MacroAssembler::EmitDecrementCounter(StatsCounter* counter, int value,
                                           Register scratch) {
   DCHECK_GT(value, 0);
   if (FLAG_native_code_counters && counter->Enabled()) {
+    ASM_CODE_COMMENT(this);
     Operand operand =
         ExternalReferenceAsOperand(ExternalReference::Create(counter), scratch);
     if (value == 1) {
@@ -2035,6 +2099,7 @@ void TurboAssembler::Check(Condition cc, AbortReason reason) {
 }
 
 void TurboAssembler::CheckStackAlignment() {
+  ASM_CODE_COMMENT(this);
   int frame_alignment = base::OS::ActivationFrameAlignment();
   int frame_alignment_mask = frame_alignment - 1;
   if (frame_alignment > kSystemPointerSize) {
@@ -2086,6 +2151,7 @@ void TurboAssembler::Abort(AbortReason reason) {
 }
 
 void TurboAssembler::PrepareCallCFunction(int num_arguments, Register scratch) {
+  ASM_CODE_COMMENT(this);
   int frame_alignment = base::OS::ActivationFrameAlignment();
   if (frame_alignment != 0) {
     // Make stack end at alignment and make room for num_arguments words
@@ -2108,6 +2174,7 @@ void TurboAssembler::CallCFunction(ExternalReference function,
 }
 
 void TurboAssembler::CallCFunction(Register function, int num_arguments) {
+  ASM_CODE_COMMENT(this);
   DCHECK_LE(num_arguments, kMaxCParameters);
   DCHECK(has_frame());
   // Check stack alignment.
@@ -2166,6 +2233,7 @@ void TurboAssembler::PushPC() {
 }
 
 void TurboAssembler::Call(Handle<Code> code_object, RelocInfo::Mode rmode) {
+  ASM_CODE_COMMENT(this);
   DCHECK_IMPLIES(options().isolate_independent_code,
                  Builtins::IsIsolateIndependentBuiltin(*code_object));
   if (options().inline_offheap_trampolines) {
@@ -2181,6 +2249,7 @@ void TurboAssembler::Call(Handle<Code> code_object, RelocInfo::Mode rmode) {
 }
 
 void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
+  ASM_CODE_COMMENT(this);
   STATIC_ASSERT(kSystemPointerSize == 4);
   STATIC_ASSERT(kSmiShiftSize == 0);
   STATIC_ASSERT(kSmiTagSize == 1);
@@ -2196,22 +2265,26 @@ void TurboAssembler::LoadEntryFromBuiltinIndex(Register builtin_index) {
 }
 
 void TurboAssembler::CallBuiltinByIndex(Register builtin_index) {
+  ASM_CODE_COMMENT(this);
   LoadEntryFromBuiltinIndex(builtin_index);
   call(builtin_index);
 }
 
 void TurboAssembler::CallBuiltin(Builtin builtin) {
-  RecordCommentForOffHeapTrampoline(builtin);
+  ASM_CODE_COMMENT_STRING(this, CommentForOffHeapTrampoline("call", builtin));
+  DCHECK(Builtins::IsBuiltinId(builtin));
   call(BuiltinEntry(builtin), RelocInfo::OFF_HEAP_TARGET);
 }
 
 Operand TurboAssembler::EntryFromBuiltinAsOperand(Builtin builtin) {
+  ASM_CODE_COMMENT(this);
   return Operand(kRootRegister,
                  IsolateData::builtin_entry_slot_offset(builtin));
 }
 
 void TurboAssembler::LoadCodeObjectEntry(Register destination,
                                          Register code_object) {
+  ASM_CODE_COMMENT(this);
   // Code objects are called differently depending on whether we are generating
   // builtin code (which will later be embedded into the binary) or compiling
   // user JS code at runtime.
@@ -2254,11 +2327,13 @@ void TurboAssembler::LoadCodeObjectEntry(Register destination,
 }
 
 void TurboAssembler::CallCodeObject(Register code_object) {
+  ASM_CODE_COMMENT(this);
   LoadCodeObjectEntry(code_object, code_object);
   call(code_object);
 }
 
 void TurboAssembler::JumpCodeObject(Register code_object, JumpMode jump_mode) {
+  ASM_CODE_COMMENT(this);
   LoadCodeObjectEntry(code_object, code_object);
   switch (jump_mode) {
     case JumpMode::kJump:
@@ -2294,6 +2369,7 @@ void TurboAssembler::Jump(Handle<Code> code_object, RelocInfo::Mode rmode) {
 }
 
 void TurboAssembler::RetpolineCall(Register reg) {
+  ASM_CODE_COMMENT(this);
   Label setup_return, setup_target, inner_indirect_branch, capture_spec;
 
   jmp(&setup_return);  // Jump past the entire retpoline below.
@@ -2314,6 +2390,7 @@ void TurboAssembler::RetpolineCall(Register reg) {
 }
 
 void TurboAssembler::RetpolineCall(Address destination, RelocInfo::Mode rmode) {
+  ASM_CODE_COMMENT(this);
   Label setup_return, setup_target, inner_indirect_branch, capture_spec;
 
   jmp(&setup_return);  // Jump past the entire retpoline below.
@@ -2334,6 +2411,7 @@ void TurboAssembler::RetpolineCall(Address destination, RelocInfo::Mode rmode) {
 }
 
 void TurboAssembler::RetpolineJump(Register reg) {
+  ASM_CODE_COMMENT(this);
   Label setup_target, capture_spec;
 
   call(&setup_target);
@@ -2350,6 +2428,7 @@ void TurboAssembler::RetpolineJump(Register reg) {
 void TurboAssembler::CheckPageFlag(Register object, Register scratch, int mask,
                                    Condition cc, Label* condition_met,
                                    Label::Distance condition_met_distance) {
+  ASM_CODE_COMMENT(this);
   DCHECK(cc == zero || cc == not_zero);
   if (scratch == object) {
     and_(scratch, Immediate(~kPageAlignmentMask));
@@ -2366,6 +2445,7 @@ void TurboAssembler::CheckPageFlag(Register object, Register scratch, int mask,
 }
 
 void TurboAssembler::ComputeCodeStartAddress(Register dst) {
+  ASM_CODE_COMMENT(this);
   // In order to get the address of the current instruction, we first need
   // to use a call and then use a pop, thus pushing the return address to
   // the stack and then popping it into the register.
@@ -2382,6 +2462,7 @@ void TurboAssembler::ComputeCodeStartAddress(Register dst) {
 void TurboAssembler::CallForDeoptimization(Builtin target, int, Label* exit,
                                            DeoptimizeKind kind, Label* ret,
                                            Label*) {
+  ASM_CODE_COMMENT(this);
   CallBuiltin(target);
   DCHECK_EQ(SizeOfCodeGeneratedSince(exit),
             (kind == DeoptimizeKind::kLazy)
diff --git a/src/codegen/macro-assembler.h b/src/codegen/macro-assembler.h
index e7aa2eb1481..cfa7a4d3416 100644
--- a/src/codegen/macro-assembler.h
+++ b/src/codegen/macro-assembler.h
@@ -83,7 +83,13 @@ static constexpr int kMaxCParameters = 256;
 class V8_NODISCARD FrameScope {
  public:
   explicit FrameScope(TurboAssembler* tasm, StackFrame::Type type)
-      : tasm_(tasm), type_(type), old_has_frame_(tasm->has_frame()) {
+      :
+#ifdef V8_CODE_COMMENTS
+        comment_(tasm, frame_name(type)),
+#endif
+        tasm_(tasm),
+        type_(type),
+        old_has_frame_(tasm->has_frame()) {
     tasm->set_has_frame(true);
     if (type != StackFrame::MANUAL && type_ != StackFrame::NONE) {
       tasm->EnterFrame(type);
@@ -98,9 +104,30 @@ class V8_NODISCARD FrameScope {
   }
 
  private:
+#ifdef V8_CODE_COMMENTS
+  const char* frame_name(StackFrame::Type type) {
+    switch (type) {
+      case StackFrame::NONE:
+        return "Frame: NONE";
+      case StackFrame::MANUAL:
+        return "Frame: MANUAL";
+#define FRAME_TYPE_CASE(type, field) \
+  case StackFrame::type:             \
+    return "Frame: " #type;
+        STACK_FRAME_TYPE_LIST(FRAME_TYPE_CASE)
+#undef FRAME_TYPE_CASE
+      case StackFrame::NUMBER_OF_TYPES:
+        break;
+    }
+    return "Frame";
+  }
+
+  Assembler::CodeComment comment_;
+#endif  // V8_CODE_COMMENTS
+
   TurboAssembler* tasm_;
-  StackFrame::Type type_;
-  bool old_has_frame_;
+  StackFrame::Type const type_;
+  bool const old_has_frame_;
 };
 
 class V8_NODISCARD FrameAndConstantPoolScope {
diff --git a/src/codegen/x64/macro-assembler-x64.cc b/src/codegen/x64/macro-assembler-x64.cc
index e5be25da326..8f73bf3778c 100644
--- a/src/codegen/x64/macro-assembler-x64.cc
+++ b/src/codegen/x64/macro-assembler-x64.cc
@@ -3015,8 +3015,11 @@ void MacroAssembler::StackOverflowCheck(
 void MacroAssembler::InvokePrologue(Register expected_parameter_count,
                                     Register actual_parameter_count,
                                     Label* done, InvokeType type) {
-  if (expected_parameter_count != actual_parameter_count) {
     ASM_CODE_COMMENT(this);
+    if (expected_parameter_count == actual_parameter_count) {
+      Move(rax, actual_parameter_count);
+      return;
+    }
     Label regular_invoke;
     // If the expected parameter count is equal to the adaptor sentinel, no need
     // to push undefined value as arguments.
@@ -3076,9 +3079,6 @@ void MacroAssembler::InvokePrologue(Register expected_parameter_count,
       int3();  // This should be unreachable.
     }
     bind(&regular_invoke);
-  } else {
-    Move(rax, actual_parameter_count);
-  }
 }
 
 void MacroAssembler::CallDebugOnFunctionCall(Register fun, Register new_target,
-- 
2.35.1

